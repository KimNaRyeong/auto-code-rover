[
    {
        "rel_file_path": "sklearn/ensemble/weight_boosting.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/ensemble/weight_boosting.py",
        "start": 1,
        "end": 1122,
        "class_name": null,
        "method_name": null,
        "code": "1 \"\"\"Weight Boosting\n2 \n3 This module contains weight boosting estimators for both classification and\n4 regression.\n5 \n6 The module structure is the following:\n7 \n8 - The ``BaseWeightBoosting`` base class implements a common ``fit`` method\n9   for all the estimators in the module. Regression and classification\n10   only differ from each other in the loss function that is optimized.\n11 \n12 - ``AdaBoostClassifier`` implements adaptive boosting (AdaBoost-SAMME) for\n13   classification problems.\n14 \n15 - ``AdaBoostRegressor`` implements adaptive boosting (AdaBoost.R2) for\n16   regression problems.\n17 \"\"\"\n18 \n19 # Authors: Noel Dawe <noel@dawe.me>\n20 #          Gilles Louppe <g.louppe@gmail.com>\n21 #          Hamzeh Alsalhi <ha258@cornell.edu>\n22 #          Arnaud Joly <arnaud.v.joly@gmail.com>\n23 #\n24 # License: BSD 3 clause\n25 \n26 from abc import ABCMeta, abstractmethod\n27 \n28 import numpy as np\n29 \n30 from scipy.special import xlogy\n31 \n32 from .base import BaseEnsemble\n33 from ..base import ClassifierMixin, RegressorMixin, is_regressor, is_classifier\n34 \n35 from .forest import BaseForest\n36 from ..tree import DecisionTreeClassifier, DecisionTreeRegressor\n37 from ..tree.tree import BaseDecisionTree\n38 from ..tree._tree import DTYPE\n39 from ..utils import check_array, check_X_y, check_random_state\n40 from ..utils.extmath import stable_cumsum\n41 from ..metrics import accuracy_score, r2_score\n42 from sklearn.utils.validation import has_fit_parameter, check_is_fitted\n43 \n44 __all__ = [\n45     'AdaBoostClassifier',\n46     'AdaBoostRegressor',\n47 ]\n48 \n49 \n50 class BaseWeightBoosting(BaseEnsemble, metaclass=ABCMeta):\n51     \"\"\"Base class for AdaBoost estimators.\n52 \n53     Warning: This class should not be used directly. Use derived classes\n54     instead.\n55     \"\"\"\n56 \n57     @abstractmethod\n58     def __init__(self,\n59                  base_estimator=None,\n60                  n_estimators=50,\n61                  estimator_params=tuple(),\n62                  learning_rate=1.,\n63                  random_state=None):\n64 \n65         super().__init__(\n66             base_estimator=base_estimator,\n67             n_estimators=n_estimators,\n68             estimator_params=estimator_params)\n69 \n70         self.learning_rate = learning_rate\n71         self.random_state = random_state\n72 \n73     def fit(self, X, y, sample_weight=None):\n74         \"\"\"Build a boosted classifier/regressor from the training set (X, y).\n75 \n76         Parameters\n77         ----------\n78         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n79             The training input samples. Sparse matrix can be CSC, CSR, COO,\n80             DOK, or LIL. COO, DOK, and LIL are converted to CSR. The dtype is\n81             forced to DTYPE from tree._tree if the base classifier of this\n82             ensemble weighted boosting classifier is a tree or forest.\n83 \n84         y : array-like of shape = [n_samples]\n85             The target values (class labels in classification, real numbers in\n86             regression).\n87 \n88         sample_weight : array-like of shape = [n_samples], optional\n89             Sample weights. If None, the sample weights are initialized to\n90             1 / n_samples.\n91 \n92         Returns\n93         -------\n94         self : object\n95         \"\"\"\n96         # Check parameters\n97         if self.learning_rate <= 0:\n98             raise ValueError(\"learning_rate must be greater than zero\")\n99 \n100         if (self.base_estimator is None or\n101                 isinstance(self.base_estimator, (BaseDecisionTree,\n102                                                  BaseForest))):\n103             dtype = DTYPE\n104             accept_sparse = 'csc'\n105         else:\n106             dtype = None\n107             accept_sparse = ['csr', 'csc']\n108 \n109         X, y = check_X_y(X, y, accept_sparse=accept_sparse, dtype=dtype,\n110                          y_numeric=is_regressor(self))\n111 \n112         if sample_weight is None:\n113             # Initialize weights to 1 / n_samples\n114             sample_weight = np.empty(X.shape[0], dtype=np.float64)\n115             sample_weight[:] = 1. / X.shape[0]\n116         else:\n117             sample_weight = check_array(sample_weight, ensure_2d=False)\n118             # Normalize existing weights\n119             sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)\n120 \n121             # Check that the sample weights sum is positive\n122             if sample_weight.sum() <= 0:\n123                 raise ValueError(\n124                     \"Attempting to fit with a non-positive \"\n125                     \"weighted number of samples.\")\n126 \n127         # Check parameters\n128         self._validate_estimator()\n129 \n130         # Clear any previous fit results\n131         self.estimators_ = []\n132         self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float64)\n133         self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float64)\n134 \n135         random_state = check_random_state(self.random_state)\n136 \n137         for iboost in range(self.n_estimators):\n138             # Boosting step\n139             sample_weight, estimator_weight, estimator_error = self._boost(\n140                 iboost,\n141                 X, y,\n142                 sample_weight,\n143                 random_state)\n144 \n145             # Early termination\n146             if sample_weight is None:\n147                 break\n148 \n149             self.estimator_weights_[iboost] = estimator_weight\n150             self.estimator_errors_[iboost] = estimator_error\n151 \n152             # Stop if error is zero\n153             if estimator_error == 0:\n154                 break\n155 \n156             sample_weight_sum = np.sum(sample_weight)\n157 \n158             # Stop if the sum of sample weights has become non-positive\n159             if sample_weight_sum <= 0:\n160                 break\n161 \n162             if iboost < self.n_estimators - 1:\n163                 # Normalize\n164                 sample_weight /= sample_weight_sum\n165 \n166         return self\n167 \n168     @abstractmethod\n169     def _boost(self, iboost, X, y, sample_weight, random_state):\n170         \"\"\"Implement a single boost.\n171 \n172         Warning: This method needs to be overridden by subclasses.\n173 \n174         Parameters\n175         ----------\n176         iboost : int\n177             The index of the current boost iteration.\n178 \n179         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n180             The training input samples. Sparse matrix can be CSC, CSR, COO,\n181             DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n182 \n183         y : array-like of shape = [n_samples]\n184             The target values (class labels).\n185 \n186         sample_weight : array-like of shape = [n_samples]\n187             The current sample weights.\n188 \n189         random_state : RandomState\n190             The current random number generator\n191 \n192         Returns\n193         -------\n194         sample_weight : array-like of shape = [n_samples] or None\n195             The reweighted sample weights.\n196             If None then boosting has terminated early.\n197 \n198         estimator_weight : float\n199             The weight for the current boost.\n200             If None then boosting has terminated early.\n201 \n202         error : float\n203             The classification error for the current boost.\n204             If None then boosting has terminated early.\n205         \"\"\"\n206         pass\n207 \n208     def staged_score(self, X, y, sample_weight=None):\n209         \"\"\"Return staged scores for X, y.\n210 \n211         This generator method yields the ensemble score after each iteration of\n212         boosting and therefore allows monitoring, such as to determine the\n213         score on a test set after each boost.\n214 \n215         Parameters\n216         ----------\n217         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n218             The training input samples. Sparse matrix can be CSC, CSR, COO,\n219             DOK, or LIL. DOK and LIL are converted to CSR.\n220 \n221         y : array-like, shape = [n_samples]\n222             Labels for X.\n223 \n224         sample_weight : array-like, shape = [n_samples], optional\n225             Sample weights.\n226 \n227         Returns\n228         -------\n229         z : float\n230         \"\"\"\n231         for y_pred in self.staged_predict(X):\n232             if is_classifier(self):\n233                 yield accuracy_score(y, y_pred, sample_weight=sample_weight)\n234             else:\n235                 yield r2_score(y, y_pred, sample_weight=sample_weight)\n236 \n237     @property\n238     def feature_importances_(self):\n239         \"\"\"Return the feature importances (the higher, the more important the\n240            feature).\n241 \n242         Returns\n243         -------\n244         feature_importances_ : array, shape = [n_features]\n245         \"\"\"\n246         if self.estimators_ is None or len(self.estimators_) == 0:\n247             raise ValueError(\"Estimator not fitted, \"\n248                              \"call `fit` before `feature_importances_`.\")\n249 \n250         try:\n251             norm = self.estimator_weights_.sum()\n252             return (sum(weight * clf.feature_importances_ for weight, clf\n253                     in zip(self.estimator_weights_, self.estimators_))\n254                     / norm)\n255 \n256         except AttributeError:\n257             raise AttributeError(\n258                 \"Unable to compute feature importances \"\n259                 \"since base_estimator does not have a \"\n260                 \"feature_importances_ attribute\")\n261 \n262     def _validate_X_predict(self, X):\n263         \"\"\"Ensure that X is in the proper format\"\"\"\n264         if (self.base_estimator is None or\n265                 isinstance(self.base_estimator,\n266                            (BaseDecisionTree, BaseForest))):\n267             X = check_array(X, accept_sparse='csr', dtype=DTYPE)\n268 \n269         else:\n270             X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n271 \n272         return X\n273 \n274 \n275 def _samme_proba(estimator, n_classes, X):\n276     \"\"\"Calculate algorithm 4, step 2, equation c) of Zhu et al [1].\n277 \n278     References\n279     ----------\n280     .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n281 \n282     \"\"\"\n283     proba = estimator.predict_proba(X)\n284 \n285     # Displace zero probabilities so the log is defined.\n286     # Also fix negative elements which may occur with\n287     # negative sample weights.\n288     np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)\n289     log_proba = np.log(proba)\n290 \n291     return (n_classes - 1) * (log_proba - (1. / n_classes)\n292                               * log_proba.sum(axis=1)[:, np.newaxis])\n293 \n294 \n295 class AdaBoostClassifier(BaseWeightBoosting, ClassifierMixin):\n296     \"\"\"An AdaBoost classifier.\n297 \n298     An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\n299     classifier on the original dataset and then fits additional copies of the\n300     classifier on the same dataset but where the weights of incorrectly\n301     classified instances are adjusted such that subsequent classifiers focus\n302     more on difficult cases.\n303 \n304     This class implements the algorithm known as AdaBoost-SAMME [2].\n305 \n306     Read more in the :ref:`User Guide <adaboost>`.\n307 \n308     Parameters\n309     ----------\n310     base_estimator : object, optional (default=None)\n311         The base estimator from which the boosted ensemble is built.\n312         Support for sample weighting is required, as well as proper\n313         ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n314         the base estimator is ``DecisionTreeClassifier(max_depth=1)``\n315 \n316     n_estimators : integer, optional (default=50)\n317         The maximum number of estimators at which boosting is terminated.\n318         In case of perfect fit, the learning procedure is stopped early.\n319 \n320     learning_rate : float, optional (default=1.)\n321         Learning rate shrinks the contribution of each classifier by\n322         ``learning_rate``. There is a trade-off between ``learning_rate`` and\n323         ``n_estimators``.\n324 \n325     algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')\n326         If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n327         ``base_estimator`` must support calculation of class probabilities.\n328         If 'SAMME' then use the SAMME discrete boosting algorithm.\n329         The SAMME.R algorithm typically converges faster than SAMME,\n330         achieving a lower test error with fewer boosting iterations.\n331 \n332     random_state : int, RandomState instance or None, optional (default=None)\n333         If int, random_state is the seed used by the random number generator;\n334         If RandomState instance, random_state is the random number generator;\n335         If None, the random number generator is the RandomState instance used\n336         by `np.random`.\n337 \n338     Attributes\n339     ----------\n340     estimators_ : list of classifiers\n341         The collection of fitted sub-estimators.\n342 \n343     classes_ : array of shape = [n_classes]\n344         The classes labels.\n345 \n346     n_classes_ : int\n347         The number of classes.\n348 \n349     estimator_weights_ : array of floats\n350         Weights for each estimator in the boosted ensemble.\n351 \n352     estimator_errors_ : array of floats\n353         Classification error for each estimator in the boosted\n354         ensemble.\n355 \n356     feature_importances_ : array of shape = [n_features]\n357         The feature importances if supported by the ``base_estimator``.\n358 \n359     See also\n360     --------\n361     AdaBoostRegressor, GradientBoostingClassifier,\n362     sklearn.tree.DecisionTreeClassifier\n363 \n364     References\n365     ----------\n366     .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n367            on-Line Learning and an Application to Boosting\", 1995.\n368 \n369     .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n370 \n371     \"\"\"\n372     def __init__(self,\n373                  base_estimator=None,\n374                  n_estimators=50,\n375                  learning_rate=1.,\n376                  algorithm='SAMME.R',\n377                  random_state=None):\n378 \n379         super().__init__(\n380             base_estimator=base_estimator,\n381             n_estimators=n_estimators,\n382             learning_rate=learning_rate,\n383             random_state=random_state)\n384 \n385         self.algorithm = algorithm\n386 \n387     def fit(self, X, y, sample_weight=None):\n388         \"\"\"Build a boosted classifier from the training set (X, y).\n389 \n390         Parameters\n391         ----------\n392         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n393             The training input samples. Sparse matrix can be CSC, CSR, COO,\n394             DOK, or LIL. DOK and LIL are converted to CSR.\n395 \n396         y : array-like of shape = [n_samples]\n397             The target values (class labels).\n398 \n399         sample_weight : array-like of shape = [n_samples], optional\n400             Sample weights. If None, the sample weights are initialized to\n401             ``1 / n_samples``.\n402 \n403         Returns\n404         -------\n405         self : object\n406         \"\"\"\n407         # Check that algorithm is supported\n408         if self.algorithm not in ('SAMME', 'SAMME.R'):\n409             raise ValueError(\"algorithm %s is not supported\" % self.algorithm)\n410 \n411         # Fit\n412         return super().fit(X, y, sample_weight)\n413 \n414     def _validate_estimator(self):\n415         \"\"\"Check the estimator and set the base_estimator_ attribute.\"\"\"\n416         super()._validate_estimator(\n417             default=DecisionTreeClassifier(max_depth=1))\n418 \n419         #  SAMME-R requires predict_proba-enabled base estimators\n420         if self.algorithm == 'SAMME.R':\n421             if not hasattr(self.base_estimator_, 'predict_proba'):\n422                 raise TypeError(\n423                     \"AdaBoostClassifier with algorithm='SAMME.R' requires \"\n424                     \"that the weak learner supports the calculation of class \"\n425                     \"probabilities with a predict_proba method.\\n\"\n426                     \"Please change the base estimator or set \"\n427                     \"algorithm='SAMME' instead.\")\n428         if not has_fit_parameter(self.base_estimator_, \"sample_weight\"):\n429             raise ValueError(\"%s doesn't support sample_weight.\"\n430                              % self.base_estimator_.__class__.__name__)\n431 \n432     def _boost(self, iboost, X, y, sample_weight, random_state):\n433         \"\"\"Implement a single boost.\n434 \n435         Perform a single boost according to the real multi-class SAMME.R\n436         algorithm or to the discrete SAMME algorithm and return the updated\n437         sample weights.\n438 \n439         Parameters\n440         ----------\n441         iboost : int\n442             The index of the current boost iteration.\n443 \n444         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n445             The training input samples. Sparse matrix can be CSC, CSR, COO,\n446             DOK, or LIL. DOK and LIL are converted to CSR.\n447 \n448         y : array-like of shape = [n_samples]\n449             The target values (class labels).\n450 \n451         sample_weight : array-like of shape = [n_samples]\n452             The current sample weights.\n453 \n454         random_state : RandomState\n455             The current random number generator\n456 \n457         Returns\n458         -------\n459         sample_weight : array-like of shape = [n_samples] or None\n460             The reweighted sample weights.\n461             If None then boosting has terminated early.\n462 \n463         estimator_weight : float\n464             The weight for the current boost.\n465             If None then boosting has terminated early.\n466 \n467         estimator_error : float\n468             The classification error for the current boost.\n469             If None then boosting has terminated early.\n470         \"\"\"\n471         if self.algorithm == 'SAMME.R':\n472             return self._boost_real(iboost, X, y, sample_weight, random_state)\n473 \n474         else:  # elif self.algorithm == \"SAMME\":\n475             return self._boost_discrete(iboost, X, y, sample_weight,\n476                                         random_state)\n477 \n478     def _boost_real(self, iboost, X, y, sample_weight, random_state):\n479         \"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\n480         estimator = self._make_estimator(random_state=random_state)\n481 \n482         estimator.fit(X, y, sample_weight=sample_weight)\n483 \n484         y_predict_proba = estimator.predict_proba(X)\n485 \n486         if iboost == 0:\n487             self.classes_ = getattr(estimator, 'classes_', None)\n488             self.n_classes_ = len(self.classes_)\n489 \n490         y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1),\n491                                        axis=0)\n492 \n493         # Instances incorrectly classified\n494         incorrect = y_predict != y\n495 \n496         # Error fraction\n497         estimator_error = np.mean(\n498             np.average(incorrect, weights=sample_weight, axis=0))\n499 \n500         # Stop if classification is perfect\n501         if estimator_error <= 0:\n502             return sample_weight, 1., 0.\n503 \n504         # Construct y coding as described in Zhu et al [2]:\n505         #\n506         #    y_k = 1 if c == k else -1 / (K - 1)\n507         #\n508         # where K == n_classes_ and c, k in [0, K) are indices along the second\n509         # axis of the y coding with c being the index corresponding to the true\n510         # class label.\n511         n_classes = self.n_classes_\n512         classes = self.classes_\n513         y_codes = np.array([-1. / (n_classes - 1), 1.])\n514         y_coding = y_codes.take(classes == y[:, np.newaxis])\n515 \n516         # Displace zero probabilities so the log is defined.\n517         # Also fix negative elements which may occur with\n518         # negative sample weights.\n519         proba = y_predict_proba  # alias for readability\n520         np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)\n521 \n522         # Boost weight using multi-class AdaBoost SAMME.R alg\n523         estimator_weight = (-1. * self.learning_rate\n524                             * ((n_classes - 1.) / n_classes)\n525                             * xlogy(y_coding, y_predict_proba).sum(axis=1))\n526 \n527         # Only boost the weights if it will fit again\n528         if not iboost == self.n_estimators - 1:\n529             # Only boost positive weights\n530             sample_weight *= np.exp(estimator_weight *\n531                                     ((sample_weight > 0) |\n532                                      (estimator_weight < 0)))\n533 \n534         return sample_weight, 1., estimator_error\n535 \n536     def _boost_discrete(self, iboost, X, y, sample_weight, random_state):\n537         \"\"\"Implement a single boost using the SAMME discrete algorithm.\"\"\"\n538         estimator = self._make_estimator(random_state=random_state)\n539 \n540         estimator.fit(X, y, sample_weight=sample_weight)\n541 \n542         y_predict = estimator.predict(X)\n543 \n544         if iboost == 0:\n545             self.classes_ = getattr(estimator, 'classes_', None)\n546             self.n_classes_ = len(self.classes_)\n547 \n548         # Instances incorrectly classified\n549         incorrect = y_predict != y\n550 \n551         # Error fraction\n552         estimator_error = np.mean(\n553             np.average(incorrect, weights=sample_weight, axis=0))\n554 \n555         # Stop if classification is perfect\n556         if estimator_error <= 0:\n557             return sample_weight, 1., 0.\n558 \n559         n_classes = self.n_classes_\n560 \n561         # Stop if the error is at least as bad as random guessing\n562         if estimator_error >= 1. - (1. / n_classes):\n563             self.estimators_.pop(-1)\n564             if len(self.estimators_) == 0:\n565                 raise ValueError('BaseClassifier in AdaBoostClassifier '\n566                                  'ensemble is worse than random, ensemble '\n567                                  'can not be fit.')\n568             return None, None, None\n569 \n570         # Boost weight using multi-class AdaBoost SAMME alg\n571         estimator_weight = self.learning_rate * (\n572             np.log((1. - estimator_error) / estimator_error) +\n573             np.log(n_classes - 1.))\n574 \n575         # Only boost the weights if I will fit again\n576         if not iboost == self.n_estimators - 1:\n577             # Only boost positive weights\n578             sample_weight *= np.exp(estimator_weight * incorrect *\n579                                     ((sample_weight > 0) |\n580                                      (estimator_weight < 0)))\n581 \n582         return sample_weight, estimator_weight, estimator_error\n583 \n584     def predict(self, X):\n585         \"\"\"Predict classes for X.\n586 \n587         The predicted class of an input sample is computed as the weighted mean\n588         prediction of the classifiers in the ensemble.\n589 \n590         Parameters\n591         ----------\n592         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n593             The training input samples. Sparse matrix can be CSC, CSR, COO,\n594             DOK, or LIL. DOK and LIL are converted to CSR.\n595 \n596         Returns\n597         -------\n598         y : array of shape = [n_samples]\n599             The predicted classes.\n600         \"\"\"\n601         pred = self.decision_function(X)\n602 \n603         if self.n_classes_ == 2:\n604             return self.classes_.take(pred > 0, axis=0)\n605 \n606         return self.classes_.take(np.argmax(pred, axis=1), axis=0)\n607 \n608     def staged_predict(self, X):\n609         \"\"\"Return staged predictions for X.\n610 \n611         The predicted class of an input sample is computed as the weighted mean\n612         prediction of the classifiers in the ensemble.\n613 \n614         This generator method yields the ensemble prediction after each\n615         iteration of boosting and therefore allows monitoring, such as to\n616         determine the prediction on a test set after each boost.\n617 \n618         Parameters\n619         ----------\n620         X : array-like of shape = [n_samples, n_features]\n621             The input samples.\n622 \n623         Returns\n624         -------\n625         y : generator of array, shape = [n_samples]\n626             The predicted classes.\n627         \"\"\"\n628         n_classes = self.n_classes_\n629         classes = self.classes_\n630 \n631         if n_classes == 2:\n632             for pred in self.staged_decision_function(X):\n633                 yield np.array(classes.take(pred > 0, axis=0))\n634 \n635         else:\n636             for pred in self.staged_decision_function(X):\n637                 yield np.array(classes.take(\n638                     np.argmax(pred, axis=1), axis=0))\n639 \n640     def decision_function(self, X):\n641         \"\"\"Compute the decision function of ``X``.\n642 \n643         Parameters\n644         ----------\n645         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n646             The training input samples. Sparse matrix can be CSC, CSR, COO,\n647             DOK, or LIL. DOK and LIL are converted to CSR.\n648 \n649         Returns\n650         -------\n651         score : array, shape = [n_samples, k]\n652             The decision function of the input samples. The order of\n653             outputs is the same of that of the `classes_` attribute.\n654             Binary classification is a special cases with ``k == 1``,\n655             otherwise ``k==n_classes``. For binary classification,\n656             values closer to -1 or 1 mean more like the first or second\n657             class in ``classes_``, respectively.\n658         \"\"\"\n659         check_is_fitted(self, \"n_classes_\")\n660         X = self._validate_X_predict(X)\n661 \n662         n_classes = self.n_classes_\n663         classes = self.classes_[:, np.newaxis]\n664 \n665         if self.algorithm == 'SAMME.R':\n666             # The weights are all 1. for SAMME.R\n667             pred = sum(_samme_proba(estimator, n_classes, X)\n668                        for estimator in self.estimators_)\n669         else:   # self.algorithm == \"SAMME\"\n670             pred = sum((estimator.predict(X) == classes).T * w\n671                        for estimator, w in zip(self.estimators_,\n672                                                self.estimator_weights_))\n673 \n674         pred /= self.estimator_weights_.sum()\n675         if n_classes == 2:\n676             pred[:, 0] *= -1\n677             return pred.sum(axis=1)\n678         return pred\n679 \n680     def staged_decision_function(self, X):\n681         \"\"\"Compute decision function of ``X`` for each boosting iteration.\n682 \n683         This method allows monitoring (i.e. determine error on testing set)\n684         after each boosting iteration.\n685 \n686         Parameters\n687         ----------\n688         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n689             The training input samples. Sparse matrix can be CSC, CSR, COO,\n690             DOK, or LIL. DOK and LIL are converted to CSR.\n691 \n692         Returns\n693         -------\n694         score : generator of array, shape = [n_samples, k]\n695             The decision function of the input samples. The order of\n696             outputs is the same of that of the `classes_` attribute.\n697             Binary classification is a special cases with ``k == 1``,\n698             otherwise ``k==n_classes``. For binary classification,\n699             values closer to -1 or 1 mean more like the first or second\n700             class in ``classes_``, respectively.\n701         \"\"\"\n702         check_is_fitted(self, \"n_classes_\")\n703         X = self._validate_X_predict(X)\n704 \n705         n_classes = self.n_classes_\n706         classes = self.classes_[:, np.newaxis]\n707         pred = None\n708         norm = 0.\n709 \n710         for weight, estimator in zip(self.estimator_weights_,\n711                                      self.estimators_):\n712             norm += weight\n713 \n714             if self.algorithm == 'SAMME.R':\n715                 # The weights are all 1. for SAMME.R\n716                 current_pred = _samme_proba(estimator, n_classes, X)\n717             else:  # elif self.algorithm == \"SAMME\":\n718                 current_pred = estimator.predict(X)\n719                 current_pred = (current_pred == classes).T * weight\n720 \n721             if pred is None:\n722                 pred = current_pred\n723             else:\n724                 pred += current_pred\n725 \n726             if n_classes == 2:\n727                 tmp_pred = np.copy(pred)\n728                 tmp_pred[:, 0] *= -1\n729                 yield (tmp_pred / norm).sum(axis=1)\n730             else:\n731                 yield pred / norm\n732 \n733     def predict_proba(self, X):\n734         \"\"\"Predict class probabilities for X.\n735 \n736         The predicted class probabilities of an input sample is computed as\n737         the weighted mean predicted class probabilities of the classifiers\n738         in the ensemble.\n739 \n740         Parameters\n741         ----------\n742         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n743             The training input samples. Sparse matrix can be CSC, CSR, COO,\n744             DOK, or LIL. DOK and LIL are converted to CSR.\n745 \n746         Returns\n747         -------\n748         p : array of shape = [n_samples, n_classes]\n749             The class probabilities of the input samples. The order of\n750             outputs is the same of that of the `classes_` attribute.\n751         \"\"\"\n752         check_is_fitted(self, \"n_classes_\")\n753 \n754         n_classes = self.n_classes_\n755         X = self._validate_X_predict(X)\n756 \n757         if n_classes == 1:\n758             return np.ones((X.shape[0], 1))\n759 \n760         if self.algorithm == 'SAMME.R':\n761             # The weights are all 1. for SAMME.R\n762             proba = sum(_samme_proba(estimator, n_classes, X)\n763                         for estimator in self.estimators_)\n764         else:   # self.algorithm == \"SAMME\"\n765             proba = sum(estimator.predict_proba(X) * w\n766                         for estimator, w in zip(self.estimators_,\n767                                                 self.estimator_weights_))\n768 \n769         proba /= self.estimator_weights_.sum()\n770         proba = np.exp((1. / (n_classes - 1)) * proba)\n771         normalizer = proba.sum(axis=1)[:, np.newaxis]\n772         normalizer[normalizer == 0.0] = 1.0\n773         proba /= normalizer\n774 \n775         return proba\n776 \n777     def staged_predict_proba(self, X):\n778         \"\"\"Predict class probabilities for X.\n779 \n780         The predicted class probabilities of an input sample is computed as\n781         the weighted mean predicted class probabilities of the classifiers\n782         in the ensemble.\n783 \n784         This generator method yields the ensemble predicted class probabilities\n785         after each iteration of boosting and therefore allows monitoring, such\n786         as to determine the predicted class probabilities on a test set after\n787         each boost.\n788 \n789         Parameters\n790         ----------\n791         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n792             The training input samples. Sparse matrix can be CSC, CSR, COO,\n793             DOK, or LIL. DOK and LIL are converted to CSR.\n794 \n795         Returns\n796         -------\n797         p : generator of array, shape = [n_samples]\n798             The class probabilities of the input samples. The order of\n799             outputs is the same of that of the `classes_` attribute.\n800         \"\"\"\n801         X = self._validate_X_predict(X)\n802 \n803         n_classes = self.n_classes_\n804         proba = None\n805         norm = 0.\n806 \n807         for weight, estimator in zip(self.estimator_weights_,\n808                                      self.estimators_):\n809             norm += weight\n810 \n811             if self.algorithm == 'SAMME.R':\n812                 # The weights are all 1. for SAMME.R\n813                 current_proba = _samme_proba(estimator, n_classes, X)\n814             else:  # elif self.algorithm == \"SAMME\":\n815                 current_proba = estimator.predict_proba(X) * weight\n816 \n817             if proba is None:\n818                 proba = current_proba\n819             else:\n820                 proba += current_proba\n821 \n822             real_proba = np.exp((1. / (n_classes - 1)) * (proba / norm))\n823             normalizer = real_proba.sum(axis=1)[:, np.newaxis]\n824             normalizer[normalizer == 0.0] = 1.0\n825             real_proba /= normalizer\n826 \n827             yield real_proba\n828 \n829     def predict_log_proba(self, X):\n830         \"\"\"Predict class log-probabilities for X.\n831 \n832         The predicted class log-probabilities of an input sample is computed as\n833         the weighted mean predicted class log-probabilities of the classifiers\n834         in the ensemble.\n835 \n836         Parameters\n837         ----------\n838         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n839             The training input samples. Sparse matrix can be CSC, CSR, COO,\n840             DOK, or LIL. DOK and LIL are converted to CSR.\n841 \n842         Returns\n843         -------\n844         p : array of shape = [n_samples, n_classes]\n845             The class probabilities of the input samples. The order of\n846             outputs is the same of that of the `classes_` attribute.\n847         \"\"\"\n848         return np.log(self.predict_proba(X))\n849 \n850 \n851 class AdaBoostRegressor(BaseWeightBoosting, RegressorMixin):\n852     \"\"\"An AdaBoost regressor.\n853 \n854     An AdaBoost [1] regressor is a meta-estimator that begins by fitting a\n855     regressor on the original dataset and then fits additional copies of the\n856     regressor on the same dataset but where the weights of instances are\n857     adjusted according to the error of the current prediction. As such,\n858     subsequent regressors focus more on difficult cases.\n859 \n860     This class implements the algorithm known as AdaBoost.R2 [2].\n861 \n862     Read more in the :ref:`User Guide <adaboost>`.\n863 \n864     Parameters\n865     ----------\n866     base_estimator : object, optional (default=None)\n867         The base estimator from which the boosted ensemble is built.\n868         Support for sample weighting is required. If ``None``, then\n869         the base estimator is ``DecisionTreeRegressor(max_depth=3)``\n870 \n871     n_estimators : integer, optional (default=50)\n872         The maximum number of estimators at which boosting is terminated.\n873         In case of perfect fit, the learning procedure is stopped early.\n874 \n875     learning_rate : float, optional (default=1.)\n876         Learning rate shrinks the contribution of each regressor by\n877         ``learning_rate``. There is a trade-off between ``learning_rate`` and\n878         ``n_estimators``.\n879 \n880     loss : {'linear', 'square', 'exponential'}, optional (default='linear')\n881         The loss function to use when updating the weights after each\n882         boosting iteration.\n883 \n884     random_state : int, RandomState instance or None, optional (default=None)\n885         If int, random_state is the seed used by the random number generator;\n886         If RandomState instance, random_state is the random number generator;\n887         If None, the random number generator is the RandomState instance used\n888         by `np.random`.\n889 \n890     Attributes\n891     ----------\n892     estimators_ : list of classifiers\n893         The collection of fitted sub-estimators.\n894 \n895     estimator_weights_ : array of floats\n896         Weights for each estimator in the boosted ensemble.\n897 \n898     estimator_errors_ : array of floats\n899         Regression error for each estimator in the boosted ensemble.\n900 \n901     feature_importances_ : array of shape = [n_features]\n902         The feature importances if supported by the ``base_estimator``.\n903 \n904     See also\n905     --------\n906     AdaBoostClassifier, GradientBoostingRegressor,\n907     sklearn.tree.DecisionTreeRegressor\n908 \n909     References\n910     ----------\n911     .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n912            on-Line Learning and an Application to Boosting\", 1995.\n913 \n914     .. [2] H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997.\n915 \n916     \"\"\"\n917     def __init__(self,\n918                  base_estimator=None,\n919                  n_estimators=50,\n920                  learning_rate=1.,\n921                  loss='linear',\n922                  random_state=None):\n923 \n924         super().__init__(\n925             base_estimator=base_estimator,\n926             n_estimators=n_estimators,\n927             learning_rate=learning_rate,\n928             random_state=random_state)\n929 \n930         self.loss = loss\n931         self.random_state = random_state\n932 \n933     def fit(self, X, y, sample_weight=None):\n934         \"\"\"Build a boosted regressor from the training set (X, y).\n935 \n936         Parameters\n937         ----------\n938         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n939             The training input samples. Sparse matrix can be CSC, CSR, COO,\n940             DOK, or LIL. DOK and LIL are converted to CSR.\n941 \n942         y : array-like of shape = [n_samples]\n943             The target values (real numbers).\n944 \n945         sample_weight : array-like of shape = [n_samples], optional\n946             Sample weights. If None, the sample weights are initialized to\n947             1 / n_samples.\n948 \n949         Returns\n950         -------\n951         self : object\n952         \"\"\"\n953         # Check loss\n954         if self.loss not in ('linear', 'square', 'exponential'):\n955             raise ValueError(\n956                 \"loss must be 'linear', 'square', or 'exponential'\")\n957 \n958         # Fit\n959         return super().fit(X, y, sample_weight)\n960 \n961     def _validate_estimator(self):\n962         \"\"\"Check the estimator and set the base_estimator_ attribute.\"\"\"\n963         super()._validate_estimator(\n964             default=DecisionTreeRegressor(max_depth=3))\n965 \n966     def _boost(self, iboost, X, y, sample_weight, random_state):\n967         \"\"\"Implement a single boost for regression\n968 \n969         Perform a single boost according to the AdaBoost.R2 algorithm and\n970         return the updated sample weights.\n971 \n972         Parameters\n973         ----------\n974         iboost : int\n975             The index of the current boost iteration.\n976 \n977         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n978             The training input samples. Sparse matrix can be CSC, CSR, COO,\n979             DOK, or LIL. DOK and LIL are converted to CSR.\n980 \n981         y : array-like of shape = [n_samples]\n982             The target values (class labels in classification, real numbers in\n983             regression).\n984 \n985         sample_weight : array-like of shape = [n_samples]\n986             The current sample weights.\n987 \n988         random_state : RandomState\n989             The current random number generator\n990 \n991         Returns\n992         -------\n993         sample_weight : array-like of shape = [n_samples] or None\n994             The reweighted sample weights.\n995             If None then boosting has terminated early.\n996 \n997         estimator_weight : float\n998             The weight for the current boost.\n999             If None then boosting has terminated early.\n1000 \n1001         estimator_error : float\n1002             The regression error for the current boost.\n1003             If None then boosting has terminated early.\n1004         \"\"\"\n1005         estimator = self._make_estimator(random_state=random_state)\n1006 \n1007         # Weighted sampling of the training set with replacement\n1008         # For NumPy >= 1.7.0 use np.random.choice\n1009         cdf = stable_cumsum(sample_weight)\n1010         cdf /= cdf[-1]\n1011         uniform_samples = random_state.random_sample(X.shape[0])\n1012         bootstrap_idx = cdf.searchsorted(uniform_samples, side='right')\n1013         # searchsorted returns a scalar\n1014         bootstrap_idx = np.array(bootstrap_idx, copy=False)\n1015 \n1016         # Fit on the bootstrapped sample and obtain a prediction\n1017         # for all samples in the training set\n1018         estimator.fit(X[bootstrap_idx], y[bootstrap_idx])\n1019         y_predict = estimator.predict(X)\n1020 \n1021         error_vect = np.abs(y_predict - y)\n1022         error_max = error_vect.max()\n1023 \n1024         if error_max != 0.:\n1025             error_vect /= error_max\n1026 \n1027         if self.loss == 'square':\n1028             error_vect **= 2\n1029         elif self.loss == 'exponential':\n1030             error_vect = 1. - np.exp(- error_vect)\n1031 \n1032         # Calculate the average loss\n1033         estimator_error = (sample_weight * error_vect).sum()\n1034 \n1035         if estimator_error <= 0:\n1036             # Stop if fit is perfect\n1037             return sample_weight, 1., 0.\n1038 \n1039         elif estimator_error >= 0.5:\n1040             # Discard current estimator only if it isn't the only one\n1041             if len(self.estimators_) > 1:\n1042                 self.estimators_.pop(-1)\n1043             return None, None, None\n1044 \n1045         beta = estimator_error / (1. - estimator_error)\n1046 \n1047         # Boost weight using AdaBoost.R2 alg\n1048         estimator_weight = self.learning_rate * np.log(1. / beta)\n1049 \n1050         if not iboost == self.n_estimators - 1:\n1051             sample_weight *= np.power(\n1052                 beta,\n1053                 (1. - error_vect) * self.learning_rate)\n1054 \n1055         return sample_weight, estimator_weight, estimator_error\n1056 \n1057     def _get_median_predict(self, X, limit):\n1058         # Evaluate predictions of all estimators\n1059         predictions = np.array([\n1060             est.predict(X) for est in self.estimators_[:limit]]).T\n1061 \n1062         # Sort the predictions\n1063         sorted_idx = np.argsort(predictions, axis=1)\n1064 \n1065         # Find index of median prediction for each sample\n1066         weight_cdf = stable_cumsum(self.estimator_weights_[sorted_idx], axis=1)\n1067         median_or_above = weight_cdf >= 0.5 * weight_cdf[:, -1][:, np.newaxis]\n1068         median_idx = median_or_above.argmax(axis=1)\n1069 \n1070         median_estimators = sorted_idx[np.arange(X.shape[0]), median_idx]\n1071 \n1072         # Return median predictions\n1073         return predictions[np.arange(X.shape[0]), median_estimators]\n1074 \n1075     def predict(self, X):\n1076         \"\"\"Predict regression value for X.\n1077 \n1078         The predicted regression value of an input sample is computed\n1079         as the weighted median prediction of the classifiers in the ensemble.\n1080 \n1081         Parameters\n1082         ----------\n1083         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n1084             The training input samples. Sparse matrix can be CSC, CSR, COO,\n1085             DOK, or LIL. DOK and LIL are converted to CSR.\n1086 \n1087         Returns\n1088         -------\n1089         y : array of shape = [n_samples]\n1090             The predicted regression values.\n1091         \"\"\"\n1092         check_is_fitted(self, \"estimator_weights_\")\n1093         X = self._validate_X_predict(X)\n1094 \n1095         return self._get_median_predict(X, len(self.estimators_))\n1096 \n1097     def staged_predict(self, X):\n1098         \"\"\"Return staged predictions for X.\n1099 \n1100         The predicted regression value of an input sample is computed\n1101         as the weighted median prediction of the classifiers in the ensemble.\n1102 \n1103         This generator method yields the ensemble prediction after each\n1104         iteration of boosting and therefore allows monitoring, such as to\n1105         determine the prediction on a test set after each boost.\n1106 \n1107         Parameters\n1108         ----------\n1109         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n1110             The training input samples. Sparse matrix can be CSC, CSR, COO,\n1111             DOK, or LIL. DOK and LIL are converted to CSR.\n1112 \n1113         Returns\n1114         -------\n1115         y : generator of array, shape = [n_samples]\n1116             The predicted regression values.\n1117         \"\"\"\n1118         check_is_fitted(self, \"estimator_weights_\")\n1119         X = self._validate_X_predict(X)\n1120 \n1121         for i, _ in enumerate(self.estimators_, 1):\n1122             yield self._get_median_predict(X, limit=i)\n",
        "intended_behavior": "This file likely handles the AdaBoost algorithm and potentially others. The validation of X should be flexible enough to allow various data types beyond arrays or sparse matrices, as long as the base estimator can process the input data type. This means adjusting or extending the validate_X utility or the validation logic within fit, predict_proba, and predict methods to not outright reject input types like lists of text blobs if the base estimator can handle them. Preserve existing checks that ensure the input is compatible with the base estimator's requirements, perhaps by introducing a pre-validation step that checks if the input type is supported by the base estimator before proceeding with any further validation or processing."
    },
    {
        "rel_file_path": "sklearn/ensemble/forest.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/ensemble/forest.py",
        "start": 1,
        "end": 2026,
        "class_name": null,
        "method_name": null,
        "code": "1 \"\"\"Forest of trees-based ensemble methods\n2 \n3 Those methods include random forests and extremely randomized trees.\n4 \n5 The module structure is the following:\n6 \n7 - The ``BaseForest`` base class implements a common ``fit`` method for all\n8   the estimators in the module. The ``fit`` method of the base ``Forest``\n9   class calls the ``fit`` method of each sub-estimator on random samples\n10   (with replacement, a.k.a. bootstrap) of the training set.\n11 \n12   The init of the sub-estimator is further delegated to the\n13   ``BaseEnsemble`` constructor.\n14 \n15 - The ``ForestClassifier`` and ``ForestRegressor`` base classes further\n16   implement the prediction logic by computing an average of the predicted\n17   outcomes of the sub-estimators.\n18 \n19 - The ``RandomForestClassifier`` and ``RandomForestRegressor`` derived\n20   classes provide the user with concrete implementations of\n21   the forest ensemble method using classical, deterministic\n22   ``DecisionTreeClassifier`` and ``DecisionTreeRegressor`` as\n23   sub-estimator implementations.\n24 \n25 - The ``ExtraTreesClassifier`` and ``ExtraTreesRegressor`` derived\n26   classes provide the user with concrete implementations of the\n27   forest ensemble method using the extremely randomized trees\n28   ``ExtraTreeClassifier`` and ``ExtraTreeRegressor`` as\n29   sub-estimator implementations.\n30 \n31 Single and multi-output problems are both handled.\n32 \n33 \"\"\"\n34 \n35 # Authors: Gilles Louppe <g.louppe@gmail.com>\n36 #          Brian Holt <bdholt1@gmail.com>\n37 #          Joly Arnaud <arnaud.v.joly@gmail.com>\n38 #          Fares Hedayati <fares.hedayati@gmail.com>\n39 #\n40 # License: BSD 3 clause\n41 \n42 \n43 from warnings import catch_warnings, simplefilter, warn\n44 import threading\n45 \n46 from abc import ABCMeta, abstractmethod\n47 import numpy as np\n48 from scipy.sparse import issparse\n49 from scipy.sparse import hstack as sparse_hstack\n50 \n51 from ..base import ClassifierMixin, RegressorMixin\n52 from ..utils._joblib import Parallel, delayed\n53 from ..metrics import r2_score\n54 from ..preprocessing import OneHotEncoder\n55 from ..tree import (DecisionTreeClassifier, DecisionTreeRegressor,\n56                     ExtraTreeClassifier, ExtraTreeRegressor)\n57 from ..tree._tree import DTYPE, DOUBLE\n58 from ..utils import check_random_state, check_array, compute_sample_weight\n59 from ..exceptions import DataConversionWarning, NotFittedError\n60 from .base import BaseEnsemble, _partition_estimators\n61 from ..utils.fixes import parallel_helper, _joblib_parallel_args\n62 from ..utils.multiclass import check_classification_targets\n63 from ..utils.validation import check_is_fitted\n64 \n65 __all__ = [\"RandomForestClassifier\",\n66            \"RandomForestRegressor\",\n67            \"ExtraTreesClassifier\",\n68            \"ExtraTreesRegressor\",\n69            \"RandomTreesEmbedding\"]\n70 \n71 MAX_INT = np.iinfo(np.int32).max\n72 \n73 \n74 def _generate_sample_indices(random_state, n_samples):\n75     \"\"\"Private function used to _parallel_build_trees function.\"\"\"\n76     random_instance = check_random_state(random_state)\n77     sample_indices = random_instance.randint(0, n_samples, n_samples)\n78 \n79     return sample_indices\n80 \n81 \n82 def _generate_unsampled_indices(random_state, n_samples):\n83     \"\"\"Private function used to forest._set_oob_score function.\"\"\"\n84     sample_indices = _generate_sample_indices(random_state, n_samples)\n85     sample_counts = np.bincount(sample_indices, minlength=n_samples)\n86     unsampled_mask = sample_counts == 0\n87     indices_range = np.arange(n_samples)\n88     unsampled_indices = indices_range[unsampled_mask]\n89 \n90     return unsampled_indices\n91 \n92 \n93 def _parallel_build_trees(tree, forest, X, y, sample_weight, tree_idx, n_trees,\n94                           verbose=0, class_weight=None):\n95     \"\"\"Private function used to fit a single tree in parallel.\"\"\"\n96     if verbose > 1:\n97         print(\"building tree %d of %d\" % (tree_idx + 1, n_trees))\n98 \n99     if forest.bootstrap:\n100         n_samples = X.shape[0]\n101         if sample_weight is None:\n102             curr_sample_weight = np.ones((n_samples,), dtype=np.float64)\n103         else:\n104             curr_sample_weight = sample_weight.copy()\n105 \n106         indices = _generate_sample_indices(tree.random_state, n_samples)\n107         sample_counts = np.bincount(indices, minlength=n_samples)\n108         curr_sample_weight *= sample_counts\n109 \n110         if class_weight == 'subsample':\n111             with catch_warnings():\n112                 simplefilter('ignore', DeprecationWarning)\n113                 curr_sample_weight *= compute_sample_weight('auto', y, indices)\n114         elif class_weight == 'balanced_subsample':\n115             curr_sample_weight *= compute_sample_weight('balanced', y, indices)\n116 \n117         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n118     else:\n119         tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n120 \n121     return tree\n122 \n123 \n124 class BaseForest(BaseEnsemble, metaclass=ABCMeta):\n125     \"\"\"Base class for forests of trees.\n126 \n127     Warning: This class should not be used directly. Use derived classes\n128     instead.\n129     \"\"\"\n130 \n131     @abstractmethod\n132     def __init__(self,\n133                  base_estimator,\n134                  n_estimators=100,\n135                  estimator_params=tuple(),\n136                  bootstrap=False,\n137                  oob_score=False,\n138                  n_jobs=None,\n139                  random_state=None,\n140                  verbose=0,\n141                  warm_start=False,\n142                  class_weight=None):\n143         super().__init__(\n144             base_estimator=base_estimator,\n145             n_estimators=n_estimators,\n146             estimator_params=estimator_params)\n147 \n148         self.bootstrap = bootstrap\n149         self.oob_score = oob_score\n150         self.n_jobs = n_jobs\n151         self.random_state = random_state\n152         self.verbose = verbose\n153         self.warm_start = warm_start\n154         self.class_weight = class_weight\n155 \n156     def apply(self, X):\n157         \"\"\"Apply trees in the forest to X, return leaf indices.\n158 \n159         Parameters\n160         ----------\n161         X : array-like or sparse matrix, shape = [n_samples, n_features]\n162             The input samples. Internally, its dtype will be converted to\n163             ``dtype=np.float32``. If a sparse matrix is provided, it will be\n164             converted into a sparse ``csr_matrix``.\n165 \n166         Returns\n167         -------\n168         X_leaves : array_like, shape = [n_samples, n_estimators]\n169             For each datapoint x in X and for each tree in the forest,\n170             return the index of the leaf x ends up in.\n171         \"\"\"\n172         X = self._validate_X_predict(X)\n173         results = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n174                            **_joblib_parallel_args(prefer=\"threads\"))(\n175             delayed(parallel_helper)(tree, 'apply', X, check_input=False)\n176             for tree in self.estimators_)\n177 \n178         return np.array(results).T\n179 \n180     def decision_path(self, X):\n181         \"\"\"Return the decision path in the forest\n182 \n183         .. versionadded:: 0.18\n184 \n185         Parameters\n186         ----------\n187         X : array-like or sparse matrix, shape = [n_samples, n_features]\n188             The input samples. Internally, its dtype will be converted to\n189             ``dtype=np.float32``. If a sparse matrix is provided, it will be\n190             converted into a sparse ``csr_matrix``.\n191 \n192         Returns\n193         -------\n194         indicator : sparse csr array, shape = [n_samples, n_nodes]\n195             Return a node indicator matrix where non zero elements\n196             indicates that the samples goes through the nodes.\n197 \n198         n_nodes_ptr : array of size (n_estimators + 1, )\n199             The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n200             gives the indicator value for the i-th estimator.\n201 \n202         \"\"\"\n203         X = self._validate_X_predict(X)\n204         indicators = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n205                               **_joblib_parallel_args(prefer='threads'))(\n206             delayed(parallel_helper)(tree, 'decision_path', X,\n207                                      check_input=False)\n208             for tree in self.estimators_)\n209 \n210         n_nodes = [0]\n211         n_nodes.extend([i.shape[1] for i in indicators])\n212         n_nodes_ptr = np.array(n_nodes).cumsum()\n213 \n214         return sparse_hstack(indicators).tocsr(), n_nodes_ptr\n215 \n216     def fit(self, X, y, sample_weight=None):\n217         \"\"\"Build a forest of trees from the training set (X, y).\n218 \n219         Parameters\n220         ----------\n221         X : array-like or sparse matrix of shape = [n_samples, n_features]\n222             The training input samples. Internally, its dtype will be converted\n223             to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n224             converted into a sparse ``csc_matrix``.\n225 \n226         y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n227             The target values (class labels in classification, real numbers in\n228             regression).\n229 \n230         sample_weight : array-like, shape = [n_samples] or None\n231             Sample weights. If None, then samples are equally weighted. Splits\n232             that would create child nodes with net zero or negative weight are\n233             ignored while searching for a split in each node. In the case of\n234             classification, splits are also ignored if they would result in any\n235             single class carrying a negative weight in either child node.\n236 \n237         Returns\n238         -------\n239         self : object\n240         \"\"\"\n241 \n242         if self.n_estimators == 'warn':\n243             warn(\"The default value of n_estimators will change from \"\n244                  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n245             self.n_estimators = 10\n246 \n247         # Validate or convert input data\n248         X = check_array(X, accept_sparse=\"csc\", dtype=DTYPE)\n249         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)\n250         if sample_weight is not None:\n251             sample_weight = check_array(sample_weight, ensure_2d=False)\n252         if issparse(X):\n253             # Pre-sort indices to avoid that each individual tree of the\n254             # ensemble sorts the indices.\n255             X.sort_indices()\n256 \n257         # Remap output\n258         self.n_features_ = X.shape[1]\n259 \n260         y = np.atleast_1d(y)\n261         if y.ndim == 2 and y.shape[1] == 1:\n262             warn(\"A column-vector y was passed when a 1d array was\"\n263                  \" expected. Please change the shape of y to \"\n264                  \"(n_samples,), for example using ravel().\",\n265                  DataConversionWarning, stacklevel=2)\n266 \n267         if y.ndim == 1:\n268             # reshape is necessary to preserve the data contiguity against vs\n269             # [:, np.newaxis] that does not.\n270             y = np.reshape(y, (-1, 1))\n271 \n272         self.n_outputs_ = y.shape[1]\n273 \n274         y, expanded_class_weight = self._validate_y_class_weight(y)\n275 \n276         if getattr(y, \"dtype\", None) != DOUBLE or not y.flags.contiguous:\n277             y = np.ascontiguousarray(y, dtype=DOUBLE)\n278 \n279         if expanded_class_weight is not None:\n280             if sample_weight is not None:\n281                 sample_weight = sample_weight * expanded_class_weight\n282             else:\n283                 sample_weight = expanded_class_weight\n284 \n285         # Check parameters\n286         self._validate_estimator()\n287 \n288         if not self.bootstrap and self.oob_score:\n289             raise ValueError(\"Out of bag estimation only available\"\n290                              \" if bootstrap=True\")\n291 \n292         random_state = check_random_state(self.random_state)\n293 \n294         if not self.warm_start or not hasattr(self, \"estimators_\"):\n295             # Free allocated memory, if any\n296             self.estimators_ = []\n297 \n298         n_more_estimators = self.n_estimators - len(self.estimators_)\n299 \n300         if n_more_estimators < 0:\n301             raise ValueError('n_estimators=%d must be larger or equal to '\n302                              'len(estimators_)=%d when warm_start==True'\n303                              % (self.n_estimators, len(self.estimators_)))\n304 \n305         elif n_more_estimators == 0:\n306             warn(\"Warm-start fitting without increasing n_estimators does not \"\n307                  \"fit new trees.\")\n308         else:\n309             if self.warm_start and len(self.estimators_) > 0:\n310                 # We draw from the random state to get the random state we\n311                 # would have got if we hadn't used a warm_start.\n312                 random_state.randint(MAX_INT, size=len(self.estimators_))\n313 \n314             trees = [self._make_estimator(append=False,\n315                                           random_state=random_state)\n316                      for i in range(n_more_estimators)]\n317 \n318             # Parallel loop: we prefer the threading backend as the Cython code\n319             # for fitting the trees is internally releasing the Python GIL\n320             # making threading more efficient than multiprocessing in\n321             # that case. However, for joblib 0.12+ we respect any\n322             # parallel_backend contexts set at a higher level,\n323             # since correctness does not rely on using threads.\n324             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n325                              **_joblib_parallel_args(prefer='threads'))(\n326                 delayed(_parallel_build_trees)(\n327                     t, self, X, y, sample_weight, i, len(trees),\n328                     verbose=self.verbose, class_weight=self.class_weight)\n329                 for i, t in enumerate(trees))\n330 \n331             # Collect newly grown trees\n332             self.estimators_.extend(trees)\n333 \n334         if self.oob_score:\n335             self._set_oob_score(X, y)\n336 \n337         # Decapsulate classes_ attributes\n338         if hasattr(self, \"classes_\") and self.n_outputs_ == 1:\n339             self.n_classes_ = self.n_classes_[0]\n340             self.classes_ = self.classes_[0]\n341 \n342         return self\n343 \n344     @abstractmethod\n345     def _set_oob_score(self, X, y):\n346         \"\"\"Calculate out of bag predictions and score.\"\"\"\n347 \n348     def _validate_y_class_weight(self, y):\n349         # Default implementation\n350         return y, None\n351 \n352     def _validate_X_predict(self, X):\n353         \"\"\"Validate X whenever one tries to predict, apply, predict_proba\"\"\"\n354         if self.estimators_ is None or len(self.estimators_) == 0:\n355             raise NotFittedError(\"Estimator not fitted, \"\n356                                  \"call `fit` before exploiting the model.\")\n357 \n358         return self.estimators_[0]._validate_X_predict(X, check_input=True)\n359 \n360     @property\n361     def feature_importances_(self):\n362         \"\"\"Return the feature importances (the higher, the more important the\n363            feature).\n364 \n365         Returns\n366         -------\n367         feature_importances_ : array, shape = [n_features]\n368         \"\"\"\n369         check_is_fitted(self, 'estimators_')\n370 \n371         all_importances = Parallel(n_jobs=self.n_jobs,\n372                                    **_joblib_parallel_args(prefer='threads'))(\n373             delayed(getattr)(tree, 'feature_importances_')\n374             for tree in self.estimators_)\n375 \n376         return sum(all_importances) / len(self.estimators_)\n377 \n378 \n379 def _accumulate_prediction(predict, X, out, lock):\n380     \"\"\"This is a utility function for joblib's Parallel.\n381 \n382     It can't go locally in ForestClassifier or ForestRegressor, because joblib\n383     complains that it cannot pickle it when placed there.\n384     \"\"\"\n385     prediction = predict(X, check_input=False)\n386     with lock:\n387         if len(out) == 1:\n388             out[0] += prediction\n389         else:\n390             for i in range(len(out)):\n391                 out[i] += prediction[i]\n392 \n393 \n394 class ForestClassifier(BaseForest, ClassifierMixin, metaclass=ABCMeta):\n395     \"\"\"Base class for forest of trees-based classifiers.\n396 \n397     Warning: This class should not be used directly. Use derived classes\n398     instead.\n399     \"\"\"\n400 \n401     @abstractmethod\n402     def __init__(self,\n403                  base_estimator,\n404                  n_estimators=100,\n405                  estimator_params=tuple(),\n406                  bootstrap=False,\n407                  oob_score=False,\n408                  n_jobs=None,\n409                  random_state=None,\n410                  verbose=0,\n411                  warm_start=False,\n412                  class_weight=None):\n413         super().__init__(\n414             base_estimator,\n415             n_estimators=n_estimators,\n416             estimator_params=estimator_params,\n417             bootstrap=bootstrap,\n418             oob_score=oob_score,\n419             n_jobs=n_jobs,\n420             random_state=random_state,\n421             verbose=verbose,\n422             warm_start=warm_start,\n423             class_weight=class_weight)\n424 \n425     def _set_oob_score(self, X, y):\n426         \"\"\"Compute out-of-bag score\"\"\"\n427         X = check_array(X, dtype=DTYPE, accept_sparse='csr')\n428 \n429         n_classes_ = self.n_classes_\n430         n_samples = y.shape[0]\n431 \n432         oob_decision_function = []\n433         oob_score = 0.0\n434         predictions = [np.zeros((n_samples, n_classes_[k]))\n435                        for k in range(self.n_outputs_)]\n436 \n437         for estimator in self.estimators_:\n438             unsampled_indices = _generate_unsampled_indices(\n439                 estimator.random_state, n_samples)\n440             p_estimator = estimator.predict_proba(X[unsampled_indices, :],\n441                                                   check_input=False)\n442 \n443             if self.n_outputs_ == 1:\n444                 p_estimator = [p_estimator]\n445 \n446             for k in range(self.n_outputs_):\n447                 predictions[k][unsampled_indices, :] += p_estimator[k]\n448 \n449         for k in range(self.n_outputs_):\n450             if (predictions[k].sum(axis=1) == 0).any():\n451                 warn(\"Some inputs do not have OOB scores. \"\n452                      \"This probably means too few trees were used \"\n453                      \"to compute any reliable oob estimates.\")\n454 \n455             decision = (predictions[k] /\n456                         predictions[k].sum(axis=1)[:, np.newaxis])\n457             oob_decision_function.append(decision)\n458             oob_score += np.mean(y[:, k] ==\n459                                  np.argmax(predictions[k], axis=1), axis=0)\n460 \n461         if self.n_outputs_ == 1:\n462             self.oob_decision_function_ = oob_decision_function[0]\n463         else:\n464             self.oob_decision_function_ = oob_decision_function\n465 \n466         self.oob_score_ = oob_score / self.n_outputs_\n467 \n468     def _validate_y_class_weight(self, y):\n469         check_classification_targets(y)\n470 \n471         y = np.copy(y)\n472         expanded_class_weight = None\n473 \n474         if self.class_weight is not None:\n475             y_original = np.copy(y)\n476 \n477         self.classes_ = []\n478         self.n_classes_ = []\n479 \n480         y_store_unique_indices = np.zeros(y.shape, dtype=np.int)\n481         for k in range(self.n_outputs_):\n482             classes_k, y_store_unique_indices[:, k] = np.unique(y[:, k], return_inverse=True)\n483             self.classes_.append(classes_k)\n484             self.n_classes_.append(classes_k.shape[0])\n485         y = y_store_unique_indices\n486 \n487         if self.class_weight is not None:\n488             valid_presets = ('balanced', 'balanced_subsample')\n489             if isinstance(self.class_weight, str):\n490                 if self.class_weight not in valid_presets:\n491                     raise ValueError('Valid presets for class_weight include '\n492                                      '\"balanced\" and \"balanced_subsample\". Given \"%s\".'\n493                                      % self.class_weight)\n494                 if self.warm_start:\n495                     warn('class_weight presets \"balanced\" or \"balanced_subsample\" are '\n496                          'not recommended for warm_start if the fitted data '\n497                          'differs from the full dataset. In order to use '\n498                          '\"balanced\" weights, use compute_class_weight(\"balanced\", '\n499                          'classes, y). In place of y you can use a large '\n500                          'enough sample of the full training set target to '\n501                          'properly estimate the class frequency '\n502                          'distributions. Pass the resulting weights as the '\n503                          'class_weight parameter.')\n504 \n505             if (self.class_weight != 'balanced_subsample' or\n506                     not self.bootstrap):\n507                 if self.class_weight == \"balanced_subsample\":\n508                     class_weight = \"balanced\"\n509                 else:\n510                     class_weight = self.class_weight\n511                 expanded_class_weight = compute_sample_weight(class_weight,\n512                                                               y_original)\n513 \n514         return y, expanded_class_weight\n515 \n516     def predict(self, X):\n517         \"\"\"Predict class for X.\n518 \n519         The predicted class of an input sample is a vote by the trees in\n520         the forest, weighted by their probability estimates. That is,\n521         the predicted class is the one with highest mean probability\n522         estimate across the trees.\n523 \n524         Parameters\n525         ----------\n526         X : array-like or sparse matrix of shape = [n_samples, n_features]\n527             The input samples. Internally, its dtype will be converted to\n528             ``dtype=np.float32``. If a sparse matrix is provided, it will be\n529             converted into a sparse ``csr_matrix``.\n530 \n531         Returns\n532         -------\n533         y : array of shape = [n_samples] or [n_samples, n_outputs]\n534             The predicted classes.\n535         \"\"\"\n536         proba = self.predict_proba(X)\n537 \n538         if self.n_outputs_ == 1:\n539             return self.classes_.take(np.argmax(proba, axis=1), axis=0)\n540 \n541         else:\n542             n_samples = proba[0].shape[0]\n543             # all dtypes should be the same, so just take the first\n544             class_type = self.classes_[0].dtype\n545             predictions = np.empty((n_samples, self.n_outputs_),\n546                                    dtype=class_type)\n547 \n548             for k in range(self.n_outputs_):\n549                 predictions[:, k] = self.classes_[k].take(np.argmax(proba[k],\n550                                                                     axis=1),\n551                                                           axis=0)\n552 \n553             return predictions\n554 \n555     def predict_proba(self, X):\n556         \"\"\"Predict class probabilities for X.\n557 \n558         The predicted class probabilities of an input sample are computed as\n559         the mean predicted class probabilities of the trees in the forest. The\n560         class probability of a single tree is the fraction of samples of the same\n561         class in a leaf.\n562 \n563         Parameters\n564         ----------\n565         X : array-like or sparse matrix of shape = [n_samples, n_features]\n566             The input samples. Internally, its dtype will be converted to\n567             ``dtype=np.float32``. If a sparse matrix is provided, it will be\n568             converted into a sparse ``csr_matrix``.\n569 \n570         Returns\n571         -------\n572         p : array of shape = [n_samples, n_classes], or a list of n_outputs\n573             such arrays if n_outputs > 1.\n574             The class probabilities of the input samples. The order of the\n575             classes corresponds to that in the attribute `classes_`.\n576         \"\"\"\n577         check_is_fitted(self, 'estimators_')\n578         # Check data\n579         X = self._validate_X_predict(X)\n580 \n581         # Assign chunk of trees to jobs\n582         n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)\n583 \n584         # avoid storing the output of every estimator by summing them here\n585         all_proba = [np.zeros((X.shape[0], j), dtype=np.float64)\n586                      for j in np.atleast_1d(self.n_classes_)]\n587         lock = threading.Lock()\n588         Parallel(n_jobs=n_jobs, verbose=self.verbose,\n589                  **_joblib_parallel_args(require=\"sharedmem\"))(\n590             delayed(_accumulate_prediction)(e.predict_proba, X, all_proba,\n591                                             lock)\n592             for e in self.estimators_)\n593 \n594         for proba in all_proba:\n595             proba /= len(self.estimators_)\n596 \n597         if len(all_proba) == 1:\n598             return all_proba[0]\n599         else:\n600             return all_proba\n601 \n602     def predict_log_proba(self, X):\n603         \"\"\"Predict class log-probabilities for X.\n604 \n605         The predicted class log-probabilities of an input sample is computed as\n606         the log of the mean predicted class probabilities of the trees in the\n607         forest.\n608 \n609         Parameters\n610         ----------\n611         X : array-like or sparse matrix of shape = [n_samples, n_features]\n612             The input samples. Internally, its dtype will be converted to\n613             ``dtype=np.float32``. If a sparse matrix is provided, it will be\n614             converted into a sparse ``csr_matrix``.\n615 \n616         Returns\n617         -------\n618         p : array of shape = [n_samples, n_classes], or a list of n_outputs\n619             such arrays if n_outputs > 1.\n620             The class probabilities of the input samples. The order of the\n621             classes corresponds to that in the attribute `classes_`.\n622         \"\"\"\n623         proba = self.predict_proba(X)\n624 \n625         if self.n_outputs_ == 1:\n626             return np.log(proba)\n627 \n628         else:\n629             for k in range(self.n_outputs_):\n630                 proba[k] = np.log(proba[k])\n631 \n632             return proba\n633 \n634 \n635 class ForestRegressor(BaseForest, RegressorMixin, metaclass=ABCMeta):\n636     \"\"\"Base class for forest of trees-based regressors.\n637 \n638     Warning: This class should not be used directly. Use derived classes\n639     instead.\n640     \"\"\"\n641 \n642     @abstractmethod\n643     def __init__(self,\n644                  base_estimator,\n645                  n_estimators=100,\n646                  estimator_params=tuple(),\n647                  bootstrap=False,\n648                  oob_score=False,\n649                  n_jobs=None,\n650                  random_state=None,\n651                  verbose=0,\n652                  warm_start=False):\n653         super().__init__(\n654             base_estimator,\n655             n_estimators=n_estimators,\n656             estimator_params=estimator_params,\n657             bootstrap=bootstrap,\n658             oob_score=oob_score,\n659             n_jobs=n_jobs,\n660             random_state=random_state,\n661             verbose=verbose,\n662             warm_start=warm_start)\n663 \n664     def predict(self, X):\n665         \"\"\"Predict regression target for X.\n666 \n667         The predicted regression target of an input sample is computed as the\n668         mean predicted regression targets of the trees in the forest.\n669 \n670         Parameters\n671         ----------\n672         X : array-like or sparse matrix of shape = [n_samples, n_features]\n673             The input samples. Internally, its dtype will be converted to\n674             ``dtype=np.float32``. If a sparse matrix is provided, it will be\n675             converted into a sparse ``csr_matrix``.\n676 \n677         Returns\n678         -------\n679         y : array of shape = [n_samples] or [n_samples, n_outputs]\n680             The predicted values.\n681         \"\"\"\n682         check_is_fitted(self, 'estimators_')\n683         # Check data\n684         X = self._validate_X_predict(X)\n685 \n686         # Assign chunk of trees to jobs\n687         n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)\n688 \n689         # avoid storing the output of every estimator by summing them here\n690         if self.n_outputs_ > 1:\n691             y_hat = np.zeros((X.shape[0], self.n_outputs_), dtype=np.float64)\n692         else:\n693             y_hat = np.zeros((X.shape[0]), dtype=np.float64)\n694 \n695         # Parallel loop\n696         lock = threading.Lock()\n697         Parallel(n_jobs=n_jobs, verbose=self.verbose,\n698                  **_joblib_parallel_args(require=\"sharedmem\"))(\n699             delayed(_accumulate_prediction)(e.predict, X, [y_hat], lock)\n700             for e in self.estimators_)\n701 \n702         y_hat /= len(self.estimators_)\n703 \n704         return y_hat\n705 \n706     def _set_oob_score(self, X, y):\n707         \"\"\"Compute out-of-bag scores\"\"\"\n708         X = check_array(X, dtype=DTYPE, accept_sparse='csr')\n709 \n710         n_samples = y.shape[0]\n711 \n712         predictions = np.zeros((n_samples, self.n_outputs_))\n713         n_predictions = np.zeros((n_samples, self.n_outputs_))\n714 \n715         for estimator in self.estimators_:\n716             unsampled_indices = _generate_unsampled_indices(\n717                 estimator.random_state, n_samples)\n718             p_estimator = estimator.predict(\n719                 X[unsampled_indices, :], check_input=False)\n720 \n721             if self.n_outputs_ == 1:\n722                 p_estimator = p_estimator[:, np.newaxis]\n723 \n724             predictions[unsampled_indices, :] += p_estimator\n725             n_predictions[unsampled_indices, :] += 1\n726 \n727         if (n_predictions == 0).any():\n728             warn(\"Some inputs do not have OOB scores. \"\n729                  \"This probably means too few trees were used \"\n730                  \"to compute any reliable oob estimates.\")\n731             n_predictions[n_predictions == 0] = 1\n732 \n733         predictions /= n_predictions\n734         self.oob_prediction_ = predictions\n735 \n736         if self.n_outputs_ == 1:\n737             self.oob_prediction_ = \\\n738                 self.oob_prediction_.reshape((n_samples, ))\n739 \n740         self.oob_score_ = 0.0\n741 \n742         for k in range(self.n_outputs_):\n743             self.oob_score_ += r2_score(y[:, k],\n744                                         predictions[:, k])\n745 \n746         self.oob_score_ /= self.n_outputs_\n747 \n748 \n749 class RandomForestClassifier(ForestClassifier):\n750     \"\"\"A random forest classifier.\n751 \n752     A random forest is a meta estimator that fits a number of decision tree\n753     classifiers on various sub-samples of the dataset and uses averaging to\n754     improve the predictive accuracy and control over-fitting.\n755     The sub-sample size is always the same as the original\n756     input sample size but the samples are drawn with replacement if\n757     `bootstrap=True` (default).\n758 \n759     Read more in the :ref:`User Guide <forest>`.\n760 \n761     Parameters\n762     ----------\n763     n_estimators : integer, optional (default=10)\n764         The number of trees in the forest.\n765 \n766         .. versionchanged:: 0.20\n767            The default value of ``n_estimators`` will change from 10 in\n768            version 0.20 to 100 in version 0.22.\n769 \n770     criterion : string, optional (default=\"gini\")\n771         The function to measure the quality of a split. Supported criteria are\n772         \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n773         Note: this parameter is tree-specific.\n774 \n775     max_depth : integer or None, optional (default=None)\n776         The maximum depth of the tree. If None, then nodes are expanded until\n777         all leaves are pure or until all leaves contain less than\n778         min_samples_split samples.\n779 \n780     min_samples_split : int, float, optional (default=2)\n781         The minimum number of samples required to split an internal node:\n782 \n783         - If int, then consider `min_samples_split` as the minimum number.\n784         - If float, then `min_samples_split` is a fraction and\n785           `ceil(min_samples_split * n_samples)` are the minimum\n786           number of samples for each split.\n787 \n788         .. versionchanged:: 0.18\n789            Added float values for fractions.\n790 \n791     min_samples_leaf : int, float, optional (default=1)\n792         The minimum number of samples required to be at a leaf node.\n793         A split point at any depth will only be considered if it leaves at\n794         least ``min_samples_leaf`` training samples in each of the left and\n795         right branches.  This may have the effect of smoothing the model,\n796         especially in regression.\n797 \n798         - If int, then consider `min_samples_leaf` as the minimum number.\n799         - If float, then `min_samples_leaf` is a fraction and\n800           `ceil(min_samples_leaf * n_samples)` are the minimum\n801           number of samples for each node.\n802 \n803         .. versionchanged:: 0.18\n804            Added float values for fractions.\n805 \n806     min_weight_fraction_leaf : float, optional (default=0.)\n807         The minimum weighted fraction of the sum total of weights (of all\n808         the input samples) required to be at a leaf node. Samples have\n809         equal weight when sample_weight is not provided.\n810 \n811     max_features : int, float, string or None, optional (default=\"auto\")\n812         The number of features to consider when looking for the best split:\n813 \n814         - If int, then consider `max_features` features at each split.\n815         - If float, then `max_features` is a fraction and\n816           `int(max_features * n_features)` features are considered at each\n817           split.\n818         - If \"auto\", then `max_features=sqrt(n_features)`.\n819         - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n820         - If \"log2\", then `max_features=log2(n_features)`.\n821         - If None, then `max_features=n_features`.\n822 \n823         Note: the search for a split does not stop until at least one\n824         valid partition of the node samples is found, even if it requires to\n825         effectively inspect more than ``max_features`` features.\n826 \n827     max_leaf_nodes : int or None, optional (default=None)\n828         Grow trees with ``max_leaf_nodes`` in best-first fashion.\n829         Best nodes are defined as relative reduction in impurity.\n830         If None then unlimited number of leaf nodes.\n831 \n832     min_impurity_decrease : float, optional (default=0.)\n833         A node will be split if this split induces a decrease of the impurity\n834         greater than or equal to this value.\n835 \n836         The weighted impurity decrease equation is the following::\n837 \n838             N_t / N * (impurity - N_t_R / N_t * right_impurity\n839                                 - N_t_L / N_t * left_impurity)\n840 \n841         where ``N`` is the total number of samples, ``N_t`` is the number of\n842         samples at the current node, ``N_t_L`` is the number of samples in the\n843         left child, and ``N_t_R`` is the number of samples in the right child.\n844 \n845         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n846         if ``sample_weight`` is passed.\n847 \n848         .. versionadded:: 0.19\n849 \n850     min_impurity_split : float, (default=1e-7)\n851         Threshold for early stopping in tree growth. A node will split\n852         if its impurity is above the threshold, otherwise it is a leaf.\n853 \n854         .. deprecated:: 0.19\n855            ``min_impurity_split`` has been deprecated in favor of\n856            ``min_impurity_decrease`` in 0.19. The default value of\n857            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n858            will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n859 \n860 \n861     bootstrap : boolean, optional (default=True)\n862         Whether bootstrap samples are used when building trees. If False, the\n863         whole datset is used to build each tree.\n864 \n865     oob_score : bool (default=False)\n866         Whether to use out-of-bag samples to estimate\n867         the generalization accuracy.\n868 \n869     n_jobs : int or None, optional (default=None)\n870         The number of jobs to run in parallel for both `fit` and `predict`.\n871         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n872         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n873         for more details.\n874 \n875     random_state : int, RandomState instance or None, optional (default=None)\n876         If int, random_state is the seed used by the random number generator;\n877         If RandomState instance, random_state is the random number generator;\n878         If None, the random number generator is the RandomState instance used\n879         by `np.random`.\n880 \n881     verbose : int, optional (default=0)\n882         Controls the verbosity when fitting and predicting.\n883 \n884     warm_start : bool, optional (default=False)\n885         When set to ``True``, reuse the solution of the previous call to fit\n886         and add more estimators to the ensemble, otherwise, just fit a whole\n887         new forest. See :term:`the Glossary <warm_start>`.\n888 \n889     class_weight : dict, list of dicts, \"balanced\", \"balanced_subsample\" or \\\n890     None, optional (default=None)\n891         Weights associated with classes in the form ``{class_label: weight}``.\n892         If not given, all classes are supposed to have weight one. For\n893         multi-output problems, a list of dicts can be provided in the same\n894         order as the columns of y.\n895 \n896         Note that for multioutput (including multilabel) weights should be\n897         defined for each class of every column in its own dict. For example,\n898         for four-class multilabel classification weights should be\n899         [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n900         [{1:1}, {2:5}, {3:1}, {4:1}].\n901 \n902         The \"balanced\" mode uses the values of y to automatically adjust\n903         weights inversely proportional to class frequencies in the input data\n904         as ``n_samples / (n_classes * np.bincount(y))``\n905 \n906         The \"balanced_subsample\" mode is the same as \"balanced\" except that\n907         weights are computed based on the bootstrap sample for every tree\n908         grown.\n909 \n910         For multi-output, the weights of each column of y will be multiplied.\n911 \n912         Note that these weights will be multiplied with sample_weight (passed\n913         through the fit method) if sample_weight is specified.\n914 \n915     Attributes\n916     ----------\n917     estimators_ : list of DecisionTreeClassifier\n918         The collection of fitted sub-estimators.\n919 \n920     classes_ : array of shape = [n_classes] or a list of such arrays\n921         The classes labels (single output problem), or a list of arrays of\n922         class labels (multi-output problem).\n923 \n924     n_classes_ : int or list\n925         The number of classes (single output problem), or a list containing the\n926         number of classes for each output (multi-output problem).\n927 \n928     n_features_ : int\n929         The number of features when ``fit`` is performed.\n930 \n931     n_outputs_ : int\n932         The number of outputs when ``fit`` is performed.\n933 \n934     feature_importances_ : array of shape = [n_features]\n935         The feature importances (the higher, the more important the feature).\n936 \n937     oob_score_ : float\n938         Score of the training dataset obtained using an out-of-bag estimate.\n939 \n940     oob_decision_function_ : array of shape = [n_samples, n_classes]\n941         Decision function computed with out-of-bag estimate on the training\n942         set. If n_estimators is small it might be possible that a data point\n943         was never left out during the bootstrap. In this case,\n944         `oob_decision_function_` might contain NaN.\n945 \n946     Examples\n947     --------\n948     >>> from sklearn.ensemble import RandomForestClassifier\n949     >>> from sklearn.datasets import make_classification\n950 \n951     >>> X, y = make_classification(n_samples=1000, n_features=4,\n952     ...                            n_informative=2, n_redundant=0,\n953     ...                            random_state=0, shuffle=False)\n954     >>> clf = RandomForestClassifier(n_estimators=100, max_depth=2,\n955     ...                              random_state=0)\n956     >>> clf.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE\n957     RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n958                 max_depth=2, max_features='auto', max_leaf_nodes=None,\n959                 min_impurity_decrease=0.0, min_impurity_split=None,\n960                 min_samples_leaf=1, min_samples_split=2,\n961                 min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n962                 oob_score=False, random_state=0, verbose=0, warm_start=False)\n963     >>> print(clf.feature_importances_)\n964     [0.14205973 0.76664038 0.0282433  0.06305659]\n965     >>> print(clf.predict([[0, 0, 0, 0]]))\n966     [1]\n967 \n968     Notes\n969     -----\n970     The default values for the parameters controlling the size of the trees\n971     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n972     unpruned trees which can potentially be very large on some data sets. To\n973     reduce memory consumption, the complexity and size of the trees should be\n974     controlled by setting those parameter values.\n975 \n976     The features are always randomly permuted at each split. Therefore,\n977     the best found split may vary, even with the same training data,\n978     ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n979     of the criterion is identical for several splits enumerated during the\n980     search of the best split. To obtain a deterministic behaviour during\n981     fitting, ``random_state`` has to be fixed.\n982 \n983     References\n984     ----------\n985 \n986     .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n987 \n988     See also\n989     --------\n990     DecisionTreeClassifier, ExtraTreesClassifier\n991     \"\"\"\n992     def __init__(self,\n993                  n_estimators='warn',\n994                  criterion=\"gini\",\n995                  max_depth=None,\n996                  min_samples_split=2,\n997                  min_samples_leaf=1,\n998                  min_weight_fraction_leaf=0.,\n999                  max_features=\"auto\",\n1000                  max_leaf_nodes=None,\n1001                  min_impurity_decrease=0.,\n1002                  min_impurity_split=None,\n1003                  bootstrap=True,\n1004                  oob_score=False,\n1005                  n_jobs=None,\n1006                  random_state=None,\n1007                  verbose=0,\n1008                  warm_start=False,\n1009                  class_weight=None):\n1010         super().__init__(\n1011             base_estimator=DecisionTreeClassifier(),\n1012             n_estimators=n_estimators,\n1013             estimator_params=(\"criterion\", \"max_depth\", \"min_samples_split\",\n1014                               \"min_samples_leaf\", \"min_weight_fraction_leaf\",\n1015                               \"max_features\", \"max_leaf_nodes\",\n1016                               \"min_impurity_decrease\", \"min_impurity_split\",\n1017                               \"random_state\"),\n1018             bootstrap=bootstrap,\n1019             oob_score=oob_score,\n1020             n_jobs=n_jobs,\n1021             random_state=random_state,\n1022             verbose=verbose,\n1023             warm_start=warm_start,\n1024             class_weight=class_weight)\n1025 \n1026         self.criterion = criterion\n1027         self.max_depth = max_depth\n1028         self.min_samples_split = min_samples_split\n1029         self.min_samples_leaf = min_samples_leaf\n1030         self.min_weight_fraction_leaf = min_weight_fraction_leaf\n1031         self.max_features = max_features\n1032         self.max_leaf_nodes = max_leaf_nodes\n1033         self.min_impurity_decrease = min_impurity_decrease\n1034         self.min_impurity_split = min_impurity_split\n1035 \n1036 \n1037 class RandomForestRegressor(ForestRegressor):\n1038     \"\"\"A random forest regressor.\n1039 \n1040     A random forest is a meta estimator that fits a number of classifying\n1041     decision trees on various sub-samples of the dataset and uses averaging\n1042     to improve the predictive accuracy and control over-fitting.\n1043     The sub-sample size is always the same as the original\n1044     input sample size but the samples are drawn with replacement if\n1045     `bootstrap=True` (default).\n1046 \n1047     Read more in the :ref:`User Guide <forest>`.\n1048 \n1049     Parameters\n1050     ----------\n1051     n_estimators : integer, optional (default=10)\n1052         The number of trees in the forest.\n1053 \n1054         .. versionchanged:: 0.20\n1055            The default value of ``n_estimators`` will change from 10 in\n1056            version 0.20 to 100 in version 0.22.\n1057 \n1058     criterion : string, optional (default=\"mse\")\n1059         The function to measure the quality of a split. Supported criteria\n1060         are \"mse\" for the mean squared error, which is equal to variance\n1061         reduction as feature selection criterion, and \"mae\" for the mean\n1062         absolute error.\n1063 \n1064         .. versionadded:: 0.18\n1065            Mean Absolute Error (MAE) criterion.\n1066 \n1067     max_depth : integer or None, optional (default=None)\n1068         The maximum depth of the tree. If None, then nodes are expanded until\n1069         all leaves are pure or until all leaves contain less than\n1070         min_samples_split samples.\n1071 \n1072     min_samples_split : int, float, optional (default=2)\n1073         The minimum number of samples required to split an internal node:\n1074 \n1075         - If int, then consider `min_samples_split` as the minimum number.\n1076         - If float, then `min_samples_split` is a fraction and\n1077           `ceil(min_samples_split * n_samples)` are the minimum\n1078           number of samples for each split.\n1079 \n1080         .. versionchanged:: 0.18\n1081            Added float values for fractions.\n1082 \n1083     min_samples_leaf : int, float, optional (default=1)\n1084         The minimum number of samples required to be at a leaf node.\n1085         A split point at any depth will only be considered if it leaves at\n1086         least ``min_samples_leaf`` training samples in each of the left and\n1087         right branches.  This may have the effect of smoothing the model,\n1088         especially in regression.\n1089 \n1090         - If int, then consider `min_samples_leaf` as the minimum number.\n1091         - If float, then `min_samples_leaf` is a fraction and\n1092           `ceil(min_samples_leaf * n_samples)` are the minimum\n1093           number of samples for each node.\n1094 \n1095         .. versionchanged:: 0.18\n1096            Added float values for fractions.\n1097 \n1098     min_weight_fraction_leaf : float, optional (default=0.)\n1099         The minimum weighted fraction of the sum total of weights (of all\n1100         the input samples) required to be at a leaf node. Samples have\n1101         equal weight when sample_weight is not provided.\n1102 \n1103     max_features : int, float, string or None, optional (default=\"auto\")\n1104         The number of features to consider when looking for the best split:\n1105 \n1106         - If int, then consider `max_features` features at each split.\n1107         - If float, then `max_features` is a fraction and\n1108           `int(max_features * n_features)` features are considered at each\n1109           split.\n1110         - If \"auto\", then `max_features=n_features`.\n1111         - If \"sqrt\", then `max_features=sqrt(n_features)`.\n1112         - If \"log2\", then `max_features=log2(n_features)`.\n1113         - If None, then `max_features=n_features`.\n1114 \n1115         Note: the search for a split does not stop until at least one\n1116         valid partition of the node samples is found, even if it requires to\n1117         effectively inspect more than ``max_features`` features.\n1118 \n1119     max_leaf_nodes : int or None, optional (default=None)\n1120         Grow trees with ``max_leaf_nodes`` in best-first fashion.\n1121         Best nodes are defined as relative reduction in impurity.\n1122         If None then unlimited number of leaf nodes.\n1123 \n1124     min_impurity_decrease : float, optional (default=0.)\n1125         A node will be split if this split induces a decrease of the impurity\n1126         greater than or equal to this value.\n1127 \n1128         The weighted impurity decrease equation is the following::\n1129 \n1130             N_t / N * (impurity - N_t_R / N_t * right_impurity\n1131                                 - N_t_L / N_t * left_impurity)\n1132 \n1133         where ``N`` is the total number of samples, ``N_t`` is the number of\n1134         samples at the current node, ``N_t_L`` is the number of samples in the\n1135         left child, and ``N_t_R`` is the number of samples in the right child.\n1136 \n1137         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n1138         if ``sample_weight`` is passed.\n1139 \n1140         .. versionadded:: 0.19\n1141 \n1142     min_impurity_split : float, (default=1e-7)\n1143         Threshold for early stopping in tree growth. A node will split\n1144         if its impurity is above the threshold, otherwise it is a leaf.\n1145 \n1146         .. deprecated:: 0.19\n1147            ``min_impurity_split`` has been deprecated in favor of\n1148            ``min_impurity_decrease`` in 0.19. The default value of\n1149            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n1150            will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n1151 \n1152     bootstrap : boolean, optional (default=True)\n1153         Whether bootstrap samples are used when building trees. If False, the\n1154         whole datset is used to build each tree.\n1155 \n1156     oob_score : bool, optional (default=False)\n1157         whether to use out-of-bag samples to estimate\n1158         the R^2 on unseen data.\n1159 \n1160     n_jobs : int or None, optional (default=None)\n1161         The number of jobs to run in parallel for both `fit` and `predict`.\n1162         `None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n1163         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n1164         for more details.\n1165 \n1166     random_state : int, RandomState instance or None, optional (default=None)\n1167         If int, random_state is the seed used by the random number generator;\n1168         If RandomState instance, random_state is the random number generator;\n1169         If None, the random number generator is the RandomState instance used\n1170         by `np.random`.\n1171 \n1172     verbose : int, optional (default=0)\n1173         Controls the verbosity when fitting and predicting.\n1174 \n1175     warm_start : bool, optional (default=False)\n1176         When set to ``True``, reuse the solution of the previous call to fit\n1177         and add more estimators to the ensemble, otherwise, just fit a whole\n1178         new forest. See :term:`the Glossary <warm_start>`.\n1179 \n1180     Attributes\n1181     ----------\n1182     estimators_ : list of DecisionTreeRegressor\n1183         The collection of fitted sub-estimators.\n1184 \n1185     feature_importances_ : array of shape = [n_features]\n1186         The feature importances (the higher, the more important the feature).\n1187 \n1188     n_features_ : int\n1189         The number of features when ``fit`` is performed.\n1190 \n1191     n_outputs_ : int\n1192         The number of outputs when ``fit`` is performed.\n1193 \n1194     oob_score_ : float\n1195         Score of the training dataset obtained using an out-of-bag estimate.\n1196 \n1197     oob_prediction_ : array of shape = [n_samples]\n1198         Prediction computed with out-of-bag estimate on the training set.\n1199 \n1200     Examples\n1201     --------\n1202     >>> from sklearn.ensemble import RandomForestRegressor\n1203     >>> from sklearn.datasets import make_regression\n1204 \n1205     >>> X, y = make_regression(n_features=4, n_informative=2,\n1206     ...                        random_state=0, shuffle=False)\n1207     >>> regr = RandomForestRegressor(max_depth=2, random_state=0,\n1208     ...                              n_estimators=100)\n1209     >>> regr.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE\n1210     RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n1211                max_features='auto', max_leaf_nodes=None,\n1212                min_impurity_decrease=0.0, min_impurity_split=None,\n1213                min_samples_leaf=1, min_samples_split=2,\n1214                min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n1215                oob_score=False, random_state=0, verbose=0, warm_start=False)\n1216     >>> print(regr.feature_importances_)\n1217     [0.18146984 0.81473937 0.00145312 0.00233767]\n1218     >>> print(regr.predict([[0, 0, 0, 0]]))\n1219     [-8.32987858]\n1220 \n1221     Notes\n1222     -----\n1223     The default values for the parameters controlling the size of the trees\n1224     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n1225     unpruned trees which can potentially be very large on some data sets. To\n1226     reduce memory consumption, the complexity and size of the trees should be\n1227     controlled by setting those parameter values.\n1228 \n1229     The features are always randomly permuted at each split. Therefore,\n1230     the best found split may vary, even with the same training data,\n1231     ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n1232     of the criterion is identical for several splits enumerated during the\n1233     search of the best split. To obtain a deterministic behaviour during\n1234     fitting, ``random_state`` has to be fixed.\n1235 \n1236     The default value ``max_features=\"auto\"`` uses ``n_features``\n1237     rather than ``n_features / 3``. The latter was originally suggested in\n1238     [1], whereas the former was more recently justified empirically in [2].\n1239 \n1240     References\n1241     ----------\n1242 \n1243     .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n1244 \n1245     .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n1246            trees\", Machine Learning, 63(1), 3-42, 2006.\n1247 \n1248     See also\n1249     --------\n1250     DecisionTreeRegressor, ExtraTreesRegressor\n1251     \"\"\"\n1252     def __init__(self,\n1253                  n_estimators='warn',\n1254                  criterion=\"mse\",\n1255                  max_depth=None,\n1256                  min_samples_split=2,\n1257                  min_samples_leaf=1,\n1258                  min_weight_fraction_leaf=0.,\n1259                  max_features=\"auto\",\n1260                  max_leaf_nodes=None,\n1261                  min_impurity_decrease=0.,\n1262                  min_impurity_split=None,\n1263                  bootstrap=True,\n1264                  oob_score=False,\n1265                  n_jobs=None,\n1266                  random_state=None,\n1267                  verbose=0,\n1268                  warm_start=False):\n1269         super().__init__(\n1270             base_estimator=DecisionTreeRegressor(),\n1271             n_estimators=n_estimators,\n1272             estimator_params=(\"criterion\", \"max_depth\", \"min_samples_split\",\n1273                               \"min_samples_leaf\", \"min_weight_fraction_leaf\",\n1274                               \"max_features\", \"max_leaf_nodes\",\n1275                               \"min_impurity_decrease\", \"min_impurity_split\",\n1276                               \"random_state\"),\n1277             bootstrap=bootstrap,\n1278             oob_score=oob_score,\n1279             n_jobs=n_jobs,\n1280             random_state=random_state,\n1281             verbose=verbose,\n1282             warm_start=warm_start)\n1283 \n1284         self.criterion = criterion\n1285         self.max_depth = max_depth\n1286         self.min_samples_split = min_samples_split\n1287         self.min_samples_leaf = min_samples_leaf\n1288         self.min_weight_fraction_leaf = min_weight_fraction_leaf\n1289         self.max_features = max_features\n1290         self.max_leaf_nodes = max_leaf_nodes\n1291         self.min_impurity_decrease = min_impurity_decrease\n1292         self.min_impurity_split = min_impurity_split\n1293 \n1294 \n1295 class ExtraTreesClassifier(ForestClassifier):\n1296     \"\"\"An extra-trees classifier.\n1297 \n1298     This class implements a meta estimator that fits a number of\n1299     randomized decision trees (a.k.a. extra-trees) on various sub-samples\n1300     of the dataset and uses averaging to improve the predictive accuracy\n1301     and control over-fitting.\n1302 \n1303     Read more in the :ref:`User Guide <forest>`.\n1304 \n1305     Parameters\n1306     ----------\n1307     n_estimators : integer, optional (default=10)\n1308         The number of trees in the forest.\n1309 \n1310         .. versionchanged:: 0.20\n1311            The default value of ``n_estimators`` will change from 10 in\n1312            version 0.20 to 100 in version 0.22.\n1313 \n1314     criterion : string, optional (default=\"gini\")\n1315         The function to measure the quality of a split. Supported criteria are\n1316         \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n1317 \n1318     max_depth : integer or None, optional (default=None)\n1319         The maximum depth of the tree. If None, then nodes are expanded until\n1320         all leaves are pure or until all leaves contain less than\n1321         min_samples_split samples.\n1322 \n1323     min_samples_split : int, float, optional (default=2)\n1324         The minimum number of samples required to split an internal node:\n1325 \n1326         - If int, then consider `min_samples_split` as the minimum number.\n1327         - If float, then `min_samples_split` is a fraction and\n1328           `ceil(min_samples_split * n_samples)` are the minimum\n1329           number of samples for each split.\n1330 \n1331         .. versionchanged:: 0.18\n1332            Added float values for fractions.\n1333 \n1334     min_samples_leaf : int, float, optional (default=1)\n1335         The minimum number of samples required to be at a leaf node.\n1336         A split point at any depth will only be considered if it leaves at\n1337         least ``min_samples_leaf`` training samples in each of the left and\n1338         right branches.  This may have the effect of smoothing the model,\n1339         especially in regression.\n1340 \n1341         - If int, then consider `min_samples_leaf` as the minimum number.\n1342         - If float, then `min_samples_leaf` is a fraction and\n1343           `ceil(min_samples_leaf * n_samples)` are the minimum\n1344           number of samples for each node.\n1345 \n1346         .. versionchanged:: 0.18\n1347            Added float values for fractions.\n1348 \n1349     min_weight_fraction_leaf : float, optional (default=0.)\n1350         The minimum weighted fraction of the sum total of weights (of all\n1351         the input samples) required to be at a leaf node. Samples have\n1352         equal weight when sample_weight is not provided.\n1353 \n1354     max_features : int, float, string or None, optional (default=\"auto\")\n1355         The number of features to consider when looking for the best split:\n1356 \n1357         - If int, then consider `max_features` features at each split.\n1358         - If float, then `max_features` is a fraction and\n1359           `int(max_features * n_features)` features are considered at each\n1360           split.\n1361         - If \"auto\", then `max_features=sqrt(n_features)`.\n1362         - If \"sqrt\", then `max_features=sqrt(n_features)`.\n1363         - If \"log2\", then `max_features=log2(n_features)`.\n1364         - If None, then `max_features=n_features`.\n1365 \n1366         Note: the search for a split does not stop until at least one\n1367         valid partition of the node samples is found, even if it requires to\n1368         effectively inspect more than ``max_features`` features.\n1369 \n1370     max_leaf_nodes : int or None, optional (default=None)\n1371         Grow trees with ``max_leaf_nodes`` in best-first fashion.\n1372         Best nodes are defined as relative reduction in impurity.\n1373         If None then unlimited number of leaf nodes.\n1374 \n1375     min_impurity_decrease : float, optional (default=0.)\n1376         A node will be split if this split induces a decrease of the impurity\n1377         greater than or equal to this value.\n1378 \n1379         The weighted impurity decrease equation is the following::\n1380 \n1381             N_t / N * (impurity - N_t_R / N_t * right_impurity\n1382                                 - N_t_L / N_t * left_impurity)\n1383 \n1384         where ``N`` is the total number of samples, ``N_t`` is the number of\n1385         samples at the current node, ``N_t_L`` is the number of samples in the\n1386         left child, and ``N_t_R`` is the number of samples in the right child.\n1387 \n1388         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n1389         if ``sample_weight`` is passed.\n1390 \n1391         .. versionadded:: 0.19\n1392 \n1393     min_impurity_split : float, (default=1e-7)\n1394         Threshold for early stopping in tree growth. A node will split\n1395         if its impurity is above the threshold, otherwise it is a leaf.\n1396 \n1397         .. deprecated:: 0.19\n1398            ``min_impurity_split`` has been deprecated in favor of\n1399            ``min_impurity_decrease`` in 0.19. The default value of\n1400            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n1401            will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n1402 \n1403     bootstrap : boolean, optional (default=False)\n1404         Whether bootstrap samples are used when building trees. If False, the\n1405         whole datset is used to build each tree.\n1406 \n1407     oob_score : bool, optional (default=False)\n1408         Whether to use out-of-bag samples to estimate\n1409         the generalization accuracy.\n1410 \n1411     n_jobs : int or None, optional (default=None)\n1412         The number of jobs to run in parallel for both `fit` and `predict`.\n1413         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n1414         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n1415         for more details.\n1416 \n1417     random_state : int, RandomState instance or None, optional (default=None)\n1418         If int, random_state is the seed used by the random number generator;\n1419         If RandomState instance, random_state is the random number generator;\n1420         If None, the random number generator is the RandomState instance used\n1421         by `np.random`.\n1422 \n1423     verbose : int, optional (default=0)\n1424         Controls the verbosity when fitting and predicting.\n1425 \n1426     warm_start : bool, optional (default=False)\n1427         When set to ``True``, reuse the solution of the previous call to fit\n1428         and add more estimators to the ensemble, otherwise, just fit a whole\n1429         new forest. See :term:`the Glossary <warm_start>`.\n1430 \n1431     class_weight : dict, list of dicts, \"balanced\", \"balanced_subsample\" or \\\n1432     None, optional (default=None)\n1433         Weights associated with classes in the form ``{class_label: weight}``.\n1434         If not given, all classes are supposed to have weight one. For\n1435         multi-output problems, a list of dicts can be provided in the same\n1436         order as the columns of y.\n1437 \n1438         Note that for multioutput (including multilabel) weights should be\n1439         defined for each class of every column in its own dict. For example,\n1440         for four-class multilabel classification weights should be\n1441         [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n1442         [{1:1}, {2:5}, {3:1}, {4:1}].\n1443 \n1444         The \"balanced\" mode uses the values of y to automatically adjust\n1445         weights inversely proportional to class frequencies in the input data\n1446         as ``n_samples / (n_classes * np.bincount(y))``\n1447 \n1448         The \"balanced_subsample\" mode is the same as \"balanced\" except that weights are\n1449         computed based on the bootstrap sample for every tree grown.\n1450 \n1451         For multi-output, the weights of each column of y will be multiplied.\n1452 \n1453         Note that these weights will be multiplied with sample_weight (passed\n1454         through the fit method) if sample_weight is specified.\n1455 \n1456     Attributes\n1457     ----------\n1458     estimators_ : list of DecisionTreeClassifier\n1459         The collection of fitted sub-estimators.\n1460 \n1461     classes_ : array of shape = [n_classes] or a list of such arrays\n1462         The classes labels (single output problem), or a list of arrays of\n1463         class labels (multi-output problem).\n1464 \n1465     n_classes_ : int or list\n1466         The number of classes (single output problem), or a list containing the\n1467         number of classes for each output (multi-output problem).\n1468 \n1469     feature_importances_ : array of shape = [n_features]\n1470         The feature importances (the higher, the more important the feature).\n1471 \n1472     n_features_ : int\n1473         The number of features when ``fit`` is performed.\n1474 \n1475     n_outputs_ : int\n1476         The number of outputs when ``fit`` is performed.\n1477 \n1478     oob_score_ : float\n1479         Score of the training dataset obtained using an out-of-bag estimate.\n1480 \n1481     oob_decision_function_ : array of shape = [n_samples, n_classes]\n1482         Decision function computed with out-of-bag estimate on the training\n1483         set. If n_estimators is small it might be possible that a data point\n1484         was never left out during the bootstrap. In this case,\n1485         `oob_decision_function_` might contain NaN.\n1486 \n1487     Notes\n1488     -----\n1489     The default values for the parameters controlling the size of the trees\n1490     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n1491     unpruned trees which can potentially be very large on some data sets. To\n1492     reduce memory consumption, the complexity and size of the trees should be\n1493     controlled by setting those parameter values.\n1494 \n1495     References\n1496     ----------\n1497 \n1498     .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n1499            trees\", Machine Learning, 63(1), 3-42, 2006.\n1500 \n1501     See also\n1502     --------\n1503     sklearn.tree.ExtraTreeClassifier : Base classifier for this ensemble.\n1504     RandomForestClassifier : Ensemble Classifier based on trees with optimal\n1505         splits.\n1506     \"\"\"\n1507     def __init__(self,\n1508                  n_estimators='warn',\n1509                  criterion=\"gini\",\n1510                  max_depth=None,\n1511                  min_samples_split=2,\n1512                  min_samples_leaf=1,\n1513                  min_weight_fraction_leaf=0.,\n1514                  max_features=\"auto\",\n1515                  max_leaf_nodes=None,\n1516                  min_impurity_decrease=0.,\n1517                  min_impurity_split=None,\n1518                  bootstrap=False,\n1519                  oob_score=False,\n1520                  n_jobs=None,\n1521                  random_state=None,\n1522                  verbose=0,\n1523                  warm_start=False,\n1524                  class_weight=None):\n1525         super().__init__(\n1526             base_estimator=ExtraTreeClassifier(),\n1527             n_estimators=n_estimators,\n1528             estimator_params=(\"criterion\", \"max_depth\", \"min_samples_split\",\n1529                               \"min_samples_leaf\", \"min_weight_fraction_leaf\",\n1530                               \"max_features\", \"max_leaf_nodes\",\n1531                               \"min_impurity_decrease\", \"min_impurity_split\",\n1532                               \"random_state\"),\n1533             bootstrap=bootstrap,\n1534             oob_score=oob_score,\n1535             n_jobs=n_jobs,\n1536             random_state=random_state,\n1537             verbose=verbose,\n1538             warm_start=warm_start,\n1539             class_weight=class_weight)\n1540 \n1541         self.criterion = criterion\n1542         self.max_depth = max_depth\n1543         self.min_samples_split = min_samples_split\n1544         self.min_samples_leaf = min_samples_leaf\n1545         self.min_weight_fraction_leaf = min_weight_fraction_leaf\n1546         self.max_features = max_features\n1547         self.max_leaf_nodes = max_leaf_nodes\n1548         self.min_impurity_decrease = min_impurity_decrease\n1549         self.min_impurity_split = min_impurity_split\n1550 \n1551 \n1552 class ExtraTreesRegressor(ForestRegressor):\n1553     \"\"\"An extra-trees regressor.\n1554 \n1555     This class implements a meta estimator that fits a number of\n1556     randomized decision trees (a.k.a. extra-trees) on various sub-samples\n1557     of the dataset and uses averaging to improve the predictive accuracy\n1558     and control over-fitting.\n1559 \n1560     Read more in the :ref:`User Guide <forest>`.\n1561 \n1562     Parameters\n1563     ----------\n1564     n_estimators : integer, optional (default=10)\n1565         The number of trees in the forest.\n1566 \n1567         .. versionchanged:: 0.20\n1568            The default value of ``n_estimators`` will change from 10 in\n1569            version 0.20 to 100 in version 0.22.\n1570 \n1571     criterion : string, optional (default=\"mse\")\n1572         The function to measure the quality of a split. Supported criteria\n1573         are \"mse\" for the mean squared error, which is equal to variance\n1574         reduction as feature selection criterion, and \"mae\" for the mean\n1575         absolute error.\n1576 \n1577         .. versionadded:: 0.18\n1578            Mean Absolute Error (MAE) criterion.\n1579 \n1580     max_depth : integer or None, optional (default=None)\n1581         The maximum depth of the tree. If None, then nodes are expanded until\n1582         all leaves are pure or until all leaves contain less than\n1583         min_samples_split samples.\n1584 \n1585     min_samples_split : int, float, optional (default=2)\n1586         The minimum number of samples required to split an internal node:\n1587 \n1588         - If int, then consider `min_samples_split` as the minimum number.\n1589         - If float, then `min_samples_split` is a fraction and\n1590           `ceil(min_samples_split * n_samples)` are the minimum\n1591           number of samples for each split.\n1592 \n1593         .. versionchanged:: 0.18\n1594            Added float values for fractions.\n1595 \n1596     min_samples_leaf : int, float, optional (default=1)\n1597         The minimum number of samples required to be at a leaf node.\n1598         A split point at any depth will only be considered if it leaves at\n1599         least ``min_samples_leaf`` training samples in each of the left and\n1600         right branches.  This may have the effect of smoothing the model,\n1601         especially in regression.\n1602 \n1603         - If int, then consider `min_samples_leaf` as the minimum number.\n1604         - If float, then `min_samples_leaf` is a fraction and\n1605           `ceil(min_samples_leaf * n_samples)` are the minimum\n1606           number of samples for each node.\n1607 \n1608         .. versionchanged:: 0.18\n1609            Added float values for fractions.\n1610 \n1611     min_weight_fraction_leaf : float, optional (default=0.)\n1612         The minimum weighted fraction of the sum total of weights (of all\n1613         the input samples) required to be at a leaf node. Samples have\n1614         equal weight when sample_weight is not provided.\n1615 \n1616     max_features : int, float, string or None, optional (default=\"auto\")\n1617         The number of features to consider when looking for the best split:\n1618 \n1619         - If int, then consider `max_features` features at each split.\n1620         - If float, then `max_features` is a fraction and\n1621           `int(max_features * n_features)` features are considered at each\n1622           split.\n1623         - If \"auto\", then `max_features=n_features`.\n1624         - If \"sqrt\", then `max_features=sqrt(n_features)`.\n1625         - If \"log2\", then `max_features=log2(n_features)`.\n1626         - If None, then `max_features=n_features`.\n1627 \n1628         Note: the search for a split does not stop until at least one\n1629         valid partition of the node samples is found, even if it requires to\n1630         effectively inspect more than ``max_features`` features.\n1631 \n1632     max_leaf_nodes : int or None, optional (default=None)\n1633         Grow trees with ``max_leaf_nodes`` in best-first fashion.\n1634         Best nodes are defined as relative reduction in impurity.\n1635         If None then unlimited number of leaf nodes.\n1636 \n1637     min_impurity_decrease : float, optional (default=0.)\n1638         A node will be split if this split induces a decrease of the impurity\n1639         greater than or equal to this value.\n1640 \n1641         The weighted impurity decrease equation is the following::\n1642 \n1643             N_t / N * (impurity - N_t_R / N_t * right_impurity\n1644                                 - N_t_L / N_t * left_impurity)\n1645 \n1646         where ``N`` is the total number of samples, ``N_t`` is the number of\n1647         samples at the current node, ``N_t_L`` is the number of samples in the\n1648         left child, and ``N_t_R`` is the number of samples in the right child.\n1649 \n1650         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n1651         if ``sample_weight`` is passed.\n1652 \n1653         .. versionadded:: 0.19\n1654 \n1655     min_impurity_split : float, (default=1e-7)\n1656         Threshold for early stopping in tree growth. A node will split\n1657         if its impurity is above the threshold, otherwise it is a leaf.\n1658 \n1659         .. deprecated:: 0.19\n1660            ``min_impurity_split`` has been deprecated in favor of\n1661            ``min_impurity_decrease`` in 0.19. The default value of\n1662            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n1663            will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n1664 \n1665     bootstrap : boolean, optional (default=False)\n1666         Whether bootstrap samples are used when building trees. If False, the\n1667         whole datset is used to build each tree.\n1668 \n1669     oob_score : bool, optional (default=False)\n1670         Whether to use out-of-bag samples to estimate the R^2 on unseen data.\n1671 \n1672     n_jobs : int or None, optional (default=None)\n1673         The number of jobs to run in parallel for both `fit` and `predict`.\n1674         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n1675         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n1676         for more details.\n1677 \n1678     random_state : int, RandomState instance or None, optional (default=None)\n1679         If int, random_state is the seed used by the random number generator;\n1680         If RandomState instance, random_state is the random number generator;\n1681         If None, the random number generator is the RandomState instance used\n1682         by `np.random`.\n1683 \n1684     verbose : int, optional (default=0)\n1685         Controls the verbosity when fitting and predicting.\n1686 \n1687     warm_start : bool, optional (default=False)\n1688         When set to ``True``, reuse the solution of the previous call to fit\n1689         and add more estimators to the ensemble, otherwise, just fit a whole\n1690         new forest. See :term:`the Glossary <warm_start>`.\n1691 \n1692     Attributes\n1693     ----------\n1694     estimators_ : list of DecisionTreeRegressor\n1695         The collection of fitted sub-estimators.\n1696 \n1697     feature_importances_ : array of shape = [n_features]\n1698         The feature importances (the higher, the more important the feature).\n1699 \n1700     n_features_ : int\n1701         The number of features.\n1702 \n1703     n_outputs_ : int\n1704         The number of outputs.\n1705 \n1706     oob_score_ : float\n1707         Score of the training dataset obtained using an out-of-bag estimate.\n1708 \n1709     oob_prediction_ : array of shape = [n_samples]\n1710         Prediction computed with out-of-bag estimate on the training set.\n1711 \n1712     Notes\n1713     -----\n1714     The default values for the parameters controlling the size of the trees\n1715     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n1716     unpruned trees which can potentially be very large on some data sets. To\n1717     reduce memory consumption, the complexity and size of the trees should be\n1718     controlled by setting those parameter values.\n1719 \n1720     References\n1721     ----------\n1722 \n1723     .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n1724            Machine Learning, 63(1), 3-42, 2006.\n1725 \n1726     See also\n1727     --------\n1728     sklearn.tree.ExtraTreeRegressor: Base estimator for this ensemble.\n1729     RandomForestRegressor: Ensemble regressor using trees with optimal splits.\n1730     \"\"\"\n1731     def __init__(self,\n1732                  n_estimators='warn',\n1733                  criterion=\"mse\",\n1734                  max_depth=None,\n1735                  min_samples_split=2,\n1736                  min_samples_leaf=1,\n1737                  min_weight_fraction_leaf=0.,\n1738                  max_features=\"auto\",\n1739                  max_leaf_nodes=None,\n1740                  min_impurity_decrease=0.,\n1741                  min_impurity_split=None,\n1742                  bootstrap=False,\n1743                  oob_score=False,\n1744                  n_jobs=None,\n1745                  random_state=None,\n1746                  verbose=0,\n1747                  warm_start=False):\n1748         super().__init__(\n1749             base_estimator=ExtraTreeRegressor(),\n1750             n_estimators=n_estimators,\n1751             estimator_params=(\"criterion\", \"max_depth\", \"min_samples_split\",\n1752                               \"min_samples_leaf\", \"min_weight_fraction_leaf\",\n1753                               \"max_features\", \"max_leaf_nodes\",\n1754                               \"min_impurity_decrease\", \"min_impurity_split\",\n1755                               \"random_state\"),\n1756             bootstrap=bootstrap,\n1757             oob_score=oob_score,\n1758             n_jobs=n_jobs,\n1759             random_state=random_state,\n1760             verbose=verbose,\n1761             warm_start=warm_start)\n1762 \n1763         self.criterion = criterion\n1764         self.max_depth = max_depth\n1765         self.min_samples_split = min_samples_split\n1766         self.min_samples_leaf = min_samples_leaf\n1767         self.min_weight_fraction_leaf = min_weight_fraction_leaf\n1768         self.max_features = max_features\n1769         self.max_leaf_nodes = max_leaf_nodes\n1770         self.min_impurity_decrease = min_impurity_decrease\n1771         self.min_impurity_split = min_impurity_split\n1772 \n1773 \n1774 class RandomTreesEmbedding(BaseForest):\n1775     \"\"\"An ensemble of totally random trees.\n1776 \n1777     An unsupervised transformation of a dataset to a high-dimensional\n1778     sparse representation. A datapoint is coded according to which leaf of\n1779     each tree it is sorted into. Using a one-hot encoding of the leaves,\n1780     this leads to a binary coding with as many ones as there are trees in\n1781     the forest.\n1782 \n1783     The dimensionality of the resulting representation is\n1784     ``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``,\n1785     the number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.\n1786 \n1787     Read more in the :ref:`User Guide <random_trees_embedding>`.\n1788 \n1789     Parameters\n1790     ----------\n1791     n_estimators : integer, optional (default=10)\n1792         Number of trees in the forest.\n1793 \n1794         .. versionchanged:: 0.20\n1795            The default value of ``n_estimators`` will change from 10 in\n1796            version 0.20 to 100 in version 0.22.\n1797 \n1798     max_depth : integer, optional (default=5)\n1799         The maximum depth of each tree. If None, then nodes are expanded until\n1800         all leaves are pure or until all leaves contain less than\n1801         min_samples_split samples.\n1802 \n1803     min_samples_split : int, float, optional (default=2)\n1804         The minimum number of samples required to split an internal node:\n1805 \n1806         - If int, then consider `min_samples_split` as the minimum number.\n1807         - If float, then `min_samples_split` is a fraction and\n1808           `ceil(min_samples_split * n_samples)` is the minimum\n1809           number of samples for each split.\n1810 \n1811         .. versionchanged:: 0.18\n1812            Added float values for fractions.\n1813 \n1814     min_samples_leaf : int, float, optional (default=1)\n1815         The minimum number of samples required to be at a leaf node.\n1816         A split point at any depth will only be considered if it leaves at\n1817         least ``min_samples_leaf`` training samples in each of the left and\n1818         right branches.  This may have the effect of smoothing the model,\n1819         especially in regression.\n1820 \n1821         - If int, then consider `min_samples_leaf` as the minimum number.\n1822         - If float, then `min_samples_leaf` is a fraction and\n1823           `ceil(min_samples_leaf * n_samples)` is the minimum\n1824           number of samples for each node.\n1825 \n1826         .. versionchanged:: 0.18\n1827            Added float values for fractions.\n1828 \n1829     min_weight_fraction_leaf : float, optional (default=0.)\n1830         The minimum weighted fraction of the sum total of weights (of all\n1831         the input samples) required to be at a leaf node. Samples have\n1832         equal weight when sample_weight is not provided.\n1833 \n1834     max_leaf_nodes : int or None, optional (default=None)\n1835         Grow trees with ``max_leaf_nodes`` in best-first fashion.\n1836         Best nodes are defined as relative reduction in impurity.\n1837         If None then unlimited number of leaf nodes.\n1838 \n1839     min_impurity_decrease : float, optional (default=0.)\n1840         A node will be split if this split induces a decrease of the impurity\n1841         greater than or equal to this value.\n1842 \n1843         The weighted impurity decrease equation is the following::\n1844 \n1845             N_t / N * (impurity - N_t_R / N_t * right_impurity\n1846                                 - N_t_L / N_t * left_impurity)\n1847 \n1848         where ``N`` is the total number of samples, ``N_t`` is the number of\n1849         samples at the current node, ``N_t_L`` is the number of samples in the\n1850         left child, and ``N_t_R`` is the number of samples in the right child.\n1851 \n1852         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n1853         if ``sample_weight`` is passed.\n1854 \n1855         .. versionadded:: 0.19\n1856 \n1857     min_impurity_split : float, (default=1e-7)\n1858         Threshold for early stopping in tree growth. A node will split\n1859         if its impurity is above the threshold, otherwise it is a leaf.\n1860 \n1861         .. deprecated:: 0.19\n1862            ``min_impurity_split`` has been deprecated in favor of\n1863            ``min_impurity_decrease`` in 0.19. The default value of\n1864            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n1865            will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n1866 \n1867     sparse_output : bool, optional (default=True)\n1868         Whether or not to return a sparse CSR matrix, as default behavior,\n1869         or to return a dense array compatible with dense pipeline operators.\n1870 \n1871     n_jobs : int or None, optional (default=None)\n1872         The number of jobs to run in parallel for both `fit` and `predict`.\n1873         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n1874         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n1875         for more details.\n1876 \n1877     random_state : int, RandomState instance or None, optional (default=None)\n1878         If int, random_state is the seed used by the random number generator;\n1879         If RandomState instance, random_state is the random number generator;\n1880         If None, the random number generator is the RandomState instance used\n1881         by `np.random`.\n1882 \n1883     verbose : int, optional (default=0)\n1884         Controls the verbosity when fitting and predicting.\n1885 \n1886     warm_start : bool, optional (default=False)\n1887         When set to ``True``, reuse the solution of the previous call to fit\n1888         and add more estimators to the ensemble, otherwise, just fit a whole\n1889         new forest. See :term:`the Glossary <warm_start>`.\n1890 \n1891     Attributes\n1892     ----------\n1893     estimators_ : list of DecisionTreeClassifier\n1894         The collection of fitted sub-estimators.\n1895 \n1896     References\n1897     ----------\n1898     .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n1899            Machine Learning, 63(1), 3-42, 2006.\n1900     .. [2] Moosmann, F. and Triggs, B. and Jurie, F.  \"Fast discriminative\n1901            visual codebooks using randomized clustering forests\"\n1902            NIPS 2007\n1903 \n1904     \"\"\"\n1905 \n1906     criterion = 'mse'\n1907     max_features = 1\n1908 \n1909     def __init__(self,\n1910                  n_estimators='warn',\n1911                  max_depth=5,\n1912                  min_samples_split=2,\n1913                  min_samples_leaf=1,\n1914                  min_weight_fraction_leaf=0.,\n1915                  max_leaf_nodes=None,\n1916                  min_impurity_decrease=0.,\n1917                  min_impurity_split=None,\n1918                  sparse_output=True,\n1919                  n_jobs=None,\n1920                  random_state=None,\n1921                  verbose=0,\n1922                  warm_start=False):\n1923         super().__init__(\n1924             base_estimator=ExtraTreeRegressor(),\n1925             n_estimators=n_estimators,\n1926             estimator_params=(\"criterion\", \"max_depth\", \"min_samples_split\",\n1927                               \"min_samples_leaf\", \"min_weight_fraction_leaf\",\n1928                               \"max_features\", \"max_leaf_nodes\",\n1929                               \"min_impurity_decrease\", \"min_impurity_split\",\n1930                               \"random_state\"),\n1931             bootstrap=False,\n1932             oob_score=False,\n1933             n_jobs=n_jobs,\n1934             random_state=random_state,\n1935             verbose=verbose,\n1936             warm_start=warm_start)\n1937 \n1938         self.max_depth = max_depth\n1939         self.min_samples_split = min_samples_split\n1940         self.min_samples_leaf = min_samples_leaf\n1941         self.min_weight_fraction_leaf = min_weight_fraction_leaf\n1942         self.max_leaf_nodes = max_leaf_nodes\n1943         self.min_impurity_decrease = min_impurity_decrease\n1944         self.min_impurity_split = min_impurity_split\n1945         self.sparse_output = sparse_output\n1946 \n1947     def _set_oob_score(self, X, y):\n1948         raise NotImplementedError(\"OOB score not supported by tree embedding\")\n1949 \n1950     def fit(self, X, y=None, sample_weight=None):\n1951         \"\"\"Fit estimator.\n1952 \n1953         Parameters\n1954         ----------\n1955         X : array-like or sparse matrix, shape=(n_samples, n_features)\n1956             The input samples. Use ``dtype=np.float32`` for maximum\n1957             efficiency. Sparse matrices are also supported, use sparse\n1958             ``csc_matrix`` for maximum efficiency.\n1959 \n1960         sample_weight : array-like, shape = [n_samples] or None\n1961             Sample weights. If None, then samples are equally weighted. Splits\n1962             that would create child nodes with net zero or negative weight are\n1963             ignored while searching for a split in each node. In the case of\n1964             classification, splits are also ignored if they would result in any\n1965             single class carrying a negative weight in either child node.\n1966 \n1967         Returns\n1968         -------\n1969         self : object\n1970 \n1971         \"\"\"\n1972         self.fit_transform(X, y, sample_weight=sample_weight)\n1973         return self\n1974 \n1975     def fit_transform(self, X, y=None, sample_weight=None):\n1976         \"\"\"Fit estimator and transform dataset.\n1977 \n1978         Parameters\n1979         ----------\n1980         X : array-like or sparse matrix, shape=(n_samples, n_features)\n1981             Input data used to build forests. Use ``dtype=np.float32`` for\n1982             maximum efficiency.\n1983 \n1984         sample_weight : array-like, shape = [n_samples] or None\n1985             Sample weights. If None, then samples are equally weighted. Splits\n1986             that would create child nodes with net zero or negative weight are\n1987             ignored while searching for a split in each node. In the case of\n1988             classification, splits are also ignored if they would result in any\n1989             single class carrying a negative weight in either child node.\n1990 \n1991         Returns\n1992         -------\n1993         X_transformed : sparse matrix, shape=(n_samples, n_out)\n1994             Transformed dataset.\n1995         \"\"\"\n1996         X = check_array(X, accept_sparse=['csc'])\n1997         if issparse(X):\n1998             # Pre-sort indices to avoid that each individual tree of the\n1999             # ensemble sorts the indices.\n2000             X.sort_indices()\n2001 \n2002         rnd = check_random_state(self.random_state)\n2003         y = rnd.uniform(size=X.shape[0])\n2004         super().fit(X, y, sample_weight=sample_weight)\n2005 \n2006         self.one_hot_encoder_ = OneHotEncoder(sparse=self.sparse_output,\n2007                                               categories='auto')\n2008         return self.one_hot_encoder_.fit_transform(self.apply(X))\n2009 \n2010     def transform(self, X):\n2011         \"\"\"Transform dataset.\n2012 \n2013         Parameters\n2014         ----------\n2015         X : array-like or sparse matrix, shape=(n_samples, n_features)\n2016             Input data to be transformed. Use ``dtype=np.float32`` for maximum\n2017             efficiency. Sparse matrices are also supported, use sparse\n2018             ``csr_matrix`` for maximum efficiency.\n2019 \n2020         Returns\n2021         -------\n2022         X_transformed : sparse matrix, shape=(n_samples, n_out)\n2023             Transformed dataset.\n2024         \"\"\"\n2025         check_is_fitted(self, 'one_hot_encoder_')\n2026         return self.one_hot_encoder_.transform(self.apply(X))\n",
        "intended_behavior": "While this file handles Random Forests and similar ensemble methods, the validation approach extracted here might serve as a blueprint for modifications in weight_boosting.py. Ensure that any validation logic within this file allows for the flexible handling of input data types based on the capabilities of the constituent base estimators. It's about enabling a broader range of inputs without compromising on data integrity checks necessary for the algorithm to function correctly."
    },
    {
        "rel_file_path": "sklearn/ensemble/gradient_boosting.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/ensemble/gradient_boosting.py",
        "start": 1,
        "end": 2473,
        "class_name": null,
        "method_name": null,
        "code": "1 \"\"\"Gradient Boosted Regression Trees\n2 \n3 This module contains methods for fitting gradient boosted regression trees for\n4 both classification and regression.\n5 \n6 The module structure is the following:\n7 \n8 - The ``BaseGradientBoosting`` base class implements a common ``fit`` method\n9   for all the estimators in the module. Regression and classification\n10   only differ in the concrete ``LossFunction`` used.\n11 \n12 - ``GradientBoostingClassifier`` implements gradient boosting for\n13   classification problems.\n14 \n15 - ``GradientBoostingRegressor`` implements gradient boosting for\n16   regression problems.\n17 \"\"\"\n18 \n19 # Authors: Peter Prettenhofer, Scott White, Gilles Louppe, Emanuele Olivetti,\n20 #          Arnaud Joly, Jacob Schreiber\n21 # License: BSD 3 clause\n22 \n23 from abc import ABCMeta\n24 from abc import abstractmethod\n25 \n26 from .base import BaseEnsemble\n27 from ..base import ClassifierMixin\n28 from ..base import RegressorMixin\n29 \n30 from ._gradient_boosting import predict_stages\n31 from ._gradient_boosting import predict_stage\n32 from ._gradient_boosting import _random_sample_mask\n33 \n34 import numbers\n35 import numpy as np\n36 \n37 from scipy.sparse import csc_matrix\n38 from scipy.sparse import csr_matrix\n39 from scipy.sparse import issparse\n40 from scipy.special import expit\n41 \n42 from time import time\n43 from ..model_selection import train_test_split\n44 from ..tree.tree import DecisionTreeRegressor\n45 from ..tree._tree import DTYPE\n46 from ..tree._tree import TREE_LEAF\n47 \n48 from ..utils import check_random_state\n49 from ..utils import check_array\n50 from ..utils import check_X_y\n51 from ..utils import column_or_1d\n52 from ..utils import check_consistent_length\n53 from ..utils import deprecated\n54 from ..utils.fixes import logsumexp\n55 from ..utils.stats import _weighted_percentile\n56 from ..utils.validation import check_is_fitted\n57 from ..utils.multiclass import check_classification_targets\n58 from ..exceptions import NotFittedError\n59 \n60 \n61 class QuantileEstimator:\n62     \"\"\"An estimator predicting the alpha-quantile of the training targets.\n63 \n64     Parameters\n65     ----------\n66     alpha : float\n67         The quantile\n68     \"\"\"\n69     def __init__(self, alpha=0.9):\n70         if not 0 < alpha < 1.0:\n71             raise ValueError(\"`alpha` must be in (0, 1.0) but was %r\" % alpha)\n72         self.alpha = alpha\n73 \n74     def fit(self, X, y, sample_weight=None):\n75         \"\"\"Fit the estimator.\n76 \n77         Parameters\n78         ----------\n79         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n80             Training data\n81 \n82         y : array, shape (n_samples, n_targets)\n83             Target values. Will be cast to X's dtype if necessary\n84 \n85         sample_weight : numpy array of shape (n_samples,)\n86             Individual weights for each sample\n87         \"\"\"\n88         if sample_weight is None:\n89             self.quantile = np.percentile(y, self.alpha * 100.0)\n90         else:\n91             self.quantile = _weighted_percentile(y, sample_weight,\n92                                                  self.alpha * 100.0)\n93 \n94     def predict(self, X):\n95         \"\"\"Predict labels\n96 \n97         Parameters\n98         ----------\n99         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n100             Samples.\n101 \n102         Returns\n103         -------\n104         y : array, shape (n_samples,)\n105             Returns predicted values.\n106         \"\"\"\n107         check_is_fitted(self, 'quantile')\n108 \n109         y = np.empty((X.shape[0], 1), dtype=np.float64)\n110         y.fill(self.quantile)\n111         return y\n112 \n113 \n114 class MeanEstimator:\n115     \"\"\"An estimator predicting the mean of the training targets.\"\"\"\n116     def fit(self, X, y, sample_weight=None):\n117         \"\"\"Fit the estimator.\n118 \n119         Parameters\n120         ----------\n121         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n122             Training data\n123 \n124         y : array, shape (n_samples, n_targets)\n125             Target values. Will be cast to X's dtype if necessary\n126 \n127         sample_weight : numpy array of shape (n_samples,)\n128             Individual weights for each sample\n129         \"\"\"\n130         if sample_weight is None:\n131             self.mean = np.mean(y)\n132         else:\n133             self.mean = np.average(y, weights=sample_weight)\n134 \n135     def predict(self, X):\n136         \"\"\"Predict labels\n137 \n138         Parameters\n139         ----------\n140         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n141             Samples.\n142 \n143         Returns\n144         -------\n145         y : array, shape (n_samples,)\n146             Returns predicted values.\n147         \"\"\"\n148         check_is_fitted(self, 'mean')\n149 \n150         y = np.empty((X.shape[0], 1), dtype=np.float64)\n151         y.fill(self.mean)\n152         return y\n153 \n154 \n155 class LogOddsEstimator:\n156     \"\"\"An estimator predicting the log odds ratio.\"\"\"\n157     scale = 1.0\n158 \n159     def fit(self, X, y, sample_weight=None):\n160         \"\"\"Fit the estimator.\n161 \n162         Parameters\n163         ----------\n164         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n165             Training data\n166 \n167         y : array, shape (n_samples, n_targets)\n168             Target values. Will be cast to X's dtype if necessary\n169 \n170         sample_weight : numpy array of shape (n_samples,)\n171             Individual weights for each sample\n172         \"\"\"\n173         # pre-cond: pos, neg are encoded as 1, 0\n174         if sample_weight is None:\n175             pos = np.sum(y)\n176             neg = y.shape[0] - pos\n177         else:\n178             pos = np.sum(sample_weight * y)\n179             neg = np.sum(sample_weight * (1 - y))\n180 \n181         if neg == 0 or pos == 0:\n182             raise ValueError('y contains non binary labels.')\n183         self.prior = self.scale * np.log(pos / neg)\n184 \n185     def predict(self, X):\n186         \"\"\"Predict labels\n187 \n188         Parameters\n189         ----------\n190         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n191             Samples.\n192 \n193         Returns\n194         -------\n195         y : array, shape (n_samples,)\n196             Returns predicted values.\n197         \"\"\"\n198         check_is_fitted(self, 'prior')\n199 \n200         y = np.empty((X.shape[0], 1), dtype=np.float64)\n201         y.fill(self.prior)\n202         return y\n203 \n204 \n205 class ScaledLogOddsEstimator(LogOddsEstimator):\n206     \"\"\"Log odds ratio scaled by 0.5 -- for exponential loss. \"\"\"\n207     scale = 0.5\n208 \n209 \n210 class PriorProbabilityEstimator:\n211     \"\"\"An estimator predicting the probability of each\n212     class in the training data.\n213     \"\"\"\n214     def fit(self, X, y, sample_weight=None):\n215         \"\"\"Fit the estimator.\n216 \n217         Parameters\n218         ----------\n219         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n220             Training data\n221 \n222         y : array, shape (n_samples, n_targets)\n223             Target values. Will be cast to X's dtype if necessary\n224 \n225         sample_weight : array, shape (n_samples,)\n226             Individual weights for each sample\n227         \"\"\"\n228         if sample_weight is None:\n229             sample_weight = np.ones_like(y, dtype=np.float64)\n230         class_counts = np.bincount(y, weights=sample_weight)\n231         self.priors = class_counts / class_counts.sum()\n232 \n233     def predict(self, X):\n234         \"\"\"Predict labels\n235 \n236         Parameters\n237         ----------\n238         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n239             Samples.\n240 \n241         Returns\n242         -------\n243         y : array, shape (n_samples,)\n244             Returns predicted values.\n245         \"\"\"\n246         check_is_fitted(self, 'priors')\n247 \n248         y = np.empty((X.shape[0], self.priors.shape[0]), dtype=np.float64)\n249         y[:] = self.priors\n250         return y\n251 \n252 \n253 class ZeroEstimator:\n254     \"\"\"An estimator that simply predicts zero. \"\"\"\n255 \n256     def fit(self, X, y, sample_weight=None):\n257         \"\"\"Fit the estimator.\n258 \n259         Parameters\n260         ----------\n261         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n262             Training data\n263 \n264         y : numpy, shape (n_samples, n_targets)\n265             Target values. Will be cast to X's dtype if necessary\n266 \n267         sample_weight : array, shape (n_samples,)\n268             Individual weights for each sample\n269         \"\"\"\n270         if np.issubdtype(y.dtype, np.signedinteger):\n271             # classification\n272             self.n_classes = np.unique(y).shape[0]\n273             if self.n_classes == 2:\n274                 self.n_classes = 1\n275         else:\n276             # regression\n277             self.n_classes = 1\n278 \n279     def predict(self, X):\n280         \"\"\"Predict labels\n281 \n282         Parameters\n283         ----------\n284         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n285             Samples.\n286 \n287         Returns\n288         -------\n289         y : array, shape (n_samples,)\n290             Returns predicted values.\n291         \"\"\"\n292         check_is_fitted(self, 'n_classes')\n293 \n294         y = np.empty((X.shape[0], self.n_classes), dtype=np.float64)\n295         y.fill(0.0)\n296         return y\n297 \n298 \n299 class LossFunction(metaclass=ABCMeta):\n300     \"\"\"Abstract base class for various loss functions.\n301 \n302     Parameters\n303     ----------\n304     n_classes : int\n305         Number of classes\n306 \n307     Attributes\n308     ----------\n309     K : int\n310         The number of regression trees to be induced;\n311         1 for regression and binary classification;\n312         ``n_classes`` for multi-class classification.\n313     \"\"\"\n314 \n315     is_multi_class = False\n316 \n317     def __init__(self, n_classes):\n318         self.K = n_classes\n319 \n320     def init_estimator(self):\n321         \"\"\"Default ``init`` estimator for loss function. \"\"\"\n322         raise NotImplementedError()\n323 \n324     @abstractmethod\n325     def __call__(self, y, pred, sample_weight=None):\n326         \"\"\"Compute the loss.\n327 \n328         Parameters\n329         ----------\n330         y : array, shape (n_samples,)\n331             True labels\n332 \n333         pred : array, shape (n_samples,)\n334             Predicted labels\n335 \n336         sample_weight : array-like, shape (n_samples,), optional\n337             Sample weights.\n338         \"\"\"\n339 \n340     @abstractmethod\n341     def negative_gradient(self, y, y_pred, **kargs):\n342         \"\"\"Compute the negative gradient.\n343 \n344         Parameters\n345         ----------\n346         y : array, shape (n_samples,)\n347             The target labels.\n348 \n349         y_pred : array, shape (n_samples,)\n350             The predictions.\n351         \"\"\"\n352 \n353     def update_terminal_regions(self, tree, X, y, residual, y_pred,\n354                                 sample_weight, sample_mask,\n355                                 learning_rate=0.1, k=0):\n356         \"\"\"Update the terminal regions (=leaves) of the given tree and\n357         updates the current predictions of the model. Traverses tree\n358         and invokes template method `_update_terminal_region`.\n359 \n360         Parameters\n361         ----------\n362         tree : tree.Tree\n363             The tree object.\n364         X : array, shape (n, m)\n365             The data array.\n366         y : array, shape (n,)\n367             The target labels.\n368         residual : array, shape (n,)\n369             The residuals (usually the negative gradient).\n370         y_pred : array, shape (n,)\n371             The predictions.\n372         sample_weight : array, shape (n,)\n373             The weight of each sample.\n374         sample_mask : array, shape (n,)\n375             The sample mask to be used.\n376         learning_rate : float, default=0.1\n377             learning rate shrinks the contribution of each tree by\n378              ``learning_rate``.\n379         k : int, default 0\n380             The index of the estimator being updated.\n381 \n382         \"\"\"\n383         # compute leaf for each sample in ``X``.\n384         terminal_regions = tree.apply(X)\n385 \n386         # mask all which are not in sample mask.\n387         masked_terminal_regions = terminal_regions.copy()\n388         masked_terminal_regions[~sample_mask] = -1\n389 \n390         # update each leaf (= perform line search)\n391         for leaf in np.where(tree.children_left == TREE_LEAF)[0]:\n392             self._update_terminal_region(tree, masked_terminal_regions,\n393                                          leaf, X, y, residual,\n394                                          y_pred[:, k], sample_weight)\n395 \n396         # update predictions (both in-bag and out-of-bag)\n397         y_pred[:, k] += (learning_rate\n398                          * tree.value[:, 0, 0].take(terminal_regions, axis=0))\n399 \n400     @abstractmethod\n401     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n402                                 residual, pred, sample_weight):\n403         \"\"\"Template method for updating terminal regions (=leaves). \"\"\"\n404 \n405 \n406 class RegressionLossFunction(LossFunction, metaclass=ABCMeta):\n407     \"\"\"Base class for regression loss functions.\n408 \n409     Parameters\n410     ----------\n411     n_classes : int\n412         Number of classes\n413     \"\"\"\n414     def __init__(self, n_classes):\n415         if n_classes != 1:\n416             raise ValueError(\"``n_classes`` must be 1 for regression but \"\n417                              \"was %r\" % n_classes)\n418         super().__init__(n_classes)\n419 \n420 \n421 class LeastSquaresError(RegressionLossFunction):\n422     \"\"\"Loss function for least squares (LS) estimation.\n423     Terminal regions need not to be updated for least squares.\n424 \n425     Parameters\n426     ----------\n427     n_classes : int\n428         Number of classes\n429     \"\"\"\n430 \n431     def init_estimator(self):\n432         return MeanEstimator()\n433 \n434     def __call__(self, y, pred, sample_weight=None):\n435         \"\"\"Compute the least squares loss.\n436 \n437         Parameters\n438         ----------\n439         y : array, shape (n_samples,)\n440             True labels\n441 \n442         pred : array, shape (n_samples,)\n443             Predicted labels\n444 \n445         sample_weight : array-like, shape (n_samples,), optional\n446             Sample weights.\n447         \"\"\"\n448         if sample_weight is None:\n449             return np.mean((y - pred.ravel()) ** 2.0)\n450         else:\n451             return (1.0 / sample_weight.sum() *\n452                     np.sum(sample_weight * ((y - pred.ravel()) ** 2.0)))\n453 \n454     def negative_gradient(self, y, pred, **kargs):\n455         \"\"\"Compute the negative gradient.\n456 \n457         Parameters\n458         ----------\n459         y : array, shape (n_samples,)\n460             The target labels.\n461 \n462         pred : array, shape (n_samples,)\n463             The predictions.\n464         \"\"\"\n465         return y - pred.ravel()\n466 \n467     def update_terminal_regions(self, tree, X, y, residual, y_pred,\n468                                 sample_weight, sample_mask,\n469                                 learning_rate=0.1, k=0):\n470         \"\"\"Least squares does not need to update terminal regions.\n471 \n472         But it has to update the predictions.\n473 \n474         Parameters\n475         ----------\n476         tree : tree.Tree\n477             The tree object.\n478         X : array, shape (n, m)\n479             The data array.\n480         y : array, shape (n,)\n481             The target labels.\n482         residual : array, shape (n,)\n483             The residuals (usually the negative gradient).\n484         y_pred : array, shape (n,)\n485             The predictions.\n486         sample_weight : array, shape (n,)\n487             The weight of each sample.\n488         sample_mask : array, shape (n,)\n489             The sample mask to be used.\n490         learning_rate : float, default=0.1\n491             learning rate shrinks the contribution of each tree by\n492              ``learning_rate``.\n493         k : int, default 0\n494             The index of the estimator being updated.\n495         \"\"\"\n496         # update predictions\n497         y_pred[:, k] += learning_rate * tree.predict(X).ravel()\n498 \n499     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n500                                 residual, pred, sample_weight):\n501         pass\n502 \n503 \n504 class LeastAbsoluteError(RegressionLossFunction):\n505     \"\"\"Loss function for least absolute deviation (LAD) regression.\n506 \n507     Parameters\n508     ----------\n509     n_classes : int\n510         Number of classes\n511     \"\"\"\n512     def init_estimator(self):\n513         return QuantileEstimator(alpha=0.5)\n514 \n515     def __call__(self, y, pred, sample_weight=None):\n516         \"\"\"Compute the least absolute error.\n517 \n518         Parameters\n519         ----------\n520         y : array, shape (n_samples,)\n521             True labels\n522 \n523         pred : array, shape (n_samples,)\n524             Predicted labels\n525 \n526         sample_weight : array-like, shape (n_samples,), optional\n527             Sample weights.\n528         \"\"\"\n529         if sample_weight is None:\n530             return np.abs(y - pred.ravel()).mean()\n531         else:\n532             return (1.0 / sample_weight.sum() *\n533                     np.sum(sample_weight * np.abs(y - pred.ravel())))\n534 \n535     def negative_gradient(self, y, pred, **kargs):\n536         \"\"\"Compute the negative gradient.\n537 \n538         1.0 if y - pred > 0.0 else -1.0\n539 \n540         Parameters\n541         ----------\n542         y : array, shape (n_samples,)\n543             The target labels.\n544 \n545         pred : array, shape (n_samples,)\n546             The predictions.\n547         \"\"\"\n548         pred = pred.ravel()\n549         return 2.0 * (y - pred > 0.0) - 1.0\n550 \n551     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n552                                 residual, pred, sample_weight):\n553         \"\"\"LAD updates terminal regions to median estimates. \"\"\"\n554         terminal_region = np.where(terminal_regions == leaf)[0]\n555         sample_weight = sample_weight.take(terminal_region, axis=0)\n556         diff = y.take(terminal_region, axis=0) - pred.take(terminal_region, axis=0)\n557         tree.value[leaf, 0, 0] = _weighted_percentile(diff, sample_weight, percentile=50)\n558 \n559 \n560 class HuberLossFunction(RegressionLossFunction):\n561     \"\"\"Huber loss function for robust regression.\n562 \n563     M-Regression proposed in Friedman 2001.\n564 \n565     References\n566     ----------\n567     J. Friedman, Greedy Function Approximation: A Gradient Boosting\n568     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n569 \n570     Parameters\n571     ----------\n572     n_classes : int\n573         Number of classes\n574 \n575     alpha : float\n576         Percentile at which to extract score\n577     \"\"\"\n578 \n579     def __init__(self, n_classes, alpha=0.9):\n580         super().__init__(n_classes)\n581         self.alpha = alpha\n582         self.gamma = None\n583 \n584     def init_estimator(self):\n585         return QuantileEstimator(alpha=0.5)\n586 \n587     def __call__(self, y, pred, sample_weight=None):\n588         \"\"\"Compute the Huber loss.\n589 \n590         Parameters\n591         ----------\n592         y : array, shape (n_samples,)\n593             True labels\n594 \n595         pred : array, shape (n_samples,)\n596             Predicted labels\n597 \n598         sample_weight : array-like, shape (n_samples,), optional\n599             Sample weights.\n600         \"\"\"\n601         pred = pred.ravel()\n602         diff = y - pred\n603         gamma = self.gamma\n604         if gamma is None:\n605             if sample_weight is None:\n606                 gamma = np.percentile(np.abs(diff), self.alpha * 100)\n607             else:\n608                 gamma = _weighted_percentile(np.abs(diff), sample_weight, self.alpha * 100)\n609 \n610         gamma_mask = np.abs(diff) <= gamma\n611         if sample_weight is None:\n612             sq_loss = np.sum(0.5 * diff[gamma_mask] ** 2.0)\n613             lin_loss = np.sum(gamma * (np.abs(diff[~gamma_mask]) - gamma / 2.0))\n614             loss = (sq_loss + lin_loss) / y.shape[0]\n615         else:\n616             sq_loss = np.sum(0.5 * sample_weight[gamma_mask] * diff[gamma_mask] ** 2.0)\n617             lin_loss = np.sum(gamma * sample_weight[~gamma_mask] *\n618                               (np.abs(diff[~gamma_mask]) - gamma / 2.0))\n619             loss = (sq_loss + lin_loss) / sample_weight.sum()\n620         return loss\n621 \n622     def negative_gradient(self, y, pred, sample_weight=None, **kargs):\n623         \"\"\"Compute the negative gradient.\n624 \n625         Parameters\n626         ----------\n627         y : array, shape (n_samples,)\n628             The target labels.\n629 \n630         pred : array, shape (n_samples,)\n631             The predictions.\n632 \n633         sample_weight : array-like, shape (n_samples,), optional\n634             Sample weights.\n635         \"\"\"\n636         pred = pred.ravel()\n637         diff = y - pred\n638         if sample_weight is None:\n639             gamma = np.percentile(np.abs(diff), self.alpha * 100)\n640         else:\n641             gamma = _weighted_percentile(np.abs(diff), sample_weight, self.alpha * 100)\n642         gamma_mask = np.abs(diff) <= gamma\n643         residual = np.zeros((y.shape[0],), dtype=np.float64)\n644         residual[gamma_mask] = diff[gamma_mask]\n645         residual[~gamma_mask] = gamma * np.sign(diff[~gamma_mask])\n646         self.gamma = gamma\n647         return residual\n648 \n649     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n650                                 residual, pred, sample_weight):\n651         terminal_region = np.where(terminal_regions == leaf)[0]\n652         sample_weight = sample_weight.take(terminal_region, axis=0)\n653         gamma = self.gamma\n654         diff = (y.take(terminal_region, axis=0)\n655                 - pred.take(terminal_region, axis=0))\n656         median = _weighted_percentile(diff, sample_weight, percentile=50)\n657         diff_minus_median = diff - median\n658         tree.value[leaf, 0] = median + np.mean(\n659             np.sign(diff_minus_median) *\n660             np.minimum(np.abs(diff_minus_median), gamma))\n661 \n662 \n663 class QuantileLossFunction(RegressionLossFunction):\n664     \"\"\"Loss function for quantile regression.\n665 \n666     Quantile regression allows to estimate the percentiles\n667     of the conditional distribution of the target.\n668 \n669     Parameters\n670     ----------\n671     n_classes : int\n672         Number of classes.\n673 \n674     alpha : float, optional (default = 0.9)\n675         The percentile\n676     \"\"\"\n677     def __init__(self, n_classes, alpha=0.9):\n678         super().__init__(n_classes)\n679         self.alpha = alpha\n680         self.percentile = alpha * 100.0\n681 \n682     def init_estimator(self):\n683         return QuantileEstimator(self.alpha)\n684 \n685     def __call__(self, y, pred, sample_weight=None):\n686         \"\"\"Compute the Quantile loss.\n687 \n688         Parameters\n689         ----------\n690         y : array, shape (n_samples,)\n691             True labels\n692 \n693         pred : array, shape (n_samples,)\n694             Predicted labels\n695 \n696         sample_weight : array-like, shape (n_samples,), optional\n697             Sample weights.\n698         \"\"\"\n699         pred = pred.ravel()\n700         diff = y - pred\n701         alpha = self.alpha\n702 \n703         mask = y > pred\n704         if sample_weight is None:\n705             loss = (alpha * diff[mask].sum() -\n706                     (1.0 - alpha) * diff[~mask].sum()) / y.shape[0]\n707         else:\n708             loss = ((alpha * np.sum(sample_weight[mask] * diff[mask]) -\n709                     (1.0 - alpha) * np.sum(sample_weight[~mask] * diff[~mask])) /\n710                     sample_weight.sum())\n711         return loss\n712 \n713     def negative_gradient(self, y, pred, **kargs):\n714         \"\"\"Compute the negative gradient.\n715 \n716         Parameters\n717         ----------\n718         y : array, shape (n_samples,)\n719             The target labels.\n720 \n721         pred : array, shape (n_samples,)\n722             The predictions.\n723         \"\"\"\n724         alpha = self.alpha\n725         pred = pred.ravel()\n726         mask = y > pred\n727         return (alpha * mask) - ((1.0 - alpha) * ~mask)\n728 \n729     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n730                                 residual, pred, sample_weight):\n731         terminal_region = np.where(terminal_regions == leaf)[0]\n732         diff = (y.take(terminal_region, axis=0)\n733                 - pred.take(terminal_region, axis=0))\n734         sample_weight = sample_weight.take(terminal_region, axis=0)\n735 \n736         val = _weighted_percentile(diff, sample_weight, self.percentile)\n737         tree.value[leaf, 0] = val\n738 \n739 \n740 class ClassificationLossFunction(LossFunction, metaclass=ABCMeta):\n741     \"\"\"Base class for classification loss functions. \"\"\"\n742 \n743     def _score_to_proba(self, score):\n744         \"\"\"Template method to convert scores to probabilities.\n745 \n746          the does not support probabilities raises AttributeError.\n747         \"\"\"\n748         raise TypeError('%s does not support predict_proba' % type(self).__name__)\n749 \n750     @abstractmethod\n751     def _score_to_decision(self, score):\n752         \"\"\"Template method to convert scores to decisions.\n753 \n754         Returns int arrays.\n755         \"\"\"\n756 \n757 \n758 class BinomialDeviance(ClassificationLossFunction):\n759     \"\"\"Binomial deviance loss function for binary classification.\n760 \n761     Binary classification is a special case; here, we only need to\n762     fit one tree instead of ``n_classes`` trees.\n763 \n764     Parameters\n765     ----------\n766     n_classes : int\n767         Number of classes.\n768     \"\"\"\n769     def __init__(self, n_classes):\n770         if n_classes != 2:\n771             raise ValueError(\"{0:s} requires 2 classes; got {1:d} class(es)\"\n772                              .format(self.__class__.__name__, n_classes))\n773         # we only need to fit one tree for binary clf.\n774         super().__init__(1)\n775 \n776     def init_estimator(self):\n777         return LogOddsEstimator()\n778 \n779     def __call__(self, y, pred, sample_weight=None):\n780         \"\"\"Compute the deviance (= 2 * negative log-likelihood).\n781 \n782         Parameters\n783         ----------\n784         y : array, shape (n_samples,)\n785             True labels\n786 \n787         pred : array, shape (n_samples,)\n788             Predicted labels\n789 \n790         sample_weight : array-like, shape (n_samples,), optional\n791             Sample weights.\n792         \"\"\"\n793         # logaddexp(0, v) == log(1.0 + exp(v))\n794         pred = pred.ravel()\n795         if sample_weight is None:\n796             return -2.0 * np.mean((y * pred) - np.logaddexp(0.0, pred))\n797         else:\n798             return (-2.0 / sample_weight.sum() *\n799                     np.sum(sample_weight * ((y * pred) - np.logaddexp(0.0, pred))))\n800 \n801     def negative_gradient(self, y, pred, **kargs):\n802         \"\"\"Compute the residual (= negative gradient).\n803 \n804         Parameters\n805         ----------\n806         y : array, shape (n_samples,)\n807             True labels\n808 \n809         pred : array, shape (n_samples,)\n810             Predicted labels\n811         \"\"\"\n812         return y - expit(pred.ravel())\n813 \n814     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n815                                 residual, pred, sample_weight):\n816         \"\"\"Make a single Newton-Raphson step.\n817 \n818         our node estimate is given by:\n819 \n820             sum(w * (y - prob)) / sum(w * prob * (1 - prob))\n821 \n822         we take advantage that: y - prob = residual\n823         \"\"\"\n824         terminal_region = np.where(terminal_regions == leaf)[0]\n825         residual = residual.take(terminal_region, axis=0)\n826         y = y.take(terminal_region, axis=0)\n827         sample_weight = sample_weight.take(terminal_region, axis=0)\n828 \n829         numerator = np.sum(sample_weight * residual)\n830         denominator = np.sum(sample_weight * (y - residual) * (1 - y + residual))\n831 \n832         # prevents overflow and division by zero\n833         if abs(denominator) < 1e-150:\n834             tree.value[leaf, 0, 0] = 0.0\n835         else:\n836             tree.value[leaf, 0, 0] = numerator / denominator\n837 \n838     def _score_to_proba(self, score):\n839         proba = np.ones((score.shape[0], 2), dtype=np.float64)\n840         proba[:, 1] = expit(score.ravel())\n841         proba[:, 0] -= proba[:, 1]\n842         return proba\n843 \n844     def _score_to_decision(self, score):\n845         proba = self._score_to_proba(score)\n846         return np.argmax(proba, axis=1)\n847 \n848 \n849 class MultinomialDeviance(ClassificationLossFunction):\n850     \"\"\"Multinomial deviance loss function for multi-class classification.\n851 \n852     For multi-class classification we need to fit ``n_classes`` trees at\n853     each stage.\n854 \n855     Parameters\n856     ----------\n857     n_classes : int\n858         Number of classes\n859     \"\"\"\n860 \n861     is_multi_class = True\n862 \n863     def __init__(self, n_classes):\n864         if n_classes < 3:\n865             raise ValueError(\"{0:s} requires more than 2 classes.\".format(\n866                 self.__class__.__name__))\n867         super().__init__(n_classes)\n868 \n869     def init_estimator(self):\n870         return PriorProbabilityEstimator()\n871 \n872     def __call__(self, y, pred, sample_weight=None):\n873         \"\"\"Compute the Multinomial deviance.\n874 \n875         Parameters\n876         ----------\n877         y : array, shape (n_samples,)\n878             True labels\n879 \n880         pred : array, shape (n_samples,)\n881             Predicted labels\n882 \n883         sample_weight : array-like, shape (n_samples,), optional\n884             Sample weights.\n885         \"\"\"\n886         # create one-hot label encoding\n887         Y = np.zeros((y.shape[0], self.K), dtype=np.float64)\n888         for k in range(self.K):\n889             Y[:, k] = y == k\n890 \n891         if sample_weight is None:\n892             return np.sum(-1 * (Y * pred).sum(axis=1) +\n893                           logsumexp(pred, axis=1))\n894         else:\n895             return np.sum(-1 * sample_weight * (Y * pred).sum(axis=1) +\n896                           logsumexp(pred, axis=1))\n897 \n898     def negative_gradient(self, y, pred, k=0, **kwargs):\n899         \"\"\"Compute negative gradient for the ``k``-th class.\n900 \n901         Parameters\n902         ----------\n903         y : array, shape (n_samples,)\n904             The target labels.\n905 \n906         pred : array, shape (n_samples,)\n907             The predictions.\n908 \n909         k : int, optional (default=0)\n910             The index of the class\n911         \"\"\"\n912         return y - np.nan_to_num(np.exp(pred[:, k] -\n913                                         logsumexp(pred, axis=1)))\n914 \n915     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n916                                 residual, pred, sample_weight):\n917         \"\"\"Make a single Newton-Raphson step. \"\"\"\n918         terminal_region = np.where(terminal_regions == leaf)[0]\n919         residual = residual.take(terminal_region, axis=0)\n920         y = y.take(terminal_region, axis=0)\n921         sample_weight = sample_weight.take(terminal_region, axis=0)\n922 \n923         numerator = np.sum(sample_weight * residual)\n924         numerator *= (self.K - 1) / self.K\n925 \n926         denominator = np.sum(sample_weight * (y - residual) *\n927                              (1.0 - y + residual))\n928 \n929         # prevents overflow and division by zero\n930         if abs(denominator) < 1e-150:\n931             tree.value[leaf, 0, 0] = 0.0\n932         else:\n933             tree.value[leaf, 0, 0] = numerator / denominator\n934 \n935     def _score_to_proba(self, score):\n936         return np.nan_to_num(\n937             np.exp(score - (logsumexp(score, axis=1)[:, np.newaxis])))\n938 \n939     def _score_to_decision(self, score):\n940         proba = self._score_to_proba(score)\n941         return np.argmax(proba, axis=1)\n942 \n943 \n944 class ExponentialLoss(ClassificationLossFunction):\n945     \"\"\"Exponential loss function for binary classification.\n946 \n947     Same loss as AdaBoost.\n948 \n949     References\n950     ----------\n951     Greg Ridgeway, Generalized Boosted Models: A guide to the gbm package, 2007\n952 \n953     Parameters\n954     ----------\n955     n_classes : int\n956         Number of classes.\n957     \"\"\"\n958     def __init__(self, n_classes):\n959         if n_classes != 2:\n960             raise ValueError(\"{0:s} requires 2 classes; got {1:d} class(es)\"\n961                              .format(self.__class__.__name__, n_classes))\n962         # we only need to fit one tree for binary clf.\n963         super().__init__(1)\n964 \n965     def init_estimator(self):\n966         return ScaledLogOddsEstimator()\n967 \n968     def __call__(self, y, pred, sample_weight=None):\n969         \"\"\"Compute the exponential loss\n970 \n971         Parameters\n972         ----------\n973         y : array, shape (n_samples,)\n974             True labels\n975 \n976         pred : array, shape (n_samples,)\n977             Predicted labels\n978 \n979         sample_weight : array-like, shape (n_samples,), optional\n980             Sample weights.\n981         \"\"\"\n982         pred = pred.ravel()\n983         if sample_weight is None:\n984             return np.mean(np.exp(-(2. * y - 1.) * pred))\n985         else:\n986             return (1.0 / sample_weight.sum() *\n987                     np.sum(sample_weight * np.exp(-(2 * y - 1) * pred)))\n988 \n989     def negative_gradient(self, y, pred, **kargs):\n990         \"\"\"Compute the residual (= negative gradient).\n991 \n992         Parameters\n993         ----------\n994         y : array, shape (n_samples,)\n995             True labels\n996 \n997         pred : array, shape (n_samples,)\n998             Predicted labels\n999         \"\"\"\n1000         y_ = -(2. * y - 1.)\n1001         return y_ * np.exp(y_ * pred.ravel())\n1002 \n1003     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n1004                                 residual, pred, sample_weight):\n1005         terminal_region = np.where(terminal_regions == leaf)[0]\n1006         pred = pred.take(terminal_region, axis=0)\n1007         y = y.take(terminal_region, axis=0)\n1008         sample_weight = sample_weight.take(terminal_region, axis=0)\n1009 \n1010         y_ = 2. * y - 1.\n1011 \n1012         numerator = np.sum(y_ * sample_weight * np.exp(-y_ * pred))\n1013         denominator = np.sum(sample_weight * np.exp(-y_ * pred))\n1014 \n1015         # prevents overflow and division by zero\n1016         if abs(denominator) < 1e-150:\n1017             tree.value[leaf, 0, 0] = 0.0\n1018         else:\n1019             tree.value[leaf, 0, 0] = numerator / denominator\n1020 \n1021     def _score_to_proba(self, score):\n1022         proba = np.ones((score.shape[0], 2), dtype=np.float64)\n1023         proba[:, 1] = expit(2.0 * score.ravel())\n1024         proba[:, 0] -= proba[:, 1]\n1025         return proba\n1026 \n1027     def _score_to_decision(self, score):\n1028         return (score.ravel() >= 0.0).astype(np.int)\n1029 \n1030 \n1031 LOSS_FUNCTIONS = {'ls': LeastSquaresError,\n1032                   'lad': LeastAbsoluteError,\n1033                   'huber': HuberLossFunction,\n1034                   'quantile': QuantileLossFunction,\n1035                   'deviance': None,    # for both, multinomial and binomial\n1036                   'exponential': ExponentialLoss,\n1037                   }\n1038 \n1039 \n1040 INIT_ESTIMATORS = {'zero': ZeroEstimator}\n1041 \n1042 \n1043 class VerboseReporter:\n1044     \"\"\"Reports verbose output to stdout.\n1045 \n1046     Parameters\n1047     ----------\n1048     verbose : int\n1049         Verbosity level. If ``verbose==1`` output is printed once in a while\n1050         (when iteration mod verbose_mod is zero).; if larger than 1 then output\n1051         is printed for each update.\n1052     \"\"\"\n1053 \n1054     def __init__(self, verbose):\n1055         self.verbose = verbose\n1056 \n1057     def init(self, est, begin_at_stage=0):\n1058         \"\"\"Initialize reporter\n1059 \n1060         Parameters\n1061         ----------\n1062         est : Estimator\n1063             The estimator\n1064 \n1065         begin_at_stage : int\n1066             stage at which to begin reporting\n1067         \"\"\"\n1068         # header fields and line format str\n1069         header_fields = ['Iter', 'Train Loss']\n1070         verbose_fmt = ['{iter:>10d}', '{train_score:>16.4f}']\n1071         # do oob?\n1072         if est.subsample < 1:\n1073             header_fields.append('OOB Improve')\n1074             verbose_fmt.append('{oob_impr:>16.4f}')\n1075         header_fields.append('Remaining Time')\n1076         verbose_fmt.append('{remaining_time:>16s}')\n1077 \n1078         # print the header line\n1079         print(('%10s ' + '%16s ' *\n1080                (len(header_fields) - 1)) % tuple(header_fields))\n1081 \n1082         self.verbose_fmt = ' '.join(verbose_fmt)\n1083         # plot verbose info each time i % verbose_mod == 0\n1084         self.verbose_mod = 1\n1085         self.start_time = time()\n1086         self.begin_at_stage = begin_at_stage\n1087 \n1088     def update(self, j, est):\n1089         \"\"\"Update reporter with new iteration.\n1090 \n1091         Parameters\n1092         ----------\n1093         j : int\n1094             The new iteration\n1095         est : Estimator\n1096             The estimator\n1097         \"\"\"\n1098         do_oob = est.subsample < 1\n1099         # we need to take into account if we fit additional estimators.\n1100         i = j - self.begin_at_stage  # iteration relative to the start iter\n1101         if (i + 1) % self.verbose_mod == 0:\n1102             oob_impr = est.oob_improvement_[j] if do_oob else 0\n1103             remaining_time = ((est.n_estimators - (j + 1)) *\n1104                               (time() - self.start_time) / float(i + 1))\n1105             if remaining_time > 60:\n1106                 remaining_time = '{0:.2f}m'.format(remaining_time / 60.0)\n1107             else:\n1108                 remaining_time = '{0:.2f}s'.format(remaining_time)\n1109             print(self.verbose_fmt.format(iter=j + 1,\n1110                                           train_score=est.train_score_[j],\n1111                                           oob_impr=oob_impr,\n1112                                           remaining_time=remaining_time))\n1113             if self.verbose == 1 and ((i + 1) // (self.verbose_mod * 10) > 0):\n1114                 # adjust verbose frequency (powers of 10)\n1115                 self.verbose_mod *= 10\n1116 \n1117 \n1118 class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):\n1119     \"\"\"Abstract base class for Gradient Boosting. \"\"\"\n1120 \n1121     @abstractmethod\n1122     def __init__(self, loss, learning_rate, n_estimators, criterion,\n1123                  min_samples_split, min_samples_leaf, min_weight_fraction_leaf,\n1124                  max_depth, min_impurity_decrease, min_impurity_split,\n1125                  init, subsample, max_features,\n1126                  random_state, alpha=0.9, verbose=0, max_leaf_nodes=None,\n1127                  warm_start=False, presort='auto',\n1128                  validation_fraction=0.1, n_iter_no_change=None,\n1129                  tol=1e-4):\n1130 \n1131         self.n_estimators = n_estimators\n1132         self.learning_rate = learning_rate\n1133         self.loss = loss\n1134         self.criterion = criterion\n1135         self.min_samples_split = min_samples_split\n1136         self.min_samples_leaf = min_samples_leaf\n1137         self.min_weight_fraction_leaf = min_weight_fraction_leaf\n1138         self.subsample = subsample\n1139         self.max_features = max_features\n1140         self.max_depth = max_depth\n1141         self.min_impurity_decrease = min_impurity_decrease\n1142         self.min_impurity_split = min_impurity_split\n1143         self.init = init\n1144         self.random_state = random_state\n1145         self.alpha = alpha\n1146         self.verbose = verbose\n1147         self.max_leaf_nodes = max_leaf_nodes\n1148         self.warm_start = warm_start\n1149         self.presort = presort\n1150         self.validation_fraction = validation_fraction\n1151         self.n_iter_no_change = n_iter_no_change\n1152         self.tol = tol\n1153 \n1154     def _fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask,\n1155                    random_state, X_idx_sorted, X_csc=None, X_csr=None):\n1156         \"\"\"Fit another stage of ``n_classes_`` trees to the boosting model. \"\"\"\n1157 \n1158         assert sample_mask.dtype == np.bool\n1159         loss = self.loss_\n1160         original_y = y\n1161 \n1162         # Need to pass a copy of y_pred to negative_gradient() because y_pred\n1163         # is partially updated at the end of the loop in\n1164         # update_terminal_regions(), and gradients need to be evaluated at\n1165         # iteration i - 1.\n1166         y_pred_copy = y_pred.copy()\n1167 \n1168         for k in range(loss.K):\n1169             if loss.is_multi_class:\n1170                 y = np.array(original_y == k, dtype=np.float64)\n1171 \n1172             residual = loss.negative_gradient(y, y_pred_copy, k=k,\n1173                                               sample_weight=sample_weight)\n1174 \n1175             # induce regression tree on residuals\n1176             tree = DecisionTreeRegressor(\n1177                 criterion=self.criterion,\n1178                 splitter='best',\n1179                 max_depth=self.max_depth,\n1180                 min_samples_split=self.min_samples_split,\n1181                 min_samples_leaf=self.min_samples_leaf,\n1182                 min_weight_fraction_leaf=self.min_weight_fraction_leaf,\n1183                 min_impurity_decrease=self.min_impurity_decrease,\n1184                 min_impurity_split=self.min_impurity_split,\n1185                 max_features=self.max_features,\n1186                 max_leaf_nodes=self.max_leaf_nodes,\n1187                 random_state=random_state,\n1188                 presort=self.presort)\n1189 \n1190             if self.subsample < 1.0:\n1191                 # no inplace multiplication!\n1192                 sample_weight = sample_weight * sample_mask.astype(np.float64)\n1193 \n1194             X = X_csr if X_csr is not None else X\n1195             tree.fit(X, residual, sample_weight=sample_weight,\n1196                      check_input=False, X_idx_sorted=X_idx_sorted)\n1197 \n1198             # update tree leaves\n1199             loss.update_terminal_regions(tree.tree_, X, y, residual, y_pred,\n1200                                          sample_weight, sample_mask,\n1201                                          learning_rate=self.learning_rate, k=k)\n1202 \n1203             # add tree to ensemble\n1204             self.estimators_[i, k] = tree\n1205 \n1206         return y_pred\n1207 \n1208     def _check_params(self):\n1209         \"\"\"Check validity of parameters and raise ValueError if not valid. \"\"\"\n1210         if self.n_estimators <= 0:\n1211             raise ValueError(\"n_estimators must be greater than 0 but \"\n1212                              \"was %r\" % self.n_estimators)\n1213 \n1214         if self.learning_rate <= 0.0:\n1215             raise ValueError(\"learning_rate must be greater than 0 but \"\n1216                              \"was %r\" % self.learning_rate)\n1217 \n1218         if (self.loss not in self._SUPPORTED_LOSS\n1219                 or self.loss not in LOSS_FUNCTIONS):\n1220             raise ValueError(\"Loss '{0:s}' not supported. \".format(self.loss))\n1221 \n1222         if self.loss == 'deviance':\n1223             loss_class = (MultinomialDeviance\n1224                           if len(self.classes_) > 2\n1225                           else BinomialDeviance)\n1226         else:\n1227             loss_class = LOSS_FUNCTIONS[self.loss]\n1228 \n1229         if self.loss in ('huber', 'quantile'):\n1230             self.loss_ = loss_class(self.n_classes_, self.alpha)\n1231         else:\n1232             self.loss_ = loss_class(self.n_classes_)\n1233 \n1234         if not (0.0 < self.subsample <= 1.0):\n1235             raise ValueError(\"subsample must be in (0,1] but \"\n1236                              \"was %r\" % self.subsample)\n1237 \n1238         if self.init is not None:\n1239             if isinstance(self.init, str):\n1240                 if self.init not in INIT_ESTIMATORS:\n1241                     raise ValueError('init=\"%s\" is not supported' % self.init)\n1242             else:\n1243                 if (not hasattr(self.init, 'fit')\n1244                         or not hasattr(self.init, 'predict')):\n1245                     raise ValueError(\"init=%r must be valid BaseEstimator \"\n1246                                      \"and support both fit and \"\n1247                                      \"predict\" % self.init)\n1248 \n1249         if not (0.0 < self.alpha < 1.0):\n1250             raise ValueError(\"alpha must be in (0.0, 1.0) but \"\n1251                              \"was %r\" % self.alpha)\n1252 \n1253         if isinstance(self.max_features, str):\n1254             if self.max_features == \"auto\":\n1255                 # if is_classification\n1256                 if self.n_classes_ > 1:\n1257                     max_features = max(1, int(np.sqrt(self.n_features_)))\n1258                 else:\n1259                     # is regression\n1260                     max_features = self.n_features_\n1261             elif self.max_features == \"sqrt\":\n1262                 max_features = max(1, int(np.sqrt(self.n_features_)))\n1263             elif self.max_features == \"log2\":\n1264                 max_features = max(1, int(np.log2(self.n_features_)))\n1265             else:\n1266                 raise ValueError(\"Invalid value for max_features: %r. \"\n1267                                  \"Allowed string values are 'auto', 'sqrt' \"\n1268                                  \"or 'log2'.\" % self.max_features)\n1269         elif self.max_features is None:\n1270             max_features = self.n_features_\n1271         elif isinstance(self.max_features, (numbers.Integral, np.integer)):\n1272             max_features = self.max_features\n1273         else:  # float\n1274             if 0. < self.max_features <= 1.:\n1275                 max_features = max(int(self.max_features *\n1276                                        self.n_features_), 1)\n1277             else:\n1278                 raise ValueError(\"max_features must be in (0, n_features]\")\n1279 \n1280         self.max_features_ = max_features\n1281 \n1282         if not isinstance(self.n_iter_no_change,\n1283                           (numbers.Integral, np.integer, type(None))):\n1284             raise ValueError(\"n_iter_no_change should either be None or an \"\n1285                              \"integer. %r was passed\"\n1286                              % self.n_iter_no_change)\n1287 \n1288         allowed_presort = ('auto', True, False)\n1289         if self.presort not in allowed_presort:\n1290             raise ValueError(\"'presort' should be in {}. Got {!r} instead.\"\n1291                              .format(allowed_presort, self.presort))\n1292 \n1293     def _init_state(self):\n1294         \"\"\"Initialize model state and allocate model state data structures. \"\"\"\n1295 \n1296         if self.init is None:\n1297             self.init_ = self.loss_.init_estimator()\n1298         elif isinstance(self.init, str):\n1299             self.init_ = INIT_ESTIMATORS[self.init]()\n1300         else:\n1301             self.init_ = self.init\n1302 \n1303         self.estimators_ = np.empty((self.n_estimators, self.loss_.K),\n1304                                     dtype=np.object)\n1305         self.train_score_ = np.zeros((self.n_estimators,), dtype=np.float64)\n1306         # do oob?\n1307         if self.subsample < 1.0:\n1308             self.oob_improvement_ = np.zeros((self.n_estimators),\n1309                                              dtype=np.float64)\n1310 \n1311     def _clear_state(self):\n1312         \"\"\"Clear the state of the gradient boosting model. \"\"\"\n1313         if hasattr(self, 'estimators_'):\n1314             self.estimators_ = np.empty((0, 0), dtype=np.object)\n1315         if hasattr(self, 'train_score_'):\n1316             del self.train_score_\n1317         if hasattr(self, 'oob_improvement_'):\n1318             del self.oob_improvement_\n1319         if hasattr(self, 'init_'):\n1320             del self.init_\n1321         if hasattr(self, '_rng'):\n1322             del self._rng\n1323 \n1324     def _resize_state(self):\n1325         \"\"\"Add additional ``n_estimators`` entries to all attributes. \"\"\"\n1326         # self.n_estimators is the number of additional est to fit\n1327         total_n_estimators = self.n_estimators\n1328         if total_n_estimators < self.estimators_.shape[0]:\n1329             raise ValueError('resize with smaller n_estimators %d < %d' %\n1330                              (total_n_estimators, self.estimators_[0]))\n1331 \n1332         self.estimators_ = np.resize(self.estimators_,\n1333                                      (total_n_estimators, self.loss_.K))\n1334         self.train_score_ = np.resize(self.train_score_, total_n_estimators)\n1335         if (self.subsample < 1 or hasattr(self, 'oob_improvement_')):\n1336             # if do oob resize arrays or create new if not available\n1337             if hasattr(self, 'oob_improvement_'):\n1338                 self.oob_improvement_ = np.resize(self.oob_improvement_,\n1339                                                   total_n_estimators)\n1340             else:\n1341                 self.oob_improvement_ = np.zeros((total_n_estimators,),\n1342                                                  dtype=np.float64)\n1343 \n1344     def _is_initialized(self):\n1345         return len(getattr(self, 'estimators_', [])) > 0\n1346 \n1347     def _check_initialized(self):\n1348         \"\"\"Check that the estimator is initialized, raising an error if not.\"\"\"\n1349         check_is_fitted(self, 'estimators_')\n1350 \n1351     @property\n1352     @deprecated(\"Attribute n_features was deprecated in version 0.19 and \"\n1353                 \"will be removed in 0.21.\")\n1354     def n_features(self):\n1355         return self.n_features_\n1356 \n1357     def fit(self, X, y, sample_weight=None, monitor=None):\n1358         \"\"\"Fit the gradient boosting model.\n1359 \n1360         Parameters\n1361         ----------\n1362         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n1363             The input samples. Internally, it will be converted to\n1364             ``dtype=np.float32`` and if a sparse matrix is provided\n1365             to a sparse ``csr_matrix``.\n1366 \n1367         y : array-like, shape (n_samples,)\n1368             Target values (strings or integers in classification, real numbers\n1369             in regression)\n1370             For classification, labels must correspond to classes.\n1371 \n1372         sample_weight : array-like, shape (n_samples,) or None\n1373             Sample weights. If None, then samples are equally weighted. Splits\n1374             that would create child nodes with net zero or negative weight are\n1375             ignored while searching for a split in each node. In the case of\n1376             classification, splits are also ignored if they would result in any\n1377             single class carrying a negative weight in either child node.\n1378 \n1379         monitor : callable, optional\n1380             The monitor is called after each iteration with the current\n1381             iteration, a reference to the estimator and the local variables of\n1382             ``_fit_stages`` as keyword arguments ``callable(i, self,\n1383             locals())``. If the callable returns ``True`` the fitting procedure\n1384             is stopped. The monitor can be used for various things such as\n1385             computing held-out estimates, early stopping, model introspect, and\n1386             snapshoting.\n1387 \n1388         Returns\n1389         -------\n1390         self : object\n1391         \"\"\"\n1392         # if not warmstart - clear the estimator state\n1393         if not self.warm_start:\n1394             self._clear_state()\n1395 \n1396         # Check input\n1397         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)\n1398         n_samples, self.n_features_ = X.shape\n1399         if sample_weight is None:\n1400             sample_weight = np.ones(n_samples, dtype=np.float32)\n1401         else:\n1402             sample_weight = column_or_1d(sample_weight, warn=True)\n1403 \n1404         check_consistent_length(X, y, sample_weight)\n1405 \n1406         y = self._validate_y(y, sample_weight)\n1407 \n1408         if self.n_iter_no_change is not None:\n1409             X, X_val, y, y_val, sample_weight, sample_weight_val = (\n1410                 train_test_split(X, y, sample_weight,\n1411                                  random_state=self.random_state,\n1412                                  test_size=self.validation_fraction))\n1413         else:\n1414             X_val = y_val = sample_weight_val = None\n1415 \n1416         self._check_params()\n1417 \n1418         if not self._is_initialized():\n1419             # init state\n1420             self._init_state()\n1421 \n1422             # fit initial model - FIXME make sample_weight optional\n1423             self.init_.fit(X, y, sample_weight)\n1424 \n1425             # init predictions\n1426             y_pred = self.init_.predict(X)\n1427             begin_at_stage = 0\n1428 \n1429             # The rng state must be preserved if warm_start is True\n1430             self._rng = check_random_state(self.random_state)\n1431 \n1432         else:\n1433             # add more estimators to fitted model\n1434             # invariant: warm_start = True\n1435             if self.n_estimators < self.estimators_.shape[0]:\n1436                 raise ValueError('n_estimators=%d must be larger or equal to '\n1437                                  'estimators_.shape[0]=%d when '\n1438                                  'warm_start==True'\n1439                                  % (self.n_estimators,\n1440                                     self.estimators_.shape[0]))\n1441             begin_at_stage = self.estimators_.shape[0]\n1442             # The requirements of _decision_function (called in two lines\n1443             # below) are more constrained than fit. It accepts only CSR\n1444             # matrices.\n1445             X = check_array(X, dtype=DTYPE, order=\"C\", accept_sparse='csr')\n1446             y_pred = self._decision_function(X)\n1447             self._resize_state()\n1448 \n1449         if self.presort is True and issparse(X):\n1450             raise ValueError(\n1451                 \"Presorting is not supported for sparse matrices.\")\n1452 \n1453         presort = self.presort\n1454         # Allow presort to be 'auto', which means True if the dataset is dense,\n1455         # otherwise it will be False.\n1456         if presort == 'auto':\n1457             presort = not issparse(X)\n1458 \n1459         X_idx_sorted = None\n1460         if presort:\n1461             X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),\n1462                                              dtype=np.int32)\n1463 \n1464         # fit the boosting stages\n1465         n_stages = self._fit_stages(X, y, y_pred, sample_weight, self._rng,\n1466                                     X_val, y_val, sample_weight_val,\n1467                                     begin_at_stage, monitor, X_idx_sorted)\n1468 \n1469         # change shape of arrays after fit (early-stopping or additional ests)\n1470         if n_stages != self.estimators_.shape[0]:\n1471             self.estimators_ = self.estimators_[:n_stages]\n1472             self.train_score_ = self.train_score_[:n_stages]\n1473             if hasattr(self, 'oob_improvement_'):\n1474                 self.oob_improvement_ = self.oob_improvement_[:n_stages]\n1475 \n1476         self.n_estimators_ = n_stages\n1477         return self\n1478 \n1479     def _fit_stages(self, X, y, y_pred, sample_weight, random_state,\n1480                     X_val, y_val, sample_weight_val,\n1481                     begin_at_stage=0, monitor=None, X_idx_sorted=None):\n1482         \"\"\"Iteratively fits the stages.\n1483 \n1484         For each stage it computes the progress (OOB, train score)\n1485         and delegates to ``_fit_stage``.\n1486         Returns the number of stages fit; might differ from ``n_estimators``\n1487         due to early stopping.\n1488         \"\"\"\n1489         n_samples = X.shape[0]\n1490         do_oob = self.subsample < 1.0\n1491         sample_mask = np.ones((n_samples, ), dtype=np.bool)\n1492         n_inbag = max(1, int(self.subsample * n_samples))\n1493         loss_ = self.loss_\n1494 \n1495         # Set min_weight_leaf from min_weight_fraction_leaf\n1496         if self.min_weight_fraction_leaf != 0. and sample_weight is not None:\n1497             min_weight_leaf = (self.min_weight_fraction_leaf *\n1498                                np.sum(sample_weight))\n1499         else:\n1500             min_weight_leaf = 0.\n1501 \n1502         if self.verbose:\n1503             verbose_reporter = VerboseReporter(self.verbose)\n1504             verbose_reporter.init(self, begin_at_stage)\n1505 \n1506         X_csc = csc_matrix(X) if issparse(X) else None\n1507         X_csr = csr_matrix(X) if issparse(X) else None\n1508 \n1509         if self.n_iter_no_change is not None:\n1510             loss_history = np.full(self.n_iter_no_change, np.inf)\n1511             # We create a generator to get the predictions for X_val after\n1512             # the addition of each successive stage\n1513             y_val_pred_iter = self._staged_decision_function(X_val)\n1514 \n1515         # perform boosting iterations\n1516         i = begin_at_stage\n1517         for i in range(begin_at_stage, self.n_estimators):\n1518 \n1519             # subsampling\n1520             if do_oob:\n1521                 sample_mask = _random_sample_mask(n_samples, n_inbag,\n1522                                                   random_state)\n1523                 # OOB score before adding this stage\n1524                 old_oob_score = loss_(y[~sample_mask],\n1525                                       y_pred[~sample_mask],\n1526                                       sample_weight[~sample_mask])\n1527 \n1528             # fit next stage of trees\n1529             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n1530                                      sample_mask, random_state, X_idx_sorted,\n1531                                      X_csc, X_csr)\n1532 \n1533             # track deviance (= loss)\n1534             if do_oob:\n1535                 self.train_score_[i] = loss_(y[sample_mask],\n1536                                              y_pred[sample_mask],\n1537                                              sample_weight[sample_mask])\n1538                 self.oob_improvement_[i] = (\n1539                     old_oob_score - loss_(y[~sample_mask],\n1540                                           y_pred[~sample_mask],\n1541                                           sample_weight[~sample_mask]))\n1542             else:\n1543                 # no need to fancy index w/ no subsampling\n1544                 self.train_score_[i] = loss_(y, y_pred, sample_weight)\n1545 \n1546             if self.verbose > 0:\n1547                 verbose_reporter.update(i, self)\n1548 \n1549             if monitor is not None:\n1550                 early_stopping = monitor(i, self, locals())\n1551                 if early_stopping:\n1552                     break\n1553 \n1554             # We also provide an early stopping based on the score from\n1555             # validation set (X_val, y_val), if n_iter_no_change is set\n1556             if self.n_iter_no_change is not None:\n1557                 # By calling next(y_val_pred_iter), we get the predictions\n1558                 # for X_val after the addition of the current stage\n1559                 validation_loss = loss_(y_val, next(y_val_pred_iter),\n1560                                         sample_weight_val)\n1561 \n1562                 # Require validation_score to be better (less) than at least\n1563                 # one of the last n_iter_no_change evaluations\n1564                 if np.any(validation_loss + self.tol < loss_history):\n1565                     loss_history[i % len(loss_history)] = validation_loss\n1566                 else:\n1567                     break\n1568 \n1569         return i + 1\n1570 \n1571     def _make_estimator(self, append=True):\n1572         # we don't need _make_estimator\n1573         raise NotImplementedError()\n1574 \n1575     def _init_decision_function(self, X):\n1576         \"\"\"Check input and compute prediction of ``init``. \"\"\"\n1577         self._check_initialized()\n1578         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)\n1579         if X.shape[1] != self.n_features_:\n1580             raise ValueError(\"X.shape[1] should be {0:d}, not {1:d}.\".format(\n1581                 self.n_features_, X.shape[1]))\n1582         score = self.init_.predict(X).astype(np.float64)\n1583         return score\n1584 \n1585     def _decision_function(self, X):\n1586         # for use in inner loop, not raveling the output in single-class case,\n1587         # not doing input validation.\n1588         score = self._init_decision_function(X)\n1589         predict_stages(self.estimators_, X, self.learning_rate, score)\n1590         return score\n1591 \n1592 \n1593     def _staged_decision_function(self, X):\n1594         \"\"\"Compute decision function of ``X`` for each iteration.\n1595 \n1596         This method allows monitoring (i.e. determine error on testing set)\n1597         after each stage.\n1598 \n1599         Parameters\n1600         ----------\n1601         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n1602             The input samples. Internally, it will be converted to\n1603             ``dtype=np.float32`` and if a sparse matrix is provided\n1604             to a sparse ``csr_matrix``.\n1605 \n1606         Returns\n1607         -------\n1608         score : generator of array, shape (n_samples, k)\n1609             The decision function of the input samples. The order of the\n1610             classes corresponds to that in the attribute `classes_`.\n1611             Regression and binary classification are special cases with\n1612             ``k == 1``, otherwise ``k==n_classes``.\n1613         \"\"\"\n1614         X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')\n1615         score = self._init_decision_function(X)\n1616         for i in range(self.estimators_.shape[0]):\n1617             predict_stage(self.estimators_, i, X, self.learning_rate, score)\n1618             yield score.copy()\n1619 \n1620     @property\n1621     def feature_importances_(self):\n1622         \"\"\"Return the feature importances (the higher, the more important the\n1623            feature).\n1624 \n1625         Returns\n1626         -------\n1627         feature_importances_ : array, shape (n_features,)\n1628         \"\"\"\n1629         self._check_initialized()\n1630 \n1631         total_sum = np.zeros((self.n_features_, ), dtype=np.float64)\n1632         for stage in self.estimators_:\n1633             stage_sum = sum(tree.tree_.compute_feature_importances(\n1634                 normalize=False) for tree in stage) / len(stage)\n1635             total_sum += stage_sum\n1636 \n1637         importances = total_sum / total_sum.sum()\n1638         return importances\n1639 \n1640     def _validate_y(self, y, sample_weight):\n1641         # 'sample_weight' is not utilised but is used for\n1642         # consistency with similar method _validate_y of GBC\n1643         self.n_classes_ = 1\n1644         if y.dtype.kind == 'O':\n1645             y = y.astype(np.float64)\n1646         # Default implementation\n1647         return y\n1648 \n1649     def apply(self, X):\n1650         \"\"\"Apply trees in the ensemble to X, return leaf indices.\n1651 \n1652         .. versionadded:: 0.17\n1653 \n1654         Parameters\n1655         ----------\n1656         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n1657             The input samples. Internally, its dtype will be converted to\n1658             ``dtype=np.float32``. If a sparse matrix is provided, it will\n1659             be converted to a sparse ``csr_matrix``.\n1660 \n1661         Returns\n1662         -------\n1663         X_leaves : array-like, shape (n_samples, n_estimators, n_classes)\n1664             For each datapoint x in X and for each tree in the ensemble,\n1665             return the index of the leaf x ends up in each estimator.\n1666             In the case of binary classification n_classes is 1.\n1667         \"\"\"\n1668 \n1669         self._check_initialized()\n1670         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)\n1671 \n1672         # n_classes will be equal to 1 in the binary classification or the\n1673         # regression case.\n1674         n_estimators, n_classes = self.estimators_.shape\n1675         leaves = np.zeros((X.shape[0], n_estimators, n_classes))\n1676 \n1677         for i in range(n_estimators):\n1678             for j in range(n_classes):\n1679                 estimator = self.estimators_[i, j]\n1680                 leaves[:, i, j] = estimator.apply(X, check_input=False)\n1681 \n1682         return leaves\n1683 \n1684 \n1685 class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):\n1686     \"\"\"Gradient Boosting for classification.\n1687 \n1688     GB builds an additive model in a\n1689     forward stage-wise fashion; it allows for the optimization of\n1690     arbitrary differentiable loss functions. In each stage ``n_classes_``\n1691     regression trees are fit on the negative gradient of the\n1692     binomial or multinomial deviance loss function. Binary classification\n1693     is a special case where only a single regression tree is induced.\n1694 \n1695     Read more in the :ref:`User Guide <gradient_boosting>`.\n1696 \n1697     Parameters\n1698     ----------\n1699     loss : {'deviance', 'exponential'}, optional (default='deviance')\n1700         loss function to be optimized. 'deviance' refers to\n1701         deviance (= logistic regression) for classification\n1702         with probabilistic outputs. For loss 'exponential' gradient\n1703         boosting recovers the AdaBoost algorithm.\n1704 \n1705     learning_rate : float, optional (default=0.1)\n1706         learning rate shrinks the contribution of each tree by `learning_rate`.\n1707         There is a trade-off between learning_rate and n_estimators.\n1708 \n1709     n_estimators : int (default=100)\n1710         The number of boosting stages to perform. Gradient boosting\n1711         is fairly robust to over-fitting so a large number usually\n1712         results in better performance.\n1713 \n1714     subsample : float, optional (default=1.0)\n1715         The fraction of samples to be used for fitting the individual base\n1716         learners. If smaller than 1.0 this results in Stochastic Gradient\n1717         Boosting. `subsample` interacts with the parameter `n_estimators`.\n1718         Choosing `subsample < 1.0` leads to a reduction of variance\n1719         and an increase in bias.\n1720 \n1721     criterion : string, optional (default=\"friedman_mse\")\n1722         The function to measure the quality of a split. Supported criteria\n1723         are \"friedman_mse\" for the mean squared error with improvement\n1724         score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n1725         the mean absolute error. The default value of \"friedman_mse\" is\n1726         generally the best as it can provide a better approximation in\n1727         some cases.\n1728 \n1729         .. versionadded:: 0.18\n1730 \n1731     min_samples_split : int, float, optional (default=2)\n1732         The minimum number of samples required to split an internal node:\n1733 \n1734         - If int, then consider `min_samples_split` as the minimum number.\n1735         - If float, then `min_samples_split` is a fraction and\n1736           `ceil(min_samples_split * n_samples)` are the minimum\n1737           number of samples for each split.\n1738 \n1739         .. versionchanged:: 0.18\n1740            Added float values for fractions.\n1741 \n1742     min_samples_leaf : int, float, optional (default=1)\n1743         The minimum number of samples required to be at a leaf node.\n1744         A split point at any depth will only be considered if it leaves at\n1745         least ``min_samples_leaf`` training samples in each of the left and\n1746         right branches.  This may have the effect of smoothing the model,\n1747         especially in regression.\n1748 \n1749         - If int, then consider `min_samples_leaf` as the minimum number.\n1750         - If float, then `min_samples_leaf` is a fraction and\n1751           `ceil(min_samples_leaf * n_samples)` are the minimum\n1752           number of samples for each node.\n1753 \n1754         .. versionchanged:: 0.18\n1755            Added float values for fractions.\n1756 \n1757     min_weight_fraction_leaf : float, optional (default=0.)\n1758         The minimum weighted fraction of the sum total of weights (of all\n1759         the input samples) required to be at a leaf node. Samples have\n1760         equal weight when sample_weight is not provided.\n1761 \n1762     max_depth : integer, optional (default=3)\n1763         maximum depth of the individual regression estimators. The maximum\n1764         depth limits the number of nodes in the tree. Tune this parameter\n1765         for best performance; the best value depends on the interaction\n1766         of the input variables.\n1767 \n1768     min_impurity_decrease : float, optional (default=0.)\n1769         A node will be split if this split induces a decrease of the impurity\n1770         greater than or equal to this value.\n1771 \n1772         The weighted impurity decrease equation is the following::\n1773 \n1774             N_t / N * (impurity - N_t_R / N_t * right_impurity\n1775                                 - N_t_L / N_t * left_impurity)\n1776 \n1777         where ``N`` is the total number of samples, ``N_t`` is the number of\n1778         samples at the current node, ``N_t_L`` is the number of samples in the\n1779         left child, and ``N_t_R`` is the number of samples in the right child.\n1780 \n1781         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n1782         if ``sample_weight`` is passed.\n1783 \n1784         .. versionadded:: 0.19\n1785 \n1786     min_impurity_split : float, (default=1e-7)\n1787         Threshold for early stopping in tree growth. A node will split\n1788         if its impurity is above the threshold, otherwise it is a leaf.\n1789 \n1790         .. deprecated:: 0.19\n1791            ``min_impurity_split`` has been deprecated in favor of\n1792            ``min_impurity_decrease`` in 0.19. The default value of\n1793            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n1794            will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n1795 \n1796     init : estimator, optional\n1797         An estimator object that is used to compute the initial\n1798         predictions. ``init`` has to provide ``fit`` and ``predict``.\n1799         If None it uses ``loss.init_estimator``.\n1800 \n1801     random_state : int, RandomState instance or None, optional (default=None)\n1802         If int, random_state is the seed used by the random number generator;\n1803         If RandomState instance, random_state is the random number generator;\n1804         If None, the random number generator is the RandomState instance used\n1805         by `np.random`.\n1806 \n1807     max_features : int, float, string or None, optional (default=None)\n1808         The number of features to consider when looking for the best split:\n1809 \n1810         - If int, then consider `max_features` features at each split.\n1811         - If float, then `max_features` is a fraction and\n1812           `int(max_features * n_features)` features are considered at each\n1813           split.\n1814         - If \"auto\", then `max_features=sqrt(n_features)`.\n1815         - If \"sqrt\", then `max_features=sqrt(n_features)`.\n1816         - If \"log2\", then `max_features=log2(n_features)`.\n1817         - If None, then `max_features=n_features`.\n1818 \n1819         Choosing `max_features < n_features` leads to a reduction of variance\n1820         and an increase in bias.\n1821 \n1822         Note: the search for a split does not stop until at least one\n1823         valid partition of the node samples is found, even if it requires to\n1824         effectively inspect more than ``max_features`` features.\n1825 \n1826     verbose : int, default: 0\n1827         Enable verbose output. If 1 then it prints progress and performance\n1828         once in a while (the more trees the lower the frequency). If greater\n1829         than 1 then it prints progress and performance for every tree.\n1830 \n1831     max_leaf_nodes : int or None, optional (default=None)\n1832         Grow trees with ``max_leaf_nodes`` in best-first fashion.\n1833         Best nodes are defined as relative reduction in impurity.\n1834         If None then unlimited number of leaf nodes.\n1835 \n1836     warm_start : bool, default: False\n1837         When set to ``True``, reuse the solution of the previous call to fit\n1838         and add more estimators to the ensemble, otherwise, just erase the\n1839         previous solution. See :term:`the Glossary <warm_start>`.\n1840 \n1841     presort : bool or 'auto', optional (default='auto')\n1842         Whether to presort the data to speed up the finding of best splits in\n1843         fitting. Auto mode by default will use presorting on dense data and\n1844         default to normal sorting on sparse data. Setting presort to true on\n1845         sparse data will raise an error.\n1846 \n1847         .. versionadded:: 0.17\n1848            *presort* parameter.\n1849 \n1850     validation_fraction : float, optional, default 0.1\n1851         The proportion of training data to set aside as validation set for\n1852         early stopping. Must be between 0 and 1.\n1853         Only used if ``n_iter_no_change`` is set to an integer.\n1854 \n1855         .. versionadded:: 0.20\n1856 \n1857     n_iter_no_change : int, default None\n1858         ``n_iter_no_change`` is used to decide if early stopping will be used\n1859         to terminate training when validation score is not improving. By\n1860         default it is set to None to disable early stopping. If set to a\n1861         number, it will set aside ``validation_fraction`` size of the training\n1862         data as validation and terminate training when validation score is not\n1863         improving in all of the previous ``n_iter_no_change`` numbers of\n1864         iterations.\n1865 \n1866         .. versionadded:: 0.20\n1867 \n1868     tol : float, optional, default 1e-4\n1869         Tolerance for the early stopping. When the loss is not improving\n1870         by at least tol for ``n_iter_no_change`` iterations (if set to a\n1871         number), the training stops.\n1872 \n1873         .. versionadded:: 0.20\n1874 \n1875     Attributes\n1876     ----------\n1877     n_estimators_ : int\n1878         The number of estimators as selected by early stopping (if\n1879         ``n_iter_no_change`` is specified). Otherwise it is set to\n1880         ``n_estimators``.\n1881 \n1882         .. versionadded:: 0.20\n1883 \n1884     feature_importances_ : array, shape (n_features,)\n1885         The feature importances (the higher, the more important the feature).\n1886 \n1887     oob_improvement_ : array, shape (n_estimators,)\n1888         The improvement in loss (= deviance) on the out-of-bag samples\n1889         relative to the previous iteration.\n1890         ``oob_improvement_[0]`` is the improvement in\n1891         loss of the first stage over the ``init`` estimator.\n1892 \n1893     train_score_ : array, shape (n_estimators,)\n1894         The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n1895         model at iteration ``i`` on the in-bag sample.\n1896         If ``subsample == 1`` this is the deviance on the training data.\n1897 \n1898     loss_ : LossFunction\n1899         The concrete ``LossFunction`` object.\n1900 \n1901     init_ : estimator\n1902         The estimator that provides the initial predictions.\n1903         Set via the ``init`` argument or ``loss.init_estimator``.\n1904 \n1905     estimators_ : ndarray of DecisionTreeRegressor,\\\n1906 shape (n_estimators, ``loss_.K``)\n1907         The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary\n1908         classification, otherwise n_classes.\n1909 \n1910     Notes\n1911     -----\n1912     The features are always randomly permuted at each split. Therefore,\n1913     the best found split may vary, even with the same training data and\n1914     ``max_features=n_features``, if the improvement of the criterion is\n1915     identical for several splits enumerated during the search of the best\n1916     split. To obtain a deterministic behaviour during fitting,\n1917     ``random_state`` has to be fixed.\n1918 \n1919     See also\n1920     --------\n1921     sklearn.tree.DecisionTreeClassifier, RandomForestClassifier\n1922     AdaBoostClassifier\n1923 \n1924     References\n1925     ----------\n1926     J. Friedman, Greedy Function Approximation: A Gradient Boosting\n1927     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n1928 \n1929     J. Friedman, Stochastic Gradient Boosting, 1999\n1930 \n1931     T. Hastie, R. Tibshirani and J. Friedman.\n1932     Elements of Statistical Learning Ed. 2, Springer, 2009.\n1933     \"\"\"\n1934 \n1935     _SUPPORTED_LOSS = ('deviance', 'exponential')\n1936 \n1937     def __init__(self, loss='deviance', learning_rate=0.1, n_estimators=100,\n1938                  subsample=1.0, criterion='friedman_mse', min_samples_split=2,\n1939                  min_samples_leaf=1, min_weight_fraction_leaf=0.,\n1940                  max_depth=3, min_impurity_decrease=0.,\n1941                  min_impurity_split=None, init=None,\n1942                  random_state=None, max_features=None, verbose=0,\n1943                  max_leaf_nodes=None, warm_start=False,\n1944                  presort='auto', validation_fraction=0.1,\n1945                  n_iter_no_change=None, tol=1e-4):\n1946 \n1947         super().__init__(\n1948             loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,\n1949             criterion=criterion, min_samples_split=min_samples_split,\n1950             min_samples_leaf=min_samples_leaf,\n1951             min_weight_fraction_leaf=min_weight_fraction_leaf,\n1952             max_depth=max_depth, init=init, subsample=subsample,\n1953             max_features=max_features,\n1954             random_state=random_state, verbose=verbose,\n1955             max_leaf_nodes=max_leaf_nodes,\n1956             min_impurity_decrease=min_impurity_decrease,\n1957             min_impurity_split=min_impurity_split,\n1958             warm_start=warm_start, presort=presort,\n1959             validation_fraction=validation_fraction,\n1960             n_iter_no_change=n_iter_no_change, tol=tol)\n1961 \n1962     def _validate_y(self, y, sample_weight):\n1963         check_classification_targets(y)\n1964         self.classes_, y = np.unique(y, return_inverse=True)\n1965         n_trim_classes = np.count_nonzero(np.bincount(y, sample_weight))\n1966         if n_trim_classes < 2:\n1967             raise ValueError(\"y contains %d class after sample_weight \"\n1968                              \"trimmed classes with zero weights, while a \"\n1969                              \"minimum of 2 classes are required.\"\n1970                              % n_trim_classes)\n1971         self.n_classes_ = len(self.classes_)\n1972         return y\n1973 \n1974     def decision_function(self, X):\n1975         \"\"\"Compute the decision function of ``X``.\n1976 \n1977         Parameters\n1978         ----------\n1979         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n1980             The input samples. Internally, it will be converted to\n1981             ``dtype=np.float32`` and if a sparse matrix is provided\n1982             to a sparse ``csr_matrix``.\n1983 \n1984         Returns\n1985         -------\n1986         score : array, shape (n_samples, n_classes) or (n_samples,)\n1987             The decision function of the input samples. The order of the\n1988             classes corresponds to that in the attribute `classes_`.\n1989             Regression and binary classification produce an array of shape\n1990             [n_samples].\n1991         \"\"\"\n1992         X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')\n1993         score = self._decision_function(X)\n1994         if score.shape[1] == 1:\n1995             return score.ravel()\n1996         return score\n1997 \n1998     def staged_decision_function(self, X):\n1999         \"\"\"Compute decision function of ``X`` for each iteration.\n2000 \n2001         This method allows monitoring (i.e. determine error on testing set)\n2002         after each stage.\n2003 \n2004         Parameters\n2005         ----------\n2006         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2007             The input samples. Internally, it will be converted to\n2008             ``dtype=np.float32`` and if a sparse matrix is provided\n2009             to a sparse ``csr_matrix``.\n2010 \n2011         Returns\n2012         -------\n2013         score : generator of array, shape (n_samples, k)\n2014             The decision function of the input samples. The order of the\n2015             classes corresponds to that in the attribute `classes_`.\n2016             Regression and binary classification are special cases with\n2017             ``k == 1``, otherwise ``k==n_classes``.\n2018         \"\"\"\n2019         yield from self._staged_decision_function(X)\n2020 \n2021     def predict(self, X):\n2022         \"\"\"Predict class for X.\n2023 \n2024         Parameters\n2025         ----------\n2026         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2027             The input samples. Internally, it will be converted to\n2028             ``dtype=np.float32`` and if a sparse matrix is provided\n2029             to a sparse ``csr_matrix``.\n2030 \n2031         Returns\n2032         -------\n2033         y : array, shape (n_samples,)\n2034             The predicted values.\n2035         \"\"\"\n2036         score = self.decision_function(X)\n2037         decisions = self.loss_._score_to_decision(score)\n2038         return self.classes_.take(decisions, axis=0)\n2039 \n2040     def staged_predict(self, X):\n2041         \"\"\"Predict class at each stage for X.\n2042 \n2043         This method allows monitoring (i.e. determine error on testing set)\n2044         after each stage.\n2045 \n2046         Parameters\n2047         ----------\n2048         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2049             The input samples. Internally, it will be converted to\n2050             ``dtype=np.float32`` and if a sparse matrix is provided\n2051             to a sparse ``csr_matrix``.\n2052 \n2053         Returns\n2054         -------\n2055         y : generator of array of shape (n_samples,)\n2056             The predicted value of the input samples.\n2057         \"\"\"\n2058         for score in self._staged_decision_function(X):\n2059             decisions = self.loss_._score_to_decision(score)\n2060             yield self.classes_.take(decisions, axis=0)\n2061 \n2062     def predict_proba(self, X):\n2063         \"\"\"Predict class probabilities for X.\n2064 \n2065         Parameters\n2066         ----------\n2067         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2068             The input samples. Internally, it will be converted to\n2069             ``dtype=np.float32`` and if a sparse matrix is provided\n2070             to a sparse ``csr_matrix``.\n2071 \n2072         Raises\n2073         ------\n2074         AttributeError\n2075             If the ``loss`` does not support probabilities.\n2076 \n2077         Returns\n2078         -------\n2079         p : array, shape (n_samples, n_classes)\n2080             The class probabilities of the input samples. The order of the\n2081             classes corresponds to that in the attribute `classes_`.\n2082         \"\"\"\n2083         score = self.decision_function(X)\n2084         try:\n2085             return self.loss_._score_to_proba(score)\n2086         except NotFittedError:\n2087             raise\n2088         except AttributeError:\n2089             raise AttributeError('loss=%r does not support predict_proba' %\n2090                                  self.loss)\n2091 \n2092     def predict_log_proba(self, X):\n2093         \"\"\"Predict class log-probabilities for X.\n2094 \n2095         Parameters\n2096         ----------\n2097         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2098             The input samples. Internally, it will be converted to\n2099             ``dtype=np.float32`` and if a sparse matrix is provided\n2100             to a sparse ``csr_matrix``.\n2101 \n2102         Raises\n2103         ------\n2104         AttributeError\n2105             If the ``loss`` does not support probabilities.\n2106 \n2107         Returns\n2108         -------\n2109         p : array, shape (n_samples, n_classes)\n2110             The class log-probabilities of the input samples. The order of the\n2111             classes corresponds to that in the attribute `classes_`.\n2112         \"\"\"\n2113         proba = self.predict_proba(X)\n2114         return np.log(proba)\n2115 \n2116     def staged_predict_proba(self, X):\n2117         \"\"\"Predict class probabilities at each stage for X.\n2118 \n2119         This method allows monitoring (i.e. determine error on testing set)\n2120         after each stage.\n2121 \n2122         Parameters\n2123         ----------\n2124         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2125             The input samples. Internally, it will be converted to\n2126             ``dtype=np.float32`` and if a sparse matrix is provided\n2127             to a sparse ``csr_matrix``.\n2128 \n2129         Returns\n2130         -------\n2131         y : generator of array of shape (n_samples,)\n2132             The predicted value of the input samples.\n2133         \"\"\"\n2134         try:\n2135             for score in self._staged_decision_function(X):\n2136                 yield self.loss_._score_to_proba(score)\n2137         except NotFittedError:\n2138             raise\n2139         except AttributeError:\n2140             raise AttributeError('loss=%r does not support predict_proba' %\n2141                                  self.loss)\n2142 \n2143 \n2144 class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):\n2145     \"\"\"Gradient Boosting for regression.\n2146 \n2147     GB builds an additive model in a forward stage-wise fashion;\n2148     it allows for the optimization of arbitrary differentiable loss functions.\n2149     In each stage a regression tree is fit on the negative gradient of the\n2150     given loss function.\n2151 \n2152     Read more in the :ref:`User Guide <gradient_boosting>`.\n2153 \n2154     Parameters\n2155     ----------\n2156     loss : {'ls', 'lad', 'huber', 'quantile'}, optional (default='ls')\n2157         loss function to be optimized. 'ls' refers to least squares\n2158         regression. 'lad' (least absolute deviation) is a highly robust\n2159         loss function solely based on order information of the input\n2160         variables. 'huber' is a combination of the two. 'quantile'\n2161         allows quantile regression (use `alpha` to specify the quantile).\n2162 \n2163     learning_rate : float, optional (default=0.1)\n2164         learning rate shrinks the contribution of each tree by `learning_rate`.\n2165         There is a trade-off between learning_rate and n_estimators.\n2166 \n2167     n_estimators : int (default=100)\n2168         The number of boosting stages to perform. Gradient boosting\n2169         is fairly robust to over-fitting so a large number usually\n2170         results in better performance.\n2171 \n2172     subsample : float, optional (default=1.0)\n2173         The fraction of samples to be used for fitting the individual base\n2174         learners. If smaller than 1.0 this results in Stochastic Gradient\n2175         Boosting. `subsample` interacts with the parameter `n_estimators`.\n2176         Choosing `subsample < 1.0` leads to a reduction of variance\n2177         and an increase in bias.\n2178 \n2179     criterion : string, optional (default=\"friedman_mse\")\n2180         The function to measure the quality of a split. Supported criteria\n2181         are \"friedman_mse\" for the mean squared error with improvement\n2182         score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n2183         the mean absolute error. The default value of \"friedman_mse\" is\n2184         generally the best as it can provide a better approximation in\n2185         some cases.\n2186 \n2187         .. versionadded:: 0.18\n2188 \n2189     min_samples_split : int, float, optional (default=2)\n2190         The minimum number of samples required to split an internal node:\n2191 \n2192         - If int, then consider `min_samples_split` as the minimum number.\n2193         - If float, then `min_samples_split` is a fraction and\n2194           `ceil(min_samples_split * n_samples)` are the minimum\n2195           number of samples for each split.\n2196 \n2197         .. versionchanged:: 0.18\n2198            Added float values for fractions.\n2199 \n2200     min_samples_leaf : int, float, optional (default=1)\n2201         The minimum number of samples required to be at a leaf node.\n2202         A split point at any depth will only be considered if it leaves at\n2203         least ``min_samples_leaf`` training samples in each of the left and\n2204         right branches.  This may have the effect of smoothing the model,\n2205         especially in regression.\n2206 \n2207         - If int, then consider `min_samples_leaf` as the minimum number.\n2208         - If float, then `min_samples_leaf` is a fraction and\n2209           `ceil(min_samples_leaf * n_samples)` are the minimum\n2210           number of samples for each node.\n2211 \n2212         .. versionchanged:: 0.18\n2213            Added float values for fractions.\n2214 \n2215     min_weight_fraction_leaf : float, optional (default=0.)\n2216         The minimum weighted fraction of the sum total of weights (of all\n2217         the input samples) required to be at a leaf node. Samples have\n2218         equal weight when sample_weight is not provided.\n2219 \n2220     max_depth : integer, optional (default=3)\n2221         maximum depth of the individual regression estimators. The maximum\n2222         depth limits the number of nodes in the tree. Tune this parameter\n2223         for best performance; the best value depends on the interaction\n2224         of the input variables.\n2225 \n2226     min_impurity_decrease : float, optional (default=0.)\n2227         A node will be split if this split induces a decrease of the impurity\n2228         greater than or equal to this value.\n2229 \n2230         The weighted impurity decrease equation is the following::\n2231 \n2232             N_t / N * (impurity - N_t_R / N_t * right_impurity\n2233                                 - N_t_L / N_t * left_impurity)\n2234 \n2235         where ``N`` is the total number of samples, ``N_t`` is the number of\n2236         samples at the current node, ``N_t_L`` is the number of samples in the\n2237         left child, and ``N_t_R`` is the number of samples in the right child.\n2238 \n2239         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n2240         if ``sample_weight`` is passed.\n2241 \n2242         .. versionadded:: 0.19\n2243 \n2244     min_impurity_split : float, (default=1e-7)\n2245         Threshold for early stopping in tree growth. A node will split\n2246         if its impurity is above the threshold, otherwise it is a leaf.\n2247 \n2248         .. deprecated:: 0.19\n2249            ``min_impurity_split`` has been deprecated in favor of\n2250            ``min_impurity_decrease`` in 0.19. The default value of\n2251            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n2252            will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n2253 \n2254     init : estimator, optional (default=None)\n2255         An estimator object that is used to compute the initial\n2256         predictions. ``init`` has to provide ``fit`` and ``predict``.\n2257         If None it uses ``loss.init_estimator``.\n2258 \n2259     random_state : int, RandomState instance or None, optional (default=None)\n2260         If int, random_state is the seed used by the random number generator;\n2261         If RandomState instance, random_state is the random number generator;\n2262         If None, the random number generator is the RandomState instance used\n2263         by `np.random`.\n2264 \n2265     max_features : int, float, string or None, optional (default=None)\n2266         The number of features to consider when looking for the best split:\n2267 \n2268         - If int, then consider `max_features` features at each split.\n2269         - If float, then `max_features` is a fraction and\n2270           `int(max_features * n_features)` features are considered at each\n2271           split.\n2272         - If \"auto\", then `max_features=n_features`.\n2273         - If \"sqrt\", then `max_features=sqrt(n_features)`.\n2274         - If \"log2\", then `max_features=log2(n_features)`.\n2275         - If None, then `max_features=n_features`.\n2276 \n2277         Choosing `max_features < n_features` leads to a reduction of variance\n2278         and an increase in bias.\n2279 \n2280         Note: the search for a split does not stop until at least one\n2281         valid partition of the node samples is found, even if it requires to\n2282         effectively inspect more than ``max_features`` features.\n2283 \n2284     alpha : float (default=0.9)\n2285         The alpha-quantile of the huber loss function and the quantile\n2286         loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n2287 \n2288     verbose : int, default: 0\n2289         Enable verbose output. If 1 then it prints progress and performance\n2290         once in a while (the more trees the lower the frequency). If greater\n2291         than 1 then it prints progress and performance for every tree.\n2292 \n2293     max_leaf_nodes : int or None, optional (default=None)\n2294         Grow trees with ``max_leaf_nodes`` in best-first fashion.\n2295         Best nodes are defined as relative reduction in impurity.\n2296         If None then unlimited number of leaf nodes.\n2297 \n2298     warm_start : bool, default: False\n2299         When set to ``True``, reuse the solution of the previous call to fit\n2300         and add more estimators to the ensemble, otherwise, just erase the\n2301         previous solution. See :term:`the Glossary <warm_start>`.\n2302 \n2303     presort : bool or 'auto', optional (default='auto')\n2304         Whether to presort the data to speed up the finding of best splits in\n2305         fitting. Auto mode by default will use presorting on dense data and\n2306         default to normal sorting on sparse data. Setting presort to true on\n2307         sparse data will raise an error.\n2308 \n2309         .. versionadded:: 0.17\n2310            optional parameter *presort*.\n2311 \n2312     validation_fraction : float, optional, default 0.1\n2313         The proportion of training data to set aside as validation set for\n2314         early stopping. Must be between 0 and 1.\n2315         Only used if ``n_iter_no_change`` is set to an integer.\n2316 \n2317         .. versionadded:: 0.20\n2318 \n2319     n_iter_no_change : int, default None\n2320         ``n_iter_no_change`` is used to decide if early stopping will be used\n2321         to terminate training when validation score is not improving. By\n2322         default it is set to None to disable early stopping. If set to a\n2323         number, it will set aside ``validation_fraction`` size of the training\n2324         data as validation and terminate training when validation score is not\n2325         improving in all of the previous ``n_iter_no_change`` numbers of\n2326         iterations.\n2327 \n2328         .. versionadded:: 0.20\n2329 \n2330     tol : float, optional, default 1e-4\n2331         Tolerance for the early stopping. When the loss is not improving\n2332         by at least tol for ``n_iter_no_change`` iterations (if set to a\n2333         number), the training stops.\n2334 \n2335         .. versionadded:: 0.20\n2336 \n2337 \n2338     Attributes\n2339     ----------\n2340     feature_importances_ : array, shape (n_features,)\n2341         The feature importances (the higher, the more important the feature).\n2342 \n2343     oob_improvement_ : array, shape (n_estimators,)\n2344         The improvement in loss (= deviance) on the out-of-bag samples\n2345         relative to the previous iteration.\n2346         ``oob_improvement_[0]`` is the improvement in\n2347         loss of the first stage over the ``init`` estimator.\n2348 \n2349     train_score_ : array, shape (n_estimators,)\n2350         The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n2351         model at iteration ``i`` on the in-bag sample.\n2352         If ``subsample == 1`` this is the deviance on the training data.\n2353 \n2354     loss_ : LossFunction\n2355         The concrete ``LossFunction`` object.\n2356 \n2357     init_ : estimator\n2358         The estimator that provides the initial predictions.\n2359         Set via the ``init`` argument or ``loss.init_estimator``.\n2360 \n2361     estimators_ : array of DecisionTreeRegressor, shape (n_estimators, 1)\n2362         The collection of fitted sub-estimators.\n2363 \n2364     Notes\n2365     -----\n2366     The features are always randomly permuted at each split. Therefore,\n2367     the best found split may vary, even with the same training data and\n2368     ``max_features=n_features``, if the improvement of the criterion is\n2369     identical for several splits enumerated during the search of the best\n2370     split. To obtain a deterministic behaviour during fitting,\n2371     ``random_state`` has to be fixed.\n2372 \n2373     See also\n2374     --------\n2375     DecisionTreeRegressor, RandomForestRegressor\n2376 \n2377     References\n2378     ----------\n2379     J. Friedman, Greedy Function Approximation: A Gradient Boosting\n2380     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n2381 \n2382     J. Friedman, Stochastic Gradient Boosting, 1999\n2383 \n2384     T. Hastie, R. Tibshirani and J. Friedman.\n2385     Elements of Statistical Learning Ed. 2, Springer, 2009.\n2386     \"\"\"\n2387 \n2388     _SUPPORTED_LOSS = ('ls', 'lad', 'huber', 'quantile')\n2389 \n2390     def __init__(self, loss='ls', learning_rate=0.1, n_estimators=100,\n2391                  subsample=1.0, criterion='friedman_mse', min_samples_split=2,\n2392                  min_samples_leaf=1, min_weight_fraction_leaf=0.,\n2393                  max_depth=3, min_impurity_decrease=0.,\n2394                  min_impurity_split=None, init=None, random_state=None,\n2395                  max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None,\n2396                  warm_start=False, presort='auto', validation_fraction=0.1,\n2397                  n_iter_no_change=None, tol=1e-4):\n2398 \n2399         super().__init__(\n2400             loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,\n2401             criterion=criterion, min_samples_split=min_samples_split,\n2402             min_samples_leaf=min_samples_leaf,\n2403             min_weight_fraction_leaf=min_weight_fraction_leaf,\n2404             max_depth=max_depth, init=init, subsample=subsample,\n2405             max_features=max_features,\n2406             min_impurity_decrease=min_impurity_decrease,\n2407             min_impurity_split=min_impurity_split,\n2408             random_state=random_state, alpha=alpha, verbose=verbose,\n2409             max_leaf_nodes=max_leaf_nodes, warm_start=warm_start,\n2410             presort=presort, validation_fraction=validation_fraction,\n2411             n_iter_no_change=n_iter_no_change, tol=tol)\n2412 \n2413     def predict(self, X):\n2414         \"\"\"Predict regression target for X.\n2415 \n2416         Parameters\n2417         ----------\n2418         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2419             The input samples. Internally, it will be converted to\n2420             ``dtype=np.float32`` and if a sparse matrix is provided\n2421             to a sparse ``csr_matrix``.\n2422 \n2423         Returns\n2424         -------\n2425         y : array, shape (n_samples,)\n2426             The predicted values.\n2427         \"\"\"\n2428         X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')\n2429         return self._decision_function(X).ravel()\n2430 \n2431     def staged_predict(self, X):\n2432         \"\"\"Predict regression target at each stage for X.\n2433 \n2434         This method allows monitoring (i.e. determine error on testing set)\n2435         after each stage.\n2436 \n2437         Parameters\n2438         ----------\n2439         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2440             The input samples. Internally, it will be converted to\n2441             ``dtype=np.float32`` and if a sparse matrix is provided\n2442             to a sparse ``csr_matrix``.\n2443 \n2444         Returns\n2445         -------\n2446         y : generator of array of shape (n_samples,)\n2447             The predicted value of the input samples.\n2448         \"\"\"\n2449         for y in self._staged_decision_function(X):\n2450             yield y.ravel()\n2451 \n2452     def apply(self, X):\n2453         \"\"\"Apply trees in the ensemble to X, return leaf indices.\n2454 \n2455         .. versionadded:: 0.17\n2456 \n2457         Parameters\n2458         ----------\n2459         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2460             The input samples. Internally, its dtype will be converted to\n2461             ``dtype=np.float32``. If a sparse matrix is provided, it will\n2462             be converted to a sparse ``csr_matrix``.\n2463 \n2464         Returns\n2465         -------\n2466         X_leaves : array-like, shape (n_samples, n_estimators)\n2467             For each datapoint x in X and for each tree in the ensemble,\n2468             return the index of the leaf x ends up in each estimator.\n2469         \"\"\"\n2470 \n2471         leaves = super().apply(X)\n2472         leaves = leaves.reshape(X.shape[0], self.estimators_.shape[0])\n2473         return leaves\n",
        "intended_behavior": "For gradient boosting methods, the intention is similar to above. Adjust the validate_X calls or similar validations to support a wider range of data types as inputs, provided the base estimators can work with those data types. It's crucial to maintain a balance between allowing flexibility and ensuring the input data's appropriacy for the algorithm to work as intended."
    }
]