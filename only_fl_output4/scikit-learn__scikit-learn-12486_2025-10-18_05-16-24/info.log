2025-10-18 05:16:25.004 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 05:16:27.515 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=710, output_tokens=12, cost=0.007460
2025-10-18 05:16:39.632 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=890, output_tokens=596, cost=0.026780
2025-10-18 05:16:40.818 | INFO     | registered non reproducing test 0
2025-10-18 05:16:53.337 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2338, output_tokens=553, cost=0.039970
2025-10-18 05:16:54.311 | INFO     | registered non reproducing test 1
2025-10-18 05:17:07.101 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2298, output_tokens=629, cost=0.041850
2025-10-18 05:17:08.076 | INFO     | registered non reproducing test 2
2025-10-18 05:17:08.076 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-18 05:17:08.083 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 05:17:14.227 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=758, output_tokens=250, cost=0.015080
2025-10-18 05:17:14.234 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 05:17:17.249 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=604, output_tokens=46, cost=0.007420
2025-10-18 05:17:17.251 | DEBUG    | Extracted a valid json.
2025-10-18 05:17:17.251 | DEBUG    | Agent proxy return the following json: {
    "API_calls": ["search_class(\"GridSearchCV\")", "search_method_in_file(\"__call__\", \"scorer.py\")", "search_class(\"GMM\")"],
    "bug_locations": []
}
2025-10-18 05:17:17.331 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 05:17:17.332 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 05:17:34.802 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3561, output_tokens=820, cost=0.060210
2025-10-18 05:17:34.819 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 05:17:47.044 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=4597, output_tokens=485, cost=0.060520
2025-10-18 05:17:47.054 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 05:17:56.960 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=839, output_tokens=408, cost=0.020630
2025-10-18 05:17:56.961 | DEBUG    | Extracted a valid json.
2025-10-18 05:17:56.961 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "sklearn/metrics/scorer.py",
            "class": "Various Scorer Classes",
            "method": "__call__",
            "intended_behavior": "The intended behavior should include validation to check if the estimator is capable of producing the expected type of predictions (e.g., binary probabilities for `roc_auc` scoring). If the estimator does not naturally support the scoring method (as might be the case with `GMM` or `GaussianMixture` for `roc_auc`), a more descriptive error message should be thrown to guide the user. This validation should preserve the functionality of allowing any classifiers that can output probabilities or decision function values to be scored appropriately while preventing or handling more gracefully the cases where an unsupervised model is incorrectly used for supervised tasks. For supervised models that do not support `predict_proba` or `decision_function`, an alternative handling or suggestion in the error message should advise on the correct use or model adaptation."
        },
        {
            "file": "sklearn/model_selection/_search.py",
            "class": "GridSearchCV",
            "method": "",
            "intended_behavior": "Given that the issue also stems from a misunderstanding or misuse of `GridSearchCV` with a non-typical classifier (`GMM`), it's crucial that `GridSearchCV` performs a check on the estimator to ensure it is suitable for the type of scoring being requested (`roc_auc`). This could involve verifying that the estimator adheres to the interface required for the specified scoring strategy. The behavior should ensure that users are immediately notified when there is a mismatch between the estimator's capabilities and the requirements of the scoring method, without altering the existing functionality of `GridSearchCV` for compatible model and scorer combinations. This might involve extending the existing validation within `GridSearchCV` to more comprehensively cover the capabilities of the provided estimator relative to the scoring method utilized."
        }
    ]
}
2025-10-18 05:17:56.990 | DEBUG    | Bug location extracted successfully: [<file>sklearn/metrics/scorer.py</file>
<class>None</class>
<method>__call__</method>
<code>
67     def __call__(self, estimator, X, y_true, sample_weight=None):
68         """Evaluate predicted target values for X relative to y_true.
69 
70         Parameters
71         ----------
72         estimator : object
73             Trained estimator to use for scoring. Must have a predict_proba
74             method; the output of that is used to compute the score.
75 
76         X : array-like or sparse matrix
77             Test data that will be fed to estimator.predict.
78 
79         y_true : array-like
80             Gold standard target values for X.
81 
82         sample_weight : array-like, optional (default=None)
83             Sample weights.
84 
85         Returns
86         -------
87         score : float
88             Score function applied to prediction of estimator on X.
89         """
90 
91         y_pred = estimator.predict(X)
92         if sample_weight is not None:
93             return self._sign * self._score_func(y_true, y_pred,
94                                                  sample_weight=sample_weight,
95                                                  **self._kwargs)
96         else:
97             return self._sign * self._score_func(y_true, y_pred,
98                                                  **self._kwargs)

</code><intended_behavior>The intended behavior should include validation to check if the estimator is capable of producing the expected type of predictions (e.g., binary probabilities for `roc_auc` scoring). If the estimator does not naturally support the scoring method (as might be the case with `GMM` or `GaussianMixture` for `roc_auc`), a more descriptive error message should be thrown to guide the user. This validation should preserve the functionality of allowing any classifiers that can output probabilities or decision function values to be scored appropriately while preventing or handling more gracefully the cases where an unsupervised model is incorrectly used for supervised tasks. For supervised models that do not support `predict_proba` or `decision_function`, an alternative handling or suggestion in the error message should advise on the correct use or model adaptation.</intended_behavior>, <file>sklearn/metrics/scorer.py</file>
<class>None</class>
<method>__call__</method>
<code>
102     def __call__(self, clf, X, y, sample_weight=None):
103         """Evaluate predicted probabilities for X relative to y_true.
104 
105         Parameters
106         ----------
107         clf : object
108             Trained classifier to use for scoring. Must have a predict_proba
109             method; the output of that is used to compute the score.
110 
111         X : array-like or sparse matrix
112             Test data that will be fed to clf.predict_proba.
113 
114         y : array-like
115             Gold standard target values for X. These must be class labels,
116             not probabilities.
117 
118         sample_weight : array-like, optional (default=None)
119             Sample weights.
120 
121         Returns
122         -------
123         score : float
124             Score function applied to prediction of estimator on X.
125         """
126         y_type = type_of_target(y)
127         y_pred = clf.predict_proba(X)
128         if y_type == "binary":
129             y_pred = y_pred[:, 1]
130         if sample_weight is not None:
131             return self._sign * self._score_func(y, y_pred,
132                                                  sample_weight=sample_weight,
133                                                  **self._kwargs)
134         else:
135             return self._sign * self._score_func(y, y_pred, **self._kwargs)

</code><intended_behavior>The intended behavior should include validation to check if the estimator is capable of producing the expected type of predictions (e.g., binary probabilities for `roc_auc` scoring). If the estimator does not naturally support the scoring method (as might be the case with `GMM` or `GaussianMixture` for `roc_auc`), a more descriptive error message should be thrown to guide the user. This validation should preserve the functionality of allowing any classifiers that can output probabilities or decision function values to be scored appropriately while preventing or handling more gracefully the cases where an unsupervised model is incorrectly used for supervised tasks. For supervised models that do not support `predict_proba` or `decision_function`, an alternative handling or suggestion in the error message should advise on the correct use or model adaptation.</intended_behavior>, <file>sklearn/metrics/scorer.py</file>
<class>None</class>
<method>__call__</method>
<code>
142     def __call__(self, clf, X, y, sample_weight=None):
143         """Evaluate decision function output for X relative to y_true.
144 
145         Parameters
146         ----------
147         clf : object
148             Trained classifier to use for scoring. Must have either a
149             decision_function method or a predict_proba method; the output of
150             that is used to compute the score.
151 
152         X : array-like or sparse matrix
153             Test data that will be fed to clf.decision_function or
154             clf.predict_proba.
155 
156         y : array-like
157             Gold standard target values for X. These must be class labels,
158             not decision function values.
159 
160         sample_weight : array-like, optional (default=None)
161             Sample weights.
162 
163         Returns
164         -------
165         score : float
166             Score function applied to prediction of estimator on X.
167         """
168         y_type = type_of_target(y)
169         if y_type not in ("binary", "multilabel-indicator"):
170             raise ValueError("{0} format is not supported".format(y_type))
171 
172         if is_regressor(clf):
173             y_pred = clf.predict(X)
174         else:
175             try:
176                 y_pred = clf.decision_function(X)
177 
178                 # For multi-output multi-class estimator
179                 if isinstance(y_pred, list):
180                     y_pred = np.vstack([p for p in y_pred]).T
181 
182             except (NotImplementedError, AttributeError):
183                 y_pred = clf.predict_proba(X)
184 
185                 if y_type == "binary":
186                     y_pred = y_pred[:, 1]
187                 elif isinstance(y_pred, list):
188                     y_pred = np.vstack([p[:, -1] for p in y_pred]).T
189 
190         if sample_weight is not None:
191             return self._sign * self._score_func(y, y_pred,
192                                                  sample_weight=sample_weight,
193                                                  **self._kwargs)
194         else:
195             return self._sign * self._score_func(y, y_pred, **self._kwargs)

</code><intended_behavior>The intended behavior should include validation to check if the estimator is capable of producing the expected type of predictions (e.g., binary probabilities for `roc_auc` scoring). If the estimator does not naturally support the scoring method (as might be the case with `GMM` or `GaussianMixture` for `roc_auc`), a more descriptive error message should be thrown to guide the user. This validation should preserve the functionality of allowing any classifiers that can output probabilities or decision function values to be scored appropriately while preventing or handling more gracefully the cases where an unsupervised model is incorrectly used for supervised tasks. For supervised models that do not support `predict_proba` or `decision_function`, an alternative handling or suggestion in the error message should advise on the correct use or model adaptation.</intended_behavior>, <file>sklearn/metrics/scorer.py</file>
<class>_PredictScorer</class>
<method>__call__</method>
<code>
67     def __call__(self, estimator, X, y_true, sample_weight=None):
68         """Evaluate predicted target values for X relative to y_true.
69 
70         Parameters
71         ----------
72         estimator : object
73             Trained estimator to use for scoring. Must have a predict_proba
74             method; the output of that is used to compute the score.
75 
76         X : array-like or sparse matrix
77             Test data that will be fed to estimator.predict.
78 
79         y_true : array-like
80             Gold standard target values for X.
81 
82         sample_weight : array-like, optional (default=None)
83             Sample weights.
84 
85         Returns
86         -------
87         score : float
88             Score function applied to prediction of estimator on X.
89         """
90 
91         y_pred = estimator.predict(X)
92         if sample_weight is not None:
93             return self._sign * self._score_func(y_true, y_pred,
94                                                  sample_weight=sample_weight,
95                                                  **self._kwargs)
96         else:
97             return self._sign * self._score_func(y_true, y_pred,
98                                                  **self._kwargs)

</code><intended_behavior>The intended behavior should include validation to check if the estimator is capable of producing the expected type of predictions (e.g., binary probabilities for `roc_auc` scoring). If the estimator does not naturally support the scoring method (as might be the case with `GMM` or `GaussianMixture` for `roc_auc`), a more descriptive error message should be thrown to guide the user. This validation should preserve the functionality of allowing any classifiers that can output probabilities or decision function values to be scored appropriately while preventing or handling more gracefully the cases where an unsupervised model is incorrectly used for supervised tasks. For supervised models that do not support `predict_proba` or `decision_function`, an alternative handling or suggestion in the error message should advise on the correct use or model adaptation.</intended_behavior>, <file>sklearn/metrics/scorer.py</file>
<class>_ProbaScorer</class>
<method>__call__</method>
<code>
102     def __call__(self, clf, X, y, sample_weight=None):
103         """Evaluate predicted probabilities for X relative to y_true.
104 
105         Parameters
106         ----------
107         clf : object
108             Trained classifier to use for scoring. Must have a predict_proba
109             method; the output of that is used to compute the score.
110 
111         X : array-like or sparse matrix
112             Test data that will be fed to clf.predict_proba.
113 
114         y : array-like
115             Gold standard target values for X. These must be class labels,
116             not probabilities.
117 
118         sample_weight : array-like, optional (default=None)
119             Sample weights.
120 
121         Returns
122         -------
123         score : float
124             Score function applied to prediction of estimator on X.
125         """
126         y_type = type_of_target(y)
127         y_pred = clf.predict_proba(X)
128         if y_type == "binary":
129             y_pred = y_pred[:, 1]
130         if sample_weight is not None:
131             return self._sign * self._score_func(y, y_pred,
132                                                  sample_weight=sample_weight,
133                                                  **self._kwargs)
134         else:
135             return self._sign * self._score_func(y, y_pred, **self._kwargs)

</code><intended_behavior>The intended behavior should include validation to check if the estimator is capable of producing the expected type of predictions (e.g., binary probabilities for `roc_auc` scoring). If the estimator does not naturally support the scoring method (as might be the case with `GMM` or `GaussianMixture` for `roc_auc`), a more descriptive error message should be thrown to guide the user. This validation should preserve the functionality of allowing any classifiers that can output probabilities or decision function values to be scored appropriately while preventing or handling more gracefully the cases where an unsupervised model is incorrectly used for supervised tasks. For supervised models that do not support `predict_proba` or `decision_function`, an alternative handling or suggestion in the error message should advise on the correct use or model adaptation.</intended_behavior>, <file>sklearn/metrics/scorer.py</file>
<class>_ThresholdScorer</class>
<method>__call__</method>
<code>
142     def __call__(self, clf, X, y, sample_weight=None):
143         """Evaluate decision function output for X relative to y_true.
144 
145         Parameters
146         ----------
147         clf : object
148             Trained classifier to use for scoring. Must have either a
149             decision_function method or a predict_proba method; the output of
150             that is used to compute the score.
151 
152         X : array-like or sparse matrix
153             Test data that will be fed to clf.decision_function or
154             clf.predict_proba.
155 
156         y : array-like
157             Gold standard target values for X. These must be class labels,
158             not decision function values.
159 
160         sample_weight : array-like, optional (default=None)
161             Sample weights.
162 
163         Returns
164         -------
165         score : float
166             Score function applied to prediction of estimator on X.
167         """
168         y_type = type_of_target(y)
169         if y_type not in ("binary", "multilabel-indicator"):
170             raise ValueError("{0} format is not supported".format(y_type))
171 
172         if is_regressor(clf):
173             y_pred = clf.predict(X)
174         else:
175             try:
176                 y_pred = clf.decision_function(X)
177 
178                 # For multi-output multi-class estimator
179                 if isinstance(y_pred, list):
180                     y_pred = np.vstack([p for p in y_pred]).T
181 
182             except (NotImplementedError, AttributeError):
183                 y_pred = clf.predict_proba(X)
184 
185                 if y_type == "binary":
186                     y_pred = y_pred[:, 1]
187                 elif isinstance(y_pred, list):
188                     y_pred = np.vstack([p[:, -1] for p in y_pred]).T
189 
190         if sample_weight is not None:
191             return self._sign * self._score_func(y, y_pred,
192                                                  sample_weight=sample_weight,
193                                                  **self._kwargs)
194         else:
195             return self._sign * self._score_func(y, y_pred, **self._kwargs)

</code><intended_behavior>The intended behavior should include validation to check if the estimator is capable of producing the expected type of predictions (e.g., binary probabilities for `roc_auc` scoring). If the estimator does not naturally support the scoring method (as might be the case with `GMM` or `GaussianMixture` for `roc_auc`), a more descriptive error message should be thrown to guide the user. This validation should preserve the functionality of allowing any classifiers that can output probabilities or decision function values to be scored appropriately while preventing or handling more gracefully the cases where an unsupervised model is incorrectly used for supervised tasks. For supervised models that do not support `predict_proba` or `decision_function`, an alternative handling or suggestion in the error message should advise on the correct use or model adaptation.</intended_behavior>, <file>sklearn/model_selection/_search.py</file>
<class>GridSearchCV</class>
<method>None</method>
<code>
818 class GridSearchCV(BaseSearchCV):
819     """Exhaustive search over specified parameter values for an estimator.
820 
821     Important members are fit, predict.
822 
823     GridSearchCV implements a "fit" and a "score" method.
824     It also implements "predict", "predict_proba", "decision_function",
825     "transform" and "inverse_transform" if they are implemented in the
826     estimator used.
827 
828     The parameters of the estimator used to apply these methods are optimized
829     by cross-validated grid-search over a parameter grid.
830 
831     Read more in the :ref:`User Guide <grid_search>`.
832 
833     Parameters
834     ----------
835     estimator : estimator object.
836         This is assumed to implement the scikit-learn estimator interface.
837         Either estimator needs to provide a ``score`` function,
838         or ``scoring`` must be passed.
839 
840     param_grid : dict or list of dictionaries
841         Dictionary with parameters names (string) as keys and lists of
842         parameter settings to try as values, or a list of such
843         dictionaries, in which case the grids spanned by each dictionary
844         in the list are explored. This enables searching over any sequence
845         of parameter settings.
846 
847     scoring : string, callable, list/tuple, dict or None, default: None
848         A single string (see :ref:`scoring_parameter`) or a callable
849         (see :ref:`scoring`) to evaluate the predictions on the test set.
850 
851         For evaluating multiple metrics, either give a list of (unique) strings
852         or a dict with names as keys and callables as values.
853 
854         NOTE that when using custom scorers, each scorer should return a single
855         value. Metric functions returning a list/array of values can be wrapped
856         into multiple scorers that return one value each.
857 
858         See :ref:`multimetric_grid_search` for an example.
859 
860         If None, the estimator's default scorer (if available) is used.
861 
862     n_jobs : int or None, optional (default=None)
863         Number of jobs to run in parallel.
864         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
865         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
866         for more details.
867 
868     pre_dispatch : int, or string, optional
869         Controls the number of jobs that get dispatched during parallel
870         execution. Reducing this number can be useful to avoid an
871         explosion of memory consumption when more jobs get dispatched
872         than CPUs can process. This parameter can be:
873 
874             - None, in which case all the jobs are immediately
875               created and spawned. Use this for lightweight and
876               fast-running jobs, to avoid delays due to on-demand
877               spawning of the jobs
878 
879             - An int, giving the exact number of total jobs that are
880               spawned
881 
882             - A string, giving an expression as a function of n_jobs,
883               as in '2*n_jobs'
884 
885     iid : boolean, default='warn'
886         If True, return the average score across folds, weighted by the number
887         of samples in each test set. In this case, the data is assumed to be
888         identically distributed across the folds, and the loss minimized is
889         the total loss per sample, and not the mean loss across the folds. If
890         False, return the average score across folds. Default is True, but
891         will change to False in version 0.22, to correspond to the standard
892         definition of cross-validation.
893 
894         .. versionchanged:: 0.20
895             Parameter ``iid`` will change from True to False by default in
896             version 0.22, and will be removed in 0.24.
897 
898     cv : int, cross-validation generator or an iterable, optional
899         Determines the cross-validation splitting strategy.
900         Possible inputs for cv are:
901 
902         - None, to use the default 3-fold cross validation,
903         - integer, to specify the number of folds in a `(Stratified)KFold`,
904         - :term:`CV splitter`,
905         - An iterable yielding (train, test) splits as arrays of indices.
906 
907         For integer/None inputs, if the estimator is a classifier and ``y`` is
908         either binary or multiclass, :class:`StratifiedKFold` is used. In all
909         other cases, :class:`KFold` is used.
910 
911         Refer :ref:`User Guide <cross_validation>` for the various
912         cross-validation strategies that can be used here.
913 
914         .. versionchanged:: 0.20
915             ``cv`` default value if None will change from 3-fold to 5-fold
916             in v0.22.
917 
918     refit : boolean, or string, default=True
919         Refit an estimator using the best found parameters on the whole
920         dataset.
921 
922         For multiple metric evaluation, this needs to be a string denoting the
923         scorer is used to find the best parameters for refitting the estimator
924         at the end.
925 
926         The refitted estimator is made available at the ``best_estimator_``
927         attribute and permits using ``predict`` directly on this
928         ``GridSearchCV`` instance.
929 
930         Also for multiple metric evaluation, the attributes ``best_index_``,
931         ``best_score_`` and ``best_params_`` will only be available if
932         ``refit`` is set and all of them will be determined w.r.t this specific
933         scorer.
934 
935         See ``scoring`` parameter to know more about multiple metric
936         evaluation.
937 
938     verbose : integer
939         Controls the verbosity: the higher, the more messages.
940 
941     error_score : 'raise' or numeric
942         Value to assign to the score if an error occurs in estimator fitting.
943         If set to 'raise', the error is raised. If a numeric value is given,
944         FitFailedWarning is raised. This parameter does not affect the refit
945         step, which will always raise the error. Default is 'raise' but from
946         version 0.22 it will change to np.nan.
947 
948     return_train_score : boolean, default=False
949         If ``False``, the ``cv_results_`` attribute will not include training
950         scores.
951         Computing training scores is used to get insights on how different
952         parameter settings impact the overfitting/underfitting trade-off.
953         However computing the scores on the training set can be computationally
954         expensive and is not strictly required to select the parameters that
955         yield the best generalization performance.
956 
957 
958     Examples
959     --------
960     >>> from sklearn import svm, datasets
961     >>> from sklearn.model_selection import GridSearchCV
962     >>> iris = datasets.load_iris()
963     >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
964     >>> svc = svm.SVC(gamma="scale")
965     >>> clf = GridSearchCV(svc, parameters, cv=5)
966     >>> clf.fit(iris.data, iris.target)
967     ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
968     GridSearchCV(cv=5, error_score=...,
969            estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,
970                          decision_function_shape='ovr', degree=..., gamma=...,
971                          kernel='rbf', max_iter=-1, probability=False,
972                          random_state=None, shrinking=True, tol=...,
973                          verbose=False),
974            iid=..., n_jobs=None,
975            param_grid=..., pre_dispatch=..., refit=..., return_train_score=...,
976            scoring=..., verbose=...)
977     >>> sorted(clf.cv_results_.keys())
978     ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
979     ['mean_fit_time', 'mean_score_time', 'mean_test_score',...
980      'param_C', 'param_kernel', 'params',...
981      'rank_test_score', 'split0_test_score',...
982      'split2_test_score', ...
983      'std_fit_time', 'std_score_time', 'std_test_score']
984 
985     Attributes
986     ----------
987     cv_results_ : dict of numpy (masked) ndarrays
988         A dict with keys as column headers and values as columns, that can be
989         imported into a pandas ``DataFrame``.
990 
991         For instance the below given table
992 
993         +------------+-----------+------------+-----------------+---+---------+
994         |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|
995         +============+===========+============+=================+===+=========+
996         |  'poly'    |     --    |      2     |       0.80      |...|    2    |
997         +------------+-----------+------------+-----------------+---+---------+
998         |  'poly'    |     --    |      3     |       0.70      |...|    4    |
999         +------------+-----------+------------+-----------------+---+---------+
1000         |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |
1001         +------------+-----------+------------+-----------------+---+---------+
1002         |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |
1003         +------------+-----------+------------+-----------------+---+---------+
1004 
1005         will be represented by a ``cv_results_`` dict of::
1006 
1007             {
1008             'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],
1009                                          mask = [False False False False]...)
1010             'param_gamma': masked_array(data = [-- -- 0.1 0.2],
1011                                         mask = [ True  True False False]...),
1012             'param_degree': masked_array(data = [2.0 3.0 -- --],
1013                                          mask = [False False  True  True]...),
1014             'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],
1015             'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],
1016             'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],
1017             'std_test_score'     : [0.01, 0.10, 0.05, 0.08],
1018             'rank_test_score'    : [2, 4, 3, 1],
1019             'split0_train_score' : [0.80, 0.92, 0.70, 0.93],
1020             'split1_train_score' : [0.82, 0.55, 0.70, 0.87],
1021             'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],
1022             'std_train_score'    : [0.01, 0.19, 0.00, 0.03],
1023             'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],
1024             'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],
1025             'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],
1026             'std_score_time'     : [0.00, 0.00, 0.00, 0.01],
1027             'params'             : [{'kernel': 'poly', 'degree': 2}, ...],
1028             }
1029 
1030         NOTE
1031 
1032         The key ``'params'`` is used to store a list of parameter
1033         settings dicts for all the parameter candidates.
1034 
1035         The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and
1036         ``std_score_time`` are all in seconds.
1037 
1038         For multi-metric evaluation, the scores for all the scorers are
1039         available in the ``cv_results_`` dict at the keys ending with that
1040         scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown
1041         above. ('split0_test_precision', 'mean_train_precision' etc.)
1042 
1043     best_estimator_ : estimator or dict
1044         Estimator that was chosen by the search, i.e. estimator
1045         which gave highest score (or smallest loss if specified)
1046         on the left out data. Not available if ``refit=False``.
1047 
1048         See ``refit`` parameter for more information on allowed values.
1049 
1050     best_score_ : float
1051         Mean cross-validated score of the best_estimator
1052 
1053         For multi-metric evaluation, this is present only if ``refit`` is
1054         specified.
1055 
1056     best_params_ : dict
1057         Parameter setting that gave the best results on the hold out data.
1058 
1059         For multi-metric evaluation, this is present only if ``refit`` is
1060         specified.
1061 
1062     best_index_ : int
1063         The index (of the ``cv_results_`` arrays) which corresponds to the best
1064         candidate parameter setting.
1065 
1066         The dict at ``search.cv_results_['params'][search.best_index_]`` gives
1067         the parameter setting for the best model, that gives the highest
1068         mean score (``search.best_score_``).
1069 
1070         For multi-metric evaluation, this is present only if ``refit`` is
1071         specified.
1072 
1073     scorer_ : function or a dict
1074         Scorer function used on the held out data to choose the best
1075         parameters for the model.
1076 
1077         For multi-metric evaluation, this attribute holds the validated
1078         ``scoring`` dict which maps the scorer key to the scorer callable.
1079 
1080     n_splits_ : int
1081         The number of cross-validation splits (folds/iterations).
1082 
1083     refit_time_ : float
1084         Seconds used for refitting the best model on the whole dataset.
1085 
1086         This is present only if ``refit`` is not False.
1087 
1088     Notes
1089     ------
1090     The parameters selected are those that maximize the score of the left out
1091     data, unless an explicit score is passed in which case it is used instead.
1092 
1093     If `n_jobs` was set to a value higher than one, the data is copied for each
1094     point in the grid (and not `n_jobs` times). This is done for efficiency
1095     reasons if individual jobs take very little time, but may raise errors if
1096     the dataset is large and not enough memory is available.  A workaround in
1097     this case is to set `pre_dispatch`. Then, the memory is copied only
1098     `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *
1099     n_jobs`.
1100 
1101     See Also
1102     ---------
1103     :class:`ParameterGrid`:
1104         generates all the combinations of a hyperparameter grid.
1105 
1106     :func:`sklearn.model_selection.train_test_split`:
1107         utility function to split the data into a development set usable
1108         for fitting a GridSearchCV instance and an evaluation set for
1109         its final evaluation.
1110 
1111     :func:`sklearn.metrics.make_scorer`:
1112         Make a scorer from a performance metric or loss function.
1113 
1114     """
1115 
1116     def __init__(self, estimator, param_grid, scoring=None,
1117                  n_jobs=None, iid='warn', refit=True, cv='warn', verbose=0,
1118                  pre_dispatch='2*n_jobs', error_score='raise-deprecating',
1119                  return_train_score=False):
1120         super(GridSearchCV, self).__init__(
1121             estimator=estimator, scoring=scoring,
1122             n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose,
1123             pre_dispatch=pre_dispatch, error_score=error_score,
1124             return_train_score=return_train_score)
1125         self.param_grid = param_grid
1126         _check_param_grid(param_grid)
1127 
1128     def _run_search(self, evaluate_candidates):
1129         """Search all candidates in param_grid"""
1130         evaluate_candidates(ParameterGrid(self.param_grid))

</code><intended_behavior>Given that the issue also stems from a misunderstanding or misuse of `GridSearchCV` with a non-typical classifier (`GMM`), it's crucial that `GridSearchCV` performs a check on the estimator to ensure it is suitable for the type of scoring being requested (`roc_auc`). This could involve verifying that the estimator adheres to the interface required for the specified scoring strategy. The behavior should ensure that users are immediately notified when there is a mismatch between the estimator's capabilities and the requirements of the scoring method, without altering the existing functionality of `GridSearchCV` for compatible model and scorer combinations. This might involve extending the existing validation within `GridSearchCV` to more comprehensively cover the capabilities of the provided estimator relative to the scoring method utilized.</intended_behavior>]
2025-10-18 05:17:56.990 | INFO     | Search completed. Bug locations: [<file>sklearn/metrics/scorer.py</file>
<class>None</class>
<method>__call__</method>
<code>
67     def __call__(self, estimator, X, y_true, sample_weight=None):
68         """Evaluate predicted target values for X relative to y_true.
69 
70         Parameters
71         ----------
72         estimator : object
73             Trained estimator to use for scoring. Must have a predict_proba
74             method; the output of that is used to compute the score.
75 
76         X : array-like or sparse matrix
77             Test data that will be fed to estimator.predict.
78 
79         y_true : array-like
80             Gold standard target values for X.
81 
82         sample_weight : array-like, optional (default=None)
83             Sample weights.
84 
85         Returns
86         -------
87         score : float
88             Score function applied to prediction of estimator on X.
89         """
90 
91         y_pred = estimator.predict(X)
92         if sample_weight is not None:
93             return self._sign * self._score_func(y_true, y_pred,
94                                                  sample_weight=sample_weight,
95                                                  **self._kwargs)
96         else:
97             return self._sign * self._score_func(y_true, y_pred,
98                                                  **self._kwargs)

</code><intended_behavior>The intended behavior should include validation to check if the estimator is capable of producing the expected type of predictions (e.g., binary probabilities for `roc_auc` scoring). If the estimator does not naturally support the scoring method (as might be the case with `GMM` or `GaussianMixture` for `roc_auc`), a more descriptive error message should be thrown to guide the user. This validation should preserve the functionality of allowing any classifiers that can output probabilities or decision function values to be scored appropriately while preventing or handling more gracefully the cases where an unsupervised model is incorrectly used for supervised tasks. For supervised models that do not support `predict_proba` or `decision_function`, an alternative handling or suggestion in the error message should advise on the correct use or model adaptation.</intended_behavior>, <file>sklearn/metrics/scorer.py</file>
<class>None</class>
<method>__call__</method>
<code>
102     def __call__(self, clf, X, y, sample_weight=None):
103         """Evaluate predicted probabilities for X relative to y_true.
104 
105         Parameters
106         ----------
107         clf : object
108             Trained classifier to use for scoring. Must have a predict_proba
109             method; the output of that is used to compute the score.
110 
111         X : array-like or sparse matrix
112             Test data that will be fed to clf.predict_proba.
113 
114         y : array-like
115             Gold standard target values for X. These must be class labels,
116             not probabilities.
117 
118         sample_weight : array-like, optional (default=None)
119             Sample weights.
120 
121         Returns
122         -------
123         score : float
124             Score function applied to prediction of estimator on X.
125         """
126         y_type = type_of_target(y)
127         y_pred = clf.predict_proba(X)
128         if y_type == "binary":
129             y_pred = y_pred[:, 1]
130         if sample_weight is not None:
131             return self._sign * self._score_func(y, y_pred,
132                                                  sample_weight=sample_weight,
133                                                  **self._kwargs)
134         else:
135             return self._sign * self._score_func(y, y_pred, **self._kwargs)

</code><intended_behavior>The intended behavior should include validation to check if the estimator is capable of producing the expected type of predictions (e.g., binary probabilities for `roc_auc` scoring). If the estimator does not naturally support the scoring method (as might be the case with `GMM` or `GaussianMixture` for `roc_auc`), a more descriptive error message should be thrown to guide the user. This validation should preserve the functionality of allowing any classifiers that can output probabilities or decision function values to be scored appropriately while preventing or handling more gracefully the cases where an unsupervised model is incorrectly used for supervised tasks. For supervised models that do not support `predict_proba` or `decision_function`, an alternative handling or suggestion in the error message should advise on the correct use or model adaptation.</intended_behavior>, <file>sklearn/metrics/scorer.py</file>
<class>None</class>
<method>__call__</method>
<code>
142     def __call__(self, clf, X, y, sample_weight=None):
143         """Evaluate decision function output for X relative to y_true.
144 
145         Parameters
146         ----------
147         clf : object
148             Trained classifier to use for scoring. Must have either a
149             decision_function method or a predict_proba method; the output of
150             that is used to compute the score.
151 
152         X : array-like or sparse matrix
153             Test data that will be fed to clf.decision_function or
154             clf.predict_proba.
155 
156         y : array-like
157             Gold standard target values for X. These must be class labels,
158             not decision function values.
159 
160         sample_weight : array-like, optional (default=None)
161             Sample weights.
162 
163         Returns
164         -------
165         score : float
166             Score function applied to prediction of estimator on X.
167         """
168         y_type = type_of_target(y)
169         if y_type not in ("binary", "multilabel-indicator"):
170             raise ValueError("{0} format is not supported".format(y_type))
171 
172         if is_regressor(clf):
173             y_pred = clf.predict(X)
174         else:
175             try:
176                 y_pred = clf.decision_function(X)
177 
178                 # For multi-output multi-class estimator
179                 if isinstance(y_pred, list):
180                     y_pred = np.vstack([p for p in y_pred]).T
181 
182             except (NotImplementedError, AttributeError):
183                 y_pred = clf.predict_proba(X)
184 
185                 if y_type == "binary":
186                     y_pred = y_pred[:, 1]
187                 elif isinstance(y_pred, list):
188                     y_pred = np.vstack([p[:, -1] for p in y_pred]).T
189 
190         if sample_weight is not None:
191             return self._sign * self._score_func(y, y_pred,
192                                                  sample_weight=sample_weight,
193                                                  **self._kwargs)
194         else:
195             return self._sign * self._score_func(y, y_pred, **self._kwargs)

</code><intended_behavior>The intended behavior should include validation to check if the estimator is capable of producing the expected type of predictions (e.g., binary probabilities for `roc_auc` scoring). If the estimator does not naturally support the scoring method (as might be the case with `GMM` or `GaussianMixture` for `roc_auc`), a more descriptive error message should be thrown to guide the user. This validation should preserve the functionality of allowing any classifiers that can output probabilities or decision function values to be scored appropriately while preventing or handling more gracefully the cases where an unsupervised model is incorrectly used for supervised tasks. For supervised models that do not support `predict_proba` or `decision_function`, an alternative handling or suggestion in the error message should advise on the correct use or model adaptation.</intended_behavior>, <file>sklearn/metrics/scorer.py</file>
<class>_PredictScorer</class>
<method>__call__</method>
<code>
67     def __call__(self, estimator, X, y_true, sample_weight=None):
68         """Evaluate predicted target values for X relative to y_true.
69 
70         Parameters
71         ----------
72         estimator : object
73             Trained estimator to use for scoring. Must have a predict_proba
74             method; the output of that is used to compute the score.
75 
76         X : array-like or sparse matrix
77             Test data that will be fed to estimator.predict.
78 
79         y_true : array-like
80             Gold standard target values for X.
81 
82         sample_weight : array-like, optional (default=None)
83             Sample weights.
84 
85         Returns
86         -------
87         score : float
88             Score function applied to prediction of estimator on X.
89         """
90 
91         y_pred = estimator.predict(X)
92         if sample_weight is not None:
93             return self._sign * self._score_func(y_true, y_pred,
94                                                  sample_weight=sample_weight,
95                                                  **self._kwargs)
96         else:
97             return self._sign * self._score_func(y_true, y_pred,
98                                                  **self._kwargs)

</code><intended_behavior>The intended behavior should include validation to check if the estimator is capable of producing the expected type of predictions (e.g., binary probabilities for `roc_auc` scoring). If the estimator does not naturally support the scoring method (as might be the case with `GMM` or `GaussianMixture` for `roc_auc`), a more descriptive error message should be thrown to guide the user. This validation should preserve the functionality of allowing any classifiers that can output probabilities or decision function values to be scored appropriately while preventing or handling more gracefully the cases where an unsupervised model is incorrectly used for supervised tasks. For supervised models that do not support `predict_proba` or `decision_function`, an alternative handling or suggestion in the error message should advise on the correct use or model adaptation.</intended_behavior>, <file>sklearn/metrics/scorer.py</file>
<class>_ProbaScorer</class>
<method>__call__</method>
<code>
102     def __call__(self, clf, X, y, sample_weight=None):
103         """Evaluate predicted probabilities for X relative to y_true.
104 
105         Parameters
106         ----------
107         clf : object
108             Trained classifier to use for scoring. Must have a predict_proba
109             method; the output of that is used to compute the score.
110 
111         X : array-like or sparse matrix
112             Test data that will be fed to clf.predict_proba.
113 
114         y : array-like
115             Gold standard target values for X. These must be class labels,
116             not probabilities.
117 
118         sample_weight : array-like, optional (default=None)
119             Sample weights.
120 
121         Returns
122         -------
123         score : float
124             Score function applied to prediction of estimator on X.
125         """
126         y_type = type_of_target(y)
127         y_pred = clf.predict_proba(X)
128         if y_type == "binary":
129             y_pred = y_pred[:, 1]
130         if sample_weight is not None:
131             return self._sign * self._score_func(y, y_pred,
132                                                  sample_weight=sample_weight,
133                                                  **self._kwargs)
134         else:
135             return self._sign * self._score_func(y, y_pred, **self._kwargs)

</code><intended_behavior>The intended behavior should include validation to check if the estimator is capable of producing the expected type of predictions (e.g., binary probabilities for `roc_auc` scoring). If the estimator does not naturally support the scoring method (as might be the case with `GMM` or `GaussianMixture` for `roc_auc`), a more descriptive error message should be thrown to guide the user. This validation should preserve the functionality of allowing any classifiers that can output probabilities or decision function values to be scored appropriately while preventing or handling more gracefully the cases where an unsupervised model is incorrectly used for supervised tasks. For supervised models that do not support `predict_proba` or `decision_function`, an alternative handling or suggestion in the error message should advise on the correct use or model adaptation.</intended_behavior>, <file>sklearn/metrics/scorer.py</file>
<class>_ThresholdScorer</class>
<method>__call__</method>
<code>
142     def __call__(self, clf, X, y, sample_weight=None):
143         """Evaluate decision function output for X relative to y_true.
144 
145         Parameters
146         ----------
147         clf : object
148             Trained classifier to use for scoring. Must have either a
149             decision_function method or a predict_proba method; the output of
150             that is used to compute the score.
151 
152         X : array-like or sparse matrix
153             Test data that will be fed to clf.decision_function or
154             clf.predict_proba.
155 
156         y : array-like
157             Gold standard target values for X. These must be class labels,
158             not decision function values.
159 
160         sample_weight : array-like, optional (default=None)
161             Sample weights.
162 
163         Returns
164         -------
165         score : float
166             Score function applied to prediction of estimator on X.
167         """
168         y_type = type_of_target(y)
169         if y_type not in ("binary", "multilabel-indicator"):
170             raise ValueError("{0} format is not supported".format(y_type))
171 
172         if is_regressor(clf):
173             y_pred = clf.predict(X)
174         else:
175             try:
176                 y_pred = clf.decision_function(X)
177 
178                 # For multi-output multi-class estimator
179                 if isinstance(y_pred, list):
180                     y_pred = np.vstack([p for p in y_pred]).T
181 
182             except (NotImplementedError, AttributeError):
183                 y_pred = clf.predict_proba(X)
184 
185                 if y_type == "binary":
186                     y_pred = y_pred[:, 1]
187                 elif isinstance(y_pred, list):
188                     y_pred = np.vstack([p[:, -1] for p in y_pred]).T
189 
190         if sample_weight is not None:
191             return self._sign * self._score_func(y, y_pred,
192                                                  sample_weight=sample_weight,
193                                                  **self._kwargs)
194         else:
195             return self._sign * self._score_func(y, y_pred, **self._kwargs)

</code><intended_behavior>The intended behavior should include validation to check if the estimator is capable of producing the expected type of predictions (e.g., binary probabilities for `roc_auc` scoring). If the estimator does not naturally support the scoring method (as might be the case with `GMM` or `GaussianMixture` for `roc_auc`), a more descriptive error message should be thrown to guide the user. This validation should preserve the functionality of allowing any classifiers that can output probabilities or decision function values to be scored appropriately while preventing or handling more gracefully the cases where an unsupervised model is incorrectly used for supervised tasks. For supervised models that do not support `predict_proba` or `decision_function`, an alternative handling or suggestion in the error message should advise on the correct use or model adaptation.</intended_behavior>, <file>sklearn/model_selection/_search.py</file>
<class>GridSearchCV</class>
<method>None</method>
<code>
818 class GridSearchCV(BaseSearchCV):
819     """Exhaustive search over specified parameter values for an estimator.
820 
821     Important members are fit, predict.
822 
823     GridSearchCV implements a "fit" and a "score" method.
824     It also implements "predict", "predict_proba", "decision_function",
825     "transform" and "inverse_transform" if they are implemented in the
826     estimator used.
827 
828     The parameters of the estimator used to apply these methods are optimized
829     by cross-validated grid-search over a parameter grid.
830 
831     Read more in the :ref:`User Guide <grid_search>`.
832 
833     Parameters
834     ----------
835     estimator : estimator object.
836         This is assumed to implement the scikit-learn estimator interface.
837         Either estimator needs to provide a ``score`` function,
838         or ``scoring`` must be passed.
839 
840     param_grid : dict or list of dictionaries
841         Dictionary with parameters names (string) as keys and lists of
842         parameter settings to try as values, or a list of such
843         dictionaries, in which case the grids spanned by each dictionary
844         in the list are explored. This enables searching over any sequence
845         of parameter settings.
846 
847     scoring : string, callable, list/tuple, dict or None, default: None
848         A single string (see :ref:`scoring_parameter`) or a callable
849         (see :ref:`scoring`) to evaluate the predictions on the test set.
850 
851         For evaluating multiple metrics, either give a list of (unique) strings
852         or a dict with names as keys and callables as values.
853 
854         NOTE that when using custom scorers, each scorer should return a single
855         value. Metric functions returning a list/array of values can be wrapped
856         into multiple scorers that return one value each.
857 
858         See :ref:`multimetric_grid_search` for an example.
859 
860         If None, the estimator's default scorer (if available) is used.
861 
862     n_jobs : int or None, optional (default=None)
863         Number of jobs to run in parallel.
864         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
865         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
866         for more details.
867 
868     pre_dispatch : int, or string, optional
869         Controls the number of jobs that get dispatched during parallel
870         execution. Reducing this number can be useful to avoid an
871         explosion of memory consumption when more jobs get dispatched
872         than CPUs can process. This parameter can be:
873 
874             - None, in which case all the jobs are immediately
875               created and spawned. Use this for lightweight and
876               fast-running jobs, to avoid delays due to on-demand
877               spawning of the jobs
878 
879             - An int, giving the exact number of total jobs that are
880               spawned
881 
882             - A string, giving an expression as a function of n_jobs,
883               as in '2*n_jobs'
884 
885     iid : boolean, default='warn'
886         If True, return the average score across folds, weighted by the number
887         of samples in each test set. In this case, the data is assumed to be
888         identically distributed across the folds, and the loss minimized is
889         the total loss per sample, and not the mean loss across the folds. If
890         False, return the average score across folds. Default is True, but
891         will change to False in version 0.22, to correspond to the standard
892         definition of cross-validation.
893 
894         .. versionchanged:: 0.20
895             Parameter ``iid`` will change from True to False by default in
896             version 0.22, and will be removed in 0.24.
897 
898     cv : int, cross-validation generator or an iterable, optional
899         Determines the cross-validation splitting strategy.
900         Possible inputs for cv are:
901 
902         - None, to use the default 3-fold cross validation,
903         - integer, to specify the number of folds in a `(Stratified)KFold`,
904         - :term:`CV splitter`,
905         - An iterable yielding (train, test) splits as arrays of indices.
906 
907         For integer/None inputs, if the estimator is a classifier and ``y`` is
908         either binary or multiclass, :class:`StratifiedKFold` is used. In all
909         other cases, :class:`KFold` is used.
910 
911         Refer :ref:`User Guide <cross_validation>` for the various
912         cross-validation strategies that can be used here.
913 
914         .. versionchanged:: 0.20
915             ``cv`` default value if None will change from 3-fold to 5-fold
916             in v0.22.
917 
918     refit : boolean, or string, default=True
919         Refit an estimator using the best found parameters on the whole
920         dataset.
921 
922         For multiple metric evaluation, this needs to be a string denoting the
923         scorer is used to find the best parameters for refitting the estimator
924         at the end.
925 
926         The refitted estimator is made available at the ``best_estimator_``
927         attribute and permits using ``predict`` directly on this
928         ``GridSearchCV`` instance.
929 
930         Also for multiple metric evaluation, the attributes ``best_index_``,
931         ``best_score_`` and ``best_params_`` will only be available if
932         ``refit`` is set and all of them will be determined w.r.t this specific
933         scorer.
934 
935         See ``scoring`` parameter to know more about multiple metric
936         evaluation.
937 
938     verbose : integer
939         Controls the verbosity: the higher, the more messages.
940 
941     error_score : 'raise' or numeric
942         Value to assign to the score if an error occurs in estimator fitting.
943         If set to 'raise', the error is raised. If a numeric value is given,
944         FitFailedWarning is raised. This parameter does not affect the refit
945         step, which will always raise the error. Default is 'raise' but from
946         version 0.22 it will change to np.nan.
947 
948     return_train_score : boolean, default=False
949         If ``False``, the ``cv_results_`` attribute will not include training
950         scores.
951         Computing training scores is used to get insights on how different
952         parameter settings impact the overfitting/underfitting trade-off.
953         However computing the scores on the training set can be computationally
954         expensive and is not strictly required to select the parameters that
955         yield the best generalization performance.
956 
957 
958     Examples
959     --------
960     >>> from sklearn import svm, datasets
961     >>> from sklearn.model_selection import GridSearchCV
962     >>> iris = datasets.load_iris()
963     >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
964     >>> svc = svm.SVC(gamma="scale")
965     >>> clf = GridSearchCV(svc, parameters, cv=5)
966     >>> clf.fit(iris.data, iris.target)
967     ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
968     GridSearchCV(cv=5, error_score=...,
969            estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,
970                          decision_function_shape='ovr', degree=..., gamma=...,
971                          kernel='rbf', max_iter=-1, probability=False,
972                          random_state=None, shrinking=True, tol=...,
973                          verbose=False),
974            iid=..., n_jobs=None,
975            param_grid=..., pre_dispatch=..., refit=..., return_train_score=...,
976            scoring=..., verbose=...)
977     >>> sorted(clf.cv_results_.keys())
978     ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
979     ['mean_fit_time', 'mean_score_time', 'mean_test_score',...
980      'param_C', 'param_kernel', 'params',...
981      'rank_test_score', 'split0_test_score',...
982      'split2_test_score', ...
983      'std_fit_time', 'std_score_time', 'std_test_score']
984 
985     Attributes
986     ----------
987     cv_results_ : dict of numpy (masked) ndarrays
988         A dict with keys as column headers and values as columns, that can be
989         imported into a pandas ``DataFrame``.
990 
991         For instance the below given table
992 
993         +------------+-----------+------------+-----------------+---+---------+
994         |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|
995         +============+===========+============+=================+===+=========+
996         |  'poly'    |     --    |      2     |       0.80      |...|    2    |
997         +------------+-----------+------------+-----------------+---+---------+
998         |  'poly'    |     --    |      3     |       0.70      |...|    4    |
999         +------------+-----------+------------+-----------------+---+---------+
1000         |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |
1001         +------------+-----------+------------+-----------------+---+---------+
1002         |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |
1003         +------------+-----------+------------+-----------------+---+---------+
1004 
1005         will be represented by a ``cv_results_`` dict of::
1006 
1007             {
1008             'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],
1009                                          mask = [False False False False]...)
1010             'param_gamma': masked_array(data = [-- -- 0.1 0.2],
1011                                         mask = [ True  True False False]...),
1012             'param_degree': masked_array(data = [2.0 3.0 -- --],
1013                                          mask = [False False  True  True]...),
1014             'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],
1015             'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],
1016             'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],
1017             'std_test_score'     : [0.01, 0.10, 0.05, 0.08],
1018             'rank_test_score'    : [2, 4, 3, 1],
1019             'split0_train_score' : [0.80, 0.92, 0.70, 0.93],
1020             'split1_train_score' : [0.82, 0.55, 0.70, 0.87],
1021             'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],
1022             'std_train_score'    : [0.01, 0.19, 0.00, 0.03],
1023             'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],
1024             'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],
1025             'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],
1026             'std_score_time'     : [0.00, 0.00, 0.00, 0.01],
1027             'params'             : [{'kernel': 'poly', 'degree': 2}, ...],
1028             }
1029 
1030         NOTE
1031 
1032         The key ``'params'`` is used to store a list of parameter
1033         settings dicts for all the parameter candidates.
1034 
1035         The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and
1036         ``std_score_time`` are all in seconds.
1037 
1038         For multi-metric evaluation, the scores for all the scorers are
1039         available in the ``cv_results_`` dict at the keys ending with that
1040         scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown
1041         above. ('split0_test_precision', 'mean_train_precision' etc.)
1042 
1043     best_estimator_ : estimator or dict
1044         Estimator that was chosen by the search, i.e. estimator
1045         which gave highest score (or smallest loss if specified)
1046         on the left out data. Not available if ``refit=False``.
1047 
1048         See ``refit`` parameter for more information on allowed values.
1049 
1050     best_score_ : float
1051         Mean cross-validated score of the best_estimator
1052 
1053         For multi-metric evaluation, this is present only if ``refit`` is
1054         specified.
1055 
1056     best_params_ : dict
1057         Parameter setting that gave the best results on the hold out data.
1058 
1059         For multi-metric evaluation, this is present only if ``refit`` is
1060         specified.
1061 
1062     best_index_ : int
1063         The index (of the ``cv_results_`` arrays) which corresponds to the best
1064         candidate parameter setting.
1065 
1066         The dict at ``search.cv_results_['params'][search.best_index_]`` gives
1067         the parameter setting for the best model, that gives the highest
1068         mean score (``search.best_score_``).
1069 
1070         For multi-metric evaluation, this is present only if ``refit`` is
1071         specified.
1072 
1073     scorer_ : function or a dict
1074         Scorer function used on the held out data to choose the best
1075         parameters for the model.
1076 
1077         For multi-metric evaluation, this attribute holds the validated
1078         ``scoring`` dict which maps the scorer key to the scorer callable.
1079 
1080     n_splits_ : int
1081         The number of cross-validation splits (folds/iterations).
1082 
1083     refit_time_ : float
1084         Seconds used for refitting the best model on the whole dataset.
1085 
1086         This is present only if ``refit`` is not False.
1087 
1088     Notes
1089     ------
1090     The parameters selected are those that maximize the score of the left out
1091     data, unless an explicit score is passed in which case it is used instead.
1092 
1093     If `n_jobs` was set to a value higher than one, the data is copied for each
1094     point in the grid (and not `n_jobs` times). This is done for efficiency
1095     reasons if individual jobs take very little time, but may raise errors if
1096     the dataset is large and not enough memory is available.  A workaround in
1097     this case is to set `pre_dispatch`. Then, the memory is copied only
1098     `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *
1099     n_jobs`.
1100 
1101     See Also
1102     ---------
1103     :class:`ParameterGrid`:
1104         generates all the combinations of a hyperparameter grid.
1105 
1106     :func:`sklearn.model_selection.train_test_split`:
1107         utility function to split the data into a development set usable
1108         for fitting a GridSearchCV instance and an evaluation set for
1109         its final evaluation.
1110 
1111     :func:`sklearn.metrics.make_scorer`:
1112         Make a scorer from a performance metric or loss function.
1113 
1114     """
1115 
1116     def __init__(self, estimator, param_grid, scoring=None,
1117                  n_jobs=None, iid='warn', refit=True, cv='warn', verbose=0,
1118                  pre_dispatch='2*n_jobs', error_score='raise-deprecating',
1119                  return_train_score=False):
1120         super(GridSearchCV, self).__init__(
1121             estimator=estimator, scoring=scoring,
1122             n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose,
1123             pre_dispatch=pre_dispatch, error_score=error_score,
1124             return_train_score=return_train_score)
1125         self.param_grid = param_grid
1126         _check_param_grid(param_grid)
1127 
1128     def _run_search(self, evaluate_candidates):
1129         """Search all candidates in param_grid"""
1130         evaluate_candidates(ParameterGrid(self.param_grid))

</code><intended_behavior>Given that the issue also stems from a misunderstanding or misuse of `GridSearchCV` with a non-typical classifier (`GMM`), it's crucial that `GridSearchCV` performs a check on the estimator to ensure it is suitable for the type of scoring being requested (`roc_auc`). This could involve verifying that the estimator adheres to the interface required for the specified scoring strategy. The behavior should ensure that users are immediately notified when there is a mismatch between the estimator's capabilities and the requirements of the scoring method, without altering the existing functionality of `GridSearchCV` for compatible model and scorer combinations. This might involve extending the existing validation within `GridSearchCV` to more comprehensively cover the capabilities of the provided estimator relative to the scoring method utilized.</intended_behavior>]
2025-10-18 05:17:56.990 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 05:17:56.990 | INFO     | Search-only: skipping patch selection
2025-10-18 05:17:57.060 | INFO     | Task scikit-learn__scikit-learn-12486 completed successfully.
2025-10-18 05:17:57.061 | INFO     | No patch generated. You can try running ACR again.
