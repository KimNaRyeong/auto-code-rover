2025-10-18 05:56:14.823 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 05:56:19.193 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=339, output_tokens=12, cost=0.003750
2025-10-18 05:56:19.193 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-18 05:56:19.197 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 05:56:26.612 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=647, output_tokens=347, cost=0.016880
2025-10-18 05:56:26.621 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 05:56:30.798 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=701, output_tokens=141, cost=0.011240
2025-10-18 05:56:30.800 | DEBUG    | Extracted a valid json.
2025-10-18 05:56:30.800 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"CalibratedClassifierCV\")",
        "search_class(\"DBSCAN\")",
        "search_class(\"DummyClassifier\")",
        "search_class(\"BaseGradientBoosting\")",
        "search_method_in_class(\"fit\", \"CalibratedClassifierCV\")",
        "search_method_in_class(\"fit\", \"DBSCAN\")",
        "search_method_in_class(\"fit\", \"DummyClassifier\")",
        "search_method_in_class(\"partial_fit\", \"BaseGradientBoosting\")",
        "search_code(\"check_consistent_length(sample_weight\")",
        "search_code(\"check_array(sample_weight\")"
    ],
    "bug_locations": []
}
2025-10-18 05:56:30.911 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 05:56:30.912 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 05:56:48.021 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=4709, output_tokens=795, cost=0.070940
2025-10-18 05:56:48.034 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 05:56:59.585 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=5720, output_tokens=495, cost=0.072050
2025-10-18 05:56:59.597 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 05:57:07.418 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=849, output_tokens=331, cost=0.018420
2025-10-18 05:57:07.419 | DEBUG    | Extracted a valid json.
2025-10-18 05:57:07.419 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "sklearn/calibration.py",
            "class": "CalibratedClassifierCV",
            "method": "fit",
            "intended_behavior": "Instead of directly using check_array(sample_weight, ensure_2d=False) and manual checks (if sample_weight is not None), replace these with _check_sample_weight(X, sample_weight) from utils.validation. This change should make sample_weight validation consistent with the project standards. Ensure that all functionality allowing for sample_weight to be None or array-like is preserved."
        },
        {
            "file": "sklearn/cluster/_dbscan.py",
            "class": "DBSCAN",
            "method": "fit",
            "intended_behavior": "Replace manual array conversion and length checking of sample_weight (np.asarray(sample_weight) and check_consistent_length(X, sample_weight)) with _check_sample_weight(X, sample_weight) ensuring sample_weight validation aligns with the unified approach. Ensure the existing logic of how sample_weight affects core sample determination remains intact."
        },
        {
            "file": "sklearn/dummy.py",
            "class": "DummyClassifier",
            "method": "fit",
            "intended_behavior": "Replace manual use of check_consistent_length(X, y, sample_weight) with _check_sample_weight(y, sample_weight), assuming this also manages consistency validation internally. Ensure the method continues to correctly account for sample_weight in its calculations related to class distribution, constant strategy, or any other logic where sample_weight is relevant."
        }
    ]
}
2025-10-18 05:57:07.429 | DEBUG    | Bug location extracted successfully: [<file>sklearn/calibration.py</file>
<class>CalibratedClassifierCV</class>
<method>fit</method>
<code>
114     def fit(self, X, y, sample_weight=None):
115         """Fit the calibrated model
116 
117         Parameters
118         ----------
119         X : array-like, shape (n_samples, n_features)
120             Training data.
121 
122         y : array-like, shape (n_samples,)
123             Target values.
124 
125         sample_weight : array-like of shape (n_samples,), default=None
126             Sample weights. If None, then samples are equally weighted.
127 
128         Returns
129         -------
130         self : object
131             Returns an instance of self.
132         """
133         X, y = check_X_y(X, y, accept_sparse=['csc', 'csr', 'coo'],
134                          force_all_finite=False, allow_nd=True)
135         X, y = indexable(X, y)
136         le = LabelBinarizer().fit(y)
137         self.classes_ = le.classes_
138 
139         # Check that each cross-validation fold can have at least one
140         # example per class
141         n_folds = self.cv if isinstance(self.cv, int) \
142             else self.cv.n_folds if hasattr(self.cv, "n_folds") else None
143         if n_folds and \
144                 np.any([np.sum(y == class_) < n_folds for class_ in
145                         self.classes_]):
146             raise ValueError("Requesting %d-fold cross-validation but provided"
147                              " less than %d examples for at least one class."
148                              % (n_folds, n_folds))
149 
150         self.calibrated_classifiers_ = []
151         if self.base_estimator is None:
152             # we want all classifiers that don't expose a random_state
153             # to be deterministic (and we don't want to expose this one).
154             base_estimator = LinearSVC(random_state=0)
155         else:
156             base_estimator = self.base_estimator
157 
158         if self.cv == "prefit":
159             calibrated_classifier = _CalibratedClassifier(
160                 base_estimator, method=self.method)
161             if sample_weight is not None:
162                 calibrated_classifier.fit(X, y, sample_weight)
163             else:
164                 calibrated_classifier.fit(X, y)
165             self.calibrated_classifiers_.append(calibrated_classifier)
166         else:
167             cv = check_cv(self.cv, y, classifier=True)
168             fit_parameters = signature(base_estimator.fit).parameters
169             estimator_name = type(base_estimator).__name__
170             if (sample_weight is not None
171                     and "sample_weight" not in fit_parameters):
172                 warnings.warn("%s does not support sample_weight. Samples"
173                               " weights are only used for the calibration"
174                               " itself." % estimator_name)
175                 sample_weight = check_array(sample_weight, ensure_2d=False)
176                 base_estimator_sample_weight = None
177             else:
178                 if sample_weight is not None:
179                     sample_weight = check_array(sample_weight, ensure_2d=False)
180                     check_consistent_length(y, sample_weight)
181                 base_estimator_sample_weight = sample_weight
182             for train, test in cv.split(X, y):
183                 this_estimator = clone(base_estimator)
184                 if base_estimator_sample_weight is not None:
185                     this_estimator.fit(
186                         X[train], y[train],
187                         sample_weight=base_estimator_sample_weight[train])
188                 else:
189                     this_estimator.fit(X[train], y[train])
190 
191                 calibrated_classifier = _CalibratedClassifier(
192                     this_estimator, method=self.method,
193                     classes=self.classes_)
194                 if sample_weight is not None:
195                     calibrated_classifier.fit(X[test], y[test],
196                                               sample_weight[test])
197                 else:
198                     calibrated_classifier.fit(X[test], y[test])
199                 self.calibrated_classifiers_.append(calibrated_classifier)
200 
201         return self

</code><intended_behavior>Instead of directly using check_array(sample_weight, ensure_2d=False) and manual checks (if sample_weight is not None), replace these with _check_sample_weight(X, sample_weight) from utils.validation. This change should make sample_weight validation consistent with the project standards. Ensure that all functionality allowing for sample_weight to be None or array-like is preserved.</intended_behavior>, <file>sklearn/calibration.py</file>
<class>CalibratedClassifierCV</class>
<method>None</method>
<code>
31 class CalibratedClassifierCV(BaseEstimator, ClassifierMixin,
32                              MetaEstimatorMixin):
33     """Probability calibration with isotonic regression or sigmoid.
34 
35     See glossary entry for :term:`cross-validation estimator`.
36 
37     With this class, the base_estimator is fit on the train set of the
38     cross-validation generator and the test set is used for calibration.
39     The probabilities for each of the folds are then averaged
40     for prediction. In case that cv="prefit" is passed to __init__,
41     it is assumed that base_estimator has been fitted already and all
42     data is used for calibration. Note that data for fitting the
43     classifier and for calibrating it must be disjoint.
44 
45     Read more in the :ref:`User Guide <calibration>`.
46 
47     Parameters
48     ----------
49     base_estimator : instance BaseEstimator
50         The classifier whose output decision function needs to be calibrated
51         to offer more accurate predict_proba outputs. If cv=prefit, the
52         classifier must have been fit already on data.
53 
54     method : 'sigmoid' or 'isotonic'
55         The method to use for calibration. Can be 'sigmoid' which
56         corresponds to Platt's method or 'isotonic' which is a
57         non-parametric approach. It is not advised to use isotonic calibration
58         with too few calibration samples ``(<<1000)`` since it tends to
59         overfit.
60         Use sigmoids (Platt's calibration) in this case.
61 
62     cv : integer, cross-validation generator, iterable or "prefit", optional
63         Determines the cross-validation splitting strategy.
64         Possible inputs for cv are:
65 
66         - None, to use the default 5-fold cross-validation,
67         - integer, to specify the number of folds.
68         - :term:`CV splitter`,
69         - An iterable yielding (train, test) splits as arrays of indices.
70 
71         For integer/None inputs, if ``y`` is binary or multiclass,
72         :class:`sklearn.model_selection.StratifiedKFold` is used. If ``y`` is
73         neither binary nor multiclass, :class:`sklearn.model_selection.KFold`
74         is used.
75 
76         Refer :ref:`User Guide <cross_validation>` for the various
77         cross-validation strategies that can be used here.
78 
79         If "prefit" is passed, it is assumed that base_estimator has been
80         fitted already and all data is used for calibration.
81 
82         .. versionchanged:: 0.22
83             ``cv`` default value if None changed from 3-fold to 5-fold.
84 
85     Attributes
86     ----------
87     classes_ : array, shape (n_classes)
88         The class labels.
89 
90     calibrated_classifiers_ : list (len() equal to cv or 1 if cv == "prefit")
91         The list of calibrated classifiers, one for each crossvalidation fold,
92         which has been fitted on all but the validation fold and calibrated
93         on the validation fold.
94 
95     References
96     ----------
97     .. [1] Obtaining calibrated probability estimates from decision trees
98            and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001
99 
100     .. [2] Transforming Classifier Scores into Accurate Multiclass
101            Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)
102 
103     .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to
104            Regularized Likelihood Methods, J. Platt, (1999)
105 
106     .. [4] Predicting Good Probabilities with Supervised Learning,
107            A. Niculescu-Mizil & R. Caruana, ICML 2005
108     """
109     def __init__(self, base_estimator=None, method='sigmoid', cv=None):
110         self.base_estimator = base_estimator
111         self.method = method
112         self.cv = cv
113 
114     def fit(self, X, y, sample_weight=None):
115         """Fit the calibrated model
116 
117         Parameters
118         ----------
119         X : array-like, shape (n_samples, n_features)
120             Training data.
121 
122         y : array-like, shape (n_samples,)
123             Target values.
124 
125         sample_weight : array-like of shape (n_samples,), default=None
126             Sample weights. If None, then samples are equally weighted.
127 
128         Returns
129         -------
130         self : object
131             Returns an instance of self.
132         """
133         X, y = check_X_y(X, y, accept_sparse=['csc', 'csr', 'coo'],
134                          force_all_finite=False, allow_nd=True)
135         X, y = indexable(X, y)
136         le = LabelBinarizer().fit(y)
137         self.classes_ = le.classes_
138 
139         # Check that each cross-validation fold can have at least one
140         # example per class
141         n_folds = self.cv if isinstance(self.cv, int) \
142             else self.cv.n_folds if hasattr(self.cv, "n_folds") else None
143         if n_folds and \
144                 np.any([np.sum(y == class_) < n_folds for class_ in
145                         self.classes_]):
146             raise ValueError("Requesting %d-fold cross-validation but provided"
147                              " less than %d examples for at least one class."
148                              % (n_folds, n_folds))
149 
150         self.calibrated_classifiers_ = []
151         if self.base_estimator is None:
152             # we want all classifiers that don't expose a random_state
153             # to be deterministic (and we don't want to expose this one).
154             base_estimator = LinearSVC(random_state=0)
155         else:
156             base_estimator = self.base_estimator
157 
158         if self.cv == "prefit":
159             calibrated_classifier = _CalibratedClassifier(
160                 base_estimator, method=self.method)
161             if sample_weight is not None:
162                 calibrated_classifier.fit(X, y, sample_weight)
163             else:
164                 calibrated_classifier.fit(X, y)
165             self.calibrated_classifiers_.append(calibrated_classifier)
166         else:
167             cv = check_cv(self.cv, y, classifier=True)
168             fit_parameters = signature(base_estimator.fit).parameters
169             estimator_name = type(base_estimator).__name__
170             if (sample_weight is not None
171                     and "sample_weight" not in fit_parameters):
172                 warnings.warn("%s does not support sample_weight. Samples"
173                               " weights are only used for the calibration"
174                               " itself." % estimator_name)
175                 sample_weight = check_array(sample_weight, ensure_2d=False)
176                 base_estimator_sample_weight = None
177             else:
178                 if sample_weight is not None:
179                     sample_weight = check_array(sample_weight, ensure_2d=False)
180                     check_consistent_length(y, sample_weight)
181                 base_estimator_sample_weight = sample_weight
182             for train, test in cv.split(X, y):
183                 this_estimator = clone(base_estimator)
184                 if base_estimator_sample_weight is not None:
185                     this_estimator.fit(
186                         X[train], y[train],
187                         sample_weight=base_estimator_sample_weight[train])
188                 else:
189                     this_estimator.fit(X[train], y[train])
190 
191                 calibrated_classifier = _CalibratedClassifier(
192                     this_estimator, method=self.method,
193                     classes=self.classes_)
194                 if sample_weight is not None:
195                     calibrated_classifier.fit(X[test], y[test],
196                                               sample_weight[test])
197                 else:
198                     calibrated_classifier.fit(X[test], y[test])
199                 self.calibrated_classifiers_.append(calibrated_classifier)
200 
201         return self
202 
203     def predict_proba(self, X):
204         """Posterior probabilities of classification
205 
206         This function returns posterior probabilities of classification
207         according to each class on an array of test vectors X.
208 
209         Parameters
210         ----------
211         X : array-like, shape (n_samples, n_features)
212             The samples.
213 
214         Returns
215         -------
216         C : array, shape (n_samples, n_classes)
217             The predicted probas.
218         """
219         check_is_fitted(self)
220         X = check_array(X, accept_sparse=['csc', 'csr', 'coo'],
221                         force_all_finite=False)
222         # Compute the arithmetic mean of the predictions of the calibrated
223         # classifiers
224         mean_proba = np.zeros((X.shape[0], len(self.classes_)))
225         for calibrated_classifier in self.calibrated_classifiers_:
226             proba = calibrated_classifier.predict_proba(X)
227             mean_proba += proba
228 
229         mean_proba /= len(self.calibrated_classifiers_)
230 
231         return mean_proba
232 
233     def predict(self, X):
234         """Predict the target of new samples. Can be different from the
235         prediction of the uncalibrated classifier.
236 
237         Parameters
238         ----------
239         X : array-like, shape (n_samples, n_features)
240             The samples.
241 
242         Returns
243         -------
244         C : array, shape (n_samples,)
245             The predicted class.
246         """
247         check_is_fitted(self)
248         return self.classes_[np.argmax(self.predict_proba(X), axis=1)]

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/cluster/_dbscan.py</file>
<class>DBSCAN</class>
<method>fit</method>
<code>
284     def fit(self, X, y=None, sample_weight=None):
285         """Perform DBSCAN clustering from features, or distance matrix.
286 
287         Parameters
288         ----------
289         X : array-like or sparse matrix, shape (n_samples, n_features), or \
290             (n_samples, n_samples)
291             Training instances to cluster, or distances between instances if
292             ``metric='precomputed'``. If a sparse matrix is provided, it will
293             be converted into a sparse ``csr_matrix``.
294 
295         sample_weight : array, shape (n_samples,), optional
296             Weight of each sample, such that a sample with a weight of at least
297             ``min_samples`` is by itself a core sample; a sample with a
298             negative weight may inhibit its eps-neighbor from being core.
299             Note that weights are absolute, and default to 1.
300 
301         y : Ignored
302             Not used, present here for API consistency by convention.
303 
304         Returns
305         -------
306         self
307 
308         """
309         X = check_array(X, accept_sparse='csr')
310 
311         if not self.eps > 0.0:
312             raise ValueError("eps must be positive.")
313 
314         if sample_weight is not None:
315             sample_weight = np.asarray(sample_weight)
316             check_consistent_length(X, sample_weight)
317 
318         # Calculate neighborhood for all samples. This leaves the original
319         # point in, which needs to be considered later (i.e. point i is in the
320         # neighborhood of point i. While True, its useless information)
321         if self.metric == 'precomputed' and sparse.issparse(X):
322             # set the diagonal to explicit values, as a point is its own
323             # neighbor
324             with warnings.catch_warnings():
325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)
326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place
327 
328         neighbors_model = NearestNeighbors(
329             radius=self.eps, algorithm=self.algorithm,
330             leaf_size=self.leaf_size, metric=self.metric,
331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)
332         neighbors_model.fit(X)
333         # This has worst case O(n^2) memory complexity
334         neighborhoods = neighbors_model.radius_neighbors(X,
335                                                          return_distance=False)
336 
337         if sample_weight is None:
338             n_neighbors = np.array([len(neighbors)
339                                     for neighbors in neighborhoods])
340         else:
341             n_neighbors = np.array([np.sum(sample_weight[neighbors])
342                                     for neighbors in neighborhoods])
343 
344         # Initially, all samples are noise.
345         labels = np.full(X.shape[0], -1, dtype=np.intp)
346 
347         # A list of all core samples found.
348         core_samples = np.asarray(n_neighbors >= self.min_samples,
349                                   dtype=np.uint8)
350         dbscan_inner(core_samples, neighborhoods, labels)
351 
352         self.core_sample_indices_ = np.where(core_samples)[0]
353         self.labels_ = labels
354 
355         if len(self.core_sample_indices_):
356             # fix for scipy sparse indexing issue
357             self.components_ = X[self.core_sample_indices_].copy()
358         else:
359             # no core samples
360             self.components_ = np.empty((0, X.shape[1]))
361         return self

</code><intended_behavior>Replace manual array conversion and length checking of sample_weight (np.asarray(sample_weight) and check_consistent_length(X, sample_weight)) with _check_sample_weight(X, sample_weight) ensuring sample_weight validation aligns with the unified approach. Ensure the existing logic of how sample_weight affects core sample determination remains intact.</intended_behavior>, <file>sklearn/cluster/_dbscan.py</file>
<class>DBSCAN</class>
<method>None</method>
<code>
147 class DBSCAN(ClusterMixin, BaseEstimator):
148     """Perform DBSCAN clustering from vector array or distance matrix.
149 
150     DBSCAN - Density-Based Spatial Clustering of Applications with Noise.
151     Finds core samples of high density and expands clusters from them.
152     Good for data which contains clusters of similar density.
153 
154     Read more in the :ref:`User Guide <dbscan>`.
155 
156     Parameters
157     ----------
158     eps : float, optional
159         The maximum distance between two samples for one to be considered
160         as in the neighborhood of the other. This is not a maximum bound
161         on the distances of points within a cluster. This is the most
162         important DBSCAN parameter to choose appropriately for your data set
163         and distance function.
164 
165     min_samples : int, optional
166         The number of samples (or total weight) in a neighborhood for a point
167         to be considered as a core point. This includes the point itself.
168 
169     metric : string, or callable
170         The metric to use when calculating distance between instances in a
171         feature array. If metric is a string or callable, it must be one of
172         the options allowed by :func:`sklearn.metrics.pairwise_distances` for
173         its metric parameter.
174         If metric is "precomputed", X is assumed to be a distance matrix and
175         must be square. X may be a :term:`Glossary <sparse graph>`, in which
176         case only "nonzero" elements may be considered neighbors for DBSCAN.
177 
178         .. versionadded:: 0.17
179            metric *precomputed* to accept precomputed sparse matrix.
180 
181     metric_params : dict, optional
182         Additional keyword arguments for the metric function.
183 
184         .. versionadded:: 0.19
185 
186     algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional
187         The algorithm to be used by the NearestNeighbors module
188         to compute pointwise distances and find nearest neighbors.
189         See NearestNeighbors module documentation for details.
190 
191     leaf_size : int, optional (default = 30)
192         Leaf size passed to BallTree or cKDTree. This can affect the speed
193         of the construction and query, as well as the memory required
194         to store the tree. The optimal value depends
195         on the nature of the problem.
196 
197     p : float, optional
198         The power of the Minkowski metric to be used to calculate distance
199         between points.
200 
201     n_jobs : int or None, optional (default=None)
202         The number of parallel jobs to run.
203         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
204         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
205         for more details.
206 
207     Attributes
208     ----------
209     core_sample_indices_ : array, shape = [n_core_samples]
210         Indices of core samples.
211 
212     components_ : array, shape = [n_core_samples, n_features]
213         Copy of each core sample found by training.
214 
215     labels_ : array, shape = [n_samples]
216         Cluster labels for each point in the dataset given to fit().
217         Noisy samples are given the label -1.
218 
219     Examples
220     --------
221     >>> from sklearn.cluster import DBSCAN
222     >>> import numpy as np
223     >>> X = np.array([[1, 2], [2, 2], [2, 3],
224     ...               [8, 7], [8, 8], [25, 80]])
225     >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)
226     >>> clustering.labels_
227     array([ 0,  0,  0,  1,  1, -1])
228     >>> clustering
229     DBSCAN(eps=3, min_samples=2)
230 
231     See also
232     --------
233     OPTICS
234         A similar clustering at multiple values of eps. Our implementation
235         is optimized for memory usage.
236 
237     Notes
238     -----
239     For an example, see :ref:`examples/cluster/plot_dbscan.py
240     <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.
241 
242     This implementation bulk-computes all neighborhood queries, which increases
243     the memory complexity to O(n.d) where d is the average number of neighbors,
244     while original DBSCAN had memory complexity O(n). It may attract a higher
245     memory complexity when querying these nearest neighborhoods, depending
246     on the ``algorithm``.
247 
248     One way to avoid the query complexity is to pre-compute sparse
249     neighborhoods in chunks using
250     :func:`NearestNeighbors.radius_neighbors_graph
251     <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with
252     ``mode='distance'``, then using ``metric='precomputed'`` here.
253 
254     Another way to reduce memory and computation time is to remove
255     (near-)duplicate points and use ``sample_weight`` instead.
256 
257     :class:`cluster.OPTICS` provides a similar clustering with lower memory
258     usage.
259 
260     References
261     ----------
262     Ester, M., H. P. Kriegel, J. Sander, and X. Xu, "A Density-Based
263     Algorithm for Discovering Clusters in Large Spatial Databases with Noise".
264     In: Proceedings of the 2nd International Conference on Knowledge Discovery
265     and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996
266 
267     Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).
268     DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.
269     ACM Transactions on Database Systems (TODS), 42(3), 19.
270     """
271 
272     def __init__(self, eps=0.5, min_samples=5, metric='euclidean',
273                  metric_params=None, algorithm='auto', leaf_size=30, p=None,
274                  n_jobs=None):
275         self.eps = eps
276         self.min_samples = min_samples
277         self.metric = metric
278         self.metric_params = metric_params
279         self.algorithm = algorithm
280         self.leaf_size = leaf_size
281         self.p = p
282         self.n_jobs = n_jobs
283 
284     def fit(self, X, y=None, sample_weight=None):
285         """Perform DBSCAN clustering from features, or distance matrix.
286 
287         Parameters
288         ----------
289         X : array-like or sparse matrix, shape (n_samples, n_features), or \
290             (n_samples, n_samples)
291             Training instances to cluster, or distances between instances if
292             ``metric='precomputed'``. If a sparse matrix is provided, it will
293             be converted into a sparse ``csr_matrix``.
294 
295         sample_weight : array, shape (n_samples,), optional
296             Weight of each sample, such that a sample with a weight of at least
297             ``min_samples`` is by itself a core sample; a sample with a
298             negative weight may inhibit its eps-neighbor from being core.
299             Note that weights are absolute, and default to 1.
300 
301         y : Ignored
302             Not used, present here for API consistency by convention.
303 
304         Returns
305         -------
306         self
307 
308         """
309         X = check_array(X, accept_sparse='csr')
310 
311         if not self.eps > 0.0:
312             raise ValueError("eps must be positive.")
313 
314         if sample_weight is not None:
315             sample_weight = np.asarray(sample_weight)
316             check_consistent_length(X, sample_weight)
317 
318         # Calculate neighborhood for all samples. This leaves the original
319         # point in, which needs to be considered later (i.e. point i is in the
320         # neighborhood of point i. While True, its useless information)
321         if self.metric == 'precomputed' and sparse.issparse(X):
322             # set the diagonal to explicit values, as a point is its own
323             # neighbor
324             with warnings.catch_warnings():
325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)
326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place
327 
328         neighbors_model = NearestNeighbors(
329             radius=self.eps, algorithm=self.algorithm,
330             leaf_size=self.leaf_size, metric=self.metric,
331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)
332         neighbors_model.fit(X)
333         # This has worst case O(n^2) memory complexity
334         neighborhoods = neighbors_model.radius_neighbors(X,
335                                                          return_distance=False)
336 
337         if sample_weight is None:
338             n_neighbors = np.array([len(neighbors)
339                                     for neighbors in neighborhoods])
340         else:
341             n_neighbors = np.array([np.sum(sample_weight[neighbors])
342                                     for neighbors in neighborhoods])
343 
344         # Initially, all samples are noise.
345         labels = np.full(X.shape[0], -1, dtype=np.intp)
346 
347         # A list of all core samples found.
348         core_samples = np.asarray(n_neighbors >= self.min_samples,
349                                   dtype=np.uint8)
350         dbscan_inner(core_samples, neighborhoods, labels)
351 
352         self.core_sample_indices_ = np.where(core_samples)[0]
353         self.labels_ = labels
354 
355         if len(self.core_sample_indices_):
356             # fix for scipy sparse indexing issue
357             self.components_ = X[self.core_sample_indices_].copy()
358         else:
359             # no core samples
360             self.components_ = np.empty((0, X.shape[1]))
361         return self
362 
363     def fit_predict(self, X, y=None, sample_weight=None):
364         """Perform DBSCAN clustering from features or distance matrix,
365         and return cluster labels.
366 
367         Parameters
368         ----------
369         X : array-like or sparse matrix, shape (n_samples, n_features), or \
370             (n_samples, n_samples)
371             Training instances to cluster, or distances between instances if
372             ``metric='precomputed'``. If a sparse matrix is provided, it will
373             be converted into a sparse ``csr_matrix``.
374 
375         sample_weight : array, shape (n_samples,), optional
376             Weight of each sample, such that a sample with a weight of at least
377             ``min_samples`` is by itself a core sample; a sample with a
378             negative weight may inhibit its eps-neighbor from being core.
379             Note that weights are absolute, and default to 1.
380 
381         y : Ignored
382             Not used, present here for API consistency by convention.
383 
384         Returns
385         -------
386         labels : ndarray, shape (n_samples,)
387             Cluster labels. Noisy samples are given the label -1.
388         """
389         self.fit(X, sample_weight=sample_weight)
390         return self.labels_

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/dummy.py</file>
<class>DummyClassifier</class>
<method>fit</method>
<code>
101     def fit(self, X, y, sample_weight=None):
102         """Fit the random classifier.
103 
104         Parameters
105         ----------
106         X : {array-like, object with finite length or shape}
107             Training data, requires length = n_samples
108 
109         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
110             Target values.
111 
112         sample_weight : array-like of shape (n_samples,), default=None
113             Sample weights.
114 
115         Returns
116         -------
117         self : object
118         """
119         allowed_strategies = ("most_frequent", "stratified", "uniform",
120                               "constant", "prior")
121         if self.strategy not in allowed_strategies:
122             raise ValueError("Unknown strategy type: %s, expected one of %s."
123                              % (self.strategy, allowed_strategies))
124 
125         if self.strategy == "uniform" and sp.issparse(y):
126             y = y.toarray()
127             warnings.warn('A local copy of the target data has been converted '
128                           'to a numpy array. Predicting on sparse target data '
129                           'with the uniform strategy would not save memory '
130                           'and would be slower.',
131                           UserWarning)
132 
133         self.sparse_output_ = sp.issparse(y)
134 
135         if not self.sparse_output_:
136             y = np.asarray(y)
137             y = np.atleast_1d(y)
138 
139         if y.ndim == 1:
140             y = np.reshape(y, (-1, 1))
141 
142         self.n_outputs_ = y.shape[1]
143 
144         check_consistent_length(X, y, sample_weight)
145 
146         if self.strategy == "constant":
147             if self.constant is None:
148                 raise ValueError("Constant target value has to be specified "
149                                  "when the constant strategy is used.")
150             else:
151                 constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))
152                 if constant.shape[0] != self.n_outputs_:
153                     raise ValueError("Constant target value should have "
154                                      "shape (%d, 1)." % self.n_outputs_)
155 
156         (self.classes_,
157          self.n_classes_,
158          self.class_prior_) = class_distribution(y, sample_weight)
159 
160         if self.strategy == "constant":
161             for k in range(self.n_outputs_):
162                 if not any(constant[k][0] == c for c in self.classes_[k]):
163                     # Checking in case of constant strategy if the constant
164                     # provided by the user is in y.
165                     err_msg = ("The constant target value must be present in "
166                                "the training data. You provided constant={}. "
167                                "Possible values are: {}."
168                                .format(self.constant, list(self.classes_[k])))
169                     raise ValueError(err_msg)
170 
171         if self.n_outputs_ == 1:
172             self.n_classes_ = self.n_classes_[0]
173             self.classes_ = self.classes_[0]
174             self.class_prior_ = self.class_prior_[0]
175 
176         return self

</code><intended_behavior>Replace manual use of check_consistent_length(X, y, sample_weight) with _check_sample_weight(y, sample_weight), assuming this also manages consistency validation internally. Ensure the method continues to correctly account for sample_weight in its calculations related to class distribution, constant strategy, or any other logic where sample_weight is relevant.</intended_behavior>, <file>sklearn/dummy.py</file>
<class>DummyClassifier</class>
<method>None</method>
<code>
23 class DummyClassifier(MultiOutputMixin, ClassifierMixin, BaseEstimator):
24     """
25     DummyClassifier is a classifier that makes predictions using simple rules.
26 
27     This classifier is useful as a simple baseline to compare with other
28     (real) classifiers. Do not use it for real problems.
29 
30     Read more in the :ref:`User Guide <dummy_estimators>`.
31 
32     Parameters
33     ----------
34     strategy : str, default="stratified"
35         Strategy to use to generate predictions.
36 
37         * "stratified": generates predictions by respecting the training
38           set's class distribution.
39         * "most_frequent": always predicts the most frequent label in the
40           training set.
41         * "prior": always predicts the class that maximizes the class prior
42           (like "most_frequent") and ``predict_proba`` returns the class prior.
43         * "uniform": generates predictions uniformly at random.
44         * "constant": always predicts a constant label that is provided by
45           the user. This is useful for metrics that evaluate a non-majority
46           class
47 
48           .. versionadded:: 0.17
49              Dummy Classifier now supports prior fitting strategy using
50              parameter *prior*.
51 
52     random_state : int, RandomState instance or None, optional, default=None
53         If int, random_state is the seed used by the random number generator;
54         If RandomState instance, random_state is the random number generator;
55         If None, the random number generator is the RandomState instance used
56         by `np.random`.
57 
58     constant : int or str or array-like of shape (n_outputs,)
59         The explicit constant as predicted by the "constant" strategy. This
60         parameter is useful only for the "constant" strategy.
61 
62     Attributes
63     ----------
64     classes_ : array or list of array of shape (n_classes,)
65         Class labels for each output.
66 
67     n_classes_ : array or list of array of shape (n_classes,)
68         Number of label for each output.
69 
70     class_prior_ : array or list of array of shape (n_classes,)
71         Probability of each class for each output.
72 
73     n_outputs_ : int,
74         Number of outputs.
75 
76     sparse_output_ : bool,
77         True if the array returned from predict is to be in sparse CSC format.
78         Is automatically set to True if the input y is passed in sparse format.
79 
80     Examples
81     --------
82     >>> import numpy as np
83     >>> from sklearn.dummy import DummyClassifier
84     >>> X = np.array([-1, 1, 1, 1])
85     >>> y = np.array([0, 1, 1, 1])
86     >>> dummy_clf = DummyClassifier(strategy="most_frequent")
87     >>> dummy_clf.fit(X, y)
88     DummyClassifier(strategy='most_frequent')
89     >>> dummy_clf.predict(X)
90     array([1, 1, 1, 1])
91     >>> dummy_clf.score(X, y)
92     0.75
93     """
94 
95     def __init__(self, strategy="stratified", random_state=None,
96                  constant=None):
97         self.strategy = strategy
98         self.random_state = random_state
99         self.constant = constant
100 
101     def fit(self, X, y, sample_weight=None):
102         """Fit the random classifier.
103 
104         Parameters
105         ----------
106         X : {array-like, object with finite length or shape}
107             Training data, requires length = n_samples
108 
109         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
110             Target values.
111 
112         sample_weight : array-like of shape (n_samples,), default=None
113             Sample weights.
114 
115         Returns
116         -------
117         self : object
118         """
119         allowed_strategies = ("most_frequent", "stratified", "uniform",
120                               "constant", "prior")
121         if self.strategy not in allowed_strategies:
122             raise ValueError("Unknown strategy type: %s, expected one of %s."
123                              % (self.strategy, allowed_strategies))
124 
125         if self.strategy == "uniform" and sp.issparse(y):
126             y = y.toarray()
127             warnings.warn('A local copy of the target data has been converted '
128                           'to a numpy array. Predicting on sparse target data '
129                           'with the uniform strategy would not save memory '
130                           'and would be slower.',
131                           UserWarning)
132 
133         self.sparse_output_ = sp.issparse(y)
134 
135         if not self.sparse_output_:
136             y = np.asarray(y)
137             y = np.atleast_1d(y)
138 
139         if y.ndim == 1:
140             y = np.reshape(y, (-1, 1))
141 
142         self.n_outputs_ = y.shape[1]
143 
144         check_consistent_length(X, y, sample_weight)
145 
146         if self.strategy == "constant":
147             if self.constant is None:
148                 raise ValueError("Constant target value has to be specified "
149                                  "when the constant strategy is used.")
150             else:
151                 constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))
152                 if constant.shape[0] != self.n_outputs_:
153                     raise ValueError("Constant target value should have "
154                                      "shape (%d, 1)." % self.n_outputs_)
155 
156         (self.classes_,
157          self.n_classes_,
158          self.class_prior_) = class_distribution(y, sample_weight)
159 
160         if self.strategy == "constant":
161             for k in range(self.n_outputs_):
162                 if not any(constant[k][0] == c for c in self.classes_[k]):
163                     # Checking in case of constant strategy if the constant
164                     # provided by the user is in y.
165                     err_msg = ("The constant target value must be present in "
166                                "the training data. You provided constant={}. "
167                                "Possible values are: {}."
168                                .format(self.constant, list(self.classes_[k])))
169                     raise ValueError(err_msg)
170 
171         if self.n_outputs_ == 1:
172             self.n_classes_ = self.n_classes_[0]
173             self.classes_ = self.classes_[0]
174             self.class_prior_ = self.class_prior_[0]
175 
176         return self
177 
178     def predict(self, X):
179         """Perform classification on test vectors X.
180 
181         Parameters
182         ----------
183         X : {array-like, object with finite length or shape}
184             Training data, requires length = n_samples
185 
186         Returns
187         -------
188         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
189             Predicted target values for X.
190         """
191         check_is_fitted(self)
192 
193         # numpy random_state expects Python int and not long as size argument
194         # under Windows
195         n_samples = _num_samples(X)
196         rs = check_random_state(self.random_state)
197 
198         n_classes_ = self.n_classes_
199         classes_ = self.classes_
200         class_prior_ = self.class_prior_
201         constant = self.constant
202         if self.n_outputs_ == 1:
203             # Get same type even for self.n_outputs_ == 1
204             n_classes_ = [n_classes_]
205             classes_ = [classes_]
206             class_prior_ = [class_prior_]
207             constant = [constant]
208         # Compute probability only once
209         if self.strategy == "stratified":
210             proba = self.predict_proba(X)
211             if self.n_outputs_ == 1:
212                 proba = [proba]
213 
214         if self.sparse_output_:
215             class_prob = None
216             if self.strategy in ("most_frequent", "prior"):
217                 classes_ = [np.array([cp.argmax()]) for cp in class_prior_]
218 
219             elif self.strategy == "stratified":
220                 class_prob = class_prior_
221 
222             elif self.strategy == "uniform":
223                 raise ValueError("Sparse target prediction is not "
224                                  "supported with the uniform strategy")
225 
226             elif self.strategy == "constant":
227                 classes_ = [np.array([c]) for c in constant]
228 
229             y = _random_choice_csc(n_samples, classes_, class_prob,
230                                   self.random_state)
231         else:
232             if self.strategy in ("most_frequent", "prior"):
233                 y = np.tile([classes_[k][class_prior_[k].argmax()] for
234                              k in range(self.n_outputs_)], [n_samples, 1])
235 
236             elif self.strategy == "stratified":
237                 y = np.vstack([classes_[k][proba[k].argmax(axis=1)] for
238                                k in range(self.n_outputs_)]).T
239 
240             elif self.strategy == "uniform":
241                 ret = [classes_[k][rs.randint(n_classes_[k], size=n_samples)]
242                        for k in range(self.n_outputs_)]
243                 y = np.vstack(ret).T
244 
245             elif self.strategy == "constant":
246                 y = np.tile(self.constant, (n_samples, 1))
247 
248             if self.n_outputs_ == 1:
249                 y = np.ravel(y)
250 
251         return y
252 
253     def predict_proba(self, X):
254         """
255         Return probability estimates for the test vectors X.
256 
257         Parameters
258         ----------
259         X : {array-like, object with finite length or shape}
260             Training data, requires length = n_samples
261 
262         Returns
263         -------
264         P : array-like or list of array-lke of shape (n_samples, n_classes)
265             Returns the probability of the sample for each class in
266             the model, where classes are ordered arithmetically, for each
267             output.
268         """
269         check_is_fitted(self)
270 
271         # numpy random_state expects Python int and not long as size argument
272         # under Windows
273         n_samples = _num_samples(X)
274         rs = check_random_state(self.random_state)
275 
276         n_classes_ = self.n_classes_
277         classes_ = self.classes_
278         class_prior_ = self.class_prior_
279         constant = self.constant
280         if self.n_outputs_ == 1:
281             # Get same type even for self.n_outputs_ == 1
282             n_classes_ = [n_classes_]
283             classes_ = [classes_]
284             class_prior_ = [class_prior_]
285             constant = [constant]
286 
287         P = []
288         for k in range(self.n_outputs_):
289             if self.strategy == "most_frequent":
290                 ind = class_prior_[k].argmax()
291                 out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)
292                 out[:, ind] = 1.0
293             elif self.strategy == "prior":
294                 out = np.ones((n_samples, 1)) * class_prior_[k]
295 
296             elif self.strategy == "stratified":
297                 out = rs.multinomial(1, class_prior_[k], size=n_samples)
298                 out = out.astype(np.float64)
299 
300             elif self.strategy == "uniform":
301                 out = np.ones((n_samples, n_classes_[k]), dtype=np.float64)
302                 out /= n_classes_[k]
303 
304             elif self.strategy == "constant":
305                 ind = np.where(classes_[k] == constant[k])
306                 out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)
307                 out[:, ind] = 1.0
308 
309             P.append(out)
310 
311         if self.n_outputs_ == 1:
312             P = P[0]
313 
314         return P
315 
316     def predict_log_proba(self, X):
317         """
318         Return log probability estimates for the test vectors X.
319 
320         Parameters
321         ----------
322         X : {array-like, object with finite length or shape}
323             Training data, requires length = n_samples
324 
325         Returns
326         -------
327         P : array-like or list of array-like of shape (n_samples, n_classes)
328             Returns the log probability of the sample for each class in
329             the model, where classes are ordered arithmetically for each
330             output.
331         """
332         proba = self.predict_proba(X)
333         if self.n_outputs_ == 1:
334             return np.log(proba)
335         else:
336             return [np.log(p) for p in proba]
337 
338     def _more_tags(self):
339         return {'poor_score': True, 'no_validation': True}
340 
341     def score(self, X, y, sample_weight=None):
342         """Returns the mean accuracy on the given test data and labels.
343 
344         In multi-label classification, this is the subset accuracy
345         which is a harsh metric since you require for each sample that
346         each label set be correctly predicted.
347 
348         Parameters
349         ----------
350         X : {array-like, None}
351             Test samples with shape = (n_samples, n_features) or
352             None. Passing None as test samples gives the same result
353             as passing real test samples, since DummyClassifier
354             operates independently of the sampled observations.
355 
356         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
357             True labels for X.
358 
359         sample_weight : array-like of shape (n_samples,), default=None
360             Sample weights.
361 
362         Returns
363         -------
364         score : float
365             Mean accuracy of self.predict(X) wrt. y.
366 
367         """
368         if X is None:
369             X = np.zeros(shape=(len(y), 1))
370         return super().score(X, y, sample_weight)
371 
372     @deprecated(
373         "The outputs_2d_ attribute is deprecated in version 0.22 "
374         "and will be removed in version 0.24. It is equivalent to "
375         "n_outputs_ > 1."
376     )
377     @property
378     def outputs_2d_(self):
379         return self.n_outputs_ != 1

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 05:57:07.429 | INFO     | Search completed. Bug locations: [<file>sklearn/calibration.py</file>
<class>CalibratedClassifierCV</class>
<method>fit</method>
<code>
114     def fit(self, X, y, sample_weight=None):
115         """Fit the calibrated model
116 
117         Parameters
118         ----------
119         X : array-like, shape (n_samples, n_features)
120             Training data.
121 
122         y : array-like, shape (n_samples,)
123             Target values.
124 
125         sample_weight : array-like of shape (n_samples,), default=None
126             Sample weights. If None, then samples are equally weighted.
127 
128         Returns
129         -------
130         self : object
131             Returns an instance of self.
132         """
133         X, y = check_X_y(X, y, accept_sparse=['csc', 'csr', 'coo'],
134                          force_all_finite=False, allow_nd=True)
135         X, y = indexable(X, y)
136         le = LabelBinarizer().fit(y)
137         self.classes_ = le.classes_
138 
139         # Check that each cross-validation fold can have at least one
140         # example per class
141         n_folds = self.cv if isinstance(self.cv, int) \
142             else self.cv.n_folds if hasattr(self.cv, "n_folds") else None
143         if n_folds and \
144                 np.any([np.sum(y == class_) < n_folds for class_ in
145                         self.classes_]):
146             raise ValueError("Requesting %d-fold cross-validation but provided"
147                              " less than %d examples for at least one class."
148                              % (n_folds, n_folds))
149 
150         self.calibrated_classifiers_ = []
151         if self.base_estimator is None:
152             # we want all classifiers that don't expose a random_state
153             # to be deterministic (and we don't want to expose this one).
154             base_estimator = LinearSVC(random_state=0)
155         else:
156             base_estimator = self.base_estimator
157 
158         if self.cv == "prefit":
159             calibrated_classifier = _CalibratedClassifier(
160                 base_estimator, method=self.method)
161             if sample_weight is not None:
162                 calibrated_classifier.fit(X, y, sample_weight)
163             else:
164                 calibrated_classifier.fit(X, y)
165             self.calibrated_classifiers_.append(calibrated_classifier)
166         else:
167             cv = check_cv(self.cv, y, classifier=True)
168             fit_parameters = signature(base_estimator.fit).parameters
169             estimator_name = type(base_estimator).__name__
170             if (sample_weight is not None
171                     and "sample_weight" not in fit_parameters):
172                 warnings.warn("%s does not support sample_weight. Samples"
173                               " weights are only used for the calibration"
174                               " itself." % estimator_name)
175                 sample_weight = check_array(sample_weight, ensure_2d=False)
176                 base_estimator_sample_weight = None
177             else:
178                 if sample_weight is not None:
179                     sample_weight = check_array(sample_weight, ensure_2d=False)
180                     check_consistent_length(y, sample_weight)
181                 base_estimator_sample_weight = sample_weight
182             for train, test in cv.split(X, y):
183                 this_estimator = clone(base_estimator)
184                 if base_estimator_sample_weight is not None:
185                     this_estimator.fit(
186                         X[train], y[train],
187                         sample_weight=base_estimator_sample_weight[train])
188                 else:
189                     this_estimator.fit(X[train], y[train])
190 
191                 calibrated_classifier = _CalibratedClassifier(
192                     this_estimator, method=self.method,
193                     classes=self.classes_)
194                 if sample_weight is not None:
195                     calibrated_classifier.fit(X[test], y[test],
196                                               sample_weight[test])
197                 else:
198                     calibrated_classifier.fit(X[test], y[test])
199                 self.calibrated_classifiers_.append(calibrated_classifier)
200 
201         return self

</code><intended_behavior>Instead of directly using check_array(sample_weight, ensure_2d=False) and manual checks (if sample_weight is not None), replace these with _check_sample_weight(X, sample_weight) from utils.validation. This change should make sample_weight validation consistent with the project standards. Ensure that all functionality allowing for sample_weight to be None or array-like is preserved.</intended_behavior>, <file>sklearn/calibration.py</file>
<class>CalibratedClassifierCV</class>
<method>None</method>
<code>
31 class CalibratedClassifierCV(BaseEstimator, ClassifierMixin,
32                              MetaEstimatorMixin):
33     """Probability calibration with isotonic regression or sigmoid.
34 
35     See glossary entry for :term:`cross-validation estimator`.
36 
37     With this class, the base_estimator is fit on the train set of the
38     cross-validation generator and the test set is used for calibration.
39     The probabilities for each of the folds are then averaged
40     for prediction. In case that cv="prefit" is passed to __init__,
41     it is assumed that base_estimator has been fitted already and all
42     data is used for calibration. Note that data for fitting the
43     classifier and for calibrating it must be disjoint.
44 
45     Read more in the :ref:`User Guide <calibration>`.
46 
47     Parameters
48     ----------
49     base_estimator : instance BaseEstimator
50         The classifier whose output decision function needs to be calibrated
51         to offer more accurate predict_proba outputs. If cv=prefit, the
52         classifier must have been fit already on data.
53 
54     method : 'sigmoid' or 'isotonic'
55         The method to use for calibration. Can be 'sigmoid' which
56         corresponds to Platt's method or 'isotonic' which is a
57         non-parametric approach. It is not advised to use isotonic calibration
58         with too few calibration samples ``(<<1000)`` since it tends to
59         overfit.
60         Use sigmoids (Platt's calibration) in this case.
61 
62     cv : integer, cross-validation generator, iterable or "prefit", optional
63         Determines the cross-validation splitting strategy.
64         Possible inputs for cv are:
65 
66         - None, to use the default 5-fold cross-validation,
67         - integer, to specify the number of folds.
68         - :term:`CV splitter`,
69         - An iterable yielding (train, test) splits as arrays of indices.
70 
71         For integer/None inputs, if ``y`` is binary or multiclass,
72         :class:`sklearn.model_selection.StratifiedKFold` is used. If ``y`` is
73         neither binary nor multiclass, :class:`sklearn.model_selection.KFold`
74         is used.
75 
76         Refer :ref:`User Guide <cross_validation>` for the various
77         cross-validation strategies that can be used here.
78 
79         If "prefit" is passed, it is assumed that base_estimator has been
80         fitted already and all data is used for calibration.
81 
82         .. versionchanged:: 0.22
83             ``cv`` default value if None changed from 3-fold to 5-fold.
84 
85     Attributes
86     ----------
87     classes_ : array, shape (n_classes)
88         The class labels.
89 
90     calibrated_classifiers_ : list (len() equal to cv or 1 if cv == "prefit")
91         The list of calibrated classifiers, one for each crossvalidation fold,
92         which has been fitted on all but the validation fold and calibrated
93         on the validation fold.
94 
95     References
96     ----------
97     .. [1] Obtaining calibrated probability estimates from decision trees
98            and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001
99 
100     .. [2] Transforming Classifier Scores into Accurate Multiclass
101            Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)
102 
103     .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to
104            Regularized Likelihood Methods, J. Platt, (1999)
105 
106     .. [4] Predicting Good Probabilities with Supervised Learning,
107            A. Niculescu-Mizil & R. Caruana, ICML 2005
108     """
109     def __init__(self, base_estimator=None, method='sigmoid', cv=None):
110         self.base_estimator = base_estimator
111         self.method = method
112         self.cv = cv
113 
114     def fit(self, X, y, sample_weight=None):
115         """Fit the calibrated model
116 
117         Parameters
118         ----------
119         X : array-like, shape (n_samples, n_features)
120             Training data.
121 
122         y : array-like, shape (n_samples,)
123             Target values.
124 
125         sample_weight : array-like of shape (n_samples,), default=None
126             Sample weights. If None, then samples are equally weighted.
127 
128         Returns
129         -------
130         self : object
131             Returns an instance of self.
132         """
133         X, y = check_X_y(X, y, accept_sparse=['csc', 'csr', 'coo'],
134                          force_all_finite=False, allow_nd=True)
135         X, y = indexable(X, y)
136         le = LabelBinarizer().fit(y)
137         self.classes_ = le.classes_
138 
139         # Check that each cross-validation fold can have at least one
140         # example per class
141         n_folds = self.cv if isinstance(self.cv, int) \
142             else self.cv.n_folds if hasattr(self.cv, "n_folds") else None
143         if n_folds and \
144                 np.any([np.sum(y == class_) < n_folds for class_ in
145                         self.classes_]):
146             raise ValueError("Requesting %d-fold cross-validation but provided"
147                              " less than %d examples for at least one class."
148                              % (n_folds, n_folds))
149 
150         self.calibrated_classifiers_ = []
151         if self.base_estimator is None:
152             # we want all classifiers that don't expose a random_state
153             # to be deterministic (and we don't want to expose this one).
154             base_estimator = LinearSVC(random_state=0)
155         else:
156             base_estimator = self.base_estimator
157 
158         if self.cv == "prefit":
159             calibrated_classifier = _CalibratedClassifier(
160                 base_estimator, method=self.method)
161             if sample_weight is not None:
162                 calibrated_classifier.fit(X, y, sample_weight)
163             else:
164                 calibrated_classifier.fit(X, y)
165             self.calibrated_classifiers_.append(calibrated_classifier)
166         else:
167             cv = check_cv(self.cv, y, classifier=True)
168             fit_parameters = signature(base_estimator.fit).parameters
169             estimator_name = type(base_estimator).__name__
170             if (sample_weight is not None
171                     and "sample_weight" not in fit_parameters):
172                 warnings.warn("%s does not support sample_weight. Samples"
173                               " weights are only used for the calibration"
174                               " itself." % estimator_name)
175                 sample_weight = check_array(sample_weight, ensure_2d=False)
176                 base_estimator_sample_weight = None
177             else:
178                 if sample_weight is not None:
179                     sample_weight = check_array(sample_weight, ensure_2d=False)
180                     check_consistent_length(y, sample_weight)
181                 base_estimator_sample_weight = sample_weight
182             for train, test in cv.split(X, y):
183                 this_estimator = clone(base_estimator)
184                 if base_estimator_sample_weight is not None:
185                     this_estimator.fit(
186                         X[train], y[train],
187                         sample_weight=base_estimator_sample_weight[train])
188                 else:
189                     this_estimator.fit(X[train], y[train])
190 
191                 calibrated_classifier = _CalibratedClassifier(
192                     this_estimator, method=self.method,
193                     classes=self.classes_)
194                 if sample_weight is not None:
195                     calibrated_classifier.fit(X[test], y[test],
196                                               sample_weight[test])
197                 else:
198                     calibrated_classifier.fit(X[test], y[test])
199                 self.calibrated_classifiers_.append(calibrated_classifier)
200 
201         return self
202 
203     def predict_proba(self, X):
204         """Posterior probabilities of classification
205 
206         This function returns posterior probabilities of classification
207         according to each class on an array of test vectors X.
208 
209         Parameters
210         ----------
211         X : array-like, shape (n_samples, n_features)
212             The samples.
213 
214         Returns
215         -------
216         C : array, shape (n_samples, n_classes)
217             The predicted probas.
218         """
219         check_is_fitted(self)
220         X = check_array(X, accept_sparse=['csc', 'csr', 'coo'],
221                         force_all_finite=False)
222         # Compute the arithmetic mean of the predictions of the calibrated
223         # classifiers
224         mean_proba = np.zeros((X.shape[0], len(self.classes_)))
225         for calibrated_classifier in self.calibrated_classifiers_:
226             proba = calibrated_classifier.predict_proba(X)
227             mean_proba += proba
228 
229         mean_proba /= len(self.calibrated_classifiers_)
230 
231         return mean_proba
232 
233     def predict(self, X):
234         """Predict the target of new samples. Can be different from the
235         prediction of the uncalibrated classifier.
236 
237         Parameters
238         ----------
239         X : array-like, shape (n_samples, n_features)
240             The samples.
241 
242         Returns
243         -------
244         C : array, shape (n_samples,)
245             The predicted class.
246         """
247         check_is_fitted(self)
248         return self.classes_[np.argmax(self.predict_proba(X), axis=1)]

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/cluster/_dbscan.py</file>
<class>DBSCAN</class>
<method>fit</method>
<code>
284     def fit(self, X, y=None, sample_weight=None):
285         """Perform DBSCAN clustering from features, or distance matrix.
286 
287         Parameters
288         ----------
289         X : array-like or sparse matrix, shape (n_samples, n_features), or \
290             (n_samples, n_samples)
291             Training instances to cluster, or distances between instances if
292             ``metric='precomputed'``. If a sparse matrix is provided, it will
293             be converted into a sparse ``csr_matrix``.
294 
295         sample_weight : array, shape (n_samples,), optional
296             Weight of each sample, such that a sample with a weight of at least
297             ``min_samples`` is by itself a core sample; a sample with a
298             negative weight may inhibit its eps-neighbor from being core.
299             Note that weights are absolute, and default to 1.
300 
301         y : Ignored
302             Not used, present here for API consistency by convention.
303 
304         Returns
305         -------
306         self
307 
308         """
309         X = check_array(X, accept_sparse='csr')
310 
311         if not self.eps > 0.0:
312             raise ValueError("eps must be positive.")
313 
314         if sample_weight is not None:
315             sample_weight = np.asarray(sample_weight)
316             check_consistent_length(X, sample_weight)
317 
318         # Calculate neighborhood for all samples. This leaves the original
319         # point in, which needs to be considered later (i.e. point i is in the
320         # neighborhood of point i. While True, its useless information)
321         if self.metric == 'precomputed' and sparse.issparse(X):
322             # set the diagonal to explicit values, as a point is its own
323             # neighbor
324             with warnings.catch_warnings():
325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)
326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place
327 
328         neighbors_model = NearestNeighbors(
329             radius=self.eps, algorithm=self.algorithm,
330             leaf_size=self.leaf_size, metric=self.metric,
331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)
332         neighbors_model.fit(X)
333         # This has worst case O(n^2) memory complexity
334         neighborhoods = neighbors_model.radius_neighbors(X,
335                                                          return_distance=False)
336 
337         if sample_weight is None:
338             n_neighbors = np.array([len(neighbors)
339                                     for neighbors in neighborhoods])
340         else:
341             n_neighbors = np.array([np.sum(sample_weight[neighbors])
342                                     for neighbors in neighborhoods])
343 
344         # Initially, all samples are noise.
345         labels = np.full(X.shape[0], -1, dtype=np.intp)
346 
347         # A list of all core samples found.
348         core_samples = np.asarray(n_neighbors >= self.min_samples,
349                                   dtype=np.uint8)
350         dbscan_inner(core_samples, neighborhoods, labels)
351 
352         self.core_sample_indices_ = np.where(core_samples)[0]
353         self.labels_ = labels
354 
355         if len(self.core_sample_indices_):
356             # fix for scipy sparse indexing issue
357             self.components_ = X[self.core_sample_indices_].copy()
358         else:
359             # no core samples
360             self.components_ = np.empty((0, X.shape[1]))
361         return self

</code><intended_behavior>Replace manual array conversion and length checking of sample_weight (np.asarray(sample_weight) and check_consistent_length(X, sample_weight)) with _check_sample_weight(X, sample_weight) ensuring sample_weight validation aligns with the unified approach. Ensure the existing logic of how sample_weight affects core sample determination remains intact.</intended_behavior>, <file>sklearn/cluster/_dbscan.py</file>
<class>DBSCAN</class>
<method>None</method>
<code>
147 class DBSCAN(ClusterMixin, BaseEstimator):
148     """Perform DBSCAN clustering from vector array or distance matrix.
149 
150     DBSCAN - Density-Based Spatial Clustering of Applications with Noise.
151     Finds core samples of high density and expands clusters from them.
152     Good for data which contains clusters of similar density.
153 
154     Read more in the :ref:`User Guide <dbscan>`.
155 
156     Parameters
157     ----------
158     eps : float, optional
159         The maximum distance between two samples for one to be considered
160         as in the neighborhood of the other. This is not a maximum bound
161         on the distances of points within a cluster. This is the most
162         important DBSCAN parameter to choose appropriately for your data set
163         and distance function.
164 
165     min_samples : int, optional
166         The number of samples (or total weight) in a neighborhood for a point
167         to be considered as a core point. This includes the point itself.
168 
169     metric : string, or callable
170         The metric to use when calculating distance between instances in a
171         feature array. If metric is a string or callable, it must be one of
172         the options allowed by :func:`sklearn.metrics.pairwise_distances` for
173         its metric parameter.
174         If metric is "precomputed", X is assumed to be a distance matrix and
175         must be square. X may be a :term:`Glossary <sparse graph>`, in which
176         case only "nonzero" elements may be considered neighbors for DBSCAN.
177 
178         .. versionadded:: 0.17
179            metric *precomputed* to accept precomputed sparse matrix.
180 
181     metric_params : dict, optional
182         Additional keyword arguments for the metric function.
183 
184         .. versionadded:: 0.19
185 
186     algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional
187         The algorithm to be used by the NearestNeighbors module
188         to compute pointwise distances and find nearest neighbors.
189         See NearestNeighbors module documentation for details.
190 
191     leaf_size : int, optional (default = 30)
192         Leaf size passed to BallTree or cKDTree. This can affect the speed
193         of the construction and query, as well as the memory required
194         to store the tree. The optimal value depends
195         on the nature of the problem.
196 
197     p : float, optional
198         The power of the Minkowski metric to be used to calculate distance
199         between points.
200 
201     n_jobs : int or None, optional (default=None)
202         The number of parallel jobs to run.
203         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
204         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
205         for more details.
206 
207     Attributes
208     ----------
209     core_sample_indices_ : array, shape = [n_core_samples]
210         Indices of core samples.
211 
212     components_ : array, shape = [n_core_samples, n_features]
213         Copy of each core sample found by training.
214 
215     labels_ : array, shape = [n_samples]
216         Cluster labels for each point in the dataset given to fit().
217         Noisy samples are given the label -1.
218 
219     Examples
220     --------
221     >>> from sklearn.cluster import DBSCAN
222     >>> import numpy as np
223     >>> X = np.array([[1, 2], [2, 2], [2, 3],
224     ...               [8, 7], [8, 8], [25, 80]])
225     >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)
226     >>> clustering.labels_
227     array([ 0,  0,  0,  1,  1, -1])
228     >>> clustering
229     DBSCAN(eps=3, min_samples=2)
230 
231     See also
232     --------
233     OPTICS
234         A similar clustering at multiple values of eps. Our implementation
235         is optimized for memory usage.
236 
237     Notes
238     -----
239     For an example, see :ref:`examples/cluster/plot_dbscan.py
240     <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.
241 
242     This implementation bulk-computes all neighborhood queries, which increases
243     the memory complexity to O(n.d) where d is the average number of neighbors,
244     while original DBSCAN had memory complexity O(n). It may attract a higher
245     memory complexity when querying these nearest neighborhoods, depending
246     on the ``algorithm``.
247 
248     One way to avoid the query complexity is to pre-compute sparse
249     neighborhoods in chunks using
250     :func:`NearestNeighbors.radius_neighbors_graph
251     <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with
252     ``mode='distance'``, then using ``metric='precomputed'`` here.
253 
254     Another way to reduce memory and computation time is to remove
255     (near-)duplicate points and use ``sample_weight`` instead.
256 
257     :class:`cluster.OPTICS` provides a similar clustering with lower memory
258     usage.
259 
260     References
261     ----------
262     Ester, M., H. P. Kriegel, J. Sander, and X. Xu, "A Density-Based
263     Algorithm for Discovering Clusters in Large Spatial Databases with Noise".
264     In: Proceedings of the 2nd International Conference on Knowledge Discovery
265     and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996
266 
267     Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).
268     DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.
269     ACM Transactions on Database Systems (TODS), 42(3), 19.
270     """
271 
272     def __init__(self, eps=0.5, min_samples=5, metric='euclidean',
273                  metric_params=None, algorithm='auto', leaf_size=30, p=None,
274                  n_jobs=None):
275         self.eps = eps
276         self.min_samples = min_samples
277         self.metric = metric
278         self.metric_params = metric_params
279         self.algorithm = algorithm
280         self.leaf_size = leaf_size
281         self.p = p
282         self.n_jobs = n_jobs
283 
284     def fit(self, X, y=None, sample_weight=None):
285         """Perform DBSCAN clustering from features, or distance matrix.
286 
287         Parameters
288         ----------
289         X : array-like or sparse matrix, shape (n_samples, n_features), or \
290             (n_samples, n_samples)
291             Training instances to cluster, or distances between instances if
292             ``metric='precomputed'``. If a sparse matrix is provided, it will
293             be converted into a sparse ``csr_matrix``.
294 
295         sample_weight : array, shape (n_samples,), optional
296             Weight of each sample, such that a sample with a weight of at least
297             ``min_samples`` is by itself a core sample; a sample with a
298             negative weight may inhibit its eps-neighbor from being core.
299             Note that weights are absolute, and default to 1.
300 
301         y : Ignored
302             Not used, present here for API consistency by convention.
303 
304         Returns
305         -------
306         self
307 
308         """
309         X = check_array(X, accept_sparse='csr')
310 
311         if not self.eps > 0.0:
312             raise ValueError("eps must be positive.")
313 
314         if sample_weight is not None:
315             sample_weight = np.asarray(sample_weight)
316             check_consistent_length(X, sample_weight)
317 
318         # Calculate neighborhood for all samples. This leaves the original
319         # point in, which needs to be considered later (i.e. point i is in the
320         # neighborhood of point i. While True, its useless information)
321         if self.metric == 'precomputed' and sparse.issparse(X):
322             # set the diagonal to explicit values, as a point is its own
323             # neighbor
324             with warnings.catch_warnings():
325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)
326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place
327 
328         neighbors_model = NearestNeighbors(
329             radius=self.eps, algorithm=self.algorithm,
330             leaf_size=self.leaf_size, metric=self.metric,
331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)
332         neighbors_model.fit(X)
333         # This has worst case O(n^2) memory complexity
334         neighborhoods = neighbors_model.radius_neighbors(X,
335                                                          return_distance=False)
336 
337         if sample_weight is None:
338             n_neighbors = np.array([len(neighbors)
339                                     for neighbors in neighborhoods])
340         else:
341             n_neighbors = np.array([np.sum(sample_weight[neighbors])
342                                     for neighbors in neighborhoods])
343 
344         # Initially, all samples are noise.
345         labels = np.full(X.shape[0], -1, dtype=np.intp)
346 
347         # A list of all core samples found.
348         core_samples = np.asarray(n_neighbors >= self.min_samples,
349                                   dtype=np.uint8)
350         dbscan_inner(core_samples, neighborhoods, labels)
351 
352         self.core_sample_indices_ = np.where(core_samples)[0]
353         self.labels_ = labels
354 
355         if len(self.core_sample_indices_):
356             # fix for scipy sparse indexing issue
357             self.components_ = X[self.core_sample_indices_].copy()
358         else:
359             # no core samples
360             self.components_ = np.empty((0, X.shape[1]))
361         return self
362 
363     def fit_predict(self, X, y=None, sample_weight=None):
364         """Perform DBSCAN clustering from features or distance matrix,
365         and return cluster labels.
366 
367         Parameters
368         ----------
369         X : array-like or sparse matrix, shape (n_samples, n_features), or \
370             (n_samples, n_samples)
371             Training instances to cluster, or distances between instances if
372             ``metric='precomputed'``. If a sparse matrix is provided, it will
373             be converted into a sparse ``csr_matrix``.
374 
375         sample_weight : array, shape (n_samples,), optional
376             Weight of each sample, such that a sample with a weight of at least
377             ``min_samples`` is by itself a core sample; a sample with a
378             negative weight may inhibit its eps-neighbor from being core.
379             Note that weights are absolute, and default to 1.
380 
381         y : Ignored
382             Not used, present here for API consistency by convention.
383 
384         Returns
385         -------
386         labels : ndarray, shape (n_samples,)
387             Cluster labels. Noisy samples are given the label -1.
388         """
389         self.fit(X, sample_weight=sample_weight)
390         return self.labels_

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/dummy.py</file>
<class>DummyClassifier</class>
<method>fit</method>
<code>
101     def fit(self, X, y, sample_weight=None):
102         """Fit the random classifier.
103 
104         Parameters
105         ----------
106         X : {array-like, object with finite length or shape}
107             Training data, requires length = n_samples
108 
109         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
110             Target values.
111 
112         sample_weight : array-like of shape (n_samples,), default=None
113             Sample weights.
114 
115         Returns
116         -------
117         self : object
118         """
119         allowed_strategies = ("most_frequent", "stratified", "uniform",
120                               "constant", "prior")
121         if self.strategy not in allowed_strategies:
122             raise ValueError("Unknown strategy type: %s, expected one of %s."
123                              % (self.strategy, allowed_strategies))
124 
125         if self.strategy == "uniform" and sp.issparse(y):
126             y = y.toarray()
127             warnings.warn('A local copy of the target data has been converted '
128                           'to a numpy array. Predicting on sparse target data '
129                           'with the uniform strategy would not save memory '
130                           'and would be slower.',
131                           UserWarning)
132 
133         self.sparse_output_ = sp.issparse(y)
134 
135         if not self.sparse_output_:
136             y = np.asarray(y)
137             y = np.atleast_1d(y)
138 
139         if y.ndim == 1:
140             y = np.reshape(y, (-1, 1))
141 
142         self.n_outputs_ = y.shape[1]
143 
144         check_consistent_length(X, y, sample_weight)
145 
146         if self.strategy == "constant":
147             if self.constant is None:
148                 raise ValueError("Constant target value has to be specified "
149                                  "when the constant strategy is used.")
150             else:
151                 constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))
152                 if constant.shape[0] != self.n_outputs_:
153                     raise ValueError("Constant target value should have "
154                                      "shape (%d, 1)." % self.n_outputs_)
155 
156         (self.classes_,
157          self.n_classes_,
158          self.class_prior_) = class_distribution(y, sample_weight)
159 
160         if self.strategy == "constant":
161             for k in range(self.n_outputs_):
162                 if not any(constant[k][0] == c for c in self.classes_[k]):
163                     # Checking in case of constant strategy if the constant
164                     # provided by the user is in y.
165                     err_msg = ("The constant target value must be present in "
166                                "the training data. You provided constant={}. "
167                                "Possible values are: {}."
168                                .format(self.constant, list(self.classes_[k])))
169                     raise ValueError(err_msg)
170 
171         if self.n_outputs_ == 1:
172             self.n_classes_ = self.n_classes_[0]
173             self.classes_ = self.classes_[0]
174             self.class_prior_ = self.class_prior_[0]
175 
176         return self

</code><intended_behavior>Replace manual use of check_consistent_length(X, y, sample_weight) with _check_sample_weight(y, sample_weight), assuming this also manages consistency validation internally. Ensure the method continues to correctly account for sample_weight in its calculations related to class distribution, constant strategy, or any other logic where sample_weight is relevant.</intended_behavior>, <file>sklearn/dummy.py</file>
<class>DummyClassifier</class>
<method>None</method>
<code>
23 class DummyClassifier(MultiOutputMixin, ClassifierMixin, BaseEstimator):
24     """
25     DummyClassifier is a classifier that makes predictions using simple rules.
26 
27     This classifier is useful as a simple baseline to compare with other
28     (real) classifiers. Do not use it for real problems.
29 
30     Read more in the :ref:`User Guide <dummy_estimators>`.
31 
32     Parameters
33     ----------
34     strategy : str, default="stratified"
35         Strategy to use to generate predictions.
36 
37         * "stratified": generates predictions by respecting the training
38           set's class distribution.
39         * "most_frequent": always predicts the most frequent label in the
40           training set.
41         * "prior": always predicts the class that maximizes the class prior
42           (like "most_frequent") and ``predict_proba`` returns the class prior.
43         * "uniform": generates predictions uniformly at random.
44         * "constant": always predicts a constant label that is provided by
45           the user. This is useful for metrics that evaluate a non-majority
46           class
47 
48           .. versionadded:: 0.17
49              Dummy Classifier now supports prior fitting strategy using
50              parameter *prior*.
51 
52     random_state : int, RandomState instance or None, optional, default=None
53         If int, random_state is the seed used by the random number generator;
54         If RandomState instance, random_state is the random number generator;
55         If None, the random number generator is the RandomState instance used
56         by `np.random`.
57 
58     constant : int or str or array-like of shape (n_outputs,)
59         The explicit constant as predicted by the "constant" strategy. This
60         parameter is useful only for the "constant" strategy.
61 
62     Attributes
63     ----------
64     classes_ : array or list of array of shape (n_classes,)
65         Class labels for each output.
66 
67     n_classes_ : array or list of array of shape (n_classes,)
68         Number of label for each output.
69 
70     class_prior_ : array or list of array of shape (n_classes,)
71         Probability of each class for each output.
72 
73     n_outputs_ : int,
74         Number of outputs.
75 
76     sparse_output_ : bool,
77         True if the array returned from predict is to be in sparse CSC format.
78         Is automatically set to True if the input y is passed in sparse format.
79 
80     Examples
81     --------
82     >>> import numpy as np
83     >>> from sklearn.dummy import DummyClassifier
84     >>> X = np.array([-1, 1, 1, 1])
85     >>> y = np.array([0, 1, 1, 1])
86     >>> dummy_clf = DummyClassifier(strategy="most_frequent")
87     >>> dummy_clf.fit(X, y)
88     DummyClassifier(strategy='most_frequent')
89     >>> dummy_clf.predict(X)
90     array([1, 1, 1, 1])
91     >>> dummy_clf.score(X, y)
92     0.75
93     """
94 
95     def __init__(self, strategy="stratified", random_state=None,
96                  constant=None):
97         self.strategy = strategy
98         self.random_state = random_state
99         self.constant = constant
100 
101     def fit(self, X, y, sample_weight=None):
102         """Fit the random classifier.
103 
104         Parameters
105         ----------
106         X : {array-like, object with finite length or shape}
107             Training data, requires length = n_samples
108 
109         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
110             Target values.
111 
112         sample_weight : array-like of shape (n_samples,), default=None
113             Sample weights.
114 
115         Returns
116         -------
117         self : object
118         """
119         allowed_strategies = ("most_frequent", "stratified", "uniform",
120                               "constant", "prior")
121         if self.strategy not in allowed_strategies:
122             raise ValueError("Unknown strategy type: %s, expected one of %s."
123                              % (self.strategy, allowed_strategies))
124 
125         if self.strategy == "uniform" and sp.issparse(y):
126             y = y.toarray()
127             warnings.warn('A local copy of the target data has been converted '
128                           'to a numpy array. Predicting on sparse target data '
129                           'with the uniform strategy would not save memory '
130                           'and would be slower.',
131                           UserWarning)
132 
133         self.sparse_output_ = sp.issparse(y)
134 
135         if not self.sparse_output_:
136             y = np.asarray(y)
137             y = np.atleast_1d(y)
138 
139         if y.ndim == 1:
140             y = np.reshape(y, (-1, 1))
141 
142         self.n_outputs_ = y.shape[1]
143 
144         check_consistent_length(X, y, sample_weight)
145 
146         if self.strategy == "constant":
147             if self.constant is None:
148                 raise ValueError("Constant target value has to be specified "
149                                  "when the constant strategy is used.")
150             else:
151                 constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))
152                 if constant.shape[0] != self.n_outputs_:
153                     raise ValueError("Constant target value should have "
154                                      "shape (%d, 1)." % self.n_outputs_)
155 
156         (self.classes_,
157          self.n_classes_,
158          self.class_prior_) = class_distribution(y, sample_weight)
159 
160         if self.strategy == "constant":
161             for k in range(self.n_outputs_):
162                 if not any(constant[k][0] == c for c in self.classes_[k]):
163                     # Checking in case of constant strategy if the constant
164                     # provided by the user is in y.
165                     err_msg = ("The constant target value must be present in "
166                                "the training data. You provided constant={}. "
167                                "Possible values are: {}."
168                                .format(self.constant, list(self.classes_[k])))
169                     raise ValueError(err_msg)
170 
171         if self.n_outputs_ == 1:
172             self.n_classes_ = self.n_classes_[0]
173             self.classes_ = self.classes_[0]
174             self.class_prior_ = self.class_prior_[0]
175 
176         return self
177 
178     def predict(self, X):
179         """Perform classification on test vectors X.
180 
181         Parameters
182         ----------
183         X : {array-like, object with finite length or shape}
184             Training data, requires length = n_samples
185 
186         Returns
187         -------
188         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
189             Predicted target values for X.
190         """
191         check_is_fitted(self)
192 
193         # numpy random_state expects Python int and not long as size argument
194         # under Windows
195         n_samples = _num_samples(X)
196         rs = check_random_state(self.random_state)
197 
198         n_classes_ = self.n_classes_
199         classes_ = self.classes_
200         class_prior_ = self.class_prior_
201         constant = self.constant
202         if self.n_outputs_ == 1:
203             # Get same type even for self.n_outputs_ == 1
204             n_classes_ = [n_classes_]
205             classes_ = [classes_]
206             class_prior_ = [class_prior_]
207             constant = [constant]
208         # Compute probability only once
209         if self.strategy == "stratified":
210             proba = self.predict_proba(X)
211             if self.n_outputs_ == 1:
212                 proba = [proba]
213 
214         if self.sparse_output_:
215             class_prob = None
216             if self.strategy in ("most_frequent", "prior"):
217                 classes_ = [np.array([cp.argmax()]) for cp in class_prior_]
218 
219             elif self.strategy == "stratified":
220                 class_prob = class_prior_
221 
222             elif self.strategy == "uniform":
223                 raise ValueError("Sparse target prediction is not "
224                                  "supported with the uniform strategy")
225 
226             elif self.strategy == "constant":
227                 classes_ = [np.array([c]) for c in constant]
228 
229             y = _random_choice_csc(n_samples, classes_, class_prob,
230                                   self.random_state)
231         else:
232             if self.strategy in ("most_frequent", "prior"):
233                 y = np.tile([classes_[k][class_prior_[k].argmax()] for
234                              k in range(self.n_outputs_)], [n_samples, 1])
235 
236             elif self.strategy == "stratified":
237                 y = np.vstack([classes_[k][proba[k].argmax(axis=1)] for
238                                k in range(self.n_outputs_)]).T
239 
240             elif self.strategy == "uniform":
241                 ret = [classes_[k][rs.randint(n_classes_[k], size=n_samples)]
242                        for k in range(self.n_outputs_)]
243                 y = np.vstack(ret).T
244 
245             elif self.strategy == "constant":
246                 y = np.tile(self.constant, (n_samples, 1))
247 
248             if self.n_outputs_ == 1:
249                 y = np.ravel(y)
250 
251         return y
252 
253     def predict_proba(self, X):
254         """
255         Return probability estimates for the test vectors X.
256 
257         Parameters
258         ----------
259         X : {array-like, object with finite length or shape}
260             Training data, requires length = n_samples
261 
262         Returns
263         -------
264         P : array-like or list of array-lke of shape (n_samples, n_classes)
265             Returns the probability of the sample for each class in
266             the model, where classes are ordered arithmetically, for each
267             output.
268         """
269         check_is_fitted(self)
270 
271         # numpy random_state expects Python int and not long as size argument
272         # under Windows
273         n_samples = _num_samples(X)
274         rs = check_random_state(self.random_state)
275 
276         n_classes_ = self.n_classes_
277         classes_ = self.classes_
278         class_prior_ = self.class_prior_
279         constant = self.constant
280         if self.n_outputs_ == 1:
281             # Get same type even for self.n_outputs_ == 1
282             n_classes_ = [n_classes_]
283             classes_ = [classes_]
284             class_prior_ = [class_prior_]
285             constant = [constant]
286 
287         P = []
288         for k in range(self.n_outputs_):
289             if self.strategy == "most_frequent":
290                 ind = class_prior_[k].argmax()
291                 out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)
292                 out[:, ind] = 1.0
293             elif self.strategy == "prior":
294                 out = np.ones((n_samples, 1)) * class_prior_[k]
295 
296             elif self.strategy == "stratified":
297                 out = rs.multinomial(1, class_prior_[k], size=n_samples)
298                 out = out.astype(np.float64)
299 
300             elif self.strategy == "uniform":
301                 out = np.ones((n_samples, n_classes_[k]), dtype=np.float64)
302                 out /= n_classes_[k]
303 
304             elif self.strategy == "constant":
305                 ind = np.where(classes_[k] == constant[k])
306                 out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)
307                 out[:, ind] = 1.0
308 
309             P.append(out)
310 
311         if self.n_outputs_ == 1:
312             P = P[0]
313 
314         return P
315 
316     def predict_log_proba(self, X):
317         """
318         Return log probability estimates for the test vectors X.
319 
320         Parameters
321         ----------
322         X : {array-like, object with finite length or shape}
323             Training data, requires length = n_samples
324 
325         Returns
326         -------
327         P : array-like or list of array-like of shape (n_samples, n_classes)
328             Returns the log probability of the sample for each class in
329             the model, where classes are ordered arithmetically for each
330             output.
331         """
332         proba = self.predict_proba(X)
333         if self.n_outputs_ == 1:
334             return np.log(proba)
335         else:
336             return [np.log(p) for p in proba]
337 
338     def _more_tags(self):
339         return {'poor_score': True, 'no_validation': True}
340 
341     def score(self, X, y, sample_weight=None):
342         """Returns the mean accuracy on the given test data and labels.
343 
344         In multi-label classification, this is the subset accuracy
345         which is a harsh metric since you require for each sample that
346         each label set be correctly predicted.
347 
348         Parameters
349         ----------
350         X : {array-like, None}
351             Test samples with shape = (n_samples, n_features) or
352             None. Passing None as test samples gives the same result
353             as passing real test samples, since DummyClassifier
354             operates independently of the sampled observations.
355 
356         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
357             True labels for X.
358 
359         sample_weight : array-like of shape (n_samples,), default=None
360             Sample weights.
361 
362         Returns
363         -------
364         score : float
365             Mean accuracy of self.predict(X) wrt. y.
366 
367         """
368         if X is None:
369             X = np.zeros(shape=(len(y), 1))
370         return super().score(X, y, sample_weight)
371 
372     @deprecated(
373         "The outputs_2d_ attribute is deprecated in version 0.22 "
374         "and will be removed in version 0.24. It is equivalent to "
375         "n_outputs_ > 1."
376     )
377     @property
378     def outputs_2d_(self):
379         return self.n_outputs_ != 1

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 05:57:07.430 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 05:57:07.430 | INFO     | Search-only: skipping patch selection
2025-10-18 05:57:07.521 | INFO     | Task scikit-learn__scikit-learn-15495 completed successfully.
2025-10-18 05:57:07.522 | INFO     | No patch generated. You can try running ACR again.
