2025-10-18 04:21:12.951 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 04:21:14.489 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=849, output_tokens=13, cost=0.008880
2025-10-18 04:21:21.765 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1029, output_tokens=320, cost=0.019890
2025-10-18 04:21:23.251 | INFO     | registered non reproducing test 0
2025-10-18 04:21:34.388 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1619, output_tokens=464, cost=0.030110
2025-10-18 04:21:35.691 | INFO     | registered non reproducing test 1
2025-10-18 04:21:47.099 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1763, output_tokens=552, cost=0.034190
2025-10-18 04:21:48.411 | INFO     | registered non reproducing test 2
2025-10-18 04:21:48.411 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-18 04:21:48.417 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 04:21:53.104 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1155, output_tokens=207, cost=0.017760
2025-10-18 04:21:53.108 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 04:21:56.174 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=561, output_tokens=20, cost=0.006210
2025-10-18 04:21:56.175 | DEBUG    | Extracted a valid json.
2025-10-18 04:21:56.175 | DEBUG    | Agent proxy return the following json: {
    "API_calls": ["search_method(\"stack\")"],
    "bug_locations": []
}
2025-10-18 04:21:56.188 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 04:21:56.189 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 04:22:07.545 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1582, output_tokens=545, cost=0.032170
2025-10-18 04:22:07.556 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 04:22:16.714 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2343, output_tokens=415, cost=0.035880
2025-10-18 04:22:16.723 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 04:22:23.899 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=769, output_tokens=242, cost=0.014950
2025-10-18 04:22:23.900 | DEBUG    | Extracted a valid json.
2025-10-18 04:22:23.900 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "xarray/core/dataset.py",
            "class": "Dataset",
            "method": "stack",
            "intended_behavior": "The `stack` method should ensure that the dtype of coordinates remains unchanged after the operation completes. It should explicitly check the dtype of coordinates before the stacking operation and preserve those dtypes throughout the process, possibly by implementing a check or conversion routine that verifies the dtype post-operation against the original dtypes, reverting to the original dtype if a change is detected."
        },
        {
            "file": "xarray/core/dataarray.py",
            "class": "DataArray",
            "method": "stack",
            "intended_behavior": "Similarly to `Dataset`, the `stack` method here should make sure that the data type (`dtype`) for any coordinates involved in a stack operation is maintained as it was before the operation. If necessary, additional logic should be included to explicitly force the stacked object to match the original dtype if the operation inherently changes it, potentially using a mechanism to specify or cast dtypes during the stacking process."
        }
    ]
}
2025-10-18 04:22:23.946 | DEBUG    | Bug location extracted successfully: [<file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>stack</method>
<code>
4554     def stack(
4555         self: T_Dataset,
4556         dimensions: Mapping[Any, Sequence[Hashable | ellipsis]] | None = None,
4557         create_index: bool | None = True,
4558         index_cls: type[Index] = PandasMultiIndex,
4559         **dimensions_kwargs: Sequence[Hashable | ellipsis],
4560     ) -> T_Dataset:
4561         """
4562         Stack any number of existing dimensions into a single new dimension.
4563 
4564         New dimensions will be added at the end, and by default the corresponding
4565         coordinate variables will be combined into a MultiIndex.
4566 
4567         Parameters
4568         ----------
4569         dimensions : mapping of hashable to sequence of hashable
4570             Mapping of the form `new_name=(dim1, dim2, ...)`. Names of new
4571             dimensions, and the existing dimensions that they replace. An
4572             ellipsis (`...`) will be replaced by all unlisted dimensions.
4573             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
4574             all dimensions.
4575         create_index : bool or None, default: True
4576 
4577             - True: create a multi-index for each of the stacked dimensions.
4578             - False: don't create any index.
4579             - None. create a multi-index only if exactly one single (1-d) coordinate
4580               index is found for every dimension to stack.
4581 
4582         index_cls: Index-class, default: PandasMultiIndex
4583             Can be used to pass a custom multi-index type (must be an Xarray index that
4584             implements `.stack()`). By default, a pandas multi-index wrapper is used.
4585         **dimensions_kwargs
4586             The keyword arguments form of ``dimensions``.
4587             One of dimensions or dimensions_kwargs must be provided.
4588 
4589         Returns
4590         -------
4591         stacked : Dataset
4592             Dataset with stacked data.
4593 
4594         See Also
4595         --------
4596         Dataset.unstack
4597         """
4598         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
4599         result = self
4600         for new_dim, dims in dimensions.items():
4601             result = result._stack_once(dims, new_dim, index_cls, create_index)
4602         return result

</code><intended_behavior>The `stack` method should ensure that the dtype of coordinates remains unchanged after the operation completes. It should explicitly check the dtype of coordinates before the stacking operation and preserve those dtypes throughout the process, possibly by implementing a check or conversion routine that verifies the dtype post-operation against the original dtypes, reverting to the original dtype if a change is detected.</intended_behavior>, <file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>None</method>
<code>
440 class Dataset(
441     DataWithCoords,
442     DatasetAggregations,
443     DatasetArithmetic,
444     Mapping[Hashable, "DataArray"],
445 ):
446     """A multi-dimensional, in memory, array database.
447 
448     A dataset resembles an in-memory representation of a NetCDF file,
449     and consists of variables, coordinates and attributes which
450     together form a self describing dataset.
451 
452     Dataset implements the mapping interface with keys given by variable
453     names and values given by DataArray objects for each variable name.
454 
455     One dimensional variables with name equal to their dimension are
456     index coordinates used for label based indexing.
457 
458     To load data from a file or file-like object, use the `open_dataset`
459     function.
460 
461     Parameters
462     ----------
463     data_vars : dict-like, optional
464         A mapping from variable names to :py:class:`~xarray.DataArray`
465         objects, :py:class:`~xarray.Variable` objects or to tuples of
466         the form ``(dims, data[, attrs])`` which can be used as
467         arguments to create a new ``Variable``. Each dimension must
468         have the same length in all variables in which it appears.
469 
470         The following notations are accepted:
471 
472         - mapping {var name: DataArray}
473         - mapping {var name: Variable}
474         - mapping {var name: (dimension name, array-like)}
475         - mapping {var name: (tuple of dimension names, array-like)}
476         - mapping {dimension name: array-like}
477           (it will be automatically moved to coords, see below)
478 
479         Each dimension must have the same length in all variables in
480         which it appears.
481     coords : dict-like, optional
482         Another mapping in similar form as the `data_vars` argument,
483         except the each item is saved on the dataset as a "coordinate".
484         These variables have an associated meaning: they describe
485         constant/fixed/independent quantities, unlike the
486         varying/measured/dependent quantities that belong in
487         `variables`. Coordinates values may be given by 1-dimensional
488         arrays or scalars, in which case `dims` do not need to be
489         supplied: 1D arrays will be assumed to give index values along
490         the dimension with the same name.
491 
492         The following notations are accepted:
493 
494         - mapping {coord name: DataArray}
495         - mapping {coord name: Variable}
496         - mapping {coord name: (dimension name, array-like)}
497         - mapping {coord name: (tuple of dimension names, array-like)}
498         - mapping {dimension name: array-like}
499           (the dimension name is implicitly set to be the same as the
500           coord name)
501 
502         The last notation implies that the coord name is the same as
503         the dimension name.
504 
505     attrs : dict-like, optional
506         Global attributes to save on this dataset.
507 
508     Examples
509     --------
510     Create data:
511 
512     >>> np.random.seed(0)
513     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
514     >>> precipitation = 10 * np.random.rand(2, 2, 3)
515     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
516     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
517     >>> time = pd.date_range("2014-09-06", periods=3)
518     >>> reference_time = pd.Timestamp("2014-09-05")
519 
520     Initialize a dataset with multiple dimensions:
521 
522     >>> ds = xr.Dataset(
523     ...     data_vars=dict(
524     ...         temperature=(["x", "y", "time"], temperature),
525     ...         precipitation=(["x", "y", "time"], precipitation),
526     ...     ),
527     ...     coords=dict(
528     ...         lon=(["x", "y"], lon),
529     ...         lat=(["x", "y"], lat),
530     ...         time=time,
531     ...         reference_time=reference_time,
532     ...     ),
533     ...     attrs=dict(description="Weather related data."),
534     ... )
535     >>> ds
536     <xarray.Dataset>
537     Dimensions:         (x: 2, y: 2, time: 3)
538     Coordinates:
539         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
540         lat             (x, y) float64 42.25 42.21 42.63 42.59
541       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
542         reference_time  datetime64[ns] 2014-09-05
543     Dimensions without coordinates: x, y
544     Data variables:
545         temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
546         precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
547     Attributes:
548         description:  Weather related data.
549 
550     Find out where the coldest temperature was and what values the
551     other variables had:
552 
553     >>> ds.isel(ds.temperature.argmin(...))
554     <xarray.Dataset>
555     Dimensions:         ()
556     Coordinates:
557         lon             float64 -99.32
558         lat             float64 42.21
559         time            datetime64[ns] 2014-09-08
560         reference_time  datetime64[ns] 2014-09-05
561     Data variables:
562         temperature     float64 7.182
563         precipitation   float64 8.326
564     Attributes:
565         description:  Weather related data.
566     """
567 
568     _attrs: dict[Hashable, Any] | None
569     _cache: dict[str, Any]
570     _coord_names: set[Hashable]
571     _dims: dict[Hashable, int]
572     _encoding: dict[Hashable, Any] | None
573     _close: Callable[[], None] | None
574     _indexes: dict[Hashable, Index]
575     _variables: dict[Hashable, Variable]
576 
577     __slots__ = (
578         "_attrs",
579         "_cache",
580         "_coord_names",
581         "_dims",
582         "_encoding",
583         "_close",
584         "_indexes",
585         "_variables",
586         "__weakref__",
587     )
588 
589     def __init__(
590         self,
591         # could make a VariableArgs to use more generally, and refine these
592         # categories
593         data_vars: Mapping[Any, Any] | None = None,
594         coords: Mapping[Any, Any] | None = None,
595         attrs: Mapping[Any, Any] | None = None,
596     ) -> None:
597         # TODO(shoyer): expose indexes as a public argument in __init__
598 
599         if data_vars is None:
600             data_vars = {}
601         if coords is None:
602             coords = {}
603 
604         both_data_and_coords = set(data_vars) & set(coords)
605         if both_data_and_coords:
606             raise ValueError(
607                 f"variables {both_data_and_coords!r} are found in both data_vars and coords"
608             )
609 
610         if isinstance(coords, Dataset):
611             coords = coords.variables
612 
613         variables, coord_names, dims, indexes, _ = merge_data_and_coords(
614             data_vars, coords, compat="broadcast_equals"
615         )
616 
617         self._attrs = dict(attrs) if attrs is not None else None
618         self._close = None
619         self._encoding = None
620         self._variables = variables
621         self._coord_names = coord_names
622         self._dims = dims
623         self._indexes = indexes
624 
625     @classmethod
626     def load_store(cls: type[T_Dataset], store, decoder=None) -> T_Dataset:
627         """Create a new dataset from the contents of a backends.*DataStore
628         object
629         """
630         variables, attributes = store.load()
631         if decoder:
632             variables, attributes = decoder(variables, attributes)
633         obj = cls(variables, attrs=attributes)
634         obj.set_close(store.close)
635         return obj
636 
637     @property
638     def variables(self) -> Frozen[Hashable, Variable]:
639         """Low level interface to Dataset contents as dict of Variable objects.
640 
641         This ordered dictionary is frozen to prevent mutation that could
642         violate Dataset invariants. It contains all variable objects
643         constituting the Dataset, including both data variables and
644         coordinates.
645         """
646         return Frozen(self._variables)
647 
648     @property
649     def attrs(self) -> dict[Any, Any]:
650         """Dictionary of global attributes on this dataset"""
651         if self._attrs is None:
652             self._attrs = {}
653         return self._attrs
654 
655     @attrs.setter
656     def attrs(self, value: Mapping[Any, Any]) -> None:
657         self._attrs = dict(value)
658 
659     @property
660     def encoding(self) -> dict[Any, Any]:
661         """Dictionary of global encoding attributes on this dataset"""
662         if self._encoding is None:
663             self._encoding = {}
664         return self._encoding
665 
666     @encoding.setter
667     def encoding(self, value: Mapping[Any, Any]) -> None:
668         self._encoding = dict(value)
669 
670     @property
671     def dims(self) -> Frozen[Hashable, int]:
672         """Mapping from dimension names to lengths.
673 
674         Cannot be modified directly, but is updated when adding new variables.
675 
676         Note that type of this object differs from `DataArray.dims`.
677         See `Dataset.sizes` and `DataArray.sizes` for consistently named
678         properties.
679 
680         See Also
681         --------
682         Dataset.sizes
683         DataArray.dims
684         """
685         return Frozen(self._dims)
686 
687     @property
688     def sizes(self) -> Frozen[Hashable, int]:
689         """Mapping from dimension names to lengths.
690 
691         Cannot be modified directly, but is updated when adding new variables.
692 
693         This is an alias for `Dataset.dims` provided for the benefit of
694         consistency with `DataArray.sizes`.
695 
696         See Also
697         --------
698         DataArray.sizes
699         """
700         return self.dims
701 
702     @property
703     def dtypes(self) -> Frozen[Hashable, np.dtype]:
704         """Mapping from data variable names to dtypes.
705 
706         Cannot be modified directly, but is updated when adding new variables.
707 
708         See Also
709         --------
710         DataArray.dtype
711         """
712         return Frozen(
713             {
714                 n: v.dtype
715                 for n, v in self._variables.items()
716                 if n not in self._coord_names
717             }
718         )
719 
720     def load(self: T_Dataset, **kwargs) -> T_Dataset:
721         """Manually trigger loading and/or computation of this dataset's data
722         from disk or a remote source into memory and return this dataset.
723         Unlike compute, the original dataset is modified and returned.
724 
725         Normally, it should not be necessary to call this method in user code,
726         because all xarray functions should either work on deferred data or
727         load data automatically. However, this method can be necessary when
728         working with many file objects on disk.
729 
730         Parameters
731         ----------
732         **kwargs : dict
733             Additional keyword arguments passed on to ``dask.compute``.
734 
735         See Also
736         --------
737         dask.compute
738         """
739         # access .data to coerce everything to numpy or dask arrays
740         lazy_data = {
741             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
742         }
743         if lazy_data:
744             import dask.array as da
745 
746             # evaluate all the dask arrays simultaneously
747             evaluated_data = da.compute(*lazy_data.values(), **kwargs)
748 
749             for k, data in zip(lazy_data, evaluated_data):
750                 self.variables[k].data = data
751 
752         # load everything else sequentially
753         for k, v in self.variables.items():
754             if k not in lazy_data:
755                 v.load()
756 
757         return self
758 
759     def __dask_tokenize__(self):
760         from dask.base import normalize_token
761 
762         return normalize_token(
763             (type(self), self._variables, self._coord_names, self._attrs)
764         )
765 
766     def __dask_graph__(self):
767         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}
768         graphs = {k: v for k, v in graphs.items() if v is not None}
769         if not graphs:
770             return None
771         else:
772             try:
773                 from dask.highlevelgraph import HighLevelGraph
774 
775                 return HighLevelGraph.merge(*graphs.values())
776             except ImportError:
777                 from dask import sharedict
778 
779                 return sharedict.merge(*graphs.values())
780 
781     def __dask_keys__(self):
782         import dask
783 
784         return [
785             v.__dask_keys__()
786             for v in self.variables.values()
787             if dask.is_dask_collection(v)
788         ]
789 
790     def __dask_layers__(self):
791         import dask
792 
793         return sum(
794             (
795                 v.__dask_layers__()
796                 for v in self.variables.values()
797                 if dask.is_dask_collection(v)
798             ),
799             (),
800         )
801 
802     @property
803     def __dask_optimize__(self):
804         import dask.array as da
805 
806         return da.Array.__dask_optimize__
807 
808     @property
809     def __dask_scheduler__(self):
810         import dask.array as da
811 
812         return da.Array.__dask_scheduler__
813 
814     def __dask_postcompute__(self):
815         return self._dask_postcompute, ()
816 
817     def __dask_postpersist__(self):
818         return self._dask_postpersist, ()
819 
820     def _dask_postcompute(self: T_Dataset, results: Iterable[Variable]) -> T_Dataset:
821         import dask
822 
823         variables = {}
824         results_iter = iter(results)
825 
826         for k, v in self._variables.items():
827             if dask.is_dask_collection(v):
828                 rebuild, args = v.__dask_postcompute__()
829                 v = rebuild(next(results_iter), *args)
830             variables[k] = v
831 
832         return type(self)._construct_direct(
833             variables,
834             self._coord_names,
835             self._dims,
836             self._attrs,
837             self._indexes,
838             self._encoding,
839             self._close,
840         )
841 
842     def _dask_postpersist(
843         self: T_Dataset, dsk: Mapping, *, rename: Mapping[str, str] | None = None
844     ) -> T_Dataset:
845         from dask import is_dask_collection
846         from dask.highlevelgraph import HighLevelGraph
847         from dask.optimization import cull
848 
849         variables = {}
850 
851         for k, v in self._variables.items():
852             if not is_dask_collection(v):
853                 variables[k] = v
854                 continue
855 
856             if isinstance(dsk, HighLevelGraph):
857                 # dask >= 2021.3
858                 # __dask_postpersist__() was called by dask.highlevelgraph.
859                 # Don't use dsk.cull(), as we need to prevent partial layers:
860                 # https://github.com/dask/dask/issues/7137
861                 layers = v.__dask_layers__()
862                 if rename:
863                     layers = [rename.get(k, k) for k in layers]
864                 dsk2 = dsk.cull_layers(layers)
865             elif rename:  # pragma: nocover
866                 # At the moment of writing, this is only for forward compatibility.
867                 # replace_name_in_key requires dask >= 2021.3.
868                 from dask.base import flatten, replace_name_in_key
869 
870                 keys = [
871                     replace_name_in_key(k, rename) for k in flatten(v.__dask_keys__())
872                 ]
873                 dsk2, _ = cull(dsk, keys)
874             else:
875                 # __dask_postpersist__() was called by dask.optimize or dask.persist
876                 dsk2, _ = cull(dsk, v.__dask_keys__())
877 
878             rebuild, args = v.__dask_postpersist__()
879             # rename was added in dask 2021.3
880             kwargs = {"rename": rename} if rename else {}
881             variables[k] = rebuild(dsk2, *args, **kwargs)
882 
883         return type(self)._construct_direct(
884             variables,
885             self._coord_names,
886             self._dims,
887             self._attrs,
888             self._indexes,
889             self._encoding,
890             self._close,
891         )
892 
893     def compute(self: T_Dataset, **kwargs) -> T_Dataset:
894         """Manually trigger loading and/or computation of this dataset's data
895         from disk or a remote source into memory and return a new dataset.
896         Unlike load, the original dataset is left unaltered.
897 
898         Normally, it should not be necessary to call this method in user code,
899         because all xarray functions should either work on deferred data or
900         load data automatically. However, this method can be necessary when
901         working with many file objects on disk.
902 
903         Parameters
904         ----------
905         **kwargs : dict
906             Additional keyword arguments passed on to ``dask.compute``.
907 
908         See Also
909         --------
910         dask.compute
911         """
912         new = self.copy(deep=False)
913         return new.load(**kwargs)
914 
915     def _persist_inplace(self: T_Dataset, **kwargs) -> T_Dataset:
916         """Persist all Dask arrays in memory"""
917         # access .data to coerce everything to numpy or dask arrays
918         lazy_data = {
919             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
920         }
921         if lazy_data:
922             import dask
923 
924             # evaluate all the dask arrays simultaneously
925             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)
926 
927             for k, data in zip(lazy_data, evaluated_data):
928                 self.variables[k].data = data
929 
930         return self
931 
932     def persist(self: T_Dataset, **kwargs) -> T_Dataset:
933         """Trigger computation, keeping data as dask arrays
934 
935         This operation can be used to trigger computation on underlying dask
936         arrays, similar to ``.compute()`` or ``.load()``.  However this
937         operation keeps the data as dask arrays. This is particularly useful
938         when using the dask.distributed scheduler and you want to load a large
939         amount of data into distributed memory.
940 
941         Parameters
942         ----------
943         **kwargs : dict
944             Additional keyword arguments passed on to ``dask.persist``.
945 
946         See Also
947         --------
948         dask.persist
949         """
950         new = self.copy(deep=False)
951         return new._persist_inplace(**kwargs)
952 
953     @classmethod
954     def _construct_direct(
955         cls: type[T_Dataset],
956         variables: dict[Any, Variable],
957         coord_names: set[Hashable],
958         dims: dict[Any, int] | None = None,
959         attrs: dict | None = None,
960         indexes: dict[Any, Index] | None = None,
961         encoding: dict | None = None,
962         close: Callable[[], None] | None = None,
963     ) -> T_Dataset:
964         """Shortcut around __init__ for internal use when we want to skip
965         costly validation
966         """
967         if dims is None:
968             dims = calculate_dimensions(variables)
969         if indexes is None:
970             indexes = {}
971         obj = object.__new__(cls)
972         obj._variables = variables
973         obj._coord_names = coord_names
974         obj._dims = dims
975         obj._indexes = indexes
976         obj._attrs = attrs
977         obj._close = close
978         obj._encoding = encoding
979         return obj
980 
981     def _replace(
982         self: T_Dataset,
983         variables: dict[Hashable, Variable] | None = None,
984         coord_names: set[Hashable] | None = None,
985         dims: dict[Any, int] | None = None,
986         attrs: dict[Hashable, Any] | None | Default = _default,
987         indexes: dict[Hashable, Index] | None = None,
988         encoding: dict | None | Default = _default,
989         inplace: bool = False,
990     ) -> T_Dataset:
991         """Fastpath constructor for internal use.
992 
993         Returns an object with optionally with replaced attributes.
994 
995         Explicitly passed arguments are *not* copied when placed on the new
996         dataset. It is up to the caller to ensure that they have the right type
997         and are not used elsewhere.
998         """
999         if inplace:
1000             if variables is not None:
1001                 self._variables = variables
1002             if coord_names is not None:
1003                 self._coord_names = coord_names
1004             if dims is not None:
1005                 self._dims = dims
1006             if attrs is not _default:
1007                 self._attrs = attrs
1008             if indexes is not None:
1009                 self._indexes = indexes
1010             if encoding is not _default:
1011                 self._encoding = encoding
1012             obj = self
1013         else:
1014             if variables is None:
1015                 variables = self._variables.copy()
1016             if coord_names is None:
1017                 coord_names = self._coord_names.copy()
1018             if dims is None:
1019                 dims = self._dims.copy()
1020             if attrs is _default:
1021                 attrs = copy.copy(self._attrs)
1022             if indexes is None:
1023                 indexes = self._indexes.copy()
1024             if encoding is _default:
1025                 encoding = copy.copy(self._encoding)
1026             obj = self._construct_direct(
1027                 variables, coord_names, dims, attrs, indexes, encoding
1028             )
1029         return obj
1030 
1031     def _replace_with_new_dims(
1032         self: T_Dataset,
1033         variables: dict[Hashable, Variable],
1034         coord_names: set | None = None,
1035         attrs: dict[Hashable, Any] | None | Default = _default,
1036         indexes: dict[Hashable, Index] | None = None,
1037         inplace: bool = False,
1038     ) -> T_Dataset:
1039         """Replace variables with recalculated dimensions."""
1040         dims = calculate_dimensions(variables)
1041         return self._replace(
1042             variables, coord_names, dims, attrs, indexes, inplace=inplace
1043         )
1044 
1045     def _replace_vars_and_dims(
1046         self: T_Dataset,
1047         variables: dict[Hashable, Variable],
1048         coord_names: set | None = None,
1049         dims: dict[Hashable, int] | None = None,
1050         attrs: dict[Hashable, Any] | None | Default = _default,
1051         inplace: bool = False,
1052     ) -> T_Dataset:
1053         """Deprecated version of _replace_with_new_dims().
1054 
1055         Unlike _replace_with_new_dims(), this method always recalculates
1056         indexes from variables.
1057         """
1058         if dims is None:
1059             dims = calculate_dimensions(variables)
1060         return self._replace(
1061             variables, coord_names, dims, attrs, indexes=None, inplace=inplace
1062         )
1063 
1064     def _overwrite_indexes(
1065         self: T_Dataset,
1066         indexes: Mapping[Hashable, Index],
1067         variables: Mapping[Hashable, Variable] | None = None,
1068         drop_variables: list[Hashable] | None = None,
1069         drop_indexes: list[Hashable] | None = None,
1070         rename_dims: Mapping[Hashable, Hashable] | None = None,
1071     ) -> T_Dataset:
1072         """Maybe replace indexes.
1073 
1074         This function may do a lot more depending on index query
1075         results.
1076 
1077         """
1078         if not indexes:
1079             return self
1080 
1081         if variables is None:
1082             variables = {}
1083         if drop_variables is None:
1084             drop_variables = []
1085         if drop_indexes is None:
1086             drop_indexes = []
1087 
1088         new_variables = self._variables.copy()
1089         new_coord_names = self._coord_names.copy()
1090         new_indexes = dict(self._indexes)
1091 
1092         index_variables = {}
1093         no_index_variables = {}
1094         for name, var in variables.items():
1095             old_var = self._variables.get(name)
1096             if old_var is not None:
1097                 var.attrs.update(old_var.attrs)
1098                 var.encoding.update(old_var.encoding)
1099             if name in indexes:
1100                 index_variables[name] = var
1101             else:
1102                 no_index_variables[name] = var
1103 
1104         for name in indexes:
1105             new_indexes[name] = indexes[name]
1106 
1107         for name, var in index_variables.items():
1108             new_coord_names.add(name)
1109             new_variables[name] = var
1110 
1111         # append no-index variables at the end
1112         for k in no_index_variables:
1113             new_variables.pop(k)
1114         new_variables.update(no_index_variables)
1115 
1116         for name in drop_indexes:
1117             new_indexes.pop(name)
1118 
1119         for name in drop_variables:
1120             new_variables.pop(name)
1121             new_indexes.pop(name, None)
1122             new_coord_names.remove(name)
1123 
1124         replaced = self._replace(
1125             variables=new_variables, coord_names=new_coord_names, indexes=new_indexes
1126         )
1127 
1128         if rename_dims:
1129             # skip rename indexes: they should already have the right name(s)
1130             dims = replaced._rename_dims(rename_dims)
1131             new_variables, new_coord_names = replaced._rename_vars({}, rename_dims)
1132             return replaced._replace(
1133                 variables=new_variables, coord_names=new_coord_names, dims=dims
1134             )
1135         else:
1136             return replaced
1137 
1138     def copy(
1139         self: T_Dataset, deep: bool = False, data: Mapping[Any, ArrayLike] | None = None
1140     ) -> T_Dataset:
1141         """Returns a copy of this dataset.
1142 
1143         If `deep=True`, a deep copy is made of each of the component variables.
1144         Otherwise, a shallow copy of each of the component variable is made, so
1145         that the underlying memory region of the new dataset is the same as in
1146         the original dataset.
1147 
1148         Use `data` to create a new object with the same structure as
1149         original but entirely new data.
1150 
1151         Parameters
1152         ----------
1153         deep : bool, default: False
1154             Whether each component variable is loaded into memory and copied onto
1155             the new object. Default is False.
1156         data : dict-like or None, optional
1157             Data to use in the new object. Each item in `data` must have same
1158             shape as corresponding data variable in original. When `data` is
1159             used, `deep` is ignored for the data variables and only used for
1160             coords.
1161 
1162         Returns
1163         -------
1164         object : Dataset
1165             New object with dimensions, attributes, coordinates, name, encoding,
1166             and optionally data copied from original.
1167 
1168         Examples
1169         --------
1170         Shallow copy versus deep copy
1171 
1172         >>> da = xr.DataArray(np.random.randn(2, 3))
1173         >>> ds = xr.Dataset(
1174         ...     {"foo": da, "bar": ("x", [-1, 2])},
1175         ...     coords={"x": ["one", "two"]},
1176         ... )
1177         >>> ds.copy()
1178         <xarray.Dataset>
1179         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1180         Coordinates:
1181           * x        (x) <U3 'one' 'two'
1182         Dimensions without coordinates: dim_0, dim_1
1183         Data variables:
1184             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
1185             bar      (x) int64 -1 2
1186 
1187         >>> ds_0 = ds.copy(deep=False)
1188         >>> ds_0["foo"][0, 0] = 7
1189         >>> ds_0
1190         <xarray.Dataset>
1191         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1192         Coordinates:
1193           * x        (x) <U3 'one' 'two'
1194         Dimensions without coordinates: dim_0, dim_1
1195         Data variables:
1196             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1197             bar      (x) int64 -1 2
1198 
1199         >>> ds
1200         <xarray.Dataset>
1201         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1202         Coordinates:
1203           * x        (x) <U3 'one' 'two'
1204         Dimensions without coordinates: dim_0, dim_1
1205         Data variables:
1206             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1207             bar      (x) int64 -1 2
1208 
1209         Changing the data using the ``data`` argument maintains the
1210         structure of the original object, but with the new data. Original
1211         object is unaffected.
1212 
1213         >>> ds.copy(data={"foo": np.arange(6).reshape(2, 3), "bar": ["a", "b"]})
1214         <xarray.Dataset>
1215         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1216         Coordinates:
1217           * x        (x) <U3 'one' 'two'
1218         Dimensions without coordinates: dim_0, dim_1
1219         Data variables:
1220             foo      (dim_0, dim_1) int64 0 1 2 3 4 5
1221             bar      (x) <U1 'a' 'b'
1222 
1223         >>> ds
1224         <xarray.Dataset>
1225         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1226         Coordinates:
1227           * x        (x) <U3 'one' 'two'
1228         Dimensions without coordinates: dim_0, dim_1
1229         Data variables:
1230             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1231             bar      (x) int64 -1 2
1232 
1233         See Also
1234         --------
1235         pandas.DataFrame.copy
1236         """
1237         return self._copy(deep=deep, data=data)
1238 
1239     def _copy(
1240         self: T_Dataset,
1241         deep: bool = False,
1242         data: Mapping[Any, ArrayLike] | None = None,
1243         memo: dict[int, Any] | None = None,
1244     ) -> T_Dataset:
1245         if data is None:
1246             data = {}
1247         elif not utils.is_dict_like(data):
1248             raise ValueError("Data must be dict-like")
1249 
1250         if data:
1251             var_keys = set(self.data_vars.keys())
1252             data_keys = set(data.keys())
1253             keys_not_in_vars = data_keys - var_keys
1254             if keys_not_in_vars:
1255                 raise ValueError(
1256                     "Data must only contain variables in original "
1257                     "dataset. Extra variables: {}".format(keys_not_in_vars)
1258                 )
1259             keys_missing_from_data = var_keys - data_keys
1260             if keys_missing_from_data:
1261                 raise ValueError(
1262                     "Data must contain all variables in original "
1263                     "dataset. Data is missing {}".format(keys_missing_from_data)
1264                 )
1265 
1266         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1267 
1268         variables = {}
1269         for k, v in self._variables.items():
1270             if k in index_vars:
1271                 variables[k] = index_vars[k]
1272             else:
1273                 variables[k] = v._copy(deep=deep, data=data.get(k), memo=memo)
1274 
1275         attrs = copy.deepcopy(self._attrs, memo) if deep else copy.copy(self._attrs)
1276         encoding = (
1277             copy.deepcopy(self._encoding, memo) if deep else copy.copy(self._encoding)
1278         )
1279 
1280         return self._replace(variables, indexes=indexes, attrs=attrs, encoding=encoding)
1281 
1282     def __copy__(self: T_Dataset) -> T_Dataset:
1283         return self._copy(deep=False)
1284 
1285     def __deepcopy__(self: T_Dataset, memo: dict[int, Any] | None = None) -> T_Dataset:
1286         return self._copy(deep=True, memo=memo)
1287 
1288     def as_numpy(self: T_Dataset) -> T_Dataset:
1289         """
1290         Coerces wrapped data and coordinates into numpy arrays, returning a Dataset.
1291 
1292         See also
1293         --------
1294         DataArray.as_numpy
1295         DataArray.to_numpy : Returns only the data as a numpy.ndarray object.
1296         """
1297         numpy_variables = {k: v.as_numpy() for k, v in self.variables.items()}
1298         return self._replace(variables=numpy_variables)
1299 
1300     def _copy_listed(self: T_Dataset, names: Iterable[Hashable]) -> T_Dataset:
1301         """Create a new Dataset with the listed variables from this dataset and
1302         the all relevant coordinates. Skips all validation.
1303         """
1304         variables: dict[Hashable, Variable] = {}
1305         coord_names = set()
1306         indexes: dict[Hashable, Index] = {}
1307 
1308         for name in names:
1309             try:
1310                 variables[name] = self._variables[name]
1311             except KeyError:
1312                 ref_name, var_name, var = _get_virtual_variable(
1313                     self._variables, name, self.dims
1314                 )
1315                 variables[var_name] = var
1316                 if ref_name in self._coord_names or ref_name in self.dims:
1317                     coord_names.add(var_name)
1318                 if (var_name,) == var.dims:
1319                     index, index_vars = create_default_index_implicit(var, names)
1320                     indexes.update({k: index for k in index_vars})
1321                     variables.update(index_vars)
1322                     coord_names.update(index_vars)
1323 
1324         needed_dims: OrderedSet[Hashable] = OrderedSet()
1325         for v in variables.values():
1326             needed_dims.update(v.dims)
1327 
1328         dims = {k: self.dims[k] for k in needed_dims}
1329 
1330         # preserves ordering of coordinates
1331         for k in self._variables:
1332             if k not in self._coord_names:
1333                 continue
1334 
1335             if set(self.variables[k].dims) <= needed_dims:
1336                 variables[k] = self._variables[k]
1337                 coord_names.add(k)
1338 
1339         indexes.update(filter_indexes_from_coords(self._indexes, coord_names))
1340 
1341         return self._replace(variables, coord_names, dims, indexes=indexes)
1342 
1343     def _construct_dataarray(self, name: Hashable) -> DataArray:
1344         """Construct a DataArray by indexing this dataset"""
1345         from xarray.core.dataarray import DataArray
1346 
1347         try:
1348             variable = self._variables[name]
1349         except KeyError:
1350             _, name, variable = _get_virtual_variable(self._variables, name, self.dims)
1351 
1352         needed_dims = set(variable.dims)
1353 
1354         coords: dict[Hashable, Variable] = {}
1355         # preserve ordering
1356         for k in self._variables:
1357             if k in self._coord_names and set(self.variables[k].dims) <= needed_dims:
1358                 coords[k] = self.variables[k]
1359 
1360         indexes = filter_indexes_from_coords(self._indexes, set(coords))
1361 
1362         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1363 
1364     @property
1365     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1366         """Places to look-up items for attribute-style access"""
1367         yield from self._item_sources
1368         yield self.attrs
1369 
1370     @property
1371     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1372         """Places to look-up items for key-completion"""
1373         yield self.data_vars
1374         yield HybridMappingProxy(keys=self._coord_names, mapping=self.coords)
1375 
1376         # virtual coordinates
1377         yield HybridMappingProxy(keys=self.dims, mapping=self)
1378 
1379     def __contains__(self, key: object) -> bool:
1380         """The 'in' operator will return true or false depending on whether
1381         'key' is an array in the dataset or not.
1382         """
1383         return key in self._variables
1384 
1385     def __len__(self) -> int:
1386         return len(self.data_vars)
1387 
1388     def __bool__(self) -> bool:
1389         return bool(self.data_vars)
1390 
1391     def __iter__(self) -> Iterator[Hashable]:
1392         return iter(self.data_vars)
1393 
1394     def __array__(self, dtype=None):
1395         raise TypeError(
1396             "cannot directly convert an xarray.Dataset into a "
1397             "numpy array. Instead, create an xarray.DataArray "
1398             "first, either with indexing on the Dataset or by "
1399             "invoking the `to_array()` method."
1400         )
1401 
1402     @property
1403     def nbytes(self) -> int:
1404         """
1405         Total bytes consumed by the data arrays of all variables in this dataset.
1406 
1407         If the backend array for any variable does not include ``nbytes``, estimates
1408         the total bytes for that array based on the ``size`` and ``dtype``.
1409         """
1410         return sum(v.nbytes for v in self.variables.values())
1411 
1412     @property
1413     def loc(self: T_Dataset) -> _LocIndexer[T_Dataset]:
1414         """Attribute for location based indexing. Only supports __getitem__,
1415         and only when the key is a dict of the form {dim: labels}.
1416         """
1417         return _LocIndexer(self)
1418 
1419     @overload
1420     def __getitem__(self, key: Hashable) -> DataArray:
1421         ...
1422 
1423     # Mapping is Iterable
1424     @overload
1425     def __getitem__(self: T_Dataset, key: Iterable[Hashable]) -> T_Dataset:
1426         ...
1427 
1428     def __getitem__(
1429         self: T_Dataset, key: Mapping[Any, Any] | Hashable | Iterable[Hashable]
1430     ) -> T_Dataset | DataArray:
1431         """Access variables or coordinates of this dataset as a
1432         :py:class:`~xarray.DataArray` or a subset of variables or a indexed dataset.
1433 
1434         Indexing with a list of names will return a new ``Dataset`` object.
1435         """
1436         if utils.is_dict_like(key):
1437             return self.isel(**key)
1438         if utils.hashable(key):
1439             return self._construct_dataarray(key)
1440         if utils.iterable_of_hashable(key):
1441             return self._copy_listed(key)
1442         raise ValueError(f"Unsupported key-type {type(key)}")
1443 
1444     def __setitem__(
1445         self, key: Hashable | Iterable[Hashable] | Mapping, value: Any
1446     ) -> None:
1447         """Add an array to this dataset.
1448         Multiple arrays can be added at the same time, in which case each of
1449         the following operations is applied to the respective value.
1450 
1451         If key is dict-like, update all variables in the dataset
1452         one by one with the given value at the given location.
1453         If the given value is also a dataset, select corresponding variables
1454         in the given value and in the dataset to be changed.
1455 
1456         If value is a `
1457         from .dataarray import DataArray`, call its `select_vars()` method, rename it
1458         to `key` and merge the contents of the resulting dataset into this
1459         dataset.
1460 
1461         If value is a `Variable` object (or tuple of form
1462         ``(dims, data[, attrs])``), add it to this dataset as a new
1463         variable.
1464         """
1465         from xarray.core.dataarray import DataArray
1466 
1467         if utils.is_dict_like(key):
1468             # check for consistency and convert value to dataset
1469             value = self._setitem_check(key, value)
1470             # loop over dataset variables and set new values
1471             processed = []
1472             for name, var in self.items():
1473                 try:
1474                     var[key] = value[name]
1475                     processed.append(name)
1476                 except Exception as e:
1477                     if processed:
1478                         raise RuntimeError(
1479                             "An error occurred while setting values of the"
1480                             f" variable '{name}'. The following variables have"
1481                             f" been successfully updated:\n{processed}"
1482                         ) from e
1483                     else:
1484                         raise e
1485 
1486         elif utils.hashable(key):
1487             if isinstance(value, Dataset):
1488                 raise TypeError(
1489                     "Cannot assign a Dataset to a single key - only a DataArray or Variable "
1490                     "object can be stored under a single key."
1491                 )
1492             self.update({key: value})
1493 
1494         elif utils.iterable_of_hashable(key):
1495             keylist = list(key)
1496             if len(keylist) == 0:
1497                 raise ValueError("Empty list of variables to be set")
1498             if len(keylist) == 1:
1499                 self.update({keylist[0]: value})
1500             else:
1501                 if len(keylist) != len(value):
1502                     raise ValueError(
1503                         f"Different lengths of variables to be set "
1504                         f"({len(keylist)}) and data used as input for "
1505                         f"setting ({len(value)})"
1506                     )
1507                 if isinstance(value, Dataset):
1508                     self.update(dict(zip(keylist, value.data_vars.values())))
1509                 elif isinstance(value, DataArray):
1510                     raise ValueError("Cannot assign single DataArray to multiple keys")
1511                 else:
1512                     self.update(dict(zip(keylist, value)))
1513 
1514         else:
1515             raise ValueError(f"Unsupported key-type {type(key)}")
1516 
1517     def _setitem_check(self, key, value):
1518         """Consistency check for __setitem__
1519 
1520         When assigning values to a subset of a Dataset, do consistency check beforehand
1521         to avoid leaving the dataset in a partially updated state when an error occurs.
1522         """
1523         from xarray.core.alignment import align
1524         from xarray.core.dataarray import DataArray
1525 
1526         if isinstance(value, Dataset):
1527             missing_vars = [
1528                 name for name in value.data_vars if name not in self.data_vars
1529             ]
1530             if missing_vars:
1531                 raise ValueError(
1532                     f"Variables {missing_vars} in new values"
1533                     f" not available in original dataset:\n{self}"
1534                 )
1535         elif not any([isinstance(value, t) for t in [DataArray, Number, str]]):
1536             raise TypeError(
1537                 "Dataset assignment only accepts DataArrays, Datasets, and scalars."
1538             )
1539 
1540         new_value = Dataset()
1541         for name, var in self.items():
1542             # test indexing
1543             try:
1544                 var_k = var[key]
1545             except Exception as e:
1546                 raise ValueError(
1547                     f"Variable '{name}': indexer {key} not available"
1548                 ) from e
1549 
1550             if isinstance(value, Dataset):
1551                 val = value[name]
1552             else:
1553                 val = value
1554 
1555             if isinstance(val, DataArray):
1556                 # check consistency of dimensions
1557                 for dim in val.dims:
1558                     if dim not in var_k.dims:
1559                         raise KeyError(
1560                             f"Variable '{name}': dimension '{dim}' appears in new values "
1561                             f"but not in the indexed original data"
1562                         )
1563                 dims = tuple(dim for dim in var_k.dims if dim in val.dims)
1564                 if dims != val.dims:
1565                     raise ValueError(
1566                         f"Variable '{name}': dimension order differs between"
1567                         f" original and new data:\n{dims}\nvs.\n{val.dims}"
1568                     )
1569             else:
1570                 val = np.array(val)
1571 
1572             # type conversion
1573             new_value[name] = val.astype(var_k.dtype, copy=False)
1574 
1575         # check consistency of dimension sizes and dimension coordinates
1576         if isinstance(value, DataArray) or isinstance(value, Dataset):
1577             align(self[key], value, join="exact", copy=False)
1578 
1579         return new_value
1580 
1581     def __delitem__(self, key: Hashable) -> None:
1582         """Remove a variable from this dataset."""
1583         assert_no_index_corrupted(self.xindexes, {key})
1584 
1585         if key in self._indexes:
1586             del self._indexes[key]
1587         del self._variables[key]
1588         self._coord_names.discard(key)
1589         self._dims = calculate_dimensions(self._variables)
1590 
1591     # mutable objects should not be hashable
1592     # https://github.com/python/mypy/issues/4266
1593     __hash__ = None  # type: ignore[assignment]
1594 
1595     def _all_compat(self, other: Dataset, compat_str: str) -> bool:
1596         """Helper function for equals and identical"""
1597 
1598         # some stores (e.g., scipy) do not seem to preserve order, so don't
1599         # require matching order for equality
1600         def compat(x: Variable, y: Variable) -> bool:
1601             return getattr(x, compat_str)(y)
1602 
1603         return self._coord_names == other._coord_names and utils.dict_equiv(
1604             self._variables, other._variables, compat=compat
1605         )
1606 
1607     def broadcast_equals(self, other: Dataset) -> bool:
1608         """Two Datasets are broadcast equal if they are equal after
1609         broadcasting all variables against each other.
1610 
1611         For example, variables that are scalar in one dataset but non-scalar in
1612         the other dataset can still be broadcast equal if the the non-scalar
1613         variable is a constant.
1614 
1615         See Also
1616         --------
1617         Dataset.equals
1618         Dataset.identical
1619         """
1620         try:
1621             return self._all_compat(other, "broadcast_equals")
1622         except (TypeError, AttributeError):
1623             return False
1624 
1625     def equals(self, other: Dataset) -> bool:
1626         """Two Datasets are equal if they have matching variables and
1627         coordinates, all of which are equal.
1628 
1629         Datasets can still be equal (like pandas objects) if they have NaN
1630         values in the same locations.
1631 
1632         This method is necessary because `v1 == v2` for ``Dataset``
1633         does element-wise comparisons (like numpy.ndarrays).
1634 
1635         See Also
1636         --------
1637         Dataset.broadcast_equals
1638         Dataset.identical
1639         """
1640         try:
1641             return self._all_compat(other, "equals")
1642         except (TypeError, AttributeError):
1643             return False
1644 
1645     def identical(self, other: Dataset) -> bool:
1646         """Like equals, but also checks all dataset attributes and the
1647         attributes on all variables and coordinates.
1648 
1649         See Also
1650         --------
1651         Dataset.broadcast_equals
1652         Dataset.equals
1653         """
1654         try:
1655             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(
1656                 other, "identical"
1657             )
1658         except (TypeError, AttributeError):
1659             return False
1660 
1661     @property
1662     def indexes(self) -> Indexes[pd.Index]:
1663         """Mapping of pandas.Index objects used for label based indexing.
1664 
1665         Raises an error if this Dataset has indexes that cannot be coerced
1666         to pandas.Index objects.
1667 
1668         See Also
1669         --------
1670         Dataset.xindexes
1671 
1672         """
1673         return self.xindexes.to_pandas_indexes()
1674 
1675     @property
1676     def xindexes(self) -> Indexes[Index]:
1677         """Mapping of xarray Index objects used for label based indexing."""
1678         return Indexes(self._indexes, {k: self._variables[k] for k in self._indexes})
1679 
1680     @property
1681     def coords(self) -> DatasetCoordinates:
1682         """Dictionary of xarray.DataArray objects corresponding to coordinate
1683         variables
1684         """
1685         return DatasetCoordinates(self)
1686 
1687     @property
1688     def data_vars(self) -> DataVariables:
1689         """Dictionary of DataArray objects corresponding to data variables"""
1690         return DataVariables(self)
1691 
1692     def set_coords(self: T_Dataset, names: Hashable | Iterable[Hashable]) -> T_Dataset:
1693         """Given names of one or more variables, set them as coordinates
1694 
1695         Parameters
1696         ----------
1697         names : hashable or iterable of hashable
1698             Name(s) of variables in this dataset to convert into coordinates.
1699 
1700         Returns
1701         -------
1702         Dataset
1703 
1704         See Also
1705         --------
1706         Dataset.swap_dims
1707         Dataset.assign_coords
1708         """
1709         # TODO: allow inserting new coordinates with this method, like
1710         # DataFrame.set_index?
1711         # nb. check in self._variables, not self.data_vars to insure that the
1712         # operation is idempotent
1713         if isinstance(names, str) or not isinstance(names, Iterable):
1714             names = [names]
1715         else:
1716             names = list(names)
1717         self._assert_all_in_dataset(names)
1718         obj = self.copy()
1719         obj._coord_names.update(names)
1720         return obj
1721 
1722     def reset_coords(
1723         self: T_Dataset,
1724         names: Dims = None,
1725         drop: bool = False,
1726     ) -> T_Dataset:
1727         """Given names of coordinates, reset them to become variables
1728 
1729         Parameters
1730         ----------
1731         names : str, Iterable of Hashable or None, optional
1732             Name(s) of non-index coordinates in this dataset to reset into
1733             variables. By default, all non-index coordinates are reset.
1734         drop : bool, default: False
1735             If True, remove coordinates instead of converting them into
1736             variables.
1737 
1738         Returns
1739         -------
1740         Dataset
1741         """
1742         if names is None:
1743             names = self._coord_names - set(self._indexes)
1744         else:
1745             if isinstance(names, str) or not isinstance(names, Iterable):
1746                 names = [names]
1747             else:
1748                 names = list(names)
1749             self._assert_all_in_dataset(names)
1750             bad_coords = set(names) & set(self._indexes)
1751             if bad_coords:
1752                 raise ValueError(
1753                     f"cannot remove index coordinates with reset_coords: {bad_coords}"
1754                 )
1755         obj = self.copy()
1756         obj._coord_names.difference_update(names)
1757         if drop:
1758             for name in names:
1759                 del obj._variables[name]
1760         return obj
1761 
1762     def dump_to_store(self, store: AbstractDataStore, **kwargs) -> None:
1763         """Store dataset contents to a backends.*DataStore object."""
1764         from xarray.backends.api import dump_to_store
1765 
1766         # TODO: rename and/or cleanup this method to make it more consistent
1767         # with to_netcdf()
1768         dump_to_store(self, store, **kwargs)
1769 
1770     # path=None writes to bytes
1771     @overload
1772     def to_netcdf(
1773         self,
1774         path: None = None,
1775         mode: Literal["w", "a"] = "w",
1776         format: T_NetcdfTypes | None = None,
1777         group: str | None = None,
1778         engine: T_NetcdfEngine | None = None,
1779         encoding: Mapping[Any, Mapping[str, Any]] | None = None,
1780         unlimited_dims: Iterable[Hashable] | None = None,
1781         compute: bool = True,
1782         invalid_netcdf: bool = False,
1783     ) -> bytes:
1784         ...
1785 
1786     # default return None
1787     @overload
1788     def to_netcdf(
1789         self,
1790         path: str | PathLike,
1791         mode: Literal["w", "a"] = "w",
1792         format: T_NetcdfTypes | None = None,
1793         group: str | None = None,
1794         engine: T_NetcdfEngine | None = None,
1795         encoding: Mapping[Any, Mapping[str, Any]] | None = None,
1796         unlimited_dims: Iterable[Hashable] | None = None,
1797         compute: Literal[True] = True,
1798         invalid_netcdf: bool = False,
1799     ) -> None:
1800         ...
1801 
1802     # compute=False returns dask.Delayed
1803     @overload
1804     def to_netcdf(
1805         self,
1806         path: str | PathLike,
1807         mode: Literal["w", "a"] = "w",
1808         format: T_NetcdfTypes | None = None,
1809         group: str | None = None,
1810         engine: T_NetcdfEngine | None = None,
1811         encoding: Mapping[Any, Mapping[str, Any]] | None = None,
1812         unlimited_dims: Iterable[Hashable] | None = None,
1813         *,
1814         compute: Literal[False],
1815         invalid_netcdf: bool = False,
1816     ) -> Delayed:
1817         ...
1818 
1819     def to_netcdf(
1820         self,
1821         path: str | PathLike | None = None,
1822         mode: Literal["w", "a"] = "w",
1823         format: T_NetcdfTypes | None = None,
1824         group: str | None = None,
1825         engine: T_NetcdfEngine | None = None,
1826         encoding: Mapping[Any, Mapping[str, Any]] | None = None,
1827         unlimited_dims: Iterable[Hashable] | None = None,
1828         compute: bool = True,
1829         invalid_netcdf: bool = False,
1830     ) -> bytes | Delayed | None:
1831         """Write dataset contents to a netCDF file.
1832 
1833         Parameters
1834         ----------
1835         path : str, path-like or file-like, optional
1836             Path to which to save this dataset. File-like objects are only
1837             supported by the scipy engine. If no path is provided, this
1838             function returns the resulting netCDF file as bytes; in this case,
1839             we need to use scipy, which does not support netCDF version 4 (the
1840             default format becomes NETCDF3_64BIT).
1841         mode : {"w", "a"}, default: "w"
1842             Write ('w') or append ('a') mode. If mode='w', any existing file at
1843             this location will be overwritten. If mode='a', existing variables
1844             will be overwritten.
1845         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
1846                   "NETCDF3_CLASSIC"}, optional
1847             File format for the resulting netCDF file:
1848 
1849             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
1850               features.
1851             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
1852               netCDF 3 compatible API features.
1853             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
1854               which fully supports 2+ GB files, but is only compatible with
1855               clients linked against netCDF version 3.6.0 or later.
1856             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
1857               handle 2+ GB files very well.
1858 
1859             All formats are supported by the netCDF4-python library.
1860             scipy.io.netcdf only supports the last two formats.
1861 
1862             The default format is NETCDF4 if you are saving a file to disk and
1863             have the netCDF4-python library available. Otherwise, xarray falls
1864             back to using scipy to write netCDF files and defaults to the
1865             NETCDF3_64BIT format (scipy does not support netCDF4).
1866         group : str, optional
1867             Path to the netCDF4 group in the given file to open (only works for
1868             format='NETCDF4'). The group(s) will be created if necessary.
1869         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
1870             Engine to use when writing netCDF files. If not provided, the
1871             default engine is chosen based on available dependencies, with a
1872             preference for 'netcdf4' if writing to a file on disk.
1873         encoding : dict, optional
1874             Nested dictionary with variable names as keys and dictionaries of
1875             variable specific encodings as values, e.g.,
1876             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
1877             "zlib": True}, ...}``
1878 
1879             The `h5netcdf` engine supports both the NetCDF4-style compression
1880             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
1881             ones ``{"compression": "gzip", "compression_opts": 9}``.
1882             This allows using any compression plugin installed in the HDF5
1883             library, e.g. LZF.
1884 
1885         unlimited_dims : iterable of hashable, optional
1886             Dimension(s) that should be serialized as unlimited dimensions.
1887             By default, no dimensions are treated as unlimited dimensions.
1888             Note that unlimited_dims may also be set via
1889             ``dataset.encoding["unlimited_dims"]``.
1890         compute: bool, default: True
1891             If true compute immediately, otherwise return a
1892             ``dask.delayed.Delayed`` object that can be computed later.
1893         invalid_netcdf: bool, default: False
1894             Only valid along with ``engine="h5netcdf"``. If True, allow writing
1895             hdf5 files which are invalid netcdf as described in
1896             https://github.com/h5netcdf/h5netcdf.
1897 
1898         Returns
1899         -------
1900             * ``bytes`` if path is None
1901             * ``dask.delayed.Delayed`` if compute is False
1902             * None otherwise
1903 
1904         See Also
1905         --------
1906         DataArray.to_netcdf
1907         """
1908         if encoding is None:
1909             encoding = {}
1910         from xarray.backends.api import to_netcdf
1911 
1912         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
1913             self,
1914             path,
1915             mode=mode,
1916             format=format,
1917             group=group,
1918             engine=engine,
1919             encoding=encoding,
1920             unlimited_dims=unlimited_dims,
1921             compute=compute,
1922             multifile=False,
1923             invalid_netcdf=invalid_netcdf,
1924         )
1925 
1926     # compute=True (default) returns ZarrStore
1927     @overload
1928     def to_zarr(
1929         self,
1930         store: MutableMapping | str | PathLike[str] | None = None,
1931         chunk_store: MutableMapping | str | PathLike | None = None,
1932         mode: Literal["w", "w-", "a", "r+", None] = None,
1933         synchronizer=None,
1934         group: str | None = None,
1935         encoding: Mapping | None = None,
1936         compute: Literal[True] = True,
1937         consolidated: bool | None = None,
1938         append_dim: Hashable | None = None,
1939         region: Mapping[str, slice] | None = None,
1940         safe_chunks: bool = True,
1941         storage_options: dict[str, str] | None = None,
1942         zarr_version: int | None = None,
1943     ) -> ZarrStore:
1944         ...
1945 
1946     # compute=False returns dask.Delayed
1947     @overload
1948     def to_zarr(
1949         self,
1950         store: MutableMapping | str | PathLike[str] | None = None,
1951         chunk_store: MutableMapping | str | PathLike | None = None,
1952         mode: Literal["w", "w-", "a", "r+", None] = None,
1953         synchronizer=None,
1954         group: str | None = None,
1955         encoding: Mapping | None = None,
1956         *,
1957         compute: Literal[False],
1958         consolidated: bool | None = None,
1959         append_dim: Hashable | None = None,
1960         region: Mapping[str, slice] | None = None,
1961         safe_chunks: bool = True,
1962         storage_options: dict[str, str] | None = None,
1963     ) -> Delayed:
1964         ...
1965 
1966     def to_zarr(
1967         self,
1968         store: MutableMapping | str | PathLike[str] | None = None,
1969         chunk_store: MutableMapping | str | PathLike | None = None,
1970         mode: Literal["w", "w-", "a", "r+", None] = None,
1971         synchronizer=None,
1972         group: str | None = None,
1973         encoding: Mapping | None = None,
1974         compute: bool = True,
1975         consolidated: bool | None = None,
1976         append_dim: Hashable | None = None,
1977         region: Mapping[str, slice] | None = None,
1978         safe_chunks: bool = True,
1979         storage_options: dict[str, str] | None = None,
1980         zarr_version: int | None = None,
1981     ) -> ZarrStore | Delayed:
1982         """Write dataset contents to a zarr group.
1983 
1984         Zarr chunks are determined in the following way:
1985 
1986         - From the ``chunks`` attribute in each variable's ``encoding``
1987           (can be set via `Dataset.chunk`).
1988         - If the variable is a Dask array, from the dask chunks
1989         - If neither Dask chunks nor encoding chunks are present, chunks will
1990           be determined automatically by Zarr
1991         - If both Dask chunks and encoding chunks are present, encoding chunks
1992           will be used, provided that there is a many-to-one relationship between
1993           encoding chunks and dask chunks (i.e. Dask chunks are bigger than and
1994           evenly divide encoding chunks); otherwise raise a ``ValueError``.
1995           This restriction ensures that no synchronization / locks are required
1996           when writing. To disable this restriction, use ``safe_chunks=False``.
1997 
1998         Parameters
1999         ----------
2000         store : MutableMapping, str or path-like, optional
2001             Store or path to directory in local or remote file system.
2002         chunk_store : MutableMapping, str or path-like, optional
2003             Store or path to directory in local or remote file system only for Zarr
2004             array chunks. Requires zarr-python v2.4.0 or later.
2005         mode : {"w", "w-", "a", "r+", None}, optional
2006             Persistence mode: "w" means create (overwrite if exists);
2007             "w-" means create (fail if exists);
2008             "a" means override existing variables (create if does not exist);
2009             "r+" means modify existing array *values* only (raise an error if
2010             any metadata or shapes would change).
2011             The default mode is "a" if ``append_dim`` is set. Otherwise, it is
2012             "r+" if ``region`` is set and ``w-`` otherwise.
2013         synchronizer : object, optional
2014             Zarr array synchronizer.
2015         group : str, optional
2016             Group path. (a.k.a. `path` in zarr terminology.)
2017         encoding : dict, optional
2018             Nested dictionary with variable names as keys and dictionaries of
2019             variable specific encodings as values, e.g.,
2020             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,}, ...}``
2021         compute : bool, optional
2022             If True write array data immediately, otherwise return a
2023             ``dask.delayed.Delayed`` object that can be computed to write
2024             array data later. Metadata is always updated eagerly.
2025         consolidated : bool, optional
2026             If True, apply zarr's `consolidate_metadata` function to the store
2027             after writing metadata and read existing stores with consolidated
2028             metadata; if False, do not. The default (`consolidated=None`) means
2029             write consolidated metadata and attempt to read consolidated
2030             metadata for existing stores (falling back to non-consolidated).
2031 
2032             When the experimental ``zarr_version=3``, ``consolidated`` must be
2033             either be ``None`` or ``False``.
2034         append_dim : hashable, optional
2035             If set, the dimension along which the data will be appended. All
2036             other dimensions on overridden variables must remain the same size.
2037         region : dict, optional
2038             Optional mapping from dimension names to integer slices along
2039             dataset dimensions to indicate the region of existing zarr array(s)
2040             in which to write this dataset's data. For example,
2041             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate
2042             that values should be written to the region ``0:1000`` along ``x``
2043             and ``10000:11000`` along ``y``.
2044 
2045             Two restrictions apply to the use of ``region``:
2046 
2047             - If ``region`` is set, _all_ variables in a dataset must have at
2048               least one dimension in common with the region. Other variables
2049               should be written in a separate call to ``to_zarr()``.
2050             - Dimensions cannot be included in both ``region`` and
2051               ``append_dim`` at the same time. To create empty arrays to fill
2052               in with ``region``, use a separate call to ``to_zarr()`` with
2053               ``compute=False``. See "Appending to existing Zarr stores" in
2054               the reference documentation for full details.
2055         safe_chunks : bool, optional
2056             If True, only allow writes to when there is a many-to-one relationship
2057             between Zarr chunks (specified in encoding) and Dask chunks.
2058             Set False to override this restriction; however, data may become corrupted
2059             if Zarr arrays are written in parallel. This option may be useful in combination
2060             with ``compute=False`` to initialize a Zarr from an existing
2061             Dataset with arbitrary chunk structure.
2062         storage_options : dict, optional
2063             Any additional parameters for the storage backend (ignored for local
2064             paths).
2065         zarr_version : int or None, optional
2066             The desired zarr spec version to target (currently 2 or 3). The
2067             default of None will attempt to determine the zarr version from
2068             ``store`` when possible, otherwise defaulting to 2.
2069 
2070         Returns
2071         -------
2072             * ``dask.delayed.Delayed`` if compute is False
2073             * ZarrStore otherwise
2074 
2075         References
2076         ----------
2077         https://zarr.readthedocs.io/
2078 
2079         Notes
2080         -----
2081         Zarr chunking behavior:
2082             If chunks are found in the encoding argument or attribute
2083             corresponding to any DataArray, those chunks are used.
2084             If a DataArray is a dask array, it is written with those chunks.
2085             If not other chunks are found, Zarr uses its own heuristics to
2086             choose automatic chunk sizes.
2087 
2088         encoding:
2089             The encoding attribute (if exists) of the DataArray(s) will be
2090             used. Override any existing encodings by providing the ``encoding`` kwarg.
2091 
2092         See Also
2093         --------
2094         :ref:`io.zarr`
2095             The I/O user guide, with more details and examples.
2096         """
2097         from xarray.backends.api import to_zarr
2098 
2099         return to_zarr(  # type: ignore
2100             self,
2101             store=store,
2102             chunk_store=chunk_store,
2103             storage_options=storage_options,
2104             mode=mode,
2105             synchronizer=synchronizer,
2106             group=group,
2107             encoding=encoding,
2108             compute=compute,
2109             consolidated=consolidated,
2110             append_dim=append_dim,
2111             region=region,
2112             safe_chunks=safe_chunks,
2113             zarr_version=zarr_version,
2114         )
2115 
2116     def __repr__(self) -> str:
2117         return formatting.dataset_repr(self)
2118 
2119     def _repr_html_(self) -> str:
2120         if OPTIONS["display_style"] == "text":
2121             return f"<pre>{escape(repr(self))}</pre>"
2122         return formatting_html.dataset_repr(self)
2123 
2124     def info(self, buf: IO | None = None) -> None:
2125         """
2126         Concise summary of a Dataset variables and attributes.
2127 
2128         Parameters
2129         ----------
2130         buf : file-like, default: sys.stdout
2131             writable buffer
2132 
2133         See Also
2134         --------
2135         pandas.DataFrame.assign
2136         ncdump : netCDF's ncdump
2137         """
2138         if buf is None:  # pragma: no cover
2139             buf = sys.stdout
2140 
2141         lines = []
2142         lines.append("xarray.Dataset {")
2143         lines.append("dimensions:")
2144         for name, size in self.dims.items():
2145             lines.append(f"\t{name} = {size} ;")
2146         lines.append("\nvariables:")
2147         for name, da in self.variables.items():
2148             dims = ", ".join(map(str, da.dims))
2149             lines.append(f"\t{da.dtype} {name}({dims}) ;")
2150             for k, v in da.attrs.items():
2151                 lines.append(f"\t\t{name}:{k} = {v} ;")
2152         lines.append("\n// global attributes:")
2153         for k, v in self.attrs.items():
2154             lines.append(f"\t:{k} = {v} ;")
2155         lines.append("}")
2156 
2157         buf.write("\n".join(lines))
2158 
2159     @property
2160     def chunks(self) -> Mapping[Hashable, tuple[int, ...]]:
2161         """
2162         Mapping from dimension names to block lengths for this dataset's data, or None if
2163         the underlying data is not a dask array.
2164         Cannot be modified directly, but can be modified by calling .chunk().
2165 
2166         Same as Dataset.chunksizes, but maintained for backwards compatibility.
2167 
2168         See Also
2169         --------
2170         Dataset.chunk
2171         Dataset.chunksizes
2172         xarray.unify_chunks
2173         """
2174         return get_chunksizes(self.variables.values())
2175 
2176     @property
2177     def chunksizes(self) -> Mapping[Hashable, tuple[int, ...]]:
2178         """
2179         Mapping from dimension names to block lengths for this dataset's data, or None if
2180         the underlying data is not a dask array.
2181         Cannot be modified directly, but can be modified by calling .chunk().
2182 
2183         Same as Dataset.chunks.
2184 
2185         See Also
2186         --------
2187         Dataset.chunk
2188         Dataset.chunks
2189         xarray.unify_chunks
2190         """
2191         return get_chunksizes(self.variables.values())
2192 
2193     def chunk(
2194         self: T_Dataset,
2195         chunks: (
2196             int | Literal["auto"] | Mapping[Any, None | int | str | tuple[int, ...]]
2197         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
2198         name_prefix: str = "xarray-",
2199         token: str | None = None,
2200         lock: bool = False,
2201         inline_array: bool = False,
2202         **chunks_kwargs: None | int | str | tuple[int, ...],
2203     ) -> T_Dataset:
2204         """Coerce all arrays in this dataset into dask arrays with the given
2205         chunks.
2206 
2207         Non-dask arrays in this dataset will be converted to dask arrays. Dask
2208         arrays will be rechunked to the given chunk sizes.
2209 
2210         If neither chunks is not provided for one or more dimensions, chunk
2211         sizes along that dimension will not be updated; non-dask arrays will be
2212         converted into dask arrays with a single block.
2213 
2214         Parameters
2215         ----------
2216         chunks : int, tuple of int, "auto" or mapping of hashable to int, optional
2217             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, or
2218             ``{"x": 5, "y": 5}``.
2219         name_prefix : str, default: "xarray-"
2220             Prefix for the name of any new dask arrays.
2221         token : str, optional
2222             Token uniquely identifying this dataset.
2223         lock : bool, default: False
2224             Passed on to :py:func:`dask.array.from_array`, if the array is not
2225             already as dask array.
2226         inline_array: bool, default: False
2227             Passed on to :py:func:`dask.array.from_array`, if the array is not
2228             already as dask array.
2229         **chunks_kwargs : {dim: chunks, ...}, optional
2230             The keyword arguments form of ``chunks``.
2231             One of chunks or chunks_kwargs must be provided
2232 
2233         Returns
2234         -------
2235         chunked : xarray.Dataset
2236 
2237         See Also
2238         --------
2239         Dataset.chunks
2240         Dataset.chunksizes
2241         xarray.unify_chunks
2242         dask.array.from_array
2243         """
2244         if chunks is None and chunks_kwargs is None:
2245             warnings.warn(
2246                 "None value for 'chunks' is deprecated. "
2247                 "It will raise an error in the future. Use instead '{}'",
2248                 category=FutureWarning,
2249             )
2250             chunks = {}
2251 
2252         if isinstance(chunks, (Number, str, int)):
2253             chunks = dict.fromkeys(self.dims, chunks)
2254         else:
2255             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
2256 
2257         bad_dims = chunks.keys() - self.dims.keys()
2258         if bad_dims:
2259             raise ValueError(
2260                 f"some chunks keys are not dimensions on this object: {bad_dims}"
2261             )
2262 
2263         variables = {
2264             k: _maybe_chunk(k, v, chunks, token, lock, name_prefix)
2265             for k, v in self.variables.items()
2266         }
2267         return self._replace(variables)
2268 
2269     def _validate_indexers(
2270         self, indexers: Mapping[Any, Any], missing_dims: ErrorOptionsWithWarn = "raise"
2271     ) -> Iterator[tuple[Hashable, int | slice | np.ndarray | Variable]]:
2272         """Here we make sure
2273         + indexer has a valid keys
2274         + indexer is in a valid data type
2275         + string indexers are cast to the appropriate date type if the
2276           associated index is a DatetimeIndex or CFTimeIndex
2277         """
2278         from xarray.coding.cftimeindex import CFTimeIndex
2279         from xarray.core.dataarray import DataArray
2280 
2281         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2282 
2283         # all indexers should be int, slice, np.ndarrays, or Variable
2284         for k, v in indexers.items():
2285             if isinstance(v, (int, slice, Variable)):
2286                 yield k, v
2287             elif isinstance(v, DataArray):
2288                 yield k, v.variable
2289             elif isinstance(v, tuple):
2290                 yield k, as_variable(v)
2291             elif isinstance(v, Dataset):
2292                 raise TypeError("cannot use a Dataset as an indexer")
2293             elif isinstance(v, Sequence) and len(v) == 0:
2294                 yield k, np.empty((0,), dtype="int64")
2295             else:
2296                 v = np.asarray(v)
2297 
2298                 if v.dtype.kind in "US":
2299                     index = self._indexes[k].to_pandas_index()
2300                     if isinstance(index, pd.DatetimeIndex):
2301                         v = v.astype("datetime64[ns]")
2302                     elif isinstance(index, CFTimeIndex):
2303                         v = _parse_array_of_cftime_strings(v, index.date_type)
2304 
2305                 if v.ndim > 1:
2306                     raise IndexError(
2307                         "Unlabeled multi-dimensional array cannot be "
2308                         "used for indexing: {}".format(k)
2309                     )
2310                 yield k, v
2311 
2312     def _validate_interp_indexers(
2313         self, indexers: Mapping[Any, Any]
2314     ) -> Iterator[tuple[Hashable, Variable]]:
2315         """Variant of _validate_indexers to be used for interpolation"""
2316         for k, v in self._validate_indexers(indexers):
2317             if isinstance(v, Variable):
2318                 if v.ndim == 1:
2319                     yield k, v.to_index_variable()
2320                 else:
2321                     yield k, v
2322             elif isinstance(v, int):
2323                 yield k, Variable((), v, attrs=self.coords[k].attrs)
2324             elif isinstance(v, np.ndarray):
2325                 if v.ndim == 0:
2326                     yield k, Variable((), v, attrs=self.coords[k].attrs)
2327                 elif v.ndim == 1:
2328                     yield k, IndexVariable((k,), v, attrs=self.coords[k].attrs)
2329                 else:
2330                     raise AssertionError()  # Already tested by _validate_indexers
2331             else:
2332                 raise TypeError(type(v))
2333 
2334     def _get_indexers_coords_and_indexes(self, indexers):
2335         """Extract coordinates and indexes from indexers.
2336 
2337         Only coordinate with a name different from any of self.variables will
2338         be attached.
2339         """
2340         from xarray.core.dataarray import DataArray
2341 
2342         coords_list = []
2343         for k, v in indexers.items():
2344             if isinstance(v, DataArray):
2345                 if v.dtype.kind == "b":
2346                     if v.ndim != 1:  # we only support 1-d boolean array
2347                         raise ValueError(
2348                             "{:d}d-boolean array is used for indexing along "
2349                             "dimension {!r}, but only 1d boolean arrays are "
2350                             "supported.".format(v.ndim, k)
2351                         )
2352                     # Make sure in case of boolean DataArray, its
2353                     # coordinate also should be indexed.
2354                     v_coords = v[v.values.nonzero()[0]].coords
2355                 else:
2356                     v_coords = v.coords
2357                 coords_list.append(v_coords)
2358 
2359         # we don't need to call align() explicitly or check indexes for
2360         # alignment, because merge_variables already checks for exact alignment
2361         # between dimension coordinates
2362         coords, indexes = merge_coordinates_without_align(coords_list)
2363         assert_coordinate_consistent(self, coords)
2364 
2365         # silently drop the conflicted variables.
2366         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}
2367         attached_indexes = {
2368             k: v for k, v in indexes.items() if k not in self._variables
2369         }
2370         return attached_coords, attached_indexes
2371 
2372     def isel(
2373         self: T_Dataset,
2374         indexers: Mapping[Any, Any] | None = None,
2375         drop: bool = False,
2376         missing_dims: ErrorOptionsWithWarn = "raise",
2377         **indexers_kwargs: Any,
2378     ) -> T_Dataset:
2379         """Returns a new dataset with each array indexed along the specified
2380         dimension(s).
2381 
2382         This method selects values from each array using its `__getitem__`
2383         method, except this method does not require knowing the order of
2384         each array's dimensions.
2385 
2386         Parameters
2387         ----------
2388         indexers : dict, optional
2389             A dict with keys matching dimensions and values given
2390             by integers, slice objects or arrays.
2391             indexer can be a integer, slice, array-like or DataArray.
2392             If DataArrays are passed as indexers, xarray-style indexing will be
2393             carried out. See :ref:`indexing` for the details.
2394             One of indexers or indexers_kwargs must be provided.
2395         drop : bool, default: False
2396             If ``drop=True``, drop coordinates variables indexed by integers
2397             instead of making them scalar.
2398         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2399             What to do if dimensions that should be selected from are not present in the
2400             Dataset:
2401             - "raise": raise an exception
2402             - "warn": raise a warning, and ignore the missing dimensions
2403             - "ignore": ignore the missing dimensions
2404 
2405         **indexers_kwargs : {dim: indexer, ...}, optional
2406             The keyword arguments form of ``indexers``.
2407             One of indexers or indexers_kwargs must be provided.
2408 
2409         Returns
2410         -------
2411         obj : Dataset
2412             A new Dataset with the same contents as this dataset, except each
2413             array and dimension is indexed by the appropriate indexers.
2414             If indexer DataArrays have coordinates that do not conflict with
2415             this object, then these coordinates will be attached.
2416             In general, each array's data will be a view of the array's data
2417             in this dataset, unless vectorized indexing was triggered by using
2418             an array indexer, in which case the data will be a copy.
2419 
2420         See Also
2421         --------
2422         Dataset.sel
2423         DataArray.isel
2424         """
2425         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
2426         if any(is_fancy_indexer(idx) for idx in indexers.values()):
2427             return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)
2428 
2429         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
2430         # lists, or zero or one-dimensional np.ndarray's
2431         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2432 
2433         variables = {}
2434         dims: dict[Hashable, int] = {}
2435         coord_names = self._coord_names.copy()
2436 
2437         indexes, index_variables = isel_indexes(self.xindexes, indexers)
2438 
2439         for name, var in self._variables.items():
2440             # preserve variable order
2441             if name in index_variables:
2442                 var = index_variables[name]
2443             else:
2444                 var_indexers = {k: v for k, v in indexers.items() if k in var.dims}
2445                 if var_indexers:
2446                     var = var.isel(var_indexers)
2447                     if drop and var.ndim == 0 and name in coord_names:
2448                         coord_names.remove(name)
2449                         continue
2450             variables[name] = var
2451             dims.update(zip(var.dims, var.shape))
2452 
2453         return self._construct_direct(
2454             variables=variables,
2455             coord_names=coord_names,
2456             dims=dims,
2457             attrs=self._attrs,
2458             indexes=indexes,
2459             encoding=self._encoding,
2460             close=self._close,
2461         )
2462 
2463     def _isel_fancy(
2464         self: T_Dataset,
2465         indexers: Mapping[Any, Any],
2466         *,
2467         drop: bool,
2468         missing_dims: ErrorOptionsWithWarn = "raise",
2469     ) -> T_Dataset:
2470         valid_indexers = dict(self._validate_indexers(indexers, missing_dims))
2471 
2472         variables: dict[Hashable, Variable] = {}
2473         indexes, index_variables = isel_indexes(self.xindexes, valid_indexers)
2474 
2475         for name, var in self.variables.items():
2476             if name in index_variables:
2477                 new_var = index_variables[name]
2478             else:
2479                 var_indexers = {
2480                     k: v for k, v in valid_indexers.items() if k in var.dims
2481                 }
2482                 if var_indexers:
2483                     new_var = var.isel(indexers=var_indexers)
2484                     # drop scalar coordinates
2485                     # https://github.com/pydata/xarray/issues/6554
2486                     if name in self.coords and drop and new_var.ndim == 0:
2487                         continue
2488                 else:
2489                     new_var = var.copy(deep=False)
2490                 if name not in indexes:
2491                     new_var = new_var.to_base_variable()
2492             variables[name] = new_var
2493 
2494         coord_names = self._coord_names & variables.keys()
2495         selected = self._replace_with_new_dims(variables, coord_names, indexes)
2496 
2497         # Extract coordinates from indexers
2498         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)
2499         variables.update(coord_vars)
2500         indexes.update(new_indexes)
2501         coord_names = self._coord_names & variables.keys() | coord_vars.keys()
2502         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2503 
2504     def sel(
2505         self: T_Dataset,
2506         indexers: Mapping[Any, Any] | None = None,
2507         method: str | None = None,
2508         tolerance: int | float | Iterable[int | float] | None = None,
2509         drop: bool = False,
2510         **indexers_kwargs: Any,
2511     ) -> T_Dataset:
2512         """Returns a new dataset with each array indexed by tick labels
2513         along the specified dimension(s).
2514 
2515         In contrast to `Dataset.isel`, indexers for this method should use
2516         labels instead of integers.
2517 
2518         Under the hood, this method is powered by using pandas's powerful Index
2519         objects. This makes label based indexing essentially just as fast as
2520         using integer indexing.
2521 
2522         It also means this method uses pandas's (well documented) logic for
2523         indexing. This means you can use string shortcuts for datetime indexes
2524         (e.g., '2000-01' to select all values in January 2000). It also means
2525         that slices are treated as inclusive of both the start and stop values,
2526         unlike normal Python indexing.
2527 
2528         Parameters
2529         ----------
2530         indexers : dict, optional
2531             A dict with keys matching dimensions and values given
2532             by scalars, slices or arrays of tick labels. For dimensions with
2533             multi-index, the indexer may also be a dict-like object with keys
2534             matching index level names.
2535             If DataArrays are passed as indexers, xarray-style indexing will be
2536             carried out. See :ref:`indexing` for the details.
2537             One of indexers or indexers_kwargs must be provided.
2538         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2539             Method to use for inexact matches:
2540 
2541             * None (default): only exact matches
2542             * pad / ffill: propagate last valid index value forward
2543             * backfill / bfill: propagate next valid index value backward
2544             * nearest: use nearest valid index value
2545         tolerance : optional
2546             Maximum distance between original and new labels for inexact
2547             matches. The values of the index at the matching locations must
2548             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2549         drop : bool, optional
2550             If ``drop=True``, drop coordinates variables in `indexers` instead
2551             of making them scalar.
2552         **indexers_kwargs : {dim: indexer, ...}, optional
2553             The keyword arguments form of ``indexers``.
2554             One of indexers or indexers_kwargs must be provided.
2555 
2556         Returns
2557         -------
2558         obj : Dataset
2559             A new Dataset with the same contents as this dataset, except each
2560             variable and dimension is indexed by the appropriate indexers.
2561             If indexer DataArrays have coordinates that do not conflict with
2562             this object, then these coordinates will be attached.
2563             In general, each array's data will be a view of the array's data
2564             in this dataset, unless vectorized indexing was triggered by using
2565             an array indexer, in which case the data will be a copy.
2566 
2567         See Also
2568         --------
2569         Dataset.isel
2570         DataArray.sel
2571         """
2572         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "sel")
2573         query_results = map_index_queries(
2574             self, indexers=indexers, method=method, tolerance=tolerance
2575         )
2576 
2577         if drop:
2578             no_scalar_variables = {}
2579             for k, v in query_results.variables.items():
2580                 if v.dims:
2581                     no_scalar_variables[k] = v
2582                 else:
2583                     if k in self._coord_names:
2584                         query_results.drop_coords.append(k)
2585             query_results.variables = no_scalar_variables
2586 
2587         result = self.isel(indexers=query_results.dim_indexers, drop=drop)
2588         return result._overwrite_indexes(*query_results.as_tuple()[1:])
2589 
2590     def head(
2591         self: T_Dataset,
2592         indexers: Mapping[Any, int] | int | None = None,
2593         **indexers_kwargs: Any,
2594     ) -> T_Dataset:
2595         """Returns a new dataset with the first `n` values of each array
2596         for the specified dimension(s).
2597 
2598         Parameters
2599         ----------
2600         indexers : dict or int, default: 5
2601             A dict with keys matching dimensions and integer values `n`
2602             or a single integer `n` applied over all dimensions.
2603             One of indexers or indexers_kwargs must be provided.
2604         **indexers_kwargs : {dim: n, ...}, optional
2605             The keyword arguments form of ``indexers``.
2606             One of indexers or indexers_kwargs must be provided.
2607 
2608         See Also
2609         --------
2610         Dataset.tail
2611         Dataset.thin
2612         DataArray.head
2613         """
2614         if not indexers_kwargs:
2615             if indexers is None:
2616                 indexers = 5
2617             if not isinstance(indexers, int) and not is_dict_like(indexers):
2618                 raise TypeError("indexers must be either dict-like or a single integer")
2619         if isinstance(indexers, int):
2620             indexers = {dim: indexers for dim in self.dims}
2621         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "head")
2622         for k, v in indexers.items():
2623             if not isinstance(v, int):
2624                 raise TypeError(
2625                     "expected integer type indexer for "
2626                     f"dimension {k!r}, found {type(v)!r}"
2627                 )
2628             elif v < 0:
2629                 raise ValueError(
2630                     "expected positive integer as indexer "
2631                     f"for dimension {k!r}, found {v}"
2632                 )
2633         indexers_slices = {k: slice(val) for k, val in indexers.items()}
2634         return self.isel(indexers_slices)
2635 
2636     def tail(
2637         self: T_Dataset,
2638         indexers: Mapping[Any, int] | int | None = None,
2639         **indexers_kwargs: Any,
2640     ) -> T_Dataset:
2641         """Returns a new dataset with the last `n` values of each array
2642         for the specified dimension(s).
2643 
2644         Parameters
2645         ----------
2646         indexers : dict or int, default: 5
2647             A dict with keys matching dimensions and integer values `n`
2648             or a single integer `n` applied over all dimensions.
2649             One of indexers or indexers_kwargs must be provided.
2650         **indexers_kwargs : {dim: n, ...}, optional
2651             The keyword arguments form of ``indexers``.
2652             One of indexers or indexers_kwargs must be provided.
2653 
2654         See Also
2655         --------
2656         Dataset.head
2657         Dataset.thin
2658         DataArray.tail
2659         """
2660         if not indexers_kwargs:
2661             if indexers is None:
2662                 indexers = 5
2663             if not isinstance(indexers, int) and not is_dict_like(indexers):
2664                 raise TypeError("indexers must be either dict-like or a single integer")
2665         if isinstance(indexers, int):
2666             indexers = {dim: indexers for dim in self.dims}
2667         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "tail")
2668         for k, v in indexers.items():
2669             if not isinstance(v, int):
2670                 raise TypeError(
2671                     "expected integer type indexer for "
2672                     f"dimension {k!r}, found {type(v)!r}"
2673                 )
2674             elif v < 0:
2675                 raise ValueError(
2676                     "expected positive integer as indexer "
2677                     f"for dimension {k!r}, found {v}"
2678                 )
2679         indexers_slices = {
2680             k: slice(-val, None) if val != 0 else slice(val)
2681             for k, val in indexers.items()
2682         }
2683         return self.isel(indexers_slices)
2684 
2685     def thin(
2686         self: T_Dataset,
2687         indexers: Mapping[Any, int] | int | None = None,
2688         **indexers_kwargs: Any,
2689     ) -> T_Dataset:
2690         """Returns a new dataset with each array indexed along every `n`-th
2691         value for the specified dimension(s)
2692 
2693         Parameters
2694         ----------
2695         indexers : dict or int
2696             A dict with keys matching dimensions and integer values `n`
2697             or a single integer `n` applied over all dimensions.
2698             One of indexers or indexers_kwargs must be provided.
2699         **indexers_kwargs : {dim: n, ...}, optional
2700             The keyword arguments form of ``indexers``.
2701             One of indexers or indexers_kwargs must be provided.
2702 
2703         Examples
2704         --------
2705         >>> x_arr = np.arange(0, 26)
2706         >>> x_arr
2707         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
2708                17, 18, 19, 20, 21, 22, 23, 24, 25])
2709         >>> x = xr.DataArray(
2710         ...     np.reshape(x_arr, (2, 13)),
2711         ...     dims=("x", "y"),
2712         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
2713         ... )
2714         >>> x_ds = xr.Dataset({"foo": x})
2715         >>> x_ds
2716         <xarray.Dataset>
2717         Dimensions:  (x: 2, y: 13)
2718         Coordinates:
2719           * x        (x) int64 0 1
2720           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
2721         Data variables:
2722             foo      (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24 25
2723 
2724         >>> x_ds.thin(3)
2725         <xarray.Dataset>
2726         Dimensions:  (x: 1, y: 5)
2727         Coordinates:
2728           * x        (x) int64 0
2729           * y        (y) int64 0 3 6 9 12
2730         Data variables:
2731             foo      (x, y) int64 0 3 6 9 12
2732         >>> x.thin({"x": 2, "y": 5})
2733         <xarray.DataArray (x: 1, y: 3)>
2734         array([[ 0,  5, 10]])
2735         Coordinates:
2736           * x        (x) int64 0
2737           * y        (y) int64 0 5 10
2738 
2739         See Also
2740         --------
2741         Dataset.head
2742         Dataset.tail
2743         DataArray.thin
2744         """
2745         if (
2746             not indexers_kwargs
2747             and not isinstance(indexers, int)
2748             and not is_dict_like(indexers)
2749         ):
2750             raise TypeError("indexers must be either dict-like or a single integer")
2751         if isinstance(indexers, int):
2752             indexers = {dim: indexers for dim in self.dims}
2753         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "thin")
2754         for k, v in indexers.items():
2755             if not isinstance(v, int):
2756                 raise TypeError(
2757                     "expected integer type indexer for "
2758                     f"dimension {k!r}, found {type(v)!r}"
2759                 )
2760             elif v < 0:
2761                 raise ValueError(
2762                     "expected positive integer as indexer "
2763                     f"for dimension {k!r}, found {v}"
2764                 )
2765             elif v == 0:
2766                 raise ValueError("step cannot be zero")
2767         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}
2768         return self.isel(indexers_slices)
2769 
2770     def broadcast_like(
2771         self: T_Dataset,
2772         other: Dataset | DataArray,
2773         exclude: Iterable[Hashable] | None = None,
2774     ) -> T_Dataset:
2775         """Broadcast this DataArray against another Dataset or DataArray.
2776         This is equivalent to xr.broadcast(other, self)[1]
2777 
2778         Parameters
2779         ----------
2780         other : Dataset or DataArray
2781             Object against which to broadcast this array.
2782         exclude : iterable of hashable, optional
2783             Dimensions that must not be broadcasted
2784 
2785         """
2786         if exclude is None:
2787             exclude = set()
2788         else:
2789             exclude = set(exclude)
2790         args = align(other, self, join="outer", copy=False, exclude=exclude)
2791 
2792         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
2793 
2794         return _broadcast_helper(
2795             cast("T_Dataset", args[1]), exclude, dims_map, common_coords
2796         )
2797 
2798     def _reindex_callback(
2799         self,
2800         aligner: alignment.Aligner,
2801         dim_pos_indexers: dict[Hashable, Any],
2802         variables: dict[Hashable, Variable],
2803         indexes: dict[Hashable, Index],
2804         fill_value: Any,
2805         exclude_dims: frozenset[Hashable],
2806         exclude_vars: frozenset[Hashable],
2807     ) -> Dataset:
2808         """Callback called from ``Aligner`` to create a new reindexed Dataset."""
2809 
2810         new_variables = variables.copy()
2811         new_indexes = indexes.copy()
2812 
2813         # re-assign variable metadata
2814         for name, new_var in new_variables.items():
2815             var = self._variables.get(name)
2816             if var is not None:
2817                 new_var.attrs = var.attrs
2818                 new_var.encoding = var.encoding
2819 
2820         # pass through indexes from excluded dimensions
2821         # no extra check needed for multi-coordinate indexes, potential conflicts
2822         # should already have been detected when aligning the indexes
2823         for name, idx in self._indexes.items():
2824             var = self._variables[name]
2825             if set(var.dims) <= exclude_dims:
2826                 new_indexes[name] = idx
2827                 new_variables[name] = var
2828 
2829         if not dim_pos_indexers:
2830             # fast path for no reindexing necessary
2831             if set(new_indexes) - set(self._indexes):
2832                 # this only adds new indexes and their coordinate variables
2833                 reindexed = self._overwrite_indexes(new_indexes, new_variables)
2834             else:
2835                 reindexed = self.copy(deep=aligner.copy)
2836         else:
2837             to_reindex = {
2838                 k: v
2839                 for k, v in self.variables.items()
2840                 if k not in variables and k not in exclude_vars
2841             }
2842             reindexed_vars = alignment.reindex_variables(
2843                 to_reindex,
2844                 dim_pos_indexers,
2845                 copy=aligner.copy,
2846                 fill_value=fill_value,
2847                 sparse=aligner.sparse,
2848             )
2849             new_variables.update(reindexed_vars)
2850             new_coord_names = self._coord_names | set(new_indexes)
2851             reindexed = self._replace_with_new_dims(
2852                 new_variables, new_coord_names, indexes=new_indexes
2853             )
2854 
2855         return reindexed
2856 
2857     def reindex_like(
2858         self: T_Dataset,
2859         other: Dataset | DataArray,
2860         method: ReindexMethodOptions = None,
2861         tolerance: int | float | Iterable[int | float] | None = None,
2862         copy: bool = True,
2863         fill_value: Any = xrdtypes.NA,
2864     ) -> T_Dataset:
2865         """Conform this object onto the indexes of another object, filling in
2866         missing values with ``fill_value``. The default fill value is NaN.
2867 
2868         Parameters
2869         ----------
2870         other : Dataset or DataArray
2871             Object with an 'indexes' attribute giving a mapping from dimension
2872             names to pandas.Index objects, which provides coordinates upon
2873             which to index the variables in this dataset. The indexes on this
2874             other object need not be the same as the indexes on this
2875             dataset. Any mis-matched index values will be filled in with
2876             NaN, and any mis-matched dimension names will simply be ignored.
2877         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2878             Method to use for filling index values from other not found in this
2879             dataset:
2880 
2881             - None (default): don't fill gaps
2882             - "pad" / "ffill": propagate last valid index value forward
2883             - "backfill" / "bfill": propagate next valid index value backward
2884             - "nearest": use nearest valid index value
2885 
2886         tolerance : optional
2887             Maximum distance between original and new labels for inexact
2888             matches. The values of the index at the matching locations must
2889             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2890             Tolerance may be a scalar value, which applies the same tolerance
2891             to all values, or list-like, which applies variable tolerance per
2892             element. List-like must be the same size as the index and its dtype
2893             must exactly match the index’s type.
2894         copy : bool, default: True
2895             If ``copy=True``, data in the return value is always copied. If
2896             ``copy=False`` and reindexing is unnecessary, or can be performed
2897             with only slice operations, then the output may share memory with
2898             the input. In either case, a new xarray object is always returned.
2899         fill_value : scalar or dict-like, optional
2900             Value to use for newly missing values. If a dict-like maps
2901             variable names to fill values.
2902 
2903         Returns
2904         -------
2905         reindexed : Dataset
2906             Another dataset, with this dataset's data but coordinates from the
2907             other object.
2908 
2909         See Also
2910         --------
2911         Dataset.reindex
2912         align
2913         """
2914         return alignment.reindex_like(
2915             self,
2916             other=other,
2917             method=method,
2918             tolerance=tolerance,
2919             copy=copy,
2920             fill_value=fill_value,
2921         )
2922 
2923     def reindex(
2924         self: T_Dataset,
2925         indexers: Mapping[Any, Any] | None = None,
2926         method: ReindexMethodOptions = None,
2927         tolerance: int | float | Iterable[int | float] | None = None,
2928         copy: bool = True,
2929         fill_value: Any = xrdtypes.NA,
2930         **indexers_kwargs: Any,
2931     ) -> T_Dataset:
2932         """Conform this object onto a new set of indexes, filling in
2933         missing values with ``fill_value``. The default fill value is NaN.
2934 
2935         Parameters
2936         ----------
2937         indexers : dict, optional
2938             Dictionary with keys given by dimension names and values given by
2939             arrays of coordinates tick labels. Any mis-matched coordinate
2940             values will be filled in with NaN, and any mis-matched dimension
2941             names will simply be ignored.
2942             One of indexers or indexers_kwargs must be provided.
2943         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2944             Method to use for filling index values in ``indexers`` not found in
2945             this dataset:
2946 
2947             - None (default): don't fill gaps
2948             - "pad" / "ffill": propagate last valid index value forward
2949             - "backfill" / "bfill": propagate next valid index value backward
2950             - "nearest": use nearest valid index value
2951 
2952         tolerance : optional
2953             Maximum distance between original and new labels for inexact
2954             matches. The values of the index at the matching locations must
2955             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2956             Tolerance may be a scalar value, which applies the same tolerance
2957             to all values, or list-like, which applies variable tolerance per
2958             element. List-like must be the same size as the index and its dtype
2959             must exactly match the index’s type.
2960         copy : bool, default: True
2961             If ``copy=True``, data in the return value is always copied. If
2962             ``copy=False`` and reindexing is unnecessary, or can be performed
2963             with only slice operations, then the output may share memory with
2964             the input. In either case, a new xarray object is always returned.
2965         fill_value : scalar or dict-like, optional
2966             Value to use for newly missing values. If a dict-like,
2967             maps variable names (including coordinates) to fill values.
2968         sparse : bool, default: False
2969             use sparse-array.
2970         **indexers_kwargs : {dim: indexer, ...}, optional
2971             Keyword arguments in the same form as ``indexers``.
2972             One of indexers or indexers_kwargs must be provided.
2973 
2974         Returns
2975         -------
2976         reindexed : Dataset
2977             Another dataset, with this dataset's data but replaced coordinates.
2978 
2979         See Also
2980         --------
2981         Dataset.reindex_like
2982         align
2983         pandas.Index.get_indexer
2984 
2985         Examples
2986         --------
2987         Create a dataset with some fictional data.
2988 
2989         >>> x = xr.Dataset(
2990         ...     {
2991         ...         "temperature": ("station", 20 * np.random.rand(4)),
2992         ...         "pressure": ("station", 500 * np.random.rand(4)),
2993         ...     },
2994         ...     coords={"station": ["boston", "nyc", "seattle", "denver"]},
2995         ... )
2996         >>> x
2997         <xarray.Dataset>
2998         Dimensions:      (station: 4)
2999         Coordinates:
3000           * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'
3001         Data variables:
3002             temperature  (station) float64 10.98 14.3 12.06 10.9
3003             pressure     (station) float64 211.8 322.9 218.8 445.9
3004         >>> x.indexes
3005         Indexes:
3006             station  Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')
3007 
3008         Create a new index and reindex the dataset. By default values in the new index that
3009         do not have corresponding records in the dataset are assigned `NaN`.
3010 
3011         >>> new_index = ["boston", "austin", "seattle", "lincoln"]
3012         >>> x.reindex({"station": new_index})
3013         <xarray.Dataset>
3014         Dimensions:      (station: 4)
3015         Coordinates:
3016           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
3017         Data variables:
3018             temperature  (station) float64 10.98 nan 12.06 nan
3019             pressure     (station) float64 211.8 nan 218.8 nan
3020 
3021         We can fill in the missing values by passing a value to the keyword `fill_value`.
3022 
3023         >>> x.reindex({"station": new_index}, fill_value=0)
3024         <xarray.Dataset>
3025         Dimensions:      (station: 4)
3026         Coordinates:
3027           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
3028         Data variables:
3029             temperature  (station) float64 10.98 0.0 12.06 0.0
3030             pressure     (station) float64 211.8 0.0 218.8 0.0
3031 
3032         We can also use different fill values for each variable.
3033 
3034         >>> x.reindex(
3035         ...     {"station": new_index}, fill_value={"temperature": 0, "pressure": 100}
3036         ... )
3037         <xarray.Dataset>
3038         Dimensions:      (station: 4)
3039         Coordinates:
3040           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
3041         Data variables:
3042             temperature  (station) float64 10.98 0.0 12.06 0.0
3043             pressure     (station) float64 211.8 100.0 218.8 100.0
3044 
3045         Because the index is not monotonically increasing or decreasing, we cannot use arguments
3046         to the keyword method to fill the `NaN` values.
3047 
3048         >>> x.reindex({"station": new_index}, method="nearest")
3049         Traceback (most recent call last):
3050         ...
3051             raise ValueError('index must be monotonic increasing or decreasing')
3052         ValueError: index must be monotonic increasing or decreasing
3053 
3054         To further illustrate the filling functionality in reindex, we will create a
3055         dataset with a monotonically increasing index (for example, a sequence of dates).
3056 
3057         >>> x2 = xr.Dataset(
3058         ...     {
3059         ...         "temperature": (
3060         ...             "time",
3061         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],
3062         ...         ),
3063         ...         "pressure": ("time", 500 * np.random.rand(6)),
3064         ...     },
3065         ...     coords={"time": pd.date_range("01/01/2019", periods=6, freq="D")},
3066         ... )
3067         >>> x2
3068         <xarray.Dataset>
3069         Dimensions:      (time: 6)
3070         Coordinates:
3071           * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06
3072         Data variables:
3073             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12
3074             pressure     (time) float64 481.8 191.7 395.9 264.4 284.0 462.8
3075 
3076         Suppose we decide to expand the dataset to cover a wider date range.
3077 
3078         >>> time_index2 = pd.date_range("12/29/2018", periods=10, freq="D")
3079         >>> x2.reindex({"time": time_index2})
3080         <xarray.Dataset>
3081         Dimensions:      (time: 10)
3082         Coordinates:
3083           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3084         Data variables:
3085             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan
3086             pressure     (time) float64 nan nan nan 481.8 ... 264.4 284.0 462.8 nan
3087 
3088         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)
3089         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.
3090 
3091         For example, to back-propagate the last valid value to fill the `NaN` values,
3092         pass `bfill` as an argument to the `method` keyword.
3093 
3094         >>> x3 = x2.reindex({"time": time_index2}, method="bfill")
3095         >>> x3
3096         <xarray.Dataset>
3097         Dimensions:      (time: 10)
3098         Coordinates:
3099           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3100         Data variables:
3101             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan
3102             pressure     (time) float64 481.8 481.8 481.8 481.8 ... 284.0 462.8 nan
3103 
3104         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)
3105         will not be filled by any of the value propagation schemes.
3106 
3107         >>> x2.where(x2.temperature.isnull(), drop=True)
3108         <xarray.Dataset>
3109         Dimensions:      (time: 1)
3110         Coordinates:
3111           * time         (time) datetime64[ns] 2019-01-03
3112         Data variables:
3113             temperature  (time) float64 nan
3114             pressure     (time) float64 395.9
3115         >>> x3.where(x3.temperature.isnull(), drop=True)
3116         <xarray.Dataset>
3117         Dimensions:      (time: 2)
3118         Coordinates:
3119           * time         (time) datetime64[ns] 2019-01-03 2019-01-07
3120         Data variables:
3121             temperature  (time) float64 nan nan
3122             pressure     (time) float64 395.9 nan
3123 
3124         This is because filling while reindexing does not look at dataset values, but only compares
3125         the original and desired indexes. If you do want to fill in the `NaN` values present in the
3126         original dataset, use the :py:meth:`~Dataset.fillna()` method.
3127 
3128         """
3129         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3130         return alignment.reindex(
3131             self,
3132             indexers=indexers,
3133             method=method,
3134             tolerance=tolerance,
3135             copy=copy,
3136             fill_value=fill_value,
3137         )
3138 
3139     def _reindex(
3140         self: T_Dataset,
3141         indexers: Mapping[Any, Any] | None = None,
3142         method: str | None = None,
3143         tolerance: int | float | Iterable[int | float] | None = None,
3144         copy: bool = True,
3145         fill_value: Any = xrdtypes.NA,
3146         sparse: bool = False,
3147         **indexers_kwargs: Any,
3148     ) -> T_Dataset:
3149         """
3150         Same as reindex but supports sparse option.
3151         """
3152         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3153         return alignment.reindex(
3154             self,
3155             indexers=indexers,
3156             method=method,
3157             tolerance=tolerance,
3158             copy=copy,
3159             fill_value=fill_value,
3160             sparse=sparse,
3161         )
3162 
3163     def interp(
3164         self: T_Dataset,
3165         coords: Mapping[Any, Any] | None = None,
3166         method: InterpOptions = "linear",
3167         assume_sorted: bool = False,
3168         kwargs: Mapping[str, Any] | None = None,
3169         method_non_numeric: str = "nearest",
3170         **coords_kwargs: Any,
3171     ) -> T_Dataset:
3172         """Interpolate a Dataset onto new coordinates
3173 
3174         Performs univariate or multivariate interpolation of a Dataset onto
3175         new coordinates using scipy's interpolation routines. If interpolating
3176         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
3177         called.  When interpolating along multiple existing dimensions, an
3178         attempt is made to decompose the interpolation into multiple
3179         1-dimensional interpolations. If this is possible,
3180         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3181         :py:func:`scipy.interpolate.interpn` is called.
3182 
3183         Parameters
3184         ----------
3185         coords : dict, optional
3186             Mapping from dimension names to the new coordinates.
3187             New coordinate can be a scalar, array-like or DataArray.
3188             If DataArrays are passed as new coordinates, their dimensions are
3189             used for the broadcasting. Missing values are skipped.
3190         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3191             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3192             String indicating which method to use for interpolation:
3193 
3194             - 'linear': linear interpolation. Additional keyword
3195               arguments are passed to :py:func:`numpy.interp`
3196             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3197               are passed to :py:func:`scipy.interpolate.interp1d`. If
3198               ``method='polynomial'``, the ``order`` keyword argument must also be
3199               provided.
3200             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3201               respective :py:class:`scipy.interpolate` classes.
3202 
3203         assume_sorted : bool, default: False
3204             If False, values of coordinates that are interpolated over can be
3205             in any order and they are sorted first. If True, interpolated
3206             coordinates are assumed to be an array of monotonically increasing
3207             values.
3208         kwargs : dict, optional
3209             Additional keyword arguments passed to scipy's interpolator. Valid
3210             options and their behavior depend whether ``interp1d`` or
3211             ``interpn`` is used.
3212         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3213             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3214             ``"nearest"`` is used by default.
3215         **coords_kwargs : {dim: coordinate, ...}, optional
3216             The keyword arguments form of ``coords``.
3217             One of coords or coords_kwargs must be provided.
3218 
3219         Returns
3220         -------
3221         interpolated : Dataset
3222             New dataset on the new coordinates.
3223 
3224         Notes
3225         -----
3226         scipy is required.
3227 
3228         See Also
3229         --------
3230         scipy.interpolate.interp1d
3231         scipy.interpolate.interpn
3232 
3233         Examples
3234         --------
3235         >>> ds = xr.Dataset(
3236         ...     data_vars={
3237         ...         "a": ("x", [5, 7, 4]),
3238         ...         "b": (
3239         ...             ("x", "y"),
3240         ...             [[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
3241         ...         ),
3242         ...     },
3243         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
3244         ... )
3245         >>> ds
3246         <xarray.Dataset>
3247         Dimensions:  (x: 3, y: 4)
3248         Coordinates:
3249           * x        (x) int64 0 1 2
3250           * y        (y) int64 10 12 14 16
3251         Data variables:
3252             a        (x) int64 5 7 4
3253             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 6.0 nan 6.0 nan 5.0 8.0
3254 
3255         1D interpolation with the default method (linear):
3256 
3257         >>> ds.interp(x=[0, 0.75, 1.25, 1.75])
3258         <xarray.Dataset>
3259         Dimensions:  (x: 4, y: 4)
3260         Coordinates:
3261           * y        (y) int64 10 12 14 16
3262           * x        (x) float64 0.0 0.75 1.25 1.75
3263         Data variables:
3264             a        (x) float64 5.0 6.5 6.25 4.75
3265             b        (x, y) float64 1.0 4.0 2.0 nan 1.75 6.25 ... nan 5.0 nan 5.25 nan
3266 
3267         1D interpolation with a different method:
3268 
3269         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
3270         <xarray.Dataset>
3271         Dimensions:  (x: 4, y: 4)
3272         Coordinates:
3273           * y        (y) int64 10 12 14 16
3274           * x        (x) float64 0.0 0.75 1.25 1.75
3275         Data variables:
3276             a        (x) float64 5.0 7.0 7.0 4.0
3277             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 ... 6.0 nan 6.0 nan 5.0 8.0
3278 
3279         1D extrapolation:
3280 
3281         >>> ds.interp(
3282         ...     x=[1, 1.5, 2.5, 3.5],
3283         ...     method="linear",
3284         ...     kwargs={"fill_value": "extrapolate"},
3285         ... )
3286         <xarray.Dataset>
3287         Dimensions:  (x: 4, y: 4)
3288         Coordinates:
3289           * y        (y) int64 10 12 14 16
3290           * x        (x) float64 1.0 1.5 2.5 3.5
3291         Data variables:
3292             a        (x) float64 7.0 5.5 2.5 -0.5
3293             b        (x, y) float64 2.0 7.0 6.0 nan 4.0 nan ... 4.5 nan 12.0 nan 3.5 nan
3294 
3295         2D interpolation:
3296 
3297         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
3298         <xarray.Dataset>
3299         Dimensions:  (x: 4, y: 3)
3300         Coordinates:
3301           * x        (x) float64 0.0 0.75 1.25 1.75
3302           * y        (y) int64 11 13 15
3303         Data variables:
3304             a        (x) float64 5.0 6.5 6.25 4.75
3305             b        (x, y) float64 2.5 3.0 nan 4.0 5.625 nan nan nan nan nan nan nan
3306         """
3307         from xarray.core import missing
3308 
3309         if kwargs is None:
3310             kwargs = {}
3311 
3312         coords = either_dict_or_kwargs(coords, coords_kwargs, "interp")
3313         indexers = dict(self._validate_interp_indexers(coords))
3314 
3315         if coords:
3316             # This avoids broadcasting over coordinates that are both in
3317             # the original array AND in the indexing array. It essentially
3318             # forces interpolation along the shared coordinates.
3319             sdims = (
3320                 set(self.dims)
3321                 .intersection(*[set(nx.dims) for nx in indexers.values()])
3322                 .difference(coords.keys())
3323             )
3324             indexers.update({d: self.variables[d] for d in sdims})
3325 
3326         obj = self if assume_sorted else self.sortby([k for k in coords])
3327 
3328         def maybe_variable(obj, k):
3329             # workaround to get variable for dimension without coordinate.
3330             try:
3331                 return obj._variables[k]
3332             except KeyError:
3333                 return as_variable((k, range(obj.dims[k])))
3334 
3335         def _validate_interp_indexer(x, new_x):
3336             # In the case of datetimes, the restrictions placed on indexers
3337             # used with interp are stronger than those which are placed on
3338             # isel, so we need an additional check after _validate_indexers.
3339             if _contains_datetime_like_objects(
3340                 x
3341             ) and not _contains_datetime_like_objects(new_x):
3342                 raise TypeError(
3343                     "When interpolating over a datetime-like "
3344                     "coordinate, the coordinates to "
3345                     "interpolate to must be either datetime "
3346                     "strings or datetimes. "
3347                     "Instead got\n{}".format(new_x)
3348                 )
3349             return x, new_x
3350 
3351         validated_indexers = {
3352             k: _validate_interp_indexer(maybe_variable(obj, k), v)
3353             for k, v in indexers.items()
3354         }
3355 
3356         # optimization: subset to coordinate range of the target index
3357         if method in ["linear", "nearest"]:
3358             for k, v in validated_indexers.items():
3359                 obj, newidx = missing._localize(obj, {k: v})
3360                 validated_indexers[k] = newidx[k]
3361 
3362         # optimization: create dask coordinate arrays once per Dataset
3363         # rather than once per Variable when dask.array.unify_chunks is called later
3364         # GH4739
3365         if obj.__dask_graph__():
3366             dask_indexers = {
3367                 k: (index.to_base_variable().chunk(), dest.to_base_variable().chunk())
3368                 for k, (index, dest) in validated_indexers.items()
3369             }
3370 
3371         variables: dict[Hashable, Variable] = {}
3372         reindex: bool = False
3373         for name, var in obj._variables.items():
3374             if name in indexers:
3375                 continue
3376 
3377             if is_duck_dask_array(var.data):
3378                 use_indexers = dask_indexers
3379             else:
3380                 use_indexers = validated_indexers
3381 
3382             dtype_kind = var.dtype.kind
3383             if dtype_kind in "uifc":
3384                 # For normal number types do the interpolation:
3385                 var_indexers = {k: v for k, v in use_indexers.items() if k in var.dims}
3386                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)
3387             elif dtype_kind in "ObU" and (use_indexers.keys() & var.dims):
3388                 # For types that we do not understand do stepwise
3389                 # interpolation to avoid modifying the elements.
3390                 # reindex the variable instead because it supports
3391                 # booleans and objects and retains the dtype but inside
3392                 # this loop there might be some duplicate code that slows it
3393                 # down, therefore collect these signals and run it later:
3394                 reindex = True
3395             elif all(d not in indexers for d in var.dims):
3396                 # For anything else we can only keep variables if they
3397                 # are not dependent on any coords that are being
3398                 # interpolated along:
3399                 variables[name] = var
3400 
3401         if reindex:
3402             reindex_indexers = {
3403                 k: v for k, (_, v) in validated_indexers.items() if v.dims == (k,)
3404             }
3405             reindexed = alignment.reindex(
3406                 obj,
3407                 indexers=reindex_indexers,
3408                 method=method_non_numeric,
3409                 exclude_vars=variables.keys(),
3410             )
3411             indexes = dict(reindexed._indexes)
3412             variables.update(reindexed.variables)
3413         else:
3414             # Get the indexes that are not being interpolated along
3415             indexes = {k: v for k, v in obj._indexes.items() if k not in indexers}
3416 
3417         # Get the coords that also exist in the variables:
3418         coord_names = obj._coord_names & variables.keys()
3419         selected = self._replace_with_new_dims(
3420             variables.copy(), coord_names, indexes=indexes
3421         )
3422 
3423         # Attach indexer as coordinate
3424         for k, v in indexers.items():
3425             assert isinstance(v, Variable)
3426             if v.dims == (k,):
3427                 index = PandasIndex(v, k, coord_dtype=v.dtype)
3428                 index_vars = index.create_variables({k: v})
3429                 indexes[k] = index
3430                 variables.update(index_vars)
3431             else:
3432                 variables[k] = v
3433 
3434         # Extract coordinates from indexers
3435         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)
3436         variables.update(coord_vars)
3437         indexes.update(new_indexes)
3438 
3439         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()
3440         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3441 
3442     def interp_like(
3443         self,
3444         other: Dataset | DataArray,
3445         method: InterpOptions = "linear",
3446         assume_sorted: bool = False,
3447         kwargs: Mapping[str, Any] | None = None,
3448         method_non_numeric: str = "nearest",
3449     ) -> Dataset:
3450         """Interpolate this object onto the coordinates of another object,
3451         filling the out of range values with NaN.
3452 
3453         If interpolating along a single existing dimension,
3454         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
3455         along multiple existing dimensions, an attempt is made to decompose the
3456         interpolation into multiple 1-dimensional interpolations. If this is
3457         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3458         :py:func:`scipy.interpolate.interpn` is called.
3459 
3460         Parameters
3461         ----------
3462         other : Dataset or DataArray
3463             Object with an 'indexes' attribute giving a mapping from dimension
3464             names to an 1d array-like, which provides coordinates upon
3465             which to index the variables in this dataset. Missing values are skipped.
3466         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3467             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3468             String indicating which method to use for interpolation:
3469 
3470             - 'linear': linear interpolation. Additional keyword
3471               arguments are passed to :py:func:`numpy.interp`
3472             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3473               are passed to :py:func:`scipy.interpolate.interp1d`. If
3474               ``method='polynomial'``, the ``order`` keyword argument must also be
3475               provided.
3476             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3477               respective :py:class:`scipy.interpolate` classes.
3478 
3479         assume_sorted : bool, default: False
3480             If False, values of coordinates that are interpolated over can be
3481             in any order and they are sorted first. If True, interpolated
3482             coordinates are assumed to be an array of monotonically increasing
3483             values.
3484         kwargs : dict, optional
3485             Additional keyword passed to scipy's interpolator.
3486         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3487             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3488             ``"nearest"`` is used by default.
3489 
3490         Returns
3491         -------
3492         interpolated : Dataset
3493             Another dataset by interpolating this dataset's data along the
3494             coordinates of the other object.
3495 
3496         Notes
3497         -----
3498         scipy is required.
3499         If the dataset has object-type coordinates, reindex is used for these
3500         coordinates instead of the interpolation.
3501 
3502         See Also
3503         --------
3504         Dataset.interp
3505         Dataset.reindex_like
3506         """
3507         if kwargs is None:
3508             kwargs = {}
3509 
3510         # pick only dimension coordinates with a single index
3511         coords = {}
3512         other_indexes = other.xindexes
3513         for dim in self.dims:
3514             other_dim_coords = other_indexes.get_all_coords(dim, errors="ignore")
3515             if len(other_dim_coords) == 1:
3516                 coords[dim] = other_dim_coords[dim]
3517 
3518         numeric_coords: dict[Hashable, pd.Index] = {}
3519         object_coords: dict[Hashable, pd.Index] = {}
3520         for k, v in coords.items():
3521             if v.dtype.kind in "uifcMm":
3522                 numeric_coords[k] = v
3523             else:
3524                 object_coords[k] = v
3525 
3526         ds = self
3527         if object_coords:
3528             # We do not support interpolation along object coordinate.
3529             # reindex instead.
3530             ds = self.reindex(object_coords)
3531         return ds.interp(
3532             coords=numeric_coords,
3533             method=method,
3534             assume_sorted=assume_sorted,
3535             kwargs=kwargs,
3536             method_non_numeric=method_non_numeric,
3537         )
3538 
3539     # Helper methods for rename()
3540     def _rename_vars(
3541         self, name_dict, dims_dict
3542     ) -> tuple[dict[Hashable, Variable], set[Hashable]]:
3543         variables = {}
3544         coord_names = set()
3545         for k, v in self.variables.items():
3546             var = v.copy(deep=False)
3547             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3548             name = name_dict.get(k, k)
3549             if name in variables:
3550                 raise ValueError(f"the new name {name!r} conflicts")
3551             variables[name] = var
3552             if k in self._coord_names:
3553                 coord_names.add(name)
3554         return variables, coord_names
3555 
3556     def _rename_dims(self, name_dict: Mapping[Any, Hashable]) -> dict[Hashable, int]:
3557         return {name_dict.get(k, k): v for k, v in self.dims.items()}
3558 
3559     def _rename_indexes(
3560         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3561     ) -> tuple[dict[Hashable, Index], dict[Hashable, Variable]]:
3562         if not self._indexes:
3563             return {}, {}
3564 
3565         indexes = {}
3566         variables = {}
3567 
3568         for index, coord_names in self.xindexes.group_by_index():
3569             new_index = index.rename(name_dict, dims_dict)
3570             new_coord_names = [name_dict.get(k, k) for k in coord_names]
3571             indexes.update({k: new_index for k in new_coord_names})
3572             new_index_vars = new_index.create_variables(
3573                 {
3574                     new: self._variables[old]
3575                     for old, new in zip(coord_names, new_coord_names)
3576                 }
3577             )
3578             variables.update(new_index_vars)
3579 
3580         return indexes, variables
3581 
3582     def _rename_all(
3583         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3584     ) -> tuple[
3585         dict[Hashable, Variable],
3586         set[Hashable],
3587         dict[Hashable, int],
3588         dict[Hashable, Index],
3589     ]:
3590         variables, coord_names = self._rename_vars(name_dict, dims_dict)
3591         dims = self._rename_dims(dims_dict)
3592 
3593         indexes, index_vars = self._rename_indexes(name_dict, dims_dict)
3594         variables = {k: index_vars.get(k, v) for k, v in variables.items()}
3595 
3596         return variables, coord_names, dims, indexes
3597 
3598     def _rename(
3599         self: T_Dataset,
3600         name_dict: Mapping[Any, Hashable] | None = None,
3601         **names: Hashable,
3602     ) -> T_Dataset:
3603         """Also used internally by DataArray so that the warning (if any)
3604         is raised at the right stack level.
3605         """
3606         name_dict = either_dict_or_kwargs(name_dict, names, "rename")
3607         for k in name_dict.keys():
3608             if k not in self and k not in self.dims:
3609                 raise ValueError(
3610                     f"cannot rename {k!r} because it is not a "
3611                     "variable or dimension in this dataset"
3612                 )
3613 
3614             create_dim_coord = False
3615             new_k = name_dict[k]
3616 
3617             if k in self.dims and new_k in self._coord_names:
3618                 coord_dims = self._variables[name_dict[k]].dims
3619                 if coord_dims == (k,):
3620                     create_dim_coord = True
3621             elif k in self._coord_names and new_k in self.dims:
3622                 coord_dims = self._variables[k].dims
3623                 if coord_dims == (new_k,):
3624                     create_dim_coord = True
3625 
3626             if create_dim_coord:
3627                 warnings.warn(
3628                     f"rename {k!r} to {name_dict[k]!r} does not create an index "
3629                     "anymore. Try using swap_dims instead or use set_index "
3630                     "after rename to create an indexed coordinate.",
3631                     UserWarning,
3632                     stacklevel=3,
3633                 )
3634 
3635         variables, coord_names, dims, indexes = self._rename_all(
3636             name_dict=name_dict, dims_dict=name_dict
3637         )
3638         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3639 
3640     def rename(
3641         self: T_Dataset,
3642         name_dict: Mapping[Any, Hashable] | None = None,
3643         **names: Hashable,
3644     ) -> T_Dataset:
3645         """Returns a new object with renamed variables, coordinates and dimensions.
3646 
3647         Parameters
3648         ----------
3649         name_dict : dict-like, optional
3650             Dictionary whose keys are current variable, coordinate or dimension names and
3651             whose values are the desired names.
3652         **names : optional
3653             Keyword form of ``name_dict``.
3654             One of name_dict or names must be provided.
3655 
3656         Returns
3657         -------
3658         renamed : Dataset
3659             Dataset with renamed variables, coordinates and dimensions.
3660 
3661         See Also
3662         --------
3663         Dataset.swap_dims
3664         Dataset.rename_vars
3665         Dataset.rename_dims
3666         DataArray.rename
3667         """
3668         return self._rename(name_dict=name_dict, **names)
3669 
3670     def rename_dims(
3671         self: T_Dataset,
3672         dims_dict: Mapping[Any, Hashable] | None = None,
3673         **dims: Hashable,
3674     ) -> T_Dataset:
3675         """Returns a new object with renamed dimensions only.
3676 
3677         Parameters
3678         ----------
3679         dims_dict : dict-like, optional
3680             Dictionary whose keys are current dimension names and
3681             whose values are the desired names. The desired names must
3682             not be the name of an existing dimension or Variable in the Dataset.
3683         **dims : optional
3684             Keyword form of ``dims_dict``.
3685             One of dims_dict or dims must be provided.
3686 
3687         Returns
3688         -------
3689         renamed : Dataset
3690             Dataset with renamed dimensions.
3691 
3692         See Also
3693         --------
3694         Dataset.swap_dims
3695         Dataset.rename
3696         Dataset.rename_vars
3697         DataArray.rename
3698         """
3699         dims_dict = either_dict_or_kwargs(dims_dict, dims, "rename_dims")
3700         for k, v in dims_dict.items():
3701             if k not in self.dims:
3702                 raise ValueError(
3703                     f"cannot rename {k!r} because it is not a "
3704                     "dimension in this dataset"
3705                 )
3706             if v in self.dims or v in self:
3707                 raise ValueError(
3708                     f"Cannot rename {k} to {v} because {v} already exists. "
3709                     "Try using swap_dims instead."
3710                 )
3711 
3712         variables, coord_names, sizes, indexes = self._rename_all(
3713             name_dict={}, dims_dict=dims_dict
3714         )
3715         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)
3716 
3717     def rename_vars(
3718         self: T_Dataset,
3719         name_dict: Mapping[Any, Hashable] | None = None,
3720         **names: Hashable,
3721     ) -> T_Dataset:
3722         """Returns a new object with renamed variables including coordinates
3723 
3724         Parameters
3725         ----------
3726         name_dict : dict-like, optional
3727             Dictionary whose keys are current variable or coordinate names and
3728             whose values are the desired names.
3729         **names : optional
3730             Keyword form of ``name_dict``.
3731             One of name_dict or names must be provided.
3732 
3733         Returns
3734         -------
3735         renamed : Dataset
3736             Dataset with renamed variables including coordinates
3737 
3738         See Also
3739         --------
3740         Dataset.swap_dims
3741         Dataset.rename
3742         Dataset.rename_dims
3743         DataArray.rename
3744         """
3745         name_dict = either_dict_or_kwargs(name_dict, names, "rename_vars")
3746         for k in name_dict:
3747             if k not in self:
3748                 raise ValueError(
3749                     f"cannot rename {k!r} because it is not a "
3750                     "variable or coordinate in this dataset"
3751                 )
3752         variables, coord_names, dims, indexes = self._rename_all(
3753             name_dict=name_dict, dims_dict={}
3754         )
3755         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3756 
3757     def swap_dims(
3758         self: T_Dataset, dims_dict: Mapping[Any, Hashable] | None = None, **dims_kwargs
3759     ) -> T_Dataset:
3760         """Returns a new object with swapped dimensions.
3761 
3762         Parameters
3763         ----------
3764         dims_dict : dict-like
3765             Dictionary whose keys are current dimension names and whose values
3766             are new names.
3767         **dims_kwargs : {existing_dim: new_dim, ...}, optional
3768             The keyword arguments form of ``dims_dict``.
3769             One of dims_dict or dims_kwargs must be provided.
3770 
3771         Returns
3772         -------
3773         swapped : Dataset
3774             Dataset with swapped dimensions.
3775 
3776         Examples
3777         --------
3778         >>> ds = xr.Dataset(
3779         ...     data_vars={"a": ("x", [5, 7]), "b": ("x", [0.1, 2.4])},
3780         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
3781         ... )
3782         >>> ds
3783         <xarray.Dataset>
3784         Dimensions:  (x: 2)
3785         Coordinates:
3786           * x        (x) <U1 'a' 'b'
3787             y        (x) int64 0 1
3788         Data variables:
3789             a        (x) int64 5 7
3790             b        (x) float64 0.1 2.4
3791 
3792         >>> ds.swap_dims({"x": "y"})
3793         <xarray.Dataset>
3794         Dimensions:  (y: 2)
3795         Coordinates:
3796             x        (y) <U1 'a' 'b'
3797           * y        (y) int64 0 1
3798         Data variables:
3799             a        (y) int64 5 7
3800             b        (y) float64 0.1 2.4
3801 
3802         >>> ds.swap_dims({"x": "z"})
3803         <xarray.Dataset>
3804         Dimensions:  (z: 2)
3805         Coordinates:
3806             x        (z) <U1 'a' 'b'
3807             y        (z) int64 0 1
3808         Dimensions without coordinates: z
3809         Data variables:
3810             a        (z) int64 5 7
3811             b        (z) float64 0.1 2.4
3812 
3813         See Also
3814         --------
3815         Dataset.rename
3816         DataArray.swap_dims
3817         """
3818         # TODO: deprecate this method in favor of a (less confusing)
3819         # rename_dims() method that only renames dimensions.
3820 
3821         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
3822         for k, v in dims_dict.items():
3823             if k not in self.dims:
3824                 raise ValueError(
3825                     f"cannot swap from dimension {k!r} because it is "
3826                     "not an existing dimension"
3827                 )
3828             if v in self.variables and self.variables[v].dims != (k,):
3829                 raise ValueError(
3830                     f"replacement dimension {v!r} is not a 1D "
3831                     f"variable along the old dimension {k!r}"
3832                 )
3833 
3834         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}
3835 
3836         coord_names = self._coord_names.copy()
3837         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})
3838 
3839         variables: dict[Hashable, Variable] = {}
3840         indexes: dict[Hashable, Index] = {}
3841         for k, v in self.variables.items():
3842             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3843             var: Variable
3844             if k in result_dims:
3845                 var = v.to_index_variable()
3846                 var.dims = dims
3847                 if k in self._indexes:
3848                     indexes[k] = self._indexes[k]
3849                     variables[k] = var
3850                 else:
3851                     index, index_vars = create_default_index_implicit(var)
3852                     indexes.update({name: index for name in index_vars})
3853                     variables.update(index_vars)
3854                     coord_names.update(index_vars)
3855             else:
3856                 var = v.to_base_variable()
3857                 var.dims = dims
3858                 variables[k] = var
3859 
3860         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3861 
3862     # change type of self and return to T_Dataset once
3863     # https://github.com/python/mypy/issues/12846 is resolved
3864     def expand_dims(
3865         self,
3866         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
3867         axis: None | int | Sequence[int] = None,
3868         **dim_kwargs: Any,
3869     ) -> Dataset:
3870         """Return a new object with an additional axis (or axes) inserted at
3871         the corresponding position in the array shape.  The new object is a
3872         view into the underlying array, not a copy.
3873 
3874         If dim is already a scalar coordinate, it will be promoted to a 1D
3875         coordinate consisting of a single value.
3876 
3877         Parameters
3878         ----------
3879         dim : hashable, sequence of hashable, mapping, or None
3880             Dimensions to include on the new variable. If provided as hashable
3881             or sequence of hashable, then dimensions are inserted with length
3882             1. If provided as a mapping, then the keys are the new dimensions
3883             and the values are either integers (giving the length of the new
3884             dimensions) or array-like (giving the coordinates of the new
3885             dimensions).
3886         axis : int, sequence of int, or None, default: None
3887             Axis position(s) where new axis is to be inserted (position(s) on
3888             the result array). If a sequence of integers is passed,
3889             multiple axes are inserted. In this case, dim arguments should be
3890             same length list. If axis=None is passed, all the axes will be
3891             inserted to the start of the result array.
3892         **dim_kwargs : int or sequence or ndarray
3893             The keywords are arbitrary dimensions being inserted and the values
3894             are either the lengths of the new dims (if int is given), or their
3895             coordinates. Note, this is an alternative to passing a dict to the
3896             dim kwarg and will only be used if dim is None.
3897 
3898         Returns
3899         -------
3900         expanded : Dataset
3901             This object, but with additional dimension(s).
3902 
3903         See Also
3904         --------
3905         DataArray.expand_dims
3906         """
3907         if dim is None:
3908             pass
3909         elif isinstance(dim, Mapping):
3910             # We're later going to modify dim in place; don't tamper with
3911             # the input
3912             dim = dict(dim)
3913         elif isinstance(dim, int):
3914             raise TypeError(
3915                 "dim should be hashable or sequence of hashables or mapping"
3916             )
3917         elif isinstance(dim, str) or not isinstance(dim, Sequence):
3918             dim = {dim: 1}
3919         elif isinstance(dim, Sequence):
3920             if len(dim) != len(set(dim)):
3921                 raise ValueError("dims should not contain duplicate values.")
3922             dim = {d: 1 for d in dim}
3923 
3924         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
3925         assert isinstance(dim, MutableMapping)
3926 
3927         if axis is None:
3928             axis = list(range(len(dim)))
3929         elif not isinstance(axis, Sequence):
3930             axis = [axis]
3931 
3932         if len(dim) != len(axis):
3933             raise ValueError("lengths of dim and axis should be identical.")
3934         for d in dim:
3935             if d in self.dims:
3936                 raise ValueError(f"Dimension {d} already exists.")
3937             if d in self._variables and not utils.is_scalar(self._variables[d]):
3938                 raise ValueError(
3939                     "{dim} already exists as coordinate or"
3940                     " variable name.".format(dim=d)
3941                 )
3942 
3943         variables: dict[Hashable, Variable] = {}
3944         indexes: dict[Hashable, Index] = dict(self._indexes)
3945         coord_names = self._coord_names.copy()
3946         # If dim is a dict, then ensure that the values are either integers
3947         # or iterables.
3948         for k, v in dim.items():
3949             if hasattr(v, "__iter__"):
3950                 # If the value for the new dimension is an iterable, then
3951                 # save the coordinates to the variables dict, and set the
3952                 # value within the dim dict to the length of the iterable
3953                 # for later use.
3954                 index = PandasIndex(v, k)
3955                 indexes[k] = index
3956                 variables.update(index.create_variables())
3957                 coord_names.add(k)
3958                 dim[k] = variables[k].size
3959             elif isinstance(v, int):
3960                 pass  # Do nothing if the dimensions value is just an int
3961             else:
3962                 raise TypeError(
3963                     "The value of new dimension {k} must be "
3964                     "an iterable or an int".format(k=k)
3965                 )
3966 
3967         for k, v in self._variables.items():
3968             if k not in dim:
3969                 if k in coord_names:  # Do not change coordinates
3970                     variables[k] = v
3971                 else:
3972                     result_ndim = len(v.dims) + len(axis)
3973                     for a in axis:
3974                         if a < -result_ndim or result_ndim - 1 < a:
3975                             raise IndexError(
3976                                 f"Axis {a} of variable {k} is out of bounds of the "
3977                                 f"expanded dimension size {result_ndim}"
3978                             )
3979 
3980                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]
3981                     if len(axis_pos) != len(set(axis_pos)):
3982                         raise ValueError("axis should not contain duplicate values")
3983                     # We need to sort them to make sure `axis` equals to the
3984                     # axis positions of the result array.
3985                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))
3986 
3987                     all_dims = list(zip(v.dims, v.shape))
3988                     for d, c in zip_axis_dim:
3989                         all_dims.insert(d, c)
3990                     variables[k] = v.set_dims(dict(all_dims))
3991             else:
3992                 if k not in variables:
3993                     # If dims includes a label of a non-dimension coordinate,
3994                     # it will be promoted to a 1D coordinate with a single value.
3995                     index, index_vars = create_default_index_implicit(v.set_dims(k))
3996                     indexes[k] = index
3997                     variables.update(index_vars)
3998 
3999         return self._replace_with_new_dims(
4000             variables, coord_names=coord_names, indexes=indexes
4001         )
4002 
4003     # change type of self and return to T_Dataset once
4004     # https://github.com/python/mypy/issues/12846 is resolved
4005     def set_index(
4006         self,
4007         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,
4008         append: bool = False,
4009         **indexes_kwargs: Hashable | Sequence[Hashable],
4010     ) -> Dataset:
4011         """Set Dataset (multi-)indexes using one or more existing coordinates
4012         or variables.
4013 
4014         This legacy method is limited to pandas (multi-)indexes and
4015         1-dimensional "dimension" coordinates. See
4016         :py:meth:`~Dataset.set_xindex` for setting a pandas or a custom
4017         Xarray-compatible index from one or more arbitrary coordinates.
4018 
4019         Parameters
4020         ----------
4021         indexes : {dim: index, ...}
4022             Mapping from names matching dimensions and values given
4023             by (lists of) the names of existing coordinates or variables to set
4024             as new (multi-)index.
4025         append : bool, default: False
4026             If True, append the supplied index(es) to the existing index(es).
4027             Otherwise replace the existing index(es) (default).
4028         **indexes_kwargs : optional
4029             The keyword arguments form of ``indexes``.
4030             One of indexes or indexes_kwargs must be provided.
4031 
4032         Returns
4033         -------
4034         obj : Dataset
4035             Another dataset, with this dataset's data but replaced coordinates.
4036 
4037         Examples
4038         --------
4039         >>> arr = xr.DataArray(
4040         ...     data=np.ones((2, 3)),
4041         ...     dims=["x", "y"],
4042         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
4043         ... )
4044         >>> ds = xr.Dataset({"v": arr})
4045         >>> ds
4046         <xarray.Dataset>
4047         Dimensions:  (x: 2, y: 3)
4048         Coordinates:
4049           * x        (x) int64 0 1
4050           * y        (y) int64 0 1 2
4051             a        (x) int64 3 4
4052         Data variables:
4053             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
4054         >>> ds.set_index(x="a")
4055         <xarray.Dataset>
4056         Dimensions:  (x: 2, y: 3)
4057         Coordinates:
4058           * x        (x) int64 3 4
4059           * y        (y) int64 0 1 2
4060         Data variables:
4061             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
4062 
4063         See Also
4064         --------
4065         Dataset.reset_index
4066         Dataset.set_xindex
4067         Dataset.swap_dims
4068         """
4069         dim_coords = either_dict_or_kwargs(indexes, indexes_kwargs, "set_index")
4070 
4071         new_indexes: dict[Hashable, Index] = {}
4072         new_variables: dict[Hashable, Variable] = {}
4073         drop_indexes: set[Hashable] = set()
4074         drop_variables: set[Hashable] = set()
4075         replace_dims: dict[Hashable, Hashable] = {}
4076         all_var_names: set[Hashable] = set()
4077 
4078         for dim, _var_names in dim_coords.items():
4079             if isinstance(_var_names, str) or not isinstance(_var_names, Sequence):
4080                 var_names = [_var_names]
4081             else:
4082                 var_names = list(_var_names)
4083 
4084             invalid_vars = set(var_names) - set(self._variables)
4085             if invalid_vars:
4086                 raise ValueError(
4087                     ", ".join([str(v) for v in invalid_vars])
4088                     + " variable(s) do not exist"
4089                 )
4090 
4091             all_var_names.update(var_names)
4092             drop_variables.update(var_names)
4093 
4094             # drop any pre-existing index involved and its corresponding coordinates
4095             index_coord_names = self.xindexes.get_all_coords(dim, errors="ignore")
4096             all_index_coord_names = set(index_coord_names)
4097             for k in var_names:
4098                 all_index_coord_names.update(
4099                     self.xindexes.get_all_coords(k, errors="ignore")
4100                 )
4101 
4102             drop_indexes.update(all_index_coord_names)
4103             drop_variables.update(all_index_coord_names)
4104 
4105             if len(var_names) == 1 and (not append or dim not in self._indexes):
4106                 var_name = var_names[0]
4107                 var = self._variables[var_name]
4108                 if var.dims != (dim,):
4109                     raise ValueError(
4110                         f"dimension mismatch: try setting an index for dimension {dim!r} with "
4111                         f"variable {var_name!r} that has dimensions {var.dims}"
4112                     )
4113                 idx = PandasIndex.from_variables({dim: var}, options={})
4114                 idx_vars = idx.create_variables({var_name: var})
4115 
4116                 # trick to preserve coordinate order in this case
4117                 if dim in self._coord_names:
4118                     drop_variables.remove(dim)
4119             else:
4120                 if append:
4121                     current_variables = {
4122                         k: self._variables[k] for k in index_coord_names
4123                     }
4124                 else:
4125                     current_variables = {}
4126                 idx, idx_vars = PandasMultiIndex.from_variables_maybe_expand(
4127                     dim,
4128                     current_variables,
4129                     {k: self._variables[k] for k in var_names},
4130                 )
4131                 for n in idx.index.names:
4132                     replace_dims[n] = dim
4133 
4134             new_indexes.update({k: idx for k in idx_vars})
4135             new_variables.update(idx_vars)
4136 
4137         # re-add deindexed coordinates (convert to base variables)
4138         for k in drop_variables:
4139             if (
4140                 k not in new_variables
4141                 and k not in all_var_names
4142                 and k in self._coord_names
4143             ):
4144                 new_variables[k] = self._variables[k].to_base_variable()
4145 
4146         indexes_: dict[Any, Index] = {
4147             k: v for k, v in self._indexes.items() if k not in drop_indexes
4148         }
4149         indexes_.update(new_indexes)
4150 
4151         variables = {
4152             k: v for k, v in self._variables.items() if k not in drop_variables
4153         }
4154         variables.update(new_variables)
4155 
4156         # update dimensions if necessary, GH: 3512
4157         for k, v in variables.items():
4158             if any(d in replace_dims for d in v.dims):
4159                 new_dims = [replace_dims.get(d, d) for d in v.dims]
4160                 variables[k] = v._replace(dims=new_dims)
4161 
4162         coord_names = self._coord_names - drop_variables | set(new_variables)
4163 
4164         return self._replace_with_new_dims(
4165             variables, coord_names=coord_names, indexes=indexes_
4166         )
4167 
4168     def reset_index(
4169         self: T_Dataset,
4170         dims_or_levels: Hashable | Sequence[Hashable],
4171         drop: bool = False,
4172     ) -> T_Dataset:
4173         """Reset the specified index(es) or multi-index level(s).
4174 
4175         This legacy method is specific to pandas (multi-)indexes and
4176         1-dimensional "dimension" coordinates. See the more generic
4177         :py:meth:`~Dataset.drop_indexes` and :py:meth:`~Dataset.set_xindex`
4178         method to respectively drop and set pandas or custom indexes for
4179         arbitrary coordinates.
4180 
4181         Parameters
4182         ----------
4183         dims_or_levels : Hashable or Sequence of Hashable
4184             Name(s) of the dimension(s) and/or multi-index level(s) that will
4185             be reset.
4186         drop : bool, default: False
4187             If True, remove the specified indexes and/or multi-index levels
4188             instead of extracting them as new coordinates (default: False).
4189 
4190         Returns
4191         -------
4192         obj : Dataset
4193             Another dataset, with this dataset's data but replaced coordinates.
4194 
4195         See Also
4196         --------
4197         Dataset.set_index
4198         Dataset.set_xindex
4199         Dataset.drop_indexes
4200         """
4201         if isinstance(dims_or_levels, str) or not isinstance(dims_or_levels, Sequence):
4202             dims_or_levels = [dims_or_levels]
4203 
4204         invalid_coords = set(dims_or_levels) - set(self._indexes)
4205         if invalid_coords:
4206             raise ValueError(
4207                 f"{tuple(invalid_coords)} are not coordinates with an index"
4208             )
4209 
4210         drop_indexes: set[Hashable] = set()
4211         drop_variables: set[Hashable] = set()
4212         seen: set[Index] = set()
4213         new_indexes: dict[Hashable, Index] = {}
4214         new_variables: dict[Hashable, Variable] = {}
4215 
4216         def drop_or_convert(var_names):
4217             if drop:
4218                 drop_variables.update(var_names)
4219             else:
4220                 base_vars = {
4221                     k: self._variables[k].to_base_variable() for k in var_names
4222                 }
4223                 new_variables.update(base_vars)
4224 
4225         for name in dims_or_levels:
4226             index = self._indexes[name]
4227 
4228             if index in seen:
4229                 continue
4230             seen.add(index)
4231 
4232             idx_var_names = set(self.xindexes.get_all_coords(name))
4233             drop_indexes.update(idx_var_names)
4234 
4235             if isinstance(index, PandasMultiIndex):
4236                 # special case for pd.MultiIndex
4237                 level_names = index.index.names
4238                 keep_level_vars = {
4239                     k: self._variables[k]
4240                     for k in level_names
4241                     if k not in dims_or_levels
4242                 }
4243 
4244                 if index.dim not in dims_or_levels and keep_level_vars:
4245                     # do not drop the multi-index completely
4246                     # instead replace it by a new (multi-)index with dropped level(s)
4247                     idx = index.keep_levels(keep_level_vars)
4248                     idx_vars = idx.create_variables(keep_level_vars)
4249                     new_indexes.update({k: idx for k in idx_vars})
4250                     new_variables.update(idx_vars)
4251                     if not isinstance(idx, PandasMultiIndex):
4252                         # multi-index reduced to single index
4253                         # backward compatibility: unique level coordinate renamed to dimension
4254                         drop_variables.update(keep_level_vars)
4255                     drop_or_convert(
4256                         [k for k in level_names if k not in keep_level_vars]
4257                     )
4258                 else:
4259                     # always drop the multi-index dimension variable
4260                     drop_variables.add(index.dim)
4261                     drop_or_convert(level_names)
4262             else:
4263                 drop_or_convert(idx_var_names)
4264 
4265         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4266         indexes.update(new_indexes)
4267 
4268         variables = {
4269             k: v for k, v in self._variables.items() if k not in drop_variables
4270         }
4271         variables.update(new_variables)
4272 
4273         coord_names = self._coord_names - drop_variables
4274 
4275         return self._replace_with_new_dims(
4276             variables, coord_names=coord_names, indexes=indexes
4277         )
4278 
4279     def set_xindex(
4280         self: T_Dataset,
4281         coord_names: str | Sequence[Hashable],
4282         index_cls: type[Index] | None = None,
4283         **options,
4284     ) -> T_Dataset:
4285         """Set a new, Xarray-compatible index from one or more existing
4286         coordinate(s).
4287 
4288         Parameters
4289         ----------
4290         coord_names : str or list
4291             Name(s) of the coordinate(s) used to build the index.
4292             If several names are given, their order matters.
4293         index_cls : subclass of :class:`~xarray.indexes.Index`, optional
4294             The type of index to create. By default, try setting
4295             a ``PandasIndex`` if ``len(coord_names) == 1``,
4296             otherwise a ``PandasMultiIndex``.
4297         **options
4298             Options passed to the index constructor.
4299 
4300         Returns
4301         -------
4302         obj : Dataset
4303             Another dataset, with this dataset's data and with a new index.
4304 
4305         """
4306         # the Sequence check is required for mypy
4307         if is_scalar(coord_names) or not isinstance(coord_names, Sequence):
4308             coord_names = [coord_names]
4309 
4310         if index_cls is None:
4311             if len(coord_names) == 1:
4312                 index_cls = PandasIndex
4313             else:
4314                 index_cls = PandasMultiIndex
4315         else:
4316             if not issubclass(index_cls, Index):
4317                 raise TypeError(f"{index_cls} is not a subclass of xarray.Index")
4318 
4319         invalid_coords = set(coord_names) - self._coord_names
4320 
4321         if invalid_coords:
4322             msg = ["invalid coordinate(s)"]
4323             no_vars = invalid_coords - set(self._variables)
4324             data_vars = invalid_coords - no_vars
4325             if no_vars:
4326                 msg.append(f"those variables don't exist: {no_vars}")
4327             if data_vars:
4328                 msg.append(
4329                     f"those variables are data variables: {data_vars}, use `set_coords` first"
4330                 )
4331             raise ValueError("\n".join(msg))
4332 
4333         # we could be more clever here (e.g., drop-in index replacement if index
4334         # coordinates do not conflict), but let's not allow this for now
4335         indexed_coords = set(coord_names) & set(self._indexes)
4336 
4337         if indexed_coords:
4338             raise ValueError(
4339                 f"those coordinates already have an index: {indexed_coords}"
4340             )
4341 
4342         coord_vars = {name: self._variables[name] for name in coord_names}
4343 
4344         index = index_cls.from_variables(coord_vars, options=options)
4345 
4346         new_coord_vars = index.create_variables(coord_vars)
4347 
4348         # special case for setting a pandas multi-index from level coordinates
4349         # TODO: remove it once we depreciate pandas multi-index dimension (tuple
4350         # elements) coordinate
4351         if isinstance(index, PandasMultiIndex):
4352             coord_names = [index.dim] + list(coord_names)
4353 
4354         variables: dict[Hashable, Variable]
4355         indexes: dict[Hashable, Index]
4356 
4357         if len(coord_names) == 1:
4358             variables = self._variables.copy()
4359             indexes = self._indexes.copy()
4360 
4361             name = list(coord_names).pop()
4362             if name in new_coord_vars:
4363                 variables[name] = new_coord_vars[name]
4364             indexes[name] = index
4365         else:
4366             # reorder variables and indexes so that coordinates having the same
4367             # index are next to each other
4368             variables = {}
4369             for name, var in self._variables.items():
4370                 if name not in coord_names:
4371                     variables[name] = var
4372 
4373             indexes = {}
4374             for name, idx in self._indexes.items():
4375                 if name not in coord_names:
4376                     indexes[name] = idx
4377 
4378             for name in coord_names:
4379                 try:
4380                     variables[name] = new_coord_vars[name]
4381                 except KeyError:
4382                     variables[name] = self._variables[name]
4383                 indexes[name] = index
4384 
4385         return self._replace(
4386             variables=variables,
4387             coord_names=self._coord_names | set(coord_names),
4388             indexes=indexes,
4389         )
4390 
4391     def reorder_levels(
4392         self: T_Dataset,
4393         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
4394         **dim_order_kwargs: Sequence[int | Hashable],
4395     ) -> T_Dataset:
4396         """Rearrange index levels using input order.
4397 
4398         Parameters
4399         ----------
4400         dim_order : dict-like of Hashable to Sequence of int or Hashable, optional
4401             Mapping from names matching dimensions and values given
4402             by lists representing new level orders. Every given dimension
4403             must have a multi-index.
4404         **dim_order_kwargs : Sequence of int or Hashable, optional
4405             The keyword arguments form of ``dim_order``.
4406             One of dim_order or dim_order_kwargs must be provided.
4407 
4408         Returns
4409         -------
4410         obj : Dataset
4411             Another dataset, with this dataset's data but replaced
4412             coordinates.
4413         """
4414         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
4415         variables = self._variables.copy()
4416         indexes = dict(self._indexes)
4417         new_indexes: dict[Hashable, Index] = {}
4418         new_variables: dict[Hashable, IndexVariable] = {}
4419 
4420         for dim, order in dim_order.items():
4421             index = self._indexes[dim]
4422 
4423             if not isinstance(index, PandasMultiIndex):
4424                 raise ValueError(f"coordinate {dim} has no MultiIndex")
4425 
4426             level_vars = {k: self._variables[k] for k in order}
4427             idx = index.reorder_levels(level_vars)
4428             idx_vars = idx.create_variables(level_vars)
4429             new_indexes.update({k: idx for k in idx_vars})
4430             new_variables.update(idx_vars)
4431 
4432         indexes = {k: v for k, v in self._indexes.items() if k not in new_indexes}
4433         indexes.update(new_indexes)
4434 
4435         variables = {k: v for k, v in self._variables.items() if k not in new_variables}
4436         variables.update(new_variables)
4437 
4438         return self._replace(variables, indexes=indexes)
4439 
4440     def _get_stack_index(
4441         self,
4442         dim,
4443         multi=False,
4444         create_index=False,
4445     ) -> tuple[Index | None, dict[Hashable, Variable]]:
4446         """Used by stack and unstack to get one pandas (multi-)index among
4447         the indexed coordinates along dimension `dim`.
4448 
4449         If exactly one index is found, return it with its corresponding
4450         coordinate variables(s), otherwise return None and an empty dict.
4451 
4452         If `create_index=True`, create a new index if none is found or raise
4453         an error if multiple indexes are found.
4454 
4455         """
4456         stack_index: Index | None = None
4457         stack_coords: dict[Hashable, Variable] = {}
4458 
4459         for name, index in self._indexes.items():
4460             var = self._variables[name]
4461             if (
4462                 var.ndim == 1
4463                 and var.dims[0] == dim
4464                 and (
4465                     # stack: must be a single coordinate index
4466                     not multi
4467                     and not self.xindexes.is_multi(name)
4468                     # unstack: must be an index that implements .unstack
4469                     or multi
4470                     and type(index).unstack is not Index.unstack
4471                 )
4472             ):
4473                 if stack_index is not None and index is not stack_index:
4474                     # more than one index found, stop
4475                     if create_index:
4476                         raise ValueError(
4477                             f"cannot stack dimension {dim!r} with `create_index=True` "
4478                             "and with more than one index found along that dimension"
4479                         )
4480                     return None, {}
4481                 stack_index = index
4482                 stack_coords[name] = var
4483 
4484         if create_index and stack_index is None:
4485             if dim in self._variables:
4486                 var = self._variables[dim]
4487             else:
4488                 _, _, var = _get_virtual_variable(self._variables, dim, self.dims)
4489             # dummy index (only `stack_coords` will be used to construct the multi-index)
4490             stack_index = PandasIndex([0], dim)
4491             stack_coords = {dim: var}
4492 
4493         return stack_index, stack_coords
4494 
4495     def _stack_once(
4496         self: T_Dataset,
4497         dims: Sequence[Hashable | ellipsis],
4498         new_dim: Hashable,
4499         index_cls: type[Index],
4500         create_index: bool | None = True,
4501     ) -> T_Dataset:
4502         if dims == ...:
4503             raise ValueError("Please use [...] for dims, rather than just ...")
4504         if ... in dims:
4505             dims = list(infix_dims(dims, self.dims))
4506 
4507         new_variables: dict[Hashable, Variable] = {}
4508         stacked_var_names: list[Hashable] = []
4509         drop_indexes: list[Hashable] = []
4510 
4511         for name, var in self.variables.items():
4512             if any(d in var.dims for d in dims):
4513                 add_dims = [d for d in dims if d not in var.dims]
4514                 vdims = list(var.dims) + add_dims
4515                 shape = [self.dims[d] for d in vdims]
4516                 exp_var = var.set_dims(vdims, shape)
4517                 stacked_var = exp_var.stack(**{new_dim: dims})
4518                 new_variables[name] = stacked_var
4519                 stacked_var_names.append(name)
4520             else:
4521                 new_variables[name] = var.copy(deep=False)
4522 
4523         # drop indexes of stacked coordinates (if any)
4524         for name in stacked_var_names:
4525             drop_indexes += list(self.xindexes.get_all_coords(name, errors="ignore"))
4526 
4527         new_indexes = {}
4528         new_coord_names = set(self._coord_names)
4529         if create_index or create_index is None:
4530             product_vars: dict[Any, Variable] = {}
4531             for dim in dims:
4532                 idx, idx_vars = self._get_stack_index(dim, create_index=create_index)
4533                 if idx is not None:
4534                     product_vars.update(idx_vars)
4535 
4536             if len(product_vars) == len(dims):
4537                 idx = index_cls.stack(product_vars, new_dim)
4538                 new_indexes[new_dim] = idx
4539                 new_indexes.update({k: idx for k in product_vars})
4540                 idx_vars = idx.create_variables(product_vars)
4541                 # keep consistent multi-index coordinate order
4542                 for k in idx_vars:
4543                     new_variables.pop(k, None)
4544                 new_variables.update(idx_vars)
4545                 new_coord_names.update(idx_vars)
4546 
4547         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4548         indexes.update(new_indexes)
4549 
4550         return self._replace_with_new_dims(
4551             new_variables, coord_names=new_coord_names, indexes=indexes
4552         )
4553 
4554     def stack(
4555         self: T_Dataset,
4556         dimensions: Mapping[Any, Sequence[Hashable | ellipsis]] | None = None,
4557         create_index: bool | None = True,
4558         index_cls: type[Index] = PandasMultiIndex,
4559         **dimensions_kwargs: Sequence[Hashable | ellipsis],
4560     ) -> T_Dataset:
4561         """
4562         Stack any number of existing dimensions into a single new dimension.
4563 
4564         New dimensions will be added at the end, and by default the corresponding
4565         coordinate variables will be combined into a MultiIndex.
4566 
4567         Parameters
4568         ----------
4569         dimensions : mapping of hashable to sequence of hashable
4570             Mapping of the form `new_name=(dim1, dim2, ...)`. Names of new
4571             dimensions, and the existing dimensions that they replace. An
4572             ellipsis (`...`) will be replaced by all unlisted dimensions.
4573             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
4574             all dimensions.
4575         create_index : bool or None, default: True
4576 
4577             - True: create a multi-index for each of the stacked dimensions.
4578             - False: don't create any index.
4579             - None. create a multi-index only if exactly one single (1-d) coordinate
4580               index is found for every dimension to stack.
4581 
4582         index_cls: Index-class, default: PandasMultiIndex
4583             Can be used to pass a custom multi-index type (must be an Xarray index that
4584             implements `.stack()`). By default, a pandas multi-index wrapper is used.
4585         **dimensions_kwargs
4586             The keyword arguments form of ``dimensions``.
4587             One of dimensions or dimensions_kwargs must be provided.
4588 
4589         Returns
4590         -------
4591         stacked : Dataset
4592             Dataset with stacked data.
4593 
4594         See Also
4595         --------
4596         Dataset.unstack
4597         """
4598         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
4599         result = self
4600         for new_dim, dims in dimensions.items():
4601             result = result._stack_once(dims, new_dim, index_cls, create_index)
4602         return result
4603 
4604     def to_stacked_array(
4605         self,
4606         new_dim: Hashable,
4607         sample_dims: Collection[Hashable],
4608         variable_dim: Hashable = "variable",
4609         name: Hashable | None = None,
4610     ) -> DataArray:
4611         """Combine variables of differing dimensionality into a DataArray
4612         without broadcasting.
4613 
4614         This method is similar to Dataset.to_array but does not broadcast the
4615         variables.
4616 
4617         Parameters
4618         ----------
4619         new_dim : hashable
4620             Name of the new stacked coordinate
4621         sample_dims : Collection of hashables
4622             List of dimensions that **will not** be stacked. Each array in the
4623             dataset must share these dimensions. For machine learning
4624             applications, these define the dimensions over which samples are
4625             drawn.
4626         variable_dim : hashable, default: "variable"
4627             Name of the level in the stacked coordinate which corresponds to
4628             the variables.
4629         name : hashable, optional
4630             Name of the new data array.
4631 
4632         Returns
4633         -------
4634         stacked : DataArray
4635             DataArray with the specified dimensions and data variables
4636             stacked together. The stacked coordinate is named ``new_dim``
4637             and represented by a MultiIndex object with a level containing the
4638             data variable names. The name of this level is controlled using
4639             the ``variable_dim`` argument.
4640 
4641         See Also
4642         --------
4643         Dataset.to_array
4644         Dataset.stack
4645         DataArray.to_unstacked_dataset
4646 
4647         Examples
4648         --------
4649         >>> data = xr.Dataset(
4650         ...     data_vars={
4651         ...         "a": (("x", "y"), [[0, 1, 2], [3, 4, 5]]),
4652         ...         "b": ("x", [6, 7]),
4653         ...     },
4654         ...     coords={"y": ["u", "v", "w"]},
4655         ... )
4656 
4657         >>> data
4658         <xarray.Dataset>
4659         Dimensions:  (x: 2, y: 3)
4660         Coordinates:
4661           * y        (y) <U1 'u' 'v' 'w'
4662         Dimensions without coordinates: x
4663         Data variables:
4664             a        (x, y) int64 0 1 2 3 4 5
4665             b        (x) int64 6 7
4666 
4667         >>> data.to_stacked_array("z", sample_dims=["x"])
4668         <xarray.DataArray 'a' (x: 2, z: 4)>
4669         array([[0, 1, 2, 6],
4670                [3, 4, 5, 7]])
4671         Coordinates:
4672           * z         (z) object MultiIndex
4673           * variable  (z) object 'a' 'a' 'a' 'b'
4674           * y         (z) object 'u' 'v' 'w' nan
4675         Dimensions without coordinates: x
4676 
4677         """
4678         from xarray.core.concat import concat
4679 
4680         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)
4681 
4682         for variable in self:
4683             dims = self[variable].dims
4684             dims_include_sample_dims = set(sample_dims) <= set(dims)
4685             if not dims_include_sample_dims:
4686                 raise ValueError(
4687                     "All variables in the dataset must contain the "
4688                     "dimensions {}.".format(dims)
4689                 )
4690 
4691         def ensure_stackable(val):
4692             assign_coords = {variable_dim: val.name}
4693             for dim in stacking_dims:
4694                 if dim not in val.dims:
4695                     assign_coords[dim] = None
4696 
4697             expand_dims = set(stacking_dims).difference(set(val.dims))
4698             expand_dims.add(variable_dim)
4699             # must be list for .expand_dims
4700             expand_dims = list(expand_dims)
4701 
4702             return (
4703                 val.assign_coords(**assign_coords)
4704                 .expand_dims(expand_dims)
4705                 .stack({new_dim: (variable_dim,) + stacking_dims})
4706             )
4707 
4708         # concatenate the arrays
4709         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]
4710         data_array = concat(stackable_vars, dim=new_dim)
4711 
4712         if name is not None:
4713             data_array.name = name
4714 
4715         return data_array
4716 
4717     def _unstack_once(
4718         self: T_Dataset,
4719         dim: Hashable,
4720         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4721         fill_value,
4722         sparse: bool = False,
4723     ) -> T_Dataset:
4724         index, index_vars = index_and_vars
4725         variables: dict[Hashable, Variable] = {}
4726         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4727 
4728         new_indexes, clean_index = index.unstack()
4729         indexes.update(new_indexes)
4730 
4731         for name, idx in new_indexes.items():
4732             variables.update(idx.create_variables(index_vars))
4733 
4734         for name, var in self.variables.items():
4735             if name not in index_vars:
4736                 if dim in var.dims:
4737                     if isinstance(fill_value, Mapping):
4738                         fill_value_ = fill_value[name]
4739                     else:
4740                         fill_value_ = fill_value
4741 
4742                     variables[name] = var._unstack_once(
4743                         index=clean_index,
4744                         dim=dim,
4745                         fill_value=fill_value_,
4746                         sparse=sparse,
4747                     )
4748                 else:
4749                     variables[name] = var
4750 
4751         coord_names = set(self._coord_names) - {dim} | set(new_indexes)
4752 
4753         return self._replace_with_new_dims(
4754             variables, coord_names=coord_names, indexes=indexes
4755         )
4756 
4757     def _unstack_full_reindex(
4758         self: T_Dataset,
4759         dim: Hashable,
4760         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4761         fill_value,
4762         sparse: bool,
4763     ) -> T_Dataset:
4764         index, index_vars = index_and_vars
4765         variables: dict[Hashable, Variable] = {}
4766         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4767 
4768         new_indexes, clean_index = index.unstack()
4769         indexes.update(new_indexes)
4770 
4771         new_index_variables = {}
4772         for name, idx in new_indexes.items():
4773             new_index_variables.update(idx.create_variables(index_vars))
4774 
4775         new_dim_sizes = {k: v.size for k, v in new_index_variables.items()}
4776         variables.update(new_index_variables)
4777 
4778         # take a shortcut in case the MultiIndex was not modified.
4779         full_idx = pd.MultiIndex.from_product(
4780             clean_index.levels, names=clean_index.names
4781         )
4782         if clean_index.equals(full_idx):
4783             obj = self
4784         else:
4785             # TODO: we may depreciate implicit re-indexing with a pandas.MultiIndex
4786             xr_full_idx = PandasMultiIndex(full_idx, dim)
4787             indexers = Indexes(
4788                 {k: xr_full_idx for k in index_vars},
4789                 xr_full_idx.create_variables(index_vars),
4790             )
4791             obj = self._reindex(
4792                 indexers, copy=False, fill_value=fill_value, sparse=sparse
4793             )
4794 
4795         for name, var in obj.variables.items():
4796             if name not in index_vars:
4797                 if dim in var.dims:
4798                     variables[name] = var.unstack({dim: new_dim_sizes})
4799                 else:
4800                     variables[name] = var
4801 
4802         coord_names = set(self._coord_names) - {dim} | set(new_dim_sizes)
4803 
4804         return self._replace_with_new_dims(
4805             variables, coord_names=coord_names, indexes=indexes
4806         )
4807 
4808     def unstack(
4809         self: T_Dataset,
4810         dim: Dims = None,
4811         fill_value: Any = xrdtypes.NA,
4812         sparse: bool = False,
4813     ) -> T_Dataset:
4814         """
4815         Unstack existing dimensions corresponding to MultiIndexes into
4816         multiple new dimensions.
4817 
4818         New dimensions will be added at the end.
4819 
4820         Parameters
4821         ----------
4822         dim : str, Iterable of Hashable or None, optional
4823             Dimension(s) over which to unstack. By default unstacks all
4824             MultiIndexes.
4825         fill_value : scalar or dict-like, default: nan
4826             value to be filled. If a dict-like, maps variable names to
4827             fill values. If not provided or if the dict-like does not
4828             contain all variables, the dtype's NA value will be used.
4829         sparse : bool, default: False
4830             use sparse-array if True
4831 
4832         Returns
4833         -------
4834         unstacked : Dataset
4835             Dataset with unstacked data.
4836 
4837         See Also
4838         --------
4839         Dataset.stack
4840         """
4841 
4842         if dim is None:
4843             dims = list(self.dims)
4844         else:
4845             if isinstance(dim, str) or not isinstance(dim, Iterable):
4846                 dims = [dim]
4847             else:
4848                 dims = list(dim)
4849 
4850             missing_dims = [d for d in dims if d not in self.dims]
4851             if missing_dims:
4852                 raise ValueError(
4853                     f"Dataset does not contain the dimensions: {missing_dims}"
4854                 )
4855 
4856         # each specified dimension must have exactly one multi-index
4857         stacked_indexes: dict[Any, tuple[Index, dict[Hashable, Variable]]] = {}
4858         for d in dims:
4859             idx, idx_vars = self._get_stack_index(d, multi=True)
4860             if idx is not None:
4861                 stacked_indexes[d] = idx, idx_vars
4862 
4863         if dim is None:
4864             dims = list(stacked_indexes)
4865         else:
4866             non_multi_dims = set(dims) - set(stacked_indexes)
4867             if non_multi_dims:
4868                 raise ValueError(
4869                     "cannot unstack dimensions that do not "
4870                     f"have exactly one multi-index: {tuple(non_multi_dims)}"
4871                 )
4872 
4873         result = self.copy(deep=False)
4874 
4875         # we want to avoid allocating an object-dtype ndarray for a MultiIndex,
4876         # so we can't just access self.variables[v].data for every variable.
4877         # We only check the non-index variables.
4878         # https://github.com/pydata/xarray/issues/5902
4879         nonindexes = [
4880             self.variables[k] for k in set(self.variables) - set(self._indexes)
4881         ]
4882         # Notes for each of these cases:
4883         # 1. Dask arrays don't support assignment by index, which the fast unstack
4884         #    function requires.
4885         #    https://github.com/pydata/xarray/pull/4746#issuecomment-753282125
4886         # 2. Sparse doesn't currently support (though we could special-case it)
4887         #    https://github.com/pydata/sparse/issues/422
4888         # 3. pint requires checking if it's a NumPy array until
4889         #    https://github.com/pydata/xarray/pull/4751 is resolved,
4890         #    Once that is resolved, explicitly exclude pint arrays.
4891         #    pint doesn't implement `np.full_like` in a way that's
4892         #    currently compatible.
4893         sparse_array_type = array_type("sparse")
4894         needs_full_reindex = any(
4895             is_duck_dask_array(v.data)
4896             or isinstance(v.data, sparse_array_type)
4897             or not isinstance(v.data, np.ndarray)
4898             for v in nonindexes
4899         )
4900 
4901         for d in dims:
4902             if needs_full_reindex:
4903                 result = result._unstack_full_reindex(
4904                     d, stacked_indexes[d], fill_value, sparse
4905                 )
4906             else:
4907                 result = result._unstack_once(d, stacked_indexes[d], fill_value, sparse)
4908         return result
4909 
4910     def update(self: T_Dataset, other: CoercibleMapping) -> T_Dataset:
4911         """Update this dataset's variables with those from another dataset.
4912 
4913         Just like :py:meth:`dict.update` this is a in-place operation.
4914         For a non-inplace version, see :py:meth:`Dataset.merge`.
4915 
4916         Parameters
4917         ----------
4918         other : Dataset or mapping
4919             Variables with which to update this dataset. One of:
4920 
4921             - Dataset
4922             - mapping {var name: DataArray}
4923             - mapping {var name: Variable}
4924             - mapping {var name: (dimension name, array-like)}
4925             - mapping {var name: (tuple of dimension names, array-like)}
4926 
4927         Returns
4928         -------
4929         updated : Dataset
4930             Updated dataset. Note that since the update is in-place this is the input
4931             dataset.
4932 
4933             It is deprecated since version 0.17 and scheduled to be removed in 0.21.
4934 
4935         Raises
4936         ------
4937         ValueError
4938             If any dimensions would have inconsistent sizes in the updated
4939             dataset.
4940 
4941         See Also
4942         --------
4943         Dataset.assign
4944         Dataset.merge
4945         """
4946         merge_result = dataset_update_method(self, other)
4947         return self._replace(inplace=True, **merge_result._asdict())
4948 
4949     def merge(
4950         self: T_Dataset,
4951         other: CoercibleMapping | DataArray,
4952         overwrite_vars: Hashable | Iterable[Hashable] = frozenset(),
4953         compat: CompatOptions = "no_conflicts",
4954         join: JoinOptions = "outer",
4955         fill_value: Any = xrdtypes.NA,
4956         combine_attrs: CombineAttrsOptions = "override",
4957     ) -> T_Dataset:
4958         """Merge the arrays of two datasets into a single dataset.
4959 
4960         This method generally does not allow for overriding data, with the
4961         exception of attributes, which are ignored on the second dataset.
4962         Variables with the same name are checked for conflicts via the equals
4963         or identical methods.
4964 
4965         Parameters
4966         ----------
4967         other : Dataset or mapping
4968             Dataset or variables to merge with this dataset.
4969         overwrite_vars : hashable or iterable of hashable, optional
4970             If provided, update variables of these name(s) without checking for
4971             conflicts in this dataset.
4972         compat : {"identical", "equals", "broadcast_equals", \
4973                   "no_conflicts", "override", "minimal"}, default: "no_conflicts"
4974             String indicating how to compare variables of the same name for
4975             potential conflicts:
4976 
4977             - 'identical': all values, dimensions and attributes must be the
4978               same.
4979             - 'equals': all values and dimensions must be the same.
4980             - 'broadcast_equals': all values must be equal when variables are
4981               broadcast against each other to ensure common dimensions.
4982             - 'no_conflicts': only values which are not null in both datasets
4983               must be equal. The returned dataset then contains the combination
4984               of all non-null values.
4985             - 'override': skip comparing and pick variable from first dataset
4986             - 'minimal': drop conflicting coordinates
4987 
4988         join : {"outer", "inner", "left", "right", "exact", "override"}, \
4989                default: "outer"
4990             Method for joining ``self`` and ``other`` along shared dimensions:
4991 
4992             - 'outer': use the union of the indexes
4993             - 'inner': use the intersection of the indexes
4994             - 'left': use indexes from ``self``
4995             - 'right': use indexes from ``other``
4996             - 'exact': error instead of aligning non-equal indexes
4997             - 'override': use indexes from ``self`` that are the same size
4998               as those of ``other`` in that dimension
4999 
5000         fill_value : scalar or dict-like, optional
5001             Value to use for newly missing values. If a dict-like, maps
5002             variable names (including coordinates) to fill values.
5003         combine_attrs : {"drop", "identical", "no_conflicts", "drop_conflicts", \
5004                          "override"} or callable, default: "override"
5005             A callable or a string indicating how to combine attrs of the objects being
5006             merged:
5007 
5008             - "drop": empty attrs on returned Dataset.
5009             - "identical": all attrs must be the same on every object.
5010             - "no_conflicts": attrs from all objects are combined, any that have
5011               the same name must also have the same value.
5012             - "drop_conflicts": attrs from all objects are combined, any that have
5013               the same name but different values are dropped.
5014             - "override": skip comparing and copy attrs from the first dataset to
5015               the result.
5016 
5017             If a callable, it must expect a sequence of ``attrs`` dicts and a context object
5018             as its only parameters.
5019 
5020         Returns
5021         -------
5022         merged : Dataset
5023             Merged dataset.
5024 
5025         Raises
5026         ------
5027         MergeError
5028             If any variables conflict (see ``compat``).
5029 
5030         See Also
5031         --------
5032         Dataset.update
5033         """
5034         from xarray.core.dataarray import DataArray
5035 
5036         other = other.to_dataset() if isinstance(other, DataArray) else other
5037         merge_result = dataset_merge_method(
5038             self,
5039             other,
5040             overwrite_vars=overwrite_vars,
5041             compat=compat,
5042             join=join,
5043             fill_value=fill_value,
5044             combine_attrs=combine_attrs,
5045         )
5046         return self._replace(**merge_result._asdict())
5047 
5048     def _assert_all_in_dataset(
5049         self, names: Iterable[Hashable], virtual_okay: bool = False
5050     ) -> None:
5051         bad_names = set(names) - set(self._variables)
5052         if virtual_okay:
5053             bad_names -= self.virtual_variables
5054         if bad_names:
5055             raise ValueError(
5056                 "One or more of the specified variables "
5057                 "cannot be found in this dataset"
5058             )
5059 
5060     def drop_vars(
5061         self: T_Dataset,
5062         names: Hashable | Iterable[Hashable],
5063         *,
5064         errors: ErrorOptions = "raise",
5065     ) -> T_Dataset:
5066         """Drop variables from this dataset.
5067 
5068         Parameters
5069         ----------
5070         names : hashable or iterable of hashable
5071             Name(s) of variables to drop.
5072         errors : {"raise", "ignore"}, default: "raise"
5073             If 'raise', raises a ValueError error if any of the variable
5074             passed are not in the dataset. If 'ignore', any given names that are in the
5075             dataset are dropped and no error is raised.
5076 
5077         Returns
5078         -------
5079         dropped : Dataset
5080 
5081         """
5082         # the Iterable check is required for mypy
5083         if is_scalar(names) or not isinstance(names, Iterable):
5084             names = {names}
5085         else:
5086             names = set(names)
5087         if errors == "raise":
5088             self._assert_all_in_dataset(names)
5089 
5090         # GH6505
5091         other_names = set()
5092         for var in names:
5093             maybe_midx = self._indexes.get(var, None)
5094             if isinstance(maybe_midx, PandasMultiIndex):
5095                 idx_coord_names = set(maybe_midx.index.names + [maybe_midx.dim])
5096                 idx_other_names = idx_coord_names - set(names)
5097                 other_names.update(idx_other_names)
5098         if other_names:
5099             names |= set(other_names)
5100             warnings.warn(
5101                 f"Deleting a single level of a MultiIndex is deprecated. Previously, this deleted all levels of a MultiIndex. "
5102                 f"Please also drop the following variables: {other_names!r} to avoid an error in the future.",
5103                 DeprecationWarning,
5104                 stacklevel=2,
5105             )
5106 
5107         assert_no_index_corrupted(self.xindexes, names)
5108 
5109         variables = {k: v for k, v in self._variables.items() if k not in names}
5110         coord_names = {k for k in self._coord_names if k in variables}
5111         indexes = {k: v for k, v in self._indexes.items() if k not in names}
5112         return self._replace_with_new_dims(
5113             variables, coord_names=coord_names, indexes=indexes
5114         )
5115 
5116     def drop_indexes(
5117         self: T_Dataset,
5118         coord_names: Hashable | Iterable[Hashable],
5119         *,
5120         errors: ErrorOptions = "raise",
5121     ) -> T_Dataset:
5122         """Drop the indexes assigned to the given coordinates.
5123 
5124         Parameters
5125         ----------
5126         coord_names : hashable or iterable of hashable
5127             Name(s) of the coordinate(s) for which to drop the index.
5128         errors : {"raise", "ignore"}, default: "raise"
5129             If 'raise', raises a ValueError error if any of the coordinates
5130             passed have no index or are not in the dataset.
5131             If 'ignore', no error is raised.
5132 
5133         Returns
5134         -------
5135         dropped : Dataset
5136             A new dataset with dropped indexes.
5137 
5138         """
5139         # the Iterable check is required for mypy
5140         if is_scalar(coord_names) or not isinstance(coord_names, Iterable):
5141             coord_names = {coord_names}
5142         else:
5143             coord_names = set(coord_names)
5144 
5145         if errors == "raise":
5146             invalid_coords = coord_names - self._coord_names
5147             if invalid_coords:
5148                 raise ValueError(f"those coordinates don't exist: {invalid_coords}")
5149 
5150             unindexed_coords = set(coord_names) - set(self._indexes)
5151             if unindexed_coords:
5152                 raise ValueError(
5153                     f"those coordinates do not have an index: {unindexed_coords}"
5154                 )
5155 
5156         assert_no_index_corrupted(self.xindexes, coord_names, action="remove index(es)")
5157 
5158         variables = {}
5159         for name, var in self._variables.items():
5160             if name in coord_names:
5161                 variables[name] = var.to_base_variable()
5162             else:
5163                 variables[name] = var
5164 
5165         indexes = {k: v for k, v in self._indexes.items() if k not in coord_names}
5166 
5167         return self._replace(variables=variables, indexes=indexes)
5168 
5169     def drop(
5170         self: T_Dataset,
5171         labels=None,
5172         dim=None,
5173         *,
5174         errors: ErrorOptions = "raise",
5175         **labels_kwargs,
5176     ) -> T_Dataset:
5177         """Backward compatible method based on `drop_vars` and `drop_sel`
5178 
5179         Using either `drop_vars` or `drop_sel` is encouraged
5180 
5181         See Also
5182         --------
5183         Dataset.drop_vars
5184         Dataset.drop_sel
5185         """
5186         if errors not in ["raise", "ignore"]:
5187             raise ValueError('errors must be either "raise" or "ignore"')
5188 
5189         if is_dict_like(labels) and not isinstance(labels, dict):
5190             warnings.warn(
5191                 "dropping coordinates using `drop` is be deprecated; use drop_vars.",
5192                 FutureWarning,
5193                 stacklevel=2,
5194             )
5195             return self.drop_vars(labels, errors=errors)
5196 
5197         if labels_kwargs or isinstance(labels, dict):
5198             if dim is not None:
5199                 raise ValueError("cannot specify dim and dict-like arguments.")
5200             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
5201 
5202         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):
5203             warnings.warn(
5204                 "dropping variables using `drop` will be deprecated; using drop_vars is encouraged.",
5205                 PendingDeprecationWarning,
5206                 stacklevel=2,
5207             )
5208             return self.drop_vars(labels, errors=errors)
5209         if dim is not None:
5210             warnings.warn(
5211                 "dropping labels using list-like labels is deprecated; using "
5212                 "dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).",
5213                 DeprecationWarning,
5214                 stacklevel=2,
5215             )
5216             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)
5217 
5218         warnings.warn(
5219             "dropping labels using `drop` will be deprecated; using drop_sel is encouraged.",
5220             PendingDeprecationWarning,
5221             stacklevel=2,
5222         )
5223         return self.drop_sel(labels, errors=errors)
5224 
5225     def drop_sel(
5226         self: T_Dataset, labels=None, *, errors: ErrorOptions = "raise", **labels_kwargs
5227     ) -> T_Dataset:
5228         """Drop index labels from this dataset.
5229 
5230         Parameters
5231         ----------
5232         labels : mapping of hashable to Any
5233             Index labels to drop
5234         errors : {"raise", "ignore"}, default: "raise"
5235             If 'raise', raises a ValueError error if
5236             any of the index labels passed are not
5237             in the dataset. If 'ignore', any given labels that are in the
5238             dataset are dropped and no error is raised.
5239         **labels_kwargs : {dim: label, ...}, optional
5240             The keyword arguments form of ``dim`` and ``labels``
5241 
5242         Returns
5243         -------
5244         dropped : Dataset
5245 
5246         Examples
5247         --------
5248         >>> data = np.arange(6).reshape(2, 3)
5249         >>> labels = ["a", "b", "c"]
5250         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
5251         >>> ds
5252         <xarray.Dataset>
5253         Dimensions:  (x: 2, y: 3)
5254         Coordinates:
5255           * y        (y) <U1 'a' 'b' 'c'
5256         Dimensions without coordinates: x
5257         Data variables:
5258             A        (x, y) int64 0 1 2 3 4 5
5259         >>> ds.drop_sel(y=["a", "c"])
5260         <xarray.Dataset>
5261         Dimensions:  (x: 2, y: 1)
5262         Coordinates:
5263           * y        (y) <U1 'b'
5264         Dimensions without coordinates: x
5265         Data variables:
5266             A        (x, y) int64 1 4
5267         >>> ds.drop_sel(y="b")
5268         <xarray.Dataset>
5269         Dimensions:  (x: 2, y: 2)
5270         Coordinates:
5271           * y        (y) <U1 'a' 'c'
5272         Dimensions without coordinates: x
5273         Data variables:
5274             A        (x, y) int64 0 2 3 5
5275         """
5276         if errors not in ["raise", "ignore"]:
5277             raise ValueError('errors must be either "raise" or "ignore"')
5278 
5279         labels = either_dict_or_kwargs(labels, labels_kwargs, "drop_sel")
5280 
5281         ds = self
5282         for dim, labels_for_dim in labels.items():
5283             # Don't cast to set, as it would harm performance when labels
5284             # is a large numpy array
5285             if utils.is_scalar(labels_for_dim):
5286                 labels_for_dim = [labels_for_dim]
5287             labels_for_dim = np.asarray(labels_for_dim)
5288             try:
5289                 index = self.get_index(dim)
5290             except KeyError:
5291                 raise ValueError(f"dimension {dim!r} does not have coordinate labels")
5292             new_index = index.drop(labels_for_dim, errors=errors)
5293             ds = ds.loc[{dim: new_index}]
5294         return ds
5295 
5296     def drop_isel(self: T_Dataset, indexers=None, **indexers_kwargs) -> T_Dataset:
5297         """Drop index positions from this Dataset.
5298 
5299         Parameters
5300         ----------
5301         indexers : mapping of hashable to Any
5302             Index locations to drop
5303         **indexers_kwargs : {dim: position, ...}, optional
5304             The keyword arguments form of ``dim`` and ``positions``
5305 
5306         Returns
5307         -------
5308         dropped : Dataset
5309 
5310         Raises
5311         ------
5312         IndexError
5313 
5314         Examples
5315         --------
5316         >>> data = np.arange(6).reshape(2, 3)
5317         >>> labels = ["a", "b", "c"]
5318         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
5319         >>> ds
5320         <xarray.Dataset>
5321         Dimensions:  (x: 2, y: 3)
5322         Coordinates:
5323           * y        (y) <U1 'a' 'b' 'c'
5324         Dimensions without coordinates: x
5325         Data variables:
5326             A        (x, y) int64 0 1 2 3 4 5
5327         >>> ds.drop_isel(y=[0, 2])
5328         <xarray.Dataset>
5329         Dimensions:  (x: 2, y: 1)
5330         Coordinates:
5331           * y        (y) <U1 'b'
5332         Dimensions without coordinates: x
5333         Data variables:
5334             A        (x, y) int64 1 4
5335         >>> ds.drop_isel(y=1)
5336         <xarray.Dataset>
5337         Dimensions:  (x: 2, y: 2)
5338         Coordinates:
5339           * y        (y) <U1 'a' 'c'
5340         Dimensions without coordinates: x
5341         Data variables:
5342             A        (x, y) int64 0 2 3 5
5343         """
5344 
5345         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "drop_isel")
5346 
5347         ds = self
5348         dimension_index = {}
5349         for dim, pos_for_dim in indexers.items():
5350             # Don't cast to set, as it would harm performance when labels
5351             # is a large numpy array
5352             if utils.is_scalar(pos_for_dim):
5353                 pos_for_dim = [pos_for_dim]
5354             pos_for_dim = np.asarray(pos_for_dim)
5355             index = self.get_index(dim)
5356             new_index = index.delete(pos_for_dim)
5357             dimension_index[dim] = new_index
5358         ds = ds.loc[dimension_index]
5359         return ds
5360 
5361     def drop_dims(
5362         self: T_Dataset,
5363         drop_dims: str | Iterable[Hashable],
5364         *,
5365         errors: ErrorOptions = "raise",
5366     ) -> T_Dataset:
5367         """Drop dimensions and associated variables from this dataset.
5368 
5369         Parameters
5370         ----------
5371         drop_dims : str or Iterable of Hashable
5372             Dimension or dimensions to drop.
5373         errors : {"raise", "ignore"}, default: "raise"
5374             If 'raise', raises a ValueError error if any of the
5375             dimensions passed are not in the dataset. If 'ignore', any given
5376             dimensions that are in the dataset are dropped and no error is raised.
5377 
5378         Returns
5379         -------
5380         obj : Dataset
5381             The dataset without the given dimensions (or any variables
5382             containing those dimensions).
5383         """
5384         if errors not in ["raise", "ignore"]:
5385             raise ValueError('errors must be either "raise" or "ignore"')
5386 
5387         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):
5388             drop_dims = {drop_dims}
5389         else:
5390             drop_dims = set(drop_dims)
5391 
5392         if errors == "raise":
5393             missing_dims = drop_dims - set(self.dims)
5394             if missing_dims:
5395                 raise ValueError(
5396                     f"Dataset does not contain the dimensions: {missing_dims}"
5397                 )
5398 
5399         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}
5400         return self.drop_vars(drop_vars)
5401 
5402     def transpose(
5403         self: T_Dataset,
5404         *dims: Hashable,
5405         missing_dims: ErrorOptionsWithWarn = "raise",
5406     ) -> T_Dataset:
5407         """Return a new Dataset object with all array dimensions transposed.
5408 
5409         Although the order of dimensions on each array will change, the dataset
5410         dimensions themselves will remain in fixed (sorted) order.
5411 
5412         Parameters
5413         ----------
5414         *dims : hashable, optional
5415             By default, reverse the dimensions on each array. Otherwise,
5416             reorder the dimensions to this order.
5417         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
5418             What to do if dimensions that should be selected from are not present in the
5419             Dataset:
5420             - "raise": raise an exception
5421             - "warn": raise a warning, and ignore the missing dimensions
5422             - "ignore": ignore the missing dimensions
5423 
5424         Returns
5425         -------
5426         transposed : Dataset
5427             Each array in the dataset (including) coordinates will be
5428             transposed to the given order.
5429 
5430         Notes
5431         -----
5432         This operation returns a view of each array's data. It is
5433         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
5434         -- the data will be fully loaded into memory.
5435 
5436         See Also
5437         --------
5438         numpy.transpose
5439         DataArray.transpose
5440         """
5441         # Raise error if list is passed as dims
5442         if (len(dims) > 0) and (isinstance(dims[0], list)):
5443             list_fix = [f"{repr(x)}" if isinstance(x, str) else f"{x}" for x in dims[0]]
5444             raise TypeError(
5445                 f'transpose requires dims to be passed as multiple arguments. Expected `{", ".join(list_fix)}`. Received `{dims[0]}` instead'
5446             )
5447 
5448         # Use infix_dims to check once for missing dimensions
5449         if len(dims) != 0:
5450             _ = list(infix_dims(dims, self.dims, missing_dims))
5451 
5452         ds = self.copy()
5453         for name, var in self._variables.items():
5454             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))
5455             ds._variables[name] = var.transpose(*var_dims)
5456         return ds
5457 
5458     def dropna(
5459         self: T_Dataset,
5460         dim: Hashable,
5461         how: Literal["any", "all"] = "any",
5462         thresh: int | None = None,
5463         subset: Iterable[Hashable] | None = None,
5464     ) -> T_Dataset:
5465         """Returns a new dataset with dropped labels for missing values along
5466         the provided dimension.
5467 
5468         Parameters
5469         ----------
5470         dim : hashable
5471             Dimension along which to drop missing values. Dropping along
5472             multiple dimensions simultaneously is not yet supported.
5473         how : {"any", "all"}, default: "any"
5474             - any : if any NA values are present, drop that label
5475             - all : if all values are NA, drop that label
5476 
5477         thresh : int or None, optional
5478             If supplied, require this many non-NA values.
5479         subset : iterable of hashable or None, optional
5480             Which variables to check for missing values. By default, all
5481             variables in the dataset are checked.
5482 
5483         Returns
5484         -------
5485         Dataset
5486         """
5487         # TODO: consider supporting multiple dimensions? Or not, given that
5488         # there are some ugly edge cases, e.g., pandas's dropna differs
5489         # depending on the order of the supplied axes.
5490 
5491         if dim not in self.dims:
5492             raise ValueError(f"{dim} must be a single dataset dimension")
5493 
5494         if subset is None:
5495             subset = iter(self.data_vars)
5496 
5497         count = np.zeros(self.dims[dim], dtype=np.int64)
5498         size = np.int_(0)  # for type checking
5499 
5500         for k in subset:
5501             array = self._variables[k]
5502             if dim in array.dims:
5503                 dims = [d for d in array.dims if d != dim]
5504                 count += np.asarray(array.count(dims))  # type: ignore[attr-defined]
5505                 size += math.prod([self.dims[d] for d in dims])
5506 
5507         if thresh is not None:
5508             mask = count >= thresh
5509         elif how == "any":
5510             mask = count == size
5511         elif how == "all":
5512             mask = count > 0
5513         elif how is not None:
5514             raise ValueError(f"invalid how option: {how}")
5515         else:
5516             raise TypeError("must specify how or thresh")
5517 
5518         return self.isel({dim: mask})
5519 
5520     def fillna(self: T_Dataset, value: Any) -> T_Dataset:
5521         """Fill missing values in this object.
5522 
5523         This operation follows the normal broadcasting and alignment rules that
5524         xarray uses for binary arithmetic, except the result is aligned to this
5525         object (``join='left'``) instead of aligned to the intersection of
5526         index coordinates (``join='inner'``).
5527 
5528         Parameters
5529         ----------
5530         value : scalar, ndarray, DataArray, dict or Dataset
5531             Used to fill all matching missing values in this dataset's data
5532             variables. Scalars, ndarrays or DataArrays arguments are used to
5533             fill all data with aligned coordinates (for DataArrays).
5534             Dictionaries or datasets match data variables and then align
5535             coordinates if necessary.
5536 
5537         Returns
5538         -------
5539         Dataset
5540 
5541         Examples
5542         --------
5543         >>> ds = xr.Dataset(
5544         ...     {
5545         ...         "A": ("x", [np.nan, 2, np.nan, 0]),
5546         ...         "B": ("x", [3, 4, np.nan, 1]),
5547         ...         "C": ("x", [np.nan, np.nan, np.nan, 5]),
5548         ...         "D": ("x", [np.nan, 3, np.nan, 4]),
5549         ...     },
5550         ...     coords={"x": [0, 1, 2, 3]},
5551         ... )
5552         >>> ds
5553         <xarray.Dataset>
5554         Dimensions:  (x: 4)
5555         Coordinates:
5556           * x        (x) int64 0 1 2 3
5557         Data variables:
5558             A        (x) float64 nan 2.0 nan 0.0
5559             B        (x) float64 3.0 4.0 nan 1.0
5560             C        (x) float64 nan nan nan 5.0
5561             D        (x) float64 nan 3.0 nan 4.0
5562 
5563         Replace all `NaN` values with 0s.
5564 
5565         >>> ds.fillna(0)
5566         <xarray.Dataset>
5567         Dimensions:  (x: 4)
5568         Coordinates:
5569           * x        (x) int64 0 1 2 3
5570         Data variables:
5571             A        (x) float64 0.0 2.0 0.0 0.0
5572             B        (x) float64 3.0 4.0 0.0 1.0
5573             C        (x) float64 0.0 0.0 0.0 5.0
5574             D        (x) float64 0.0 3.0 0.0 4.0
5575 
5576         Replace all `NaN` elements in column ‘A’, ‘B’, ‘C’, and ‘D’, with 0, 1, 2, and 3 respectively.
5577 
5578         >>> values = {"A": 0, "B": 1, "C": 2, "D": 3}
5579         >>> ds.fillna(value=values)
5580         <xarray.Dataset>
5581         Dimensions:  (x: 4)
5582         Coordinates:
5583           * x        (x) int64 0 1 2 3
5584         Data variables:
5585             A        (x) float64 0.0 2.0 0.0 0.0
5586             B        (x) float64 3.0 4.0 1.0 1.0
5587             C        (x) float64 2.0 2.0 2.0 5.0
5588             D        (x) float64 3.0 3.0 3.0 4.0
5589         """
5590         if utils.is_dict_like(value):
5591             value_keys = getattr(value, "data_vars", value).keys()
5592             if not set(value_keys) <= set(self.data_vars.keys()):
5593                 raise ValueError(
5594                     "all variables in the argument to `fillna` "
5595                     "must be contained in the original dataset"
5596                 )
5597         out = ops.fillna(self, value)
5598         return out
5599 
5600     def interpolate_na(
5601         self: T_Dataset,
5602         dim: Hashable | None = None,
5603         method: InterpOptions = "linear",
5604         limit: int | None = None,
5605         use_coordinate: bool | Hashable = True,
5606         max_gap: (
5607             int | float | str | pd.Timedelta | np.timedelta64 | datetime.timedelta
5608         ) = None,
5609         **kwargs: Any,
5610     ) -> T_Dataset:
5611         """Fill in NaNs by interpolating according to different methods.
5612 
5613         Parameters
5614         ----------
5615         dim : Hashable or None, optional
5616             Specifies the dimension along which to interpolate.
5617         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
5618             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
5619             String indicating which method to use for interpolation:
5620 
5621             - 'linear': linear interpolation. Additional keyword
5622               arguments are passed to :py:func:`numpy.interp`
5623             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
5624               are passed to :py:func:`scipy.interpolate.interp1d`. If
5625               ``method='polynomial'``, the ``order`` keyword argument must also be
5626               provided.
5627             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
5628               respective :py:class:`scipy.interpolate` classes.
5629 
5630         use_coordinate : bool or Hashable, default: True
5631             Specifies which index to use as the x values in the interpolation
5632             formulated as `y = f(x)`. If False, values are treated as if
5633             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
5634             used. If ``use_coordinate`` is a string, it specifies the name of a
5635             coordinate variariable to use as the index.
5636         limit : int, default: None
5637             Maximum number of consecutive NaNs to fill. Must be greater than 0
5638             or None for no limit. This filling is done regardless of the size of
5639             the gap in the data. To only interpolate over gaps less than a given length,
5640             see ``max_gap``.
5641         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
5642             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
5643             Use None for no limit. When interpolating along a datetime64 dimension
5644             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
5645 
5646             - a string that is valid input for pandas.to_timedelta
5647             - a :py:class:`numpy.timedelta64` object
5648             - a :py:class:`pandas.Timedelta` object
5649             - a :py:class:`datetime.timedelta` object
5650 
5651             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
5652             dimensions has not been implemented yet. Gap length is defined as the difference
5653             between coordinate values at the first data point after a gap and the last value
5654             before a gap. For gaps at the beginning (end), gap length is defined as the difference
5655             between coordinate values at the first (last) valid data point and the first (last) NaN.
5656             For example, consider::
5657 
5658                 <xarray.DataArray (x: 9)>
5659                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
5660                 Coordinates:
5661                   * x        (x) int64 0 1 2 3 4 5 6 7 8
5662 
5663             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
5664         **kwargs : dict, optional
5665             parameters passed verbatim to the underlying interpolation function
5666 
5667         Returns
5668         -------
5669         interpolated: Dataset
5670             Filled in Dataset.
5671 
5672         See Also
5673         --------
5674         numpy.interp
5675         scipy.interpolate
5676 
5677         Examples
5678         --------
5679         >>> ds = xr.Dataset(
5680         ...     {
5681         ...         "A": ("x", [np.nan, 2, 3, np.nan, 0]),
5682         ...         "B": ("x", [3, 4, np.nan, 1, 7]),
5683         ...         "C": ("x", [np.nan, np.nan, np.nan, 5, 0]),
5684         ...         "D": ("x", [np.nan, 3, np.nan, -1, 4]),
5685         ...     },
5686         ...     coords={"x": [0, 1, 2, 3, 4]},
5687         ... )
5688         >>> ds
5689         <xarray.Dataset>
5690         Dimensions:  (x: 5)
5691         Coordinates:
5692           * x        (x) int64 0 1 2 3 4
5693         Data variables:
5694             A        (x) float64 nan 2.0 3.0 nan 0.0
5695             B        (x) float64 3.0 4.0 nan 1.0 7.0
5696             C        (x) float64 nan nan nan 5.0 0.0
5697             D        (x) float64 nan 3.0 nan -1.0 4.0
5698 
5699         >>> ds.interpolate_na(dim="x", method="linear")
5700         <xarray.Dataset>
5701         Dimensions:  (x: 5)
5702         Coordinates:
5703           * x        (x) int64 0 1 2 3 4
5704         Data variables:
5705             A        (x) float64 nan 2.0 3.0 1.5 0.0
5706             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5707             C        (x) float64 nan nan nan 5.0 0.0
5708             D        (x) float64 nan 3.0 1.0 -1.0 4.0
5709 
5710         >>> ds.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
5711         <xarray.Dataset>
5712         Dimensions:  (x: 5)
5713         Coordinates:
5714           * x        (x) int64 0 1 2 3 4
5715         Data variables:
5716             A        (x) float64 1.0 2.0 3.0 1.5 0.0
5717             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5718             C        (x) float64 20.0 15.0 10.0 5.0 0.0
5719             D        (x) float64 5.0 3.0 1.0 -1.0 4.0
5720         """
5721         from xarray.core.missing import _apply_over_vars_with_dim, interp_na
5722 
5723         new = _apply_over_vars_with_dim(
5724             interp_na,
5725             self,
5726             dim=dim,
5727             method=method,
5728             limit=limit,
5729             use_coordinate=use_coordinate,
5730             max_gap=max_gap,
5731             **kwargs,
5732         )
5733         return new
5734 
5735     def ffill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5736         """Fill NaN values by propagating values forward
5737 
5738         *Requires bottleneck.*
5739 
5740         Parameters
5741         ----------
5742         dim : Hashable
5743             Specifies the dimension along which to propagate values when
5744             filling.
5745         limit : int or None, optional
5746             The maximum number of consecutive NaN values to forward fill. In
5747             other words, if there is a gap with more than this number of
5748             consecutive NaNs, it will only be partially filled. Must be greater
5749             than 0 or None for no limit. Must be None or greater than or equal
5750             to axis length if filling along chunked axes (dimensions).
5751 
5752         Returns
5753         -------
5754         Dataset
5755         """
5756         from xarray.core.missing import _apply_over_vars_with_dim, ffill
5757 
5758         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)
5759         return new
5760 
5761     def bfill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5762         """Fill NaN values by propagating values backward
5763 
5764         *Requires bottleneck.*
5765 
5766         Parameters
5767         ----------
5768         dim : Hashable
5769             Specifies the dimension along which to propagate values when
5770             filling.
5771         limit : int or None, optional
5772             The maximum number of consecutive NaN values to backward fill. In
5773             other words, if there is a gap with more than this number of
5774             consecutive NaNs, it will only be partially filled. Must be greater
5775             than 0 or None for no limit. Must be None or greater than or equal
5776             to axis length if filling along chunked axes (dimensions).
5777 
5778         Returns
5779         -------
5780         Dataset
5781         """
5782         from xarray.core.missing import _apply_over_vars_with_dim, bfill
5783 
5784         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)
5785         return new
5786 
5787     def combine_first(self: T_Dataset, other: T_Dataset) -> T_Dataset:
5788         """Combine two Datasets, default to data_vars of self.
5789 
5790         The new coordinates follow the normal broadcasting and alignment rules
5791         of ``join='outer'``.  Vacant cells in the expanded coordinates are
5792         filled with np.nan.
5793 
5794         Parameters
5795         ----------
5796         other : Dataset
5797             Used to fill all matching missing values in this array.
5798 
5799         Returns
5800         -------
5801         Dataset
5802         """
5803         out = ops.fillna(self, other, join="outer", dataset_join="outer")
5804         return out
5805 
5806     def reduce(
5807         self: T_Dataset,
5808         func: Callable,
5809         dim: Dims = None,
5810         *,
5811         keep_attrs: bool | None = None,
5812         keepdims: bool = False,
5813         numeric_only: bool = False,
5814         **kwargs: Any,
5815     ) -> T_Dataset:
5816         """Reduce this dataset by applying `func` along some dimension(s).
5817 
5818         Parameters
5819         ----------
5820         func : callable
5821             Function which can be called in the form
5822             `f(x, axis=axis, **kwargs)` to return the result of reducing an
5823             np.ndarray over an integer valued axis.
5824         dim : str, Iterable of Hashable or None, optional
5825             Dimension(s) over which to apply `func`. By default `func` is
5826             applied over all dimensions.
5827         keep_attrs : bool or None, optional
5828             If True, the dataset's attributes (`attrs`) will be copied from
5829             the original object to the new one.  If False (default), the new
5830             object will be returned without attributes.
5831         keepdims : bool, default: False
5832             If True, the dimensions which are reduced are left in the result
5833             as dimensions of size one. Coordinates that use these dimensions
5834             are removed.
5835         numeric_only : bool, default: False
5836             If True, only apply ``func`` to variables with a numeric dtype.
5837         **kwargs : Any
5838             Additional keyword arguments passed on to ``func``.
5839 
5840         Returns
5841         -------
5842         reduced : Dataset
5843             Dataset with this object's DataArrays replaced with new DataArrays
5844             of summarized data and the indicated dimension(s) removed.
5845         """
5846         if kwargs.get("axis", None) is not None:
5847             raise ValueError(
5848                 "passing 'axis' to Dataset reduce methods is ambiguous."
5849                 " Please use 'dim' instead."
5850             )
5851 
5852         if dim is None or dim is ...:
5853             dims = set(self.dims)
5854         elif isinstance(dim, str) or not isinstance(dim, Iterable):
5855             dims = {dim}
5856         else:
5857             dims = set(dim)
5858 
5859         missing_dimensions = [d for d in dims if d not in self.dims]
5860         if missing_dimensions:
5861             raise ValueError(
5862                 f"Dataset does not contain the dimensions: {missing_dimensions}"
5863             )
5864 
5865         if keep_attrs is None:
5866             keep_attrs = _get_keep_attrs(default=False)
5867 
5868         variables: dict[Hashable, Variable] = {}
5869         for name, var in self._variables.items():
5870             reduce_dims = [d for d in var.dims if d in dims]
5871             if name in self.coords:
5872                 if not reduce_dims:
5873                     variables[name] = var
5874             else:
5875                 if (
5876                     # Some reduction functions (e.g. std, var) need to run on variables
5877                     # that don't have the reduce dims: PR5393
5878                     not reduce_dims
5879                     or not numeric_only
5880                     or np.issubdtype(var.dtype, np.number)
5881                     or (var.dtype == np.bool_)
5882                 ):
5883                     # prefer to aggregate over axis=None rather than
5884                     # axis=(0, 1) if they will be equivalent, because
5885                     # the former is often more efficient
5886                     # keep single-element dims as list, to support Hashables
5887                     reduce_maybe_single = (
5888                         None
5889                         if len(reduce_dims) == var.ndim and var.ndim != 1
5890                         else reduce_dims
5891                     )
5892                     variables[name] = var.reduce(
5893                         func,
5894                         dim=reduce_maybe_single,
5895                         keep_attrs=keep_attrs,
5896                         keepdims=keepdims,
5897                         **kwargs,
5898                     )
5899 
5900         coord_names = {k for k in self.coords if k in variables}
5901         indexes = {k: v for k, v in self._indexes.items() if k in variables}
5902         attrs = self.attrs if keep_attrs else None
5903         return self._replace_with_new_dims(
5904             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
5905         )
5906 
5907     def map(
5908         self: T_Dataset,
5909         func: Callable,
5910         keep_attrs: bool | None = None,
5911         args: Iterable[Any] = (),
5912         **kwargs: Any,
5913     ) -> T_Dataset:
5914         """Apply a function to each data variable in this dataset
5915 
5916         Parameters
5917         ----------
5918         func : callable
5919             Function which can be called in the form `func(x, *args, **kwargs)`
5920             to transform each DataArray `x` in this dataset into another
5921             DataArray.
5922         keep_attrs : bool or None, optional
5923             If True, both the dataset's and variables' attributes (`attrs`) will be
5924             copied from the original objects to the new ones. If False, the new dataset
5925             and variables will be returned without copying the attributes.
5926         args : iterable, optional
5927             Positional arguments passed on to `func`.
5928         **kwargs : Any
5929             Keyword arguments passed on to `func`.
5930 
5931         Returns
5932         -------
5933         applied : Dataset
5934             Resulting dataset from applying ``func`` to each data variable.
5935 
5936         Examples
5937         --------
5938         >>> da = xr.DataArray(np.random.randn(2, 3))
5939         >>> ds = xr.Dataset({"foo": da, "bar": ("x", [-1, 2])})
5940         >>> ds
5941         <xarray.Dataset>
5942         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5943         Dimensions without coordinates: dim_0, dim_1, x
5944         Data variables:
5945             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
5946             bar      (x) int64 -1 2
5947         >>> ds.map(np.fabs)
5948         <xarray.Dataset>
5949         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5950         Dimensions without coordinates: dim_0, dim_1, x
5951         Data variables:
5952             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 0.9773
5953             bar      (x) float64 1.0 2.0
5954         """
5955         if keep_attrs is None:
5956             keep_attrs = _get_keep_attrs(default=False)
5957         variables = {
5958             k: maybe_wrap_array(v, func(v, *args, **kwargs))
5959             for k, v in self.data_vars.items()
5960         }
5961         if keep_attrs:
5962             for k, v in variables.items():
5963                 v._copy_attrs_from(self.data_vars[k])
5964         attrs = self.attrs if keep_attrs else None
5965         return type(self)(variables, attrs=attrs)
5966 
5967     def apply(
5968         self: T_Dataset,
5969         func: Callable,
5970         keep_attrs: bool | None = None,
5971         args: Iterable[Any] = (),
5972         **kwargs: Any,
5973     ) -> T_Dataset:
5974         """
5975         Backward compatible implementation of ``map``
5976 
5977         See Also
5978         --------
5979         Dataset.map
5980         """
5981         warnings.warn(
5982             "Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged",
5983             PendingDeprecationWarning,
5984             stacklevel=2,
5985         )
5986         return self.map(func, keep_attrs, args, **kwargs)
5987 
5988     def assign(
5989         self: T_Dataset,
5990         variables: Mapping[Any, Any] | None = None,
5991         **variables_kwargs: Any,
5992     ) -> T_Dataset:
5993         """Assign new data variables to a Dataset, returning a new object
5994         with all the original variables in addition to the new ones.
5995 
5996         Parameters
5997         ----------
5998         variables : mapping of hashable to Any
5999             Mapping from variables names to the new values. If the new values
6000             are callable, they are computed on the Dataset and assigned to new
6001             data variables. If the values are not callable, (e.g. a DataArray,
6002             scalar, or array), they are simply assigned.
6003         **variables_kwargs
6004             The keyword arguments form of ``variables``.
6005             One of variables or variables_kwargs must be provided.
6006 
6007         Returns
6008         -------
6009         ds : Dataset
6010             A new Dataset with the new variables in addition to all the
6011             existing variables.
6012 
6013         Notes
6014         -----
6015         Since ``kwargs`` is a dictionary, the order of your arguments may not
6016         be preserved, and so the order of the new variables is not well
6017         defined. Assigning multiple variables within the same ``assign`` is
6018         possible, but you cannot reference other variables created within the
6019         same ``assign`` call.
6020 
6021         See Also
6022         --------
6023         pandas.DataFrame.assign
6024 
6025         Examples
6026         --------
6027         >>> x = xr.Dataset(
6028         ...     {
6029         ...         "temperature_c": (
6030         ...             ("lat", "lon"),
6031         ...             20 * np.random.rand(4).reshape(2, 2),
6032         ...         ),
6033         ...         "precipitation": (("lat", "lon"), np.random.rand(4).reshape(2, 2)),
6034         ...     },
6035         ...     coords={"lat": [10, 20], "lon": [150, 160]},
6036         ... )
6037         >>> x
6038         <xarray.Dataset>
6039         Dimensions:        (lat: 2, lon: 2)
6040         Coordinates:
6041           * lat            (lat) int64 10 20
6042           * lon            (lon) int64 150 160
6043         Data variables:
6044             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6045             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6046 
6047         Where the value is a callable, evaluated on dataset:
6048 
6049         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)
6050         <xarray.Dataset>
6051         Dimensions:        (lat: 2, lon: 2)
6052         Coordinates:
6053           * lat            (lat) int64 10 20
6054           * lon            (lon) int64 150 160
6055         Data variables:
6056             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6057             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6058             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
6059 
6060         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:
6061 
6062         >>> x.assign(temperature_f=x["temperature_c"] * 9 / 5 + 32)
6063         <xarray.Dataset>
6064         Dimensions:        (lat: 2, lon: 2)
6065         Coordinates:
6066           * lat            (lat) int64 10 20
6067           * lon            (lon) int64 150 160
6068         Data variables:
6069             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6070             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6071             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
6072 
6073         """
6074         variables = either_dict_or_kwargs(variables, variables_kwargs, "assign")
6075         data = self.copy()
6076         # do all calculations first...
6077         results: CoercibleMapping = data._calc_assign_results(variables)
6078         data.coords._maybe_drop_multiindex_coords(set(results.keys()))
6079         # ... and then assign
6080         data.update(results)
6081         return data
6082 
6083     def to_array(
6084         self, dim: Hashable = "variable", name: Hashable | None = None
6085     ) -> DataArray:
6086         """Convert this dataset into an xarray.DataArray
6087 
6088         The data variables of this dataset will be broadcast against each other
6089         and stacked along the first axis of the new array. All coordinates of
6090         this dataset will remain coordinates.
6091 
6092         Parameters
6093         ----------
6094         dim : Hashable, default: "variable"
6095             Name of the new dimension.
6096         name : Hashable or None, optional
6097             Name of the new data array.
6098 
6099         Returns
6100         -------
6101         array : xarray.DataArray
6102         """
6103         from xarray.core.dataarray import DataArray
6104 
6105         data_vars = [self.variables[k] for k in self.data_vars]
6106         broadcast_vars = broadcast_variables(*data_vars)
6107         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)
6108 
6109         dims = (dim,) + broadcast_vars[0].dims
6110         variable = Variable(dims, data, self.attrs, fastpath=True)
6111 
6112         coords = {k: v.variable for k, v in self.coords.items()}
6113         indexes = filter_indexes_from_coords(self._indexes, set(coords))
6114         new_dim_index = PandasIndex(list(self.data_vars), dim)
6115         indexes[dim] = new_dim_index
6116         coords.update(new_dim_index.create_variables())
6117 
6118         return DataArray._construct_direct(variable, coords, name, indexes)
6119 
6120     def _normalize_dim_order(
6121         self, dim_order: Sequence[Hashable] | None = None
6122     ) -> dict[Hashable, int]:
6123         """
6124         Check the validity of the provided dimensions if any and return the mapping
6125         between dimension name and their size.
6126 
6127         Parameters
6128         ----------
6129         dim_order: Sequence of Hashable or None, optional
6130             Dimension order to validate (default to the alphabetical order if None).
6131 
6132         Returns
6133         -------
6134         result : dict[Hashable, int]
6135             Validated dimensions mapping.
6136 
6137         """
6138         if dim_order is None:
6139             dim_order = list(self.dims)
6140         elif set(dim_order) != set(self.dims):
6141             raise ValueError(
6142                 "dim_order {} does not match the set of dimensions of this "
6143                 "Dataset: {}".format(dim_order, list(self.dims))
6144             )
6145 
6146         ordered_dims = {k: self.dims[k] for k in dim_order}
6147 
6148         return ordered_dims
6149 
6150     def to_pandas(self) -> pd.Series | pd.DataFrame:
6151         """Convert this dataset into a pandas object without changing the number of dimensions.
6152 
6153         The type of the returned object depends on the number of Dataset
6154         dimensions:
6155 
6156         * 0D -> `pandas.Series`
6157         * 1D -> `pandas.DataFrame`
6158 
6159         Only works for Datasets with 1 or fewer dimensions.
6160         """
6161         if len(self.dims) == 0:
6162             return pd.Series({k: v.item() for k, v in self.items()})
6163         if len(self.dims) == 1:
6164             return self.to_dataframe()
6165         raise ValueError(
6166             "cannot convert Datasets with %s dimensions into "
6167             "pandas objects without changing the number of dimensions. "
6168             "Please use Dataset.to_dataframe() instead." % len(self.dims)
6169         )
6170 
6171     def _to_dataframe(self, ordered_dims: Mapping[Any, int]):
6172         columns = [k for k in self.variables if k not in self.dims]
6173         data = [
6174             self._variables[k].set_dims(ordered_dims).values.reshape(-1)
6175             for k in columns
6176         ]
6177         index = self.coords.to_index([*ordered_dims])
6178         return pd.DataFrame(dict(zip(columns, data)), index=index)
6179 
6180     def to_dataframe(self, dim_order: Sequence[Hashable] | None = None) -> pd.DataFrame:
6181         """Convert this dataset into a pandas.DataFrame.
6182 
6183         Non-index variables in this dataset form the columns of the
6184         DataFrame. The DataFrame is indexed by the Cartesian product of
6185         this dataset's indices.
6186 
6187         Parameters
6188         ----------
6189         dim_order: Sequence of Hashable or None, optional
6190             Hierarchical dimension order for the resulting dataframe. All
6191             arrays are transposed to this order and then written out as flat
6192             vectors in contiguous order, so the last dimension in this list
6193             will be contiguous in the resulting DataFrame. This has a major
6194             influence on which operations are efficient on the resulting
6195             dataframe.
6196 
6197             If provided, must include all dimensions of this dataset. By
6198             default, dimensions are sorted alphabetically.
6199 
6200         Returns
6201         -------
6202         result : DataFrame
6203             Dataset as a pandas DataFrame.
6204 
6205         """
6206 
6207         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
6208 
6209         return self._to_dataframe(ordered_dims=ordered_dims)
6210 
6211     def _set_sparse_data_from_dataframe(
6212         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
6213     ) -> None:
6214         from sparse import COO
6215 
6216         if isinstance(idx, pd.MultiIndex):
6217             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)
6218             is_sorted = idx.is_monotonic_increasing
6219             shape = tuple(lev.size for lev in idx.levels)
6220         else:
6221             coords = np.arange(idx.size).reshape(1, -1)
6222             is_sorted = True
6223             shape = (idx.size,)
6224 
6225         for name, values in arrays:
6226             # In virtually all real use cases, the sparse array will now have
6227             # missing values and needs a fill_value. For consistency, don't
6228             # special case the rare exceptions (e.g., dtype=int without a
6229             # MultiIndex).
6230             dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
6231             values = np.asarray(values, dtype=dtype)
6232 
6233             data = COO(
6234                 coords,
6235                 values,
6236                 shape,
6237                 has_duplicates=False,
6238                 sorted=is_sorted,
6239                 fill_value=fill_value,
6240             )
6241             self[name] = (dims, data)
6242 
6243     def _set_numpy_data_from_dataframe(
6244         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
6245     ) -> None:
6246         if not isinstance(idx, pd.MultiIndex):
6247             for name, values in arrays:
6248                 self[name] = (dims, values)
6249             return
6250 
6251         # NB: similar, more general logic, now exists in
6252         # variable.unstack_once; we could consider combining them at some
6253         # point.
6254 
6255         shape = tuple(lev.size for lev in idx.levels)
6256         indexer = tuple(idx.codes)
6257 
6258         # We already verified that the MultiIndex has all unique values, so
6259         # there are missing values if and only if the size of output arrays is
6260         # larger that the index.
6261         missing_values = math.prod(shape) > idx.shape[0]
6262 
6263         for name, values in arrays:
6264             # NumPy indexing is much faster than using DataFrame.reindex() to
6265             # fill in missing values:
6266             # https://stackoverflow.com/a/35049899/809705
6267             if missing_values:
6268                 dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
6269                 data = np.full(shape, fill_value, dtype)
6270             else:
6271                 # If there are no missing values, keep the existing dtype
6272                 # instead of promoting to support NA, e.g., keep integer
6273                 # columns as integers.
6274                 # TODO: consider removing this special case, which doesn't
6275                 # exist for sparse=True.
6276                 data = np.zeros(shape, values.dtype)
6277             data[indexer] = values
6278             self[name] = (dims, data)
6279 
6280     @classmethod
6281     def from_dataframe(
6282         cls: type[T_Dataset], dataframe: pd.DataFrame, sparse: bool = False
6283     ) -> T_Dataset:
6284         """Convert a pandas.DataFrame into an xarray.Dataset
6285 
6286         Each column will be converted into an independent variable in the
6287         Dataset. If the dataframe's index is a MultiIndex, it will be expanded
6288         into a tensor product of one-dimensional indices (filling in missing
6289         values with NaN). This method will produce a Dataset very similar to
6290         that on which the 'to_dataframe' method was called, except with
6291         possibly redundant dimensions (since all dataset variables will have
6292         the same dimensionality)
6293 
6294         Parameters
6295         ----------
6296         dataframe : DataFrame
6297             DataFrame from which to copy data and indices.
6298         sparse : bool, default: False
6299             If true, create a sparse arrays instead of dense numpy arrays. This
6300             can potentially save a large amount of memory if the DataFrame has
6301             a MultiIndex. Requires the sparse package (sparse.pydata.org).
6302 
6303         Returns
6304         -------
6305         New Dataset.
6306 
6307         See Also
6308         --------
6309         xarray.DataArray.from_series
6310         pandas.DataFrame.to_xarray
6311         """
6312         # TODO: Add an option to remove dimensions along which the variables
6313         # are constant, to enable consistent serialization to/from a dataframe,
6314         # even if some variables have different dimensionality.
6315 
6316         if not dataframe.columns.is_unique:
6317             raise ValueError("cannot convert DataFrame with non-unique columns")
6318 
6319         idx = remove_unused_levels_categories(dataframe.index)
6320 
6321         if isinstance(idx, pd.MultiIndex) and not idx.is_unique:
6322             raise ValueError(
6323                 "cannot convert a DataFrame with a non-unique MultiIndex into xarray"
6324             )
6325 
6326         # Cast to a NumPy array first, in case the Series is a pandas Extension
6327         # array (which doesn't have a valid NumPy dtype)
6328         # TODO: allow users to control how this casting happens, e.g., by
6329         # forwarding arguments to pandas.Series.to_numpy?
6330         arrays = [(k, np.asarray(v)) for k, v in dataframe.items()]
6331 
6332         indexes: dict[Hashable, Index] = {}
6333         index_vars: dict[Hashable, Variable] = {}
6334 
6335         if isinstance(idx, pd.MultiIndex):
6336             dims = tuple(
6337                 name if name is not None else "level_%i" % n
6338                 for n, name in enumerate(idx.names)
6339             )
6340             for dim, lev in zip(dims, idx.levels):
6341                 xr_idx = PandasIndex(lev, dim)
6342                 indexes[dim] = xr_idx
6343                 index_vars.update(xr_idx.create_variables())
6344         else:
6345             index_name = idx.name if idx.name is not None else "index"
6346             dims = (index_name,)
6347             xr_idx = PandasIndex(idx, index_name)
6348             indexes[index_name] = xr_idx
6349             index_vars.update(xr_idx.create_variables())
6350 
6351         obj = cls._construct_direct(index_vars, set(index_vars), indexes=indexes)
6352 
6353         if sparse:
6354             obj._set_sparse_data_from_dataframe(idx, arrays, dims)
6355         else:
6356             obj._set_numpy_data_from_dataframe(idx, arrays, dims)
6357         return obj
6358 
6359     def to_dask_dataframe(
6360         self, dim_order: Sequence[Hashable] | None = None, set_index: bool = False
6361     ) -> DaskDataFrame:
6362         """
6363         Convert this dataset into a dask.dataframe.DataFrame.
6364 
6365         The dimensions, coordinates and data variables in this dataset form
6366         the columns of the DataFrame.
6367 
6368         Parameters
6369         ----------
6370         dim_order : list, optional
6371             Hierarchical dimension order for the resulting dataframe. All
6372             arrays are transposed to this order and then written out as flat
6373             vectors in contiguous order, so the last dimension in this list
6374             will be contiguous in the resulting DataFrame. This has a major
6375             influence on which operations are efficient on the resulting dask
6376             dataframe.
6377 
6378             If provided, must include all dimensions of this dataset. By
6379             default, dimensions are sorted alphabetically.
6380         set_index : bool, default: False
6381             If set_index=True, the dask DataFrame is indexed by this dataset's
6382             coordinate. Since dask DataFrames do not support multi-indexes,
6383             set_index only works if the dataset only contains one dimension.
6384 
6385         Returns
6386         -------
6387         dask.dataframe.DataFrame
6388         """
6389 
6390         import dask.array as da
6391         import dask.dataframe as dd
6392 
6393         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
6394 
6395         columns = list(ordered_dims)
6396         columns.extend(k for k in self.coords if k not in self.dims)
6397         columns.extend(self.data_vars)
6398 
6399         series_list = []
6400         for name in columns:
6401             try:
6402                 var = self.variables[name]
6403             except KeyError:
6404                 # dimension without a matching coordinate
6405                 size = self.dims[name]
6406                 data = da.arange(size, chunks=size, dtype=np.int64)
6407                 var = Variable((name,), data)
6408 
6409             # IndexVariable objects have a dummy .chunk() method
6410             if isinstance(var, IndexVariable):
6411                 var = var.to_base_variable()
6412 
6413             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data
6414             series = dd.from_array(dask_array.reshape(-1), columns=[name])
6415             series_list.append(series)
6416 
6417         df = dd.concat(series_list, axis=1)
6418 
6419         if set_index:
6420             dim_order = [*ordered_dims]
6421 
6422             if len(dim_order) == 1:
6423                 (dim,) = dim_order
6424                 df = df.set_index(dim)
6425             else:
6426                 # triggers an error about multi-indexes, even if only one
6427                 # dimension is passed
6428                 df = df.set_index(dim_order)
6429 
6430         return df
6431 
6432     def to_dict(self, data: bool = True, encoding: bool = False) -> dict[str, Any]:
6433         """
6434         Convert this dataset to a dictionary following xarray naming
6435         conventions.
6436 
6437         Converts all variables and attributes to native Python objects
6438         Useful for converting to json. To avoid datetime incompatibility
6439         use decode_times=False kwarg in xarrray.open_dataset.
6440 
6441         Parameters
6442         ----------
6443         data : bool, default: True
6444             Whether to include the actual data in the dictionary. When set to
6445             False, returns just the schema.
6446         encoding : bool, default: False
6447             Whether to include the Dataset's encoding in the dictionary.
6448 
6449         Returns
6450         -------
6451         d : dict
6452             Dict with keys: "coords", "attrs", "dims", "data_vars" and optionally
6453             "encoding".
6454 
6455         See Also
6456         --------
6457         Dataset.from_dict
6458         DataArray.to_dict
6459         """
6460         d: dict = {
6461             "coords": {},
6462             "attrs": decode_numpy_dict_values(self.attrs),
6463             "dims": dict(self.dims),
6464             "data_vars": {},
6465         }
6466         for k in self.coords:
6467             d["coords"].update(
6468                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6469             )
6470         for k in self.data_vars:
6471             d["data_vars"].update(
6472                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6473             )
6474         if encoding:
6475             d["encoding"] = dict(self.encoding)
6476         return d
6477 
6478     @classmethod
6479     def from_dict(cls: type[T_Dataset], d: Mapping[Any, Any]) -> T_Dataset:
6480         """Convert a dictionary into an xarray.Dataset.
6481 
6482         Parameters
6483         ----------
6484         d : dict-like
6485             Mapping with a minimum structure of
6486                 ``{"var_0": {"dims": [..], "data": [..]}, \
6487                             ...}``
6488 
6489         Returns
6490         -------
6491         obj : Dataset
6492 
6493         See also
6494         --------
6495         Dataset.to_dict
6496         DataArray.from_dict
6497 
6498         Examples
6499         --------
6500         >>> d = {
6501         ...     "t": {"dims": ("t"), "data": [0, 1, 2]},
6502         ...     "a": {"dims": ("t"), "data": ["a", "b", "c"]},
6503         ...     "b": {"dims": ("t"), "data": [10, 20, 30]},
6504         ... }
6505         >>> ds = xr.Dataset.from_dict(d)
6506         >>> ds
6507         <xarray.Dataset>
6508         Dimensions:  (t: 3)
6509         Coordinates:
6510           * t        (t) int64 0 1 2
6511         Data variables:
6512             a        (t) <U1 'a' 'b' 'c'
6513             b        (t) int64 10 20 30
6514 
6515         >>> d = {
6516         ...     "coords": {
6517         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
6518         ...     },
6519         ...     "attrs": {"title": "air temperature"},
6520         ...     "dims": "t",
6521         ...     "data_vars": {
6522         ...         "a": {"dims": "t", "data": [10, 20, 30]},
6523         ...         "b": {"dims": "t", "data": ["a", "b", "c"]},
6524         ...     },
6525         ... }
6526         >>> ds = xr.Dataset.from_dict(d)
6527         >>> ds
6528         <xarray.Dataset>
6529         Dimensions:  (t: 3)
6530         Coordinates:
6531           * t        (t) int64 0 1 2
6532         Data variables:
6533             a        (t) int64 10 20 30
6534             b        (t) <U1 'a' 'b' 'c'
6535         Attributes:
6536             title:    air temperature
6537 
6538         """
6539 
6540         variables: Iterable[tuple[Hashable, Any]]
6541         if not {"coords", "data_vars"}.issubset(set(d)):
6542             variables = d.items()
6543         else:
6544             import itertools
6545 
6546             variables = itertools.chain(
6547                 d.get("coords", {}).items(), d.get("data_vars", {}).items()
6548             )
6549         try:
6550             variable_dict = {
6551                 k: (v["dims"], v["data"], v.get("attrs")) for k, v in variables
6552             }
6553         except KeyError as e:
6554             raise ValueError(
6555                 "cannot convert dict without the key "
6556                 "'{dims_data}'".format(dims_data=str(e.args[0]))
6557             )
6558         obj = cls(variable_dict)
6559 
6560         # what if coords aren't dims?
6561         coords = set(d.get("coords", {})) - set(d.get("dims", {}))
6562         obj = obj.set_coords(coords)
6563 
6564         obj.attrs.update(d.get("attrs", {}))
6565         obj.encoding.update(d.get("encoding", {}))
6566 
6567         return obj
6568 
6569     def _unary_op(self: T_Dataset, f, *args, **kwargs) -> T_Dataset:
6570         variables = {}
6571         keep_attrs = kwargs.pop("keep_attrs", None)
6572         if keep_attrs is None:
6573             keep_attrs = _get_keep_attrs(default=True)
6574         for k, v in self._variables.items():
6575             if k in self._coord_names:
6576                 variables[k] = v
6577             else:
6578                 variables[k] = f(v, *args, **kwargs)
6579                 if keep_attrs:
6580                     variables[k].attrs = v._attrs
6581         attrs = self._attrs if keep_attrs else None
6582         return self._replace_with_new_dims(variables, attrs=attrs)
6583 
6584     def _binary_op(self, other, f, reflexive=False, join=None) -> Dataset:
6585         from xarray.core.dataarray import DataArray
6586         from xarray.core.groupby import GroupBy
6587 
6588         if isinstance(other, GroupBy):
6589             return NotImplemented
6590         align_type = OPTIONS["arithmetic_join"] if join is None else join
6591         if isinstance(other, (DataArray, Dataset)):
6592             self, other = align(self, other, join=align_type, copy=False)  # type: ignore[assignment]
6593         g = f if not reflexive else lambda x, y: f(y, x)
6594         ds = self._calculate_binary_op(g, other, join=align_type)
6595         return ds
6596 
6597     def _inplace_binary_op(self: T_Dataset, other, f) -> T_Dataset:
6598         from xarray.core.dataarray import DataArray
6599         from xarray.core.groupby import GroupBy
6600 
6601         if isinstance(other, GroupBy):
6602             raise TypeError(
6603                 "in-place operations between a Dataset and "
6604                 "a grouped object are not permitted"
6605             )
6606         # we don't actually modify arrays in-place with in-place Dataset
6607         # arithmetic -- this lets us automatically align things
6608         if isinstance(other, (DataArray, Dataset)):
6609             other = other.reindex_like(self, copy=False)
6610         g = ops.inplace_to_noninplace_op(f)
6611         ds = self._calculate_binary_op(g, other, inplace=True)
6612         self._replace_with_new_dims(
6613             ds._variables,
6614             ds._coord_names,
6615             attrs=ds._attrs,
6616             indexes=ds._indexes,
6617             inplace=True,
6618         )
6619         return self
6620 
6621     def _calculate_binary_op(
6622         self, f, other, join="inner", inplace: bool = False
6623     ) -> Dataset:
6624         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):
6625             if inplace and set(lhs_data_vars) != set(rhs_data_vars):
6626                 raise ValueError(
6627                     "datasets must have the same data variables "
6628                     f"for in-place arithmetic operations: {list(lhs_data_vars)}, {list(rhs_data_vars)}"
6629                 )
6630 
6631             dest_vars = {}
6632 
6633             for k in lhs_data_vars:
6634                 if k in rhs_data_vars:
6635                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])
6636                 elif join in ["left", "outer"]:
6637                     dest_vars[k] = f(lhs_vars[k], np.nan)
6638             for k in rhs_data_vars:
6639                 if k not in dest_vars and join in ["right", "outer"]:
6640                     dest_vars[k] = f(rhs_vars[k], np.nan)
6641             return dest_vars
6642 
6643         if utils.is_dict_like(other) and not isinstance(other, Dataset):
6644             # can't use our shortcut of doing the binary operation with
6645             # Variable objects, so apply over our data vars instead.
6646             new_data_vars = apply_over_both(
6647                 self.data_vars, other, self.data_vars, other
6648             )
6649             return type(self)(new_data_vars)
6650 
6651         other_coords: Coordinates | None = getattr(other, "coords", None)
6652         ds = self.coords.merge(other_coords)
6653 
6654         if isinstance(other, Dataset):
6655             new_vars = apply_over_both(
6656                 self.data_vars, other.data_vars, self.variables, other.variables
6657             )
6658         else:
6659             other_variable = getattr(other, "variable", other)
6660             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}
6661         ds._variables.update(new_vars)
6662         ds._dims = calculate_dimensions(ds._variables)
6663         return ds
6664 
6665     def _copy_attrs_from(self, other):
6666         self.attrs = other.attrs
6667         for v in other.variables:
6668             if v in self.variables:
6669                 self.variables[v].attrs = other.variables[v].attrs
6670 
6671     def diff(
6672         self: T_Dataset,
6673         dim: Hashable,
6674         n: int = 1,
6675         label: Literal["upper", "lower"] = "upper",
6676     ) -> T_Dataset:
6677         """Calculate the n-th order discrete difference along given axis.
6678 
6679         Parameters
6680         ----------
6681         dim : Hashable
6682             Dimension over which to calculate the finite difference.
6683         n : int, default: 1
6684             The number of times values are differenced.
6685         label : {"upper", "lower"}, default: "upper"
6686             The new coordinate in dimension ``dim`` will have the
6687             values of either the minuend's or subtrahend's coordinate
6688             for values 'upper' and 'lower', respectively.
6689 
6690         Returns
6691         -------
6692         difference : Dataset
6693             The n-th order finite difference of this object.
6694 
6695         Notes
6696         -----
6697         `n` matches numpy's behavior and is different from pandas' first argument named
6698         `periods`.
6699 
6700         Examples
6701         --------
6702         >>> ds = xr.Dataset({"foo": ("x", [5, 5, 6, 6])})
6703         >>> ds.diff("x")
6704         <xarray.Dataset>
6705         Dimensions:  (x: 3)
6706         Dimensions without coordinates: x
6707         Data variables:
6708             foo      (x) int64 0 1 0
6709         >>> ds.diff("x", 2)
6710         <xarray.Dataset>
6711         Dimensions:  (x: 2)
6712         Dimensions without coordinates: x
6713         Data variables:
6714             foo      (x) int64 1 -1
6715 
6716         See Also
6717         --------
6718         Dataset.differentiate
6719         """
6720         if n == 0:
6721             return self
6722         if n < 0:
6723             raise ValueError(f"order `n` must be non-negative but got {n}")
6724 
6725         # prepare slices
6726         slice_start = {dim: slice(None, -1)}
6727         slice_end = {dim: slice(1, None)}
6728 
6729         # prepare new coordinate
6730         if label == "upper":
6731             slice_new = slice_end
6732         elif label == "lower":
6733             slice_new = slice_start
6734         else:
6735             raise ValueError("The 'label' argument has to be either 'upper' or 'lower'")
6736 
6737         indexes, index_vars = isel_indexes(self.xindexes, slice_new)
6738         variables = {}
6739 
6740         for name, var in self.variables.items():
6741             if name in index_vars:
6742                 variables[name] = index_vars[name]
6743             elif dim in var.dims:
6744                 if name in self.data_vars:
6745                     variables[name] = var.isel(slice_end) - var.isel(slice_start)
6746                 else:
6747                     variables[name] = var.isel(slice_new)
6748             else:
6749                 variables[name] = var
6750 
6751         difference = self._replace_with_new_dims(variables, indexes=indexes)
6752 
6753         if n > 1:
6754             return difference.diff(dim, n - 1)
6755         else:
6756             return difference
6757 
6758     def shift(
6759         self: T_Dataset,
6760         shifts: Mapping[Any, int] | None = None,
6761         fill_value: Any = xrdtypes.NA,
6762         **shifts_kwargs: int,
6763     ) -> T_Dataset:
6764 
6765         """Shift this dataset by an offset along one or more dimensions.
6766 
6767         Only data variables are moved; coordinates stay in place. This is
6768         consistent with the behavior of ``shift`` in pandas.
6769 
6770         Values shifted from beyond array bounds will appear at one end of
6771         each dimension, which are filled according to `fill_value`. For periodic
6772         offsets instead see `roll`.
6773 
6774         Parameters
6775         ----------
6776         shifts : mapping of hashable to int
6777             Integer offset to shift along each of the given dimensions.
6778             Positive offsets shift to the right; negative offsets shift to the
6779             left.
6780         fill_value : scalar or dict-like, optional
6781             Value to use for newly missing values. If a dict-like, maps
6782             variable names (including coordinates) to fill values.
6783         **shifts_kwargs
6784             The keyword arguments form of ``shifts``.
6785             One of shifts or shifts_kwargs must be provided.
6786 
6787         Returns
6788         -------
6789         shifted : Dataset
6790             Dataset with the same coordinates and attributes but shifted data
6791             variables.
6792 
6793         See Also
6794         --------
6795         roll
6796 
6797         Examples
6798         --------
6799         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
6800         >>> ds.shift(x=2)
6801         <xarray.Dataset>
6802         Dimensions:  (x: 5)
6803         Dimensions without coordinates: x
6804         Data variables:
6805             foo      (x) object nan nan 'a' 'b' 'c'
6806         """
6807         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
6808         invalid = [k for k in shifts if k not in self.dims]
6809         if invalid:
6810             raise ValueError(f"dimensions {invalid!r} do not exist")
6811 
6812         variables = {}
6813         for name, var in self.variables.items():
6814             if name in self.data_vars:
6815                 fill_value_ = (
6816                     fill_value.get(name, xrdtypes.NA)
6817                     if isinstance(fill_value, dict)
6818                     else fill_value
6819                 )
6820 
6821                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}
6822                 variables[name] = var.shift(fill_value=fill_value_, shifts=var_shifts)
6823             else:
6824                 variables[name] = var
6825 
6826         return self._replace(variables)
6827 
6828     def roll(
6829         self: T_Dataset,
6830         shifts: Mapping[Any, int] | None = None,
6831         roll_coords: bool = False,
6832         **shifts_kwargs: int,
6833     ) -> T_Dataset:
6834         """Roll this dataset by an offset along one or more dimensions.
6835 
6836         Unlike shift, roll treats the given dimensions as periodic, so will not
6837         create any missing values to be filled.
6838 
6839         Also unlike shift, roll may rotate all variables, including coordinates
6840         if specified. The direction of rotation is consistent with
6841         :py:func:`numpy.roll`.
6842 
6843         Parameters
6844         ----------
6845         shifts : mapping of hashable to int, optional
6846             A dict with keys matching dimensions and values given
6847             by integers to rotate each of the given dimensions. Positive
6848             offsets roll to the right; negative offsets roll to the left.
6849         roll_coords : bool, default: False
6850             Indicates whether to roll the coordinates by the offset too.
6851         **shifts_kwargs : {dim: offset, ...}, optional
6852             The keyword arguments form of ``shifts``.
6853             One of shifts or shifts_kwargs must be provided.
6854 
6855         Returns
6856         -------
6857         rolled : Dataset
6858             Dataset with the same attributes but rolled data and coordinates.
6859 
6860         See Also
6861         --------
6862         shift
6863 
6864         Examples
6865         --------
6866         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))}, coords={"x": np.arange(5)})
6867         >>> ds.roll(x=2)
6868         <xarray.Dataset>
6869         Dimensions:  (x: 5)
6870         Coordinates:
6871           * x        (x) int64 0 1 2 3 4
6872         Data variables:
6873             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6874 
6875         >>> ds.roll(x=2, roll_coords=True)
6876         <xarray.Dataset>
6877         Dimensions:  (x: 5)
6878         Coordinates:
6879           * x        (x) int64 3 4 0 1 2
6880         Data variables:
6881             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6882 
6883         """
6884         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
6885         invalid = [k for k in shifts if k not in self.dims]
6886         if invalid:
6887             raise ValueError(f"dimensions {invalid!r} do not exist")
6888 
6889         unrolled_vars: tuple[Hashable, ...]
6890 
6891         if roll_coords:
6892             indexes, index_vars = roll_indexes(self.xindexes, shifts)
6893             unrolled_vars = ()
6894         else:
6895             indexes = dict(self._indexes)
6896             index_vars = dict(self.xindexes.variables)
6897             unrolled_vars = tuple(self.coords)
6898 
6899         variables = {}
6900         for k, var in self.variables.items():
6901             if k in index_vars:
6902                 variables[k] = index_vars[k]
6903             elif k not in unrolled_vars:
6904                 variables[k] = var.roll(
6905                     shifts={k: s for k, s in shifts.items() if k in var.dims}
6906                 )
6907             else:
6908                 variables[k] = var
6909 
6910         return self._replace(variables, indexes=indexes)
6911 
6912     def sortby(
6913         self: T_Dataset,
6914         variables: Hashable | DataArray | list[Hashable | DataArray],
6915         ascending: bool = True,
6916     ) -> T_Dataset:
6917         """
6918         Sort object by labels or values (along an axis).
6919 
6920         Sorts the dataset, either along specified dimensions,
6921         or according to values of 1-D dataarrays that share dimension
6922         with calling object.
6923 
6924         If the input variables are dataarrays, then the dataarrays are aligned
6925         (via left-join) to the calling object prior to sorting by cell values.
6926         NaNs are sorted to the end, following Numpy convention.
6927 
6928         If multiple sorts along the same dimension is
6929         given, numpy's lexsort is performed along that dimension:
6930         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
6931         and the FIRST key in the sequence is used as the primary sort key,
6932         followed by the 2nd key, etc.
6933 
6934         Parameters
6935         ----------
6936         variables : Hashable, DataArray, or list of hashable or DataArray
6937             1D DataArray objects or name(s) of 1D variable(s) in
6938             coords/data_vars whose values are used to sort the dataset.
6939         ascending : bool, default: True
6940             Whether to sort by ascending or descending order.
6941 
6942         Returns
6943         -------
6944         sorted : Dataset
6945             A new dataset where all the specified dims are sorted by dim
6946             labels.
6947 
6948         See Also
6949         --------
6950         DataArray.sortby
6951         numpy.sort
6952         pandas.sort_values
6953         pandas.sort_index
6954 
6955         Examples
6956         --------
6957         >>> ds = xr.Dataset(
6958         ...     {
6959         ...         "A": (("x", "y"), [[1, 2], [3, 4]]),
6960         ...         "B": (("x", "y"), [[5, 6], [7, 8]]),
6961         ...     },
6962         ...     coords={"x": ["b", "a"], "y": [1, 0]},
6963         ... )
6964         >>> ds = ds.sortby("x")
6965         >>> ds
6966         <xarray.Dataset>
6967         Dimensions:  (x: 2, y: 2)
6968         Coordinates:
6969           * x        (x) <U1 'a' 'b'
6970           * y        (y) int64 1 0
6971         Data variables:
6972             A        (x, y) int64 3 4 1 2
6973             B        (x, y) int64 7 8 5 6
6974         """
6975         from xarray.core.dataarray import DataArray
6976 
6977         if not isinstance(variables, list):
6978             variables = [variables]
6979         else:
6980             variables = variables
6981         arrays = [v if isinstance(v, DataArray) else self[v] for v in variables]
6982         aligned_vars = align(self, *arrays, join="left")  # type: ignore[type-var]
6983         aligned_self: T_Dataset = aligned_vars[0]  # type: ignore[assignment]
6984         aligned_other_vars: tuple[DataArray, ...] = aligned_vars[1:]  # type: ignore[assignment]
6985         vars_by_dim = defaultdict(list)
6986         for data_array in aligned_other_vars:
6987             if data_array.ndim != 1:
6988                 raise ValueError("Input DataArray is not 1-D.")
6989             (key,) = data_array.dims
6990             vars_by_dim[key].append(data_array)
6991 
6992         indices = {}
6993         for key, arrays in vars_by_dim.items():
6994             order = np.lexsort(tuple(reversed(arrays)))
6995             indices[key] = order if ascending else order[::-1]
6996         return aligned_self.isel(indices)
6997 
6998     def quantile(
6999         self: T_Dataset,
7000         q: ArrayLike,
7001         dim: Dims = None,
7002         method: QuantileMethods = "linear",
7003         numeric_only: bool = False,
7004         keep_attrs: bool | None = None,
7005         skipna: bool | None = None,
7006         interpolation: QuantileMethods | None = None,
7007     ) -> T_Dataset:
7008         """Compute the qth quantile of the data along the specified dimension.
7009 
7010         Returns the qth quantiles(s) of the array elements for each variable
7011         in the Dataset.
7012 
7013         Parameters
7014         ----------
7015         q : float or array-like of float
7016             Quantile to compute, which must be between 0 and 1 inclusive.
7017         dim : str or Iterable of Hashable, optional
7018             Dimension(s) over which to apply quantile.
7019         method : str, default: "linear"
7020             This optional parameter specifies the interpolation method to use when the
7021             desired quantile lies between two data points. The options sorted by their R
7022             type as summarized in the H&F paper [1]_ are:
7023 
7024                 1. "inverted_cdf" (*)
7025                 2. "averaged_inverted_cdf" (*)
7026                 3. "closest_observation" (*)
7027                 4. "interpolated_inverted_cdf" (*)
7028                 5. "hazen" (*)
7029                 6. "weibull" (*)
7030                 7. "linear"  (default)
7031                 8. "median_unbiased" (*)
7032                 9. "normal_unbiased" (*)
7033 
7034             The first three methods are discontiuous.  The following discontinuous
7035             variations of the default "linear" (7.) option are also available:
7036 
7037                 * "lower"
7038                 * "higher"
7039                 * "midpoint"
7040                 * "nearest"
7041 
7042             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
7043             was previously called "interpolation", renamed in accordance with numpy
7044             version 1.22.0.
7045 
7046             (*) These methods require numpy version 1.22 or newer.
7047 
7048         keep_attrs : bool, optional
7049             If True, the dataset's attributes (`attrs`) will be copied from
7050             the original object to the new one.  If False (default), the new
7051             object will be returned without attributes.
7052         numeric_only : bool, optional
7053             If True, only apply ``func`` to variables with a numeric dtype.
7054         skipna : bool, optional
7055             If True, skip missing values (as marked by NaN). By default, only
7056             skips missing values for float dtypes; other dtypes either do not
7057             have a sentinel missing value (int) or skipna=True has not been
7058             implemented (object, datetime64 or timedelta64).
7059 
7060         Returns
7061         -------
7062         quantiles : Dataset
7063             If `q` is a single quantile, then the result is a scalar for each
7064             variable in data_vars. If multiple percentiles are given, first
7065             axis of the result corresponds to the quantile and a quantile
7066             dimension is added to the return Dataset. The other dimensions are
7067             the dimensions that remain after the reduction of the array.
7068 
7069         See Also
7070         --------
7071         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile
7072 
7073         Examples
7074         --------
7075         >>> ds = xr.Dataset(
7076         ...     {"a": (("x", "y"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},
7077         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
7078         ... )
7079         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)
7080         <xarray.Dataset>
7081         Dimensions:   ()
7082         Coordinates:
7083             quantile  float64 0.0
7084         Data variables:
7085             a         float64 0.7
7086         >>> ds.quantile(0, dim="x")
7087         <xarray.Dataset>
7088         Dimensions:   (y: 4)
7089         Coordinates:
7090           * y         (y) float64 1.0 1.5 2.0 2.5
7091             quantile  float64 0.0
7092         Data variables:
7093             a         (y) float64 0.7 4.2 2.6 1.5
7094         >>> ds.quantile([0, 0.5, 1])
7095         <xarray.Dataset>
7096         Dimensions:   (quantile: 3)
7097         Coordinates:
7098           * quantile  (quantile) float64 0.0 0.5 1.0
7099         Data variables:
7100             a         (quantile) float64 0.7 3.4 9.4
7101         >>> ds.quantile([0, 0.5, 1], dim="x")
7102         <xarray.Dataset>
7103         Dimensions:   (quantile: 3, y: 4)
7104         Coordinates:
7105           * y         (y) float64 1.0 1.5 2.0 2.5
7106           * quantile  (quantile) float64 0.0 0.5 1.0
7107         Data variables:
7108             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9
7109 
7110         References
7111         ----------
7112         .. [1] R. J. Hyndman and Y. Fan,
7113            "Sample quantiles in statistical packages,"
7114            The American Statistician, 50(4), pp. 361-365, 1996
7115         """
7116 
7117         # interpolation renamed to method in version 0.21.0
7118         # check here and in variable to avoid repeated warnings
7119         if interpolation is not None:
7120             warnings.warn(
7121                 "The `interpolation` argument to quantile was renamed to `method`.",
7122                 FutureWarning,
7123             )
7124 
7125             if method != "linear":
7126                 raise TypeError("Cannot pass interpolation and method keywords!")
7127 
7128             method = interpolation
7129 
7130         dims: set[Hashable]
7131         if isinstance(dim, str):
7132             dims = {dim}
7133         elif dim is None or dim is ...:
7134             dims = set(self.dims)
7135         else:
7136             dims = set(dim)
7137 
7138         _assert_empty(
7139             tuple(d for d in dims if d not in self.dims),
7140             "Dataset does not contain the dimensions: %s",
7141         )
7142 
7143         q = np.asarray(q, dtype=np.float64)
7144 
7145         variables = {}
7146         for name, var in self.variables.items():
7147             reduce_dims = [d for d in var.dims if d in dims]
7148             if reduce_dims or not var.dims:
7149                 if name not in self.coords:
7150                     if (
7151                         not numeric_only
7152                         or np.issubdtype(var.dtype, np.number)
7153                         or var.dtype == np.bool_
7154                     ):
7155                         variables[name] = var.quantile(
7156                             q,
7157                             dim=reduce_dims,
7158                             method=method,
7159                             keep_attrs=keep_attrs,
7160                             skipna=skipna,
7161                         )
7162 
7163             else:
7164                 variables[name] = var
7165 
7166         # construct the new dataset
7167         coord_names = {k for k in self.coords if k in variables}
7168         indexes = {k: v for k, v in self._indexes.items() if k in variables}
7169         if keep_attrs is None:
7170             keep_attrs = _get_keep_attrs(default=False)
7171         attrs = self.attrs if keep_attrs else None
7172         new = self._replace_with_new_dims(
7173             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
7174         )
7175         return new.assign_coords(quantile=q)
7176 
7177     def rank(
7178         self: T_Dataset,
7179         dim: Hashable,
7180         pct: bool = False,
7181         keep_attrs: bool | None = None,
7182     ) -> T_Dataset:
7183         """Ranks the data.
7184 
7185         Equal values are assigned a rank that is the average of the ranks that
7186         would have been otherwise assigned to all of the values within
7187         that set.
7188         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.
7189 
7190         NaNs in the input array are returned as NaNs.
7191 
7192         The `bottleneck` library is required.
7193 
7194         Parameters
7195         ----------
7196         dim : Hashable
7197             Dimension over which to compute rank.
7198         pct : bool, default: False
7199             If True, compute percentage ranks, otherwise compute integer ranks.
7200         keep_attrs : bool or None, optional
7201             If True, the dataset's attributes (`attrs`) will be copied from
7202             the original object to the new one.  If False, the new
7203             object will be returned without attributes.
7204 
7205         Returns
7206         -------
7207         ranked : Dataset
7208             Variables that do not depend on `dim` are dropped.
7209         """
7210         if not OPTIONS["use_bottleneck"]:
7211             raise RuntimeError(
7212                 "rank requires bottleneck to be enabled."
7213                 " Call `xr.set_options(use_bottleneck=True)` to enable it."
7214             )
7215 
7216         if dim not in self.dims:
7217             raise ValueError(f"Dataset does not contain the dimension: {dim}")
7218 
7219         variables = {}
7220         for name, var in self.variables.items():
7221             if name in self.data_vars:
7222                 if dim in var.dims:
7223                     variables[name] = var.rank(dim, pct=pct)
7224             else:
7225                 variables[name] = var
7226 
7227         coord_names = set(self.coords)
7228         if keep_attrs is None:
7229             keep_attrs = _get_keep_attrs(default=False)
7230         attrs = self.attrs if keep_attrs else None
7231         return self._replace(variables, coord_names, attrs=attrs)
7232 
7233     def differentiate(
7234         self: T_Dataset,
7235         coord: Hashable,
7236         edge_order: Literal[1, 2] = 1,
7237         datetime_unit: DatetimeUnitOptions | None = None,
7238     ) -> T_Dataset:
7239         """ Differentiate with the second order accurate central
7240         differences.
7241 
7242         .. note::
7243             This feature is limited to simple cartesian geometry, i.e. coord
7244             must be one dimensional.
7245 
7246         Parameters
7247         ----------
7248         coord : Hashable
7249             The coordinate to be used to compute the gradient.
7250         edge_order : {1, 2}, default: 1
7251             N-th order accurate differences at the boundaries.
7252         datetime_unit : None or {"Y", "M", "W", "D", "h", "m", "s", "ms", \
7253             "us", "ns", "ps", "fs", "as", None}, default: None
7254             Unit to compute gradient. Only valid for datetime coordinate.
7255 
7256         Returns
7257         -------
7258         differentiated: Dataset
7259 
7260         See also
7261         --------
7262         numpy.gradient: corresponding numpy function
7263         """
7264         from xarray.core.variable import Variable
7265 
7266         if coord not in self.variables and coord not in self.dims:
7267             raise ValueError(f"Coordinate {coord} does not exist.")
7268 
7269         coord_var = self[coord].variable
7270         if coord_var.ndim != 1:
7271             raise ValueError(
7272                 "Coordinate {} must be 1 dimensional but is {}"
7273                 " dimensional".format(coord, coord_var.ndim)
7274             )
7275 
7276         dim = coord_var.dims[0]
7277         if _contains_datetime_like_objects(coord_var):
7278             if coord_var.dtype.kind in "mM" and datetime_unit is None:
7279                 datetime_unit = cast(
7280                     "DatetimeUnitOptions", np.datetime_data(coord_var.dtype)[0]
7281                 )
7282             elif datetime_unit is None:
7283                 datetime_unit = "s"  # Default to seconds for cftime objects
7284             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)
7285 
7286         variables = {}
7287         for k, v in self.variables.items():
7288             if k in self.data_vars and dim in v.dims and k not in self.coords:
7289                 if _contains_datetime_like_objects(v):
7290                     v = v._to_numeric(datetime_unit=datetime_unit)
7291                 grad = duck_array_ops.gradient(
7292                     v.data,
7293                     coord_var.data,
7294                     edge_order=edge_order,
7295                     axis=v.get_axis_num(dim),
7296                 )
7297                 variables[k] = Variable(v.dims, grad)
7298             else:
7299                 variables[k] = v
7300         return self._replace(variables)
7301 
7302     def integrate(
7303         self: T_Dataset,
7304         coord: Hashable | Sequence[Hashable],
7305         datetime_unit: DatetimeUnitOptions = None,
7306     ) -> T_Dataset:
7307         """Integrate along the given coordinate using the trapezoidal rule.
7308 
7309         .. note::
7310             This feature is limited to simple cartesian geometry, i.e. coord
7311             must be one dimensional.
7312 
7313         Parameters
7314         ----------
7315         coord : hashable, or sequence of hashable
7316             Coordinate(s) used for the integration.
7317         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7318                         'ps', 'fs', 'as', None}, optional
7319             Specify the unit if datetime coordinate is used.
7320 
7321         Returns
7322         -------
7323         integrated : Dataset
7324 
7325         See also
7326         --------
7327         DataArray.integrate
7328         numpy.trapz : corresponding numpy function
7329 
7330         Examples
7331         --------
7332         >>> ds = xr.Dataset(
7333         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7334         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7335         ... )
7336         >>> ds
7337         <xarray.Dataset>
7338         Dimensions:  (x: 4)
7339         Coordinates:
7340           * x        (x) int64 0 1 2 3
7341             y        (x) int64 1 7 3 5
7342         Data variables:
7343             a        (x) int64 5 5 6 6
7344             b        (x) int64 1 2 1 0
7345         >>> ds.integrate("x")
7346         <xarray.Dataset>
7347         Dimensions:  ()
7348         Data variables:
7349             a        float64 16.5
7350             b        float64 3.5
7351         >>> ds.integrate("y")
7352         <xarray.Dataset>
7353         Dimensions:  ()
7354         Data variables:
7355             a        float64 20.0
7356             b        float64 4.0
7357         """
7358         if not isinstance(coord, (list, tuple)):
7359             coord = (coord,)
7360         result = self
7361         for c in coord:
7362             result = result._integrate_one(c, datetime_unit=datetime_unit)
7363         return result
7364 
7365     def _integrate_one(self, coord, datetime_unit=None, cumulative=False):
7366         from xarray.core.variable import Variable
7367 
7368         if coord not in self.variables and coord not in self.dims:
7369             raise ValueError(f"Coordinate {coord} does not exist.")
7370 
7371         coord_var = self[coord].variable
7372         if coord_var.ndim != 1:
7373             raise ValueError(
7374                 "Coordinate {} must be 1 dimensional but is {}"
7375                 " dimensional".format(coord, coord_var.ndim)
7376             )
7377 
7378         dim = coord_var.dims[0]
7379         if _contains_datetime_like_objects(coord_var):
7380             if coord_var.dtype.kind in "mM" and datetime_unit is None:
7381                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
7382             elif datetime_unit is None:
7383                 datetime_unit = "s"  # Default to seconds for cftime objects
7384             coord_var = coord_var._replace(
7385                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)
7386             )
7387 
7388         variables = {}
7389         coord_names = set()
7390         for k, v in self.variables.items():
7391             if k in self.coords:
7392                 if dim not in v.dims or cumulative:
7393                     variables[k] = v
7394                     coord_names.add(k)
7395             else:
7396                 if k in self.data_vars and dim in v.dims:
7397                     if _contains_datetime_like_objects(v):
7398                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)
7399                     if cumulative:
7400                         integ = duck_array_ops.cumulative_trapezoid(
7401                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7402                         )
7403                         v_dims = v.dims
7404                     else:
7405                         integ = duck_array_ops.trapz(
7406                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7407                         )
7408                         v_dims = list(v.dims)
7409                         v_dims.remove(dim)
7410                     variables[k] = Variable(v_dims, integ)
7411                 else:
7412                     variables[k] = v
7413         indexes = {k: v for k, v in self._indexes.items() if k in variables}
7414         return self._replace_with_new_dims(
7415             variables, coord_names=coord_names, indexes=indexes
7416         )
7417 
7418     def cumulative_integrate(
7419         self: T_Dataset,
7420         coord: Hashable | Sequence[Hashable],
7421         datetime_unit: DatetimeUnitOptions = None,
7422     ) -> T_Dataset:
7423         """Integrate along the given coordinate using the trapezoidal rule.
7424 
7425         .. note::
7426             This feature is limited to simple cartesian geometry, i.e. coord
7427             must be one dimensional.
7428 
7429             The first entry of the cumulative integral of each variable is always 0, in
7430             order to keep the length of the dimension unchanged between input and
7431             output.
7432 
7433         Parameters
7434         ----------
7435         coord : hashable, or sequence of hashable
7436             Coordinate(s) used for the integration.
7437         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7438                         'ps', 'fs', 'as', None}, optional
7439             Specify the unit if datetime coordinate is used.
7440 
7441         Returns
7442         -------
7443         integrated : Dataset
7444 
7445         See also
7446         --------
7447         DataArray.cumulative_integrate
7448         scipy.integrate.cumulative_trapezoid : corresponding scipy function
7449 
7450         Examples
7451         --------
7452         >>> ds = xr.Dataset(
7453         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7454         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7455         ... )
7456         >>> ds
7457         <xarray.Dataset>
7458         Dimensions:  (x: 4)
7459         Coordinates:
7460           * x        (x) int64 0 1 2 3
7461             y        (x) int64 1 7 3 5
7462         Data variables:
7463             a        (x) int64 5 5 6 6
7464             b        (x) int64 1 2 1 0
7465         >>> ds.cumulative_integrate("x")
7466         <xarray.Dataset>
7467         Dimensions:  (x: 4)
7468         Coordinates:
7469           * x        (x) int64 0 1 2 3
7470             y        (x) int64 1 7 3 5
7471         Data variables:
7472             a        (x) float64 0.0 5.0 10.5 16.5
7473             b        (x) float64 0.0 1.5 3.0 3.5
7474         >>> ds.cumulative_integrate("y")
7475         <xarray.Dataset>
7476         Dimensions:  (x: 4)
7477         Coordinates:
7478           * x        (x) int64 0 1 2 3
7479             y        (x) int64 1 7 3 5
7480         Data variables:
7481             a        (x) float64 0.0 30.0 8.0 20.0
7482             b        (x) float64 0.0 9.0 3.0 4.0
7483         """
7484         if not isinstance(coord, (list, tuple)):
7485             coord = (coord,)
7486         result = self
7487         for c in coord:
7488             result = result._integrate_one(
7489                 c, datetime_unit=datetime_unit, cumulative=True
7490             )
7491         return result
7492 
7493     @property
7494     def real(self: T_Dataset) -> T_Dataset:
7495         """
7496         The real part of each data variable.
7497 
7498         See Also
7499         --------
7500         numpy.ndarray.real
7501         """
7502         return self.map(lambda x: x.real, keep_attrs=True)
7503 
7504     @property
7505     def imag(self: T_Dataset) -> T_Dataset:
7506         """
7507         The imaginary part of each data variable.
7508 
7509         See Also
7510         --------
7511         numpy.ndarray.imag
7512         """
7513         return self.map(lambda x: x.imag, keep_attrs=True)
7514 
7515     plot = utils.UncachedAccessor(DatasetPlotAccessor)
7516 
7517     def filter_by_attrs(self: T_Dataset, **kwargs) -> T_Dataset:
7518         """Returns a ``Dataset`` with variables that match specific conditions.
7519 
7520         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned
7521         containing only the variables for which all the filter tests pass.
7522         These tests are either ``key=value`` for which the attribute ``key``
7523         has the exact value ``value`` or the callable passed into
7524         ``key=callable`` returns True. The callable will be passed a single
7525         value, either the value of the attribute ``key`` or ``None`` if the
7526         DataArray does not have an attribute with the name ``key``.
7527 
7528         Parameters
7529         ----------
7530         **kwargs
7531             key : str
7532                 Attribute name.
7533             value : callable or obj
7534                 If value is a callable, it should return a boolean in the form
7535                 of bool = func(attr) where attr is da.attrs[key].
7536                 Otherwise, value will be compared to the each
7537                 DataArray's attrs[key].
7538 
7539         Returns
7540         -------
7541         new : Dataset
7542             New dataset with variables filtered by attribute.
7543 
7544         Examples
7545         --------
7546         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)
7547         >>> precip = 10 * np.random.rand(2, 2, 3)
7548         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
7549         >>> lat = [[42.25, 42.21], [42.63, 42.59]]
7550         >>> dims = ["x", "y", "time"]
7551         >>> temp_attr = dict(standard_name="air_potential_temperature")
7552         >>> precip_attr = dict(standard_name="convective_precipitation_flux")
7553 
7554         >>> ds = xr.Dataset(
7555         ...     dict(
7556         ...         temperature=(dims, temp, temp_attr),
7557         ...         precipitation=(dims, precip, precip_attr),
7558         ...     ),
7559         ...     coords=dict(
7560         ...         lon=(["x", "y"], lon),
7561         ...         lat=(["x", "y"], lat),
7562         ...         time=pd.date_range("2014-09-06", periods=3),
7563         ...         reference_time=pd.Timestamp("2014-09-05"),
7564         ...     ),
7565         ... )
7566 
7567         Get variables matching a specific standard_name:
7568 
7569         >>> ds.filter_by_attrs(standard_name="convective_precipitation_flux")
7570         <xarray.Dataset>
7571         Dimensions:         (x: 2, y: 2, time: 3)
7572         Coordinates:
7573             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7574             lat             (x, y) float64 42.25 42.21 42.63 42.59
7575           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7576             reference_time  datetime64[ns] 2014-09-05
7577         Dimensions without coordinates: x, y
7578         Data variables:
7579             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7580 
7581         Get all variables that have a standard_name attribute:
7582 
7583         >>> standard_name = lambda v: v is not None
7584         >>> ds.filter_by_attrs(standard_name=standard_name)
7585         <xarray.Dataset>
7586         Dimensions:         (x: 2, y: 2, time: 3)
7587         Coordinates:
7588             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7589             lat             (x, y) float64 42.25 42.21 42.63 42.59
7590           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7591             reference_time  datetime64[ns] 2014-09-05
7592         Dimensions without coordinates: x, y
7593         Data variables:
7594             temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
7595             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7596 
7597         """
7598         selection = []
7599         for var_name, variable in self.variables.items():
7600             has_value_flag = False
7601             for attr_name, pattern in kwargs.items():
7602                 attr_value = variable.attrs.get(attr_name)
7603                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:
7604                     has_value_flag = True
7605                 else:
7606                     has_value_flag = False
7607                     break
7608             if has_value_flag is True:
7609                 selection.append(var_name)
7610         return self[selection]
7611 
7612     def unify_chunks(self: T_Dataset) -> T_Dataset:
7613         """Unify chunk size along all chunked dimensions of this Dataset.
7614 
7615         Returns
7616         -------
7617         Dataset with consistent chunk sizes for all dask-array variables
7618 
7619         See Also
7620         --------
7621         dask.array.core.unify_chunks
7622         """
7623 
7624         return unify_chunks(self)[0]
7625 
7626     def map_blocks(
7627         self,
7628         func: Callable[..., T_Xarray],
7629         args: Sequence[Any] = (),
7630         kwargs: Mapping[str, Any] | None = None,
7631         template: DataArray | Dataset | None = None,
7632     ) -> T_Xarray:
7633         """
7634         Apply a function to each block of this Dataset.
7635 
7636         .. warning::
7637             This method is experimental and its signature may change.
7638 
7639         Parameters
7640         ----------
7641         func : callable
7642             User-provided function that accepts a Dataset as its first
7643             parameter. The function will receive a subset or 'block' of this Dataset (see below),
7644             corresponding to one chunk along each chunked dimension. ``func`` will be
7645             executed as ``func(subset_dataset, *subset_args, **kwargs)``.
7646 
7647             This function must return either a single DataArray or a single Dataset.
7648 
7649             This function cannot add a new chunked dimension.
7650         args : sequence
7651             Passed to func after unpacking and subsetting any xarray objects by blocks.
7652             xarray objects in args must be aligned with obj, otherwise an error is raised.
7653         kwargs : Mapping or None
7654             Passed verbatim to func after unpacking. xarray objects, if any, will not be
7655             subset to blocks. Passing dask collections in kwargs is not allowed.
7656         template : DataArray, Dataset or None, optional
7657             xarray object representing the final result after compute is called. If not provided,
7658             the function will be first run on mocked-up data, that looks like this object but
7659             has sizes 0, to determine properties of the returned object such as dtype,
7660             variable names, attributes, new dimensions and new indexes (if any).
7661             ``template`` must be provided if the function changes the size of existing dimensions.
7662             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
7663             ``attrs`` set by ``func`` will be ignored.
7664 
7665         Returns
7666         -------
7667         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
7668         function.
7669 
7670         Notes
7671         -----
7672         This function is designed for when ``func`` needs to manipulate a whole xarray object
7673         subset to each block. Each block is loaded into memory. In the more common case where
7674         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
7675 
7676         If none of the variables in this object is backed by dask arrays, calling this function is
7677         equivalent to calling ``func(obj, *args, **kwargs)``.
7678 
7679         See Also
7680         --------
7681         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
7682         xarray.DataArray.map_blocks
7683 
7684         Examples
7685         --------
7686         Calculate an anomaly from climatology using ``.groupby()``. Using
7687         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
7688         its indices, and its methods like ``.groupby()``.
7689 
7690         >>> def calculate_anomaly(da, groupby_type="time.month"):
7691         ...     gb = da.groupby(groupby_type)
7692         ...     clim = gb.mean(dim="time")
7693         ...     return gb - clim
7694         ...
7695         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
7696         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
7697         >>> np.random.seed(123)
7698         >>> array = xr.DataArray(
7699         ...     np.random.rand(len(time)),
7700         ...     dims=["time"],
7701         ...     coords={"time": time, "month": month},
7702         ... ).chunk()
7703         >>> ds = xr.Dataset({"a": array})
7704         >>> ds.map_blocks(calculate_anomaly, template=ds).compute()
7705         <xarray.Dataset>
7706         Dimensions:  (time: 24)
7707         Coordinates:
7708           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7709             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
7710         Data variables:
7711             a        (time) float64 0.1289 0.1132 -0.0856 ... 0.2287 0.1906 -0.05901
7712 
7713         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
7714         to the function being applied in ``xr.map_blocks()``:
7715 
7716         >>> ds.map_blocks(
7717         ...     calculate_anomaly,
7718         ...     kwargs={"groupby_type": "time.year"},
7719         ...     template=ds,
7720         ... )
7721         <xarray.Dataset>
7722         Dimensions:  (time: 24)
7723         Coordinates:
7724           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7725             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
7726         Data variables:
7727             a        (time) float64 dask.array<chunksize=(24,), meta=np.ndarray>
7728         """
7729         from xarray.core.parallel import map_blocks
7730 
7731         return map_blocks(func, self, args, kwargs, template)
7732 
7733     def polyfit(
7734         self: T_Dataset,
7735         dim: Hashable,
7736         deg: int,
7737         skipna: bool | None = None,
7738         rcond: float | None = None,
7739         w: Hashable | Any = None,
7740         full: bool = False,
7741         cov: bool | Literal["unscaled"] = False,
7742     ) -> T_Dataset:
7743         """
7744         Least squares polynomial fit.
7745 
7746         This replicates the behaviour of `numpy.polyfit` but differs by skipping
7747         invalid values when `skipna = True`.
7748 
7749         Parameters
7750         ----------
7751         dim : hashable
7752             Coordinate along which to fit the polynomials.
7753         deg : int
7754             Degree of the fitting polynomial.
7755         skipna : bool or None, optional
7756             If True, removes all invalid values before fitting each 1D slices of the array.
7757             Default is True if data is stored in a dask.array or if there is any
7758             invalid values, False otherwise.
7759         rcond : float or None, optional
7760             Relative condition number to the fit.
7761         w : hashable or Any, optional
7762             Weights to apply to the y-coordinate of the sample points.
7763             Can be an array-like object or the name of a coordinate in the dataset.
7764         full : bool, default: False
7765             Whether to return the residuals, matrix rank and singular values in addition
7766             to the coefficients.
7767         cov : bool or "unscaled", default: False
7768             Whether to return to the covariance matrix in addition to the coefficients.
7769             The matrix is not scaled if `cov='unscaled'`.
7770 
7771         Returns
7772         -------
7773         polyfit_results : Dataset
7774             A single dataset which contains (for each "var" in the input dataset):
7775 
7776             [var]_polyfit_coefficients
7777                 The coefficients of the best fit for each variable in this dataset.
7778             [var]_polyfit_residuals
7779                 The residuals of the least-square computation for each variable (only included if `full=True`)
7780                 When the matrix rank is deficient, np.nan is returned.
7781             [dim]_matrix_rank
7782                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7783                 The rank is computed ignoring the NaN values that might be skipped.
7784             [dim]_singular_values
7785                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7786             [var]_polyfit_covariance
7787                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
7788 
7789         Warns
7790         -----
7791         RankWarning
7792             The rank of the coefficient matrix in the least-squares fit is deficient.
7793             The warning is not raised with in-memory (not dask) data and `full=True`.
7794 
7795         See Also
7796         --------
7797         numpy.polyfit
7798         numpy.polyval
7799         xarray.polyval
7800         """
7801         from xarray.core.dataarray import DataArray
7802 
7803         variables = {}
7804         skipna_da = skipna
7805 
7806         x = get_clean_interp_index(self, dim, strict=False)
7807         xname = f"{self[dim].name}_"
7808         order = int(deg) + 1
7809         lhs = np.vander(x, order)
7810 
7811         if rcond is None:
7812             rcond = (
7813                 x.shape[0] * np.core.finfo(x.dtype).eps  # type: ignore[attr-defined]
7814             )
7815 
7816         # Weights:
7817         if w is not None:
7818             if isinstance(w, Hashable):
7819                 w = self.coords[w]
7820             w = np.asarray(w)
7821             if w.ndim != 1:
7822                 raise TypeError("Expected a 1-d array for weights.")
7823             if w.shape[0] != lhs.shape[0]:
7824                 raise TypeError(f"Expected w and {dim} to have the same length")
7825             lhs *= w[:, np.newaxis]
7826 
7827         # Scaling
7828         scale = np.sqrt((lhs * lhs).sum(axis=0))
7829         lhs /= scale
7830 
7831         degree_dim = utils.get_temp_dimname(self.dims, "degree")
7832 
7833         rank = np.linalg.matrix_rank(lhs)
7834 
7835         if full:
7836             rank = DataArray(rank, name=xname + "matrix_rank")
7837             variables[rank.name] = rank
7838             _sing = np.linalg.svd(lhs, compute_uv=False)
7839             sing = DataArray(
7840                 _sing,
7841                 dims=(degree_dim,),
7842                 coords={degree_dim: np.arange(rank - 1, -1, -1)},
7843                 name=xname + "singular_values",
7844             )
7845             variables[sing.name] = sing
7846 
7847         for name, da in self.data_vars.items():
7848             if dim not in da.dims:
7849                 continue
7850 
7851             if is_duck_dask_array(da.data) and (
7852                 rank != order or full or skipna is None
7853             ):
7854                 # Current algorithm with dask and skipna=False neither supports
7855                 # deficient ranks nor does it output the "full" info (issue dask/dask#6516)
7856                 skipna_da = True
7857             elif skipna is None:
7858                 skipna_da = bool(np.any(da.isnull()))
7859 
7860             dims_to_stack = [dimname for dimname in da.dims if dimname != dim]
7861             stacked_coords: dict[Hashable, DataArray] = {}
7862             if dims_to_stack:
7863                 stacked_dim = utils.get_temp_dimname(dims_to_stack, "stacked")
7864                 rhs = da.transpose(dim, *dims_to_stack).stack(
7865                     {stacked_dim: dims_to_stack}
7866                 )
7867                 stacked_coords = {stacked_dim: rhs[stacked_dim]}
7868                 scale_da = scale[:, np.newaxis]
7869             else:
7870                 rhs = da
7871                 scale_da = scale
7872 
7873             if w is not None:
7874                 rhs *= w[:, np.newaxis]
7875 
7876             with warnings.catch_warnings():
7877                 if full:  # Copy np.polyfit behavior
7878                     warnings.simplefilter("ignore", np.RankWarning)
7879                 else:  # Raise only once per variable
7880                     warnings.simplefilter("once", np.RankWarning)
7881 
7882                 coeffs, residuals = duck_array_ops.least_squares(
7883                     lhs, rhs.data, rcond=rcond, skipna=skipna_da
7884                 )
7885 
7886             if isinstance(name, str):
7887                 name = f"{name}_"
7888             else:
7889                 # Thus a ReprObject => polyfit was called on a DataArray
7890                 name = ""
7891 
7892             coeffs = DataArray(
7893                 coeffs / scale_da,
7894                 dims=[degree_dim] + list(stacked_coords.keys()),
7895                 coords={degree_dim: np.arange(order)[::-1], **stacked_coords},
7896                 name=name + "polyfit_coefficients",
7897             )
7898             if dims_to_stack:
7899                 coeffs = coeffs.unstack(stacked_dim)
7900             variables[coeffs.name] = coeffs
7901 
7902             if full or (cov is True):
7903                 residuals = DataArray(
7904                     residuals if dims_to_stack else residuals.squeeze(),
7905                     dims=list(stacked_coords.keys()),
7906                     coords=stacked_coords,
7907                     name=name + "polyfit_residuals",
7908                 )
7909                 if dims_to_stack:
7910                     residuals = residuals.unstack(stacked_dim)
7911                 variables[residuals.name] = residuals
7912 
7913             if cov:
7914                 Vbase = np.linalg.inv(np.dot(lhs.T, lhs))
7915                 Vbase /= np.outer(scale, scale)
7916                 if cov == "unscaled":
7917                     fac = 1
7918                 else:
7919                     if x.shape[0] <= order:
7920                         raise ValueError(
7921                             "The number of data points must exceed order to scale the covariance matrix."
7922                         )
7923                     fac = residuals / (x.shape[0] - order)
7924                 covariance = DataArray(Vbase, dims=("cov_i", "cov_j")) * fac
7925                 variables[name + "polyfit_covariance"] = covariance
7926 
7927         return type(self)(data_vars=variables, attrs=self.attrs.copy())
7928 
7929     def pad(
7930         self: T_Dataset,
7931         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,
7932         mode: PadModeOptions = "constant",
7933         stat_length: int
7934         | tuple[int, int]
7935         | Mapping[Any, tuple[int, int]]
7936         | None = None,
7937         constant_values: (
7938             float | tuple[float, float] | Mapping[Any, tuple[float, float]] | None
7939         ) = None,
7940         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
7941         reflect_type: PadReflectOptions = None,
7942         keep_attrs: bool | None = None,
7943         **pad_width_kwargs: Any,
7944     ) -> T_Dataset:
7945         """Pad this dataset along one or more dimensions.
7946 
7947         .. warning::
7948             This function is experimental and its behaviour is likely to change
7949             especially regarding padding of dimension coordinates (or IndexVariables).
7950 
7951         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
7952         coordinates will be padded with the same mode, otherwise coordinates
7953         are padded using the "constant" mode with fill_value dtypes.NA.
7954 
7955         Parameters
7956         ----------
7957         pad_width : mapping of hashable to tuple of int
7958             Mapping with the form of {dim: (pad_before, pad_after)}
7959             describing the number of values padded along each dimension.
7960             {dim: pad} is a shortcut for pad_before = pad_after = pad
7961         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
7962             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
7963             How to pad the DataArray (taken from numpy docs):
7964 
7965             - "constant": Pads with a constant value.
7966             - "edge": Pads with the edge values of array.
7967             - "linear_ramp": Pads with the linear ramp between end_value and the
7968               array edge value.
7969             - "maximum": Pads with the maximum value of all or part of the
7970               vector along each axis.
7971             - "mean": Pads with the mean value of all or part of the
7972               vector along each axis.
7973             - "median": Pads with the median value of all or part of the
7974               vector along each axis.
7975             - "minimum": Pads with the minimum value of all or part of the
7976               vector along each axis.
7977             - "reflect": Pads with the reflection of the vector mirrored on
7978               the first and last values of the vector along each axis.
7979             - "symmetric": Pads with the reflection of the vector mirrored
7980               along the edge of the array.
7981             - "wrap": Pads with the wrap of the vector along the axis.
7982               The first values are used to pad the end and the
7983               end values are used to pad the beginning.
7984 
7985         stat_length : int, tuple or mapping of hashable to tuple, default: None
7986             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
7987             values at edge of each axis used to calculate the statistic value.
7988             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
7989             statistic lengths along each dimension.
7990             ((before, after),) yields same before and after statistic lengths
7991             for each dimension.
7992             (stat_length,) or int is a shortcut for before = after = statistic
7993             length for all axes.
7994             Default is ``None``, to use the entire axis.
7995         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
7996             Used in 'constant'.  The values to set the padded values for each
7997             axis.
7998             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
7999             pad constants along each dimension.
8000             ``((before, after),)`` yields same before and after constants for each
8001             dimension.
8002             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
8003             all dimensions.
8004             Default is 0.
8005         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
8006             Used in 'linear_ramp'.  The values used for the ending value of the
8007             linear_ramp and that will form the edge of the padded array.
8008             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
8009             end values along each dimension.
8010             ``((before, after),)`` yields same before and after end values for each
8011             axis.
8012             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
8013             all axes.
8014             Default is 0.
8015         reflect_type : {"even", "odd", None}, optional
8016             Used in "reflect", and "symmetric".  The "even" style is the
8017             default with an unaltered reflection around the edge value.  For
8018             the "odd" style, the extended part of the array is created by
8019             subtracting the reflected values from two times the edge value.
8020         keep_attrs : bool or None, optional
8021             If True, the attributes (``attrs``) will be copied from the
8022             original object to the new one. If False, the new object
8023             will be returned without attributes.
8024         **pad_width_kwargs
8025             The keyword arguments form of ``pad_width``.
8026             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
8027 
8028         Returns
8029         -------
8030         padded : Dataset
8031             Dataset with the padded coordinates and data.
8032 
8033         See Also
8034         --------
8035         Dataset.shift, Dataset.roll, Dataset.bfill, Dataset.ffill, numpy.pad, dask.array.pad
8036 
8037         Notes
8038         -----
8039         By default when ``mode="constant"`` and ``constant_values=None``, integer types will be
8040         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion
8041         specify ``constant_values=np.nan``
8042 
8043         Padding coordinates will drop their corresponding index (if any) and will reset default
8044         indexes for dimension coordinates.
8045 
8046         Examples
8047         --------
8048         >>> ds = xr.Dataset({"foo": ("x", range(5))})
8049         >>> ds.pad(x=(1, 2))
8050         <xarray.Dataset>
8051         Dimensions:  (x: 8)
8052         Dimensions without coordinates: x
8053         Data variables:
8054             foo      (x) float64 nan 0.0 1.0 2.0 3.0 4.0 nan nan
8055         """
8056         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, "pad")
8057 
8058         if mode in ("edge", "reflect", "symmetric", "wrap"):
8059             coord_pad_mode = mode
8060             coord_pad_options = {
8061                 "stat_length": stat_length,
8062                 "constant_values": constant_values,
8063                 "end_values": end_values,
8064                 "reflect_type": reflect_type,
8065             }
8066         else:
8067             coord_pad_mode = "constant"
8068             coord_pad_options = {}
8069 
8070         if keep_attrs is None:
8071             keep_attrs = _get_keep_attrs(default=True)
8072 
8073         variables = {}
8074 
8075         # keep indexes that won't be affected by pad and drop all other indexes
8076         xindexes = self.xindexes
8077         pad_dims = set(pad_width)
8078         indexes = {}
8079         for k, idx in xindexes.items():
8080             if not pad_dims.intersection(xindexes.get_all_dims(k)):
8081                 indexes[k] = idx
8082 
8083         for name, var in self.variables.items():
8084             var_pad_width = {k: v for k, v in pad_width.items() if k in var.dims}
8085             if not var_pad_width:
8086                 variables[name] = var
8087             elif name in self.data_vars:
8088                 variables[name] = var.pad(
8089                     pad_width=var_pad_width,
8090                     mode=mode,
8091                     stat_length=stat_length,
8092                     constant_values=constant_values,
8093                     end_values=end_values,
8094                     reflect_type=reflect_type,
8095                     keep_attrs=keep_attrs,
8096                 )
8097             else:
8098                 variables[name] = var.pad(
8099                     pad_width=var_pad_width,
8100                     mode=coord_pad_mode,
8101                     keep_attrs=keep_attrs,
8102                     **coord_pad_options,  # type: ignore[arg-type]
8103                 )
8104                 # reset default index of dimension coordinates
8105                 if (name,) == var.dims:
8106                     dim_var = {name: variables[name]}
8107                     index = PandasIndex.from_variables(dim_var, options={})
8108                     index_vars = index.create_variables(dim_var)
8109                     indexes[name] = index
8110                     variables[name] = index_vars[name]
8111 
8112         attrs = self._attrs if keep_attrs else None
8113         return self._replace_with_new_dims(variables, indexes=indexes, attrs=attrs)
8114 
8115     def idxmin(
8116         self: T_Dataset,
8117         dim: Hashable | None = None,
8118         skipna: bool | None = None,
8119         fill_value: Any = xrdtypes.NA,
8120         keep_attrs: bool | None = None,
8121     ) -> T_Dataset:
8122         """Return the coordinate label of the minimum value along a dimension.
8123 
8124         Returns a new `Dataset` named after the dimension with the values of
8125         the coordinate labels along that dimension corresponding to minimum
8126         values along that dimension.
8127 
8128         In comparison to :py:meth:`~Dataset.argmin`, this returns the
8129         coordinate label while :py:meth:`~Dataset.argmin` returns the index.
8130 
8131         Parameters
8132         ----------
8133         dim : Hashable, optional
8134             Dimension over which to apply `idxmin`.  This is optional for 1D
8135             variables, but required for variables with 2 or more dimensions.
8136         skipna : bool or None, optional
8137             If True, skip missing values (as marked by NaN). By default, only
8138             skips missing values for ``float``, ``complex``, and ``object``
8139             dtypes; other dtypes either do not have a sentinel missing value
8140             (``int``) or ``skipna=True`` has not been implemented
8141             (``datetime64`` or ``timedelta64``).
8142         fill_value : Any, default: NaN
8143             Value to be filled in case all of the values along a dimension are
8144             null.  By default this is NaN.  The fill value and result are
8145             automatically converted to a compatible dtype if possible.
8146             Ignored if ``skipna`` is False.
8147         keep_attrs : bool or None, optional
8148             If True, the attributes (``attrs``) will be copied from the
8149             original object to the new one. If False, the new object
8150             will be returned without attributes.
8151 
8152         Returns
8153         -------
8154         reduced : Dataset
8155             New `Dataset` object with `idxmin` applied to its data and the
8156             indicated dimension removed.
8157 
8158         See Also
8159         --------
8160         DataArray.idxmin, Dataset.idxmax, Dataset.min, Dataset.argmin
8161 
8162         Examples
8163         --------
8164         >>> array1 = xr.DataArray(
8165         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
8166         ... )
8167         >>> array2 = xr.DataArray(
8168         ...     [
8169         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
8170         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
8171         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
8172         ...     ],
8173         ...     dims=["y", "x"],
8174         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
8175         ... )
8176         >>> ds = xr.Dataset({"int": array1, "float": array2})
8177         >>> ds.min(dim="x")
8178         <xarray.Dataset>
8179         Dimensions:  (y: 3)
8180         Coordinates:
8181           * y        (y) int64 -1 0 1
8182         Data variables:
8183             int      int64 -2
8184             float    (y) float64 -2.0 -4.0 1.0
8185         >>> ds.argmin(dim="x")
8186         <xarray.Dataset>
8187         Dimensions:  (y: 3)
8188         Coordinates:
8189           * y        (y) int64 -1 0 1
8190         Data variables:
8191             int      int64 4
8192             float    (y) int64 4 0 2
8193         >>> ds.idxmin(dim="x")
8194         <xarray.Dataset>
8195         Dimensions:  (y: 3)
8196         Coordinates:
8197           * y        (y) int64 -1 0 1
8198         Data variables:
8199             int      <U1 'e'
8200             float    (y) object 'e' 'a' 'c'
8201         """
8202         return self.map(
8203             methodcaller(
8204                 "idxmin",
8205                 dim=dim,
8206                 skipna=skipna,
8207                 fill_value=fill_value,
8208                 keep_attrs=keep_attrs,
8209             )
8210         )
8211 
8212     def idxmax(
8213         self: T_Dataset,
8214         dim: Hashable | None = None,
8215         skipna: bool | None = None,
8216         fill_value: Any = xrdtypes.NA,
8217         keep_attrs: bool | None = None,
8218     ) -> T_Dataset:
8219         """Return the coordinate label of the maximum value along a dimension.
8220 
8221         Returns a new `Dataset` named after the dimension with the values of
8222         the coordinate labels along that dimension corresponding to maximum
8223         values along that dimension.
8224 
8225         In comparison to :py:meth:`~Dataset.argmax`, this returns the
8226         coordinate label while :py:meth:`~Dataset.argmax` returns the index.
8227 
8228         Parameters
8229         ----------
8230         dim : str, optional
8231             Dimension over which to apply `idxmax`.  This is optional for 1D
8232             variables, but required for variables with 2 or more dimensions.
8233         skipna : bool or None, optional
8234             If True, skip missing values (as marked by NaN). By default, only
8235             skips missing values for ``float``, ``complex``, and ``object``
8236             dtypes; other dtypes either do not have a sentinel missing value
8237             (``int``) or ``skipna=True`` has not been implemented
8238             (``datetime64`` or ``timedelta64``).
8239         fill_value : Any, default: NaN
8240             Value to be filled in case all of the values along a dimension are
8241             null.  By default this is NaN.  The fill value and result are
8242             automatically converted to a compatible dtype if possible.
8243             Ignored if ``skipna`` is False.
8244         keep_attrs : bool or None, optional
8245             If True, the attributes (``attrs``) will be copied from the
8246             original object to the new one. If False, the new object
8247             will be returned without attributes.
8248 
8249         Returns
8250         -------
8251         reduced : Dataset
8252             New `Dataset` object with `idxmax` applied to its data and the
8253             indicated dimension removed.
8254 
8255         See Also
8256         --------
8257         DataArray.idxmax, Dataset.idxmin, Dataset.max, Dataset.argmax
8258 
8259         Examples
8260         --------
8261         >>> array1 = xr.DataArray(
8262         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
8263         ... )
8264         >>> array2 = xr.DataArray(
8265         ...     [
8266         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
8267         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
8268         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
8269         ...     ],
8270         ...     dims=["y", "x"],
8271         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
8272         ... )
8273         >>> ds = xr.Dataset({"int": array1, "float": array2})
8274         >>> ds.max(dim="x")
8275         <xarray.Dataset>
8276         Dimensions:  (y: 3)
8277         Coordinates:
8278           * y        (y) int64 -1 0 1
8279         Data variables:
8280             int      int64 2
8281             float    (y) float64 2.0 2.0 1.0
8282         >>> ds.argmax(dim="x")
8283         <xarray.Dataset>
8284         Dimensions:  (y: 3)
8285         Coordinates:
8286           * y        (y) int64 -1 0 1
8287         Data variables:
8288             int      int64 1
8289             float    (y) int64 0 2 2
8290         >>> ds.idxmax(dim="x")
8291         <xarray.Dataset>
8292         Dimensions:  (y: 3)
8293         Coordinates:
8294           * y        (y) int64 -1 0 1
8295         Data variables:
8296             int      <U1 'b'
8297             float    (y) object 'a' 'c' 'c'
8298         """
8299         return self.map(
8300             methodcaller(
8301                 "idxmax",
8302                 dim=dim,
8303                 skipna=skipna,
8304                 fill_value=fill_value,
8305                 keep_attrs=keep_attrs,
8306             )
8307         )
8308 
8309     def argmin(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
8310         """Indices of the minima of the member variables.
8311 
8312         If there are multiple minima, the indices of the first one found will be
8313         returned.
8314 
8315         Parameters
8316         ----------
8317         dim : Hashable, optional
8318             The dimension over which to find the minimum. By default, finds minimum over
8319             all dimensions - for now returning an int for backward compatibility, but
8320             this is deprecated, in future will be an error, since DataArray.argmin will
8321             return a dict with indices for all dimensions, which does not make sense for
8322             a Dataset.
8323         keep_attrs : bool, optional
8324             If True, the attributes (`attrs`) will be copied from the original
8325             object to the new one.  If False (default), the new object will be
8326             returned without attributes.
8327         skipna : bool, optional
8328             If True, skip missing values (as marked by NaN). By default, only
8329             skips missing values for float dtypes; other dtypes either do not
8330             have a sentinel missing value (int) or skipna=True has not been
8331             implemented (object, datetime64 or timedelta64).
8332 
8333         Returns
8334         -------
8335         result : Dataset
8336 
8337         See Also
8338         --------
8339         DataArray.argmin
8340         """
8341         if dim is None:
8342             warnings.warn(
8343                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8344                 "dim changes to return a dict of indices of each dimension, for "
8345                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8346                 "since we don't return a dict of Datasets.",
8347                 DeprecationWarning,
8348                 stacklevel=2,
8349             )
8350         if (
8351             dim is None
8352             or (not isinstance(dim, Sequence) and dim is not ...)
8353             or isinstance(dim, str)
8354         ):
8355             # Return int index if single dimension is passed, and is not part of a
8356             # sequence
8357             argmin_func = getattr(duck_array_ops, "argmin")
8358             return self.reduce(
8359                 argmin_func, dim=None if dim is None else [dim], **kwargs
8360             )
8361         else:
8362             raise ValueError(
8363                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8364                 "dicts cannot be contained in a Dataset, so cannot call "
8365                 "Dataset.argmin() with a sequence or ... for dim"
8366             )
8367 
8368     def argmax(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
8369         """Indices of the maxima of the member variables.
8370 
8371         If there are multiple maxima, the indices of the first one found will be
8372         returned.
8373 
8374         Parameters
8375         ----------
8376         dim : str, optional
8377             The dimension over which to find the maximum. By default, finds maximum over
8378             all dimensions - for now returning an int for backward compatibility, but
8379             this is deprecated, in future will be an error, since DataArray.argmax will
8380             return a dict with indices for all dimensions, which does not make sense for
8381             a Dataset.
8382         keep_attrs : bool, optional
8383             If True, the attributes (`attrs`) will be copied from the original
8384             object to the new one.  If False (default), the new object will be
8385             returned without attributes.
8386         skipna : bool, optional
8387             If True, skip missing values (as marked by NaN). By default, only
8388             skips missing values for float dtypes; other dtypes either do not
8389             have a sentinel missing value (int) or skipna=True has not been
8390             implemented (object, datetime64 or timedelta64).
8391 
8392         Returns
8393         -------
8394         result : Dataset
8395 
8396         See Also
8397         --------
8398         DataArray.argmax
8399 
8400         """
8401         if dim is None:
8402             warnings.warn(
8403                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8404                 "dim changes to return a dict of indices of each dimension, for "
8405                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8406                 "since we don't return a dict of Datasets.",
8407                 DeprecationWarning,
8408                 stacklevel=2,
8409             )
8410         if (
8411             dim is None
8412             or (not isinstance(dim, Sequence) and dim is not ...)
8413             or isinstance(dim, str)
8414         ):
8415             # Return int index if single dimension is passed, and is not part of a
8416             # sequence
8417             argmax_func = getattr(duck_array_ops, "argmax")
8418             return self.reduce(
8419                 argmax_func, dim=None if dim is None else [dim], **kwargs
8420             )
8421         else:
8422             raise ValueError(
8423                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8424                 "dicts cannot be contained in a Dataset, so cannot call "
8425                 "Dataset.argmin() with a sequence or ... for dim"
8426             )
8427 
8428     def query(
8429         self: T_Dataset,
8430         queries: Mapping[Any, Any] | None = None,
8431         parser: QueryParserOptions = "pandas",
8432         engine: QueryEngineOptions = None,
8433         missing_dims: ErrorOptionsWithWarn = "raise",
8434         **queries_kwargs: Any,
8435     ) -> T_Dataset:
8436         """Return a new dataset with each array indexed along the specified
8437         dimension(s), where the indexers are given as strings containing
8438         Python expressions to be evaluated against the data variables in the
8439         dataset.
8440 
8441         Parameters
8442         ----------
8443         queries : dict-like, optional
8444             A dict-like with keys matching dimensions and values given by strings
8445             containing Python expressions to be evaluated against the data variables
8446             in the dataset. The expressions will be evaluated using the pandas
8447             eval() function, and can contain any valid Python expressions but cannot
8448             contain any Python statements.
8449         parser : {"pandas", "python"}, default: "pandas"
8450             The parser to use to construct the syntax tree from the expression.
8451             The default of 'pandas' parses code slightly different than standard
8452             Python. Alternatively, you can parse an expression using the 'python'
8453             parser to retain strict Python semantics.
8454         engine : {"python", "numexpr", None}, default: None
8455             The engine used to evaluate the expression. Supported engines are:
8456 
8457             - None: tries to use numexpr, falls back to python
8458             - "numexpr": evaluates expressions using numexpr
8459             - "python": performs operations as if you had eval’d in top level python
8460 
8461         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
8462             What to do if dimensions that should be selected from are not present in the
8463             Dataset:
8464 
8465             - "raise": raise an exception
8466             - "warn": raise a warning, and ignore the missing dimensions
8467             - "ignore": ignore the missing dimensions
8468 
8469         **queries_kwargs : {dim: query, ...}, optional
8470             The keyword arguments form of ``queries``.
8471             One of queries or queries_kwargs must be provided.
8472 
8473         Returns
8474         -------
8475         obj : Dataset
8476             A new Dataset with the same contents as this dataset, except each
8477             array and dimension is indexed by the results of the appropriate
8478             queries.
8479 
8480         See Also
8481         --------
8482         Dataset.isel
8483         pandas.eval
8484 
8485         Examples
8486         --------
8487         >>> a = np.arange(0, 5, 1)
8488         >>> b = np.linspace(0, 1, 5)
8489         >>> ds = xr.Dataset({"a": ("x", a), "b": ("x", b)})
8490         >>> ds
8491         <xarray.Dataset>
8492         Dimensions:  (x: 5)
8493         Dimensions without coordinates: x
8494         Data variables:
8495             a        (x) int64 0 1 2 3 4
8496             b        (x) float64 0.0 0.25 0.5 0.75 1.0
8497         >>> ds.query(x="a > 2")
8498         <xarray.Dataset>
8499         Dimensions:  (x: 2)
8500         Dimensions without coordinates: x
8501         Data variables:
8502             a        (x) int64 3 4
8503             b        (x) float64 0.75 1.0
8504         """
8505 
8506         # allow queries to be given either as a dict or as kwargs
8507         queries = either_dict_or_kwargs(queries, queries_kwargs, "query")
8508 
8509         # check queries
8510         for dim, expr in queries.items():
8511             if not isinstance(expr, str):
8512                 msg = f"expr for dim {dim} must be a string to be evaluated, {type(expr)} given"
8513                 raise ValueError(msg)
8514 
8515         # evaluate the queries to create the indexers
8516         indexers = {
8517             dim: pd.eval(expr, resolvers=[self], parser=parser, engine=engine)
8518             for dim, expr in queries.items()
8519         }
8520 
8521         # apply the selection
8522         return self.isel(indexers, missing_dims=missing_dims)
8523 
8524     def curvefit(
8525         self: T_Dataset,
8526         coords: str | DataArray | Iterable[str | DataArray],
8527         func: Callable[..., Any],
8528         reduce_dims: Dims = None,
8529         skipna: bool = True,
8530         p0: dict[str, Any] | None = None,
8531         bounds: dict[str, Any] | None = None,
8532         param_names: Sequence[str] | None = None,
8533         kwargs: dict[str, Any] | None = None,
8534     ) -> T_Dataset:
8535         """
8536         Curve fitting optimization for arbitrary functions.
8537 
8538         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
8539 
8540         Parameters
8541         ----------
8542         coords : hashable, DataArray, or sequence of hashable or DataArray
8543             Independent coordinate(s) over which to perform the curve fitting. Must share
8544             at least one dimension with the calling object. When fitting multi-dimensional
8545             functions, supply `coords` as a sequence in the same order as arguments in
8546             `func`. To fit along existing dimensions of the calling object, `coords` can
8547             also be specified as a str or sequence of strs.
8548         func : callable
8549             User specified function in the form `f(x, *params)` which returns a numpy
8550             array of length `len(x)`. `params` are the fittable parameters which are optimized
8551             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
8552             coordinates, e.g. `f((x0, x1), *params)`.
8553         reduce_dims : str, Iterable of Hashable or None, optional
8554             Additional dimension(s) over which to aggregate while fitting. For example,
8555             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
8556             aggregate all lat and lon points and fit the specified function along the
8557             time dimension.
8558         skipna : bool, default: True
8559             Whether to skip missing values when fitting. Default is True.
8560         p0 : dict-like, optional
8561             Optional dictionary of parameter names to initial guesses passed to the
8562             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
8563             be assigned initial values following the default scipy behavior.
8564         bounds : dict-like, optional
8565             Optional dictionary of parameter names to bounding values passed to the
8566             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
8567             will be unbounded following the default scipy behavior.
8568         param_names : sequence of hashable, optional
8569             Sequence of names for the fittable parameters of `func`. If not supplied,
8570             this will be automatically determined by arguments of `func`. `param_names`
8571             should be manually supplied when fitting a function that takes a variable
8572             number of parameters.
8573         **kwargs : optional
8574             Additional keyword arguments to passed to scipy curve_fit.
8575 
8576         Returns
8577         -------
8578         curvefit_results : Dataset
8579             A single dataset which contains:
8580 
8581             [var]_curvefit_coefficients
8582                 The coefficients of the best fit.
8583             [var]_curvefit_covariance
8584                 The covariance matrix of the coefficient estimates.
8585 
8586         See Also
8587         --------
8588         Dataset.polyfit
8589         scipy.optimize.curve_fit
8590         """
8591         from scipy.optimize import curve_fit
8592 
8593         from xarray.core.alignment import broadcast
8594         from xarray.core.computation import apply_ufunc
8595         from xarray.core.dataarray import _THIS_ARRAY, DataArray
8596 
8597         if p0 is None:
8598             p0 = {}
8599         if bounds is None:
8600             bounds = {}
8601         if kwargs is None:
8602             kwargs = {}
8603 
8604         reduce_dims_: list[Hashable]
8605         if not reduce_dims:
8606             reduce_dims_ = []
8607         elif isinstance(reduce_dims, str) or not isinstance(reduce_dims, Iterable):
8608             reduce_dims_ = [reduce_dims]
8609         else:
8610             reduce_dims_ = list(reduce_dims)
8611 
8612         if (
8613             isinstance(coords, str)
8614             or isinstance(coords, DataArray)
8615             or not isinstance(coords, Iterable)
8616         ):
8617             coords = [coords]
8618         coords_: Sequence[DataArray] = [
8619             self[coord] if isinstance(coord, str) else coord for coord in coords
8620         ]
8621 
8622         # Determine whether any coords are dims on self
8623         for coord in coords_:
8624             reduce_dims_ += [c for c in self.dims if coord.equals(self[c])]
8625         reduce_dims_ = list(set(reduce_dims_))
8626         preserved_dims = list(set(self.dims) - set(reduce_dims_))
8627         if not reduce_dims_:
8628             raise ValueError(
8629                 "No arguments to `coords` were identified as a dimension on the calling "
8630                 "object, and no dims were supplied to `reduce_dims`. This would result "
8631                 "in fitting on scalar data."
8632             )
8633 
8634         # Broadcast all coords with each other
8635         coords_ = broadcast(*coords_)
8636         coords_ = [
8637             coord.broadcast_like(self, exclude=preserved_dims) for coord in coords_
8638         ]
8639 
8640         params, func_args = _get_func_args(func, param_names)
8641         param_defaults, bounds_defaults = _initialize_curvefit_params(
8642             params, p0, bounds, func_args
8643         )
8644         n_params = len(params)
8645         kwargs.setdefault("p0", [param_defaults[p] for p in params])
8646         kwargs.setdefault(
8647             "bounds",
8648             [
8649                 [bounds_defaults[p][0] for p in params],
8650                 [bounds_defaults[p][1] for p in params],
8651             ],
8652         )
8653 
8654         def _wrapper(Y, *coords_, **kwargs):
8655             # Wrap curve_fit with raveled coordinates and pointwise NaN handling
8656             x = np.vstack([c.ravel() for c in coords_])
8657             y = Y.ravel()
8658             if skipna:
8659                 mask = np.all([np.any(~np.isnan(x), axis=0), ~np.isnan(y)], axis=0)
8660                 x = x[:, mask]
8661                 y = y[mask]
8662                 if not len(y):
8663                     popt = np.full([n_params], np.nan)
8664                     pcov = np.full([n_params, n_params], np.nan)
8665                     return popt, pcov
8666             x = np.squeeze(x)
8667             popt, pcov = curve_fit(func, x, y, **kwargs)
8668             return popt, pcov
8669 
8670         result = type(self)()
8671         for name, da in self.data_vars.items():
8672             if name is _THIS_ARRAY:
8673                 name = ""
8674             else:
8675                 name = f"{str(name)}_"
8676 
8677             popt, pcov = apply_ufunc(
8678                 _wrapper,
8679                 da,
8680                 *coords_,
8681                 vectorize=True,
8682                 dask="parallelized",
8683                 input_core_dims=[reduce_dims_ for d in range(len(coords_) + 1)],
8684                 output_core_dims=[["param"], ["cov_i", "cov_j"]],
8685                 dask_gufunc_kwargs={
8686                     "output_sizes": {
8687                         "param": n_params,
8688                         "cov_i": n_params,
8689                         "cov_j": n_params,
8690                     },
8691                 },
8692                 output_dtypes=(np.float64, np.float64),
8693                 exclude_dims=set(reduce_dims_),
8694                 kwargs=kwargs,
8695             )
8696             result[name + "curvefit_coefficients"] = popt
8697             result[name + "curvefit_covariance"] = pcov
8698 
8699         result = result.assign_coords(
8700             {"param": params, "cov_i": params, "cov_j": params}
8701         )
8702         result.attrs = self.attrs.copy()
8703 
8704         return result
8705 
8706     def drop_duplicates(
8707         self: T_Dataset,
8708         dim: Hashable | Iterable[Hashable],
8709         keep: Literal["first", "last", False] = "first",
8710     ) -> T_Dataset:
8711         """Returns a new Dataset with duplicate dimension values removed.
8712 
8713         Parameters
8714         ----------
8715         dim : dimension label or labels
8716             Pass `...` to drop duplicates along all dimensions.
8717         keep : {"first", "last", False}, default: "first"
8718             Determines which duplicates (if any) to keep.
8719             - ``"first"`` : Drop duplicates except for the first occurrence.
8720             - ``"last"`` : Drop duplicates except for the last occurrence.
8721             - False : Drop all duplicates.
8722 
8723         Returns
8724         -------
8725         Dataset
8726 
8727         See Also
8728         --------
8729         DataArray.drop_duplicates
8730         """
8731         if isinstance(dim, str):
8732             dims: Iterable = (dim,)
8733         elif dim is ...:
8734             dims = self.dims
8735         elif not isinstance(dim, Iterable):
8736             dims = [dim]
8737         else:
8738             dims = dim
8739 
8740         missing_dims = set(dims) - set(self.dims)
8741         if missing_dims:
8742             raise ValueError(f"'{missing_dims}' not found in dimensions")
8743 
8744         indexes = {dim: ~self.get_index(dim).duplicated(keep=keep) for dim in dims}
8745         return self.isel(indexes)
8746 
8747     def convert_calendar(
8748         self: T_Dataset,
8749         calendar: CFCalendar,
8750         dim: Hashable = "time",
8751         align_on: Literal["date", "year", None] = None,
8752         missing: Any | None = None,
8753         use_cftime: bool | None = None,
8754     ) -> T_Dataset:
8755         """Convert the Dataset to another calendar.
8756 
8757         Only converts the individual timestamps, does not modify any data except
8758         in dropping invalid/surplus dates or inserting missing dates.
8759 
8760         If the source and target calendars are either no_leap, all_leap or a
8761         standard type, only the type of the time array is modified.
8762         When converting to a leap year from a non-leap year, the 29th of February
8763         is removed from the array. In the other direction the 29th of February
8764         will be missing in the output, unless `missing` is specified,
8765         in which case that value is inserted.
8766 
8767         For conversions involving `360_day` calendars, see Notes.
8768 
8769         This method is safe to use with sub-daily data as it doesn't touch the
8770         time part of the timestamps.
8771 
8772         Parameters
8773         ---------
8774         calendar : str
8775             The target calendar name.
8776         dim : Hashable, default: "time"
8777             Name of the time coordinate.
8778         align_on : {None, 'date', 'year'}, optional
8779             Must be specified when either source or target is a `360_day` calendar,
8780             ignored otherwise. See Notes.
8781         missing : Any or None, optional
8782             By default, i.e. if the value is None, this method will simply attempt
8783             to convert the dates in the source calendar to the same dates in the
8784             target calendar, and drop any of those that are not possible to
8785             represent.  If a value is provided, a new time coordinate will be
8786             created in the target calendar with the same frequency as the original
8787             time coordinate; for any dates that are not present in the source, the
8788             data will be filled with this value.  Note that using this mode requires
8789             that the source data have an inferable frequency; for more information
8790             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
8791             target calendar combinations, this could result in many missing values, see notes.
8792         use_cftime : bool or None, optional
8793             Whether to use cftime objects in the output, only used if `calendar`
8794             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
8795             If True, the new time axis uses cftime objects.
8796             If None (default), it uses :py:class:`numpy.datetime64` values if the
8797             date range permits it, and :py:class:`cftime.datetime` objects if not.
8798             If False, it uses :py:class:`numpy.datetime64`  or fails.
8799 
8800         Returns
8801         -------
8802         Dataset
8803             Copy of the dataarray with the time coordinate converted to the
8804             target calendar. If 'missing' was None (default), invalid dates in
8805             the new calendar are dropped, but missing dates are not inserted.
8806             If `missing` was given, the new data is reindexed to have a time axis
8807             with the same frequency as the source, but in the new calendar; any
8808             missing datapoints are filled with `missing`.
8809 
8810         Notes
8811         -----
8812         Passing a value to `missing` is only usable if the source's time coordinate as an
8813         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
8814         if the target coordinate, generated from this frequency, has dates equivalent to the
8815         source. It is usually **not** appropriate to use this mode with:
8816 
8817         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
8818         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
8819             or 'mH' where 24 % m != 0).
8820 
8821         If one of the source or target calendars is `"360_day"`, `align_on` must
8822         be specified and two options are offered.
8823 
8824         - "year"
8825             The dates are translated according to their relative position in the year,
8826             ignoring their original month and day information, meaning that the
8827             missing/surplus days are added/removed at regular intervals.
8828 
8829             From a `360_day` to a standard calendar, the output will be missing the
8830             following dates (day of year in parentheses):
8831 
8832             To a leap year:
8833                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
8834                 September 31st (275) and November 30th (335).
8835             To a non-leap year:
8836                 February 6th (36), April 19th (109), July 2nd (183),
8837                 September 12th (255), November 25th (329).
8838 
8839             From a standard calendar to a `"360_day"`, the following dates in the
8840             source array will be dropped:
8841 
8842             From a leap year:
8843                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
8844                 September 31st (275), December 1st (336)
8845             From a non-leap year:
8846                 February 6th (37), April 20th (110), July 2nd (183),
8847                 September 13th (256), November 25th (329)
8848 
8849             This option is best used on daily and subdaily data.
8850 
8851         - "date"
8852             The month/day information is conserved and invalid dates are dropped
8853             from the output. This means that when converting from a `"360_day"` to a
8854             standard calendar, all 31st (Jan, March, May, July, August, October and
8855             December) will be missing as there is no equivalent dates in the
8856             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
8857             will be dropped as there are no equivalent dates in a standard calendar.
8858 
8859             This option is best used with data on a frequency coarser than daily.
8860         """
8861         return convert_calendar(
8862             self,
8863             calendar,
8864             dim=dim,
8865             align_on=align_on,
8866             missing=missing,
8867             use_cftime=use_cftime,
8868         )
8869 
8870     def interp_calendar(
8871         self: T_Dataset,
8872         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
8873         dim: Hashable = "time",
8874     ) -> T_Dataset:
8875         """Interpolates the Dataset to another calendar based on decimal year measure.
8876 
8877         Each timestamp in `source` and `target` are first converted to their decimal
8878         year equivalent then `source` is interpolated on the target coordinate.
8879         The decimal year of a timestamp is its year plus its sub-year component
8880         converted to the fraction of its year. For example "2000-03-01 12:00" is
8881         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
8882 
8883         This method should only be used when the time (HH:MM:SS) information of
8884         time coordinate is not important.
8885 
8886         Parameters
8887         ----------
8888         target: DataArray or DatetimeIndex or CFTimeIndex
8889             The target time coordinate of a valid dtype
8890             (np.datetime64 or cftime objects)
8891         dim : Hashable, default: "time"
8892             The time coordinate name.
8893 
8894         Return
8895         ------
8896         DataArray
8897             The source interpolated on the decimal years of target,
8898         """
8899         return interp_calendar(self, target, dim=dim)
8900 
8901     def groupby(
8902         self,
8903         group: Hashable | DataArray | IndexVariable,
8904         squeeze: bool = True,
8905         restore_coord_dims: bool = False,
8906     ) -> DatasetGroupBy:
8907         """Returns a DatasetGroupBy object for performing grouped operations.
8908 
8909         Parameters
8910         ----------
8911         group : Hashable, DataArray or IndexVariable
8912             Array whose unique values should be used to group this array. If a
8913             string, must be the name of a variable contained in this dataset.
8914         squeeze : bool, default: True
8915             If "group" is a dimension of any arrays in this dataset, `squeeze`
8916             controls whether the subarrays have a dimension of length 1 along
8917             that dimension or if the dimension is squeezed out.
8918         restore_coord_dims : bool, default: False
8919             If True, also restore the dimension order of multi-dimensional
8920             coordinates.
8921 
8922         Returns
8923         -------
8924         grouped : DatasetGroupBy
8925             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
8926             iterated over in the form of `(unique_value, grouped_array)` pairs.
8927 
8928         See Also
8929         --------
8930         Dataset.groupby_bins
8931         DataArray.groupby
8932         core.groupby.DatasetGroupBy
8933         pandas.DataFrame.groupby
8934         """
8935         from xarray.core.groupby import DatasetGroupBy
8936 
8937         # While we don't generally check the type of every arg, passing
8938         # multiple dimensions as multiple arguments is common enough, and the
8939         # consequences hidden enough (strings evaluate as true) to warrant
8940         # checking here.
8941         # A future version could make squeeze kwarg only, but would face
8942         # backward-compat issues.
8943         if not isinstance(squeeze, bool):
8944             raise TypeError(
8945                 f"`squeeze` must be True or False, but {squeeze} was supplied"
8946             )
8947 
8948         return DatasetGroupBy(
8949             self, group, squeeze=squeeze, restore_coord_dims=restore_coord_dims
8950         )
8951 
8952     def groupby_bins(
8953         self,
8954         group: Hashable | DataArray | IndexVariable,
8955         bins: ArrayLike,
8956         right: bool = True,
8957         labels: ArrayLike | None = None,
8958         precision: int = 3,
8959         include_lowest: bool = False,
8960         squeeze: bool = True,
8961         restore_coord_dims: bool = False,
8962     ) -> DatasetGroupBy:
8963         """Returns a DatasetGroupBy object for performing grouped operations.
8964 
8965         Rather than using all unique values of `group`, the values are discretized
8966         first by applying `pandas.cut` [1]_ to `group`.
8967 
8968         Parameters
8969         ----------
8970         group : Hashable, DataArray or IndexVariable
8971             Array whose binned values should be used to group this array. If a
8972             string, must be the name of a variable contained in this dataset.
8973         bins : int or array-like
8974             If bins is an int, it defines the number of equal-width bins in the
8975             range of x. However, in this case, the range of x is extended by .1%
8976             on each side to include the min or max values of x. If bins is a
8977             sequence it defines the bin edges allowing for non-uniform bin
8978             width. No extension of the range of x is done in this case.
8979         right : bool, default: True
8980             Indicates whether the bins include the rightmost edge or not. If
8981             right == True (the default), then the bins [1,2,3,4] indicate
8982             (1,2], (2,3], (3,4].
8983         labels : array-like or bool, default: None
8984             Used as labels for the resulting bins. Must be of the same length as
8985             the resulting bins. If False, string bin labels are assigned by
8986             `pandas.cut`.
8987         precision : int, default: 3
8988             The precision at which to store and display the bins labels.
8989         include_lowest : bool, default: False
8990             Whether the first interval should be left-inclusive or not.
8991         squeeze : bool, default: True
8992             If "group" is a dimension of any arrays in this dataset, `squeeze`
8993             controls whether the subarrays have a dimension of length 1 along
8994             that dimension or if the dimension is squeezed out.
8995         restore_coord_dims : bool, default: False
8996             If True, also restore the dimension order of multi-dimensional
8997             coordinates.
8998 
8999         Returns
9000         -------
9001         grouped : DatasetGroupBy
9002             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
9003             iterated over in the form of `(unique_value, grouped_array)` pairs.
9004             The name of the group has the added suffix `_bins` in order to
9005             distinguish it from the original variable.
9006 
9007         See Also
9008         --------
9009         Dataset.groupby
9010         DataArray.groupby_bins
9011         core.groupby.DatasetGroupBy
9012         pandas.DataFrame.groupby
9013 
9014         References
9015         ----------
9016         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
9017         """
9018         from xarray.core.groupby import DatasetGroupBy
9019 
9020         return DatasetGroupBy(
9021             self,
9022             group,
9023             squeeze=squeeze,
9024             bins=bins,
9025             restore_coord_dims=restore_coord_dims,
9026             cut_kwargs={
9027                 "right": right,
9028                 "labels": labels,
9029                 "precision": precision,
9030                 "include_lowest": include_lowest,
9031             },
9032         )
9033 
9034     def weighted(self, weights: DataArray) -> DatasetWeighted:
9035         """
9036         Weighted Dataset operations.
9037 
9038         Parameters
9039         ----------
9040         weights : DataArray
9041             An array of weights associated with the values in this Dataset.
9042             Each value in the data contributes to the reduction operation
9043             according to its associated weight.
9044 
9045         Notes
9046         -----
9047         ``weights`` must be a DataArray and cannot contain missing values.
9048         Missing values can be replaced by ``weights.fillna(0)``.
9049 
9050         Returns
9051         -------
9052         core.weighted.DatasetWeighted
9053 
9054         See Also
9055         --------
9056         DataArray.weighted
9057         """
9058         from xarray.core.weighted import DatasetWeighted
9059 
9060         return DatasetWeighted(self, weights)
9061 
9062     def rolling(
9063         self,
9064         dim: Mapping[Any, int] | None = None,
9065         min_periods: int | None = None,
9066         center: bool | Mapping[Any, bool] = False,
9067         **window_kwargs: int,
9068     ) -> DatasetRolling:
9069         """
9070         Rolling window object for Datasets.
9071 
9072         Parameters
9073         ----------
9074         dim : dict, optional
9075             Mapping from the dimension name to create the rolling iterator
9076             along (e.g. `time`) to its moving window size.
9077         min_periods : int or None, default: None
9078             Minimum number of observations in window required to have a value
9079             (otherwise result is NA). The default, None, is equivalent to
9080             setting min_periods equal to the size of the window.
9081         center : bool or Mapping to int, default: False
9082             Set the labels at the center of the window.
9083         **window_kwargs : optional
9084             The keyword arguments form of ``dim``.
9085             One of dim or window_kwargs must be provided.
9086 
9087         Returns
9088         -------
9089         core.rolling.DatasetRolling
9090 
9091         See Also
9092         --------
9093         core.rolling.DatasetRolling
9094         DataArray.rolling
9095         """
9096         from xarray.core.rolling import DatasetRolling
9097 
9098         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
9099         return DatasetRolling(self, dim, min_periods=min_periods, center=center)
9100 
9101     def coarsen(
9102         self,
9103         dim: Mapping[Any, int] | None = None,
9104         boundary: CoarsenBoundaryOptions = "exact",
9105         side: SideOptions | Mapping[Any, SideOptions] = "left",
9106         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
9107         **window_kwargs: int,
9108     ) -> DatasetCoarsen:
9109         """
9110         Coarsen object for Datasets.
9111 
9112         Parameters
9113         ----------
9114         dim : mapping of hashable to int, optional
9115             Mapping from the dimension name to the window size.
9116         boundary : {"exact", "trim", "pad"}, default: "exact"
9117             If 'exact', a ValueError will be raised if dimension size is not a
9118             multiple of the window size. If 'trim', the excess entries are
9119             dropped. If 'pad', NA will be padded.
9120         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
9121         coord_func : str or mapping of hashable to str, default: "mean"
9122             function (name) that is applied to the coordinates,
9123             or a mapping from coordinate name to function (name).
9124 
9125         Returns
9126         -------
9127         core.rolling.DatasetCoarsen
9128 
9129         See Also
9130         --------
9131         core.rolling.DatasetCoarsen
9132         DataArray.coarsen
9133         """
9134         from xarray.core.rolling import DatasetCoarsen
9135 
9136         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
9137         return DatasetCoarsen(
9138             self,
9139             dim,
9140             boundary=boundary,
9141             side=side,
9142             coord_func=coord_func,
9143         )
9144 
9145     def resample(
9146         self,
9147         indexer: Mapping[Any, str] | None = None,
9148         skipna: bool | None = None,
9149         closed: SideOptions | None = None,
9150         label: SideOptions | None = None,
9151         base: int | None = None,
9152         offset: pd.Timedelta | datetime.timedelta | str | None = None,
9153         origin: str | DatetimeLike = "start_day",
9154         keep_attrs: bool | None = None,
9155         loffset: datetime.timedelta | str | None = None,
9156         restore_coord_dims: bool | None = None,
9157         **indexer_kwargs: str,
9158     ) -> DatasetResample:
9159         """Returns a Resample object for performing resampling operations.
9160 
9161         Handles both downsampling and upsampling. The resampled
9162         dimension must be a datetime-like coordinate. If any intervals
9163         contain no values from the original object, they will be given
9164         the value ``NaN``.
9165 
9166         Parameters
9167         ----------
9168         indexer : Mapping of Hashable to str, optional
9169             Mapping from the dimension name to resample frequency [1]_. The
9170             dimension must be datetime-like.
9171         skipna : bool, optional
9172             Whether to skip missing values when aggregating in downsampling.
9173         closed : {"left", "right"}, optional
9174             Side of each interval to treat as closed.
9175         label : {"left", "right"}, optional
9176             Side of each interval to use for labeling.
9177         base : int, optional
9178             For frequencies that evenly subdivide 1 day, the "origin" of the
9179             aggregated intervals. For example, for "24H" frequency, base could
9180             range from 0 through 23.
9181         origin : {'epoch', 'start', 'start_day', 'end', 'end_day'}, pd.Timestamp, datetime.datetime, np.datetime64, or cftime.datetime, default 'start_day'
9182             The datetime on which to adjust the grouping. The timezone of origin
9183             must match the timezone of the index.
9184 
9185             If a datetime is not used, these values are also supported:
9186             - 'epoch': `origin` is 1970-01-01
9187             - 'start': `origin` is the first value of the timeseries
9188             - 'start_day': `origin` is the first day at midnight of the timeseries
9189             - 'end': `origin` is the last value of the timeseries
9190             - 'end_day': `origin` is the ceiling midnight of the last day
9191         offset : pd.Timedelta, datetime.timedelta, or str, default is None
9192             An offset timedelta added to the origin.
9193         loffset : timedelta or str, optional
9194             Offset used to adjust the resampled time labels. Some pandas date
9195             offset strings are supported.
9196         restore_coord_dims : bool, optional
9197             If True, also restore the dimension order of multi-dimensional
9198             coordinates.
9199         **indexer_kwargs : str
9200             The keyword arguments form of ``indexer``.
9201             One of indexer or indexer_kwargs must be provided.
9202 
9203         Returns
9204         -------
9205         resampled : core.resample.DataArrayResample
9206             This object resampled.
9207 
9208         See Also
9209         --------
9210         DataArray.resample
9211         pandas.Series.resample
9212         pandas.DataFrame.resample
9213 
9214         References
9215         ----------
9216         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
9217         """
9218         from xarray.core.resample import DatasetResample
9219 
9220         return self._resample(
9221             resample_cls=DatasetResample,
9222             indexer=indexer,
9223             skipna=skipna,
9224             closed=closed,
9225             label=label,
9226             base=base,
9227             offset=offset,
9228             origin=origin,
9229             keep_attrs=keep_attrs,
9230             loffset=loffset,
9231             restore_coord_dims=restore_coord_dims,
9232             **indexer_kwargs,
9233         )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>stack</method>
<code>
2683     def stack(
2684         self: T_DataArray,
2685         dimensions: Mapping[Any, Sequence[Hashable]] | None = None,
2686         create_index: bool | None = True,
2687         index_cls: type[Index] = PandasMultiIndex,
2688         **dimensions_kwargs: Sequence[Hashable],
2689     ) -> T_DataArray:
2690         """
2691         Stack any number of existing dimensions into a single new dimension.
2692 
2693         New dimensions will be added at the end, and the corresponding
2694         coordinate variables will be combined into a MultiIndex.
2695 
2696         Parameters
2697         ----------
2698         dimensions : mapping of Hashable to sequence of Hashable
2699             Mapping of the form `new_name=(dim1, dim2, ...)`.
2700             Names of new dimensions, and the existing dimensions that they
2701             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
2702             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
2703             all dimensions.
2704         create_index : bool or None, default: True
2705             If True, create a multi-index for each of the stacked dimensions.
2706             If False, don't create any index.
2707             If None, create a multi-index only if exactly one single (1-d) coordinate
2708             index is found for every dimension to stack.
2709         index_cls: class, optional
2710             Can be used to pass a custom multi-index type. Must be an Xarray index that
2711             implements `.stack()`. By default, a pandas multi-index wrapper is used.
2712         **dimensions_kwargs
2713             The keyword arguments form of ``dimensions``.
2714             One of dimensions or dimensions_kwargs must be provided.
2715 
2716         Returns
2717         -------
2718         stacked : DataArray
2719             DataArray with stacked data.
2720 
2721         Examples
2722         --------
2723         >>> arr = xr.DataArray(
2724         ...     np.arange(6).reshape(2, 3),
2725         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2726         ... )
2727         >>> arr
2728         <xarray.DataArray (x: 2, y: 3)>
2729         array([[0, 1, 2],
2730                [3, 4, 5]])
2731         Coordinates:
2732           * x        (x) <U1 'a' 'b'
2733           * y        (y) int64 0 1 2
2734         >>> stacked = arr.stack(z=("x", "y"))
2735         >>> stacked.indexes["z"]
2736         MultiIndex([('a', 0),
2737                     ('a', 1),
2738                     ('a', 2),
2739                     ('b', 0),
2740                     ('b', 1),
2741                     ('b', 2)],
2742                    name='z')
2743 
2744         See Also
2745         --------
2746         DataArray.unstack
2747         """
2748         ds = self._to_temp_dataset().stack(
2749             dimensions,
2750             create_index=create_index,
2751             index_cls=index_cls,
2752             **dimensions_kwargs,
2753         )
2754         return self._from_temp_dataset(ds)

</code><intended_behavior>Similarly to `Dataset`, the `stack` method here should make sure that the data type (`dtype`) for any coordinates involved in a stack operation is maintained as it was before the operation. If necessary, additional logic should be included to explicitly force the stacked object to match the original dtype if the operation inherently changes it, potentially using a mechanism to specify or cast dtypes during the stacking process.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
235 class DataArray(
236     AbstractArray,
237     DataWithCoords,
238     DataArrayArithmetic,
239     DataArrayAggregations,
240 ):
241     """N-dimensional array with labeled coordinates and dimensions.
242 
243     DataArray provides a wrapper around numpy ndarrays that uses
244     labeled dimensions and coordinates to support metadata aware
245     operations. The API is similar to that for the pandas Series or
246     DataFrame, but DataArray objects can have any number of dimensions,
247     and their contents have fixed data types.
248 
249     Additional features over raw numpy arrays:
250 
251     - Apply operations over dimensions by name: ``x.sum('time')``.
252     - Select or assign values by integer location (like numpy):
253       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or
254       ``x.sel(time='2014-01-01')``.
255     - Mathematical operations (e.g., ``x - y``) vectorize across
256       multiple dimensions (known in numpy as "broadcasting") based on
257       dimension names, regardless of their original order.
258     - Keep track of arbitrary metadata in the form of a Python
259       dictionary: ``x.attrs``
260     - Convert to a pandas Series: ``x.to_series()``.
261 
262     Getting items from or doing mathematical operations with a
263     DataArray always returns another DataArray.
264 
265     Parameters
266     ----------
267     data : array_like
268         Values for this array. Must be an ``numpy.ndarray``, ndarray
269         like, or castable to an ``ndarray``. If a self-described xarray
270         or pandas object, attempts are made to use this array's
271         metadata to fill in other unspecified arguments. A view of the
272         array's data is used instead of a copy if possible.
273     coords : sequence or dict of array_like, optional
274         Coordinates (tick labels) to use for indexing along each
275         dimension. The following notations are accepted:
276 
277         - mapping {dimension name: array-like}
278         - sequence of tuples that are valid arguments for
279           ``xarray.Variable()``
280           - (dims, data)
281           - (dims, data, attrs)
282           - (dims, data, attrs, encoding)
283 
284         Additionally, it is possible to define a coord whose name
285         does not match the dimension name, or a coord based on multiple
286         dimensions, with one of the following notations:
287 
288         - mapping {coord name: DataArray}
289         - mapping {coord name: Variable}
290         - mapping {coord name: (dimension name, array-like)}
291         - mapping {coord name: (tuple of dimension names, array-like)}
292 
293     dims : Hashable or sequence of Hashable, optional
294         Name(s) of the data dimension(s). Must be either a Hashable
295         (only for 1D data) or a sequence of Hashables with length equal
296         to the number of dimensions. If this argument is omitted,
297         dimension names are taken from ``coords`` (if possible) and
298         otherwise default to ``['dim_0', ... 'dim_n']``.
299     name : str or None, optional
300         Name of this array.
301     attrs : dict_like or None, optional
302         Attributes to assign to the new instance. By default, an empty
303         attribute dictionary is initialized.
304 
305     Examples
306     --------
307     Create data:
308 
309     >>> np.random.seed(0)
310     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
311     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
312     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
313     >>> time = pd.date_range("2014-09-06", periods=3)
314     >>> reference_time = pd.Timestamp("2014-09-05")
315 
316     Initialize a dataarray with multiple dimensions:
317 
318     >>> da = xr.DataArray(
319     ...     data=temperature,
320     ...     dims=["x", "y", "time"],
321     ...     coords=dict(
322     ...         lon=(["x", "y"], lon),
323     ...         lat=(["x", "y"], lat),
324     ...         time=time,
325     ...         reference_time=reference_time,
326     ...     ),
327     ...     attrs=dict(
328     ...         description="Ambient temperature.",
329     ...         units="degC",
330     ...     ),
331     ... )
332     >>> da
333     <xarray.DataArray (x: 2, y: 2, time: 3)>
334     array([[[29.11241877, 18.20125767, 22.82990387],
335             [32.92714559, 29.94046392,  7.18177696]],
336     <BLANKLINE>
337            [[22.60070734, 13.78914233, 14.17424919],
338             [18.28478802, 16.15234857, 26.63418806]]])
339     Coordinates:
340         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
341         lat             (x, y) float64 42.25 42.21 42.63 42.59
342       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
343         reference_time  datetime64[ns] 2014-09-05
344     Dimensions without coordinates: x, y
345     Attributes:
346         description:  Ambient temperature.
347         units:        degC
348 
349     Find out where the coldest temperature was:
350 
351     >>> da.isel(da.argmin(...))
352     <xarray.DataArray ()>
353     array(7.18177696)
354     Coordinates:
355         lon             float64 -99.32
356         lat             float64 42.21
357         time            datetime64[ns] 2014-09-08
358         reference_time  datetime64[ns] 2014-09-05
359     Attributes:
360         description:  Ambient temperature.
361         units:        degC
362     """
363 
364     _cache: dict[str, Any]
365     _coords: dict[Any, Variable]
366     _close: Callable[[], None] | None
367     _indexes: dict[Hashable, Index]
368     _name: Hashable | None
369     _variable: Variable
370 
371     __slots__ = (
372         "_cache",
373         "_coords",
374         "_close",
375         "_indexes",
376         "_name",
377         "_variable",
378         "__weakref__",
379     )
380 
381     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor["DataArray"])
382 
383     def __init__(
384         self,
385         data: Any = dtypes.NA,
386         coords: Sequence[Sequence[Any] | pd.Index | DataArray]
387         | Mapping[Any, Any]
388         | None = None,
389         dims: Hashable | Sequence[Hashable] | None = None,
390         name: Hashable | None = None,
391         attrs: Mapping | None = None,
392         # internal parameters
393         indexes: dict[Hashable, Index] | None = None,
394         fastpath: bool = False,
395     ) -> None:
396         if fastpath:
397             variable = data
398             assert dims is None
399             assert attrs is None
400             assert indexes is not None
401         else:
402             # TODO: (benbovy - explicit indexes) remove
403             # once it becomes part of the public interface
404             if indexes is not None:
405                 raise ValueError("Providing explicit indexes is not supported yet")
406 
407             # try to fill in arguments from data if they weren't supplied
408             if coords is None:
409 
410                 if isinstance(data, DataArray):
411                     coords = data.coords
412                 elif isinstance(data, pd.Series):
413                     coords = [data.index]
414                 elif isinstance(data, pd.DataFrame):
415                     coords = [data.index, data.columns]
416                 elif isinstance(data, (pd.Index, IndexVariable)):
417                     coords = [data]
418 
419             if dims is None:
420                 dims = getattr(data, "dims", getattr(coords, "dims", None))
421             if name is None:
422                 name = getattr(data, "name", None)
423             if attrs is None and not isinstance(data, PANDAS_TYPES):
424                 attrs = getattr(data, "attrs", None)
425 
426             data = _check_data_shape(data, coords, dims)
427             data = as_compatible_data(data)
428             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
429             variable = Variable(dims, data, attrs, fastpath=True)
430             indexes, coords = _create_indexes_from_coords(coords)
431 
432         # These fully describe a DataArray
433         self._variable = variable
434         assert isinstance(coords, dict)
435         self._coords = coords
436         self._name = name
437 
438         # TODO(shoyer): document this argument, once it becomes part of the
439         # public interface.
440         self._indexes = indexes
441 
442         self._close = None
443 
444     @classmethod
445     def _construct_direct(
446         cls: type[T_DataArray],
447         variable: Variable,
448         coords: dict[Any, Variable],
449         name: Hashable,
450         indexes: dict[Hashable, Index],
451     ) -> T_DataArray:
452         """Shortcut around __init__ for internal use when we want to skip
453         costly validation
454         """
455         obj = object.__new__(cls)
456         obj._variable = variable
457         obj._coords = coords
458         obj._name = name
459         obj._indexes = indexes
460         obj._close = None
461         return obj
462 
463     def _replace(
464         self: T_DataArray,
465         variable: Variable | None = None,
466         coords=None,
467         name: Hashable | None | Default = _default,
468         indexes=None,
469     ) -> T_DataArray:
470         if variable is None:
471             variable = self.variable
472         if coords is None:
473             coords = self._coords
474         if indexes is None:
475             indexes = self._indexes
476         if name is _default:
477             name = self.name
478         return type(self)(variable, coords, name=name, indexes=indexes, fastpath=True)
479 
480     def _replace_maybe_drop_dims(
481         self: T_DataArray,
482         variable: Variable,
483         name: Hashable | None | Default = _default,
484     ) -> T_DataArray:
485         if variable.dims == self.dims and variable.shape == self.shape:
486             coords = self._coords.copy()
487             indexes = self._indexes
488         elif variable.dims == self.dims:
489             # Shape has changed (e.g. from reduce(..., keepdims=True)
490             new_sizes = dict(zip(self.dims, variable.shape))
491             coords = {
492                 k: v
493                 for k, v in self._coords.items()
494                 if v.shape == tuple(new_sizes[d] for d in v.dims)
495             }
496             indexes = filter_indexes_from_coords(self._indexes, set(coords))
497         else:
498             allowed_dims = set(variable.dims)
499             coords = {
500                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
501             }
502             indexes = filter_indexes_from_coords(self._indexes, set(coords))
503         return self._replace(variable, coords, name, indexes=indexes)
504 
505     def _overwrite_indexes(
506         self: T_DataArray,
507         indexes: Mapping[Any, Index],
508         coords: Mapping[Any, Variable] | None = None,
509         drop_coords: list[Hashable] | None = None,
510         rename_dims: Mapping[Any, Any] | None = None,
511     ) -> T_DataArray:
512         """Maybe replace indexes and their corresponding coordinates."""
513         if not indexes:
514             return self
515 
516         if coords is None:
517             coords = {}
518         if drop_coords is None:
519             drop_coords = []
520 
521         new_variable = self.variable.copy()
522         new_coords = self._coords.copy()
523         new_indexes = dict(self._indexes)
524 
525         for name in indexes:
526             new_coords[name] = coords[name]
527             new_indexes[name] = indexes[name]
528 
529         for name in drop_coords:
530             new_coords.pop(name)
531             new_indexes.pop(name)
532 
533         if rename_dims:
534             new_variable.dims = tuple(rename_dims.get(d, d) for d in new_variable.dims)
535 
536         return self._replace(
537             variable=new_variable, coords=new_coords, indexes=new_indexes
538         )
539 
540     def _to_temp_dataset(self) -> Dataset:
541         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
542 
543     def _from_temp_dataset(
544         self: T_DataArray, dataset: Dataset, name: Hashable | None | Default = _default
545     ) -> T_DataArray:
546         variable = dataset._variables.pop(_THIS_ARRAY)
547         coords = dataset._variables
548         indexes = dataset._indexes
549         return self._replace(variable, coords, name, indexes=indexes)
550 
551     def _to_dataset_split(self, dim: Hashable) -> Dataset:
552         """splits dataarray along dimension 'dim'"""
553 
554         def subset(dim, label):
555             array = self.loc[{dim: label}]
556             array.attrs = {}
557             return as_variable(array)
558 
559         variables = {label: subset(dim, label) for label in self.get_index(dim)}
560         variables.update({k: v for k, v in self._coords.items() if k != dim})
561         coord_names = set(self._coords) - {dim}
562         indexes = filter_indexes_from_coords(self._indexes, coord_names)
563         dataset = Dataset._construct_direct(
564             variables, coord_names, indexes=indexes, attrs=self.attrs
565         )
566         return dataset
567 
568     def _to_dataset_whole(
569         self, name: Hashable = None, shallow_copy: bool = True
570     ) -> Dataset:
571         if name is None:
572             name = self.name
573         if name is None:
574             raise ValueError(
575                 "unable to convert unnamed DataArray to a "
576                 "Dataset without providing an explicit name"
577             )
578         if name in self.coords:
579             raise ValueError(
580                 "cannot create a Dataset from a DataArray with "
581                 "the same name as one of its coordinates"
582             )
583         # use private APIs for speed: this is called by _to_temp_dataset(),
584         # which is used in the guts of a lot of operations (e.g., reindex)
585         variables = self._coords.copy()
586         variables[name] = self.variable
587         if shallow_copy:
588             for k in variables:
589                 variables[k] = variables[k].copy(deep=False)
590         indexes = self._indexes
591 
592         coord_names = set(self._coords)
593         return Dataset._construct_direct(variables, coord_names, indexes=indexes)
594 
595     def to_dataset(
596         self,
597         dim: Hashable = None,
598         *,
599         name: Hashable = None,
600         promote_attrs: bool = False,
601     ) -> Dataset:
602         """Convert a DataArray to a Dataset.
603 
604         Parameters
605         ----------
606         dim : Hashable, optional
607             Name of the dimension on this array along which to split this array
608             into separate variables. If not provided, this array is converted
609             into a Dataset of one variable.
610         name : Hashable, optional
611             Name to substitute for this array's name. Only valid if ``dim`` is
612             not provided.
613         promote_attrs : bool, default: False
614             Set to True to shallow copy attrs of DataArray to returned Dataset.
615 
616         Returns
617         -------
618         dataset : Dataset
619         """
620         if dim is not None and dim not in self.dims:
621             raise TypeError(
622                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
623             )
624 
625         if dim is not None:
626             if name is not None:
627                 raise TypeError("cannot supply both dim and name arguments")
628             result = self._to_dataset_split(dim)
629         else:
630             result = self._to_dataset_whole(name)
631 
632         if promote_attrs:
633             result.attrs = dict(self.attrs)
634 
635         return result
636 
637     @property
638     def name(self) -> Hashable | None:
639         """The name of this array."""
640         return self._name
641 
642     @name.setter
643     def name(self, value: Hashable | None) -> None:
644         self._name = value
645 
646     @property
647     def variable(self) -> Variable:
648         """Low level interface to the Variable object for this DataArray."""
649         return self._variable
650 
651     @property
652     def dtype(self) -> np.dtype:
653         """
654         Data-type of the array’s elements.
655 
656         See Also
657         --------
658         ndarray.dtype
659         numpy.dtype
660         """
661         return self.variable.dtype
662 
663     @property
664     def shape(self) -> tuple[int, ...]:
665         """
666         Tuple of array dimensions.
667 
668         See Also
669         --------
670         numpy.ndarray.shape
671         """
672         return self.variable.shape
673 
674     @property
675     def size(self) -> int:
676         """
677         Number of elements in the array.
678 
679         Equal to ``np.prod(a.shape)``, i.e., the product of the array’s dimensions.
680 
681         See Also
682         --------
683         numpy.ndarray.size
684         """
685         return self.variable.size
686 
687     @property
688     def nbytes(self) -> int:
689         """
690         Total bytes consumed by the elements of this DataArray's data.
691 
692         If the underlying data array does not include ``nbytes``, estimates
693         the bytes consumed based on the ``size`` and ``dtype``.
694         """
695         return self.variable.nbytes
696 
697     @property
698     def ndim(self) -> int:
699         """
700         Number of array dimensions.
701 
702         See Also
703         --------
704         numpy.ndarray.ndim
705         """
706         return self.variable.ndim
707 
708     def __len__(self) -> int:
709         return len(self.variable)
710 
711     @property
712     def data(self) -> Any:
713         """
714         The DataArray's data as an array. The underlying array type
715         (e.g. dask, sparse, pint) is preserved.
716 
717         See Also
718         --------
719         DataArray.to_numpy
720         DataArray.as_numpy
721         DataArray.values
722         """
723         return self.variable.data
724 
725     @data.setter
726     def data(self, value: Any) -> None:
727         self.variable.data = value
728 
729     @property
730     def values(self) -> np.ndarray:
731         """
732         The array's data as a numpy.ndarray.
733 
734         If the array's data is not a numpy.ndarray this will attempt to convert
735         it naively using np.array(), which will raise an error if the array
736         type does not support coercion like this (e.g. cupy).
737         """
738         return self.variable.values
739 
740     @values.setter
741     def values(self, value: Any) -> None:
742         self.variable.values = value
743 
744     def to_numpy(self) -> np.ndarray:
745         """
746         Coerces wrapped data to numpy and returns a numpy.ndarray.
747 
748         See Also
749         --------
750         DataArray.as_numpy : Same but returns the surrounding DataArray instead.
751         Dataset.as_numpy
752         DataArray.values
753         DataArray.data
754         """
755         return self.variable.to_numpy()
756 
757     def as_numpy(self: T_DataArray) -> T_DataArray:
758         """
759         Coerces wrapped data and coordinates into numpy arrays, returning a DataArray.
760 
761         See Also
762         --------
763         DataArray.to_numpy : Same but returns only the data as a numpy.ndarray object.
764         Dataset.as_numpy : Converts all variables in a Dataset.
765         DataArray.values
766         DataArray.data
767         """
768         coords = {k: v.as_numpy() for k, v in self._coords.items()}
769         return self._replace(self.variable.as_numpy(), coords, indexes=self._indexes)
770 
771     @property
772     def _in_memory(self) -> bool:
773         return self.variable._in_memory
774 
775     def _to_index(self) -> pd.Index:
776         return self.variable._to_index()
777 
778     def to_index(self) -> pd.Index:
779         """Convert this variable to a pandas.Index. Only possible for 1D
780         arrays.
781         """
782         return self.variable.to_index()
783 
784     @property
785     def dims(self) -> tuple[Hashable, ...]:
786         """Tuple of dimension names associated with this array.
787 
788         Note that the type of this property is inconsistent with
789         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
790         consistently named properties.
791 
792         See Also
793         --------
794         DataArray.sizes
795         Dataset.dims
796         """
797         return self.variable.dims
798 
799     @dims.setter
800     def dims(self, value: Any) -> NoReturn:
801         raise AttributeError(
802             "you cannot assign dims on a DataArray. Use "
803             ".rename() or .swap_dims() instead."
804         )
805 
806     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
807         if utils.is_dict_like(key):
808             return key
809         key = indexing.expanded_indexer(key, self.ndim)
810         return dict(zip(self.dims, key))
811 
812     def _getitem_coord(self: T_DataArray, key: Any) -> T_DataArray:
813         from xarray.core.dataset import _get_virtual_variable
814 
815         try:
816             var = self._coords[key]
817         except KeyError:
818             dim_sizes = dict(zip(self.dims, self.shape))
819             _, key, var = _get_virtual_variable(self._coords, key, dim_sizes)
820 
821         return self._replace_maybe_drop_dims(var, name=key)
822 
823     def __getitem__(self: T_DataArray, key: Any) -> T_DataArray:
824         if isinstance(key, str):
825             return self._getitem_coord(key)
826         else:
827             # xarray-style array indexing
828             return self.isel(indexers=self._item_key_to_dict(key))
829 
830     def __setitem__(self, key: Any, value: Any) -> None:
831         if isinstance(key, str):
832             self.coords[key] = value
833         else:
834             # Coordinates in key, value and self[key] should be consistent.
835             # TODO Coordinate consistency in key is checked here, but it
836             # causes unnecessary indexing. It should be optimized.
837             obj = self[key]
838             if isinstance(value, DataArray):
839                 assert_coordinate_consistent(value, obj.coords.variables)
840             # DataArray key -> Variable key
841             key = {
842                 k: v.variable if isinstance(v, DataArray) else v
843                 for k, v in self._item_key_to_dict(key).items()
844             }
845             self.variable[key] = value
846 
847     def __delitem__(self, key: Any) -> None:
848         del self.coords[key]
849 
850     @property
851     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
852         """Places to look-up items for attribute-style access"""
853         yield from self._item_sources
854         yield self.attrs
855 
856     @property
857     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
858         """Places to look-up items for key-completion"""
859         yield HybridMappingProxy(keys=self._coords, mapping=self.coords)
860 
861         # virtual coordinates
862         # uses empty dict -- everything here can already be found in self.coords.
863         yield HybridMappingProxy(keys=self.dims, mapping={})
864 
865     def __contains__(self, key: Any) -> bool:
866         return key in self.data
867 
868     @property
869     def loc(self) -> _LocIndexer:
870         """Attribute for location based indexing like pandas."""
871         return _LocIndexer(self)
872 
873     @property
874     def attrs(self) -> dict[Any, Any]:
875         """Dictionary storing arbitrary metadata with this array."""
876         return self.variable.attrs
877 
878     @attrs.setter
879     def attrs(self, value: Mapping[Any, Any]) -> None:
880         self.variable.attrs = dict(value)
881 
882     @property
883     def encoding(self) -> dict[Any, Any]:
884         """Dictionary of format-specific settings for how this array should be
885         serialized."""
886         return self.variable.encoding
887 
888     @encoding.setter
889     def encoding(self, value: Mapping[Any, Any]) -> None:
890         self.variable.encoding = dict(value)
891 
892     @property
893     def indexes(self) -> Indexes:
894         """Mapping of pandas.Index objects used for label based indexing.
895 
896         Raises an error if this Dataset has indexes that cannot be coerced
897         to pandas.Index objects.
898 
899         See Also
900         --------
901         DataArray.xindexes
902 
903         """
904         return self.xindexes.to_pandas_indexes()
905 
906     @property
907     def xindexes(self) -> Indexes:
908         """Mapping of xarray Index objects used for label based indexing."""
909         return Indexes(self._indexes, {k: self._coords[k] for k in self._indexes})
910 
911     @property
912     def coords(self) -> DataArrayCoordinates:
913         """Dictionary-like container of coordinate arrays."""
914         return DataArrayCoordinates(self)
915 
916     @overload
917     def reset_coords(
918         self: T_DataArray,
919         names: Dims = None,
920         drop: Literal[False] = False,
921     ) -> Dataset:
922         ...
923 
924     @overload
925     def reset_coords(
926         self: T_DataArray,
927         names: Dims = None,
928         *,
929         drop: Literal[True],
930     ) -> T_DataArray:
931         ...
932 
933     def reset_coords(
934         self: T_DataArray,
935         names: Dims = None,
936         drop: bool = False,
937     ) -> T_DataArray | Dataset:
938         """Given names of coordinates, reset them to become variables.
939 
940         Parameters
941         ----------
942         names : str, Iterable of Hashable or None, optional
943             Name(s) of non-index coordinates in this dataset to reset into
944             variables. By default, all non-index coordinates are reset.
945         drop : bool, default: False
946             If True, remove coordinates instead of converting them into
947             variables.
948 
949         Returns
950         -------
951         Dataset, or DataArray if ``drop == True``
952 
953         Examples
954         --------
955         >>> temperature = np.arange(25).reshape(5, 5)
956         >>> pressure = np.arange(50, 75).reshape(5, 5)
957         >>> da = xr.DataArray(
958         ...     data=temperature,
959         ...     dims=["x", "y"],
960         ...     coords=dict(
961         ...         lon=("x", np.arange(10, 15)),
962         ...         lat=("y", np.arange(20, 25)),
963         ...         Pressure=(["x", "y"], pressure),
964         ...     ),
965         ...     name="Temperature",
966         ... )
967         >>> da
968         <xarray.DataArray 'Temperature' (x: 5, y: 5)>
969         array([[ 0,  1,  2,  3,  4],
970                [ 5,  6,  7,  8,  9],
971                [10, 11, 12, 13, 14],
972                [15, 16, 17, 18, 19],
973                [20, 21, 22, 23, 24]])
974         Coordinates:
975             lon       (x) int64 10 11 12 13 14
976             lat       (y) int64 20 21 22 23 24
977             Pressure  (x, y) int64 50 51 52 53 54 55 56 57 ... 67 68 69 70 71 72 73 74
978         Dimensions without coordinates: x, y
979 
980         Return Dataset with target coordinate as a data variable rather than a coordinate variable:
981 
982         >>> da.reset_coords(names="Pressure")
983         <xarray.Dataset>
984         Dimensions:      (x: 5, y: 5)
985         Coordinates:
986             lon          (x) int64 10 11 12 13 14
987             lat          (y) int64 20 21 22 23 24
988         Dimensions without coordinates: x, y
989         Data variables:
990             Pressure     (x, y) int64 50 51 52 53 54 55 56 57 ... 68 69 70 71 72 73 74
991             Temperature  (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24
992 
993         Return DataArray without targeted coordinate:
994 
995         >>> da.reset_coords(names="Pressure", drop=True)
996         <xarray.DataArray 'Temperature' (x: 5, y: 5)>
997         array([[ 0,  1,  2,  3,  4],
998                [ 5,  6,  7,  8,  9],
999                [10, 11, 12, 13, 14],
1000                [15, 16, 17, 18, 19],
1001                [20, 21, 22, 23, 24]])
1002         Coordinates:
1003             lon      (x) int64 10 11 12 13 14
1004             lat      (y) int64 20 21 22 23 24
1005         Dimensions without coordinates: x, y
1006         """
1007         if names is None:
1008             names = set(self.coords) - set(self._indexes)
1009         dataset = self.coords.to_dataset().reset_coords(names, drop)
1010         if drop:
1011             return self._replace(coords=dataset._variables)
1012         if self.name is None:
1013             raise ValueError(
1014                 "cannot reset_coords with drop=False on an unnamed DataArrray"
1015             )
1016         dataset[self.name] = self.variable
1017         return dataset
1018 
1019     def __dask_tokenize__(self):
1020         from dask.base import normalize_token
1021 
1022         return normalize_token((type(self), self._variable, self._coords, self._name))
1023 
1024     def __dask_graph__(self):
1025         return self._to_temp_dataset().__dask_graph__()
1026 
1027     def __dask_keys__(self):
1028         return self._to_temp_dataset().__dask_keys__()
1029 
1030     def __dask_layers__(self):
1031         return self._to_temp_dataset().__dask_layers__()
1032 
1033     @property
1034     def __dask_optimize__(self):
1035         return self._to_temp_dataset().__dask_optimize__
1036 
1037     @property
1038     def __dask_scheduler__(self):
1039         return self._to_temp_dataset().__dask_scheduler__
1040 
1041     def __dask_postcompute__(self):
1042         func, args = self._to_temp_dataset().__dask_postcompute__()
1043         return self._dask_finalize, (self.name, func) + args
1044 
1045     def __dask_postpersist__(self):
1046         func, args = self._to_temp_dataset().__dask_postpersist__()
1047         return self._dask_finalize, (self.name, func) + args
1048 
1049     @staticmethod
1050     def _dask_finalize(results, name, func, *args, **kwargs) -> DataArray:
1051         ds = func(results, *args, **kwargs)
1052         variable = ds._variables.pop(_THIS_ARRAY)
1053         coords = ds._variables
1054         indexes = ds._indexes
1055         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1056 
1057     def load(self: T_DataArray, **kwargs) -> T_DataArray:
1058         """Manually trigger loading of this array's data from disk or a
1059         remote source into memory and return this array.
1060 
1061         Normally, it should not be necessary to call this method in user code,
1062         because all xarray functions should either work on deferred data or
1063         load data automatically. However, this method can be necessary when
1064         working with many file objects on disk.
1065 
1066         Parameters
1067         ----------
1068         **kwargs : dict
1069             Additional keyword arguments passed on to ``dask.compute``.
1070 
1071         See Also
1072         --------
1073         dask.compute
1074         """
1075         ds = self._to_temp_dataset().load(**kwargs)
1076         new = self._from_temp_dataset(ds)
1077         self._variable = new._variable
1078         self._coords = new._coords
1079         return self
1080 
1081     def compute(self: T_DataArray, **kwargs) -> T_DataArray:
1082         """Manually trigger loading of this array's data from disk or a
1083         remote source into memory and return a new array. The original is
1084         left unaltered.
1085 
1086         Normally, it should not be necessary to call this method in user code,
1087         because all xarray functions should either work on deferred data or
1088         load data automatically. However, this method can be necessary when
1089         working with many file objects on disk.
1090 
1091         Parameters
1092         ----------
1093         **kwargs : dict
1094             Additional keyword arguments passed on to ``dask.compute``.
1095 
1096         See Also
1097         --------
1098         dask.compute
1099         """
1100         new = self.copy(deep=False)
1101         return new.load(**kwargs)
1102 
1103     def persist(self: T_DataArray, **kwargs) -> T_DataArray:
1104         """Trigger computation in constituent dask arrays
1105 
1106         This keeps them as dask arrays but encourages them to keep data in
1107         memory.  This is particularly useful when on a distributed machine.
1108         When on a single machine consider using ``.compute()`` instead.
1109 
1110         Parameters
1111         ----------
1112         **kwargs : dict
1113             Additional keyword arguments passed on to ``dask.persist``.
1114 
1115         See Also
1116         --------
1117         dask.persist
1118         """
1119         ds = self._to_temp_dataset().persist(**kwargs)
1120         return self._from_temp_dataset(ds)
1121 
1122     def copy(self: T_DataArray, deep: bool = True, data: Any = None) -> T_DataArray:
1123         """Returns a copy of this array.
1124 
1125         If `deep=True`, a deep copy is made of the data array.
1126         Otherwise, a shallow copy is made, and the returned data array's
1127         values are a new view of this data array's values.
1128 
1129         Use `data` to create a new object with the same structure as
1130         original but entirely new data.
1131 
1132         Parameters
1133         ----------
1134         deep : bool, optional
1135             Whether the data array and its coordinates are loaded into memory
1136             and copied onto the new object. Default is True.
1137         data : array_like, optional
1138             Data to use in the new object. Must have same shape as original.
1139             When `data` is used, `deep` is ignored for all data variables,
1140             and only used for coords.
1141 
1142         Returns
1143         -------
1144         copy : DataArray
1145             New object with dimensions, attributes, coordinates, name,
1146             encoding, and optionally data copied from original.
1147 
1148         Examples
1149         --------
1150         Shallow versus deep copy
1151 
1152         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
1153         >>> array.copy()
1154         <xarray.DataArray (x: 3)>
1155         array([1, 2, 3])
1156         Coordinates:
1157           * x        (x) <U1 'a' 'b' 'c'
1158         >>> array_0 = array.copy(deep=False)
1159         >>> array_0[0] = 7
1160         >>> array_0
1161         <xarray.DataArray (x: 3)>
1162         array([7, 2, 3])
1163         Coordinates:
1164           * x        (x) <U1 'a' 'b' 'c'
1165         >>> array
1166         <xarray.DataArray (x: 3)>
1167         array([7, 2, 3])
1168         Coordinates:
1169           * x        (x) <U1 'a' 'b' 'c'
1170 
1171         Changing the data using the ``data`` argument maintains the
1172         structure of the original object, but with the new data. Original
1173         object is unaffected.
1174 
1175         >>> array.copy(data=[0.1, 0.2, 0.3])
1176         <xarray.DataArray (x: 3)>
1177         array([0.1, 0.2, 0.3])
1178         Coordinates:
1179           * x        (x) <U1 'a' 'b' 'c'
1180         >>> array
1181         <xarray.DataArray (x: 3)>
1182         array([7, 2, 3])
1183         Coordinates:
1184           * x        (x) <U1 'a' 'b' 'c'
1185 
1186         See Also
1187         --------
1188         pandas.DataFrame.copy
1189         """
1190         return self._copy(deep=deep, data=data)
1191 
1192     def _copy(
1193         self: T_DataArray,
1194         deep: bool = True,
1195         data: Any = None,
1196         memo: dict[int, Any] | None = None,
1197     ) -> T_DataArray:
1198         variable = self.variable._copy(deep=deep, data=data, memo=memo)
1199         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1200 
1201         coords = {}
1202         for k, v in self._coords.items():
1203             if k in index_vars:
1204                 coords[k] = index_vars[k]
1205             else:
1206                 coords[k] = v._copy(deep=deep, memo=memo)
1207 
1208         return self._replace(variable, coords, indexes=indexes)
1209 
1210     def __copy__(self: T_DataArray) -> T_DataArray:
1211         return self._copy(deep=False)
1212 
1213     def __deepcopy__(
1214         self: T_DataArray, memo: dict[int, Any] | None = None
1215     ) -> T_DataArray:
1216         return self._copy(deep=True, memo=memo)
1217 
1218     # mutable objects should not be Hashable
1219     # https://github.com/python/mypy/issues/4266
1220     __hash__ = None  # type: ignore[assignment]
1221 
1222     @property
1223     def chunks(self) -> tuple[tuple[int, ...], ...] | None:
1224         """
1225         Tuple of block lengths for this dataarray's data, in order of dimensions, or None if
1226         the underlying data is not a dask array.
1227 
1228         See Also
1229         --------
1230         DataArray.chunk
1231         DataArray.chunksizes
1232         xarray.unify_chunks
1233         """
1234         return self.variable.chunks
1235 
1236     @property
1237     def chunksizes(self) -> Mapping[Any, tuple[int, ...]]:
1238         """
1239         Mapping from dimension names to block lengths for this dataarray's data, or None if
1240         the underlying data is not a dask array.
1241         Cannot be modified directly, but can be modified by calling .chunk().
1242 
1243         Differs from DataArray.chunks because it returns a mapping of dimensions to chunk shapes
1244         instead of a tuple of chunk shapes.
1245 
1246         See Also
1247         --------
1248         DataArray.chunk
1249         DataArray.chunks
1250         xarray.unify_chunks
1251         """
1252         all_variables = [self.variable] + [c.variable for c in self.coords.values()]
1253         return get_chunksizes(all_variables)
1254 
1255     def chunk(
1256         self: T_DataArray,
1257         chunks: (
1258             int
1259             | Literal["auto"]
1260             | tuple[int, ...]
1261             | tuple[tuple[int, ...], ...]
1262             | Mapping[Any, None | int | tuple[int, ...]]
1263         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1264         name_prefix: str = "xarray-",
1265         token: str | None = None,
1266         lock: bool = False,
1267         inline_array: bool = False,
1268         **chunks_kwargs: Any,
1269     ) -> T_DataArray:
1270         """Coerce this array's data into a dask arrays with the given chunks.
1271 
1272         If this variable is a non-dask array, it will be converted to dask
1273         array. If it's a dask array, it will be rechunked to the given chunk
1274         sizes.
1275 
1276         If neither chunks is not provided for one or more dimensions, chunk
1277         sizes along that dimension will not be updated; non-dask arrays will be
1278         converted into dask arrays with a single block.
1279 
1280         Parameters
1281         ----------
1282         chunks : int, "auto", tuple of int or mapping of Hashable to int, optional
1283             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, ``(5, 5)`` or
1284             ``{"x": 5, "y": 5}``.
1285         name_prefix : str, optional
1286             Prefix for the name of the new dask array.
1287         token : str, optional
1288             Token uniquely identifying this array.
1289         lock : optional
1290             Passed on to :py:func:`dask.array.from_array`, if the array is not
1291             already as dask array.
1292         inline_array: optional
1293             Passed on to :py:func:`dask.array.from_array`, if the array is not
1294             already as dask array.
1295         **chunks_kwargs : {dim: chunks, ...}, optional
1296             The keyword arguments form of ``chunks``.
1297             One of chunks or chunks_kwargs must be provided.
1298 
1299         Returns
1300         -------
1301         chunked : xarray.DataArray
1302 
1303         See Also
1304         --------
1305         DataArray.chunks
1306         DataArray.chunksizes
1307         xarray.unify_chunks
1308         dask.array.from_array
1309         """
1310         if chunks is None:
1311             warnings.warn(
1312                 "None value for 'chunks' is deprecated. "
1313                 "It will raise an error in the future. Use instead '{}'",
1314                 category=FutureWarning,
1315             )
1316             chunks = {}
1317 
1318         if isinstance(chunks, (float, str, int)):
1319             # ignoring type; unclear why it won't accept a Literal into the value.
1320             chunks = dict.fromkeys(self.dims, chunks)  # type: ignore
1321         elif isinstance(chunks, (tuple, list)):
1322             chunks = dict(zip(self.dims, chunks))
1323         else:
1324             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
1325 
1326         ds = self._to_temp_dataset().chunk(
1327             chunks,
1328             name_prefix=name_prefix,
1329             token=token,
1330             lock=lock,
1331             inline_array=inline_array,
1332         )
1333         return self._from_temp_dataset(ds)
1334 
1335     def isel(
1336         self: T_DataArray,
1337         indexers: Mapping[Any, Any] | None = None,
1338         drop: bool = False,
1339         missing_dims: ErrorOptionsWithWarn = "raise",
1340         **indexers_kwargs: Any,
1341     ) -> T_DataArray:
1342         """Return a new DataArray whose data is given by selecting indexes
1343         along the specified dimension(s).
1344 
1345         Parameters
1346         ----------
1347         indexers : dict, optional
1348             A dict with keys matching dimensions and values given
1349             by integers, slice objects or arrays.
1350             indexer can be a integer, slice, array-like or DataArray.
1351             If DataArrays are passed as indexers, xarray-style indexing will be
1352             carried out. See :ref:`indexing` for the details.
1353             One of indexers or indexers_kwargs must be provided.
1354         drop : bool, default: False
1355             If ``drop=True``, drop coordinates variables indexed by integers
1356             instead of making them scalar.
1357         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1358             What to do if dimensions that should be selected from are not present in the
1359             DataArray:
1360             - "raise": raise an exception
1361             - "warn": raise a warning, and ignore the missing dimensions
1362             - "ignore": ignore the missing dimensions
1363         **indexers_kwargs : {dim: indexer, ...}, optional
1364             The keyword arguments form of ``indexers``.
1365 
1366         Returns
1367         -------
1368         indexed : xarray.DataArray
1369 
1370         See Also
1371         --------
1372         Dataset.isel
1373         DataArray.sel
1374 
1375         Examples
1376         --------
1377         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("x", "y"))
1378         >>> da
1379         <xarray.DataArray (x: 5, y: 5)>
1380         array([[ 0,  1,  2,  3,  4],
1381                [ 5,  6,  7,  8,  9],
1382                [10, 11, 12, 13, 14],
1383                [15, 16, 17, 18, 19],
1384                [20, 21, 22, 23, 24]])
1385         Dimensions without coordinates: x, y
1386 
1387         >>> tgt_x = xr.DataArray(np.arange(0, 5), dims="points")
1388         >>> tgt_y = xr.DataArray(np.arange(0, 5), dims="points")
1389         >>> da = da.isel(x=tgt_x, y=tgt_y)
1390         >>> da
1391         <xarray.DataArray (points: 5)>
1392         array([ 0,  6, 12, 18, 24])
1393         Dimensions without coordinates: points
1394         """
1395 
1396         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1397 
1398         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1399             ds = self._to_temp_dataset()._isel_fancy(
1400                 indexers, drop=drop, missing_dims=missing_dims
1401             )
1402             return self._from_temp_dataset(ds)
1403 
1404         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1405         # lists, or zero or one-dimensional np.ndarray's
1406 
1407         variable = self._variable.isel(indexers, missing_dims=missing_dims)
1408         indexes, index_variables = isel_indexes(self.xindexes, indexers)
1409 
1410         coords = {}
1411         for coord_name, coord_value in self._coords.items():
1412             if coord_name in index_variables:
1413                 coord_value = index_variables[coord_name]
1414             else:
1415                 coord_indexers = {
1416                     k: v for k, v in indexers.items() if k in coord_value.dims
1417                 }
1418                 if coord_indexers:
1419                     coord_value = coord_value.isel(coord_indexers)
1420                     if drop and coord_value.ndim == 0:
1421                         continue
1422             coords[coord_name] = coord_value
1423 
1424         return self._replace(variable=variable, coords=coords, indexes=indexes)
1425 
1426     def sel(
1427         self: T_DataArray,
1428         indexers: Mapping[Any, Any] | None = None,
1429         method: str | None = None,
1430         tolerance=None,
1431         drop: bool = False,
1432         **indexers_kwargs: Any,
1433     ) -> T_DataArray:
1434         """Return a new DataArray whose data is given by selecting index
1435         labels along the specified dimension(s).
1436 
1437         In contrast to `DataArray.isel`, indexers for this method should use
1438         labels instead of integers.
1439 
1440         Under the hood, this method is powered by using pandas's powerful Index
1441         objects. This makes label based indexing essentially just as fast as
1442         using integer indexing.
1443 
1444         It also means this method uses pandas's (well documented) logic for
1445         indexing. This means you can use string shortcuts for datetime indexes
1446         (e.g., '2000-01' to select all values in January 2000). It also means
1447         that slices are treated as inclusive of both the start and stop values,
1448         unlike normal Python indexing.
1449 
1450         .. warning::
1451 
1452           Do not try to assign values when using any of the indexing methods
1453           ``isel`` or ``sel``::
1454 
1455             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1456             # DO NOT do this
1457             da.isel(x=[0, 1, 2])[1] = -1
1458 
1459           Assigning values with the chained indexing using ``.sel`` or
1460           ``.isel`` fails silently.
1461 
1462         Parameters
1463         ----------
1464         indexers : dict, optional
1465             A dict with keys matching dimensions and values given
1466             by scalars, slices or arrays of tick labels. For dimensions with
1467             multi-index, the indexer may also be a dict-like object with keys
1468             matching index level names.
1469             If DataArrays are passed as indexers, xarray-style indexing will be
1470             carried out. See :ref:`indexing` for the details.
1471             One of indexers or indexers_kwargs must be provided.
1472         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1473             Method to use for inexact matches:
1474 
1475             - None (default): only exact matches
1476             - pad / ffill: propagate last valid index value forward
1477             - backfill / bfill: propagate next valid index value backward
1478             - nearest: use nearest valid index value
1479 
1480         tolerance : optional
1481             Maximum distance between original and new labels for inexact
1482             matches. The values of the index at the matching locations must
1483             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1484         drop : bool, optional
1485             If ``drop=True``, drop coordinates variables in `indexers` instead
1486             of making them scalar.
1487         **indexers_kwargs : {dim: indexer, ...}, optional
1488             The keyword arguments form of ``indexers``.
1489             One of indexers or indexers_kwargs must be provided.
1490 
1491         Returns
1492         -------
1493         obj : DataArray
1494             A new DataArray with the same contents as this DataArray, except the
1495             data and each dimension is indexed by the appropriate indexers.
1496             If indexer DataArrays have coordinates that do not conflict with
1497             this object, then these coordinates will be attached.
1498             In general, each array's data will be a view of the array's data
1499             in this DataArray, unless vectorized indexing was triggered by using
1500             an array indexer, in which case the data will be a copy.
1501 
1502         See Also
1503         --------
1504         Dataset.sel
1505         DataArray.isel
1506 
1507         Examples
1508         --------
1509         >>> da = xr.DataArray(
1510         ...     np.arange(25).reshape(5, 5),
1511         ...     coords={"x": np.arange(5), "y": np.arange(5)},
1512         ...     dims=("x", "y"),
1513         ... )
1514         >>> da
1515         <xarray.DataArray (x: 5, y: 5)>
1516         array([[ 0,  1,  2,  3,  4],
1517                [ 5,  6,  7,  8,  9],
1518                [10, 11, 12, 13, 14],
1519                [15, 16, 17, 18, 19],
1520                [20, 21, 22, 23, 24]])
1521         Coordinates:
1522           * x        (x) int64 0 1 2 3 4
1523           * y        (y) int64 0 1 2 3 4
1524 
1525         >>> tgt_x = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1526         >>> tgt_y = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1527         >>> da = da.sel(x=tgt_x, y=tgt_y, method="nearest")
1528         >>> da
1529         <xarray.DataArray (points: 5)>
1530         array([ 0,  6, 12, 18, 24])
1531         Coordinates:
1532             x        (points) int64 0 1 2 3 4
1533             y        (points) int64 0 1 2 3 4
1534         Dimensions without coordinates: points
1535         """
1536         ds = self._to_temp_dataset().sel(
1537             indexers=indexers,
1538             drop=drop,
1539             method=method,
1540             tolerance=tolerance,
1541             **indexers_kwargs,
1542         )
1543         return self._from_temp_dataset(ds)
1544 
1545     def head(
1546         self: T_DataArray,
1547         indexers: Mapping[Any, int] | int | None = None,
1548         **indexers_kwargs: Any,
1549     ) -> T_DataArray:
1550         """Return a new DataArray whose data is given by the the first `n`
1551         values along the specified dimension(s). Default `n` = 5
1552 
1553         See Also
1554         --------
1555         Dataset.head
1556         DataArray.tail
1557         DataArray.thin
1558 
1559         Examples
1560         --------
1561         >>> da = xr.DataArray(
1562         ...     np.arange(25).reshape(5, 5),
1563         ...     dims=("x", "y"),
1564         ... )
1565         >>> da
1566         <xarray.DataArray (x: 5, y: 5)>
1567         array([[ 0,  1,  2,  3,  4],
1568                [ 5,  6,  7,  8,  9],
1569                [10, 11, 12, 13, 14],
1570                [15, 16, 17, 18, 19],
1571                [20, 21, 22, 23, 24]])
1572         Dimensions without coordinates: x, y
1573 
1574         >>> da.head(x=1)
1575         <xarray.DataArray (x: 1, y: 5)>
1576         array([[0, 1, 2, 3, 4]])
1577         Dimensions without coordinates: x, y
1578 
1579         >>> da.head({"x": 2, "y": 2})
1580         <xarray.DataArray (x: 2, y: 2)>
1581         array([[0, 1],
1582                [5, 6]])
1583         Dimensions without coordinates: x, y
1584         """
1585         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1586         return self._from_temp_dataset(ds)
1587 
1588     def tail(
1589         self: T_DataArray,
1590         indexers: Mapping[Any, int] | int | None = None,
1591         **indexers_kwargs: Any,
1592     ) -> T_DataArray:
1593         """Return a new DataArray whose data is given by the the last `n`
1594         values along the specified dimension(s). Default `n` = 5
1595 
1596         See Also
1597         --------
1598         Dataset.tail
1599         DataArray.head
1600         DataArray.thin
1601 
1602         Examples
1603         --------
1604         >>> da = xr.DataArray(
1605         ...     np.arange(25).reshape(5, 5),
1606         ...     dims=("x", "y"),
1607         ... )
1608         >>> da
1609         <xarray.DataArray (x: 5, y: 5)>
1610         array([[ 0,  1,  2,  3,  4],
1611                [ 5,  6,  7,  8,  9],
1612                [10, 11, 12, 13, 14],
1613                [15, 16, 17, 18, 19],
1614                [20, 21, 22, 23, 24]])
1615         Dimensions without coordinates: x, y
1616 
1617         >>> da.tail(y=1)
1618         <xarray.DataArray (x: 5, y: 1)>
1619         array([[ 4],
1620                [ 9],
1621                [14],
1622                [19],
1623                [24]])
1624         Dimensions without coordinates: x, y
1625 
1626         >>> da.tail({"x": 2, "y": 2})
1627         <xarray.DataArray (x: 2, y: 2)>
1628         array([[18, 19],
1629                [23, 24]])
1630         Dimensions without coordinates: x, y
1631         """
1632         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1633         return self._from_temp_dataset(ds)
1634 
1635     def thin(
1636         self: T_DataArray,
1637         indexers: Mapping[Any, int] | int | None = None,
1638         **indexers_kwargs: Any,
1639     ) -> T_DataArray:
1640         """Return a new DataArray whose data is given by each `n` value
1641         along the specified dimension(s).
1642 
1643         Examples
1644         --------
1645         >>> x_arr = np.arange(0, 26)
1646         >>> x_arr
1647         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
1648                17, 18, 19, 20, 21, 22, 23, 24, 25])
1649         >>> x = xr.DataArray(
1650         ...     np.reshape(x_arr, (2, 13)),
1651         ...     dims=("x", "y"),
1652         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
1653         ... )
1654         >>> x
1655         <xarray.DataArray (x: 2, y: 13)>
1656         array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],
1657                [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]])
1658         Coordinates:
1659           * x        (x) int64 0 1
1660           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
1661 
1662         >>>
1663         >>> x.thin(3)
1664         <xarray.DataArray (x: 1, y: 5)>
1665         array([[ 0,  3,  6,  9, 12]])
1666         Coordinates:
1667           * x        (x) int64 0
1668           * y        (y) int64 0 3 6 9 12
1669         >>> x.thin({"x": 2, "y": 5})
1670         <xarray.DataArray (x: 1, y: 3)>
1671         array([[ 0,  5, 10]])
1672         Coordinates:
1673           * x        (x) int64 0
1674           * y        (y) int64 0 5 10
1675 
1676         See Also
1677         --------
1678         Dataset.thin
1679         DataArray.head
1680         DataArray.tail
1681         """
1682         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1683         return self._from_temp_dataset(ds)
1684 
1685     def broadcast_like(
1686         self: T_DataArray,
1687         other: DataArray | Dataset,
1688         exclude: Iterable[Hashable] | None = None,
1689     ) -> T_DataArray:
1690         """Broadcast this DataArray against another Dataset or DataArray.
1691 
1692         This is equivalent to xr.broadcast(other, self)[1]
1693 
1694         xarray objects are broadcast against each other in arithmetic
1695         operations, so this method is not be necessary for most uses.
1696 
1697         If no change is needed, the input data is returned to the output
1698         without being copied.
1699 
1700         If new coords are added by the broadcast, their values are
1701         NaN filled.
1702 
1703         Parameters
1704         ----------
1705         other : Dataset or DataArray
1706             Object against which to broadcast this array.
1707         exclude : iterable of Hashable, optional
1708             Dimensions that must not be broadcasted
1709 
1710         Returns
1711         -------
1712         new_da : DataArray
1713             The caller broadcasted against ``other``.
1714 
1715         Examples
1716         --------
1717         >>> arr1 = xr.DataArray(
1718         ...     np.random.randn(2, 3),
1719         ...     dims=("x", "y"),
1720         ...     coords={"x": ["a", "b"], "y": ["a", "b", "c"]},
1721         ... )
1722         >>> arr2 = xr.DataArray(
1723         ...     np.random.randn(3, 2),
1724         ...     dims=("x", "y"),
1725         ...     coords={"x": ["a", "b", "c"], "y": ["a", "b"]},
1726         ... )
1727         >>> arr1
1728         <xarray.DataArray (x: 2, y: 3)>
1729         array([[ 1.76405235,  0.40015721,  0.97873798],
1730                [ 2.2408932 ,  1.86755799, -0.97727788]])
1731         Coordinates:
1732           * x        (x) <U1 'a' 'b'
1733           * y        (y) <U1 'a' 'b' 'c'
1734         >>> arr2
1735         <xarray.DataArray (x: 3, y: 2)>
1736         array([[ 0.95008842, -0.15135721],
1737                [-0.10321885,  0.4105985 ],
1738                [ 0.14404357,  1.45427351]])
1739         Coordinates:
1740           * x        (x) <U1 'a' 'b' 'c'
1741           * y        (y) <U1 'a' 'b'
1742         >>> arr1.broadcast_like(arr2)
1743         <xarray.DataArray (x: 3, y: 3)>
1744         array([[ 1.76405235,  0.40015721,  0.97873798],
1745                [ 2.2408932 ,  1.86755799, -0.97727788],
1746                [        nan,         nan,         nan]])
1747         Coordinates:
1748           * x        (x) <U1 'a' 'b' 'c'
1749           * y        (y) <U1 'a' 'b' 'c'
1750         """
1751         if exclude is None:
1752             exclude = set()
1753         else:
1754             exclude = set(exclude)
1755         args = align(other, self, join="outer", copy=False, exclude=exclude)
1756 
1757         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1758 
1759         return _broadcast_helper(
1760             cast("T_DataArray", args[1]), exclude, dims_map, common_coords
1761         )
1762 
1763     def _reindex_callback(
1764         self: T_DataArray,
1765         aligner: alignment.Aligner,
1766         dim_pos_indexers: dict[Hashable, Any],
1767         variables: dict[Hashable, Variable],
1768         indexes: dict[Hashable, Index],
1769         fill_value: Any,
1770         exclude_dims: frozenset[Hashable],
1771         exclude_vars: frozenset[Hashable],
1772     ) -> T_DataArray:
1773         """Callback called from ``Aligner`` to create a new reindexed DataArray."""
1774 
1775         if isinstance(fill_value, dict):
1776             fill_value = fill_value.copy()
1777             sentinel = object()
1778             value = fill_value.pop(self.name, sentinel)
1779             if value is not sentinel:
1780                 fill_value[_THIS_ARRAY] = value
1781 
1782         ds = self._to_temp_dataset()
1783         reindexed = ds._reindex_callback(
1784             aligner,
1785             dim_pos_indexers,
1786             variables,
1787             indexes,
1788             fill_value,
1789             exclude_dims,
1790             exclude_vars,
1791         )
1792         return self._from_temp_dataset(reindexed)
1793 
1794     def reindex_like(
1795         self: T_DataArray,
1796         other: DataArray | Dataset,
1797         method: ReindexMethodOptions = None,
1798         tolerance: int | float | Iterable[int | float] | None = None,
1799         copy: bool = True,
1800         fill_value=dtypes.NA,
1801     ) -> T_DataArray:
1802         """Conform this object onto the indexes of another object, filling in
1803         missing values with ``fill_value``. The default fill value is NaN.
1804 
1805         Parameters
1806         ----------
1807         other : Dataset or DataArray
1808             Object with an 'indexes' attribute giving a mapping from dimension
1809             names to pandas.Index objects, which provides coordinates upon
1810             which to index the variables in this dataset. The indexes on this
1811             other object need not be the same as the indexes on this
1812             dataset. Any mis-matched index values will be filled in with
1813             NaN, and any mis-matched dimension names will simply be ignored.
1814         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1815             Method to use for filling index values from other not found on this
1816             data array:
1817 
1818             - None (default): don't fill gaps
1819             - pad / ffill: propagate last valid index value forward
1820             - backfill / bfill: propagate next valid index value backward
1821             - nearest: use nearest valid index value
1822 
1823         tolerance : optional
1824             Maximum distance between original and new labels for inexact
1825             matches. The values of the index at the matching locations must
1826             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1827             Tolerance may be a scalar value, which applies the same tolerance
1828             to all values, or list-like, which applies variable tolerance per
1829             element. List-like must be the same size as the index and its dtype
1830             must exactly match the index’s type.
1831         copy : bool, default: True
1832             If ``copy=True``, data in the return value is always copied. If
1833             ``copy=False`` and reindexing is unnecessary, or can be performed
1834             with only slice operations, then the output may share memory with
1835             the input. In either case, a new xarray object is always returned.
1836         fill_value : scalar or dict-like, optional
1837             Value to use for newly missing values. If a dict-like, maps
1838             variable names (including coordinates) to fill values. Use this
1839             data array's name to refer to the data array's values.
1840 
1841         Returns
1842         -------
1843         reindexed : DataArray
1844             Another dataset array, with this array's data but coordinates from
1845             the other object.
1846 
1847         Examples
1848         --------
1849         >>> data = np.arange(12).reshape(4, 3)
1850         >>> da1 = xr.DataArray(
1851         ...     data=data,
1852         ...     dims=["x", "y"],
1853         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
1854         ... )
1855         >>> da1
1856         <xarray.DataArray (x: 4, y: 3)>
1857         array([[ 0,  1,  2],
1858                [ 3,  4,  5],
1859                [ 6,  7,  8],
1860                [ 9, 10, 11]])
1861         Coordinates:
1862           * x        (x) int64 10 20 30 40
1863           * y        (y) int64 70 80 90
1864         >>> da2 = xr.DataArray(
1865         ...     data=data,
1866         ...     dims=["x", "y"],
1867         ...     coords={"x": [40, 30, 20, 10], "y": [90, 80, 70]},
1868         ... )
1869         >>> da2
1870         <xarray.DataArray (x: 4, y: 3)>
1871         array([[ 0,  1,  2],
1872                [ 3,  4,  5],
1873                [ 6,  7,  8],
1874                [ 9, 10, 11]])
1875         Coordinates:
1876           * x        (x) int64 40 30 20 10
1877           * y        (y) int64 90 80 70
1878 
1879         Reindexing with both DataArrays having the same coordinates set, but in different order:
1880 
1881         >>> da1.reindex_like(da2)
1882         <xarray.DataArray (x: 4, y: 3)>
1883         array([[11, 10,  9],
1884                [ 8,  7,  6],
1885                [ 5,  4,  3],
1886                [ 2,  1,  0]])
1887         Coordinates:
1888           * x        (x) int64 40 30 20 10
1889           * y        (y) int64 90 80 70
1890 
1891         Reindexing with the other array having coordinates which the source array doesn't have:
1892 
1893         >>> data = np.arange(12).reshape(4, 3)
1894         >>> da1 = xr.DataArray(
1895         ...     data=data,
1896         ...     dims=["x", "y"],
1897         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
1898         ... )
1899         >>> da2 = xr.DataArray(
1900         ...     data=data,
1901         ...     dims=["x", "y"],
1902         ...     coords={"x": [20, 10, 29, 39], "y": [70, 80, 90]},
1903         ... )
1904         >>> da1.reindex_like(da2)
1905         <xarray.DataArray (x: 4, y: 3)>
1906         array([[ 3.,  4.,  5.],
1907                [ 0.,  1.,  2.],
1908                [nan, nan, nan],
1909                [nan, nan, nan]])
1910         Coordinates:
1911           * x        (x) int64 20 10 29 39
1912           * y        (y) int64 70 80 90
1913 
1914         Filling missing values with the previous valid index with respect to the coordinates' value:
1915 
1916         >>> da1.reindex_like(da2, method="ffill")
1917         <xarray.DataArray (x: 4, y: 3)>
1918         array([[3, 4, 5],
1919                [0, 1, 2],
1920                [3, 4, 5],
1921                [6, 7, 8]])
1922         Coordinates:
1923           * x        (x) int64 20 10 29 39
1924           * y        (y) int64 70 80 90
1925 
1926         Filling missing values while tolerating specified error for inexact matches:
1927 
1928         >>> da1.reindex_like(da2, method="ffill", tolerance=5)
1929         <xarray.DataArray (x: 4, y: 3)>
1930         array([[ 3.,  4.,  5.],
1931                [ 0.,  1.,  2.],
1932                [nan, nan, nan],
1933                [nan, nan, nan]])
1934         Coordinates:
1935           * x        (x) int64 20 10 29 39
1936           * y        (y) int64 70 80 90
1937 
1938         Filling missing values with manually specified values:
1939 
1940         >>> da1.reindex_like(da2, fill_value=19)
1941         <xarray.DataArray (x: 4, y: 3)>
1942         array([[ 3,  4,  5],
1943                [ 0,  1,  2],
1944                [19, 19, 19],
1945                [19, 19, 19]])
1946         Coordinates:
1947           * x        (x) int64 20 10 29 39
1948           * y        (y) int64 70 80 90
1949 
1950         See Also
1951         --------
1952         DataArray.reindex
1953         align
1954         """
1955         return alignment.reindex_like(
1956             self,
1957             other=other,
1958             method=method,
1959             tolerance=tolerance,
1960             copy=copy,
1961             fill_value=fill_value,
1962         )
1963 
1964     def reindex(
1965         self: T_DataArray,
1966         indexers: Mapping[Any, Any] | None = None,
1967         method: ReindexMethodOptions = None,
1968         tolerance: float | Iterable[float] | None = None,
1969         copy: bool = True,
1970         fill_value=dtypes.NA,
1971         **indexers_kwargs: Any,
1972     ) -> T_DataArray:
1973         """Conform this object onto the indexes of another object, filling in
1974         missing values with ``fill_value``. The default fill value is NaN.
1975 
1976         Parameters
1977         ----------
1978         indexers : dict, optional
1979             Dictionary with keys given by dimension names and values given by
1980             arrays of coordinates tick labels. Any mis-matched coordinate
1981             values will be filled in with NaN, and any mis-matched dimension
1982             names will simply be ignored.
1983             One of indexers or indexers_kwargs must be provided.
1984         copy : bool, optional
1985             If ``copy=True``, data in the return value is always copied. If
1986             ``copy=False`` and reindexing is unnecessary, or can be performed
1987             with only slice operations, then the output may share memory with
1988             the input. In either case, a new xarray object is always returned.
1989         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1990             Method to use for filling index values in ``indexers`` not found on
1991             this data array:
1992 
1993             - None (default): don't fill gaps
1994             - pad / ffill: propagate last valid index value forward
1995             - backfill / bfill: propagate next valid index value backward
1996             - nearest: use nearest valid index value
1997 
1998         tolerance : float | Iterable[float] | None, default: None
1999             Maximum distance between original and new labels for inexact
2000             matches. The values of the index at the matching locations must
2001             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2002             Tolerance may be a scalar value, which applies the same tolerance
2003             to all values, or list-like, which applies variable tolerance per
2004             element. List-like must be the same size as the index and its dtype
2005             must exactly match the index’s type.
2006         fill_value : scalar or dict-like, optional
2007             Value to use for newly missing values. If a dict-like, maps
2008             variable names (including coordinates) to fill values. Use this
2009             data array's name to refer to the data array's values.
2010         **indexers_kwargs : {dim: indexer, ...}, optional
2011             The keyword arguments form of ``indexers``.
2012             One of indexers or indexers_kwargs must be provided.
2013 
2014         Returns
2015         -------
2016         reindexed : DataArray
2017             Another dataset array, with this array's data but replaced
2018             coordinates.
2019 
2020         Examples
2021         --------
2022         Reverse latitude:
2023 
2024         >>> da = xr.DataArray(
2025         ...     np.arange(4),
2026         ...     coords=[np.array([90, 89, 88, 87])],
2027         ...     dims="lat",
2028         ... )
2029         >>> da
2030         <xarray.DataArray (lat: 4)>
2031         array([0, 1, 2, 3])
2032         Coordinates:
2033           * lat      (lat) int64 90 89 88 87
2034         >>> da.reindex(lat=da.lat[::-1])
2035         <xarray.DataArray (lat: 4)>
2036         array([3, 2, 1, 0])
2037         Coordinates:
2038           * lat      (lat) int64 87 88 89 90
2039 
2040         See Also
2041         --------
2042         DataArray.reindex_like
2043         align
2044         """
2045         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
2046         return alignment.reindex(
2047             self,
2048             indexers=indexers,
2049             method=method,
2050             tolerance=tolerance,
2051             copy=copy,
2052             fill_value=fill_value,
2053         )
2054 
2055     def interp(
2056         self: T_DataArray,
2057         coords: Mapping[Any, Any] | None = None,
2058         method: InterpOptions = "linear",
2059         assume_sorted: bool = False,
2060         kwargs: Mapping[str, Any] | None = None,
2061         **coords_kwargs: Any,
2062     ) -> T_DataArray:
2063         """Interpolate a DataArray onto new coordinates
2064 
2065         Performs univariate or multivariate interpolation of a DataArray onto
2066         new coordinates using scipy's interpolation routines. If interpolating
2067         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
2068         called. When interpolating along multiple existing dimensions, an
2069         attempt is made to decompose the interpolation into multiple
2070         1-dimensional interpolations. If this is possible,
2071         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
2072         :py:func:`scipy.interpolate.interpn` is called.
2073 
2074         Parameters
2075         ----------
2076         coords : dict, optional
2077             Mapping from dimension names to the new coordinates.
2078             New coordinate can be a scalar, array-like or DataArray.
2079             If DataArrays are passed as new coordinates, their dimensions are
2080             used for the broadcasting. Missing values are skipped.
2081         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
2082             The method used to interpolate. The method should be supported by
2083             the scipy interpolator:
2084 
2085             - ``interp1d``: {"linear", "nearest", "zero", "slinear",
2086               "quadratic", "cubic", "polynomial"}
2087             - ``interpn``: {"linear", "nearest"}
2088 
2089             If ``"polynomial"`` is passed, the ``order`` keyword argument must
2090             also be provided.
2091         assume_sorted : bool, default: False
2092             If False, values of x can be in any order and they are sorted
2093             first. If True, x has to be an array of monotonically increasing
2094             values.
2095         kwargs : dict-like or None, default: None
2096             Additional keyword arguments passed to scipy's interpolator. Valid
2097             options and their behavior depend whether ``interp1d`` or
2098             ``interpn`` is used.
2099         **coords_kwargs : {dim: coordinate, ...}, optional
2100             The keyword arguments form of ``coords``.
2101             One of coords or coords_kwargs must be provided.
2102 
2103         Returns
2104         -------
2105         interpolated : DataArray
2106             New dataarray on the new coordinates.
2107 
2108         Notes
2109         -----
2110         scipy is required.
2111 
2112         See Also
2113         --------
2114         scipy.interpolate.interp1d
2115         scipy.interpolate.interpn
2116 
2117         Examples
2118         --------
2119         >>> da = xr.DataArray(
2120         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
2121         ...     dims=("x", "y"),
2122         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
2123         ... )
2124         >>> da
2125         <xarray.DataArray (x: 3, y: 4)>
2126         array([[ 1.,  4.,  2.,  9.],
2127                [ 2.,  7.,  6., nan],
2128                [ 6., nan,  5.,  8.]])
2129         Coordinates:
2130           * x        (x) int64 0 1 2
2131           * y        (y) int64 10 12 14 16
2132 
2133         1D linear interpolation (the default):
2134 
2135         >>> da.interp(x=[0, 0.75, 1.25, 1.75])
2136         <xarray.DataArray (x: 4, y: 4)>
2137         array([[1.  , 4.  , 2.  ,  nan],
2138                [1.75, 6.25, 5.  ,  nan],
2139                [3.  ,  nan, 5.75,  nan],
2140                [5.  ,  nan, 5.25,  nan]])
2141         Coordinates:
2142           * y        (y) int64 10 12 14 16
2143           * x        (x) float64 0.0 0.75 1.25 1.75
2144 
2145         1D nearest interpolation:
2146 
2147         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
2148         <xarray.DataArray (x: 4, y: 4)>
2149         array([[ 1.,  4.,  2.,  9.],
2150                [ 2.,  7.,  6., nan],
2151                [ 2.,  7.,  6., nan],
2152                [ 6., nan,  5.,  8.]])
2153         Coordinates:
2154           * y        (y) int64 10 12 14 16
2155           * x        (x) float64 0.0 0.75 1.25 1.75
2156 
2157         1D linear extrapolation:
2158 
2159         >>> da.interp(
2160         ...     x=[1, 1.5, 2.5, 3.5],
2161         ...     method="linear",
2162         ...     kwargs={"fill_value": "extrapolate"},
2163         ... )
2164         <xarray.DataArray (x: 4, y: 4)>
2165         array([[ 2. ,  7. ,  6. ,  nan],
2166                [ 4. ,  nan,  5.5,  nan],
2167                [ 8. ,  nan,  4.5,  nan],
2168                [12. ,  nan,  3.5,  nan]])
2169         Coordinates:
2170           * y        (y) int64 10 12 14 16
2171           * x        (x) float64 1.0 1.5 2.5 3.5
2172 
2173         2D linear interpolation:
2174 
2175         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
2176         <xarray.DataArray (x: 4, y: 3)>
2177         array([[2.5  , 3.   ,   nan],
2178                [4.   , 5.625,   nan],
2179                [  nan,   nan,   nan],
2180                [  nan,   nan,   nan]])
2181         Coordinates:
2182           * x        (x) float64 0.0 0.75 1.25 1.75
2183           * y        (y) int64 11 13 15
2184         """
2185         if self.dtype.kind not in "uifc":
2186             raise TypeError(
2187                 "interp only works for a numeric type array. "
2188                 "Given {}.".format(self.dtype)
2189             )
2190         ds = self._to_temp_dataset().interp(
2191             coords,
2192             method=method,
2193             kwargs=kwargs,
2194             assume_sorted=assume_sorted,
2195             **coords_kwargs,
2196         )
2197         return self._from_temp_dataset(ds)
2198 
2199     def interp_like(
2200         self: T_DataArray,
2201         other: DataArray | Dataset,
2202         method: InterpOptions = "linear",
2203         assume_sorted: bool = False,
2204         kwargs: Mapping[str, Any] | None = None,
2205     ) -> T_DataArray:
2206         """Interpolate this object onto the coordinates of another object,
2207         filling out of range values with NaN.
2208 
2209         If interpolating along a single existing dimension,
2210         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
2211         along multiple existing dimensions, an attempt is made to decompose the
2212         interpolation into multiple 1-dimensional interpolations. If this is
2213         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
2214         :py:func:`scipy.interpolate.interpn` is called.
2215 
2216         Parameters
2217         ----------
2218         other : Dataset or DataArray
2219             Object with an 'indexes' attribute giving a mapping from dimension
2220             names to an 1d array-like, which provides coordinates upon
2221             which to index the variables in this dataset. Missing values are skipped.
2222         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
2223             The method used to interpolate. The method should be supported by
2224             the scipy interpolator:
2225 
2226             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic",
2227               "polynomial"} when ``interp1d`` is called.
2228             - {"linear", "nearest"} when ``interpn`` is called.
2229 
2230             If ``"polynomial"`` is passed, the ``order`` keyword argument must
2231             also be provided.
2232         assume_sorted : bool, default: False
2233             If False, values of coordinates that are interpolated over can be
2234             in any order and they are sorted first. If True, interpolated
2235             coordinates are assumed to be an array of monotonically increasing
2236             values.
2237         kwargs : dict, optional
2238             Additional keyword passed to scipy's interpolator.
2239 
2240         Returns
2241         -------
2242         interpolated : DataArray
2243             Another dataarray by interpolating this dataarray's data along the
2244             coordinates of the other object.
2245 
2246         Examples
2247         --------
2248         >>> data = np.arange(12).reshape(4, 3)
2249         >>> da1 = xr.DataArray(
2250         ...     data=data,
2251         ...     dims=["x", "y"],
2252         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
2253         ... )
2254         >>> da1
2255         <xarray.DataArray (x: 4, y: 3)>
2256         array([[ 0,  1,  2],
2257                [ 3,  4,  5],
2258                [ 6,  7,  8],
2259                [ 9, 10, 11]])
2260         Coordinates:
2261           * x        (x) int64 10 20 30 40
2262           * y        (y) int64 70 80 90
2263         >>> da2 = xr.DataArray(
2264         ...     data=data,
2265         ...     dims=["x", "y"],
2266         ...     coords={"x": [10, 20, 29, 39], "y": [70, 80, 90]},
2267         ... )
2268         >>> da2
2269         <xarray.DataArray (x: 4, y: 3)>
2270         array([[ 0,  1,  2],
2271                [ 3,  4,  5],
2272                [ 6,  7,  8],
2273                [ 9, 10, 11]])
2274         Coordinates:
2275           * x        (x) int64 10 20 29 39
2276           * y        (y) int64 70 80 90
2277 
2278         Interpolate the values in the coordinates of the other DataArray with respect to the source's values:
2279 
2280         >>> da2.interp_like(da1)
2281         <xarray.DataArray (x: 4, y: 3)>
2282         array([[0. , 1. , 2. ],
2283                [3. , 4. , 5. ],
2284                [6.3, 7.3, 8.3],
2285                [nan, nan, nan]])
2286         Coordinates:
2287           * x        (x) int64 10 20 30 40
2288           * y        (y) int64 70 80 90
2289 
2290         Could also extrapolate missing values:
2291 
2292         >>> da2.interp_like(da1, kwargs={"fill_value": "extrapolate"})
2293         <xarray.DataArray (x: 4, y: 3)>
2294         array([[ 0. ,  1. ,  2. ],
2295                [ 3. ,  4. ,  5. ],
2296                [ 6.3,  7.3,  8.3],
2297                [ 9.3, 10.3, 11.3]])
2298         Coordinates:
2299           * x        (x) int64 10 20 30 40
2300           * y        (y) int64 70 80 90
2301 
2302         Notes
2303         -----
2304         scipy is required.
2305         If the dataarray has object-type coordinates, reindex is used for these
2306         coordinates instead of the interpolation.
2307 
2308         See Also
2309         --------
2310         DataArray.interp
2311         DataArray.reindex_like
2312         """
2313         if self.dtype.kind not in "uifc":
2314             raise TypeError(
2315                 "interp only works for a numeric type array. "
2316                 "Given {}.".format(self.dtype)
2317             )
2318         ds = self._to_temp_dataset().interp_like(
2319             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
2320         )
2321         return self._from_temp_dataset(ds)
2322 
2323     # change type of self and return to T_DataArray once
2324     # https://github.com/python/mypy/issues/12846 is resolved
2325     def rename(
2326         self,
2327         new_name_or_name_dict: Hashable | Mapping[Any, Hashable] | None = None,
2328         **names: Hashable,
2329     ) -> DataArray:
2330         """Returns a new DataArray with renamed coordinates, dimensions or a new name.
2331 
2332         Parameters
2333         ----------
2334         new_name_or_name_dict : str or dict-like, optional
2335             If the argument is dict-like, it used as a mapping from old
2336             names to new names for coordinates or dimensions. Otherwise,
2337             use the argument as the new name for this array.
2338         **names : Hashable, optional
2339             The keyword arguments form of a mapping from old names to
2340             new names for coordinates or dimensions.
2341             One of new_name_or_name_dict or names must be provided.
2342 
2343         Returns
2344         -------
2345         renamed : DataArray
2346             Renamed array or array with renamed coordinates.
2347 
2348         See Also
2349         --------
2350         Dataset.rename
2351         DataArray.swap_dims
2352         """
2353         if new_name_or_name_dict is None and not names:
2354             # change name to None?
2355             return self._replace(name=None)
2356         if utils.is_dict_like(new_name_or_name_dict) or new_name_or_name_dict is None:
2357             # change dims/coords
2358             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
2359             dataset = self._to_temp_dataset()._rename(name_dict)
2360             return self._from_temp_dataset(dataset)
2361         if utils.hashable(new_name_or_name_dict) and names:
2362             # change name + dims/coords
2363             dataset = self._to_temp_dataset()._rename(names)
2364             dataarray = self._from_temp_dataset(dataset)
2365             return dataarray._replace(name=new_name_or_name_dict)
2366         # only change name
2367         return self._replace(name=new_name_or_name_dict)
2368 
2369     def swap_dims(
2370         self: T_DataArray,
2371         dims_dict: Mapping[Any, Hashable] | None = None,
2372         **dims_kwargs,
2373     ) -> T_DataArray:
2374         """Returns a new DataArray with swapped dimensions.
2375 
2376         Parameters
2377         ----------
2378         dims_dict : dict-like
2379             Dictionary whose keys are current dimension names and whose values
2380             are new names.
2381         **dims_kwargs : {existing_dim: new_dim, ...}, optional
2382             The keyword arguments form of ``dims_dict``.
2383             One of dims_dict or dims_kwargs must be provided.
2384 
2385         Returns
2386         -------
2387         swapped : DataArray
2388             DataArray with swapped dimensions.
2389 
2390         Examples
2391         --------
2392         >>> arr = xr.DataArray(
2393         ...     data=[0, 1],
2394         ...     dims="x",
2395         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
2396         ... )
2397         >>> arr
2398         <xarray.DataArray (x: 2)>
2399         array([0, 1])
2400         Coordinates:
2401           * x        (x) <U1 'a' 'b'
2402             y        (x) int64 0 1
2403 
2404         >>> arr.swap_dims({"x": "y"})
2405         <xarray.DataArray (y: 2)>
2406         array([0, 1])
2407         Coordinates:
2408             x        (y) <U1 'a' 'b'
2409           * y        (y) int64 0 1
2410 
2411         >>> arr.swap_dims({"x": "z"})
2412         <xarray.DataArray (z: 2)>
2413         array([0, 1])
2414         Coordinates:
2415             x        (z) <U1 'a' 'b'
2416             y        (z) int64 0 1
2417         Dimensions without coordinates: z
2418 
2419         See Also
2420         --------
2421         DataArray.rename
2422         Dataset.swap_dims
2423         """
2424         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
2425         ds = self._to_temp_dataset().swap_dims(dims_dict)
2426         return self._from_temp_dataset(ds)
2427 
2428     # change type of self and return to T_DataArray once
2429     # https://github.com/python/mypy/issues/12846 is resolved
2430     def expand_dims(
2431         self,
2432         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
2433         axis: None | int | Sequence[int] = None,
2434         **dim_kwargs: Any,
2435     ) -> DataArray:
2436         """Return a new object with an additional axis (or axes) inserted at
2437         the corresponding position in the array shape. The new object is a
2438         view into the underlying array, not a copy.
2439 
2440         If dim is already a scalar coordinate, it will be promoted to a 1D
2441         coordinate consisting of a single value.
2442 
2443         Parameters
2444         ----------
2445         dim : Hashable, sequence of Hashable, dict, or None, optional
2446             Dimensions to include on the new variable.
2447             If provided as str or sequence of str, then dimensions are inserted
2448             with length 1. If provided as a dict, then the keys are the new
2449             dimensions and the values are either integers (giving the length of
2450             the new dimensions) or sequence/ndarray (giving the coordinates of
2451             the new dimensions).
2452         axis : int, sequence of int, or None, default: None
2453             Axis position(s) where new axis is to be inserted (position(s) on
2454             the result array). If a sequence of integers is passed,
2455             multiple axes are inserted. In this case, dim arguments should be
2456             same length list. If axis=None is passed, all the axes will be
2457             inserted to the start of the result array.
2458         **dim_kwargs : int or sequence or ndarray
2459             The keywords are arbitrary dimensions being inserted and the values
2460             are either the lengths of the new dims (if int is given), or their
2461             coordinates. Note, this is an alternative to passing a dict to the
2462             dim kwarg and will only be used if dim is None.
2463 
2464         Returns
2465         -------
2466         expanded : DataArray
2467             This object, but with additional dimension(s).
2468 
2469         See Also
2470         --------
2471         Dataset.expand_dims
2472 
2473         Examples
2474         --------
2475         >>> da = xr.DataArray(np.arange(5), dims=("x"))
2476         >>> da
2477         <xarray.DataArray (x: 5)>
2478         array([0, 1, 2, 3, 4])
2479         Dimensions without coordinates: x
2480 
2481         Add new dimension of length 2:
2482 
2483         >>> da.expand_dims(dim={"y": 2})
2484         <xarray.DataArray (y: 2, x: 5)>
2485         array([[0, 1, 2, 3, 4],
2486                [0, 1, 2, 3, 4]])
2487         Dimensions without coordinates: y, x
2488 
2489         >>> da.expand_dims(dim={"y": 2}, axis=1)
2490         <xarray.DataArray (x: 5, y: 2)>
2491         array([[0, 0],
2492                [1, 1],
2493                [2, 2],
2494                [3, 3],
2495                [4, 4]])
2496         Dimensions without coordinates: x, y
2497 
2498         Add a new dimension with coordinates from array:
2499 
2500         >>> da.expand_dims(dim={"y": np.arange(5)}, axis=0)
2501         <xarray.DataArray (y: 5, x: 5)>
2502         array([[0, 1, 2, 3, 4],
2503                [0, 1, 2, 3, 4],
2504                [0, 1, 2, 3, 4],
2505                [0, 1, 2, 3, 4],
2506                [0, 1, 2, 3, 4]])
2507         Coordinates:
2508           * y        (y) int64 0 1 2 3 4
2509         Dimensions without coordinates: x
2510         """
2511         if isinstance(dim, int):
2512             raise TypeError("dim should be Hashable or sequence/mapping of Hashables")
2513         elif isinstance(dim, Sequence) and not isinstance(dim, str):
2514             if len(dim) != len(set(dim)):
2515                 raise ValueError("dims should not contain duplicate values.")
2516             dim = dict.fromkeys(dim, 1)
2517         elif dim is not None and not isinstance(dim, Mapping):
2518             dim = {cast(Hashable, dim): 1}
2519 
2520         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
2521         ds = self._to_temp_dataset().expand_dims(dim, axis)
2522         return self._from_temp_dataset(ds)
2523 
2524     # change type of self and return to T_DataArray once
2525     # https://github.com/python/mypy/issues/12846 is resolved
2526     def set_index(
2527         self,
2528         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,
2529         append: bool = False,
2530         **indexes_kwargs: Hashable | Sequence[Hashable],
2531     ) -> DataArray:
2532         """Set DataArray (multi-)indexes using one or more existing
2533         coordinates.
2534 
2535         This legacy method is limited to pandas (multi-)indexes and
2536         1-dimensional "dimension" coordinates. See
2537         :py:meth:`~DataArray.set_xindex` for setting a pandas or a custom
2538         Xarray-compatible index from one or more arbitrary coordinates.
2539 
2540         Parameters
2541         ----------
2542         indexes : {dim: index, ...}
2543             Mapping from names matching dimensions and values given
2544             by (lists of) the names of existing coordinates or variables to set
2545             as new (multi-)index.
2546         append : bool, default: False
2547             If True, append the supplied index(es) to the existing index(es).
2548             Otherwise replace the existing index(es).
2549         **indexes_kwargs : optional
2550             The keyword arguments form of ``indexes``.
2551             One of indexes or indexes_kwargs must be provided.
2552 
2553         Returns
2554         -------
2555         obj : DataArray
2556             Another DataArray, with this data but replaced coordinates.
2557 
2558         Examples
2559         --------
2560         >>> arr = xr.DataArray(
2561         ...     data=np.ones((2, 3)),
2562         ...     dims=["x", "y"],
2563         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
2564         ... )
2565         >>> arr
2566         <xarray.DataArray (x: 2, y: 3)>
2567         array([[1., 1., 1.],
2568                [1., 1., 1.]])
2569         Coordinates:
2570           * x        (x) int64 0 1
2571           * y        (y) int64 0 1 2
2572             a        (x) int64 3 4
2573         >>> arr.set_index(x="a")
2574         <xarray.DataArray (x: 2, y: 3)>
2575         array([[1., 1., 1.],
2576                [1., 1., 1.]])
2577         Coordinates:
2578           * x        (x) int64 3 4
2579           * y        (y) int64 0 1 2
2580 
2581         See Also
2582         --------
2583         DataArray.reset_index
2584         DataArray.set_xindex
2585         """
2586         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)
2587         return self._from_temp_dataset(ds)
2588 
2589     # change type of self and return to T_DataArray once
2590     # https://github.com/python/mypy/issues/12846 is resolved
2591     def reset_index(
2592         self,
2593         dims_or_levels: Hashable | Sequence[Hashable],
2594         drop: bool = False,
2595     ) -> DataArray:
2596         """Reset the specified index(es) or multi-index level(s).
2597 
2598         This legacy method is specific to pandas (multi-)indexes and
2599         1-dimensional "dimension" coordinates. See the more generic
2600         :py:meth:`~DataArray.drop_indexes` and :py:meth:`~DataArray.set_xindex`
2601         method to respectively drop and set pandas or custom indexes for
2602         arbitrary coordinates.
2603 
2604         Parameters
2605         ----------
2606         dims_or_levels : Hashable or sequence of Hashable
2607             Name(s) of the dimension(s) and/or multi-index level(s) that will
2608             be reset.
2609         drop : bool, default: False
2610             If True, remove the specified indexes and/or multi-index levels
2611             instead of extracting them as new coordinates (default: False).
2612 
2613         Returns
2614         -------
2615         obj : DataArray
2616             Another dataarray, with this dataarray's data but replaced
2617             coordinates.
2618 
2619         See Also
2620         --------
2621         DataArray.set_index
2622         DataArray.set_xindex
2623         DataArray.drop_indexes
2624         """
2625         ds = self._to_temp_dataset().reset_index(dims_or_levels, drop=drop)
2626         return self._from_temp_dataset(ds)
2627 
2628     def set_xindex(
2629         self: T_DataArray,
2630         coord_names: str | Sequence[Hashable],
2631         index_cls: type[Index] | None = None,
2632         **options,
2633     ) -> T_DataArray:
2634         """Set a new, Xarray-compatible index from one or more existing
2635         coordinate(s).
2636 
2637         Parameters
2638         ----------
2639         coord_names : str or list
2640             Name(s) of the coordinate(s) used to build the index.
2641             If several names are given, their order matters.
2642         index_cls : subclass of :class:`~xarray.indexes.Index`
2643             The type of index to create. By default, try setting
2644             a pandas (multi-)index from the supplied coordinates.
2645         **options
2646             Options passed to the index constructor.
2647 
2648         Returns
2649         -------
2650         obj : DataArray
2651             Another dataarray, with this dataarray's data and with a new index.
2652 
2653         """
2654         ds = self._to_temp_dataset().set_xindex(coord_names, index_cls, **options)
2655         return self._from_temp_dataset(ds)
2656 
2657     def reorder_levels(
2658         self: T_DataArray,
2659         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
2660         **dim_order_kwargs: Sequence[int | Hashable],
2661     ) -> T_DataArray:
2662         """Rearrange index levels using input order.
2663 
2664         Parameters
2665         ----------
2666         dim_order dict-like of Hashable to int or Hashable: optional
2667             Mapping from names matching dimensions and values given
2668             by lists representing new level orders. Every given dimension
2669             must have a multi-index.
2670         **dim_order_kwargs : optional
2671             The keyword arguments form of ``dim_order``.
2672             One of dim_order or dim_order_kwargs must be provided.
2673 
2674         Returns
2675         -------
2676         obj : DataArray
2677             Another dataarray, with this dataarray's data but replaced
2678             coordinates.
2679         """
2680         ds = self._to_temp_dataset().reorder_levels(dim_order, **dim_order_kwargs)
2681         return self._from_temp_dataset(ds)
2682 
2683     def stack(
2684         self: T_DataArray,
2685         dimensions: Mapping[Any, Sequence[Hashable]] | None = None,
2686         create_index: bool | None = True,
2687         index_cls: type[Index] = PandasMultiIndex,
2688         **dimensions_kwargs: Sequence[Hashable],
2689     ) -> T_DataArray:
2690         """
2691         Stack any number of existing dimensions into a single new dimension.
2692 
2693         New dimensions will be added at the end, and the corresponding
2694         coordinate variables will be combined into a MultiIndex.
2695 
2696         Parameters
2697         ----------
2698         dimensions : mapping of Hashable to sequence of Hashable
2699             Mapping of the form `new_name=(dim1, dim2, ...)`.
2700             Names of new dimensions, and the existing dimensions that they
2701             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
2702             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
2703             all dimensions.
2704         create_index : bool or None, default: True
2705             If True, create a multi-index for each of the stacked dimensions.
2706             If False, don't create any index.
2707             If None, create a multi-index only if exactly one single (1-d) coordinate
2708             index is found for every dimension to stack.
2709         index_cls: class, optional
2710             Can be used to pass a custom multi-index type. Must be an Xarray index that
2711             implements `.stack()`. By default, a pandas multi-index wrapper is used.
2712         **dimensions_kwargs
2713             The keyword arguments form of ``dimensions``.
2714             One of dimensions or dimensions_kwargs must be provided.
2715 
2716         Returns
2717         -------
2718         stacked : DataArray
2719             DataArray with stacked data.
2720 
2721         Examples
2722         --------
2723         >>> arr = xr.DataArray(
2724         ...     np.arange(6).reshape(2, 3),
2725         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2726         ... )
2727         >>> arr
2728         <xarray.DataArray (x: 2, y: 3)>
2729         array([[0, 1, 2],
2730                [3, 4, 5]])
2731         Coordinates:
2732           * x        (x) <U1 'a' 'b'
2733           * y        (y) int64 0 1 2
2734         >>> stacked = arr.stack(z=("x", "y"))
2735         >>> stacked.indexes["z"]
2736         MultiIndex([('a', 0),
2737                     ('a', 1),
2738                     ('a', 2),
2739                     ('b', 0),
2740                     ('b', 1),
2741                     ('b', 2)],
2742                    name='z')
2743 
2744         See Also
2745         --------
2746         DataArray.unstack
2747         """
2748         ds = self._to_temp_dataset().stack(
2749             dimensions,
2750             create_index=create_index,
2751             index_cls=index_cls,
2752             **dimensions_kwargs,
2753         )
2754         return self._from_temp_dataset(ds)
2755 
2756     # change type of self and return to T_DataArray once
2757     # https://github.com/python/mypy/issues/12846 is resolved
2758     def unstack(
2759         self,
2760         dim: Dims = None,
2761         fill_value: Any = dtypes.NA,
2762         sparse: bool = False,
2763     ) -> DataArray:
2764         """
2765         Unstack existing dimensions corresponding to MultiIndexes into
2766         multiple new dimensions.
2767 
2768         New dimensions will be added at the end.
2769 
2770         Parameters
2771         ----------
2772         dim : str, Iterable of Hashable or None, optional
2773             Dimension(s) over which to unstack. By default unstacks all
2774             MultiIndexes.
2775         fill_value : scalar or dict-like, default: nan
2776             Value to be filled. If a dict-like, maps variable names to
2777             fill values. Use the data array's name to refer to its
2778             name. If not provided or if the dict-like does not contain
2779             all variables, the dtype's NA value will be used.
2780         sparse : bool, default: False
2781             Use sparse-array if True
2782 
2783         Returns
2784         -------
2785         unstacked : DataArray
2786             Array with unstacked data.
2787 
2788         Examples
2789         --------
2790         >>> arr = xr.DataArray(
2791         ...     np.arange(6).reshape(2, 3),
2792         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2793         ... )
2794         >>> arr
2795         <xarray.DataArray (x: 2, y: 3)>
2796         array([[0, 1, 2],
2797                [3, 4, 5]])
2798         Coordinates:
2799           * x        (x) <U1 'a' 'b'
2800           * y        (y) int64 0 1 2
2801         >>> stacked = arr.stack(z=("x", "y"))
2802         >>> stacked.indexes["z"]
2803         MultiIndex([('a', 0),
2804                     ('a', 1),
2805                     ('a', 2),
2806                     ('b', 0),
2807                     ('b', 1),
2808                     ('b', 2)],
2809                    name='z')
2810         >>> roundtripped = stacked.unstack()
2811         >>> arr.identical(roundtripped)
2812         True
2813 
2814         See Also
2815         --------
2816         DataArray.stack
2817         """
2818         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
2819         return self._from_temp_dataset(ds)
2820 
2821     def to_unstacked_dataset(self, dim: Hashable, level: int | Hashable = 0) -> Dataset:
2822         """Unstack DataArray expanding to Dataset along a given level of a
2823         stacked coordinate.
2824 
2825         This is the inverse operation of Dataset.to_stacked_array.
2826 
2827         Parameters
2828         ----------
2829         dim : Hashable
2830             Name of existing dimension to unstack
2831         level : int or Hashable, default: 0
2832             The MultiIndex level to expand to a dataset along. Can either be
2833             the integer index of the level or its name.
2834 
2835         Returns
2836         -------
2837         unstacked: Dataset
2838 
2839         Examples
2840         --------
2841         >>> arr = xr.DataArray(
2842         ...     np.arange(6).reshape(2, 3),
2843         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2844         ... )
2845         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
2846         >>> data
2847         <xarray.Dataset>
2848         Dimensions:  (x: 2, y: 3)
2849         Coordinates:
2850           * x        (x) <U1 'a' 'b'
2851           * y        (y) int64 0 1 2
2852         Data variables:
2853             a        (x, y) int64 0 1 2 3 4 5
2854             b        (x) int64 0 3
2855         >>> stacked = data.to_stacked_array("z", ["x"])
2856         >>> stacked.indexes["z"]
2857         MultiIndex([('a', 0.0),
2858                     ('a', 1.0),
2859                     ('a', 2.0),
2860                     ('b', nan)],
2861                    name='z')
2862         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
2863         >>> data.identical(roundtripped)
2864         True
2865 
2866         See Also
2867         --------
2868         Dataset.to_stacked_array
2869         """
2870         idx = self._indexes[dim].to_pandas_index()
2871         if not isinstance(idx, pd.MultiIndex):
2872             raise ValueError(f"'{dim}' is not a stacked coordinate")
2873 
2874         level_number = idx._get_level_number(level)
2875         variables = idx.levels[level_number]
2876         variable_dim = idx.names[level_number]
2877 
2878         # pull variables out of datarray
2879         data_dict = {}
2880         for k in variables:
2881             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)
2882 
2883         # unstacked dataset
2884         return Dataset(data_dict)
2885 
2886     def transpose(
2887         self: T_DataArray,
2888         *dims: Hashable,
2889         transpose_coords: bool = True,
2890         missing_dims: ErrorOptionsWithWarn = "raise",
2891     ) -> T_DataArray:
2892         """Return a new DataArray object with transposed dimensions.
2893 
2894         Parameters
2895         ----------
2896         *dims : Hashable, optional
2897             By default, reverse the dimensions. Otherwise, reorder the
2898             dimensions to this order.
2899         transpose_coords : bool, default: True
2900             If True, also transpose the coordinates of this DataArray.
2901         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2902             What to do if dimensions that should be selected from are not present in the
2903             DataArray:
2904             - "raise": raise an exception
2905             - "warn": raise a warning, and ignore the missing dimensions
2906             - "ignore": ignore the missing dimensions
2907 
2908         Returns
2909         -------
2910         transposed : DataArray
2911             The returned DataArray's array is transposed.
2912 
2913         Notes
2914         -----
2915         This operation returns a view of this array's data. It is
2916         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
2917         -- the data will be fully loaded.
2918 
2919         See Also
2920         --------
2921         numpy.transpose
2922         Dataset.transpose
2923         """
2924         if dims:
2925             dims = tuple(utils.infix_dims(dims, self.dims, missing_dims))
2926         variable = self.variable.transpose(*dims)
2927         if transpose_coords:
2928             coords: dict[Hashable, Variable] = {}
2929             for name, coord in self.coords.items():
2930                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
2931                 coords[name] = coord.variable.transpose(*coord_dims)
2932             return self._replace(variable, coords)
2933         else:
2934             return self._replace(variable)
2935 
2936     @property
2937     def T(self: T_DataArray) -> T_DataArray:
2938         return self.transpose()
2939 
2940     # change type of self and return to T_DataArray once
2941     # https://github.com/python/mypy/issues/12846 is resolved
2942     def drop_vars(
2943         self,
2944         names: Hashable | Iterable[Hashable],
2945         *,
2946         errors: ErrorOptions = "raise",
2947     ) -> DataArray:
2948         """Returns an array with dropped variables.
2949 
2950         Parameters
2951         ----------
2952         names : Hashable or iterable of Hashable
2953             Name(s) of variables to drop.
2954         errors : {"raise", "ignore"}, default: "raise"
2955             If 'raise', raises a ValueError error if any of the variable
2956             passed are not in the dataset. If 'ignore', any given names that are in the
2957             DataArray are dropped and no error is raised.
2958 
2959         Returns
2960         -------
2961         dropped : Dataset
2962             New Dataset copied from `self` with variables removed.
2963 
2964         Examples
2965         -------
2966         >>> data = np.arange(12).reshape(4, 3)
2967         >>> da = xr.DataArray(
2968         ...     data=data,
2969         ...     dims=["x", "y"],
2970         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
2971         ... )
2972         >>> da
2973         <xarray.DataArray (x: 4, y: 3)>
2974         array([[ 0,  1,  2],
2975                [ 3,  4,  5],
2976                [ 6,  7,  8],
2977                [ 9, 10, 11]])
2978         Coordinates:
2979           * x        (x) int64 10 20 30 40
2980           * y        (y) int64 70 80 90
2981 
2982         Removing a single variable:
2983 
2984         >>> da.drop_vars("x")
2985         <xarray.DataArray (x: 4, y: 3)>
2986         array([[ 0,  1,  2],
2987                [ 3,  4,  5],
2988                [ 6,  7,  8],
2989                [ 9, 10, 11]])
2990         Coordinates:
2991           * y        (y) int64 70 80 90
2992         Dimensions without coordinates: x
2993 
2994         Removing a list of variables:
2995 
2996         >>> da.drop_vars(["x", "y"])
2997         <xarray.DataArray (x: 4, y: 3)>
2998         array([[ 0,  1,  2],
2999                [ 3,  4,  5],
3000                [ 6,  7,  8],
3001                [ 9, 10, 11]])
3002         Dimensions without coordinates: x, y
3003         """
3004         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
3005         return self._from_temp_dataset(ds)
3006 
3007     def drop_indexes(
3008         self: T_DataArray,
3009         coord_names: Hashable | Iterable[Hashable],
3010         *,
3011         errors: ErrorOptions = "raise",
3012     ) -> T_DataArray:
3013         """Drop the indexes assigned to the given coordinates.
3014 
3015         Parameters
3016         ----------
3017         coord_names : hashable or iterable of hashable
3018             Name(s) of the coordinate(s) for which to drop the index.
3019         errors : {"raise", "ignore"}, default: "raise"
3020             If 'raise', raises a ValueError error if any of the coordinates
3021             passed have no index or are not in the dataset.
3022             If 'ignore', no error is raised.
3023 
3024         Returns
3025         -------
3026         dropped : DataArray
3027             A new dataarray with dropped indexes.
3028         """
3029         ds = self._to_temp_dataset().drop_indexes(coord_names, errors=errors)
3030         return self._from_temp_dataset(ds)
3031 
3032     def drop(
3033         self: T_DataArray,
3034         labels: Mapping[Any, Any] | None = None,
3035         dim: Hashable | None = None,
3036         *,
3037         errors: ErrorOptions = "raise",
3038         **labels_kwargs,
3039     ) -> T_DataArray:
3040         """Backward compatible method based on `drop_vars` and `drop_sel`
3041 
3042         Using either `drop_vars` or `drop_sel` is encouraged
3043 
3044         See Also
3045         --------
3046         DataArray.drop_vars
3047         DataArray.drop_sel
3048         """
3049         ds = self._to_temp_dataset().drop(labels, dim, errors=errors, **labels_kwargs)
3050         return self._from_temp_dataset(ds)
3051 
3052     def drop_sel(
3053         self: T_DataArray,
3054         labels: Mapping[Any, Any] | None = None,
3055         *,
3056         errors: ErrorOptions = "raise",
3057         **labels_kwargs,
3058     ) -> T_DataArray:
3059         """Drop index labels from this DataArray.
3060 
3061         Parameters
3062         ----------
3063         labels : mapping of Hashable to Any
3064             Index labels to drop
3065         errors : {"raise", "ignore"}, default: "raise"
3066             If 'raise', raises a ValueError error if
3067             any of the index labels passed are not
3068             in the dataset. If 'ignore', any given labels that are in the
3069             dataset are dropped and no error is raised.
3070         **labels_kwargs : {dim: label, ...}, optional
3071             The keyword arguments form of ``dim`` and ``labels``
3072 
3073         Returns
3074         -------
3075         dropped : DataArray
3076 
3077         Examples
3078         --------
3079         >>> da = xr.DataArray(
3080         ...     np.arange(25).reshape(5, 5),
3081         ...     coords={"x": np.arange(0, 9, 2), "y": np.arange(0, 13, 3)},
3082         ...     dims=("x", "y"),
3083         ... )
3084         >>> da
3085         <xarray.DataArray (x: 5, y: 5)>
3086         array([[ 0,  1,  2,  3,  4],
3087                [ 5,  6,  7,  8,  9],
3088                [10, 11, 12, 13, 14],
3089                [15, 16, 17, 18, 19],
3090                [20, 21, 22, 23, 24]])
3091         Coordinates:
3092           * x        (x) int64 0 2 4 6 8
3093           * y        (y) int64 0 3 6 9 12
3094 
3095         >>> da.drop_sel(x=[0, 2], y=9)
3096         <xarray.DataArray (x: 3, y: 4)>
3097         array([[10, 11, 12, 14],
3098                [15, 16, 17, 19],
3099                [20, 21, 22, 24]])
3100         Coordinates:
3101           * x        (x) int64 4 6 8
3102           * y        (y) int64 0 3 6 12
3103 
3104         >>> da.drop_sel({"x": 6, "y": [0, 3]})
3105         <xarray.DataArray (x: 4, y: 3)>
3106         array([[ 2,  3,  4],
3107                [ 7,  8,  9],
3108                [12, 13, 14],
3109                [22, 23, 24]])
3110         Coordinates:
3111           * x        (x) int64 0 2 4 8
3112           * y        (y) int64 6 9 12
3113         """
3114         if labels_kwargs or isinstance(labels, dict):
3115             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
3116 
3117         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
3118         return self._from_temp_dataset(ds)
3119 
3120     def drop_isel(
3121         self: T_DataArray, indexers: Mapping[Any, Any] | None = None, **indexers_kwargs
3122     ) -> T_DataArray:
3123         """Drop index positions from this DataArray.
3124 
3125         Parameters
3126         ----------
3127         indexers : mapping of Hashable to Any or None, default: None
3128             Index locations to drop
3129         **indexers_kwargs : {dim: position, ...}, optional
3130             The keyword arguments form of ``dim`` and ``positions``
3131 
3132         Returns
3133         -------
3134         dropped : DataArray
3135 
3136         Raises
3137         ------
3138         IndexError
3139 
3140         Examples
3141         --------
3142         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("X", "Y"))
3143         >>> da
3144         <xarray.DataArray (X: 5, Y: 5)>
3145         array([[ 0,  1,  2,  3,  4],
3146                [ 5,  6,  7,  8,  9],
3147                [10, 11, 12, 13, 14],
3148                [15, 16, 17, 18, 19],
3149                [20, 21, 22, 23, 24]])
3150         Dimensions without coordinates: X, Y
3151 
3152         >>> da.drop_isel(X=[0, 4], Y=2)
3153         <xarray.DataArray (X: 3, Y: 4)>
3154         array([[ 5,  6,  8,  9],
3155                [10, 11, 13, 14],
3156                [15, 16, 18, 19]])
3157         Dimensions without coordinates: X, Y
3158 
3159         >>> da.drop_isel({"X": 3, "Y": 3})
3160         <xarray.DataArray (X: 4, Y: 4)>
3161         array([[ 0,  1,  2,  4],
3162                [ 5,  6,  7,  9],
3163                [10, 11, 12, 14],
3164                [20, 21, 22, 24]])
3165         Dimensions without coordinates: X, Y
3166         """
3167         dataset = self._to_temp_dataset()
3168         dataset = dataset.drop_isel(indexers=indexers, **indexers_kwargs)
3169         return self._from_temp_dataset(dataset)
3170 
3171     def dropna(
3172         self: T_DataArray,
3173         dim: Hashable,
3174         how: Literal["any", "all"] = "any",
3175         thresh: int | None = None,
3176     ) -> T_DataArray:
3177         """Returns a new array with dropped labels for missing values along
3178         the provided dimension.
3179 
3180         Parameters
3181         ----------
3182         dim : Hashable
3183             Dimension along which to drop missing values. Dropping along
3184             multiple dimensions simultaneously is not yet supported.
3185         how : {"any", "all"}, default: "any"
3186             - any : if any NA values are present, drop that label
3187             - all : if all values are NA, drop that label
3188 
3189         thresh : int or None, default: None
3190             If supplied, require this many non-NA values.
3191 
3192         Returns
3193         -------
3194         dropped : DataArray
3195 
3196         Examples
3197         --------
3198         >>> temperature = [
3199         ...     [0, 4, 2, 9],
3200         ...     [np.nan, np.nan, np.nan, np.nan],
3201         ...     [np.nan, 4, 2, 0],
3202         ...     [3, 1, 0, 0],
3203         ... ]
3204         >>> da = xr.DataArray(
3205         ...     data=temperature,
3206         ...     dims=["Y", "X"],
3207         ...     coords=dict(
3208         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75])),
3209         ...         lon=("X", np.array([10.0, 10.25, 10.5, 10.75])),
3210         ...     ),
3211         ... )
3212         >>> da
3213         <xarray.DataArray (Y: 4, X: 4)>
3214         array([[ 0.,  4.,  2.,  9.],
3215                [nan, nan, nan, nan],
3216                [nan,  4.,  2.,  0.],
3217                [ 3.,  1.,  0.,  0.]])
3218         Coordinates:
3219             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75
3220             lon      (X) float64 10.0 10.25 10.5 10.75
3221         Dimensions without coordinates: Y, X
3222 
3223         >>> da.dropna(dim="Y", how="any")
3224         <xarray.DataArray (Y: 2, X: 4)>
3225         array([[0., 4., 2., 9.],
3226                [3., 1., 0., 0.]])
3227         Coordinates:
3228             lat      (Y) float64 -20.0 -20.75
3229             lon      (X) float64 10.0 10.25 10.5 10.75
3230         Dimensions without coordinates: Y, X
3231 
3232         Drop values only if all values along the dimension are NaN:
3233 
3234         >>> da.dropna(dim="Y", how="all")
3235         <xarray.DataArray (Y: 3, X: 4)>
3236         array([[ 0.,  4.,  2.,  9.],
3237                [nan,  4.,  2.,  0.],
3238                [ 3.,  1.,  0.,  0.]])
3239         Coordinates:
3240             lat      (Y) float64 -20.0 -20.5 -20.75
3241             lon      (X) float64 10.0 10.25 10.5 10.75
3242         Dimensions without coordinates: Y, X
3243         """
3244         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
3245         return self._from_temp_dataset(ds)
3246 
3247     def fillna(self: T_DataArray, value: Any) -> T_DataArray:
3248         """Fill missing values in this object.
3249 
3250         This operation follows the normal broadcasting and alignment rules that
3251         xarray uses for binary arithmetic, except the result is aligned to this
3252         object (``join='left'``) instead of aligned to the intersection of
3253         index coordinates (``join='inner'``).
3254 
3255         Parameters
3256         ----------
3257         value : scalar, ndarray or DataArray
3258             Used to fill all matching missing values in this array. If the
3259             argument is a DataArray, it is first aligned with (reindexed to)
3260             this array.
3261 
3262         Returns
3263         -------
3264         filled : DataArray
3265 
3266         Examples
3267         --------
3268         >>> da = xr.DataArray(
3269         ...     np.array([1, 4, np.nan, 0, 3, np.nan]),
3270         ...     dims="Z",
3271         ...     coords=dict(
3272         ...         Z=("Z", np.arange(6)),
3273         ...         height=("Z", np.array([0, 10, 20, 30, 40, 50])),
3274         ...     ),
3275         ... )
3276         >>> da
3277         <xarray.DataArray (Z: 6)>
3278         array([ 1.,  4., nan,  0.,  3., nan])
3279         Coordinates:
3280           * Z        (Z) int64 0 1 2 3 4 5
3281             height   (Z) int64 0 10 20 30 40 50
3282 
3283         Fill all NaN values with 0:
3284 
3285         >>> da.fillna(0)
3286         <xarray.DataArray (Z: 6)>
3287         array([1., 4., 0., 0., 3., 0.])
3288         Coordinates:
3289           * Z        (Z) int64 0 1 2 3 4 5
3290             height   (Z) int64 0 10 20 30 40 50
3291 
3292         Fill NaN values with corresponding values in array:
3293 
3294         >>> da.fillna(np.array([2, 9, 4, 2, 8, 9]))
3295         <xarray.DataArray (Z: 6)>
3296         array([1., 4., 4., 0., 3., 9.])
3297         Coordinates:
3298           * Z        (Z) int64 0 1 2 3 4 5
3299             height   (Z) int64 0 10 20 30 40 50
3300         """
3301         if utils.is_dict_like(value):
3302             raise TypeError(
3303                 "cannot provide fill value as a dictionary with "
3304                 "fillna on a DataArray"
3305             )
3306         out = ops.fillna(self, value)
3307         return out
3308 
3309     def interpolate_na(
3310         self: T_DataArray,
3311         dim: Hashable | None = None,
3312         method: InterpOptions = "linear",
3313         limit: int | None = None,
3314         use_coordinate: bool | str = True,
3315         max_gap: (
3316             None
3317             | int
3318             | float
3319             | str
3320             | pd.Timedelta
3321             | np.timedelta64
3322             | datetime.timedelta
3323         ) = None,
3324         keep_attrs: bool | None = None,
3325         **kwargs: Any,
3326     ) -> T_DataArray:
3327         """Fill in NaNs by interpolating according to different methods.
3328 
3329         Parameters
3330         ----------
3331         dim : Hashable or None, optional
3332             Specifies the dimension along which to interpolate.
3333         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3334             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3335             String indicating which method to use for interpolation:
3336 
3337             - 'linear': linear interpolation. Additional keyword
3338               arguments are passed to :py:func:`numpy.interp`
3339             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3340               are passed to :py:func:`scipy.interpolate.interp1d`. If
3341               ``method='polynomial'``, the ``order`` keyword argument must also be
3342               provided.
3343             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3344               respective :py:class:`scipy.interpolate` classes.
3345 
3346         use_coordinate : bool or str, default: True
3347             Specifies which index to use as the x values in the interpolation
3348             formulated as `y = f(x)`. If False, values are treated as if
3349             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
3350             used. If ``use_coordinate`` is a string, it specifies the name of a
3351             coordinate variariable to use as the index.
3352         limit : int or None, default: None
3353             Maximum number of consecutive NaNs to fill. Must be greater than 0
3354             or None for no limit. This filling is done regardless of the size of
3355             the gap in the data. To only interpolate over gaps less than a given length,
3356             see ``max_gap``.
3357         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
3358             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
3359             Use None for no limit. When interpolating along a datetime64 dimension
3360             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
3361 
3362             - a string that is valid input for pandas.to_timedelta
3363             - a :py:class:`numpy.timedelta64` object
3364             - a :py:class:`pandas.Timedelta` object
3365             - a :py:class:`datetime.timedelta` object
3366 
3367             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
3368             dimensions has not been implemented yet. Gap length is defined as the difference
3369             between coordinate values at the first data point after a gap and the last value
3370             before a gap. For gaps at the beginning (end), gap length is defined as the difference
3371             between coordinate values at the first (last) valid data point and the first (last) NaN.
3372             For example, consider::
3373 
3374                 <xarray.DataArray (x: 9)>
3375                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
3376                 Coordinates:
3377                   * x        (x) int64 0 1 2 3 4 5 6 7 8
3378 
3379             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
3380         keep_attrs : bool or None, default: None
3381             If True, the dataarray's attributes (`attrs`) will be copied from
3382             the original object to the new one.  If False, the new
3383             object will be returned without attributes.
3384         **kwargs : dict, optional
3385             parameters passed verbatim to the underlying interpolation function
3386 
3387         Returns
3388         -------
3389         interpolated: DataArray
3390             Filled in DataArray.
3391 
3392         See Also
3393         --------
3394         numpy.interp
3395         scipy.interpolate
3396 
3397         Examples
3398         --------
3399         >>> da = xr.DataArray(
3400         ...     [np.nan, 2, 3, np.nan, 0], dims="x", coords={"x": [0, 1, 2, 3, 4]}
3401         ... )
3402         >>> da
3403         <xarray.DataArray (x: 5)>
3404         array([nan,  2.,  3., nan,  0.])
3405         Coordinates:
3406           * x        (x) int64 0 1 2 3 4
3407 
3408         >>> da.interpolate_na(dim="x", method="linear")
3409         <xarray.DataArray (x: 5)>
3410         array([nan, 2. , 3. , 1.5, 0. ])
3411         Coordinates:
3412           * x        (x) int64 0 1 2 3 4
3413 
3414         >>> da.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
3415         <xarray.DataArray (x: 5)>
3416         array([1. , 2. , 3. , 1.5, 0. ])
3417         Coordinates:
3418           * x        (x) int64 0 1 2 3 4
3419         """
3420         from xarray.core.missing import interp_na
3421 
3422         return interp_na(
3423             self,
3424             dim=dim,
3425             method=method,
3426             limit=limit,
3427             use_coordinate=use_coordinate,
3428             max_gap=max_gap,
3429             keep_attrs=keep_attrs,
3430             **kwargs,
3431         )
3432 
3433     def ffill(
3434         self: T_DataArray, dim: Hashable, limit: int | None = None
3435     ) -> T_DataArray:
3436         """Fill NaN values by propagating values forward
3437 
3438         *Requires bottleneck.*
3439 
3440         Parameters
3441         ----------
3442         dim : Hashable
3443             Specifies the dimension along which to propagate values when
3444             filling.
3445         limit : int or None, default: None
3446             The maximum number of consecutive NaN values to forward fill. In
3447             other words, if there is a gap with more than this number of
3448             consecutive NaNs, it will only be partially filled. Must be greater
3449             than 0 or None for no limit. Must be None or greater than or equal
3450             to axis length if filling along chunked axes (dimensions).
3451 
3452         Returns
3453         -------
3454         filled : DataArray
3455 
3456         Examples
3457         --------
3458         >>> temperature = np.array(
3459         ...     [
3460         ...         [np.nan, 1, 3],
3461         ...         [0, np.nan, 5],
3462         ...         [5, np.nan, np.nan],
3463         ...         [3, np.nan, np.nan],
3464         ...         [0, 2, 0],
3465         ...     ]
3466         ... )
3467         >>> da = xr.DataArray(
3468         ...     data=temperature,
3469         ...     dims=["Y", "X"],
3470         ...     coords=dict(
3471         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75, -21.0])),
3472         ...         lon=("X", np.array([10.0, 10.25, 10.5])),
3473         ...     ),
3474         ... )
3475         >>> da
3476         <xarray.DataArray (Y: 5, X: 3)>
3477         array([[nan,  1.,  3.],
3478                [ 0., nan,  5.],
3479                [ 5., nan, nan],
3480                [ 3., nan, nan],
3481                [ 0.,  2.,  0.]])
3482         Coordinates:
3483             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3484             lon      (X) float64 10.0 10.25 10.5
3485         Dimensions without coordinates: Y, X
3486 
3487         Fill all NaN values:
3488 
3489         >>> da.ffill(dim="Y", limit=None)
3490         <xarray.DataArray (Y: 5, X: 3)>
3491         array([[nan,  1.,  3.],
3492                [ 0.,  1.,  5.],
3493                [ 5.,  1.,  5.],
3494                [ 3.,  1.,  5.],
3495                [ 0.,  2.,  0.]])
3496         Coordinates:
3497             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3498             lon      (X) float64 10.0 10.25 10.5
3499         Dimensions without coordinates: Y, X
3500 
3501         Fill only the first of consecutive NaN values:
3502 
3503         >>> da.ffill(dim="Y", limit=1)
3504         <xarray.DataArray (Y: 5, X: 3)>
3505         array([[nan,  1.,  3.],
3506                [ 0.,  1.,  5.],
3507                [ 5., nan,  5.],
3508                [ 3., nan, nan],
3509                [ 0.,  2.,  0.]])
3510         Coordinates:
3511             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3512             lon      (X) float64 10.0 10.25 10.5
3513         Dimensions without coordinates: Y, X
3514         """
3515         from xarray.core.missing import ffill
3516 
3517         return ffill(self, dim, limit=limit)
3518 
3519     def bfill(
3520         self: T_DataArray, dim: Hashable, limit: int | None = None
3521     ) -> T_DataArray:
3522         """Fill NaN values by propagating values backward
3523 
3524         *Requires bottleneck.*
3525 
3526         Parameters
3527         ----------
3528         dim : str
3529             Specifies the dimension along which to propagate values when
3530             filling.
3531         limit : int or None, default: None
3532             The maximum number of consecutive NaN values to backward fill. In
3533             other words, if there is a gap with more than this number of
3534             consecutive NaNs, it will only be partially filled. Must be greater
3535             than 0 or None for no limit. Must be None or greater than or equal
3536             to axis length if filling along chunked axes (dimensions).
3537 
3538         Returns
3539         -------
3540         filled : DataArray
3541 
3542         Examples
3543         --------
3544         >>> temperature = np.array(
3545         ...     [
3546         ...         [0, 1, 3],
3547         ...         [0, np.nan, 5],
3548         ...         [5, np.nan, np.nan],
3549         ...         [3, np.nan, np.nan],
3550         ...         [np.nan, 2, 0],
3551         ...     ]
3552         ... )
3553         >>> da = xr.DataArray(
3554         ...     data=temperature,
3555         ...     dims=["Y", "X"],
3556         ...     coords=dict(
3557         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75, -21.0])),
3558         ...         lon=("X", np.array([10.0, 10.25, 10.5])),
3559         ...     ),
3560         ... )
3561         >>> da
3562         <xarray.DataArray (Y: 5, X: 3)>
3563         array([[ 0.,  1.,  3.],
3564                [ 0., nan,  5.],
3565                [ 5., nan, nan],
3566                [ 3., nan, nan],
3567                [nan,  2.,  0.]])
3568         Coordinates:
3569             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3570             lon      (X) float64 10.0 10.25 10.5
3571         Dimensions without coordinates: Y, X
3572 
3573         Fill all NaN values:
3574 
3575         >>> da.bfill(dim="Y", limit=None)
3576         <xarray.DataArray (Y: 5, X: 3)>
3577         array([[ 0.,  1.,  3.],
3578                [ 0.,  2.,  5.],
3579                [ 5.,  2.,  0.],
3580                [ 3.,  2.,  0.],
3581                [nan,  2.,  0.]])
3582         Coordinates:
3583             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3584             lon      (X) float64 10.0 10.25 10.5
3585         Dimensions without coordinates: Y, X
3586 
3587         Fill only the first of consecutive NaN values:
3588 
3589         >>> da.bfill(dim="Y", limit=1)
3590         <xarray.DataArray (Y: 5, X: 3)>
3591         array([[ 0.,  1.,  3.],
3592                [ 0., nan,  5.],
3593                [ 5., nan, nan],
3594                [ 3.,  2.,  0.],
3595                [nan,  2.,  0.]])
3596         Coordinates:
3597             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3598             lon      (X) float64 10.0 10.25 10.5
3599         Dimensions without coordinates: Y, X
3600         """
3601         from xarray.core.missing import bfill
3602 
3603         return bfill(self, dim, limit=limit)
3604 
3605     def combine_first(self: T_DataArray, other: T_DataArray) -> T_DataArray:
3606         """Combine two DataArray objects, with union of coordinates.
3607 
3608         This operation follows the normal broadcasting and alignment rules of
3609         ``join='outer'``.  Default to non-null values of array calling the
3610         method.  Use np.nan to fill in vacant cells after alignment.
3611 
3612         Parameters
3613         ----------
3614         other : DataArray
3615             Used to fill all matching missing values in this array.
3616 
3617         Returns
3618         -------
3619         DataArray
3620         """
3621         return ops.fillna(self, other, join="outer")
3622 
3623     def reduce(
3624         self: T_DataArray,
3625         func: Callable[..., Any],
3626         dim: Dims = None,
3627         *,
3628         axis: int | Sequence[int] | None = None,
3629         keep_attrs: bool | None = None,
3630         keepdims: bool = False,
3631         **kwargs: Any,
3632     ) -> T_DataArray:
3633         """Reduce this array by applying `func` along some dimension(s).
3634 
3635         Parameters
3636         ----------
3637         func : callable
3638             Function which can be called in the form
3639             `f(x, axis=axis, **kwargs)` to return the result of reducing an
3640             np.ndarray over an integer valued axis.
3641         dim : "...", str, Iterable of Hashable or None, optional
3642             Dimension(s) over which to apply `func`. By default `func` is
3643             applied over all dimensions.
3644         axis : int or sequence of int, optional
3645             Axis(es) over which to repeatedly apply `func`. Only one of the
3646             'dim' and 'axis' arguments can be supplied. If neither are
3647             supplied, then the reduction is calculated over the flattened array
3648             (by calling `f(x)` without an axis argument).
3649         keep_attrs : bool or None, optional
3650             If True, the variable's attributes (`attrs`) will be copied from
3651             the original object to the new one.  If False (default), the new
3652             object will be returned without attributes.
3653         keepdims : bool, default: False
3654             If True, the dimensions which are reduced are left in the result
3655             as dimensions of size one. Coordinates that use these dimensions
3656             are removed.
3657         **kwargs : dict
3658             Additional keyword arguments passed on to `func`.
3659 
3660         Returns
3661         -------
3662         reduced : DataArray
3663             DataArray with this object's array replaced with an array with
3664             summarized data and the indicated dimension(s) removed.
3665         """
3666 
3667         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
3668         return self._replace_maybe_drop_dims(var)
3669 
3670     def to_pandas(self) -> DataArray | pd.Series | pd.DataFrame:
3671         """Convert this array into a pandas object with the same shape.
3672 
3673         The type of the returned object depends on the number of DataArray
3674         dimensions:
3675 
3676         * 0D -> `xarray.DataArray`
3677         * 1D -> `pandas.Series`
3678         * 2D -> `pandas.DataFrame`
3679 
3680         Only works for arrays with 2 or fewer dimensions.
3681 
3682         The DataArray constructor performs the inverse transformation.
3683 
3684         Returns
3685         -------
3686         result : DataArray | Series | DataFrame
3687             DataArray, pandas Series or pandas DataFrame.
3688         """
3689         # TODO: consolidate the info about pandas constructors and the
3690         # attributes that correspond to their indexes into a separate module?
3691         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
3692         try:
3693             constructor = constructors[self.ndim]
3694         except KeyError:
3695             raise ValueError(
3696                 f"Cannot convert arrays with {self.ndim} dimensions into "
3697                 "pandas objects. Requires 2 or fewer dimensions."
3698             )
3699         indexes = [self.get_index(dim) for dim in self.dims]
3700         return constructor(self.values, *indexes)
3701 
3702     def to_dataframe(
3703         self, name: Hashable | None = None, dim_order: Sequence[Hashable] | None = None
3704     ) -> pd.DataFrame:
3705         """Convert this array and its coordinates into a tidy pandas.DataFrame.
3706 
3707         The DataFrame is indexed by the Cartesian product of index coordinates
3708         (in the form of a :py:class:`pandas.MultiIndex`). Other coordinates are
3709         included as columns in the DataFrame.
3710 
3711         For 1D and 2D DataArrays, see also :py:func:`DataArray.to_pandas` which
3712         doesn't rely on a MultiIndex to build the DataFrame.
3713 
3714         Parameters
3715         ----------
3716         name: Hashable or None, optional
3717             Name to give to this array (required if unnamed).
3718         dim_order: Sequence of Hashable or None, optional
3719             Hierarchical dimension order for the resulting dataframe.
3720             Array content is transposed to this order and then written out as flat
3721             vectors in contiguous order, so the last dimension in this list
3722             will be contiguous in the resulting DataFrame. This has a major
3723             influence on which operations are efficient on the resulting
3724             dataframe.
3725 
3726             If provided, must include all dimensions of this DataArray. By default,
3727             dimensions are sorted according to the DataArray dimensions order.
3728 
3729         Returns
3730         -------
3731         result: DataFrame
3732             DataArray as a pandas DataFrame.
3733 
3734         See also
3735         --------
3736         DataArray.to_pandas
3737         DataArray.to_series
3738         """
3739         if name is None:
3740             name = self.name
3741         if name is None:
3742             raise ValueError(
3743                 "cannot convert an unnamed DataArray to a "
3744                 "DataFrame: use the ``name`` parameter"
3745             )
3746         if self.ndim == 0:
3747             raise ValueError("cannot convert a scalar to a DataFrame")
3748 
3749         # By using a unique name, we can convert a DataArray into a DataFrame
3750         # even if it shares a name with one of its coordinates.
3751         # I would normally use unique_name = object() but that results in a
3752         # dataframe with columns in the wrong order, for reasons I have not
3753         # been able to debug (possibly a pandas bug?).
3754         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
3755         ds = self._to_dataset_whole(name=unique_name)
3756 
3757         if dim_order is None:
3758             ordered_dims = dict(zip(self.dims, self.shape))
3759         else:
3760             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)
3761 
3762         df = ds._to_dataframe(ordered_dims)
3763         df.columns = [name if c == unique_name else c for c in df.columns]
3764         return df
3765 
3766     def to_series(self) -> pd.Series:
3767         """Convert this array into a pandas.Series.
3768 
3769         The Series is indexed by the Cartesian product of index coordinates
3770         (in the form of a :py:class:`pandas.MultiIndex`).
3771 
3772         Returns
3773         -------
3774         result : Series
3775             DataArray as a pandas Series.
3776 
3777         See also
3778         --------
3779         DataArray.to_pandas
3780         DataArray.to_dataframe
3781         """
3782         index = self.coords.to_index()
3783         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
3784 
3785     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
3786         """Convert this array into a numpy.ma.MaskedArray
3787 
3788         Parameters
3789         ----------
3790         copy : bool, default: True
3791             If True make a copy of the array in the result. If False,
3792             a MaskedArray view of DataArray.values is returned.
3793 
3794         Returns
3795         -------
3796         result : MaskedArray
3797             Masked where invalid values (nan or inf) occur.
3798         """
3799         values = self.to_numpy()  # only compute lazy arrays once
3800         isnull = pd.isnull(values)
3801         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
3802 
3803     # path=None writes to bytes
3804     @overload
3805     def to_netcdf(
3806         self,
3807         path: None = None,
3808         mode: Literal["w", "a"] = "w",
3809         format: T_NetcdfTypes | None = None,
3810         group: str | None = None,
3811         engine: T_NetcdfEngine | None = None,
3812         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3813         unlimited_dims: Iterable[Hashable] | None = None,
3814         compute: bool = True,
3815         invalid_netcdf: bool = False,
3816     ) -> bytes:
3817         ...
3818 
3819     # default return None
3820     @overload
3821     def to_netcdf(
3822         self,
3823         path: str | PathLike,
3824         mode: Literal["w", "a"] = "w",
3825         format: T_NetcdfTypes | None = None,
3826         group: str | None = None,
3827         engine: T_NetcdfEngine | None = None,
3828         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3829         unlimited_dims: Iterable[Hashable] | None = None,
3830         compute: Literal[True] = True,
3831         invalid_netcdf: bool = False,
3832     ) -> None:
3833         ...
3834 
3835     # compute=False returns dask.Delayed
3836     @overload
3837     def to_netcdf(
3838         self,
3839         path: str | PathLike,
3840         mode: Literal["w", "a"] = "w",
3841         format: T_NetcdfTypes | None = None,
3842         group: str | None = None,
3843         engine: T_NetcdfEngine | None = None,
3844         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3845         unlimited_dims: Iterable[Hashable] | None = None,
3846         *,
3847         compute: Literal[False],
3848         invalid_netcdf: bool = False,
3849     ) -> Delayed:
3850         ...
3851 
3852     def to_netcdf(
3853         self,
3854         path: str | PathLike | None = None,
3855         mode: Literal["w", "a"] = "w",
3856         format: T_NetcdfTypes | None = None,
3857         group: str | None = None,
3858         engine: T_NetcdfEngine | None = None,
3859         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3860         unlimited_dims: Iterable[Hashable] | None = None,
3861         compute: bool = True,
3862         invalid_netcdf: bool = False,
3863     ) -> bytes | Delayed | None:
3864         """Write dataset contents to a netCDF file.
3865 
3866         Parameters
3867         ----------
3868         path : str, path-like or None, optional
3869             Path to which to save this dataset. File-like objects are only
3870             supported by the scipy engine. If no path is provided, this
3871             function returns the resulting netCDF file as bytes; in this case,
3872             we need to use scipy, which does not support netCDF version 4 (the
3873             default format becomes NETCDF3_64BIT).
3874         mode : {"w", "a"}, default: "w"
3875             Write ('w') or append ('a') mode. If mode='w', any existing file at
3876             this location will be overwritten. If mode='a', existing variables
3877             will be overwritten.
3878         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
3879                   "NETCDF3_CLASSIC"}, optional
3880             File format for the resulting netCDF file:
3881 
3882             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
3883               features.
3884             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
3885               netCDF 3 compatible API features.
3886             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
3887               which fully supports 2+ GB files, but is only compatible with
3888               clients linked against netCDF version 3.6.0 or later.
3889             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
3890               handle 2+ GB files very well.
3891 
3892             All formats are supported by the netCDF4-python library.
3893             scipy.io.netcdf only supports the last two formats.
3894 
3895             The default format is NETCDF4 if you are saving a file to disk and
3896             have the netCDF4-python library available. Otherwise, xarray falls
3897             back to using scipy to write netCDF files and defaults to the
3898             NETCDF3_64BIT format (scipy does not support netCDF4).
3899         group : str, optional
3900             Path to the netCDF4 group in the given file to open (only works for
3901             format='NETCDF4'). The group(s) will be created if necessary.
3902         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
3903             Engine to use when writing netCDF files. If not provided, the
3904             default engine is chosen based on available dependencies, with a
3905             preference for 'netcdf4' if writing to a file on disk.
3906         encoding : dict, optional
3907             Nested dictionary with variable names as keys and dictionaries of
3908             variable specific encodings as values, e.g.,
3909             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
3910             "zlib": True}, ...}``
3911 
3912             The `h5netcdf` engine supports both the NetCDF4-style compression
3913             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
3914             ones ``{"compression": "gzip", "compression_opts": 9}``.
3915             This allows using any compression plugin installed in the HDF5
3916             library, e.g. LZF.
3917 
3918         unlimited_dims : iterable of Hashable, optional
3919             Dimension(s) that should be serialized as unlimited dimensions.
3920             By default, no dimensions are treated as unlimited dimensions.
3921             Note that unlimited_dims may also be set via
3922             ``dataset.encoding["unlimited_dims"]``.
3923         compute: bool, default: True
3924             If true compute immediately, otherwise return a
3925             ``dask.delayed.Delayed`` object that can be computed later.
3926         invalid_netcdf: bool, default: False
3927             Only valid along with ``engine="h5netcdf"``. If True, allow writing
3928             hdf5 files which are invalid netcdf as described in
3929             https://github.com/h5netcdf/h5netcdf.
3930 
3931         Returns
3932         -------
3933         store: bytes or Delayed or None
3934             * ``bytes`` if path is None
3935             * ``dask.delayed.Delayed`` if compute is False
3936             * None otherwise
3937 
3938         Notes
3939         -----
3940         Only xarray.Dataset objects can be written to netCDF files, so
3941         the xarray.DataArray is converted to a xarray.Dataset object
3942         containing a single variable. If the DataArray has no name, or if the
3943         name is the same as a coordinate name, then it is given the name
3944         ``"__xarray_dataarray_variable__"``.
3945 
3946         See Also
3947         --------
3948         Dataset.to_netcdf
3949         """
3950         from xarray.backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_netcdf
3951 
3952         if self.name is None:
3953             # If no name is set then use a generic xarray name
3954             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3955         elif self.name in self.coords or self.name in self.dims:
3956             # The name is the same as one of the coords names, which netCDF
3957             # doesn't support, so rename it but keep track of the old name
3958             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3959             dataset.attrs[DATAARRAY_NAME] = self.name
3960         else:
3961             # No problems with the name - so we're fine!
3962             dataset = self.to_dataset()
3963 
3964         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
3965             dataset,
3966             path,
3967             mode=mode,
3968             format=format,
3969             group=group,
3970             engine=engine,
3971             encoding=encoding,
3972             unlimited_dims=unlimited_dims,
3973             compute=compute,
3974             multifile=False,
3975             invalid_netcdf=invalid_netcdf,
3976         )
3977 
3978     def to_dict(self, data: bool = True, encoding: bool = False) -> dict[str, Any]:
3979         """
3980         Convert this xarray.DataArray into a dictionary following xarray
3981         naming conventions.
3982 
3983         Converts all variables and attributes to native Python objects.
3984         Useful for converting to json. To avoid datetime incompatibility
3985         use decode_times=False kwarg in xarray.open_dataset.
3986 
3987         Parameters
3988         ----------
3989         data : bool, default: True
3990             Whether to include the actual data in the dictionary. When set to
3991             False, returns just the schema.
3992         encoding : bool, default: False
3993             Whether to include the Dataset's encoding in the dictionary.
3994 
3995         Returns
3996         -------
3997         dict: dict
3998 
3999         See Also
4000         --------
4001         DataArray.from_dict
4002         Dataset.to_dict
4003         """
4004         d = self.variable.to_dict(data=data)
4005         d.update({"coords": {}, "name": self.name})
4006         for k, coord in self.coords.items():
4007             d["coords"][k] = coord.variable.to_dict(data=data)
4008         if encoding:
4009             d["encoding"] = dict(self.encoding)
4010         return d
4011 
4012     @classmethod
4013     def from_dict(cls: type[T_DataArray], d: Mapping[str, Any]) -> T_DataArray:
4014         """Convert a dictionary into an xarray.DataArray
4015 
4016         Parameters
4017         ----------
4018         d : dict
4019             Mapping with a minimum structure of {"dims": [...], "data": [...]}
4020 
4021         Returns
4022         -------
4023         obj : xarray.DataArray
4024 
4025         See Also
4026         --------
4027         DataArray.to_dict
4028         Dataset.from_dict
4029 
4030         Examples
4031         --------
4032         >>> d = {"dims": "t", "data": [1, 2, 3]}
4033         >>> da = xr.DataArray.from_dict(d)
4034         >>> da
4035         <xarray.DataArray (t: 3)>
4036         array([1, 2, 3])
4037         Dimensions without coordinates: t
4038 
4039         >>> d = {
4040         ...     "coords": {
4041         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
4042         ...     },
4043         ...     "attrs": {"title": "air temperature"},
4044         ...     "dims": "t",
4045         ...     "data": [10, 20, 30],
4046         ...     "name": "a",
4047         ... }
4048         >>> da = xr.DataArray.from_dict(d)
4049         >>> da
4050         <xarray.DataArray 'a' (t: 3)>
4051         array([10, 20, 30])
4052         Coordinates:
4053           * t        (t) int64 0 1 2
4054         Attributes:
4055             title:    air temperature
4056         """
4057         coords = None
4058         if "coords" in d:
4059             try:
4060                 coords = {
4061                     k: (v["dims"], v["data"], v.get("attrs"))
4062                     for k, v in d["coords"].items()
4063                 }
4064             except KeyError as e:
4065                 raise ValueError(
4066                     "cannot convert dict when coords are missing the key "
4067                     "'{dims_data}'".format(dims_data=str(e.args[0]))
4068                 )
4069         try:
4070             data = d["data"]
4071         except KeyError:
4072             raise ValueError("cannot convert dict without the key 'data''")
4073         else:
4074             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
4075 
4076         obj.encoding.update(d.get("encoding", {}))
4077 
4078         return obj
4079 
4080     @classmethod
4081     def from_series(cls, series: pd.Series, sparse: bool = False) -> DataArray:
4082         """Convert a pandas.Series into an xarray.DataArray.
4083 
4084         If the series's index is a MultiIndex, it will be expanded into a
4085         tensor product of one-dimensional coordinates (filling in missing
4086         values with NaN). Thus this operation should be the inverse of the
4087         `to_series` method.
4088 
4089         Parameters
4090         ----------
4091         series : Series
4092             Pandas Series object to convert.
4093         sparse : bool, default: False
4094             If sparse=True, creates a sparse array instead of a dense NumPy array.
4095             Requires the pydata/sparse package.
4096 
4097         See Also
4098         --------
4099         DataArray.to_series
4100         Dataset.from_dataframe
4101         """
4102         temp_name = "__temporary_name"
4103         df = pd.DataFrame({temp_name: series})
4104         ds = Dataset.from_dataframe(df, sparse=sparse)
4105         result = cast(DataArray, ds[temp_name])
4106         result.name = series.name
4107         return result
4108 
4109     def to_cdms2(self) -> cdms2_Variable:
4110         """Convert this array into a cdms2.Variable"""
4111         from xarray.convert import to_cdms2
4112 
4113         return to_cdms2(self)
4114 
4115     @classmethod
4116     def from_cdms2(cls, variable: cdms2_Variable) -> DataArray:
4117         """Convert a cdms2.Variable into an xarray.DataArray"""
4118         from xarray.convert import from_cdms2
4119 
4120         return from_cdms2(variable)
4121 
4122     def to_iris(self) -> iris_Cube:
4123         """Convert this array into a iris.cube.Cube"""
4124         from xarray.convert import to_iris
4125 
4126         return to_iris(self)
4127 
4128     @classmethod
4129     def from_iris(cls, cube: iris_Cube) -> DataArray:
4130         """Convert a iris.cube.Cube into an xarray.DataArray"""
4131         from xarray.convert import from_iris
4132 
4133         return from_iris(cube)
4134 
4135     def _all_compat(self: T_DataArray, other: T_DataArray, compat_str: str) -> bool:
4136         """Helper function for equals, broadcast_equals, and identical"""
4137 
4138         def compat(x, y):
4139             return getattr(x.variable, compat_str)(y.variable)
4140 
4141         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
4142             self, other
4143         )
4144 
4145     def broadcast_equals(self: T_DataArray, other: T_DataArray) -> bool:
4146         """Two DataArrays are broadcast equal if they are equal after
4147         broadcasting them against each other such that they have the same
4148         dimensions.
4149 
4150         Parameters
4151         ----------
4152         other : DataArray
4153             DataArray to compare to.
4154 
4155         Returns
4156         ----------
4157         equal : bool
4158             True if the two DataArrays are broadcast equal.
4159 
4160         See Also
4161         --------
4162         DataArray.equals
4163         DataArray.identical
4164 
4165         Examples
4166         --------
4167         >>> a = xr.DataArray([1, 2], dims="X")
4168         >>> b = xr.DataArray([[1, 1], [2, 2]], dims=["X", "Y"])
4169         >>> a
4170         <xarray.DataArray (X: 2)>
4171         array([1, 2])
4172         Dimensions without coordinates: X
4173         >>> b
4174         <xarray.DataArray (X: 2, Y: 2)>
4175         array([[1, 1],
4176                [2, 2]])
4177         Dimensions without coordinates: X, Y
4178 
4179         .equals returns True if two DataArrays have the same values, dimensions, and coordinates. .broadcast_equals returns True if the results of broadcasting two DataArrays against eachother have the same values, dimensions, and coordinates.
4180 
4181         >>> a.equals(b)
4182         False
4183         >>> a2, b2 = xr.broadcast(a, b)
4184         >>> a2.equals(b2)
4185         True
4186         >>> a.broadcast_equals(b)
4187         True
4188         """
4189         try:
4190             return self._all_compat(other, "broadcast_equals")
4191         except (TypeError, AttributeError):
4192             return False
4193 
4194     def equals(self: T_DataArray, other: T_DataArray) -> bool:
4195         """True if two DataArrays have the same dimensions, coordinates and
4196         values; otherwise False.
4197 
4198         DataArrays can still be equal (like pandas objects) if they have NaN
4199         values in the same locations.
4200 
4201         This method is necessary because `v1 == v2` for ``DataArray``
4202         does element-wise comparisons (like numpy.ndarrays).
4203 
4204         Parameters
4205         ----------
4206         other : DataArray
4207             DataArray to compare to.
4208 
4209         Returns
4210         ----------
4211         equal : bool
4212             True if the two DataArrays are equal.
4213 
4214         See Also
4215         --------
4216         DataArray.broadcast_equals
4217         DataArray.identical
4218 
4219         Examples
4220         --------
4221         >>> a = xr.DataArray([1, 2, 3], dims="X")
4222         >>> b = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"))
4223         >>> c = xr.DataArray([1, 2, 3], dims="Y")
4224         >>> d = xr.DataArray([3, 2, 1], dims="X")
4225         >>> a
4226         <xarray.DataArray (X: 3)>
4227         array([1, 2, 3])
4228         Dimensions without coordinates: X
4229         >>> b
4230         <xarray.DataArray (X: 3)>
4231         array([1, 2, 3])
4232         Dimensions without coordinates: X
4233         Attributes:
4234             units:    m
4235         >>> c
4236         <xarray.DataArray (Y: 3)>
4237         array([1, 2, 3])
4238         Dimensions without coordinates: Y
4239         >>> d
4240         <xarray.DataArray (X: 3)>
4241         array([3, 2, 1])
4242         Dimensions without coordinates: X
4243 
4244         >>> a.equals(b)
4245         True
4246         >>> a.equals(c)
4247         False
4248         >>> a.equals(d)
4249         False
4250         """
4251         try:
4252             return self._all_compat(other, "equals")
4253         except (TypeError, AttributeError):
4254             return False
4255 
4256     def identical(self: T_DataArray, other: T_DataArray) -> bool:
4257         """Like equals, but also checks the array name and attributes, and
4258         attributes on all coordinates.
4259 
4260         Parameters
4261         ----------
4262         other : DataArray
4263             DataArray to compare to.
4264 
4265         Returns
4266         ----------
4267         equal : bool
4268             True if the two DataArrays are identical.
4269 
4270         See Also
4271         --------
4272         DataArray.broadcast_equals
4273         DataArray.equals
4274 
4275         Examples
4276         --------
4277         >>> a = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"), name="Width")
4278         >>> b = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"), name="Width")
4279         >>> c = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="ft"), name="Width")
4280         >>> a
4281         <xarray.DataArray 'Width' (X: 3)>
4282         array([1, 2, 3])
4283         Dimensions without coordinates: X
4284         Attributes:
4285             units:    m
4286         >>> b
4287         <xarray.DataArray 'Width' (X: 3)>
4288         array([1, 2, 3])
4289         Dimensions without coordinates: X
4290         Attributes:
4291             units:    m
4292         >>> c
4293         <xarray.DataArray 'Width' (X: 3)>
4294         array([1, 2, 3])
4295         Dimensions without coordinates: X
4296         Attributes:
4297             units:    ft
4298 
4299         >>> a.equals(b)
4300         True
4301         >>> a.identical(b)
4302         True
4303 
4304         >>> a.equals(c)
4305         True
4306         >>> a.identical(c)
4307         False
4308         """
4309         try:
4310             return self.name == other.name and self._all_compat(other, "identical")
4311         except (TypeError, AttributeError):
4312             return False
4313 
4314     def _result_name(self, other: Any = None) -> Hashable | None:
4315         # use the same naming heuristics as pandas:
4316         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
4317         other_name = getattr(other, "name", _default)
4318         if other_name is _default or other_name == self.name:
4319             return self.name
4320         else:
4321             return None
4322 
4323     def __array_wrap__(self: T_DataArray, obj, context=None) -> T_DataArray:
4324         new_var = self.variable.__array_wrap__(obj, context)
4325         return self._replace(new_var)
4326 
4327     def __matmul__(self: T_DataArray, obj: T_DataArray) -> T_DataArray:
4328         return self.dot(obj)
4329 
4330     def __rmatmul__(self: T_DataArray, other: T_DataArray) -> T_DataArray:
4331         # currently somewhat duplicative, as only other DataArrays are
4332         # compatible with matmul
4333         return computation.dot(other, self)
4334 
4335     def _unary_op(self: T_DataArray, f: Callable, *args, **kwargs) -> T_DataArray:
4336         keep_attrs = kwargs.pop("keep_attrs", None)
4337         if keep_attrs is None:
4338             keep_attrs = _get_keep_attrs(default=True)
4339         with warnings.catch_warnings():
4340             warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
4341             warnings.filterwarnings(
4342                 "ignore", r"Mean of empty slice", category=RuntimeWarning
4343             )
4344             with np.errstate(all="ignore"):
4345                 da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))
4346             if keep_attrs:
4347                 da.attrs = self.attrs
4348             return da
4349 
4350     def _binary_op(
4351         self: T_DataArray,
4352         other: Any,
4353         f: Callable,
4354         reflexive: bool = False,
4355     ) -> T_DataArray:
4356         from xarray.core.groupby import GroupBy
4357 
4358         if isinstance(other, (Dataset, GroupBy)):
4359             return NotImplemented
4360         if isinstance(other, DataArray):
4361             align_type = OPTIONS["arithmetic_join"]
4362             self, other = align(self, other, join=align_type, copy=False)  # type: ignore
4363         other_variable = getattr(other, "variable", other)
4364         other_coords = getattr(other, "coords", None)
4365 
4366         variable = (
4367             f(self.variable, other_variable)
4368             if not reflexive
4369             else f(other_variable, self.variable)
4370         )
4371         coords, indexes = self.coords._merge_raw(other_coords, reflexive)
4372         name = self._result_name(other)
4373 
4374         return self._replace(variable, coords, name, indexes=indexes)
4375 
4376     def _inplace_binary_op(self: T_DataArray, other: Any, f: Callable) -> T_DataArray:
4377         from xarray.core.groupby import GroupBy
4378 
4379         if isinstance(other, GroupBy):
4380             raise TypeError(
4381                 "in-place operations between a DataArray and "
4382                 "a grouped object are not permitted"
4383             )
4384         # n.b. we can't align other to self (with other.reindex_like(self))
4385         # because `other` may be converted into floats, which would cause
4386         # in-place arithmetic to fail unpredictably. Instead, we simply
4387         # don't support automatic alignment with in-place arithmetic.
4388         other_coords = getattr(other, "coords", None)
4389         other_variable = getattr(other, "variable", other)
4390         try:
4391             with self.coords._merge_inplace(other_coords):
4392                 f(self.variable, other_variable)
4393         except MergeError as exc:
4394             raise MergeError(
4395                 "Automatic alignment is not supported for in-place operations.\n"
4396                 "Consider aligning the indices manually or using a not-in-place operation.\n"
4397                 "See https://github.com/pydata/xarray/issues/3910 for more explanations."
4398             ) from exc
4399         return self
4400 
4401     def _copy_attrs_from(self, other: DataArray | Dataset | Variable) -> None:
4402         self.attrs = other.attrs
4403 
4404     plot = utils.UncachedAccessor(DataArrayPlotAccessor)
4405 
4406     def _title_for_slice(self, truncate: int = 50) -> str:
4407         """
4408         If the dataarray has 1 dimensional coordinates or comes from a slice
4409         we can show that info in the title
4410 
4411         Parameters
4412         ----------
4413         truncate : int, default: 50
4414             maximum number of characters for title
4415 
4416         Returns
4417         -------
4418         title : string
4419             Can be used for plot titles
4420 
4421         """
4422         one_dims = []
4423         for dim, coord in self.coords.items():
4424             if coord.size == 1:
4425                 one_dims.append(
4426                     "{dim} = {v}{unit}".format(
4427                         dim=dim,
4428                         v=format_item(coord.values),
4429                         unit=_get_units_from_attrs(coord),
4430                     )
4431                 )
4432 
4433         title = ", ".join(one_dims)
4434         if len(title) > truncate:
4435             title = title[: (truncate - 3)] + "..."
4436 
4437         return title
4438 
4439     def diff(
4440         self: T_DataArray,
4441         dim: Hashable,
4442         n: int = 1,
4443         label: Literal["upper", "lower"] = "upper",
4444     ) -> T_DataArray:
4445         """Calculate the n-th order discrete difference along given axis.
4446 
4447         Parameters
4448         ----------
4449         dim : Hashable
4450             Dimension over which to calculate the finite difference.
4451         n : int, default: 1
4452             The number of times values are differenced.
4453         label : {"upper", "lower"}, default: "upper"
4454             The new coordinate in dimension ``dim`` will have the
4455             values of either the minuend's or subtrahend's coordinate
4456             for values 'upper' and 'lower', respectively.
4457 
4458         Returns
4459         -------
4460         difference : DataArray
4461             The n-th order finite difference of this object.
4462 
4463         Notes
4464         -----
4465         `n` matches numpy's behavior and is different from pandas' first argument named
4466         `periods`.
4467 
4468         Examples
4469         --------
4470         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
4471         >>> arr.diff("x")
4472         <xarray.DataArray (x: 3)>
4473         array([0, 1, 0])
4474         Coordinates:
4475           * x        (x) int64 2 3 4
4476         >>> arr.diff("x", 2)
4477         <xarray.DataArray (x: 2)>
4478         array([ 1, -1])
4479         Coordinates:
4480           * x        (x) int64 3 4
4481 
4482         See Also
4483         --------
4484         DataArray.differentiate
4485         """
4486         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
4487         return self._from_temp_dataset(ds)
4488 
4489     def shift(
4490         self: T_DataArray,
4491         shifts: Mapping[Any, int] | None = None,
4492         fill_value: Any = dtypes.NA,
4493         **shifts_kwargs: int,
4494     ) -> T_DataArray:
4495         """Shift this DataArray by an offset along one or more dimensions.
4496 
4497         Only the data is moved; coordinates stay in place. This is consistent
4498         with the behavior of ``shift`` in pandas.
4499 
4500         Values shifted from beyond array bounds will appear at one end of
4501         each dimension, which are filled according to `fill_value`. For periodic
4502         offsets instead see `roll`.
4503 
4504         Parameters
4505         ----------
4506         shifts : mapping of Hashable to int or None, optional
4507             Integer offset to shift along each of the given dimensions.
4508             Positive offsets shift to the right; negative offsets shift to the
4509             left.
4510         fill_value : scalar, optional
4511             Value to use for newly missing values
4512         **shifts_kwargs
4513             The keyword arguments form of ``shifts``.
4514             One of shifts or shifts_kwargs must be provided.
4515 
4516         Returns
4517         -------
4518         shifted : DataArray
4519             DataArray with the same coordinates and attributes but shifted
4520             data.
4521 
4522         See Also
4523         --------
4524         roll
4525 
4526         Examples
4527         --------
4528         >>> arr = xr.DataArray([5, 6, 7], dims="x")
4529         >>> arr.shift(x=1)
4530         <xarray.DataArray (x: 3)>
4531         array([nan,  5.,  6.])
4532         Dimensions without coordinates: x
4533         """
4534         variable = self.variable.shift(
4535             shifts=shifts, fill_value=fill_value, **shifts_kwargs
4536         )
4537         return self._replace(variable=variable)
4538 
4539     def roll(
4540         self: T_DataArray,
4541         shifts: Mapping[Hashable, int] | None = None,
4542         roll_coords: bool = False,
4543         **shifts_kwargs: int,
4544     ) -> T_DataArray:
4545         """Roll this array by an offset along one or more dimensions.
4546 
4547         Unlike shift, roll treats the given dimensions as periodic, so will not
4548         create any missing values to be filled.
4549 
4550         Unlike shift, roll may rotate all variables, including coordinates
4551         if specified. The direction of rotation is consistent with
4552         :py:func:`numpy.roll`.
4553 
4554         Parameters
4555         ----------
4556         shifts : mapping of Hashable to int, optional
4557             Integer offset to rotate each of the given dimensions.
4558             Positive offsets roll to the right; negative offsets roll to the
4559             left.
4560         roll_coords : bool, default: False
4561             Indicates whether to roll the coordinates by the offset too.
4562         **shifts_kwargs : {dim: offset, ...}, optional
4563             The keyword arguments form of ``shifts``.
4564             One of shifts or shifts_kwargs must be provided.
4565 
4566         Returns
4567         -------
4568         rolled : DataArray
4569             DataArray with the same attributes but rolled data and coordinates.
4570 
4571         See Also
4572         --------
4573         shift
4574 
4575         Examples
4576         --------
4577         >>> arr = xr.DataArray([5, 6, 7], dims="x")
4578         >>> arr.roll(x=1)
4579         <xarray.DataArray (x: 3)>
4580         array([7, 5, 6])
4581         Dimensions without coordinates: x
4582         """
4583         ds = self._to_temp_dataset().roll(
4584             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
4585         )
4586         return self._from_temp_dataset(ds)
4587 
4588     @property
4589     def real(self: T_DataArray) -> T_DataArray:
4590         """
4591         The real part of the array.
4592 
4593         See Also
4594         --------
4595         numpy.ndarray.real
4596         """
4597         return self._replace(self.variable.real)
4598 
4599     @property
4600     def imag(self: T_DataArray) -> T_DataArray:
4601         """
4602         The imaginary part of the array.
4603 
4604         See Also
4605         --------
4606         numpy.ndarray.imag
4607         """
4608         return self._replace(self.variable.imag)
4609 
4610     def dot(
4611         self: T_DataArray,
4612         other: T_DataArray,
4613         dims: Dims = None,
4614     ) -> T_DataArray:
4615         """Perform dot product of two DataArrays along their shared dims.
4616 
4617         Equivalent to taking taking tensordot over all shared dims.
4618 
4619         Parameters
4620         ----------
4621         other : DataArray
4622             The other array with which the dot product is performed.
4623         dims : ..., str, Iterable of Hashable or None, optional
4624             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.
4625             If not specified, then all the common dimensions are summed over.
4626 
4627         Returns
4628         -------
4629         result : DataArray
4630             Array resulting from the dot product over all shared dimensions.
4631 
4632         See Also
4633         --------
4634         dot
4635         numpy.tensordot
4636 
4637         Examples
4638         --------
4639         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
4640         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
4641         >>> dm_vals = np.arange(4)
4642         >>> dm = xr.DataArray(dm_vals, dims=["z"])
4643 
4644         >>> dm.dims
4645         ('z',)
4646 
4647         >>> da.dims
4648         ('x', 'y', 'z')
4649 
4650         >>> dot_result = da.dot(dm)
4651         >>> dot_result.dims
4652         ('x', 'y')
4653 
4654         """
4655         if isinstance(other, Dataset):
4656             raise NotImplementedError(
4657                 "dot products are not yet supported with Dataset objects."
4658             )
4659         if not isinstance(other, DataArray):
4660             raise TypeError("dot only operates on DataArrays.")
4661 
4662         return computation.dot(self, other, dims=dims)
4663 
4664     # change type of self and return to T_DataArray once
4665     # https://github.com/python/mypy/issues/12846 is resolved
4666     def sortby(
4667         self,
4668         variables: Hashable | DataArray | Sequence[Hashable | DataArray],
4669         ascending: bool = True,
4670     ) -> DataArray:
4671         """Sort object by labels or values (along an axis).
4672 
4673         Sorts the dataarray, either along specified dimensions,
4674         or according to values of 1-D dataarrays that share dimension
4675         with calling object.
4676 
4677         If the input variables are dataarrays, then the dataarrays are aligned
4678         (via left-join) to the calling object prior to sorting by cell values.
4679         NaNs are sorted to the end, following Numpy convention.
4680 
4681         If multiple sorts along the same dimension is
4682         given, numpy's lexsort is performed along that dimension:
4683         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
4684         and the FIRST key in the sequence is used as the primary sort key,
4685         followed by the 2nd key, etc.
4686 
4687         Parameters
4688         ----------
4689         variables : Hashable, DataArray, or sequence of Hashable or DataArray
4690             1D DataArray objects or name(s) of 1D variable(s) in
4691             coords whose values are used to sort this array.
4692         ascending : bool, default: True
4693             Whether to sort by ascending or descending order.
4694 
4695         Returns
4696         -------
4697         sorted : DataArray
4698             A new dataarray where all the specified dims are sorted by dim
4699             labels.
4700 
4701         See Also
4702         --------
4703         Dataset.sortby
4704         numpy.sort
4705         pandas.sort_values
4706         pandas.sort_index
4707 
4708         Examples
4709         --------
4710         >>> da = xr.DataArray(
4711         ...     np.random.rand(5),
4712         ...     coords=[pd.date_range("1/1/2000", periods=5)],
4713         ...     dims="time",
4714         ... )
4715         >>> da
4716         <xarray.DataArray (time: 5)>
4717         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])
4718         Coordinates:
4719           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05
4720 
4721         >>> da.sortby(da)
4722         <xarray.DataArray (time: 5)>
4723         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])
4724         Coordinates:
4725           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02
4726         """
4727         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
4728         return self._from_temp_dataset(ds)
4729 
4730     def quantile(
4731         self: T_DataArray,
4732         q: ArrayLike,
4733         dim: Dims = None,
4734         method: QuantileMethods = "linear",
4735         keep_attrs: bool | None = None,
4736         skipna: bool | None = None,
4737         interpolation: QuantileMethods | None = None,
4738     ) -> T_DataArray:
4739         """Compute the qth quantile of the data along the specified dimension.
4740 
4741         Returns the qth quantiles(s) of the array elements.
4742 
4743         Parameters
4744         ----------
4745         q : float or array-like of float
4746             Quantile to compute, which must be between 0 and 1 inclusive.
4747         dim : str or Iterable of Hashable, optional
4748             Dimension(s) over which to apply quantile.
4749         method : str, default: "linear"
4750             This optional parameter specifies the interpolation method to use when the
4751             desired quantile lies between two data points. The options sorted by their R
4752             type as summarized in the H&F paper [1]_ are:
4753 
4754                 1. "inverted_cdf" (*)
4755                 2. "averaged_inverted_cdf" (*)
4756                 3. "closest_observation" (*)
4757                 4. "interpolated_inverted_cdf" (*)
4758                 5. "hazen" (*)
4759                 6. "weibull" (*)
4760                 7. "linear"  (default)
4761                 8. "median_unbiased" (*)
4762                 9. "normal_unbiased" (*)
4763 
4764             The first three methods are discontiuous. The following discontinuous
4765             variations of the default "linear" (7.) option are also available:
4766 
4767                 * "lower"
4768                 * "higher"
4769                 * "midpoint"
4770                 * "nearest"
4771 
4772             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
4773             was previously called "interpolation", renamed in accordance with numpy
4774             version 1.22.0.
4775 
4776             (*) These methods require numpy version 1.22 or newer.
4777 
4778         keep_attrs : bool or None, optional
4779             If True, the dataset's attributes (`attrs`) will be copied from
4780             the original object to the new one.  If False (default), the new
4781             object will be returned without attributes.
4782         skipna : bool or None, optional
4783             If True, skip missing values (as marked by NaN). By default, only
4784             skips missing values for float dtypes; other dtypes either do not
4785             have a sentinel missing value (int) or skipna=True has not been
4786             implemented (object, datetime64 or timedelta64).
4787 
4788         Returns
4789         -------
4790         quantiles : DataArray
4791             If `q` is a single quantile, then the result
4792             is a scalar. If multiple percentiles are given, first axis of
4793             the result corresponds to the quantile and a quantile dimension
4794             is added to the return array. The other dimensions are the
4795             dimensions that remain after the reduction of the array.
4796 
4797         See Also
4798         --------
4799         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
4800 
4801         Examples
4802         --------
4803         >>> da = xr.DataArray(
4804         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
4805         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
4806         ...     dims=("x", "y"),
4807         ... )
4808         >>> da.quantile(0)  # or da.quantile(0, dim=...)
4809         <xarray.DataArray ()>
4810         array(0.7)
4811         Coordinates:
4812             quantile  float64 0.0
4813         >>> da.quantile(0, dim="x")
4814         <xarray.DataArray (y: 4)>
4815         array([0.7, 4.2, 2.6, 1.5])
4816         Coordinates:
4817           * y         (y) float64 1.0 1.5 2.0 2.5
4818             quantile  float64 0.0
4819         >>> da.quantile([0, 0.5, 1])
4820         <xarray.DataArray (quantile: 3)>
4821         array([0.7, 3.4, 9.4])
4822         Coordinates:
4823           * quantile  (quantile) float64 0.0 0.5 1.0
4824         >>> da.quantile([0, 0.5, 1], dim="x")
4825         <xarray.DataArray (quantile: 3, y: 4)>
4826         array([[0.7 , 4.2 , 2.6 , 1.5 ],
4827                [3.6 , 5.75, 6.  , 1.7 ],
4828                [6.5 , 7.3 , 9.4 , 1.9 ]])
4829         Coordinates:
4830           * y         (y) float64 1.0 1.5 2.0 2.5
4831           * quantile  (quantile) float64 0.0 0.5 1.0
4832 
4833         References
4834         ----------
4835         .. [1] R. J. Hyndman and Y. Fan,
4836            "Sample quantiles in statistical packages,"
4837            The American Statistician, 50(4), pp. 361-365, 1996
4838         """
4839 
4840         ds = self._to_temp_dataset().quantile(
4841             q,
4842             dim=dim,
4843             keep_attrs=keep_attrs,
4844             method=method,
4845             skipna=skipna,
4846             interpolation=interpolation,
4847         )
4848         return self._from_temp_dataset(ds)
4849 
4850     def rank(
4851         self: T_DataArray,
4852         dim: Hashable,
4853         pct: bool = False,
4854         keep_attrs: bool | None = None,
4855     ) -> T_DataArray:
4856         """Ranks the data.
4857 
4858         Equal values are assigned a rank that is the average of the ranks that
4859         would have been otherwise assigned to all of the values within that
4860         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
4861 
4862         NaNs in the input array are returned as NaNs.
4863 
4864         The `bottleneck` library is required.
4865 
4866         Parameters
4867         ----------
4868         dim : Hashable
4869             Dimension over which to compute rank.
4870         pct : bool, default: False
4871             If True, compute percentage ranks, otherwise compute integer ranks.
4872         keep_attrs : bool or None, optional
4873             If True, the dataset's attributes (`attrs`) will be copied from
4874             the original object to the new one.  If False (default), the new
4875             object will be returned without attributes.
4876 
4877         Returns
4878         -------
4879         ranked : DataArray
4880             DataArray with the same coordinates and dtype 'float64'.
4881 
4882         Examples
4883         --------
4884         >>> arr = xr.DataArray([5, 6, 7], dims="x")
4885         >>> arr.rank("x")
4886         <xarray.DataArray (x: 3)>
4887         array([1., 2., 3.])
4888         Dimensions without coordinates: x
4889         """
4890 
4891         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
4892         return self._from_temp_dataset(ds)
4893 
4894     def differentiate(
4895         self: T_DataArray,
4896         coord: Hashable,
4897         edge_order: Literal[1, 2] = 1,
4898         datetime_unit: DatetimeUnitOptions = None,
4899     ) -> T_DataArray:
4900         """ Differentiate the array with the second order accurate central
4901         differences.
4902 
4903         .. note::
4904             This feature is limited to simple cartesian geometry, i.e. coord
4905             must be one dimensional.
4906 
4907         Parameters
4908         ----------
4909         coord : Hashable
4910             The coordinate to be used to compute the gradient.
4911         edge_order : {1, 2}, default: 1
4912             N-th order accurate differences at the boundaries.
4913         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", \
4914                          "us", "ns", "ps", "fs", "as", None}, optional
4915             Unit to compute gradient. Only valid for datetime coordinate.
4916 
4917         Returns
4918         -------
4919         differentiated: DataArray
4920 
4921         See also
4922         --------
4923         numpy.gradient: corresponding numpy function
4924 
4925         Examples
4926         --------
4927 
4928         >>> da = xr.DataArray(
4929         ...     np.arange(12).reshape(4, 3),
4930         ...     dims=["x", "y"],
4931         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
4932         ... )
4933         >>> da
4934         <xarray.DataArray (x: 4, y: 3)>
4935         array([[ 0,  1,  2],
4936                [ 3,  4,  5],
4937                [ 6,  7,  8],
4938                [ 9, 10, 11]])
4939         Coordinates:
4940           * x        (x) float64 0.0 0.1 1.1 1.2
4941         Dimensions without coordinates: y
4942         >>>
4943         >>> da.differentiate("x")
4944         <xarray.DataArray (x: 4, y: 3)>
4945         array([[30.        , 30.        , 30.        ],
4946                [27.54545455, 27.54545455, 27.54545455],
4947                [27.54545455, 27.54545455, 27.54545455],
4948                [30.        , 30.        , 30.        ]])
4949         Coordinates:
4950           * x        (x) float64 0.0 0.1 1.1 1.2
4951         Dimensions without coordinates: y
4952         """
4953         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
4954         return self._from_temp_dataset(ds)
4955 
4956     # change type of self and return to T_DataArray once
4957     # https://github.com/python/mypy/issues/12846 is resolved
4958     def integrate(
4959         self,
4960         coord: Hashable | Sequence[Hashable] = None,
4961         datetime_unit: DatetimeUnitOptions = None,
4962     ) -> DataArray:
4963         """Integrate along the given coordinate using the trapezoidal rule.
4964 
4965         .. note::
4966             This feature is limited to simple cartesian geometry, i.e. coord
4967             must be one dimensional.
4968 
4969         Parameters
4970         ----------
4971         coord : Hashable, or sequence of Hashable
4972             Coordinate(s) used for the integration.
4973         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
4974                         'ps', 'fs', 'as', None}, optional
4975             Specify the unit if a datetime coordinate is used.
4976 
4977         Returns
4978         -------
4979         integrated : DataArray
4980 
4981         See also
4982         --------
4983         Dataset.integrate
4984         numpy.trapz : corresponding numpy function
4985 
4986         Examples
4987         --------
4988 
4989         >>> da = xr.DataArray(
4990         ...     np.arange(12).reshape(4, 3),
4991         ...     dims=["x", "y"],
4992         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
4993         ... )
4994         >>> da
4995         <xarray.DataArray (x: 4, y: 3)>
4996         array([[ 0,  1,  2],
4997                [ 3,  4,  5],
4998                [ 6,  7,  8],
4999                [ 9, 10, 11]])
5000         Coordinates:
5001           * x        (x) float64 0.0 0.1 1.1 1.2
5002         Dimensions without coordinates: y
5003         >>>
5004         >>> da.integrate("x")
5005         <xarray.DataArray (y: 3)>
5006         array([5.4, 6.6, 7.8])
5007         Dimensions without coordinates: y
5008         """
5009         ds = self._to_temp_dataset().integrate(coord, datetime_unit)
5010         return self._from_temp_dataset(ds)
5011 
5012     # change type of self and return to T_DataArray once
5013     # https://github.com/python/mypy/issues/12846 is resolved
5014     def cumulative_integrate(
5015         self,
5016         coord: Hashable | Sequence[Hashable] = None,
5017         datetime_unit: DatetimeUnitOptions = None,
5018     ) -> DataArray:
5019         """Integrate cumulatively along the given coordinate using the trapezoidal rule.
5020 
5021         .. note::
5022             This feature is limited to simple cartesian geometry, i.e. coord
5023             must be one dimensional.
5024 
5025             The first entry of the cumulative integral is always 0, in order to keep the
5026             length of the dimension unchanged between input and output.
5027 
5028         Parameters
5029         ----------
5030         coord : Hashable, or sequence of Hashable
5031             Coordinate(s) used for the integration.
5032         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
5033                         'ps', 'fs', 'as', None}, optional
5034             Specify the unit if a datetime coordinate is used.
5035 
5036         Returns
5037         -------
5038         integrated : DataArray
5039 
5040         See also
5041         --------
5042         Dataset.cumulative_integrate
5043         scipy.integrate.cumulative_trapezoid : corresponding scipy function
5044 
5045         Examples
5046         --------
5047 
5048         >>> da = xr.DataArray(
5049         ...     np.arange(12).reshape(4, 3),
5050         ...     dims=["x", "y"],
5051         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
5052         ... )
5053         >>> da
5054         <xarray.DataArray (x: 4, y: 3)>
5055         array([[ 0,  1,  2],
5056                [ 3,  4,  5],
5057                [ 6,  7,  8],
5058                [ 9, 10, 11]])
5059         Coordinates:
5060           * x        (x) float64 0.0 0.1 1.1 1.2
5061         Dimensions without coordinates: y
5062         >>>
5063         >>> da.cumulative_integrate("x")
5064         <xarray.DataArray (x: 4, y: 3)>
5065         array([[0.  , 0.  , 0.  ],
5066                [0.15, 0.25, 0.35],
5067                [4.65, 5.75, 6.85],
5068                [5.4 , 6.6 , 7.8 ]])
5069         Coordinates:
5070           * x        (x) float64 0.0 0.1 1.1 1.2
5071         Dimensions without coordinates: y
5072         """
5073         ds = self._to_temp_dataset().cumulative_integrate(coord, datetime_unit)
5074         return self._from_temp_dataset(ds)
5075 
5076     def unify_chunks(self) -> DataArray:
5077         """Unify chunk size along all chunked dimensions of this DataArray.
5078 
5079         Returns
5080         -------
5081         DataArray with consistent chunk sizes for all dask-array variables
5082 
5083         See Also
5084         --------
5085         dask.array.core.unify_chunks
5086         """
5087 
5088         return unify_chunks(self)[0]
5089 
5090     def map_blocks(
5091         self,
5092         func: Callable[..., T_Xarray],
5093         args: Sequence[Any] = (),
5094         kwargs: Mapping[str, Any] | None = None,
5095         template: DataArray | Dataset | None = None,
5096     ) -> T_Xarray:
5097         """
5098         Apply a function to each block of this DataArray.
5099 
5100         .. warning::
5101             This method is experimental and its signature may change.
5102 
5103         Parameters
5104         ----------
5105         func : callable
5106             User-provided function that accepts a DataArray as its first
5107             parameter. The function will receive a subset or 'block' of this DataArray (see below),
5108             corresponding to one chunk along each chunked dimension. ``func`` will be
5109             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.
5110 
5111             This function must return either a single DataArray or a single Dataset.
5112 
5113             This function cannot add a new chunked dimension.
5114         args : sequence
5115             Passed to func after unpacking and subsetting any xarray objects by blocks.
5116             xarray objects in args must be aligned with this object, otherwise an error is raised.
5117         kwargs : mapping
5118             Passed verbatim to func after unpacking. xarray objects, if any, will not be
5119             subset to blocks. Passing dask collections in kwargs is not allowed.
5120         template : DataArray or Dataset, optional
5121             xarray object representing the final result after compute is called. If not provided,
5122             the function will be first run on mocked-up data, that looks like this object but
5123             has sizes 0, to determine properties of the returned object such as dtype,
5124             variable names, attributes, new dimensions and new indexes (if any).
5125             ``template`` must be provided if the function changes the size of existing dimensions.
5126             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
5127             ``attrs`` set by ``func`` will be ignored.
5128 
5129         Returns
5130         -------
5131         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
5132         function.
5133 
5134         Notes
5135         -----
5136         This function is designed for when ``func`` needs to manipulate a whole xarray object
5137         subset to each block. Each block is loaded into memory. In the more common case where
5138         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
5139 
5140         If none of the variables in this object is backed by dask arrays, calling this function is
5141         equivalent to calling ``func(obj, *args, **kwargs)``.
5142 
5143         See Also
5144         --------
5145         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
5146         xarray.DataArray.map_blocks
5147 
5148         Examples
5149         --------
5150         Calculate an anomaly from climatology using ``.groupby()``. Using
5151         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
5152         its indices, and its methods like ``.groupby()``.
5153 
5154         >>> def calculate_anomaly(da, groupby_type="time.month"):
5155         ...     gb = da.groupby(groupby_type)
5156         ...     clim = gb.mean(dim="time")
5157         ...     return gb - clim
5158         ...
5159         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
5160         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
5161         >>> np.random.seed(123)
5162         >>> array = xr.DataArray(
5163         ...     np.random.rand(len(time)),
5164         ...     dims=["time"],
5165         ...     coords={"time": time, "month": month},
5166         ... ).chunk()
5167         >>> array.map_blocks(calculate_anomaly, template=array).compute()
5168         <xarray.DataArray (time: 24)>
5169         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,
5170                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,
5171                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,
5172                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,
5173                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])
5174         Coordinates:
5175           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
5176             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
5177 
5178         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
5179         to the function being applied in ``xr.map_blocks()``:
5180 
5181         >>> array.map_blocks(
5182         ...     calculate_anomaly, kwargs={"groupby_type": "time.year"}, template=array
5183         ... )  # doctest: +ELLIPSIS
5184         <xarray.DataArray (time: 24)>
5185         dask.array<<this-array>-calculate_anomaly, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>
5186         Coordinates:
5187           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
5188             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
5189         """
5190         from xarray.core.parallel import map_blocks
5191 
5192         return map_blocks(func, self, args, kwargs, template)
5193 
5194     def polyfit(
5195         self,
5196         dim: Hashable,
5197         deg: int,
5198         skipna: bool | None = None,
5199         rcond: float | None = None,
5200         w: Hashable | Any | None = None,
5201         full: bool = False,
5202         cov: bool | Literal["unscaled"] = False,
5203     ) -> Dataset:
5204         """
5205         Least squares polynomial fit.
5206 
5207         This replicates the behaviour of `numpy.polyfit` but differs by skipping
5208         invalid values when `skipna = True`.
5209 
5210         Parameters
5211         ----------
5212         dim : Hashable
5213             Coordinate along which to fit the polynomials.
5214         deg : int
5215             Degree of the fitting polynomial.
5216         skipna : bool or None, optional
5217             If True, removes all invalid values before fitting each 1D slices of the array.
5218             Default is True if data is stored in a dask.array or if there is any
5219             invalid values, False otherwise.
5220         rcond : float or None, optional
5221             Relative condition number to the fit.
5222         w : Hashable, array-like or None, optional
5223             Weights to apply to the y-coordinate of the sample points.
5224             Can be an array-like object or the name of a coordinate in the dataset.
5225         full : bool, default: False
5226             Whether to return the residuals, matrix rank and singular values in addition
5227             to the coefficients.
5228         cov : bool or "unscaled", default: False
5229             Whether to return to the covariance matrix in addition to the coefficients.
5230             The matrix is not scaled if `cov='unscaled'`.
5231 
5232         Returns
5233         -------
5234         polyfit_results : Dataset
5235             A single dataset which contains:
5236 
5237             polyfit_coefficients
5238                 The coefficients of the best fit.
5239             polyfit_residuals
5240                 The residuals of the least-square computation (only included if `full=True`).
5241                 When the matrix rank is deficient, np.nan is returned.
5242             [dim]_matrix_rank
5243                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
5244             [dim]_singular_value
5245                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
5246             polyfit_covariance
5247                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
5248 
5249         See Also
5250         --------
5251         numpy.polyfit
5252         numpy.polyval
5253         xarray.polyval
5254         """
5255         return self._to_temp_dataset().polyfit(
5256             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov
5257         )
5258 
5259     def pad(
5260         self: T_DataArray,
5261         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,
5262         mode: PadModeOptions = "constant",
5263         stat_length: int
5264         | tuple[int, int]
5265         | Mapping[Any, tuple[int, int]]
5266         | None = None,
5267         constant_values: float
5268         | tuple[float, float]
5269         | Mapping[Any, tuple[float, float]]
5270         | None = None,
5271         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
5272         reflect_type: PadReflectOptions = None,
5273         keep_attrs: bool | None = None,
5274         **pad_width_kwargs: Any,
5275     ) -> T_DataArray:
5276         """Pad this array along one or more dimensions.
5277 
5278         .. warning::
5279             This function is experimental and its behaviour is likely to change
5280             especially regarding padding of dimension coordinates (or IndexVariables).
5281 
5282         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
5283         coordinates will be padded with the same mode, otherwise coordinates
5284         are padded using the "constant" mode with fill_value dtypes.NA.
5285 
5286         Parameters
5287         ----------
5288         pad_width : mapping of Hashable to tuple of int
5289             Mapping with the form of {dim: (pad_before, pad_after)}
5290             describing the number of values padded along each dimension.
5291             {dim: pad} is a shortcut for pad_before = pad_after = pad
5292         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
5293             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
5294             How to pad the DataArray (taken from numpy docs):
5295 
5296             - "constant": Pads with a constant value.
5297             - "edge": Pads with the edge values of array.
5298             - "linear_ramp": Pads with the linear ramp between end_value and the
5299               array edge value.
5300             - "maximum": Pads with the maximum value of all or part of the
5301               vector along each axis.
5302             - "mean": Pads with the mean value of all or part of the
5303               vector along each axis.
5304             - "median": Pads with the median value of all or part of the
5305               vector along each axis.
5306             - "minimum": Pads with the minimum value of all or part of the
5307               vector along each axis.
5308             - "reflect": Pads with the reflection of the vector mirrored on
5309               the first and last values of the vector along each axis.
5310             - "symmetric": Pads with the reflection of the vector mirrored
5311               along the edge of the array.
5312             - "wrap": Pads with the wrap of the vector along the axis.
5313               The first values are used to pad the end and the
5314               end values are used to pad the beginning.
5315 
5316         stat_length : int, tuple or mapping of Hashable to tuple, default: None
5317             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
5318             values at edge of each axis used to calculate the statistic value.
5319             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
5320             statistic lengths along each dimension.
5321             ((before, after),) yields same before and after statistic lengths
5322             for each dimension.
5323             (stat_length,) or int is a shortcut for before = after = statistic
5324             length for all axes.
5325             Default is ``None``, to use the entire axis.
5326         constant_values : scalar, tuple or mapping of Hashable to tuple, default: 0
5327             Used in 'constant'.  The values to set the padded values for each
5328             axis.
5329             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
5330             pad constants along each dimension.
5331             ``((before, after),)`` yields same before and after constants for each
5332             dimension.
5333             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
5334             all dimensions.
5335             Default is 0.
5336         end_values : scalar, tuple or mapping of Hashable to tuple, default: 0
5337             Used in 'linear_ramp'.  The values used for the ending value of the
5338             linear_ramp and that will form the edge of the padded array.
5339             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
5340             end values along each dimension.
5341             ``((before, after),)`` yields same before and after end values for each
5342             axis.
5343             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
5344             all axes.
5345             Default is 0.
5346         reflect_type : {"even", "odd", None}, optional
5347             Used in "reflect", and "symmetric". The "even" style is the
5348             default with an unaltered reflection around the edge value. For
5349             the "odd" style, the extended part of the array is created by
5350             subtracting the reflected values from two times the edge value.
5351         keep_attrs : bool or None, optional
5352             If True, the attributes (``attrs``) will be copied from the
5353             original object to the new one. If False, the new object
5354             will be returned without attributes.
5355         **pad_width_kwargs
5356             The keyword arguments form of ``pad_width``.
5357             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
5358 
5359         Returns
5360         -------
5361         padded : DataArray
5362             DataArray with the padded coordinates and data.
5363 
5364         See Also
5365         --------
5366         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad
5367 
5368         Notes
5369         -----
5370         For ``mode="constant"`` and ``constant_values=None``, integer types will be
5371         promoted to ``float`` and padded with ``np.nan``.
5372 
5373         Padding coordinates will drop their corresponding index (if any) and will reset default
5374         indexes for dimension coordinates.
5375 
5376         Examples
5377         --------
5378         >>> arr = xr.DataArray([5, 6, 7], coords=[("x", [0, 1, 2])])
5379         >>> arr.pad(x=(1, 2), constant_values=0)
5380         <xarray.DataArray (x: 6)>
5381         array([0, 5, 6, 7, 0, 0])
5382         Coordinates:
5383           * x        (x) float64 nan 0.0 1.0 2.0 nan nan
5384 
5385         >>> da = xr.DataArray(
5386         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],
5387         ...     dims=["x", "y"],
5388         ...     coords={"x": [0, 1], "y": [10, 20, 30, 40], "z": ("x", [100, 200])},
5389         ... )
5390         >>> da.pad(x=1)
5391         <xarray.DataArray (x: 4, y: 4)>
5392         array([[nan, nan, nan, nan],
5393                [ 0.,  1.,  2.,  3.],
5394                [10., 11., 12., 13.],
5395                [nan, nan, nan, nan]])
5396         Coordinates:
5397           * x        (x) float64 nan 0.0 1.0 nan
5398           * y        (y) int64 10 20 30 40
5399             z        (x) float64 nan 100.0 200.0 nan
5400 
5401         Careful, ``constant_values`` are coerced to the data type of the array which may
5402         lead to a loss of precision:
5403 
5404         >>> da.pad(x=1, constant_values=1.23456789)
5405         <xarray.DataArray (x: 4, y: 4)>
5406         array([[ 1,  1,  1,  1],
5407                [ 0,  1,  2,  3],
5408                [10, 11, 12, 13],
5409                [ 1,  1,  1,  1]])
5410         Coordinates:
5411           * x        (x) float64 nan 0.0 1.0 nan
5412           * y        (y) int64 10 20 30 40
5413             z        (x) float64 nan 100.0 200.0 nan
5414         """
5415         ds = self._to_temp_dataset().pad(
5416             pad_width=pad_width,
5417             mode=mode,
5418             stat_length=stat_length,
5419             constant_values=constant_values,
5420             end_values=end_values,
5421             reflect_type=reflect_type,
5422             keep_attrs=keep_attrs,
5423             **pad_width_kwargs,
5424         )
5425         return self._from_temp_dataset(ds)
5426 
5427     def idxmin(
5428         self,
5429         dim: Hashable | None = None,
5430         skipna: bool | None = None,
5431         fill_value: Any = dtypes.NA,
5432         keep_attrs: bool | None = None,
5433     ) -> DataArray:
5434         """Return the coordinate label of the minimum value along a dimension.
5435 
5436         Returns a new `DataArray` named after the dimension with the values of
5437         the coordinate labels along that dimension corresponding to minimum
5438         values along that dimension.
5439 
5440         In comparison to :py:meth:`~DataArray.argmin`, this returns the
5441         coordinate label while :py:meth:`~DataArray.argmin` returns the index.
5442 
5443         Parameters
5444         ----------
5445         dim : str, optional
5446             Dimension over which to apply `idxmin`.  This is optional for 1D
5447             arrays, but required for arrays with 2 or more dimensions.
5448         skipna : bool or None, default: None
5449             If True, skip missing values (as marked by NaN). By default, only
5450             skips missing values for ``float``, ``complex``, and ``object``
5451             dtypes; other dtypes either do not have a sentinel missing value
5452             (``int``) or ``skipna=True`` has not been implemented
5453             (``datetime64`` or ``timedelta64``).
5454         fill_value : Any, default: NaN
5455             Value to be filled in case all of the values along a dimension are
5456             null.  By default this is NaN.  The fill value and result are
5457             automatically converted to a compatible dtype if possible.
5458             Ignored if ``skipna`` is False.
5459         keep_attrs : bool or None, optional
5460             If True, the attributes (``attrs``) will be copied from the
5461             original object to the new one. If False, the new object
5462             will be returned without attributes.
5463 
5464         Returns
5465         -------
5466         reduced : DataArray
5467             New `DataArray` object with `idxmin` applied to its data and the
5468             indicated dimension removed.
5469 
5470         See Also
5471         --------
5472         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin
5473 
5474         Examples
5475         --------
5476         >>> array = xr.DataArray(
5477         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
5478         ... )
5479         >>> array.min()
5480         <xarray.DataArray ()>
5481         array(-2)
5482         >>> array.argmin(...)
5483         {'x': <xarray.DataArray ()>
5484         array(4)}
5485         >>> array.idxmin()
5486         <xarray.DataArray 'x' ()>
5487         array('e', dtype='<U1')
5488 
5489         >>> array = xr.DataArray(
5490         ...     [
5491         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
5492         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
5493         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
5494         ...     ],
5495         ...     dims=["y", "x"],
5496         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
5497         ... )
5498         >>> array.min(dim="x")
5499         <xarray.DataArray (y: 3)>
5500         array([-2., -4.,  1.])
5501         Coordinates:
5502           * y        (y) int64 -1 0 1
5503         >>> array.argmin(dim="x")
5504         <xarray.DataArray (y: 3)>
5505         array([4, 0, 2])
5506         Coordinates:
5507           * y        (y) int64 -1 0 1
5508         >>> array.idxmin(dim="x")
5509         <xarray.DataArray 'x' (y: 3)>
5510         array([16.,  0.,  4.])
5511         Coordinates:
5512           * y        (y) int64 -1 0 1
5513         """
5514         return computation._calc_idxminmax(
5515             array=self,
5516             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),
5517             dim=dim,
5518             skipna=skipna,
5519             fill_value=fill_value,
5520             keep_attrs=keep_attrs,
5521         )
5522 
5523     def idxmax(
5524         self,
5525         dim: Hashable = None,
5526         skipna: bool | None = None,
5527         fill_value: Any = dtypes.NA,
5528         keep_attrs: bool | None = None,
5529     ) -> DataArray:
5530         """Return the coordinate label of the maximum value along a dimension.
5531 
5532         Returns a new `DataArray` named after the dimension with the values of
5533         the coordinate labels along that dimension corresponding to maximum
5534         values along that dimension.
5535 
5536         In comparison to :py:meth:`~DataArray.argmax`, this returns the
5537         coordinate label while :py:meth:`~DataArray.argmax` returns the index.
5538 
5539         Parameters
5540         ----------
5541         dim : Hashable, optional
5542             Dimension over which to apply `idxmax`.  This is optional for 1D
5543             arrays, but required for arrays with 2 or more dimensions.
5544         skipna : bool or None, default: None
5545             If True, skip missing values (as marked by NaN). By default, only
5546             skips missing values for ``float``, ``complex``, and ``object``
5547             dtypes; other dtypes either do not have a sentinel missing value
5548             (``int``) or ``skipna=True`` has not been implemented
5549             (``datetime64`` or ``timedelta64``).
5550         fill_value : Any, default: NaN
5551             Value to be filled in case all of the values along a dimension are
5552             null.  By default this is NaN.  The fill value and result are
5553             automatically converted to a compatible dtype if possible.
5554             Ignored if ``skipna`` is False.
5555         keep_attrs : bool or None, optional
5556             If True, the attributes (``attrs``) will be copied from the
5557             original object to the new one. If False, the new object
5558             will be returned without attributes.
5559 
5560         Returns
5561         -------
5562         reduced : DataArray
5563             New `DataArray` object with `idxmax` applied to its data and the
5564             indicated dimension removed.
5565 
5566         See Also
5567         --------
5568         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax
5569 
5570         Examples
5571         --------
5572         >>> array = xr.DataArray(
5573         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
5574         ... )
5575         >>> array.max()
5576         <xarray.DataArray ()>
5577         array(2)
5578         >>> array.argmax(...)
5579         {'x': <xarray.DataArray ()>
5580         array(1)}
5581         >>> array.idxmax()
5582         <xarray.DataArray 'x' ()>
5583         array('b', dtype='<U1')
5584 
5585         >>> array = xr.DataArray(
5586         ...     [
5587         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
5588         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
5589         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
5590         ...     ],
5591         ...     dims=["y", "x"],
5592         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
5593         ... )
5594         >>> array.max(dim="x")
5595         <xarray.DataArray (y: 3)>
5596         array([2., 2., 1.])
5597         Coordinates:
5598           * y        (y) int64 -1 0 1
5599         >>> array.argmax(dim="x")
5600         <xarray.DataArray (y: 3)>
5601         array([0, 2, 2])
5602         Coordinates:
5603           * y        (y) int64 -1 0 1
5604         >>> array.idxmax(dim="x")
5605         <xarray.DataArray 'x' (y: 3)>
5606         array([0., 4., 4.])
5607         Coordinates:
5608           * y        (y) int64 -1 0 1
5609         """
5610         return computation._calc_idxminmax(
5611             array=self,
5612             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),
5613             dim=dim,
5614             skipna=skipna,
5615             fill_value=fill_value,
5616             keep_attrs=keep_attrs,
5617         )
5618 
5619     # change type of self and return to T_DataArray once
5620     # https://github.com/python/mypy/issues/12846 is resolved
5621     def argmin(
5622         self,
5623         dim: Dims = None,
5624         axis: int | None = None,
5625         keep_attrs: bool | None = None,
5626         skipna: bool | None = None,
5627     ) -> DataArray | dict[Hashable, DataArray]:
5628         """Index or indices of the minimum of the DataArray over one or more dimensions.
5629 
5630         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
5631         which can be passed directly to isel(). If a single str is passed to 'dim' then
5632         returns a DataArray with dtype int.
5633 
5634         If there are multiple minima, the indices of the first one found will be
5635         returned.
5636 
5637         Parameters
5638         ----------
5639         dim : "...", str, Iterable of Hashable or None, optional
5640             The dimensions over which to find the minimum. By default, finds minimum over
5641             all dimensions - for now returning an int for backward compatibility, but
5642             this is deprecated, in future will return a dict with indices for all
5643             dimensions; to return a dict with all dimensions now, pass '...'.
5644         axis : int or None, optional
5645             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
5646             can be supplied.
5647         keep_attrs : bool or None, optional
5648             If True, the attributes (`attrs`) will be copied from the original
5649             object to the new one. If False, the new object will be
5650             returned without attributes.
5651         skipna : bool or None, optional
5652             If True, skip missing values (as marked by NaN). By default, only
5653             skips missing values for float dtypes; other dtypes either do not
5654             have a sentinel missing value (int) or skipna=True has not been
5655             implemented (object, datetime64 or timedelta64).
5656 
5657         Returns
5658         -------
5659         result : DataArray or dict of DataArray
5660 
5661         See Also
5662         --------
5663         Variable.argmin, DataArray.idxmin
5664 
5665         Examples
5666         --------
5667         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
5668         >>> array.min()
5669         <xarray.DataArray ()>
5670         array(-1)
5671         >>> array.argmin(...)
5672         {'x': <xarray.DataArray ()>
5673         array(2)}
5674         >>> array.isel(array.argmin(...))
5675         <xarray.DataArray ()>
5676         array(-1)
5677 
5678         >>> array = xr.DataArray(
5679         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],
5680         ...     dims=("x", "y", "z"),
5681         ... )
5682         >>> array.min(dim="x")
5683         <xarray.DataArray (y: 3, z: 3)>
5684         array([[ 1,  2,  1],
5685                [ 2, -5,  1],
5686                [ 2,  1,  1]])
5687         Dimensions without coordinates: y, z
5688         >>> array.argmin(dim="x")
5689         <xarray.DataArray (y: 3, z: 3)>
5690         array([[1, 0, 0],
5691                [1, 1, 1],
5692                [0, 0, 1]])
5693         Dimensions without coordinates: y, z
5694         >>> array.argmin(dim=["x"])
5695         {'x': <xarray.DataArray (y: 3, z: 3)>
5696         array([[1, 0, 0],
5697                [1, 1, 1],
5698                [0, 0, 1]])
5699         Dimensions without coordinates: y, z}
5700         >>> array.min(dim=("x", "z"))
5701         <xarray.DataArray (y: 3)>
5702         array([ 1, -5,  1])
5703         Dimensions without coordinates: y
5704         >>> array.argmin(dim=["x", "z"])
5705         {'x': <xarray.DataArray (y: 3)>
5706         array([0, 1, 0])
5707         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
5708         array([2, 1, 1])
5709         Dimensions without coordinates: y}
5710         >>> array.isel(array.argmin(dim=["x", "z"]))
5711         <xarray.DataArray (y: 3)>
5712         array([ 1, -5,  1])
5713         Dimensions without coordinates: y
5714         """
5715         result = self.variable.argmin(dim, axis, keep_attrs, skipna)
5716         if isinstance(result, dict):
5717             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
5718         else:
5719             return self._replace_maybe_drop_dims(result)
5720 
5721     # change type of self and return to T_DataArray once
5722     # https://github.com/python/mypy/issues/12846 is resolved
5723     def argmax(
5724         self,
5725         dim: Dims = None,
5726         axis: int | None = None,
5727         keep_attrs: bool | None = None,
5728         skipna: bool | None = None,
5729     ) -> DataArray | dict[Hashable, DataArray]:
5730         """Index or indices of the maximum of the DataArray over one or more dimensions.
5731 
5732         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
5733         which can be passed directly to isel(). If a single str is passed to 'dim' then
5734         returns a DataArray with dtype int.
5735 
5736         If there are multiple maxima, the indices of the first one found will be
5737         returned.
5738 
5739         Parameters
5740         ----------
5741         dim : "...", str, Iterable of Hashable or None, optional
5742             The dimensions over which to find the maximum. By default, finds maximum over
5743             all dimensions - for now returning an int for backward compatibility, but
5744             this is deprecated, in future will return a dict with indices for all
5745             dimensions; to return a dict with all dimensions now, pass '...'.
5746         axis : int or None, optional
5747             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
5748             can be supplied.
5749         keep_attrs : bool or None, optional
5750             If True, the attributes (`attrs`) will be copied from the original
5751             object to the new one. If False, the new object will be
5752             returned without attributes.
5753         skipna : bool or None, optional
5754             If True, skip missing values (as marked by NaN). By default, only
5755             skips missing values for float dtypes; other dtypes either do not
5756             have a sentinel missing value (int) or skipna=True has not been
5757             implemented (object, datetime64 or timedelta64).
5758 
5759         Returns
5760         -------
5761         result : DataArray or dict of DataArray
5762 
5763         See Also
5764         --------
5765         Variable.argmax, DataArray.idxmax
5766 
5767         Examples
5768         --------
5769         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
5770         >>> array.max()
5771         <xarray.DataArray ()>
5772         array(3)
5773         >>> array.argmax(...)
5774         {'x': <xarray.DataArray ()>
5775         array(3)}
5776         >>> array.isel(array.argmax(...))
5777         <xarray.DataArray ()>
5778         array(3)
5779 
5780         >>> array = xr.DataArray(
5781         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],
5782         ...     dims=("x", "y", "z"),
5783         ... )
5784         >>> array.max(dim="x")
5785         <xarray.DataArray (y: 3, z: 3)>
5786         array([[3, 3, 2],
5787                [3, 5, 2],
5788                [2, 3, 3]])
5789         Dimensions without coordinates: y, z
5790         >>> array.argmax(dim="x")
5791         <xarray.DataArray (y: 3, z: 3)>
5792         array([[0, 1, 1],
5793                [0, 1, 0],
5794                [0, 1, 0]])
5795         Dimensions without coordinates: y, z
5796         >>> array.argmax(dim=["x"])
5797         {'x': <xarray.DataArray (y: 3, z: 3)>
5798         array([[0, 1, 1],
5799                [0, 1, 0],
5800                [0, 1, 0]])
5801         Dimensions without coordinates: y, z}
5802         >>> array.max(dim=("x", "z"))
5803         <xarray.DataArray (y: 3)>
5804         array([3, 5, 3])
5805         Dimensions without coordinates: y
5806         >>> array.argmax(dim=["x", "z"])
5807         {'x': <xarray.DataArray (y: 3)>
5808         array([0, 1, 0])
5809         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
5810         array([0, 1, 2])
5811         Dimensions without coordinates: y}
5812         >>> array.isel(array.argmax(dim=["x", "z"]))
5813         <xarray.DataArray (y: 3)>
5814         array([3, 5, 3])
5815         Dimensions without coordinates: y
5816         """
5817         result = self.variable.argmax(dim, axis, keep_attrs, skipna)
5818         if isinstance(result, dict):
5819             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
5820         else:
5821             return self._replace_maybe_drop_dims(result)
5822 
5823     def query(
5824         self,
5825         queries: Mapping[Any, Any] | None = None,
5826         parser: QueryParserOptions = "pandas",
5827         engine: QueryEngineOptions = None,
5828         missing_dims: ErrorOptionsWithWarn = "raise",
5829         **queries_kwargs: Any,
5830     ) -> DataArray:
5831         """Return a new data array indexed along the specified
5832         dimension(s), where the indexers are given as strings containing
5833         Python expressions to be evaluated against the values in the array.
5834 
5835         Parameters
5836         ----------
5837         queries : dict-like or None, optional
5838             A dict-like with keys matching dimensions and values given by strings
5839             containing Python expressions to be evaluated against the data variables
5840             in the dataset. The expressions will be evaluated using the pandas
5841             eval() function, and can contain any valid Python expressions but cannot
5842             contain any Python statements.
5843         parser : {"pandas", "python"}, default: "pandas"
5844             The parser to use to construct the syntax tree from the expression.
5845             The default of 'pandas' parses code slightly different than standard
5846             Python. Alternatively, you can parse an expression using the 'python'
5847             parser to retain strict Python semantics.
5848         engine : {"python", "numexpr", None}, default: None
5849             The engine used to evaluate the expression. Supported engines are:
5850 
5851             - None: tries to use numexpr, falls back to python
5852             - "numexpr": evaluates expressions using numexpr
5853             - "python": performs operations as if you had eval’d in top level python
5854 
5855         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
5856             What to do if dimensions that should be selected from are not present in the
5857             DataArray:
5858 
5859             - "raise": raise an exception
5860             - "warn": raise a warning, and ignore the missing dimensions
5861             - "ignore": ignore the missing dimensions
5862 
5863         **queries_kwargs : {dim: query, ...}, optional
5864             The keyword arguments form of ``queries``.
5865             One of queries or queries_kwargs must be provided.
5866 
5867         Returns
5868         -------
5869         obj : DataArray
5870             A new DataArray with the same contents as this dataset, indexed by
5871             the results of the appropriate queries.
5872 
5873         See Also
5874         --------
5875         DataArray.isel
5876         Dataset.query
5877         pandas.eval
5878 
5879         Examples
5880         --------
5881         >>> da = xr.DataArray(np.arange(0, 5, 1), dims="x", name="a")
5882         >>> da
5883         <xarray.DataArray 'a' (x: 5)>
5884         array([0, 1, 2, 3, 4])
5885         Dimensions without coordinates: x
5886         >>> da.query(x="a > 2")
5887         <xarray.DataArray 'a' (x: 2)>
5888         array([3, 4])
5889         Dimensions without coordinates: x
5890         """
5891 
5892         ds = self._to_dataset_whole(shallow_copy=True)
5893         ds = ds.query(
5894             queries=queries,
5895             parser=parser,
5896             engine=engine,
5897             missing_dims=missing_dims,
5898             **queries_kwargs,
5899         )
5900         return ds[self.name]
5901 
5902     def curvefit(
5903         self,
5904         coords: str | DataArray | Iterable[str | DataArray],
5905         func: Callable[..., Any],
5906         reduce_dims: Dims = None,
5907         skipna: bool = True,
5908         p0: dict[str, Any] | None = None,
5909         bounds: dict[str, Any] | None = None,
5910         param_names: Sequence[str] | None = None,
5911         kwargs: dict[str, Any] | None = None,
5912     ) -> Dataset:
5913         """
5914         Curve fitting optimization for arbitrary functions.
5915 
5916         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
5917 
5918         Parameters
5919         ----------
5920         coords : Hashable, DataArray, or sequence of DataArray or Hashable
5921             Independent coordinate(s) over which to perform the curve fitting. Must share
5922             at least one dimension with the calling object. When fitting multi-dimensional
5923             functions, supply `coords` as a sequence in the same order as arguments in
5924             `func`. To fit along existing dimensions of the calling object, `coords` can
5925             also be specified as a str or sequence of strs.
5926         func : callable
5927             User specified function in the form `f(x, *params)` which returns a numpy
5928             array of length `len(x)`. `params` are the fittable parameters which are optimized
5929             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
5930             coordinates, e.g. `f((x0, x1), *params)`.
5931         reduce_dims : str, Iterable of Hashable or None, optional
5932             Additional dimension(s) over which to aggregate while fitting. For example,
5933             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
5934             aggregate all lat and lon points and fit the specified function along the
5935             time dimension.
5936         skipna : bool, default: True
5937             Whether to skip missing values when fitting. Default is True.
5938         p0 : dict-like or None, optional
5939             Optional dictionary of parameter names to initial guesses passed to the
5940             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
5941             be assigned initial values following the default scipy behavior.
5942         bounds : dict-like or None, optional
5943             Optional dictionary of parameter names to bounding values passed to the
5944             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
5945             will be unbounded following the default scipy behavior.
5946         param_names : sequence of Hashable or None, optional
5947             Sequence of names for the fittable parameters of `func`. If not supplied,
5948             this will be automatically determined by arguments of `func`. `param_names`
5949             should be manually supplied when fitting a function that takes a variable
5950             number of parameters.
5951         **kwargs : optional
5952             Additional keyword arguments to passed to scipy curve_fit.
5953 
5954         Returns
5955         -------
5956         curvefit_results : Dataset
5957             A single dataset which contains:
5958 
5959             [var]_curvefit_coefficients
5960                 The coefficients of the best fit.
5961             [var]_curvefit_covariance
5962                 The covariance matrix of the coefficient estimates.
5963 
5964         See Also
5965         --------
5966         DataArray.polyfit
5967         scipy.optimize.curve_fit
5968         """
5969         return self._to_temp_dataset().curvefit(
5970             coords,
5971             func,
5972             reduce_dims=reduce_dims,
5973             skipna=skipna,
5974             p0=p0,
5975             bounds=bounds,
5976             param_names=param_names,
5977             kwargs=kwargs,
5978         )
5979 
5980     def drop_duplicates(
5981         self: T_DataArray,
5982         dim: Hashable | Iterable[Hashable],
5983         keep: Literal["first", "last", False] = "first",
5984     ) -> T_DataArray:
5985         """Returns a new DataArray with duplicate dimension values removed.
5986 
5987         Parameters
5988         ----------
5989         dim : dimension label or labels
5990             Pass `...` to drop duplicates along all dimensions.
5991         keep : {"first", "last", False}, default: "first"
5992             Determines which duplicates (if any) to keep.
5993 
5994             - ``"first"`` : Drop duplicates except for the first occurrence.
5995             - ``"last"`` : Drop duplicates except for the last occurrence.
5996             - False : Drop all duplicates.
5997 
5998         Returns
5999         -------
6000         DataArray
6001 
6002         See Also
6003         --------
6004         Dataset.drop_duplicates
6005 
6006         Examples
6007         --------
6008         >>> da = xr.DataArray(
6009         ...     np.arange(25).reshape(5, 5),
6010         ...     dims=("x", "y"),
6011         ...     coords={"x": np.array([0, 0, 1, 2, 3]), "y": np.array([0, 1, 2, 3, 3])},
6012         ... )
6013         >>> da
6014         <xarray.DataArray (x: 5, y: 5)>
6015         array([[ 0,  1,  2,  3,  4],
6016                [ 5,  6,  7,  8,  9],
6017                [10, 11, 12, 13, 14],
6018                [15, 16, 17, 18, 19],
6019                [20, 21, 22, 23, 24]])
6020         Coordinates:
6021           * x        (x) int64 0 0 1 2 3
6022           * y        (y) int64 0 1 2 3 3
6023 
6024         >>> da.drop_duplicates(dim="x")
6025         <xarray.DataArray (x: 4, y: 5)>
6026         array([[ 0,  1,  2,  3,  4],
6027                [10, 11, 12, 13, 14],
6028                [15, 16, 17, 18, 19],
6029                [20, 21, 22, 23, 24]])
6030         Coordinates:
6031           * x        (x) int64 0 1 2 3
6032           * y        (y) int64 0 1 2 3 3
6033 
6034         >>> da.drop_duplicates(dim="x", keep="last")
6035         <xarray.DataArray (x: 4, y: 5)>
6036         array([[ 5,  6,  7,  8,  9],
6037                [10, 11, 12, 13, 14],
6038                [15, 16, 17, 18, 19],
6039                [20, 21, 22, 23, 24]])
6040         Coordinates:
6041           * x        (x) int64 0 1 2 3
6042           * y        (y) int64 0 1 2 3 3
6043 
6044         Drop all duplicate dimension values:
6045 
6046         >>> da.drop_duplicates(dim=...)
6047         <xarray.DataArray (x: 4, y: 4)>
6048         array([[ 0,  1,  2,  3],
6049                [10, 11, 12, 13],
6050                [15, 16, 17, 18],
6051                [20, 21, 22, 23]])
6052         Coordinates:
6053           * x        (x) int64 0 1 2 3
6054           * y        (y) int64 0 1 2 3
6055         """
6056         deduplicated = self._to_temp_dataset().drop_duplicates(dim, keep=keep)
6057         return self._from_temp_dataset(deduplicated)
6058 
6059     def convert_calendar(
6060         self,
6061         calendar: str,
6062         dim: str = "time",
6063         align_on: str | None = None,
6064         missing: Any | None = None,
6065         use_cftime: bool | None = None,
6066     ) -> DataArray:
6067         """Convert the DataArray to another calendar.
6068 
6069         Only converts the individual timestamps, does not modify any data except
6070         in dropping invalid/surplus dates or inserting missing dates.
6071 
6072         If the source and target calendars are either no_leap, all_leap or a
6073         standard type, only the type of the time array is modified.
6074         When converting to a leap year from a non-leap year, the 29th of February
6075         is removed from the array. In the other direction the 29th of February
6076         will be missing in the output, unless `missing` is specified,
6077         in which case that value is inserted.
6078 
6079         For conversions involving `360_day` calendars, see Notes.
6080 
6081         This method is safe to use with sub-daily data as it doesn't touch the
6082         time part of the timestamps.
6083 
6084         Parameters
6085         ---------
6086         calendar : str
6087             The target calendar name.
6088         dim : str
6089             Name of the time coordinate.
6090         align_on : {None, 'date', 'year'}
6091             Must be specified when either source or target is a `360_day` calendar,
6092            ignored otherwise. See Notes.
6093         missing : Optional[any]
6094             By default, i.e. if the value is None, this method will simply attempt
6095             to convert the dates in the source calendar to the same dates in the
6096             target calendar, and drop any of those that are not possible to
6097             represent.  If a value is provided, a new time coordinate will be
6098             created in the target calendar with the same frequency as the original
6099             time coordinate; for any dates that are not present in the source, the
6100             data will be filled with this value.  Note that using this mode requires
6101             that the source data have an inferable frequency; for more information
6102             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
6103             target calendar combinations, this could result in many missing values, see notes.
6104         use_cftime : boolean, optional
6105             Whether to use cftime objects in the output, only used if `calendar`
6106             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
6107             If True, the new time axis uses cftime objects.
6108             If None (default), it uses :py:class:`numpy.datetime64` values if the
6109             date range permits it, and :py:class:`cftime.datetime` objects if not.
6110             If False, it uses :py:class:`numpy.datetime64`  or fails.
6111 
6112         Returns
6113         -------
6114         DataArray
6115             Copy of the dataarray with the time coordinate converted to the
6116             target calendar. If 'missing' was None (default), invalid dates in
6117             the new calendar are dropped, but missing dates are not inserted.
6118             If `missing` was given, the new data is reindexed to have a time axis
6119             with the same frequency as the source, but in the new calendar; any
6120             missing datapoints are filled with `missing`.
6121 
6122         Notes
6123         -----
6124         Passing a value to `missing` is only usable if the source's time coordinate as an
6125         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
6126         if the target coordinate, generated from this frequency, has dates equivalent to the
6127         source. It is usually **not** appropriate to use this mode with:
6128 
6129         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
6130         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
6131             or 'mH' where 24 % m != 0).
6132 
6133         If one of the source or target calendars is `"360_day"`, `align_on` must
6134         be specified and two options are offered.
6135 
6136         - "year"
6137             The dates are translated according to their relative position in the year,
6138             ignoring their original month and day information, meaning that the
6139             missing/surplus days are added/removed at regular intervals.
6140 
6141             From a `360_day` to a standard calendar, the output will be missing the
6142             following dates (day of year in parentheses):
6143 
6144             To a leap year:
6145                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
6146                 September 31st (275) and November 30th (335).
6147             To a non-leap year:
6148                 February 6th (36), April 19th (109), July 2nd (183),
6149                 September 12th (255), November 25th (329).
6150 
6151             From a standard calendar to a `"360_day"`, the following dates in the
6152             source array will be dropped:
6153 
6154             From a leap year:
6155                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
6156                 September 31st (275), December 1st (336)
6157             From a non-leap year:
6158                 February 6th (37), April 20th (110), July 2nd (183),
6159                 September 13th (256), November 25th (329)
6160 
6161             This option is best used on daily and subdaily data.
6162 
6163         - "date"
6164             The month/day information is conserved and invalid dates are dropped
6165             from the output. This means that when converting from a `"360_day"` to a
6166             standard calendar, all 31st (Jan, March, May, July, August, October and
6167             December) will be missing as there is no equivalent dates in the
6168             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
6169             will be dropped as there are no equivalent dates in a standard calendar.
6170 
6171             This option is best used with data on a frequency coarser than daily.
6172         """
6173         return convert_calendar(
6174             self,
6175             calendar,
6176             dim=dim,
6177             align_on=align_on,
6178             missing=missing,
6179             use_cftime=use_cftime,
6180         )
6181 
6182     def interp_calendar(
6183         self,
6184         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
6185         dim: str = "time",
6186     ) -> DataArray:
6187         """Interpolates the DataArray to another calendar based on decimal year measure.
6188 
6189         Each timestamp in `source` and `target` are first converted to their decimal
6190         year equivalent then `source` is interpolated on the target coordinate.
6191         The decimal year of a timestamp is its year plus its sub-year component
6192         converted to the fraction of its year. For example "2000-03-01 12:00" is
6193         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
6194 
6195         This method should only be used when the time (HH:MM:SS) information of
6196         time coordinate is not important.
6197 
6198         Parameters
6199         ----------
6200         target: DataArray or DatetimeIndex or CFTimeIndex
6201             The target time coordinate of a valid dtype
6202             (np.datetime64 or cftime objects)
6203         dim : str
6204             The time coordinate name.
6205 
6206         Return
6207         ------
6208         DataArray
6209             The source interpolated on the decimal years of target,
6210         """
6211         return interp_calendar(self, target, dim=dim)
6212 
6213     def groupby(
6214         self,
6215         group: Hashable | DataArray | IndexVariable,
6216         squeeze: bool = True,
6217         restore_coord_dims: bool = False,
6218     ) -> DataArrayGroupBy:
6219         """Returns a DataArrayGroupBy object for performing grouped operations.
6220 
6221         Parameters
6222         ----------
6223         group : Hashable, DataArray or IndexVariable
6224             Array whose unique values should be used to group this array. If a
6225             Hashable, must be the name of a coordinate contained in this dataarray.
6226         squeeze : bool, default: True
6227             If "group" is a dimension of any arrays in this dataset, `squeeze`
6228             controls whether the subarrays have a dimension of length 1 along
6229             that dimension or if the dimension is squeezed out.
6230         restore_coord_dims : bool, default: False
6231             If True, also restore the dimension order of multi-dimensional
6232             coordinates.
6233 
6234         Returns
6235         -------
6236         grouped : DataArrayGroupBy
6237             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
6238             iterated over in the form of `(unique_value, grouped_array)` pairs.
6239 
6240         Examples
6241         --------
6242         Calculate daily anomalies for daily data:
6243 
6244         >>> da = xr.DataArray(
6245         ...     np.linspace(0, 1826, num=1827),
6246         ...     coords=[pd.date_range("2000-01-01", "2004-12-31", freq="D")],
6247         ...     dims="time",
6248         ... )
6249         >>> da
6250         <xarray.DataArray (time: 1827)>
6251         array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03,
6252                1.826e+03])
6253         Coordinates:
6254           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
6255         >>> da.groupby("time.dayofyear") - da.groupby("time.dayofyear").mean("time")
6256         <xarray.DataArray (time: 1827)>
6257         array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])
6258         Coordinates:
6259           * time       (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
6260             dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 359 360 361 362 363 364 365 366
6261 
6262         See Also
6263         --------
6264         DataArray.groupby_bins
6265         Dataset.groupby
6266         core.groupby.DataArrayGroupBy
6267         pandas.DataFrame.groupby
6268         """
6269         from xarray.core.groupby import DataArrayGroupBy
6270 
6271         # While we don't generally check the type of every arg, passing
6272         # multiple dimensions as multiple arguments is common enough, and the
6273         # consequences hidden enough (strings evaluate as true) to warrant
6274         # checking here.
6275         # A future version could make squeeze kwarg only, but would face
6276         # backward-compat issues.
6277         if not isinstance(squeeze, bool):
6278             raise TypeError(
6279                 f"`squeeze` must be True or False, but {squeeze} was supplied"
6280             )
6281 
6282         return DataArrayGroupBy(
6283             self, group, squeeze=squeeze, restore_coord_dims=restore_coord_dims
6284         )
6285 
6286     def groupby_bins(
6287         self,
6288         group: Hashable | DataArray | IndexVariable,
6289         bins: ArrayLike,
6290         right: bool = True,
6291         labels: ArrayLike | Literal[False] | None = None,
6292         precision: int = 3,
6293         include_lowest: bool = False,
6294         squeeze: bool = True,
6295         restore_coord_dims: bool = False,
6296     ) -> DataArrayGroupBy:
6297         """Returns a DataArrayGroupBy object for performing grouped operations.
6298 
6299         Rather than using all unique values of `group`, the values are discretized
6300         first by applying `pandas.cut` [1]_ to `group`.
6301 
6302         Parameters
6303         ----------
6304         group : Hashable, DataArray or IndexVariable
6305             Array whose binned values should be used to group this array. If a
6306             Hashable, must be the name of a coordinate contained in this dataarray.
6307         bins : int or array-like
6308             If bins is an int, it defines the number of equal-width bins in the
6309             range of x. However, in this case, the range of x is extended by .1%
6310             on each side to include the min or max values of x. If bins is a
6311             sequence it defines the bin edges allowing for non-uniform bin
6312             width. No extension of the range of x is done in this case.
6313         right : bool, default: True
6314             Indicates whether the bins include the rightmost edge or not. If
6315             right == True (the default), then the bins [1,2,3,4] indicate
6316             (1,2], (2,3], (3,4].
6317         labels : array-like, False or None, default: None
6318             Used as labels for the resulting bins. Must be of the same length as
6319             the resulting bins. If False, string bin labels are assigned by
6320             `pandas.cut`.
6321         precision : int, default: 3
6322             The precision at which to store and display the bins labels.
6323         include_lowest : bool, default: False
6324             Whether the first interval should be left-inclusive or not.
6325         squeeze : bool, default: True
6326             If "group" is a dimension of any arrays in this dataset, `squeeze`
6327             controls whether the subarrays have a dimension of length 1 along
6328             that dimension or if the dimension is squeezed out.
6329         restore_coord_dims : bool, default: False
6330             If True, also restore the dimension order of multi-dimensional
6331             coordinates.
6332 
6333         Returns
6334         -------
6335         grouped : DataArrayGroupBy
6336             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
6337             iterated over in the form of `(unique_value, grouped_array)` pairs.
6338             The name of the group has the added suffix `_bins` in order to
6339             distinguish it from the original variable.
6340 
6341         See Also
6342         --------
6343         DataArray.groupby
6344         Dataset.groupby_bins
6345         core.groupby.DataArrayGroupBy
6346         pandas.DataFrame.groupby
6347 
6348         References
6349         ----------
6350         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
6351         """
6352         from xarray.core.groupby import DataArrayGroupBy
6353 
6354         return DataArrayGroupBy(
6355             self,
6356             group,
6357             squeeze=squeeze,
6358             bins=bins,
6359             restore_coord_dims=restore_coord_dims,
6360             cut_kwargs={
6361                 "right": right,
6362                 "labels": labels,
6363                 "precision": precision,
6364                 "include_lowest": include_lowest,
6365             },
6366         )
6367 
6368     def weighted(self, weights: DataArray) -> DataArrayWeighted:
6369         """
6370         Weighted DataArray operations.
6371 
6372         Parameters
6373         ----------
6374         weights : DataArray
6375             An array of weights associated with the values in this Dataset.
6376             Each value in the data contributes to the reduction operation
6377             according to its associated weight.
6378 
6379         Notes
6380         -----
6381         ``weights`` must be a DataArray and cannot contain missing values.
6382         Missing values can be replaced by ``weights.fillna(0)``.
6383 
6384         Returns
6385         -------
6386         core.weighted.DataArrayWeighted
6387 
6388         See Also
6389         --------
6390         Dataset.weighted
6391         """
6392         from xarray.core.weighted import DataArrayWeighted
6393 
6394         return DataArrayWeighted(self, weights)
6395 
6396     def rolling(
6397         self,
6398         dim: Mapping[Any, int] | None = None,
6399         min_periods: int | None = None,
6400         center: bool | Mapping[Any, bool] = False,
6401         **window_kwargs: int,
6402     ) -> DataArrayRolling:
6403         """
6404         Rolling window object for DataArrays.
6405 
6406         Parameters
6407         ----------
6408         dim : dict, optional
6409             Mapping from the dimension name to create the rolling iterator
6410             along (e.g. `time`) to its moving window size.
6411         min_periods : int or None, default: None
6412             Minimum number of observations in window required to have a value
6413             (otherwise result is NA). The default, None, is equivalent to
6414             setting min_periods equal to the size of the window.
6415         center : bool or Mapping to int, default: False
6416             Set the labels at the center of the window.
6417         **window_kwargs : optional
6418             The keyword arguments form of ``dim``.
6419             One of dim or window_kwargs must be provided.
6420 
6421         Returns
6422         -------
6423         core.rolling.DataArrayRolling
6424 
6425         Examples
6426         --------
6427         Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:
6428 
6429         >>> da = xr.DataArray(
6430         ...     np.linspace(0, 11, num=12),
6431         ...     coords=[
6432         ...         pd.date_range(
6433         ...             "1999-12-15",
6434         ...             periods=12,
6435         ...             freq=pd.DateOffset(months=1),
6436         ...         )
6437         ...     ],
6438         ...     dims="time",
6439         ... )
6440         >>> da
6441         <xarray.DataArray (time: 12)>
6442         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
6443         Coordinates:
6444           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6445         >>> da.rolling(time=3, center=True).mean()
6446         <xarray.DataArray (time: 12)>
6447         array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])
6448         Coordinates:
6449           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6450 
6451         Remove the NaNs using ``dropna()``:
6452 
6453         >>> da.rolling(time=3, center=True).mean().dropna("time")
6454         <xarray.DataArray (time: 10)>
6455         array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])
6456         Coordinates:
6457           * time     (time) datetime64[ns] 2000-01-15 2000-02-15 ... 2000-10-15
6458 
6459         See Also
6460         --------
6461         core.rolling.DataArrayRolling
6462         Dataset.rolling
6463         """
6464         from xarray.core.rolling import DataArrayRolling
6465 
6466         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
6467         return DataArrayRolling(self, dim, min_periods=min_periods, center=center)
6468 
6469     def coarsen(
6470         self,
6471         dim: Mapping[Any, int] | None = None,
6472         boundary: CoarsenBoundaryOptions = "exact",
6473         side: SideOptions | Mapping[Any, SideOptions] = "left",
6474         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
6475         **window_kwargs: int,
6476     ) -> DataArrayCoarsen:
6477         """
6478         Coarsen object for DataArrays.
6479 
6480         Parameters
6481         ----------
6482         dim : mapping of hashable to int, optional
6483             Mapping from the dimension name to the window size.
6484         boundary : {"exact", "trim", "pad"}, default: "exact"
6485             If 'exact', a ValueError will be raised if dimension size is not a
6486             multiple of the window size. If 'trim', the excess entries are
6487             dropped. If 'pad', NA will be padded.
6488         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
6489         coord_func : str or mapping of hashable to str, default: "mean"
6490             function (name) that is applied to the coordinates,
6491             or a mapping from coordinate name to function (name).
6492 
6493         Returns
6494         -------
6495         core.rolling.DataArrayCoarsen
6496 
6497         Examples
6498         --------
6499         Coarsen the long time series by averaging over every four days.
6500 
6501         >>> da = xr.DataArray(
6502         ...     np.linspace(0, 364, num=364),
6503         ...     dims="time",
6504         ...     coords={"time": pd.date_range("1999-12-15", periods=364)},
6505         ... )
6506         >>> da  # +doctest: ELLIPSIS
6507         <xarray.DataArray (time: 364)>
6508         array([  0.        ,   1.00275482,   2.00550964,   3.00826446,
6509                  4.01101928,   5.0137741 ,   6.01652893,   7.01928375,
6510                  8.02203857,   9.02479339,  10.02754821,  11.03030303,
6511         ...
6512                356.98071625, 357.98347107, 358.9862259 , 359.98898072,
6513                360.99173554, 361.99449036, 362.99724518, 364.        ])
6514         Coordinates:
6515           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-12-12
6516         >>> da.coarsen(time=3, boundary="trim").mean()  # +doctest: ELLIPSIS
6517         <xarray.DataArray (time: 121)>
6518         array([  1.00275482,   4.01101928,   7.01928375,  10.02754821,
6519                 13.03581267,  16.04407713,  19.0523416 ,  22.06060606,
6520                 25.06887052,  28.07713499,  31.08539945,  34.09366391,
6521         ...
6522                349.96143251, 352.96969697, 355.97796143, 358.9862259 ,
6523                361.99449036])
6524         Coordinates:
6525           * time     (time) datetime64[ns] 1999-12-16 1999-12-19 ... 2000-12-10
6526         >>>
6527 
6528         See Also
6529         --------
6530         core.rolling.DataArrayCoarsen
6531         Dataset.coarsen
6532         """
6533         from xarray.core.rolling import DataArrayCoarsen
6534 
6535         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
6536         return DataArrayCoarsen(
6537             self,
6538             dim,
6539             boundary=boundary,
6540             side=side,
6541             coord_func=coord_func,
6542         )
6543 
6544     def resample(
6545         self,
6546         indexer: Mapping[Any, str] | None = None,
6547         skipna: bool | None = None,
6548         closed: SideOptions | None = None,
6549         label: SideOptions | None = None,
6550         base: int | None = None,
6551         offset: pd.Timedelta | datetime.timedelta | str | None = None,
6552         origin: str | DatetimeLike = "start_day",
6553         keep_attrs: bool | None = None,
6554         loffset: datetime.timedelta | str | None = None,
6555         restore_coord_dims: bool | None = None,
6556         **indexer_kwargs: str,
6557     ) -> DataArrayResample:
6558         """Returns a Resample object for performing resampling operations.
6559 
6560         Handles both downsampling and upsampling. The resampled
6561         dimension must be a datetime-like coordinate. If any intervals
6562         contain no values from the original object, they will be given
6563         the value ``NaN``.
6564 
6565         Parameters
6566         ----------
6567         indexer : Mapping of Hashable to str, optional
6568             Mapping from the dimension name to resample frequency [1]_. The
6569             dimension must be datetime-like.
6570         skipna : bool, optional
6571             Whether to skip missing values when aggregating in downsampling.
6572         closed : {"left", "right"}, optional
6573             Side of each interval to treat as closed.
6574         label : {"left", "right"}, optional
6575             Side of each interval to use for labeling.
6576         base : int, optional
6577             For frequencies that evenly subdivide 1 day, the "origin" of the
6578             aggregated intervals. For example, for "24H" frequency, base could
6579             range from 0 through 23.
6580         origin : {'epoch', 'start', 'start_day', 'end', 'end_day'}, pd.Timestamp, datetime.datetime, np.datetime64, or cftime.datetime, default 'start_day'
6581             The datetime on which to adjust the grouping. The timezone of origin
6582             must match the timezone of the index.
6583 
6584             If a datetime is not used, these values are also supported:
6585             - 'epoch': `origin` is 1970-01-01
6586             - 'start': `origin` is the first value of the timeseries
6587             - 'start_day': `origin` is the first day at midnight of the timeseries
6588             - 'end': `origin` is the last value of the timeseries
6589             - 'end_day': `origin` is the ceiling midnight of the last day
6590         offset : pd.Timedelta, datetime.timedelta, or str, default is None
6591             An offset timedelta added to the origin.
6592         loffset : timedelta or str, optional
6593             Offset used to adjust the resampled time labels. Some pandas date
6594             offset strings are supported.
6595         restore_coord_dims : bool, optional
6596             If True, also restore the dimension order of multi-dimensional
6597             coordinates.
6598         **indexer_kwargs : str
6599             The keyword arguments form of ``indexer``.
6600             One of indexer or indexer_kwargs must be provided.
6601 
6602         Returns
6603         -------
6604         resampled : core.resample.DataArrayResample
6605             This object resampled.
6606 
6607         Examples
6608         --------
6609         Downsample monthly time-series data to seasonal data:
6610 
6611         >>> da = xr.DataArray(
6612         ...     np.linspace(0, 11, num=12),
6613         ...     coords=[
6614         ...         pd.date_range(
6615         ...             "1999-12-15",
6616         ...             periods=12,
6617         ...             freq=pd.DateOffset(months=1),
6618         ...         )
6619         ...     ],
6620         ...     dims="time",
6621         ... )
6622         >>> da
6623         <xarray.DataArray (time: 12)>
6624         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
6625         Coordinates:
6626           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6627         >>> da.resample(time="QS-DEC").mean()
6628         <xarray.DataArray (time: 4)>
6629         array([ 1.,  4.,  7., 10.])
6630         Coordinates:
6631           * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01
6632 
6633         Upsample monthly time-series data to daily data:
6634 
6635         >>> da.resample(time="1D").interpolate("linear")  # +doctest: ELLIPSIS
6636         <xarray.DataArray (time: 337)>
6637         array([ 0.        ,  0.03225806,  0.06451613,  0.09677419,  0.12903226,
6638                 0.16129032,  0.19354839,  0.22580645,  0.25806452,  0.29032258,
6639                 0.32258065,  0.35483871,  0.38709677,  0.41935484,  0.4516129 ,
6640         ...
6641                10.80645161, 10.83870968, 10.87096774, 10.90322581, 10.93548387,
6642                10.96774194, 11.        ])
6643         Coordinates:
6644           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
6645 
6646         Limit scope of upsampling method
6647 
6648         >>> da.resample(time="1D").nearest(tolerance="1D")
6649         <xarray.DataArray (time: 337)>
6650         array([ 0.,  0., nan, ..., nan, 11., 11.])
6651         Coordinates:
6652           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
6653 
6654         See Also
6655         --------
6656         Dataset.resample
6657         pandas.Series.resample
6658         pandas.DataFrame.resample
6659 
6660         References
6661         ----------
6662         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
6663         """
6664         from xarray.core.resample import DataArrayResample
6665 
6666         return self._resample(
6667             resample_cls=DataArrayResample,
6668             indexer=indexer,
6669             skipna=skipna,
6670             closed=closed,
6671             label=label,
6672             base=base,
6673             offset=offset,
6674             origin=origin,
6675             keep_attrs=keep_attrs,
6676             loffset=loffset,
6677             restore_coord_dims=restore_coord_dims,
6678             **indexer_kwargs,
6679         )
6680 
6681     # this needs to be at the end, or mypy will confuse with `str`
6682     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
6683     str = utils.UncachedAccessor(StringAccessor["DataArray"])

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 04:22:23.948 | INFO     | Search completed. Bug locations: [<file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>stack</method>
<code>
4554     def stack(
4555         self: T_Dataset,
4556         dimensions: Mapping[Any, Sequence[Hashable | ellipsis]] | None = None,
4557         create_index: bool | None = True,
4558         index_cls: type[Index] = PandasMultiIndex,
4559         **dimensions_kwargs: Sequence[Hashable | ellipsis],
4560     ) -> T_Dataset:
4561         """
4562         Stack any number of existing dimensions into a single new dimension.
4563 
4564         New dimensions will be added at the end, and by default the corresponding
4565         coordinate variables will be combined into a MultiIndex.
4566 
4567         Parameters
4568         ----------
4569         dimensions : mapping of hashable to sequence of hashable
4570             Mapping of the form `new_name=(dim1, dim2, ...)`. Names of new
4571             dimensions, and the existing dimensions that they replace. An
4572             ellipsis (`...`) will be replaced by all unlisted dimensions.
4573             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
4574             all dimensions.
4575         create_index : bool or None, default: True
4576 
4577             - True: create a multi-index for each of the stacked dimensions.
4578             - False: don't create any index.
4579             - None. create a multi-index only if exactly one single (1-d) coordinate
4580               index is found for every dimension to stack.
4581 
4582         index_cls: Index-class, default: PandasMultiIndex
4583             Can be used to pass a custom multi-index type (must be an Xarray index that
4584             implements `.stack()`). By default, a pandas multi-index wrapper is used.
4585         **dimensions_kwargs
4586             The keyword arguments form of ``dimensions``.
4587             One of dimensions or dimensions_kwargs must be provided.
4588 
4589         Returns
4590         -------
4591         stacked : Dataset
4592             Dataset with stacked data.
4593 
4594         See Also
4595         --------
4596         Dataset.unstack
4597         """
4598         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
4599         result = self
4600         for new_dim, dims in dimensions.items():
4601             result = result._stack_once(dims, new_dim, index_cls, create_index)
4602         return result

</code><intended_behavior>The `stack` method should ensure that the dtype of coordinates remains unchanged after the operation completes. It should explicitly check the dtype of coordinates before the stacking operation and preserve those dtypes throughout the process, possibly by implementing a check or conversion routine that verifies the dtype post-operation against the original dtypes, reverting to the original dtype if a change is detected.</intended_behavior>, <file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>None</method>
<code>
440 class Dataset(
441     DataWithCoords,
442     DatasetAggregations,
443     DatasetArithmetic,
444     Mapping[Hashable, "DataArray"],
445 ):
446     """A multi-dimensional, in memory, array database.
447 
448     A dataset resembles an in-memory representation of a NetCDF file,
449     and consists of variables, coordinates and attributes which
450     together form a self describing dataset.
451 
452     Dataset implements the mapping interface with keys given by variable
453     names and values given by DataArray objects for each variable name.
454 
455     One dimensional variables with name equal to their dimension are
456     index coordinates used for label based indexing.
457 
458     To load data from a file or file-like object, use the `open_dataset`
459     function.
460 
461     Parameters
462     ----------
463     data_vars : dict-like, optional
464         A mapping from variable names to :py:class:`~xarray.DataArray`
465         objects, :py:class:`~xarray.Variable` objects or to tuples of
466         the form ``(dims, data[, attrs])`` which can be used as
467         arguments to create a new ``Variable``. Each dimension must
468         have the same length in all variables in which it appears.
469 
470         The following notations are accepted:
471 
472         - mapping {var name: DataArray}
473         - mapping {var name: Variable}
474         - mapping {var name: (dimension name, array-like)}
475         - mapping {var name: (tuple of dimension names, array-like)}
476         - mapping {dimension name: array-like}
477           (it will be automatically moved to coords, see below)
478 
479         Each dimension must have the same length in all variables in
480         which it appears.
481     coords : dict-like, optional
482         Another mapping in similar form as the `data_vars` argument,
483         except the each item is saved on the dataset as a "coordinate".
484         These variables have an associated meaning: they describe
485         constant/fixed/independent quantities, unlike the
486         varying/measured/dependent quantities that belong in
487         `variables`. Coordinates values may be given by 1-dimensional
488         arrays or scalars, in which case `dims` do not need to be
489         supplied: 1D arrays will be assumed to give index values along
490         the dimension with the same name.
491 
492         The following notations are accepted:
493 
494         - mapping {coord name: DataArray}
495         - mapping {coord name: Variable}
496         - mapping {coord name: (dimension name, array-like)}
497         - mapping {coord name: (tuple of dimension names, array-like)}
498         - mapping {dimension name: array-like}
499           (the dimension name is implicitly set to be the same as the
500           coord name)
501 
502         The last notation implies that the coord name is the same as
503         the dimension name.
504 
505     attrs : dict-like, optional
506         Global attributes to save on this dataset.
507 
508     Examples
509     --------
510     Create data:
511 
512     >>> np.random.seed(0)
513     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
514     >>> precipitation = 10 * np.random.rand(2, 2, 3)
515     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
516     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
517     >>> time = pd.date_range("2014-09-06", periods=3)
518     >>> reference_time = pd.Timestamp("2014-09-05")
519 
520     Initialize a dataset with multiple dimensions:
521 
522     >>> ds = xr.Dataset(
523     ...     data_vars=dict(
524     ...         temperature=(["x", "y", "time"], temperature),
525     ...         precipitation=(["x", "y", "time"], precipitation),
526     ...     ),
527     ...     coords=dict(
528     ...         lon=(["x", "y"], lon),
529     ...         lat=(["x", "y"], lat),
530     ...         time=time,
531     ...         reference_time=reference_time,
532     ...     ),
533     ...     attrs=dict(description="Weather related data."),
534     ... )
535     >>> ds
536     <xarray.Dataset>
537     Dimensions:         (x: 2, y: 2, time: 3)
538     Coordinates:
539         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
540         lat             (x, y) float64 42.25 42.21 42.63 42.59
541       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
542         reference_time  datetime64[ns] 2014-09-05
543     Dimensions without coordinates: x, y
544     Data variables:
545         temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
546         precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
547     Attributes:
548         description:  Weather related data.
549 
550     Find out where the coldest temperature was and what values the
551     other variables had:
552 
553     >>> ds.isel(ds.temperature.argmin(...))
554     <xarray.Dataset>
555     Dimensions:         ()
556     Coordinates:
557         lon             float64 -99.32
558         lat             float64 42.21
559         time            datetime64[ns] 2014-09-08
560         reference_time  datetime64[ns] 2014-09-05
561     Data variables:
562         temperature     float64 7.182
563         precipitation   float64 8.326
564     Attributes:
565         description:  Weather related data.
566     """
567 
568     _attrs: dict[Hashable, Any] | None
569     _cache: dict[str, Any]
570     _coord_names: set[Hashable]
571     _dims: dict[Hashable, int]
572     _encoding: dict[Hashable, Any] | None
573     _close: Callable[[], None] | None
574     _indexes: dict[Hashable, Index]
575     _variables: dict[Hashable, Variable]
576 
577     __slots__ = (
578         "_attrs",
579         "_cache",
580         "_coord_names",
581         "_dims",
582         "_encoding",
583         "_close",
584         "_indexes",
585         "_variables",
586         "__weakref__",
587     )
588 
589     def __init__(
590         self,
591         # could make a VariableArgs to use more generally, and refine these
592         # categories
593         data_vars: Mapping[Any, Any] | None = None,
594         coords: Mapping[Any, Any] | None = None,
595         attrs: Mapping[Any, Any] | None = None,
596     ) -> None:
597         # TODO(shoyer): expose indexes as a public argument in __init__
598 
599         if data_vars is None:
600             data_vars = {}
601         if coords is None:
602             coords = {}
603 
604         both_data_and_coords = set(data_vars) & set(coords)
605         if both_data_and_coords:
606             raise ValueError(
607                 f"variables {both_data_and_coords!r} are found in both data_vars and coords"
608             )
609 
610         if isinstance(coords, Dataset):
611             coords = coords.variables
612 
613         variables, coord_names, dims, indexes, _ = merge_data_and_coords(
614             data_vars, coords, compat="broadcast_equals"
615         )
616 
617         self._attrs = dict(attrs) if attrs is not None else None
618         self._close = None
619         self._encoding = None
620         self._variables = variables
621         self._coord_names = coord_names
622         self._dims = dims
623         self._indexes = indexes
624 
625     @classmethod
626     def load_store(cls: type[T_Dataset], store, decoder=None) -> T_Dataset:
627         """Create a new dataset from the contents of a backends.*DataStore
628         object
629         """
630         variables, attributes = store.load()
631         if decoder:
632             variables, attributes = decoder(variables, attributes)
633         obj = cls(variables, attrs=attributes)
634         obj.set_close(store.close)
635         return obj
636 
637     @property
638     def variables(self) -> Frozen[Hashable, Variable]:
639         """Low level interface to Dataset contents as dict of Variable objects.
640 
641         This ordered dictionary is frozen to prevent mutation that could
642         violate Dataset invariants. It contains all variable objects
643         constituting the Dataset, including both data variables and
644         coordinates.
645         """
646         return Frozen(self._variables)
647 
648     @property
649     def attrs(self) -> dict[Any, Any]:
650         """Dictionary of global attributes on this dataset"""
651         if self._attrs is None:
652             self._attrs = {}
653         return self._attrs
654 
655     @attrs.setter
656     def attrs(self, value: Mapping[Any, Any]) -> None:
657         self._attrs = dict(value)
658 
659     @property
660     def encoding(self) -> dict[Any, Any]:
661         """Dictionary of global encoding attributes on this dataset"""
662         if self._encoding is None:
663             self._encoding = {}
664         return self._encoding
665 
666     @encoding.setter
667     def encoding(self, value: Mapping[Any, Any]) -> None:
668         self._encoding = dict(value)
669 
670     @property
671     def dims(self) -> Frozen[Hashable, int]:
672         """Mapping from dimension names to lengths.
673 
674         Cannot be modified directly, but is updated when adding new variables.
675 
676         Note that type of this object differs from `DataArray.dims`.
677         See `Dataset.sizes` and `DataArray.sizes` for consistently named
678         properties.
679 
680         See Also
681         --------
682         Dataset.sizes
683         DataArray.dims
684         """
685         return Frozen(self._dims)
686 
687     @property
688     def sizes(self) -> Frozen[Hashable, int]:
689         """Mapping from dimension names to lengths.
690 
691         Cannot be modified directly, but is updated when adding new variables.
692 
693         This is an alias for `Dataset.dims` provided for the benefit of
694         consistency with `DataArray.sizes`.
695 
696         See Also
697         --------
698         DataArray.sizes
699         """
700         return self.dims
701 
702     @property
703     def dtypes(self) -> Frozen[Hashable, np.dtype]:
704         """Mapping from data variable names to dtypes.
705 
706         Cannot be modified directly, but is updated when adding new variables.
707 
708         See Also
709         --------
710         DataArray.dtype
711         """
712         return Frozen(
713             {
714                 n: v.dtype
715                 for n, v in self._variables.items()
716                 if n not in self._coord_names
717             }
718         )
719 
720     def load(self: T_Dataset, **kwargs) -> T_Dataset:
721         """Manually trigger loading and/or computation of this dataset's data
722         from disk or a remote source into memory and return this dataset.
723         Unlike compute, the original dataset is modified and returned.
724 
725         Normally, it should not be necessary to call this method in user code,
726         because all xarray functions should either work on deferred data or
727         load data automatically. However, this method can be necessary when
728         working with many file objects on disk.
729 
730         Parameters
731         ----------
732         **kwargs : dict
733             Additional keyword arguments passed on to ``dask.compute``.
734 
735         See Also
736         --------
737         dask.compute
738         """
739         # access .data to coerce everything to numpy or dask arrays
740         lazy_data = {
741             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
742         }
743         if lazy_data:
744             import dask.array as da
745 
746             # evaluate all the dask arrays simultaneously
747             evaluated_data = da.compute(*lazy_data.values(), **kwargs)
748 
749             for k, data in zip(lazy_data, evaluated_data):
750                 self.variables[k].data = data
751 
752         # load everything else sequentially
753         for k, v in self.variables.items():
754             if k not in lazy_data:
755                 v.load()
756 
757         return self
758 
759     def __dask_tokenize__(self):
760         from dask.base import normalize_token
761 
762         return normalize_token(
763             (type(self), self._variables, self._coord_names, self._attrs)
764         )
765 
766     def __dask_graph__(self):
767         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}
768         graphs = {k: v for k, v in graphs.items() if v is not None}
769         if not graphs:
770             return None
771         else:
772             try:
773                 from dask.highlevelgraph import HighLevelGraph
774 
775                 return HighLevelGraph.merge(*graphs.values())
776             except ImportError:
777                 from dask import sharedict
778 
779                 return sharedict.merge(*graphs.values())
780 
781     def __dask_keys__(self):
782         import dask
783 
784         return [
785             v.__dask_keys__()
786             for v in self.variables.values()
787             if dask.is_dask_collection(v)
788         ]
789 
790     def __dask_layers__(self):
791         import dask
792 
793         return sum(
794             (
795                 v.__dask_layers__()
796                 for v in self.variables.values()
797                 if dask.is_dask_collection(v)
798             ),
799             (),
800         )
801 
802     @property
803     def __dask_optimize__(self):
804         import dask.array as da
805 
806         return da.Array.__dask_optimize__
807 
808     @property
809     def __dask_scheduler__(self):
810         import dask.array as da
811 
812         return da.Array.__dask_scheduler__
813 
814     def __dask_postcompute__(self):
815         return self._dask_postcompute, ()
816 
817     def __dask_postpersist__(self):
818         return self._dask_postpersist, ()
819 
820     def _dask_postcompute(self: T_Dataset, results: Iterable[Variable]) -> T_Dataset:
821         import dask
822 
823         variables = {}
824         results_iter = iter(results)
825 
826         for k, v in self._variables.items():
827             if dask.is_dask_collection(v):
828                 rebuild, args = v.__dask_postcompute__()
829                 v = rebuild(next(results_iter), *args)
830             variables[k] = v
831 
832         return type(self)._construct_direct(
833             variables,
834             self._coord_names,
835             self._dims,
836             self._attrs,
837             self._indexes,
838             self._encoding,
839             self._close,
840         )
841 
842     def _dask_postpersist(
843         self: T_Dataset, dsk: Mapping, *, rename: Mapping[str, str] | None = None
844     ) -> T_Dataset:
845         from dask import is_dask_collection
846         from dask.highlevelgraph import HighLevelGraph
847         from dask.optimization import cull
848 
849         variables = {}
850 
851         for k, v in self._variables.items():
852             if not is_dask_collection(v):
853                 variables[k] = v
854                 continue
855 
856             if isinstance(dsk, HighLevelGraph):
857                 # dask >= 2021.3
858                 # __dask_postpersist__() was called by dask.highlevelgraph.
859                 # Don't use dsk.cull(), as we need to prevent partial layers:
860                 # https://github.com/dask/dask/issues/7137
861                 layers = v.__dask_layers__()
862                 if rename:
863                     layers = [rename.get(k, k) for k in layers]
864                 dsk2 = dsk.cull_layers(layers)
865             elif rename:  # pragma: nocover
866                 # At the moment of writing, this is only for forward compatibility.
867                 # replace_name_in_key requires dask >= 2021.3.
868                 from dask.base import flatten, replace_name_in_key
869 
870                 keys = [
871                     replace_name_in_key(k, rename) for k in flatten(v.__dask_keys__())
872                 ]
873                 dsk2, _ = cull(dsk, keys)
874             else:
875                 # __dask_postpersist__() was called by dask.optimize or dask.persist
876                 dsk2, _ = cull(dsk, v.__dask_keys__())
877 
878             rebuild, args = v.__dask_postpersist__()
879             # rename was added in dask 2021.3
880             kwargs = {"rename": rename} if rename else {}
881             variables[k] = rebuild(dsk2, *args, **kwargs)
882 
883         return type(self)._construct_direct(
884             variables,
885             self._coord_names,
886             self._dims,
887             self._attrs,
888             self._indexes,
889             self._encoding,
890             self._close,
891         )
892 
893     def compute(self: T_Dataset, **kwargs) -> T_Dataset:
894         """Manually trigger loading and/or computation of this dataset's data
895         from disk or a remote source into memory and return a new dataset.
896         Unlike load, the original dataset is left unaltered.
897 
898         Normally, it should not be necessary to call this method in user code,
899         because all xarray functions should either work on deferred data or
900         load data automatically. However, this method can be necessary when
901         working with many file objects on disk.
902 
903         Parameters
904         ----------
905         **kwargs : dict
906             Additional keyword arguments passed on to ``dask.compute``.
907 
908         See Also
909         --------
910         dask.compute
911         """
912         new = self.copy(deep=False)
913         return new.load(**kwargs)
914 
915     def _persist_inplace(self: T_Dataset, **kwargs) -> T_Dataset:
916         """Persist all Dask arrays in memory"""
917         # access .data to coerce everything to numpy or dask arrays
918         lazy_data = {
919             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
920         }
921         if lazy_data:
922             import dask
923 
924             # evaluate all the dask arrays simultaneously
925             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)
926 
927             for k, data in zip(lazy_data, evaluated_data):
928                 self.variables[k].data = data
929 
930         return self
931 
932     def persist(self: T_Dataset, **kwargs) -> T_Dataset:
933         """Trigger computation, keeping data as dask arrays
934 
935         This operation can be used to trigger computation on underlying dask
936         arrays, similar to ``.compute()`` or ``.load()``.  However this
937         operation keeps the data as dask arrays. This is particularly useful
938         when using the dask.distributed scheduler and you want to load a large
939         amount of data into distributed memory.
940 
941         Parameters
942         ----------
943         **kwargs : dict
944             Additional keyword arguments passed on to ``dask.persist``.
945 
946         See Also
947         --------
948         dask.persist
949         """
950         new = self.copy(deep=False)
951         return new._persist_inplace(**kwargs)
952 
953     @classmethod
954     def _construct_direct(
955         cls: type[T_Dataset],
956         variables: dict[Any, Variable],
957         coord_names: set[Hashable],
958         dims: dict[Any, int] | None = None,
959         attrs: dict | None = None,
960         indexes: dict[Any, Index] | None = None,
961         encoding: dict | None = None,
962         close: Callable[[], None] | None = None,
963     ) -> T_Dataset:
964         """Shortcut around __init__ for internal use when we want to skip
965         costly validation
966         """
967         if dims is None:
968             dims = calculate_dimensions(variables)
969         if indexes is None:
970             indexes = {}
971         obj = object.__new__(cls)
972         obj._variables = variables
973         obj._coord_names = coord_names
974         obj._dims = dims
975         obj._indexes = indexes
976         obj._attrs = attrs
977         obj._close = close
978         obj._encoding = encoding
979         return obj
980 
981     def _replace(
982         self: T_Dataset,
983         variables: dict[Hashable, Variable] | None = None,
984         coord_names: set[Hashable] | None = None,
985         dims: dict[Any, int] | None = None,
986         attrs: dict[Hashable, Any] | None | Default = _default,
987         indexes: dict[Hashable, Index] | None = None,
988         encoding: dict | None | Default = _default,
989         inplace: bool = False,
990     ) -> T_Dataset:
991         """Fastpath constructor for internal use.
992 
993         Returns an object with optionally with replaced attributes.
994 
995         Explicitly passed arguments are *not* copied when placed on the new
996         dataset. It is up to the caller to ensure that they have the right type
997         and are not used elsewhere.
998         """
999         if inplace:
1000             if variables is not None:
1001                 self._variables = variables
1002             if coord_names is not None:
1003                 self._coord_names = coord_names
1004             if dims is not None:
1005                 self._dims = dims
1006             if attrs is not _default:
1007                 self._attrs = attrs
1008             if indexes is not None:
1009                 self._indexes = indexes
1010             if encoding is not _default:
1011                 self._encoding = encoding
1012             obj = self
1013         else:
1014             if variables is None:
1015                 variables = self._variables.copy()
1016             if coord_names is None:
1017                 coord_names = self._coord_names.copy()
1018             if dims is None:
1019                 dims = self._dims.copy()
1020             if attrs is _default:
1021                 attrs = copy.copy(self._attrs)
1022             if indexes is None:
1023                 indexes = self._indexes.copy()
1024             if encoding is _default:
1025                 encoding = copy.copy(self._encoding)
1026             obj = self._construct_direct(
1027                 variables, coord_names, dims, attrs, indexes, encoding
1028             )
1029         return obj
1030 
1031     def _replace_with_new_dims(
1032         self: T_Dataset,
1033         variables: dict[Hashable, Variable],
1034         coord_names: set | None = None,
1035         attrs: dict[Hashable, Any] | None | Default = _default,
1036         indexes: dict[Hashable, Index] | None = None,
1037         inplace: bool = False,
1038     ) -> T_Dataset:
1039         """Replace variables with recalculated dimensions."""
1040         dims = calculate_dimensions(variables)
1041         return self._replace(
1042             variables, coord_names, dims, attrs, indexes, inplace=inplace
1043         )
1044 
1045     def _replace_vars_and_dims(
1046         self: T_Dataset,
1047         variables: dict[Hashable, Variable],
1048         coord_names: set | None = None,
1049         dims: dict[Hashable, int] | None = None,
1050         attrs: dict[Hashable, Any] | None | Default = _default,
1051         inplace: bool = False,
1052     ) -> T_Dataset:
1053         """Deprecated version of _replace_with_new_dims().
1054 
1055         Unlike _replace_with_new_dims(), this method always recalculates
1056         indexes from variables.
1057         """
1058         if dims is None:
1059             dims = calculate_dimensions(variables)
1060         return self._replace(
1061             variables, coord_names, dims, attrs, indexes=None, inplace=inplace
1062         )
1063 
1064     def _overwrite_indexes(
1065         self: T_Dataset,
1066         indexes: Mapping[Hashable, Index],
1067         variables: Mapping[Hashable, Variable] | None = None,
1068         drop_variables: list[Hashable] | None = None,
1069         drop_indexes: list[Hashable] | None = None,
1070         rename_dims: Mapping[Hashable, Hashable] | None = None,
1071     ) -> T_Dataset:
1072         """Maybe replace indexes.
1073 
1074         This function may do a lot more depending on index query
1075         results.
1076 
1077         """
1078         if not indexes:
1079             return self
1080 
1081         if variables is None:
1082             variables = {}
1083         if drop_variables is None:
1084             drop_variables = []
1085         if drop_indexes is None:
1086             drop_indexes = []
1087 
1088         new_variables = self._variables.copy()
1089         new_coord_names = self._coord_names.copy()
1090         new_indexes = dict(self._indexes)
1091 
1092         index_variables = {}
1093         no_index_variables = {}
1094         for name, var in variables.items():
1095             old_var = self._variables.get(name)
1096             if old_var is not None:
1097                 var.attrs.update(old_var.attrs)
1098                 var.encoding.update(old_var.encoding)
1099             if name in indexes:
1100                 index_variables[name] = var
1101             else:
1102                 no_index_variables[name] = var
1103 
1104         for name in indexes:
1105             new_indexes[name] = indexes[name]
1106 
1107         for name, var in index_variables.items():
1108             new_coord_names.add(name)
1109             new_variables[name] = var
1110 
1111         # append no-index variables at the end
1112         for k in no_index_variables:
1113             new_variables.pop(k)
1114         new_variables.update(no_index_variables)
1115 
1116         for name in drop_indexes:
1117             new_indexes.pop(name)
1118 
1119         for name in drop_variables:
1120             new_variables.pop(name)
1121             new_indexes.pop(name, None)
1122             new_coord_names.remove(name)
1123 
1124         replaced = self._replace(
1125             variables=new_variables, coord_names=new_coord_names, indexes=new_indexes
1126         )
1127 
1128         if rename_dims:
1129             # skip rename indexes: they should already have the right name(s)
1130             dims = replaced._rename_dims(rename_dims)
1131             new_variables, new_coord_names = replaced._rename_vars({}, rename_dims)
1132             return replaced._replace(
1133                 variables=new_variables, coord_names=new_coord_names, dims=dims
1134             )
1135         else:
1136             return replaced
1137 
1138     def copy(
1139         self: T_Dataset, deep: bool = False, data: Mapping[Any, ArrayLike] | None = None
1140     ) -> T_Dataset:
1141         """Returns a copy of this dataset.
1142 
1143         If `deep=True`, a deep copy is made of each of the component variables.
1144         Otherwise, a shallow copy of each of the component variable is made, so
1145         that the underlying memory region of the new dataset is the same as in
1146         the original dataset.
1147 
1148         Use `data` to create a new object with the same structure as
1149         original but entirely new data.
1150 
1151         Parameters
1152         ----------
1153         deep : bool, default: False
1154             Whether each component variable is loaded into memory and copied onto
1155             the new object. Default is False.
1156         data : dict-like or None, optional
1157             Data to use in the new object. Each item in `data` must have same
1158             shape as corresponding data variable in original. When `data` is
1159             used, `deep` is ignored for the data variables and only used for
1160             coords.
1161 
1162         Returns
1163         -------
1164         object : Dataset
1165             New object with dimensions, attributes, coordinates, name, encoding,
1166             and optionally data copied from original.
1167 
1168         Examples
1169         --------
1170         Shallow copy versus deep copy
1171 
1172         >>> da = xr.DataArray(np.random.randn(2, 3))
1173         >>> ds = xr.Dataset(
1174         ...     {"foo": da, "bar": ("x", [-1, 2])},
1175         ...     coords={"x": ["one", "two"]},
1176         ... )
1177         >>> ds.copy()
1178         <xarray.Dataset>
1179         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1180         Coordinates:
1181           * x        (x) <U3 'one' 'two'
1182         Dimensions without coordinates: dim_0, dim_1
1183         Data variables:
1184             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
1185             bar      (x) int64 -1 2
1186 
1187         >>> ds_0 = ds.copy(deep=False)
1188         >>> ds_0["foo"][0, 0] = 7
1189         >>> ds_0
1190         <xarray.Dataset>
1191         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1192         Coordinates:
1193           * x        (x) <U3 'one' 'two'
1194         Dimensions without coordinates: dim_0, dim_1
1195         Data variables:
1196             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1197             bar      (x) int64 -1 2
1198 
1199         >>> ds
1200         <xarray.Dataset>
1201         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1202         Coordinates:
1203           * x        (x) <U3 'one' 'two'
1204         Dimensions without coordinates: dim_0, dim_1
1205         Data variables:
1206             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1207             bar      (x) int64 -1 2
1208 
1209         Changing the data using the ``data`` argument maintains the
1210         structure of the original object, but with the new data. Original
1211         object is unaffected.
1212 
1213         >>> ds.copy(data={"foo": np.arange(6).reshape(2, 3), "bar": ["a", "b"]})
1214         <xarray.Dataset>
1215         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1216         Coordinates:
1217           * x        (x) <U3 'one' 'two'
1218         Dimensions without coordinates: dim_0, dim_1
1219         Data variables:
1220             foo      (dim_0, dim_1) int64 0 1 2 3 4 5
1221             bar      (x) <U1 'a' 'b'
1222 
1223         >>> ds
1224         <xarray.Dataset>
1225         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1226         Coordinates:
1227           * x        (x) <U3 'one' 'two'
1228         Dimensions without coordinates: dim_0, dim_1
1229         Data variables:
1230             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1231             bar      (x) int64 -1 2
1232 
1233         See Also
1234         --------
1235         pandas.DataFrame.copy
1236         """
1237         return self._copy(deep=deep, data=data)
1238 
1239     def _copy(
1240         self: T_Dataset,
1241         deep: bool = False,
1242         data: Mapping[Any, ArrayLike] | None = None,
1243         memo: dict[int, Any] | None = None,
1244     ) -> T_Dataset:
1245         if data is None:
1246             data = {}
1247         elif not utils.is_dict_like(data):
1248             raise ValueError("Data must be dict-like")
1249 
1250         if data:
1251             var_keys = set(self.data_vars.keys())
1252             data_keys = set(data.keys())
1253             keys_not_in_vars = data_keys - var_keys
1254             if keys_not_in_vars:
1255                 raise ValueError(
1256                     "Data must only contain variables in original "
1257                     "dataset. Extra variables: {}".format(keys_not_in_vars)
1258                 )
1259             keys_missing_from_data = var_keys - data_keys
1260             if keys_missing_from_data:
1261                 raise ValueError(
1262                     "Data must contain all variables in original "
1263                     "dataset. Data is missing {}".format(keys_missing_from_data)
1264                 )
1265 
1266         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1267 
1268         variables = {}
1269         for k, v in self._variables.items():
1270             if k in index_vars:
1271                 variables[k] = index_vars[k]
1272             else:
1273                 variables[k] = v._copy(deep=deep, data=data.get(k), memo=memo)
1274 
1275         attrs = copy.deepcopy(self._attrs, memo) if deep else copy.copy(self._attrs)
1276         encoding = (
1277             copy.deepcopy(self._encoding, memo) if deep else copy.copy(self._encoding)
1278         )
1279 
1280         return self._replace(variables, indexes=indexes, attrs=attrs, encoding=encoding)
1281 
1282     def __copy__(self: T_Dataset) -> T_Dataset:
1283         return self._copy(deep=False)
1284 
1285     def __deepcopy__(self: T_Dataset, memo: dict[int, Any] | None = None) -> T_Dataset:
1286         return self._copy(deep=True, memo=memo)
1287 
1288     def as_numpy(self: T_Dataset) -> T_Dataset:
1289         """
1290         Coerces wrapped data and coordinates into numpy arrays, returning a Dataset.
1291 
1292         See also
1293         --------
1294         DataArray.as_numpy
1295         DataArray.to_numpy : Returns only the data as a numpy.ndarray object.
1296         """
1297         numpy_variables = {k: v.as_numpy() for k, v in self.variables.items()}
1298         return self._replace(variables=numpy_variables)
1299 
1300     def _copy_listed(self: T_Dataset, names: Iterable[Hashable]) -> T_Dataset:
1301         """Create a new Dataset with the listed variables from this dataset and
1302         the all relevant coordinates. Skips all validation.
1303         """
1304         variables: dict[Hashable, Variable] = {}
1305         coord_names = set()
1306         indexes: dict[Hashable, Index] = {}
1307 
1308         for name in names:
1309             try:
1310                 variables[name] = self._variables[name]
1311             except KeyError:
1312                 ref_name, var_name, var = _get_virtual_variable(
1313                     self._variables, name, self.dims
1314                 )
1315                 variables[var_name] = var
1316                 if ref_name in self._coord_names or ref_name in self.dims:
1317                     coord_names.add(var_name)
1318                 if (var_name,) == var.dims:
1319                     index, index_vars = create_default_index_implicit(var, names)
1320                     indexes.update({k: index for k in index_vars})
1321                     variables.update(index_vars)
1322                     coord_names.update(index_vars)
1323 
1324         needed_dims: OrderedSet[Hashable] = OrderedSet()
1325         for v in variables.values():
1326             needed_dims.update(v.dims)
1327 
1328         dims = {k: self.dims[k] for k in needed_dims}
1329 
1330         # preserves ordering of coordinates
1331         for k in self._variables:
1332             if k not in self._coord_names:
1333                 continue
1334 
1335             if set(self.variables[k].dims) <= needed_dims:
1336                 variables[k] = self._variables[k]
1337                 coord_names.add(k)
1338 
1339         indexes.update(filter_indexes_from_coords(self._indexes, coord_names))
1340 
1341         return self._replace(variables, coord_names, dims, indexes=indexes)
1342 
1343     def _construct_dataarray(self, name: Hashable) -> DataArray:
1344         """Construct a DataArray by indexing this dataset"""
1345         from xarray.core.dataarray import DataArray
1346 
1347         try:
1348             variable = self._variables[name]
1349         except KeyError:
1350             _, name, variable = _get_virtual_variable(self._variables, name, self.dims)
1351 
1352         needed_dims = set(variable.dims)
1353 
1354         coords: dict[Hashable, Variable] = {}
1355         # preserve ordering
1356         for k in self._variables:
1357             if k in self._coord_names and set(self.variables[k].dims) <= needed_dims:
1358                 coords[k] = self.variables[k]
1359 
1360         indexes = filter_indexes_from_coords(self._indexes, set(coords))
1361 
1362         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1363 
1364     @property
1365     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1366         """Places to look-up items for attribute-style access"""
1367         yield from self._item_sources
1368         yield self.attrs
1369 
1370     @property
1371     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1372         """Places to look-up items for key-completion"""
1373         yield self.data_vars
1374         yield HybridMappingProxy(keys=self._coord_names, mapping=self.coords)
1375 
1376         # virtual coordinates
1377         yield HybridMappingProxy(keys=self.dims, mapping=self)
1378 
1379     def __contains__(self, key: object) -> bool:
1380         """The 'in' operator will return true or false depending on whether
1381         'key' is an array in the dataset or not.
1382         """
1383         return key in self._variables
1384 
1385     def __len__(self) -> int:
1386         return len(self.data_vars)
1387 
1388     def __bool__(self) -> bool:
1389         return bool(self.data_vars)
1390 
1391     def __iter__(self) -> Iterator[Hashable]:
1392         return iter(self.data_vars)
1393 
1394     def __array__(self, dtype=None):
1395         raise TypeError(
1396             "cannot directly convert an xarray.Dataset into a "
1397             "numpy array. Instead, create an xarray.DataArray "
1398             "first, either with indexing on the Dataset or by "
1399             "invoking the `to_array()` method."
1400         )
1401 
1402     @property
1403     def nbytes(self) -> int:
1404         """
1405         Total bytes consumed by the data arrays of all variables in this dataset.
1406 
1407         If the backend array for any variable does not include ``nbytes``, estimates
1408         the total bytes for that array based on the ``size`` and ``dtype``.
1409         """
1410         return sum(v.nbytes for v in self.variables.values())
1411 
1412     @property
1413     def loc(self: T_Dataset) -> _LocIndexer[T_Dataset]:
1414         """Attribute for location based indexing. Only supports __getitem__,
1415         and only when the key is a dict of the form {dim: labels}.
1416         """
1417         return _LocIndexer(self)
1418 
1419     @overload
1420     def __getitem__(self, key: Hashable) -> DataArray:
1421         ...
1422 
1423     # Mapping is Iterable
1424     @overload
1425     def __getitem__(self: T_Dataset, key: Iterable[Hashable]) -> T_Dataset:
1426         ...
1427 
1428     def __getitem__(
1429         self: T_Dataset, key: Mapping[Any, Any] | Hashable | Iterable[Hashable]
1430     ) -> T_Dataset | DataArray:
1431         """Access variables or coordinates of this dataset as a
1432         :py:class:`~xarray.DataArray` or a subset of variables or a indexed dataset.
1433 
1434         Indexing with a list of names will return a new ``Dataset`` object.
1435         """
1436         if utils.is_dict_like(key):
1437             return self.isel(**key)
1438         if utils.hashable(key):
1439             return self._construct_dataarray(key)
1440         if utils.iterable_of_hashable(key):
1441             return self._copy_listed(key)
1442         raise ValueError(f"Unsupported key-type {type(key)}")
1443 
1444     def __setitem__(
1445         self, key: Hashable | Iterable[Hashable] | Mapping, value: Any
1446     ) -> None:
1447         """Add an array to this dataset.
1448         Multiple arrays can be added at the same time, in which case each of
1449         the following operations is applied to the respective value.
1450 
1451         If key is dict-like, update all variables in the dataset
1452         one by one with the given value at the given location.
1453         If the given value is also a dataset, select corresponding variables
1454         in the given value and in the dataset to be changed.
1455 
1456         If value is a `
1457         from .dataarray import DataArray`, call its `select_vars()` method, rename it
1458         to `key` and merge the contents of the resulting dataset into this
1459         dataset.
1460 
1461         If value is a `Variable` object (or tuple of form
1462         ``(dims, data[, attrs])``), add it to this dataset as a new
1463         variable.
1464         """
1465         from xarray.core.dataarray import DataArray
1466 
1467         if utils.is_dict_like(key):
1468             # check for consistency and convert value to dataset
1469             value = self._setitem_check(key, value)
1470             # loop over dataset variables and set new values
1471             processed = []
1472             for name, var in self.items():
1473                 try:
1474                     var[key] = value[name]
1475                     processed.append(name)
1476                 except Exception as e:
1477                     if processed:
1478                         raise RuntimeError(
1479                             "An error occurred while setting values of the"
1480                             f" variable '{name}'. The following variables have"
1481                             f" been successfully updated:\n{processed}"
1482                         ) from e
1483                     else:
1484                         raise e
1485 
1486         elif utils.hashable(key):
1487             if isinstance(value, Dataset):
1488                 raise TypeError(
1489                     "Cannot assign a Dataset to a single key - only a DataArray or Variable "
1490                     "object can be stored under a single key."
1491                 )
1492             self.update({key: value})
1493 
1494         elif utils.iterable_of_hashable(key):
1495             keylist = list(key)
1496             if len(keylist) == 0:
1497                 raise ValueError("Empty list of variables to be set")
1498             if len(keylist) == 1:
1499                 self.update({keylist[0]: value})
1500             else:
1501                 if len(keylist) != len(value):
1502                     raise ValueError(
1503                         f"Different lengths of variables to be set "
1504                         f"({len(keylist)}) and data used as input for "
1505                         f"setting ({len(value)})"
1506                     )
1507                 if isinstance(value, Dataset):
1508                     self.update(dict(zip(keylist, value.data_vars.values())))
1509                 elif isinstance(value, DataArray):
1510                     raise ValueError("Cannot assign single DataArray to multiple keys")
1511                 else:
1512                     self.update(dict(zip(keylist, value)))
1513 
1514         else:
1515             raise ValueError(f"Unsupported key-type {type(key)}")
1516 
1517     def _setitem_check(self, key, value):
1518         """Consistency check for __setitem__
1519 
1520         When assigning values to a subset of a Dataset, do consistency check beforehand
1521         to avoid leaving the dataset in a partially updated state when an error occurs.
1522         """
1523         from xarray.core.alignment import align
1524         from xarray.core.dataarray import DataArray
1525 
1526         if isinstance(value, Dataset):
1527             missing_vars = [
1528                 name for name in value.data_vars if name not in self.data_vars
1529             ]
1530             if missing_vars:
1531                 raise ValueError(
1532                     f"Variables {missing_vars} in new values"
1533                     f" not available in original dataset:\n{self}"
1534                 )
1535         elif not any([isinstance(value, t) for t in [DataArray, Number, str]]):
1536             raise TypeError(
1537                 "Dataset assignment only accepts DataArrays, Datasets, and scalars."
1538             )
1539 
1540         new_value = Dataset()
1541         for name, var in self.items():
1542             # test indexing
1543             try:
1544                 var_k = var[key]
1545             except Exception as e:
1546                 raise ValueError(
1547                     f"Variable '{name}': indexer {key} not available"
1548                 ) from e
1549 
1550             if isinstance(value, Dataset):
1551                 val = value[name]
1552             else:
1553                 val = value
1554 
1555             if isinstance(val, DataArray):
1556                 # check consistency of dimensions
1557                 for dim in val.dims:
1558                     if dim not in var_k.dims:
1559                         raise KeyError(
1560                             f"Variable '{name}': dimension '{dim}' appears in new values "
1561                             f"but not in the indexed original data"
1562                         )
1563                 dims = tuple(dim for dim in var_k.dims if dim in val.dims)
1564                 if dims != val.dims:
1565                     raise ValueError(
1566                         f"Variable '{name}': dimension order differs between"
1567                         f" original and new data:\n{dims}\nvs.\n{val.dims}"
1568                     )
1569             else:
1570                 val = np.array(val)
1571 
1572             # type conversion
1573             new_value[name] = val.astype(var_k.dtype, copy=False)
1574 
1575         # check consistency of dimension sizes and dimension coordinates
1576         if isinstance(value, DataArray) or isinstance(value, Dataset):
1577             align(self[key], value, join="exact", copy=False)
1578 
1579         return new_value
1580 
1581     def __delitem__(self, key: Hashable) -> None:
1582         """Remove a variable from this dataset."""
1583         assert_no_index_corrupted(self.xindexes, {key})
1584 
1585         if key in self._indexes:
1586             del self._indexes[key]
1587         del self._variables[key]
1588         self._coord_names.discard(key)
1589         self._dims = calculate_dimensions(self._variables)
1590 
1591     # mutable objects should not be hashable
1592     # https://github.com/python/mypy/issues/4266
1593     __hash__ = None  # type: ignore[assignment]
1594 
1595     def _all_compat(self, other: Dataset, compat_str: str) -> bool:
1596         """Helper function for equals and identical"""
1597 
1598         # some stores (e.g., scipy) do not seem to preserve order, so don't
1599         # require matching order for equality
1600         def compat(x: Variable, y: Variable) -> bool:
1601             return getattr(x, compat_str)(y)
1602 
1603         return self._coord_names == other._coord_names and utils.dict_equiv(
1604             self._variables, other._variables, compat=compat
1605         )
1606 
1607     def broadcast_equals(self, other: Dataset) -> bool:
1608         """Two Datasets are broadcast equal if they are equal after
1609         broadcasting all variables against each other.
1610 
1611         For example, variables that are scalar in one dataset but non-scalar in
1612         the other dataset can still be broadcast equal if the the non-scalar
1613         variable is a constant.
1614 
1615         See Also
1616         --------
1617         Dataset.equals
1618         Dataset.identical
1619         """
1620         try:
1621             return self._all_compat(other, "broadcast_equals")
1622         except (TypeError, AttributeError):
1623             return False
1624 
1625     def equals(self, other: Dataset) -> bool:
1626         """Two Datasets are equal if they have matching variables and
1627         coordinates, all of which are equal.
1628 
1629         Datasets can still be equal (like pandas objects) if they have NaN
1630         values in the same locations.
1631 
1632         This method is necessary because `v1 == v2` for ``Dataset``
1633         does element-wise comparisons (like numpy.ndarrays).
1634 
1635         See Also
1636         --------
1637         Dataset.broadcast_equals
1638         Dataset.identical
1639         """
1640         try:
1641             return self._all_compat(other, "equals")
1642         except (TypeError, AttributeError):
1643             return False
1644 
1645     def identical(self, other: Dataset) -> bool:
1646         """Like equals, but also checks all dataset attributes and the
1647         attributes on all variables and coordinates.
1648 
1649         See Also
1650         --------
1651         Dataset.broadcast_equals
1652         Dataset.equals
1653         """
1654         try:
1655             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(
1656                 other, "identical"
1657             )
1658         except (TypeError, AttributeError):
1659             return False
1660 
1661     @property
1662     def indexes(self) -> Indexes[pd.Index]:
1663         """Mapping of pandas.Index objects used for label based indexing.
1664 
1665         Raises an error if this Dataset has indexes that cannot be coerced
1666         to pandas.Index objects.
1667 
1668         See Also
1669         --------
1670         Dataset.xindexes
1671 
1672         """
1673         return self.xindexes.to_pandas_indexes()
1674 
1675     @property
1676     def xindexes(self) -> Indexes[Index]:
1677         """Mapping of xarray Index objects used for label based indexing."""
1678         return Indexes(self._indexes, {k: self._variables[k] for k in self._indexes})
1679 
1680     @property
1681     def coords(self) -> DatasetCoordinates:
1682         """Dictionary of xarray.DataArray objects corresponding to coordinate
1683         variables
1684         """
1685         return DatasetCoordinates(self)
1686 
1687     @property
1688     def data_vars(self) -> DataVariables:
1689         """Dictionary of DataArray objects corresponding to data variables"""
1690         return DataVariables(self)
1691 
1692     def set_coords(self: T_Dataset, names: Hashable | Iterable[Hashable]) -> T_Dataset:
1693         """Given names of one or more variables, set them as coordinates
1694 
1695         Parameters
1696         ----------
1697         names : hashable or iterable of hashable
1698             Name(s) of variables in this dataset to convert into coordinates.
1699 
1700         Returns
1701         -------
1702         Dataset
1703 
1704         See Also
1705         --------
1706         Dataset.swap_dims
1707         Dataset.assign_coords
1708         """
1709         # TODO: allow inserting new coordinates with this method, like
1710         # DataFrame.set_index?
1711         # nb. check in self._variables, not self.data_vars to insure that the
1712         # operation is idempotent
1713         if isinstance(names, str) or not isinstance(names, Iterable):
1714             names = [names]
1715         else:
1716             names = list(names)
1717         self._assert_all_in_dataset(names)
1718         obj = self.copy()
1719         obj._coord_names.update(names)
1720         return obj
1721 
1722     def reset_coords(
1723         self: T_Dataset,
1724         names: Dims = None,
1725         drop: bool = False,
1726     ) -> T_Dataset:
1727         """Given names of coordinates, reset them to become variables
1728 
1729         Parameters
1730         ----------
1731         names : str, Iterable of Hashable or None, optional
1732             Name(s) of non-index coordinates in this dataset to reset into
1733             variables. By default, all non-index coordinates are reset.
1734         drop : bool, default: False
1735             If True, remove coordinates instead of converting them into
1736             variables.
1737 
1738         Returns
1739         -------
1740         Dataset
1741         """
1742         if names is None:
1743             names = self._coord_names - set(self._indexes)
1744         else:
1745             if isinstance(names, str) or not isinstance(names, Iterable):
1746                 names = [names]
1747             else:
1748                 names = list(names)
1749             self._assert_all_in_dataset(names)
1750             bad_coords = set(names) & set(self._indexes)
1751             if bad_coords:
1752                 raise ValueError(
1753                     f"cannot remove index coordinates with reset_coords: {bad_coords}"
1754                 )
1755         obj = self.copy()
1756         obj._coord_names.difference_update(names)
1757         if drop:
1758             for name in names:
1759                 del obj._variables[name]
1760         return obj
1761 
1762     def dump_to_store(self, store: AbstractDataStore, **kwargs) -> None:
1763         """Store dataset contents to a backends.*DataStore object."""
1764         from xarray.backends.api import dump_to_store
1765 
1766         # TODO: rename and/or cleanup this method to make it more consistent
1767         # with to_netcdf()
1768         dump_to_store(self, store, **kwargs)
1769 
1770     # path=None writes to bytes
1771     @overload
1772     def to_netcdf(
1773         self,
1774         path: None = None,
1775         mode: Literal["w", "a"] = "w",
1776         format: T_NetcdfTypes | None = None,
1777         group: str | None = None,
1778         engine: T_NetcdfEngine | None = None,
1779         encoding: Mapping[Any, Mapping[str, Any]] | None = None,
1780         unlimited_dims: Iterable[Hashable] | None = None,
1781         compute: bool = True,
1782         invalid_netcdf: bool = False,
1783     ) -> bytes:
1784         ...
1785 
1786     # default return None
1787     @overload
1788     def to_netcdf(
1789         self,
1790         path: str | PathLike,
1791         mode: Literal["w", "a"] = "w",
1792         format: T_NetcdfTypes | None = None,
1793         group: str | None = None,
1794         engine: T_NetcdfEngine | None = None,
1795         encoding: Mapping[Any, Mapping[str, Any]] | None = None,
1796         unlimited_dims: Iterable[Hashable] | None = None,
1797         compute: Literal[True] = True,
1798         invalid_netcdf: bool = False,
1799     ) -> None:
1800         ...
1801 
1802     # compute=False returns dask.Delayed
1803     @overload
1804     def to_netcdf(
1805         self,
1806         path: str | PathLike,
1807         mode: Literal["w", "a"] = "w",
1808         format: T_NetcdfTypes | None = None,
1809         group: str | None = None,
1810         engine: T_NetcdfEngine | None = None,
1811         encoding: Mapping[Any, Mapping[str, Any]] | None = None,
1812         unlimited_dims: Iterable[Hashable] | None = None,
1813         *,
1814         compute: Literal[False],
1815         invalid_netcdf: bool = False,
1816     ) -> Delayed:
1817         ...
1818 
1819     def to_netcdf(
1820         self,
1821         path: str | PathLike | None = None,
1822         mode: Literal["w", "a"] = "w",
1823         format: T_NetcdfTypes | None = None,
1824         group: str | None = None,
1825         engine: T_NetcdfEngine | None = None,
1826         encoding: Mapping[Any, Mapping[str, Any]] | None = None,
1827         unlimited_dims: Iterable[Hashable] | None = None,
1828         compute: bool = True,
1829         invalid_netcdf: bool = False,
1830     ) -> bytes | Delayed | None:
1831         """Write dataset contents to a netCDF file.
1832 
1833         Parameters
1834         ----------
1835         path : str, path-like or file-like, optional
1836             Path to which to save this dataset. File-like objects are only
1837             supported by the scipy engine. If no path is provided, this
1838             function returns the resulting netCDF file as bytes; in this case,
1839             we need to use scipy, which does not support netCDF version 4 (the
1840             default format becomes NETCDF3_64BIT).
1841         mode : {"w", "a"}, default: "w"
1842             Write ('w') or append ('a') mode. If mode='w', any existing file at
1843             this location will be overwritten. If mode='a', existing variables
1844             will be overwritten.
1845         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
1846                   "NETCDF3_CLASSIC"}, optional
1847             File format for the resulting netCDF file:
1848 
1849             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
1850               features.
1851             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
1852               netCDF 3 compatible API features.
1853             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
1854               which fully supports 2+ GB files, but is only compatible with
1855               clients linked against netCDF version 3.6.0 or later.
1856             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
1857               handle 2+ GB files very well.
1858 
1859             All formats are supported by the netCDF4-python library.
1860             scipy.io.netcdf only supports the last two formats.
1861 
1862             The default format is NETCDF4 if you are saving a file to disk and
1863             have the netCDF4-python library available. Otherwise, xarray falls
1864             back to using scipy to write netCDF files and defaults to the
1865             NETCDF3_64BIT format (scipy does not support netCDF4).
1866         group : str, optional
1867             Path to the netCDF4 group in the given file to open (only works for
1868             format='NETCDF4'). The group(s) will be created if necessary.
1869         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
1870             Engine to use when writing netCDF files. If not provided, the
1871             default engine is chosen based on available dependencies, with a
1872             preference for 'netcdf4' if writing to a file on disk.
1873         encoding : dict, optional
1874             Nested dictionary with variable names as keys and dictionaries of
1875             variable specific encodings as values, e.g.,
1876             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
1877             "zlib": True}, ...}``
1878 
1879             The `h5netcdf` engine supports both the NetCDF4-style compression
1880             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
1881             ones ``{"compression": "gzip", "compression_opts": 9}``.
1882             This allows using any compression plugin installed in the HDF5
1883             library, e.g. LZF.
1884 
1885         unlimited_dims : iterable of hashable, optional
1886             Dimension(s) that should be serialized as unlimited dimensions.
1887             By default, no dimensions are treated as unlimited dimensions.
1888             Note that unlimited_dims may also be set via
1889             ``dataset.encoding["unlimited_dims"]``.
1890         compute: bool, default: True
1891             If true compute immediately, otherwise return a
1892             ``dask.delayed.Delayed`` object that can be computed later.
1893         invalid_netcdf: bool, default: False
1894             Only valid along with ``engine="h5netcdf"``. If True, allow writing
1895             hdf5 files which are invalid netcdf as described in
1896             https://github.com/h5netcdf/h5netcdf.
1897 
1898         Returns
1899         -------
1900             * ``bytes`` if path is None
1901             * ``dask.delayed.Delayed`` if compute is False
1902             * None otherwise
1903 
1904         See Also
1905         --------
1906         DataArray.to_netcdf
1907         """
1908         if encoding is None:
1909             encoding = {}
1910         from xarray.backends.api import to_netcdf
1911 
1912         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
1913             self,
1914             path,
1915             mode=mode,
1916             format=format,
1917             group=group,
1918             engine=engine,
1919             encoding=encoding,
1920             unlimited_dims=unlimited_dims,
1921             compute=compute,
1922             multifile=False,
1923             invalid_netcdf=invalid_netcdf,
1924         )
1925 
1926     # compute=True (default) returns ZarrStore
1927     @overload
1928     def to_zarr(
1929         self,
1930         store: MutableMapping | str | PathLike[str] | None = None,
1931         chunk_store: MutableMapping | str | PathLike | None = None,
1932         mode: Literal["w", "w-", "a", "r+", None] = None,
1933         synchronizer=None,
1934         group: str | None = None,
1935         encoding: Mapping | None = None,
1936         compute: Literal[True] = True,
1937         consolidated: bool | None = None,
1938         append_dim: Hashable | None = None,
1939         region: Mapping[str, slice] | None = None,
1940         safe_chunks: bool = True,
1941         storage_options: dict[str, str] | None = None,
1942         zarr_version: int | None = None,
1943     ) -> ZarrStore:
1944         ...
1945 
1946     # compute=False returns dask.Delayed
1947     @overload
1948     def to_zarr(
1949         self,
1950         store: MutableMapping | str | PathLike[str] | None = None,
1951         chunk_store: MutableMapping | str | PathLike | None = None,
1952         mode: Literal["w", "w-", "a", "r+", None] = None,
1953         synchronizer=None,
1954         group: str | None = None,
1955         encoding: Mapping | None = None,
1956         *,
1957         compute: Literal[False],
1958         consolidated: bool | None = None,
1959         append_dim: Hashable | None = None,
1960         region: Mapping[str, slice] | None = None,
1961         safe_chunks: bool = True,
1962         storage_options: dict[str, str] | None = None,
1963     ) -> Delayed:
1964         ...
1965 
1966     def to_zarr(
1967         self,
1968         store: MutableMapping | str | PathLike[str] | None = None,
1969         chunk_store: MutableMapping | str | PathLike | None = None,
1970         mode: Literal["w", "w-", "a", "r+", None] = None,
1971         synchronizer=None,
1972         group: str | None = None,
1973         encoding: Mapping | None = None,
1974         compute: bool = True,
1975         consolidated: bool | None = None,
1976         append_dim: Hashable | None = None,
1977         region: Mapping[str, slice] | None = None,
1978         safe_chunks: bool = True,
1979         storage_options: dict[str, str] | None = None,
1980         zarr_version: int | None = None,
1981     ) -> ZarrStore | Delayed:
1982         """Write dataset contents to a zarr group.
1983 
1984         Zarr chunks are determined in the following way:
1985 
1986         - From the ``chunks`` attribute in each variable's ``encoding``
1987           (can be set via `Dataset.chunk`).
1988         - If the variable is a Dask array, from the dask chunks
1989         - If neither Dask chunks nor encoding chunks are present, chunks will
1990           be determined automatically by Zarr
1991         - If both Dask chunks and encoding chunks are present, encoding chunks
1992           will be used, provided that there is a many-to-one relationship between
1993           encoding chunks and dask chunks (i.e. Dask chunks are bigger than and
1994           evenly divide encoding chunks); otherwise raise a ``ValueError``.
1995           This restriction ensures that no synchronization / locks are required
1996           when writing. To disable this restriction, use ``safe_chunks=False``.
1997 
1998         Parameters
1999         ----------
2000         store : MutableMapping, str or path-like, optional
2001             Store or path to directory in local or remote file system.
2002         chunk_store : MutableMapping, str or path-like, optional
2003             Store or path to directory in local or remote file system only for Zarr
2004             array chunks. Requires zarr-python v2.4.0 or later.
2005         mode : {"w", "w-", "a", "r+", None}, optional
2006             Persistence mode: "w" means create (overwrite if exists);
2007             "w-" means create (fail if exists);
2008             "a" means override existing variables (create if does not exist);
2009             "r+" means modify existing array *values* only (raise an error if
2010             any metadata or shapes would change).
2011             The default mode is "a" if ``append_dim`` is set. Otherwise, it is
2012             "r+" if ``region`` is set and ``w-`` otherwise.
2013         synchronizer : object, optional
2014             Zarr array synchronizer.
2015         group : str, optional
2016             Group path. (a.k.a. `path` in zarr terminology.)
2017         encoding : dict, optional
2018             Nested dictionary with variable names as keys and dictionaries of
2019             variable specific encodings as values, e.g.,
2020             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,}, ...}``
2021         compute : bool, optional
2022             If True write array data immediately, otherwise return a
2023             ``dask.delayed.Delayed`` object that can be computed to write
2024             array data later. Metadata is always updated eagerly.
2025         consolidated : bool, optional
2026             If True, apply zarr's `consolidate_metadata` function to the store
2027             after writing metadata and read existing stores with consolidated
2028             metadata; if False, do not. The default (`consolidated=None`) means
2029             write consolidated metadata and attempt to read consolidated
2030             metadata for existing stores (falling back to non-consolidated).
2031 
2032             When the experimental ``zarr_version=3``, ``consolidated`` must be
2033             either be ``None`` or ``False``.
2034         append_dim : hashable, optional
2035             If set, the dimension along which the data will be appended. All
2036             other dimensions on overridden variables must remain the same size.
2037         region : dict, optional
2038             Optional mapping from dimension names to integer slices along
2039             dataset dimensions to indicate the region of existing zarr array(s)
2040             in which to write this dataset's data. For example,
2041             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate
2042             that values should be written to the region ``0:1000`` along ``x``
2043             and ``10000:11000`` along ``y``.
2044 
2045             Two restrictions apply to the use of ``region``:
2046 
2047             - If ``region`` is set, _all_ variables in a dataset must have at
2048               least one dimension in common with the region. Other variables
2049               should be written in a separate call to ``to_zarr()``.
2050             - Dimensions cannot be included in both ``region`` and
2051               ``append_dim`` at the same time. To create empty arrays to fill
2052               in with ``region``, use a separate call to ``to_zarr()`` with
2053               ``compute=False``. See "Appending to existing Zarr stores" in
2054               the reference documentation for full details.
2055         safe_chunks : bool, optional
2056             If True, only allow writes to when there is a many-to-one relationship
2057             between Zarr chunks (specified in encoding) and Dask chunks.
2058             Set False to override this restriction; however, data may become corrupted
2059             if Zarr arrays are written in parallel. This option may be useful in combination
2060             with ``compute=False`` to initialize a Zarr from an existing
2061             Dataset with arbitrary chunk structure.
2062         storage_options : dict, optional
2063             Any additional parameters for the storage backend (ignored for local
2064             paths).
2065         zarr_version : int or None, optional
2066             The desired zarr spec version to target (currently 2 or 3). The
2067             default of None will attempt to determine the zarr version from
2068             ``store`` when possible, otherwise defaulting to 2.
2069 
2070         Returns
2071         -------
2072             * ``dask.delayed.Delayed`` if compute is False
2073             * ZarrStore otherwise
2074 
2075         References
2076         ----------
2077         https://zarr.readthedocs.io/
2078 
2079         Notes
2080         -----
2081         Zarr chunking behavior:
2082             If chunks are found in the encoding argument or attribute
2083             corresponding to any DataArray, those chunks are used.
2084             If a DataArray is a dask array, it is written with those chunks.
2085             If not other chunks are found, Zarr uses its own heuristics to
2086             choose automatic chunk sizes.
2087 
2088         encoding:
2089             The encoding attribute (if exists) of the DataArray(s) will be
2090             used. Override any existing encodings by providing the ``encoding`` kwarg.
2091 
2092         See Also
2093         --------
2094         :ref:`io.zarr`
2095             The I/O user guide, with more details and examples.
2096         """
2097         from xarray.backends.api import to_zarr
2098 
2099         return to_zarr(  # type: ignore
2100             self,
2101             store=store,
2102             chunk_store=chunk_store,
2103             storage_options=storage_options,
2104             mode=mode,
2105             synchronizer=synchronizer,
2106             group=group,
2107             encoding=encoding,
2108             compute=compute,
2109             consolidated=consolidated,
2110             append_dim=append_dim,
2111             region=region,
2112             safe_chunks=safe_chunks,
2113             zarr_version=zarr_version,
2114         )
2115 
2116     def __repr__(self) -> str:
2117         return formatting.dataset_repr(self)
2118 
2119     def _repr_html_(self) -> str:
2120         if OPTIONS["display_style"] == "text":
2121             return f"<pre>{escape(repr(self))}</pre>"
2122         return formatting_html.dataset_repr(self)
2123 
2124     def info(self, buf: IO | None = None) -> None:
2125         """
2126         Concise summary of a Dataset variables and attributes.
2127 
2128         Parameters
2129         ----------
2130         buf : file-like, default: sys.stdout
2131             writable buffer
2132 
2133         See Also
2134         --------
2135         pandas.DataFrame.assign
2136         ncdump : netCDF's ncdump
2137         """
2138         if buf is None:  # pragma: no cover
2139             buf = sys.stdout
2140 
2141         lines = []
2142         lines.append("xarray.Dataset {")
2143         lines.append("dimensions:")
2144         for name, size in self.dims.items():
2145             lines.append(f"\t{name} = {size} ;")
2146         lines.append("\nvariables:")
2147         for name, da in self.variables.items():
2148             dims = ", ".join(map(str, da.dims))
2149             lines.append(f"\t{da.dtype} {name}({dims}) ;")
2150             for k, v in da.attrs.items():
2151                 lines.append(f"\t\t{name}:{k} = {v} ;")
2152         lines.append("\n// global attributes:")
2153         for k, v in self.attrs.items():
2154             lines.append(f"\t:{k} = {v} ;")
2155         lines.append("}")
2156 
2157         buf.write("\n".join(lines))
2158 
2159     @property
2160     def chunks(self) -> Mapping[Hashable, tuple[int, ...]]:
2161         """
2162         Mapping from dimension names to block lengths for this dataset's data, or None if
2163         the underlying data is not a dask array.
2164         Cannot be modified directly, but can be modified by calling .chunk().
2165 
2166         Same as Dataset.chunksizes, but maintained for backwards compatibility.
2167 
2168         See Also
2169         --------
2170         Dataset.chunk
2171         Dataset.chunksizes
2172         xarray.unify_chunks
2173         """
2174         return get_chunksizes(self.variables.values())
2175 
2176     @property
2177     def chunksizes(self) -> Mapping[Hashable, tuple[int, ...]]:
2178         """
2179         Mapping from dimension names to block lengths for this dataset's data, or None if
2180         the underlying data is not a dask array.
2181         Cannot be modified directly, but can be modified by calling .chunk().
2182 
2183         Same as Dataset.chunks.
2184 
2185         See Also
2186         --------
2187         Dataset.chunk
2188         Dataset.chunks
2189         xarray.unify_chunks
2190         """
2191         return get_chunksizes(self.variables.values())
2192 
2193     def chunk(
2194         self: T_Dataset,
2195         chunks: (
2196             int | Literal["auto"] | Mapping[Any, None | int | str | tuple[int, ...]]
2197         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
2198         name_prefix: str = "xarray-",
2199         token: str | None = None,
2200         lock: bool = False,
2201         inline_array: bool = False,
2202         **chunks_kwargs: None | int | str | tuple[int, ...],
2203     ) -> T_Dataset:
2204         """Coerce all arrays in this dataset into dask arrays with the given
2205         chunks.
2206 
2207         Non-dask arrays in this dataset will be converted to dask arrays. Dask
2208         arrays will be rechunked to the given chunk sizes.
2209 
2210         If neither chunks is not provided for one or more dimensions, chunk
2211         sizes along that dimension will not be updated; non-dask arrays will be
2212         converted into dask arrays with a single block.
2213 
2214         Parameters
2215         ----------
2216         chunks : int, tuple of int, "auto" or mapping of hashable to int, optional
2217             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, or
2218             ``{"x": 5, "y": 5}``.
2219         name_prefix : str, default: "xarray-"
2220             Prefix for the name of any new dask arrays.
2221         token : str, optional
2222             Token uniquely identifying this dataset.
2223         lock : bool, default: False
2224             Passed on to :py:func:`dask.array.from_array`, if the array is not
2225             already as dask array.
2226         inline_array: bool, default: False
2227             Passed on to :py:func:`dask.array.from_array`, if the array is not
2228             already as dask array.
2229         **chunks_kwargs : {dim: chunks, ...}, optional
2230             The keyword arguments form of ``chunks``.
2231             One of chunks or chunks_kwargs must be provided
2232 
2233         Returns
2234         -------
2235         chunked : xarray.Dataset
2236 
2237         See Also
2238         --------
2239         Dataset.chunks
2240         Dataset.chunksizes
2241         xarray.unify_chunks
2242         dask.array.from_array
2243         """
2244         if chunks is None and chunks_kwargs is None:
2245             warnings.warn(
2246                 "None value for 'chunks' is deprecated. "
2247                 "It will raise an error in the future. Use instead '{}'",
2248                 category=FutureWarning,
2249             )
2250             chunks = {}
2251 
2252         if isinstance(chunks, (Number, str, int)):
2253             chunks = dict.fromkeys(self.dims, chunks)
2254         else:
2255             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
2256 
2257         bad_dims = chunks.keys() - self.dims.keys()
2258         if bad_dims:
2259             raise ValueError(
2260                 f"some chunks keys are not dimensions on this object: {bad_dims}"
2261             )
2262 
2263         variables = {
2264             k: _maybe_chunk(k, v, chunks, token, lock, name_prefix)
2265             for k, v in self.variables.items()
2266         }
2267         return self._replace(variables)
2268 
2269     def _validate_indexers(
2270         self, indexers: Mapping[Any, Any], missing_dims: ErrorOptionsWithWarn = "raise"
2271     ) -> Iterator[tuple[Hashable, int | slice | np.ndarray | Variable]]:
2272         """Here we make sure
2273         + indexer has a valid keys
2274         + indexer is in a valid data type
2275         + string indexers are cast to the appropriate date type if the
2276           associated index is a DatetimeIndex or CFTimeIndex
2277         """
2278         from xarray.coding.cftimeindex import CFTimeIndex
2279         from xarray.core.dataarray import DataArray
2280 
2281         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2282 
2283         # all indexers should be int, slice, np.ndarrays, or Variable
2284         for k, v in indexers.items():
2285             if isinstance(v, (int, slice, Variable)):
2286                 yield k, v
2287             elif isinstance(v, DataArray):
2288                 yield k, v.variable
2289             elif isinstance(v, tuple):
2290                 yield k, as_variable(v)
2291             elif isinstance(v, Dataset):
2292                 raise TypeError("cannot use a Dataset as an indexer")
2293             elif isinstance(v, Sequence) and len(v) == 0:
2294                 yield k, np.empty((0,), dtype="int64")
2295             else:
2296                 v = np.asarray(v)
2297 
2298                 if v.dtype.kind in "US":
2299                     index = self._indexes[k].to_pandas_index()
2300                     if isinstance(index, pd.DatetimeIndex):
2301                         v = v.astype("datetime64[ns]")
2302                     elif isinstance(index, CFTimeIndex):
2303                         v = _parse_array_of_cftime_strings(v, index.date_type)
2304 
2305                 if v.ndim > 1:
2306                     raise IndexError(
2307                         "Unlabeled multi-dimensional array cannot be "
2308                         "used for indexing: {}".format(k)
2309                     )
2310                 yield k, v
2311 
2312     def _validate_interp_indexers(
2313         self, indexers: Mapping[Any, Any]
2314     ) -> Iterator[tuple[Hashable, Variable]]:
2315         """Variant of _validate_indexers to be used for interpolation"""
2316         for k, v in self._validate_indexers(indexers):
2317             if isinstance(v, Variable):
2318                 if v.ndim == 1:
2319                     yield k, v.to_index_variable()
2320                 else:
2321                     yield k, v
2322             elif isinstance(v, int):
2323                 yield k, Variable((), v, attrs=self.coords[k].attrs)
2324             elif isinstance(v, np.ndarray):
2325                 if v.ndim == 0:
2326                     yield k, Variable((), v, attrs=self.coords[k].attrs)
2327                 elif v.ndim == 1:
2328                     yield k, IndexVariable((k,), v, attrs=self.coords[k].attrs)
2329                 else:
2330                     raise AssertionError()  # Already tested by _validate_indexers
2331             else:
2332                 raise TypeError(type(v))
2333 
2334     def _get_indexers_coords_and_indexes(self, indexers):
2335         """Extract coordinates and indexes from indexers.
2336 
2337         Only coordinate with a name different from any of self.variables will
2338         be attached.
2339         """
2340         from xarray.core.dataarray import DataArray
2341 
2342         coords_list = []
2343         for k, v in indexers.items():
2344             if isinstance(v, DataArray):
2345                 if v.dtype.kind == "b":
2346                     if v.ndim != 1:  # we only support 1-d boolean array
2347                         raise ValueError(
2348                             "{:d}d-boolean array is used for indexing along "
2349                             "dimension {!r}, but only 1d boolean arrays are "
2350                             "supported.".format(v.ndim, k)
2351                         )
2352                     # Make sure in case of boolean DataArray, its
2353                     # coordinate also should be indexed.
2354                     v_coords = v[v.values.nonzero()[0]].coords
2355                 else:
2356                     v_coords = v.coords
2357                 coords_list.append(v_coords)
2358 
2359         # we don't need to call align() explicitly or check indexes for
2360         # alignment, because merge_variables already checks for exact alignment
2361         # between dimension coordinates
2362         coords, indexes = merge_coordinates_without_align(coords_list)
2363         assert_coordinate_consistent(self, coords)
2364 
2365         # silently drop the conflicted variables.
2366         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}
2367         attached_indexes = {
2368             k: v for k, v in indexes.items() if k not in self._variables
2369         }
2370         return attached_coords, attached_indexes
2371 
2372     def isel(
2373         self: T_Dataset,
2374         indexers: Mapping[Any, Any] | None = None,
2375         drop: bool = False,
2376         missing_dims: ErrorOptionsWithWarn = "raise",
2377         **indexers_kwargs: Any,
2378     ) -> T_Dataset:
2379         """Returns a new dataset with each array indexed along the specified
2380         dimension(s).
2381 
2382         This method selects values from each array using its `__getitem__`
2383         method, except this method does not require knowing the order of
2384         each array's dimensions.
2385 
2386         Parameters
2387         ----------
2388         indexers : dict, optional
2389             A dict with keys matching dimensions and values given
2390             by integers, slice objects or arrays.
2391             indexer can be a integer, slice, array-like or DataArray.
2392             If DataArrays are passed as indexers, xarray-style indexing will be
2393             carried out. See :ref:`indexing` for the details.
2394             One of indexers or indexers_kwargs must be provided.
2395         drop : bool, default: False
2396             If ``drop=True``, drop coordinates variables indexed by integers
2397             instead of making them scalar.
2398         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2399             What to do if dimensions that should be selected from are not present in the
2400             Dataset:
2401             - "raise": raise an exception
2402             - "warn": raise a warning, and ignore the missing dimensions
2403             - "ignore": ignore the missing dimensions
2404 
2405         **indexers_kwargs : {dim: indexer, ...}, optional
2406             The keyword arguments form of ``indexers``.
2407             One of indexers or indexers_kwargs must be provided.
2408 
2409         Returns
2410         -------
2411         obj : Dataset
2412             A new Dataset with the same contents as this dataset, except each
2413             array and dimension is indexed by the appropriate indexers.
2414             If indexer DataArrays have coordinates that do not conflict with
2415             this object, then these coordinates will be attached.
2416             In general, each array's data will be a view of the array's data
2417             in this dataset, unless vectorized indexing was triggered by using
2418             an array indexer, in which case the data will be a copy.
2419 
2420         See Also
2421         --------
2422         Dataset.sel
2423         DataArray.isel
2424         """
2425         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
2426         if any(is_fancy_indexer(idx) for idx in indexers.values()):
2427             return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)
2428 
2429         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
2430         # lists, or zero or one-dimensional np.ndarray's
2431         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2432 
2433         variables = {}
2434         dims: dict[Hashable, int] = {}
2435         coord_names = self._coord_names.copy()
2436 
2437         indexes, index_variables = isel_indexes(self.xindexes, indexers)
2438 
2439         for name, var in self._variables.items():
2440             # preserve variable order
2441             if name in index_variables:
2442                 var = index_variables[name]
2443             else:
2444                 var_indexers = {k: v for k, v in indexers.items() if k in var.dims}
2445                 if var_indexers:
2446                     var = var.isel(var_indexers)
2447                     if drop and var.ndim == 0 and name in coord_names:
2448                         coord_names.remove(name)
2449                         continue
2450             variables[name] = var
2451             dims.update(zip(var.dims, var.shape))
2452 
2453         return self._construct_direct(
2454             variables=variables,
2455             coord_names=coord_names,
2456             dims=dims,
2457             attrs=self._attrs,
2458             indexes=indexes,
2459             encoding=self._encoding,
2460             close=self._close,
2461         )
2462 
2463     def _isel_fancy(
2464         self: T_Dataset,
2465         indexers: Mapping[Any, Any],
2466         *,
2467         drop: bool,
2468         missing_dims: ErrorOptionsWithWarn = "raise",
2469     ) -> T_Dataset:
2470         valid_indexers = dict(self._validate_indexers(indexers, missing_dims))
2471 
2472         variables: dict[Hashable, Variable] = {}
2473         indexes, index_variables = isel_indexes(self.xindexes, valid_indexers)
2474 
2475         for name, var in self.variables.items():
2476             if name in index_variables:
2477                 new_var = index_variables[name]
2478             else:
2479                 var_indexers = {
2480                     k: v for k, v in valid_indexers.items() if k in var.dims
2481                 }
2482                 if var_indexers:
2483                     new_var = var.isel(indexers=var_indexers)
2484                     # drop scalar coordinates
2485                     # https://github.com/pydata/xarray/issues/6554
2486                     if name in self.coords and drop and new_var.ndim == 0:
2487                         continue
2488                 else:
2489                     new_var = var.copy(deep=False)
2490                 if name not in indexes:
2491                     new_var = new_var.to_base_variable()
2492             variables[name] = new_var
2493 
2494         coord_names = self._coord_names & variables.keys()
2495         selected = self._replace_with_new_dims(variables, coord_names, indexes)
2496 
2497         # Extract coordinates from indexers
2498         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)
2499         variables.update(coord_vars)
2500         indexes.update(new_indexes)
2501         coord_names = self._coord_names & variables.keys() | coord_vars.keys()
2502         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2503 
2504     def sel(
2505         self: T_Dataset,
2506         indexers: Mapping[Any, Any] | None = None,
2507         method: str | None = None,
2508         tolerance: int | float | Iterable[int | float] | None = None,
2509         drop: bool = False,
2510         **indexers_kwargs: Any,
2511     ) -> T_Dataset:
2512         """Returns a new dataset with each array indexed by tick labels
2513         along the specified dimension(s).
2514 
2515         In contrast to `Dataset.isel`, indexers for this method should use
2516         labels instead of integers.
2517 
2518         Under the hood, this method is powered by using pandas's powerful Index
2519         objects. This makes label based indexing essentially just as fast as
2520         using integer indexing.
2521 
2522         It also means this method uses pandas's (well documented) logic for
2523         indexing. This means you can use string shortcuts for datetime indexes
2524         (e.g., '2000-01' to select all values in January 2000). It also means
2525         that slices are treated as inclusive of both the start and stop values,
2526         unlike normal Python indexing.
2527 
2528         Parameters
2529         ----------
2530         indexers : dict, optional
2531             A dict with keys matching dimensions and values given
2532             by scalars, slices or arrays of tick labels. For dimensions with
2533             multi-index, the indexer may also be a dict-like object with keys
2534             matching index level names.
2535             If DataArrays are passed as indexers, xarray-style indexing will be
2536             carried out. See :ref:`indexing` for the details.
2537             One of indexers or indexers_kwargs must be provided.
2538         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2539             Method to use for inexact matches:
2540 
2541             * None (default): only exact matches
2542             * pad / ffill: propagate last valid index value forward
2543             * backfill / bfill: propagate next valid index value backward
2544             * nearest: use nearest valid index value
2545         tolerance : optional
2546             Maximum distance between original and new labels for inexact
2547             matches. The values of the index at the matching locations must
2548             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2549         drop : bool, optional
2550             If ``drop=True``, drop coordinates variables in `indexers` instead
2551             of making them scalar.
2552         **indexers_kwargs : {dim: indexer, ...}, optional
2553             The keyword arguments form of ``indexers``.
2554             One of indexers or indexers_kwargs must be provided.
2555 
2556         Returns
2557         -------
2558         obj : Dataset
2559             A new Dataset with the same contents as this dataset, except each
2560             variable and dimension is indexed by the appropriate indexers.
2561             If indexer DataArrays have coordinates that do not conflict with
2562             this object, then these coordinates will be attached.
2563             In general, each array's data will be a view of the array's data
2564             in this dataset, unless vectorized indexing was triggered by using
2565             an array indexer, in which case the data will be a copy.
2566 
2567         See Also
2568         --------
2569         Dataset.isel
2570         DataArray.sel
2571         """
2572         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "sel")
2573         query_results = map_index_queries(
2574             self, indexers=indexers, method=method, tolerance=tolerance
2575         )
2576 
2577         if drop:
2578             no_scalar_variables = {}
2579             for k, v in query_results.variables.items():
2580                 if v.dims:
2581                     no_scalar_variables[k] = v
2582                 else:
2583                     if k in self._coord_names:
2584                         query_results.drop_coords.append(k)
2585             query_results.variables = no_scalar_variables
2586 
2587         result = self.isel(indexers=query_results.dim_indexers, drop=drop)
2588         return result._overwrite_indexes(*query_results.as_tuple()[1:])
2589 
2590     def head(
2591         self: T_Dataset,
2592         indexers: Mapping[Any, int] | int | None = None,
2593         **indexers_kwargs: Any,
2594     ) -> T_Dataset:
2595         """Returns a new dataset with the first `n` values of each array
2596         for the specified dimension(s).
2597 
2598         Parameters
2599         ----------
2600         indexers : dict or int, default: 5
2601             A dict with keys matching dimensions and integer values `n`
2602             or a single integer `n` applied over all dimensions.
2603             One of indexers or indexers_kwargs must be provided.
2604         **indexers_kwargs : {dim: n, ...}, optional
2605             The keyword arguments form of ``indexers``.
2606             One of indexers or indexers_kwargs must be provided.
2607 
2608         See Also
2609         --------
2610         Dataset.tail
2611         Dataset.thin
2612         DataArray.head
2613         """
2614         if not indexers_kwargs:
2615             if indexers is None:
2616                 indexers = 5
2617             if not isinstance(indexers, int) and not is_dict_like(indexers):
2618                 raise TypeError("indexers must be either dict-like or a single integer")
2619         if isinstance(indexers, int):
2620             indexers = {dim: indexers for dim in self.dims}
2621         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "head")
2622         for k, v in indexers.items():
2623             if not isinstance(v, int):
2624                 raise TypeError(
2625                     "expected integer type indexer for "
2626                     f"dimension {k!r}, found {type(v)!r}"
2627                 )
2628             elif v < 0:
2629                 raise ValueError(
2630                     "expected positive integer as indexer "
2631                     f"for dimension {k!r}, found {v}"
2632                 )
2633         indexers_slices = {k: slice(val) for k, val in indexers.items()}
2634         return self.isel(indexers_slices)
2635 
2636     def tail(
2637         self: T_Dataset,
2638         indexers: Mapping[Any, int] | int | None = None,
2639         **indexers_kwargs: Any,
2640     ) -> T_Dataset:
2641         """Returns a new dataset with the last `n` values of each array
2642         for the specified dimension(s).
2643 
2644         Parameters
2645         ----------
2646         indexers : dict or int, default: 5
2647             A dict with keys matching dimensions and integer values `n`
2648             or a single integer `n` applied over all dimensions.
2649             One of indexers or indexers_kwargs must be provided.
2650         **indexers_kwargs : {dim: n, ...}, optional
2651             The keyword arguments form of ``indexers``.
2652             One of indexers or indexers_kwargs must be provided.
2653 
2654         See Also
2655         --------
2656         Dataset.head
2657         Dataset.thin
2658         DataArray.tail
2659         """
2660         if not indexers_kwargs:
2661             if indexers is None:
2662                 indexers = 5
2663             if not isinstance(indexers, int) and not is_dict_like(indexers):
2664                 raise TypeError("indexers must be either dict-like or a single integer")
2665         if isinstance(indexers, int):
2666             indexers = {dim: indexers for dim in self.dims}
2667         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "tail")
2668         for k, v in indexers.items():
2669             if not isinstance(v, int):
2670                 raise TypeError(
2671                     "expected integer type indexer for "
2672                     f"dimension {k!r}, found {type(v)!r}"
2673                 )
2674             elif v < 0:
2675                 raise ValueError(
2676                     "expected positive integer as indexer "
2677                     f"for dimension {k!r}, found {v}"
2678                 )
2679         indexers_slices = {
2680             k: slice(-val, None) if val != 0 else slice(val)
2681             for k, val in indexers.items()
2682         }
2683         return self.isel(indexers_slices)
2684 
2685     def thin(
2686         self: T_Dataset,
2687         indexers: Mapping[Any, int] | int | None = None,
2688         **indexers_kwargs: Any,
2689     ) -> T_Dataset:
2690         """Returns a new dataset with each array indexed along every `n`-th
2691         value for the specified dimension(s)
2692 
2693         Parameters
2694         ----------
2695         indexers : dict or int
2696             A dict with keys matching dimensions and integer values `n`
2697             or a single integer `n` applied over all dimensions.
2698             One of indexers or indexers_kwargs must be provided.
2699         **indexers_kwargs : {dim: n, ...}, optional
2700             The keyword arguments form of ``indexers``.
2701             One of indexers or indexers_kwargs must be provided.
2702 
2703         Examples
2704         --------
2705         >>> x_arr = np.arange(0, 26)
2706         >>> x_arr
2707         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
2708                17, 18, 19, 20, 21, 22, 23, 24, 25])
2709         >>> x = xr.DataArray(
2710         ...     np.reshape(x_arr, (2, 13)),
2711         ...     dims=("x", "y"),
2712         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
2713         ... )
2714         >>> x_ds = xr.Dataset({"foo": x})
2715         >>> x_ds
2716         <xarray.Dataset>
2717         Dimensions:  (x: 2, y: 13)
2718         Coordinates:
2719           * x        (x) int64 0 1
2720           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
2721         Data variables:
2722             foo      (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24 25
2723 
2724         >>> x_ds.thin(3)
2725         <xarray.Dataset>
2726         Dimensions:  (x: 1, y: 5)
2727         Coordinates:
2728           * x        (x) int64 0
2729           * y        (y) int64 0 3 6 9 12
2730         Data variables:
2731             foo      (x, y) int64 0 3 6 9 12
2732         >>> x.thin({"x": 2, "y": 5})
2733         <xarray.DataArray (x: 1, y: 3)>
2734         array([[ 0,  5, 10]])
2735         Coordinates:
2736           * x        (x) int64 0
2737           * y        (y) int64 0 5 10
2738 
2739         See Also
2740         --------
2741         Dataset.head
2742         Dataset.tail
2743         DataArray.thin
2744         """
2745         if (
2746             not indexers_kwargs
2747             and not isinstance(indexers, int)
2748             and not is_dict_like(indexers)
2749         ):
2750             raise TypeError("indexers must be either dict-like or a single integer")
2751         if isinstance(indexers, int):
2752             indexers = {dim: indexers for dim in self.dims}
2753         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "thin")
2754         for k, v in indexers.items():
2755             if not isinstance(v, int):
2756                 raise TypeError(
2757                     "expected integer type indexer for "
2758                     f"dimension {k!r}, found {type(v)!r}"
2759                 )
2760             elif v < 0:
2761                 raise ValueError(
2762                     "expected positive integer as indexer "
2763                     f"for dimension {k!r}, found {v}"
2764                 )
2765             elif v == 0:
2766                 raise ValueError("step cannot be zero")
2767         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}
2768         return self.isel(indexers_slices)
2769 
2770     def broadcast_like(
2771         self: T_Dataset,
2772         other: Dataset | DataArray,
2773         exclude: Iterable[Hashable] | None = None,
2774     ) -> T_Dataset:
2775         """Broadcast this DataArray against another Dataset or DataArray.
2776         This is equivalent to xr.broadcast(other, self)[1]
2777 
2778         Parameters
2779         ----------
2780         other : Dataset or DataArray
2781             Object against which to broadcast this array.
2782         exclude : iterable of hashable, optional
2783             Dimensions that must not be broadcasted
2784 
2785         """
2786         if exclude is None:
2787             exclude = set()
2788         else:
2789             exclude = set(exclude)
2790         args = align(other, self, join="outer", copy=False, exclude=exclude)
2791 
2792         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
2793 
2794         return _broadcast_helper(
2795             cast("T_Dataset", args[1]), exclude, dims_map, common_coords
2796         )
2797 
2798     def _reindex_callback(
2799         self,
2800         aligner: alignment.Aligner,
2801         dim_pos_indexers: dict[Hashable, Any],
2802         variables: dict[Hashable, Variable],
2803         indexes: dict[Hashable, Index],
2804         fill_value: Any,
2805         exclude_dims: frozenset[Hashable],
2806         exclude_vars: frozenset[Hashable],
2807     ) -> Dataset:
2808         """Callback called from ``Aligner`` to create a new reindexed Dataset."""
2809 
2810         new_variables = variables.copy()
2811         new_indexes = indexes.copy()
2812 
2813         # re-assign variable metadata
2814         for name, new_var in new_variables.items():
2815             var = self._variables.get(name)
2816             if var is not None:
2817                 new_var.attrs = var.attrs
2818                 new_var.encoding = var.encoding
2819 
2820         # pass through indexes from excluded dimensions
2821         # no extra check needed for multi-coordinate indexes, potential conflicts
2822         # should already have been detected when aligning the indexes
2823         for name, idx in self._indexes.items():
2824             var = self._variables[name]
2825             if set(var.dims) <= exclude_dims:
2826                 new_indexes[name] = idx
2827                 new_variables[name] = var
2828 
2829         if not dim_pos_indexers:
2830             # fast path for no reindexing necessary
2831             if set(new_indexes) - set(self._indexes):
2832                 # this only adds new indexes and their coordinate variables
2833                 reindexed = self._overwrite_indexes(new_indexes, new_variables)
2834             else:
2835                 reindexed = self.copy(deep=aligner.copy)
2836         else:
2837             to_reindex = {
2838                 k: v
2839                 for k, v in self.variables.items()
2840                 if k not in variables and k not in exclude_vars
2841             }
2842             reindexed_vars = alignment.reindex_variables(
2843                 to_reindex,
2844                 dim_pos_indexers,
2845                 copy=aligner.copy,
2846                 fill_value=fill_value,
2847                 sparse=aligner.sparse,
2848             )
2849             new_variables.update(reindexed_vars)
2850             new_coord_names = self._coord_names | set(new_indexes)
2851             reindexed = self._replace_with_new_dims(
2852                 new_variables, new_coord_names, indexes=new_indexes
2853             )
2854 
2855         return reindexed
2856 
2857     def reindex_like(
2858         self: T_Dataset,
2859         other: Dataset | DataArray,
2860         method: ReindexMethodOptions = None,
2861         tolerance: int | float | Iterable[int | float] | None = None,
2862         copy: bool = True,
2863         fill_value: Any = xrdtypes.NA,
2864     ) -> T_Dataset:
2865         """Conform this object onto the indexes of another object, filling in
2866         missing values with ``fill_value``. The default fill value is NaN.
2867 
2868         Parameters
2869         ----------
2870         other : Dataset or DataArray
2871             Object with an 'indexes' attribute giving a mapping from dimension
2872             names to pandas.Index objects, which provides coordinates upon
2873             which to index the variables in this dataset. The indexes on this
2874             other object need not be the same as the indexes on this
2875             dataset. Any mis-matched index values will be filled in with
2876             NaN, and any mis-matched dimension names will simply be ignored.
2877         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2878             Method to use for filling index values from other not found in this
2879             dataset:
2880 
2881             - None (default): don't fill gaps
2882             - "pad" / "ffill": propagate last valid index value forward
2883             - "backfill" / "bfill": propagate next valid index value backward
2884             - "nearest": use nearest valid index value
2885 
2886         tolerance : optional
2887             Maximum distance between original and new labels for inexact
2888             matches. The values of the index at the matching locations must
2889             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2890             Tolerance may be a scalar value, which applies the same tolerance
2891             to all values, or list-like, which applies variable tolerance per
2892             element. List-like must be the same size as the index and its dtype
2893             must exactly match the index’s type.
2894         copy : bool, default: True
2895             If ``copy=True``, data in the return value is always copied. If
2896             ``copy=False`` and reindexing is unnecessary, or can be performed
2897             with only slice operations, then the output may share memory with
2898             the input. In either case, a new xarray object is always returned.
2899         fill_value : scalar or dict-like, optional
2900             Value to use for newly missing values. If a dict-like maps
2901             variable names to fill values.
2902 
2903         Returns
2904         -------
2905         reindexed : Dataset
2906             Another dataset, with this dataset's data but coordinates from the
2907             other object.
2908 
2909         See Also
2910         --------
2911         Dataset.reindex
2912         align
2913         """
2914         return alignment.reindex_like(
2915             self,
2916             other=other,
2917             method=method,
2918             tolerance=tolerance,
2919             copy=copy,
2920             fill_value=fill_value,
2921         )
2922 
2923     def reindex(
2924         self: T_Dataset,
2925         indexers: Mapping[Any, Any] | None = None,
2926         method: ReindexMethodOptions = None,
2927         tolerance: int | float | Iterable[int | float] | None = None,
2928         copy: bool = True,
2929         fill_value: Any = xrdtypes.NA,
2930         **indexers_kwargs: Any,
2931     ) -> T_Dataset:
2932         """Conform this object onto a new set of indexes, filling in
2933         missing values with ``fill_value``. The default fill value is NaN.
2934 
2935         Parameters
2936         ----------
2937         indexers : dict, optional
2938             Dictionary with keys given by dimension names and values given by
2939             arrays of coordinates tick labels. Any mis-matched coordinate
2940             values will be filled in with NaN, and any mis-matched dimension
2941             names will simply be ignored.
2942             One of indexers or indexers_kwargs must be provided.
2943         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2944             Method to use for filling index values in ``indexers`` not found in
2945             this dataset:
2946 
2947             - None (default): don't fill gaps
2948             - "pad" / "ffill": propagate last valid index value forward
2949             - "backfill" / "bfill": propagate next valid index value backward
2950             - "nearest": use nearest valid index value
2951 
2952         tolerance : optional
2953             Maximum distance between original and new labels for inexact
2954             matches. The values of the index at the matching locations must
2955             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2956             Tolerance may be a scalar value, which applies the same tolerance
2957             to all values, or list-like, which applies variable tolerance per
2958             element. List-like must be the same size as the index and its dtype
2959             must exactly match the index’s type.
2960         copy : bool, default: True
2961             If ``copy=True``, data in the return value is always copied. If
2962             ``copy=False`` and reindexing is unnecessary, or can be performed
2963             with only slice operations, then the output may share memory with
2964             the input. In either case, a new xarray object is always returned.
2965         fill_value : scalar or dict-like, optional
2966             Value to use for newly missing values. If a dict-like,
2967             maps variable names (including coordinates) to fill values.
2968         sparse : bool, default: False
2969             use sparse-array.
2970         **indexers_kwargs : {dim: indexer, ...}, optional
2971             Keyword arguments in the same form as ``indexers``.
2972             One of indexers or indexers_kwargs must be provided.
2973 
2974         Returns
2975         -------
2976         reindexed : Dataset
2977             Another dataset, with this dataset's data but replaced coordinates.
2978 
2979         See Also
2980         --------
2981         Dataset.reindex_like
2982         align
2983         pandas.Index.get_indexer
2984 
2985         Examples
2986         --------
2987         Create a dataset with some fictional data.
2988 
2989         >>> x = xr.Dataset(
2990         ...     {
2991         ...         "temperature": ("station", 20 * np.random.rand(4)),
2992         ...         "pressure": ("station", 500 * np.random.rand(4)),
2993         ...     },
2994         ...     coords={"station": ["boston", "nyc", "seattle", "denver"]},
2995         ... )
2996         >>> x
2997         <xarray.Dataset>
2998         Dimensions:      (station: 4)
2999         Coordinates:
3000           * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'
3001         Data variables:
3002             temperature  (station) float64 10.98 14.3 12.06 10.9
3003             pressure     (station) float64 211.8 322.9 218.8 445.9
3004         >>> x.indexes
3005         Indexes:
3006             station  Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')
3007 
3008         Create a new index and reindex the dataset. By default values in the new index that
3009         do not have corresponding records in the dataset are assigned `NaN`.
3010 
3011         >>> new_index = ["boston", "austin", "seattle", "lincoln"]
3012         >>> x.reindex({"station": new_index})
3013         <xarray.Dataset>
3014         Dimensions:      (station: 4)
3015         Coordinates:
3016           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
3017         Data variables:
3018             temperature  (station) float64 10.98 nan 12.06 nan
3019             pressure     (station) float64 211.8 nan 218.8 nan
3020 
3021         We can fill in the missing values by passing a value to the keyword `fill_value`.
3022 
3023         >>> x.reindex({"station": new_index}, fill_value=0)
3024         <xarray.Dataset>
3025         Dimensions:      (station: 4)
3026         Coordinates:
3027           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
3028         Data variables:
3029             temperature  (station) float64 10.98 0.0 12.06 0.0
3030             pressure     (station) float64 211.8 0.0 218.8 0.0
3031 
3032         We can also use different fill values for each variable.
3033 
3034         >>> x.reindex(
3035         ...     {"station": new_index}, fill_value={"temperature": 0, "pressure": 100}
3036         ... )
3037         <xarray.Dataset>
3038         Dimensions:      (station: 4)
3039         Coordinates:
3040           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
3041         Data variables:
3042             temperature  (station) float64 10.98 0.0 12.06 0.0
3043             pressure     (station) float64 211.8 100.0 218.8 100.0
3044 
3045         Because the index is not monotonically increasing or decreasing, we cannot use arguments
3046         to the keyword method to fill the `NaN` values.
3047 
3048         >>> x.reindex({"station": new_index}, method="nearest")
3049         Traceback (most recent call last):
3050         ...
3051             raise ValueError('index must be monotonic increasing or decreasing')
3052         ValueError: index must be monotonic increasing or decreasing
3053 
3054         To further illustrate the filling functionality in reindex, we will create a
3055         dataset with a monotonically increasing index (for example, a sequence of dates).
3056 
3057         >>> x2 = xr.Dataset(
3058         ...     {
3059         ...         "temperature": (
3060         ...             "time",
3061         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],
3062         ...         ),
3063         ...         "pressure": ("time", 500 * np.random.rand(6)),
3064         ...     },
3065         ...     coords={"time": pd.date_range("01/01/2019", periods=6, freq="D")},
3066         ... )
3067         >>> x2
3068         <xarray.Dataset>
3069         Dimensions:      (time: 6)
3070         Coordinates:
3071           * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06
3072         Data variables:
3073             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12
3074             pressure     (time) float64 481.8 191.7 395.9 264.4 284.0 462.8
3075 
3076         Suppose we decide to expand the dataset to cover a wider date range.
3077 
3078         >>> time_index2 = pd.date_range("12/29/2018", periods=10, freq="D")
3079         >>> x2.reindex({"time": time_index2})
3080         <xarray.Dataset>
3081         Dimensions:      (time: 10)
3082         Coordinates:
3083           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3084         Data variables:
3085             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan
3086             pressure     (time) float64 nan nan nan 481.8 ... 264.4 284.0 462.8 nan
3087 
3088         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)
3089         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.
3090 
3091         For example, to back-propagate the last valid value to fill the `NaN` values,
3092         pass `bfill` as an argument to the `method` keyword.
3093 
3094         >>> x3 = x2.reindex({"time": time_index2}, method="bfill")
3095         >>> x3
3096         <xarray.Dataset>
3097         Dimensions:      (time: 10)
3098         Coordinates:
3099           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3100         Data variables:
3101             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan
3102             pressure     (time) float64 481.8 481.8 481.8 481.8 ... 284.0 462.8 nan
3103 
3104         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)
3105         will not be filled by any of the value propagation schemes.
3106 
3107         >>> x2.where(x2.temperature.isnull(), drop=True)
3108         <xarray.Dataset>
3109         Dimensions:      (time: 1)
3110         Coordinates:
3111           * time         (time) datetime64[ns] 2019-01-03
3112         Data variables:
3113             temperature  (time) float64 nan
3114             pressure     (time) float64 395.9
3115         >>> x3.where(x3.temperature.isnull(), drop=True)
3116         <xarray.Dataset>
3117         Dimensions:      (time: 2)
3118         Coordinates:
3119           * time         (time) datetime64[ns] 2019-01-03 2019-01-07
3120         Data variables:
3121             temperature  (time) float64 nan nan
3122             pressure     (time) float64 395.9 nan
3123 
3124         This is because filling while reindexing does not look at dataset values, but only compares
3125         the original and desired indexes. If you do want to fill in the `NaN` values present in the
3126         original dataset, use the :py:meth:`~Dataset.fillna()` method.
3127 
3128         """
3129         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3130         return alignment.reindex(
3131             self,
3132             indexers=indexers,
3133             method=method,
3134             tolerance=tolerance,
3135             copy=copy,
3136             fill_value=fill_value,
3137         )
3138 
3139     def _reindex(
3140         self: T_Dataset,
3141         indexers: Mapping[Any, Any] | None = None,
3142         method: str | None = None,
3143         tolerance: int | float | Iterable[int | float] | None = None,
3144         copy: bool = True,
3145         fill_value: Any = xrdtypes.NA,
3146         sparse: bool = False,
3147         **indexers_kwargs: Any,
3148     ) -> T_Dataset:
3149         """
3150         Same as reindex but supports sparse option.
3151         """
3152         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3153         return alignment.reindex(
3154             self,
3155             indexers=indexers,
3156             method=method,
3157             tolerance=tolerance,
3158             copy=copy,
3159             fill_value=fill_value,
3160             sparse=sparse,
3161         )
3162 
3163     def interp(
3164         self: T_Dataset,
3165         coords: Mapping[Any, Any] | None = None,
3166         method: InterpOptions = "linear",
3167         assume_sorted: bool = False,
3168         kwargs: Mapping[str, Any] | None = None,
3169         method_non_numeric: str = "nearest",
3170         **coords_kwargs: Any,
3171     ) -> T_Dataset:
3172         """Interpolate a Dataset onto new coordinates
3173 
3174         Performs univariate or multivariate interpolation of a Dataset onto
3175         new coordinates using scipy's interpolation routines. If interpolating
3176         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
3177         called.  When interpolating along multiple existing dimensions, an
3178         attempt is made to decompose the interpolation into multiple
3179         1-dimensional interpolations. If this is possible,
3180         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3181         :py:func:`scipy.interpolate.interpn` is called.
3182 
3183         Parameters
3184         ----------
3185         coords : dict, optional
3186             Mapping from dimension names to the new coordinates.
3187             New coordinate can be a scalar, array-like or DataArray.
3188             If DataArrays are passed as new coordinates, their dimensions are
3189             used for the broadcasting. Missing values are skipped.
3190         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3191             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3192             String indicating which method to use for interpolation:
3193 
3194             - 'linear': linear interpolation. Additional keyword
3195               arguments are passed to :py:func:`numpy.interp`
3196             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3197               are passed to :py:func:`scipy.interpolate.interp1d`. If
3198               ``method='polynomial'``, the ``order`` keyword argument must also be
3199               provided.
3200             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3201               respective :py:class:`scipy.interpolate` classes.
3202 
3203         assume_sorted : bool, default: False
3204             If False, values of coordinates that are interpolated over can be
3205             in any order and they are sorted first. If True, interpolated
3206             coordinates are assumed to be an array of monotonically increasing
3207             values.
3208         kwargs : dict, optional
3209             Additional keyword arguments passed to scipy's interpolator. Valid
3210             options and their behavior depend whether ``interp1d`` or
3211             ``interpn`` is used.
3212         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3213             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3214             ``"nearest"`` is used by default.
3215         **coords_kwargs : {dim: coordinate, ...}, optional
3216             The keyword arguments form of ``coords``.
3217             One of coords or coords_kwargs must be provided.
3218 
3219         Returns
3220         -------
3221         interpolated : Dataset
3222             New dataset on the new coordinates.
3223 
3224         Notes
3225         -----
3226         scipy is required.
3227 
3228         See Also
3229         --------
3230         scipy.interpolate.interp1d
3231         scipy.interpolate.interpn
3232 
3233         Examples
3234         --------
3235         >>> ds = xr.Dataset(
3236         ...     data_vars={
3237         ...         "a": ("x", [5, 7, 4]),
3238         ...         "b": (
3239         ...             ("x", "y"),
3240         ...             [[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
3241         ...         ),
3242         ...     },
3243         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
3244         ... )
3245         >>> ds
3246         <xarray.Dataset>
3247         Dimensions:  (x: 3, y: 4)
3248         Coordinates:
3249           * x        (x) int64 0 1 2
3250           * y        (y) int64 10 12 14 16
3251         Data variables:
3252             a        (x) int64 5 7 4
3253             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 6.0 nan 6.0 nan 5.0 8.0
3254 
3255         1D interpolation with the default method (linear):
3256 
3257         >>> ds.interp(x=[0, 0.75, 1.25, 1.75])
3258         <xarray.Dataset>
3259         Dimensions:  (x: 4, y: 4)
3260         Coordinates:
3261           * y        (y) int64 10 12 14 16
3262           * x        (x) float64 0.0 0.75 1.25 1.75
3263         Data variables:
3264             a        (x) float64 5.0 6.5 6.25 4.75
3265             b        (x, y) float64 1.0 4.0 2.0 nan 1.75 6.25 ... nan 5.0 nan 5.25 nan
3266 
3267         1D interpolation with a different method:
3268 
3269         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
3270         <xarray.Dataset>
3271         Dimensions:  (x: 4, y: 4)
3272         Coordinates:
3273           * y        (y) int64 10 12 14 16
3274           * x        (x) float64 0.0 0.75 1.25 1.75
3275         Data variables:
3276             a        (x) float64 5.0 7.0 7.0 4.0
3277             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 ... 6.0 nan 6.0 nan 5.0 8.0
3278 
3279         1D extrapolation:
3280 
3281         >>> ds.interp(
3282         ...     x=[1, 1.5, 2.5, 3.5],
3283         ...     method="linear",
3284         ...     kwargs={"fill_value": "extrapolate"},
3285         ... )
3286         <xarray.Dataset>
3287         Dimensions:  (x: 4, y: 4)
3288         Coordinates:
3289           * y        (y) int64 10 12 14 16
3290           * x        (x) float64 1.0 1.5 2.5 3.5
3291         Data variables:
3292             a        (x) float64 7.0 5.5 2.5 -0.5
3293             b        (x, y) float64 2.0 7.0 6.0 nan 4.0 nan ... 4.5 nan 12.0 nan 3.5 nan
3294 
3295         2D interpolation:
3296 
3297         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
3298         <xarray.Dataset>
3299         Dimensions:  (x: 4, y: 3)
3300         Coordinates:
3301           * x        (x) float64 0.0 0.75 1.25 1.75
3302           * y        (y) int64 11 13 15
3303         Data variables:
3304             a        (x) float64 5.0 6.5 6.25 4.75
3305             b        (x, y) float64 2.5 3.0 nan 4.0 5.625 nan nan nan nan nan nan nan
3306         """
3307         from xarray.core import missing
3308 
3309         if kwargs is None:
3310             kwargs = {}
3311 
3312         coords = either_dict_or_kwargs(coords, coords_kwargs, "interp")
3313         indexers = dict(self._validate_interp_indexers(coords))
3314 
3315         if coords:
3316             # This avoids broadcasting over coordinates that are both in
3317             # the original array AND in the indexing array. It essentially
3318             # forces interpolation along the shared coordinates.
3319             sdims = (
3320                 set(self.dims)
3321                 .intersection(*[set(nx.dims) for nx in indexers.values()])
3322                 .difference(coords.keys())
3323             )
3324             indexers.update({d: self.variables[d] for d in sdims})
3325 
3326         obj = self if assume_sorted else self.sortby([k for k in coords])
3327 
3328         def maybe_variable(obj, k):
3329             # workaround to get variable for dimension without coordinate.
3330             try:
3331                 return obj._variables[k]
3332             except KeyError:
3333                 return as_variable((k, range(obj.dims[k])))
3334 
3335         def _validate_interp_indexer(x, new_x):
3336             # In the case of datetimes, the restrictions placed on indexers
3337             # used with interp are stronger than those which are placed on
3338             # isel, so we need an additional check after _validate_indexers.
3339             if _contains_datetime_like_objects(
3340                 x
3341             ) and not _contains_datetime_like_objects(new_x):
3342                 raise TypeError(
3343                     "When interpolating over a datetime-like "
3344                     "coordinate, the coordinates to "
3345                     "interpolate to must be either datetime "
3346                     "strings or datetimes. "
3347                     "Instead got\n{}".format(new_x)
3348                 )
3349             return x, new_x
3350 
3351         validated_indexers = {
3352             k: _validate_interp_indexer(maybe_variable(obj, k), v)
3353             for k, v in indexers.items()
3354         }
3355 
3356         # optimization: subset to coordinate range of the target index
3357         if method in ["linear", "nearest"]:
3358             for k, v in validated_indexers.items():
3359                 obj, newidx = missing._localize(obj, {k: v})
3360                 validated_indexers[k] = newidx[k]
3361 
3362         # optimization: create dask coordinate arrays once per Dataset
3363         # rather than once per Variable when dask.array.unify_chunks is called later
3364         # GH4739
3365         if obj.__dask_graph__():
3366             dask_indexers = {
3367                 k: (index.to_base_variable().chunk(), dest.to_base_variable().chunk())
3368                 for k, (index, dest) in validated_indexers.items()
3369             }
3370 
3371         variables: dict[Hashable, Variable] = {}
3372         reindex: bool = False
3373         for name, var in obj._variables.items():
3374             if name in indexers:
3375                 continue
3376 
3377             if is_duck_dask_array(var.data):
3378                 use_indexers = dask_indexers
3379             else:
3380                 use_indexers = validated_indexers
3381 
3382             dtype_kind = var.dtype.kind
3383             if dtype_kind in "uifc":
3384                 # For normal number types do the interpolation:
3385                 var_indexers = {k: v for k, v in use_indexers.items() if k in var.dims}
3386                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)
3387             elif dtype_kind in "ObU" and (use_indexers.keys() & var.dims):
3388                 # For types that we do not understand do stepwise
3389                 # interpolation to avoid modifying the elements.
3390                 # reindex the variable instead because it supports
3391                 # booleans and objects and retains the dtype but inside
3392                 # this loop there might be some duplicate code that slows it
3393                 # down, therefore collect these signals and run it later:
3394                 reindex = True
3395             elif all(d not in indexers for d in var.dims):
3396                 # For anything else we can only keep variables if they
3397                 # are not dependent on any coords that are being
3398                 # interpolated along:
3399                 variables[name] = var
3400 
3401         if reindex:
3402             reindex_indexers = {
3403                 k: v for k, (_, v) in validated_indexers.items() if v.dims == (k,)
3404             }
3405             reindexed = alignment.reindex(
3406                 obj,
3407                 indexers=reindex_indexers,
3408                 method=method_non_numeric,
3409                 exclude_vars=variables.keys(),
3410             )
3411             indexes = dict(reindexed._indexes)
3412             variables.update(reindexed.variables)
3413         else:
3414             # Get the indexes that are not being interpolated along
3415             indexes = {k: v for k, v in obj._indexes.items() if k not in indexers}
3416 
3417         # Get the coords that also exist in the variables:
3418         coord_names = obj._coord_names & variables.keys()
3419         selected = self._replace_with_new_dims(
3420             variables.copy(), coord_names, indexes=indexes
3421         )
3422 
3423         # Attach indexer as coordinate
3424         for k, v in indexers.items():
3425             assert isinstance(v, Variable)
3426             if v.dims == (k,):
3427                 index = PandasIndex(v, k, coord_dtype=v.dtype)
3428                 index_vars = index.create_variables({k: v})
3429                 indexes[k] = index
3430                 variables.update(index_vars)
3431             else:
3432                 variables[k] = v
3433 
3434         # Extract coordinates from indexers
3435         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)
3436         variables.update(coord_vars)
3437         indexes.update(new_indexes)
3438 
3439         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()
3440         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3441 
3442     def interp_like(
3443         self,
3444         other: Dataset | DataArray,
3445         method: InterpOptions = "linear",
3446         assume_sorted: bool = False,
3447         kwargs: Mapping[str, Any] | None = None,
3448         method_non_numeric: str = "nearest",
3449     ) -> Dataset:
3450         """Interpolate this object onto the coordinates of another object,
3451         filling the out of range values with NaN.
3452 
3453         If interpolating along a single existing dimension,
3454         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
3455         along multiple existing dimensions, an attempt is made to decompose the
3456         interpolation into multiple 1-dimensional interpolations. If this is
3457         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3458         :py:func:`scipy.interpolate.interpn` is called.
3459 
3460         Parameters
3461         ----------
3462         other : Dataset or DataArray
3463             Object with an 'indexes' attribute giving a mapping from dimension
3464             names to an 1d array-like, which provides coordinates upon
3465             which to index the variables in this dataset. Missing values are skipped.
3466         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3467             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3468             String indicating which method to use for interpolation:
3469 
3470             - 'linear': linear interpolation. Additional keyword
3471               arguments are passed to :py:func:`numpy.interp`
3472             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3473               are passed to :py:func:`scipy.interpolate.interp1d`. If
3474               ``method='polynomial'``, the ``order`` keyword argument must also be
3475               provided.
3476             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3477               respective :py:class:`scipy.interpolate` classes.
3478 
3479         assume_sorted : bool, default: False
3480             If False, values of coordinates that are interpolated over can be
3481             in any order and they are sorted first. If True, interpolated
3482             coordinates are assumed to be an array of monotonically increasing
3483             values.
3484         kwargs : dict, optional
3485             Additional keyword passed to scipy's interpolator.
3486         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3487             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3488             ``"nearest"`` is used by default.
3489 
3490         Returns
3491         -------
3492         interpolated : Dataset
3493             Another dataset by interpolating this dataset's data along the
3494             coordinates of the other object.
3495 
3496         Notes
3497         -----
3498         scipy is required.
3499         If the dataset has object-type coordinates, reindex is used for these
3500         coordinates instead of the interpolation.
3501 
3502         See Also
3503         --------
3504         Dataset.interp
3505         Dataset.reindex_like
3506         """
3507         if kwargs is None:
3508             kwargs = {}
3509 
3510         # pick only dimension coordinates with a single index
3511         coords = {}
3512         other_indexes = other.xindexes
3513         for dim in self.dims:
3514             other_dim_coords = other_indexes.get_all_coords(dim, errors="ignore")
3515             if len(other_dim_coords) == 1:
3516                 coords[dim] = other_dim_coords[dim]
3517 
3518         numeric_coords: dict[Hashable, pd.Index] = {}
3519         object_coords: dict[Hashable, pd.Index] = {}
3520         for k, v in coords.items():
3521             if v.dtype.kind in "uifcMm":
3522                 numeric_coords[k] = v
3523             else:
3524                 object_coords[k] = v
3525 
3526         ds = self
3527         if object_coords:
3528             # We do not support interpolation along object coordinate.
3529             # reindex instead.
3530             ds = self.reindex(object_coords)
3531         return ds.interp(
3532             coords=numeric_coords,
3533             method=method,
3534             assume_sorted=assume_sorted,
3535             kwargs=kwargs,
3536             method_non_numeric=method_non_numeric,
3537         )
3538 
3539     # Helper methods for rename()
3540     def _rename_vars(
3541         self, name_dict, dims_dict
3542     ) -> tuple[dict[Hashable, Variable], set[Hashable]]:
3543         variables = {}
3544         coord_names = set()
3545         for k, v in self.variables.items():
3546             var = v.copy(deep=False)
3547             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3548             name = name_dict.get(k, k)
3549             if name in variables:
3550                 raise ValueError(f"the new name {name!r} conflicts")
3551             variables[name] = var
3552             if k in self._coord_names:
3553                 coord_names.add(name)
3554         return variables, coord_names
3555 
3556     def _rename_dims(self, name_dict: Mapping[Any, Hashable]) -> dict[Hashable, int]:
3557         return {name_dict.get(k, k): v for k, v in self.dims.items()}
3558 
3559     def _rename_indexes(
3560         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3561     ) -> tuple[dict[Hashable, Index], dict[Hashable, Variable]]:
3562         if not self._indexes:
3563             return {}, {}
3564 
3565         indexes = {}
3566         variables = {}
3567 
3568         for index, coord_names in self.xindexes.group_by_index():
3569             new_index = index.rename(name_dict, dims_dict)
3570             new_coord_names = [name_dict.get(k, k) for k in coord_names]
3571             indexes.update({k: new_index for k in new_coord_names})
3572             new_index_vars = new_index.create_variables(
3573                 {
3574                     new: self._variables[old]
3575                     for old, new in zip(coord_names, new_coord_names)
3576                 }
3577             )
3578             variables.update(new_index_vars)
3579 
3580         return indexes, variables
3581 
3582     def _rename_all(
3583         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3584     ) -> tuple[
3585         dict[Hashable, Variable],
3586         set[Hashable],
3587         dict[Hashable, int],
3588         dict[Hashable, Index],
3589     ]:
3590         variables, coord_names = self._rename_vars(name_dict, dims_dict)
3591         dims = self._rename_dims(dims_dict)
3592 
3593         indexes, index_vars = self._rename_indexes(name_dict, dims_dict)
3594         variables = {k: index_vars.get(k, v) for k, v in variables.items()}
3595 
3596         return variables, coord_names, dims, indexes
3597 
3598     def _rename(
3599         self: T_Dataset,
3600         name_dict: Mapping[Any, Hashable] | None = None,
3601         **names: Hashable,
3602     ) -> T_Dataset:
3603         """Also used internally by DataArray so that the warning (if any)
3604         is raised at the right stack level.
3605         """
3606         name_dict = either_dict_or_kwargs(name_dict, names, "rename")
3607         for k in name_dict.keys():
3608             if k not in self and k not in self.dims:
3609                 raise ValueError(
3610                     f"cannot rename {k!r} because it is not a "
3611                     "variable or dimension in this dataset"
3612                 )
3613 
3614             create_dim_coord = False
3615             new_k = name_dict[k]
3616 
3617             if k in self.dims and new_k in self._coord_names:
3618                 coord_dims = self._variables[name_dict[k]].dims
3619                 if coord_dims == (k,):
3620                     create_dim_coord = True
3621             elif k in self._coord_names and new_k in self.dims:
3622                 coord_dims = self._variables[k].dims
3623                 if coord_dims == (new_k,):
3624                     create_dim_coord = True
3625 
3626             if create_dim_coord:
3627                 warnings.warn(
3628                     f"rename {k!r} to {name_dict[k]!r} does not create an index "
3629                     "anymore. Try using swap_dims instead or use set_index "
3630                     "after rename to create an indexed coordinate.",
3631                     UserWarning,
3632                     stacklevel=3,
3633                 )
3634 
3635         variables, coord_names, dims, indexes = self._rename_all(
3636             name_dict=name_dict, dims_dict=name_dict
3637         )
3638         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3639 
3640     def rename(
3641         self: T_Dataset,
3642         name_dict: Mapping[Any, Hashable] | None = None,
3643         **names: Hashable,
3644     ) -> T_Dataset:
3645         """Returns a new object with renamed variables, coordinates and dimensions.
3646 
3647         Parameters
3648         ----------
3649         name_dict : dict-like, optional
3650             Dictionary whose keys are current variable, coordinate or dimension names and
3651             whose values are the desired names.
3652         **names : optional
3653             Keyword form of ``name_dict``.
3654             One of name_dict or names must be provided.
3655 
3656         Returns
3657         -------
3658         renamed : Dataset
3659             Dataset with renamed variables, coordinates and dimensions.
3660 
3661         See Also
3662         --------
3663         Dataset.swap_dims
3664         Dataset.rename_vars
3665         Dataset.rename_dims
3666         DataArray.rename
3667         """
3668         return self._rename(name_dict=name_dict, **names)
3669 
3670     def rename_dims(
3671         self: T_Dataset,
3672         dims_dict: Mapping[Any, Hashable] | None = None,
3673         **dims: Hashable,
3674     ) -> T_Dataset:
3675         """Returns a new object with renamed dimensions only.
3676 
3677         Parameters
3678         ----------
3679         dims_dict : dict-like, optional
3680             Dictionary whose keys are current dimension names and
3681             whose values are the desired names. The desired names must
3682             not be the name of an existing dimension or Variable in the Dataset.
3683         **dims : optional
3684             Keyword form of ``dims_dict``.
3685             One of dims_dict or dims must be provided.
3686 
3687         Returns
3688         -------
3689         renamed : Dataset
3690             Dataset with renamed dimensions.
3691 
3692         See Also
3693         --------
3694         Dataset.swap_dims
3695         Dataset.rename
3696         Dataset.rename_vars
3697         DataArray.rename
3698         """
3699         dims_dict = either_dict_or_kwargs(dims_dict, dims, "rename_dims")
3700         for k, v in dims_dict.items():
3701             if k not in self.dims:
3702                 raise ValueError(
3703                     f"cannot rename {k!r} because it is not a "
3704                     "dimension in this dataset"
3705                 )
3706             if v in self.dims or v in self:
3707                 raise ValueError(
3708                     f"Cannot rename {k} to {v} because {v} already exists. "
3709                     "Try using swap_dims instead."
3710                 )
3711 
3712         variables, coord_names, sizes, indexes = self._rename_all(
3713             name_dict={}, dims_dict=dims_dict
3714         )
3715         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)
3716 
3717     def rename_vars(
3718         self: T_Dataset,
3719         name_dict: Mapping[Any, Hashable] | None = None,
3720         **names: Hashable,
3721     ) -> T_Dataset:
3722         """Returns a new object with renamed variables including coordinates
3723 
3724         Parameters
3725         ----------
3726         name_dict : dict-like, optional
3727             Dictionary whose keys are current variable or coordinate names and
3728             whose values are the desired names.
3729         **names : optional
3730             Keyword form of ``name_dict``.
3731             One of name_dict or names must be provided.
3732 
3733         Returns
3734         -------
3735         renamed : Dataset
3736             Dataset with renamed variables including coordinates
3737 
3738         See Also
3739         --------
3740         Dataset.swap_dims
3741         Dataset.rename
3742         Dataset.rename_dims
3743         DataArray.rename
3744         """
3745         name_dict = either_dict_or_kwargs(name_dict, names, "rename_vars")
3746         for k in name_dict:
3747             if k not in self:
3748                 raise ValueError(
3749                     f"cannot rename {k!r} because it is not a "
3750                     "variable or coordinate in this dataset"
3751                 )
3752         variables, coord_names, dims, indexes = self._rename_all(
3753             name_dict=name_dict, dims_dict={}
3754         )
3755         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3756 
3757     def swap_dims(
3758         self: T_Dataset, dims_dict: Mapping[Any, Hashable] | None = None, **dims_kwargs
3759     ) -> T_Dataset:
3760         """Returns a new object with swapped dimensions.
3761 
3762         Parameters
3763         ----------
3764         dims_dict : dict-like
3765             Dictionary whose keys are current dimension names and whose values
3766             are new names.
3767         **dims_kwargs : {existing_dim: new_dim, ...}, optional
3768             The keyword arguments form of ``dims_dict``.
3769             One of dims_dict or dims_kwargs must be provided.
3770 
3771         Returns
3772         -------
3773         swapped : Dataset
3774             Dataset with swapped dimensions.
3775 
3776         Examples
3777         --------
3778         >>> ds = xr.Dataset(
3779         ...     data_vars={"a": ("x", [5, 7]), "b": ("x", [0.1, 2.4])},
3780         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
3781         ... )
3782         >>> ds
3783         <xarray.Dataset>
3784         Dimensions:  (x: 2)
3785         Coordinates:
3786           * x        (x) <U1 'a' 'b'
3787             y        (x) int64 0 1
3788         Data variables:
3789             a        (x) int64 5 7
3790             b        (x) float64 0.1 2.4
3791 
3792         >>> ds.swap_dims({"x": "y"})
3793         <xarray.Dataset>
3794         Dimensions:  (y: 2)
3795         Coordinates:
3796             x        (y) <U1 'a' 'b'
3797           * y        (y) int64 0 1
3798         Data variables:
3799             a        (y) int64 5 7
3800             b        (y) float64 0.1 2.4
3801 
3802         >>> ds.swap_dims({"x": "z"})
3803         <xarray.Dataset>
3804         Dimensions:  (z: 2)
3805         Coordinates:
3806             x        (z) <U1 'a' 'b'
3807             y        (z) int64 0 1
3808         Dimensions without coordinates: z
3809         Data variables:
3810             a        (z) int64 5 7
3811             b        (z) float64 0.1 2.4
3812 
3813         See Also
3814         --------
3815         Dataset.rename
3816         DataArray.swap_dims
3817         """
3818         # TODO: deprecate this method in favor of a (less confusing)
3819         # rename_dims() method that only renames dimensions.
3820 
3821         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
3822         for k, v in dims_dict.items():
3823             if k not in self.dims:
3824                 raise ValueError(
3825                     f"cannot swap from dimension {k!r} because it is "
3826                     "not an existing dimension"
3827                 )
3828             if v in self.variables and self.variables[v].dims != (k,):
3829                 raise ValueError(
3830                     f"replacement dimension {v!r} is not a 1D "
3831                     f"variable along the old dimension {k!r}"
3832                 )
3833 
3834         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}
3835 
3836         coord_names = self._coord_names.copy()
3837         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})
3838 
3839         variables: dict[Hashable, Variable] = {}
3840         indexes: dict[Hashable, Index] = {}
3841         for k, v in self.variables.items():
3842             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3843             var: Variable
3844             if k in result_dims:
3845                 var = v.to_index_variable()
3846                 var.dims = dims
3847                 if k in self._indexes:
3848                     indexes[k] = self._indexes[k]
3849                     variables[k] = var
3850                 else:
3851                     index, index_vars = create_default_index_implicit(var)
3852                     indexes.update({name: index for name in index_vars})
3853                     variables.update(index_vars)
3854                     coord_names.update(index_vars)
3855             else:
3856                 var = v.to_base_variable()
3857                 var.dims = dims
3858                 variables[k] = var
3859 
3860         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3861 
3862     # change type of self and return to T_Dataset once
3863     # https://github.com/python/mypy/issues/12846 is resolved
3864     def expand_dims(
3865         self,
3866         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
3867         axis: None | int | Sequence[int] = None,
3868         **dim_kwargs: Any,
3869     ) -> Dataset:
3870         """Return a new object with an additional axis (or axes) inserted at
3871         the corresponding position in the array shape.  The new object is a
3872         view into the underlying array, not a copy.
3873 
3874         If dim is already a scalar coordinate, it will be promoted to a 1D
3875         coordinate consisting of a single value.
3876 
3877         Parameters
3878         ----------
3879         dim : hashable, sequence of hashable, mapping, or None
3880             Dimensions to include on the new variable. If provided as hashable
3881             or sequence of hashable, then dimensions are inserted with length
3882             1. If provided as a mapping, then the keys are the new dimensions
3883             and the values are either integers (giving the length of the new
3884             dimensions) or array-like (giving the coordinates of the new
3885             dimensions).
3886         axis : int, sequence of int, or None, default: None
3887             Axis position(s) where new axis is to be inserted (position(s) on
3888             the result array). If a sequence of integers is passed,
3889             multiple axes are inserted. In this case, dim arguments should be
3890             same length list. If axis=None is passed, all the axes will be
3891             inserted to the start of the result array.
3892         **dim_kwargs : int or sequence or ndarray
3893             The keywords are arbitrary dimensions being inserted and the values
3894             are either the lengths of the new dims (if int is given), or their
3895             coordinates. Note, this is an alternative to passing a dict to the
3896             dim kwarg and will only be used if dim is None.
3897 
3898         Returns
3899         -------
3900         expanded : Dataset
3901             This object, but with additional dimension(s).
3902 
3903         See Also
3904         --------
3905         DataArray.expand_dims
3906         """
3907         if dim is None:
3908             pass
3909         elif isinstance(dim, Mapping):
3910             # We're later going to modify dim in place; don't tamper with
3911             # the input
3912             dim = dict(dim)
3913         elif isinstance(dim, int):
3914             raise TypeError(
3915                 "dim should be hashable or sequence of hashables or mapping"
3916             )
3917         elif isinstance(dim, str) or not isinstance(dim, Sequence):
3918             dim = {dim: 1}
3919         elif isinstance(dim, Sequence):
3920             if len(dim) != len(set(dim)):
3921                 raise ValueError("dims should not contain duplicate values.")
3922             dim = {d: 1 for d in dim}
3923 
3924         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
3925         assert isinstance(dim, MutableMapping)
3926 
3927         if axis is None:
3928             axis = list(range(len(dim)))
3929         elif not isinstance(axis, Sequence):
3930             axis = [axis]
3931 
3932         if len(dim) != len(axis):
3933             raise ValueError("lengths of dim and axis should be identical.")
3934         for d in dim:
3935             if d in self.dims:
3936                 raise ValueError(f"Dimension {d} already exists.")
3937             if d in self._variables and not utils.is_scalar(self._variables[d]):
3938                 raise ValueError(
3939                     "{dim} already exists as coordinate or"
3940                     " variable name.".format(dim=d)
3941                 )
3942 
3943         variables: dict[Hashable, Variable] = {}
3944         indexes: dict[Hashable, Index] = dict(self._indexes)
3945         coord_names = self._coord_names.copy()
3946         # If dim is a dict, then ensure that the values are either integers
3947         # or iterables.
3948         for k, v in dim.items():
3949             if hasattr(v, "__iter__"):
3950                 # If the value for the new dimension is an iterable, then
3951                 # save the coordinates to the variables dict, and set the
3952                 # value within the dim dict to the length of the iterable
3953                 # for later use.
3954                 index = PandasIndex(v, k)
3955                 indexes[k] = index
3956                 variables.update(index.create_variables())
3957                 coord_names.add(k)
3958                 dim[k] = variables[k].size
3959             elif isinstance(v, int):
3960                 pass  # Do nothing if the dimensions value is just an int
3961             else:
3962                 raise TypeError(
3963                     "The value of new dimension {k} must be "
3964                     "an iterable or an int".format(k=k)
3965                 )
3966 
3967         for k, v in self._variables.items():
3968             if k not in dim:
3969                 if k in coord_names:  # Do not change coordinates
3970                     variables[k] = v
3971                 else:
3972                     result_ndim = len(v.dims) + len(axis)
3973                     for a in axis:
3974                         if a < -result_ndim or result_ndim - 1 < a:
3975                             raise IndexError(
3976                                 f"Axis {a} of variable {k} is out of bounds of the "
3977                                 f"expanded dimension size {result_ndim}"
3978                             )
3979 
3980                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]
3981                     if len(axis_pos) != len(set(axis_pos)):
3982                         raise ValueError("axis should not contain duplicate values")
3983                     # We need to sort them to make sure `axis` equals to the
3984                     # axis positions of the result array.
3985                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))
3986 
3987                     all_dims = list(zip(v.dims, v.shape))
3988                     for d, c in zip_axis_dim:
3989                         all_dims.insert(d, c)
3990                     variables[k] = v.set_dims(dict(all_dims))
3991             else:
3992                 if k not in variables:
3993                     # If dims includes a label of a non-dimension coordinate,
3994                     # it will be promoted to a 1D coordinate with a single value.
3995                     index, index_vars = create_default_index_implicit(v.set_dims(k))
3996                     indexes[k] = index
3997                     variables.update(index_vars)
3998 
3999         return self._replace_with_new_dims(
4000             variables, coord_names=coord_names, indexes=indexes
4001         )
4002 
4003     # change type of self and return to T_Dataset once
4004     # https://github.com/python/mypy/issues/12846 is resolved
4005     def set_index(
4006         self,
4007         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,
4008         append: bool = False,
4009         **indexes_kwargs: Hashable | Sequence[Hashable],
4010     ) -> Dataset:
4011         """Set Dataset (multi-)indexes using one or more existing coordinates
4012         or variables.
4013 
4014         This legacy method is limited to pandas (multi-)indexes and
4015         1-dimensional "dimension" coordinates. See
4016         :py:meth:`~Dataset.set_xindex` for setting a pandas or a custom
4017         Xarray-compatible index from one or more arbitrary coordinates.
4018 
4019         Parameters
4020         ----------
4021         indexes : {dim: index, ...}
4022             Mapping from names matching dimensions and values given
4023             by (lists of) the names of existing coordinates or variables to set
4024             as new (multi-)index.
4025         append : bool, default: False
4026             If True, append the supplied index(es) to the existing index(es).
4027             Otherwise replace the existing index(es) (default).
4028         **indexes_kwargs : optional
4029             The keyword arguments form of ``indexes``.
4030             One of indexes or indexes_kwargs must be provided.
4031 
4032         Returns
4033         -------
4034         obj : Dataset
4035             Another dataset, with this dataset's data but replaced coordinates.
4036 
4037         Examples
4038         --------
4039         >>> arr = xr.DataArray(
4040         ...     data=np.ones((2, 3)),
4041         ...     dims=["x", "y"],
4042         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
4043         ... )
4044         >>> ds = xr.Dataset({"v": arr})
4045         >>> ds
4046         <xarray.Dataset>
4047         Dimensions:  (x: 2, y: 3)
4048         Coordinates:
4049           * x        (x) int64 0 1
4050           * y        (y) int64 0 1 2
4051             a        (x) int64 3 4
4052         Data variables:
4053             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
4054         >>> ds.set_index(x="a")
4055         <xarray.Dataset>
4056         Dimensions:  (x: 2, y: 3)
4057         Coordinates:
4058           * x        (x) int64 3 4
4059           * y        (y) int64 0 1 2
4060         Data variables:
4061             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
4062 
4063         See Also
4064         --------
4065         Dataset.reset_index
4066         Dataset.set_xindex
4067         Dataset.swap_dims
4068         """
4069         dim_coords = either_dict_or_kwargs(indexes, indexes_kwargs, "set_index")
4070 
4071         new_indexes: dict[Hashable, Index] = {}
4072         new_variables: dict[Hashable, Variable] = {}
4073         drop_indexes: set[Hashable] = set()
4074         drop_variables: set[Hashable] = set()
4075         replace_dims: dict[Hashable, Hashable] = {}
4076         all_var_names: set[Hashable] = set()
4077 
4078         for dim, _var_names in dim_coords.items():
4079             if isinstance(_var_names, str) or not isinstance(_var_names, Sequence):
4080                 var_names = [_var_names]
4081             else:
4082                 var_names = list(_var_names)
4083 
4084             invalid_vars = set(var_names) - set(self._variables)
4085             if invalid_vars:
4086                 raise ValueError(
4087                     ", ".join([str(v) for v in invalid_vars])
4088                     + " variable(s) do not exist"
4089                 )
4090 
4091             all_var_names.update(var_names)
4092             drop_variables.update(var_names)
4093 
4094             # drop any pre-existing index involved and its corresponding coordinates
4095             index_coord_names = self.xindexes.get_all_coords(dim, errors="ignore")
4096             all_index_coord_names = set(index_coord_names)
4097             for k in var_names:
4098                 all_index_coord_names.update(
4099                     self.xindexes.get_all_coords(k, errors="ignore")
4100                 )
4101 
4102             drop_indexes.update(all_index_coord_names)
4103             drop_variables.update(all_index_coord_names)
4104 
4105             if len(var_names) == 1 and (not append or dim not in self._indexes):
4106                 var_name = var_names[0]
4107                 var = self._variables[var_name]
4108                 if var.dims != (dim,):
4109                     raise ValueError(
4110                         f"dimension mismatch: try setting an index for dimension {dim!r} with "
4111                         f"variable {var_name!r} that has dimensions {var.dims}"
4112                     )
4113                 idx = PandasIndex.from_variables({dim: var}, options={})
4114                 idx_vars = idx.create_variables({var_name: var})
4115 
4116                 # trick to preserve coordinate order in this case
4117                 if dim in self._coord_names:
4118                     drop_variables.remove(dim)
4119             else:
4120                 if append:
4121                     current_variables = {
4122                         k: self._variables[k] for k in index_coord_names
4123                     }
4124                 else:
4125                     current_variables = {}
4126                 idx, idx_vars = PandasMultiIndex.from_variables_maybe_expand(
4127                     dim,
4128                     current_variables,
4129                     {k: self._variables[k] for k in var_names},
4130                 )
4131                 for n in idx.index.names:
4132                     replace_dims[n] = dim
4133 
4134             new_indexes.update({k: idx for k in idx_vars})
4135             new_variables.update(idx_vars)
4136 
4137         # re-add deindexed coordinates (convert to base variables)
4138         for k in drop_variables:
4139             if (
4140                 k not in new_variables
4141                 and k not in all_var_names
4142                 and k in self._coord_names
4143             ):
4144                 new_variables[k] = self._variables[k].to_base_variable()
4145 
4146         indexes_: dict[Any, Index] = {
4147             k: v for k, v in self._indexes.items() if k not in drop_indexes
4148         }
4149         indexes_.update(new_indexes)
4150 
4151         variables = {
4152             k: v for k, v in self._variables.items() if k not in drop_variables
4153         }
4154         variables.update(new_variables)
4155 
4156         # update dimensions if necessary, GH: 3512
4157         for k, v in variables.items():
4158             if any(d in replace_dims for d in v.dims):
4159                 new_dims = [replace_dims.get(d, d) for d in v.dims]
4160                 variables[k] = v._replace(dims=new_dims)
4161 
4162         coord_names = self._coord_names - drop_variables | set(new_variables)
4163 
4164         return self._replace_with_new_dims(
4165             variables, coord_names=coord_names, indexes=indexes_
4166         )
4167 
4168     def reset_index(
4169         self: T_Dataset,
4170         dims_or_levels: Hashable | Sequence[Hashable],
4171         drop: bool = False,
4172     ) -> T_Dataset:
4173         """Reset the specified index(es) or multi-index level(s).
4174 
4175         This legacy method is specific to pandas (multi-)indexes and
4176         1-dimensional "dimension" coordinates. See the more generic
4177         :py:meth:`~Dataset.drop_indexes` and :py:meth:`~Dataset.set_xindex`
4178         method to respectively drop and set pandas or custom indexes for
4179         arbitrary coordinates.
4180 
4181         Parameters
4182         ----------
4183         dims_or_levels : Hashable or Sequence of Hashable
4184             Name(s) of the dimension(s) and/or multi-index level(s) that will
4185             be reset.
4186         drop : bool, default: False
4187             If True, remove the specified indexes and/or multi-index levels
4188             instead of extracting them as new coordinates (default: False).
4189 
4190         Returns
4191         -------
4192         obj : Dataset
4193             Another dataset, with this dataset's data but replaced coordinates.
4194 
4195         See Also
4196         --------
4197         Dataset.set_index
4198         Dataset.set_xindex
4199         Dataset.drop_indexes
4200         """
4201         if isinstance(dims_or_levels, str) or not isinstance(dims_or_levels, Sequence):
4202             dims_or_levels = [dims_or_levels]
4203 
4204         invalid_coords = set(dims_or_levels) - set(self._indexes)
4205         if invalid_coords:
4206             raise ValueError(
4207                 f"{tuple(invalid_coords)} are not coordinates with an index"
4208             )
4209 
4210         drop_indexes: set[Hashable] = set()
4211         drop_variables: set[Hashable] = set()
4212         seen: set[Index] = set()
4213         new_indexes: dict[Hashable, Index] = {}
4214         new_variables: dict[Hashable, Variable] = {}
4215 
4216         def drop_or_convert(var_names):
4217             if drop:
4218                 drop_variables.update(var_names)
4219             else:
4220                 base_vars = {
4221                     k: self._variables[k].to_base_variable() for k in var_names
4222                 }
4223                 new_variables.update(base_vars)
4224 
4225         for name in dims_or_levels:
4226             index = self._indexes[name]
4227 
4228             if index in seen:
4229                 continue
4230             seen.add(index)
4231 
4232             idx_var_names = set(self.xindexes.get_all_coords(name))
4233             drop_indexes.update(idx_var_names)
4234 
4235             if isinstance(index, PandasMultiIndex):
4236                 # special case for pd.MultiIndex
4237                 level_names = index.index.names
4238                 keep_level_vars = {
4239                     k: self._variables[k]
4240                     for k in level_names
4241                     if k not in dims_or_levels
4242                 }
4243 
4244                 if index.dim not in dims_or_levels and keep_level_vars:
4245                     # do not drop the multi-index completely
4246                     # instead replace it by a new (multi-)index with dropped level(s)
4247                     idx = index.keep_levels(keep_level_vars)
4248                     idx_vars = idx.create_variables(keep_level_vars)
4249                     new_indexes.update({k: idx for k in idx_vars})
4250                     new_variables.update(idx_vars)
4251                     if not isinstance(idx, PandasMultiIndex):
4252                         # multi-index reduced to single index
4253                         # backward compatibility: unique level coordinate renamed to dimension
4254                         drop_variables.update(keep_level_vars)
4255                     drop_or_convert(
4256                         [k for k in level_names if k not in keep_level_vars]
4257                     )
4258                 else:
4259                     # always drop the multi-index dimension variable
4260                     drop_variables.add(index.dim)
4261                     drop_or_convert(level_names)
4262             else:
4263                 drop_or_convert(idx_var_names)
4264 
4265         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4266         indexes.update(new_indexes)
4267 
4268         variables = {
4269             k: v for k, v in self._variables.items() if k not in drop_variables
4270         }
4271         variables.update(new_variables)
4272 
4273         coord_names = self._coord_names - drop_variables
4274 
4275         return self._replace_with_new_dims(
4276             variables, coord_names=coord_names, indexes=indexes
4277         )
4278 
4279     def set_xindex(
4280         self: T_Dataset,
4281         coord_names: str | Sequence[Hashable],
4282         index_cls: type[Index] | None = None,
4283         **options,
4284     ) -> T_Dataset:
4285         """Set a new, Xarray-compatible index from one or more existing
4286         coordinate(s).
4287 
4288         Parameters
4289         ----------
4290         coord_names : str or list
4291             Name(s) of the coordinate(s) used to build the index.
4292             If several names are given, their order matters.
4293         index_cls : subclass of :class:`~xarray.indexes.Index`, optional
4294             The type of index to create. By default, try setting
4295             a ``PandasIndex`` if ``len(coord_names) == 1``,
4296             otherwise a ``PandasMultiIndex``.
4297         **options
4298             Options passed to the index constructor.
4299 
4300         Returns
4301         -------
4302         obj : Dataset
4303             Another dataset, with this dataset's data and with a new index.
4304 
4305         """
4306         # the Sequence check is required for mypy
4307         if is_scalar(coord_names) or not isinstance(coord_names, Sequence):
4308             coord_names = [coord_names]
4309 
4310         if index_cls is None:
4311             if len(coord_names) == 1:
4312                 index_cls = PandasIndex
4313             else:
4314                 index_cls = PandasMultiIndex
4315         else:
4316             if not issubclass(index_cls, Index):
4317                 raise TypeError(f"{index_cls} is not a subclass of xarray.Index")
4318 
4319         invalid_coords = set(coord_names) - self._coord_names
4320 
4321         if invalid_coords:
4322             msg = ["invalid coordinate(s)"]
4323             no_vars = invalid_coords - set(self._variables)
4324             data_vars = invalid_coords - no_vars
4325             if no_vars:
4326                 msg.append(f"those variables don't exist: {no_vars}")
4327             if data_vars:
4328                 msg.append(
4329                     f"those variables are data variables: {data_vars}, use `set_coords` first"
4330                 )
4331             raise ValueError("\n".join(msg))
4332 
4333         # we could be more clever here (e.g., drop-in index replacement if index
4334         # coordinates do not conflict), but let's not allow this for now
4335         indexed_coords = set(coord_names) & set(self._indexes)
4336 
4337         if indexed_coords:
4338             raise ValueError(
4339                 f"those coordinates already have an index: {indexed_coords}"
4340             )
4341 
4342         coord_vars = {name: self._variables[name] for name in coord_names}
4343 
4344         index = index_cls.from_variables(coord_vars, options=options)
4345 
4346         new_coord_vars = index.create_variables(coord_vars)
4347 
4348         # special case for setting a pandas multi-index from level coordinates
4349         # TODO: remove it once we depreciate pandas multi-index dimension (tuple
4350         # elements) coordinate
4351         if isinstance(index, PandasMultiIndex):
4352             coord_names = [index.dim] + list(coord_names)
4353 
4354         variables: dict[Hashable, Variable]
4355         indexes: dict[Hashable, Index]
4356 
4357         if len(coord_names) == 1:
4358             variables = self._variables.copy()
4359             indexes = self._indexes.copy()
4360 
4361             name = list(coord_names).pop()
4362             if name in new_coord_vars:
4363                 variables[name] = new_coord_vars[name]
4364             indexes[name] = index
4365         else:
4366             # reorder variables and indexes so that coordinates having the same
4367             # index are next to each other
4368             variables = {}
4369             for name, var in self._variables.items():
4370                 if name not in coord_names:
4371                     variables[name] = var
4372 
4373             indexes = {}
4374             for name, idx in self._indexes.items():
4375                 if name not in coord_names:
4376                     indexes[name] = idx
4377 
4378             for name in coord_names:
4379                 try:
4380                     variables[name] = new_coord_vars[name]
4381                 except KeyError:
4382                     variables[name] = self._variables[name]
4383                 indexes[name] = index
4384 
4385         return self._replace(
4386             variables=variables,
4387             coord_names=self._coord_names | set(coord_names),
4388             indexes=indexes,
4389         )
4390 
4391     def reorder_levels(
4392         self: T_Dataset,
4393         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
4394         **dim_order_kwargs: Sequence[int | Hashable],
4395     ) -> T_Dataset:
4396         """Rearrange index levels using input order.
4397 
4398         Parameters
4399         ----------
4400         dim_order : dict-like of Hashable to Sequence of int or Hashable, optional
4401             Mapping from names matching dimensions and values given
4402             by lists representing new level orders. Every given dimension
4403             must have a multi-index.
4404         **dim_order_kwargs : Sequence of int or Hashable, optional
4405             The keyword arguments form of ``dim_order``.
4406             One of dim_order or dim_order_kwargs must be provided.
4407 
4408         Returns
4409         -------
4410         obj : Dataset
4411             Another dataset, with this dataset's data but replaced
4412             coordinates.
4413         """
4414         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
4415         variables = self._variables.copy()
4416         indexes = dict(self._indexes)
4417         new_indexes: dict[Hashable, Index] = {}
4418         new_variables: dict[Hashable, IndexVariable] = {}
4419 
4420         for dim, order in dim_order.items():
4421             index = self._indexes[dim]
4422 
4423             if not isinstance(index, PandasMultiIndex):
4424                 raise ValueError(f"coordinate {dim} has no MultiIndex")
4425 
4426             level_vars = {k: self._variables[k] for k in order}
4427             idx = index.reorder_levels(level_vars)
4428             idx_vars = idx.create_variables(level_vars)
4429             new_indexes.update({k: idx for k in idx_vars})
4430             new_variables.update(idx_vars)
4431 
4432         indexes = {k: v for k, v in self._indexes.items() if k not in new_indexes}
4433         indexes.update(new_indexes)
4434 
4435         variables = {k: v for k, v in self._variables.items() if k not in new_variables}
4436         variables.update(new_variables)
4437 
4438         return self._replace(variables, indexes=indexes)
4439 
4440     def _get_stack_index(
4441         self,
4442         dim,
4443         multi=False,
4444         create_index=False,
4445     ) -> tuple[Index | None, dict[Hashable, Variable]]:
4446         """Used by stack and unstack to get one pandas (multi-)index among
4447         the indexed coordinates along dimension `dim`.
4448 
4449         If exactly one index is found, return it with its corresponding
4450         coordinate variables(s), otherwise return None and an empty dict.
4451 
4452         If `create_index=True`, create a new index if none is found or raise
4453         an error if multiple indexes are found.
4454 
4455         """
4456         stack_index: Index | None = None
4457         stack_coords: dict[Hashable, Variable] = {}
4458 
4459         for name, index in self._indexes.items():
4460             var = self._variables[name]
4461             if (
4462                 var.ndim == 1
4463                 and var.dims[0] == dim
4464                 and (
4465                     # stack: must be a single coordinate index
4466                     not multi
4467                     and not self.xindexes.is_multi(name)
4468                     # unstack: must be an index that implements .unstack
4469                     or multi
4470                     and type(index).unstack is not Index.unstack
4471                 )
4472             ):
4473                 if stack_index is not None and index is not stack_index:
4474                     # more than one index found, stop
4475                     if create_index:
4476                         raise ValueError(
4477                             f"cannot stack dimension {dim!r} with `create_index=True` "
4478                             "and with more than one index found along that dimension"
4479                         )
4480                     return None, {}
4481                 stack_index = index
4482                 stack_coords[name] = var
4483 
4484         if create_index and stack_index is None:
4485             if dim in self._variables:
4486                 var = self._variables[dim]
4487             else:
4488                 _, _, var = _get_virtual_variable(self._variables, dim, self.dims)
4489             # dummy index (only `stack_coords` will be used to construct the multi-index)
4490             stack_index = PandasIndex([0], dim)
4491             stack_coords = {dim: var}
4492 
4493         return stack_index, stack_coords
4494 
4495     def _stack_once(
4496         self: T_Dataset,
4497         dims: Sequence[Hashable | ellipsis],
4498         new_dim: Hashable,
4499         index_cls: type[Index],
4500         create_index: bool | None = True,
4501     ) -> T_Dataset:
4502         if dims == ...:
4503             raise ValueError("Please use [...] for dims, rather than just ...")
4504         if ... in dims:
4505             dims = list(infix_dims(dims, self.dims))
4506 
4507         new_variables: dict[Hashable, Variable] = {}
4508         stacked_var_names: list[Hashable] = []
4509         drop_indexes: list[Hashable] = []
4510 
4511         for name, var in self.variables.items():
4512             if any(d in var.dims for d in dims):
4513                 add_dims = [d for d in dims if d not in var.dims]
4514                 vdims = list(var.dims) + add_dims
4515                 shape = [self.dims[d] for d in vdims]
4516                 exp_var = var.set_dims(vdims, shape)
4517                 stacked_var = exp_var.stack(**{new_dim: dims})
4518                 new_variables[name] = stacked_var
4519                 stacked_var_names.append(name)
4520             else:
4521                 new_variables[name] = var.copy(deep=False)
4522 
4523         # drop indexes of stacked coordinates (if any)
4524         for name in stacked_var_names:
4525             drop_indexes += list(self.xindexes.get_all_coords(name, errors="ignore"))
4526 
4527         new_indexes = {}
4528         new_coord_names = set(self._coord_names)
4529         if create_index or create_index is None:
4530             product_vars: dict[Any, Variable] = {}
4531             for dim in dims:
4532                 idx, idx_vars = self._get_stack_index(dim, create_index=create_index)
4533                 if idx is not None:
4534                     product_vars.update(idx_vars)
4535 
4536             if len(product_vars) == len(dims):
4537                 idx = index_cls.stack(product_vars, new_dim)
4538                 new_indexes[new_dim] = idx
4539                 new_indexes.update({k: idx for k in product_vars})
4540                 idx_vars = idx.create_variables(product_vars)
4541                 # keep consistent multi-index coordinate order
4542                 for k in idx_vars:
4543                     new_variables.pop(k, None)
4544                 new_variables.update(idx_vars)
4545                 new_coord_names.update(idx_vars)
4546 
4547         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4548         indexes.update(new_indexes)
4549 
4550         return self._replace_with_new_dims(
4551             new_variables, coord_names=new_coord_names, indexes=indexes
4552         )
4553 
4554     def stack(
4555         self: T_Dataset,
4556         dimensions: Mapping[Any, Sequence[Hashable | ellipsis]] | None = None,
4557         create_index: bool | None = True,
4558         index_cls: type[Index] = PandasMultiIndex,
4559         **dimensions_kwargs: Sequence[Hashable | ellipsis],
4560     ) -> T_Dataset:
4561         """
4562         Stack any number of existing dimensions into a single new dimension.
4563 
4564         New dimensions will be added at the end, and by default the corresponding
4565         coordinate variables will be combined into a MultiIndex.
4566 
4567         Parameters
4568         ----------
4569         dimensions : mapping of hashable to sequence of hashable
4570             Mapping of the form `new_name=(dim1, dim2, ...)`. Names of new
4571             dimensions, and the existing dimensions that they replace. An
4572             ellipsis (`...`) will be replaced by all unlisted dimensions.
4573             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
4574             all dimensions.
4575         create_index : bool or None, default: True
4576 
4577             - True: create a multi-index for each of the stacked dimensions.
4578             - False: don't create any index.
4579             - None. create a multi-index only if exactly one single (1-d) coordinate
4580               index is found for every dimension to stack.
4581 
4582         index_cls: Index-class, default: PandasMultiIndex
4583             Can be used to pass a custom multi-index type (must be an Xarray index that
4584             implements `.stack()`). By default, a pandas multi-index wrapper is used.
4585         **dimensions_kwargs
4586             The keyword arguments form of ``dimensions``.
4587             One of dimensions or dimensions_kwargs must be provided.
4588 
4589         Returns
4590         -------
4591         stacked : Dataset
4592             Dataset with stacked data.
4593 
4594         See Also
4595         --------
4596         Dataset.unstack
4597         """
4598         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
4599         result = self
4600         for new_dim, dims in dimensions.items():
4601             result = result._stack_once(dims, new_dim, index_cls, create_index)
4602         return result
4603 
4604     def to_stacked_array(
4605         self,
4606         new_dim: Hashable,
4607         sample_dims: Collection[Hashable],
4608         variable_dim: Hashable = "variable",
4609         name: Hashable | None = None,
4610     ) -> DataArray:
4611         """Combine variables of differing dimensionality into a DataArray
4612         without broadcasting.
4613 
4614         This method is similar to Dataset.to_array but does not broadcast the
4615         variables.
4616 
4617         Parameters
4618         ----------
4619         new_dim : hashable
4620             Name of the new stacked coordinate
4621         sample_dims : Collection of hashables
4622             List of dimensions that **will not** be stacked. Each array in the
4623             dataset must share these dimensions. For machine learning
4624             applications, these define the dimensions over which samples are
4625             drawn.
4626         variable_dim : hashable, default: "variable"
4627             Name of the level in the stacked coordinate which corresponds to
4628             the variables.
4629         name : hashable, optional
4630             Name of the new data array.
4631 
4632         Returns
4633         -------
4634         stacked : DataArray
4635             DataArray with the specified dimensions and data variables
4636             stacked together. The stacked coordinate is named ``new_dim``
4637             and represented by a MultiIndex object with a level containing the
4638             data variable names. The name of this level is controlled using
4639             the ``variable_dim`` argument.
4640 
4641         See Also
4642         --------
4643         Dataset.to_array
4644         Dataset.stack
4645         DataArray.to_unstacked_dataset
4646 
4647         Examples
4648         --------
4649         >>> data = xr.Dataset(
4650         ...     data_vars={
4651         ...         "a": (("x", "y"), [[0, 1, 2], [3, 4, 5]]),
4652         ...         "b": ("x", [6, 7]),
4653         ...     },
4654         ...     coords={"y": ["u", "v", "w"]},
4655         ... )
4656 
4657         >>> data
4658         <xarray.Dataset>
4659         Dimensions:  (x: 2, y: 3)
4660         Coordinates:
4661           * y        (y) <U1 'u' 'v' 'w'
4662         Dimensions without coordinates: x
4663         Data variables:
4664             a        (x, y) int64 0 1 2 3 4 5
4665             b        (x) int64 6 7
4666 
4667         >>> data.to_stacked_array("z", sample_dims=["x"])
4668         <xarray.DataArray 'a' (x: 2, z: 4)>
4669         array([[0, 1, 2, 6],
4670                [3, 4, 5, 7]])
4671         Coordinates:
4672           * z         (z) object MultiIndex
4673           * variable  (z) object 'a' 'a' 'a' 'b'
4674           * y         (z) object 'u' 'v' 'w' nan
4675         Dimensions without coordinates: x
4676 
4677         """
4678         from xarray.core.concat import concat
4679 
4680         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)
4681 
4682         for variable in self:
4683             dims = self[variable].dims
4684             dims_include_sample_dims = set(sample_dims) <= set(dims)
4685             if not dims_include_sample_dims:
4686                 raise ValueError(
4687                     "All variables in the dataset must contain the "
4688                     "dimensions {}.".format(dims)
4689                 )
4690 
4691         def ensure_stackable(val):
4692             assign_coords = {variable_dim: val.name}
4693             for dim in stacking_dims:
4694                 if dim not in val.dims:
4695                     assign_coords[dim] = None
4696 
4697             expand_dims = set(stacking_dims).difference(set(val.dims))
4698             expand_dims.add(variable_dim)
4699             # must be list for .expand_dims
4700             expand_dims = list(expand_dims)
4701 
4702             return (
4703                 val.assign_coords(**assign_coords)
4704                 .expand_dims(expand_dims)
4705                 .stack({new_dim: (variable_dim,) + stacking_dims})
4706             )
4707 
4708         # concatenate the arrays
4709         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]
4710         data_array = concat(stackable_vars, dim=new_dim)
4711 
4712         if name is not None:
4713             data_array.name = name
4714 
4715         return data_array
4716 
4717     def _unstack_once(
4718         self: T_Dataset,
4719         dim: Hashable,
4720         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4721         fill_value,
4722         sparse: bool = False,
4723     ) -> T_Dataset:
4724         index, index_vars = index_and_vars
4725         variables: dict[Hashable, Variable] = {}
4726         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4727 
4728         new_indexes, clean_index = index.unstack()
4729         indexes.update(new_indexes)
4730 
4731         for name, idx in new_indexes.items():
4732             variables.update(idx.create_variables(index_vars))
4733 
4734         for name, var in self.variables.items():
4735             if name not in index_vars:
4736                 if dim in var.dims:
4737                     if isinstance(fill_value, Mapping):
4738                         fill_value_ = fill_value[name]
4739                     else:
4740                         fill_value_ = fill_value
4741 
4742                     variables[name] = var._unstack_once(
4743                         index=clean_index,
4744                         dim=dim,
4745                         fill_value=fill_value_,
4746                         sparse=sparse,
4747                     )
4748                 else:
4749                     variables[name] = var
4750 
4751         coord_names = set(self._coord_names) - {dim} | set(new_indexes)
4752 
4753         return self._replace_with_new_dims(
4754             variables, coord_names=coord_names, indexes=indexes
4755         )
4756 
4757     def _unstack_full_reindex(
4758         self: T_Dataset,
4759         dim: Hashable,
4760         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4761         fill_value,
4762         sparse: bool,
4763     ) -> T_Dataset:
4764         index, index_vars = index_and_vars
4765         variables: dict[Hashable, Variable] = {}
4766         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4767 
4768         new_indexes, clean_index = index.unstack()
4769         indexes.update(new_indexes)
4770 
4771         new_index_variables = {}
4772         for name, idx in new_indexes.items():
4773             new_index_variables.update(idx.create_variables(index_vars))
4774 
4775         new_dim_sizes = {k: v.size for k, v in new_index_variables.items()}
4776         variables.update(new_index_variables)
4777 
4778         # take a shortcut in case the MultiIndex was not modified.
4779         full_idx = pd.MultiIndex.from_product(
4780             clean_index.levels, names=clean_index.names
4781         )
4782         if clean_index.equals(full_idx):
4783             obj = self
4784         else:
4785             # TODO: we may depreciate implicit re-indexing with a pandas.MultiIndex
4786             xr_full_idx = PandasMultiIndex(full_idx, dim)
4787             indexers = Indexes(
4788                 {k: xr_full_idx for k in index_vars},
4789                 xr_full_idx.create_variables(index_vars),
4790             )
4791             obj = self._reindex(
4792                 indexers, copy=False, fill_value=fill_value, sparse=sparse
4793             )
4794 
4795         for name, var in obj.variables.items():
4796             if name not in index_vars:
4797                 if dim in var.dims:
4798                     variables[name] = var.unstack({dim: new_dim_sizes})
4799                 else:
4800                     variables[name] = var
4801 
4802         coord_names = set(self._coord_names) - {dim} | set(new_dim_sizes)
4803 
4804         return self._replace_with_new_dims(
4805             variables, coord_names=coord_names, indexes=indexes
4806         )
4807 
4808     def unstack(
4809         self: T_Dataset,
4810         dim: Dims = None,
4811         fill_value: Any = xrdtypes.NA,
4812         sparse: bool = False,
4813     ) -> T_Dataset:
4814         """
4815         Unstack existing dimensions corresponding to MultiIndexes into
4816         multiple new dimensions.
4817 
4818         New dimensions will be added at the end.
4819 
4820         Parameters
4821         ----------
4822         dim : str, Iterable of Hashable or None, optional
4823             Dimension(s) over which to unstack. By default unstacks all
4824             MultiIndexes.
4825         fill_value : scalar or dict-like, default: nan
4826             value to be filled. If a dict-like, maps variable names to
4827             fill values. If not provided or if the dict-like does not
4828             contain all variables, the dtype's NA value will be used.
4829         sparse : bool, default: False
4830             use sparse-array if True
4831 
4832         Returns
4833         -------
4834         unstacked : Dataset
4835             Dataset with unstacked data.
4836 
4837         See Also
4838         --------
4839         Dataset.stack
4840         """
4841 
4842         if dim is None:
4843             dims = list(self.dims)
4844         else:
4845             if isinstance(dim, str) or not isinstance(dim, Iterable):
4846                 dims = [dim]
4847             else:
4848                 dims = list(dim)
4849 
4850             missing_dims = [d for d in dims if d not in self.dims]
4851             if missing_dims:
4852                 raise ValueError(
4853                     f"Dataset does not contain the dimensions: {missing_dims}"
4854                 )
4855 
4856         # each specified dimension must have exactly one multi-index
4857         stacked_indexes: dict[Any, tuple[Index, dict[Hashable, Variable]]] = {}
4858         for d in dims:
4859             idx, idx_vars = self._get_stack_index(d, multi=True)
4860             if idx is not None:
4861                 stacked_indexes[d] = idx, idx_vars
4862 
4863         if dim is None:
4864             dims = list(stacked_indexes)
4865         else:
4866             non_multi_dims = set(dims) - set(stacked_indexes)
4867             if non_multi_dims:
4868                 raise ValueError(
4869                     "cannot unstack dimensions that do not "
4870                     f"have exactly one multi-index: {tuple(non_multi_dims)}"
4871                 )
4872 
4873         result = self.copy(deep=False)
4874 
4875         # we want to avoid allocating an object-dtype ndarray for a MultiIndex,
4876         # so we can't just access self.variables[v].data for every variable.
4877         # We only check the non-index variables.
4878         # https://github.com/pydata/xarray/issues/5902
4879         nonindexes = [
4880             self.variables[k] for k in set(self.variables) - set(self._indexes)
4881         ]
4882         # Notes for each of these cases:
4883         # 1. Dask arrays don't support assignment by index, which the fast unstack
4884         #    function requires.
4885         #    https://github.com/pydata/xarray/pull/4746#issuecomment-753282125
4886         # 2. Sparse doesn't currently support (though we could special-case it)
4887         #    https://github.com/pydata/sparse/issues/422
4888         # 3. pint requires checking if it's a NumPy array until
4889         #    https://github.com/pydata/xarray/pull/4751 is resolved,
4890         #    Once that is resolved, explicitly exclude pint arrays.
4891         #    pint doesn't implement `np.full_like` in a way that's
4892         #    currently compatible.
4893         sparse_array_type = array_type("sparse")
4894         needs_full_reindex = any(
4895             is_duck_dask_array(v.data)
4896             or isinstance(v.data, sparse_array_type)
4897             or not isinstance(v.data, np.ndarray)
4898             for v in nonindexes
4899         )
4900 
4901         for d in dims:
4902             if needs_full_reindex:
4903                 result = result._unstack_full_reindex(
4904                     d, stacked_indexes[d], fill_value, sparse
4905                 )
4906             else:
4907                 result = result._unstack_once(d, stacked_indexes[d], fill_value, sparse)
4908         return result
4909 
4910     def update(self: T_Dataset, other: CoercibleMapping) -> T_Dataset:
4911         """Update this dataset's variables with those from another dataset.
4912 
4913         Just like :py:meth:`dict.update` this is a in-place operation.
4914         For a non-inplace version, see :py:meth:`Dataset.merge`.
4915 
4916         Parameters
4917         ----------
4918         other : Dataset or mapping
4919             Variables with which to update this dataset. One of:
4920 
4921             - Dataset
4922             - mapping {var name: DataArray}
4923             - mapping {var name: Variable}
4924             - mapping {var name: (dimension name, array-like)}
4925             - mapping {var name: (tuple of dimension names, array-like)}
4926 
4927         Returns
4928         -------
4929         updated : Dataset
4930             Updated dataset. Note that since the update is in-place this is the input
4931             dataset.
4932 
4933             It is deprecated since version 0.17 and scheduled to be removed in 0.21.
4934 
4935         Raises
4936         ------
4937         ValueError
4938             If any dimensions would have inconsistent sizes in the updated
4939             dataset.
4940 
4941         See Also
4942         --------
4943         Dataset.assign
4944         Dataset.merge
4945         """
4946         merge_result = dataset_update_method(self, other)
4947         return self._replace(inplace=True, **merge_result._asdict())
4948 
4949     def merge(
4950         self: T_Dataset,
4951         other: CoercibleMapping | DataArray,
4952         overwrite_vars: Hashable | Iterable[Hashable] = frozenset(),
4953         compat: CompatOptions = "no_conflicts",
4954         join: JoinOptions = "outer",
4955         fill_value: Any = xrdtypes.NA,
4956         combine_attrs: CombineAttrsOptions = "override",
4957     ) -> T_Dataset:
4958         """Merge the arrays of two datasets into a single dataset.
4959 
4960         This method generally does not allow for overriding data, with the
4961         exception of attributes, which are ignored on the second dataset.
4962         Variables with the same name are checked for conflicts via the equals
4963         or identical methods.
4964 
4965         Parameters
4966         ----------
4967         other : Dataset or mapping
4968             Dataset or variables to merge with this dataset.
4969         overwrite_vars : hashable or iterable of hashable, optional
4970             If provided, update variables of these name(s) without checking for
4971             conflicts in this dataset.
4972         compat : {"identical", "equals", "broadcast_equals", \
4973                   "no_conflicts", "override", "minimal"}, default: "no_conflicts"
4974             String indicating how to compare variables of the same name for
4975             potential conflicts:
4976 
4977             - 'identical': all values, dimensions and attributes must be the
4978               same.
4979             - 'equals': all values and dimensions must be the same.
4980             - 'broadcast_equals': all values must be equal when variables are
4981               broadcast against each other to ensure common dimensions.
4982             - 'no_conflicts': only values which are not null in both datasets
4983               must be equal. The returned dataset then contains the combination
4984               of all non-null values.
4985             - 'override': skip comparing and pick variable from first dataset
4986             - 'minimal': drop conflicting coordinates
4987 
4988         join : {"outer", "inner", "left", "right", "exact", "override"}, \
4989                default: "outer"
4990             Method for joining ``self`` and ``other`` along shared dimensions:
4991 
4992             - 'outer': use the union of the indexes
4993             - 'inner': use the intersection of the indexes
4994             - 'left': use indexes from ``self``
4995             - 'right': use indexes from ``other``
4996             - 'exact': error instead of aligning non-equal indexes
4997             - 'override': use indexes from ``self`` that are the same size
4998               as those of ``other`` in that dimension
4999 
5000         fill_value : scalar or dict-like, optional
5001             Value to use for newly missing values. If a dict-like, maps
5002             variable names (including coordinates) to fill values.
5003         combine_attrs : {"drop", "identical", "no_conflicts", "drop_conflicts", \
5004                          "override"} or callable, default: "override"
5005             A callable or a string indicating how to combine attrs of the objects being
5006             merged:
5007 
5008             - "drop": empty attrs on returned Dataset.
5009             - "identical": all attrs must be the same on every object.
5010             - "no_conflicts": attrs from all objects are combined, any that have
5011               the same name must also have the same value.
5012             - "drop_conflicts": attrs from all objects are combined, any that have
5013               the same name but different values are dropped.
5014             - "override": skip comparing and copy attrs from the first dataset to
5015               the result.
5016 
5017             If a callable, it must expect a sequence of ``attrs`` dicts and a context object
5018             as its only parameters.
5019 
5020         Returns
5021         -------
5022         merged : Dataset
5023             Merged dataset.
5024 
5025         Raises
5026         ------
5027         MergeError
5028             If any variables conflict (see ``compat``).
5029 
5030         See Also
5031         --------
5032         Dataset.update
5033         """
5034         from xarray.core.dataarray import DataArray
5035 
5036         other = other.to_dataset() if isinstance(other, DataArray) else other
5037         merge_result = dataset_merge_method(
5038             self,
5039             other,
5040             overwrite_vars=overwrite_vars,
5041             compat=compat,
5042             join=join,
5043             fill_value=fill_value,
5044             combine_attrs=combine_attrs,
5045         )
5046         return self._replace(**merge_result._asdict())
5047 
5048     def _assert_all_in_dataset(
5049         self, names: Iterable[Hashable], virtual_okay: bool = False
5050     ) -> None:
5051         bad_names = set(names) - set(self._variables)
5052         if virtual_okay:
5053             bad_names -= self.virtual_variables
5054         if bad_names:
5055             raise ValueError(
5056                 "One or more of the specified variables "
5057                 "cannot be found in this dataset"
5058             )
5059 
5060     def drop_vars(
5061         self: T_Dataset,
5062         names: Hashable | Iterable[Hashable],
5063         *,
5064         errors: ErrorOptions = "raise",
5065     ) -> T_Dataset:
5066         """Drop variables from this dataset.
5067 
5068         Parameters
5069         ----------
5070         names : hashable or iterable of hashable
5071             Name(s) of variables to drop.
5072         errors : {"raise", "ignore"}, default: "raise"
5073             If 'raise', raises a ValueError error if any of the variable
5074             passed are not in the dataset. If 'ignore', any given names that are in the
5075             dataset are dropped and no error is raised.
5076 
5077         Returns
5078         -------
5079         dropped : Dataset
5080 
5081         """
5082         # the Iterable check is required for mypy
5083         if is_scalar(names) or not isinstance(names, Iterable):
5084             names = {names}
5085         else:
5086             names = set(names)
5087         if errors == "raise":
5088             self._assert_all_in_dataset(names)
5089 
5090         # GH6505
5091         other_names = set()
5092         for var in names:
5093             maybe_midx = self._indexes.get(var, None)
5094             if isinstance(maybe_midx, PandasMultiIndex):
5095                 idx_coord_names = set(maybe_midx.index.names + [maybe_midx.dim])
5096                 idx_other_names = idx_coord_names - set(names)
5097                 other_names.update(idx_other_names)
5098         if other_names:
5099             names |= set(other_names)
5100             warnings.warn(
5101                 f"Deleting a single level of a MultiIndex is deprecated. Previously, this deleted all levels of a MultiIndex. "
5102                 f"Please also drop the following variables: {other_names!r} to avoid an error in the future.",
5103                 DeprecationWarning,
5104                 stacklevel=2,
5105             )
5106 
5107         assert_no_index_corrupted(self.xindexes, names)
5108 
5109         variables = {k: v for k, v in self._variables.items() if k not in names}
5110         coord_names = {k for k in self._coord_names if k in variables}
5111         indexes = {k: v for k, v in self._indexes.items() if k not in names}
5112         return self._replace_with_new_dims(
5113             variables, coord_names=coord_names, indexes=indexes
5114         )
5115 
5116     def drop_indexes(
5117         self: T_Dataset,
5118         coord_names: Hashable | Iterable[Hashable],
5119         *,
5120         errors: ErrorOptions = "raise",
5121     ) -> T_Dataset:
5122         """Drop the indexes assigned to the given coordinates.
5123 
5124         Parameters
5125         ----------
5126         coord_names : hashable or iterable of hashable
5127             Name(s) of the coordinate(s) for which to drop the index.
5128         errors : {"raise", "ignore"}, default: "raise"
5129             If 'raise', raises a ValueError error if any of the coordinates
5130             passed have no index or are not in the dataset.
5131             If 'ignore', no error is raised.
5132 
5133         Returns
5134         -------
5135         dropped : Dataset
5136             A new dataset with dropped indexes.
5137 
5138         """
5139         # the Iterable check is required for mypy
5140         if is_scalar(coord_names) or not isinstance(coord_names, Iterable):
5141             coord_names = {coord_names}
5142         else:
5143             coord_names = set(coord_names)
5144 
5145         if errors == "raise":
5146             invalid_coords = coord_names - self._coord_names
5147             if invalid_coords:
5148                 raise ValueError(f"those coordinates don't exist: {invalid_coords}")
5149 
5150             unindexed_coords = set(coord_names) - set(self._indexes)
5151             if unindexed_coords:
5152                 raise ValueError(
5153                     f"those coordinates do not have an index: {unindexed_coords}"
5154                 )
5155 
5156         assert_no_index_corrupted(self.xindexes, coord_names, action="remove index(es)")
5157 
5158         variables = {}
5159         for name, var in self._variables.items():
5160             if name in coord_names:
5161                 variables[name] = var.to_base_variable()
5162             else:
5163                 variables[name] = var
5164 
5165         indexes = {k: v for k, v in self._indexes.items() if k not in coord_names}
5166 
5167         return self._replace(variables=variables, indexes=indexes)
5168 
5169     def drop(
5170         self: T_Dataset,
5171         labels=None,
5172         dim=None,
5173         *,
5174         errors: ErrorOptions = "raise",
5175         **labels_kwargs,
5176     ) -> T_Dataset:
5177         """Backward compatible method based on `drop_vars` and `drop_sel`
5178 
5179         Using either `drop_vars` or `drop_sel` is encouraged
5180 
5181         See Also
5182         --------
5183         Dataset.drop_vars
5184         Dataset.drop_sel
5185         """
5186         if errors not in ["raise", "ignore"]:
5187             raise ValueError('errors must be either "raise" or "ignore"')
5188 
5189         if is_dict_like(labels) and not isinstance(labels, dict):
5190             warnings.warn(
5191                 "dropping coordinates using `drop` is be deprecated; use drop_vars.",
5192                 FutureWarning,
5193                 stacklevel=2,
5194             )
5195             return self.drop_vars(labels, errors=errors)
5196 
5197         if labels_kwargs or isinstance(labels, dict):
5198             if dim is not None:
5199                 raise ValueError("cannot specify dim and dict-like arguments.")
5200             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
5201 
5202         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):
5203             warnings.warn(
5204                 "dropping variables using `drop` will be deprecated; using drop_vars is encouraged.",
5205                 PendingDeprecationWarning,
5206                 stacklevel=2,
5207             )
5208             return self.drop_vars(labels, errors=errors)
5209         if dim is not None:
5210             warnings.warn(
5211                 "dropping labels using list-like labels is deprecated; using "
5212                 "dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).",
5213                 DeprecationWarning,
5214                 stacklevel=2,
5215             )
5216             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)
5217 
5218         warnings.warn(
5219             "dropping labels using `drop` will be deprecated; using drop_sel is encouraged.",
5220             PendingDeprecationWarning,
5221             stacklevel=2,
5222         )
5223         return self.drop_sel(labels, errors=errors)
5224 
5225     def drop_sel(
5226         self: T_Dataset, labels=None, *, errors: ErrorOptions = "raise", **labels_kwargs
5227     ) -> T_Dataset:
5228         """Drop index labels from this dataset.
5229 
5230         Parameters
5231         ----------
5232         labels : mapping of hashable to Any
5233             Index labels to drop
5234         errors : {"raise", "ignore"}, default: "raise"
5235             If 'raise', raises a ValueError error if
5236             any of the index labels passed are not
5237             in the dataset. If 'ignore', any given labels that are in the
5238             dataset are dropped and no error is raised.
5239         **labels_kwargs : {dim: label, ...}, optional
5240             The keyword arguments form of ``dim`` and ``labels``
5241 
5242         Returns
5243         -------
5244         dropped : Dataset
5245 
5246         Examples
5247         --------
5248         >>> data = np.arange(6).reshape(2, 3)
5249         >>> labels = ["a", "b", "c"]
5250         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
5251         >>> ds
5252         <xarray.Dataset>
5253         Dimensions:  (x: 2, y: 3)
5254         Coordinates:
5255           * y        (y) <U1 'a' 'b' 'c'
5256         Dimensions without coordinates: x
5257         Data variables:
5258             A        (x, y) int64 0 1 2 3 4 5
5259         >>> ds.drop_sel(y=["a", "c"])
5260         <xarray.Dataset>
5261         Dimensions:  (x: 2, y: 1)
5262         Coordinates:
5263           * y        (y) <U1 'b'
5264         Dimensions without coordinates: x
5265         Data variables:
5266             A        (x, y) int64 1 4
5267         >>> ds.drop_sel(y="b")
5268         <xarray.Dataset>
5269         Dimensions:  (x: 2, y: 2)
5270         Coordinates:
5271           * y        (y) <U1 'a' 'c'
5272         Dimensions without coordinates: x
5273         Data variables:
5274             A        (x, y) int64 0 2 3 5
5275         """
5276         if errors not in ["raise", "ignore"]:
5277             raise ValueError('errors must be either "raise" or "ignore"')
5278 
5279         labels = either_dict_or_kwargs(labels, labels_kwargs, "drop_sel")
5280 
5281         ds = self
5282         for dim, labels_for_dim in labels.items():
5283             # Don't cast to set, as it would harm performance when labels
5284             # is a large numpy array
5285             if utils.is_scalar(labels_for_dim):
5286                 labels_for_dim = [labels_for_dim]
5287             labels_for_dim = np.asarray(labels_for_dim)
5288             try:
5289                 index = self.get_index(dim)
5290             except KeyError:
5291                 raise ValueError(f"dimension {dim!r} does not have coordinate labels")
5292             new_index = index.drop(labels_for_dim, errors=errors)
5293             ds = ds.loc[{dim: new_index}]
5294         return ds
5295 
5296     def drop_isel(self: T_Dataset, indexers=None, **indexers_kwargs) -> T_Dataset:
5297         """Drop index positions from this Dataset.
5298 
5299         Parameters
5300         ----------
5301         indexers : mapping of hashable to Any
5302             Index locations to drop
5303         **indexers_kwargs : {dim: position, ...}, optional
5304             The keyword arguments form of ``dim`` and ``positions``
5305 
5306         Returns
5307         -------
5308         dropped : Dataset
5309 
5310         Raises
5311         ------
5312         IndexError
5313 
5314         Examples
5315         --------
5316         >>> data = np.arange(6).reshape(2, 3)
5317         >>> labels = ["a", "b", "c"]
5318         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
5319         >>> ds
5320         <xarray.Dataset>
5321         Dimensions:  (x: 2, y: 3)
5322         Coordinates:
5323           * y        (y) <U1 'a' 'b' 'c'
5324         Dimensions without coordinates: x
5325         Data variables:
5326             A        (x, y) int64 0 1 2 3 4 5
5327         >>> ds.drop_isel(y=[0, 2])
5328         <xarray.Dataset>
5329         Dimensions:  (x: 2, y: 1)
5330         Coordinates:
5331           * y        (y) <U1 'b'
5332         Dimensions without coordinates: x
5333         Data variables:
5334             A        (x, y) int64 1 4
5335         >>> ds.drop_isel(y=1)
5336         <xarray.Dataset>
5337         Dimensions:  (x: 2, y: 2)
5338         Coordinates:
5339           * y        (y) <U1 'a' 'c'
5340         Dimensions without coordinates: x
5341         Data variables:
5342             A        (x, y) int64 0 2 3 5
5343         """
5344 
5345         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "drop_isel")
5346 
5347         ds = self
5348         dimension_index = {}
5349         for dim, pos_for_dim in indexers.items():
5350             # Don't cast to set, as it would harm performance when labels
5351             # is a large numpy array
5352             if utils.is_scalar(pos_for_dim):
5353                 pos_for_dim = [pos_for_dim]
5354             pos_for_dim = np.asarray(pos_for_dim)
5355             index = self.get_index(dim)
5356             new_index = index.delete(pos_for_dim)
5357             dimension_index[dim] = new_index
5358         ds = ds.loc[dimension_index]
5359         return ds
5360 
5361     def drop_dims(
5362         self: T_Dataset,
5363         drop_dims: str | Iterable[Hashable],
5364         *,
5365         errors: ErrorOptions = "raise",
5366     ) -> T_Dataset:
5367         """Drop dimensions and associated variables from this dataset.
5368 
5369         Parameters
5370         ----------
5371         drop_dims : str or Iterable of Hashable
5372             Dimension or dimensions to drop.
5373         errors : {"raise", "ignore"}, default: "raise"
5374             If 'raise', raises a ValueError error if any of the
5375             dimensions passed are not in the dataset. If 'ignore', any given
5376             dimensions that are in the dataset are dropped and no error is raised.
5377 
5378         Returns
5379         -------
5380         obj : Dataset
5381             The dataset without the given dimensions (or any variables
5382             containing those dimensions).
5383         """
5384         if errors not in ["raise", "ignore"]:
5385             raise ValueError('errors must be either "raise" or "ignore"')
5386 
5387         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):
5388             drop_dims = {drop_dims}
5389         else:
5390             drop_dims = set(drop_dims)
5391 
5392         if errors == "raise":
5393             missing_dims = drop_dims - set(self.dims)
5394             if missing_dims:
5395                 raise ValueError(
5396                     f"Dataset does not contain the dimensions: {missing_dims}"
5397                 )
5398 
5399         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}
5400         return self.drop_vars(drop_vars)
5401 
5402     def transpose(
5403         self: T_Dataset,
5404         *dims: Hashable,
5405         missing_dims: ErrorOptionsWithWarn = "raise",
5406     ) -> T_Dataset:
5407         """Return a new Dataset object with all array dimensions transposed.
5408 
5409         Although the order of dimensions on each array will change, the dataset
5410         dimensions themselves will remain in fixed (sorted) order.
5411 
5412         Parameters
5413         ----------
5414         *dims : hashable, optional
5415             By default, reverse the dimensions on each array. Otherwise,
5416             reorder the dimensions to this order.
5417         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
5418             What to do if dimensions that should be selected from are not present in the
5419             Dataset:
5420             - "raise": raise an exception
5421             - "warn": raise a warning, and ignore the missing dimensions
5422             - "ignore": ignore the missing dimensions
5423 
5424         Returns
5425         -------
5426         transposed : Dataset
5427             Each array in the dataset (including) coordinates will be
5428             transposed to the given order.
5429 
5430         Notes
5431         -----
5432         This operation returns a view of each array's data. It is
5433         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
5434         -- the data will be fully loaded into memory.
5435 
5436         See Also
5437         --------
5438         numpy.transpose
5439         DataArray.transpose
5440         """
5441         # Raise error if list is passed as dims
5442         if (len(dims) > 0) and (isinstance(dims[0], list)):
5443             list_fix = [f"{repr(x)}" if isinstance(x, str) else f"{x}" for x in dims[0]]
5444             raise TypeError(
5445                 f'transpose requires dims to be passed as multiple arguments. Expected `{", ".join(list_fix)}`. Received `{dims[0]}` instead'
5446             )
5447 
5448         # Use infix_dims to check once for missing dimensions
5449         if len(dims) != 0:
5450             _ = list(infix_dims(dims, self.dims, missing_dims))
5451 
5452         ds = self.copy()
5453         for name, var in self._variables.items():
5454             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))
5455             ds._variables[name] = var.transpose(*var_dims)
5456         return ds
5457 
5458     def dropna(
5459         self: T_Dataset,
5460         dim: Hashable,
5461         how: Literal["any", "all"] = "any",
5462         thresh: int | None = None,
5463         subset: Iterable[Hashable] | None = None,
5464     ) -> T_Dataset:
5465         """Returns a new dataset with dropped labels for missing values along
5466         the provided dimension.
5467 
5468         Parameters
5469         ----------
5470         dim : hashable
5471             Dimension along which to drop missing values. Dropping along
5472             multiple dimensions simultaneously is not yet supported.
5473         how : {"any", "all"}, default: "any"
5474             - any : if any NA values are present, drop that label
5475             - all : if all values are NA, drop that label
5476 
5477         thresh : int or None, optional
5478             If supplied, require this many non-NA values.
5479         subset : iterable of hashable or None, optional
5480             Which variables to check for missing values. By default, all
5481             variables in the dataset are checked.
5482 
5483         Returns
5484         -------
5485         Dataset
5486         """
5487         # TODO: consider supporting multiple dimensions? Or not, given that
5488         # there are some ugly edge cases, e.g., pandas's dropna differs
5489         # depending on the order of the supplied axes.
5490 
5491         if dim not in self.dims:
5492             raise ValueError(f"{dim} must be a single dataset dimension")
5493 
5494         if subset is None:
5495             subset = iter(self.data_vars)
5496 
5497         count = np.zeros(self.dims[dim], dtype=np.int64)
5498         size = np.int_(0)  # for type checking
5499 
5500         for k in subset:
5501             array = self._variables[k]
5502             if dim in array.dims:
5503                 dims = [d for d in array.dims if d != dim]
5504                 count += np.asarray(array.count(dims))  # type: ignore[attr-defined]
5505                 size += math.prod([self.dims[d] for d in dims])
5506 
5507         if thresh is not None:
5508             mask = count >= thresh
5509         elif how == "any":
5510             mask = count == size
5511         elif how == "all":
5512             mask = count > 0
5513         elif how is not None:
5514             raise ValueError(f"invalid how option: {how}")
5515         else:
5516             raise TypeError("must specify how or thresh")
5517 
5518         return self.isel({dim: mask})
5519 
5520     def fillna(self: T_Dataset, value: Any) -> T_Dataset:
5521         """Fill missing values in this object.
5522 
5523         This operation follows the normal broadcasting and alignment rules that
5524         xarray uses for binary arithmetic, except the result is aligned to this
5525         object (``join='left'``) instead of aligned to the intersection of
5526         index coordinates (``join='inner'``).
5527 
5528         Parameters
5529         ----------
5530         value : scalar, ndarray, DataArray, dict or Dataset
5531             Used to fill all matching missing values in this dataset's data
5532             variables. Scalars, ndarrays or DataArrays arguments are used to
5533             fill all data with aligned coordinates (for DataArrays).
5534             Dictionaries or datasets match data variables and then align
5535             coordinates if necessary.
5536 
5537         Returns
5538         -------
5539         Dataset
5540 
5541         Examples
5542         --------
5543         >>> ds = xr.Dataset(
5544         ...     {
5545         ...         "A": ("x", [np.nan, 2, np.nan, 0]),
5546         ...         "B": ("x", [3, 4, np.nan, 1]),
5547         ...         "C": ("x", [np.nan, np.nan, np.nan, 5]),
5548         ...         "D": ("x", [np.nan, 3, np.nan, 4]),
5549         ...     },
5550         ...     coords={"x": [0, 1, 2, 3]},
5551         ... )
5552         >>> ds
5553         <xarray.Dataset>
5554         Dimensions:  (x: 4)
5555         Coordinates:
5556           * x        (x) int64 0 1 2 3
5557         Data variables:
5558             A        (x) float64 nan 2.0 nan 0.0
5559             B        (x) float64 3.0 4.0 nan 1.0
5560             C        (x) float64 nan nan nan 5.0
5561             D        (x) float64 nan 3.0 nan 4.0
5562 
5563         Replace all `NaN` values with 0s.
5564 
5565         >>> ds.fillna(0)
5566         <xarray.Dataset>
5567         Dimensions:  (x: 4)
5568         Coordinates:
5569           * x        (x) int64 0 1 2 3
5570         Data variables:
5571             A        (x) float64 0.0 2.0 0.0 0.0
5572             B        (x) float64 3.0 4.0 0.0 1.0
5573             C        (x) float64 0.0 0.0 0.0 5.0
5574             D        (x) float64 0.0 3.0 0.0 4.0
5575 
5576         Replace all `NaN` elements in column ‘A’, ‘B’, ‘C’, and ‘D’, with 0, 1, 2, and 3 respectively.
5577 
5578         >>> values = {"A": 0, "B": 1, "C": 2, "D": 3}
5579         >>> ds.fillna(value=values)
5580         <xarray.Dataset>
5581         Dimensions:  (x: 4)
5582         Coordinates:
5583           * x        (x) int64 0 1 2 3
5584         Data variables:
5585             A        (x) float64 0.0 2.0 0.0 0.0
5586             B        (x) float64 3.0 4.0 1.0 1.0
5587             C        (x) float64 2.0 2.0 2.0 5.0
5588             D        (x) float64 3.0 3.0 3.0 4.0
5589         """
5590         if utils.is_dict_like(value):
5591             value_keys = getattr(value, "data_vars", value).keys()
5592             if not set(value_keys) <= set(self.data_vars.keys()):
5593                 raise ValueError(
5594                     "all variables in the argument to `fillna` "
5595                     "must be contained in the original dataset"
5596                 )
5597         out = ops.fillna(self, value)
5598         return out
5599 
5600     def interpolate_na(
5601         self: T_Dataset,
5602         dim: Hashable | None = None,
5603         method: InterpOptions = "linear",
5604         limit: int | None = None,
5605         use_coordinate: bool | Hashable = True,
5606         max_gap: (
5607             int | float | str | pd.Timedelta | np.timedelta64 | datetime.timedelta
5608         ) = None,
5609         **kwargs: Any,
5610     ) -> T_Dataset:
5611         """Fill in NaNs by interpolating according to different methods.
5612 
5613         Parameters
5614         ----------
5615         dim : Hashable or None, optional
5616             Specifies the dimension along which to interpolate.
5617         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
5618             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
5619             String indicating which method to use for interpolation:
5620 
5621             - 'linear': linear interpolation. Additional keyword
5622               arguments are passed to :py:func:`numpy.interp`
5623             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
5624               are passed to :py:func:`scipy.interpolate.interp1d`. If
5625               ``method='polynomial'``, the ``order`` keyword argument must also be
5626               provided.
5627             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
5628               respective :py:class:`scipy.interpolate` classes.
5629 
5630         use_coordinate : bool or Hashable, default: True
5631             Specifies which index to use as the x values in the interpolation
5632             formulated as `y = f(x)`. If False, values are treated as if
5633             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
5634             used. If ``use_coordinate`` is a string, it specifies the name of a
5635             coordinate variariable to use as the index.
5636         limit : int, default: None
5637             Maximum number of consecutive NaNs to fill. Must be greater than 0
5638             or None for no limit. This filling is done regardless of the size of
5639             the gap in the data. To only interpolate over gaps less than a given length,
5640             see ``max_gap``.
5641         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
5642             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
5643             Use None for no limit. When interpolating along a datetime64 dimension
5644             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
5645 
5646             - a string that is valid input for pandas.to_timedelta
5647             - a :py:class:`numpy.timedelta64` object
5648             - a :py:class:`pandas.Timedelta` object
5649             - a :py:class:`datetime.timedelta` object
5650 
5651             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
5652             dimensions has not been implemented yet. Gap length is defined as the difference
5653             between coordinate values at the first data point after a gap and the last value
5654             before a gap. For gaps at the beginning (end), gap length is defined as the difference
5655             between coordinate values at the first (last) valid data point and the first (last) NaN.
5656             For example, consider::
5657 
5658                 <xarray.DataArray (x: 9)>
5659                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
5660                 Coordinates:
5661                   * x        (x) int64 0 1 2 3 4 5 6 7 8
5662 
5663             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
5664         **kwargs : dict, optional
5665             parameters passed verbatim to the underlying interpolation function
5666 
5667         Returns
5668         -------
5669         interpolated: Dataset
5670             Filled in Dataset.
5671 
5672         See Also
5673         --------
5674         numpy.interp
5675         scipy.interpolate
5676 
5677         Examples
5678         --------
5679         >>> ds = xr.Dataset(
5680         ...     {
5681         ...         "A": ("x", [np.nan, 2, 3, np.nan, 0]),
5682         ...         "B": ("x", [3, 4, np.nan, 1, 7]),
5683         ...         "C": ("x", [np.nan, np.nan, np.nan, 5, 0]),
5684         ...         "D": ("x", [np.nan, 3, np.nan, -1, 4]),
5685         ...     },
5686         ...     coords={"x": [0, 1, 2, 3, 4]},
5687         ... )
5688         >>> ds
5689         <xarray.Dataset>
5690         Dimensions:  (x: 5)
5691         Coordinates:
5692           * x        (x) int64 0 1 2 3 4
5693         Data variables:
5694             A        (x) float64 nan 2.0 3.0 nan 0.0
5695             B        (x) float64 3.0 4.0 nan 1.0 7.0
5696             C        (x) float64 nan nan nan 5.0 0.0
5697             D        (x) float64 nan 3.0 nan -1.0 4.0
5698 
5699         >>> ds.interpolate_na(dim="x", method="linear")
5700         <xarray.Dataset>
5701         Dimensions:  (x: 5)
5702         Coordinates:
5703           * x        (x) int64 0 1 2 3 4
5704         Data variables:
5705             A        (x) float64 nan 2.0 3.0 1.5 0.0
5706             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5707             C        (x) float64 nan nan nan 5.0 0.0
5708             D        (x) float64 nan 3.0 1.0 -1.0 4.0
5709 
5710         >>> ds.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
5711         <xarray.Dataset>
5712         Dimensions:  (x: 5)
5713         Coordinates:
5714           * x        (x) int64 0 1 2 3 4
5715         Data variables:
5716             A        (x) float64 1.0 2.0 3.0 1.5 0.0
5717             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5718             C        (x) float64 20.0 15.0 10.0 5.0 0.0
5719             D        (x) float64 5.0 3.0 1.0 -1.0 4.0
5720         """
5721         from xarray.core.missing import _apply_over_vars_with_dim, interp_na
5722 
5723         new = _apply_over_vars_with_dim(
5724             interp_na,
5725             self,
5726             dim=dim,
5727             method=method,
5728             limit=limit,
5729             use_coordinate=use_coordinate,
5730             max_gap=max_gap,
5731             **kwargs,
5732         )
5733         return new
5734 
5735     def ffill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5736         """Fill NaN values by propagating values forward
5737 
5738         *Requires bottleneck.*
5739 
5740         Parameters
5741         ----------
5742         dim : Hashable
5743             Specifies the dimension along which to propagate values when
5744             filling.
5745         limit : int or None, optional
5746             The maximum number of consecutive NaN values to forward fill. In
5747             other words, if there is a gap with more than this number of
5748             consecutive NaNs, it will only be partially filled. Must be greater
5749             than 0 or None for no limit. Must be None or greater than or equal
5750             to axis length if filling along chunked axes (dimensions).
5751 
5752         Returns
5753         -------
5754         Dataset
5755         """
5756         from xarray.core.missing import _apply_over_vars_with_dim, ffill
5757 
5758         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)
5759         return new
5760 
5761     def bfill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5762         """Fill NaN values by propagating values backward
5763 
5764         *Requires bottleneck.*
5765 
5766         Parameters
5767         ----------
5768         dim : Hashable
5769             Specifies the dimension along which to propagate values when
5770             filling.
5771         limit : int or None, optional
5772             The maximum number of consecutive NaN values to backward fill. In
5773             other words, if there is a gap with more than this number of
5774             consecutive NaNs, it will only be partially filled. Must be greater
5775             than 0 or None for no limit. Must be None or greater than or equal
5776             to axis length if filling along chunked axes (dimensions).
5777 
5778         Returns
5779         -------
5780         Dataset
5781         """
5782         from xarray.core.missing import _apply_over_vars_with_dim, bfill
5783 
5784         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)
5785         return new
5786 
5787     def combine_first(self: T_Dataset, other: T_Dataset) -> T_Dataset:
5788         """Combine two Datasets, default to data_vars of self.
5789 
5790         The new coordinates follow the normal broadcasting and alignment rules
5791         of ``join='outer'``.  Vacant cells in the expanded coordinates are
5792         filled with np.nan.
5793 
5794         Parameters
5795         ----------
5796         other : Dataset
5797             Used to fill all matching missing values in this array.
5798 
5799         Returns
5800         -------
5801         Dataset
5802         """
5803         out = ops.fillna(self, other, join="outer", dataset_join="outer")
5804         return out
5805 
5806     def reduce(
5807         self: T_Dataset,
5808         func: Callable,
5809         dim: Dims = None,
5810         *,
5811         keep_attrs: bool | None = None,
5812         keepdims: bool = False,
5813         numeric_only: bool = False,
5814         **kwargs: Any,
5815     ) -> T_Dataset:
5816         """Reduce this dataset by applying `func` along some dimension(s).
5817 
5818         Parameters
5819         ----------
5820         func : callable
5821             Function which can be called in the form
5822             `f(x, axis=axis, **kwargs)` to return the result of reducing an
5823             np.ndarray over an integer valued axis.
5824         dim : str, Iterable of Hashable or None, optional
5825             Dimension(s) over which to apply `func`. By default `func` is
5826             applied over all dimensions.
5827         keep_attrs : bool or None, optional
5828             If True, the dataset's attributes (`attrs`) will be copied from
5829             the original object to the new one.  If False (default), the new
5830             object will be returned without attributes.
5831         keepdims : bool, default: False
5832             If True, the dimensions which are reduced are left in the result
5833             as dimensions of size one. Coordinates that use these dimensions
5834             are removed.
5835         numeric_only : bool, default: False
5836             If True, only apply ``func`` to variables with a numeric dtype.
5837         **kwargs : Any
5838             Additional keyword arguments passed on to ``func``.
5839 
5840         Returns
5841         -------
5842         reduced : Dataset
5843             Dataset with this object's DataArrays replaced with new DataArrays
5844             of summarized data and the indicated dimension(s) removed.
5845         """
5846         if kwargs.get("axis", None) is not None:
5847             raise ValueError(
5848                 "passing 'axis' to Dataset reduce methods is ambiguous."
5849                 " Please use 'dim' instead."
5850             )
5851 
5852         if dim is None or dim is ...:
5853             dims = set(self.dims)
5854         elif isinstance(dim, str) or not isinstance(dim, Iterable):
5855             dims = {dim}
5856         else:
5857             dims = set(dim)
5858 
5859         missing_dimensions = [d for d in dims if d not in self.dims]
5860         if missing_dimensions:
5861             raise ValueError(
5862                 f"Dataset does not contain the dimensions: {missing_dimensions}"
5863             )
5864 
5865         if keep_attrs is None:
5866             keep_attrs = _get_keep_attrs(default=False)
5867 
5868         variables: dict[Hashable, Variable] = {}
5869         for name, var in self._variables.items():
5870             reduce_dims = [d for d in var.dims if d in dims]
5871             if name in self.coords:
5872                 if not reduce_dims:
5873                     variables[name] = var
5874             else:
5875                 if (
5876                     # Some reduction functions (e.g. std, var) need to run on variables
5877                     # that don't have the reduce dims: PR5393
5878                     not reduce_dims
5879                     or not numeric_only
5880                     or np.issubdtype(var.dtype, np.number)
5881                     or (var.dtype == np.bool_)
5882                 ):
5883                     # prefer to aggregate over axis=None rather than
5884                     # axis=(0, 1) if they will be equivalent, because
5885                     # the former is often more efficient
5886                     # keep single-element dims as list, to support Hashables
5887                     reduce_maybe_single = (
5888                         None
5889                         if len(reduce_dims) == var.ndim and var.ndim != 1
5890                         else reduce_dims
5891                     )
5892                     variables[name] = var.reduce(
5893                         func,
5894                         dim=reduce_maybe_single,
5895                         keep_attrs=keep_attrs,
5896                         keepdims=keepdims,
5897                         **kwargs,
5898                     )
5899 
5900         coord_names = {k for k in self.coords if k in variables}
5901         indexes = {k: v for k, v in self._indexes.items() if k in variables}
5902         attrs = self.attrs if keep_attrs else None
5903         return self._replace_with_new_dims(
5904             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
5905         )
5906 
5907     def map(
5908         self: T_Dataset,
5909         func: Callable,
5910         keep_attrs: bool | None = None,
5911         args: Iterable[Any] = (),
5912         **kwargs: Any,
5913     ) -> T_Dataset:
5914         """Apply a function to each data variable in this dataset
5915 
5916         Parameters
5917         ----------
5918         func : callable
5919             Function which can be called in the form `func(x, *args, **kwargs)`
5920             to transform each DataArray `x` in this dataset into another
5921             DataArray.
5922         keep_attrs : bool or None, optional
5923             If True, both the dataset's and variables' attributes (`attrs`) will be
5924             copied from the original objects to the new ones. If False, the new dataset
5925             and variables will be returned without copying the attributes.
5926         args : iterable, optional
5927             Positional arguments passed on to `func`.
5928         **kwargs : Any
5929             Keyword arguments passed on to `func`.
5930 
5931         Returns
5932         -------
5933         applied : Dataset
5934             Resulting dataset from applying ``func`` to each data variable.
5935 
5936         Examples
5937         --------
5938         >>> da = xr.DataArray(np.random.randn(2, 3))
5939         >>> ds = xr.Dataset({"foo": da, "bar": ("x", [-1, 2])})
5940         >>> ds
5941         <xarray.Dataset>
5942         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5943         Dimensions without coordinates: dim_0, dim_1, x
5944         Data variables:
5945             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
5946             bar      (x) int64 -1 2
5947         >>> ds.map(np.fabs)
5948         <xarray.Dataset>
5949         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5950         Dimensions without coordinates: dim_0, dim_1, x
5951         Data variables:
5952             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 0.9773
5953             bar      (x) float64 1.0 2.0
5954         """
5955         if keep_attrs is None:
5956             keep_attrs = _get_keep_attrs(default=False)
5957         variables = {
5958             k: maybe_wrap_array(v, func(v, *args, **kwargs))
5959             for k, v in self.data_vars.items()
5960         }
5961         if keep_attrs:
5962             for k, v in variables.items():
5963                 v._copy_attrs_from(self.data_vars[k])
5964         attrs = self.attrs if keep_attrs else None
5965         return type(self)(variables, attrs=attrs)
5966 
5967     def apply(
5968         self: T_Dataset,
5969         func: Callable,
5970         keep_attrs: bool | None = None,
5971         args: Iterable[Any] = (),
5972         **kwargs: Any,
5973     ) -> T_Dataset:
5974         """
5975         Backward compatible implementation of ``map``
5976 
5977         See Also
5978         --------
5979         Dataset.map
5980         """
5981         warnings.warn(
5982             "Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged",
5983             PendingDeprecationWarning,
5984             stacklevel=2,
5985         )
5986         return self.map(func, keep_attrs, args, **kwargs)
5987 
5988     def assign(
5989         self: T_Dataset,
5990         variables: Mapping[Any, Any] | None = None,
5991         **variables_kwargs: Any,
5992     ) -> T_Dataset:
5993         """Assign new data variables to a Dataset, returning a new object
5994         with all the original variables in addition to the new ones.
5995 
5996         Parameters
5997         ----------
5998         variables : mapping of hashable to Any
5999             Mapping from variables names to the new values. If the new values
6000             are callable, they are computed on the Dataset and assigned to new
6001             data variables. If the values are not callable, (e.g. a DataArray,
6002             scalar, or array), they are simply assigned.
6003         **variables_kwargs
6004             The keyword arguments form of ``variables``.
6005             One of variables or variables_kwargs must be provided.
6006 
6007         Returns
6008         -------
6009         ds : Dataset
6010             A new Dataset with the new variables in addition to all the
6011             existing variables.
6012 
6013         Notes
6014         -----
6015         Since ``kwargs`` is a dictionary, the order of your arguments may not
6016         be preserved, and so the order of the new variables is not well
6017         defined. Assigning multiple variables within the same ``assign`` is
6018         possible, but you cannot reference other variables created within the
6019         same ``assign`` call.
6020 
6021         See Also
6022         --------
6023         pandas.DataFrame.assign
6024 
6025         Examples
6026         --------
6027         >>> x = xr.Dataset(
6028         ...     {
6029         ...         "temperature_c": (
6030         ...             ("lat", "lon"),
6031         ...             20 * np.random.rand(4).reshape(2, 2),
6032         ...         ),
6033         ...         "precipitation": (("lat", "lon"), np.random.rand(4).reshape(2, 2)),
6034         ...     },
6035         ...     coords={"lat": [10, 20], "lon": [150, 160]},
6036         ... )
6037         >>> x
6038         <xarray.Dataset>
6039         Dimensions:        (lat: 2, lon: 2)
6040         Coordinates:
6041           * lat            (lat) int64 10 20
6042           * lon            (lon) int64 150 160
6043         Data variables:
6044             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6045             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6046 
6047         Where the value is a callable, evaluated on dataset:
6048 
6049         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)
6050         <xarray.Dataset>
6051         Dimensions:        (lat: 2, lon: 2)
6052         Coordinates:
6053           * lat            (lat) int64 10 20
6054           * lon            (lon) int64 150 160
6055         Data variables:
6056             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6057             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6058             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
6059 
6060         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:
6061 
6062         >>> x.assign(temperature_f=x["temperature_c"] * 9 / 5 + 32)
6063         <xarray.Dataset>
6064         Dimensions:        (lat: 2, lon: 2)
6065         Coordinates:
6066           * lat            (lat) int64 10 20
6067           * lon            (lon) int64 150 160
6068         Data variables:
6069             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6070             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6071             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
6072 
6073         """
6074         variables = either_dict_or_kwargs(variables, variables_kwargs, "assign")
6075         data = self.copy()
6076         # do all calculations first...
6077         results: CoercibleMapping = data._calc_assign_results(variables)
6078         data.coords._maybe_drop_multiindex_coords(set(results.keys()))
6079         # ... and then assign
6080         data.update(results)
6081         return data
6082 
6083     def to_array(
6084         self, dim: Hashable = "variable", name: Hashable | None = None
6085     ) -> DataArray:
6086         """Convert this dataset into an xarray.DataArray
6087 
6088         The data variables of this dataset will be broadcast against each other
6089         and stacked along the first axis of the new array. All coordinates of
6090         this dataset will remain coordinates.
6091 
6092         Parameters
6093         ----------
6094         dim : Hashable, default: "variable"
6095             Name of the new dimension.
6096         name : Hashable or None, optional
6097             Name of the new data array.
6098 
6099         Returns
6100         -------
6101         array : xarray.DataArray
6102         """
6103         from xarray.core.dataarray import DataArray
6104 
6105         data_vars = [self.variables[k] for k in self.data_vars]
6106         broadcast_vars = broadcast_variables(*data_vars)
6107         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)
6108 
6109         dims = (dim,) + broadcast_vars[0].dims
6110         variable = Variable(dims, data, self.attrs, fastpath=True)
6111 
6112         coords = {k: v.variable for k, v in self.coords.items()}
6113         indexes = filter_indexes_from_coords(self._indexes, set(coords))
6114         new_dim_index = PandasIndex(list(self.data_vars), dim)
6115         indexes[dim] = new_dim_index
6116         coords.update(new_dim_index.create_variables())
6117 
6118         return DataArray._construct_direct(variable, coords, name, indexes)
6119 
6120     def _normalize_dim_order(
6121         self, dim_order: Sequence[Hashable] | None = None
6122     ) -> dict[Hashable, int]:
6123         """
6124         Check the validity of the provided dimensions if any and return the mapping
6125         between dimension name and their size.
6126 
6127         Parameters
6128         ----------
6129         dim_order: Sequence of Hashable or None, optional
6130             Dimension order to validate (default to the alphabetical order if None).
6131 
6132         Returns
6133         -------
6134         result : dict[Hashable, int]
6135             Validated dimensions mapping.
6136 
6137         """
6138         if dim_order is None:
6139             dim_order = list(self.dims)
6140         elif set(dim_order) != set(self.dims):
6141             raise ValueError(
6142                 "dim_order {} does not match the set of dimensions of this "
6143                 "Dataset: {}".format(dim_order, list(self.dims))
6144             )
6145 
6146         ordered_dims = {k: self.dims[k] for k in dim_order}
6147 
6148         return ordered_dims
6149 
6150     def to_pandas(self) -> pd.Series | pd.DataFrame:
6151         """Convert this dataset into a pandas object without changing the number of dimensions.
6152 
6153         The type of the returned object depends on the number of Dataset
6154         dimensions:
6155 
6156         * 0D -> `pandas.Series`
6157         * 1D -> `pandas.DataFrame`
6158 
6159         Only works for Datasets with 1 or fewer dimensions.
6160         """
6161         if len(self.dims) == 0:
6162             return pd.Series({k: v.item() for k, v in self.items()})
6163         if len(self.dims) == 1:
6164             return self.to_dataframe()
6165         raise ValueError(
6166             "cannot convert Datasets with %s dimensions into "
6167             "pandas objects without changing the number of dimensions. "
6168             "Please use Dataset.to_dataframe() instead." % len(self.dims)
6169         )
6170 
6171     def _to_dataframe(self, ordered_dims: Mapping[Any, int]):
6172         columns = [k for k in self.variables if k not in self.dims]
6173         data = [
6174             self._variables[k].set_dims(ordered_dims).values.reshape(-1)
6175             for k in columns
6176         ]
6177         index = self.coords.to_index([*ordered_dims])
6178         return pd.DataFrame(dict(zip(columns, data)), index=index)
6179 
6180     def to_dataframe(self, dim_order: Sequence[Hashable] | None = None) -> pd.DataFrame:
6181         """Convert this dataset into a pandas.DataFrame.
6182 
6183         Non-index variables in this dataset form the columns of the
6184         DataFrame. The DataFrame is indexed by the Cartesian product of
6185         this dataset's indices.
6186 
6187         Parameters
6188         ----------
6189         dim_order: Sequence of Hashable or None, optional
6190             Hierarchical dimension order for the resulting dataframe. All
6191             arrays are transposed to this order and then written out as flat
6192             vectors in contiguous order, so the last dimension in this list
6193             will be contiguous in the resulting DataFrame. This has a major
6194             influence on which operations are efficient on the resulting
6195             dataframe.
6196 
6197             If provided, must include all dimensions of this dataset. By
6198             default, dimensions are sorted alphabetically.
6199 
6200         Returns
6201         -------
6202         result : DataFrame
6203             Dataset as a pandas DataFrame.
6204 
6205         """
6206 
6207         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
6208 
6209         return self._to_dataframe(ordered_dims=ordered_dims)
6210 
6211     def _set_sparse_data_from_dataframe(
6212         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
6213     ) -> None:
6214         from sparse import COO
6215 
6216         if isinstance(idx, pd.MultiIndex):
6217             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)
6218             is_sorted = idx.is_monotonic_increasing
6219             shape = tuple(lev.size for lev in idx.levels)
6220         else:
6221             coords = np.arange(idx.size).reshape(1, -1)
6222             is_sorted = True
6223             shape = (idx.size,)
6224 
6225         for name, values in arrays:
6226             # In virtually all real use cases, the sparse array will now have
6227             # missing values and needs a fill_value. For consistency, don't
6228             # special case the rare exceptions (e.g., dtype=int without a
6229             # MultiIndex).
6230             dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
6231             values = np.asarray(values, dtype=dtype)
6232 
6233             data = COO(
6234                 coords,
6235                 values,
6236                 shape,
6237                 has_duplicates=False,
6238                 sorted=is_sorted,
6239                 fill_value=fill_value,
6240             )
6241             self[name] = (dims, data)
6242 
6243     def _set_numpy_data_from_dataframe(
6244         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
6245     ) -> None:
6246         if not isinstance(idx, pd.MultiIndex):
6247             for name, values in arrays:
6248                 self[name] = (dims, values)
6249             return
6250 
6251         # NB: similar, more general logic, now exists in
6252         # variable.unstack_once; we could consider combining them at some
6253         # point.
6254 
6255         shape = tuple(lev.size for lev in idx.levels)
6256         indexer = tuple(idx.codes)
6257 
6258         # We already verified that the MultiIndex has all unique values, so
6259         # there are missing values if and only if the size of output arrays is
6260         # larger that the index.
6261         missing_values = math.prod(shape) > idx.shape[0]
6262 
6263         for name, values in arrays:
6264             # NumPy indexing is much faster than using DataFrame.reindex() to
6265             # fill in missing values:
6266             # https://stackoverflow.com/a/35049899/809705
6267             if missing_values:
6268                 dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
6269                 data = np.full(shape, fill_value, dtype)
6270             else:
6271                 # If there are no missing values, keep the existing dtype
6272                 # instead of promoting to support NA, e.g., keep integer
6273                 # columns as integers.
6274                 # TODO: consider removing this special case, which doesn't
6275                 # exist for sparse=True.
6276                 data = np.zeros(shape, values.dtype)
6277             data[indexer] = values
6278             self[name] = (dims, data)
6279 
6280     @classmethod
6281     def from_dataframe(
6282         cls: type[T_Dataset], dataframe: pd.DataFrame, sparse: bool = False
6283     ) -> T_Dataset:
6284         """Convert a pandas.DataFrame into an xarray.Dataset
6285 
6286         Each column will be converted into an independent variable in the
6287         Dataset. If the dataframe's index is a MultiIndex, it will be expanded
6288         into a tensor product of one-dimensional indices (filling in missing
6289         values with NaN). This method will produce a Dataset very similar to
6290         that on which the 'to_dataframe' method was called, except with
6291         possibly redundant dimensions (since all dataset variables will have
6292         the same dimensionality)
6293 
6294         Parameters
6295         ----------
6296         dataframe : DataFrame
6297             DataFrame from which to copy data and indices.
6298         sparse : bool, default: False
6299             If true, create a sparse arrays instead of dense numpy arrays. This
6300             can potentially save a large amount of memory if the DataFrame has
6301             a MultiIndex. Requires the sparse package (sparse.pydata.org).
6302 
6303         Returns
6304         -------
6305         New Dataset.
6306 
6307         See Also
6308         --------
6309         xarray.DataArray.from_series
6310         pandas.DataFrame.to_xarray
6311         """
6312         # TODO: Add an option to remove dimensions along which the variables
6313         # are constant, to enable consistent serialization to/from a dataframe,
6314         # even if some variables have different dimensionality.
6315 
6316         if not dataframe.columns.is_unique:
6317             raise ValueError("cannot convert DataFrame with non-unique columns")
6318 
6319         idx = remove_unused_levels_categories(dataframe.index)
6320 
6321         if isinstance(idx, pd.MultiIndex) and not idx.is_unique:
6322             raise ValueError(
6323                 "cannot convert a DataFrame with a non-unique MultiIndex into xarray"
6324             )
6325 
6326         # Cast to a NumPy array first, in case the Series is a pandas Extension
6327         # array (which doesn't have a valid NumPy dtype)
6328         # TODO: allow users to control how this casting happens, e.g., by
6329         # forwarding arguments to pandas.Series.to_numpy?
6330         arrays = [(k, np.asarray(v)) for k, v in dataframe.items()]
6331 
6332         indexes: dict[Hashable, Index] = {}
6333         index_vars: dict[Hashable, Variable] = {}
6334 
6335         if isinstance(idx, pd.MultiIndex):
6336             dims = tuple(
6337                 name if name is not None else "level_%i" % n
6338                 for n, name in enumerate(idx.names)
6339             )
6340             for dim, lev in zip(dims, idx.levels):
6341                 xr_idx = PandasIndex(lev, dim)
6342                 indexes[dim] = xr_idx
6343                 index_vars.update(xr_idx.create_variables())
6344         else:
6345             index_name = idx.name if idx.name is not None else "index"
6346             dims = (index_name,)
6347             xr_idx = PandasIndex(idx, index_name)
6348             indexes[index_name] = xr_idx
6349             index_vars.update(xr_idx.create_variables())
6350 
6351         obj = cls._construct_direct(index_vars, set(index_vars), indexes=indexes)
6352 
6353         if sparse:
6354             obj._set_sparse_data_from_dataframe(idx, arrays, dims)
6355         else:
6356             obj._set_numpy_data_from_dataframe(idx, arrays, dims)
6357         return obj
6358 
6359     def to_dask_dataframe(
6360         self, dim_order: Sequence[Hashable] | None = None, set_index: bool = False
6361     ) -> DaskDataFrame:
6362         """
6363         Convert this dataset into a dask.dataframe.DataFrame.
6364 
6365         The dimensions, coordinates and data variables in this dataset form
6366         the columns of the DataFrame.
6367 
6368         Parameters
6369         ----------
6370         dim_order : list, optional
6371             Hierarchical dimension order for the resulting dataframe. All
6372             arrays are transposed to this order and then written out as flat
6373             vectors in contiguous order, so the last dimension in this list
6374             will be contiguous in the resulting DataFrame. This has a major
6375             influence on which operations are efficient on the resulting dask
6376             dataframe.
6377 
6378             If provided, must include all dimensions of this dataset. By
6379             default, dimensions are sorted alphabetically.
6380         set_index : bool, default: False
6381             If set_index=True, the dask DataFrame is indexed by this dataset's
6382             coordinate. Since dask DataFrames do not support multi-indexes,
6383             set_index only works if the dataset only contains one dimension.
6384 
6385         Returns
6386         -------
6387         dask.dataframe.DataFrame
6388         """
6389 
6390         import dask.array as da
6391         import dask.dataframe as dd
6392 
6393         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
6394 
6395         columns = list(ordered_dims)
6396         columns.extend(k for k in self.coords if k not in self.dims)
6397         columns.extend(self.data_vars)
6398 
6399         series_list = []
6400         for name in columns:
6401             try:
6402                 var = self.variables[name]
6403             except KeyError:
6404                 # dimension without a matching coordinate
6405                 size = self.dims[name]
6406                 data = da.arange(size, chunks=size, dtype=np.int64)
6407                 var = Variable((name,), data)
6408 
6409             # IndexVariable objects have a dummy .chunk() method
6410             if isinstance(var, IndexVariable):
6411                 var = var.to_base_variable()
6412 
6413             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data
6414             series = dd.from_array(dask_array.reshape(-1), columns=[name])
6415             series_list.append(series)
6416 
6417         df = dd.concat(series_list, axis=1)
6418 
6419         if set_index:
6420             dim_order = [*ordered_dims]
6421 
6422             if len(dim_order) == 1:
6423                 (dim,) = dim_order
6424                 df = df.set_index(dim)
6425             else:
6426                 # triggers an error about multi-indexes, even if only one
6427                 # dimension is passed
6428                 df = df.set_index(dim_order)
6429 
6430         return df
6431 
6432     def to_dict(self, data: bool = True, encoding: bool = False) -> dict[str, Any]:
6433         """
6434         Convert this dataset to a dictionary following xarray naming
6435         conventions.
6436 
6437         Converts all variables and attributes to native Python objects
6438         Useful for converting to json. To avoid datetime incompatibility
6439         use decode_times=False kwarg in xarrray.open_dataset.
6440 
6441         Parameters
6442         ----------
6443         data : bool, default: True
6444             Whether to include the actual data in the dictionary. When set to
6445             False, returns just the schema.
6446         encoding : bool, default: False
6447             Whether to include the Dataset's encoding in the dictionary.
6448 
6449         Returns
6450         -------
6451         d : dict
6452             Dict with keys: "coords", "attrs", "dims", "data_vars" and optionally
6453             "encoding".
6454 
6455         See Also
6456         --------
6457         Dataset.from_dict
6458         DataArray.to_dict
6459         """
6460         d: dict = {
6461             "coords": {},
6462             "attrs": decode_numpy_dict_values(self.attrs),
6463             "dims": dict(self.dims),
6464             "data_vars": {},
6465         }
6466         for k in self.coords:
6467             d["coords"].update(
6468                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6469             )
6470         for k in self.data_vars:
6471             d["data_vars"].update(
6472                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6473             )
6474         if encoding:
6475             d["encoding"] = dict(self.encoding)
6476         return d
6477 
6478     @classmethod
6479     def from_dict(cls: type[T_Dataset], d: Mapping[Any, Any]) -> T_Dataset:
6480         """Convert a dictionary into an xarray.Dataset.
6481 
6482         Parameters
6483         ----------
6484         d : dict-like
6485             Mapping with a minimum structure of
6486                 ``{"var_0": {"dims": [..], "data": [..]}, \
6487                             ...}``
6488 
6489         Returns
6490         -------
6491         obj : Dataset
6492 
6493         See also
6494         --------
6495         Dataset.to_dict
6496         DataArray.from_dict
6497 
6498         Examples
6499         --------
6500         >>> d = {
6501         ...     "t": {"dims": ("t"), "data": [0, 1, 2]},
6502         ...     "a": {"dims": ("t"), "data": ["a", "b", "c"]},
6503         ...     "b": {"dims": ("t"), "data": [10, 20, 30]},
6504         ... }
6505         >>> ds = xr.Dataset.from_dict(d)
6506         >>> ds
6507         <xarray.Dataset>
6508         Dimensions:  (t: 3)
6509         Coordinates:
6510           * t        (t) int64 0 1 2
6511         Data variables:
6512             a        (t) <U1 'a' 'b' 'c'
6513             b        (t) int64 10 20 30
6514 
6515         >>> d = {
6516         ...     "coords": {
6517         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
6518         ...     },
6519         ...     "attrs": {"title": "air temperature"},
6520         ...     "dims": "t",
6521         ...     "data_vars": {
6522         ...         "a": {"dims": "t", "data": [10, 20, 30]},
6523         ...         "b": {"dims": "t", "data": ["a", "b", "c"]},
6524         ...     },
6525         ... }
6526         >>> ds = xr.Dataset.from_dict(d)
6527         >>> ds
6528         <xarray.Dataset>
6529         Dimensions:  (t: 3)
6530         Coordinates:
6531           * t        (t) int64 0 1 2
6532         Data variables:
6533             a        (t) int64 10 20 30
6534             b        (t) <U1 'a' 'b' 'c'
6535         Attributes:
6536             title:    air temperature
6537 
6538         """
6539 
6540         variables: Iterable[tuple[Hashable, Any]]
6541         if not {"coords", "data_vars"}.issubset(set(d)):
6542             variables = d.items()
6543         else:
6544             import itertools
6545 
6546             variables = itertools.chain(
6547                 d.get("coords", {}).items(), d.get("data_vars", {}).items()
6548             )
6549         try:
6550             variable_dict = {
6551                 k: (v["dims"], v["data"], v.get("attrs")) for k, v in variables
6552             }
6553         except KeyError as e:
6554             raise ValueError(
6555                 "cannot convert dict without the key "
6556                 "'{dims_data}'".format(dims_data=str(e.args[0]))
6557             )
6558         obj = cls(variable_dict)
6559 
6560         # what if coords aren't dims?
6561         coords = set(d.get("coords", {})) - set(d.get("dims", {}))
6562         obj = obj.set_coords(coords)
6563 
6564         obj.attrs.update(d.get("attrs", {}))
6565         obj.encoding.update(d.get("encoding", {}))
6566 
6567         return obj
6568 
6569     def _unary_op(self: T_Dataset, f, *args, **kwargs) -> T_Dataset:
6570         variables = {}
6571         keep_attrs = kwargs.pop("keep_attrs", None)
6572         if keep_attrs is None:
6573             keep_attrs = _get_keep_attrs(default=True)
6574         for k, v in self._variables.items():
6575             if k in self._coord_names:
6576                 variables[k] = v
6577             else:
6578                 variables[k] = f(v, *args, **kwargs)
6579                 if keep_attrs:
6580                     variables[k].attrs = v._attrs
6581         attrs = self._attrs if keep_attrs else None
6582         return self._replace_with_new_dims(variables, attrs=attrs)
6583 
6584     def _binary_op(self, other, f, reflexive=False, join=None) -> Dataset:
6585         from xarray.core.dataarray import DataArray
6586         from xarray.core.groupby import GroupBy
6587 
6588         if isinstance(other, GroupBy):
6589             return NotImplemented
6590         align_type = OPTIONS["arithmetic_join"] if join is None else join
6591         if isinstance(other, (DataArray, Dataset)):
6592             self, other = align(self, other, join=align_type, copy=False)  # type: ignore[assignment]
6593         g = f if not reflexive else lambda x, y: f(y, x)
6594         ds = self._calculate_binary_op(g, other, join=align_type)
6595         return ds
6596 
6597     def _inplace_binary_op(self: T_Dataset, other, f) -> T_Dataset:
6598         from xarray.core.dataarray import DataArray
6599         from xarray.core.groupby import GroupBy
6600 
6601         if isinstance(other, GroupBy):
6602             raise TypeError(
6603                 "in-place operations between a Dataset and "
6604                 "a grouped object are not permitted"
6605             )
6606         # we don't actually modify arrays in-place with in-place Dataset
6607         # arithmetic -- this lets us automatically align things
6608         if isinstance(other, (DataArray, Dataset)):
6609             other = other.reindex_like(self, copy=False)
6610         g = ops.inplace_to_noninplace_op(f)
6611         ds = self._calculate_binary_op(g, other, inplace=True)
6612         self._replace_with_new_dims(
6613             ds._variables,
6614             ds._coord_names,
6615             attrs=ds._attrs,
6616             indexes=ds._indexes,
6617             inplace=True,
6618         )
6619         return self
6620 
6621     def _calculate_binary_op(
6622         self, f, other, join="inner", inplace: bool = False
6623     ) -> Dataset:
6624         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):
6625             if inplace and set(lhs_data_vars) != set(rhs_data_vars):
6626                 raise ValueError(
6627                     "datasets must have the same data variables "
6628                     f"for in-place arithmetic operations: {list(lhs_data_vars)}, {list(rhs_data_vars)}"
6629                 )
6630 
6631             dest_vars = {}
6632 
6633             for k in lhs_data_vars:
6634                 if k in rhs_data_vars:
6635                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])
6636                 elif join in ["left", "outer"]:
6637                     dest_vars[k] = f(lhs_vars[k], np.nan)
6638             for k in rhs_data_vars:
6639                 if k not in dest_vars and join in ["right", "outer"]:
6640                     dest_vars[k] = f(rhs_vars[k], np.nan)
6641             return dest_vars
6642 
6643         if utils.is_dict_like(other) and not isinstance(other, Dataset):
6644             # can't use our shortcut of doing the binary operation with
6645             # Variable objects, so apply over our data vars instead.
6646             new_data_vars = apply_over_both(
6647                 self.data_vars, other, self.data_vars, other
6648             )
6649             return type(self)(new_data_vars)
6650 
6651         other_coords: Coordinates | None = getattr(other, "coords", None)
6652         ds = self.coords.merge(other_coords)
6653 
6654         if isinstance(other, Dataset):
6655             new_vars = apply_over_both(
6656                 self.data_vars, other.data_vars, self.variables, other.variables
6657             )
6658         else:
6659             other_variable = getattr(other, "variable", other)
6660             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}
6661         ds._variables.update(new_vars)
6662         ds._dims = calculate_dimensions(ds._variables)
6663         return ds
6664 
6665     def _copy_attrs_from(self, other):
6666         self.attrs = other.attrs
6667         for v in other.variables:
6668             if v in self.variables:
6669                 self.variables[v].attrs = other.variables[v].attrs
6670 
6671     def diff(
6672         self: T_Dataset,
6673         dim: Hashable,
6674         n: int = 1,
6675         label: Literal["upper", "lower"] = "upper",
6676     ) -> T_Dataset:
6677         """Calculate the n-th order discrete difference along given axis.
6678 
6679         Parameters
6680         ----------
6681         dim : Hashable
6682             Dimension over which to calculate the finite difference.
6683         n : int, default: 1
6684             The number of times values are differenced.
6685         label : {"upper", "lower"}, default: "upper"
6686             The new coordinate in dimension ``dim`` will have the
6687             values of either the minuend's or subtrahend's coordinate
6688             for values 'upper' and 'lower', respectively.
6689 
6690         Returns
6691         -------
6692         difference : Dataset
6693             The n-th order finite difference of this object.
6694 
6695         Notes
6696         -----
6697         `n` matches numpy's behavior and is different from pandas' first argument named
6698         `periods`.
6699 
6700         Examples
6701         --------
6702         >>> ds = xr.Dataset({"foo": ("x", [5, 5, 6, 6])})
6703         >>> ds.diff("x")
6704         <xarray.Dataset>
6705         Dimensions:  (x: 3)
6706         Dimensions without coordinates: x
6707         Data variables:
6708             foo      (x) int64 0 1 0
6709         >>> ds.diff("x", 2)
6710         <xarray.Dataset>
6711         Dimensions:  (x: 2)
6712         Dimensions without coordinates: x
6713         Data variables:
6714             foo      (x) int64 1 -1
6715 
6716         See Also
6717         --------
6718         Dataset.differentiate
6719         """
6720         if n == 0:
6721             return self
6722         if n < 0:
6723             raise ValueError(f"order `n` must be non-negative but got {n}")
6724 
6725         # prepare slices
6726         slice_start = {dim: slice(None, -1)}
6727         slice_end = {dim: slice(1, None)}
6728 
6729         # prepare new coordinate
6730         if label == "upper":
6731             slice_new = slice_end
6732         elif label == "lower":
6733             slice_new = slice_start
6734         else:
6735             raise ValueError("The 'label' argument has to be either 'upper' or 'lower'")
6736 
6737         indexes, index_vars = isel_indexes(self.xindexes, slice_new)
6738         variables = {}
6739 
6740         for name, var in self.variables.items():
6741             if name in index_vars:
6742                 variables[name] = index_vars[name]
6743             elif dim in var.dims:
6744                 if name in self.data_vars:
6745                     variables[name] = var.isel(slice_end) - var.isel(slice_start)
6746                 else:
6747                     variables[name] = var.isel(slice_new)
6748             else:
6749                 variables[name] = var
6750 
6751         difference = self._replace_with_new_dims(variables, indexes=indexes)
6752 
6753         if n > 1:
6754             return difference.diff(dim, n - 1)
6755         else:
6756             return difference
6757 
6758     def shift(
6759         self: T_Dataset,
6760         shifts: Mapping[Any, int] | None = None,
6761         fill_value: Any = xrdtypes.NA,
6762         **shifts_kwargs: int,
6763     ) -> T_Dataset:
6764 
6765         """Shift this dataset by an offset along one or more dimensions.
6766 
6767         Only data variables are moved; coordinates stay in place. This is
6768         consistent with the behavior of ``shift`` in pandas.
6769 
6770         Values shifted from beyond array bounds will appear at one end of
6771         each dimension, which are filled according to `fill_value`. For periodic
6772         offsets instead see `roll`.
6773 
6774         Parameters
6775         ----------
6776         shifts : mapping of hashable to int
6777             Integer offset to shift along each of the given dimensions.
6778             Positive offsets shift to the right; negative offsets shift to the
6779             left.
6780         fill_value : scalar or dict-like, optional
6781             Value to use for newly missing values. If a dict-like, maps
6782             variable names (including coordinates) to fill values.
6783         **shifts_kwargs
6784             The keyword arguments form of ``shifts``.
6785             One of shifts or shifts_kwargs must be provided.
6786 
6787         Returns
6788         -------
6789         shifted : Dataset
6790             Dataset with the same coordinates and attributes but shifted data
6791             variables.
6792 
6793         See Also
6794         --------
6795         roll
6796 
6797         Examples
6798         --------
6799         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
6800         >>> ds.shift(x=2)
6801         <xarray.Dataset>
6802         Dimensions:  (x: 5)
6803         Dimensions without coordinates: x
6804         Data variables:
6805             foo      (x) object nan nan 'a' 'b' 'c'
6806         """
6807         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
6808         invalid = [k for k in shifts if k not in self.dims]
6809         if invalid:
6810             raise ValueError(f"dimensions {invalid!r} do not exist")
6811 
6812         variables = {}
6813         for name, var in self.variables.items():
6814             if name in self.data_vars:
6815                 fill_value_ = (
6816                     fill_value.get(name, xrdtypes.NA)
6817                     if isinstance(fill_value, dict)
6818                     else fill_value
6819                 )
6820 
6821                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}
6822                 variables[name] = var.shift(fill_value=fill_value_, shifts=var_shifts)
6823             else:
6824                 variables[name] = var
6825 
6826         return self._replace(variables)
6827 
6828     def roll(
6829         self: T_Dataset,
6830         shifts: Mapping[Any, int] | None = None,
6831         roll_coords: bool = False,
6832         **shifts_kwargs: int,
6833     ) -> T_Dataset:
6834         """Roll this dataset by an offset along one or more dimensions.
6835 
6836         Unlike shift, roll treats the given dimensions as periodic, so will not
6837         create any missing values to be filled.
6838 
6839         Also unlike shift, roll may rotate all variables, including coordinates
6840         if specified. The direction of rotation is consistent with
6841         :py:func:`numpy.roll`.
6842 
6843         Parameters
6844         ----------
6845         shifts : mapping of hashable to int, optional
6846             A dict with keys matching dimensions and values given
6847             by integers to rotate each of the given dimensions. Positive
6848             offsets roll to the right; negative offsets roll to the left.
6849         roll_coords : bool, default: False
6850             Indicates whether to roll the coordinates by the offset too.
6851         **shifts_kwargs : {dim: offset, ...}, optional
6852             The keyword arguments form of ``shifts``.
6853             One of shifts or shifts_kwargs must be provided.
6854 
6855         Returns
6856         -------
6857         rolled : Dataset
6858             Dataset with the same attributes but rolled data and coordinates.
6859 
6860         See Also
6861         --------
6862         shift
6863 
6864         Examples
6865         --------
6866         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))}, coords={"x": np.arange(5)})
6867         >>> ds.roll(x=2)
6868         <xarray.Dataset>
6869         Dimensions:  (x: 5)
6870         Coordinates:
6871           * x        (x) int64 0 1 2 3 4
6872         Data variables:
6873             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6874 
6875         >>> ds.roll(x=2, roll_coords=True)
6876         <xarray.Dataset>
6877         Dimensions:  (x: 5)
6878         Coordinates:
6879           * x        (x) int64 3 4 0 1 2
6880         Data variables:
6881             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6882 
6883         """
6884         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
6885         invalid = [k for k in shifts if k not in self.dims]
6886         if invalid:
6887             raise ValueError(f"dimensions {invalid!r} do not exist")
6888 
6889         unrolled_vars: tuple[Hashable, ...]
6890 
6891         if roll_coords:
6892             indexes, index_vars = roll_indexes(self.xindexes, shifts)
6893             unrolled_vars = ()
6894         else:
6895             indexes = dict(self._indexes)
6896             index_vars = dict(self.xindexes.variables)
6897             unrolled_vars = tuple(self.coords)
6898 
6899         variables = {}
6900         for k, var in self.variables.items():
6901             if k in index_vars:
6902                 variables[k] = index_vars[k]
6903             elif k not in unrolled_vars:
6904                 variables[k] = var.roll(
6905                     shifts={k: s for k, s in shifts.items() if k in var.dims}
6906                 )
6907             else:
6908                 variables[k] = var
6909 
6910         return self._replace(variables, indexes=indexes)
6911 
6912     def sortby(
6913         self: T_Dataset,
6914         variables: Hashable | DataArray | list[Hashable | DataArray],
6915         ascending: bool = True,
6916     ) -> T_Dataset:
6917         """
6918         Sort object by labels or values (along an axis).
6919 
6920         Sorts the dataset, either along specified dimensions,
6921         or according to values of 1-D dataarrays that share dimension
6922         with calling object.
6923 
6924         If the input variables are dataarrays, then the dataarrays are aligned
6925         (via left-join) to the calling object prior to sorting by cell values.
6926         NaNs are sorted to the end, following Numpy convention.
6927 
6928         If multiple sorts along the same dimension is
6929         given, numpy's lexsort is performed along that dimension:
6930         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
6931         and the FIRST key in the sequence is used as the primary sort key,
6932         followed by the 2nd key, etc.
6933 
6934         Parameters
6935         ----------
6936         variables : Hashable, DataArray, or list of hashable or DataArray
6937             1D DataArray objects or name(s) of 1D variable(s) in
6938             coords/data_vars whose values are used to sort the dataset.
6939         ascending : bool, default: True
6940             Whether to sort by ascending or descending order.
6941 
6942         Returns
6943         -------
6944         sorted : Dataset
6945             A new dataset where all the specified dims are sorted by dim
6946             labels.
6947 
6948         See Also
6949         --------
6950         DataArray.sortby
6951         numpy.sort
6952         pandas.sort_values
6953         pandas.sort_index
6954 
6955         Examples
6956         --------
6957         >>> ds = xr.Dataset(
6958         ...     {
6959         ...         "A": (("x", "y"), [[1, 2], [3, 4]]),
6960         ...         "B": (("x", "y"), [[5, 6], [7, 8]]),
6961         ...     },
6962         ...     coords={"x": ["b", "a"], "y": [1, 0]},
6963         ... )
6964         >>> ds = ds.sortby("x")
6965         >>> ds
6966         <xarray.Dataset>
6967         Dimensions:  (x: 2, y: 2)
6968         Coordinates:
6969           * x        (x) <U1 'a' 'b'
6970           * y        (y) int64 1 0
6971         Data variables:
6972             A        (x, y) int64 3 4 1 2
6973             B        (x, y) int64 7 8 5 6
6974         """
6975         from xarray.core.dataarray import DataArray
6976 
6977         if not isinstance(variables, list):
6978             variables = [variables]
6979         else:
6980             variables = variables
6981         arrays = [v if isinstance(v, DataArray) else self[v] for v in variables]
6982         aligned_vars = align(self, *arrays, join="left")  # type: ignore[type-var]
6983         aligned_self: T_Dataset = aligned_vars[0]  # type: ignore[assignment]
6984         aligned_other_vars: tuple[DataArray, ...] = aligned_vars[1:]  # type: ignore[assignment]
6985         vars_by_dim = defaultdict(list)
6986         for data_array in aligned_other_vars:
6987             if data_array.ndim != 1:
6988                 raise ValueError("Input DataArray is not 1-D.")
6989             (key,) = data_array.dims
6990             vars_by_dim[key].append(data_array)
6991 
6992         indices = {}
6993         for key, arrays in vars_by_dim.items():
6994             order = np.lexsort(tuple(reversed(arrays)))
6995             indices[key] = order if ascending else order[::-1]
6996         return aligned_self.isel(indices)
6997 
6998     def quantile(
6999         self: T_Dataset,
7000         q: ArrayLike,
7001         dim: Dims = None,
7002         method: QuantileMethods = "linear",
7003         numeric_only: bool = False,
7004         keep_attrs: bool | None = None,
7005         skipna: bool | None = None,
7006         interpolation: QuantileMethods | None = None,
7007     ) -> T_Dataset:
7008         """Compute the qth quantile of the data along the specified dimension.
7009 
7010         Returns the qth quantiles(s) of the array elements for each variable
7011         in the Dataset.
7012 
7013         Parameters
7014         ----------
7015         q : float or array-like of float
7016             Quantile to compute, which must be between 0 and 1 inclusive.
7017         dim : str or Iterable of Hashable, optional
7018             Dimension(s) over which to apply quantile.
7019         method : str, default: "linear"
7020             This optional parameter specifies the interpolation method to use when the
7021             desired quantile lies between two data points. The options sorted by their R
7022             type as summarized in the H&F paper [1]_ are:
7023 
7024                 1. "inverted_cdf" (*)
7025                 2. "averaged_inverted_cdf" (*)
7026                 3. "closest_observation" (*)
7027                 4. "interpolated_inverted_cdf" (*)
7028                 5. "hazen" (*)
7029                 6. "weibull" (*)
7030                 7. "linear"  (default)
7031                 8. "median_unbiased" (*)
7032                 9. "normal_unbiased" (*)
7033 
7034             The first three methods are discontiuous.  The following discontinuous
7035             variations of the default "linear" (7.) option are also available:
7036 
7037                 * "lower"
7038                 * "higher"
7039                 * "midpoint"
7040                 * "nearest"
7041 
7042             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
7043             was previously called "interpolation", renamed in accordance with numpy
7044             version 1.22.0.
7045 
7046             (*) These methods require numpy version 1.22 or newer.
7047 
7048         keep_attrs : bool, optional
7049             If True, the dataset's attributes (`attrs`) will be copied from
7050             the original object to the new one.  If False (default), the new
7051             object will be returned without attributes.
7052         numeric_only : bool, optional
7053             If True, only apply ``func`` to variables with a numeric dtype.
7054         skipna : bool, optional
7055             If True, skip missing values (as marked by NaN). By default, only
7056             skips missing values for float dtypes; other dtypes either do not
7057             have a sentinel missing value (int) or skipna=True has not been
7058             implemented (object, datetime64 or timedelta64).
7059 
7060         Returns
7061         -------
7062         quantiles : Dataset
7063             If `q` is a single quantile, then the result is a scalar for each
7064             variable in data_vars. If multiple percentiles are given, first
7065             axis of the result corresponds to the quantile and a quantile
7066             dimension is added to the return Dataset. The other dimensions are
7067             the dimensions that remain after the reduction of the array.
7068 
7069         See Also
7070         --------
7071         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile
7072 
7073         Examples
7074         --------
7075         >>> ds = xr.Dataset(
7076         ...     {"a": (("x", "y"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},
7077         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
7078         ... )
7079         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)
7080         <xarray.Dataset>
7081         Dimensions:   ()
7082         Coordinates:
7083             quantile  float64 0.0
7084         Data variables:
7085             a         float64 0.7
7086         >>> ds.quantile(0, dim="x")
7087         <xarray.Dataset>
7088         Dimensions:   (y: 4)
7089         Coordinates:
7090           * y         (y) float64 1.0 1.5 2.0 2.5
7091             quantile  float64 0.0
7092         Data variables:
7093             a         (y) float64 0.7 4.2 2.6 1.5
7094         >>> ds.quantile([0, 0.5, 1])
7095         <xarray.Dataset>
7096         Dimensions:   (quantile: 3)
7097         Coordinates:
7098           * quantile  (quantile) float64 0.0 0.5 1.0
7099         Data variables:
7100             a         (quantile) float64 0.7 3.4 9.4
7101         >>> ds.quantile([0, 0.5, 1], dim="x")
7102         <xarray.Dataset>
7103         Dimensions:   (quantile: 3, y: 4)
7104         Coordinates:
7105           * y         (y) float64 1.0 1.5 2.0 2.5
7106           * quantile  (quantile) float64 0.0 0.5 1.0
7107         Data variables:
7108             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9
7109 
7110         References
7111         ----------
7112         .. [1] R. J. Hyndman and Y. Fan,
7113            "Sample quantiles in statistical packages,"
7114            The American Statistician, 50(4), pp. 361-365, 1996
7115         """
7116 
7117         # interpolation renamed to method in version 0.21.0
7118         # check here and in variable to avoid repeated warnings
7119         if interpolation is not None:
7120             warnings.warn(
7121                 "The `interpolation` argument to quantile was renamed to `method`.",
7122                 FutureWarning,
7123             )
7124 
7125             if method != "linear":
7126                 raise TypeError("Cannot pass interpolation and method keywords!")
7127 
7128             method = interpolation
7129 
7130         dims: set[Hashable]
7131         if isinstance(dim, str):
7132             dims = {dim}
7133         elif dim is None or dim is ...:
7134             dims = set(self.dims)
7135         else:
7136             dims = set(dim)
7137 
7138         _assert_empty(
7139             tuple(d for d in dims if d not in self.dims),
7140             "Dataset does not contain the dimensions: %s",
7141         )
7142 
7143         q = np.asarray(q, dtype=np.float64)
7144 
7145         variables = {}
7146         for name, var in self.variables.items():
7147             reduce_dims = [d for d in var.dims if d in dims]
7148             if reduce_dims or not var.dims:
7149                 if name not in self.coords:
7150                     if (
7151                         not numeric_only
7152                         or np.issubdtype(var.dtype, np.number)
7153                         or var.dtype == np.bool_
7154                     ):
7155                         variables[name] = var.quantile(
7156                             q,
7157                             dim=reduce_dims,
7158                             method=method,
7159                             keep_attrs=keep_attrs,
7160                             skipna=skipna,
7161                         )
7162 
7163             else:
7164                 variables[name] = var
7165 
7166         # construct the new dataset
7167         coord_names = {k for k in self.coords if k in variables}
7168         indexes = {k: v for k, v in self._indexes.items() if k in variables}
7169         if keep_attrs is None:
7170             keep_attrs = _get_keep_attrs(default=False)
7171         attrs = self.attrs if keep_attrs else None
7172         new = self._replace_with_new_dims(
7173             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
7174         )
7175         return new.assign_coords(quantile=q)
7176 
7177     def rank(
7178         self: T_Dataset,
7179         dim: Hashable,
7180         pct: bool = False,
7181         keep_attrs: bool | None = None,
7182     ) -> T_Dataset:
7183         """Ranks the data.
7184 
7185         Equal values are assigned a rank that is the average of the ranks that
7186         would have been otherwise assigned to all of the values within
7187         that set.
7188         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.
7189 
7190         NaNs in the input array are returned as NaNs.
7191 
7192         The `bottleneck` library is required.
7193 
7194         Parameters
7195         ----------
7196         dim : Hashable
7197             Dimension over which to compute rank.
7198         pct : bool, default: False
7199             If True, compute percentage ranks, otherwise compute integer ranks.
7200         keep_attrs : bool or None, optional
7201             If True, the dataset's attributes (`attrs`) will be copied from
7202             the original object to the new one.  If False, the new
7203             object will be returned without attributes.
7204 
7205         Returns
7206         -------
7207         ranked : Dataset
7208             Variables that do not depend on `dim` are dropped.
7209         """
7210         if not OPTIONS["use_bottleneck"]:
7211             raise RuntimeError(
7212                 "rank requires bottleneck to be enabled."
7213                 " Call `xr.set_options(use_bottleneck=True)` to enable it."
7214             )
7215 
7216         if dim not in self.dims:
7217             raise ValueError(f"Dataset does not contain the dimension: {dim}")
7218 
7219         variables = {}
7220         for name, var in self.variables.items():
7221             if name in self.data_vars:
7222                 if dim in var.dims:
7223                     variables[name] = var.rank(dim, pct=pct)
7224             else:
7225                 variables[name] = var
7226 
7227         coord_names = set(self.coords)
7228         if keep_attrs is None:
7229             keep_attrs = _get_keep_attrs(default=False)
7230         attrs = self.attrs if keep_attrs else None
7231         return self._replace(variables, coord_names, attrs=attrs)
7232 
7233     def differentiate(
7234         self: T_Dataset,
7235         coord: Hashable,
7236         edge_order: Literal[1, 2] = 1,
7237         datetime_unit: DatetimeUnitOptions | None = None,
7238     ) -> T_Dataset:
7239         """ Differentiate with the second order accurate central
7240         differences.
7241 
7242         .. note::
7243             This feature is limited to simple cartesian geometry, i.e. coord
7244             must be one dimensional.
7245 
7246         Parameters
7247         ----------
7248         coord : Hashable
7249             The coordinate to be used to compute the gradient.
7250         edge_order : {1, 2}, default: 1
7251             N-th order accurate differences at the boundaries.
7252         datetime_unit : None or {"Y", "M", "W", "D", "h", "m", "s", "ms", \
7253             "us", "ns", "ps", "fs", "as", None}, default: None
7254             Unit to compute gradient. Only valid for datetime coordinate.
7255 
7256         Returns
7257         -------
7258         differentiated: Dataset
7259 
7260         See also
7261         --------
7262         numpy.gradient: corresponding numpy function
7263         """
7264         from xarray.core.variable import Variable
7265 
7266         if coord not in self.variables and coord not in self.dims:
7267             raise ValueError(f"Coordinate {coord} does not exist.")
7268 
7269         coord_var = self[coord].variable
7270         if coord_var.ndim != 1:
7271             raise ValueError(
7272                 "Coordinate {} must be 1 dimensional but is {}"
7273                 " dimensional".format(coord, coord_var.ndim)
7274             )
7275 
7276         dim = coord_var.dims[0]
7277         if _contains_datetime_like_objects(coord_var):
7278             if coord_var.dtype.kind in "mM" and datetime_unit is None:
7279                 datetime_unit = cast(
7280                     "DatetimeUnitOptions", np.datetime_data(coord_var.dtype)[0]
7281                 )
7282             elif datetime_unit is None:
7283                 datetime_unit = "s"  # Default to seconds for cftime objects
7284             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)
7285 
7286         variables = {}
7287         for k, v in self.variables.items():
7288             if k in self.data_vars and dim in v.dims and k not in self.coords:
7289                 if _contains_datetime_like_objects(v):
7290                     v = v._to_numeric(datetime_unit=datetime_unit)
7291                 grad = duck_array_ops.gradient(
7292                     v.data,
7293                     coord_var.data,
7294                     edge_order=edge_order,
7295                     axis=v.get_axis_num(dim),
7296                 )
7297                 variables[k] = Variable(v.dims, grad)
7298             else:
7299                 variables[k] = v
7300         return self._replace(variables)
7301 
7302     def integrate(
7303         self: T_Dataset,
7304         coord: Hashable | Sequence[Hashable],
7305         datetime_unit: DatetimeUnitOptions = None,
7306     ) -> T_Dataset:
7307         """Integrate along the given coordinate using the trapezoidal rule.
7308 
7309         .. note::
7310             This feature is limited to simple cartesian geometry, i.e. coord
7311             must be one dimensional.
7312 
7313         Parameters
7314         ----------
7315         coord : hashable, or sequence of hashable
7316             Coordinate(s) used for the integration.
7317         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7318                         'ps', 'fs', 'as', None}, optional
7319             Specify the unit if datetime coordinate is used.
7320 
7321         Returns
7322         -------
7323         integrated : Dataset
7324 
7325         See also
7326         --------
7327         DataArray.integrate
7328         numpy.trapz : corresponding numpy function
7329 
7330         Examples
7331         --------
7332         >>> ds = xr.Dataset(
7333         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7334         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7335         ... )
7336         >>> ds
7337         <xarray.Dataset>
7338         Dimensions:  (x: 4)
7339         Coordinates:
7340           * x        (x) int64 0 1 2 3
7341             y        (x) int64 1 7 3 5
7342         Data variables:
7343             a        (x) int64 5 5 6 6
7344             b        (x) int64 1 2 1 0
7345         >>> ds.integrate("x")
7346         <xarray.Dataset>
7347         Dimensions:  ()
7348         Data variables:
7349             a        float64 16.5
7350             b        float64 3.5
7351         >>> ds.integrate("y")
7352         <xarray.Dataset>
7353         Dimensions:  ()
7354         Data variables:
7355             a        float64 20.0
7356             b        float64 4.0
7357         """
7358         if not isinstance(coord, (list, tuple)):
7359             coord = (coord,)
7360         result = self
7361         for c in coord:
7362             result = result._integrate_one(c, datetime_unit=datetime_unit)
7363         return result
7364 
7365     def _integrate_one(self, coord, datetime_unit=None, cumulative=False):
7366         from xarray.core.variable import Variable
7367 
7368         if coord not in self.variables and coord not in self.dims:
7369             raise ValueError(f"Coordinate {coord} does not exist.")
7370 
7371         coord_var = self[coord].variable
7372         if coord_var.ndim != 1:
7373             raise ValueError(
7374                 "Coordinate {} must be 1 dimensional but is {}"
7375                 " dimensional".format(coord, coord_var.ndim)
7376             )
7377 
7378         dim = coord_var.dims[0]
7379         if _contains_datetime_like_objects(coord_var):
7380             if coord_var.dtype.kind in "mM" and datetime_unit is None:
7381                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
7382             elif datetime_unit is None:
7383                 datetime_unit = "s"  # Default to seconds for cftime objects
7384             coord_var = coord_var._replace(
7385                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)
7386             )
7387 
7388         variables = {}
7389         coord_names = set()
7390         for k, v in self.variables.items():
7391             if k in self.coords:
7392                 if dim not in v.dims or cumulative:
7393                     variables[k] = v
7394                     coord_names.add(k)
7395             else:
7396                 if k in self.data_vars and dim in v.dims:
7397                     if _contains_datetime_like_objects(v):
7398                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)
7399                     if cumulative:
7400                         integ = duck_array_ops.cumulative_trapezoid(
7401                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7402                         )
7403                         v_dims = v.dims
7404                     else:
7405                         integ = duck_array_ops.trapz(
7406                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7407                         )
7408                         v_dims = list(v.dims)
7409                         v_dims.remove(dim)
7410                     variables[k] = Variable(v_dims, integ)
7411                 else:
7412                     variables[k] = v
7413         indexes = {k: v for k, v in self._indexes.items() if k in variables}
7414         return self._replace_with_new_dims(
7415             variables, coord_names=coord_names, indexes=indexes
7416         )
7417 
7418     def cumulative_integrate(
7419         self: T_Dataset,
7420         coord: Hashable | Sequence[Hashable],
7421         datetime_unit: DatetimeUnitOptions = None,
7422     ) -> T_Dataset:
7423         """Integrate along the given coordinate using the trapezoidal rule.
7424 
7425         .. note::
7426             This feature is limited to simple cartesian geometry, i.e. coord
7427             must be one dimensional.
7428 
7429             The first entry of the cumulative integral of each variable is always 0, in
7430             order to keep the length of the dimension unchanged between input and
7431             output.
7432 
7433         Parameters
7434         ----------
7435         coord : hashable, or sequence of hashable
7436             Coordinate(s) used for the integration.
7437         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7438                         'ps', 'fs', 'as', None}, optional
7439             Specify the unit if datetime coordinate is used.
7440 
7441         Returns
7442         -------
7443         integrated : Dataset
7444 
7445         See also
7446         --------
7447         DataArray.cumulative_integrate
7448         scipy.integrate.cumulative_trapezoid : corresponding scipy function
7449 
7450         Examples
7451         --------
7452         >>> ds = xr.Dataset(
7453         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7454         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7455         ... )
7456         >>> ds
7457         <xarray.Dataset>
7458         Dimensions:  (x: 4)
7459         Coordinates:
7460           * x        (x) int64 0 1 2 3
7461             y        (x) int64 1 7 3 5
7462         Data variables:
7463             a        (x) int64 5 5 6 6
7464             b        (x) int64 1 2 1 0
7465         >>> ds.cumulative_integrate("x")
7466         <xarray.Dataset>
7467         Dimensions:  (x: 4)
7468         Coordinates:
7469           * x        (x) int64 0 1 2 3
7470             y        (x) int64 1 7 3 5
7471         Data variables:
7472             a        (x) float64 0.0 5.0 10.5 16.5
7473             b        (x) float64 0.0 1.5 3.0 3.5
7474         >>> ds.cumulative_integrate("y")
7475         <xarray.Dataset>
7476         Dimensions:  (x: 4)
7477         Coordinates:
7478           * x        (x) int64 0 1 2 3
7479             y        (x) int64 1 7 3 5
7480         Data variables:
7481             a        (x) float64 0.0 30.0 8.0 20.0
7482             b        (x) float64 0.0 9.0 3.0 4.0
7483         """
7484         if not isinstance(coord, (list, tuple)):
7485             coord = (coord,)
7486         result = self
7487         for c in coord:
7488             result = result._integrate_one(
7489                 c, datetime_unit=datetime_unit, cumulative=True
7490             )
7491         return result
7492 
7493     @property
7494     def real(self: T_Dataset) -> T_Dataset:
7495         """
7496         The real part of each data variable.
7497 
7498         See Also
7499         --------
7500         numpy.ndarray.real
7501         """
7502         return self.map(lambda x: x.real, keep_attrs=True)
7503 
7504     @property
7505     def imag(self: T_Dataset) -> T_Dataset:
7506         """
7507         The imaginary part of each data variable.
7508 
7509         See Also
7510         --------
7511         numpy.ndarray.imag
7512         """
7513         return self.map(lambda x: x.imag, keep_attrs=True)
7514 
7515     plot = utils.UncachedAccessor(DatasetPlotAccessor)
7516 
7517     def filter_by_attrs(self: T_Dataset, **kwargs) -> T_Dataset:
7518         """Returns a ``Dataset`` with variables that match specific conditions.
7519 
7520         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned
7521         containing only the variables for which all the filter tests pass.
7522         These tests are either ``key=value`` for which the attribute ``key``
7523         has the exact value ``value`` or the callable passed into
7524         ``key=callable`` returns True. The callable will be passed a single
7525         value, either the value of the attribute ``key`` or ``None`` if the
7526         DataArray does not have an attribute with the name ``key``.
7527 
7528         Parameters
7529         ----------
7530         **kwargs
7531             key : str
7532                 Attribute name.
7533             value : callable or obj
7534                 If value is a callable, it should return a boolean in the form
7535                 of bool = func(attr) where attr is da.attrs[key].
7536                 Otherwise, value will be compared to the each
7537                 DataArray's attrs[key].
7538 
7539         Returns
7540         -------
7541         new : Dataset
7542             New dataset with variables filtered by attribute.
7543 
7544         Examples
7545         --------
7546         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)
7547         >>> precip = 10 * np.random.rand(2, 2, 3)
7548         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
7549         >>> lat = [[42.25, 42.21], [42.63, 42.59]]
7550         >>> dims = ["x", "y", "time"]
7551         >>> temp_attr = dict(standard_name="air_potential_temperature")
7552         >>> precip_attr = dict(standard_name="convective_precipitation_flux")
7553 
7554         >>> ds = xr.Dataset(
7555         ...     dict(
7556         ...         temperature=(dims, temp, temp_attr),
7557         ...         precipitation=(dims, precip, precip_attr),
7558         ...     ),
7559         ...     coords=dict(
7560         ...         lon=(["x", "y"], lon),
7561         ...         lat=(["x", "y"], lat),
7562         ...         time=pd.date_range("2014-09-06", periods=3),
7563         ...         reference_time=pd.Timestamp("2014-09-05"),
7564         ...     ),
7565         ... )
7566 
7567         Get variables matching a specific standard_name:
7568 
7569         >>> ds.filter_by_attrs(standard_name="convective_precipitation_flux")
7570         <xarray.Dataset>
7571         Dimensions:         (x: 2, y: 2, time: 3)
7572         Coordinates:
7573             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7574             lat             (x, y) float64 42.25 42.21 42.63 42.59
7575           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7576             reference_time  datetime64[ns] 2014-09-05
7577         Dimensions without coordinates: x, y
7578         Data variables:
7579             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7580 
7581         Get all variables that have a standard_name attribute:
7582 
7583         >>> standard_name = lambda v: v is not None
7584         >>> ds.filter_by_attrs(standard_name=standard_name)
7585         <xarray.Dataset>
7586         Dimensions:         (x: 2, y: 2, time: 3)
7587         Coordinates:
7588             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7589             lat             (x, y) float64 42.25 42.21 42.63 42.59
7590           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7591             reference_time  datetime64[ns] 2014-09-05
7592         Dimensions without coordinates: x, y
7593         Data variables:
7594             temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
7595             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7596 
7597         """
7598         selection = []
7599         for var_name, variable in self.variables.items():
7600             has_value_flag = False
7601             for attr_name, pattern in kwargs.items():
7602                 attr_value = variable.attrs.get(attr_name)
7603                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:
7604                     has_value_flag = True
7605                 else:
7606                     has_value_flag = False
7607                     break
7608             if has_value_flag is True:
7609                 selection.append(var_name)
7610         return self[selection]
7611 
7612     def unify_chunks(self: T_Dataset) -> T_Dataset:
7613         """Unify chunk size along all chunked dimensions of this Dataset.
7614 
7615         Returns
7616         -------
7617         Dataset with consistent chunk sizes for all dask-array variables
7618 
7619         See Also
7620         --------
7621         dask.array.core.unify_chunks
7622         """
7623 
7624         return unify_chunks(self)[0]
7625 
7626     def map_blocks(
7627         self,
7628         func: Callable[..., T_Xarray],
7629         args: Sequence[Any] = (),
7630         kwargs: Mapping[str, Any] | None = None,
7631         template: DataArray | Dataset | None = None,
7632     ) -> T_Xarray:
7633         """
7634         Apply a function to each block of this Dataset.
7635 
7636         .. warning::
7637             This method is experimental and its signature may change.
7638 
7639         Parameters
7640         ----------
7641         func : callable
7642             User-provided function that accepts a Dataset as its first
7643             parameter. The function will receive a subset or 'block' of this Dataset (see below),
7644             corresponding to one chunk along each chunked dimension. ``func`` will be
7645             executed as ``func(subset_dataset, *subset_args, **kwargs)``.
7646 
7647             This function must return either a single DataArray or a single Dataset.
7648 
7649             This function cannot add a new chunked dimension.
7650         args : sequence
7651             Passed to func after unpacking and subsetting any xarray objects by blocks.
7652             xarray objects in args must be aligned with obj, otherwise an error is raised.
7653         kwargs : Mapping or None
7654             Passed verbatim to func after unpacking. xarray objects, if any, will not be
7655             subset to blocks. Passing dask collections in kwargs is not allowed.
7656         template : DataArray, Dataset or None, optional
7657             xarray object representing the final result after compute is called. If not provided,
7658             the function will be first run on mocked-up data, that looks like this object but
7659             has sizes 0, to determine properties of the returned object such as dtype,
7660             variable names, attributes, new dimensions and new indexes (if any).
7661             ``template`` must be provided if the function changes the size of existing dimensions.
7662             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
7663             ``attrs`` set by ``func`` will be ignored.
7664 
7665         Returns
7666         -------
7667         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
7668         function.
7669 
7670         Notes
7671         -----
7672         This function is designed for when ``func`` needs to manipulate a whole xarray object
7673         subset to each block. Each block is loaded into memory. In the more common case where
7674         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
7675 
7676         If none of the variables in this object is backed by dask arrays, calling this function is
7677         equivalent to calling ``func(obj, *args, **kwargs)``.
7678 
7679         See Also
7680         --------
7681         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
7682         xarray.DataArray.map_blocks
7683 
7684         Examples
7685         --------
7686         Calculate an anomaly from climatology using ``.groupby()``. Using
7687         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
7688         its indices, and its methods like ``.groupby()``.
7689 
7690         >>> def calculate_anomaly(da, groupby_type="time.month"):
7691         ...     gb = da.groupby(groupby_type)
7692         ...     clim = gb.mean(dim="time")
7693         ...     return gb - clim
7694         ...
7695         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
7696         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
7697         >>> np.random.seed(123)
7698         >>> array = xr.DataArray(
7699         ...     np.random.rand(len(time)),
7700         ...     dims=["time"],
7701         ...     coords={"time": time, "month": month},
7702         ... ).chunk()
7703         >>> ds = xr.Dataset({"a": array})
7704         >>> ds.map_blocks(calculate_anomaly, template=ds).compute()
7705         <xarray.Dataset>
7706         Dimensions:  (time: 24)
7707         Coordinates:
7708           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7709             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
7710         Data variables:
7711             a        (time) float64 0.1289 0.1132 -0.0856 ... 0.2287 0.1906 -0.05901
7712 
7713         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
7714         to the function being applied in ``xr.map_blocks()``:
7715 
7716         >>> ds.map_blocks(
7717         ...     calculate_anomaly,
7718         ...     kwargs={"groupby_type": "time.year"},
7719         ...     template=ds,
7720         ... )
7721         <xarray.Dataset>
7722         Dimensions:  (time: 24)
7723         Coordinates:
7724           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7725             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
7726         Data variables:
7727             a        (time) float64 dask.array<chunksize=(24,), meta=np.ndarray>
7728         """
7729         from xarray.core.parallel import map_blocks
7730 
7731         return map_blocks(func, self, args, kwargs, template)
7732 
7733     def polyfit(
7734         self: T_Dataset,
7735         dim: Hashable,
7736         deg: int,
7737         skipna: bool | None = None,
7738         rcond: float | None = None,
7739         w: Hashable | Any = None,
7740         full: bool = False,
7741         cov: bool | Literal["unscaled"] = False,
7742     ) -> T_Dataset:
7743         """
7744         Least squares polynomial fit.
7745 
7746         This replicates the behaviour of `numpy.polyfit` but differs by skipping
7747         invalid values when `skipna = True`.
7748 
7749         Parameters
7750         ----------
7751         dim : hashable
7752             Coordinate along which to fit the polynomials.
7753         deg : int
7754             Degree of the fitting polynomial.
7755         skipna : bool or None, optional
7756             If True, removes all invalid values before fitting each 1D slices of the array.
7757             Default is True if data is stored in a dask.array or if there is any
7758             invalid values, False otherwise.
7759         rcond : float or None, optional
7760             Relative condition number to the fit.
7761         w : hashable or Any, optional
7762             Weights to apply to the y-coordinate of the sample points.
7763             Can be an array-like object or the name of a coordinate in the dataset.
7764         full : bool, default: False
7765             Whether to return the residuals, matrix rank and singular values in addition
7766             to the coefficients.
7767         cov : bool or "unscaled", default: False
7768             Whether to return to the covariance matrix in addition to the coefficients.
7769             The matrix is not scaled if `cov='unscaled'`.
7770 
7771         Returns
7772         -------
7773         polyfit_results : Dataset
7774             A single dataset which contains (for each "var" in the input dataset):
7775 
7776             [var]_polyfit_coefficients
7777                 The coefficients of the best fit for each variable in this dataset.
7778             [var]_polyfit_residuals
7779                 The residuals of the least-square computation for each variable (only included if `full=True`)
7780                 When the matrix rank is deficient, np.nan is returned.
7781             [dim]_matrix_rank
7782                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7783                 The rank is computed ignoring the NaN values that might be skipped.
7784             [dim]_singular_values
7785                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7786             [var]_polyfit_covariance
7787                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
7788 
7789         Warns
7790         -----
7791         RankWarning
7792             The rank of the coefficient matrix in the least-squares fit is deficient.
7793             The warning is not raised with in-memory (not dask) data and `full=True`.
7794 
7795         See Also
7796         --------
7797         numpy.polyfit
7798         numpy.polyval
7799         xarray.polyval
7800         """
7801         from xarray.core.dataarray import DataArray
7802 
7803         variables = {}
7804         skipna_da = skipna
7805 
7806         x = get_clean_interp_index(self, dim, strict=False)
7807         xname = f"{self[dim].name}_"
7808         order = int(deg) + 1
7809         lhs = np.vander(x, order)
7810 
7811         if rcond is None:
7812             rcond = (
7813                 x.shape[0] * np.core.finfo(x.dtype).eps  # type: ignore[attr-defined]
7814             )
7815 
7816         # Weights:
7817         if w is not None:
7818             if isinstance(w, Hashable):
7819                 w = self.coords[w]
7820             w = np.asarray(w)
7821             if w.ndim != 1:
7822                 raise TypeError("Expected a 1-d array for weights.")
7823             if w.shape[0] != lhs.shape[0]:
7824                 raise TypeError(f"Expected w and {dim} to have the same length")
7825             lhs *= w[:, np.newaxis]
7826 
7827         # Scaling
7828         scale = np.sqrt((lhs * lhs).sum(axis=0))
7829         lhs /= scale
7830 
7831         degree_dim = utils.get_temp_dimname(self.dims, "degree")
7832 
7833         rank = np.linalg.matrix_rank(lhs)
7834 
7835         if full:
7836             rank = DataArray(rank, name=xname + "matrix_rank")
7837             variables[rank.name] = rank
7838             _sing = np.linalg.svd(lhs, compute_uv=False)
7839             sing = DataArray(
7840                 _sing,
7841                 dims=(degree_dim,),
7842                 coords={degree_dim: np.arange(rank - 1, -1, -1)},
7843                 name=xname + "singular_values",
7844             )
7845             variables[sing.name] = sing
7846 
7847         for name, da in self.data_vars.items():
7848             if dim not in da.dims:
7849                 continue
7850 
7851             if is_duck_dask_array(da.data) and (
7852                 rank != order or full or skipna is None
7853             ):
7854                 # Current algorithm with dask and skipna=False neither supports
7855                 # deficient ranks nor does it output the "full" info (issue dask/dask#6516)
7856                 skipna_da = True
7857             elif skipna is None:
7858                 skipna_da = bool(np.any(da.isnull()))
7859 
7860             dims_to_stack = [dimname for dimname in da.dims if dimname != dim]
7861             stacked_coords: dict[Hashable, DataArray] = {}
7862             if dims_to_stack:
7863                 stacked_dim = utils.get_temp_dimname(dims_to_stack, "stacked")
7864                 rhs = da.transpose(dim, *dims_to_stack).stack(
7865                     {stacked_dim: dims_to_stack}
7866                 )
7867                 stacked_coords = {stacked_dim: rhs[stacked_dim]}
7868                 scale_da = scale[:, np.newaxis]
7869             else:
7870                 rhs = da
7871                 scale_da = scale
7872 
7873             if w is not None:
7874                 rhs *= w[:, np.newaxis]
7875 
7876             with warnings.catch_warnings():
7877                 if full:  # Copy np.polyfit behavior
7878                     warnings.simplefilter("ignore", np.RankWarning)
7879                 else:  # Raise only once per variable
7880                     warnings.simplefilter("once", np.RankWarning)
7881 
7882                 coeffs, residuals = duck_array_ops.least_squares(
7883                     lhs, rhs.data, rcond=rcond, skipna=skipna_da
7884                 )
7885 
7886             if isinstance(name, str):
7887                 name = f"{name}_"
7888             else:
7889                 # Thus a ReprObject => polyfit was called on a DataArray
7890                 name = ""
7891 
7892             coeffs = DataArray(
7893                 coeffs / scale_da,
7894                 dims=[degree_dim] + list(stacked_coords.keys()),
7895                 coords={degree_dim: np.arange(order)[::-1], **stacked_coords},
7896                 name=name + "polyfit_coefficients",
7897             )
7898             if dims_to_stack:
7899                 coeffs = coeffs.unstack(stacked_dim)
7900             variables[coeffs.name] = coeffs
7901 
7902             if full or (cov is True):
7903                 residuals = DataArray(
7904                     residuals if dims_to_stack else residuals.squeeze(),
7905                     dims=list(stacked_coords.keys()),
7906                     coords=stacked_coords,
7907                     name=name + "polyfit_residuals",
7908                 )
7909                 if dims_to_stack:
7910                     residuals = residuals.unstack(stacked_dim)
7911                 variables[residuals.name] = residuals
7912 
7913             if cov:
7914                 Vbase = np.linalg.inv(np.dot(lhs.T, lhs))
7915                 Vbase /= np.outer(scale, scale)
7916                 if cov == "unscaled":
7917                     fac = 1
7918                 else:
7919                     if x.shape[0] <= order:
7920                         raise ValueError(
7921                             "The number of data points must exceed order to scale the covariance matrix."
7922                         )
7923                     fac = residuals / (x.shape[0] - order)
7924                 covariance = DataArray(Vbase, dims=("cov_i", "cov_j")) * fac
7925                 variables[name + "polyfit_covariance"] = covariance
7926 
7927         return type(self)(data_vars=variables, attrs=self.attrs.copy())
7928 
7929     def pad(
7930         self: T_Dataset,
7931         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,
7932         mode: PadModeOptions = "constant",
7933         stat_length: int
7934         | tuple[int, int]
7935         | Mapping[Any, tuple[int, int]]
7936         | None = None,
7937         constant_values: (
7938             float | tuple[float, float] | Mapping[Any, tuple[float, float]] | None
7939         ) = None,
7940         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
7941         reflect_type: PadReflectOptions = None,
7942         keep_attrs: bool | None = None,
7943         **pad_width_kwargs: Any,
7944     ) -> T_Dataset:
7945         """Pad this dataset along one or more dimensions.
7946 
7947         .. warning::
7948             This function is experimental and its behaviour is likely to change
7949             especially regarding padding of dimension coordinates (or IndexVariables).
7950 
7951         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
7952         coordinates will be padded with the same mode, otherwise coordinates
7953         are padded using the "constant" mode with fill_value dtypes.NA.
7954 
7955         Parameters
7956         ----------
7957         pad_width : mapping of hashable to tuple of int
7958             Mapping with the form of {dim: (pad_before, pad_after)}
7959             describing the number of values padded along each dimension.
7960             {dim: pad} is a shortcut for pad_before = pad_after = pad
7961         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
7962             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
7963             How to pad the DataArray (taken from numpy docs):
7964 
7965             - "constant": Pads with a constant value.
7966             - "edge": Pads with the edge values of array.
7967             - "linear_ramp": Pads with the linear ramp between end_value and the
7968               array edge value.
7969             - "maximum": Pads with the maximum value of all or part of the
7970               vector along each axis.
7971             - "mean": Pads with the mean value of all or part of the
7972               vector along each axis.
7973             - "median": Pads with the median value of all or part of the
7974               vector along each axis.
7975             - "minimum": Pads with the minimum value of all or part of the
7976               vector along each axis.
7977             - "reflect": Pads with the reflection of the vector mirrored on
7978               the first and last values of the vector along each axis.
7979             - "symmetric": Pads with the reflection of the vector mirrored
7980               along the edge of the array.
7981             - "wrap": Pads with the wrap of the vector along the axis.
7982               The first values are used to pad the end and the
7983               end values are used to pad the beginning.
7984 
7985         stat_length : int, tuple or mapping of hashable to tuple, default: None
7986             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
7987             values at edge of each axis used to calculate the statistic value.
7988             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
7989             statistic lengths along each dimension.
7990             ((before, after),) yields same before and after statistic lengths
7991             for each dimension.
7992             (stat_length,) or int is a shortcut for before = after = statistic
7993             length for all axes.
7994             Default is ``None``, to use the entire axis.
7995         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
7996             Used in 'constant'.  The values to set the padded values for each
7997             axis.
7998             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
7999             pad constants along each dimension.
8000             ``((before, after),)`` yields same before and after constants for each
8001             dimension.
8002             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
8003             all dimensions.
8004             Default is 0.
8005         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
8006             Used in 'linear_ramp'.  The values used for the ending value of the
8007             linear_ramp and that will form the edge of the padded array.
8008             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
8009             end values along each dimension.
8010             ``((before, after),)`` yields same before and after end values for each
8011             axis.
8012             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
8013             all axes.
8014             Default is 0.
8015         reflect_type : {"even", "odd", None}, optional
8016             Used in "reflect", and "symmetric".  The "even" style is the
8017             default with an unaltered reflection around the edge value.  For
8018             the "odd" style, the extended part of the array is created by
8019             subtracting the reflected values from two times the edge value.
8020         keep_attrs : bool or None, optional
8021             If True, the attributes (``attrs``) will be copied from the
8022             original object to the new one. If False, the new object
8023             will be returned without attributes.
8024         **pad_width_kwargs
8025             The keyword arguments form of ``pad_width``.
8026             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
8027 
8028         Returns
8029         -------
8030         padded : Dataset
8031             Dataset with the padded coordinates and data.
8032 
8033         See Also
8034         --------
8035         Dataset.shift, Dataset.roll, Dataset.bfill, Dataset.ffill, numpy.pad, dask.array.pad
8036 
8037         Notes
8038         -----
8039         By default when ``mode="constant"`` and ``constant_values=None``, integer types will be
8040         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion
8041         specify ``constant_values=np.nan``
8042 
8043         Padding coordinates will drop their corresponding index (if any) and will reset default
8044         indexes for dimension coordinates.
8045 
8046         Examples
8047         --------
8048         >>> ds = xr.Dataset({"foo": ("x", range(5))})
8049         >>> ds.pad(x=(1, 2))
8050         <xarray.Dataset>
8051         Dimensions:  (x: 8)
8052         Dimensions without coordinates: x
8053         Data variables:
8054             foo      (x) float64 nan 0.0 1.0 2.0 3.0 4.0 nan nan
8055         """
8056         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, "pad")
8057 
8058         if mode in ("edge", "reflect", "symmetric", "wrap"):
8059             coord_pad_mode = mode
8060             coord_pad_options = {
8061                 "stat_length": stat_length,
8062                 "constant_values": constant_values,
8063                 "end_values": end_values,
8064                 "reflect_type": reflect_type,
8065             }
8066         else:
8067             coord_pad_mode = "constant"
8068             coord_pad_options = {}
8069 
8070         if keep_attrs is None:
8071             keep_attrs = _get_keep_attrs(default=True)
8072 
8073         variables = {}
8074 
8075         # keep indexes that won't be affected by pad and drop all other indexes
8076         xindexes = self.xindexes
8077         pad_dims = set(pad_width)
8078         indexes = {}
8079         for k, idx in xindexes.items():
8080             if not pad_dims.intersection(xindexes.get_all_dims(k)):
8081                 indexes[k] = idx
8082 
8083         for name, var in self.variables.items():
8084             var_pad_width = {k: v for k, v in pad_width.items() if k in var.dims}
8085             if not var_pad_width:
8086                 variables[name] = var
8087             elif name in self.data_vars:
8088                 variables[name] = var.pad(
8089                     pad_width=var_pad_width,
8090                     mode=mode,
8091                     stat_length=stat_length,
8092                     constant_values=constant_values,
8093                     end_values=end_values,
8094                     reflect_type=reflect_type,
8095                     keep_attrs=keep_attrs,
8096                 )
8097             else:
8098                 variables[name] = var.pad(
8099                     pad_width=var_pad_width,
8100                     mode=coord_pad_mode,
8101                     keep_attrs=keep_attrs,
8102                     **coord_pad_options,  # type: ignore[arg-type]
8103                 )
8104                 # reset default index of dimension coordinates
8105                 if (name,) == var.dims:
8106                     dim_var = {name: variables[name]}
8107                     index = PandasIndex.from_variables(dim_var, options={})
8108                     index_vars = index.create_variables(dim_var)
8109                     indexes[name] = index
8110                     variables[name] = index_vars[name]
8111 
8112         attrs = self._attrs if keep_attrs else None
8113         return self._replace_with_new_dims(variables, indexes=indexes, attrs=attrs)
8114 
8115     def idxmin(
8116         self: T_Dataset,
8117         dim: Hashable | None = None,
8118         skipna: bool | None = None,
8119         fill_value: Any = xrdtypes.NA,
8120         keep_attrs: bool | None = None,
8121     ) -> T_Dataset:
8122         """Return the coordinate label of the minimum value along a dimension.
8123 
8124         Returns a new `Dataset` named after the dimension with the values of
8125         the coordinate labels along that dimension corresponding to minimum
8126         values along that dimension.
8127 
8128         In comparison to :py:meth:`~Dataset.argmin`, this returns the
8129         coordinate label while :py:meth:`~Dataset.argmin` returns the index.
8130 
8131         Parameters
8132         ----------
8133         dim : Hashable, optional
8134             Dimension over which to apply `idxmin`.  This is optional for 1D
8135             variables, but required for variables with 2 or more dimensions.
8136         skipna : bool or None, optional
8137             If True, skip missing values (as marked by NaN). By default, only
8138             skips missing values for ``float``, ``complex``, and ``object``
8139             dtypes; other dtypes either do not have a sentinel missing value
8140             (``int``) or ``skipna=True`` has not been implemented
8141             (``datetime64`` or ``timedelta64``).
8142         fill_value : Any, default: NaN
8143             Value to be filled in case all of the values along a dimension are
8144             null.  By default this is NaN.  The fill value and result are
8145             automatically converted to a compatible dtype if possible.
8146             Ignored if ``skipna`` is False.
8147         keep_attrs : bool or None, optional
8148             If True, the attributes (``attrs``) will be copied from the
8149             original object to the new one. If False, the new object
8150             will be returned without attributes.
8151 
8152         Returns
8153         -------
8154         reduced : Dataset
8155             New `Dataset` object with `idxmin` applied to its data and the
8156             indicated dimension removed.
8157 
8158         See Also
8159         --------
8160         DataArray.idxmin, Dataset.idxmax, Dataset.min, Dataset.argmin
8161 
8162         Examples
8163         --------
8164         >>> array1 = xr.DataArray(
8165         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
8166         ... )
8167         >>> array2 = xr.DataArray(
8168         ...     [
8169         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
8170         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
8171         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
8172         ...     ],
8173         ...     dims=["y", "x"],
8174         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
8175         ... )
8176         >>> ds = xr.Dataset({"int": array1, "float": array2})
8177         >>> ds.min(dim="x")
8178         <xarray.Dataset>
8179         Dimensions:  (y: 3)
8180         Coordinates:
8181           * y        (y) int64 -1 0 1
8182         Data variables:
8183             int      int64 -2
8184             float    (y) float64 -2.0 -4.0 1.0
8185         >>> ds.argmin(dim="x")
8186         <xarray.Dataset>
8187         Dimensions:  (y: 3)
8188         Coordinates:
8189           * y        (y) int64 -1 0 1
8190         Data variables:
8191             int      int64 4
8192             float    (y) int64 4 0 2
8193         >>> ds.idxmin(dim="x")
8194         <xarray.Dataset>
8195         Dimensions:  (y: 3)
8196         Coordinates:
8197           * y        (y) int64 -1 0 1
8198         Data variables:
8199             int      <U1 'e'
8200             float    (y) object 'e' 'a' 'c'
8201         """
8202         return self.map(
8203             methodcaller(
8204                 "idxmin",
8205                 dim=dim,
8206                 skipna=skipna,
8207                 fill_value=fill_value,
8208                 keep_attrs=keep_attrs,
8209             )
8210         )
8211 
8212     def idxmax(
8213         self: T_Dataset,
8214         dim: Hashable | None = None,
8215         skipna: bool | None = None,
8216         fill_value: Any = xrdtypes.NA,
8217         keep_attrs: bool | None = None,
8218     ) -> T_Dataset:
8219         """Return the coordinate label of the maximum value along a dimension.
8220 
8221         Returns a new `Dataset` named after the dimension with the values of
8222         the coordinate labels along that dimension corresponding to maximum
8223         values along that dimension.
8224 
8225         In comparison to :py:meth:`~Dataset.argmax`, this returns the
8226         coordinate label while :py:meth:`~Dataset.argmax` returns the index.
8227 
8228         Parameters
8229         ----------
8230         dim : str, optional
8231             Dimension over which to apply `idxmax`.  This is optional for 1D
8232             variables, but required for variables with 2 or more dimensions.
8233         skipna : bool or None, optional
8234             If True, skip missing values (as marked by NaN). By default, only
8235             skips missing values for ``float``, ``complex``, and ``object``
8236             dtypes; other dtypes either do not have a sentinel missing value
8237             (``int``) or ``skipna=True`` has not been implemented
8238             (``datetime64`` or ``timedelta64``).
8239         fill_value : Any, default: NaN
8240             Value to be filled in case all of the values along a dimension are
8241             null.  By default this is NaN.  The fill value and result are
8242             automatically converted to a compatible dtype if possible.
8243             Ignored if ``skipna`` is False.
8244         keep_attrs : bool or None, optional
8245             If True, the attributes (``attrs``) will be copied from the
8246             original object to the new one. If False, the new object
8247             will be returned without attributes.
8248 
8249         Returns
8250         -------
8251         reduced : Dataset
8252             New `Dataset` object with `idxmax` applied to its data and the
8253             indicated dimension removed.
8254 
8255         See Also
8256         --------
8257         DataArray.idxmax, Dataset.idxmin, Dataset.max, Dataset.argmax
8258 
8259         Examples
8260         --------
8261         >>> array1 = xr.DataArray(
8262         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
8263         ... )
8264         >>> array2 = xr.DataArray(
8265         ...     [
8266         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
8267         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
8268         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
8269         ...     ],
8270         ...     dims=["y", "x"],
8271         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
8272         ... )
8273         >>> ds = xr.Dataset({"int": array1, "float": array2})
8274         >>> ds.max(dim="x")
8275         <xarray.Dataset>
8276         Dimensions:  (y: 3)
8277         Coordinates:
8278           * y        (y) int64 -1 0 1
8279         Data variables:
8280             int      int64 2
8281             float    (y) float64 2.0 2.0 1.0
8282         >>> ds.argmax(dim="x")
8283         <xarray.Dataset>
8284         Dimensions:  (y: 3)
8285         Coordinates:
8286           * y        (y) int64 -1 0 1
8287         Data variables:
8288             int      int64 1
8289             float    (y) int64 0 2 2
8290         >>> ds.idxmax(dim="x")
8291         <xarray.Dataset>
8292         Dimensions:  (y: 3)
8293         Coordinates:
8294           * y        (y) int64 -1 0 1
8295         Data variables:
8296             int      <U1 'b'
8297             float    (y) object 'a' 'c' 'c'
8298         """
8299         return self.map(
8300             methodcaller(
8301                 "idxmax",
8302                 dim=dim,
8303                 skipna=skipna,
8304                 fill_value=fill_value,
8305                 keep_attrs=keep_attrs,
8306             )
8307         )
8308 
8309     def argmin(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
8310         """Indices of the minima of the member variables.
8311 
8312         If there are multiple minima, the indices of the first one found will be
8313         returned.
8314 
8315         Parameters
8316         ----------
8317         dim : Hashable, optional
8318             The dimension over which to find the minimum. By default, finds minimum over
8319             all dimensions - for now returning an int for backward compatibility, but
8320             this is deprecated, in future will be an error, since DataArray.argmin will
8321             return a dict with indices for all dimensions, which does not make sense for
8322             a Dataset.
8323         keep_attrs : bool, optional
8324             If True, the attributes (`attrs`) will be copied from the original
8325             object to the new one.  If False (default), the new object will be
8326             returned without attributes.
8327         skipna : bool, optional
8328             If True, skip missing values (as marked by NaN). By default, only
8329             skips missing values for float dtypes; other dtypes either do not
8330             have a sentinel missing value (int) or skipna=True has not been
8331             implemented (object, datetime64 or timedelta64).
8332 
8333         Returns
8334         -------
8335         result : Dataset
8336 
8337         See Also
8338         --------
8339         DataArray.argmin
8340         """
8341         if dim is None:
8342             warnings.warn(
8343                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8344                 "dim changes to return a dict of indices of each dimension, for "
8345                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8346                 "since we don't return a dict of Datasets.",
8347                 DeprecationWarning,
8348                 stacklevel=2,
8349             )
8350         if (
8351             dim is None
8352             or (not isinstance(dim, Sequence) and dim is not ...)
8353             or isinstance(dim, str)
8354         ):
8355             # Return int index if single dimension is passed, and is not part of a
8356             # sequence
8357             argmin_func = getattr(duck_array_ops, "argmin")
8358             return self.reduce(
8359                 argmin_func, dim=None if dim is None else [dim], **kwargs
8360             )
8361         else:
8362             raise ValueError(
8363                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8364                 "dicts cannot be contained in a Dataset, so cannot call "
8365                 "Dataset.argmin() with a sequence or ... for dim"
8366             )
8367 
8368     def argmax(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
8369         """Indices of the maxima of the member variables.
8370 
8371         If there are multiple maxima, the indices of the first one found will be
8372         returned.
8373 
8374         Parameters
8375         ----------
8376         dim : str, optional
8377             The dimension over which to find the maximum. By default, finds maximum over
8378             all dimensions - for now returning an int for backward compatibility, but
8379             this is deprecated, in future will be an error, since DataArray.argmax will
8380             return a dict with indices for all dimensions, which does not make sense for
8381             a Dataset.
8382         keep_attrs : bool, optional
8383             If True, the attributes (`attrs`) will be copied from the original
8384             object to the new one.  If False (default), the new object will be
8385             returned without attributes.
8386         skipna : bool, optional
8387             If True, skip missing values (as marked by NaN). By default, only
8388             skips missing values for float dtypes; other dtypes either do not
8389             have a sentinel missing value (int) or skipna=True has not been
8390             implemented (object, datetime64 or timedelta64).
8391 
8392         Returns
8393         -------
8394         result : Dataset
8395 
8396         See Also
8397         --------
8398         DataArray.argmax
8399 
8400         """
8401         if dim is None:
8402             warnings.warn(
8403                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8404                 "dim changes to return a dict of indices of each dimension, for "
8405                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8406                 "since we don't return a dict of Datasets.",
8407                 DeprecationWarning,
8408                 stacklevel=2,
8409             )
8410         if (
8411             dim is None
8412             or (not isinstance(dim, Sequence) and dim is not ...)
8413             or isinstance(dim, str)
8414         ):
8415             # Return int index if single dimension is passed, and is not part of a
8416             # sequence
8417             argmax_func = getattr(duck_array_ops, "argmax")
8418             return self.reduce(
8419                 argmax_func, dim=None if dim is None else [dim], **kwargs
8420             )
8421         else:
8422             raise ValueError(
8423                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8424                 "dicts cannot be contained in a Dataset, so cannot call "
8425                 "Dataset.argmin() with a sequence or ... for dim"
8426             )
8427 
8428     def query(
8429         self: T_Dataset,
8430         queries: Mapping[Any, Any] | None = None,
8431         parser: QueryParserOptions = "pandas",
8432         engine: QueryEngineOptions = None,
8433         missing_dims: ErrorOptionsWithWarn = "raise",
8434         **queries_kwargs: Any,
8435     ) -> T_Dataset:
8436         """Return a new dataset with each array indexed along the specified
8437         dimension(s), where the indexers are given as strings containing
8438         Python expressions to be evaluated against the data variables in the
8439         dataset.
8440 
8441         Parameters
8442         ----------
8443         queries : dict-like, optional
8444             A dict-like with keys matching dimensions and values given by strings
8445             containing Python expressions to be evaluated against the data variables
8446             in the dataset. The expressions will be evaluated using the pandas
8447             eval() function, and can contain any valid Python expressions but cannot
8448             contain any Python statements.
8449         parser : {"pandas", "python"}, default: "pandas"
8450             The parser to use to construct the syntax tree from the expression.
8451             The default of 'pandas' parses code slightly different than standard
8452             Python. Alternatively, you can parse an expression using the 'python'
8453             parser to retain strict Python semantics.
8454         engine : {"python", "numexpr", None}, default: None
8455             The engine used to evaluate the expression. Supported engines are:
8456 
8457             - None: tries to use numexpr, falls back to python
8458             - "numexpr": evaluates expressions using numexpr
8459             - "python": performs operations as if you had eval’d in top level python
8460 
8461         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
8462             What to do if dimensions that should be selected from are not present in the
8463             Dataset:
8464 
8465             - "raise": raise an exception
8466             - "warn": raise a warning, and ignore the missing dimensions
8467             - "ignore": ignore the missing dimensions
8468 
8469         **queries_kwargs : {dim: query, ...}, optional
8470             The keyword arguments form of ``queries``.
8471             One of queries or queries_kwargs must be provided.
8472 
8473         Returns
8474         -------
8475         obj : Dataset
8476             A new Dataset with the same contents as this dataset, except each
8477             array and dimension is indexed by the results of the appropriate
8478             queries.
8479 
8480         See Also
8481         --------
8482         Dataset.isel
8483         pandas.eval
8484 
8485         Examples
8486         --------
8487         >>> a = np.arange(0, 5, 1)
8488         >>> b = np.linspace(0, 1, 5)
8489         >>> ds = xr.Dataset({"a": ("x", a), "b": ("x", b)})
8490         >>> ds
8491         <xarray.Dataset>
8492         Dimensions:  (x: 5)
8493         Dimensions without coordinates: x
8494         Data variables:
8495             a        (x) int64 0 1 2 3 4
8496             b        (x) float64 0.0 0.25 0.5 0.75 1.0
8497         >>> ds.query(x="a > 2")
8498         <xarray.Dataset>
8499         Dimensions:  (x: 2)
8500         Dimensions without coordinates: x
8501         Data variables:
8502             a        (x) int64 3 4
8503             b        (x) float64 0.75 1.0
8504         """
8505 
8506         # allow queries to be given either as a dict or as kwargs
8507         queries = either_dict_or_kwargs(queries, queries_kwargs, "query")
8508 
8509         # check queries
8510         for dim, expr in queries.items():
8511             if not isinstance(expr, str):
8512                 msg = f"expr for dim {dim} must be a string to be evaluated, {type(expr)} given"
8513                 raise ValueError(msg)
8514 
8515         # evaluate the queries to create the indexers
8516         indexers = {
8517             dim: pd.eval(expr, resolvers=[self], parser=parser, engine=engine)
8518             for dim, expr in queries.items()
8519         }
8520 
8521         # apply the selection
8522         return self.isel(indexers, missing_dims=missing_dims)
8523 
8524     def curvefit(
8525         self: T_Dataset,
8526         coords: str | DataArray | Iterable[str | DataArray],
8527         func: Callable[..., Any],
8528         reduce_dims: Dims = None,
8529         skipna: bool = True,
8530         p0: dict[str, Any] | None = None,
8531         bounds: dict[str, Any] | None = None,
8532         param_names: Sequence[str] | None = None,
8533         kwargs: dict[str, Any] | None = None,
8534     ) -> T_Dataset:
8535         """
8536         Curve fitting optimization for arbitrary functions.
8537 
8538         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
8539 
8540         Parameters
8541         ----------
8542         coords : hashable, DataArray, or sequence of hashable or DataArray
8543             Independent coordinate(s) over which to perform the curve fitting. Must share
8544             at least one dimension with the calling object. When fitting multi-dimensional
8545             functions, supply `coords` as a sequence in the same order as arguments in
8546             `func`. To fit along existing dimensions of the calling object, `coords` can
8547             also be specified as a str or sequence of strs.
8548         func : callable
8549             User specified function in the form `f(x, *params)` which returns a numpy
8550             array of length `len(x)`. `params` are the fittable parameters which are optimized
8551             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
8552             coordinates, e.g. `f((x0, x1), *params)`.
8553         reduce_dims : str, Iterable of Hashable or None, optional
8554             Additional dimension(s) over which to aggregate while fitting. For example,
8555             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
8556             aggregate all lat and lon points and fit the specified function along the
8557             time dimension.
8558         skipna : bool, default: True
8559             Whether to skip missing values when fitting. Default is True.
8560         p0 : dict-like, optional
8561             Optional dictionary of parameter names to initial guesses passed to the
8562             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
8563             be assigned initial values following the default scipy behavior.
8564         bounds : dict-like, optional
8565             Optional dictionary of parameter names to bounding values passed to the
8566             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
8567             will be unbounded following the default scipy behavior.
8568         param_names : sequence of hashable, optional
8569             Sequence of names for the fittable parameters of `func`. If not supplied,
8570             this will be automatically determined by arguments of `func`. `param_names`
8571             should be manually supplied when fitting a function that takes a variable
8572             number of parameters.
8573         **kwargs : optional
8574             Additional keyword arguments to passed to scipy curve_fit.
8575 
8576         Returns
8577         -------
8578         curvefit_results : Dataset
8579             A single dataset which contains:
8580 
8581             [var]_curvefit_coefficients
8582                 The coefficients of the best fit.
8583             [var]_curvefit_covariance
8584                 The covariance matrix of the coefficient estimates.
8585 
8586         See Also
8587         --------
8588         Dataset.polyfit
8589         scipy.optimize.curve_fit
8590         """
8591         from scipy.optimize import curve_fit
8592 
8593         from xarray.core.alignment import broadcast
8594         from xarray.core.computation import apply_ufunc
8595         from xarray.core.dataarray import _THIS_ARRAY, DataArray
8596 
8597         if p0 is None:
8598             p0 = {}
8599         if bounds is None:
8600             bounds = {}
8601         if kwargs is None:
8602             kwargs = {}
8603 
8604         reduce_dims_: list[Hashable]
8605         if not reduce_dims:
8606             reduce_dims_ = []
8607         elif isinstance(reduce_dims, str) or not isinstance(reduce_dims, Iterable):
8608             reduce_dims_ = [reduce_dims]
8609         else:
8610             reduce_dims_ = list(reduce_dims)
8611 
8612         if (
8613             isinstance(coords, str)
8614             or isinstance(coords, DataArray)
8615             or not isinstance(coords, Iterable)
8616         ):
8617             coords = [coords]
8618         coords_: Sequence[DataArray] = [
8619             self[coord] if isinstance(coord, str) else coord for coord in coords
8620         ]
8621 
8622         # Determine whether any coords are dims on self
8623         for coord in coords_:
8624             reduce_dims_ += [c for c in self.dims if coord.equals(self[c])]
8625         reduce_dims_ = list(set(reduce_dims_))
8626         preserved_dims = list(set(self.dims) - set(reduce_dims_))
8627         if not reduce_dims_:
8628             raise ValueError(
8629                 "No arguments to `coords` were identified as a dimension on the calling "
8630                 "object, and no dims were supplied to `reduce_dims`. This would result "
8631                 "in fitting on scalar data."
8632             )
8633 
8634         # Broadcast all coords with each other
8635         coords_ = broadcast(*coords_)
8636         coords_ = [
8637             coord.broadcast_like(self, exclude=preserved_dims) for coord in coords_
8638         ]
8639 
8640         params, func_args = _get_func_args(func, param_names)
8641         param_defaults, bounds_defaults = _initialize_curvefit_params(
8642             params, p0, bounds, func_args
8643         )
8644         n_params = len(params)
8645         kwargs.setdefault("p0", [param_defaults[p] for p in params])
8646         kwargs.setdefault(
8647             "bounds",
8648             [
8649                 [bounds_defaults[p][0] for p in params],
8650                 [bounds_defaults[p][1] for p in params],
8651             ],
8652         )
8653 
8654         def _wrapper(Y, *coords_, **kwargs):
8655             # Wrap curve_fit with raveled coordinates and pointwise NaN handling
8656             x = np.vstack([c.ravel() for c in coords_])
8657             y = Y.ravel()
8658             if skipna:
8659                 mask = np.all([np.any(~np.isnan(x), axis=0), ~np.isnan(y)], axis=0)
8660                 x = x[:, mask]
8661                 y = y[mask]
8662                 if not len(y):
8663                     popt = np.full([n_params], np.nan)
8664                     pcov = np.full([n_params, n_params], np.nan)
8665                     return popt, pcov
8666             x = np.squeeze(x)
8667             popt, pcov = curve_fit(func, x, y, **kwargs)
8668             return popt, pcov
8669 
8670         result = type(self)()
8671         for name, da in self.data_vars.items():
8672             if name is _THIS_ARRAY:
8673                 name = ""
8674             else:
8675                 name = f"{str(name)}_"
8676 
8677             popt, pcov = apply_ufunc(
8678                 _wrapper,
8679                 da,
8680                 *coords_,
8681                 vectorize=True,
8682                 dask="parallelized",
8683                 input_core_dims=[reduce_dims_ for d in range(len(coords_) + 1)],
8684                 output_core_dims=[["param"], ["cov_i", "cov_j"]],
8685                 dask_gufunc_kwargs={
8686                     "output_sizes": {
8687                         "param": n_params,
8688                         "cov_i": n_params,
8689                         "cov_j": n_params,
8690                     },
8691                 },
8692                 output_dtypes=(np.float64, np.float64),
8693                 exclude_dims=set(reduce_dims_),
8694                 kwargs=kwargs,
8695             )
8696             result[name + "curvefit_coefficients"] = popt
8697             result[name + "curvefit_covariance"] = pcov
8698 
8699         result = result.assign_coords(
8700             {"param": params, "cov_i": params, "cov_j": params}
8701         )
8702         result.attrs = self.attrs.copy()
8703 
8704         return result
8705 
8706     def drop_duplicates(
8707         self: T_Dataset,
8708         dim: Hashable | Iterable[Hashable],
8709         keep: Literal["first", "last", False] = "first",
8710     ) -> T_Dataset:
8711         """Returns a new Dataset with duplicate dimension values removed.
8712 
8713         Parameters
8714         ----------
8715         dim : dimension label or labels
8716             Pass `...` to drop duplicates along all dimensions.
8717         keep : {"first", "last", False}, default: "first"
8718             Determines which duplicates (if any) to keep.
8719             - ``"first"`` : Drop duplicates except for the first occurrence.
8720             - ``"last"`` : Drop duplicates except for the last occurrence.
8721             - False : Drop all duplicates.
8722 
8723         Returns
8724         -------
8725         Dataset
8726 
8727         See Also
8728         --------
8729         DataArray.drop_duplicates
8730         """
8731         if isinstance(dim, str):
8732             dims: Iterable = (dim,)
8733         elif dim is ...:
8734             dims = self.dims
8735         elif not isinstance(dim, Iterable):
8736             dims = [dim]
8737         else:
8738             dims = dim
8739 
8740         missing_dims = set(dims) - set(self.dims)
8741         if missing_dims:
8742             raise ValueError(f"'{missing_dims}' not found in dimensions")
8743 
8744         indexes = {dim: ~self.get_index(dim).duplicated(keep=keep) for dim in dims}
8745         return self.isel(indexes)
8746 
8747     def convert_calendar(
8748         self: T_Dataset,
8749         calendar: CFCalendar,
8750         dim: Hashable = "time",
8751         align_on: Literal["date", "year", None] = None,
8752         missing: Any | None = None,
8753         use_cftime: bool | None = None,
8754     ) -> T_Dataset:
8755         """Convert the Dataset to another calendar.
8756 
8757         Only converts the individual timestamps, does not modify any data except
8758         in dropping invalid/surplus dates or inserting missing dates.
8759 
8760         If the source and target calendars are either no_leap, all_leap or a
8761         standard type, only the type of the time array is modified.
8762         When converting to a leap year from a non-leap year, the 29th of February
8763         is removed from the array. In the other direction the 29th of February
8764         will be missing in the output, unless `missing` is specified,
8765         in which case that value is inserted.
8766 
8767         For conversions involving `360_day` calendars, see Notes.
8768 
8769         This method is safe to use with sub-daily data as it doesn't touch the
8770         time part of the timestamps.
8771 
8772         Parameters
8773         ---------
8774         calendar : str
8775             The target calendar name.
8776         dim : Hashable, default: "time"
8777             Name of the time coordinate.
8778         align_on : {None, 'date', 'year'}, optional
8779             Must be specified when either source or target is a `360_day` calendar,
8780             ignored otherwise. See Notes.
8781         missing : Any or None, optional
8782             By default, i.e. if the value is None, this method will simply attempt
8783             to convert the dates in the source calendar to the same dates in the
8784             target calendar, and drop any of those that are not possible to
8785             represent.  If a value is provided, a new time coordinate will be
8786             created in the target calendar with the same frequency as the original
8787             time coordinate; for any dates that are not present in the source, the
8788             data will be filled with this value.  Note that using this mode requires
8789             that the source data have an inferable frequency; for more information
8790             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
8791             target calendar combinations, this could result in many missing values, see notes.
8792         use_cftime : bool or None, optional
8793             Whether to use cftime objects in the output, only used if `calendar`
8794             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
8795             If True, the new time axis uses cftime objects.
8796             If None (default), it uses :py:class:`numpy.datetime64` values if the
8797             date range permits it, and :py:class:`cftime.datetime` objects if not.
8798             If False, it uses :py:class:`numpy.datetime64`  or fails.
8799 
8800         Returns
8801         -------
8802         Dataset
8803             Copy of the dataarray with the time coordinate converted to the
8804             target calendar. If 'missing' was None (default), invalid dates in
8805             the new calendar are dropped, but missing dates are not inserted.
8806             If `missing` was given, the new data is reindexed to have a time axis
8807             with the same frequency as the source, but in the new calendar; any
8808             missing datapoints are filled with `missing`.
8809 
8810         Notes
8811         -----
8812         Passing a value to `missing` is only usable if the source's time coordinate as an
8813         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
8814         if the target coordinate, generated from this frequency, has dates equivalent to the
8815         source. It is usually **not** appropriate to use this mode with:
8816 
8817         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
8818         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
8819             or 'mH' where 24 % m != 0).
8820 
8821         If one of the source or target calendars is `"360_day"`, `align_on` must
8822         be specified and two options are offered.
8823 
8824         - "year"
8825             The dates are translated according to their relative position in the year,
8826             ignoring their original month and day information, meaning that the
8827             missing/surplus days are added/removed at regular intervals.
8828 
8829             From a `360_day` to a standard calendar, the output will be missing the
8830             following dates (day of year in parentheses):
8831 
8832             To a leap year:
8833                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
8834                 September 31st (275) and November 30th (335).
8835             To a non-leap year:
8836                 February 6th (36), April 19th (109), July 2nd (183),
8837                 September 12th (255), November 25th (329).
8838 
8839             From a standard calendar to a `"360_day"`, the following dates in the
8840             source array will be dropped:
8841 
8842             From a leap year:
8843                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
8844                 September 31st (275), December 1st (336)
8845             From a non-leap year:
8846                 February 6th (37), April 20th (110), July 2nd (183),
8847                 September 13th (256), November 25th (329)
8848 
8849             This option is best used on daily and subdaily data.
8850 
8851         - "date"
8852             The month/day information is conserved and invalid dates are dropped
8853             from the output. This means that when converting from a `"360_day"` to a
8854             standard calendar, all 31st (Jan, March, May, July, August, October and
8855             December) will be missing as there is no equivalent dates in the
8856             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
8857             will be dropped as there are no equivalent dates in a standard calendar.
8858 
8859             This option is best used with data on a frequency coarser than daily.
8860         """
8861         return convert_calendar(
8862             self,
8863             calendar,
8864             dim=dim,
8865             align_on=align_on,
8866             missing=missing,
8867             use_cftime=use_cftime,
8868         )
8869 
8870     def interp_calendar(
8871         self: T_Dataset,
8872         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
8873         dim: Hashable = "time",
8874     ) -> T_Dataset:
8875         """Interpolates the Dataset to another calendar based on decimal year measure.
8876 
8877         Each timestamp in `source` and `target` are first converted to their decimal
8878         year equivalent then `source` is interpolated on the target coordinate.
8879         The decimal year of a timestamp is its year plus its sub-year component
8880         converted to the fraction of its year. For example "2000-03-01 12:00" is
8881         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
8882 
8883         This method should only be used when the time (HH:MM:SS) information of
8884         time coordinate is not important.
8885 
8886         Parameters
8887         ----------
8888         target: DataArray or DatetimeIndex or CFTimeIndex
8889             The target time coordinate of a valid dtype
8890             (np.datetime64 or cftime objects)
8891         dim : Hashable, default: "time"
8892             The time coordinate name.
8893 
8894         Return
8895         ------
8896         DataArray
8897             The source interpolated on the decimal years of target,
8898         """
8899         return interp_calendar(self, target, dim=dim)
8900 
8901     def groupby(
8902         self,
8903         group: Hashable | DataArray | IndexVariable,
8904         squeeze: bool = True,
8905         restore_coord_dims: bool = False,
8906     ) -> DatasetGroupBy:
8907         """Returns a DatasetGroupBy object for performing grouped operations.
8908 
8909         Parameters
8910         ----------
8911         group : Hashable, DataArray or IndexVariable
8912             Array whose unique values should be used to group this array. If a
8913             string, must be the name of a variable contained in this dataset.
8914         squeeze : bool, default: True
8915             If "group" is a dimension of any arrays in this dataset, `squeeze`
8916             controls whether the subarrays have a dimension of length 1 along
8917             that dimension or if the dimension is squeezed out.
8918         restore_coord_dims : bool, default: False
8919             If True, also restore the dimension order of multi-dimensional
8920             coordinates.
8921 
8922         Returns
8923         -------
8924         grouped : DatasetGroupBy
8925             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
8926             iterated over in the form of `(unique_value, grouped_array)` pairs.
8927 
8928         See Also
8929         --------
8930         Dataset.groupby_bins
8931         DataArray.groupby
8932         core.groupby.DatasetGroupBy
8933         pandas.DataFrame.groupby
8934         """
8935         from xarray.core.groupby import DatasetGroupBy
8936 
8937         # While we don't generally check the type of every arg, passing
8938         # multiple dimensions as multiple arguments is common enough, and the
8939         # consequences hidden enough (strings evaluate as true) to warrant
8940         # checking here.
8941         # A future version could make squeeze kwarg only, but would face
8942         # backward-compat issues.
8943         if not isinstance(squeeze, bool):
8944             raise TypeError(
8945                 f"`squeeze` must be True or False, but {squeeze} was supplied"
8946             )
8947 
8948         return DatasetGroupBy(
8949             self, group, squeeze=squeeze, restore_coord_dims=restore_coord_dims
8950         )
8951 
8952     def groupby_bins(
8953         self,
8954         group: Hashable | DataArray | IndexVariable,
8955         bins: ArrayLike,
8956         right: bool = True,
8957         labels: ArrayLike | None = None,
8958         precision: int = 3,
8959         include_lowest: bool = False,
8960         squeeze: bool = True,
8961         restore_coord_dims: bool = False,
8962     ) -> DatasetGroupBy:
8963         """Returns a DatasetGroupBy object for performing grouped operations.
8964 
8965         Rather than using all unique values of `group`, the values are discretized
8966         first by applying `pandas.cut` [1]_ to `group`.
8967 
8968         Parameters
8969         ----------
8970         group : Hashable, DataArray or IndexVariable
8971             Array whose binned values should be used to group this array. If a
8972             string, must be the name of a variable contained in this dataset.
8973         bins : int or array-like
8974             If bins is an int, it defines the number of equal-width bins in the
8975             range of x. However, in this case, the range of x is extended by .1%
8976             on each side to include the min or max values of x. If bins is a
8977             sequence it defines the bin edges allowing for non-uniform bin
8978             width. No extension of the range of x is done in this case.
8979         right : bool, default: True
8980             Indicates whether the bins include the rightmost edge or not. If
8981             right == True (the default), then the bins [1,2,3,4] indicate
8982             (1,2], (2,3], (3,4].
8983         labels : array-like or bool, default: None
8984             Used as labels for the resulting bins. Must be of the same length as
8985             the resulting bins. If False, string bin labels are assigned by
8986             `pandas.cut`.
8987         precision : int, default: 3
8988             The precision at which to store and display the bins labels.
8989         include_lowest : bool, default: False
8990             Whether the first interval should be left-inclusive or not.
8991         squeeze : bool, default: True
8992             If "group" is a dimension of any arrays in this dataset, `squeeze`
8993             controls whether the subarrays have a dimension of length 1 along
8994             that dimension or if the dimension is squeezed out.
8995         restore_coord_dims : bool, default: False
8996             If True, also restore the dimension order of multi-dimensional
8997             coordinates.
8998 
8999         Returns
9000         -------
9001         grouped : DatasetGroupBy
9002             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
9003             iterated over in the form of `(unique_value, grouped_array)` pairs.
9004             The name of the group has the added suffix `_bins` in order to
9005             distinguish it from the original variable.
9006 
9007         See Also
9008         --------
9009         Dataset.groupby
9010         DataArray.groupby_bins
9011         core.groupby.DatasetGroupBy
9012         pandas.DataFrame.groupby
9013 
9014         References
9015         ----------
9016         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
9017         """
9018         from xarray.core.groupby import DatasetGroupBy
9019 
9020         return DatasetGroupBy(
9021             self,
9022             group,
9023             squeeze=squeeze,
9024             bins=bins,
9025             restore_coord_dims=restore_coord_dims,
9026             cut_kwargs={
9027                 "right": right,
9028                 "labels": labels,
9029                 "precision": precision,
9030                 "include_lowest": include_lowest,
9031             },
9032         )
9033 
9034     def weighted(self, weights: DataArray) -> DatasetWeighted:
9035         """
9036         Weighted Dataset operations.
9037 
9038         Parameters
9039         ----------
9040         weights : DataArray
9041             An array of weights associated with the values in this Dataset.
9042             Each value in the data contributes to the reduction operation
9043             according to its associated weight.
9044 
9045         Notes
9046         -----
9047         ``weights`` must be a DataArray and cannot contain missing values.
9048         Missing values can be replaced by ``weights.fillna(0)``.
9049 
9050         Returns
9051         -------
9052         core.weighted.DatasetWeighted
9053 
9054         See Also
9055         --------
9056         DataArray.weighted
9057         """
9058         from xarray.core.weighted import DatasetWeighted
9059 
9060         return DatasetWeighted(self, weights)
9061 
9062     def rolling(
9063         self,
9064         dim: Mapping[Any, int] | None = None,
9065         min_periods: int | None = None,
9066         center: bool | Mapping[Any, bool] = False,
9067         **window_kwargs: int,
9068     ) -> DatasetRolling:
9069         """
9070         Rolling window object for Datasets.
9071 
9072         Parameters
9073         ----------
9074         dim : dict, optional
9075             Mapping from the dimension name to create the rolling iterator
9076             along (e.g. `time`) to its moving window size.
9077         min_periods : int or None, default: None
9078             Minimum number of observations in window required to have a value
9079             (otherwise result is NA). The default, None, is equivalent to
9080             setting min_periods equal to the size of the window.
9081         center : bool or Mapping to int, default: False
9082             Set the labels at the center of the window.
9083         **window_kwargs : optional
9084             The keyword arguments form of ``dim``.
9085             One of dim or window_kwargs must be provided.
9086 
9087         Returns
9088         -------
9089         core.rolling.DatasetRolling
9090 
9091         See Also
9092         --------
9093         core.rolling.DatasetRolling
9094         DataArray.rolling
9095         """
9096         from xarray.core.rolling import DatasetRolling
9097 
9098         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
9099         return DatasetRolling(self, dim, min_periods=min_periods, center=center)
9100 
9101     def coarsen(
9102         self,
9103         dim: Mapping[Any, int] | None = None,
9104         boundary: CoarsenBoundaryOptions = "exact",
9105         side: SideOptions | Mapping[Any, SideOptions] = "left",
9106         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
9107         **window_kwargs: int,
9108     ) -> DatasetCoarsen:
9109         """
9110         Coarsen object for Datasets.
9111 
9112         Parameters
9113         ----------
9114         dim : mapping of hashable to int, optional
9115             Mapping from the dimension name to the window size.
9116         boundary : {"exact", "trim", "pad"}, default: "exact"
9117             If 'exact', a ValueError will be raised if dimension size is not a
9118             multiple of the window size. If 'trim', the excess entries are
9119             dropped. If 'pad', NA will be padded.
9120         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
9121         coord_func : str or mapping of hashable to str, default: "mean"
9122             function (name) that is applied to the coordinates,
9123             or a mapping from coordinate name to function (name).
9124 
9125         Returns
9126         -------
9127         core.rolling.DatasetCoarsen
9128 
9129         See Also
9130         --------
9131         core.rolling.DatasetCoarsen
9132         DataArray.coarsen
9133         """
9134         from xarray.core.rolling import DatasetCoarsen
9135 
9136         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
9137         return DatasetCoarsen(
9138             self,
9139             dim,
9140             boundary=boundary,
9141             side=side,
9142             coord_func=coord_func,
9143         )
9144 
9145     def resample(
9146         self,
9147         indexer: Mapping[Any, str] | None = None,
9148         skipna: bool | None = None,
9149         closed: SideOptions | None = None,
9150         label: SideOptions | None = None,
9151         base: int | None = None,
9152         offset: pd.Timedelta | datetime.timedelta | str | None = None,
9153         origin: str | DatetimeLike = "start_day",
9154         keep_attrs: bool | None = None,
9155         loffset: datetime.timedelta | str | None = None,
9156         restore_coord_dims: bool | None = None,
9157         **indexer_kwargs: str,
9158     ) -> DatasetResample:
9159         """Returns a Resample object for performing resampling operations.
9160 
9161         Handles both downsampling and upsampling. The resampled
9162         dimension must be a datetime-like coordinate. If any intervals
9163         contain no values from the original object, they will be given
9164         the value ``NaN``.
9165 
9166         Parameters
9167         ----------
9168         indexer : Mapping of Hashable to str, optional
9169             Mapping from the dimension name to resample frequency [1]_. The
9170             dimension must be datetime-like.
9171         skipna : bool, optional
9172             Whether to skip missing values when aggregating in downsampling.
9173         closed : {"left", "right"}, optional
9174             Side of each interval to treat as closed.
9175         label : {"left", "right"}, optional
9176             Side of each interval to use for labeling.
9177         base : int, optional
9178             For frequencies that evenly subdivide 1 day, the "origin" of the
9179             aggregated intervals. For example, for "24H" frequency, base could
9180             range from 0 through 23.
9181         origin : {'epoch', 'start', 'start_day', 'end', 'end_day'}, pd.Timestamp, datetime.datetime, np.datetime64, or cftime.datetime, default 'start_day'
9182             The datetime on which to adjust the grouping. The timezone of origin
9183             must match the timezone of the index.
9184 
9185             If a datetime is not used, these values are also supported:
9186             - 'epoch': `origin` is 1970-01-01
9187             - 'start': `origin` is the first value of the timeseries
9188             - 'start_day': `origin` is the first day at midnight of the timeseries
9189             - 'end': `origin` is the last value of the timeseries
9190             - 'end_day': `origin` is the ceiling midnight of the last day
9191         offset : pd.Timedelta, datetime.timedelta, or str, default is None
9192             An offset timedelta added to the origin.
9193         loffset : timedelta or str, optional
9194             Offset used to adjust the resampled time labels. Some pandas date
9195             offset strings are supported.
9196         restore_coord_dims : bool, optional
9197             If True, also restore the dimension order of multi-dimensional
9198             coordinates.
9199         **indexer_kwargs : str
9200             The keyword arguments form of ``indexer``.
9201             One of indexer or indexer_kwargs must be provided.
9202 
9203         Returns
9204         -------
9205         resampled : core.resample.DataArrayResample
9206             This object resampled.
9207 
9208         See Also
9209         --------
9210         DataArray.resample
9211         pandas.Series.resample
9212         pandas.DataFrame.resample
9213 
9214         References
9215         ----------
9216         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
9217         """
9218         from xarray.core.resample import DatasetResample
9219 
9220         return self._resample(
9221             resample_cls=DatasetResample,
9222             indexer=indexer,
9223             skipna=skipna,
9224             closed=closed,
9225             label=label,
9226             base=base,
9227             offset=offset,
9228             origin=origin,
9229             keep_attrs=keep_attrs,
9230             loffset=loffset,
9231             restore_coord_dims=restore_coord_dims,
9232             **indexer_kwargs,
9233         )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>stack</method>
<code>
2683     def stack(
2684         self: T_DataArray,
2685         dimensions: Mapping[Any, Sequence[Hashable]] | None = None,
2686         create_index: bool | None = True,
2687         index_cls: type[Index] = PandasMultiIndex,
2688         **dimensions_kwargs: Sequence[Hashable],
2689     ) -> T_DataArray:
2690         """
2691         Stack any number of existing dimensions into a single new dimension.
2692 
2693         New dimensions will be added at the end, and the corresponding
2694         coordinate variables will be combined into a MultiIndex.
2695 
2696         Parameters
2697         ----------
2698         dimensions : mapping of Hashable to sequence of Hashable
2699             Mapping of the form `new_name=(dim1, dim2, ...)`.
2700             Names of new dimensions, and the existing dimensions that they
2701             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
2702             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
2703             all dimensions.
2704         create_index : bool or None, default: True
2705             If True, create a multi-index for each of the stacked dimensions.
2706             If False, don't create any index.
2707             If None, create a multi-index only if exactly one single (1-d) coordinate
2708             index is found for every dimension to stack.
2709         index_cls: class, optional
2710             Can be used to pass a custom multi-index type. Must be an Xarray index that
2711             implements `.stack()`. By default, a pandas multi-index wrapper is used.
2712         **dimensions_kwargs
2713             The keyword arguments form of ``dimensions``.
2714             One of dimensions or dimensions_kwargs must be provided.
2715 
2716         Returns
2717         -------
2718         stacked : DataArray
2719             DataArray with stacked data.
2720 
2721         Examples
2722         --------
2723         >>> arr = xr.DataArray(
2724         ...     np.arange(6).reshape(2, 3),
2725         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2726         ... )
2727         >>> arr
2728         <xarray.DataArray (x: 2, y: 3)>
2729         array([[0, 1, 2],
2730                [3, 4, 5]])
2731         Coordinates:
2732           * x        (x) <U1 'a' 'b'
2733           * y        (y) int64 0 1 2
2734         >>> stacked = arr.stack(z=("x", "y"))
2735         >>> stacked.indexes["z"]
2736         MultiIndex([('a', 0),
2737                     ('a', 1),
2738                     ('a', 2),
2739                     ('b', 0),
2740                     ('b', 1),
2741                     ('b', 2)],
2742                    name='z')
2743 
2744         See Also
2745         --------
2746         DataArray.unstack
2747         """
2748         ds = self._to_temp_dataset().stack(
2749             dimensions,
2750             create_index=create_index,
2751             index_cls=index_cls,
2752             **dimensions_kwargs,
2753         )
2754         return self._from_temp_dataset(ds)

</code><intended_behavior>Similarly to `Dataset`, the `stack` method here should make sure that the data type (`dtype`) for any coordinates involved in a stack operation is maintained as it was before the operation. If necessary, additional logic should be included to explicitly force the stacked object to match the original dtype if the operation inherently changes it, potentially using a mechanism to specify or cast dtypes during the stacking process.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
235 class DataArray(
236     AbstractArray,
237     DataWithCoords,
238     DataArrayArithmetic,
239     DataArrayAggregations,
240 ):
241     """N-dimensional array with labeled coordinates and dimensions.
242 
243     DataArray provides a wrapper around numpy ndarrays that uses
244     labeled dimensions and coordinates to support metadata aware
245     operations. The API is similar to that for the pandas Series or
246     DataFrame, but DataArray objects can have any number of dimensions,
247     and their contents have fixed data types.
248 
249     Additional features over raw numpy arrays:
250 
251     - Apply operations over dimensions by name: ``x.sum('time')``.
252     - Select or assign values by integer location (like numpy):
253       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or
254       ``x.sel(time='2014-01-01')``.
255     - Mathematical operations (e.g., ``x - y``) vectorize across
256       multiple dimensions (known in numpy as "broadcasting") based on
257       dimension names, regardless of their original order.
258     - Keep track of arbitrary metadata in the form of a Python
259       dictionary: ``x.attrs``
260     - Convert to a pandas Series: ``x.to_series()``.
261 
262     Getting items from or doing mathematical operations with a
263     DataArray always returns another DataArray.
264 
265     Parameters
266     ----------
267     data : array_like
268         Values for this array. Must be an ``numpy.ndarray``, ndarray
269         like, or castable to an ``ndarray``. If a self-described xarray
270         or pandas object, attempts are made to use this array's
271         metadata to fill in other unspecified arguments. A view of the
272         array's data is used instead of a copy if possible.
273     coords : sequence or dict of array_like, optional
274         Coordinates (tick labels) to use for indexing along each
275         dimension. The following notations are accepted:
276 
277         - mapping {dimension name: array-like}
278         - sequence of tuples that are valid arguments for
279           ``xarray.Variable()``
280           - (dims, data)
281           - (dims, data, attrs)
282           - (dims, data, attrs, encoding)
283 
284         Additionally, it is possible to define a coord whose name
285         does not match the dimension name, or a coord based on multiple
286         dimensions, with one of the following notations:
287 
288         - mapping {coord name: DataArray}
289         - mapping {coord name: Variable}
290         - mapping {coord name: (dimension name, array-like)}
291         - mapping {coord name: (tuple of dimension names, array-like)}
292 
293     dims : Hashable or sequence of Hashable, optional
294         Name(s) of the data dimension(s). Must be either a Hashable
295         (only for 1D data) or a sequence of Hashables with length equal
296         to the number of dimensions. If this argument is omitted,
297         dimension names are taken from ``coords`` (if possible) and
298         otherwise default to ``['dim_0', ... 'dim_n']``.
299     name : str or None, optional
300         Name of this array.
301     attrs : dict_like or None, optional
302         Attributes to assign to the new instance. By default, an empty
303         attribute dictionary is initialized.
304 
305     Examples
306     --------
307     Create data:
308 
309     >>> np.random.seed(0)
310     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
311     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
312     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
313     >>> time = pd.date_range("2014-09-06", periods=3)
314     >>> reference_time = pd.Timestamp("2014-09-05")
315 
316     Initialize a dataarray with multiple dimensions:
317 
318     >>> da = xr.DataArray(
319     ...     data=temperature,
320     ...     dims=["x", "y", "time"],
321     ...     coords=dict(
322     ...         lon=(["x", "y"], lon),
323     ...         lat=(["x", "y"], lat),
324     ...         time=time,
325     ...         reference_time=reference_time,
326     ...     ),
327     ...     attrs=dict(
328     ...         description="Ambient temperature.",
329     ...         units="degC",
330     ...     ),
331     ... )
332     >>> da
333     <xarray.DataArray (x: 2, y: 2, time: 3)>
334     array([[[29.11241877, 18.20125767, 22.82990387],
335             [32.92714559, 29.94046392,  7.18177696]],
336     <BLANKLINE>
337            [[22.60070734, 13.78914233, 14.17424919],
338             [18.28478802, 16.15234857, 26.63418806]]])
339     Coordinates:
340         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
341         lat             (x, y) float64 42.25 42.21 42.63 42.59
342       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
343         reference_time  datetime64[ns] 2014-09-05
344     Dimensions without coordinates: x, y
345     Attributes:
346         description:  Ambient temperature.
347         units:        degC
348 
349     Find out where the coldest temperature was:
350 
351     >>> da.isel(da.argmin(...))
352     <xarray.DataArray ()>
353     array(7.18177696)
354     Coordinates:
355         lon             float64 -99.32
356         lat             float64 42.21
357         time            datetime64[ns] 2014-09-08
358         reference_time  datetime64[ns] 2014-09-05
359     Attributes:
360         description:  Ambient temperature.
361         units:        degC
362     """
363 
364     _cache: dict[str, Any]
365     _coords: dict[Any, Variable]
366     _close: Callable[[], None] | None
367     _indexes: dict[Hashable, Index]
368     _name: Hashable | None
369     _variable: Variable
370 
371     __slots__ = (
372         "_cache",
373         "_coords",
374         "_close",
375         "_indexes",
376         "_name",
377         "_variable",
378         "__weakref__",
379     )
380 
381     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor["DataArray"])
382 
383     def __init__(
384         self,
385         data: Any = dtypes.NA,
386         coords: Sequence[Sequence[Any] | pd.Index | DataArray]
387         | Mapping[Any, Any]
388         | None = None,
389         dims: Hashable | Sequence[Hashable] | None = None,
390         name: Hashable | None = None,
391         attrs: Mapping | None = None,
392         # internal parameters
393         indexes: dict[Hashable, Index] | None = None,
394         fastpath: bool = False,
395     ) -> None:
396         if fastpath:
397             variable = data
398             assert dims is None
399             assert attrs is None
400             assert indexes is not None
401         else:
402             # TODO: (benbovy - explicit indexes) remove
403             # once it becomes part of the public interface
404             if indexes is not None:
405                 raise ValueError("Providing explicit indexes is not supported yet")
406 
407             # try to fill in arguments from data if they weren't supplied
408             if coords is None:
409 
410                 if isinstance(data, DataArray):
411                     coords = data.coords
412                 elif isinstance(data, pd.Series):
413                     coords = [data.index]
414                 elif isinstance(data, pd.DataFrame):
415                     coords = [data.index, data.columns]
416                 elif isinstance(data, (pd.Index, IndexVariable)):
417                     coords = [data]
418 
419             if dims is None:
420                 dims = getattr(data, "dims", getattr(coords, "dims", None))
421             if name is None:
422                 name = getattr(data, "name", None)
423             if attrs is None and not isinstance(data, PANDAS_TYPES):
424                 attrs = getattr(data, "attrs", None)
425 
426             data = _check_data_shape(data, coords, dims)
427             data = as_compatible_data(data)
428             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
429             variable = Variable(dims, data, attrs, fastpath=True)
430             indexes, coords = _create_indexes_from_coords(coords)
431 
432         # These fully describe a DataArray
433         self._variable = variable
434         assert isinstance(coords, dict)
435         self._coords = coords
436         self._name = name
437 
438         # TODO(shoyer): document this argument, once it becomes part of the
439         # public interface.
440         self._indexes = indexes
441 
442         self._close = None
443 
444     @classmethod
445     def _construct_direct(
446         cls: type[T_DataArray],
447         variable: Variable,
448         coords: dict[Any, Variable],
449         name: Hashable,
450         indexes: dict[Hashable, Index],
451     ) -> T_DataArray:
452         """Shortcut around __init__ for internal use when we want to skip
453         costly validation
454         """
455         obj = object.__new__(cls)
456         obj._variable = variable
457         obj._coords = coords
458         obj._name = name
459         obj._indexes = indexes
460         obj._close = None
461         return obj
462 
463     def _replace(
464         self: T_DataArray,
465         variable: Variable | None = None,
466         coords=None,
467         name: Hashable | None | Default = _default,
468         indexes=None,
469     ) -> T_DataArray:
470         if variable is None:
471             variable = self.variable
472         if coords is None:
473             coords = self._coords
474         if indexes is None:
475             indexes = self._indexes
476         if name is _default:
477             name = self.name
478         return type(self)(variable, coords, name=name, indexes=indexes, fastpath=True)
479 
480     def _replace_maybe_drop_dims(
481         self: T_DataArray,
482         variable: Variable,
483         name: Hashable | None | Default = _default,
484     ) -> T_DataArray:
485         if variable.dims == self.dims and variable.shape == self.shape:
486             coords = self._coords.copy()
487             indexes = self._indexes
488         elif variable.dims == self.dims:
489             # Shape has changed (e.g. from reduce(..., keepdims=True)
490             new_sizes = dict(zip(self.dims, variable.shape))
491             coords = {
492                 k: v
493                 for k, v in self._coords.items()
494                 if v.shape == tuple(new_sizes[d] for d in v.dims)
495             }
496             indexes = filter_indexes_from_coords(self._indexes, set(coords))
497         else:
498             allowed_dims = set(variable.dims)
499             coords = {
500                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
501             }
502             indexes = filter_indexes_from_coords(self._indexes, set(coords))
503         return self._replace(variable, coords, name, indexes=indexes)
504 
505     def _overwrite_indexes(
506         self: T_DataArray,
507         indexes: Mapping[Any, Index],
508         coords: Mapping[Any, Variable] | None = None,
509         drop_coords: list[Hashable] | None = None,
510         rename_dims: Mapping[Any, Any] | None = None,
511     ) -> T_DataArray:
512         """Maybe replace indexes and their corresponding coordinates."""
513         if not indexes:
514             return self
515 
516         if coords is None:
517             coords = {}
518         if drop_coords is None:
519             drop_coords = []
520 
521         new_variable = self.variable.copy()
522         new_coords = self._coords.copy()
523         new_indexes = dict(self._indexes)
524 
525         for name in indexes:
526             new_coords[name] = coords[name]
527             new_indexes[name] = indexes[name]
528 
529         for name in drop_coords:
530             new_coords.pop(name)
531             new_indexes.pop(name)
532 
533         if rename_dims:
534             new_variable.dims = tuple(rename_dims.get(d, d) for d in new_variable.dims)
535 
536         return self._replace(
537             variable=new_variable, coords=new_coords, indexes=new_indexes
538         )
539 
540     def _to_temp_dataset(self) -> Dataset:
541         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
542 
543     def _from_temp_dataset(
544         self: T_DataArray, dataset: Dataset, name: Hashable | None | Default = _default
545     ) -> T_DataArray:
546         variable = dataset._variables.pop(_THIS_ARRAY)
547         coords = dataset._variables
548         indexes = dataset._indexes
549         return self._replace(variable, coords, name, indexes=indexes)
550 
551     def _to_dataset_split(self, dim: Hashable) -> Dataset:
552         """splits dataarray along dimension 'dim'"""
553 
554         def subset(dim, label):
555             array = self.loc[{dim: label}]
556             array.attrs = {}
557             return as_variable(array)
558 
559         variables = {label: subset(dim, label) for label in self.get_index(dim)}
560         variables.update({k: v for k, v in self._coords.items() if k != dim})
561         coord_names = set(self._coords) - {dim}
562         indexes = filter_indexes_from_coords(self._indexes, coord_names)
563         dataset = Dataset._construct_direct(
564             variables, coord_names, indexes=indexes, attrs=self.attrs
565         )
566         return dataset
567 
568     def _to_dataset_whole(
569         self, name: Hashable = None, shallow_copy: bool = True
570     ) -> Dataset:
571         if name is None:
572             name = self.name
573         if name is None:
574             raise ValueError(
575                 "unable to convert unnamed DataArray to a "
576                 "Dataset without providing an explicit name"
577             )
578         if name in self.coords:
579             raise ValueError(
580                 "cannot create a Dataset from a DataArray with "
581                 "the same name as one of its coordinates"
582             )
583         # use private APIs for speed: this is called by _to_temp_dataset(),
584         # which is used in the guts of a lot of operations (e.g., reindex)
585         variables = self._coords.copy()
586         variables[name] = self.variable
587         if shallow_copy:
588             for k in variables:
589                 variables[k] = variables[k].copy(deep=False)
590         indexes = self._indexes
591 
592         coord_names = set(self._coords)
593         return Dataset._construct_direct(variables, coord_names, indexes=indexes)
594 
595     def to_dataset(
596         self,
597         dim: Hashable = None,
598         *,
599         name: Hashable = None,
600         promote_attrs: bool = False,
601     ) -> Dataset:
602         """Convert a DataArray to a Dataset.
603 
604         Parameters
605         ----------
606         dim : Hashable, optional
607             Name of the dimension on this array along which to split this array
608             into separate variables. If not provided, this array is converted
609             into a Dataset of one variable.
610         name : Hashable, optional
611             Name to substitute for this array's name. Only valid if ``dim`` is
612             not provided.
613         promote_attrs : bool, default: False
614             Set to True to shallow copy attrs of DataArray to returned Dataset.
615 
616         Returns
617         -------
618         dataset : Dataset
619         """
620         if dim is not None and dim not in self.dims:
621             raise TypeError(
622                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
623             )
624 
625         if dim is not None:
626             if name is not None:
627                 raise TypeError("cannot supply both dim and name arguments")
628             result = self._to_dataset_split(dim)
629         else:
630             result = self._to_dataset_whole(name)
631 
632         if promote_attrs:
633             result.attrs = dict(self.attrs)
634 
635         return result
636 
637     @property
638     def name(self) -> Hashable | None:
639         """The name of this array."""
640         return self._name
641 
642     @name.setter
643     def name(self, value: Hashable | None) -> None:
644         self._name = value
645 
646     @property
647     def variable(self) -> Variable:
648         """Low level interface to the Variable object for this DataArray."""
649         return self._variable
650 
651     @property
652     def dtype(self) -> np.dtype:
653         """
654         Data-type of the array’s elements.
655 
656         See Also
657         --------
658         ndarray.dtype
659         numpy.dtype
660         """
661         return self.variable.dtype
662 
663     @property
664     def shape(self) -> tuple[int, ...]:
665         """
666         Tuple of array dimensions.
667 
668         See Also
669         --------
670         numpy.ndarray.shape
671         """
672         return self.variable.shape
673 
674     @property
675     def size(self) -> int:
676         """
677         Number of elements in the array.
678 
679         Equal to ``np.prod(a.shape)``, i.e., the product of the array’s dimensions.
680 
681         See Also
682         --------
683         numpy.ndarray.size
684         """
685         return self.variable.size
686 
687     @property
688     def nbytes(self) -> int:
689         """
690         Total bytes consumed by the elements of this DataArray's data.
691 
692         If the underlying data array does not include ``nbytes``, estimates
693         the bytes consumed based on the ``size`` and ``dtype``.
694         """
695         return self.variable.nbytes
696 
697     @property
698     def ndim(self) -> int:
699         """
700         Number of array dimensions.
701 
702         See Also
703         --------
704         numpy.ndarray.ndim
705         """
706         return self.variable.ndim
707 
708     def __len__(self) -> int:
709         return len(self.variable)
710 
711     @property
712     def data(self) -> Any:
713         """
714         The DataArray's data as an array. The underlying array type
715         (e.g. dask, sparse, pint) is preserved.
716 
717         See Also
718         --------
719         DataArray.to_numpy
720         DataArray.as_numpy
721         DataArray.values
722         """
723         return self.variable.data
724 
725     @data.setter
726     def data(self, value: Any) -> None:
727         self.variable.data = value
728 
729     @property
730     def values(self) -> np.ndarray:
731         """
732         The array's data as a numpy.ndarray.
733 
734         If the array's data is not a numpy.ndarray this will attempt to convert
735         it naively using np.array(), which will raise an error if the array
736         type does not support coercion like this (e.g. cupy).
737         """
738         return self.variable.values
739 
740     @values.setter
741     def values(self, value: Any) -> None:
742         self.variable.values = value
743 
744     def to_numpy(self) -> np.ndarray:
745         """
746         Coerces wrapped data to numpy and returns a numpy.ndarray.
747 
748         See Also
749         --------
750         DataArray.as_numpy : Same but returns the surrounding DataArray instead.
751         Dataset.as_numpy
752         DataArray.values
753         DataArray.data
754         """
755         return self.variable.to_numpy()
756 
757     def as_numpy(self: T_DataArray) -> T_DataArray:
758         """
759         Coerces wrapped data and coordinates into numpy arrays, returning a DataArray.
760 
761         See Also
762         --------
763         DataArray.to_numpy : Same but returns only the data as a numpy.ndarray object.
764         Dataset.as_numpy : Converts all variables in a Dataset.
765         DataArray.values
766         DataArray.data
767         """
768         coords = {k: v.as_numpy() for k, v in self._coords.items()}
769         return self._replace(self.variable.as_numpy(), coords, indexes=self._indexes)
770 
771     @property
772     def _in_memory(self) -> bool:
773         return self.variable._in_memory
774 
775     def _to_index(self) -> pd.Index:
776         return self.variable._to_index()
777 
778     def to_index(self) -> pd.Index:
779         """Convert this variable to a pandas.Index. Only possible for 1D
780         arrays.
781         """
782         return self.variable.to_index()
783 
784     @property
785     def dims(self) -> tuple[Hashable, ...]:
786         """Tuple of dimension names associated with this array.
787 
788         Note that the type of this property is inconsistent with
789         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
790         consistently named properties.
791 
792         See Also
793         --------
794         DataArray.sizes
795         Dataset.dims
796         """
797         return self.variable.dims
798 
799     @dims.setter
800     def dims(self, value: Any) -> NoReturn:
801         raise AttributeError(
802             "you cannot assign dims on a DataArray. Use "
803             ".rename() or .swap_dims() instead."
804         )
805 
806     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
807         if utils.is_dict_like(key):
808             return key
809         key = indexing.expanded_indexer(key, self.ndim)
810         return dict(zip(self.dims, key))
811 
812     def _getitem_coord(self: T_DataArray, key: Any) -> T_DataArray:
813         from xarray.core.dataset import _get_virtual_variable
814 
815         try:
816             var = self._coords[key]
817         except KeyError:
818             dim_sizes = dict(zip(self.dims, self.shape))
819             _, key, var = _get_virtual_variable(self._coords, key, dim_sizes)
820 
821         return self._replace_maybe_drop_dims(var, name=key)
822 
823     def __getitem__(self: T_DataArray, key: Any) -> T_DataArray:
824         if isinstance(key, str):
825             return self._getitem_coord(key)
826         else:
827             # xarray-style array indexing
828             return self.isel(indexers=self._item_key_to_dict(key))
829 
830     def __setitem__(self, key: Any, value: Any) -> None:
831         if isinstance(key, str):
832             self.coords[key] = value
833         else:
834             # Coordinates in key, value and self[key] should be consistent.
835             # TODO Coordinate consistency in key is checked here, but it
836             # causes unnecessary indexing. It should be optimized.
837             obj = self[key]
838             if isinstance(value, DataArray):
839                 assert_coordinate_consistent(value, obj.coords.variables)
840             # DataArray key -> Variable key
841             key = {
842                 k: v.variable if isinstance(v, DataArray) else v
843                 for k, v in self._item_key_to_dict(key).items()
844             }
845             self.variable[key] = value
846 
847     def __delitem__(self, key: Any) -> None:
848         del self.coords[key]
849 
850     @property
851     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
852         """Places to look-up items for attribute-style access"""
853         yield from self._item_sources
854         yield self.attrs
855 
856     @property
857     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
858         """Places to look-up items for key-completion"""
859         yield HybridMappingProxy(keys=self._coords, mapping=self.coords)
860 
861         # virtual coordinates
862         # uses empty dict -- everything here can already be found in self.coords.
863         yield HybridMappingProxy(keys=self.dims, mapping={})
864 
865     def __contains__(self, key: Any) -> bool:
866         return key in self.data
867 
868     @property
869     def loc(self) -> _LocIndexer:
870         """Attribute for location based indexing like pandas."""
871         return _LocIndexer(self)
872 
873     @property
874     def attrs(self) -> dict[Any, Any]:
875         """Dictionary storing arbitrary metadata with this array."""
876         return self.variable.attrs
877 
878     @attrs.setter
879     def attrs(self, value: Mapping[Any, Any]) -> None:
880         self.variable.attrs = dict(value)
881 
882     @property
883     def encoding(self) -> dict[Any, Any]:
884         """Dictionary of format-specific settings for how this array should be
885         serialized."""
886         return self.variable.encoding
887 
888     @encoding.setter
889     def encoding(self, value: Mapping[Any, Any]) -> None:
890         self.variable.encoding = dict(value)
891 
892     @property
893     def indexes(self) -> Indexes:
894         """Mapping of pandas.Index objects used for label based indexing.
895 
896         Raises an error if this Dataset has indexes that cannot be coerced
897         to pandas.Index objects.
898 
899         See Also
900         --------
901         DataArray.xindexes
902 
903         """
904         return self.xindexes.to_pandas_indexes()
905 
906     @property
907     def xindexes(self) -> Indexes:
908         """Mapping of xarray Index objects used for label based indexing."""
909         return Indexes(self._indexes, {k: self._coords[k] for k in self._indexes})
910 
911     @property
912     def coords(self) -> DataArrayCoordinates:
913         """Dictionary-like container of coordinate arrays."""
914         return DataArrayCoordinates(self)
915 
916     @overload
917     def reset_coords(
918         self: T_DataArray,
919         names: Dims = None,
920         drop: Literal[False] = False,
921     ) -> Dataset:
922         ...
923 
924     @overload
925     def reset_coords(
926         self: T_DataArray,
927         names: Dims = None,
928         *,
929         drop: Literal[True],
930     ) -> T_DataArray:
931         ...
932 
933     def reset_coords(
934         self: T_DataArray,
935         names: Dims = None,
936         drop: bool = False,
937     ) -> T_DataArray | Dataset:
938         """Given names of coordinates, reset them to become variables.
939 
940         Parameters
941         ----------
942         names : str, Iterable of Hashable or None, optional
943             Name(s) of non-index coordinates in this dataset to reset into
944             variables. By default, all non-index coordinates are reset.
945         drop : bool, default: False
946             If True, remove coordinates instead of converting them into
947             variables.
948 
949         Returns
950         -------
951         Dataset, or DataArray if ``drop == True``
952 
953         Examples
954         --------
955         >>> temperature = np.arange(25).reshape(5, 5)
956         >>> pressure = np.arange(50, 75).reshape(5, 5)
957         >>> da = xr.DataArray(
958         ...     data=temperature,
959         ...     dims=["x", "y"],
960         ...     coords=dict(
961         ...         lon=("x", np.arange(10, 15)),
962         ...         lat=("y", np.arange(20, 25)),
963         ...         Pressure=(["x", "y"], pressure),
964         ...     ),
965         ...     name="Temperature",
966         ... )
967         >>> da
968         <xarray.DataArray 'Temperature' (x: 5, y: 5)>
969         array([[ 0,  1,  2,  3,  4],
970                [ 5,  6,  7,  8,  9],
971                [10, 11, 12, 13, 14],
972                [15, 16, 17, 18, 19],
973                [20, 21, 22, 23, 24]])
974         Coordinates:
975             lon       (x) int64 10 11 12 13 14
976             lat       (y) int64 20 21 22 23 24
977             Pressure  (x, y) int64 50 51 52 53 54 55 56 57 ... 67 68 69 70 71 72 73 74
978         Dimensions without coordinates: x, y
979 
980         Return Dataset with target coordinate as a data variable rather than a coordinate variable:
981 
982         >>> da.reset_coords(names="Pressure")
983         <xarray.Dataset>
984         Dimensions:      (x: 5, y: 5)
985         Coordinates:
986             lon          (x) int64 10 11 12 13 14
987             lat          (y) int64 20 21 22 23 24
988         Dimensions without coordinates: x, y
989         Data variables:
990             Pressure     (x, y) int64 50 51 52 53 54 55 56 57 ... 68 69 70 71 72 73 74
991             Temperature  (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24
992 
993         Return DataArray without targeted coordinate:
994 
995         >>> da.reset_coords(names="Pressure", drop=True)
996         <xarray.DataArray 'Temperature' (x: 5, y: 5)>
997         array([[ 0,  1,  2,  3,  4],
998                [ 5,  6,  7,  8,  9],
999                [10, 11, 12, 13, 14],
1000                [15, 16, 17, 18, 19],
1001                [20, 21, 22, 23, 24]])
1002         Coordinates:
1003             lon      (x) int64 10 11 12 13 14
1004             lat      (y) int64 20 21 22 23 24
1005         Dimensions without coordinates: x, y
1006         """
1007         if names is None:
1008             names = set(self.coords) - set(self._indexes)
1009         dataset = self.coords.to_dataset().reset_coords(names, drop)
1010         if drop:
1011             return self._replace(coords=dataset._variables)
1012         if self.name is None:
1013             raise ValueError(
1014                 "cannot reset_coords with drop=False on an unnamed DataArrray"
1015             )
1016         dataset[self.name] = self.variable
1017         return dataset
1018 
1019     def __dask_tokenize__(self):
1020         from dask.base import normalize_token
1021 
1022         return normalize_token((type(self), self._variable, self._coords, self._name))
1023 
1024     def __dask_graph__(self):
1025         return self._to_temp_dataset().__dask_graph__()
1026 
1027     def __dask_keys__(self):
1028         return self._to_temp_dataset().__dask_keys__()
1029 
1030     def __dask_layers__(self):
1031         return self._to_temp_dataset().__dask_layers__()
1032 
1033     @property
1034     def __dask_optimize__(self):
1035         return self._to_temp_dataset().__dask_optimize__
1036 
1037     @property
1038     def __dask_scheduler__(self):
1039         return self._to_temp_dataset().__dask_scheduler__
1040 
1041     def __dask_postcompute__(self):
1042         func, args = self._to_temp_dataset().__dask_postcompute__()
1043         return self._dask_finalize, (self.name, func) + args
1044 
1045     def __dask_postpersist__(self):
1046         func, args = self._to_temp_dataset().__dask_postpersist__()
1047         return self._dask_finalize, (self.name, func) + args
1048 
1049     @staticmethod
1050     def _dask_finalize(results, name, func, *args, **kwargs) -> DataArray:
1051         ds = func(results, *args, **kwargs)
1052         variable = ds._variables.pop(_THIS_ARRAY)
1053         coords = ds._variables
1054         indexes = ds._indexes
1055         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1056 
1057     def load(self: T_DataArray, **kwargs) -> T_DataArray:
1058         """Manually trigger loading of this array's data from disk or a
1059         remote source into memory and return this array.
1060 
1061         Normally, it should not be necessary to call this method in user code,
1062         because all xarray functions should either work on deferred data or
1063         load data automatically. However, this method can be necessary when
1064         working with many file objects on disk.
1065 
1066         Parameters
1067         ----------
1068         **kwargs : dict
1069             Additional keyword arguments passed on to ``dask.compute``.
1070 
1071         See Also
1072         --------
1073         dask.compute
1074         """
1075         ds = self._to_temp_dataset().load(**kwargs)
1076         new = self._from_temp_dataset(ds)
1077         self._variable = new._variable
1078         self._coords = new._coords
1079         return self
1080 
1081     def compute(self: T_DataArray, **kwargs) -> T_DataArray:
1082         """Manually trigger loading of this array's data from disk or a
1083         remote source into memory and return a new array. The original is
1084         left unaltered.
1085 
1086         Normally, it should not be necessary to call this method in user code,
1087         because all xarray functions should either work on deferred data or
1088         load data automatically. However, this method can be necessary when
1089         working with many file objects on disk.
1090 
1091         Parameters
1092         ----------
1093         **kwargs : dict
1094             Additional keyword arguments passed on to ``dask.compute``.
1095 
1096         See Also
1097         --------
1098         dask.compute
1099         """
1100         new = self.copy(deep=False)
1101         return new.load(**kwargs)
1102 
1103     def persist(self: T_DataArray, **kwargs) -> T_DataArray:
1104         """Trigger computation in constituent dask arrays
1105 
1106         This keeps them as dask arrays but encourages them to keep data in
1107         memory.  This is particularly useful when on a distributed machine.
1108         When on a single machine consider using ``.compute()`` instead.
1109 
1110         Parameters
1111         ----------
1112         **kwargs : dict
1113             Additional keyword arguments passed on to ``dask.persist``.
1114 
1115         See Also
1116         --------
1117         dask.persist
1118         """
1119         ds = self._to_temp_dataset().persist(**kwargs)
1120         return self._from_temp_dataset(ds)
1121 
1122     def copy(self: T_DataArray, deep: bool = True, data: Any = None) -> T_DataArray:
1123         """Returns a copy of this array.
1124 
1125         If `deep=True`, a deep copy is made of the data array.
1126         Otherwise, a shallow copy is made, and the returned data array's
1127         values are a new view of this data array's values.
1128 
1129         Use `data` to create a new object with the same structure as
1130         original but entirely new data.
1131 
1132         Parameters
1133         ----------
1134         deep : bool, optional
1135             Whether the data array and its coordinates are loaded into memory
1136             and copied onto the new object. Default is True.
1137         data : array_like, optional
1138             Data to use in the new object. Must have same shape as original.
1139             When `data` is used, `deep` is ignored for all data variables,
1140             and only used for coords.
1141 
1142         Returns
1143         -------
1144         copy : DataArray
1145             New object with dimensions, attributes, coordinates, name,
1146             encoding, and optionally data copied from original.
1147 
1148         Examples
1149         --------
1150         Shallow versus deep copy
1151 
1152         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
1153         >>> array.copy()
1154         <xarray.DataArray (x: 3)>
1155         array([1, 2, 3])
1156         Coordinates:
1157           * x        (x) <U1 'a' 'b' 'c'
1158         >>> array_0 = array.copy(deep=False)
1159         >>> array_0[0] = 7
1160         >>> array_0
1161         <xarray.DataArray (x: 3)>
1162         array([7, 2, 3])
1163         Coordinates:
1164           * x        (x) <U1 'a' 'b' 'c'
1165         >>> array
1166         <xarray.DataArray (x: 3)>
1167         array([7, 2, 3])
1168         Coordinates:
1169           * x        (x) <U1 'a' 'b' 'c'
1170 
1171         Changing the data using the ``data`` argument maintains the
1172         structure of the original object, but with the new data. Original
1173         object is unaffected.
1174 
1175         >>> array.copy(data=[0.1, 0.2, 0.3])
1176         <xarray.DataArray (x: 3)>
1177         array([0.1, 0.2, 0.3])
1178         Coordinates:
1179           * x        (x) <U1 'a' 'b' 'c'
1180         >>> array
1181         <xarray.DataArray (x: 3)>
1182         array([7, 2, 3])
1183         Coordinates:
1184           * x        (x) <U1 'a' 'b' 'c'
1185 
1186         See Also
1187         --------
1188         pandas.DataFrame.copy
1189         """
1190         return self._copy(deep=deep, data=data)
1191 
1192     def _copy(
1193         self: T_DataArray,
1194         deep: bool = True,
1195         data: Any = None,
1196         memo: dict[int, Any] | None = None,
1197     ) -> T_DataArray:
1198         variable = self.variable._copy(deep=deep, data=data, memo=memo)
1199         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1200 
1201         coords = {}
1202         for k, v in self._coords.items():
1203             if k in index_vars:
1204                 coords[k] = index_vars[k]
1205             else:
1206                 coords[k] = v._copy(deep=deep, memo=memo)
1207 
1208         return self._replace(variable, coords, indexes=indexes)
1209 
1210     def __copy__(self: T_DataArray) -> T_DataArray:
1211         return self._copy(deep=False)
1212 
1213     def __deepcopy__(
1214         self: T_DataArray, memo: dict[int, Any] | None = None
1215     ) -> T_DataArray:
1216         return self._copy(deep=True, memo=memo)
1217 
1218     # mutable objects should not be Hashable
1219     # https://github.com/python/mypy/issues/4266
1220     __hash__ = None  # type: ignore[assignment]
1221 
1222     @property
1223     def chunks(self) -> tuple[tuple[int, ...], ...] | None:
1224         """
1225         Tuple of block lengths for this dataarray's data, in order of dimensions, or None if
1226         the underlying data is not a dask array.
1227 
1228         See Also
1229         --------
1230         DataArray.chunk
1231         DataArray.chunksizes
1232         xarray.unify_chunks
1233         """
1234         return self.variable.chunks
1235 
1236     @property
1237     def chunksizes(self) -> Mapping[Any, tuple[int, ...]]:
1238         """
1239         Mapping from dimension names to block lengths for this dataarray's data, or None if
1240         the underlying data is not a dask array.
1241         Cannot be modified directly, but can be modified by calling .chunk().
1242 
1243         Differs from DataArray.chunks because it returns a mapping of dimensions to chunk shapes
1244         instead of a tuple of chunk shapes.
1245 
1246         See Also
1247         --------
1248         DataArray.chunk
1249         DataArray.chunks
1250         xarray.unify_chunks
1251         """
1252         all_variables = [self.variable] + [c.variable for c in self.coords.values()]
1253         return get_chunksizes(all_variables)
1254 
1255     def chunk(
1256         self: T_DataArray,
1257         chunks: (
1258             int
1259             | Literal["auto"]
1260             | tuple[int, ...]
1261             | tuple[tuple[int, ...], ...]
1262             | Mapping[Any, None | int | tuple[int, ...]]
1263         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1264         name_prefix: str = "xarray-",
1265         token: str | None = None,
1266         lock: bool = False,
1267         inline_array: bool = False,
1268         **chunks_kwargs: Any,
1269     ) -> T_DataArray:
1270         """Coerce this array's data into a dask arrays with the given chunks.
1271 
1272         If this variable is a non-dask array, it will be converted to dask
1273         array. If it's a dask array, it will be rechunked to the given chunk
1274         sizes.
1275 
1276         If neither chunks is not provided for one or more dimensions, chunk
1277         sizes along that dimension will not be updated; non-dask arrays will be
1278         converted into dask arrays with a single block.
1279 
1280         Parameters
1281         ----------
1282         chunks : int, "auto", tuple of int or mapping of Hashable to int, optional
1283             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, ``(5, 5)`` or
1284             ``{"x": 5, "y": 5}``.
1285         name_prefix : str, optional
1286             Prefix for the name of the new dask array.
1287         token : str, optional
1288             Token uniquely identifying this array.
1289         lock : optional
1290             Passed on to :py:func:`dask.array.from_array`, if the array is not
1291             already as dask array.
1292         inline_array: optional
1293             Passed on to :py:func:`dask.array.from_array`, if the array is not
1294             already as dask array.
1295         **chunks_kwargs : {dim: chunks, ...}, optional
1296             The keyword arguments form of ``chunks``.
1297             One of chunks or chunks_kwargs must be provided.
1298 
1299         Returns
1300         -------
1301         chunked : xarray.DataArray
1302 
1303         See Also
1304         --------
1305         DataArray.chunks
1306         DataArray.chunksizes
1307         xarray.unify_chunks
1308         dask.array.from_array
1309         """
1310         if chunks is None:
1311             warnings.warn(
1312                 "None value for 'chunks' is deprecated. "
1313                 "It will raise an error in the future. Use instead '{}'",
1314                 category=FutureWarning,
1315             )
1316             chunks = {}
1317 
1318         if isinstance(chunks, (float, str, int)):
1319             # ignoring type; unclear why it won't accept a Literal into the value.
1320             chunks = dict.fromkeys(self.dims, chunks)  # type: ignore
1321         elif isinstance(chunks, (tuple, list)):
1322             chunks = dict(zip(self.dims, chunks))
1323         else:
1324             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
1325 
1326         ds = self._to_temp_dataset().chunk(
1327             chunks,
1328             name_prefix=name_prefix,
1329             token=token,
1330             lock=lock,
1331             inline_array=inline_array,
1332         )
1333         return self._from_temp_dataset(ds)
1334 
1335     def isel(
1336         self: T_DataArray,
1337         indexers: Mapping[Any, Any] | None = None,
1338         drop: bool = False,
1339         missing_dims: ErrorOptionsWithWarn = "raise",
1340         **indexers_kwargs: Any,
1341     ) -> T_DataArray:
1342         """Return a new DataArray whose data is given by selecting indexes
1343         along the specified dimension(s).
1344 
1345         Parameters
1346         ----------
1347         indexers : dict, optional
1348             A dict with keys matching dimensions and values given
1349             by integers, slice objects or arrays.
1350             indexer can be a integer, slice, array-like or DataArray.
1351             If DataArrays are passed as indexers, xarray-style indexing will be
1352             carried out. See :ref:`indexing` for the details.
1353             One of indexers or indexers_kwargs must be provided.
1354         drop : bool, default: False
1355             If ``drop=True``, drop coordinates variables indexed by integers
1356             instead of making them scalar.
1357         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1358             What to do if dimensions that should be selected from are not present in the
1359             DataArray:
1360             - "raise": raise an exception
1361             - "warn": raise a warning, and ignore the missing dimensions
1362             - "ignore": ignore the missing dimensions
1363         **indexers_kwargs : {dim: indexer, ...}, optional
1364             The keyword arguments form of ``indexers``.
1365 
1366         Returns
1367         -------
1368         indexed : xarray.DataArray
1369 
1370         See Also
1371         --------
1372         Dataset.isel
1373         DataArray.sel
1374 
1375         Examples
1376         --------
1377         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("x", "y"))
1378         >>> da
1379         <xarray.DataArray (x: 5, y: 5)>
1380         array([[ 0,  1,  2,  3,  4],
1381                [ 5,  6,  7,  8,  9],
1382                [10, 11, 12, 13, 14],
1383                [15, 16, 17, 18, 19],
1384                [20, 21, 22, 23, 24]])
1385         Dimensions without coordinates: x, y
1386 
1387         >>> tgt_x = xr.DataArray(np.arange(0, 5), dims="points")
1388         >>> tgt_y = xr.DataArray(np.arange(0, 5), dims="points")
1389         >>> da = da.isel(x=tgt_x, y=tgt_y)
1390         >>> da
1391         <xarray.DataArray (points: 5)>
1392         array([ 0,  6, 12, 18, 24])
1393         Dimensions without coordinates: points
1394         """
1395 
1396         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1397 
1398         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1399             ds = self._to_temp_dataset()._isel_fancy(
1400                 indexers, drop=drop, missing_dims=missing_dims
1401             )
1402             return self._from_temp_dataset(ds)
1403 
1404         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1405         # lists, or zero or one-dimensional np.ndarray's
1406 
1407         variable = self._variable.isel(indexers, missing_dims=missing_dims)
1408         indexes, index_variables = isel_indexes(self.xindexes, indexers)
1409 
1410         coords = {}
1411         for coord_name, coord_value in self._coords.items():
1412             if coord_name in index_variables:
1413                 coord_value = index_variables[coord_name]
1414             else:
1415                 coord_indexers = {
1416                     k: v for k, v in indexers.items() if k in coord_value.dims
1417                 }
1418                 if coord_indexers:
1419                     coord_value = coord_value.isel(coord_indexers)
1420                     if drop and coord_value.ndim == 0:
1421                         continue
1422             coords[coord_name] = coord_value
1423 
1424         return self._replace(variable=variable, coords=coords, indexes=indexes)
1425 
1426     def sel(
1427         self: T_DataArray,
1428         indexers: Mapping[Any, Any] | None = None,
1429         method: str | None = None,
1430         tolerance=None,
1431         drop: bool = False,
1432         **indexers_kwargs: Any,
1433     ) -> T_DataArray:
1434         """Return a new DataArray whose data is given by selecting index
1435         labels along the specified dimension(s).
1436 
1437         In contrast to `DataArray.isel`, indexers for this method should use
1438         labels instead of integers.
1439 
1440         Under the hood, this method is powered by using pandas's powerful Index
1441         objects. This makes label based indexing essentially just as fast as
1442         using integer indexing.
1443 
1444         It also means this method uses pandas's (well documented) logic for
1445         indexing. This means you can use string shortcuts for datetime indexes
1446         (e.g., '2000-01' to select all values in January 2000). It also means
1447         that slices are treated as inclusive of both the start and stop values,
1448         unlike normal Python indexing.
1449 
1450         .. warning::
1451 
1452           Do not try to assign values when using any of the indexing methods
1453           ``isel`` or ``sel``::
1454 
1455             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1456             # DO NOT do this
1457             da.isel(x=[0, 1, 2])[1] = -1
1458 
1459           Assigning values with the chained indexing using ``.sel`` or
1460           ``.isel`` fails silently.
1461 
1462         Parameters
1463         ----------
1464         indexers : dict, optional
1465             A dict with keys matching dimensions and values given
1466             by scalars, slices or arrays of tick labels. For dimensions with
1467             multi-index, the indexer may also be a dict-like object with keys
1468             matching index level names.
1469             If DataArrays are passed as indexers, xarray-style indexing will be
1470             carried out. See :ref:`indexing` for the details.
1471             One of indexers or indexers_kwargs must be provided.
1472         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1473             Method to use for inexact matches:
1474 
1475             - None (default): only exact matches
1476             - pad / ffill: propagate last valid index value forward
1477             - backfill / bfill: propagate next valid index value backward
1478             - nearest: use nearest valid index value
1479 
1480         tolerance : optional
1481             Maximum distance between original and new labels for inexact
1482             matches. The values of the index at the matching locations must
1483             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1484         drop : bool, optional
1485             If ``drop=True``, drop coordinates variables in `indexers` instead
1486             of making them scalar.
1487         **indexers_kwargs : {dim: indexer, ...}, optional
1488             The keyword arguments form of ``indexers``.
1489             One of indexers or indexers_kwargs must be provided.
1490 
1491         Returns
1492         -------
1493         obj : DataArray
1494             A new DataArray with the same contents as this DataArray, except the
1495             data and each dimension is indexed by the appropriate indexers.
1496             If indexer DataArrays have coordinates that do not conflict with
1497             this object, then these coordinates will be attached.
1498             In general, each array's data will be a view of the array's data
1499             in this DataArray, unless vectorized indexing was triggered by using
1500             an array indexer, in which case the data will be a copy.
1501 
1502         See Also
1503         --------
1504         Dataset.sel
1505         DataArray.isel
1506 
1507         Examples
1508         --------
1509         >>> da = xr.DataArray(
1510         ...     np.arange(25).reshape(5, 5),
1511         ...     coords={"x": np.arange(5), "y": np.arange(5)},
1512         ...     dims=("x", "y"),
1513         ... )
1514         >>> da
1515         <xarray.DataArray (x: 5, y: 5)>
1516         array([[ 0,  1,  2,  3,  4],
1517                [ 5,  6,  7,  8,  9],
1518                [10, 11, 12, 13, 14],
1519                [15, 16, 17, 18, 19],
1520                [20, 21, 22, 23, 24]])
1521         Coordinates:
1522           * x        (x) int64 0 1 2 3 4
1523           * y        (y) int64 0 1 2 3 4
1524 
1525         >>> tgt_x = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1526         >>> tgt_y = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1527         >>> da = da.sel(x=tgt_x, y=tgt_y, method="nearest")
1528         >>> da
1529         <xarray.DataArray (points: 5)>
1530         array([ 0,  6, 12, 18, 24])
1531         Coordinates:
1532             x        (points) int64 0 1 2 3 4
1533             y        (points) int64 0 1 2 3 4
1534         Dimensions without coordinates: points
1535         """
1536         ds = self._to_temp_dataset().sel(
1537             indexers=indexers,
1538             drop=drop,
1539             method=method,
1540             tolerance=tolerance,
1541             **indexers_kwargs,
1542         )
1543         return self._from_temp_dataset(ds)
1544 
1545     def head(
1546         self: T_DataArray,
1547         indexers: Mapping[Any, int] | int | None = None,
1548         **indexers_kwargs: Any,
1549     ) -> T_DataArray:
1550         """Return a new DataArray whose data is given by the the first `n`
1551         values along the specified dimension(s). Default `n` = 5
1552 
1553         See Also
1554         --------
1555         Dataset.head
1556         DataArray.tail
1557         DataArray.thin
1558 
1559         Examples
1560         --------
1561         >>> da = xr.DataArray(
1562         ...     np.arange(25).reshape(5, 5),
1563         ...     dims=("x", "y"),
1564         ... )
1565         >>> da
1566         <xarray.DataArray (x: 5, y: 5)>
1567         array([[ 0,  1,  2,  3,  4],
1568                [ 5,  6,  7,  8,  9],
1569                [10, 11, 12, 13, 14],
1570                [15, 16, 17, 18, 19],
1571                [20, 21, 22, 23, 24]])
1572         Dimensions without coordinates: x, y
1573 
1574         >>> da.head(x=1)
1575         <xarray.DataArray (x: 1, y: 5)>
1576         array([[0, 1, 2, 3, 4]])
1577         Dimensions without coordinates: x, y
1578 
1579         >>> da.head({"x": 2, "y": 2})
1580         <xarray.DataArray (x: 2, y: 2)>
1581         array([[0, 1],
1582                [5, 6]])
1583         Dimensions without coordinates: x, y
1584         """
1585         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1586         return self._from_temp_dataset(ds)
1587 
1588     def tail(
1589         self: T_DataArray,
1590         indexers: Mapping[Any, int] | int | None = None,
1591         **indexers_kwargs: Any,
1592     ) -> T_DataArray:
1593         """Return a new DataArray whose data is given by the the last `n`
1594         values along the specified dimension(s). Default `n` = 5
1595 
1596         See Also
1597         --------
1598         Dataset.tail
1599         DataArray.head
1600         DataArray.thin
1601 
1602         Examples
1603         --------
1604         >>> da = xr.DataArray(
1605         ...     np.arange(25).reshape(5, 5),
1606         ...     dims=("x", "y"),
1607         ... )
1608         >>> da
1609         <xarray.DataArray (x: 5, y: 5)>
1610         array([[ 0,  1,  2,  3,  4],
1611                [ 5,  6,  7,  8,  9],
1612                [10, 11, 12, 13, 14],
1613                [15, 16, 17, 18, 19],
1614                [20, 21, 22, 23, 24]])
1615         Dimensions without coordinates: x, y
1616 
1617         >>> da.tail(y=1)
1618         <xarray.DataArray (x: 5, y: 1)>
1619         array([[ 4],
1620                [ 9],
1621                [14],
1622                [19],
1623                [24]])
1624         Dimensions without coordinates: x, y
1625 
1626         >>> da.tail({"x": 2, "y": 2})
1627         <xarray.DataArray (x: 2, y: 2)>
1628         array([[18, 19],
1629                [23, 24]])
1630         Dimensions without coordinates: x, y
1631         """
1632         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1633         return self._from_temp_dataset(ds)
1634 
1635     def thin(
1636         self: T_DataArray,
1637         indexers: Mapping[Any, int] | int | None = None,
1638         **indexers_kwargs: Any,
1639     ) -> T_DataArray:
1640         """Return a new DataArray whose data is given by each `n` value
1641         along the specified dimension(s).
1642 
1643         Examples
1644         --------
1645         >>> x_arr = np.arange(0, 26)
1646         >>> x_arr
1647         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
1648                17, 18, 19, 20, 21, 22, 23, 24, 25])
1649         >>> x = xr.DataArray(
1650         ...     np.reshape(x_arr, (2, 13)),
1651         ...     dims=("x", "y"),
1652         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
1653         ... )
1654         >>> x
1655         <xarray.DataArray (x: 2, y: 13)>
1656         array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],
1657                [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]])
1658         Coordinates:
1659           * x        (x) int64 0 1
1660           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
1661 
1662         >>>
1663         >>> x.thin(3)
1664         <xarray.DataArray (x: 1, y: 5)>
1665         array([[ 0,  3,  6,  9, 12]])
1666         Coordinates:
1667           * x        (x) int64 0
1668           * y        (y) int64 0 3 6 9 12
1669         >>> x.thin({"x": 2, "y": 5})
1670         <xarray.DataArray (x: 1, y: 3)>
1671         array([[ 0,  5, 10]])
1672         Coordinates:
1673           * x        (x) int64 0
1674           * y        (y) int64 0 5 10
1675 
1676         See Also
1677         --------
1678         Dataset.thin
1679         DataArray.head
1680         DataArray.tail
1681         """
1682         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1683         return self._from_temp_dataset(ds)
1684 
1685     def broadcast_like(
1686         self: T_DataArray,
1687         other: DataArray | Dataset,
1688         exclude: Iterable[Hashable] | None = None,
1689     ) -> T_DataArray:
1690         """Broadcast this DataArray against another Dataset or DataArray.
1691 
1692         This is equivalent to xr.broadcast(other, self)[1]
1693 
1694         xarray objects are broadcast against each other in arithmetic
1695         operations, so this method is not be necessary for most uses.
1696 
1697         If no change is needed, the input data is returned to the output
1698         without being copied.
1699 
1700         If new coords are added by the broadcast, their values are
1701         NaN filled.
1702 
1703         Parameters
1704         ----------
1705         other : Dataset or DataArray
1706             Object against which to broadcast this array.
1707         exclude : iterable of Hashable, optional
1708             Dimensions that must not be broadcasted
1709 
1710         Returns
1711         -------
1712         new_da : DataArray
1713             The caller broadcasted against ``other``.
1714 
1715         Examples
1716         --------
1717         >>> arr1 = xr.DataArray(
1718         ...     np.random.randn(2, 3),
1719         ...     dims=("x", "y"),
1720         ...     coords={"x": ["a", "b"], "y": ["a", "b", "c"]},
1721         ... )
1722         >>> arr2 = xr.DataArray(
1723         ...     np.random.randn(3, 2),
1724         ...     dims=("x", "y"),
1725         ...     coords={"x": ["a", "b", "c"], "y": ["a", "b"]},
1726         ... )
1727         >>> arr1
1728         <xarray.DataArray (x: 2, y: 3)>
1729         array([[ 1.76405235,  0.40015721,  0.97873798],
1730                [ 2.2408932 ,  1.86755799, -0.97727788]])
1731         Coordinates:
1732           * x        (x) <U1 'a' 'b'
1733           * y        (y) <U1 'a' 'b' 'c'
1734         >>> arr2
1735         <xarray.DataArray (x: 3, y: 2)>
1736         array([[ 0.95008842, -0.15135721],
1737                [-0.10321885,  0.4105985 ],
1738                [ 0.14404357,  1.45427351]])
1739         Coordinates:
1740           * x        (x) <U1 'a' 'b' 'c'
1741           * y        (y) <U1 'a' 'b'
1742         >>> arr1.broadcast_like(arr2)
1743         <xarray.DataArray (x: 3, y: 3)>
1744         array([[ 1.76405235,  0.40015721,  0.97873798],
1745                [ 2.2408932 ,  1.86755799, -0.97727788],
1746                [        nan,         nan,         nan]])
1747         Coordinates:
1748           * x        (x) <U1 'a' 'b' 'c'
1749           * y        (y) <U1 'a' 'b' 'c'
1750         """
1751         if exclude is None:
1752             exclude = set()
1753         else:
1754             exclude = set(exclude)
1755         args = align(other, self, join="outer", copy=False, exclude=exclude)
1756 
1757         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1758 
1759         return _broadcast_helper(
1760             cast("T_DataArray", args[1]), exclude, dims_map, common_coords
1761         )
1762 
1763     def _reindex_callback(
1764         self: T_DataArray,
1765         aligner: alignment.Aligner,
1766         dim_pos_indexers: dict[Hashable, Any],
1767         variables: dict[Hashable, Variable],
1768         indexes: dict[Hashable, Index],
1769         fill_value: Any,
1770         exclude_dims: frozenset[Hashable],
1771         exclude_vars: frozenset[Hashable],
1772     ) -> T_DataArray:
1773         """Callback called from ``Aligner`` to create a new reindexed DataArray."""
1774 
1775         if isinstance(fill_value, dict):
1776             fill_value = fill_value.copy()
1777             sentinel = object()
1778             value = fill_value.pop(self.name, sentinel)
1779             if value is not sentinel:
1780                 fill_value[_THIS_ARRAY] = value
1781 
1782         ds = self._to_temp_dataset()
1783         reindexed = ds._reindex_callback(
1784             aligner,
1785             dim_pos_indexers,
1786             variables,
1787             indexes,
1788             fill_value,
1789             exclude_dims,
1790             exclude_vars,
1791         )
1792         return self._from_temp_dataset(reindexed)
1793 
1794     def reindex_like(
1795         self: T_DataArray,
1796         other: DataArray | Dataset,
1797         method: ReindexMethodOptions = None,
1798         tolerance: int | float | Iterable[int | float] | None = None,
1799         copy: bool = True,
1800         fill_value=dtypes.NA,
1801     ) -> T_DataArray:
1802         """Conform this object onto the indexes of another object, filling in
1803         missing values with ``fill_value``. The default fill value is NaN.
1804 
1805         Parameters
1806         ----------
1807         other : Dataset or DataArray
1808             Object with an 'indexes' attribute giving a mapping from dimension
1809             names to pandas.Index objects, which provides coordinates upon
1810             which to index the variables in this dataset. The indexes on this
1811             other object need not be the same as the indexes on this
1812             dataset. Any mis-matched index values will be filled in with
1813             NaN, and any mis-matched dimension names will simply be ignored.
1814         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1815             Method to use for filling index values from other not found on this
1816             data array:
1817 
1818             - None (default): don't fill gaps
1819             - pad / ffill: propagate last valid index value forward
1820             - backfill / bfill: propagate next valid index value backward
1821             - nearest: use nearest valid index value
1822 
1823         tolerance : optional
1824             Maximum distance between original and new labels for inexact
1825             matches. The values of the index at the matching locations must
1826             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1827             Tolerance may be a scalar value, which applies the same tolerance
1828             to all values, or list-like, which applies variable tolerance per
1829             element. List-like must be the same size as the index and its dtype
1830             must exactly match the index’s type.
1831         copy : bool, default: True
1832             If ``copy=True``, data in the return value is always copied. If
1833             ``copy=False`` and reindexing is unnecessary, or can be performed
1834             with only slice operations, then the output may share memory with
1835             the input. In either case, a new xarray object is always returned.
1836         fill_value : scalar or dict-like, optional
1837             Value to use for newly missing values. If a dict-like, maps
1838             variable names (including coordinates) to fill values. Use this
1839             data array's name to refer to the data array's values.
1840 
1841         Returns
1842         -------
1843         reindexed : DataArray
1844             Another dataset array, with this array's data but coordinates from
1845             the other object.
1846 
1847         Examples
1848         --------
1849         >>> data = np.arange(12).reshape(4, 3)
1850         >>> da1 = xr.DataArray(
1851         ...     data=data,
1852         ...     dims=["x", "y"],
1853         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
1854         ... )
1855         >>> da1
1856         <xarray.DataArray (x: 4, y: 3)>
1857         array([[ 0,  1,  2],
1858                [ 3,  4,  5],
1859                [ 6,  7,  8],
1860                [ 9, 10, 11]])
1861         Coordinates:
1862           * x        (x) int64 10 20 30 40
1863           * y        (y) int64 70 80 90
1864         >>> da2 = xr.DataArray(
1865         ...     data=data,
1866         ...     dims=["x", "y"],
1867         ...     coords={"x": [40, 30, 20, 10], "y": [90, 80, 70]},
1868         ... )
1869         >>> da2
1870         <xarray.DataArray (x: 4, y: 3)>
1871         array([[ 0,  1,  2],
1872                [ 3,  4,  5],
1873                [ 6,  7,  8],
1874                [ 9, 10, 11]])
1875         Coordinates:
1876           * x        (x) int64 40 30 20 10
1877           * y        (y) int64 90 80 70
1878 
1879         Reindexing with both DataArrays having the same coordinates set, but in different order:
1880 
1881         >>> da1.reindex_like(da2)
1882         <xarray.DataArray (x: 4, y: 3)>
1883         array([[11, 10,  9],
1884                [ 8,  7,  6],
1885                [ 5,  4,  3],
1886                [ 2,  1,  0]])
1887         Coordinates:
1888           * x        (x) int64 40 30 20 10
1889           * y        (y) int64 90 80 70
1890 
1891         Reindexing with the other array having coordinates which the source array doesn't have:
1892 
1893         >>> data = np.arange(12).reshape(4, 3)
1894         >>> da1 = xr.DataArray(
1895         ...     data=data,
1896         ...     dims=["x", "y"],
1897         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
1898         ... )
1899         >>> da2 = xr.DataArray(
1900         ...     data=data,
1901         ...     dims=["x", "y"],
1902         ...     coords={"x": [20, 10, 29, 39], "y": [70, 80, 90]},
1903         ... )
1904         >>> da1.reindex_like(da2)
1905         <xarray.DataArray (x: 4, y: 3)>
1906         array([[ 3.,  4.,  5.],
1907                [ 0.,  1.,  2.],
1908                [nan, nan, nan],
1909                [nan, nan, nan]])
1910         Coordinates:
1911           * x        (x) int64 20 10 29 39
1912           * y        (y) int64 70 80 90
1913 
1914         Filling missing values with the previous valid index with respect to the coordinates' value:
1915 
1916         >>> da1.reindex_like(da2, method="ffill")
1917         <xarray.DataArray (x: 4, y: 3)>
1918         array([[3, 4, 5],
1919                [0, 1, 2],
1920                [3, 4, 5],
1921                [6, 7, 8]])
1922         Coordinates:
1923           * x        (x) int64 20 10 29 39
1924           * y        (y) int64 70 80 90
1925 
1926         Filling missing values while tolerating specified error for inexact matches:
1927 
1928         >>> da1.reindex_like(da2, method="ffill", tolerance=5)
1929         <xarray.DataArray (x: 4, y: 3)>
1930         array([[ 3.,  4.,  5.],
1931                [ 0.,  1.,  2.],
1932                [nan, nan, nan],
1933                [nan, nan, nan]])
1934         Coordinates:
1935           * x        (x) int64 20 10 29 39
1936           * y        (y) int64 70 80 90
1937 
1938         Filling missing values with manually specified values:
1939 
1940         >>> da1.reindex_like(da2, fill_value=19)
1941         <xarray.DataArray (x: 4, y: 3)>
1942         array([[ 3,  4,  5],
1943                [ 0,  1,  2],
1944                [19, 19, 19],
1945                [19, 19, 19]])
1946         Coordinates:
1947           * x        (x) int64 20 10 29 39
1948           * y        (y) int64 70 80 90
1949 
1950         See Also
1951         --------
1952         DataArray.reindex
1953         align
1954         """
1955         return alignment.reindex_like(
1956             self,
1957             other=other,
1958             method=method,
1959             tolerance=tolerance,
1960             copy=copy,
1961             fill_value=fill_value,
1962         )
1963 
1964     def reindex(
1965         self: T_DataArray,
1966         indexers: Mapping[Any, Any] | None = None,
1967         method: ReindexMethodOptions = None,
1968         tolerance: float | Iterable[float] | None = None,
1969         copy: bool = True,
1970         fill_value=dtypes.NA,
1971         **indexers_kwargs: Any,
1972     ) -> T_DataArray:
1973         """Conform this object onto the indexes of another object, filling in
1974         missing values with ``fill_value``. The default fill value is NaN.
1975 
1976         Parameters
1977         ----------
1978         indexers : dict, optional
1979             Dictionary with keys given by dimension names and values given by
1980             arrays of coordinates tick labels. Any mis-matched coordinate
1981             values will be filled in with NaN, and any mis-matched dimension
1982             names will simply be ignored.
1983             One of indexers or indexers_kwargs must be provided.
1984         copy : bool, optional
1985             If ``copy=True``, data in the return value is always copied. If
1986             ``copy=False`` and reindexing is unnecessary, or can be performed
1987             with only slice operations, then the output may share memory with
1988             the input. In either case, a new xarray object is always returned.
1989         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1990             Method to use for filling index values in ``indexers`` not found on
1991             this data array:
1992 
1993             - None (default): don't fill gaps
1994             - pad / ffill: propagate last valid index value forward
1995             - backfill / bfill: propagate next valid index value backward
1996             - nearest: use nearest valid index value
1997 
1998         tolerance : float | Iterable[float] | None, default: None
1999             Maximum distance between original and new labels for inexact
2000             matches. The values of the index at the matching locations must
2001             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2002             Tolerance may be a scalar value, which applies the same tolerance
2003             to all values, or list-like, which applies variable tolerance per
2004             element. List-like must be the same size as the index and its dtype
2005             must exactly match the index’s type.
2006         fill_value : scalar or dict-like, optional
2007             Value to use for newly missing values. If a dict-like, maps
2008             variable names (including coordinates) to fill values. Use this
2009             data array's name to refer to the data array's values.
2010         **indexers_kwargs : {dim: indexer, ...}, optional
2011             The keyword arguments form of ``indexers``.
2012             One of indexers or indexers_kwargs must be provided.
2013 
2014         Returns
2015         -------
2016         reindexed : DataArray
2017             Another dataset array, with this array's data but replaced
2018             coordinates.
2019 
2020         Examples
2021         --------
2022         Reverse latitude:
2023 
2024         >>> da = xr.DataArray(
2025         ...     np.arange(4),
2026         ...     coords=[np.array([90, 89, 88, 87])],
2027         ...     dims="lat",
2028         ... )
2029         >>> da
2030         <xarray.DataArray (lat: 4)>
2031         array([0, 1, 2, 3])
2032         Coordinates:
2033           * lat      (lat) int64 90 89 88 87
2034         >>> da.reindex(lat=da.lat[::-1])
2035         <xarray.DataArray (lat: 4)>
2036         array([3, 2, 1, 0])
2037         Coordinates:
2038           * lat      (lat) int64 87 88 89 90
2039 
2040         See Also
2041         --------
2042         DataArray.reindex_like
2043         align
2044         """
2045         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
2046         return alignment.reindex(
2047             self,
2048             indexers=indexers,
2049             method=method,
2050             tolerance=tolerance,
2051             copy=copy,
2052             fill_value=fill_value,
2053         )
2054 
2055     def interp(
2056         self: T_DataArray,
2057         coords: Mapping[Any, Any] | None = None,
2058         method: InterpOptions = "linear",
2059         assume_sorted: bool = False,
2060         kwargs: Mapping[str, Any] | None = None,
2061         **coords_kwargs: Any,
2062     ) -> T_DataArray:
2063         """Interpolate a DataArray onto new coordinates
2064 
2065         Performs univariate or multivariate interpolation of a DataArray onto
2066         new coordinates using scipy's interpolation routines. If interpolating
2067         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
2068         called. When interpolating along multiple existing dimensions, an
2069         attempt is made to decompose the interpolation into multiple
2070         1-dimensional interpolations. If this is possible,
2071         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
2072         :py:func:`scipy.interpolate.interpn` is called.
2073 
2074         Parameters
2075         ----------
2076         coords : dict, optional
2077             Mapping from dimension names to the new coordinates.
2078             New coordinate can be a scalar, array-like or DataArray.
2079             If DataArrays are passed as new coordinates, their dimensions are
2080             used for the broadcasting. Missing values are skipped.
2081         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
2082             The method used to interpolate. The method should be supported by
2083             the scipy interpolator:
2084 
2085             - ``interp1d``: {"linear", "nearest", "zero", "slinear",
2086               "quadratic", "cubic", "polynomial"}
2087             - ``interpn``: {"linear", "nearest"}
2088 
2089             If ``"polynomial"`` is passed, the ``order`` keyword argument must
2090             also be provided.
2091         assume_sorted : bool, default: False
2092             If False, values of x can be in any order and they are sorted
2093             first. If True, x has to be an array of monotonically increasing
2094             values.
2095         kwargs : dict-like or None, default: None
2096             Additional keyword arguments passed to scipy's interpolator. Valid
2097             options and their behavior depend whether ``interp1d`` or
2098             ``interpn`` is used.
2099         **coords_kwargs : {dim: coordinate, ...}, optional
2100             The keyword arguments form of ``coords``.
2101             One of coords or coords_kwargs must be provided.
2102 
2103         Returns
2104         -------
2105         interpolated : DataArray
2106             New dataarray on the new coordinates.
2107 
2108         Notes
2109         -----
2110         scipy is required.
2111 
2112         See Also
2113         --------
2114         scipy.interpolate.interp1d
2115         scipy.interpolate.interpn
2116 
2117         Examples
2118         --------
2119         >>> da = xr.DataArray(
2120         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
2121         ...     dims=("x", "y"),
2122         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
2123         ... )
2124         >>> da
2125         <xarray.DataArray (x: 3, y: 4)>
2126         array([[ 1.,  4.,  2.,  9.],
2127                [ 2.,  7.,  6., nan],
2128                [ 6., nan,  5.,  8.]])
2129         Coordinates:
2130           * x        (x) int64 0 1 2
2131           * y        (y) int64 10 12 14 16
2132 
2133         1D linear interpolation (the default):
2134 
2135         >>> da.interp(x=[0, 0.75, 1.25, 1.75])
2136         <xarray.DataArray (x: 4, y: 4)>
2137         array([[1.  , 4.  , 2.  ,  nan],
2138                [1.75, 6.25, 5.  ,  nan],
2139                [3.  ,  nan, 5.75,  nan],
2140                [5.  ,  nan, 5.25,  nan]])
2141         Coordinates:
2142           * y        (y) int64 10 12 14 16
2143           * x        (x) float64 0.0 0.75 1.25 1.75
2144 
2145         1D nearest interpolation:
2146 
2147         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
2148         <xarray.DataArray (x: 4, y: 4)>
2149         array([[ 1.,  4.,  2.,  9.],
2150                [ 2.,  7.,  6., nan],
2151                [ 2.,  7.,  6., nan],
2152                [ 6., nan,  5.,  8.]])
2153         Coordinates:
2154           * y        (y) int64 10 12 14 16
2155           * x        (x) float64 0.0 0.75 1.25 1.75
2156 
2157         1D linear extrapolation:
2158 
2159         >>> da.interp(
2160         ...     x=[1, 1.5, 2.5, 3.5],
2161         ...     method="linear",
2162         ...     kwargs={"fill_value": "extrapolate"},
2163         ... )
2164         <xarray.DataArray (x: 4, y: 4)>
2165         array([[ 2. ,  7. ,  6. ,  nan],
2166                [ 4. ,  nan,  5.5,  nan],
2167                [ 8. ,  nan,  4.5,  nan],
2168                [12. ,  nan,  3.5,  nan]])
2169         Coordinates:
2170           * y        (y) int64 10 12 14 16
2171           * x        (x) float64 1.0 1.5 2.5 3.5
2172 
2173         2D linear interpolation:
2174 
2175         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
2176         <xarray.DataArray (x: 4, y: 3)>
2177         array([[2.5  , 3.   ,   nan],
2178                [4.   , 5.625,   nan],
2179                [  nan,   nan,   nan],
2180                [  nan,   nan,   nan]])
2181         Coordinates:
2182           * x        (x) float64 0.0 0.75 1.25 1.75
2183           * y        (y) int64 11 13 15
2184         """
2185         if self.dtype.kind not in "uifc":
2186             raise TypeError(
2187                 "interp only works for a numeric type array. "
2188                 "Given {}.".format(self.dtype)
2189             )
2190         ds = self._to_temp_dataset().interp(
2191             coords,
2192             method=method,
2193             kwargs=kwargs,
2194             assume_sorted=assume_sorted,
2195             **coords_kwargs,
2196         )
2197         return self._from_temp_dataset(ds)
2198 
2199     def interp_like(
2200         self: T_DataArray,
2201         other: DataArray | Dataset,
2202         method: InterpOptions = "linear",
2203         assume_sorted: bool = False,
2204         kwargs: Mapping[str, Any] | None = None,
2205     ) -> T_DataArray:
2206         """Interpolate this object onto the coordinates of another object,
2207         filling out of range values with NaN.
2208 
2209         If interpolating along a single existing dimension,
2210         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
2211         along multiple existing dimensions, an attempt is made to decompose the
2212         interpolation into multiple 1-dimensional interpolations. If this is
2213         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
2214         :py:func:`scipy.interpolate.interpn` is called.
2215 
2216         Parameters
2217         ----------
2218         other : Dataset or DataArray
2219             Object with an 'indexes' attribute giving a mapping from dimension
2220             names to an 1d array-like, which provides coordinates upon
2221             which to index the variables in this dataset. Missing values are skipped.
2222         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
2223             The method used to interpolate. The method should be supported by
2224             the scipy interpolator:
2225 
2226             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic",
2227               "polynomial"} when ``interp1d`` is called.
2228             - {"linear", "nearest"} when ``interpn`` is called.
2229 
2230             If ``"polynomial"`` is passed, the ``order`` keyword argument must
2231             also be provided.
2232         assume_sorted : bool, default: False
2233             If False, values of coordinates that are interpolated over can be
2234             in any order and they are sorted first. If True, interpolated
2235             coordinates are assumed to be an array of monotonically increasing
2236             values.
2237         kwargs : dict, optional
2238             Additional keyword passed to scipy's interpolator.
2239 
2240         Returns
2241         -------
2242         interpolated : DataArray
2243             Another dataarray by interpolating this dataarray's data along the
2244             coordinates of the other object.
2245 
2246         Examples
2247         --------
2248         >>> data = np.arange(12).reshape(4, 3)
2249         >>> da1 = xr.DataArray(
2250         ...     data=data,
2251         ...     dims=["x", "y"],
2252         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
2253         ... )
2254         >>> da1
2255         <xarray.DataArray (x: 4, y: 3)>
2256         array([[ 0,  1,  2],
2257                [ 3,  4,  5],
2258                [ 6,  7,  8],
2259                [ 9, 10, 11]])
2260         Coordinates:
2261           * x        (x) int64 10 20 30 40
2262           * y        (y) int64 70 80 90
2263         >>> da2 = xr.DataArray(
2264         ...     data=data,
2265         ...     dims=["x", "y"],
2266         ...     coords={"x": [10, 20, 29, 39], "y": [70, 80, 90]},
2267         ... )
2268         >>> da2
2269         <xarray.DataArray (x: 4, y: 3)>
2270         array([[ 0,  1,  2],
2271                [ 3,  4,  5],
2272                [ 6,  7,  8],
2273                [ 9, 10, 11]])
2274         Coordinates:
2275           * x        (x) int64 10 20 29 39
2276           * y        (y) int64 70 80 90
2277 
2278         Interpolate the values in the coordinates of the other DataArray with respect to the source's values:
2279 
2280         >>> da2.interp_like(da1)
2281         <xarray.DataArray (x: 4, y: 3)>
2282         array([[0. , 1. , 2. ],
2283                [3. , 4. , 5. ],
2284                [6.3, 7.3, 8.3],
2285                [nan, nan, nan]])
2286         Coordinates:
2287           * x        (x) int64 10 20 30 40
2288           * y        (y) int64 70 80 90
2289 
2290         Could also extrapolate missing values:
2291 
2292         >>> da2.interp_like(da1, kwargs={"fill_value": "extrapolate"})
2293         <xarray.DataArray (x: 4, y: 3)>
2294         array([[ 0. ,  1. ,  2. ],
2295                [ 3. ,  4. ,  5. ],
2296                [ 6.3,  7.3,  8.3],
2297                [ 9.3, 10.3, 11.3]])
2298         Coordinates:
2299           * x        (x) int64 10 20 30 40
2300           * y        (y) int64 70 80 90
2301 
2302         Notes
2303         -----
2304         scipy is required.
2305         If the dataarray has object-type coordinates, reindex is used for these
2306         coordinates instead of the interpolation.
2307 
2308         See Also
2309         --------
2310         DataArray.interp
2311         DataArray.reindex_like
2312         """
2313         if self.dtype.kind not in "uifc":
2314             raise TypeError(
2315                 "interp only works for a numeric type array. "
2316                 "Given {}.".format(self.dtype)
2317             )
2318         ds = self._to_temp_dataset().interp_like(
2319             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
2320         )
2321         return self._from_temp_dataset(ds)
2322 
2323     # change type of self and return to T_DataArray once
2324     # https://github.com/python/mypy/issues/12846 is resolved
2325     def rename(
2326         self,
2327         new_name_or_name_dict: Hashable | Mapping[Any, Hashable] | None = None,
2328         **names: Hashable,
2329     ) -> DataArray:
2330         """Returns a new DataArray with renamed coordinates, dimensions or a new name.
2331 
2332         Parameters
2333         ----------
2334         new_name_or_name_dict : str or dict-like, optional
2335             If the argument is dict-like, it used as a mapping from old
2336             names to new names for coordinates or dimensions. Otherwise,
2337             use the argument as the new name for this array.
2338         **names : Hashable, optional
2339             The keyword arguments form of a mapping from old names to
2340             new names for coordinates or dimensions.
2341             One of new_name_or_name_dict or names must be provided.
2342 
2343         Returns
2344         -------
2345         renamed : DataArray
2346             Renamed array or array with renamed coordinates.
2347 
2348         See Also
2349         --------
2350         Dataset.rename
2351         DataArray.swap_dims
2352         """
2353         if new_name_or_name_dict is None and not names:
2354             # change name to None?
2355             return self._replace(name=None)
2356         if utils.is_dict_like(new_name_or_name_dict) or new_name_or_name_dict is None:
2357             # change dims/coords
2358             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
2359             dataset = self._to_temp_dataset()._rename(name_dict)
2360             return self._from_temp_dataset(dataset)
2361         if utils.hashable(new_name_or_name_dict) and names:
2362             # change name + dims/coords
2363             dataset = self._to_temp_dataset()._rename(names)
2364             dataarray = self._from_temp_dataset(dataset)
2365             return dataarray._replace(name=new_name_or_name_dict)
2366         # only change name
2367         return self._replace(name=new_name_or_name_dict)
2368 
2369     def swap_dims(
2370         self: T_DataArray,
2371         dims_dict: Mapping[Any, Hashable] | None = None,
2372         **dims_kwargs,
2373     ) -> T_DataArray:
2374         """Returns a new DataArray with swapped dimensions.
2375 
2376         Parameters
2377         ----------
2378         dims_dict : dict-like
2379             Dictionary whose keys are current dimension names and whose values
2380             are new names.
2381         **dims_kwargs : {existing_dim: new_dim, ...}, optional
2382             The keyword arguments form of ``dims_dict``.
2383             One of dims_dict or dims_kwargs must be provided.
2384 
2385         Returns
2386         -------
2387         swapped : DataArray
2388             DataArray with swapped dimensions.
2389 
2390         Examples
2391         --------
2392         >>> arr = xr.DataArray(
2393         ...     data=[0, 1],
2394         ...     dims="x",
2395         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
2396         ... )
2397         >>> arr
2398         <xarray.DataArray (x: 2)>
2399         array([0, 1])
2400         Coordinates:
2401           * x        (x) <U1 'a' 'b'
2402             y        (x) int64 0 1
2403 
2404         >>> arr.swap_dims({"x": "y"})
2405         <xarray.DataArray (y: 2)>
2406         array([0, 1])
2407         Coordinates:
2408             x        (y) <U1 'a' 'b'
2409           * y        (y) int64 0 1
2410 
2411         >>> arr.swap_dims({"x": "z"})
2412         <xarray.DataArray (z: 2)>
2413         array([0, 1])
2414         Coordinates:
2415             x        (z) <U1 'a' 'b'
2416             y        (z) int64 0 1
2417         Dimensions without coordinates: z
2418 
2419         See Also
2420         --------
2421         DataArray.rename
2422         Dataset.swap_dims
2423         """
2424         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
2425         ds = self._to_temp_dataset().swap_dims(dims_dict)
2426         return self._from_temp_dataset(ds)
2427 
2428     # change type of self and return to T_DataArray once
2429     # https://github.com/python/mypy/issues/12846 is resolved
2430     def expand_dims(
2431         self,
2432         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
2433         axis: None | int | Sequence[int] = None,
2434         **dim_kwargs: Any,
2435     ) -> DataArray:
2436         """Return a new object with an additional axis (or axes) inserted at
2437         the corresponding position in the array shape. The new object is a
2438         view into the underlying array, not a copy.
2439 
2440         If dim is already a scalar coordinate, it will be promoted to a 1D
2441         coordinate consisting of a single value.
2442 
2443         Parameters
2444         ----------
2445         dim : Hashable, sequence of Hashable, dict, or None, optional
2446             Dimensions to include on the new variable.
2447             If provided as str or sequence of str, then dimensions are inserted
2448             with length 1. If provided as a dict, then the keys are the new
2449             dimensions and the values are either integers (giving the length of
2450             the new dimensions) or sequence/ndarray (giving the coordinates of
2451             the new dimensions).
2452         axis : int, sequence of int, or None, default: None
2453             Axis position(s) where new axis is to be inserted (position(s) on
2454             the result array). If a sequence of integers is passed,
2455             multiple axes are inserted. In this case, dim arguments should be
2456             same length list. If axis=None is passed, all the axes will be
2457             inserted to the start of the result array.
2458         **dim_kwargs : int or sequence or ndarray
2459             The keywords are arbitrary dimensions being inserted and the values
2460             are either the lengths of the new dims (if int is given), or their
2461             coordinates. Note, this is an alternative to passing a dict to the
2462             dim kwarg and will only be used if dim is None.
2463 
2464         Returns
2465         -------
2466         expanded : DataArray
2467             This object, but with additional dimension(s).
2468 
2469         See Also
2470         --------
2471         Dataset.expand_dims
2472 
2473         Examples
2474         --------
2475         >>> da = xr.DataArray(np.arange(5), dims=("x"))
2476         >>> da
2477         <xarray.DataArray (x: 5)>
2478         array([0, 1, 2, 3, 4])
2479         Dimensions without coordinates: x
2480 
2481         Add new dimension of length 2:
2482 
2483         >>> da.expand_dims(dim={"y": 2})
2484         <xarray.DataArray (y: 2, x: 5)>
2485         array([[0, 1, 2, 3, 4],
2486                [0, 1, 2, 3, 4]])
2487         Dimensions without coordinates: y, x
2488 
2489         >>> da.expand_dims(dim={"y": 2}, axis=1)
2490         <xarray.DataArray (x: 5, y: 2)>
2491         array([[0, 0],
2492                [1, 1],
2493                [2, 2],
2494                [3, 3],
2495                [4, 4]])
2496         Dimensions without coordinates: x, y
2497 
2498         Add a new dimension with coordinates from array:
2499 
2500         >>> da.expand_dims(dim={"y": np.arange(5)}, axis=0)
2501         <xarray.DataArray (y: 5, x: 5)>
2502         array([[0, 1, 2, 3, 4],
2503                [0, 1, 2, 3, 4],
2504                [0, 1, 2, 3, 4],
2505                [0, 1, 2, 3, 4],
2506                [0, 1, 2, 3, 4]])
2507         Coordinates:
2508           * y        (y) int64 0 1 2 3 4
2509         Dimensions without coordinates: x
2510         """
2511         if isinstance(dim, int):
2512             raise TypeError("dim should be Hashable or sequence/mapping of Hashables")
2513         elif isinstance(dim, Sequence) and not isinstance(dim, str):
2514             if len(dim) != len(set(dim)):
2515                 raise ValueError("dims should not contain duplicate values.")
2516             dim = dict.fromkeys(dim, 1)
2517         elif dim is not None and not isinstance(dim, Mapping):
2518             dim = {cast(Hashable, dim): 1}
2519 
2520         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
2521         ds = self._to_temp_dataset().expand_dims(dim, axis)
2522         return self._from_temp_dataset(ds)
2523 
2524     # change type of self and return to T_DataArray once
2525     # https://github.com/python/mypy/issues/12846 is resolved
2526     def set_index(
2527         self,
2528         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,
2529         append: bool = False,
2530         **indexes_kwargs: Hashable | Sequence[Hashable],
2531     ) -> DataArray:
2532         """Set DataArray (multi-)indexes using one or more existing
2533         coordinates.
2534 
2535         This legacy method is limited to pandas (multi-)indexes and
2536         1-dimensional "dimension" coordinates. See
2537         :py:meth:`~DataArray.set_xindex` for setting a pandas or a custom
2538         Xarray-compatible index from one or more arbitrary coordinates.
2539 
2540         Parameters
2541         ----------
2542         indexes : {dim: index, ...}
2543             Mapping from names matching dimensions and values given
2544             by (lists of) the names of existing coordinates or variables to set
2545             as new (multi-)index.
2546         append : bool, default: False
2547             If True, append the supplied index(es) to the existing index(es).
2548             Otherwise replace the existing index(es).
2549         **indexes_kwargs : optional
2550             The keyword arguments form of ``indexes``.
2551             One of indexes or indexes_kwargs must be provided.
2552 
2553         Returns
2554         -------
2555         obj : DataArray
2556             Another DataArray, with this data but replaced coordinates.
2557 
2558         Examples
2559         --------
2560         >>> arr = xr.DataArray(
2561         ...     data=np.ones((2, 3)),
2562         ...     dims=["x", "y"],
2563         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
2564         ... )
2565         >>> arr
2566         <xarray.DataArray (x: 2, y: 3)>
2567         array([[1., 1., 1.],
2568                [1., 1., 1.]])
2569         Coordinates:
2570           * x        (x) int64 0 1
2571           * y        (y) int64 0 1 2
2572             a        (x) int64 3 4
2573         >>> arr.set_index(x="a")
2574         <xarray.DataArray (x: 2, y: 3)>
2575         array([[1., 1., 1.],
2576                [1., 1., 1.]])
2577         Coordinates:
2578           * x        (x) int64 3 4
2579           * y        (y) int64 0 1 2
2580 
2581         See Also
2582         --------
2583         DataArray.reset_index
2584         DataArray.set_xindex
2585         """
2586         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)
2587         return self._from_temp_dataset(ds)
2588 
2589     # change type of self and return to T_DataArray once
2590     # https://github.com/python/mypy/issues/12846 is resolved
2591     def reset_index(
2592         self,
2593         dims_or_levels: Hashable | Sequence[Hashable],
2594         drop: bool = False,
2595     ) -> DataArray:
2596         """Reset the specified index(es) or multi-index level(s).
2597 
2598         This legacy method is specific to pandas (multi-)indexes and
2599         1-dimensional "dimension" coordinates. See the more generic
2600         :py:meth:`~DataArray.drop_indexes` and :py:meth:`~DataArray.set_xindex`
2601         method to respectively drop and set pandas or custom indexes for
2602         arbitrary coordinates.
2603 
2604         Parameters
2605         ----------
2606         dims_or_levels : Hashable or sequence of Hashable
2607             Name(s) of the dimension(s) and/or multi-index level(s) that will
2608             be reset.
2609         drop : bool, default: False
2610             If True, remove the specified indexes and/or multi-index levels
2611             instead of extracting them as new coordinates (default: False).
2612 
2613         Returns
2614         -------
2615         obj : DataArray
2616             Another dataarray, with this dataarray's data but replaced
2617             coordinates.
2618 
2619         See Also
2620         --------
2621         DataArray.set_index
2622         DataArray.set_xindex
2623         DataArray.drop_indexes
2624         """
2625         ds = self._to_temp_dataset().reset_index(dims_or_levels, drop=drop)
2626         return self._from_temp_dataset(ds)
2627 
2628     def set_xindex(
2629         self: T_DataArray,
2630         coord_names: str | Sequence[Hashable],
2631         index_cls: type[Index] | None = None,
2632         **options,
2633     ) -> T_DataArray:
2634         """Set a new, Xarray-compatible index from one or more existing
2635         coordinate(s).
2636 
2637         Parameters
2638         ----------
2639         coord_names : str or list
2640             Name(s) of the coordinate(s) used to build the index.
2641             If several names are given, their order matters.
2642         index_cls : subclass of :class:`~xarray.indexes.Index`
2643             The type of index to create. By default, try setting
2644             a pandas (multi-)index from the supplied coordinates.
2645         **options
2646             Options passed to the index constructor.
2647 
2648         Returns
2649         -------
2650         obj : DataArray
2651             Another dataarray, with this dataarray's data and with a new index.
2652 
2653         """
2654         ds = self._to_temp_dataset().set_xindex(coord_names, index_cls, **options)
2655         return self._from_temp_dataset(ds)
2656 
2657     def reorder_levels(
2658         self: T_DataArray,
2659         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
2660         **dim_order_kwargs: Sequence[int | Hashable],
2661     ) -> T_DataArray:
2662         """Rearrange index levels using input order.
2663 
2664         Parameters
2665         ----------
2666         dim_order dict-like of Hashable to int or Hashable: optional
2667             Mapping from names matching dimensions and values given
2668             by lists representing new level orders. Every given dimension
2669             must have a multi-index.
2670         **dim_order_kwargs : optional
2671             The keyword arguments form of ``dim_order``.
2672             One of dim_order or dim_order_kwargs must be provided.
2673 
2674         Returns
2675         -------
2676         obj : DataArray
2677             Another dataarray, with this dataarray's data but replaced
2678             coordinates.
2679         """
2680         ds = self._to_temp_dataset().reorder_levels(dim_order, **dim_order_kwargs)
2681         return self._from_temp_dataset(ds)
2682 
2683     def stack(
2684         self: T_DataArray,
2685         dimensions: Mapping[Any, Sequence[Hashable]] | None = None,
2686         create_index: bool | None = True,
2687         index_cls: type[Index] = PandasMultiIndex,
2688         **dimensions_kwargs: Sequence[Hashable],
2689     ) -> T_DataArray:
2690         """
2691         Stack any number of existing dimensions into a single new dimension.
2692 
2693         New dimensions will be added at the end, and the corresponding
2694         coordinate variables will be combined into a MultiIndex.
2695 
2696         Parameters
2697         ----------
2698         dimensions : mapping of Hashable to sequence of Hashable
2699             Mapping of the form `new_name=(dim1, dim2, ...)`.
2700             Names of new dimensions, and the existing dimensions that they
2701             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
2702             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
2703             all dimensions.
2704         create_index : bool or None, default: True
2705             If True, create a multi-index for each of the stacked dimensions.
2706             If False, don't create any index.
2707             If None, create a multi-index only if exactly one single (1-d) coordinate
2708             index is found for every dimension to stack.
2709         index_cls: class, optional
2710             Can be used to pass a custom multi-index type. Must be an Xarray index that
2711             implements `.stack()`. By default, a pandas multi-index wrapper is used.
2712         **dimensions_kwargs
2713             The keyword arguments form of ``dimensions``.
2714             One of dimensions or dimensions_kwargs must be provided.
2715 
2716         Returns
2717         -------
2718         stacked : DataArray
2719             DataArray with stacked data.
2720 
2721         Examples
2722         --------
2723         >>> arr = xr.DataArray(
2724         ...     np.arange(6).reshape(2, 3),
2725         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2726         ... )
2727         >>> arr
2728         <xarray.DataArray (x: 2, y: 3)>
2729         array([[0, 1, 2],
2730                [3, 4, 5]])
2731         Coordinates:
2732           * x        (x) <U1 'a' 'b'
2733           * y        (y) int64 0 1 2
2734         >>> stacked = arr.stack(z=("x", "y"))
2735         >>> stacked.indexes["z"]
2736         MultiIndex([('a', 0),
2737                     ('a', 1),
2738                     ('a', 2),
2739                     ('b', 0),
2740                     ('b', 1),
2741                     ('b', 2)],
2742                    name='z')
2743 
2744         See Also
2745         --------
2746         DataArray.unstack
2747         """
2748         ds = self._to_temp_dataset().stack(
2749             dimensions,
2750             create_index=create_index,
2751             index_cls=index_cls,
2752             **dimensions_kwargs,
2753         )
2754         return self._from_temp_dataset(ds)
2755 
2756     # change type of self and return to T_DataArray once
2757     # https://github.com/python/mypy/issues/12846 is resolved
2758     def unstack(
2759         self,
2760         dim: Dims = None,
2761         fill_value: Any = dtypes.NA,
2762         sparse: bool = False,
2763     ) -> DataArray:
2764         """
2765         Unstack existing dimensions corresponding to MultiIndexes into
2766         multiple new dimensions.
2767 
2768         New dimensions will be added at the end.
2769 
2770         Parameters
2771         ----------
2772         dim : str, Iterable of Hashable or None, optional
2773             Dimension(s) over which to unstack. By default unstacks all
2774             MultiIndexes.
2775         fill_value : scalar or dict-like, default: nan
2776             Value to be filled. If a dict-like, maps variable names to
2777             fill values. Use the data array's name to refer to its
2778             name. If not provided or if the dict-like does not contain
2779             all variables, the dtype's NA value will be used.
2780         sparse : bool, default: False
2781             Use sparse-array if True
2782 
2783         Returns
2784         -------
2785         unstacked : DataArray
2786             Array with unstacked data.
2787 
2788         Examples
2789         --------
2790         >>> arr = xr.DataArray(
2791         ...     np.arange(6).reshape(2, 3),
2792         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2793         ... )
2794         >>> arr
2795         <xarray.DataArray (x: 2, y: 3)>
2796         array([[0, 1, 2],
2797                [3, 4, 5]])
2798         Coordinates:
2799           * x        (x) <U1 'a' 'b'
2800           * y        (y) int64 0 1 2
2801         >>> stacked = arr.stack(z=("x", "y"))
2802         >>> stacked.indexes["z"]
2803         MultiIndex([('a', 0),
2804                     ('a', 1),
2805                     ('a', 2),
2806                     ('b', 0),
2807                     ('b', 1),
2808                     ('b', 2)],
2809                    name='z')
2810         >>> roundtripped = stacked.unstack()
2811         >>> arr.identical(roundtripped)
2812         True
2813 
2814         See Also
2815         --------
2816         DataArray.stack
2817         """
2818         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
2819         return self._from_temp_dataset(ds)
2820 
2821     def to_unstacked_dataset(self, dim: Hashable, level: int | Hashable = 0) -> Dataset:
2822         """Unstack DataArray expanding to Dataset along a given level of a
2823         stacked coordinate.
2824 
2825         This is the inverse operation of Dataset.to_stacked_array.
2826 
2827         Parameters
2828         ----------
2829         dim : Hashable
2830             Name of existing dimension to unstack
2831         level : int or Hashable, default: 0
2832             The MultiIndex level to expand to a dataset along. Can either be
2833             the integer index of the level or its name.
2834 
2835         Returns
2836         -------
2837         unstacked: Dataset
2838 
2839         Examples
2840         --------
2841         >>> arr = xr.DataArray(
2842         ...     np.arange(6).reshape(2, 3),
2843         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2844         ... )
2845         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
2846         >>> data
2847         <xarray.Dataset>
2848         Dimensions:  (x: 2, y: 3)
2849         Coordinates:
2850           * x        (x) <U1 'a' 'b'
2851           * y        (y) int64 0 1 2
2852         Data variables:
2853             a        (x, y) int64 0 1 2 3 4 5
2854             b        (x) int64 0 3
2855         >>> stacked = data.to_stacked_array("z", ["x"])
2856         >>> stacked.indexes["z"]
2857         MultiIndex([('a', 0.0),
2858                     ('a', 1.0),
2859                     ('a', 2.0),
2860                     ('b', nan)],
2861                    name='z')
2862         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
2863         >>> data.identical(roundtripped)
2864         True
2865 
2866         See Also
2867         --------
2868         Dataset.to_stacked_array
2869         """
2870         idx = self._indexes[dim].to_pandas_index()
2871         if not isinstance(idx, pd.MultiIndex):
2872             raise ValueError(f"'{dim}' is not a stacked coordinate")
2873 
2874         level_number = idx._get_level_number(level)
2875         variables = idx.levels[level_number]
2876         variable_dim = idx.names[level_number]
2877 
2878         # pull variables out of datarray
2879         data_dict = {}
2880         for k in variables:
2881             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)
2882 
2883         # unstacked dataset
2884         return Dataset(data_dict)
2885 
2886     def transpose(
2887         self: T_DataArray,
2888         *dims: Hashable,
2889         transpose_coords: bool = True,
2890         missing_dims: ErrorOptionsWithWarn = "raise",
2891     ) -> T_DataArray:
2892         """Return a new DataArray object with transposed dimensions.
2893 
2894         Parameters
2895         ----------
2896         *dims : Hashable, optional
2897             By default, reverse the dimensions. Otherwise, reorder the
2898             dimensions to this order.
2899         transpose_coords : bool, default: True
2900             If True, also transpose the coordinates of this DataArray.
2901         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2902             What to do if dimensions that should be selected from are not present in the
2903             DataArray:
2904             - "raise": raise an exception
2905             - "warn": raise a warning, and ignore the missing dimensions
2906             - "ignore": ignore the missing dimensions
2907 
2908         Returns
2909         -------
2910         transposed : DataArray
2911             The returned DataArray's array is transposed.
2912 
2913         Notes
2914         -----
2915         This operation returns a view of this array's data. It is
2916         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
2917         -- the data will be fully loaded.
2918 
2919         See Also
2920         --------
2921         numpy.transpose
2922         Dataset.transpose
2923         """
2924         if dims:
2925             dims = tuple(utils.infix_dims(dims, self.dims, missing_dims))
2926         variable = self.variable.transpose(*dims)
2927         if transpose_coords:
2928             coords: dict[Hashable, Variable] = {}
2929             for name, coord in self.coords.items():
2930                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
2931                 coords[name] = coord.variable.transpose(*coord_dims)
2932             return self._replace(variable, coords)
2933         else:
2934             return self._replace(variable)
2935 
2936     @property
2937     def T(self: T_DataArray) -> T_DataArray:
2938         return self.transpose()
2939 
2940     # change type of self and return to T_DataArray once
2941     # https://github.com/python/mypy/issues/12846 is resolved
2942     def drop_vars(
2943         self,
2944         names: Hashable | Iterable[Hashable],
2945         *,
2946         errors: ErrorOptions = "raise",
2947     ) -> DataArray:
2948         """Returns an array with dropped variables.
2949 
2950         Parameters
2951         ----------
2952         names : Hashable or iterable of Hashable
2953             Name(s) of variables to drop.
2954         errors : {"raise", "ignore"}, default: "raise"
2955             If 'raise', raises a ValueError error if any of the variable
2956             passed are not in the dataset. If 'ignore', any given names that are in the
2957             DataArray are dropped and no error is raised.
2958 
2959         Returns
2960         -------
2961         dropped : Dataset
2962             New Dataset copied from `self` with variables removed.
2963 
2964         Examples
2965         -------
2966         >>> data = np.arange(12).reshape(4, 3)
2967         >>> da = xr.DataArray(
2968         ...     data=data,
2969         ...     dims=["x", "y"],
2970         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
2971         ... )
2972         >>> da
2973         <xarray.DataArray (x: 4, y: 3)>
2974         array([[ 0,  1,  2],
2975                [ 3,  4,  5],
2976                [ 6,  7,  8],
2977                [ 9, 10, 11]])
2978         Coordinates:
2979           * x        (x) int64 10 20 30 40
2980           * y        (y) int64 70 80 90
2981 
2982         Removing a single variable:
2983 
2984         >>> da.drop_vars("x")
2985         <xarray.DataArray (x: 4, y: 3)>
2986         array([[ 0,  1,  2],
2987                [ 3,  4,  5],
2988                [ 6,  7,  8],
2989                [ 9, 10, 11]])
2990         Coordinates:
2991           * y        (y) int64 70 80 90
2992         Dimensions without coordinates: x
2993 
2994         Removing a list of variables:
2995 
2996         >>> da.drop_vars(["x", "y"])
2997         <xarray.DataArray (x: 4, y: 3)>
2998         array([[ 0,  1,  2],
2999                [ 3,  4,  5],
3000                [ 6,  7,  8],
3001                [ 9, 10, 11]])
3002         Dimensions without coordinates: x, y
3003         """
3004         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
3005         return self._from_temp_dataset(ds)
3006 
3007     def drop_indexes(
3008         self: T_DataArray,
3009         coord_names: Hashable | Iterable[Hashable],
3010         *,
3011         errors: ErrorOptions = "raise",
3012     ) -> T_DataArray:
3013         """Drop the indexes assigned to the given coordinates.
3014 
3015         Parameters
3016         ----------
3017         coord_names : hashable or iterable of hashable
3018             Name(s) of the coordinate(s) for which to drop the index.
3019         errors : {"raise", "ignore"}, default: "raise"
3020             If 'raise', raises a ValueError error if any of the coordinates
3021             passed have no index or are not in the dataset.
3022             If 'ignore', no error is raised.
3023 
3024         Returns
3025         -------
3026         dropped : DataArray
3027             A new dataarray with dropped indexes.
3028         """
3029         ds = self._to_temp_dataset().drop_indexes(coord_names, errors=errors)
3030         return self._from_temp_dataset(ds)
3031 
3032     def drop(
3033         self: T_DataArray,
3034         labels: Mapping[Any, Any] | None = None,
3035         dim: Hashable | None = None,
3036         *,
3037         errors: ErrorOptions = "raise",
3038         **labels_kwargs,
3039     ) -> T_DataArray:
3040         """Backward compatible method based on `drop_vars` and `drop_sel`
3041 
3042         Using either `drop_vars` or `drop_sel` is encouraged
3043 
3044         See Also
3045         --------
3046         DataArray.drop_vars
3047         DataArray.drop_sel
3048         """
3049         ds = self._to_temp_dataset().drop(labels, dim, errors=errors, **labels_kwargs)
3050         return self._from_temp_dataset(ds)
3051 
3052     def drop_sel(
3053         self: T_DataArray,
3054         labels: Mapping[Any, Any] | None = None,
3055         *,
3056         errors: ErrorOptions = "raise",
3057         **labels_kwargs,
3058     ) -> T_DataArray:
3059         """Drop index labels from this DataArray.
3060 
3061         Parameters
3062         ----------
3063         labels : mapping of Hashable to Any
3064             Index labels to drop
3065         errors : {"raise", "ignore"}, default: "raise"
3066             If 'raise', raises a ValueError error if
3067             any of the index labels passed are not
3068             in the dataset. If 'ignore', any given labels that are in the
3069             dataset are dropped and no error is raised.
3070         **labels_kwargs : {dim: label, ...}, optional
3071             The keyword arguments form of ``dim`` and ``labels``
3072 
3073         Returns
3074         -------
3075         dropped : DataArray
3076 
3077         Examples
3078         --------
3079         >>> da = xr.DataArray(
3080         ...     np.arange(25).reshape(5, 5),
3081         ...     coords={"x": np.arange(0, 9, 2), "y": np.arange(0, 13, 3)},
3082         ...     dims=("x", "y"),
3083         ... )
3084         >>> da
3085         <xarray.DataArray (x: 5, y: 5)>
3086         array([[ 0,  1,  2,  3,  4],
3087                [ 5,  6,  7,  8,  9],
3088                [10, 11, 12, 13, 14],
3089                [15, 16, 17, 18, 19],
3090                [20, 21, 22, 23, 24]])
3091         Coordinates:
3092           * x        (x) int64 0 2 4 6 8
3093           * y        (y) int64 0 3 6 9 12
3094 
3095         >>> da.drop_sel(x=[0, 2], y=9)
3096         <xarray.DataArray (x: 3, y: 4)>
3097         array([[10, 11, 12, 14],
3098                [15, 16, 17, 19],
3099                [20, 21, 22, 24]])
3100         Coordinates:
3101           * x        (x) int64 4 6 8
3102           * y        (y) int64 0 3 6 12
3103 
3104         >>> da.drop_sel({"x": 6, "y": [0, 3]})
3105         <xarray.DataArray (x: 4, y: 3)>
3106         array([[ 2,  3,  4],
3107                [ 7,  8,  9],
3108                [12, 13, 14],
3109                [22, 23, 24]])
3110         Coordinates:
3111           * x        (x) int64 0 2 4 8
3112           * y        (y) int64 6 9 12
3113         """
3114         if labels_kwargs or isinstance(labels, dict):
3115             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
3116 
3117         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
3118         return self._from_temp_dataset(ds)
3119 
3120     def drop_isel(
3121         self: T_DataArray, indexers: Mapping[Any, Any] | None = None, **indexers_kwargs
3122     ) -> T_DataArray:
3123         """Drop index positions from this DataArray.
3124 
3125         Parameters
3126         ----------
3127         indexers : mapping of Hashable to Any or None, default: None
3128             Index locations to drop
3129         **indexers_kwargs : {dim: position, ...}, optional
3130             The keyword arguments form of ``dim`` and ``positions``
3131 
3132         Returns
3133         -------
3134         dropped : DataArray
3135 
3136         Raises
3137         ------
3138         IndexError
3139 
3140         Examples
3141         --------
3142         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("X", "Y"))
3143         >>> da
3144         <xarray.DataArray (X: 5, Y: 5)>
3145         array([[ 0,  1,  2,  3,  4],
3146                [ 5,  6,  7,  8,  9],
3147                [10, 11, 12, 13, 14],
3148                [15, 16, 17, 18, 19],
3149                [20, 21, 22, 23, 24]])
3150         Dimensions without coordinates: X, Y
3151 
3152         >>> da.drop_isel(X=[0, 4], Y=2)
3153         <xarray.DataArray (X: 3, Y: 4)>
3154         array([[ 5,  6,  8,  9],
3155                [10, 11, 13, 14],
3156                [15, 16, 18, 19]])
3157         Dimensions without coordinates: X, Y
3158 
3159         >>> da.drop_isel({"X": 3, "Y": 3})
3160         <xarray.DataArray (X: 4, Y: 4)>
3161         array([[ 0,  1,  2,  4],
3162                [ 5,  6,  7,  9],
3163                [10, 11, 12, 14],
3164                [20, 21, 22, 24]])
3165         Dimensions without coordinates: X, Y
3166         """
3167         dataset = self._to_temp_dataset()
3168         dataset = dataset.drop_isel(indexers=indexers, **indexers_kwargs)
3169         return self._from_temp_dataset(dataset)
3170 
3171     def dropna(
3172         self: T_DataArray,
3173         dim: Hashable,
3174         how: Literal["any", "all"] = "any",
3175         thresh: int | None = None,
3176     ) -> T_DataArray:
3177         """Returns a new array with dropped labels for missing values along
3178         the provided dimension.
3179 
3180         Parameters
3181         ----------
3182         dim : Hashable
3183             Dimension along which to drop missing values. Dropping along
3184             multiple dimensions simultaneously is not yet supported.
3185         how : {"any", "all"}, default: "any"
3186             - any : if any NA values are present, drop that label
3187             - all : if all values are NA, drop that label
3188 
3189         thresh : int or None, default: None
3190             If supplied, require this many non-NA values.
3191 
3192         Returns
3193         -------
3194         dropped : DataArray
3195 
3196         Examples
3197         --------
3198         >>> temperature = [
3199         ...     [0, 4, 2, 9],
3200         ...     [np.nan, np.nan, np.nan, np.nan],
3201         ...     [np.nan, 4, 2, 0],
3202         ...     [3, 1, 0, 0],
3203         ... ]
3204         >>> da = xr.DataArray(
3205         ...     data=temperature,
3206         ...     dims=["Y", "X"],
3207         ...     coords=dict(
3208         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75])),
3209         ...         lon=("X", np.array([10.0, 10.25, 10.5, 10.75])),
3210         ...     ),
3211         ... )
3212         >>> da
3213         <xarray.DataArray (Y: 4, X: 4)>
3214         array([[ 0.,  4.,  2.,  9.],
3215                [nan, nan, nan, nan],
3216                [nan,  4.,  2.,  0.],
3217                [ 3.,  1.,  0.,  0.]])
3218         Coordinates:
3219             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75
3220             lon      (X) float64 10.0 10.25 10.5 10.75
3221         Dimensions without coordinates: Y, X
3222 
3223         >>> da.dropna(dim="Y", how="any")
3224         <xarray.DataArray (Y: 2, X: 4)>
3225         array([[0., 4., 2., 9.],
3226                [3., 1., 0., 0.]])
3227         Coordinates:
3228             lat      (Y) float64 -20.0 -20.75
3229             lon      (X) float64 10.0 10.25 10.5 10.75
3230         Dimensions without coordinates: Y, X
3231 
3232         Drop values only if all values along the dimension are NaN:
3233 
3234         >>> da.dropna(dim="Y", how="all")
3235         <xarray.DataArray (Y: 3, X: 4)>
3236         array([[ 0.,  4.,  2.,  9.],
3237                [nan,  4.,  2.,  0.],
3238                [ 3.,  1.,  0.,  0.]])
3239         Coordinates:
3240             lat      (Y) float64 -20.0 -20.5 -20.75
3241             lon      (X) float64 10.0 10.25 10.5 10.75
3242         Dimensions without coordinates: Y, X
3243         """
3244         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
3245         return self._from_temp_dataset(ds)
3246 
3247     def fillna(self: T_DataArray, value: Any) -> T_DataArray:
3248         """Fill missing values in this object.
3249 
3250         This operation follows the normal broadcasting and alignment rules that
3251         xarray uses for binary arithmetic, except the result is aligned to this
3252         object (``join='left'``) instead of aligned to the intersection of
3253         index coordinates (``join='inner'``).
3254 
3255         Parameters
3256         ----------
3257         value : scalar, ndarray or DataArray
3258             Used to fill all matching missing values in this array. If the
3259             argument is a DataArray, it is first aligned with (reindexed to)
3260             this array.
3261 
3262         Returns
3263         -------
3264         filled : DataArray
3265 
3266         Examples
3267         --------
3268         >>> da = xr.DataArray(
3269         ...     np.array([1, 4, np.nan, 0, 3, np.nan]),
3270         ...     dims="Z",
3271         ...     coords=dict(
3272         ...         Z=("Z", np.arange(6)),
3273         ...         height=("Z", np.array([0, 10, 20, 30, 40, 50])),
3274         ...     ),
3275         ... )
3276         >>> da
3277         <xarray.DataArray (Z: 6)>
3278         array([ 1.,  4., nan,  0.,  3., nan])
3279         Coordinates:
3280           * Z        (Z) int64 0 1 2 3 4 5
3281             height   (Z) int64 0 10 20 30 40 50
3282 
3283         Fill all NaN values with 0:
3284 
3285         >>> da.fillna(0)
3286         <xarray.DataArray (Z: 6)>
3287         array([1., 4., 0., 0., 3., 0.])
3288         Coordinates:
3289           * Z        (Z) int64 0 1 2 3 4 5
3290             height   (Z) int64 0 10 20 30 40 50
3291 
3292         Fill NaN values with corresponding values in array:
3293 
3294         >>> da.fillna(np.array([2, 9, 4, 2, 8, 9]))
3295         <xarray.DataArray (Z: 6)>
3296         array([1., 4., 4., 0., 3., 9.])
3297         Coordinates:
3298           * Z        (Z) int64 0 1 2 3 4 5
3299             height   (Z) int64 0 10 20 30 40 50
3300         """
3301         if utils.is_dict_like(value):
3302             raise TypeError(
3303                 "cannot provide fill value as a dictionary with "
3304                 "fillna on a DataArray"
3305             )
3306         out = ops.fillna(self, value)
3307         return out
3308 
3309     def interpolate_na(
3310         self: T_DataArray,
3311         dim: Hashable | None = None,
3312         method: InterpOptions = "linear",
3313         limit: int | None = None,
3314         use_coordinate: bool | str = True,
3315         max_gap: (
3316             None
3317             | int
3318             | float
3319             | str
3320             | pd.Timedelta
3321             | np.timedelta64
3322             | datetime.timedelta
3323         ) = None,
3324         keep_attrs: bool | None = None,
3325         **kwargs: Any,
3326     ) -> T_DataArray:
3327         """Fill in NaNs by interpolating according to different methods.
3328 
3329         Parameters
3330         ----------
3331         dim : Hashable or None, optional
3332             Specifies the dimension along which to interpolate.
3333         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3334             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3335             String indicating which method to use for interpolation:
3336 
3337             - 'linear': linear interpolation. Additional keyword
3338               arguments are passed to :py:func:`numpy.interp`
3339             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3340               are passed to :py:func:`scipy.interpolate.interp1d`. If
3341               ``method='polynomial'``, the ``order`` keyword argument must also be
3342               provided.
3343             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3344               respective :py:class:`scipy.interpolate` classes.
3345 
3346         use_coordinate : bool or str, default: True
3347             Specifies which index to use as the x values in the interpolation
3348             formulated as `y = f(x)`. If False, values are treated as if
3349             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
3350             used. If ``use_coordinate`` is a string, it specifies the name of a
3351             coordinate variariable to use as the index.
3352         limit : int or None, default: None
3353             Maximum number of consecutive NaNs to fill. Must be greater than 0
3354             or None for no limit. This filling is done regardless of the size of
3355             the gap in the data. To only interpolate over gaps less than a given length,
3356             see ``max_gap``.
3357         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
3358             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
3359             Use None for no limit. When interpolating along a datetime64 dimension
3360             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
3361 
3362             - a string that is valid input for pandas.to_timedelta
3363             - a :py:class:`numpy.timedelta64` object
3364             - a :py:class:`pandas.Timedelta` object
3365             - a :py:class:`datetime.timedelta` object
3366 
3367             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
3368             dimensions has not been implemented yet. Gap length is defined as the difference
3369             between coordinate values at the first data point after a gap and the last value
3370             before a gap. For gaps at the beginning (end), gap length is defined as the difference
3371             between coordinate values at the first (last) valid data point and the first (last) NaN.
3372             For example, consider::
3373 
3374                 <xarray.DataArray (x: 9)>
3375                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
3376                 Coordinates:
3377                   * x        (x) int64 0 1 2 3 4 5 6 7 8
3378 
3379             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
3380         keep_attrs : bool or None, default: None
3381             If True, the dataarray's attributes (`attrs`) will be copied from
3382             the original object to the new one.  If False, the new
3383             object will be returned without attributes.
3384         **kwargs : dict, optional
3385             parameters passed verbatim to the underlying interpolation function
3386 
3387         Returns
3388         -------
3389         interpolated: DataArray
3390             Filled in DataArray.
3391 
3392         See Also
3393         --------
3394         numpy.interp
3395         scipy.interpolate
3396 
3397         Examples
3398         --------
3399         >>> da = xr.DataArray(
3400         ...     [np.nan, 2, 3, np.nan, 0], dims="x", coords={"x": [0, 1, 2, 3, 4]}
3401         ... )
3402         >>> da
3403         <xarray.DataArray (x: 5)>
3404         array([nan,  2.,  3., nan,  0.])
3405         Coordinates:
3406           * x        (x) int64 0 1 2 3 4
3407 
3408         >>> da.interpolate_na(dim="x", method="linear")
3409         <xarray.DataArray (x: 5)>
3410         array([nan, 2. , 3. , 1.5, 0. ])
3411         Coordinates:
3412           * x        (x) int64 0 1 2 3 4
3413 
3414         >>> da.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
3415         <xarray.DataArray (x: 5)>
3416         array([1. , 2. , 3. , 1.5, 0. ])
3417         Coordinates:
3418           * x        (x) int64 0 1 2 3 4
3419         """
3420         from xarray.core.missing import interp_na
3421 
3422         return interp_na(
3423             self,
3424             dim=dim,
3425             method=method,
3426             limit=limit,
3427             use_coordinate=use_coordinate,
3428             max_gap=max_gap,
3429             keep_attrs=keep_attrs,
3430             **kwargs,
3431         )
3432 
3433     def ffill(
3434         self: T_DataArray, dim: Hashable, limit: int | None = None
3435     ) -> T_DataArray:
3436         """Fill NaN values by propagating values forward
3437 
3438         *Requires bottleneck.*
3439 
3440         Parameters
3441         ----------
3442         dim : Hashable
3443             Specifies the dimension along which to propagate values when
3444             filling.
3445         limit : int or None, default: None
3446             The maximum number of consecutive NaN values to forward fill. In
3447             other words, if there is a gap with more than this number of
3448             consecutive NaNs, it will only be partially filled. Must be greater
3449             than 0 or None for no limit. Must be None or greater than or equal
3450             to axis length if filling along chunked axes (dimensions).
3451 
3452         Returns
3453         -------
3454         filled : DataArray
3455 
3456         Examples
3457         --------
3458         >>> temperature = np.array(
3459         ...     [
3460         ...         [np.nan, 1, 3],
3461         ...         [0, np.nan, 5],
3462         ...         [5, np.nan, np.nan],
3463         ...         [3, np.nan, np.nan],
3464         ...         [0, 2, 0],
3465         ...     ]
3466         ... )
3467         >>> da = xr.DataArray(
3468         ...     data=temperature,
3469         ...     dims=["Y", "X"],
3470         ...     coords=dict(
3471         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75, -21.0])),
3472         ...         lon=("X", np.array([10.0, 10.25, 10.5])),
3473         ...     ),
3474         ... )
3475         >>> da
3476         <xarray.DataArray (Y: 5, X: 3)>
3477         array([[nan,  1.,  3.],
3478                [ 0., nan,  5.],
3479                [ 5., nan, nan],
3480                [ 3., nan, nan],
3481                [ 0.,  2.,  0.]])
3482         Coordinates:
3483             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3484             lon      (X) float64 10.0 10.25 10.5
3485         Dimensions without coordinates: Y, X
3486 
3487         Fill all NaN values:
3488 
3489         >>> da.ffill(dim="Y", limit=None)
3490         <xarray.DataArray (Y: 5, X: 3)>
3491         array([[nan,  1.,  3.],
3492                [ 0.,  1.,  5.],
3493                [ 5.,  1.,  5.],
3494                [ 3.,  1.,  5.],
3495                [ 0.,  2.,  0.]])
3496         Coordinates:
3497             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3498             lon      (X) float64 10.0 10.25 10.5
3499         Dimensions without coordinates: Y, X
3500 
3501         Fill only the first of consecutive NaN values:
3502 
3503         >>> da.ffill(dim="Y", limit=1)
3504         <xarray.DataArray (Y: 5, X: 3)>
3505         array([[nan,  1.,  3.],
3506                [ 0.,  1.,  5.],
3507                [ 5., nan,  5.],
3508                [ 3., nan, nan],
3509                [ 0.,  2.,  0.]])
3510         Coordinates:
3511             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3512             lon      (X) float64 10.0 10.25 10.5
3513         Dimensions without coordinates: Y, X
3514         """
3515         from xarray.core.missing import ffill
3516 
3517         return ffill(self, dim, limit=limit)
3518 
3519     def bfill(
3520         self: T_DataArray, dim: Hashable, limit: int | None = None
3521     ) -> T_DataArray:
3522         """Fill NaN values by propagating values backward
3523 
3524         *Requires bottleneck.*
3525 
3526         Parameters
3527         ----------
3528         dim : str
3529             Specifies the dimension along which to propagate values when
3530             filling.
3531         limit : int or None, default: None
3532             The maximum number of consecutive NaN values to backward fill. In
3533             other words, if there is a gap with more than this number of
3534             consecutive NaNs, it will only be partially filled. Must be greater
3535             than 0 or None for no limit. Must be None or greater than or equal
3536             to axis length if filling along chunked axes (dimensions).
3537 
3538         Returns
3539         -------
3540         filled : DataArray
3541 
3542         Examples
3543         --------
3544         >>> temperature = np.array(
3545         ...     [
3546         ...         [0, 1, 3],
3547         ...         [0, np.nan, 5],
3548         ...         [5, np.nan, np.nan],
3549         ...         [3, np.nan, np.nan],
3550         ...         [np.nan, 2, 0],
3551         ...     ]
3552         ... )
3553         >>> da = xr.DataArray(
3554         ...     data=temperature,
3555         ...     dims=["Y", "X"],
3556         ...     coords=dict(
3557         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75, -21.0])),
3558         ...         lon=("X", np.array([10.0, 10.25, 10.5])),
3559         ...     ),
3560         ... )
3561         >>> da
3562         <xarray.DataArray (Y: 5, X: 3)>
3563         array([[ 0.,  1.,  3.],
3564                [ 0., nan,  5.],
3565                [ 5., nan, nan],
3566                [ 3., nan, nan],
3567                [nan,  2.,  0.]])
3568         Coordinates:
3569             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3570             lon      (X) float64 10.0 10.25 10.5
3571         Dimensions without coordinates: Y, X
3572 
3573         Fill all NaN values:
3574 
3575         >>> da.bfill(dim="Y", limit=None)
3576         <xarray.DataArray (Y: 5, X: 3)>
3577         array([[ 0.,  1.,  3.],
3578                [ 0.,  2.,  5.],
3579                [ 5.,  2.,  0.],
3580                [ 3.,  2.,  0.],
3581                [nan,  2.,  0.]])
3582         Coordinates:
3583             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3584             lon      (X) float64 10.0 10.25 10.5
3585         Dimensions without coordinates: Y, X
3586 
3587         Fill only the first of consecutive NaN values:
3588 
3589         >>> da.bfill(dim="Y", limit=1)
3590         <xarray.DataArray (Y: 5, X: 3)>
3591         array([[ 0.,  1.,  3.],
3592                [ 0., nan,  5.],
3593                [ 5., nan, nan],
3594                [ 3.,  2.,  0.],
3595                [nan,  2.,  0.]])
3596         Coordinates:
3597             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3598             lon      (X) float64 10.0 10.25 10.5
3599         Dimensions without coordinates: Y, X
3600         """
3601         from xarray.core.missing import bfill
3602 
3603         return bfill(self, dim, limit=limit)
3604 
3605     def combine_first(self: T_DataArray, other: T_DataArray) -> T_DataArray:
3606         """Combine two DataArray objects, with union of coordinates.
3607 
3608         This operation follows the normal broadcasting and alignment rules of
3609         ``join='outer'``.  Default to non-null values of array calling the
3610         method.  Use np.nan to fill in vacant cells after alignment.
3611 
3612         Parameters
3613         ----------
3614         other : DataArray
3615             Used to fill all matching missing values in this array.
3616 
3617         Returns
3618         -------
3619         DataArray
3620         """
3621         return ops.fillna(self, other, join="outer")
3622 
3623     def reduce(
3624         self: T_DataArray,
3625         func: Callable[..., Any],
3626         dim: Dims = None,
3627         *,
3628         axis: int | Sequence[int] | None = None,
3629         keep_attrs: bool | None = None,
3630         keepdims: bool = False,
3631         **kwargs: Any,
3632     ) -> T_DataArray:
3633         """Reduce this array by applying `func` along some dimension(s).
3634 
3635         Parameters
3636         ----------
3637         func : callable
3638             Function which can be called in the form
3639             `f(x, axis=axis, **kwargs)` to return the result of reducing an
3640             np.ndarray over an integer valued axis.
3641         dim : "...", str, Iterable of Hashable or None, optional
3642             Dimension(s) over which to apply `func`. By default `func` is
3643             applied over all dimensions.
3644         axis : int or sequence of int, optional
3645             Axis(es) over which to repeatedly apply `func`. Only one of the
3646             'dim' and 'axis' arguments can be supplied. If neither are
3647             supplied, then the reduction is calculated over the flattened array
3648             (by calling `f(x)` without an axis argument).
3649         keep_attrs : bool or None, optional
3650             If True, the variable's attributes (`attrs`) will be copied from
3651             the original object to the new one.  If False (default), the new
3652             object will be returned without attributes.
3653         keepdims : bool, default: False
3654             If True, the dimensions which are reduced are left in the result
3655             as dimensions of size one. Coordinates that use these dimensions
3656             are removed.
3657         **kwargs : dict
3658             Additional keyword arguments passed on to `func`.
3659 
3660         Returns
3661         -------
3662         reduced : DataArray
3663             DataArray with this object's array replaced with an array with
3664             summarized data and the indicated dimension(s) removed.
3665         """
3666 
3667         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
3668         return self._replace_maybe_drop_dims(var)
3669 
3670     def to_pandas(self) -> DataArray | pd.Series | pd.DataFrame:
3671         """Convert this array into a pandas object with the same shape.
3672 
3673         The type of the returned object depends on the number of DataArray
3674         dimensions:
3675 
3676         * 0D -> `xarray.DataArray`
3677         * 1D -> `pandas.Series`
3678         * 2D -> `pandas.DataFrame`
3679 
3680         Only works for arrays with 2 or fewer dimensions.
3681 
3682         The DataArray constructor performs the inverse transformation.
3683 
3684         Returns
3685         -------
3686         result : DataArray | Series | DataFrame
3687             DataArray, pandas Series or pandas DataFrame.
3688         """
3689         # TODO: consolidate the info about pandas constructors and the
3690         # attributes that correspond to their indexes into a separate module?
3691         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
3692         try:
3693             constructor = constructors[self.ndim]
3694         except KeyError:
3695             raise ValueError(
3696                 f"Cannot convert arrays with {self.ndim} dimensions into "
3697                 "pandas objects. Requires 2 or fewer dimensions."
3698             )
3699         indexes = [self.get_index(dim) for dim in self.dims]
3700         return constructor(self.values, *indexes)
3701 
3702     def to_dataframe(
3703         self, name: Hashable | None = None, dim_order: Sequence[Hashable] | None = None
3704     ) -> pd.DataFrame:
3705         """Convert this array and its coordinates into a tidy pandas.DataFrame.
3706 
3707         The DataFrame is indexed by the Cartesian product of index coordinates
3708         (in the form of a :py:class:`pandas.MultiIndex`). Other coordinates are
3709         included as columns in the DataFrame.
3710 
3711         For 1D and 2D DataArrays, see also :py:func:`DataArray.to_pandas` which
3712         doesn't rely on a MultiIndex to build the DataFrame.
3713 
3714         Parameters
3715         ----------
3716         name: Hashable or None, optional
3717             Name to give to this array (required if unnamed).
3718         dim_order: Sequence of Hashable or None, optional
3719             Hierarchical dimension order for the resulting dataframe.
3720             Array content is transposed to this order and then written out as flat
3721             vectors in contiguous order, so the last dimension in this list
3722             will be contiguous in the resulting DataFrame. This has a major
3723             influence on which operations are efficient on the resulting
3724             dataframe.
3725 
3726             If provided, must include all dimensions of this DataArray. By default,
3727             dimensions are sorted according to the DataArray dimensions order.
3728 
3729         Returns
3730         -------
3731         result: DataFrame
3732             DataArray as a pandas DataFrame.
3733 
3734         See also
3735         --------
3736         DataArray.to_pandas
3737         DataArray.to_series
3738         """
3739         if name is None:
3740             name = self.name
3741         if name is None:
3742             raise ValueError(
3743                 "cannot convert an unnamed DataArray to a "
3744                 "DataFrame: use the ``name`` parameter"
3745             )
3746         if self.ndim == 0:
3747             raise ValueError("cannot convert a scalar to a DataFrame")
3748 
3749         # By using a unique name, we can convert a DataArray into a DataFrame
3750         # even if it shares a name with one of its coordinates.
3751         # I would normally use unique_name = object() but that results in a
3752         # dataframe with columns in the wrong order, for reasons I have not
3753         # been able to debug (possibly a pandas bug?).
3754         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
3755         ds = self._to_dataset_whole(name=unique_name)
3756 
3757         if dim_order is None:
3758             ordered_dims = dict(zip(self.dims, self.shape))
3759         else:
3760             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)
3761 
3762         df = ds._to_dataframe(ordered_dims)
3763         df.columns = [name if c == unique_name else c for c in df.columns]
3764         return df
3765 
3766     def to_series(self) -> pd.Series:
3767         """Convert this array into a pandas.Series.
3768 
3769         The Series is indexed by the Cartesian product of index coordinates
3770         (in the form of a :py:class:`pandas.MultiIndex`).
3771 
3772         Returns
3773         -------
3774         result : Series
3775             DataArray as a pandas Series.
3776 
3777         See also
3778         --------
3779         DataArray.to_pandas
3780         DataArray.to_dataframe
3781         """
3782         index = self.coords.to_index()
3783         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
3784 
3785     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
3786         """Convert this array into a numpy.ma.MaskedArray
3787 
3788         Parameters
3789         ----------
3790         copy : bool, default: True
3791             If True make a copy of the array in the result. If False,
3792             a MaskedArray view of DataArray.values is returned.
3793 
3794         Returns
3795         -------
3796         result : MaskedArray
3797             Masked where invalid values (nan or inf) occur.
3798         """
3799         values = self.to_numpy()  # only compute lazy arrays once
3800         isnull = pd.isnull(values)
3801         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
3802 
3803     # path=None writes to bytes
3804     @overload
3805     def to_netcdf(
3806         self,
3807         path: None = None,
3808         mode: Literal["w", "a"] = "w",
3809         format: T_NetcdfTypes | None = None,
3810         group: str | None = None,
3811         engine: T_NetcdfEngine | None = None,
3812         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3813         unlimited_dims: Iterable[Hashable] | None = None,
3814         compute: bool = True,
3815         invalid_netcdf: bool = False,
3816     ) -> bytes:
3817         ...
3818 
3819     # default return None
3820     @overload
3821     def to_netcdf(
3822         self,
3823         path: str | PathLike,
3824         mode: Literal["w", "a"] = "w",
3825         format: T_NetcdfTypes | None = None,
3826         group: str | None = None,
3827         engine: T_NetcdfEngine | None = None,
3828         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3829         unlimited_dims: Iterable[Hashable] | None = None,
3830         compute: Literal[True] = True,
3831         invalid_netcdf: bool = False,
3832     ) -> None:
3833         ...
3834 
3835     # compute=False returns dask.Delayed
3836     @overload
3837     def to_netcdf(
3838         self,
3839         path: str | PathLike,
3840         mode: Literal["w", "a"] = "w",
3841         format: T_NetcdfTypes | None = None,
3842         group: str | None = None,
3843         engine: T_NetcdfEngine | None = None,
3844         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3845         unlimited_dims: Iterable[Hashable] | None = None,
3846         *,
3847         compute: Literal[False],
3848         invalid_netcdf: bool = False,
3849     ) -> Delayed:
3850         ...
3851 
3852     def to_netcdf(
3853         self,
3854         path: str | PathLike | None = None,
3855         mode: Literal["w", "a"] = "w",
3856         format: T_NetcdfTypes | None = None,
3857         group: str | None = None,
3858         engine: T_NetcdfEngine | None = None,
3859         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3860         unlimited_dims: Iterable[Hashable] | None = None,
3861         compute: bool = True,
3862         invalid_netcdf: bool = False,
3863     ) -> bytes | Delayed | None:
3864         """Write dataset contents to a netCDF file.
3865 
3866         Parameters
3867         ----------
3868         path : str, path-like or None, optional
3869             Path to which to save this dataset. File-like objects are only
3870             supported by the scipy engine. If no path is provided, this
3871             function returns the resulting netCDF file as bytes; in this case,
3872             we need to use scipy, which does not support netCDF version 4 (the
3873             default format becomes NETCDF3_64BIT).
3874         mode : {"w", "a"}, default: "w"
3875             Write ('w') or append ('a') mode. If mode='w', any existing file at
3876             this location will be overwritten. If mode='a', existing variables
3877             will be overwritten.
3878         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
3879                   "NETCDF3_CLASSIC"}, optional
3880             File format for the resulting netCDF file:
3881 
3882             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
3883               features.
3884             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
3885               netCDF 3 compatible API features.
3886             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
3887               which fully supports 2+ GB files, but is only compatible with
3888               clients linked against netCDF version 3.6.0 or later.
3889             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
3890               handle 2+ GB files very well.
3891 
3892             All formats are supported by the netCDF4-python library.
3893             scipy.io.netcdf only supports the last two formats.
3894 
3895             The default format is NETCDF4 if you are saving a file to disk and
3896             have the netCDF4-python library available. Otherwise, xarray falls
3897             back to using scipy to write netCDF files and defaults to the
3898             NETCDF3_64BIT format (scipy does not support netCDF4).
3899         group : str, optional
3900             Path to the netCDF4 group in the given file to open (only works for
3901             format='NETCDF4'). The group(s) will be created if necessary.
3902         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
3903             Engine to use when writing netCDF files. If not provided, the
3904             default engine is chosen based on available dependencies, with a
3905             preference for 'netcdf4' if writing to a file on disk.
3906         encoding : dict, optional
3907             Nested dictionary with variable names as keys and dictionaries of
3908             variable specific encodings as values, e.g.,
3909             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
3910             "zlib": True}, ...}``
3911 
3912             The `h5netcdf` engine supports both the NetCDF4-style compression
3913             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
3914             ones ``{"compression": "gzip", "compression_opts": 9}``.
3915             This allows using any compression plugin installed in the HDF5
3916             library, e.g. LZF.
3917 
3918         unlimited_dims : iterable of Hashable, optional
3919             Dimension(s) that should be serialized as unlimited dimensions.
3920             By default, no dimensions are treated as unlimited dimensions.
3921             Note that unlimited_dims may also be set via
3922             ``dataset.encoding["unlimited_dims"]``.
3923         compute: bool, default: True
3924             If true compute immediately, otherwise return a
3925             ``dask.delayed.Delayed`` object that can be computed later.
3926         invalid_netcdf: bool, default: False
3927             Only valid along with ``engine="h5netcdf"``. If True, allow writing
3928             hdf5 files which are invalid netcdf as described in
3929             https://github.com/h5netcdf/h5netcdf.
3930 
3931         Returns
3932         -------
3933         store: bytes or Delayed or None
3934             * ``bytes`` if path is None
3935             * ``dask.delayed.Delayed`` if compute is False
3936             * None otherwise
3937 
3938         Notes
3939         -----
3940         Only xarray.Dataset objects can be written to netCDF files, so
3941         the xarray.DataArray is converted to a xarray.Dataset object
3942         containing a single variable. If the DataArray has no name, or if the
3943         name is the same as a coordinate name, then it is given the name
3944         ``"__xarray_dataarray_variable__"``.
3945 
3946         See Also
3947         --------
3948         Dataset.to_netcdf
3949         """
3950         from xarray.backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_netcdf
3951 
3952         if self.name is None:
3953             # If no name is set then use a generic xarray name
3954             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3955         elif self.name in self.coords or self.name in self.dims:
3956             # The name is the same as one of the coords names, which netCDF
3957             # doesn't support, so rename it but keep track of the old name
3958             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3959             dataset.attrs[DATAARRAY_NAME] = self.name
3960         else:
3961             # No problems with the name - so we're fine!
3962             dataset = self.to_dataset()
3963 
3964         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
3965             dataset,
3966             path,
3967             mode=mode,
3968             format=format,
3969             group=group,
3970             engine=engine,
3971             encoding=encoding,
3972             unlimited_dims=unlimited_dims,
3973             compute=compute,
3974             multifile=False,
3975             invalid_netcdf=invalid_netcdf,
3976         )
3977 
3978     def to_dict(self, data: bool = True, encoding: bool = False) -> dict[str, Any]:
3979         """
3980         Convert this xarray.DataArray into a dictionary following xarray
3981         naming conventions.
3982 
3983         Converts all variables and attributes to native Python objects.
3984         Useful for converting to json. To avoid datetime incompatibility
3985         use decode_times=False kwarg in xarray.open_dataset.
3986 
3987         Parameters
3988         ----------
3989         data : bool, default: True
3990             Whether to include the actual data in the dictionary. When set to
3991             False, returns just the schema.
3992         encoding : bool, default: False
3993             Whether to include the Dataset's encoding in the dictionary.
3994 
3995         Returns
3996         -------
3997         dict: dict
3998 
3999         See Also
4000         --------
4001         DataArray.from_dict
4002         Dataset.to_dict
4003         """
4004         d = self.variable.to_dict(data=data)
4005         d.update({"coords": {}, "name": self.name})
4006         for k, coord in self.coords.items():
4007             d["coords"][k] = coord.variable.to_dict(data=data)
4008         if encoding:
4009             d["encoding"] = dict(self.encoding)
4010         return d
4011 
4012     @classmethod
4013     def from_dict(cls: type[T_DataArray], d: Mapping[str, Any]) -> T_DataArray:
4014         """Convert a dictionary into an xarray.DataArray
4015 
4016         Parameters
4017         ----------
4018         d : dict
4019             Mapping with a minimum structure of {"dims": [...], "data": [...]}
4020 
4021         Returns
4022         -------
4023         obj : xarray.DataArray
4024 
4025         See Also
4026         --------
4027         DataArray.to_dict
4028         Dataset.from_dict
4029 
4030         Examples
4031         --------
4032         >>> d = {"dims": "t", "data": [1, 2, 3]}
4033         >>> da = xr.DataArray.from_dict(d)
4034         >>> da
4035         <xarray.DataArray (t: 3)>
4036         array([1, 2, 3])
4037         Dimensions without coordinates: t
4038 
4039         >>> d = {
4040         ...     "coords": {
4041         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
4042         ...     },
4043         ...     "attrs": {"title": "air temperature"},
4044         ...     "dims": "t",
4045         ...     "data": [10, 20, 30],
4046         ...     "name": "a",
4047         ... }
4048         >>> da = xr.DataArray.from_dict(d)
4049         >>> da
4050         <xarray.DataArray 'a' (t: 3)>
4051         array([10, 20, 30])
4052         Coordinates:
4053           * t        (t) int64 0 1 2
4054         Attributes:
4055             title:    air temperature
4056         """
4057         coords = None
4058         if "coords" in d:
4059             try:
4060                 coords = {
4061                     k: (v["dims"], v["data"], v.get("attrs"))
4062                     for k, v in d["coords"].items()
4063                 }
4064             except KeyError as e:
4065                 raise ValueError(
4066                     "cannot convert dict when coords are missing the key "
4067                     "'{dims_data}'".format(dims_data=str(e.args[0]))
4068                 )
4069         try:
4070             data = d["data"]
4071         except KeyError:
4072             raise ValueError("cannot convert dict without the key 'data''")
4073         else:
4074             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
4075 
4076         obj.encoding.update(d.get("encoding", {}))
4077 
4078         return obj
4079 
4080     @classmethod
4081     def from_series(cls, series: pd.Series, sparse: bool = False) -> DataArray:
4082         """Convert a pandas.Series into an xarray.DataArray.
4083 
4084         If the series's index is a MultiIndex, it will be expanded into a
4085         tensor product of one-dimensional coordinates (filling in missing
4086         values with NaN). Thus this operation should be the inverse of the
4087         `to_series` method.
4088 
4089         Parameters
4090         ----------
4091         series : Series
4092             Pandas Series object to convert.
4093         sparse : bool, default: False
4094             If sparse=True, creates a sparse array instead of a dense NumPy array.
4095             Requires the pydata/sparse package.
4096 
4097         See Also
4098         --------
4099         DataArray.to_series
4100         Dataset.from_dataframe
4101         """
4102         temp_name = "__temporary_name"
4103         df = pd.DataFrame({temp_name: series})
4104         ds = Dataset.from_dataframe(df, sparse=sparse)
4105         result = cast(DataArray, ds[temp_name])
4106         result.name = series.name
4107         return result
4108 
4109     def to_cdms2(self) -> cdms2_Variable:
4110         """Convert this array into a cdms2.Variable"""
4111         from xarray.convert import to_cdms2
4112 
4113         return to_cdms2(self)
4114 
4115     @classmethod
4116     def from_cdms2(cls, variable: cdms2_Variable) -> DataArray:
4117         """Convert a cdms2.Variable into an xarray.DataArray"""
4118         from xarray.convert import from_cdms2
4119 
4120         return from_cdms2(variable)
4121 
4122     def to_iris(self) -> iris_Cube:
4123         """Convert this array into a iris.cube.Cube"""
4124         from xarray.convert import to_iris
4125 
4126         return to_iris(self)
4127 
4128     @classmethod
4129     def from_iris(cls, cube: iris_Cube) -> DataArray:
4130         """Convert a iris.cube.Cube into an xarray.DataArray"""
4131         from xarray.convert import from_iris
4132 
4133         return from_iris(cube)
4134 
4135     def _all_compat(self: T_DataArray, other: T_DataArray, compat_str: str) -> bool:
4136         """Helper function for equals, broadcast_equals, and identical"""
4137 
4138         def compat(x, y):
4139             return getattr(x.variable, compat_str)(y.variable)
4140 
4141         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
4142             self, other
4143         )
4144 
4145     def broadcast_equals(self: T_DataArray, other: T_DataArray) -> bool:
4146         """Two DataArrays are broadcast equal if they are equal after
4147         broadcasting them against each other such that they have the same
4148         dimensions.
4149 
4150         Parameters
4151         ----------
4152         other : DataArray
4153             DataArray to compare to.
4154 
4155         Returns
4156         ----------
4157         equal : bool
4158             True if the two DataArrays are broadcast equal.
4159 
4160         See Also
4161         --------
4162         DataArray.equals
4163         DataArray.identical
4164 
4165         Examples
4166         --------
4167         >>> a = xr.DataArray([1, 2], dims="X")
4168         >>> b = xr.DataArray([[1, 1], [2, 2]], dims=["X", "Y"])
4169         >>> a
4170         <xarray.DataArray (X: 2)>
4171         array([1, 2])
4172         Dimensions without coordinates: X
4173         >>> b
4174         <xarray.DataArray (X: 2, Y: 2)>
4175         array([[1, 1],
4176                [2, 2]])
4177         Dimensions without coordinates: X, Y
4178 
4179         .equals returns True if two DataArrays have the same values, dimensions, and coordinates. .broadcast_equals returns True if the results of broadcasting two DataArrays against eachother have the same values, dimensions, and coordinates.
4180 
4181         >>> a.equals(b)
4182         False
4183         >>> a2, b2 = xr.broadcast(a, b)
4184         >>> a2.equals(b2)
4185         True
4186         >>> a.broadcast_equals(b)
4187         True
4188         """
4189         try:
4190             return self._all_compat(other, "broadcast_equals")
4191         except (TypeError, AttributeError):
4192             return False
4193 
4194     def equals(self: T_DataArray, other: T_DataArray) -> bool:
4195         """True if two DataArrays have the same dimensions, coordinates and
4196         values; otherwise False.
4197 
4198         DataArrays can still be equal (like pandas objects) if they have NaN
4199         values in the same locations.
4200 
4201         This method is necessary because `v1 == v2` for ``DataArray``
4202         does element-wise comparisons (like numpy.ndarrays).
4203 
4204         Parameters
4205         ----------
4206         other : DataArray
4207             DataArray to compare to.
4208 
4209         Returns
4210         ----------
4211         equal : bool
4212             True if the two DataArrays are equal.
4213 
4214         See Also
4215         --------
4216         DataArray.broadcast_equals
4217         DataArray.identical
4218 
4219         Examples
4220         --------
4221         >>> a = xr.DataArray([1, 2, 3], dims="X")
4222         >>> b = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"))
4223         >>> c = xr.DataArray([1, 2, 3], dims="Y")
4224         >>> d = xr.DataArray([3, 2, 1], dims="X")
4225         >>> a
4226         <xarray.DataArray (X: 3)>
4227         array([1, 2, 3])
4228         Dimensions without coordinates: X
4229         >>> b
4230         <xarray.DataArray (X: 3)>
4231         array([1, 2, 3])
4232         Dimensions without coordinates: X
4233         Attributes:
4234             units:    m
4235         >>> c
4236         <xarray.DataArray (Y: 3)>
4237         array([1, 2, 3])
4238         Dimensions without coordinates: Y
4239         >>> d
4240         <xarray.DataArray (X: 3)>
4241         array([3, 2, 1])
4242         Dimensions without coordinates: X
4243 
4244         >>> a.equals(b)
4245         True
4246         >>> a.equals(c)
4247         False
4248         >>> a.equals(d)
4249         False
4250         """
4251         try:
4252             return self._all_compat(other, "equals")
4253         except (TypeError, AttributeError):
4254             return False
4255 
4256     def identical(self: T_DataArray, other: T_DataArray) -> bool:
4257         """Like equals, but also checks the array name and attributes, and
4258         attributes on all coordinates.
4259 
4260         Parameters
4261         ----------
4262         other : DataArray
4263             DataArray to compare to.
4264 
4265         Returns
4266         ----------
4267         equal : bool
4268             True if the two DataArrays are identical.
4269 
4270         See Also
4271         --------
4272         DataArray.broadcast_equals
4273         DataArray.equals
4274 
4275         Examples
4276         --------
4277         >>> a = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"), name="Width")
4278         >>> b = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"), name="Width")
4279         >>> c = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="ft"), name="Width")
4280         >>> a
4281         <xarray.DataArray 'Width' (X: 3)>
4282         array([1, 2, 3])
4283         Dimensions without coordinates: X
4284         Attributes:
4285             units:    m
4286         >>> b
4287         <xarray.DataArray 'Width' (X: 3)>
4288         array([1, 2, 3])
4289         Dimensions without coordinates: X
4290         Attributes:
4291             units:    m
4292         >>> c
4293         <xarray.DataArray 'Width' (X: 3)>
4294         array([1, 2, 3])
4295         Dimensions without coordinates: X
4296         Attributes:
4297             units:    ft
4298 
4299         >>> a.equals(b)
4300         True
4301         >>> a.identical(b)
4302         True
4303 
4304         >>> a.equals(c)
4305         True
4306         >>> a.identical(c)
4307         False
4308         """
4309         try:
4310             return self.name == other.name and self._all_compat(other, "identical")
4311         except (TypeError, AttributeError):
4312             return False
4313 
4314     def _result_name(self, other: Any = None) -> Hashable | None:
4315         # use the same naming heuristics as pandas:
4316         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
4317         other_name = getattr(other, "name", _default)
4318         if other_name is _default or other_name == self.name:
4319             return self.name
4320         else:
4321             return None
4322 
4323     def __array_wrap__(self: T_DataArray, obj, context=None) -> T_DataArray:
4324         new_var = self.variable.__array_wrap__(obj, context)
4325         return self._replace(new_var)
4326 
4327     def __matmul__(self: T_DataArray, obj: T_DataArray) -> T_DataArray:
4328         return self.dot(obj)
4329 
4330     def __rmatmul__(self: T_DataArray, other: T_DataArray) -> T_DataArray:
4331         # currently somewhat duplicative, as only other DataArrays are
4332         # compatible with matmul
4333         return computation.dot(other, self)
4334 
4335     def _unary_op(self: T_DataArray, f: Callable, *args, **kwargs) -> T_DataArray:
4336         keep_attrs = kwargs.pop("keep_attrs", None)
4337         if keep_attrs is None:
4338             keep_attrs = _get_keep_attrs(default=True)
4339         with warnings.catch_warnings():
4340             warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
4341             warnings.filterwarnings(
4342                 "ignore", r"Mean of empty slice", category=RuntimeWarning
4343             )
4344             with np.errstate(all="ignore"):
4345                 da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))
4346             if keep_attrs:
4347                 da.attrs = self.attrs
4348             return da
4349 
4350     def _binary_op(
4351         self: T_DataArray,
4352         other: Any,
4353         f: Callable,
4354         reflexive: bool = False,
4355     ) -> T_DataArray:
4356         from xarray.core.groupby import GroupBy
4357 
4358         if isinstance(other, (Dataset, GroupBy)):
4359             return NotImplemented
4360         if isinstance(other, DataArray):
4361             align_type = OPTIONS["arithmetic_join"]
4362             self, other = align(self, other, join=align_type, copy=False)  # type: ignore
4363         other_variable = getattr(other, "variable", other)
4364         other_coords = getattr(other, "coords", None)
4365 
4366         variable = (
4367             f(self.variable, other_variable)
4368             if not reflexive
4369             else f(other_variable, self.variable)
4370         )
4371         coords, indexes = self.coords._merge_raw(other_coords, reflexive)
4372         name = self._result_name(other)
4373 
4374         return self._replace(variable, coords, name, indexes=indexes)
4375 
4376     def _inplace_binary_op(self: T_DataArray, other: Any, f: Callable) -> T_DataArray:
4377         from xarray.core.groupby import GroupBy
4378 
4379         if isinstance(other, GroupBy):
4380             raise TypeError(
4381                 "in-place operations between a DataArray and "
4382                 "a grouped object are not permitted"
4383             )
4384         # n.b. we can't align other to self (with other.reindex_like(self))
4385         # because `other` may be converted into floats, which would cause
4386         # in-place arithmetic to fail unpredictably. Instead, we simply
4387         # don't support automatic alignment with in-place arithmetic.
4388         other_coords = getattr(other, "coords", None)
4389         other_variable = getattr(other, "variable", other)
4390         try:
4391             with self.coords._merge_inplace(other_coords):
4392                 f(self.variable, other_variable)
4393         except MergeError as exc:
4394             raise MergeError(
4395                 "Automatic alignment is not supported for in-place operations.\n"
4396                 "Consider aligning the indices manually or using a not-in-place operation.\n"
4397                 "See https://github.com/pydata/xarray/issues/3910 for more explanations."
4398             ) from exc
4399         return self
4400 
4401     def _copy_attrs_from(self, other: DataArray | Dataset | Variable) -> None:
4402         self.attrs = other.attrs
4403 
4404     plot = utils.UncachedAccessor(DataArrayPlotAccessor)
4405 
4406     def _title_for_slice(self, truncate: int = 50) -> str:
4407         """
4408         If the dataarray has 1 dimensional coordinates or comes from a slice
4409         we can show that info in the title
4410 
4411         Parameters
4412         ----------
4413         truncate : int, default: 50
4414             maximum number of characters for title
4415 
4416         Returns
4417         -------
4418         title : string
4419             Can be used for plot titles
4420 
4421         """
4422         one_dims = []
4423         for dim, coord in self.coords.items():
4424             if coord.size == 1:
4425                 one_dims.append(
4426                     "{dim} = {v}{unit}".format(
4427                         dim=dim,
4428                         v=format_item(coord.values),
4429                         unit=_get_units_from_attrs(coord),
4430                     )
4431                 )
4432 
4433         title = ", ".join(one_dims)
4434         if len(title) > truncate:
4435             title = title[: (truncate - 3)] + "..."
4436 
4437         return title
4438 
4439     def diff(
4440         self: T_DataArray,
4441         dim: Hashable,
4442         n: int = 1,
4443         label: Literal["upper", "lower"] = "upper",
4444     ) -> T_DataArray:
4445         """Calculate the n-th order discrete difference along given axis.
4446 
4447         Parameters
4448         ----------
4449         dim : Hashable
4450             Dimension over which to calculate the finite difference.
4451         n : int, default: 1
4452             The number of times values are differenced.
4453         label : {"upper", "lower"}, default: "upper"
4454             The new coordinate in dimension ``dim`` will have the
4455             values of either the minuend's or subtrahend's coordinate
4456             for values 'upper' and 'lower', respectively.
4457 
4458         Returns
4459         -------
4460         difference : DataArray
4461             The n-th order finite difference of this object.
4462 
4463         Notes
4464         -----
4465         `n` matches numpy's behavior and is different from pandas' first argument named
4466         `periods`.
4467 
4468         Examples
4469         --------
4470         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
4471         >>> arr.diff("x")
4472         <xarray.DataArray (x: 3)>
4473         array([0, 1, 0])
4474         Coordinates:
4475           * x        (x) int64 2 3 4
4476         >>> arr.diff("x", 2)
4477         <xarray.DataArray (x: 2)>
4478         array([ 1, -1])
4479         Coordinates:
4480           * x        (x) int64 3 4
4481 
4482         See Also
4483         --------
4484         DataArray.differentiate
4485         """
4486         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
4487         return self._from_temp_dataset(ds)
4488 
4489     def shift(
4490         self: T_DataArray,
4491         shifts: Mapping[Any, int] | None = None,
4492         fill_value: Any = dtypes.NA,
4493         **shifts_kwargs: int,
4494     ) -> T_DataArray:
4495         """Shift this DataArray by an offset along one or more dimensions.
4496 
4497         Only the data is moved; coordinates stay in place. This is consistent
4498         with the behavior of ``shift`` in pandas.
4499 
4500         Values shifted from beyond array bounds will appear at one end of
4501         each dimension, which are filled according to `fill_value`. For periodic
4502         offsets instead see `roll`.
4503 
4504         Parameters
4505         ----------
4506         shifts : mapping of Hashable to int or None, optional
4507             Integer offset to shift along each of the given dimensions.
4508             Positive offsets shift to the right; negative offsets shift to the
4509             left.
4510         fill_value : scalar, optional
4511             Value to use for newly missing values
4512         **shifts_kwargs
4513             The keyword arguments form of ``shifts``.
4514             One of shifts or shifts_kwargs must be provided.
4515 
4516         Returns
4517         -------
4518         shifted : DataArray
4519             DataArray with the same coordinates and attributes but shifted
4520             data.
4521 
4522         See Also
4523         --------
4524         roll
4525 
4526         Examples
4527         --------
4528         >>> arr = xr.DataArray([5, 6, 7], dims="x")
4529         >>> arr.shift(x=1)
4530         <xarray.DataArray (x: 3)>
4531         array([nan,  5.,  6.])
4532         Dimensions without coordinates: x
4533         """
4534         variable = self.variable.shift(
4535             shifts=shifts, fill_value=fill_value, **shifts_kwargs
4536         )
4537         return self._replace(variable=variable)
4538 
4539     def roll(
4540         self: T_DataArray,
4541         shifts: Mapping[Hashable, int] | None = None,
4542         roll_coords: bool = False,
4543         **shifts_kwargs: int,
4544     ) -> T_DataArray:
4545         """Roll this array by an offset along one or more dimensions.
4546 
4547         Unlike shift, roll treats the given dimensions as periodic, so will not
4548         create any missing values to be filled.
4549 
4550         Unlike shift, roll may rotate all variables, including coordinates
4551         if specified. The direction of rotation is consistent with
4552         :py:func:`numpy.roll`.
4553 
4554         Parameters
4555         ----------
4556         shifts : mapping of Hashable to int, optional
4557             Integer offset to rotate each of the given dimensions.
4558             Positive offsets roll to the right; negative offsets roll to the
4559             left.
4560         roll_coords : bool, default: False
4561             Indicates whether to roll the coordinates by the offset too.
4562         **shifts_kwargs : {dim: offset, ...}, optional
4563             The keyword arguments form of ``shifts``.
4564             One of shifts or shifts_kwargs must be provided.
4565 
4566         Returns
4567         -------
4568         rolled : DataArray
4569             DataArray with the same attributes but rolled data and coordinates.
4570 
4571         See Also
4572         --------
4573         shift
4574 
4575         Examples
4576         --------
4577         >>> arr = xr.DataArray([5, 6, 7], dims="x")
4578         >>> arr.roll(x=1)
4579         <xarray.DataArray (x: 3)>
4580         array([7, 5, 6])
4581         Dimensions without coordinates: x
4582         """
4583         ds = self._to_temp_dataset().roll(
4584             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
4585         )
4586         return self._from_temp_dataset(ds)
4587 
4588     @property
4589     def real(self: T_DataArray) -> T_DataArray:
4590         """
4591         The real part of the array.
4592 
4593         See Also
4594         --------
4595         numpy.ndarray.real
4596         """
4597         return self._replace(self.variable.real)
4598 
4599     @property
4600     def imag(self: T_DataArray) -> T_DataArray:
4601         """
4602         The imaginary part of the array.
4603 
4604         See Also
4605         --------
4606         numpy.ndarray.imag
4607         """
4608         return self._replace(self.variable.imag)
4609 
4610     def dot(
4611         self: T_DataArray,
4612         other: T_DataArray,
4613         dims: Dims = None,
4614     ) -> T_DataArray:
4615         """Perform dot product of two DataArrays along their shared dims.
4616 
4617         Equivalent to taking taking tensordot over all shared dims.
4618 
4619         Parameters
4620         ----------
4621         other : DataArray
4622             The other array with which the dot product is performed.
4623         dims : ..., str, Iterable of Hashable or None, optional
4624             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.
4625             If not specified, then all the common dimensions are summed over.
4626 
4627         Returns
4628         -------
4629         result : DataArray
4630             Array resulting from the dot product over all shared dimensions.
4631 
4632         See Also
4633         --------
4634         dot
4635         numpy.tensordot
4636 
4637         Examples
4638         --------
4639         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
4640         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
4641         >>> dm_vals = np.arange(4)
4642         >>> dm = xr.DataArray(dm_vals, dims=["z"])
4643 
4644         >>> dm.dims
4645         ('z',)
4646 
4647         >>> da.dims
4648         ('x', 'y', 'z')
4649 
4650         >>> dot_result = da.dot(dm)
4651         >>> dot_result.dims
4652         ('x', 'y')
4653 
4654         """
4655         if isinstance(other, Dataset):
4656             raise NotImplementedError(
4657                 "dot products are not yet supported with Dataset objects."
4658             )
4659         if not isinstance(other, DataArray):
4660             raise TypeError("dot only operates on DataArrays.")
4661 
4662         return computation.dot(self, other, dims=dims)
4663 
4664     # change type of self and return to T_DataArray once
4665     # https://github.com/python/mypy/issues/12846 is resolved
4666     def sortby(
4667         self,
4668         variables: Hashable | DataArray | Sequence[Hashable | DataArray],
4669         ascending: bool = True,
4670     ) -> DataArray:
4671         """Sort object by labels or values (along an axis).
4672 
4673         Sorts the dataarray, either along specified dimensions,
4674         or according to values of 1-D dataarrays that share dimension
4675         with calling object.
4676 
4677         If the input variables are dataarrays, then the dataarrays are aligned
4678         (via left-join) to the calling object prior to sorting by cell values.
4679         NaNs are sorted to the end, following Numpy convention.
4680 
4681         If multiple sorts along the same dimension is
4682         given, numpy's lexsort is performed along that dimension:
4683         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
4684         and the FIRST key in the sequence is used as the primary sort key,
4685         followed by the 2nd key, etc.
4686 
4687         Parameters
4688         ----------
4689         variables : Hashable, DataArray, or sequence of Hashable or DataArray
4690             1D DataArray objects or name(s) of 1D variable(s) in
4691             coords whose values are used to sort this array.
4692         ascending : bool, default: True
4693             Whether to sort by ascending or descending order.
4694 
4695         Returns
4696         -------
4697         sorted : DataArray
4698             A new dataarray where all the specified dims are sorted by dim
4699             labels.
4700 
4701         See Also
4702         --------
4703         Dataset.sortby
4704         numpy.sort
4705         pandas.sort_values
4706         pandas.sort_index
4707 
4708         Examples
4709         --------
4710         >>> da = xr.DataArray(
4711         ...     np.random.rand(5),
4712         ...     coords=[pd.date_range("1/1/2000", periods=5)],
4713         ...     dims="time",
4714         ... )
4715         >>> da
4716         <xarray.DataArray (time: 5)>
4717         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])
4718         Coordinates:
4719           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05
4720 
4721         >>> da.sortby(da)
4722         <xarray.DataArray (time: 5)>
4723         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])
4724         Coordinates:
4725           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02
4726         """
4727         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
4728         return self._from_temp_dataset(ds)
4729 
4730     def quantile(
4731         self: T_DataArray,
4732         q: ArrayLike,
4733         dim: Dims = None,
4734         method: QuantileMethods = "linear",
4735         keep_attrs: bool | None = None,
4736         skipna: bool | None = None,
4737         interpolation: QuantileMethods | None = None,
4738     ) -> T_DataArray:
4739         """Compute the qth quantile of the data along the specified dimension.
4740 
4741         Returns the qth quantiles(s) of the array elements.
4742 
4743         Parameters
4744         ----------
4745         q : float or array-like of float
4746             Quantile to compute, which must be between 0 and 1 inclusive.
4747         dim : str or Iterable of Hashable, optional
4748             Dimension(s) over which to apply quantile.
4749         method : str, default: "linear"
4750             This optional parameter specifies the interpolation method to use when the
4751             desired quantile lies between two data points. The options sorted by their R
4752             type as summarized in the H&F paper [1]_ are:
4753 
4754                 1. "inverted_cdf" (*)
4755                 2. "averaged_inverted_cdf" (*)
4756                 3. "closest_observation" (*)
4757                 4. "interpolated_inverted_cdf" (*)
4758                 5. "hazen" (*)
4759                 6. "weibull" (*)
4760                 7. "linear"  (default)
4761                 8. "median_unbiased" (*)
4762                 9. "normal_unbiased" (*)
4763 
4764             The first three methods are discontiuous. The following discontinuous
4765             variations of the default "linear" (7.) option are also available:
4766 
4767                 * "lower"
4768                 * "higher"
4769                 * "midpoint"
4770                 * "nearest"
4771 
4772             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
4773             was previously called "interpolation", renamed in accordance with numpy
4774             version 1.22.0.
4775 
4776             (*) These methods require numpy version 1.22 or newer.
4777 
4778         keep_attrs : bool or None, optional
4779             If True, the dataset's attributes (`attrs`) will be copied from
4780             the original object to the new one.  If False (default), the new
4781             object will be returned without attributes.
4782         skipna : bool or None, optional
4783             If True, skip missing values (as marked by NaN). By default, only
4784             skips missing values for float dtypes; other dtypes either do not
4785             have a sentinel missing value (int) or skipna=True has not been
4786             implemented (object, datetime64 or timedelta64).
4787 
4788         Returns
4789         -------
4790         quantiles : DataArray
4791             If `q` is a single quantile, then the result
4792             is a scalar. If multiple percentiles are given, first axis of
4793             the result corresponds to the quantile and a quantile dimension
4794             is added to the return array. The other dimensions are the
4795             dimensions that remain after the reduction of the array.
4796 
4797         See Also
4798         --------
4799         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
4800 
4801         Examples
4802         --------
4803         >>> da = xr.DataArray(
4804         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
4805         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
4806         ...     dims=("x", "y"),
4807         ... )
4808         >>> da.quantile(0)  # or da.quantile(0, dim=...)
4809         <xarray.DataArray ()>
4810         array(0.7)
4811         Coordinates:
4812             quantile  float64 0.0
4813         >>> da.quantile(0, dim="x")
4814         <xarray.DataArray (y: 4)>
4815         array([0.7, 4.2, 2.6, 1.5])
4816         Coordinates:
4817           * y         (y) float64 1.0 1.5 2.0 2.5
4818             quantile  float64 0.0
4819         >>> da.quantile([0, 0.5, 1])
4820         <xarray.DataArray (quantile: 3)>
4821         array([0.7, 3.4, 9.4])
4822         Coordinates:
4823           * quantile  (quantile) float64 0.0 0.5 1.0
4824         >>> da.quantile([0, 0.5, 1], dim="x")
4825         <xarray.DataArray (quantile: 3, y: 4)>
4826         array([[0.7 , 4.2 , 2.6 , 1.5 ],
4827                [3.6 , 5.75, 6.  , 1.7 ],
4828                [6.5 , 7.3 , 9.4 , 1.9 ]])
4829         Coordinates:
4830           * y         (y) float64 1.0 1.5 2.0 2.5
4831           * quantile  (quantile) float64 0.0 0.5 1.0
4832 
4833         References
4834         ----------
4835         .. [1] R. J. Hyndman and Y. Fan,
4836            "Sample quantiles in statistical packages,"
4837            The American Statistician, 50(4), pp. 361-365, 1996
4838         """
4839 
4840         ds = self._to_temp_dataset().quantile(
4841             q,
4842             dim=dim,
4843             keep_attrs=keep_attrs,
4844             method=method,
4845             skipna=skipna,
4846             interpolation=interpolation,
4847         )
4848         return self._from_temp_dataset(ds)
4849 
4850     def rank(
4851         self: T_DataArray,
4852         dim: Hashable,
4853         pct: bool = False,
4854         keep_attrs: bool | None = None,
4855     ) -> T_DataArray:
4856         """Ranks the data.
4857 
4858         Equal values are assigned a rank that is the average of the ranks that
4859         would have been otherwise assigned to all of the values within that
4860         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
4861 
4862         NaNs in the input array are returned as NaNs.
4863 
4864         The `bottleneck` library is required.
4865 
4866         Parameters
4867         ----------
4868         dim : Hashable
4869             Dimension over which to compute rank.
4870         pct : bool, default: False
4871             If True, compute percentage ranks, otherwise compute integer ranks.
4872         keep_attrs : bool or None, optional
4873             If True, the dataset's attributes (`attrs`) will be copied from
4874             the original object to the new one.  If False (default), the new
4875             object will be returned without attributes.
4876 
4877         Returns
4878         -------
4879         ranked : DataArray
4880             DataArray with the same coordinates and dtype 'float64'.
4881 
4882         Examples
4883         --------
4884         >>> arr = xr.DataArray([5, 6, 7], dims="x")
4885         >>> arr.rank("x")
4886         <xarray.DataArray (x: 3)>
4887         array([1., 2., 3.])
4888         Dimensions without coordinates: x
4889         """
4890 
4891         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
4892         return self._from_temp_dataset(ds)
4893 
4894     def differentiate(
4895         self: T_DataArray,
4896         coord: Hashable,
4897         edge_order: Literal[1, 2] = 1,
4898         datetime_unit: DatetimeUnitOptions = None,
4899     ) -> T_DataArray:
4900         """ Differentiate the array with the second order accurate central
4901         differences.
4902 
4903         .. note::
4904             This feature is limited to simple cartesian geometry, i.e. coord
4905             must be one dimensional.
4906 
4907         Parameters
4908         ----------
4909         coord : Hashable
4910             The coordinate to be used to compute the gradient.
4911         edge_order : {1, 2}, default: 1
4912             N-th order accurate differences at the boundaries.
4913         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", \
4914                          "us", "ns", "ps", "fs", "as", None}, optional
4915             Unit to compute gradient. Only valid for datetime coordinate.
4916 
4917         Returns
4918         -------
4919         differentiated: DataArray
4920 
4921         See also
4922         --------
4923         numpy.gradient: corresponding numpy function
4924 
4925         Examples
4926         --------
4927 
4928         >>> da = xr.DataArray(
4929         ...     np.arange(12).reshape(4, 3),
4930         ...     dims=["x", "y"],
4931         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
4932         ... )
4933         >>> da
4934         <xarray.DataArray (x: 4, y: 3)>
4935         array([[ 0,  1,  2],
4936                [ 3,  4,  5],
4937                [ 6,  7,  8],
4938                [ 9, 10, 11]])
4939         Coordinates:
4940           * x        (x) float64 0.0 0.1 1.1 1.2
4941         Dimensions without coordinates: y
4942         >>>
4943         >>> da.differentiate("x")
4944         <xarray.DataArray (x: 4, y: 3)>
4945         array([[30.        , 30.        , 30.        ],
4946                [27.54545455, 27.54545455, 27.54545455],
4947                [27.54545455, 27.54545455, 27.54545455],
4948                [30.        , 30.        , 30.        ]])
4949         Coordinates:
4950           * x        (x) float64 0.0 0.1 1.1 1.2
4951         Dimensions without coordinates: y
4952         """
4953         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
4954         return self._from_temp_dataset(ds)
4955 
4956     # change type of self and return to T_DataArray once
4957     # https://github.com/python/mypy/issues/12846 is resolved
4958     def integrate(
4959         self,
4960         coord: Hashable | Sequence[Hashable] = None,
4961         datetime_unit: DatetimeUnitOptions = None,
4962     ) -> DataArray:
4963         """Integrate along the given coordinate using the trapezoidal rule.
4964 
4965         .. note::
4966             This feature is limited to simple cartesian geometry, i.e. coord
4967             must be one dimensional.
4968 
4969         Parameters
4970         ----------
4971         coord : Hashable, or sequence of Hashable
4972             Coordinate(s) used for the integration.
4973         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
4974                         'ps', 'fs', 'as', None}, optional
4975             Specify the unit if a datetime coordinate is used.
4976 
4977         Returns
4978         -------
4979         integrated : DataArray
4980 
4981         See also
4982         --------
4983         Dataset.integrate
4984         numpy.trapz : corresponding numpy function
4985 
4986         Examples
4987         --------
4988 
4989         >>> da = xr.DataArray(
4990         ...     np.arange(12).reshape(4, 3),
4991         ...     dims=["x", "y"],
4992         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
4993         ... )
4994         >>> da
4995         <xarray.DataArray (x: 4, y: 3)>
4996         array([[ 0,  1,  2],
4997                [ 3,  4,  5],
4998                [ 6,  7,  8],
4999                [ 9, 10, 11]])
5000         Coordinates:
5001           * x        (x) float64 0.0 0.1 1.1 1.2
5002         Dimensions without coordinates: y
5003         >>>
5004         >>> da.integrate("x")
5005         <xarray.DataArray (y: 3)>
5006         array([5.4, 6.6, 7.8])
5007         Dimensions without coordinates: y
5008         """
5009         ds = self._to_temp_dataset().integrate(coord, datetime_unit)
5010         return self._from_temp_dataset(ds)
5011 
5012     # change type of self and return to T_DataArray once
5013     # https://github.com/python/mypy/issues/12846 is resolved
5014     def cumulative_integrate(
5015         self,
5016         coord: Hashable | Sequence[Hashable] = None,
5017         datetime_unit: DatetimeUnitOptions = None,
5018     ) -> DataArray:
5019         """Integrate cumulatively along the given coordinate using the trapezoidal rule.
5020 
5021         .. note::
5022             This feature is limited to simple cartesian geometry, i.e. coord
5023             must be one dimensional.
5024 
5025             The first entry of the cumulative integral is always 0, in order to keep the
5026             length of the dimension unchanged between input and output.
5027 
5028         Parameters
5029         ----------
5030         coord : Hashable, or sequence of Hashable
5031             Coordinate(s) used for the integration.
5032         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
5033                         'ps', 'fs', 'as', None}, optional
5034             Specify the unit if a datetime coordinate is used.
5035 
5036         Returns
5037         -------
5038         integrated : DataArray
5039 
5040         See also
5041         --------
5042         Dataset.cumulative_integrate
5043         scipy.integrate.cumulative_trapezoid : corresponding scipy function
5044 
5045         Examples
5046         --------
5047 
5048         >>> da = xr.DataArray(
5049         ...     np.arange(12).reshape(4, 3),
5050         ...     dims=["x", "y"],
5051         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
5052         ... )
5053         >>> da
5054         <xarray.DataArray (x: 4, y: 3)>
5055         array([[ 0,  1,  2],
5056                [ 3,  4,  5],
5057                [ 6,  7,  8],
5058                [ 9, 10, 11]])
5059         Coordinates:
5060           * x        (x) float64 0.0 0.1 1.1 1.2
5061         Dimensions without coordinates: y
5062         >>>
5063         >>> da.cumulative_integrate("x")
5064         <xarray.DataArray (x: 4, y: 3)>
5065         array([[0.  , 0.  , 0.  ],
5066                [0.15, 0.25, 0.35],
5067                [4.65, 5.75, 6.85],
5068                [5.4 , 6.6 , 7.8 ]])
5069         Coordinates:
5070           * x        (x) float64 0.0 0.1 1.1 1.2
5071         Dimensions without coordinates: y
5072         """
5073         ds = self._to_temp_dataset().cumulative_integrate(coord, datetime_unit)
5074         return self._from_temp_dataset(ds)
5075 
5076     def unify_chunks(self) -> DataArray:
5077         """Unify chunk size along all chunked dimensions of this DataArray.
5078 
5079         Returns
5080         -------
5081         DataArray with consistent chunk sizes for all dask-array variables
5082 
5083         See Also
5084         --------
5085         dask.array.core.unify_chunks
5086         """
5087 
5088         return unify_chunks(self)[0]
5089 
5090     def map_blocks(
5091         self,
5092         func: Callable[..., T_Xarray],
5093         args: Sequence[Any] = (),
5094         kwargs: Mapping[str, Any] | None = None,
5095         template: DataArray | Dataset | None = None,
5096     ) -> T_Xarray:
5097         """
5098         Apply a function to each block of this DataArray.
5099 
5100         .. warning::
5101             This method is experimental and its signature may change.
5102 
5103         Parameters
5104         ----------
5105         func : callable
5106             User-provided function that accepts a DataArray as its first
5107             parameter. The function will receive a subset or 'block' of this DataArray (see below),
5108             corresponding to one chunk along each chunked dimension. ``func`` will be
5109             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.
5110 
5111             This function must return either a single DataArray or a single Dataset.
5112 
5113             This function cannot add a new chunked dimension.
5114         args : sequence
5115             Passed to func after unpacking and subsetting any xarray objects by blocks.
5116             xarray objects in args must be aligned with this object, otherwise an error is raised.
5117         kwargs : mapping
5118             Passed verbatim to func after unpacking. xarray objects, if any, will not be
5119             subset to blocks. Passing dask collections in kwargs is not allowed.
5120         template : DataArray or Dataset, optional
5121             xarray object representing the final result after compute is called. If not provided,
5122             the function will be first run on mocked-up data, that looks like this object but
5123             has sizes 0, to determine properties of the returned object such as dtype,
5124             variable names, attributes, new dimensions and new indexes (if any).
5125             ``template`` must be provided if the function changes the size of existing dimensions.
5126             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
5127             ``attrs`` set by ``func`` will be ignored.
5128 
5129         Returns
5130         -------
5131         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
5132         function.
5133 
5134         Notes
5135         -----
5136         This function is designed for when ``func`` needs to manipulate a whole xarray object
5137         subset to each block. Each block is loaded into memory. In the more common case where
5138         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
5139 
5140         If none of the variables in this object is backed by dask arrays, calling this function is
5141         equivalent to calling ``func(obj, *args, **kwargs)``.
5142 
5143         See Also
5144         --------
5145         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
5146         xarray.DataArray.map_blocks
5147 
5148         Examples
5149         --------
5150         Calculate an anomaly from climatology using ``.groupby()``. Using
5151         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
5152         its indices, and its methods like ``.groupby()``.
5153 
5154         >>> def calculate_anomaly(da, groupby_type="time.month"):
5155         ...     gb = da.groupby(groupby_type)
5156         ...     clim = gb.mean(dim="time")
5157         ...     return gb - clim
5158         ...
5159         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
5160         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
5161         >>> np.random.seed(123)
5162         >>> array = xr.DataArray(
5163         ...     np.random.rand(len(time)),
5164         ...     dims=["time"],
5165         ...     coords={"time": time, "month": month},
5166         ... ).chunk()
5167         >>> array.map_blocks(calculate_anomaly, template=array).compute()
5168         <xarray.DataArray (time: 24)>
5169         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,
5170                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,
5171                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,
5172                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,
5173                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])
5174         Coordinates:
5175           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
5176             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
5177 
5178         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
5179         to the function being applied in ``xr.map_blocks()``:
5180 
5181         >>> array.map_blocks(
5182         ...     calculate_anomaly, kwargs={"groupby_type": "time.year"}, template=array
5183         ... )  # doctest: +ELLIPSIS
5184         <xarray.DataArray (time: 24)>
5185         dask.array<<this-array>-calculate_anomaly, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>
5186         Coordinates:
5187           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
5188             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
5189         """
5190         from xarray.core.parallel import map_blocks
5191 
5192         return map_blocks(func, self, args, kwargs, template)
5193 
5194     def polyfit(
5195         self,
5196         dim: Hashable,
5197         deg: int,
5198         skipna: bool | None = None,
5199         rcond: float | None = None,
5200         w: Hashable | Any | None = None,
5201         full: bool = False,
5202         cov: bool | Literal["unscaled"] = False,
5203     ) -> Dataset:
5204         """
5205         Least squares polynomial fit.
5206 
5207         This replicates the behaviour of `numpy.polyfit` but differs by skipping
5208         invalid values when `skipna = True`.
5209 
5210         Parameters
5211         ----------
5212         dim : Hashable
5213             Coordinate along which to fit the polynomials.
5214         deg : int
5215             Degree of the fitting polynomial.
5216         skipna : bool or None, optional
5217             If True, removes all invalid values before fitting each 1D slices of the array.
5218             Default is True if data is stored in a dask.array or if there is any
5219             invalid values, False otherwise.
5220         rcond : float or None, optional
5221             Relative condition number to the fit.
5222         w : Hashable, array-like or None, optional
5223             Weights to apply to the y-coordinate of the sample points.
5224             Can be an array-like object or the name of a coordinate in the dataset.
5225         full : bool, default: False
5226             Whether to return the residuals, matrix rank and singular values in addition
5227             to the coefficients.
5228         cov : bool or "unscaled", default: False
5229             Whether to return to the covariance matrix in addition to the coefficients.
5230             The matrix is not scaled if `cov='unscaled'`.
5231 
5232         Returns
5233         -------
5234         polyfit_results : Dataset
5235             A single dataset which contains:
5236 
5237             polyfit_coefficients
5238                 The coefficients of the best fit.
5239             polyfit_residuals
5240                 The residuals of the least-square computation (only included if `full=True`).
5241                 When the matrix rank is deficient, np.nan is returned.
5242             [dim]_matrix_rank
5243                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
5244             [dim]_singular_value
5245                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
5246             polyfit_covariance
5247                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
5248 
5249         See Also
5250         --------
5251         numpy.polyfit
5252         numpy.polyval
5253         xarray.polyval
5254         """
5255         return self._to_temp_dataset().polyfit(
5256             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov
5257         )
5258 
5259     def pad(
5260         self: T_DataArray,
5261         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,
5262         mode: PadModeOptions = "constant",
5263         stat_length: int
5264         | tuple[int, int]
5265         | Mapping[Any, tuple[int, int]]
5266         | None = None,
5267         constant_values: float
5268         | tuple[float, float]
5269         | Mapping[Any, tuple[float, float]]
5270         | None = None,
5271         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
5272         reflect_type: PadReflectOptions = None,
5273         keep_attrs: bool | None = None,
5274         **pad_width_kwargs: Any,
5275     ) -> T_DataArray:
5276         """Pad this array along one or more dimensions.
5277 
5278         .. warning::
5279             This function is experimental and its behaviour is likely to change
5280             especially regarding padding of dimension coordinates (or IndexVariables).
5281 
5282         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
5283         coordinates will be padded with the same mode, otherwise coordinates
5284         are padded using the "constant" mode with fill_value dtypes.NA.
5285 
5286         Parameters
5287         ----------
5288         pad_width : mapping of Hashable to tuple of int
5289             Mapping with the form of {dim: (pad_before, pad_after)}
5290             describing the number of values padded along each dimension.
5291             {dim: pad} is a shortcut for pad_before = pad_after = pad
5292         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
5293             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
5294             How to pad the DataArray (taken from numpy docs):
5295 
5296             - "constant": Pads with a constant value.
5297             - "edge": Pads with the edge values of array.
5298             - "linear_ramp": Pads with the linear ramp between end_value and the
5299               array edge value.
5300             - "maximum": Pads with the maximum value of all or part of the
5301               vector along each axis.
5302             - "mean": Pads with the mean value of all or part of the
5303               vector along each axis.
5304             - "median": Pads with the median value of all or part of the
5305               vector along each axis.
5306             - "minimum": Pads with the minimum value of all or part of the
5307               vector along each axis.
5308             - "reflect": Pads with the reflection of the vector mirrored on
5309               the first and last values of the vector along each axis.
5310             - "symmetric": Pads with the reflection of the vector mirrored
5311               along the edge of the array.
5312             - "wrap": Pads with the wrap of the vector along the axis.
5313               The first values are used to pad the end and the
5314               end values are used to pad the beginning.
5315 
5316         stat_length : int, tuple or mapping of Hashable to tuple, default: None
5317             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
5318             values at edge of each axis used to calculate the statistic value.
5319             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
5320             statistic lengths along each dimension.
5321             ((before, after),) yields same before and after statistic lengths
5322             for each dimension.
5323             (stat_length,) or int is a shortcut for before = after = statistic
5324             length for all axes.
5325             Default is ``None``, to use the entire axis.
5326         constant_values : scalar, tuple or mapping of Hashable to tuple, default: 0
5327             Used in 'constant'.  The values to set the padded values for each
5328             axis.
5329             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
5330             pad constants along each dimension.
5331             ``((before, after),)`` yields same before and after constants for each
5332             dimension.
5333             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
5334             all dimensions.
5335             Default is 0.
5336         end_values : scalar, tuple or mapping of Hashable to tuple, default: 0
5337             Used in 'linear_ramp'.  The values used for the ending value of the
5338             linear_ramp and that will form the edge of the padded array.
5339             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
5340             end values along each dimension.
5341             ``((before, after),)`` yields same before and after end values for each
5342             axis.
5343             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
5344             all axes.
5345             Default is 0.
5346         reflect_type : {"even", "odd", None}, optional
5347             Used in "reflect", and "symmetric". The "even" style is the
5348             default with an unaltered reflection around the edge value. For
5349             the "odd" style, the extended part of the array is created by
5350             subtracting the reflected values from two times the edge value.
5351         keep_attrs : bool or None, optional
5352             If True, the attributes (``attrs``) will be copied from the
5353             original object to the new one. If False, the new object
5354             will be returned without attributes.
5355         **pad_width_kwargs
5356             The keyword arguments form of ``pad_width``.
5357             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
5358 
5359         Returns
5360         -------
5361         padded : DataArray
5362             DataArray with the padded coordinates and data.
5363 
5364         See Also
5365         --------
5366         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad
5367 
5368         Notes
5369         -----
5370         For ``mode="constant"`` and ``constant_values=None``, integer types will be
5371         promoted to ``float`` and padded with ``np.nan``.
5372 
5373         Padding coordinates will drop their corresponding index (if any) and will reset default
5374         indexes for dimension coordinates.
5375 
5376         Examples
5377         --------
5378         >>> arr = xr.DataArray([5, 6, 7], coords=[("x", [0, 1, 2])])
5379         >>> arr.pad(x=(1, 2), constant_values=0)
5380         <xarray.DataArray (x: 6)>
5381         array([0, 5, 6, 7, 0, 0])
5382         Coordinates:
5383           * x        (x) float64 nan 0.0 1.0 2.0 nan nan
5384 
5385         >>> da = xr.DataArray(
5386         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],
5387         ...     dims=["x", "y"],
5388         ...     coords={"x": [0, 1], "y": [10, 20, 30, 40], "z": ("x", [100, 200])},
5389         ... )
5390         >>> da.pad(x=1)
5391         <xarray.DataArray (x: 4, y: 4)>
5392         array([[nan, nan, nan, nan],
5393                [ 0.,  1.,  2.,  3.],
5394                [10., 11., 12., 13.],
5395                [nan, nan, nan, nan]])
5396         Coordinates:
5397           * x        (x) float64 nan 0.0 1.0 nan
5398           * y        (y) int64 10 20 30 40
5399             z        (x) float64 nan 100.0 200.0 nan
5400 
5401         Careful, ``constant_values`` are coerced to the data type of the array which may
5402         lead to a loss of precision:
5403 
5404         >>> da.pad(x=1, constant_values=1.23456789)
5405         <xarray.DataArray (x: 4, y: 4)>
5406         array([[ 1,  1,  1,  1],
5407                [ 0,  1,  2,  3],
5408                [10, 11, 12, 13],
5409                [ 1,  1,  1,  1]])
5410         Coordinates:
5411           * x        (x) float64 nan 0.0 1.0 nan
5412           * y        (y) int64 10 20 30 40
5413             z        (x) float64 nan 100.0 200.0 nan
5414         """
5415         ds = self._to_temp_dataset().pad(
5416             pad_width=pad_width,
5417             mode=mode,
5418             stat_length=stat_length,
5419             constant_values=constant_values,
5420             end_values=end_values,
5421             reflect_type=reflect_type,
5422             keep_attrs=keep_attrs,
5423             **pad_width_kwargs,
5424         )
5425         return self._from_temp_dataset(ds)
5426 
5427     def idxmin(
5428         self,
5429         dim: Hashable | None = None,
5430         skipna: bool | None = None,
5431         fill_value: Any = dtypes.NA,
5432         keep_attrs: bool | None = None,
5433     ) -> DataArray:
5434         """Return the coordinate label of the minimum value along a dimension.
5435 
5436         Returns a new `DataArray` named after the dimension with the values of
5437         the coordinate labels along that dimension corresponding to minimum
5438         values along that dimension.
5439 
5440         In comparison to :py:meth:`~DataArray.argmin`, this returns the
5441         coordinate label while :py:meth:`~DataArray.argmin` returns the index.
5442 
5443         Parameters
5444         ----------
5445         dim : str, optional
5446             Dimension over which to apply `idxmin`.  This is optional for 1D
5447             arrays, but required for arrays with 2 or more dimensions.
5448         skipna : bool or None, default: None
5449             If True, skip missing values (as marked by NaN). By default, only
5450             skips missing values for ``float``, ``complex``, and ``object``
5451             dtypes; other dtypes either do not have a sentinel missing value
5452             (``int``) or ``skipna=True`` has not been implemented
5453             (``datetime64`` or ``timedelta64``).
5454         fill_value : Any, default: NaN
5455             Value to be filled in case all of the values along a dimension are
5456             null.  By default this is NaN.  The fill value and result are
5457             automatically converted to a compatible dtype if possible.
5458             Ignored if ``skipna`` is False.
5459         keep_attrs : bool or None, optional
5460             If True, the attributes (``attrs``) will be copied from the
5461             original object to the new one. If False, the new object
5462             will be returned without attributes.
5463 
5464         Returns
5465         -------
5466         reduced : DataArray
5467             New `DataArray` object with `idxmin` applied to its data and the
5468             indicated dimension removed.
5469 
5470         See Also
5471         --------
5472         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin
5473 
5474         Examples
5475         --------
5476         >>> array = xr.DataArray(
5477         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
5478         ... )
5479         >>> array.min()
5480         <xarray.DataArray ()>
5481         array(-2)
5482         >>> array.argmin(...)
5483         {'x': <xarray.DataArray ()>
5484         array(4)}
5485         >>> array.idxmin()
5486         <xarray.DataArray 'x' ()>
5487         array('e', dtype='<U1')
5488 
5489         >>> array = xr.DataArray(
5490         ...     [
5491         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
5492         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
5493         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
5494         ...     ],
5495         ...     dims=["y", "x"],
5496         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
5497         ... )
5498         >>> array.min(dim="x")
5499         <xarray.DataArray (y: 3)>
5500         array([-2., -4.,  1.])
5501         Coordinates:
5502           * y        (y) int64 -1 0 1
5503         >>> array.argmin(dim="x")
5504         <xarray.DataArray (y: 3)>
5505         array([4, 0, 2])
5506         Coordinates:
5507           * y        (y) int64 -1 0 1
5508         >>> array.idxmin(dim="x")
5509         <xarray.DataArray 'x' (y: 3)>
5510         array([16.,  0.,  4.])
5511         Coordinates:
5512           * y        (y) int64 -1 0 1
5513         """
5514         return computation._calc_idxminmax(
5515             array=self,
5516             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),
5517             dim=dim,
5518             skipna=skipna,
5519             fill_value=fill_value,
5520             keep_attrs=keep_attrs,
5521         )
5522 
5523     def idxmax(
5524         self,
5525         dim: Hashable = None,
5526         skipna: bool | None = None,
5527         fill_value: Any = dtypes.NA,
5528         keep_attrs: bool | None = None,
5529     ) -> DataArray:
5530         """Return the coordinate label of the maximum value along a dimension.
5531 
5532         Returns a new `DataArray` named after the dimension with the values of
5533         the coordinate labels along that dimension corresponding to maximum
5534         values along that dimension.
5535 
5536         In comparison to :py:meth:`~DataArray.argmax`, this returns the
5537         coordinate label while :py:meth:`~DataArray.argmax` returns the index.
5538 
5539         Parameters
5540         ----------
5541         dim : Hashable, optional
5542             Dimension over which to apply `idxmax`.  This is optional for 1D
5543             arrays, but required for arrays with 2 or more dimensions.
5544         skipna : bool or None, default: None
5545             If True, skip missing values (as marked by NaN). By default, only
5546             skips missing values for ``float``, ``complex``, and ``object``
5547             dtypes; other dtypes either do not have a sentinel missing value
5548             (``int``) or ``skipna=True`` has not been implemented
5549             (``datetime64`` or ``timedelta64``).
5550         fill_value : Any, default: NaN
5551             Value to be filled in case all of the values along a dimension are
5552             null.  By default this is NaN.  The fill value and result are
5553             automatically converted to a compatible dtype if possible.
5554             Ignored if ``skipna`` is False.
5555         keep_attrs : bool or None, optional
5556             If True, the attributes (``attrs``) will be copied from the
5557             original object to the new one. If False, the new object
5558             will be returned without attributes.
5559 
5560         Returns
5561         -------
5562         reduced : DataArray
5563             New `DataArray` object with `idxmax` applied to its data and the
5564             indicated dimension removed.
5565 
5566         See Also
5567         --------
5568         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax
5569 
5570         Examples
5571         --------
5572         >>> array = xr.DataArray(
5573         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
5574         ... )
5575         >>> array.max()
5576         <xarray.DataArray ()>
5577         array(2)
5578         >>> array.argmax(...)
5579         {'x': <xarray.DataArray ()>
5580         array(1)}
5581         >>> array.idxmax()
5582         <xarray.DataArray 'x' ()>
5583         array('b', dtype='<U1')
5584 
5585         >>> array = xr.DataArray(
5586         ...     [
5587         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
5588         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
5589         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
5590         ...     ],
5591         ...     dims=["y", "x"],
5592         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
5593         ... )
5594         >>> array.max(dim="x")
5595         <xarray.DataArray (y: 3)>
5596         array([2., 2., 1.])
5597         Coordinates:
5598           * y        (y) int64 -1 0 1
5599         >>> array.argmax(dim="x")
5600         <xarray.DataArray (y: 3)>
5601         array([0, 2, 2])
5602         Coordinates:
5603           * y        (y) int64 -1 0 1
5604         >>> array.idxmax(dim="x")
5605         <xarray.DataArray 'x' (y: 3)>
5606         array([0., 4., 4.])
5607         Coordinates:
5608           * y        (y) int64 -1 0 1
5609         """
5610         return computation._calc_idxminmax(
5611             array=self,
5612             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),
5613             dim=dim,
5614             skipna=skipna,
5615             fill_value=fill_value,
5616             keep_attrs=keep_attrs,
5617         )
5618 
5619     # change type of self and return to T_DataArray once
5620     # https://github.com/python/mypy/issues/12846 is resolved
5621     def argmin(
5622         self,
5623         dim: Dims = None,
5624         axis: int | None = None,
5625         keep_attrs: bool | None = None,
5626         skipna: bool | None = None,
5627     ) -> DataArray | dict[Hashable, DataArray]:
5628         """Index or indices of the minimum of the DataArray over one or more dimensions.
5629 
5630         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
5631         which can be passed directly to isel(). If a single str is passed to 'dim' then
5632         returns a DataArray with dtype int.
5633 
5634         If there are multiple minima, the indices of the first one found will be
5635         returned.
5636 
5637         Parameters
5638         ----------
5639         dim : "...", str, Iterable of Hashable or None, optional
5640             The dimensions over which to find the minimum. By default, finds minimum over
5641             all dimensions - for now returning an int for backward compatibility, but
5642             this is deprecated, in future will return a dict with indices for all
5643             dimensions; to return a dict with all dimensions now, pass '...'.
5644         axis : int or None, optional
5645             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
5646             can be supplied.
5647         keep_attrs : bool or None, optional
5648             If True, the attributes (`attrs`) will be copied from the original
5649             object to the new one. If False, the new object will be
5650             returned without attributes.
5651         skipna : bool or None, optional
5652             If True, skip missing values (as marked by NaN). By default, only
5653             skips missing values for float dtypes; other dtypes either do not
5654             have a sentinel missing value (int) or skipna=True has not been
5655             implemented (object, datetime64 or timedelta64).
5656 
5657         Returns
5658         -------
5659         result : DataArray or dict of DataArray
5660 
5661         See Also
5662         --------
5663         Variable.argmin, DataArray.idxmin
5664 
5665         Examples
5666         --------
5667         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
5668         >>> array.min()
5669         <xarray.DataArray ()>
5670         array(-1)
5671         >>> array.argmin(...)
5672         {'x': <xarray.DataArray ()>
5673         array(2)}
5674         >>> array.isel(array.argmin(...))
5675         <xarray.DataArray ()>
5676         array(-1)
5677 
5678         >>> array = xr.DataArray(
5679         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],
5680         ...     dims=("x", "y", "z"),
5681         ... )
5682         >>> array.min(dim="x")
5683         <xarray.DataArray (y: 3, z: 3)>
5684         array([[ 1,  2,  1],
5685                [ 2, -5,  1],
5686                [ 2,  1,  1]])
5687         Dimensions without coordinates: y, z
5688         >>> array.argmin(dim="x")
5689         <xarray.DataArray (y: 3, z: 3)>
5690         array([[1, 0, 0],
5691                [1, 1, 1],
5692                [0, 0, 1]])
5693         Dimensions without coordinates: y, z
5694         >>> array.argmin(dim=["x"])
5695         {'x': <xarray.DataArray (y: 3, z: 3)>
5696         array([[1, 0, 0],
5697                [1, 1, 1],
5698                [0, 0, 1]])
5699         Dimensions without coordinates: y, z}
5700         >>> array.min(dim=("x", "z"))
5701         <xarray.DataArray (y: 3)>
5702         array([ 1, -5,  1])
5703         Dimensions without coordinates: y
5704         >>> array.argmin(dim=["x", "z"])
5705         {'x': <xarray.DataArray (y: 3)>
5706         array([0, 1, 0])
5707         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
5708         array([2, 1, 1])
5709         Dimensions without coordinates: y}
5710         >>> array.isel(array.argmin(dim=["x", "z"]))
5711         <xarray.DataArray (y: 3)>
5712         array([ 1, -5,  1])
5713         Dimensions without coordinates: y
5714         """
5715         result = self.variable.argmin(dim, axis, keep_attrs, skipna)
5716         if isinstance(result, dict):
5717             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
5718         else:
5719             return self._replace_maybe_drop_dims(result)
5720 
5721     # change type of self and return to T_DataArray once
5722     # https://github.com/python/mypy/issues/12846 is resolved
5723     def argmax(
5724         self,
5725         dim: Dims = None,
5726         axis: int | None = None,
5727         keep_attrs: bool | None = None,
5728         skipna: bool | None = None,
5729     ) -> DataArray | dict[Hashable, DataArray]:
5730         """Index or indices of the maximum of the DataArray over one or more dimensions.
5731 
5732         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
5733         which can be passed directly to isel(). If a single str is passed to 'dim' then
5734         returns a DataArray with dtype int.
5735 
5736         If there are multiple maxima, the indices of the first one found will be
5737         returned.
5738 
5739         Parameters
5740         ----------
5741         dim : "...", str, Iterable of Hashable or None, optional
5742             The dimensions over which to find the maximum. By default, finds maximum over
5743             all dimensions - for now returning an int for backward compatibility, but
5744             this is deprecated, in future will return a dict with indices for all
5745             dimensions; to return a dict with all dimensions now, pass '...'.
5746         axis : int or None, optional
5747             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
5748             can be supplied.
5749         keep_attrs : bool or None, optional
5750             If True, the attributes (`attrs`) will be copied from the original
5751             object to the new one. If False, the new object will be
5752             returned without attributes.
5753         skipna : bool or None, optional
5754             If True, skip missing values (as marked by NaN). By default, only
5755             skips missing values for float dtypes; other dtypes either do not
5756             have a sentinel missing value (int) or skipna=True has not been
5757             implemented (object, datetime64 or timedelta64).
5758 
5759         Returns
5760         -------
5761         result : DataArray or dict of DataArray
5762 
5763         See Also
5764         --------
5765         Variable.argmax, DataArray.idxmax
5766 
5767         Examples
5768         --------
5769         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
5770         >>> array.max()
5771         <xarray.DataArray ()>
5772         array(3)
5773         >>> array.argmax(...)
5774         {'x': <xarray.DataArray ()>
5775         array(3)}
5776         >>> array.isel(array.argmax(...))
5777         <xarray.DataArray ()>
5778         array(3)
5779 
5780         >>> array = xr.DataArray(
5781         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],
5782         ...     dims=("x", "y", "z"),
5783         ... )
5784         >>> array.max(dim="x")
5785         <xarray.DataArray (y: 3, z: 3)>
5786         array([[3, 3, 2],
5787                [3, 5, 2],
5788                [2, 3, 3]])
5789         Dimensions without coordinates: y, z
5790         >>> array.argmax(dim="x")
5791         <xarray.DataArray (y: 3, z: 3)>
5792         array([[0, 1, 1],
5793                [0, 1, 0],
5794                [0, 1, 0]])
5795         Dimensions without coordinates: y, z
5796         >>> array.argmax(dim=["x"])
5797         {'x': <xarray.DataArray (y: 3, z: 3)>
5798         array([[0, 1, 1],
5799                [0, 1, 0],
5800                [0, 1, 0]])
5801         Dimensions without coordinates: y, z}
5802         >>> array.max(dim=("x", "z"))
5803         <xarray.DataArray (y: 3)>
5804         array([3, 5, 3])
5805         Dimensions without coordinates: y
5806         >>> array.argmax(dim=["x", "z"])
5807         {'x': <xarray.DataArray (y: 3)>
5808         array([0, 1, 0])
5809         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
5810         array([0, 1, 2])
5811         Dimensions without coordinates: y}
5812         >>> array.isel(array.argmax(dim=["x", "z"]))
5813         <xarray.DataArray (y: 3)>
5814         array([3, 5, 3])
5815         Dimensions without coordinates: y
5816         """
5817         result = self.variable.argmax(dim, axis, keep_attrs, skipna)
5818         if isinstance(result, dict):
5819             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
5820         else:
5821             return self._replace_maybe_drop_dims(result)
5822 
5823     def query(
5824         self,
5825         queries: Mapping[Any, Any] | None = None,
5826         parser: QueryParserOptions = "pandas",
5827         engine: QueryEngineOptions = None,
5828         missing_dims: ErrorOptionsWithWarn = "raise",
5829         **queries_kwargs: Any,
5830     ) -> DataArray:
5831         """Return a new data array indexed along the specified
5832         dimension(s), where the indexers are given as strings containing
5833         Python expressions to be evaluated against the values in the array.
5834 
5835         Parameters
5836         ----------
5837         queries : dict-like or None, optional
5838             A dict-like with keys matching dimensions and values given by strings
5839             containing Python expressions to be evaluated against the data variables
5840             in the dataset. The expressions will be evaluated using the pandas
5841             eval() function, and can contain any valid Python expressions but cannot
5842             contain any Python statements.
5843         parser : {"pandas", "python"}, default: "pandas"
5844             The parser to use to construct the syntax tree from the expression.
5845             The default of 'pandas' parses code slightly different than standard
5846             Python. Alternatively, you can parse an expression using the 'python'
5847             parser to retain strict Python semantics.
5848         engine : {"python", "numexpr", None}, default: None
5849             The engine used to evaluate the expression. Supported engines are:
5850 
5851             - None: tries to use numexpr, falls back to python
5852             - "numexpr": evaluates expressions using numexpr
5853             - "python": performs operations as if you had eval’d in top level python
5854 
5855         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
5856             What to do if dimensions that should be selected from are not present in the
5857             DataArray:
5858 
5859             - "raise": raise an exception
5860             - "warn": raise a warning, and ignore the missing dimensions
5861             - "ignore": ignore the missing dimensions
5862 
5863         **queries_kwargs : {dim: query, ...}, optional
5864             The keyword arguments form of ``queries``.
5865             One of queries or queries_kwargs must be provided.
5866 
5867         Returns
5868         -------
5869         obj : DataArray
5870             A new DataArray with the same contents as this dataset, indexed by
5871             the results of the appropriate queries.
5872 
5873         See Also
5874         --------
5875         DataArray.isel
5876         Dataset.query
5877         pandas.eval
5878 
5879         Examples
5880         --------
5881         >>> da = xr.DataArray(np.arange(0, 5, 1), dims="x", name="a")
5882         >>> da
5883         <xarray.DataArray 'a' (x: 5)>
5884         array([0, 1, 2, 3, 4])
5885         Dimensions without coordinates: x
5886         >>> da.query(x="a > 2")
5887         <xarray.DataArray 'a' (x: 2)>
5888         array([3, 4])
5889         Dimensions without coordinates: x
5890         """
5891 
5892         ds = self._to_dataset_whole(shallow_copy=True)
5893         ds = ds.query(
5894             queries=queries,
5895             parser=parser,
5896             engine=engine,
5897             missing_dims=missing_dims,
5898             **queries_kwargs,
5899         )
5900         return ds[self.name]
5901 
5902     def curvefit(
5903         self,
5904         coords: str | DataArray | Iterable[str | DataArray],
5905         func: Callable[..., Any],
5906         reduce_dims: Dims = None,
5907         skipna: bool = True,
5908         p0: dict[str, Any] | None = None,
5909         bounds: dict[str, Any] | None = None,
5910         param_names: Sequence[str] | None = None,
5911         kwargs: dict[str, Any] | None = None,
5912     ) -> Dataset:
5913         """
5914         Curve fitting optimization for arbitrary functions.
5915 
5916         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
5917 
5918         Parameters
5919         ----------
5920         coords : Hashable, DataArray, or sequence of DataArray or Hashable
5921             Independent coordinate(s) over which to perform the curve fitting. Must share
5922             at least one dimension with the calling object. When fitting multi-dimensional
5923             functions, supply `coords` as a sequence in the same order as arguments in
5924             `func`. To fit along existing dimensions of the calling object, `coords` can
5925             also be specified as a str or sequence of strs.
5926         func : callable
5927             User specified function in the form `f(x, *params)` which returns a numpy
5928             array of length `len(x)`. `params` are the fittable parameters which are optimized
5929             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
5930             coordinates, e.g. `f((x0, x1), *params)`.
5931         reduce_dims : str, Iterable of Hashable or None, optional
5932             Additional dimension(s) over which to aggregate while fitting. For example,
5933             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
5934             aggregate all lat and lon points and fit the specified function along the
5935             time dimension.
5936         skipna : bool, default: True
5937             Whether to skip missing values when fitting. Default is True.
5938         p0 : dict-like or None, optional
5939             Optional dictionary of parameter names to initial guesses passed to the
5940             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
5941             be assigned initial values following the default scipy behavior.
5942         bounds : dict-like or None, optional
5943             Optional dictionary of parameter names to bounding values passed to the
5944             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
5945             will be unbounded following the default scipy behavior.
5946         param_names : sequence of Hashable or None, optional
5947             Sequence of names for the fittable parameters of `func`. If not supplied,
5948             this will be automatically determined by arguments of `func`. `param_names`
5949             should be manually supplied when fitting a function that takes a variable
5950             number of parameters.
5951         **kwargs : optional
5952             Additional keyword arguments to passed to scipy curve_fit.
5953 
5954         Returns
5955         -------
5956         curvefit_results : Dataset
5957             A single dataset which contains:
5958 
5959             [var]_curvefit_coefficients
5960                 The coefficients of the best fit.
5961             [var]_curvefit_covariance
5962                 The covariance matrix of the coefficient estimates.
5963 
5964         See Also
5965         --------
5966         DataArray.polyfit
5967         scipy.optimize.curve_fit
5968         """
5969         return self._to_temp_dataset().curvefit(
5970             coords,
5971             func,
5972             reduce_dims=reduce_dims,
5973             skipna=skipna,
5974             p0=p0,
5975             bounds=bounds,
5976             param_names=param_names,
5977             kwargs=kwargs,
5978         )
5979 
5980     def drop_duplicates(
5981         self: T_DataArray,
5982         dim: Hashable | Iterable[Hashable],
5983         keep: Literal["first", "last", False] = "first",
5984     ) -> T_DataArray:
5985         """Returns a new DataArray with duplicate dimension values removed.
5986 
5987         Parameters
5988         ----------
5989         dim : dimension label or labels
5990             Pass `...` to drop duplicates along all dimensions.
5991         keep : {"first", "last", False}, default: "first"
5992             Determines which duplicates (if any) to keep.
5993 
5994             - ``"first"`` : Drop duplicates except for the first occurrence.
5995             - ``"last"`` : Drop duplicates except for the last occurrence.
5996             - False : Drop all duplicates.
5997 
5998         Returns
5999         -------
6000         DataArray
6001 
6002         See Also
6003         --------
6004         Dataset.drop_duplicates
6005 
6006         Examples
6007         --------
6008         >>> da = xr.DataArray(
6009         ...     np.arange(25).reshape(5, 5),
6010         ...     dims=("x", "y"),
6011         ...     coords={"x": np.array([0, 0, 1, 2, 3]), "y": np.array([0, 1, 2, 3, 3])},
6012         ... )
6013         >>> da
6014         <xarray.DataArray (x: 5, y: 5)>
6015         array([[ 0,  1,  2,  3,  4],
6016                [ 5,  6,  7,  8,  9],
6017                [10, 11, 12, 13, 14],
6018                [15, 16, 17, 18, 19],
6019                [20, 21, 22, 23, 24]])
6020         Coordinates:
6021           * x        (x) int64 0 0 1 2 3
6022           * y        (y) int64 0 1 2 3 3
6023 
6024         >>> da.drop_duplicates(dim="x")
6025         <xarray.DataArray (x: 4, y: 5)>
6026         array([[ 0,  1,  2,  3,  4],
6027                [10, 11, 12, 13, 14],
6028                [15, 16, 17, 18, 19],
6029                [20, 21, 22, 23, 24]])
6030         Coordinates:
6031           * x        (x) int64 0 1 2 3
6032           * y        (y) int64 0 1 2 3 3
6033 
6034         >>> da.drop_duplicates(dim="x", keep="last")
6035         <xarray.DataArray (x: 4, y: 5)>
6036         array([[ 5,  6,  7,  8,  9],
6037                [10, 11, 12, 13, 14],
6038                [15, 16, 17, 18, 19],
6039                [20, 21, 22, 23, 24]])
6040         Coordinates:
6041           * x        (x) int64 0 1 2 3
6042           * y        (y) int64 0 1 2 3 3
6043 
6044         Drop all duplicate dimension values:
6045 
6046         >>> da.drop_duplicates(dim=...)
6047         <xarray.DataArray (x: 4, y: 4)>
6048         array([[ 0,  1,  2,  3],
6049                [10, 11, 12, 13],
6050                [15, 16, 17, 18],
6051                [20, 21, 22, 23]])
6052         Coordinates:
6053           * x        (x) int64 0 1 2 3
6054           * y        (y) int64 0 1 2 3
6055         """
6056         deduplicated = self._to_temp_dataset().drop_duplicates(dim, keep=keep)
6057         return self._from_temp_dataset(deduplicated)
6058 
6059     def convert_calendar(
6060         self,
6061         calendar: str,
6062         dim: str = "time",
6063         align_on: str | None = None,
6064         missing: Any | None = None,
6065         use_cftime: bool | None = None,
6066     ) -> DataArray:
6067         """Convert the DataArray to another calendar.
6068 
6069         Only converts the individual timestamps, does not modify any data except
6070         in dropping invalid/surplus dates or inserting missing dates.
6071 
6072         If the source and target calendars are either no_leap, all_leap or a
6073         standard type, only the type of the time array is modified.
6074         When converting to a leap year from a non-leap year, the 29th of February
6075         is removed from the array. In the other direction the 29th of February
6076         will be missing in the output, unless `missing` is specified,
6077         in which case that value is inserted.
6078 
6079         For conversions involving `360_day` calendars, see Notes.
6080 
6081         This method is safe to use with sub-daily data as it doesn't touch the
6082         time part of the timestamps.
6083 
6084         Parameters
6085         ---------
6086         calendar : str
6087             The target calendar name.
6088         dim : str
6089             Name of the time coordinate.
6090         align_on : {None, 'date', 'year'}
6091             Must be specified when either source or target is a `360_day` calendar,
6092            ignored otherwise. See Notes.
6093         missing : Optional[any]
6094             By default, i.e. if the value is None, this method will simply attempt
6095             to convert the dates in the source calendar to the same dates in the
6096             target calendar, and drop any of those that are not possible to
6097             represent.  If a value is provided, a new time coordinate will be
6098             created in the target calendar with the same frequency as the original
6099             time coordinate; for any dates that are not present in the source, the
6100             data will be filled with this value.  Note that using this mode requires
6101             that the source data have an inferable frequency; for more information
6102             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
6103             target calendar combinations, this could result in many missing values, see notes.
6104         use_cftime : boolean, optional
6105             Whether to use cftime objects in the output, only used if `calendar`
6106             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
6107             If True, the new time axis uses cftime objects.
6108             If None (default), it uses :py:class:`numpy.datetime64` values if the
6109             date range permits it, and :py:class:`cftime.datetime` objects if not.
6110             If False, it uses :py:class:`numpy.datetime64`  or fails.
6111 
6112         Returns
6113         -------
6114         DataArray
6115             Copy of the dataarray with the time coordinate converted to the
6116             target calendar. If 'missing' was None (default), invalid dates in
6117             the new calendar are dropped, but missing dates are not inserted.
6118             If `missing` was given, the new data is reindexed to have a time axis
6119             with the same frequency as the source, but in the new calendar; any
6120             missing datapoints are filled with `missing`.
6121 
6122         Notes
6123         -----
6124         Passing a value to `missing` is only usable if the source's time coordinate as an
6125         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
6126         if the target coordinate, generated from this frequency, has dates equivalent to the
6127         source. It is usually **not** appropriate to use this mode with:
6128 
6129         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
6130         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
6131             or 'mH' where 24 % m != 0).
6132 
6133         If one of the source or target calendars is `"360_day"`, `align_on` must
6134         be specified and two options are offered.
6135 
6136         - "year"
6137             The dates are translated according to their relative position in the year,
6138             ignoring their original month and day information, meaning that the
6139             missing/surplus days are added/removed at regular intervals.
6140 
6141             From a `360_day` to a standard calendar, the output will be missing the
6142             following dates (day of year in parentheses):
6143 
6144             To a leap year:
6145                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
6146                 September 31st (275) and November 30th (335).
6147             To a non-leap year:
6148                 February 6th (36), April 19th (109), July 2nd (183),
6149                 September 12th (255), November 25th (329).
6150 
6151             From a standard calendar to a `"360_day"`, the following dates in the
6152             source array will be dropped:
6153 
6154             From a leap year:
6155                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
6156                 September 31st (275), December 1st (336)
6157             From a non-leap year:
6158                 February 6th (37), April 20th (110), July 2nd (183),
6159                 September 13th (256), November 25th (329)
6160 
6161             This option is best used on daily and subdaily data.
6162 
6163         - "date"
6164             The month/day information is conserved and invalid dates are dropped
6165             from the output. This means that when converting from a `"360_day"` to a
6166             standard calendar, all 31st (Jan, March, May, July, August, October and
6167             December) will be missing as there is no equivalent dates in the
6168             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
6169             will be dropped as there are no equivalent dates in a standard calendar.
6170 
6171             This option is best used with data on a frequency coarser than daily.
6172         """
6173         return convert_calendar(
6174             self,
6175             calendar,
6176             dim=dim,
6177             align_on=align_on,
6178             missing=missing,
6179             use_cftime=use_cftime,
6180         )
6181 
6182     def interp_calendar(
6183         self,
6184         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
6185         dim: str = "time",
6186     ) -> DataArray:
6187         """Interpolates the DataArray to another calendar based on decimal year measure.
6188 
6189         Each timestamp in `source` and `target` are first converted to their decimal
6190         year equivalent then `source` is interpolated on the target coordinate.
6191         The decimal year of a timestamp is its year plus its sub-year component
6192         converted to the fraction of its year. For example "2000-03-01 12:00" is
6193         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
6194 
6195         This method should only be used when the time (HH:MM:SS) information of
6196         time coordinate is not important.
6197 
6198         Parameters
6199         ----------
6200         target: DataArray or DatetimeIndex or CFTimeIndex
6201             The target time coordinate of a valid dtype
6202             (np.datetime64 or cftime objects)
6203         dim : str
6204             The time coordinate name.
6205 
6206         Return
6207         ------
6208         DataArray
6209             The source interpolated on the decimal years of target,
6210         """
6211         return interp_calendar(self, target, dim=dim)
6212 
6213     def groupby(
6214         self,
6215         group: Hashable | DataArray | IndexVariable,
6216         squeeze: bool = True,
6217         restore_coord_dims: bool = False,
6218     ) -> DataArrayGroupBy:
6219         """Returns a DataArrayGroupBy object for performing grouped operations.
6220 
6221         Parameters
6222         ----------
6223         group : Hashable, DataArray or IndexVariable
6224             Array whose unique values should be used to group this array. If a
6225             Hashable, must be the name of a coordinate contained in this dataarray.
6226         squeeze : bool, default: True
6227             If "group" is a dimension of any arrays in this dataset, `squeeze`
6228             controls whether the subarrays have a dimension of length 1 along
6229             that dimension or if the dimension is squeezed out.
6230         restore_coord_dims : bool, default: False
6231             If True, also restore the dimension order of multi-dimensional
6232             coordinates.
6233 
6234         Returns
6235         -------
6236         grouped : DataArrayGroupBy
6237             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
6238             iterated over in the form of `(unique_value, grouped_array)` pairs.
6239 
6240         Examples
6241         --------
6242         Calculate daily anomalies for daily data:
6243 
6244         >>> da = xr.DataArray(
6245         ...     np.linspace(0, 1826, num=1827),
6246         ...     coords=[pd.date_range("2000-01-01", "2004-12-31", freq="D")],
6247         ...     dims="time",
6248         ... )
6249         >>> da
6250         <xarray.DataArray (time: 1827)>
6251         array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03,
6252                1.826e+03])
6253         Coordinates:
6254           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
6255         >>> da.groupby("time.dayofyear") - da.groupby("time.dayofyear").mean("time")
6256         <xarray.DataArray (time: 1827)>
6257         array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])
6258         Coordinates:
6259           * time       (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
6260             dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 359 360 361 362 363 364 365 366
6261 
6262         See Also
6263         --------
6264         DataArray.groupby_bins
6265         Dataset.groupby
6266         core.groupby.DataArrayGroupBy
6267         pandas.DataFrame.groupby
6268         """
6269         from xarray.core.groupby import DataArrayGroupBy
6270 
6271         # While we don't generally check the type of every arg, passing
6272         # multiple dimensions as multiple arguments is common enough, and the
6273         # consequences hidden enough (strings evaluate as true) to warrant
6274         # checking here.
6275         # A future version could make squeeze kwarg only, but would face
6276         # backward-compat issues.
6277         if not isinstance(squeeze, bool):
6278             raise TypeError(
6279                 f"`squeeze` must be True or False, but {squeeze} was supplied"
6280             )
6281 
6282         return DataArrayGroupBy(
6283             self, group, squeeze=squeeze, restore_coord_dims=restore_coord_dims
6284         )
6285 
6286     def groupby_bins(
6287         self,
6288         group: Hashable | DataArray | IndexVariable,
6289         bins: ArrayLike,
6290         right: bool = True,
6291         labels: ArrayLike | Literal[False] | None = None,
6292         precision: int = 3,
6293         include_lowest: bool = False,
6294         squeeze: bool = True,
6295         restore_coord_dims: bool = False,
6296     ) -> DataArrayGroupBy:
6297         """Returns a DataArrayGroupBy object for performing grouped operations.
6298 
6299         Rather than using all unique values of `group`, the values are discretized
6300         first by applying `pandas.cut` [1]_ to `group`.
6301 
6302         Parameters
6303         ----------
6304         group : Hashable, DataArray or IndexVariable
6305             Array whose binned values should be used to group this array. If a
6306             Hashable, must be the name of a coordinate contained in this dataarray.
6307         bins : int or array-like
6308             If bins is an int, it defines the number of equal-width bins in the
6309             range of x. However, in this case, the range of x is extended by .1%
6310             on each side to include the min or max values of x. If bins is a
6311             sequence it defines the bin edges allowing for non-uniform bin
6312             width. No extension of the range of x is done in this case.
6313         right : bool, default: True
6314             Indicates whether the bins include the rightmost edge or not. If
6315             right == True (the default), then the bins [1,2,3,4] indicate
6316             (1,2], (2,3], (3,4].
6317         labels : array-like, False or None, default: None
6318             Used as labels for the resulting bins. Must be of the same length as
6319             the resulting bins. If False, string bin labels are assigned by
6320             `pandas.cut`.
6321         precision : int, default: 3
6322             The precision at which to store and display the bins labels.
6323         include_lowest : bool, default: False
6324             Whether the first interval should be left-inclusive or not.
6325         squeeze : bool, default: True
6326             If "group" is a dimension of any arrays in this dataset, `squeeze`
6327             controls whether the subarrays have a dimension of length 1 along
6328             that dimension or if the dimension is squeezed out.
6329         restore_coord_dims : bool, default: False
6330             If True, also restore the dimension order of multi-dimensional
6331             coordinates.
6332 
6333         Returns
6334         -------
6335         grouped : DataArrayGroupBy
6336             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
6337             iterated over in the form of `(unique_value, grouped_array)` pairs.
6338             The name of the group has the added suffix `_bins` in order to
6339             distinguish it from the original variable.
6340 
6341         See Also
6342         --------
6343         DataArray.groupby
6344         Dataset.groupby_bins
6345         core.groupby.DataArrayGroupBy
6346         pandas.DataFrame.groupby
6347 
6348         References
6349         ----------
6350         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
6351         """
6352         from xarray.core.groupby import DataArrayGroupBy
6353 
6354         return DataArrayGroupBy(
6355             self,
6356             group,
6357             squeeze=squeeze,
6358             bins=bins,
6359             restore_coord_dims=restore_coord_dims,
6360             cut_kwargs={
6361                 "right": right,
6362                 "labels": labels,
6363                 "precision": precision,
6364                 "include_lowest": include_lowest,
6365             },
6366         )
6367 
6368     def weighted(self, weights: DataArray) -> DataArrayWeighted:
6369         """
6370         Weighted DataArray operations.
6371 
6372         Parameters
6373         ----------
6374         weights : DataArray
6375             An array of weights associated with the values in this Dataset.
6376             Each value in the data contributes to the reduction operation
6377             according to its associated weight.
6378 
6379         Notes
6380         -----
6381         ``weights`` must be a DataArray and cannot contain missing values.
6382         Missing values can be replaced by ``weights.fillna(0)``.
6383 
6384         Returns
6385         -------
6386         core.weighted.DataArrayWeighted
6387 
6388         See Also
6389         --------
6390         Dataset.weighted
6391         """
6392         from xarray.core.weighted import DataArrayWeighted
6393 
6394         return DataArrayWeighted(self, weights)
6395 
6396     def rolling(
6397         self,
6398         dim: Mapping[Any, int] | None = None,
6399         min_periods: int | None = None,
6400         center: bool | Mapping[Any, bool] = False,
6401         **window_kwargs: int,
6402     ) -> DataArrayRolling:
6403         """
6404         Rolling window object for DataArrays.
6405 
6406         Parameters
6407         ----------
6408         dim : dict, optional
6409             Mapping from the dimension name to create the rolling iterator
6410             along (e.g. `time`) to its moving window size.
6411         min_periods : int or None, default: None
6412             Minimum number of observations in window required to have a value
6413             (otherwise result is NA). The default, None, is equivalent to
6414             setting min_periods equal to the size of the window.
6415         center : bool or Mapping to int, default: False
6416             Set the labels at the center of the window.
6417         **window_kwargs : optional
6418             The keyword arguments form of ``dim``.
6419             One of dim or window_kwargs must be provided.
6420 
6421         Returns
6422         -------
6423         core.rolling.DataArrayRolling
6424 
6425         Examples
6426         --------
6427         Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:
6428 
6429         >>> da = xr.DataArray(
6430         ...     np.linspace(0, 11, num=12),
6431         ...     coords=[
6432         ...         pd.date_range(
6433         ...             "1999-12-15",
6434         ...             periods=12,
6435         ...             freq=pd.DateOffset(months=1),
6436         ...         )
6437         ...     ],
6438         ...     dims="time",
6439         ... )
6440         >>> da
6441         <xarray.DataArray (time: 12)>
6442         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
6443         Coordinates:
6444           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6445         >>> da.rolling(time=3, center=True).mean()
6446         <xarray.DataArray (time: 12)>
6447         array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])
6448         Coordinates:
6449           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6450 
6451         Remove the NaNs using ``dropna()``:
6452 
6453         >>> da.rolling(time=3, center=True).mean().dropna("time")
6454         <xarray.DataArray (time: 10)>
6455         array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])
6456         Coordinates:
6457           * time     (time) datetime64[ns] 2000-01-15 2000-02-15 ... 2000-10-15
6458 
6459         See Also
6460         --------
6461         core.rolling.DataArrayRolling
6462         Dataset.rolling
6463         """
6464         from xarray.core.rolling import DataArrayRolling
6465 
6466         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
6467         return DataArrayRolling(self, dim, min_periods=min_periods, center=center)
6468 
6469     def coarsen(
6470         self,
6471         dim: Mapping[Any, int] | None = None,
6472         boundary: CoarsenBoundaryOptions = "exact",
6473         side: SideOptions | Mapping[Any, SideOptions] = "left",
6474         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
6475         **window_kwargs: int,
6476     ) -> DataArrayCoarsen:
6477         """
6478         Coarsen object for DataArrays.
6479 
6480         Parameters
6481         ----------
6482         dim : mapping of hashable to int, optional
6483             Mapping from the dimension name to the window size.
6484         boundary : {"exact", "trim", "pad"}, default: "exact"
6485             If 'exact', a ValueError will be raised if dimension size is not a
6486             multiple of the window size. If 'trim', the excess entries are
6487             dropped. If 'pad', NA will be padded.
6488         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
6489         coord_func : str or mapping of hashable to str, default: "mean"
6490             function (name) that is applied to the coordinates,
6491             or a mapping from coordinate name to function (name).
6492 
6493         Returns
6494         -------
6495         core.rolling.DataArrayCoarsen
6496 
6497         Examples
6498         --------
6499         Coarsen the long time series by averaging over every four days.
6500 
6501         >>> da = xr.DataArray(
6502         ...     np.linspace(0, 364, num=364),
6503         ...     dims="time",
6504         ...     coords={"time": pd.date_range("1999-12-15", periods=364)},
6505         ... )
6506         >>> da  # +doctest: ELLIPSIS
6507         <xarray.DataArray (time: 364)>
6508         array([  0.        ,   1.00275482,   2.00550964,   3.00826446,
6509                  4.01101928,   5.0137741 ,   6.01652893,   7.01928375,
6510                  8.02203857,   9.02479339,  10.02754821,  11.03030303,
6511         ...
6512                356.98071625, 357.98347107, 358.9862259 , 359.98898072,
6513                360.99173554, 361.99449036, 362.99724518, 364.        ])
6514         Coordinates:
6515           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-12-12
6516         >>> da.coarsen(time=3, boundary="trim").mean()  # +doctest: ELLIPSIS
6517         <xarray.DataArray (time: 121)>
6518         array([  1.00275482,   4.01101928,   7.01928375,  10.02754821,
6519                 13.03581267,  16.04407713,  19.0523416 ,  22.06060606,
6520                 25.06887052,  28.07713499,  31.08539945,  34.09366391,
6521         ...
6522                349.96143251, 352.96969697, 355.97796143, 358.9862259 ,
6523                361.99449036])
6524         Coordinates:
6525           * time     (time) datetime64[ns] 1999-12-16 1999-12-19 ... 2000-12-10
6526         >>>
6527 
6528         See Also
6529         --------
6530         core.rolling.DataArrayCoarsen
6531         Dataset.coarsen
6532         """
6533         from xarray.core.rolling import DataArrayCoarsen
6534 
6535         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
6536         return DataArrayCoarsen(
6537             self,
6538             dim,
6539             boundary=boundary,
6540             side=side,
6541             coord_func=coord_func,
6542         )
6543 
6544     def resample(
6545         self,
6546         indexer: Mapping[Any, str] | None = None,
6547         skipna: bool | None = None,
6548         closed: SideOptions | None = None,
6549         label: SideOptions | None = None,
6550         base: int | None = None,
6551         offset: pd.Timedelta | datetime.timedelta | str | None = None,
6552         origin: str | DatetimeLike = "start_day",
6553         keep_attrs: bool | None = None,
6554         loffset: datetime.timedelta | str | None = None,
6555         restore_coord_dims: bool | None = None,
6556         **indexer_kwargs: str,
6557     ) -> DataArrayResample:
6558         """Returns a Resample object for performing resampling operations.
6559 
6560         Handles both downsampling and upsampling. The resampled
6561         dimension must be a datetime-like coordinate. If any intervals
6562         contain no values from the original object, they will be given
6563         the value ``NaN``.
6564 
6565         Parameters
6566         ----------
6567         indexer : Mapping of Hashable to str, optional
6568             Mapping from the dimension name to resample frequency [1]_. The
6569             dimension must be datetime-like.
6570         skipna : bool, optional
6571             Whether to skip missing values when aggregating in downsampling.
6572         closed : {"left", "right"}, optional
6573             Side of each interval to treat as closed.
6574         label : {"left", "right"}, optional
6575             Side of each interval to use for labeling.
6576         base : int, optional
6577             For frequencies that evenly subdivide 1 day, the "origin" of the
6578             aggregated intervals. For example, for "24H" frequency, base could
6579             range from 0 through 23.
6580         origin : {'epoch', 'start', 'start_day', 'end', 'end_day'}, pd.Timestamp, datetime.datetime, np.datetime64, or cftime.datetime, default 'start_day'
6581             The datetime on which to adjust the grouping. The timezone of origin
6582             must match the timezone of the index.
6583 
6584             If a datetime is not used, these values are also supported:
6585             - 'epoch': `origin` is 1970-01-01
6586             - 'start': `origin` is the first value of the timeseries
6587             - 'start_day': `origin` is the first day at midnight of the timeseries
6588             - 'end': `origin` is the last value of the timeseries
6589             - 'end_day': `origin` is the ceiling midnight of the last day
6590         offset : pd.Timedelta, datetime.timedelta, or str, default is None
6591             An offset timedelta added to the origin.
6592         loffset : timedelta or str, optional
6593             Offset used to adjust the resampled time labels. Some pandas date
6594             offset strings are supported.
6595         restore_coord_dims : bool, optional
6596             If True, also restore the dimension order of multi-dimensional
6597             coordinates.
6598         **indexer_kwargs : str
6599             The keyword arguments form of ``indexer``.
6600             One of indexer or indexer_kwargs must be provided.
6601 
6602         Returns
6603         -------
6604         resampled : core.resample.DataArrayResample
6605             This object resampled.
6606 
6607         Examples
6608         --------
6609         Downsample monthly time-series data to seasonal data:
6610 
6611         >>> da = xr.DataArray(
6612         ...     np.linspace(0, 11, num=12),
6613         ...     coords=[
6614         ...         pd.date_range(
6615         ...             "1999-12-15",
6616         ...             periods=12,
6617         ...             freq=pd.DateOffset(months=1),
6618         ...         )
6619         ...     ],
6620         ...     dims="time",
6621         ... )
6622         >>> da
6623         <xarray.DataArray (time: 12)>
6624         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
6625         Coordinates:
6626           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6627         >>> da.resample(time="QS-DEC").mean()
6628         <xarray.DataArray (time: 4)>
6629         array([ 1.,  4.,  7., 10.])
6630         Coordinates:
6631           * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01
6632 
6633         Upsample monthly time-series data to daily data:
6634 
6635         >>> da.resample(time="1D").interpolate("linear")  # +doctest: ELLIPSIS
6636         <xarray.DataArray (time: 337)>
6637         array([ 0.        ,  0.03225806,  0.06451613,  0.09677419,  0.12903226,
6638                 0.16129032,  0.19354839,  0.22580645,  0.25806452,  0.29032258,
6639                 0.32258065,  0.35483871,  0.38709677,  0.41935484,  0.4516129 ,
6640         ...
6641                10.80645161, 10.83870968, 10.87096774, 10.90322581, 10.93548387,
6642                10.96774194, 11.        ])
6643         Coordinates:
6644           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
6645 
6646         Limit scope of upsampling method
6647 
6648         >>> da.resample(time="1D").nearest(tolerance="1D")
6649         <xarray.DataArray (time: 337)>
6650         array([ 0.,  0., nan, ..., nan, 11., 11.])
6651         Coordinates:
6652           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
6653 
6654         See Also
6655         --------
6656         Dataset.resample
6657         pandas.Series.resample
6658         pandas.DataFrame.resample
6659 
6660         References
6661         ----------
6662         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
6663         """
6664         from xarray.core.resample import DataArrayResample
6665 
6666         return self._resample(
6667             resample_cls=DataArrayResample,
6668             indexer=indexer,
6669             skipna=skipna,
6670             closed=closed,
6671             label=label,
6672             base=base,
6673             offset=offset,
6674             origin=origin,
6675             keep_attrs=keep_attrs,
6676             loffset=loffset,
6677             restore_coord_dims=restore_coord_dims,
6678             **indexer_kwargs,
6679         )
6680 
6681     # this needs to be at the end, or mypy will confuse with `str`
6682     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
6683     str = utils.UncachedAccessor(StringAccessor["DataArray"])

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 04:22:23.950 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 04:22:23.950 | INFO     | Search-only: skipping patch selection
2025-10-18 04:22:24.042 | INFO     | Task pydata__xarray-7393 completed successfully.
2025-10-18 04:22:24.043 | INFO     | No patch generated. You can try running ACR again.
