2025-10-17 19:16:52.932 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-17 19:16:55.606 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=364, output_tokens=12, cost=0.004000
2025-10-17 19:16:55.607 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-17 19:16:55.611 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 19:17:02.765 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=671, output_tokens=316, cost=0.016190
2025-10-17 19:17:02.770 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 19:17:05.504 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=670, output_tokens=52, cost=0.008260
2025-10-17 19:17:05.505 | DEBUG    | Extracted a valid json.
2025-10-17 19:17:05.506 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"LinkChecker\")",
        "search_method(\"handle_redirect\")",
        "search_method(\"report_error\")",
        "search_code(\"allowed redirects\")"
    ],
    "bug_locations": []
}
2025-10-17 19:17:05.521 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-17 19:17:05.522 | DEBUG    | <Agent search> Analyzing search results.
2025-10-17 19:17:17.581 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1178, output_tokens=473, cost=0.025970
2025-10-17 19:17:17.592 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 19:17:19.154 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1867, output_tokens=28, cost=0.019510
2025-10-17 19:17:19.157 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 19:17:23.180 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=382, output_tokens=48, cost=0.005260
2025-10-17 19:17:23.181 | DEBUG    | Extracted a valid json.
2025-10-17 19:17:23.181 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_method(\"get\")",
        "search_method(\"post\")",
        "search_code(\"HTTP\")",
        "search_code(\"error\")"
    ],
    "bug_locations": []
}
2025-10-17 19:17:23.447 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-17 19:17:23.447 | DEBUG    | <Agent search> Analyzing search results.
2025-10-17 19:17:36.804 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3523, output_tokens=629, cost=0.054100
2025-10-17 19:17:36.811 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 19:17:45.268 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=4368, output_tokens=378, cost=0.055020
2025-10-17 19:17:45.276 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 19:17:51.213 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=732, output_tokens=196, cost=0.013200
2025-10-17 19:17:51.214 | DEBUG    | Extracted a valid json.
2025-10-17 19:17:51.214 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "sphinx/util/requests.py",
            "class": "",
            "method": "get",
            "intended_behavior": "This method should be enhanced to allow tracking of redirects during the link checking process. It should check if the redirect URL matches an entry in a predefined list of acceptable redirects. If not, the redirect should be reported as a 'bad' redirect, while preserving existing functionality for making HTTP requests and handling typical responses."
        },
        {
            "file": "sphinx/builders/linkcheck.py",
            "class": "",
            "method": "",
            "intended_behavior": "This file requires modification to check the final URL (post-redirect) against a predefined list of acceptable redirects and report an error or specific warning for non-matching redirects. This enhancement should integrate with the current link verification process without impacting its functionality."
        }
    ]
}
2025-10-17 19:17:51.220 | DEBUG    | Bug location extracted successfully: [<file>sphinx/util/requests.py</file>
<class>None</class>
<method>get</method>
<code>
103 def get(url: str, **kwargs: Any) -> requests.Response:
104     """Sends a GET request like requests.get().
105 
106     This sets up User-Agent header and TLS verification automatically."""
107     headers = kwargs.setdefault('headers', {})
108     config = kwargs.pop('config', None)
109     if config:
110         kwargs.setdefault('verify', _get_tls_cacert(url, config))
111         headers.setdefault('User-Agent', _get_user_agent(config))
112     else:
113         headers.setdefault('User-Agent', useragent_header[0][1])
114 
115     with ignore_insecure_warning(**kwargs):
116         return requests.get(url, **kwargs)

</code><intended_behavior>This method should be enhanced to allow tracking of redirects during the link checking process. It should check if the redirect URL matches an entry in a predefined list of acceptable redirects. If not, the redirect should be reported as a 'bad' redirect, while preserving existing functionality for making HTTP requests and handling typical responses.</intended_behavior>, <file>sphinx/builders/linkcheck.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2     sphinx.builders.linkcheck
3     ~~~~~~~~~~~~~~~~~~~~~~~~~
4 
5     The CheckExternalLinksBuilder class.
6 
7     :copyright: Copyright 2007-2021 by the Sphinx team, see AUTHORS.
8     :license: BSD, see LICENSE for details.
9 """
10 
11 import json
12 import re
13 import socket
14 import time
15 import warnings
16 from datetime import datetime, timezone
17 from email.utils import parsedate_to_datetime
18 from html.parser import HTMLParser
19 from os import path
20 from queue import PriorityQueue, Queue
21 from threading import Thread
22 from typing import (Any, Dict, Generator, List, NamedTuple, Optional, Pattern, Set, Tuple,
23                     Union, cast)
24 from urllib.parse import unquote, urlparse, urlunparse
25 
26 from docutils import nodes
27 from docutils.nodes import Element
28 from requests import Response
29 from requests.exceptions import ConnectionError, HTTPError, TooManyRedirects
30 
31 from sphinx.application import Sphinx
32 from sphinx.builders.dummy import DummyBuilder
33 from sphinx.config import Config
34 from sphinx.deprecation import RemovedInSphinx50Warning
35 from sphinx.environment import BuildEnvironment
36 from sphinx.locale import __
37 from sphinx.transforms.post_transforms import SphinxPostTransform
38 from sphinx.util import encode_uri, logging, requests
39 from sphinx.util.console import darkgray, darkgreen, purple, red, turquoise  # type: ignore
40 from sphinx.util.nodes import get_node_line
41 
42 logger = logging.getLogger(__name__)
43 
44 uri_re = re.compile('([a-z]+:)?//')  # matches to foo:// and // (a protocol relative URL)
45 
46 Hyperlink = NamedTuple('Hyperlink', (('uri', str),
47                                      ('docname', str),
48                                      ('lineno', Optional[int])))
49 CheckRequest = NamedTuple('CheckRequest', (('next_check', float),
50                                            ('hyperlink', Optional[Hyperlink])))
51 CheckResult = NamedTuple('CheckResult', (('uri', str),
52                                          ('docname', str),
53                                          ('lineno', int),
54                                          ('status', str),
55                                          ('message', str),
56                                          ('code', int)))
57 RateLimit = NamedTuple('RateLimit', (('delay', float), ('next_check', float)))
58 
59 # Tuple is old styled CheckRequest
60 CheckRequestType = Union[CheckRequest, Tuple[float, str, str, int]]
61 
62 DEFAULT_REQUEST_HEADERS = {
63     'Accept': 'text/html,application/xhtml+xml;q=0.9,*/*;q=0.8',
64 }
65 CHECK_IMMEDIATELY = 0
66 QUEUE_POLL_SECS = 1
67 DEFAULT_DELAY = 60.0
68 
69 
70 def node_line_or_0(node: Element) -> int:
71     """
72     PriorityQueue items must be comparable. The line number is part of the
73     tuple used by the PriorityQueue, keep an homogeneous type for comparison.
74     """
75     warnings.warn('node_line_or_0() is deprecated.',
76                   RemovedInSphinx50Warning, stacklevel=2)
77     return get_node_line(node) or 0
78 
79 
80 class AnchorCheckParser(HTMLParser):
81     """Specialized HTML parser that looks for a specific anchor."""
82 
83     def __init__(self, search_anchor: str) -> None:
84         super().__init__()
85 
86         self.search_anchor = search_anchor
87         self.found = False
88 
89     def handle_starttag(self, tag: Any, attrs: Any) -> None:
90         for key, value in attrs:
91             if key in ('id', 'name') and value == self.search_anchor:
92                 self.found = True
93                 break
94 
95 
96 def check_anchor(response: requests.requests.Response, anchor: str) -> bool:
97     """Reads HTML data from a response object `response` searching for `anchor`.
98     Returns True if anchor was found, False otherwise.
99     """
100     parser = AnchorCheckParser(anchor)
101     # Read file in chunks. If we find a matching anchor, we break
102     # the loop early in hopes not to have to download the whole thing.
103     for chunk in response.iter_content(chunk_size=4096, decode_unicode=True):
104         if isinstance(chunk, bytes):    # requests failed to decode
105             chunk = chunk.decode()      # manually try to decode it
106 
107         parser.feed(chunk)
108         if parser.found:
109             break
110     parser.close()
111     return parser.found
112 
113 
114 class CheckExternalLinksBuilder(DummyBuilder):
115     """
116     Checks for broken external links.
117     """
118     name = 'linkcheck'
119     epilog = __('Look for any errors in the above output or in '
120                 '%(outdir)s/output.txt')
121 
122     def init(self) -> None:
123         self.hyperlinks: Dict[str, Hyperlink] = {}
124         self._good: Set[str] = set()
125         self._broken: Dict[str, str] = {}
126         self._redirected: Dict[str, Tuple[str, int]] = {}
127         # set a timeout for non-responding servers
128         socket.setdefaulttimeout(5.0)
129 
130         # create queues and worker threads
131         self._wqueue: PriorityQueue[CheckRequestType] = PriorityQueue()
132         self._rqueue: Queue[CheckResult] = Queue()
133 
134     @property
135     def anchors_ignore(self) -> List[Pattern]:
136         warnings.warn(
137             "%s.%s is deprecated." % (self.__class__.__name__, "anchors_ignore"),
138             RemovedInSphinx50Warning,
139             stacklevel=2,
140         )
141         return [re.compile(x) for x in self.config.linkcheck_anchors_ignore]
142 
143     @property
144     def auth(self) -> List[Tuple[Pattern, Any]]:
145         warnings.warn(
146             "%s.%s is deprecated." % (self.__class__.__name__, "auth"),
147             RemovedInSphinx50Warning,
148             stacklevel=2,
149         )
150         return [(re.compile(pattern), auth_info) for pattern, auth_info
151                 in self.config.linkcheck_auth]
152 
153     @property
154     def to_ignore(self) -> List[Pattern]:
155         warnings.warn(
156             "%s.%s is deprecated." % (self.__class__.__name__, "to_ignore"),
157             RemovedInSphinx50Warning,
158             stacklevel=2,
159         )
160         return [re.compile(x) for x in self.config.linkcheck_ignore]
161 
162     @property
163     def good(self) -> Set[str]:
164         warnings.warn(
165             "%s.%s is deprecated." % (self.__class__.__name__, "good"),
166             RemovedInSphinx50Warning,
167             stacklevel=2,
168         )
169         return self._good
170 
171     @property
172     def broken(self) -> Dict[str, str]:
173         warnings.warn(
174             "%s.%s is deprecated." % (self.__class__.__name__, "broken"),
175             RemovedInSphinx50Warning,
176             stacklevel=2,
177         )
178         return self._broken
179 
180     @property
181     def redirected(self) -> Dict[str, Tuple[str, int]]:
182         warnings.warn(
183             "%s.%s is deprecated." % (self.__class__.__name__, "redirected"),
184             RemovedInSphinx50Warning,
185             stacklevel=2,
186         )
187         return self._redirected
188 
189     def check_thread(self) -> None:
190         warnings.warn(
191             "%s.%s is deprecated." % (self.__class__.__name__, "check_thread"),
192             RemovedInSphinx50Warning,
193             stacklevel=2,
194         )
195         # do nothing.
196 
197     def limit_rate(self, response: Response) -> Optional[float]:
198         warnings.warn(
199             "%s.%s is deprecated." % (self.__class__.__name__, "limit_rate"),
200             RemovedInSphinx50Warning,
201             stacklevel=2,
202         )
203         worker = HyperlinkAvailabilityCheckWorker(self.env, self.config,
204                                                   None, None, {})
205         return worker.limit_rate(response)
206 
207     def rqueue(self, response: Response) -> Queue:
208         warnings.warn(
209             "%s.%s is deprecated." % (self.__class__.__name__, "rqueue"),
210             RemovedInSphinx50Warning,
211             stacklevel=2,
212         )
213         return self._rqueue
214 
215     def workers(self, response: Response) -> List[Thread]:
216         warnings.warn(
217             "%s.%s is deprecated." % (self.__class__.__name__, "workers"),
218             RemovedInSphinx50Warning,
219             stacklevel=2,
220         )
221         return []
222 
223     def wqueue(self, response: Response) -> Queue:
224         warnings.warn(
225             "%s.%s is deprecated." % (self.__class__.__name__, "wqueue"),
226             RemovedInSphinx50Warning,
227             stacklevel=2,
228         )
229         return self._wqueue
230 
231     def process_result(self, result: CheckResult) -> None:
232         filename = self.env.doc2path(result.docname, None)
233 
234         linkstat = dict(filename=filename, lineno=result.lineno,
235                         status=result.status, code=result.code, uri=result.uri,
236                         info=result.message)
237         self.write_linkstat(linkstat)
238 
239         if result.status == 'unchecked':
240             return
241         if result.status == 'working' and result.message == 'old':
242             return
243         if result.lineno:
244             logger.info('(%16s: line %4d) ', result.docname, result.lineno, nonl=True)
245         if result.status == 'ignored':
246             if result.message:
247                 logger.info(darkgray('-ignored- ') + result.uri + ': ' + result.message)
248             else:
249                 logger.info(darkgray('-ignored- ') + result.uri)
250         elif result.status == 'local':
251             logger.info(darkgray('-local-   ') + result.uri)
252             self.write_entry('local', result.docname, filename, result.lineno, result.uri)
253         elif result.status == 'working':
254             logger.info(darkgreen('ok        ') + result.uri + result.message)
255         elif result.status == 'broken':
256             if self.app.quiet or self.app.warningiserror:
257                 logger.warning(__('broken link: %s (%s)'), result.uri, result.message,
258                                location=(filename, result.lineno))
259             else:
260                 logger.info(red('broken    ') + result.uri + red(' - ' + result.message))
261             self.write_entry('broken', result.docname, filename, result.lineno,
262                              result.uri + ': ' + result.message)
263         elif result.status == 'redirected':
264             try:
265                 text, color = {
266                     301: ('permanently', purple),
267                     302: ('with Found', purple),
268                     303: ('with See Other', purple),
269                     307: ('temporarily', turquoise),
270                     308: ('permanently', purple),
271                 }[result.code]
272             except KeyError:
273                 text, color = ('with unknown code', purple)
274             linkstat['text'] = text
275             logger.info(color('redirect  ') + result.uri +
276                         color(' - ' + text + ' to ' + result.message))
277             self.write_entry('redirected ' + text, result.docname, filename,
278                              result.lineno, result.uri + ' to ' + result.message)
279         else:
280             raise ValueError("Unknown status %s." % result.status)
281 
282     def write_entry(self, what: str, docname: str, filename: str, line: int,
283                     uri: str) -> None:
284         self.txt_outfile.write("%s:%s: [%s] %s\n" % (filename, line, what, uri))
285 
286     def write_linkstat(self, data: dict) -> None:
287         self.json_outfile.write(json.dumps(data))
288         self.json_outfile.write('\n')
289 
290     def finish(self) -> None:
291         checker = HyperlinkAvailabilityChecker(self.env, self.config, self)
292         logger.info('')
293 
294         with open(path.join(self.outdir, 'output.txt'), 'w') as self.txt_outfile,\
295              open(path.join(self.outdir, 'output.json'), 'w') as self.json_outfile:
296             for result in checker.check(self.hyperlinks):
297                 self.process_result(result)
298 
299         if self._broken:
300             self.app.statuscode = 1
301 
302 
303 class HyperlinkAvailabilityChecker:
304     def __init__(self, env: BuildEnvironment, config: Config,
305                  builder: CheckExternalLinksBuilder = None) -> None:
306         # Warning: builder argument will be removed in the sphinx-5.0.
307         # Don't use it from extensions.
308         # tag: RemovedInSphinx50Warning
309         self.builder = builder
310         self.config = config
311         self.env = env
312         self.rate_limits: Dict[str, RateLimit] = {}
313         self.workers: List[Thread] = []
314 
315         self.to_ignore = [re.compile(x) for x in self.config.linkcheck_ignore]
316 
317         if builder:
318             self.rqueue = builder._rqueue
319             self.wqueue = builder._wqueue
320         else:
321             self.rqueue = Queue()
322             self.wqueue = PriorityQueue()
323 
324     def invoke_threads(self) -> None:
325         for i in range(self.config.linkcheck_workers):
326             thread = HyperlinkAvailabilityCheckWorker(self.env, self.config,
327                                                       self.rqueue, self.wqueue,
328                                                       self.rate_limits, self.builder)
329             thread.start()
330             self.workers.append(thread)
331 
332     def shutdown_threads(self) -> None:
333         self.wqueue.join()
334         for worker in self.workers:
335             self.wqueue.put(CheckRequest(CHECK_IMMEDIATELY, None), False)
336 
337     def check(self, hyperlinks: Dict[str, Hyperlink]) -> Generator[CheckResult, None, None]:
338         self.invoke_threads()
339 
340         total_links = 0
341         for hyperlink in hyperlinks.values():
342             if self.is_ignored_uri(hyperlink.uri):
343                 yield CheckResult(hyperlink.uri, hyperlink.docname, hyperlink.lineno,
344                                   'ignored', '', 0)
345             else:
346                 self.wqueue.put(CheckRequest(CHECK_IMMEDIATELY, hyperlink), False)
347                 total_links += 1
348 
349         done = 0
350         while done < total_links:
351             yield self.rqueue.get()
352             done += 1
353 
354         self.shutdown_threads()
355 
356     def is_ignored_uri(self, uri: str) -> bool:
357         return any(pat.match(uri) for pat in self.to_ignore)
358 
359 
360 class HyperlinkAvailabilityCheckWorker(Thread):
361     """A worker class for checking the availability of hyperlinks."""
362 
363     def __init__(self, env: BuildEnvironment, config: Config, rqueue: Queue,
364                  wqueue: Queue, rate_limits: Dict[str, RateLimit],
365                  builder: CheckExternalLinksBuilder = None) -> None:
366         # Warning: builder argument will be removed in the sphinx-5.0.
367         # Don't use it from extensions.
368         # tag: RemovedInSphinx50Warning
369         self.config = config
370         self.env = env
371         self.rate_limits = rate_limits
372         self.rqueue = rqueue
373         self.wqueue = wqueue
374 
375         self.anchors_ignore = [re.compile(x)
376                                for x in self.config.linkcheck_anchors_ignore]
377         self.auth = [(re.compile(pattern), auth_info) for pattern, auth_info
378                      in self.config.linkcheck_auth]
379 
380         if builder:
381             # if given, fill the result of checks as cache
382             self._good = builder._good
383             self._broken = builder._broken
384             self._redirected = builder._redirected
385         else:
386             # only for compatibility. Will be removed in Sphinx-5.0
387             self._good = set()
388             self._broken = {}
389             self._redirected = {}
390 
391         super().__init__(daemon=True)
392 
393     def run(self) -> None:
394         kwargs = {}
395         if self.config.linkcheck_timeout:
396             kwargs['timeout'] = self.config.linkcheck_timeout
397 
398         def get_request_headers() -> Dict:
399             url = urlparse(uri)
400             candidates = ["%s://%s" % (url.scheme, url.netloc),
401                           "%s://%s/" % (url.scheme, url.netloc),
402                           uri,
403                           "*"]
404 
405             for u in candidates:
406                 if u in self.config.linkcheck_request_headers:
407                     headers = dict(DEFAULT_REQUEST_HEADERS)
408                     headers.update(self.config.linkcheck_request_headers[u])
409                     return headers
410 
411             return {}
412 
413         def check_uri() -> Tuple[str, str, int]:
414             # split off anchor
415             if '#' in uri:
416                 req_url, anchor = uri.split('#', 1)
417                 for rex in self.anchors_ignore:
418                     if rex.match(anchor):
419                         anchor = None
420                         break
421             else:
422                 req_url = uri
423                 anchor = None
424 
425             # handle non-ASCII URIs
426             try:
427                 req_url.encode('ascii')
428             except UnicodeError:
429                 req_url = encode_uri(req_url)
430 
431             # Get auth info, if any
432             for pattern, auth_info in self.auth:
433                 if pattern.match(uri):
434                     break
435             else:
436                 auth_info = None
437 
438             # update request headers for the URL
439             kwargs['headers'] = get_request_headers()
440 
441             try:
442                 if anchor and self.config.linkcheck_anchors:
443                     # Read the whole document and see if #anchor exists
444                     response = requests.get(req_url, stream=True, config=self.config,
445                                             auth=auth_info, **kwargs)
446                     response.raise_for_status()
447                     found = check_anchor(response, unquote(anchor))
448 
449                     if not found:
450                         raise Exception(__("Anchor '%s' not found") % anchor)
451                 else:
452                     try:
453                         # try a HEAD request first, which should be easier on
454                         # the server and the network
455                         response = requests.head(req_url, allow_redirects=True,
456                                                  config=self.config, auth=auth_info,
457                                                  **kwargs)
458                         response.raise_for_status()
459                     # Servers drop the connection on HEAD requests, causing
460                     # ConnectionError.
461                     except (ConnectionError, HTTPError, TooManyRedirects) as err:
462                         if isinstance(err, HTTPError) and err.response.status_code == 429:
463                             raise
464                         # retry with GET request if that fails, some servers
465                         # don't like HEAD requests.
466                         response = requests.get(req_url, stream=True,
467                                                 config=self.config,
468                                                 auth=auth_info, **kwargs)
469                         response.raise_for_status()
470             except HTTPError as err:
471                 if err.response.status_code == 401:
472                     # We'll take "Unauthorized" as working.
473                     return 'working', ' - unauthorized', 0
474                 elif err.response.status_code == 429:
475                     next_check = self.limit_rate(err.response)
476                     if next_check is not None:
477                         self.wqueue.put(CheckRequest(next_check, hyperlink), False)
478                         return 'rate-limited', '', 0
479                     return 'broken', str(err), 0
480                 elif err.response.status_code == 503:
481                     # We'll take "Service Unavailable" as ignored.
482                     return 'ignored', str(err), 0
483                 else:
484                     return 'broken', str(err), 0
485             except Exception as err:
486                 return 'broken', str(err), 0
487             else:
488                 netloc = urlparse(req_url).netloc
489                 try:
490                     del self.rate_limits[netloc]
491                 except KeyError:
492                     pass
493             if response.url.rstrip('/') == req_url.rstrip('/'):
494                 return 'working', '', 0
495             else:
496                 new_url = response.url
497                 if anchor:
498                     new_url += '#' + anchor
499                 # history contains any redirects, get last
500                 if response.history:
501                     code = response.history[-1].status_code
502                     return 'redirected', new_url, code
503                 else:
504                     return 'redirected', new_url, 0
505 
506         def check(docname: str) -> Tuple[str, str, int]:
507             # check for various conditions without bothering the network
508             if len(uri) == 0 or uri.startswith(('#', 'mailto:', 'tel:')):
509                 return 'unchecked', '', 0
510             elif not uri.startswith(('http:', 'https:')):
511                 if uri_re.match(uri):
512                     # non supported URI schemes (ex. ftp)
513                     return 'unchecked', '', 0
514                 else:
515                     srcdir = path.dirname(self.env.doc2path(docname))
516                     if path.exists(path.join(srcdir, uri)):
517                         return 'working', '', 0
518                     else:
519                         self._broken[uri] = ''
520                         return 'broken', '', 0
521             elif uri in self._good:
522                 return 'working', 'old', 0
523             elif uri in self._broken:
524                 return 'broken', self._broken[uri], 0
525             elif uri in self._redirected:
526                 return 'redirected', self._redirected[uri][0], self._redirected[uri][1]
527 
528             # need to actually check the URI
529             for _ in range(self.config.linkcheck_retries):
530                 status, info, code = check_uri()
531                 if status != "broken":
532                     break
533 
534             if status == "working":
535                 self._good.add(uri)
536             elif status == "broken":
537                 self._broken[uri] = info
538             elif status == "redirected":
539                 self._redirected[uri] = (info, code)
540 
541             return (status, info, code)
542 
543         while True:
544             check_request = self.wqueue.get()
545             try:
546                 next_check, hyperlink = check_request
547                 if hyperlink is None:
548                     break
549 
550                 uri, docname, lineno = hyperlink
551             except ValueError:
552                 # old styled check_request (will be deprecated in Sphinx-5.0)
553                 next_check, uri, docname, lineno = check_request
554 
555             if uri is None:
556                 break
557             netloc = urlparse(uri).netloc
558             try:
559                 # Refresh rate limit.
560                 # When there are many links in the queue, workers are all stuck waiting
561                 # for responses, but the builder keeps queuing. Links in the queue may
562                 # have been queued before rate limits were discovered.
563                 next_check = self.rate_limits[netloc].next_check
564             except KeyError:
565                 pass
566             if next_check > time.time():
567                 # Sleep before putting message back in the queue to avoid
568                 # waking up other threads.
569                 time.sleep(QUEUE_POLL_SECS)
570                 self.wqueue.put(CheckRequest(next_check, hyperlink), False)
571                 self.wqueue.task_done()
572                 continue
573             status, info, code = check(docname)
574             if status == 'rate-limited':
575                 logger.info(darkgray('-rate limited-   ') + uri + darkgray(' | sleeping...'))
576             else:
577                 self.rqueue.put(CheckResult(uri, docname, lineno, status, info, code))
578             self.wqueue.task_done()
579 
580     def limit_rate(self, response: Response) -> Optional[float]:
581         next_check = None
582         retry_after = response.headers.get("Retry-After")
583         if retry_after:
584             try:
585                 # Integer: time to wait before next attempt.
586                 delay = float(retry_after)
587             except ValueError:
588                 try:
589                     # An HTTP-date: time of next attempt.
590                     until = parsedate_to_datetime(retry_after)
591                 except (TypeError, ValueError):
592                     # TypeError: Invalid date format.
593                     # ValueError: Invalid date, e.g. Oct 52th.
594                     pass
595                 else:
596                     next_check = datetime.timestamp(until)
597                     delay = (until - datetime.now(timezone.utc)).total_seconds()
598             else:
599                 next_check = time.time() + delay
600         netloc = urlparse(response.url).netloc
601         if next_check is None:
602             max_delay = self.config.linkcheck_rate_limit_timeout
603             try:
604                 rate_limit = self.rate_limits[netloc]
605             except KeyError:
606                 delay = DEFAULT_DELAY
607             else:
608                 last_wait_time = rate_limit.delay
609                 delay = 2.0 * last_wait_time
610                 if delay > max_delay and last_wait_time < max_delay:
611                     delay = max_delay
612             if delay > max_delay:
613                 return None
614             next_check = time.time() + delay
615         self.rate_limits[netloc] = RateLimit(delay, next_check)
616         return next_check
617 
618 
619 class HyperlinkCollector(SphinxPostTransform):
620     builders = ('linkcheck',)
621     default_priority = 800
622 
623     def run(self, **kwargs: Any) -> None:
624         builder = cast(CheckExternalLinksBuilder, self.app.builder)
625         hyperlinks = builder.hyperlinks
626 
627         # reference nodes
628         for refnode in self.document.traverse(nodes.reference):
629             if 'refuri' not in refnode:
630                 continue
631             uri = refnode['refuri']
632             newuri = self.app.emit_firstresult('linkcheck-process-uri', uri)
633             if newuri:
634                 uri = newuri
635 
636             lineno = get_node_line(refnode)
637             uri_info = Hyperlink(uri, self.env.docname, lineno)
638             if uri not in hyperlinks:
639                 hyperlinks[uri] = uri_info
640 
641         # image nodes
642         for imgnode in self.document.traverse(nodes.image):
643             uri = imgnode['candidates'].get('?')
644             if uri and '://' in uri:
645                 newuri = self.app.emit_firstresult('linkcheck-process-uri', uri)
646                 if newuri:
647                     uri = newuri
648 
649                 lineno = get_node_line(imgnode)
650                 uri_info = Hyperlink(uri, self.env.docname, lineno)
651                 if uri not in hyperlinks:
652                     hyperlinks[uri] = uri_info
653 
654 
655 def rewrite_github_anchor(app: Sphinx, uri: str) -> Optional[str]:
656     """Rewrite anchor name of the hyperlink to github.com
657 
658     The hyperlink anchors in github.com are dynamically generated.  This rewrites
659     them before checking and makes them comparable.
660     """
661     parsed = urlparse(uri)
662     if parsed.hostname == "github.com" and parsed.fragment:
663         prefixed = parsed.fragment.startswith('user-content-')
664         if not prefixed:
665             fragment = f'user-content-{parsed.fragment}'
666             return urlunparse(parsed._replace(fragment=fragment))
667     return None
668 
669 
670 def setup(app: Sphinx) -> Dict[str, Any]:
671     app.add_builder(CheckExternalLinksBuilder)
672     app.add_post_transform(HyperlinkCollector)
673 
674     app.add_config_value('linkcheck_ignore', [], None)
675     app.add_config_value('linkcheck_auth', [], None)
676     app.add_config_value('linkcheck_request_headers', {}, None)
677     app.add_config_value('linkcheck_retries', 1, None)
678     app.add_config_value('linkcheck_timeout', None, None, [int])
679     app.add_config_value('linkcheck_workers', 5, None)
680     app.add_config_value('linkcheck_anchors', True, None)
681     # Anchors starting with ! are ignored since they are
682     # commonly used for dynamic pages
683     app.add_config_value('linkcheck_anchors_ignore', ["^!"], None)
684     app.add_config_value('linkcheck_rate_limit_timeout', 300.0, None)
685 
686     app.add_event('linkcheck-process-uri')
687     app.connect('linkcheck-process-uri', rewrite_github_anchor)
688 
689     return {
690         'version': 'builtin',
691         'parallel_read_safe': True,
692         'parallel_write_safe': True,
693     }

</code><intended_behavior>This file requires modification to check the final URL (post-redirect) against a predefined list of acceptable redirects and report an error or specific warning for non-matching redirects. This enhancement should integrate with the current link verification process without impacting its functionality.</intended_behavior>]
2025-10-17 19:17:51.220 | INFO     | Search completed. Bug locations: [<file>sphinx/util/requests.py</file>
<class>None</class>
<method>get</method>
<code>
103 def get(url: str, **kwargs: Any) -> requests.Response:
104     """Sends a GET request like requests.get().
105 
106     This sets up User-Agent header and TLS verification automatically."""
107     headers = kwargs.setdefault('headers', {})
108     config = kwargs.pop('config', None)
109     if config:
110         kwargs.setdefault('verify', _get_tls_cacert(url, config))
111         headers.setdefault('User-Agent', _get_user_agent(config))
112     else:
113         headers.setdefault('User-Agent', useragent_header[0][1])
114 
115     with ignore_insecure_warning(**kwargs):
116         return requests.get(url, **kwargs)

</code><intended_behavior>This method should be enhanced to allow tracking of redirects during the link checking process. It should check if the redirect URL matches an entry in a predefined list of acceptable redirects. If not, the redirect should be reported as a 'bad' redirect, while preserving existing functionality for making HTTP requests and handling typical responses.</intended_behavior>, <file>sphinx/builders/linkcheck.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2     sphinx.builders.linkcheck
3     ~~~~~~~~~~~~~~~~~~~~~~~~~
4 
5     The CheckExternalLinksBuilder class.
6 
7     :copyright: Copyright 2007-2021 by the Sphinx team, see AUTHORS.
8     :license: BSD, see LICENSE for details.
9 """
10 
11 import json
12 import re
13 import socket
14 import time
15 import warnings
16 from datetime import datetime, timezone
17 from email.utils import parsedate_to_datetime
18 from html.parser import HTMLParser
19 from os import path
20 from queue import PriorityQueue, Queue
21 from threading import Thread
22 from typing import (Any, Dict, Generator, List, NamedTuple, Optional, Pattern, Set, Tuple,
23                     Union, cast)
24 from urllib.parse import unquote, urlparse, urlunparse
25 
26 from docutils import nodes
27 from docutils.nodes import Element
28 from requests import Response
29 from requests.exceptions import ConnectionError, HTTPError, TooManyRedirects
30 
31 from sphinx.application import Sphinx
32 from sphinx.builders.dummy import DummyBuilder
33 from sphinx.config import Config
34 from sphinx.deprecation import RemovedInSphinx50Warning
35 from sphinx.environment import BuildEnvironment
36 from sphinx.locale import __
37 from sphinx.transforms.post_transforms import SphinxPostTransform
38 from sphinx.util import encode_uri, logging, requests
39 from sphinx.util.console import darkgray, darkgreen, purple, red, turquoise  # type: ignore
40 from sphinx.util.nodes import get_node_line
41 
42 logger = logging.getLogger(__name__)
43 
44 uri_re = re.compile('([a-z]+:)?//')  # matches to foo:// and // (a protocol relative URL)
45 
46 Hyperlink = NamedTuple('Hyperlink', (('uri', str),
47                                      ('docname', str),
48                                      ('lineno', Optional[int])))
49 CheckRequest = NamedTuple('CheckRequest', (('next_check', float),
50                                            ('hyperlink', Optional[Hyperlink])))
51 CheckResult = NamedTuple('CheckResult', (('uri', str),
52                                          ('docname', str),
53                                          ('lineno', int),
54                                          ('status', str),
55                                          ('message', str),
56                                          ('code', int)))
57 RateLimit = NamedTuple('RateLimit', (('delay', float), ('next_check', float)))
58 
59 # Tuple is old styled CheckRequest
60 CheckRequestType = Union[CheckRequest, Tuple[float, str, str, int]]
61 
62 DEFAULT_REQUEST_HEADERS = {
63     'Accept': 'text/html,application/xhtml+xml;q=0.9,*/*;q=0.8',
64 }
65 CHECK_IMMEDIATELY = 0
66 QUEUE_POLL_SECS = 1
67 DEFAULT_DELAY = 60.0
68 
69 
70 def node_line_or_0(node: Element) -> int:
71     """
72     PriorityQueue items must be comparable. The line number is part of the
73     tuple used by the PriorityQueue, keep an homogeneous type for comparison.
74     """
75     warnings.warn('node_line_or_0() is deprecated.',
76                   RemovedInSphinx50Warning, stacklevel=2)
77     return get_node_line(node) or 0
78 
79 
80 class AnchorCheckParser(HTMLParser):
81     """Specialized HTML parser that looks for a specific anchor."""
82 
83     def __init__(self, search_anchor: str) -> None:
84         super().__init__()
85 
86         self.search_anchor = search_anchor
87         self.found = False
88 
89     def handle_starttag(self, tag: Any, attrs: Any) -> None:
90         for key, value in attrs:
91             if key in ('id', 'name') and value == self.search_anchor:
92                 self.found = True
93                 break
94 
95 
96 def check_anchor(response: requests.requests.Response, anchor: str) -> bool:
97     """Reads HTML data from a response object `response` searching for `anchor`.
98     Returns True if anchor was found, False otherwise.
99     """
100     parser = AnchorCheckParser(anchor)
101     # Read file in chunks. If we find a matching anchor, we break
102     # the loop early in hopes not to have to download the whole thing.
103     for chunk in response.iter_content(chunk_size=4096, decode_unicode=True):
104         if isinstance(chunk, bytes):    # requests failed to decode
105             chunk = chunk.decode()      # manually try to decode it
106 
107         parser.feed(chunk)
108         if parser.found:
109             break
110     parser.close()
111     return parser.found
112 
113 
114 class CheckExternalLinksBuilder(DummyBuilder):
115     """
116     Checks for broken external links.
117     """
118     name = 'linkcheck'
119     epilog = __('Look for any errors in the above output or in '
120                 '%(outdir)s/output.txt')
121 
122     def init(self) -> None:
123         self.hyperlinks: Dict[str, Hyperlink] = {}
124         self._good: Set[str] = set()
125         self._broken: Dict[str, str] = {}
126         self._redirected: Dict[str, Tuple[str, int]] = {}
127         # set a timeout for non-responding servers
128         socket.setdefaulttimeout(5.0)
129 
130         # create queues and worker threads
131         self._wqueue: PriorityQueue[CheckRequestType] = PriorityQueue()
132         self._rqueue: Queue[CheckResult] = Queue()
133 
134     @property
135     def anchors_ignore(self) -> List[Pattern]:
136         warnings.warn(
137             "%s.%s is deprecated." % (self.__class__.__name__, "anchors_ignore"),
138             RemovedInSphinx50Warning,
139             stacklevel=2,
140         )
141         return [re.compile(x) for x in self.config.linkcheck_anchors_ignore]
142 
143     @property
144     def auth(self) -> List[Tuple[Pattern, Any]]:
145         warnings.warn(
146             "%s.%s is deprecated." % (self.__class__.__name__, "auth"),
147             RemovedInSphinx50Warning,
148             stacklevel=2,
149         )
150         return [(re.compile(pattern), auth_info) for pattern, auth_info
151                 in self.config.linkcheck_auth]
152 
153     @property
154     def to_ignore(self) -> List[Pattern]:
155         warnings.warn(
156             "%s.%s is deprecated." % (self.__class__.__name__, "to_ignore"),
157             RemovedInSphinx50Warning,
158             stacklevel=2,
159         )
160         return [re.compile(x) for x in self.config.linkcheck_ignore]
161 
162     @property
163     def good(self) -> Set[str]:
164         warnings.warn(
165             "%s.%s is deprecated." % (self.__class__.__name__, "good"),
166             RemovedInSphinx50Warning,
167             stacklevel=2,
168         )
169         return self._good
170 
171     @property
172     def broken(self) -> Dict[str, str]:
173         warnings.warn(
174             "%s.%s is deprecated." % (self.__class__.__name__, "broken"),
175             RemovedInSphinx50Warning,
176             stacklevel=2,
177         )
178         return self._broken
179 
180     @property
181     def redirected(self) -> Dict[str, Tuple[str, int]]:
182         warnings.warn(
183             "%s.%s is deprecated." % (self.__class__.__name__, "redirected"),
184             RemovedInSphinx50Warning,
185             stacklevel=2,
186         )
187         return self._redirected
188 
189     def check_thread(self) -> None:
190         warnings.warn(
191             "%s.%s is deprecated." % (self.__class__.__name__, "check_thread"),
192             RemovedInSphinx50Warning,
193             stacklevel=2,
194         )
195         # do nothing.
196 
197     def limit_rate(self, response: Response) -> Optional[float]:
198         warnings.warn(
199             "%s.%s is deprecated." % (self.__class__.__name__, "limit_rate"),
200             RemovedInSphinx50Warning,
201             stacklevel=2,
202         )
203         worker = HyperlinkAvailabilityCheckWorker(self.env, self.config,
204                                                   None, None, {})
205         return worker.limit_rate(response)
206 
207     def rqueue(self, response: Response) -> Queue:
208         warnings.warn(
209             "%s.%s is deprecated." % (self.__class__.__name__, "rqueue"),
210             RemovedInSphinx50Warning,
211             stacklevel=2,
212         )
213         return self._rqueue
214 
215     def workers(self, response: Response) -> List[Thread]:
216         warnings.warn(
217             "%s.%s is deprecated." % (self.__class__.__name__, "workers"),
218             RemovedInSphinx50Warning,
219             stacklevel=2,
220         )
221         return []
222 
223     def wqueue(self, response: Response) -> Queue:
224         warnings.warn(
225             "%s.%s is deprecated." % (self.__class__.__name__, "wqueue"),
226             RemovedInSphinx50Warning,
227             stacklevel=2,
228         )
229         return self._wqueue
230 
231     def process_result(self, result: CheckResult) -> None:
232         filename = self.env.doc2path(result.docname, None)
233 
234         linkstat = dict(filename=filename, lineno=result.lineno,
235                         status=result.status, code=result.code, uri=result.uri,
236                         info=result.message)
237         self.write_linkstat(linkstat)
238 
239         if result.status == 'unchecked':
240             return
241         if result.status == 'working' and result.message == 'old':
242             return
243         if result.lineno:
244             logger.info('(%16s: line %4d) ', result.docname, result.lineno, nonl=True)
245         if result.status == 'ignored':
246             if result.message:
247                 logger.info(darkgray('-ignored- ') + result.uri + ': ' + result.message)
248             else:
249                 logger.info(darkgray('-ignored- ') + result.uri)
250         elif result.status == 'local':
251             logger.info(darkgray('-local-   ') + result.uri)
252             self.write_entry('local', result.docname, filename, result.lineno, result.uri)
253         elif result.status == 'working':
254             logger.info(darkgreen('ok        ') + result.uri + result.message)
255         elif result.status == 'broken':
256             if self.app.quiet or self.app.warningiserror:
257                 logger.warning(__('broken link: %s (%s)'), result.uri, result.message,
258                                location=(filename, result.lineno))
259             else:
260                 logger.info(red('broken    ') + result.uri + red(' - ' + result.message))
261             self.write_entry('broken', result.docname, filename, result.lineno,
262                              result.uri + ': ' + result.message)
263         elif result.status == 'redirected':
264             try:
265                 text, color = {
266                     301: ('permanently', purple),
267                     302: ('with Found', purple),
268                     303: ('with See Other', purple),
269                     307: ('temporarily', turquoise),
270                     308: ('permanently', purple),
271                 }[result.code]
272             except KeyError:
273                 text, color = ('with unknown code', purple)
274             linkstat['text'] = text
275             logger.info(color('redirect  ') + result.uri +
276                         color(' - ' + text + ' to ' + result.message))
277             self.write_entry('redirected ' + text, result.docname, filename,
278                              result.lineno, result.uri + ' to ' + result.message)
279         else:
280             raise ValueError("Unknown status %s." % result.status)
281 
282     def write_entry(self, what: str, docname: str, filename: str, line: int,
283                     uri: str) -> None:
284         self.txt_outfile.write("%s:%s: [%s] %s\n" % (filename, line, what, uri))
285 
286     def write_linkstat(self, data: dict) -> None:
287         self.json_outfile.write(json.dumps(data))
288         self.json_outfile.write('\n')
289 
290     def finish(self) -> None:
291         checker = HyperlinkAvailabilityChecker(self.env, self.config, self)
292         logger.info('')
293 
294         with open(path.join(self.outdir, 'output.txt'), 'w') as self.txt_outfile,\
295              open(path.join(self.outdir, 'output.json'), 'w') as self.json_outfile:
296             for result in checker.check(self.hyperlinks):
297                 self.process_result(result)
298 
299         if self._broken:
300             self.app.statuscode = 1
301 
302 
303 class HyperlinkAvailabilityChecker:
304     def __init__(self, env: BuildEnvironment, config: Config,
305                  builder: CheckExternalLinksBuilder = None) -> None:
306         # Warning: builder argument will be removed in the sphinx-5.0.
307         # Don't use it from extensions.
308         # tag: RemovedInSphinx50Warning
309         self.builder = builder
310         self.config = config
311         self.env = env
312         self.rate_limits: Dict[str, RateLimit] = {}
313         self.workers: List[Thread] = []
314 
315         self.to_ignore = [re.compile(x) for x in self.config.linkcheck_ignore]
316 
317         if builder:
318             self.rqueue = builder._rqueue
319             self.wqueue = builder._wqueue
320         else:
321             self.rqueue = Queue()
322             self.wqueue = PriorityQueue()
323 
324     def invoke_threads(self) -> None:
325         for i in range(self.config.linkcheck_workers):
326             thread = HyperlinkAvailabilityCheckWorker(self.env, self.config,
327                                                       self.rqueue, self.wqueue,
328                                                       self.rate_limits, self.builder)
329             thread.start()
330             self.workers.append(thread)
331 
332     def shutdown_threads(self) -> None:
333         self.wqueue.join()
334         for worker in self.workers:
335             self.wqueue.put(CheckRequest(CHECK_IMMEDIATELY, None), False)
336 
337     def check(self, hyperlinks: Dict[str, Hyperlink]) -> Generator[CheckResult, None, None]:
338         self.invoke_threads()
339 
340         total_links = 0
341         for hyperlink in hyperlinks.values():
342             if self.is_ignored_uri(hyperlink.uri):
343                 yield CheckResult(hyperlink.uri, hyperlink.docname, hyperlink.lineno,
344                                   'ignored', '', 0)
345             else:
346                 self.wqueue.put(CheckRequest(CHECK_IMMEDIATELY, hyperlink), False)
347                 total_links += 1
348 
349         done = 0
350         while done < total_links:
351             yield self.rqueue.get()
352             done += 1
353 
354         self.shutdown_threads()
355 
356     def is_ignored_uri(self, uri: str) -> bool:
357         return any(pat.match(uri) for pat in self.to_ignore)
358 
359 
360 class HyperlinkAvailabilityCheckWorker(Thread):
361     """A worker class for checking the availability of hyperlinks."""
362 
363     def __init__(self, env: BuildEnvironment, config: Config, rqueue: Queue,
364                  wqueue: Queue, rate_limits: Dict[str, RateLimit],
365                  builder: CheckExternalLinksBuilder = None) -> None:
366         # Warning: builder argument will be removed in the sphinx-5.0.
367         # Don't use it from extensions.
368         # tag: RemovedInSphinx50Warning
369         self.config = config
370         self.env = env
371         self.rate_limits = rate_limits
372         self.rqueue = rqueue
373         self.wqueue = wqueue
374 
375         self.anchors_ignore = [re.compile(x)
376                                for x in self.config.linkcheck_anchors_ignore]
377         self.auth = [(re.compile(pattern), auth_info) for pattern, auth_info
378                      in self.config.linkcheck_auth]
379 
380         if builder:
381             # if given, fill the result of checks as cache
382             self._good = builder._good
383             self._broken = builder._broken
384             self._redirected = builder._redirected
385         else:
386             # only for compatibility. Will be removed in Sphinx-5.0
387             self._good = set()
388             self._broken = {}
389             self._redirected = {}
390 
391         super().__init__(daemon=True)
392 
393     def run(self) -> None:
394         kwargs = {}
395         if self.config.linkcheck_timeout:
396             kwargs['timeout'] = self.config.linkcheck_timeout
397 
398         def get_request_headers() -> Dict:
399             url = urlparse(uri)
400             candidates = ["%s://%s" % (url.scheme, url.netloc),
401                           "%s://%s/" % (url.scheme, url.netloc),
402                           uri,
403                           "*"]
404 
405             for u in candidates:
406                 if u in self.config.linkcheck_request_headers:
407                     headers = dict(DEFAULT_REQUEST_HEADERS)
408                     headers.update(self.config.linkcheck_request_headers[u])
409                     return headers
410 
411             return {}
412 
413         def check_uri() -> Tuple[str, str, int]:
414             # split off anchor
415             if '#' in uri:
416                 req_url, anchor = uri.split('#', 1)
417                 for rex in self.anchors_ignore:
418                     if rex.match(anchor):
419                         anchor = None
420                         break
421             else:
422                 req_url = uri
423                 anchor = None
424 
425             # handle non-ASCII URIs
426             try:
427                 req_url.encode('ascii')
428             except UnicodeError:
429                 req_url = encode_uri(req_url)
430 
431             # Get auth info, if any
432             for pattern, auth_info in self.auth:
433                 if pattern.match(uri):
434                     break
435             else:
436                 auth_info = None
437 
438             # update request headers for the URL
439             kwargs['headers'] = get_request_headers()
440 
441             try:
442                 if anchor and self.config.linkcheck_anchors:
443                     # Read the whole document and see if #anchor exists
444                     response = requests.get(req_url, stream=True, config=self.config,
445                                             auth=auth_info, **kwargs)
446                     response.raise_for_status()
447                     found = check_anchor(response, unquote(anchor))
448 
449                     if not found:
450                         raise Exception(__("Anchor '%s' not found") % anchor)
451                 else:
452                     try:
453                         # try a HEAD request first, which should be easier on
454                         # the server and the network
455                         response = requests.head(req_url, allow_redirects=True,
456                                                  config=self.config, auth=auth_info,
457                                                  **kwargs)
458                         response.raise_for_status()
459                     # Servers drop the connection on HEAD requests, causing
460                     # ConnectionError.
461                     except (ConnectionError, HTTPError, TooManyRedirects) as err:
462                         if isinstance(err, HTTPError) and err.response.status_code == 429:
463                             raise
464                         # retry with GET request if that fails, some servers
465                         # don't like HEAD requests.
466                         response = requests.get(req_url, stream=True,
467                                                 config=self.config,
468                                                 auth=auth_info, **kwargs)
469                         response.raise_for_status()
470             except HTTPError as err:
471                 if err.response.status_code == 401:
472                     # We'll take "Unauthorized" as working.
473                     return 'working', ' - unauthorized', 0
474                 elif err.response.status_code == 429:
475                     next_check = self.limit_rate(err.response)
476                     if next_check is not None:
477                         self.wqueue.put(CheckRequest(next_check, hyperlink), False)
478                         return 'rate-limited', '', 0
479                     return 'broken', str(err), 0
480                 elif err.response.status_code == 503:
481                     # We'll take "Service Unavailable" as ignored.
482                     return 'ignored', str(err), 0
483                 else:
484                     return 'broken', str(err), 0
485             except Exception as err:
486                 return 'broken', str(err), 0
487             else:
488                 netloc = urlparse(req_url).netloc
489                 try:
490                     del self.rate_limits[netloc]
491                 except KeyError:
492                     pass
493             if response.url.rstrip('/') == req_url.rstrip('/'):
494                 return 'working', '', 0
495             else:
496                 new_url = response.url
497                 if anchor:
498                     new_url += '#' + anchor
499                 # history contains any redirects, get last
500                 if response.history:
501                     code = response.history[-1].status_code
502                     return 'redirected', new_url, code
503                 else:
504                     return 'redirected', new_url, 0
505 
506         def check(docname: str) -> Tuple[str, str, int]:
507             # check for various conditions without bothering the network
508             if len(uri) == 0 or uri.startswith(('#', 'mailto:', 'tel:')):
509                 return 'unchecked', '', 0
510             elif not uri.startswith(('http:', 'https:')):
511                 if uri_re.match(uri):
512                     # non supported URI schemes (ex. ftp)
513                     return 'unchecked', '', 0
514                 else:
515                     srcdir = path.dirname(self.env.doc2path(docname))
516                     if path.exists(path.join(srcdir, uri)):
517                         return 'working', '', 0
518                     else:
519                         self._broken[uri] = ''
520                         return 'broken', '', 0
521             elif uri in self._good:
522                 return 'working', 'old', 0
523             elif uri in self._broken:
524                 return 'broken', self._broken[uri], 0
525             elif uri in self._redirected:
526                 return 'redirected', self._redirected[uri][0], self._redirected[uri][1]
527 
528             # need to actually check the URI
529             for _ in range(self.config.linkcheck_retries):
530                 status, info, code = check_uri()
531                 if status != "broken":
532                     break
533 
534             if status == "working":
535                 self._good.add(uri)
536             elif status == "broken":
537                 self._broken[uri] = info
538             elif status == "redirected":
539                 self._redirected[uri] = (info, code)
540 
541             return (status, info, code)
542 
543         while True:
544             check_request = self.wqueue.get()
545             try:
546                 next_check, hyperlink = check_request
547                 if hyperlink is None:
548                     break
549 
550                 uri, docname, lineno = hyperlink
551             except ValueError:
552                 # old styled check_request (will be deprecated in Sphinx-5.0)
553                 next_check, uri, docname, lineno = check_request
554 
555             if uri is None:
556                 break
557             netloc = urlparse(uri).netloc
558             try:
559                 # Refresh rate limit.
560                 # When there are many links in the queue, workers are all stuck waiting
561                 # for responses, but the builder keeps queuing. Links in the queue may
562                 # have been queued before rate limits were discovered.
563                 next_check = self.rate_limits[netloc].next_check
564             except KeyError:
565                 pass
566             if next_check > time.time():
567                 # Sleep before putting message back in the queue to avoid
568                 # waking up other threads.
569                 time.sleep(QUEUE_POLL_SECS)
570                 self.wqueue.put(CheckRequest(next_check, hyperlink), False)
571                 self.wqueue.task_done()
572                 continue
573             status, info, code = check(docname)
574             if status == 'rate-limited':
575                 logger.info(darkgray('-rate limited-   ') + uri + darkgray(' | sleeping...'))
576             else:
577                 self.rqueue.put(CheckResult(uri, docname, lineno, status, info, code))
578             self.wqueue.task_done()
579 
580     def limit_rate(self, response: Response) -> Optional[float]:
581         next_check = None
582         retry_after = response.headers.get("Retry-After")
583         if retry_after:
584             try:
585                 # Integer: time to wait before next attempt.
586                 delay = float(retry_after)
587             except ValueError:
588                 try:
589                     # An HTTP-date: time of next attempt.
590                     until = parsedate_to_datetime(retry_after)
591                 except (TypeError, ValueError):
592                     # TypeError: Invalid date format.
593                     # ValueError: Invalid date, e.g. Oct 52th.
594                     pass
595                 else:
596                     next_check = datetime.timestamp(until)
597                     delay = (until - datetime.now(timezone.utc)).total_seconds()
598             else:
599                 next_check = time.time() + delay
600         netloc = urlparse(response.url).netloc
601         if next_check is None:
602             max_delay = self.config.linkcheck_rate_limit_timeout
603             try:
604                 rate_limit = self.rate_limits[netloc]
605             except KeyError:
606                 delay = DEFAULT_DELAY
607             else:
608                 last_wait_time = rate_limit.delay
609                 delay = 2.0 * last_wait_time
610                 if delay > max_delay and last_wait_time < max_delay:
611                     delay = max_delay
612             if delay > max_delay:
613                 return None
614             next_check = time.time() + delay
615         self.rate_limits[netloc] = RateLimit(delay, next_check)
616         return next_check
617 
618 
619 class HyperlinkCollector(SphinxPostTransform):
620     builders = ('linkcheck',)
621     default_priority = 800
622 
623     def run(self, **kwargs: Any) -> None:
624         builder = cast(CheckExternalLinksBuilder, self.app.builder)
625         hyperlinks = builder.hyperlinks
626 
627         # reference nodes
628         for refnode in self.document.traverse(nodes.reference):
629             if 'refuri' not in refnode:
630                 continue
631             uri = refnode['refuri']
632             newuri = self.app.emit_firstresult('linkcheck-process-uri', uri)
633             if newuri:
634                 uri = newuri
635 
636             lineno = get_node_line(refnode)
637             uri_info = Hyperlink(uri, self.env.docname, lineno)
638             if uri not in hyperlinks:
639                 hyperlinks[uri] = uri_info
640 
641         # image nodes
642         for imgnode in self.document.traverse(nodes.image):
643             uri = imgnode['candidates'].get('?')
644             if uri and '://' in uri:
645                 newuri = self.app.emit_firstresult('linkcheck-process-uri', uri)
646                 if newuri:
647                     uri = newuri
648 
649                 lineno = get_node_line(imgnode)
650                 uri_info = Hyperlink(uri, self.env.docname, lineno)
651                 if uri not in hyperlinks:
652                     hyperlinks[uri] = uri_info
653 
654 
655 def rewrite_github_anchor(app: Sphinx, uri: str) -> Optional[str]:
656     """Rewrite anchor name of the hyperlink to github.com
657 
658     The hyperlink anchors in github.com are dynamically generated.  This rewrites
659     them before checking and makes them comparable.
660     """
661     parsed = urlparse(uri)
662     if parsed.hostname == "github.com" and parsed.fragment:
663         prefixed = parsed.fragment.startswith('user-content-')
664         if not prefixed:
665             fragment = f'user-content-{parsed.fragment}'
666             return urlunparse(parsed._replace(fragment=fragment))
667     return None
668 
669 
670 def setup(app: Sphinx) -> Dict[str, Any]:
671     app.add_builder(CheckExternalLinksBuilder)
672     app.add_post_transform(HyperlinkCollector)
673 
674     app.add_config_value('linkcheck_ignore', [], None)
675     app.add_config_value('linkcheck_auth', [], None)
676     app.add_config_value('linkcheck_request_headers', {}, None)
677     app.add_config_value('linkcheck_retries', 1, None)
678     app.add_config_value('linkcheck_timeout', None, None, [int])
679     app.add_config_value('linkcheck_workers', 5, None)
680     app.add_config_value('linkcheck_anchors', True, None)
681     # Anchors starting with ! are ignored since they are
682     # commonly used for dynamic pages
683     app.add_config_value('linkcheck_anchors_ignore', ["^!"], None)
684     app.add_config_value('linkcheck_rate_limit_timeout', 300.0, None)
685 
686     app.add_event('linkcheck-process-uri')
687     app.connect('linkcheck-process-uri', rewrite_github_anchor)
688 
689     return {
690         'version': 'builtin',
691         'parallel_read_safe': True,
692         'parallel_write_safe': True,
693     }

</code><intended_behavior>This file requires modification to check the final URL (post-redirect) against a predefined list of acceptable redirects and report an error or specific warning for non-matching redirects. This enhancement should integrate with the current link verification process without impacting its functionality.</intended_behavior>]
2025-10-17 19:17:51.220 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-17 19:17:51.221 | INFO     | Search-only: skipping patch selection
2025-10-17 19:17:51.344 | INFO     | Task sphinx-doc__sphinx-9234 completed successfully.
2025-10-17 19:17:51.345 | INFO     | No patch generated. You can try running ACR again.
