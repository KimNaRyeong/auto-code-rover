2025-10-17 15:34:46.984 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-17 15:34:48.535 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=743, output_tokens=13, cost=0.007820
2025-10-17 15:34:56.870 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=923, output_tokens=354, cost=0.019850
2025-10-17 15:34:58.194 | INFO     | registered non reproducing test 0
2025-10-17 15:35:09.842 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1842, output_tokens=526, cost=0.034200
2025-10-17 15:35:11.119 | INFO     | registered non reproducing test 1
2025-10-17 15:35:23.074 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2008, output_tokens=586, cost=0.037660
2025-10-17 15:35:24.346 | INFO     | registered non reproducing test 2
2025-10-17 15:35:24.346 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-17 15:35:24.349 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 15:35:31.952 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=993, output_tokens=231, cost=0.016860
2025-10-17 15:35:31.955 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 15:35:34.224 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=585, output_tokens=51, cost=0.007380
2025-10-17 15:35:34.225 | DEBUG    | Extracted a valid json.
2025-10-17 15:35:34.225 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"DataArray\")",
        "search_method_in_class(\"quantile\", \"DataArray\")",
        "search_code(\"np.nanpercentile\")"
    ],
    "bug_locations": []
}
2025-10-17 15:35:34.301 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-17 15:35:34.301 | DEBUG    | <Agent search> Analyzing search results.
2025-10-17 15:35:49.297 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=6557, output_tokens=614, cost=0.083990
2025-10-17 15:35:49.303 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 15:35:57.139 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=7387, output_tokens=287, cost=0.082480
2025-10-17 15:35:57.142 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 15:36:05.246 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=641, output_tokens=310, cost=0.015710
2025-10-17 15:36:05.247 | DEBUG    | Extracted a valid json.
2025-10-17 15:36:05.247 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "xarray/core/dataarray.py", 
            "class": "DataArray", 
            "method": "quantile", 
            "intended_behavior": "The `quantile` method should validate the `q` parameter to ensure it falls within the 0 to 1 range, corresponding to quantiles, not percentiles. If a value outside this range is detected, the method should raise a `ValueError` with a corrected message: `\"Quantiles must be in the range [0, 1]\"`. This adjustment ensures users receive accurate feedback aligned with the method's expected inputs, preserving the existing functionality while correcting the misleading error message."
        },
        {
            "file": "xarray/core/variable.py", 
            "class": "Variable", 
            "method": "quantile", 
            "intended_behavior": "Although the direct issue doesn't lie here, reviewing how errors are propagated when `np.nanpercentile` is called could be beneficial. Ensure that any error messages about input range exceptions are consistent with the quantile range [0, 1]. If `Variable.quantile` is where the numpy function calls are made, it should either handle the error and rethrow it with a message consistent with xarray's expected input range, or it should contain preemptive input validation to provide a clear and correct error message before deference to numpy's functions."
        }
    ]
}
2025-10-17 15:36:05.262 | DEBUG    | Bug location extracted successfully: [<file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>quantile</method>
<code>
2935     def quantile(
2936         self,
2937         q: Any,
2938         dim: Union[Hashable, Sequence[Hashable], None] = None,
2939         interpolation: str = "linear",
2940         keep_attrs: bool = None,
2941     ) -> "DataArray":
2942         """Compute the qth quantile of the data along the specified dimension.
2943 
2944         Returns the qth quantiles(s) of the array elements.
2945 
2946         Parameters
2947         ----------
2948         q : float in range of [0,1] or array-like of floats
2949             Quantile to compute, which must be between 0 and 1 inclusive.
2950         dim : hashable or sequence of hashable, optional
2951             Dimension(s) over which to apply quantile.
2952         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
2953             This optional parameter specifies the interpolation method to
2954             use when the desired quantile lies between two data points
2955             ``i < j``:
2956 
2957                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is
2958                   the fractional part of the index surrounded by ``i`` and
2959                   ``j``.
2960                 - lower: ``i``.
2961                 - higher: ``j``.
2962                 - nearest: ``i`` or ``j``, whichever is nearest.
2963                 - midpoint: ``(i + j) / 2``.
2964         keep_attrs : bool, optional
2965             If True, the dataset's attributes (`attrs`) will be copied from
2966             the original object to the new one.  If False (default), the new
2967             object will be returned without attributes.
2968 
2969         Returns
2970         -------
2971         quantiles : DataArray
2972             If `q` is a single quantile, then the result
2973             is a scalar. If multiple percentiles are given, first axis of
2974             the result corresponds to the quantile and a quantile dimension
2975             is added to the return array. The other dimensions are the
2976             dimensions that remain after the reduction of the array.
2977 
2978         See Also
2979         --------
2980         numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile
2981 
2982         Examples
2983         --------
2984 
2985         >>> da = xr.DataArray(
2986         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
2987         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
2988         ...     dims=("x", "y"),
2989         ... )
2990         >>> da.quantile(0)  # or da.quantile(0, dim=...)
2991         <xarray.DataArray ()>
2992         array(0.7)
2993         Coordinates:
2994             quantile  float64 0.0
2995         >>> da.quantile(0, dim="x")
2996         <xarray.DataArray (y: 4)>
2997         array([0.7, 4.2, 2.6, 1.5])
2998         Coordinates:
2999           * y         (y) float64 1.0 1.5 2.0 2.5
3000             quantile  float64 0.0
3001         >>> da.quantile([0, 0.5, 1])
3002         <xarray.DataArray (quantile: 3)>
3003         array([0.7, 3.4, 9.4])
3004         Coordinates:
3005           * quantile  (quantile) float64 0.0 0.5 1.0
3006         >>> da.quantile([0, 0.5, 1], dim="x")
3007         <xarray.DataArray (quantile: 3, y: 4)>
3008         array([[0.7 , 4.2 , 2.6 , 1.5 ],
3009                [3.6 , 5.75, 6.  , 1.7 ],
3010                [6.5 , 7.3 , 9.4 , 1.9 ]])
3011         Coordinates:
3012           * y         (y) float64 1.0 1.5 2.0 2.5
3013           * quantile  (quantile) float64 0.0 0.5 1.0
3014         """
3015 
3016         ds = self._to_temp_dataset().quantile(
3017             q, dim=dim, keep_attrs=keep_attrs, interpolation=interpolation
3018         )
3019         return self._from_temp_dataset(ds)

</code><intended_behavior>The `quantile` method should validate the `q` parameter to ensure it falls within the 0 to 1 range, corresponding to quantiles, not percentiles. If a value outside this range is detected, the method should raise a `ValueError` with a corrected message: `"Quantiles must be in the range [0, 1]"`. This adjustment ensures users receive accurate feedback aligned with the method's expected inputs, preserving the existing functionality while correcting the misleading error message.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
214 class DataArray(AbstractArray, DataWithCoords):
215     """N-dimensional array with labeled coordinates and dimensions.
216 
217     DataArray provides a wrapper around numpy ndarrays that uses labeled
218     dimensions and coordinates to support metadata aware operations. The API is
219     similar to that for the pandas Series or DataFrame, but DataArray objects
220     can have any number of dimensions, and their contents have fixed data
221     types.
222 
223     Additional features over raw numpy arrays:
224 
225     - Apply operations over dimensions by name: ``x.sum('time')``.
226     - Select or assign values by integer location (like numpy): ``x[:10]``
227       or by label (like pandas): ``x.loc['2014-01-01']`` or
228       ``x.sel(time='2014-01-01')``.
229     - Mathematical operations (e.g., ``x - y``) vectorize across multiple
230       dimensions (known in numpy as "broadcasting") based on dimension names,
231       regardless of their original order.
232     - Keep track of arbitrary metadata in the form of a Python dictionary:
233       ``x.attrs``
234     - Convert to a pandas Series: ``x.to_series()``.
235 
236     Getting items from or doing mathematical operations with a DataArray
237     always returns another DataArray.
238     """
239 
240     _cache: Dict[str, Any]
241     _coords: Dict[Any, Variable]
242     _indexes: Optional[Dict[Hashable, pd.Index]]
243     _name: Optional[Hashable]
244     _variable: Variable
245 
246     __slots__ = (
247         "_cache",
248         "_coords",
249         "_file_obj",
250         "_indexes",
251         "_name",
252         "_variable",
253         "__weakref__",
254     )
255 
256     _groupby_cls = groupby.DataArrayGroupBy
257     _rolling_cls = rolling.DataArrayRolling
258     _coarsen_cls = rolling.DataArrayCoarsen
259     _resample_cls = resample.DataArrayResample
260 
261     dt = property(DatetimeAccessor)
262 
263     def __init__(
264         self,
265         data: Any = dtypes.NA,
266         coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None] = None,
267         dims: Union[Hashable, Sequence[Hashable], None] = None,
268         name: Hashable = None,
269         attrs: Mapping = None,
270         # deprecated parameters
271         encoding=None,
272         # internal parameters
273         indexes: Dict[Hashable, pd.Index] = None,
274         fastpath: bool = False,
275     ):
276         """
277         Parameters
278         ----------
279         data : array_like
280             Values for this array. Must be an ``numpy.ndarray``, ndarray like,
281             or castable to an ``ndarray``. If a self-described xarray or pandas
282             object, attempts are made to use this array's metadata to fill in
283             other unspecified arguments. A view of the array's data is used
284             instead of a copy if possible.
285         coords : sequence or dict of array_like objects, optional
286             Coordinates (tick labels) to use for indexing along each dimension.
287             The following notations are accepted:
288 
289             - mapping {dimension name: array-like}
290             - sequence of tuples that are valid arguments for xarray.Variable()
291               - (dims, data)
292               - (dims, data, attrs)
293               - (dims, data, attrs, encoding)
294 
295             Additionally, it is possible to define a coord whose name
296             does not match the dimension name, or a coord based on multiple
297             dimensions, with one of the following notations:
298 
299             - mapping {coord name: DataArray}
300             - mapping {coord name: Variable}
301             - mapping {coord name: (dimension name, array-like)}
302             - mapping {coord name: (tuple of dimension names, array-like)}
303 
304         dims : hashable or sequence of hashable, optional
305             Name(s) of the data dimension(s). Must be either a hashable (only
306             for 1D data) or a sequence of hashables with length equal to the
307             number of dimensions. If this argument is omitted, dimension names
308             are taken from ``coords`` (if possible) and otherwise default to
309             ``['dim_0', ... 'dim_n']``.
310         name : str or None, optional
311             Name of this array.
312         attrs : dict_like or None, optional
313             Attributes to assign to the new instance. By default, an empty
314             attribute dictionary is initialized.
315         """
316         if encoding is not None:
317             warnings.warn(
318                 "The `encoding` argument to `DataArray` is deprecated, and . "
319                 "will be removed in 0.15. "
320                 "Instead, specify the encoding when writing to disk or "
321                 "set the `encoding` attribute directly.",
322                 FutureWarning,
323                 stacklevel=2,
324             )
325         if fastpath:
326             variable = data
327             assert dims is None
328             assert attrs is None
329             assert encoding is None
330         else:
331             # try to fill in arguments from data if they weren't supplied
332             if coords is None:
333 
334                 if isinstance(data, DataArray):
335                     coords = data.coords
336                 elif isinstance(data, pd.Series):
337                     coords = [data.index]
338                 elif isinstance(data, pd.DataFrame):
339                     coords = [data.index, data.columns]
340                 elif isinstance(data, (pd.Index, IndexVariable)):
341                     coords = [data]
342                 elif isinstance(data, pdcompat.Panel):
343                     coords = [data.items, data.major_axis, data.minor_axis]
344 
345             if dims is None:
346                 dims = getattr(data, "dims", getattr(coords, "dims", None))
347             if name is None:
348                 name = getattr(data, "name", None)
349             if attrs is None and not isinstance(data, PANDAS_TYPES):
350                 attrs = getattr(data, "attrs", None)
351             if encoding is None:
352                 encoding = getattr(data, "encoding", None)
353 
354             data = _check_data_shape(data, coords, dims)
355             data = as_compatible_data(data)
356             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
357             variable = Variable(dims, data, attrs, encoding, fastpath=True)
358             indexes = dict(
359                 _extract_indexes_from_coords(coords)
360             )  # needed for to_dataset
361 
362         # These fully describe a DataArray
363         self._variable = variable
364         assert isinstance(coords, dict)
365         self._coords = coords
366         self._name = name
367 
368         # TODO(shoyer): document this argument, once it becomes part of the
369         # public interface.
370         self._indexes = indexes
371 
372         self._file_obj = None
373 
374     def _replace(
375         self,
376         variable: Variable = None,
377         coords=None,
378         name: Union[Hashable, None, Default] = _default,
379         indexes=None,
380     ) -> "DataArray":
381         if variable is None:
382             variable = self.variable
383         if coords is None:
384             coords = self._coords
385         if name is _default:
386             name = self.name
387         return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)
388 
389     def _replace_maybe_drop_dims(
390         self, variable: Variable, name: Union[Hashable, None, Default] = _default
391     ) -> "DataArray":
392         if variable.dims == self.dims and variable.shape == self.shape:
393             coords = self._coords.copy()
394             indexes = self._indexes
395         elif variable.dims == self.dims:
396             # Shape has changed (e.g. from reduce(..., keepdims=True)
397             new_sizes = dict(zip(self.dims, variable.shape))
398             coords = {
399                 k: v
400                 for k, v in self._coords.items()
401                 if v.shape == tuple(new_sizes[d] for d in v.dims)
402             }
403             changed_dims = [
404                 k for k in variable.dims if variable.sizes[k] != self.sizes[k]
405             ]
406             indexes = propagate_indexes(self._indexes, exclude=changed_dims)
407         else:
408             allowed_dims = set(variable.dims)
409             coords = {
410                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
411             }
412             indexes = propagate_indexes(
413                 self._indexes, exclude=(set(self.dims) - allowed_dims)
414             )
415         return self._replace(variable, coords, name, indexes=indexes)
416 
417     def _overwrite_indexes(self, indexes: Mapping[Hashable, Any]) -> "DataArray":
418         if not len(indexes):
419             return self
420         coords = self._coords.copy()
421         for name, idx in indexes.items():
422             coords[name] = IndexVariable(name, idx)
423         obj = self._replace(coords=coords)
424 
425         # switch from dimension to level names, if necessary
426         dim_names: Dict[Any, str] = {}
427         for dim, idx in indexes.items():
428             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:
429                 dim_names[dim] = idx.name
430         if dim_names:
431             obj = obj.rename(dim_names)
432         return obj
433 
434     def _to_temp_dataset(self) -> Dataset:
435         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
436 
437     def _from_temp_dataset(
438         self, dataset: Dataset, name: Hashable = _default
439     ) -> "DataArray":
440         variable = dataset._variables.pop(_THIS_ARRAY)
441         coords = dataset._variables
442         indexes = dataset._indexes
443         return self._replace(variable, coords, name, indexes=indexes)
444 
445     def _to_dataset_split(self, dim: Hashable) -> Dataset:
446         """ splits dataarray along dimension 'dim' """
447 
448         def subset(dim, label):
449             array = self.loc[{dim: label}]
450             array.attrs = {}
451             return as_variable(array)
452 
453         variables = {label: subset(dim, label) for label in self.get_index(dim)}
454         variables.update({k: v for k, v in self._coords.items() if k != dim})
455         indexes = propagate_indexes(self._indexes, exclude=dim)
456         coord_names = set(self._coords) - set([dim])
457         dataset = Dataset._construct_direct(
458             variables, coord_names, indexes=indexes, attrs=self.attrs
459         )
460         return dataset
461 
462     def _to_dataset_whole(
463         self, name: Hashable = None, shallow_copy: bool = True
464     ) -> Dataset:
465         if name is None:
466             name = self.name
467         if name is None:
468             raise ValueError(
469                 "unable to convert unnamed DataArray to a "
470                 "Dataset without providing an explicit name"
471             )
472         if name in self.coords:
473             raise ValueError(
474                 "cannot create a Dataset from a DataArray with "
475                 "the same name as one of its coordinates"
476             )
477         # use private APIs for speed: this is called by _to_temp_dataset(),
478         # which is used in the guts of a lot of operations (e.g., reindex)
479         variables = self._coords.copy()
480         variables[name] = self.variable
481         if shallow_copy:
482             for k in variables:
483                 variables[k] = variables[k].copy(deep=False)
484         indexes = self._indexes
485 
486         coord_names = set(self._coords)
487         dataset = Dataset._construct_direct(variables, coord_names, indexes=indexes)
488         return dataset
489 
490     def to_dataset(self, dim: Hashable = None, *, name: Hashable = None) -> Dataset:
491         """Convert a DataArray to a Dataset.
492 
493         Parameters
494         ----------
495         dim : hashable, optional
496             Name of the dimension on this array along which to split this array
497             into separate variables. If not provided, this array is converted
498             into a Dataset of one variable.
499         name : hashable, optional
500             Name to substitute for this array's name. Only valid if ``dim`` is
501             not provided.
502 
503         Returns
504         -------
505         dataset : Dataset
506         """
507         if dim is not None and dim not in self.dims:
508             raise TypeError(
509                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
510             )
511 
512         if dim is not None:
513             if name is not None:
514                 raise TypeError("cannot supply both dim and name arguments")
515             return self._to_dataset_split(dim)
516         else:
517             return self._to_dataset_whole(name)
518 
519     @property
520     def name(self) -> Optional[Hashable]:
521         """The name of this array.
522         """
523         return self._name
524 
525     @name.setter
526     def name(self, value: Optional[Hashable]) -> None:
527         self._name = value
528 
529     @property
530     def variable(self) -> Variable:
531         """Low level interface to the Variable object for this DataArray."""
532         return self._variable
533 
534     @property
535     def dtype(self) -> np.dtype:
536         return self.variable.dtype
537 
538     @property
539     def shape(self) -> Tuple[int, ...]:
540         return self.variable.shape
541 
542     @property
543     def size(self) -> int:
544         return self.variable.size
545 
546     @property
547     def nbytes(self) -> int:
548         return self.variable.nbytes
549 
550     @property
551     def ndim(self) -> int:
552         return self.variable.ndim
553 
554     def __len__(self) -> int:
555         return len(self.variable)
556 
557     @property
558     def data(self) -> Any:
559         """The array's data as a dask or numpy array
560         """
561         return self.variable.data
562 
563     @data.setter
564     def data(self, value: Any) -> None:
565         self.variable.data = value
566 
567     @property
568     def values(self) -> np.ndarray:
569         """The array's data as a numpy.ndarray"""
570         return self.variable.values
571 
572     @values.setter
573     def values(self, value: Any) -> None:
574         self.variable.values = value
575 
576     @property
577     def _in_memory(self) -> bool:
578         return self.variable._in_memory
579 
580     def to_index(self) -> pd.Index:
581         """Convert this variable to a pandas.Index. Only possible for 1D
582         arrays.
583         """
584         return self.variable.to_index()
585 
586     @property
587     def dims(self) -> Tuple[Hashable, ...]:
588         """Tuple of dimension names associated with this array.
589 
590         Note that the type of this property is inconsistent with
591         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
592         consistently named properties.
593         """
594         return self.variable.dims
595 
596     @dims.setter
597     def dims(self, value):
598         raise AttributeError(
599             "you cannot assign dims on a DataArray. Use "
600             ".rename() or .swap_dims() instead."
601         )
602 
603     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
604         if utils.is_dict_like(key):
605             return key
606         else:
607             key = indexing.expanded_indexer(key, self.ndim)
608             return dict(zip(self.dims, key))
609 
610     @property
611     def _level_coords(self) -> Dict[Hashable, Hashable]:
612         """Return a mapping of all MultiIndex levels and their corresponding
613         coordinate name.
614         """
615         level_coords: Dict[Hashable, Hashable] = {}
616 
617         for cname, var in self._coords.items():
618             if var.ndim == 1 and isinstance(var, IndexVariable):
619                 level_names = var.level_names
620                 if level_names is not None:
621                     (dim,) = var.dims
622                     level_coords.update({lname: dim for lname in level_names})
623         return level_coords
624 
625     def _getitem_coord(self, key):
626         from .dataset import _get_virtual_variable
627 
628         try:
629             var = self._coords[key]
630         except KeyError:
631             dim_sizes = dict(zip(self.dims, self.shape))
632             _, key, var = _get_virtual_variable(
633                 self._coords, key, self._level_coords, dim_sizes
634             )
635 
636         return self._replace_maybe_drop_dims(var, name=key)
637 
638     def __getitem__(self, key: Any) -> "DataArray":
639         if isinstance(key, str):
640             return self._getitem_coord(key)
641         else:
642             # xarray-style array indexing
643             return self.isel(indexers=self._item_key_to_dict(key))
644 
645     def __setitem__(self, key: Any, value: Any) -> None:
646         if isinstance(key, str):
647             self.coords[key] = value
648         else:
649             # Coordinates in key, value and self[key] should be consistent.
650             # TODO Coordinate consistency in key is checked here, but it
651             # causes unnecessary indexing. It should be optimized.
652             obj = self[key]
653             if isinstance(value, DataArray):
654                 assert_coordinate_consistent(value, obj.coords.variables)
655             # DataArray key -> Variable key
656             key = {
657                 k: v.variable if isinstance(v, DataArray) else v
658                 for k, v in self._item_key_to_dict(key).items()
659             }
660             self.variable[key] = value
661 
662     def __delitem__(self, key: Any) -> None:
663         del self.coords[key]
664 
665     @property
666     def _attr_sources(self) -> List[Mapping[Hashable, Any]]:
667         """List of places to look-up items for attribute-style access
668         """
669         return self._item_sources + [self.attrs]
670 
671     @property
672     def _item_sources(self) -> List[Mapping[Hashable, Any]]:
673         """List of places to look-up items for key-completion
674         """
675         return [
676             self.coords,
677             {d: self.coords[d] for d in self.dims},
678             LevelCoordinatesSource(self),
679         ]
680 
681     def __contains__(self, key: Any) -> bool:
682         return key in self.data
683 
684     @property
685     def loc(self) -> _LocIndexer:
686         """Attribute for location based indexing like pandas.
687         """
688         return _LocIndexer(self)
689 
690     @property
691     def attrs(self) -> Dict[Hashable, Any]:
692         """Dictionary storing arbitrary metadata with this array."""
693         return self.variable.attrs
694 
695     @attrs.setter
696     def attrs(self, value: Mapping[Hashable, Any]) -> None:
697         # Disable type checking to work around mypy bug - see mypy#4167
698         self.variable.attrs = value  # type: ignore
699 
700     @property
701     def encoding(self) -> Dict[Hashable, Any]:
702         """Dictionary of format-specific settings for how this array should be
703         serialized."""
704         return self.variable.encoding
705 
706     @encoding.setter
707     def encoding(self, value: Mapping[Hashable, Any]) -> None:
708         self.variable.encoding = value
709 
710     @property
711     def indexes(self) -> Indexes:
712         """Mapping of pandas.Index objects used for label based indexing
713         """
714         if self._indexes is None:
715             self._indexes = default_indexes(self._coords, self.dims)
716         return Indexes(self._indexes)
717 
718     @property
719     def coords(self) -> DataArrayCoordinates:
720         """Dictionary-like container of coordinate arrays.
721         """
722         return DataArrayCoordinates(self)
723 
724     def reset_coords(
725         self,
726         names: Union[Iterable[Hashable], Hashable, None] = None,
727         drop: bool = False,
728         inplace: bool = None,
729     ) -> Union[None, "DataArray", Dataset]:
730         """Given names of coordinates, reset them to become variables.
731 
732         Parameters
733         ----------
734         names : hashable or iterable of hashables, optional
735             Name(s) of non-index coordinates in this dataset to reset into
736             variables. By default, all non-index coordinates are reset.
737         drop : bool, optional
738             If True, remove coordinates instead of converting them into
739             variables.
740 
741         Returns
742         -------
743         Dataset, or DataArray if ``drop == True``
744         """
745         _check_inplace(inplace)
746         if names is None:
747             names = set(self.coords) - set(self.dims)
748         dataset = self.coords.to_dataset().reset_coords(names, drop)
749         if drop:
750             return self._replace(coords=dataset._variables)
751         else:
752             if self.name is None:
753                 raise ValueError(
754                     "cannot reset_coords with drop=False on an unnamed DataArrray"
755                 )
756             dataset[self.name] = self.variable
757             return dataset
758 
759     def __dask_tokenize__(self):
760         from dask.base import normalize_token
761 
762         return normalize_token((type(self), self._variable, self._coords, self._name))
763 
764     def __dask_graph__(self):
765         return self._to_temp_dataset().__dask_graph__()
766 
767     def __dask_keys__(self):
768         return self._to_temp_dataset().__dask_keys__()
769 
770     def __dask_layers__(self):
771         return self._to_temp_dataset().__dask_layers__()
772 
773     @property
774     def __dask_optimize__(self):
775         return self._to_temp_dataset().__dask_optimize__
776 
777     @property
778     def __dask_scheduler__(self):
779         return self._to_temp_dataset().__dask_scheduler__
780 
781     def __dask_postcompute__(self):
782         func, args = self._to_temp_dataset().__dask_postcompute__()
783         return self._dask_finalize, (func, args, self.name)
784 
785     def __dask_postpersist__(self):
786         func, args = self._to_temp_dataset().__dask_postpersist__()
787         return self._dask_finalize, (func, args, self.name)
788 
789     @staticmethod
790     def _dask_finalize(results, func, args, name):
791         ds = func(results, *args)
792         variable = ds._variables.pop(_THIS_ARRAY)
793         coords = ds._variables
794         return DataArray(variable, coords, name=name, fastpath=True)
795 
796     def load(self, **kwargs) -> "DataArray":
797         """Manually trigger loading of this array's data from disk or a
798         remote source into memory and return this array.
799 
800         Normally, it should not be necessary to call this method in user code,
801         because all xarray functions should either work on deferred data or
802         load data automatically. However, this method can be necessary when
803         working with many file objects on disk.
804 
805         Parameters
806         ----------
807         **kwargs : dict
808             Additional keyword arguments passed on to ``dask.array.compute``.
809 
810         See Also
811         --------
812         dask.array.compute
813         """
814         ds = self._to_temp_dataset().load(**kwargs)
815         new = self._from_temp_dataset(ds)
816         self._variable = new._variable
817         self._coords = new._coords
818         return self
819 
820     def compute(self, **kwargs) -> "DataArray":
821         """Manually trigger loading of this array's data from disk or a
822         remote source into memory and return a new array. The original is
823         left unaltered.
824 
825         Normally, it should not be necessary to call this method in user code,
826         because all xarray functions should either work on deferred data or
827         load data automatically. However, this method can be necessary when
828         working with many file objects on disk.
829 
830         Parameters
831         ----------
832         **kwargs : dict
833             Additional keyword arguments passed on to ``dask.array.compute``.
834 
835         See Also
836         --------
837         dask.array.compute
838         """
839         new = self.copy(deep=False)
840         return new.load(**kwargs)
841 
842     def persist(self, **kwargs) -> "DataArray":
843         """ Trigger computation in constituent dask arrays
844 
845         This keeps them as dask arrays but encourages them to keep data in
846         memory.  This is particularly useful when on a distributed machine.
847         When on a single machine consider using ``.compute()`` instead.
848 
849         Parameters
850         ----------
851         **kwargs : dict
852             Additional keyword arguments passed on to ``dask.persist``.
853 
854         See Also
855         --------
856         dask.persist
857         """
858         ds = self._to_temp_dataset().persist(**kwargs)
859         return self._from_temp_dataset(ds)
860 
861     def copy(self, deep: bool = True, data: Any = None) -> "DataArray":
862         """Returns a copy of this array.
863 
864         If `deep=True`, a deep copy is made of the data array.
865         Otherwise, a shallow copy is made, so each variable in the new
866         array's dataset is also a variable in this array's dataset.
867 
868         Use `data` to create a new object with the same structure as
869         original but entirely new data.
870 
871         Parameters
872         ----------
873         deep : bool, optional
874             Whether the data array and its coordinates are loaded into memory
875             and copied onto the new object. Default is True.
876         data : array_like, optional
877             Data to use in the new object. Must have same shape as original.
878             When `data` is used, `deep` is ignored for all data variables,
879             and only used for coords.
880 
881         Returns
882         -------
883         object : DataArray
884             New object with dimensions, attributes, coordinates, name,
885             encoding, and optionally data copied from original.
886 
887         Examples
888         --------
889 
890         Shallow versus deep copy
891 
892         >>> array = xr.DataArray([1, 2, 3], dims='x',
893         ...                      coords={'x': ['a', 'b', 'c']})
894         >>> array.copy()
895         <xarray.DataArray (x: 3)>
896         array([1, 2, 3])
897         Coordinates:
898         * x        (x) <U1 'a' 'b' 'c'
899         >>> array_0 = array.copy(deep=False)
900         >>> array_0[0] = 7
901         >>> array_0
902         <xarray.DataArray (x: 3)>
903         array([7, 2, 3])
904         Coordinates:
905         * x        (x) <U1 'a' 'b' 'c'
906         >>> array
907         <xarray.DataArray (x: 3)>
908         array([7, 2, 3])
909         Coordinates:
910         * x        (x) <U1 'a' 'b' 'c'
911 
912         Changing the data using the ``data`` argument maintains the
913         structure of the original object, but with the new data. Original
914         object is unaffected.
915 
916         >>> array.copy(data=[0.1, 0.2, 0.3])
917         <xarray.DataArray (x: 3)>
918         array([ 0.1,  0.2,  0.3])
919         Coordinates:
920         * x        (x) <U1 'a' 'b' 'c'
921         >>> array
922         <xarray.DataArray (x: 3)>
923         array([1, 2, 3])
924         Coordinates:
925         * x        (x) <U1 'a' 'b' 'c'
926 
927         See Also
928         --------
929         pandas.DataFrame.copy
930         """
931         variable = self.variable.copy(deep=deep, data=data)
932         coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}
933         indexes = self._indexes
934         return self._replace(variable, coords, indexes=indexes)
935 
936     def __copy__(self) -> "DataArray":
937         return self.copy(deep=False)
938 
939     def __deepcopy__(self, memo=None) -> "DataArray":
940         # memo does nothing but is required for compatibility with
941         # copy.deepcopy
942         return self.copy(deep=True)
943 
944     # mutable objects should not be hashable
945     # https://github.com/python/mypy/issues/4266
946     __hash__ = None  # type: ignore
947 
948     @property
949     def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:
950         """Block dimensions for this array's data or None if it's not a dask
951         array.
952         """
953         return self.variable.chunks
954 
955     def chunk(
956         self,
957         chunks: Union[
958             None,
959             Number,
960             Tuple[Number, ...],
961             Tuple[Tuple[Number, ...], ...],
962             Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]],
963         ] = None,
964         name_prefix: str = "xarray-",
965         token: str = None,
966         lock: bool = False,
967     ) -> "DataArray":
968         """Coerce this array's data into a dask arrays with the given chunks.
969 
970         If this variable is a non-dask array, it will be converted to dask
971         array. If it's a dask array, it will be rechunked to the given chunk
972         sizes.
973 
974         If neither chunks is not provided for one or more dimensions, chunk
975         sizes along that dimension will not be updated; non-dask arrays will be
976         converted into dask arrays with a single block.
977 
978         Parameters
979         ----------
980         chunks : int, tuple or mapping, optional
981             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
982             ``{'x': 5, 'y': 5}``.
983         name_prefix : str, optional
984             Prefix for the name of the new dask array.
985         token : str, optional
986             Token uniquely identifying this array.
987         lock : optional
988             Passed on to :py:func:`dask.array.from_array`, if the array is not
989             already as dask array.
990 
991         Returns
992         -------
993         chunked : xarray.DataArray
994         """
995         if isinstance(chunks, (tuple, list)):
996             chunks = dict(zip(self.dims, chunks))
997 
998         ds = self._to_temp_dataset().chunk(
999             chunks, name_prefix=name_prefix, token=token, lock=lock
1000         )
1001         return self._from_temp_dataset(ds)
1002 
1003     def isel(
1004         self,
1005         indexers: Mapping[Hashable, Any] = None,
1006         drop: bool = False,
1007         **indexers_kwargs: Any,
1008     ) -> "DataArray":
1009         """Return a new DataArray whose data is given by integer indexing
1010         along the specified dimension(s).
1011 
1012         See Also
1013         --------
1014         Dataset.isel
1015         DataArray.sel
1016         """
1017         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1018         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1019             ds = self._to_temp_dataset()._isel_fancy(indexers, drop=drop)
1020             return self._from_temp_dataset(ds)
1021 
1022         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1023         # lists, or zero or one-dimensional np.ndarray's
1024 
1025         variable = self._variable.isel(indexers)
1026 
1027         coords = {}
1028         for coord_name, coord_value in self._coords.items():
1029             coord_indexers = {
1030                 k: v for k, v in indexers.items() if k in coord_value.dims
1031             }
1032             if coord_indexers:
1033                 coord_value = coord_value.isel(coord_indexers)
1034                 if drop and coord_value.ndim == 0:
1035                     continue
1036             coords[coord_name] = coord_value
1037 
1038         return self._replace(variable=variable, coords=coords)
1039 
1040     def sel(
1041         self,
1042         indexers: Mapping[Hashable, Any] = None,
1043         method: str = None,
1044         tolerance=None,
1045         drop: bool = False,
1046         **indexers_kwargs: Any,
1047     ) -> "DataArray":
1048         """Return a new DataArray whose data is given by selecting index
1049         labels along the specified dimension(s).
1050 
1051         .. warning::
1052 
1053           Do not try to assign values when using any of the indexing methods
1054           ``isel`` or ``sel``::
1055 
1056             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1057             # DO NOT do this
1058             da.isel(x=[0, 1, 2])[1] = -1
1059 
1060           Assigning values with the chained indexing using ``.sel`` or
1061           ``.isel`` fails silently.
1062 
1063         See Also
1064         --------
1065         Dataset.sel
1066         DataArray.isel
1067 
1068         """
1069         ds = self._to_temp_dataset().sel(
1070             indexers=indexers,
1071             drop=drop,
1072             method=method,
1073             tolerance=tolerance,
1074             **indexers_kwargs,
1075         )
1076         return self._from_temp_dataset(ds)
1077 
1078     def head(
1079         self,
1080         indexers: Union[Mapping[Hashable, int], int] = None,
1081         **indexers_kwargs: Any,
1082     ) -> "DataArray":
1083         """Return a new DataArray whose data is given by the the first `n`
1084         values along the specified dimension(s). Default `n` = 5
1085 
1086         See Also
1087         --------
1088         Dataset.head
1089         DataArray.tail
1090         DataArray.thin
1091         """
1092         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1093         return self._from_temp_dataset(ds)
1094 
1095     def tail(
1096         self,
1097         indexers: Union[Mapping[Hashable, int], int] = None,
1098         **indexers_kwargs: Any,
1099     ) -> "DataArray":
1100         """Return a new DataArray whose data is given by the the last `n`
1101         values along the specified dimension(s). Default `n` = 5
1102 
1103         See Also
1104         --------
1105         Dataset.tail
1106         DataArray.head
1107         DataArray.thin
1108         """
1109         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1110         return self._from_temp_dataset(ds)
1111 
1112     def thin(
1113         self,
1114         indexers: Union[Mapping[Hashable, int], int] = None,
1115         **indexers_kwargs: Any,
1116     ) -> "DataArray":
1117         """Return a new DataArray whose data is given by each `n` value
1118         along the specified dimension(s).
1119 
1120         See Also
1121         --------
1122         Dataset.thin
1123         DataArray.head
1124         DataArray.tail
1125         """
1126         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1127         return self._from_temp_dataset(ds)
1128 
1129     def broadcast_like(
1130         self, other: Union["DataArray", Dataset], exclude: Iterable[Hashable] = None
1131     ) -> "DataArray":
1132         """Broadcast this DataArray against another Dataset or DataArray.
1133 
1134         This is equivalent to xr.broadcast(other, self)[1]
1135 
1136         xarray objects are broadcast against each other in arithmetic
1137         operations, so this method is not be necessary for most uses.
1138 
1139         If no change is needed, the input data is returned to the output
1140         without being copied.
1141 
1142         If new coords are added by the broadcast, their values are
1143         NaN filled.
1144 
1145         Parameters
1146         ----------
1147         other : Dataset or DataArray
1148             Object against which to broadcast this array.
1149         exclude : iterable of hashable, optional
1150             Dimensions that must not be broadcasted
1151 
1152         Returns
1153         -------
1154         new_da: xr.DataArray
1155 
1156         Examples
1157         --------
1158 
1159         >>> arr1
1160         <xarray.DataArray (x: 2, y: 3)>
1161         array([[0.840235, 0.215216, 0.77917 ],
1162                [0.726351, 0.543824, 0.875115]])
1163         Coordinates:
1164           * x        (x) <U1 'a' 'b'
1165           * y        (y) <U1 'a' 'b' 'c'
1166         >>> arr2
1167         <xarray.DataArray (x: 3, y: 2)>
1168         array([[0.612611, 0.125753],
1169                [0.853181, 0.948818],
1170                [0.180885, 0.33363 ]])
1171         Coordinates:
1172           * x        (x) <U1 'a' 'b' 'c'
1173           * y        (y) <U1 'a' 'b'
1174         >>> arr1.broadcast_like(arr2)
1175         <xarray.DataArray (x: 3, y: 3)>
1176         array([[0.840235, 0.215216, 0.77917 ],
1177                [0.726351, 0.543824, 0.875115],
1178                [     nan,      nan,      nan]])
1179         Coordinates:
1180           * x        (x) object 'a' 'b' 'c'
1181           * y        (y) object 'a' 'b' 'c'
1182         """
1183         if exclude is None:
1184             exclude = set()
1185         else:
1186             exclude = set(exclude)
1187         args = align(other, self, join="outer", copy=False, exclude=exclude)
1188 
1189         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1190 
1191         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
1192 
1193     def reindex_like(
1194         self,
1195         other: Union["DataArray", Dataset],
1196         method: str = None,
1197         tolerance=None,
1198         copy: bool = True,
1199         fill_value=dtypes.NA,
1200     ) -> "DataArray":
1201         """Conform this object onto the indexes of another object, filling in
1202         missing values with ``fill_value``. The default fill value is NaN.
1203 
1204         Parameters
1205         ----------
1206         other : Dataset or DataArray
1207             Object with an 'indexes' attribute giving a mapping from dimension
1208             names to pandas.Index objects, which provides coordinates upon
1209             which to index the variables in this dataset. The indexes on this
1210             other object need not be the same as the indexes on this
1211             dataset. Any mis-matched index values will be filled in with
1212             NaN, and any mis-matched dimension names will simply be ignored.
1213         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1214             Method to use for filling index values from other not found on this
1215             data array:
1216 
1217             * None (default): don't fill gaps
1218             * pad / ffill: propagate last valid index value forward
1219             * backfill / bfill: propagate next valid index value backward
1220             * nearest: use nearest valid index value
1221         tolerance : optional
1222             Maximum distance between original and new labels for inexact
1223             matches. The values of the index at the matching locations must
1224             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1225         copy : bool, optional
1226             If ``copy=True``, data in the return value is always copied. If
1227             ``copy=False`` and reindexing is unnecessary, or can be performed
1228             with only slice operations, then the output may share memory with
1229             the input. In either case, a new xarray object is always returned.
1230         fill_value : scalar, optional
1231             Value to use for newly missing values
1232 
1233         Returns
1234         -------
1235         reindexed : DataArray
1236             Another dataset array, with this array's data but coordinates from
1237             the other object.
1238 
1239         See Also
1240         --------
1241         DataArray.reindex
1242         align
1243         """
1244         indexers = reindex_like_indexers(self, other)
1245         return self.reindex(
1246             indexers=indexers,
1247             method=method,
1248             tolerance=tolerance,
1249             copy=copy,
1250             fill_value=fill_value,
1251         )
1252 
1253     def reindex(
1254         self,
1255         indexers: Mapping[Hashable, Any] = None,
1256         method: str = None,
1257         tolerance=None,
1258         copy: bool = True,
1259         fill_value=dtypes.NA,
1260         **indexers_kwargs: Any,
1261     ) -> "DataArray":
1262         """Conform this object onto the indexes of another object, filling in
1263         missing values with ``fill_value``. The default fill value is NaN.
1264 
1265         Parameters
1266         ----------
1267         indexers : dict, optional
1268             Dictionary with keys given by dimension names and values given by
1269             arrays of coordinates tick labels. Any mis-matched coordinate
1270             values will be filled in with NaN, and any mis-matched dimension
1271             names will simply be ignored.
1272             One of indexers or indexers_kwargs must be provided.
1273         copy : bool, optional
1274             If ``copy=True``, data in the return value is always copied. If
1275             ``copy=False`` and reindexing is unnecessary, or can be performed
1276             with only slice operations, then the output may share memory with
1277             the input. In either case, a new xarray object is always returned.
1278         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1279             Method to use for filling index values in ``indexers`` not found on
1280             this data array:
1281 
1282             * None (default): don't fill gaps
1283             * pad / ffill: propagate last valid index value forward
1284             * backfill / bfill: propagate next valid index value backward
1285             * nearest: use nearest valid index value
1286         tolerance : optional
1287             Maximum distance between original and new labels for inexact
1288             matches. The values of the index at the matching locations must
1289             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1290         fill_value : scalar, optional
1291             Value to use for newly missing values
1292         **indexers_kwargs : {dim: indexer, ...}, optional
1293             The keyword arguments form of ``indexers``.
1294             One of indexers or indexers_kwargs must be provided.
1295 
1296         Returns
1297         -------
1298         reindexed : DataArray
1299             Another dataset array, with this array's data but replaced
1300             coordinates.
1301 
1302         See Also
1303         --------
1304         DataArray.reindex_like
1305         align
1306         """
1307         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
1308         ds = self._to_temp_dataset().reindex(
1309             indexers=indexers,
1310             method=method,
1311             tolerance=tolerance,
1312             copy=copy,
1313             fill_value=fill_value,
1314         )
1315         return self._from_temp_dataset(ds)
1316 
1317     def interp(
1318         self,
1319         coords: Mapping[Hashable, Any] = None,
1320         method: str = "linear",
1321         assume_sorted: bool = False,
1322         kwargs: Mapping[str, Any] = None,
1323         **coords_kwargs: Any,
1324     ) -> "DataArray":
1325         """ Multidimensional interpolation of variables.
1326 
1327         coords : dict, optional
1328             Mapping from dimension names to the new coordinates.
1329             new coordinate can be an scalar, array-like or DataArray.
1330             If DataArrays are passed as new coordates, their dimensions are
1331             used for the broadcasting.
1332         method: {'linear', 'nearest'} for multidimensional array,
1333             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
1334             for 1-dimensional array.
1335         assume_sorted: boolean, optional
1336             If False, values of x can be in any order and they are sorted
1337             first. If True, x has to be an array of monotonically increasing
1338             values.
1339         kwargs: dictionary
1340             Additional keyword passed to scipy's interpolator.
1341         ``**coords_kwargs`` : {dim: coordinate, ...}, optional
1342             The keyword arguments form of ``coords``.
1343             One of coords or coords_kwargs must be provided.
1344 
1345         Returns
1346         -------
1347         interpolated: xr.DataArray
1348             New dataarray on the new coordinates.
1349 
1350         Notes
1351         -----
1352         scipy is required.
1353 
1354         See Also
1355         --------
1356         scipy.interpolate.interp1d
1357         scipy.interpolate.interpn
1358 
1359         Examples
1360         --------
1361         >>> da = xr.DataArray([1, 3], [('x', np.arange(2))])
1362         >>> da.interp(x=0.5)
1363         <xarray.DataArray ()>
1364         array(2.0)
1365         Coordinates:
1366             x        float64 0.5
1367         """
1368         if self.dtype.kind not in "uifc":
1369             raise TypeError(
1370                 "interp only works for a numeric type array. "
1371                 "Given {}.".format(self.dtype)
1372             )
1373         ds = self._to_temp_dataset().interp(
1374             coords,
1375             method=method,
1376             kwargs=kwargs,
1377             assume_sorted=assume_sorted,
1378             **coords_kwargs,
1379         )
1380         return self._from_temp_dataset(ds)
1381 
1382     def interp_like(
1383         self,
1384         other: Union["DataArray", Dataset],
1385         method: str = "linear",
1386         assume_sorted: bool = False,
1387         kwargs: Mapping[str, Any] = None,
1388     ) -> "DataArray":
1389         """Interpolate this object onto the coordinates of another object,
1390         filling out of range values with NaN.
1391 
1392         Parameters
1393         ----------
1394         other : Dataset or DataArray
1395             Object with an 'indexes' attribute giving a mapping from dimension
1396             names to an 1d array-like, which provides coordinates upon
1397             which to index the variables in this dataset.
1398         method: string, optional.
1399             {'linear', 'nearest'} for multidimensional array,
1400             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
1401             for 1-dimensional array. 'linear' is used by default.
1402         assume_sorted: boolean, optional
1403             If False, values of coordinates that are interpolated over can be
1404             in any order and they are sorted first. If True, interpolated
1405             coordinates are assumed to be an array of monotonically increasing
1406             values.
1407         kwargs: dictionary, optional
1408             Additional keyword passed to scipy's interpolator.
1409 
1410         Returns
1411         -------
1412         interpolated: xr.DataArray
1413             Another dataarray by interpolating this dataarray's data along the
1414             coordinates of the other object.
1415 
1416         Notes
1417         -----
1418         scipy is required.
1419         If the dataarray has object-type coordinates, reindex is used for these
1420         coordinates instead of the interpolation.
1421 
1422         See Also
1423         --------
1424         DataArray.interp
1425         DataArray.reindex_like
1426         """
1427         if self.dtype.kind not in "uifc":
1428             raise TypeError(
1429                 "interp only works for a numeric type array. "
1430                 "Given {}.".format(self.dtype)
1431             )
1432         ds = self._to_temp_dataset().interp_like(
1433             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
1434         )
1435         return self._from_temp_dataset(ds)
1436 
1437     def rename(
1438         self,
1439         new_name_or_name_dict: Union[Hashable, Mapping[Hashable, Hashable]] = None,
1440         **names: Hashable,
1441     ) -> "DataArray":
1442         """Returns a new DataArray with renamed coordinates or a new name.
1443 
1444         Parameters
1445         ----------
1446         new_name_or_name_dict : str or dict-like, optional
1447             If the argument is dict-like, it used as a mapping from old
1448             names to new names for coordinates. Otherwise, use the argument
1449             as the new name for this array.
1450         **names: hashable, optional
1451             The keyword arguments form of a mapping from old names to
1452             new names for coordinates.
1453             One of new_name_or_name_dict or names must be provided.
1454 
1455         Returns
1456         -------
1457         renamed : DataArray
1458             Renamed array or array with renamed coordinates.
1459 
1460         See Also
1461         --------
1462         Dataset.rename
1463         DataArray.swap_dims
1464         """
1465         if names or utils.is_dict_like(new_name_or_name_dict):
1466             new_name_or_name_dict = cast(
1467                 Mapping[Hashable, Hashable], new_name_or_name_dict
1468             )
1469             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
1470             dataset = self._to_temp_dataset().rename(name_dict)
1471             return self._from_temp_dataset(dataset)
1472         else:
1473             new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)
1474             return self._replace(name=new_name_or_name_dict)
1475 
1476     def swap_dims(self, dims_dict: Mapping[Hashable, Hashable]) -> "DataArray":
1477         """Returns a new DataArray with swapped dimensions.
1478 
1479         Parameters
1480         ----------
1481         dims_dict : dict-like
1482             Dictionary whose keys are current dimension names and whose values
1483             are new names. Each value must already be a coordinate on this
1484             array.
1485 
1486         Returns
1487         -------
1488         swapped : DataArray
1489             DataArray with swapped dimensions.
1490 
1491         Examples
1492         --------
1493         >>> arr = xr.DataArray(data=[0, 1], dims="x",
1494                                coords={"x": ["a", "b"], "y": ("x", [0, 1])})
1495         >>> arr
1496         <xarray.DataArray (x: 2)>
1497         array([0, 1])
1498         Coordinates:
1499           * x        (x) <U1 'a' 'b'
1500             y        (x) int64 0 1
1501         >>> arr.swap_dims({"x": "y"})
1502         <xarray.DataArray (y: 2)>
1503         array([0, 1])
1504         Coordinates:
1505             x        (y) <U1 'a' 'b'
1506           * y        (y) int64 0 1
1507 
1508         See Also
1509         --------
1510 
1511         DataArray.rename
1512         Dataset.swap_dims
1513         """
1514         ds = self._to_temp_dataset().swap_dims(dims_dict)
1515         return self._from_temp_dataset(ds)
1516 
1517     def expand_dims(
1518         self,
1519         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,
1520         axis=None,
1521         **dim_kwargs: Any,
1522     ) -> "DataArray":
1523         """Return a new object with an additional axis (or axes) inserted at
1524         the corresponding position in the array shape. The new object is a
1525         view into the underlying array, not a copy.
1526 
1527 
1528         If dim is already a scalar coordinate, it will be promoted to a 1D
1529         coordinate consisting of a single value.
1530 
1531         Parameters
1532         ----------
1533         dim : hashable, sequence of hashable, dict, or None
1534             Dimensions to include on the new variable.
1535             If provided as str or sequence of str, then dimensions are inserted
1536             with length 1. If provided as a dict, then the keys are the new
1537             dimensions and the values are either integers (giving the length of
1538             the new dimensions) or sequence/ndarray (giving the coordinates of
1539             the new dimensions).
1540         axis : integer, list (or tuple) of integers, or None
1541             Axis position(s) where new axis is to be inserted (position(s) on
1542             the result array). If a list (or tuple) of integers is passed,
1543             multiple axes are inserted. In this case, dim arguments should be
1544             same length list. If axis=None is passed, all the axes will be
1545             inserted to the start of the result array.
1546         **dim_kwargs : int or sequence/ndarray
1547             The keywords are arbitrary dimensions being inserted and the values
1548             are either the lengths of the new dims (if int is given), or their
1549             coordinates. Note, this is an alternative to passing a dict to the
1550             dim kwarg and will only be used if dim is None.
1551 
1552         Returns
1553         -------
1554         expanded : same type as caller
1555             This object, but with an additional dimension(s).
1556         """
1557         if isinstance(dim, int):
1558             raise TypeError("dim should be hashable or sequence/mapping of hashables")
1559         elif isinstance(dim, Sequence) and not isinstance(dim, str):
1560             if len(dim) != len(set(dim)):
1561                 raise ValueError("dims should not contain duplicate values.")
1562             dim = dict.fromkeys(dim, 1)
1563         elif dim is not None and not isinstance(dim, Mapping):
1564             dim = {cast(Hashable, dim): 1}
1565 
1566         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
1567         ds = self._to_temp_dataset().expand_dims(dim, axis)
1568         return self._from_temp_dataset(ds)
1569 
1570     def set_index(
1571         self,
1572         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
1573         append: bool = False,
1574         inplace: bool = None,
1575         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],
1576     ) -> Optional["DataArray"]:
1577         """Set DataArray (multi-)indexes using one or more existing
1578         coordinates.
1579 
1580         Parameters
1581         ----------
1582         indexes : {dim: index, ...}
1583             Mapping from names matching dimensions and values given
1584             by (lists of) the names of existing coordinates or variables to set
1585             as new (multi-)index.
1586         append : bool, optional
1587             If True, append the supplied index(es) to the existing index(es).
1588             Otherwise replace the existing index(es) (default).
1589         **indexes_kwargs: optional
1590             The keyword arguments form of ``indexes``.
1591             One of indexes or indexes_kwargs must be provided.
1592 
1593         Returns
1594         -------
1595         obj : DataArray
1596             Another DataArray, with this data but replaced coordinates.
1597 
1598         Examples
1599         --------
1600         >>> arr = xr.DataArray(data=np.ones((2, 3)),
1601         ...                    dims=['x', 'y'],
1602         ...                    coords={'x':
1603         ...                        range(2), 'y':
1604         ...                        range(3), 'a': ('x', [3, 4])
1605         ...                    })
1606         >>> arr
1607         <xarray.DataArray (x: 2, y: 3)>
1608         array([[1., 1., 1.],
1609                [1., 1., 1.]])
1610         Coordinates:
1611           * x        (x) int64 0 1
1612           * y        (y) int64 0 1 2
1613             a        (x) int64 3 4
1614         >>> arr.set_index(x='a')
1615         <xarray.DataArray (x: 2, y: 3)>
1616         array([[1., 1., 1.],
1617                [1., 1., 1.]])
1618         Coordinates:
1619           * x        (x) int64 3 4
1620           * y        (y) int64 0 1 2
1621 
1622         See Also
1623         --------
1624         DataArray.reset_index
1625         """
1626         ds = self._to_temp_dataset().set_index(
1627             indexes, append=append, inplace=inplace, **indexes_kwargs
1628         )
1629         return self._from_temp_dataset(ds)
1630 
1631     def reset_index(
1632         self,
1633         dims_or_levels: Union[Hashable, Sequence[Hashable]],
1634         drop: bool = False,
1635         inplace: bool = None,
1636     ) -> Optional["DataArray"]:
1637         """Reset the specified index(es) or multi-index level(s).
1638 
1639         Parameters
1640         ----------
1641         dims_or_levels : hashable or sequence of hashables
1642             Name(s) of the dimension(s) and/or multi-index level(s) that will
1643             be reset.
1644         drop : bool, optional
1645             If True, remove the specified indexes and/or multi-index levels
1646             instead of extracting them as new coordinates (default: False).
1647 
1648         Returns
1649         -------
1650         obj : DataArray
1651             Another dataarray, with this dataarray's data but replaced
1652             coordinates.
1653 
1654         See Also
1655         --------
1656         DataArray.set_index
1657         """
1658         _check_inplace(inplace)
1659         coords, _ = split_indexes(
1660             dims_or_levels, self._coords, set(), self._level_coords, drop=drop
1661         )
1662         return self._replace(coords=coords)
1663 
1664     def reorder_levels(
1665         self,
1666         dim_order: Mapping[Hashable, Sequence[int]] = None,
1667         inplace: bool = None,
1668         **dim_order_kwargs: Sequence[int],
1669     ) -> "DataArray":
1670         """Rearrange index levels using input order.
1671 
1672         Parameters
1673         ----------
1674         dim_order : optional
1675             Mapping from names matching dimensions and values given
1676             by lists representing new level orders. Every given dimension
1677             must have a multi-index.
1678         **dim_order_kwargs: optional
1679             The keyword arguments form of ``dim_order``.
1680             One of dim_order or dim_order_kwargs must be provided.
1681 
1682         Returns
1683         -------
1684         obj : DataArray
1685             Another dataarray, with this dataarray's data but replaced
1686             coordinates.
1687         """
1688         _check_inplace(inplace)
1689         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
1690         replace_coords = {}
1691         for dim, order in dim_order.items():
1692             coord = self._coords[dim]
1693             index = coord.to_index()
1694             if not isinstance(index, pd.MultiIndex):
1695                 raise ValueError("coordinate %r has no MultiIndex" % dim)
1696             replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))
1697         coords = self._coords.copy()
1698         coords.update(replace_coords)
1699         return self._replace(coords=coords)
1700 
1701     def stack(
1702         self,
1703         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
1704         **dimensions_kwargs: Sequence[Hashable],
1705     ) -> "DataArray":
1706         """
1707         Stack any number of existing dimensions into a single new dimension.
1708 
1709         New dimensions will be added at the end, and the corresponding
1710         coordinate variables will be combined into a MultiIndex.
1711 
1712         Parameters
1713         ----------
1714         dimensions : Mapping of the form new_name=(dim1, dim2, ...)
1715             Names of new dimensions, and the existing dimensions that they
1716             replace.
1717         **dimensions_kwargs:
1718             The keyword arguments form of ``dimensions``.
1719             One of dimensions or dimensions_kwargs must be provided.
1720 
1721         Returns
1722         -------
1723         stacked : DataArray
1724             DataArray with stacked data.
1725 
1726         Examples
1727         --------
1728 
1729         >>> arr = DataArray(np.arange(6).reshape(2, 3),
1730         ...                 coords=[('x', ['a', 'b']), ('y', [0, 1, 2])])
1731         >>> arr
1732         <xarray.DataArray (x: 2, y: 3)>
1733         array([[0, 1, 2],
1734                [3, 4, 5]])
1735         Coordinates:
1736           * x        (x) |S1 'a' 'b'
1737           * y        (y) int64 0 1 2
1738         >>> stacked = arr.stack(z=('x', 'y'))
1739         >>> stacked.indexes['z']
1740         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1741                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],
1742                    names=['x', 'y'])
1743 
1744         See Also
1745         --------
1746         DataArray.unstack
1747         """
1748         ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)
1749         return self._from_temp_dataset(ds)
1750 
1751     def unstack(
1752         self,
1753         dim: Union[Hashable, Sequence[Hashable], None] = None,
1754         fill_value: Any = dtypes.NA,
1755         sparse: bool = False,
1756     ) -> "DataArray":
1757         """
1758         Unstack existing dimensions corresponding to MultiIndexes into
1759         multiple new dimensions.
1760 
1761         New dimensions will be added at the end.
1762 
1763         Parameters
1764         ----------
1765         dim : hashable or sequence of hashable, optional
1766             Dimension(s) over which to unstack. By default unstacks all
1767             MultiIndexes.
1768         fill_value: value to be filled. By default, np.nan
1769         sparse: use sparse-array if True
1770 
1771         Returns
1772         -------
1773         unstacked : DataArray
1774             Array with unstacked data.
1775 
1776         Examples
1777         --------
1778 
1779         >>> arr = DataArray(np.arange(6).reshape(2, 3),
1780         ...                 coords=[('x', ['a', 'b']), ('y', [0, 1, 2])])
1781         >>> arr
1782         <xarray.DataArray (x: 2, y: 3)>
1783         array([[0, 1, 2],
1784                [3, 4, 5]])
1785         Coordinates:
1786           * x        (x) |S1 'a' 'b'
1787           * y        (y) int64 0 1 2
1788         >>> stacked = arr.stack(z=('x', 'y'))
1789         >>> stacked.indexes['z']
1790         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1791                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],
1792                    names=['x', 'y'])
1793         >>> roundtripped = stacked.unstack()
1794         >>> arr.identical(roundtripped)
1795         True
1796 
1797         See Also
1798         --------
1799         DataArray.stack
1800         """
1801         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
1802         return self._from_temp_dataset(ds)
1803 
1804     def to_unstacked_dataset(self, dim, level=0):
1805         """Unstack DataArray expanding to Dataset along a given level of a
1806         stacked coordinate.
1807 
1808         This is the inverse operation of Dataset.to_stacked_array.
1809 
1810         Parameters
1811         ----------
1812         dim : str
1813             Name of existing dimension to unstack
1814         level : int or str
1815             The MultiIndex level to expand to a dataset along. Can either be
1816             the integer index of the level or its name.
1817         label : int, default 0
1818             Label of the level to expand dataset along. Overrides the label
1819             argument if given.
1820 
1821         Returns
1822         -------
1823         unstacked: Dataset
1824 
1825         Examples
1826         --------
1827         >>> import xarray as xr
1828         >>> arr = DataArray(np.arange(6).reshape(2, 3),
1829         ...                 coords=[('x', ['a', 'b']), ('y', [0, 1, 2])])
1830         >>> data = xr.Dataset({'a': arr, 'b': arr.isel(y=0)})
1831         >>> data
1832         <xarray.Dataset>
1833         Dimensions:  (x: 2, y: 3)
1834         Coordinates:
1835           * x        (x) <U1 'a' 'b'
1836           * y        (y) int64 0 1 2
1837         Data variables:
1838             a        (x, y) int64 0 1 2 3 4 5
1839             b        (x) int64 0 3
1840         >>> stacked = data.to_stacked_array("z", ['y'])
1841         >>> stacked.indexes['z']
1842         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1843                 labels=[[0, 0, 0, 1], [0, 1, 2, -1]],
1844                 names=['variable', 'y'])
1845         >>> roundtripped = stacked.to_unstacked_dataset(dim='z')
1846         >>> data.identical(roundtripped)
1847         True
1848 
1849         See Also
1850         --------
1851         Dataset.to_stacked_array
1852         """
1853 
1854         idx = self.indexes[dim]
1855         if not isinstance(idx, pd.MultiIndex):
1856             raise ValueError(f"'{dim}' is not a stacked coordinate")
1857 
1858         level_number = idx._get_level_number(level)
1859         variables = idx.levels[level_number]
1860         variable_dim = idx.names[level_number]
1861 
1862         # pull variables out of datarray
1863         data_dict = {}
1864         for k in variables:
1865             data_dict[k] = self.sel({variable_dim: k}).squeeze(drop=True)
1866 
1867         # unstacked dataset
1868         return Dataset(data_dict)
1869 
1870     def transpose(self, *dims: Hashable, transpose_coords: bool = None) -> "DataArray":
1871         """Return a new DataArray object with transposed dimensions.
1872 
1873         Parameters
1874         ----------
1875         *dims : hashable, optional
1876             By default, reverse the dimensions. Otherwise, reorder the
1877             dimensions to this order.
1878         transpose_coords : boolean, optional
1879             If True, also transpose the coordinates of this DataArray.
1880 
1881         Returns
1882         -------
1883         transposed : DataArray
1884             The returned DataArray's array is transposed.
1885 
1886         Notes
1887         -----
1888         This operation returns a view of this array's data. It is
1889         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
1890         -- the data will be fully loaded.
1891 
1892         See Also
1893         --------
1894         numpy.transpose
1895         Dataset.transpose
1896         """
1897         if dims:
1898             dims = tuple(utils.infix_dims(dims, self.dims))
1899         variable = self.variable.transpose(*dims)
1900         if transpose_coords:
1901             coords: Dict[Hashable, Variable] = {}
1902             for name, coord in self.coords.items():
1903                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
1904                 coords[name] = coord.variable.transpose(*coord_dims)
1905             return self._replace(variable, coords)
1906         else:
1907             if transpose_coords is None and any(self[c].ndim > 1 for c in self.coords):
1908                 warnings.warn(
1909                     "This DataArray contains multi-dimensional "
1910                     "coordinates. In the future, these coordinates "
1911                     "will be transposed as well unless you specify "
1912                     "transpose_coords=False.",
1913                     FutureWarning,
1914                     stacklevel=2,
1915                 )
1916             return self._replace(variable)
1917 
1918     @property
1919     def T(self) -> "DataArray":
1920         return self.transpose()
1921 
1922     def drop_vars(
1923         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
1924     ) -> "DataArray":
1925         """Drop variables from this DataArray.
1926 
1927         Parameters
1928         ----------
1929         names : hashable or iterable of hashables
1930             Name(s) of variables to drop.
1931         errors: {'raise', 'ignore'}, optional
1932             If 'raise' (default), raises a ValueError error if any of the variable
1933             passed are not in the dataset. If 'ignore', any given names that are in the
1934             DataArray are dropped and no error is raised.
1935 
1936         Returns
1937         -------
1938         dropped : Dataset
1939 
1940         """
1941         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
1942         return self._from_temp_dataset(ds)
1943 
1944     def drop(
1945         self,
1946         labels: Mapping = None,
1947         dim: Hashable = None,
1948         *,
1949         errors: str = "raise",
1950         **labels_kwargs,
1951     ) -> "DataArray":
1952         """Backward compatible method based on `drop_vars` and `drop_sel`
1953 
1954         Using either `drop_vars` or `drop_sel` is encouraged
1955 
1956         See Also
1957         --------
1958         DataArray.drop_vars
1959         DataArray.drop_sel
1960         """
1961         ds = self._to_temp_dataset().drop(labels, dim, errors=errors)
1962         return self._from_temp_dataset(ds)
1963 
1964     def drop_sel(
1965         self,
1966         labels: Mapping[Hashable, Any] = None,
1967         *,
1968         errors: str = "raise",
1969         **labels_kwargs,
1970     ) -> "DataArray":
1971         """Drop index labels from this DataArray.
1972 
1973         Parameters
1974         ----------
1975         labels : Mapping[Hashable, Any]
1976             Index labels to drop
1977         errors: {'raise', 'ignore'}, optional
1978             If 'raise' (default), raises a ValueError error if
1979             any of the index labels passed are not
1980             in the dataset. If 'ignore', any given labels that are in the
1981             dataset are dropped and no error is raised.
1982         **labels_kwargs : {dim: label, ...}, optional
1983             The keyword arguments form of ``dim`` and ``labels``
1984 
1985         Returns
1986         -------
1987         dropped : DataArray
1988         """
1989         if labels_kwargs or isinstance(labels, dict):
1990             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
1991 
1992         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
1993         return self._from_temp_dataset(ds)
1994 
1995     def dropna(
1996         self, dim: Hashable, how: str = "any", thresh: int = None
1997     ) -> "DataArray":
1998         """Returns a new array with dropped labels for missing values along
1999         the provided dimension.
2000 
2001         Parameters
2002         ----------
2003         dim : hashable
2004             Dimension along which to drop missing values. Dropping along
2005             multiple dimensions simultaneously is not yet supported.
2006         how : {'any', 'all'}, optional
2007             * any : if any NA values are present, drop that label
2008             * all : if all values are NA, drop that label
2009         thresh : int, default None
2010             If supplied, require this many non-NA values.
2011 
2012         Returns
2013         -------
2014         DataArray
2015         """
2016         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
2017         return self._from_temp_dataset(ds)
2018 
2019     def fillna(self, value: Any) -> "DataArray":
2020         """Fill missing values in this object.
2021 
2022         This operation follows the normal broadcasting and alignment rules that
2023         xarray uses for binary arithmetic, except the result is aligned to this
2024         object (``join='left'``) instead of aligned to the intersection of
2025         index coordinates (``join='inner'``).
2026 
2027         Parameters
2028         ----------
2029         value : scalar, ndarray or DataArray
2030             Used to fill all matching missing values in this array. If the
2031             argument is a DataArray, it is first aligned with (reindexed to)
2032             this array.
2033 
2034         Returns
2035         -------
2036         DataArray
2037         """
2038         if utils.is_dict_like(value):
2039             raise TypeError(
2040                 "cannot provide fill value as a dictionary with "
2041                 "fillna on a DataArray"
2042             )
2043         out = ops.fillna(self, value)
2044         return out
2045 
2046     def interpolate_na(
2047         self,
2048         dim: Hashable = None,
2049         method: str = "linear",
2050         limit: int = None,
2051         use_coordinate: Union[bool, str] = True,
2052         max_gap: Union[int, float, str, pd.Timedelta, np.timedelta64] = None,
2053         **kwargs: Any,
2054     ) -> "DataArray":
2055         """Fill in NaNs by interpolating according to different methods.
2056 
2057         Parameters
2058         ----------
2059         dim : str
2060             Specifies the dimension along which to interpolate.
2061         method : str, optional
2062             String indicating which method to use for interpolation:
2063 
2064             - 'linear': linear interpolation (Default). Additional keyword
2065               arguments are passed to :py:func:`numpy.interp`
2066             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
2067               are passed to :py:func:`scipy.interpolate.interp1d`. If
2068               ``method='polynomial'``, the ``order`` keyword argument must also be
2069               provided.
2070             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
2071               respective :py:class:`scipy.interpolate` classes.
2072 
2073         use_coordinate : bool, str, default True
2074             Specifies which index to use as the x values in the interpolation
2075             formulated as `y = f(x)`. If False, values are treated as if
2076             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
2077             used. If ``use_coordinate`` is a string, it specifies the name of a
2078             coordinate variariable to use as the index.
2079         limit : int, default None
2080             Maximum number of consecutive NaNs to fill. Must be greater than 0
2081             or None for no limit. This filling is done regardless of the size of
2082             the gap in the data. To only interpolate over gaps less than a given length,
2083             see ``max_gap``.
2084         max_gap: int, float, str, pandas.Timedelta, numpy.timedelta64, default None.
2085             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
2086             Use None for no limit. When interpolating along a datetime64 dimension
2087             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
2088 
2089             - a string that is valid input for pandas.to_timedelta
2090             - a :py:class:`numpy.timedelta64` object
2091             - a :py:class:`pandas.Timedelta` object
2092 
2093             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
2094             dimensions has not been implemented yet. Gap length is defined as the difference
2095             between coordinate values at the first data point after a gap and the last value
2096             before a gap. For gaps at the beginning (end), gap length is defined as the difference
2097             between coordinate values at the first (last) valid data point and the first (last) NaN.
2098             For example, consider::
2099 
2100                 <xarray.DataArray (x: 9)>
2101                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
2102                 Coordinates:
2103                   * x        (x) int64 0 1 2 3 4 5 6 7 8
2104 
2105             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
2106         kwargs : dict, optional
2107             parameters passed verbatim to the underlying interpolation function
2108 
2109         Returns
2110         -------
2111         interpolated: DataArray
2112             Filled in DataArray.
2113 
2114         See also
2115         --------
2116         numpy.interp
2117         scipy.interpolate
2118         """
2119         from .missing import interp_na
2120 
2121         return interp_na(
2122             self,
2123             dim=dim,
2124             method=method,
2125             limit=limit,
2126             use_coordinate=use_coordinate,
2127             max_gap=max_gap,
2128             **kwargs,
2129         )
2130 
2131     def ffill(self, dim: Hashable, limit: int = None) -> "DataArray":
2132         """Fill NaN values by propogating values forward
2133 
2134         *Requires bottleneck.*
2135 
2136         Parameters
2137         ----------
2138         dim : hashable
2139             Specifies the dimension along which to propagate values when
2140             filling.
2141         limit : int, default None
2142             The maximum number of consecutive NaN values to forward fill. In
2143             other words, if there is a gap with more than this number of
2144             consecutive NaNs, it will only be partially filled. Must be greater
2145             than 0 or None for no limit.
2146 
2147         Returns
2148         -------
2149         DataArray
2150         """
2151         from .missing import ffill
2152 
2153         return ffill(self, dim, limit=limit)
2154 
2155     def bfill(self, dim: Hashable, limit: int = None) -> "DataArray":
2156         """Fill NaN values by propogating values backward
2157 
2158         *Requires bottleneck.*
2159 
2160         Parameters
2161         ----------
2162         dim : str
2163             Specifies the dimension along which to propagate values when
2164             filling.
2165         limit : int, default None
2166             The maximum number of consecutive NaN values to backward fill. In
2167             other words, if there is a gap with more than this number of
2168             consecutive NaNs, it will only be partially filled. Must be greater
2169             than 0 or None for no limit.
2170 
2171         Returns
2172         -------
2173         DataArray
2174         """
2175         from .missing import bfill
2176 
2177         return bfill(self, dim, limit=limit)
2178 
2179     def combine_first(self, other: "DataArray") -> "DataArray":
2180         """Combine two DataArray objects, with union of coordinates.
2181 
2182         This operation follows the normal broadcasting and alignment rules of
2183         ``join='outer'``.  Default to non-null values of array calling the
2184         method.  Use np.nan to fill in vacant cells after alignment.
2185 
2186         Parameters
2187         ----------
2188         other : DataArray
2189             Used to fill all matching missing values in this array.
2190 
2191         Returns
2192         -------
2193         DataArray
2194         """
2195         return ops.fillna(self, other, join="outer")
2196 
2197     def reduce(
2198         self,
2199         func: Callable[..., Any],
2200         dim: Union[None, Hashable, Sequence[Hashable]] = None,
2201         axis: Union[None, int, Sequence[int]] = None,
2202         keep_attrs: bool = None,
2203         keepdims: bool = False,
2204         **kwargs: Any,
2205     ) -> "DataArray":
2206         """Reduce this array by applying `func` along some dimension(s).
2207 
2208         Parameters
2209         ----------
2210         func : function
2211             Function which can be called in the form
2212             `f(x, axis=axis, **kwargs)` to return the result of reducing an
2213             np.ndarray over an integer valued axis.
2214         dim : hashable or sequence of hashables, optional
2215             Dimension(s) over which to apply `func`.
2216         axis : int or sequence of int, optional
2217             Axis(es) over which to repeatedly apply `func`. Only one of the
2218             'dim' and 'axis' arguments can be supplied. If neither are
2219             supplied, then the reduction is calculated over the flattened array
2220             (by calling `f(x)` without an axis argument).
2221         keep_attrs : bool, optional
2222             If True, the variable's attributes (`attrs`) will be copied from
2223             the original object to the new one.  If False (default), the new
2224             object will be returned without attributes.
2225         keepdims : bool, default False
2226             If True, the dimensions which are reduced are left in the result
2227             as dimensions of size one. Coordinates that use these dimensions
2228             are removed.
2229         **kwargs : dict
2230             Additional keyword arguments passed on to `func`.
2231 
2232         Returns
2233         -------
2234         reduced : DataArray
2235             DataArray with this object's array replaced with an array with
2236             summarized data and the indicated dimension(s) removed.
2237         """
2238 
2239         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
2240         return self._replace_maybe_drop_dims(var)
2241 
2242     def to_pandas(self) -> Union["DataArray", pd.Series, pd.DataFrame]:
2243         """Convert this array into a pandas object with the same shape.
2244 
2245         The type of the returned object depends on the number of DataArray
2246         dimensions:
2247 
2248         * 0D -> `xarray.DataArray`
2249         * 1D -> `pandas.Series`
2250         * 2D -> `pandas.DataFrame`
2251         * 3D -> `pandas.Panel` *(deprecated)*
2252 
2253         Only works for arrays with 3 or fewer dimensions.
2254 
2255         The DataArray constructor performs the inverse transformation.
2256         """
2257         # TODO: consolidate the info about pandas constructors and the
2258         # attributes that correspond to their indexes into a separate module?
2259         constructors = {
2260             0: lambda x: x,
2261             1: pd.Series,
2262             2: pd.DataFrame,
2263             3: pdcompat.Panel,
2264         }
2265         try:
2266             constructor = constructors[self.ndim]
2267         except KeyError:
2268             raise ValueError(
2269                 "cannot convert arrays with %s dimensions into "
2270                 "pandas objects" % self.ndim
2271             )
2272         indexes = [self.get_index(dim) for dim in self.dims]
2273         return constructor(self.values, *indexes)
2274 
2275     def to_dataframe(self, name: Hashable = None) -> pd.DataFrame:
2276         """Convert this array and its coordinates into a tidy pandas.DataFrame.
2277 
2278         The DataFrame is indexed by the Cartesian product of index coordinates
2279         (in the form of a :py:class:`pandas.MultiIndex`).
2280 
2281         Other coordinates are included as columns in the DataFrame.
2282         """
2283         if name is None:
2284             name = self.name
2285         if name is None:
2286             raise ValueError(
2287                 "cannot convert an unnamed DataArray to a "
2288                 "DataFrame: use the ``name`` parameter"
2289             )
2290 
2291         dims = dict(zip(self.dims, self.shape))
2292         # By using a unique name, we can convert a DataArray into a DataFrame
2293         # even if it shares a name with one of its coordinates.
2294         # I would normally use unique_name = object() but that results in a
2295         # dataframe with columns in the wrong order, for reasons I have not
2296         # been able to debug (possibly a pandas bug?).
2297         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
2298         ds = self._to_dataset_whole(name=unique_name)
2299         df = ds._to_dataframe(dims)
2300         df.columns = [name if c == unique_name else c for c in df.columns]
2301         return df
2302 
2303     def to_series(self) -> pd.Series:
2304         """Convert this array into a pandas.Series.
2305 
2306         The Series is indexed by the Cartesian product of index coordinates
2307         (in the form of a :py:class:`pandas.MultiIndex`).
2308         """
2309         index = self.coords.to_index()
2310         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
2311 
2312     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
2313         """Convert this array into a numpy.ma.MaskedArray
2314 
2315         Parameters
2316         ----------
2317         copy : bool
2318             If True (default) make a copy of the array in the result. If False,
2319             a MaskedArray view of DataArray.values is returned.
2320 
2321         Returns
2322         -------
2323         result : MaskedArray
2324             Masked where invalid values (nan or inf) occur.
2325         """
2326         values = self.values  # only compute lazy arrays once
2327         isnull = pd.isnull(values)
2328         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
2329 
2330     def to_netcdf(self, *args, **kwargs) -> Union[bytes, "Delayed", None]:
2331         """Write DataArray contents to a netCDF file.
2332 
2333         All parameters are passed directly to `xarray.Dataset.to_netcdf`.
2334 
2335         Notes
2336         -----
2337         Only xarray.Dataset objects can be written to netCDF files, so
2338         the xarray.DataArray is converted to a xarray.Dataset object
2339         containing a single variable. If the DataArray has no name, or if the
2340         name is the same as a co-ordinate name, then it is given the name
2341         '__xarray_dataarray_variable__'.
2342         """
2343         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE
2344 
2345         if self.name is None:
2346             # If no name is set then use a generic xarray name
2347             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2348         elif self.name in self.coords or self.name in self.dims:
2349             # The name is the same as one of the coords names, which netCDF
2350             # doesn't support, so rename it but keep track of the old name
2351             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2352             dataset.attrs[DATAARRAY_NAME] = self.name
2353         else:
2354             # No problems with the name - so we're fine!
2355             dataset = self.to_dataset()
2356 
2357         return dataset.to_netcdf(*args, **kwargs)
2358 
2359     def to_dict(self, data: bool = True) -> dict:
2360         """
2361         Convert this xarray.DataArray into a dictionary following xarray
2362         naming conventions.
2363 
2364         Converts all variables and attributes to native Python objects.
2365         Useful for coverting to json. To avoid datetime incompatibility
2366         use decode_times=False kwarg in xarrray.open_dataset.
2367 
2368         Parameters
2369         ----------
2370         data : bool, optional
2371             Whether to include the actual data in the dictionary. When set to
2372             False, returns just the schema.
2373 
2374         See also
2375         --------
2376         DataArray.from_dict
2377         """
2378         d = self.variable.to_dict(data=data)
2379         d.update({"coords": {}, "name": self.name})
2380         for k in self.coords:
2381             d["coords"][k] = self.coords[k].variable.to_dict(data=data)
2382         return d
2383 
2384     @classmethod
2385     def from_dict(cls, d: dict) -> "DataArray":
2386         """
2387         Convert a dictionary into an xarray.DataArray
2388 
2389         Input dict can take several forms::
2390 
2391             d = {'dims': ('t'), 'data': x}
2392 
2393             d = {'coords': {'t': {'dims': 't', 'data': t,
2394                                   'attrs': {'units':'s'}}},
2395                  'attrs': {'title': 'air temperature'},
2396                  'dims': 't',
2397                  'data': x,
2398                  'name': 'a'}
2399 
2400         where 't' is the name of the dimesion, 'a' is the name of the array,
2401         and  x and t are lists, numpy.arrays, or pandas objects.
2402 
2403         Parameters
2404         ----------
2405         d : dict, with a minimum structure of {'dims': [..], 'data': [..]}
2406 
2407         Returns
2408         -------
2409         obj : xarray.DataArray
2410 
2411         See also
2412         --------
2413         DataArray.to_dict
2414         Dataset.from_dict
2415         """
2416         coords = None
2417         if "coords" in d:
2418             try:
2419                 coords = {
2420                     k: (v["dims"], v["data"], v.get("attrs"))
2421                     for k, v in d["coords"].items()
2422                 }
2423             except KeyError as e:
2424                 raise ValueError(
2425                     "cannot convert dict when coords are missing the key "
2426                     "'{dims_data}'".format(dims_data=str(e.args[0]))
2427                 )
2428         try:
2429             data = d["data"]
2430         except KeyError:
2431             raise ValueError("cannot convert dict without the key 'data''")
2432         else:
2433             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
2434         return obj
2435 
2436     @classmethod
2437     def from_series(cls, series: pd.Series, sparse: bool = False) -> "DataArray":
2438         """Convert a pandas.Series into an xarray.DataArray.
2439 
2440         If the series's index is a MultiIndex, it will be expanded into a
2441         tensor product of one-dimensional coordinates (filling in missing
2442         values with NaN). Thus this operation should be the inverse of the
2443         `to_series` method.
2444 
2445         If sparse=True, creates a sparse array instead of a dense NumPy array.
2446         Requires the pydata/sparse package.
2447 
2448         See also
2449         --------
2450         xarray.Dataset.from_dataframe
2451         """
2452         temp_name = "__temporary_name"
2453         df = pd.DataFrame({temp_name: series})
2454         ds = Dataset.from_dataframe(df, sparse=sparse)
2455         result = cast(DataArray, ds[temp_name])
2456         result.name = series.name
2457         return result
2458 
2459     def to_cdms2(self) -> "cdms2_Variable":
2460         """Convert this array into a cdms2.Variable
2461         """
2462         from ..convert import to_cdms2
2463 
2464         return to_cdms2(self)
2465 
2466     @classmethod
2467     def from_cdms2(cls, variable: "cdms2_Variable") -> "DataArray":
2468         """Convert a cdms2.Variable into an xarray.DataArray
2469         """
2470         from ..convert import from_cdms2
2471 
2472         return from_cdms2(variable)
2473 
2474     def to_iris(self) -> "iris_Cube":
2475         """Convert this array into a iris.cube.Cube
2476         """
2477         from ..convert import to_iris
2478 
2479         return to_iris(self)
2480 
2481     @classmethod
2482     def from_iris(cls, cube: "iris_Cube") -> "DataArray":
2483         """Convert a iris.cube.Cube into an xarray.DataArray
2484         """
2485         from ..convert import from_iris
2486 
2487         return from_iris(cube)
2488 
2489     def _all_compat(self, other: "DataArray", compat_str: str) -> bool:
2490         """Helper function for equals, broadcast_equals, and identical
2491         """
2492 
2493         def compat(x, y):
2494             return getattr(x.variable, compat_str)(y.variable)
2495 
2496         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
2497             self, other
2498         )
2499 
2500     def broadcast_equals(self, other: "DataArray") -> bool:
2501         """Two DataArrays are broadcast equal if they are equal after
2502         broadcasting them against each other such that they have the same
2503         dimensions.
2504 
2505         See Also
2506         --------
2507         DataArray.equals
2508         DataArray.identical
2509         """
2510         try:
2511             return self._all_compat(other, "broadcast_equals")
2512         except (TypeError, AttributeError):
2513             return False
2514 
2515     def equals(self, other: "DataArray") -> bool:
2516         """True if two DataArrays have the same dimensions, coordinates and
2517         values; otherwise False.
2518 
2519         DataArrays can still be equal (like pandas objects) if they have NaN
2520         values in the same locations.
2521 
2522         This method is necessary because `v1 == v2` for ``DataArray``
2523         does element-wise comparisons (like numpy.ndarrays).
2524 
2525         See Also
2526         --------
2527         DataArray.broadcast_equals
2528         DataArray.identical
2529         """
2530         try:
2531             return self._all_compat(other, "equals")
2532         except (TypeError, AttributeError):
2533             return False
2534 
2535     def identical(self, other: "DataArray") -> bool:
2536         """Like equals, but also checks the array name and attributes, and
2537         attributes on all coordinates.
2538 
2539         See Also
2540         --------
2541         DataArray.broadcast_equals
2542         DataArray.equal
2543         """
2544         try:
2545             return self.name == other.name and self._all_compat(other, "identical")
2546         except (TypeError, AttributeError):
2547             return False
2548 
2549     def _result_name(self, other: Any = None) -> Optional[Hashable]:
2550         # use the same naming heuristics as pandas:
2551         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
2552         other_name = getattr(other, "name", _default)
2553         if other_name is _default or other_name == self.name:
2554             return self.name
2555         else:
2556             return None
2557 
2558     def __array_wrap__(self, obj, context=None) -> "DataArray":
2559         new_var = self.variable.__array_wrap__(obj, context)
2560         return self._replace(new_var)
2561 
2562     def __matmul__(self, obj):
2563         return self.dot(obj)
2564 
2565     def __rmatmul__(self, other):
2566         # currently somewhat duplicative, as only other DataArrays are
2567         # compatible with matmul
2568         return computation.dot(other, self)
2569 
2570     @staticmethod
2571     def _unary_op(f: Callable[..., Any]) -> Callable[..., "DataArray"]:
2572         @functools.wraps(f)
2573         def func(self, *args, **kwargs):
2574             with np.errstate(all="ignore"):
2575                 return self.__array_wrap__(f(self.variable.data, *args, **kwargs))
2576 
2577         return func
2578 
2579     @staticmethod
2580     def _binary_op(
2581         f: Callable[..., Any],
2582         reflexive: bool = False,
2583         join: str = None,  # see xarray.align
2584         **ignored_kwargs,
2585     ) -> Callable[..., "DataArray"]:
2586         @functools.wraps(f)
2587         def func(self, other):
2588             if isinstance(other, (Dataset, groupby.GroupBy)):
2589                 return NotImplemented
2590             if isinstance(other, DataArray):
2591                 align_type = OPTIONS["arithmetic_join"] if join is None else join
2592                 self, other = align(self, other, join=align_type, copy=False)
2593             other_variable = getattr(other, "variable", other)
2594             other_coords = getattr(other, "coords", None)
2595 
2596             variable = (
2597                 f(self.variable, other_variable)
2598                 if not reflexive
2599                 else f(other_variable, self.variable)
2600             )
2601             coords, indexes = self.coords._merge_raw(other_coords)
2602             name = self._result_name(other)
2603 
2604             return self._replace(variable, coords, name, indexes=indexes)
2605 
2606         return func
2607 
2608     @staticmethod
2609     def _inplace_binary_op(f: Callable) -> Callable[..., "DataArray"]:
2610         @functools.wraps(f)
2611         def func(self, other):
2612             if isinstance(other, groupby.GroupBy):
2613                 raise TypeError(
2614                     "in-place operations between a DataArray and "
2615                     "a grouped object are not permitted"
2616                 )
2617             # n.b. we can't align other to self (with other.reindex_like(self))
2618             # because `other` may be converted into floats, which would cause
2619             # in-place arithmetic to fail unpredictably. Instead, we simply
2620             # don't support automatic alignment with in-place arithmetic.
2621             other_coords = getattr(other, "coords", None)
2622             other_variable = getattr(other, "variable", other)
2623             with self.coords._merge_inplace(other_coords):
2624                 f(self.variable, other_variable)
2625             return self
2626 
2627         return func
2628 
2629     def _copy_attrs_from(self, other: Union["DataArray", Dataset, Variable]) -> None:
2630         self.attrs = other.attrs
2631 
2632     @property
2633     def plot(self) -> _PlotMethods:
2634         """
2635         Access plotting functions for DataArray's
2636 
2637         >>> d = DataArray([[1, 2], [3, 4]])
2638 
2639         For convenience just call this directly
2640 
2641         >>> d.plot()
2642 
2643         Or use it as a namespace to use xarray.plot functions as
2644         DataArray methods
2645 
2646         >>> d.plot.imshow()  # equivalent to xarray.plot.imshow(d)
2647 
2648         """
2649         return _PlotMethods(self)
2650 
2651     def _title_for_slice(self, truncate: int = 50) -> str:
2652         """
2653         If the dataarray has 1 dimensional coordinates or comes from a slice
2654         we can show that info in the title
2655 
2656         Parameters
2657         ----------
2658         truncate : integer
2659             maximum number of characters for title
2660 
2661         Returns
2662         -------
2663         title : string
2664             Can be used for plot titles
2665 
2666         """
2667         one_dims = []
2668         for dim, coord in self.coords.items():
2669             if coord.size == 1:
2670                 one_dims.append(
2671                     "{dim} = {v}".format(dim=dim, v=format_item(coord.values))
2672                 )
2673 
2674         title = ", ".join(one_dims)
2675         if len(title) > truncate:
2676             title = title[: (truncate - 3)] + "..."
2677 
2678         return title
2679 
2680     def diff(self, dim: Hashable, n: int = 1, label: Hashable = "upper") -> "DataArray":
2681         """Calculate the n-th order discrete difference along given axis.
2682 
2683         Parameters
2684         ----------
2685         dim : hashable, optional
2686             Dimension over which to calculate the finite difference.
2687         n : int, optional
2688             The number of times values are differenced.
2689         label : hashable, optional
2690             The new coordinate in dimension ``dim`` will have the
2691             values of either the minuend's or subtrahend's coordinate
2692             for values 'upper' and 'lower', respectively.  Other
2693             values are not supported.
2694 
2695         Returns
2696         -------
2697         difference : same type as caller
2698             The n-th order finite difference of this object.
2699 
2700         Examples
2701         --------
2702         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ['x'])
2703         >>> arr.diff('x')
2704         <xarray.DataArray (x: 3)>
2705         array([0, 1, 0])
2706         Coordinates:
2707         * x        (x) int64 2 3 4
2708         >>> arr.diff('x', 2)
2709         <xarray.DataArray (x: 2)>
2710         array([ 1, -1])
2711         Coordinates:
2712         * x        (x) int64 3 4
2713 
2714         See Also
2715         --------
2716         DataArray.differentiate
2717         """
2718         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
2719         return self._from_temp_dataset(ds)
2720 
2721     def shift(
2722         self,
2723         shifts: Mapping[Hashable, int] = None,
2724         fill_value: Any = dtypes.NA,
2725         **shifts_kwargs: int,
2726     ) -> "DataArray":
2727         """Shift this array by an offset along one or more dimensions.
2728 
2729         Only the data is moved; coordinates stay in place. Values shifted from
2730         beyond array bounds are replaced by NaN. This is consistent with the
2731         behavior of ``shift`` in pandas.
2732 
2733         Parameters
2734         ----------
2735         shifts : Mapping with the form of {dim: offset}
2736             Integer offset to shift along each of the given dimensions.
2737             Positive offsets shift to the right; negative offsets shift to the
2738             left.
2739         fill_value: scalar, optional
2740             Value to use for newly missing values
2741         **shifts_kwargs:
2742             The keyword arguments form of ``shifts``.
2743             One of shifts or shifts_kwargs must be provided.
2744 
2745         Returns
2746         -------
2747         shifted : DataArray
2748             DataArray with the same coordinates and attributes but shifted
2749             data.
2750 
2751         See also
2752         --------
2753         roll
2754 
2755         Examples
2756         --------
2757 
2758         >>> arr = xr.DataArray([5, 6, 7], dims='x')
2759         >>> arr.shift(x=1)
2760         <xarray.DataArray (x: 3)>
2761         array([ nan,   5.,   6.])
2762         Coordinates:
2763           * x        (x) int64 0 1 2
2764         """
2765         variable = self.variable.shift(
2766             shifts=shifts, fill_value=fill_value, **shifts_kwargs
2767         )
2768         return self._replace(variable=variable)
2769 
2770     def roll(
2771         self,
2772         shifts: Mapping[Hashable, int] = None,
2773         roll_coords: bool = None,
2774         **shifts_kwargs: int,
2775     ) -> "DataArray":
2776         """Roll this array by an offset along one or more dimensions.
2777 
2778         Unlike shift, roll may rotate all variables, including coordinates
2779         if specified. The direction of rotation is consistent with
2780         :py:func:`numpy.roll`.
2781 
2782         Parameters
2783         ----------
2784         shifts : Mapping with the form of {dim: offset}
2785             Integer offset to rotate each of the given dimensions.
2786             Positive offsets roll to the right; negative offsets roll to the
2787             left.
2788         roll_coords : bool
2789             Indicates whether to  roll the coordinates by the offset
2790             The current default of roll_coords (None, equivalent to True) is
2791             deprecated and will change to False in a future version.
2792             Explicitly pass roll_coords to silence the warning.
2793         **shifts_kwargs : The keyword arguments form of ``shifts``.
2794             One of shifts or shifts_kwargs must be provided.
2795 
2796         Returns
2797         -------
2798         rolled : DataArray
2799             DataArray with the same attributes but rolled data and coordinates.
2800 
2801         See also
2802         --------
2803         shift
2804 
2805         Examples
2806         --------
2807 
2808         >>> arr = xr.DataArray([5, 6, 7], dims='x')
2809         >>> arr.roll(x=1)
2810         <xarray.DataArray (x: 3)>
2811         array([7, 5, 6])
2812         Coordinates:
2813           * x        (x) int64 2 0 1
2814         """
2815         ds = self._to_temp_dataset().roll(
2816             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
2817         )
2818         return self._from_temp_dataset(ds)
2819 
2820     @property
2821     def real(self) -> "DataArray":
2822         return self._replace(self.variable.real)
2823 
2824     @property
2825     def imag(self) -> "DataArray":
2826         return self._replace(self.variable.imag)
2827 
2828     def dot(
2829         self, other: "DataArray", dims: Union[Hashable, Sequence[Hashable], None] = None
2830     ) -> "DataArray":
2831         """Perform dot product of two DataArrays along their shared dims.
2832 
2833         Equivalent to taking taking tensordot over all shared dims.
2834 
2835         Parameters
2836         ----------
2837         other : DataArray
2838             The other array with which the dot product is performed.
2839         dims: '...', hashable or sequence of hashables, optional
2840             Which dimensions to sum over. Ellipsis ('...') sums over all dimensions.
2841             If not specified, then all the common dimensions are summed over.
2842 
2843         Returns
2844         -------
2845         result : DataArray
2846             Array resulting from the dot product over all shared dimensions.
2847 
2848         See also
2849         --------
2850         dot
2851         numpy.tensordot
2852 
2853         Examples
2854         --------
2855 
2856         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
2857         >>> da = DataArray(da_vals, dims=['x', 'y', 'z'])
2858         >>> dm_vals = np.arange(4)
2859         >>> dm = DataArray(dm_vals, dims=['z'])
2860 
2861         >>> dm.dims
2862         ('z')
2863         >>> da.dims
2864         ('x', 'y', 'z')
2865 
2866         >>> dot_result = da.dot(dm)
2867         >>> dot_result.dims
2868         ('x', 'y')
2869         """
2870         if isinstance(other, Dataset):
2871             raise NotImplementedError(
2872                 "dot products are not yet supported with Dataset objects."
2873             )
2874         if not isinstance(other, DataArray):
2875             raise TypeError("dot only operates on DataArrays.")
2876 
2877         return computation.dot(self, other, dims=dims)
2878 
2879     def sortby(
2880         self,
2881         variables: Union[Hashable, "DataArray", Sequence[Union[Hashable, "DataArray"]]],
2882         ascending: bool = True,
2883     ) -> "DataArray":
2884         """Sort object by labels or values (along an axis).
2885 
2886         Sorts the dataarray, either along specified dimensions,
2887         or according to values of 1-D dataarrays that share dimension
2888         with calling object.
2889 
2890         If the input variables are dataarrays, then the dataarrays are aligned
2891         (via left-join) to the calling object prior to sorting by cell values.
2892         NaNs are sorted to the end, following Numpy convention.
2893 
2894         If multiple sorts along the same dimension is
2895         given, numpy's lexsort is performed along that dimension:
2896         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
2897         and the FIRST key in the sequence is used as the primary sort key,
2898         followed by the 2nd key, etc.
2899 
2900         Parameters
2901         ----------
2902         variables: hashable, DataArray, or sequence of either
2903             1D DataArray objects or name(s) of 1D variable(s) in
2904             coords whose values are used to sort this array.
2905         ascending: boolean, optional
2906             Whether to sort by ascending or descending order.
2907 
2908         Returns
2909         -------
2910         sorted: DataArray
2911             A new dataarray where all the specified dims are sorted by dim
2912             labels.
2913 
2914         Examples
2915         --------
2916 
2917         >>> da = xr.DataArray(np.random.rand(5),
2918         ...                   coords=[pd.date_range('1/1/2000', periods=5)],
2919         ...                   dims='time')
2920         >>> da
2921         <xarray.DataArray (time: 5)>
2922         array([ 0.965471,  0.615637,  0.26532 ,  0.270962,  0.552878])
2923         Coordinates:
2924           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...
2925 
2926         >>> da.sortby(da)
2927         <xarray.DataArray (time: 5)>
2928         array([ 0.26532 ,  0.270962,  0.552878,  0.615637,  0.965471])
2929         Coordinates:
2930           * time     (time) datetime64[ns] 2000-01-03 2000-01-04 2000-01-05 ...
2931         """
2932         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
2933         return self._from_temp_dataset(ds)
2934 
2935     def quantile(
2936         self,
2937         q: Any,
2938         dim: Union[Hashable, Sequence[Hashable], None] = None,
2939         interpolation: str = "linear",
2940         keep_attrs: bool = None,
2941     ) -> "DataArray":
2942         """Compute the qth quantile of the data along the specified dimension.
2943 
2944         Returns the qth quantiles(s) of the array elements.
2945 
2946         Parameters
2947         ----------
2948         q : float in range of [0,1] or array-like of floats
2949             Quantile to compute, which must be between 0 and 1 inclusive.
2950         dim : hashable or sequence of hashable, optional
2951             Dimension(s) over which to apply quantile.
2952         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
2953             This optional parameter specifies the interpolation method to
2954             use when the desired quantile lies between two data points
2955             ``i < j``:
2956 
2957                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is
2958                   the fractional part of the index surrounded by ``i`` and
2959                   ``j``.
2960                 - lower: ``i``.
2961                 - higher: ``j``.
2962                 - nearest: ``i`` or ``j``, whichever is nearest.
2963                 - midpoint: ``(i + j) / 2``.
2964         keep_attrs : bool, optional
2965             If True, the dataset's attributes (`attrs`) will be copied from
2966             the original object to the new one.  If False (default), the new
2967             object will be returned without attributes.
2968 
2969         Returns
2970         -------
2971         quantiles : DataArray
2972             If `q` is a single quantile, then the result
2973             is a scalar. If multiple percentiles are given, first axis of
2974             the result corresponds to the quantile and a quantile dimension
2975             is added to the return array. The other dimensions are the
2976             dimensions that remain after the reduction of the array.
2977 
2978         See Also
2979         --------
2980         numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile
2981 
2982         Examples
2983         --------
2984 
2985         >>> da = xr.DataArray(
2986         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
2987         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
2988         ...     dims=("x", "y"),
2989         ... )
2990         >>> da.quantile(0)  # or da.quantile(0, dim=...)
2991         <xarray.DataArray ()>
2992         array(0.7)
2993         Coordinates:
2994             quantile  float64 0.0
2995         >>> da.quantile(0, dim="x")
2996         <xarray.DataArray (y: 4)>
2997         array([0.7, 4.2, 2.6, 1.5])
2998         Coordinates:
2999           * y         (y) float64 1.0 1.5 2.0 2.5
3000             quantile  float64 0.0
3001         >>> da.quantile([0, 0.5, 1])
3002         <xarray.DataArray (quantile: 3)>
3003         array([0.7, 3.4, 9.4])
3004         Coordinates:
3005           * quantile  (quantile) float64 0.0 0.5 1.0
3006         >>> da.quantile([0, 0.5, 1], dim="x")
3007         <xarray.DataArray (quantile: 3, y: 4)>
3008         array([[0.7 , 4.2 , 2.6 , 1.5 ],
3009                [3.6 , 5.75, 6.  , 1.7 ],
3010                [6.5 , 7.3 , 9.4 , 1.9 ]])
3011         Coordinates:
3012           * y         (y) float64 1.0 1.5 2.0 2.5
3013           * quantile  (quantile) float64 0.0 0.5 1.0
3014         """
3015 
3016         ds = self._to_temp_dataset().quantile(
3017             q, dim=dim, keep_attrs=keep_attrs, interpolation=interpolation
3018         )
3019         return self._from_temp_dataset(ds)
3020 
3021     def rank(
3022         self, dim: Hashable, pct: bool = False, keep_attrs: bool = None
3023     ) -> "DataArray":
3024         """Ranks the data.
3025 
3026         Equal values are assigned a rank that is the average of the ranks that
3027         would have been otherwise assigned to all of the values within that
3028         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
3029 
3030         NaNs in the input array are returned as NaNs.
3031 
3032         The `bottleneck` library is required.
3033 
3034         Parameters
3035         ----------
3036         dim : hashable
3037             Dimension over which to compute rank.
3038         pct : bool, optional
3039             If True, compute percentage ranks, otherwise compute integer ranks.
3040         keep_attrs : bool, optional
3041             If True, the dataset's attributes (`attrs`) will be copied from
3042             the original object to the new one.  If False (default), the new
3043             object will be returned without attributes.
3044 
3045         Returns
3046         -------
3047         ranked : DataArray
3048             DataArray with the same coordinates and dtype 'float64'.
3049 
3050         Examples
3051         --------
3052 
3053         >>> arr = xr.DataArray([5, 6, 7], dims='x')
3054         >>> arr.rank('x')
3055         <xarray.DataArray (x: 3)>
3056         array([ 1.,   2.,   3.])
3057         Dimensions without coordinates: x
3058         """
3059 
3060         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
3061         return self._from_temp_dataset(ds)
3062 
3063     def differentiate(
3064         self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None
3065     ) -> "DataArray":
3066         """ Differentiate the array with the second order accurate central
3067         differences.
3068 
3069         .. note::
3070             This feature is limited to simple cartesian geometry, i.e. coord
3071             must be one dimensional.
3072 
3073         Parameters
3074         ----------
3075         coord: hashable
3076             The coordinate to be used to compute the gradient.
3077         edge_order: 1 or 2. Default 1
3078             N-th order accurate differences at the boundaries.
3079         datetime_unit: None or any of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms',
3080             'us', 'ns', 'ps', 'fs', 'as'}
3081             Unit to compute gradient. Only valid for datetime coordinate.
3082 
3083         Returns
3084         -------
3085         differentiated: DataArray
3086 
3087         See also
3088         --------
3089         numpy.gradient: corresponding numpy function
3090 
3091         Examples
3092         --------
3093 
3094         >>> da = xr.DataArray(np.arange(12).reshape(4, 3), dims=['x', 'y'],
3095         ...                   coords={'x': [0, 0.1, 1.1, 1.2]})
3096         >>> da
3097         <xarray.DataArray (x: 4, y: 3)>
3098         array([[ 0,  1,  2],
3099                [ 3,  4,  5],
3100                [ 6,  7,  8],
3101                [ 9, 10, 11]])
3102         Coordinates:
3103           * x        (x) float64 0.0 0.1 1.1 1.2
3104         Dimensions without coordinates: y
3105         >>>
3106         >>> da.differentiate('x')
3107         <xarray.DataArray (x: 4, y: 3)>
3108         array([[30.      , 30.      , 30.      ],
3109                [27.545455, 27.545455, 27.545455],
3110                [27.545455, 27.545455, 27.545455],
3111                [30.      , 30.      , 30.      ]])
3112         Coordinates:
3113           * x        (x) float64 0.0 0.1 1.1 1.2
3114         Dimensions without coordinates: y
3115         """
3116         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
3117         return self._from_temp_dataset(ds)
3118 
3119     def integrate(
3120         self, dim: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None
3121     ) -> "DataArray":
3122         """ integrate the array with the trapezoidal rule.
3123 
3124         .. note::
3125             This feature is limited to simple cartesian geometry, i.e. dim
3126             must be one dimensional.
3127 
3128         Parameters
3129         ----------
3130         dim: hashable, or a sequence of hashable
3131             Coordinate(s) used for the integration.
3132         datetime_unit: str, optional
3133             Can be used to specify the unit if datetime coordinate is used.
3134             One of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', 'ps',
3135             'fs', 'as'}
3136 
3137         Returns
3138         -------
3139         integrated: DataArray
3140 
3141         See also
3142         --------
3143         numpy.trapz: corresponding numpy function
3144 
3145         Examples
3146         --------
3147 
3148         >>> da = xr.DataArray(np.arange(12).reshape(4, 3), dims=['x', 'y'],
3149         ...                   coords={'x': [0, 0.1, 1.1, 1.2]})
3150         >>> da
3151         <xarray.DataArray (x: 4, y: 3)>
3152         array([[ 0,  1,  2],
3153                [ 3,  4,  5],
3154                [ 6,  7,  8],
3155                [ 9, 10, 11]])
3156         Coordinates:
3157           * x        (x) float64 0.0 0.1 1.1 1.2
3158         Dimensions without coordinates: y
3159         >>>
3160         >>> da.integrate('x')
3161         <xarray.DataArray (y: 3)>
3162         array([5.4, 6.6, 7.8])
3163         Dimensions without coordinates: y
3164         """
3165         ds = self._to_temp_dataset().integrate(dim, datetime_unit)
3166         return self._from_temp_dataset(ds)
3167 
3168     def unify_chunks(self) -> "DataArray":
3169         """ Unify chunk size along all chunked dimensions of this DataArray.
3170 
3171         Returns
3172         -------
3173 
3174         DataArray with consistent chunk sizes for all dask-array variables
3175 
3176         See Also
3177         --------
3178 
3179         dask.array.core.unify_chunks
3180         """
3181         ds = self._to_temp_dataset().unify_chunks()
3182         return self._from_temp_dataset(ds)
3183 
3184     def map_blocks(
3185         self,
3186         func: "Callable[..., T_DSorDA]",
3187         args: Sequence[Any] = (),
3188         kwargs: Mapping[str, Any] = None,
3189     ) -> "T_DSorDA":
3190         """
3191         Apply a function to each chunk of this DataArray. This method is experimental
3192         and its signature may change.
3193 
3194         Parameters
3195         ----------
3196         func: callable
3197             User-provided function that accepts a DataArray as its first parameter. The
3198             function will receive a subset of this DataArray, corresponding to one chunk
3199             along each chunked dimension. ``func`` will be executed as
3200             ``func(obj_subset, *args, **kwargs)``.
3201 
3202             The function will be first run on mocked-up data, that looks like this array
3203             but has sizes 0, to determine properties of the returned object such as
3204             dtype, variable names, new dimensions and new indexes (if any).
3205 
3206             This function must return either a single DataArray or a single Dataset.
3207 
3208             This function cannot change size of existing dimensions, or add new chunked
3209             dimensions.
3210         args: Sequence
3211             Passed verbatim to func after unpacking, after the sliced DataArray. xarray
3212             objects, if any, will not be split by chunks. Passing dask collections is
3213             not allowed.
3214         kwargs: Mapping
3215             Passed verbatim to func after unpacking. xarray objects, if any, will not be
3216             split by chunks. Passing dask collections is not allowed.
3217 
3218         Returns
3219         -------
3220         A single DataArray or Dataset with dask backend, reassembled from the outputs of
3221         the function.
3222 
3223         Notes
3224         -----
3225         This method is designed for when one needs to manipulate a whole xarray object
3226         within each chunk. In the more common case where one can work on numpy arrays,
3227         it is recommended to use apply_ufunc.
3228 
3229         If none of the variables in this DataArray is backed by dask, calling this
3230         method is equivalent to calling ``func(self, *args, **kwargs)``.
3231 
3232         See Also
3233         --------
3234         dask.array.map_blocks, xarray.apply_ufunc, xarray.map_blocks,
3235         xarray.Dataset.map_blocks
3236         """
3237         from .parallel import map_blocks
3238 
3239         return map_blocks(func, self, args, kwargs)
3240 
3241     # this needs to be at the end, or mypy will confuse with `str`
3242     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
3243     str = property(StringAccessor)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>xarray/core/variable.py</file>
<class>Variable</class>
<method>quantile</method>
<code>
1683     def quantile(self, q, dim=None, interpolation="linear", keep_attrs=None):
1684         """Compute the qth quantile of the data along the specified dimension.
1685 
1686         Returns the qth quantiles(s) of the array elements.
1687 
1688         Parameters
1689         ----------
1690         q : float in range of [0,1] (or sequence of floats)
1691             Quantile to compute, which must be between 0 and 1
1692             inclusive.
1693         dim : str or sequence of str, optional
1694             Dimension(s) over which to apply quantile.
1695         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
1696             This optional parameter specifies the interpolation method to
1697             use when the desired quantile lies between two data points
1698             ``i < j``:
1699                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is
1700                   the fractional part of the index surrounded by ``i`` and
1701                   ``j``.
1702                 * lower: ``i``.
1703                 * higher: ``j``.
1704                 * nearest: ``i`` or ``j``, whichever is nearest.
1705                 * midpoint: ``(i + j) / 2``.
1706         keep_attrs : bool, optional
1707             If True, the variable's attributes (`attrs`) will be copied from
1708             the original object to the new one.  If False (default), the new
1709             object will be returned without attributes.
1710 
1711         Returns
1712         -------
1713         quantiles : Variable
1714             If `q` is a single quantile, then the result
1715             is a scalar. If multiple percentiles are given, first axis of
1716             the result corresponds to the quantile and a quantile dimension
1717             is added to the return array. The other dimensions are the
1718             dimensions that remain after the reduction of the array.
1719 
1720         See Also
1721         --------
1722         numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile,
1723         DataArray.quantile
1724         """
1725 
1726         from .computation import apply_ufunc
1727 
1728         if keep_attrs is None:
1729             keep_attrs = _get_keep_attrs(default=False)
1730 
1731         scalar = utils.is_scalar(q)
1732         q = np.atleast_1d(np.asarray(q, dtype=np.float64))
1733 
1734         if dim is None:
1735             dim = self.dims
1736 
1737         if utils.is_scalar(dim):
1738             dim = [dim]
1739 
1740         def _wrapper(npa, **kwargs):
1741             # move quantile axis to end. required for apply_ufunc
1742             return np.moveaxis(np.nanpercentile(npa, **kwargs), 0, -1)
1743 
1744         axis = np.arange(-1, -1 * len(dim) - 1, -1)
1745         result = apply_ufunc(
1746             _wrapper,
1747             self,
1748             input_core_dims=[dim],
1749             exclude_dims=set(dim),
1750             output_core_dims=[["quantile"]],
1751             output_dtypes=[np.float64],
1752             output_sizes={"quantile": len(q)},
1753             dask="parallelized",
1754             kwargs={"q": q * 100, "axis": axis, "interpolation": interpolation},
1755         )
1756 
1757         # for backward compatibility
1758         result = result.transpose("quantile", ...)
1759         if scalar:
1760             result = result.squeeze("quantile")
1761         if keep_attrs:
1762             result.attrs = self._attrs
1763         return result

</code><intended_behavior>Although the direct issue doesn't lie here, reviewing how errors are propagated when `np.nanpercentile` is called could be beneficial. Ensure that any error messages about input range exceptions are consistent with the quantile range [0, 1]. If `Variable.quantile` is where the numpy function calls are made, it should either handle the error and rethrow it with a message consistent with xarray's expected input range, or it should contain preemptive input validation to provide a clear and correct error message before deference to numpy's functions.</intended_behavior>, <file>xarray/core/variable.py</file>
<class>Variable</class>
<method>None</method>
<code>
263 class Variable(
264     common.AbstractArray, arithmetic.SupportsArithmetic, utils.NdimSizeLenMixin
265 ):
266     """A netcdf-like variable consisting of dimensions, data and attributes
267     which describe a single Array. A single Variable object is not fully
268     described outside the context of its parent Dataset (if you want such a
269     fully described object, use a DataArray instead).
270 
271     The main functional difference between Variables and numpy arrays is that
272     numerical operations on Variables implement array broadcasting by dimension
273     name. For example, adding an Variable with dimensions `('time',)` to
274     another Variable with dimensions `('space',)` results in a new Variable
275     with dimensions `('time', 'space')`. Furthermore, numpy reduce operations
276     like ``mean`` or ``sum`` are overwritten to take a "dimension" argument
277     instead of an "axis".
278 
279     Variables are light-weight objects used as the building block for datasets.
280     They are more primitive objects, so operations with them provide marginally
281     higher performance than using DataArrays. However, manipulating data in the
282     form of a Dataset or DataArray should almost always be preferred, because
283     they can use more complete metadata in context of coordinate labels.
284     """
285 
286     __slots__ = ("_dims", "_data", "_attrs", "_encoding")
287 
288     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):
289         """
290         Parameters
291         ----------
292         dims : str or sequence of str
293             Name(s) of the the data dimension(s). Must be either a string (only
294             for 1D data) or a sequence of strings with length equal to the
295             number of dimensions.
296         data : array_like
297             Data array which supports numpy-like data access.
298         attrs : dict_like or None, optional
299             Attributes to assign to the new variable. If None (default), an
300             empty attribute dictionary is initialized.
301         encoding : dict_like or None, optional
302             Dictionary specifying how to encode this array's data into a
303             serialized format like netCDF4. Currently used keys (for netCDF)
304             include '_FillValue', 'scale_factor', 'add_offset' and 'dtype'.
305             Well-behaved code to serialize a Variable should ignore
306             unrecognized encoding items.
307         """
308         self._data = as_compatible_data(data, fastpath=fastpath)
309         self._dims = self._parse_dimensions(dims)
310         self._attrs = None
311         self._encoding = None
312         if attrs is not None:
313             self.attrs = attrs
314         if encoding is not None:
315             self.encoding = encoding
316 
317     @property
318     def dtype(self):
319         return self._data.dtype
320 
321     @property
322     def shape(self):
323         return self._data.shape
324 
325     @property
326     def nbytes(self):
327         return self.size * self.dtype.itemsize
328 
329     @property
330     def _in_memory(self):
331         return isinstance(self._data, (np.ndarray, np.number, PandasIndexAdapter)) or (
332             isinstance(self._data, indexing.MemoryCachedArray)
333             and isinstance(self._data.array, indexing.NumpyIndexingAdapter)
334         )
335 
336     @property
337     def data(self):
338         if hasattr(self._data, "__array_function__") or isinstance(
339             self._data, dask_array_type
340         ):
341             return self._data
342         else:
343             return self.values
344 
345     @data.setter
346     def data(self, data):
347         data = as_compatible_data(data)
348         if data.shape != self.shape:
349             raise ValueError(
350                 f"replacement data must match the Variable's shape. "
351                 f"replacement data has shape {data.shape}; Variable has shape {self.shape}"
352             )
353         self._data = data
354 
355     def load(self, **kwargs):
356         """Manually trigger loading of this variable's data from disk or a
357         remote source into memory and return this variable.
358 
359         Normally, it should not be necessary to call this method in user code,
360         because all xarray functions should either work on deferred data or
361         load data automatically.
362 
363         Parameters
364         ----------
365         **kwargs : dict
366             Additional keyword arguments passed on to ``dask.array.compute``.
367 
368         See Also
369         --------
370         dask.array.compute
371         """
372         if isinstance(self._data, dask_array_type):
373             self._data = as_compatible_data(self._data.compute(**kwargs))
374         elif not hasattr(self._data, "__array_function__"):
375             self._data = np.asarray(self._data)
376         return self
377 
378     def compute(self, **kwargs):
379         """Manually trigger loading of this variable's data from disk or a
380         remote source into memory and return a new variable. The original is
381         left unaltered.
382 
383         Normally, it should not be necessary to call this method in user code,
384         because all xarray functions should either work on deferred data or
385         load data automatically.
386 
387         Parameters
388         ----------
389         **kwargs : dict
390             Additional keyword arguments passed on to ``dask.array.compute``.
391 
392         See Also
393         --------
394         dask.array.compute
395         """
396         new = self.copy(deep=False)
397         return new.load(**kwargs)
398 
399     def __dask_tokenize__(self):
400         # Use v.data, instead of v._data, in order to cope with the wrappers
401         # around NetCDF and the like
402         from dask.base import normalize_token
403 
404         return normalize_token((type(self), self._dims, self.data, self._attrs))
405 
406     def __dask_graph__(self):
407         if isinstance(self._data, dask_array_type):
408             return self._data.__dask_graph__()
409         else:
410             return None
411 
412     def __dask_keys__(self):
413         return self._data.__dask_keys__()
414 
415     def __dask_layers__(self):
416         return self._data.__dask_layers__()
417 
418     @property
419     def __dask_optimize__(self):
420         return self._data.__dask_optimize__
421 
422     @property
423     def __dask_scheduler__(self):
424         return self._data.__dask_scheduler__
425 
426     def __dask_postcompute__(self):
427         array_func, array_args = self._data.__dask_postcompute__()
428         return (
429             self._dask_finalize,
430             (array_func, array_args, self._dims, self._attrs, self._encoding),
431         )
432 
433     def __dask_postpersist__(self):
434         array_func, array_args = self._data.__dask_postpersist__()
435         return (
436             self._dask_finalize,
437             (array_func, array_args, self._dims, self._attrs, self._encoding),
438         )
439 
440     @staticmethod
441     def _dask_finalize(results, array_func, array_args, dims, attrs, encoding):
442         if isinstance(results, dict):  # persist case
443             name = array_args[0]
444             results = {k: v for k, v in results.items() if k[0] == name}
445         data = array_func(results, *array_args)
446         return Variable(dims, data, attrs=attrs, encoding=encoding)
447 
448     @property
449     def values(self):
450         """The variable's data as a numpy.ndarray"""
451         return _as_array_or_item(self._data)
452 
453     @values.setter
454     def values(self, values):
455         self.data = values
456 
457     def to_base_variable(self):
458         """Return this variable as a base xarray.Variable"""
459         return Variable(
460             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True
461         )
462 
463     to_variable = utils.alias(to_base_variable, "to_variable")
464 
465     def to_index_variable(self):
466         """Return this variable as an xarray.IndexVariable"""
467         return IndexVariable(
468             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True
469         )
470 
471     to_coord = utils.alias(to_index_variable, "to_coord")
472 
473     def to_index(self):
474         """Convert this variable to a pandas.Index"""
475         return self.to_index_variable().to_index()
476 
477     def to_dict(self, data=True):
478         """Dictionary representation of variable."""
479         item = {"dims": self.dims, "attrs": decode_numpy_dict_values(self.attrs)}
480         if data:
481             item["data"] = ensure_us_time_resolution(self.values).tolist()
482         else:
483             item.update({"dtype": str(self.dtype), "shape": self.shape})
484         return item
485 
486     @property
487     def dims(self):
488         """Tuple of dimension names with which this variable is associated.
489         """
490         return self._dims
491 
492     @dims.setter
493     def dims(self, value):
494         self._dims = self._parse_dimensions(value)
495 
496     def _parse_dimensions(self, dims):
497         if isinstance(dims, str):
498             dims = (dims,)
499         dims = tuple(dims)
500         if len(dims) != self.ndim:
501             raise ValueError(
502                 "dimensions %s must have the same length as the "
503                 "number of data dimensions, ndim=%s" % (dims, self.ndim)
504             )
505         return dims
506 
507     def _item_key_to_tuple(self, key):
508         if utils.is_dict_like(key):
509             return tuple(key.get(dim, slice(None)) for dim in self.dims)
510         else:
511             return key
512 
513     def _broadcast_indexes(self, key):
514         """Prepare an indexing key for an indexing operation.
515 
516         Parameters
517         -----------
518         key: int, slice, array, dict or tuple of integer, slices and arrays
519             Any valid input for indexing.
520 
521         Returns
522         -------
523         dims: tuple
524             Dimension of the resultant variable.
525         indexers: IndexingTuple subclass
526             Tuple of integer, array-like, or slices to use when indexing
527             self._data. The type of this argument indicates the type of
528             indexing to perform, either basic, outer or vectorized.
529         new_order : Optional[Sequence[int]]
530             Optional reordering to do on the result of indexing. If not None,
531             the first len(new_order) indexing should be moved to these
532             positions.
533         """
534         key = self._item_key_to_tuple(key)  # key is a tuple
535         # key is a tuple of full size
536         key = indexing.expanded_indexer(key, self.ndim)
537         # Convert a scalar Variable to an integer
538         key = tuple(
539             k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k for k in key
540         )
541         # Convert a 0d-array to an integer
542         key = tuple(
543             k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key
544         )
545 
546         if all(isinstance(k, BASIC_INDEXING_TYPES) for k in key):
547             return self._broadcast_indexes_basic(key)
548 
549         self._validate_indexers(key)
550         # Detect it can be mapped as an outer indexer
551         # If all key is unlabeled, or
552         # key can be mapped as an OuterIndexer.
553         if all(not isinstance(k, Variable) for k in key):
554             return self._broadcast_indexes_outer(key)
555 
556         # If all key is 1-dimensional and there are no duplicate labels,
557         # key can be mapped as an OuterIndexer.
558         dims = []
559         for k, d in zip(key, self.dims):
560             if isinstance(k, Variable):
561                 if len(k.dims) > 1:
562                     return self._broadcast_indexes_vectorized(key)
563                 dims.append(k.dims[0])
564             elif not isinstance(k, integer_types):
565                 dims.append(d)
566         if len(set(dims)) == len(dims):
567             return self._broadcast_indexes_outer(key)
568 
569         return self._broadcast_indexes_vectorized(key)
570 
571     def _broadcast_indexes_basic(self, key):
572         dims = tuple(
573             dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)
574         )
575         return dims, BasicIndexer(key), None
576 
577     def _validate_indexers(self, key):
578         """ Make sanity checks """
579         for dim, k in zip(self.dims, key):
580             if isinstance(k, BASIC_INDEXING_TYPES):
581                 pass
582             else:
583                 if not isinstance(k, Variable):
584                     k = np.asarray(k)
585                     if k.ndim > 1:
586                         raise IndexError(
587                             "Unlabeled multi-dimensional array cannot be "
588                             "used for indexing: {}".format(k)
589                         )
590                 if k.dtype.kind == "b":
591                     if self.shape[self.get_axis_num(dim)] != len(k):
592                         raise IndexError(
593                             "Boolean array size {:d} is used to index array "
594                             "with shape {:s}.".format(len(k), str(self.shape))
595                         )
596                     if k.ndim > 1:
597                         raise IndexError(
598                             "{}-dimensional boolean indexing is "
599                             "not supported. ".format(k.ndim)
600                         )
601                     if getattr(k, "dims", (dim,)) != (dim,):
602                         raise IndexError(
603                             "Boolean indexer should be unlabeled or on the "
604                             "same dimension to the indexed array. Indexer is "
605                             "on {:s} but the target dimension is {:s}.".format(
606                                 str(k.dims), dim
607                             )
608                         )
609 
610     def _broadcast_indexes_outer(self, key):
611         dims = tuple(
612             k.dims[0] if isinstance(k, Variable) else dim
613             for k, dim in zip(key, self.dims)
614             if not isinstance(k, integer_types)
615         )
616 
617         new_key = []
618         for k in key:
619             if isinstance(k, Variable):
620                 k = k.data
621             if not isinstance(k, BASIC_INDEXING_TYPES):
622                 k = np.asarray(k)
623                 if k.size == 0:
624                     # Slice by empty list; numpy could not infer the dtype
625                     k = k.astype(int)
626                 elif k.dtype.kind == "b":
627                     (k,) = np.nonzero(k)
628             new_key.append(k)
629 
630         return dims, OuterIndexer(tuple(new_key)), None
631 
632     def _nonzero(self):
633         """ Equivalent numpy's nonzero but returns a tuple of Varibles. """
634         # TODO we should replace dask's native nonzero
635         # after https://github.com/dask/dask/issues/1076 is implemented.
636         nonzeros = np.nonzero(self.data)
637         return tuple(Variable((dim), nz) for nz, dim in zip(nonzeros, self.dims))
638 
639     def _broadcast_indexes_vectorized(self, key):
640         variables = []
641         out_dims_set = OrderedSet()
642         for dim, value in zip(self.dims, key):
643             if isinstance(value, slice):
644                 out_dims_set.add(dim)
645             else:
646                 variable = (
647                     value
648                     if isinstance(value, Variable)
649                     else as_variable(value, name=dim)
650                 )
651                 if variable.dtype.kind == "b":  # boolean indexing case
652                     (variable,) = variable._nonzero()
653 
654                 variables.append(variable)
655                 out_dims_set.update(variable.dims)
656 
657         variable_dims = set()
658         for variable in variables:
659             variable_dims.update(variable.dims)
660 
661         slices = []
662         for i, (dim, value) in enumerate(zip(self.dims, key)):
663             if isinstance(value, slice):
664                 if dim in variable_dims:
665                     # We only convert slice objects to variables if they share
666                     # a dimension with at least one other variable. Otherwise,
667                     # we can equivalently leave them as slices aknd transpose
668                     # the result. This is significantly faster/more efficient
669                     # for most array backends.
670                     values = np.arange(*value.indices(self.sizes[dim]))
671                     variables.insert(i - len(slices), Variable((dim,), values))
672                 else:
673                     slices.append((i, value))
674 
675         try:
676             variables = _broadcast_compat_variables(*variables)
677         except ValueError:
678             raise IndexError(f"Dimensions of indexers mismatch: {key}")
679 
680         out_key = [variable.data for variable in variables]
681         out_dims = tuple(out_dims_set)
682         slice_positions = set()
683         for i, value in slices:
684             out_key.insert(i, value)
685             new_position = out_dims.index(self.dims[i])
686             slice_positions.add(new_position)
687 
688         if slice_positions:
689             new_order = [i for i in range(len(out_dims)) if i not in slice_positions]
690         else:
691             new_order = None
692 
693         return out_dims, VectorizedIndexer(tuple(out_key)), new_order
694 
695     def __getitem__(self: VariableType, key) -> VariableType:
696         """Return a new Variable object whose contents are consistent with
697         getting the provided key from the underlying data.
698 
699         NB. __getitem__ and __setitem__ implement xarray-style indexing,
700         where if keys are unlabeled arrays, we index the array orthogonally
701         with them. If keys are labeled array (such as Variables), they are
702         broadcasted with our usual scheme and then the array is indexed with
703         the broadcasted key, like numpy's fancy indexing.
704 
705         If you really want to do indexing like `x[x > 0]`, manipulate the numpy
706         array `x.values` directly.
707         """
708         dims, indexer, new_order = self._broadcast_indexes(key)
709         data = as_indexable(self._data)[indexer]
710         if new_order:
711             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)
712         return self._finalize_indexing_result(dims, data)
713 
714     def _finalize_indexing_result(self: VariableType, dims, data) -> VariableType:
715         """Used by IndexVariable to return IndexVariable objects when possible.
716         """
717         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)
718 
719     def _getitem_with_mask(self, key, fill_value=dtypes.NA):
720         """Index this Variable with -1 remapped to fill_value."""
721         # TODO(shoyer): expose this method in public API somewhere (isel?) and
722         # use it for reindex.
723         # TODO(shoyer): add a sanity check that all other integers are
724         # non-negative
725         # TODO(shoyer): add an optimization, remapping -1 to an adjacent value
726         # that is actually indexed rather than mapping it to the last value
727         # along each axis.
728 
729         if fill_value is dtypes.NA:
730             fill_value = dtypes.get_fill_value(self.dtype)
731 
732         dims, indexer, new_order = self._broadcast_indexes(key)
733 
734         if self.size:
735             if isinstance(self._data, dask_array_type):
736                 # dask's indexing is faster this way; also vindex does not
737                 # support negative indices yet:
738                 # https://github.com/dask/dask/pull/2967
739                 actual_indexer = indexing.posify_mask_indexer(indexer)
740             else:
741                 actual_indexer = indexer
742 
743             data = as_indexable(self._data)[actual_indexer]
744             mask = indexing.create_mask(indexer, self.shape, data)
745             data = duck_array_ops.where(mask, fill_value, data)
746         else:
747             # array cannot be indexed along dimensions of size 0, so just
748             # build the mask directly instead.
749             mask = indexing.create_mask(indexer, self.shape)
750             data = np.broadcast_to(fill_value, getattr(mask, "shape", ()))
751 
752         if new_order:
753             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)
754         return self._finalize_indexing_result(dims, data)
755 
756     def __setitem__(self, key, value):
757         """__setitem__ is overloaded to access the underlying numpy values with
758         orthogonal indexing.
759 
760         See __getitem__ for more details.
761         """
762         dims, index_tuple, new_order = self._broadcast_indexes(key)
763 
764         if not isinstance(value, Variable):
765             value = as_compatible_data(value)
766             if value.ndim > len(dims):
767                 raise ValueError(
768                     "shape mismatch: value array of shape %s could not be "
769                     "broadcast to indexing result with %s dimensions"
770                     % (value.shape, len(dims))
771                 )
772             if value.ndim == 0:
773                 value = Variable((), value)
774             else:
775                 value = Variable(dims[-value.ndim :], value)
776         # broadcast to become assignable
777         value = value.set_dims(dims).data
778 
779         if new_order:
780             value = duck_array_ops.asarray(value)
781             value = value[(len(dims) - value.ndim) * (np.newaxis,) + (Ellipsis,)]
782             value = duck_array_ops.moveaxis(value, new_order, range(len(new_order)))
783 
784         indexable = as_indexable(self._data)
785         indexable[index_tuple] = value
786 
787     @property
788     def attrs(self) -> Dict[Hashable, Any]:
789         """Dictionary of local attributes on this variable.
790         """
791         if self._attrs is None:
792             self._attrs = {}
793         return self._attrs
794 
795     @attrs.setter
796     def attrs(self, value: Mapping[Hashable, Any]) -> None:
797         self._attrs = dict(value)
798 
799     @property
800     def encoding(self):
801         """Dictionary of encodings on this variable.
802         """
803         if self._encoding is None:
804             self._encoding = {}
805         return self._encoding
806 
807     @encoding.setter
808     def encoding(self, value):
809         try:
810             self._encoding = dict(value)
811         except ValueError:
812             raise ValueError("encoding must be castable to a dictionary")
813 
814     def copy(self, deep=True, data=None):
815         """Returns a copy of this object.
816 
817         If `deep=True`, the data array is loaded into memory and copied onto
818         the new object. Dimensions, attributes and encodings are always copied.
819 
820         Use `data` to create a new object with the same structure as
821         original but entirely new data.
822 
823         Parameters
824         ----------
825         deep : bool, optional
826             Whether the data array is loaded into memory and copied onto
827             the new object. Default is True.
828         data : array_like, optional
829             Data to use in the new object. Must have same shape as original.
830             When `data` is used, `deep` is ignored.
831 
832         Returns
833         -------
834         object : Variable
835             New object with dimensions, attributes, encodings, and optionally
836             data copied from original.
837 
838         Examples
839         --------
840 
841         Shallow copy versus deep copy
842 
843         >>> var = xr.Variable(data=[1, 2, 3], dims='x')
844         >>> var.copy()
845         <xarray.Variable (x: 3)>
846         array([1, 2, 3])
847         >>> var_0 = var.copy(deep=False)
848         >>> var_0[0] = 7
849         >>> var_0
850         <xarray.Variable (x: 3)>
851         array([7, 2, 3])
852         >>> var
853         <xarray.Variable (x: 3)>
854         array([7, 2, 3])
855 
856         Changing the data using the ``data`` argument maintains the
857         structure of the original object, but with the new data. Original
858         object is unaffected.
859 
860         >>> var.copy(data=[0.1, 0.2, 0.3])
861         <xarray.Variable (x: 3)>
862         array([ 0.1,  0.2,  0.3])
863         >>> var
864         <xarray.Variable (x: 3)>
865         array([7, 2, 3])
866 
867         See Also
868         --------
869         pandas.DataFrame.copy
870         """
871         if data is None:
872             data = self._data
873 
874             if isinstance(data, indexing.MemoryCachedArray):
875                 # don't share caching between copies
876                 data = indexing.MemoryCachedArray(data.array)
877 
878             if deep:
879                 if hasattr(data, "__array_function__") or isinstance(
880                     data, dask_array_type
881                 ):
882                     data = data.copy()
883                 elif not isinstance(data, PandasIndexAdapter):
884                     # pandas.Index is immutable
885                     data = np.array(data)
886         else:
887             data = as_compatible_data(data)
888             if self.shape != data.shape:
889                 raise ValueError(
890                     "Data shape {} must match shape of object {}".format(
891                         data.shape, self.shape
892                     )
893                 )
894 
895         # note:
896         # dims is already an immutable tuple
897         # attributes and encoding will be copied when the new Array is created
898         return self._replace(data=data)
899 
900     def _replace(
901         self, dims=_default, data=_default, attrs=_default, encoding=_default
902     ) -> "Variable":
903         if dims is _default:
904             dims = copy.copy(self._dims)
905         if data is _default:
906             data = copy.copy(self.data)
907         if attrs is _default:
908             attrs = copy.copy(self._attrs)
909         if encoding is _default:
910             encoding = copy.copy(self._encoding)
911         return type(self)(dims, data, attrs, encoding, fastpath=True)
912 
913     def __copy__(self):
914         return self.copy(deep=False)
915 
916     def __deepcopy__(self, memo=None):
917         # memo does nothing but is required for compatibility with
918         # copy.deepcopy
919         return self.copy(deep=True)
920 
921     # mutable objects should not be hashable
922     # https://github.com/python/mypy/issues/4266
923     __hash__ = None  # type: ignore
924 
925     @property
926     def chunks(self):
927         """Block dimensions for this array's data or None if it's not a dask
928         array.
929         """
930         return getattr(self._data, "chunks", None)
931 
932     _array_counter = itertools.count()
933 
934     def chunk(self, chunks=None, name=None, lock=False):
935         """Coerce this array's data into a dask arrays with the given chunks.
936 
937         If this variable is a non-dask array, it will be converted to dask
938         array. If it's a dask array, it will be rechunked to the given chunk
939         sizes.
940 
941         If neither chunks is not provided for one or more dimensions, chunk
942         sizes along that dimension will not be updated; non-dask arrays will be
943         converted into dask arrays with a single block.
944 
945         Parameters
946         ----------
947         chunks : int, tuple or dict, optional
948             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
949             ``{'x': 5, 'y': 5}``.
950         name : str, optional
951             Used to generate the name for this array in the internal dask
952             graph. Does not need not be unique.
953         lock : optional
954             Passed on to :py:func:`dask.array.from_array`, if the array is not
955             already as dask array.
956 
957         Returns
958         -------
959         chunked : xarray.Variable
960         """
961         import dask
962         import dask.array as da
963 
964         if utils.is_dict_like(chunks):
965             chunks = {self.get_axis_num(dim): chunk for dim, chunk in chunks.items()}
966 
967         if chunks is None:
968             chunks = self.chunks or self.shape
969 
970         data = self._data
971         if isinstance(data, da.Array):
972             data = data.rechunk(chunks)
973         else:
974             if isinstance(data, indexing.ExplicitlyIndexed):
975                 # Unambiguously handle array storage backends (like NetCDF4 and h5py)
976                 # that can't handle general array indexing. For example, in netCDF4 you
977                 # can do "outer" indexing along two dimensions independent, which works
978                 # differently from how NumPy handles it.
979                 # da.from_array works by using lazy indexing with a tuple of slices.
980                 # Using OuterIndexer is a pragmatic choice: dask does not yet handle
981                 # different indexing types in an explicit way:
982                 # https://github.com/dask/dask/issues/2883
983                 data = indexing.ImplicitToExplicitIndexingAdapter(
984                     data, indexing.OuterIndexer
985                 )
986                 if LooseVersion(dask.__version__) < "2.0.0":
987                     kwargs = {}
988                 else:
989                     # All of our lazily loaded backend array classes should use NumPy
990                     # array operations.
991                     kwargs = {"meta": np.ndarray}
992             else:
993                 kwargs = {}
994 
995             if utils.is_dict_like(chunks):
996                 chunks = tuple(chunks.get(n, s) for n, s in enumerate(self.shape))
997 
998             data = da.from_array(data, chunks, name=name, lock=lock, **kwargs)
999 
1000         return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)
1001 
1002     def _as_sparse(self, sparse_format=_default, fill_value=dtypes.NA):
1003         """
1004         use sparse-array as backend.
1005         """
1006         import sparse
1007 
1008         # TODO  what to do if dask-backended?
1009         if fill_value is dtypes.NA:
1010             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1011         else:
1012             dtype = dtypes.result_type(self.dtype, fill_value)
1013 
1014         if sparse_format is _default:
1015             sparse_format = "coo"
1016         try:
1017             as_sparse = getattr(sparse, "as_{}".format(sparse_format.lower()))
1018         except AttributeError:
1019             raise ValueError("{} is not a valid sparse format".format(sparse_format))
1020 
1021         data = as_sparse(self.data.astype(dtype), fill_value=fill_value)
1022         return self._replace(data=data)
1023 
1024     def _to_dense(self):
1025         """
1026         Change backend from sparse to np.array
1027         """
1028         if hasattr(self._data, "todense"):
1029             return self._replace(data=self._data.todense())
1030         return self.copy(deep=False)
1031 
1032     def isel(
1033         self: VariableType,
1034         indexers: Mapping[Hashable, Any] = None,
1035         **indexers_kwargs: Any,
1036     ) -> VariableType:
1037         """Return a new array indexed along the specified dimension(s).
1038 
1039         Parameters
1040         ----------
1041         **indexers : {dim: indexer, ...}
1042             Keyword arguments with names matching dimensions and values given
1043             by integers, slice objects or arrays.
1044 
1045         Returns
1046         -------
1047         obj : Array object
1048             A new Array with the selected data and dimensions. In general,
1049             the new variable's data will be a view of this variable's data,
1050             unless numpy fancy indexing was triggered by using an array
1051             indexer, in which case the data will be a copy.
1052         """
1053         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1054 
1055         invalid = indexers.keys() - set(self.dims)
1056         if invalid:
1057             raise ValueError("dimensions %r do not exist" % invalid)
1058 
1059         key = tuple(indexers.get(dim, slice(None)) for dim in self.dims)
1060         return self[key]
1061 
1062     def squeeze(self, dim=None):
1063         """Return a new object with squeezed data.
1064 
1065         Parameters
1066         ----------
1067         dim : None or str or tuple of str, optional
1068             Selects a subset of the length one dimensions. If a dimension is
1069             selected with length greater than one, an error is raised. If
1070             None, all length one dimensions are squeezed.
1071 
1072         Returns
1073         -------
1074         squeezed : same type as caller
1075             This object, but with with all or a subset of the dimensions of
1076             length 1 removed.
1077 
1078         See Also
1079         --------
1080         numpy.squeeze
1081         """
1082         dims = common.get_squeeze_dims(self, dim)
1083         return self.isel({d: 0 for d in dims})
1084 
1085     def _shift_one_dim(self, dim, count, fill_value=dtypes.NA):
1086         axis = self.get_axis_num(dim)
1087 
1088         if count > 0:
1089             keep = slice(None, -count)
1090         elif count < 0:
1091             keep = slice(-count, None)
1092         else:
1093             keep = slice(None)
1094 
1095         trimmed_data = self[(slice(None),) * axis + (keep,)].data
1096 
1097         if fill_value is dtypes.NA:
1098             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1099         else:
1100             dtype = self.dtype
1101 
1102         shape = list(self.shape)
1103         shape[axis] = min(abs(count), shape[axis])
1104 
1105         if isinstance(trimmed_data, dask_array_type):
1106             chunks = list(trimmed_data.chunks)
1107             chunks[axis] = (shape[axis],)
1108             full = functools.partial(da.full, chunks=chunks)
1109         else:
1110             full = np.full
1111 
1112         filler = full(shape, fill_value, dtype=dtype)
1113 
1114         if count > 0:
1115             arrays = [filler, trimmed_data]
1116         else:
1117             arrays = [trimmed_data, filler]
1118 
1119         data = duck_array_ops.concatenate(arrays, axis)
1120 
1121         if isinstance(data, dask_array_type):
1122             # chunked data should come out with the same chunks; this makes
1123             # it feasible to combine shifted and unshifted data
1124             # TODO: remove this once dask.array automatically aligns chunks
1125             data = data.rechunk(self.data.chunks)
1126 
1127         return type(self)(self.dims, data, self._attrs, fastpath=True)
1128 
1129     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):
1130         """
1131         Return a new Variable with shifted data.
1132 
1133         Parameters
1134         ----------
1135         shifts : mapping of the form {dim: offset}
1136             Integer offset to shift along each of the given dimensions.
1137             Positive offsets shift to the right; negative offsets shift to the
1138             left.
1139         fill_value: scalar, optional
1140             Value to use for newly missing values
1141         **shifts_kwargs:
1142             The keyword arguments form of ``shifts``.
1143             One of shifts or shifts_kwargs must be provided.
1144 
1145         Returns
1146         -------
1147         shifted : Variable
1148             Variable with the same dimensions and attributes but shifted data.
1149         """
1150         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
1151         result = self
1152         for dim, count in shifts.items():
1153             result = result._shift_one_dim(dim, count, fill_value=fill_value)
1154         return result
1155 
1156     def pad_with_fill_value(
1157         self, pad_widths=None, fill_value=dtypes.NA, **pad_widths_kwargs
1158     ):
1159         """
1160         Return a new Variable with paddings.
1161 
1162         Parameters
1163         ----------
1164         pad_width: Mapping of the form {dim: (before, after)}
1165             Number of values padded to the edges of each dimension.
1166         **pad_widths_kwargs:
1167             Keyword argument for pad_widths
1168         """
1169         pad_widths = either_dict_or_kwargs(pad_widths, pad_widths_kwargs, "pad")
1170 
1171         if fill_value is dtypes.NA:
1172             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1173         else:
1174             dtype = self.dtype
1175 
1176         if isinstance(self.data, dask_array_type):
1177             array = self.data
1178 
1179             # Dask does not yet support pad. We manually implement it.
1180             # https://github.com/dask/dask/issues/1926
1181             for d, pad in pad_widths.items():
1182                 axis = self.get_axis_num(d)
1183                 before_shape = list(array.shape)
1184                 before_shape[axis] = pad[0]
1185                 before_chunks = list(array.chunks)
1186                 before_chunks[axis] = (pad[0],)
1187                 after_shape = list(array.shape)
1188                 after_shape[axis] = pad[1]
1189                 after_chunks = list(array.chunks)
1190                 after_chunks[axis] = (pad[1],)
1191 
1192                 arrays = []
1193                 if pad[0] > 0:
1194                     arrays.append(
1195                         da.full(
1196                             before_shape, fill_value, dtype=dtype, chunks=before_chunks
1197                         )
1198                     )
1199                 arrays.append(array)
1200                 if pad[1] > 0:
1201                     arrays.append(
1202                         da.full(
1203                             after_shape, fill_value, dtype=dtype, chunks=after_chunks
1204                         )
1205                     )
1206                 if len(arrays) > 1:
1207                     array = da.concatenate(arrays, axis=axis)
1208         else:
1209             pads = [(0, 0) if d not in pad_widths else pad_widths[d] for d in self.dims]
1210             array = np.pad(
1211                 self.data.astype(dtype, copy=False),
1212                 pads,
1213                 mode="constant",
1214                 constant_values=fill_value,
1215             )
1216         return type(self)(self.dims, array)
1217 
1218     def _roll_one_dim(self, dim, count):
1219         axis = self.get_axis_num(dim)
1220 
1221         count %= self.shape[axis]
1222         if count != 0:
1223             indices = [slice(-count, None), slice(None, -count)]
1224         else:
1225             indices = [slice(None)]
1226 
1227         arrays = [self[(slice(None),) * axis + (idx,)].data for idx in indices]
1228 
1229         data = duck_array_ops.concatenate(arrays, axis)
1230 
1231         if isinstance(data, dask_array_type):
1232             # chunked data should come out with the same chunks; this makes
1233             # it feasible to combine shifted and unshifted data
1234             # TODO: remove this once dask.array automatically aligns chunks
1235             data = data.rechunk(self.data.chunks)
1236 
1237         return type(self)(self.dims, data, self._attrs, fastpath=True)
1238 
1239     def roll(self, shifts=None, **shifts_kwargs):
1240         """
1241         Return a new Variable with rolld data.
1242 
1243         Parameters
1244         ----------
1245         shifts : mapping of the form {dim: offset}
1246             Integer offset to roll along each of the given dimensions.
1247             Positive offsets roll to the right; negative offsets roll to the
1248             left.
1249         **shifts_kwargs:
1250             The keyword arguments form of ``shifts``.
1251             One of shifts or shifts_kwargs must be provided.
1252 
1253         Returns
1254         -------
1255         shifted : Variable
1256             Variable with the same dimensions and attributes but rolled data.
1257         """
1258         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
1259 
1260         result = self
1261         for dim, count in shifts.items():
1262             result = result._roll_one_dim(dim, count)
1263         return result
1264 
1265     def transpose(self, *dims) -> "Variable":
1266         """Return a new Variable object with transposed dimensions.
1267 
1268         Parameters
1269         ----------
1270         *dims : str, optional
1271             By default, reverse the dimensions. Otherwise, reorder the
1272             dimensions to this order.
1273 
1274         Returns
1275         -------
1276         transposed : Variable
1277             The returned object has transposed data and dimensions with the
1278             same attributes as the original.
1279 
1280         Notes
1281         -----
1282         This operation returns a view of this variable's data. It is
1283         lazy for dask-backed Variables but not for numpy-backed Variables.
1284 
1285         See Also
1286         --------
1287         numpy.transpose
1288         """
1289         if len(dims) == 0:
1290             dims = self.dims[::-1]
1291         dims = tuple(infix_dims(dims, self.dims))
1292         axes = self.get_axis_num(dims)
1293         if len(dims) < 2 or dims == self.dims:
1294             # no need to transpose if only one dimension
1295             # or dims are in same order
1296             return self.copy(deep=False)
1297 
1298         data = as_indexable(self._data).transpose(axes)
1299         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)
1300 
1301     @property
1302     def T(self) -> "Variable":
1303         return self.transpose()
1304 
1305     def set_dims(self, dims, shape=None):
1306         """Return a new variable with given set of dimensions.
1307         This method might be used to attach new dimension(s) to variable.
1308 
1309         When possible, this operation does not copy this variable's data.
1310 
1311         Parameters
1312         ----------
1313         dims : str or sequence of str or dict
1314             Dimensions to include on the new variable. If a dict, values are
1315             used to provide the sizes of new dimensions; otherwise, new
1316             dimensions are inserted with length 1.
1317 
1318         Returns
1319         -------
1320         Variable
1321         """
1322         if isinstance(dims, str):
1323             dims = [dims]
1324 
1325         if shape is None and utils.is_dict_like(dims):
1326             shape = dims.values()
1327 
1328         missing_dims = set(self.dims) - set(dims)
1329         if missing_dims:
1330             raise ValueError(
1331                 "new dimensions %r must be a superset of "
1332                 "existing dimensions %r" % (dims, self.dims)
1333             )
1334 
1335         self_dims = set(self.dims)
1336         expanded_dims = tuple(d for d in dims if d not in self_dims) + self.dims
1337 
1338         if self.dims == expanded_dims:
1339             # don't use broadcast_to unless necessary so the result remains
1340             # writeable if possible
1341             expanded_data = self.data
1342         elif shape is not None:
1343             dims_map = dict(zip(dims, shape))
1344             tmp_shape = tuple(dims_map[d] for d in expanded_dims)
1345             expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)
1346         else:
1347             expanded_data = self.data[(None,) * (len(expanded_dims) - self.ndim)]
1348 
1349         expanded_var = Variable(
1350             expanded_dims, expanded_data, self._attrs, self._encoding, fastpath=True
1351         )
1352         return expanded_var.transpose(*dims)
1353 
1354     def _stack_once(self, dims, new_dim):
1355         if not set(dims) <= set(self.dims):
1356             raise ValueError("invalid existing dimensions: %s" % dims)
1357 
1358         if new_dim in self.dims:
1359             raise ValueError(
1360                 "cannot create a new dimension with the same "
1361                 "name as an existing dimension"
1362             )
1363 
1364         if len(dims) == 0:
1365             # don't stack
1366             return self.copy(deep=False)
1367 
1368         other_dims = [d for d in self.dims if d not in dims]
1369         dim_order = other_dims + list(dims)
1370         reordered = self.transpose(*dim_order)
1371 
1372         new_shape = reordered.shape[: len(other_dims)] + (-1,)
1373         new_data = reordered.data.reshape(new_shape)
1374         new_dims = reordered.dims[: len(other_dims)] + (new_dim,)
1375 
1376         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)
1377 
1378     def stack(self, dimensions=None, **dimensions_kwargs):
1379         """
1380         Stack any number of existing dimensions into a single new dimension.
1381 
1382         New dimensions will be added at the end, and the order of the data
1383         along each new dimension will be in contiguous (C) order.
1384 
1385         Parameters
1386         ----------
1387         dimensions : Mapping of form new_name=(dim1, dim2, ...)
1388             Names of new dimensions, and the existing dimensions that they
1389             replace.
1390         **dimensions_kwargs:
1391             The keyword arguments form of ``dimensions``.
1392             One of dimensions or dimensions_kwargs must be provided.
1393 
1394         Returns
1395         -------
1396         stacked : Variable
1397             Variable with the same attributes but stacked data.
1398 
1399         See also
1400         --------
1401         Variable.unstack
1402         """
1403         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
1404         result = self
1405         for new_dim, dims in dimensions.items():
1406             result = result._stack_once(dims, new_dim)
1407         return result
1408 
1409     def _unstack_once(self, dims, old_dim):
1410         new_dim_names = tuple(dims.keys())
1411         new_dim_sizes = tuple(dims.values())
1412 
1413         if old_dim not in self.dims:
1414             raise ValueError("invalid existing dimension: %s" % old_dim)
1415 
1416         if set(new_dim_names).intersection(self.dims):
1417             raise ValueError(
1418                 "cannot create a new dimension with the same "
1419                 "name as an existing dimension"
1420             )
1421 
1422         if np.prod(new_dim_sizes) != self.sizes[old_dim]:
1423             raise ValueError(
1424                 "the product of the new dimension sizes must "
1425                 "equal the size of the old dimension"
1426             )
1427 
1428         other_dims = [d for d in self.dims if d != old_dim]
1429         dim_order = other_dims + [old_dim]
1430         reordered = self.transpose(*dim_order)
1431 
1432         new_shape = reordered.shape[: len(other_dims)] + new_dim_sizes
1433         new_data = reordered.data.reshape(new_shape)
1434         new_dims = reordered.dims[: len(other_dims)] + new_dim_names
1435 
1436         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)
1437 
1438     def unstack(self, dimensions=None, **dimensions_kwargs):
1439         """
1440         Unstack an existing dimension into multiple new dimensions.
1441 
1442         New dimensions will be added at the end, and the order of the data
1443         along each new dimension will be in contiguous (C) order.
1444 
1445         Parameters
1446         ----------
1447         dimensions : mapping of the form old_dim={dim1: size1, ...}
1448             Names of existing dimensions, and the new dimensions and sizes
1449             that they map to.
1450         **dimensions_kwargs:
1451             The keyword arguments form of ``dimensions``.
1452             One of dimensions or dimensions_kwargs must be provided.
1453 
1454         Returns
1455         -------
1456         unstacked : Variable
1457             Variable with the same attributes but unstacked data.
1458 
1459         See also
1460         --------
1461         Variable.stack
1462         """
1463         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "unstack")
1464         result = self
1465         for old_dim, dims in dimensions.items():
1466             result = result._unstack_once(dims, old_dim)
1467         return result
1468 
1469     def fillna(self, value):
1470         return ops.fillna(self, value)
1471 
1472     def where(self, cond, other=dtypes.NA):
1473         return ops.where_method(self, cond, other)
1474 
1475     def reduce(
1476         self,
1477         func,
1478         dim=None,
1479         axis=None,
1480         keep_attrs=None,
1481         keepdims=False,
1482         allow_lazy=None,
1483         **kwargs,
1484     ):
1485         """Reduce this array by applying `func` along some dimension(s).
1486 
1487         Parameters
1488         ----------
1489         func : function
1490             Function which can be called in the form
1491             `func(x, axis=axis, **kwargs)` to return the result of reducing an
1492             np.ndarray over an integer valued axis.
1493         dim : str or sequence of str, optional
1494             Dimension(s) over which to apply `func`.
1495         axis : int or sequence of int, optional
1496             Axis(es) over which to apply `func`. Only one of the 'dim'
1497             and 'axis' arguments can be supplied. If neither are supplied, then
1498             the reduction is calculated over the flattened array (by calling
1499             `func(x)` without an axis argument).
1500         keep_attrs : bool, optional
1501             If True, the variable's attributes (`attrs`) will be copied from
1502             the original object to the new one.  If False (default), the new
1503             object will be returned without attributes.
1504         keepdims : bool, default False
1505             If True, the dimensions which are reduced are left in the result
1506             as dimensions of size one
1507         **kwargs : dict
1508             Additional keyword arguments passed on to `func`.
1509 
1510         Returns
1511         -------
1512         reduced : Array
1513             Array with summarized data and the indicated dimension(s)
1514             removed.
1515         """
1516         if dim == ...:
1517             dim = None
1518         if dim is not None and axis is not None:
1519             raise ValueError("cannot supply both 'axis' and 'dim' arguments")
1520 
1521         if dim is not None:
1522             axis = self.get_axis_num(dim)
1523 
1524         if allow_lazy is not None:
1525             warnings.warn(
1526                 "allow_lazy is deprecated and will be removed in version 0.16.0. It is now True by default.",
1527                 DeprecationWarning,
1528             )
1529         else:
1530             allow_lazy = True
1531 
1532         input_data = self.data if allow_lazy else self.values
1533 
1534         if axis is not None:
1535             data = func(input_data, axis=axis, **kwargs)
1536         else:
1537             data = func(input_data, **kwargs)
1538 
1539         if getattr(data, "shape", ()) == self.shape:
1540             dims = self.dims
1541         else:
1542             removed_axes = (
1543                 range(self.ndim) if axis is None else np.atleast_1d(axis) % self.ndim
1544             )
1545             if keepdims:
1546                 # Insert np.newaxis for removed dims
1547                 slices = tuple(
1548                     np.newaxis if i in removed_axes else slice(None, None)
1549                     for i in range(self.ndim)
1550                 )
1551                 if getattr(data, "shape", None) is None:
1552                     # Reduce has produced a scalar value, not an array-like
1553                     data = np.asanyarray(data)[slices]
1554                 else:
1555                     data = data[slices]
1556                 dims = self.dims
1557             else:
1558                 dims = [
1559                     adim for n, adim in enumerate(self.dims) if n not in removed_axes
1560                 ]
1561 
1562         if keep_attrs is None:
1563             keep_attrs = _get_keep_attrs(default=False)
1564         attrs = self._attrs if keep_attrs else None
1565 
1566         return Variable(dims, data, attrs=attrs)
1567 
1568     @classmethod
1569     def concat(cls, variables, dim="concat_dim", positions=None, shortcut=False):
1570         """Concatenate variables along a new or existing dimension.
1571 
1572         Parameters
1573         ----------
1574         variables : iterable of Array
1575             Arrays to stack together. Each variable is expected to have
1576             matching dimensions and shape except for along the stacked
1577             dimension.
1578         dim : str or DataArray, optional
1579             Name of the dimension to stack along. This can either be a new
1580             dimension name, in which case it is added along axis=0, or an
1581             existing dimension name, in which case the location of the
1582             dimension is unchanged. Where to insert the new dimension is
1583             determined by the first variable.
1584         positions : None or list of integer arrays, optional
1585             List of integer arrays which specifies the integer positions to
1586             which to assign each dataset along the concatenated dimension.
1587             If not supplied, objects are concatenated in the provided order.
1588         shortcut : bool, optional
1589             This option is used internally to speed-up groupby operations.
1590             If `shortcut` is True, some checks of internal consistency between
1591             arrays to concatenate are skipped.
1592 
1593         Returns
1594         -------
1595         stacked : Variable
1596             Concatenated Variable formed by stacking all the supplied variables
1597             along the given dimension.
1598         """
1599         if not isinstance(dim, str):
1600             (dim,) = dim.dims
1601 
1602         # can't do this lazily: we need to loop through variables at least
1603         # twice
1604         variables = list(variables)
1605         first_var = variables[0]
1606 
1607         arrays = [v.data for v in variables]
1608 
1609         if dim in first_var.dims:
1610             axis = first_var.get_axis_num(dim)
1611             dims = first_var.dims
1612             data = duck_array_ops.concatenate(arrays, axis=axis)
1613             if positions is not None:
1614                 # TODO: deprecate this option -- we don't need it for groupby
1615                 # any more.
1616                 indices = nputils.inverse_permutation(np.concatenate(positions))
1617                 data = duck_array_ops.take(data, indices, axis=axis)
1618         else:
1619             axis = 0
1620             dims = (dim,) + first_var.dims
1621             data = duck_array_ops.stack(arrays, axis=axis)
1622 
1623         attrs = dict(first_var.attrs)
1624         encoding = dict(first_var.encoding)
1625         if not shortcut:
1626             for var in variables:
1627                 if var.dims != first_var.dims:
1628                     raise ValueError("inconsistent dimensions")
1629                 utils.remove_incompatible_items(attrs, var.attrs)
1630 
1631         return cls(dims, data, attrs, encoding)
1632 
1633     def equals(self, other, equiv=duck_array_ops.array_equiv):
1634         """True if two Variables have the same dimensions and values;
1635         otherwise False.
1636 
1637         Variables can still be equal (like pandas objects) if they have NaN
1638         values in the same locations.
1639 
1640         This method is necessary because `v1 == v2` for Variables
1641         does element-wise comparisons (like numpy.ndarrays).
1642         """
1643         other = getattr(other, "variable", other)
1644         try:
1645             return self.dims == other.dims and (
1646                 self._data is other._data or equiv(self.data, other.data)
1647             )
1648         except (TypeError, AttributeError):
1649             return False
1650 
1651     def broadcast_equals(self, other, equiv=duck_array_ops.array_equiv):
1652         """True if two Variables have the values after being broadcast against
1653         each other; otherwise False.
1654 
1655         Variables can still be equal (like pandas objects) if they have NaN
1656         values in the same locations.
1657         """
1658         try:
1659             self, other = broadcast_variables(self, other)
1660         except (ValueError, AttributeError):
1661             return False
1662         return self.equals(other, equiv=equiv)
1663 
1664     def identical(self, other, equiv=duck_array_ops.array_equiv):
1665         """Like equals, but also checks attributes.
1666         """
1667         try:
1668             return utils.dict_equiv(self.attrs, other.attrs) and self.equals(
1669                 other, equiv=equiv
1670             )
1671         except (TypeError, AttributeError):
1672             return False
1673 
1674     def no_conflicts(self, other, equiv=duck_array_ops.array_notnull_equiv):
1675         """True if the intersection of two Variable's non-null data is
1676         equal; otherwise false.
1677 
1678         Variables can thus still be equal if there are locations where either,
1679         or both, contain NaN values.
1680         """
1681         return self.broadcast_equals(other, equiv=equiv)
1682 
1683     def quantile(self, q, dim=None, interpolation="linear", keep_attrs=None):
1684         """Compute the qth quantile of the data along the specified dimension.
1685 
1686         Returns the qth quantiles(s) of the array elements.
1687 
1688         Parameters
1689         ----------
1690         q : float in range of [0,1] (or sequence of floats)
1691             Quantile to compute, which must be between 0 and 1
1692             inclusive.
1693         dim : str or sequence of str, optional
1694             Dimension(s) over which to apply quantile.
1695         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
1696             This optional parameter specifies the interpolation method to
1697             use when the desired quantile lies between two data points
1698             ``i < j``:
1699                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is
1700                   the fractional part of the index surrounded by ``i`` and
1701                   ``j``.
1702                 * lower: ``i``.
1703                 * higher: ``j``.
1704                 * nearest: ``i`` or ``j``, whichever is nearest.
1705                 * midpoint: ``(i + j) / 2``.
1706         keep_attrs : bool, optional
1707             If True, the variable's attributes (`attrs`) will be copied from
1708             the original object to the new one.  If False (default), the new
1709             object will be returned without attributes.
1710 
1711         Returns
1712         -------
1713         quantiles : Variable
1714             If `q` is a single quantile, then the result
1715             is a scalar. If multiple percentiles are given, first axis of
1716             the result corresponds to the quantile and a quantile dimension
1717             is added to the return array. The other dimensions are the
1718             dimensions that remain after the reduction of the array.
1719 
1720         See Also
1721         --------
1722         numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile,
1723         DataArray.quantile
1724         """
1725 
1726         from .computation import apply_ufunc
1727 
1728         if keep_attrs is None:
1729             keep_attrs = _get_keep_attrs(default=False)
1730 
1731         scalar = utils.is_scalar(q)
1732         q = np.atleast_1d(np.asarray(q, dtype=np.float64))
1733 
1734         if dim is None:
1735             dim = self.dims
1736 
1737         if utils.is_scalar(dim):
1738             dim = [dim]
1739 
1740         def _wrapper(npa, **kwargs):
1741             # move quantile axis to end. required for apply_ufunc
1742             return np.moveaxis(np.nanpercentile(npa, **kwargs), 0, -1)
1743 
1744         axis = np.arange(-1, -1 * len(dim) - 1, -1)
1745         result = apply_ufunc(
1746             _wrapper,
1747             self,
1748             input_core_dims=[dim],
1749             exclude_dims=set(dim),
1750             output_core_dims=[["quantile"]],
1751             output_dtypes=[np.float64],
1752             output_sizes={"quantile": len(q)},
1753             dask="parallelized",
1754             kwargs={"q": q * 100, "axis": axis, "interpolation": interpolation},
1755         )
1756 
1757         # for backward compatibility
1758         result = result.transpose("quantile", ...)
1759         if scalar:
1760             result = result.squeeze("quantile")
1761         if keep_attrs:
1762             result.attrs = self._attrs
1763         return result
1764 
1765     def rank(self, dim, pct=False):
1766         """Ranks the data.
1767 
1768         Equal values are assigned a rank that is the average of the ranks that
1769         would have been otherwise assigned to all of the values within that
1770         set.  Ranks begin at 1, not 0. If `pct`, computes percentage ranks.
1771 
1772         NaNs in the input array are returned as NaNs.
1773 
1774         The `bottleneck` library is required.
1775 
1776         Parameters
1777         ----------
1778         dim : str
1779             Dimension over which to compute rank.
1780         pct : bool, optional
1781             If True, compute percentage ranks, otherwise compute integer ranks.
1782 
1783         Returns
1784         -------
1785         ranked : Variable
1786 
1787         See Also
1788         --------
1789         Dataset.rank, DataArray.rank
1790         """
1791         import bottleneck as bn
1792 
1793         data = self.data
1794 
1795         if isinstance(data, dask_array_type):
1796             raise TypeError(
1797                 "rank does not work for arrays stored as dask "
1798                 "arrays. Load the data via .compute() or .load() "
1799                 "prior to calling this method."
1800             )
1801         elif not isinstance(data, np.ndarray):
1802             raise TypeError(
1803                 "rank is not implemented for {} objects.".format(type(data))
1804             )
1805 
1806         axis = self.get_axis_num(dim)
1807         func = bn.nanrankdata if self.dtype.kind == "f" else bn.rankdata
1808         ranked = func(data, axis=axis)
1809         if pct:
1810             count = np.sum(~np.isnan(data), axis=axis, keepdims=True)
1811             ranked /= count
1812         return Variable(self.dims, ranked)
1813 
1814     def rolling_window(
1815         self, dim, window, window_dim, center=False, fill_value=dtypes.NA
1816     ):
1817         """
1818         Make a rolling_window along dim and add a new_dim to the last place.
1819 
1820         Parameters
1821         ----------
1822         dim: str
1823             Dimension over which to compute rolling_window
1824         window: int
1825             Window size of the rolling
1826         window_dim: str
1827             New name of the window dimension.
1828         center: boolean. default False.
1829             If True, pad fill_value for both ends. Otherwise, pad in the head
1830             of the axis.
1831         fill_value:
1832             value to be filled.
1833 
1834         Returns
1835         -------
1836         Variable that is a view of the original array with a added dimension of
1837         size w.
1838         The return dim: self.dims + (window_dim, )
1839         The return shape: self.shape + (window, )
1840 
1841         Examples
1842         --------
1843         >>> v=Variable(('a', 'b'), np.arange(8).reshape((2,4)))
1844         >>> v.rolling_window(x, 'b', 3, 'window_dim')
1845         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
1846         array([[[nan, nan, 0], [nan, 0, 1], [0, 1, 2], [1, 2, 3]],
1847                [[nan, nan, 4], [nan, 4, 5], [4, 5, 6], [5, 6, 7]]])
1848 
1849         >>> v.rolling_window(x, 'b', 3, 'window_dim', center=True)
1850         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
1851         array([[[nan, 0, 1], [0, 1, 2], [1, 2, 3], [2, 3, nan]],
1852                [[nan, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, nan]]])
1853         """
1854         if fill_value is dtypes.NA:  # np.nan is passed
1855             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1856             array = self.astype(dtype, copy=False).data
1857         else:
1858             dtype = self.dtype
1859             array = self.data
1860 
1861         new_dims = self.dims + (window_dim,)
1862         return Variable(
1863             new_dims,
1864             duck_array_ops.rolling_window(
1865                 array,
1866                 axis=self.get_axis_num(dim),
1867                 window=window,
1868                 center=center,
1869                 fill_value=fill_value,
1870             ),
1871         )
1872 
1873     def coarsen(self, windows, func, boundary="exact", side="left", **kwargs):
1874         """
1875         Apply reduction function.
1876         """
1877         windows = {k: v for k, v in windows.items() if k in self.dims}
1878         if not windows:
1879             return self.copy()
1880 
1881         reshaped, axes = self._coarsen_reshape(windows, boundary, side)
1882         if isinstance(func, str):
1883             name = func
1884             func = getattr(duck_array_ops, name, None)
1885             if func is None:
1886                 raise NameError(f"{name} is not a valid method.")
1887         return self._replace(data=func(reshaped, axis=axes, **kwargs))
1888 
1889     def _coarsen_reshape(self, windows, boundary, side):
1890         """
1891         Construct a reshaped-array for coarsen
1892         """
1893         if not utils.is_dict_like(boundary):
1894             boundary = {d: boundary for d in windows.keys()}
1895 
1896         if not utils.is_dict_like(side):
1897             side = {d: side for d in windows.keys()}
1898 
1899         # remove unrelated dimensions
1900         boundary = {k: v for k, v in boundary.items() if k in windows}
1901         side = {k: v for k, v in side.items() if k in windows}
1902 
1903         for d, window in windows.items():
1904             if window <= 0:
1905                 raise ValueError(f"window must be > 0. Given {window}")
1906 
1907         variable = self
1908         for d, window in windows.items():
1909             # trim or pad the object
1910             size = variable.shape[self._get_axis_num(d)]
1911             n = int(size / window)
1912             if boundary[d] == "exact":
1913                 if n * window != size:
1914                     raise ValueError(
1915                         "Could not coarsen a dimension of size {} with "
1916                         "window {}".format(size, window)
1917                     )
1918             elif boundary[d] == "trim":
1919                 if side[d] == "left":
1920                     variable = variable.isel({d: slice(0, window * n)})
1921                 else:
1922                     excess = size - window * n
1923                     variable = variable.isel({d: slice(excess, None)})
1924             elif boundary[d] == "pad":  # pad
1925                 pad = window * n - size
1926                 if pad < 0:
1927                     pad += window
1928                 if side[d] == "left":
1929                     pad_widths = {d: (0, pad)}
1930                 else:
1931                     pad_widths = {d: (pad, 0)}
1932                 variable = variable.pad_with_fill_value(pad_widths)
1933             else:
1934                 raise TypeError(
1935                     "{} is invalid for boundary. Valid option is 'exact', "
1936                     "'trim' and 'pad'".format(boundary[d])
1937                 )
1938 
1939         shape = []
1940         axes = []
1941         axis_count = 0
1942         for i, d in enumerate(variable.dims):
1943             if d in windows:
1944                 size = variable.shape[i]
1945                 shape.append(int(size / windows[d]))
1946                 shape.append(windows[d])
1947                 axis_count += 1
1948                 axes.append(i + axis_count)
1949             else:
1950                 shape.append(variable.shape[i])
1951 
1952         return variable.data.reshape(shape), tuple(axes)
1953 
1954     @property
1955     def real(self):
1956         return type(self)(self.dims, self.data.real, self._attrs)
1957 
1958     @property
1959     def imag(self):
1960         return type(self)(self.dims, self.data.imag, self._attrs)
1961 
1962     def __array_wrap__(self, obj, context=None):
1963         return Variable(self.dims, obj)
1964 
1965     @staticmethod
1966     def _unary_op(f):
1967         @functools.wraps(f)
1968         def func(self, *args, **kwargs):
1969             with np.errstate(all="ignore"):
1970                 return self.__array_wrap__(f(self.data, *args, **kwargs))
1971 
1972         return func
1973 
1974     @staticmethod
1975     def _binary_op(f, reflexive=False, **ignored_kwargs):
1976         @functools.wraps(f)
1977         def func(self, other):
1978             if isinstance(other, (xr.DataArray, xr.Dataset)):
1979                 return NotImplemented
1980             self_data, other_data, dims = _broadcast_compat_data(self, other)
1981             keep_attrs = _get_keep_attrs(default=False)
1982             attrs = self._attrs if keep_attrs else None
1983             with np.errstate(all="ignore"):
1984                 new_data = (
1985                     f(self_data, other_data)
1986                     if not reflexive
1987                     else f(other_data, self_data)
1988                 )
1989             result = Variable(dims, new_data, attrs=attrs)
1990             return result
1991 
1992         return func
1993 
1994     @staticmethod
1995     def _inplace_binary_op(f):
1996         @functools.wraps(f)
1997         def func(self, other):
1998             if isinstance(other, xr.Dataset):
1999                 raise TypeError("cannot add a Dataset to a Variable in-place")
2000             self_data, other_data, dims = _broadcast_compat_data(self, other)
2001             if dims != self.dims:
2002                 raise ValueError("dimensions cannot change for in-place " "operations")
2003             with np.errstate(all="ignore"):
2004                 self.values = f(self_data, other_data)
2005             return self
2006 
2007         return func
2008 
2009     def _to_numeric(self, offset=None, datetime_unit=None, dtype=float):
2010         """ A (private) method to convert datetime array to numeric dtype
2011         See duck_array_ops.datetime_to_numeric
2012         """
2013         numeric_array = duck_array_ops.datetime_to_numeric(
2014             self.data, offset, datetime_unit, dtype
2015         )
2016         return type(self)(self.dims, numeric_array, self._attrs)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-17 15:36:05.262 | INFO     | Search completed. Bug locations: [<file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>quantile</method>
<code>
2935     def quantile(
2936         self,
2937         q: Any,
2938         dim: Union[Hashable, Sequence[Hashable], None] = None,
2939         interpolation: str = "linear",
2940         keep_attrs: bool = None,
2941     ) -> "DataArray":
2942         """Compute the qth quantile of the data along the specified dimension.
2943 
2944         Returns the qth quantiles(s) of the array elements.
2945 
2946         Parameters
2947         ----------
2948         q : float in range of [0,1] or array-like of floats
2949             Quantile to compute, which must be between 0 and 1 inclusive.
2950         dim : hashable or sequence of hashable, optional
2951             Dimension(s) over which to apply quantile.
2952         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
2953             This optional parameter specifies the interpolation method to
2954             use when the desired quantile lies between two data points
2955             ``i < j``:
2956 
2957                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is
2958                   the fractional part of the index surrounded by ``i`` and
2959                   ``j``.
2960                 - lower: ``i``.
2961                 - higher: ``j``.
2962                 - nearest: ``i`` or ``j``, whichever is nearest.
2963                 - midpoint: ``(i + j) / 2``.
2964         keep_attrs : bool, optional
2965             If True, the dataset's attributes (`attrs`) will be copied from
2966             the original object to the new one.  If False (default), the new
2967             object will be returned without attributes.
2968 
2969         Returns
2970         -------
2971         quantiles : DataArray
2972             If `q` is a single quantile, then the result
2973             is a scalar. If multiple percentiles are given, first axis of
2974             the result corresponds to the quantile and a quantile dimension
2975             is added to the return array. The other dimensions are the
2976             dimensions that remain after the reduction of the array.
2977 
2978         See Also
2979         --------
2980         numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile
2981 
2982         Examples
2983         --------
2984 
2985         >>> da = xr.DataArray(
2986         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
2987         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
2988         ...     dims=("x", "y"),
2989         ... )
2990         >>> da.quantile(0)  # or da.quantile(0, dim=...)
2991         <xarray.DataArray ()>
2992         array(0.7)
2993         Coordinates:
2994             quantile  float64 0.0
2995         >>> da.quantile(0, dim="x")
2996         <xarray.DataArray (y: 4)>
2997         array([0.7, 4.2, 2.6, 1.5])
2998         Coordinates:
2999           * y         (y) float64 1.0 1.5 2.0 2.5
3000             quantile  float64 0.0
3001         >>> da.quantile([0, 0.5, 1])
3002         <xarray.DataArray (quantile: 3)>
3003         array([0.7, 3.4, 9.4])
3004         Coordinates:
3005           * quantile  (quantile) float64 0.0 0.5 1.0
3006         >>> da.quantile([0, 0.5, 1], dim="x")
3007         <xarray.DataArray (quantile: 3, y: 4)>
3008         array([[0.7 , 4.2 , 2.6 , 1.5 ],
3009                [3.6 , 5.75, 6.  , 1.7 ],
3010                [6.5 , 7.3 , 9.4 , 1.9 ]])
3011         Coordinates:
3012           * y         (y) float64 1.0 1.5 2.0 2.5
3013           * quantile  (quantile) float64 0.0 0.5 1.0
3014         """
3015 
3016         ds = self._to_temp_dataset().quantile(
3017             q, dim=dim, keep_attrs=keep_attrs, interpolation=interpolation
3018         )
3019         return self._from_temp_dataset(ds)

</code><intended_behavior>The `quantile` method should validate the `q` parameter to ensure it falls within the 0 to 1 range, corresponding to quantiles, not percentiles. If a value outside this range is detected, the method should raise a `ValueError` with a corrected message: `"Quantiles must be in the range [0, 1]"`. This adjustment ensures users receive accurate feedback aligned with the method's expected inputs, preserving the existing functionality while correcting the misleading error message.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
214 class DataArray(AbstractArray, DataWithCoords):
215     """N-dimensional array with labeled coordinates and dimensions.
216 
217     DataArray provides a wrapper around numpy ndarrays that uses labeled
218     dimensions and coordinates to support metadata aware operations. The API is
219     similar to that for the pandas Series or DataFrame, but DataArray objects
220     can have any number of dimensions, and their contents have fixed data
221     types.
222 
223     Additional features over raw numpy arrays:
224 
225     - Apply operations over dimensions by name: ``x.sum('time')``.
226     - Select or assign values by integer location (like numpy): ``x[:10]``
227       or by label (like pandas): ``x.loc['2014-01-01']`` or
228       ``x.sel(time='2014-01-01')``.
229     - Mathematical operations (e.g., ``x - y``) vectorize across multiple
230       dimensions (known in numpy as "broadcasting") based on dimension names,
231       regardless of their original order.
232     - Keep track of arbitrary metadata in the form of a Python dictionary:
233       ``x.attrs``
234     - Convert to a pandas Series: ``x.to_series()``.
235 
236     Getting items from or doing mathematical operations with a DataArray
237     always returns another DataArray.
238     """
239 
240     _cache: Dict[str, Any]
241     _coords: Dict[Any, Variable]
242     _indexes: Optional[Dict[Hashable, pd.Index]]
243     _name: Optional[Hashable]
244     _variable: Variable
245 
246     __slots__ = (
247         "_cache",
248         "_coords",
249         "_file_obj",
250         "_indexes",
251         "_name",
252         "_variable",
253         "__weakref__",
254     )
255 
256     _groupby_cls = groupby.DataArrayGroupBy
257     _rolling_cls = rolling.DataArrayRolling
258     _coarsen_cls = rolling.DataArrayCoarsen
259     _resample_cls = resample.DataArrayResample
260 
261     dt = property(DatetimeAccessor)
262 
263     def __init__(
264         self,
265         data: Any = dtypes.NA,
266         coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None] = None,
267         dims: Union[Hashable, Sequence[Hashable], None] = None,
268         name: Hashable = None,
269         attrs: Mapping = None,
270         # deprecated parameters
271         encoding=None,
272         # internal parameters
273         indexes: Dict[Hashable, pd.Index] = None,
274         fastpath: bool = False,
275     ):
276         """
277         Parameters
278         ----------
279         data : array_like
280             Values for this array. Must be an ``numpy.ndarray``, ndarray like,
281             or castable to an ``ndarray``. If a self-described xarray or pandas
282             object, attempts are made to use this array's metadata to fill in
283             other unspecified arguments. A view of the array's data is used
284             instead of a copy if possible.
285         coords : sequence or dict of array_like objects, optional
286             Coordinates (tick labels) to use for indexing along each dimension.
287             The following notations are accepted:
288 
289             - mapping {dimension name: array-like}
290             - sequence of tuples that are valid arguments for xarray.Variable()
291               - (dims, data)
292               - (dims, data, attrs)
293               - (dims, data, attrs, encoding)
294 
295             Additionally, it is possible to define a coord whose name
296             does not match the dimension name, or a coord based on multiple
297             dimensions, with one of the following notations:
298 
299             - mapping {coord name: DataArray}
300             - mapping {coord name: Variable}
301             - mapping {coord name: (dimension name, array-like)}
302             - mapping {coord name: (tuple of dimension names, array-like)}
303 
304         dims : hashable or sequence of hashable, optional
305             Name(s) of the data dimension(s). Must be either a hashable (only
306             for 1D data) or a sequence of hashables with length equal to the
307             number of dimensions. If this argument is omitted, dimension names
308             are taken from ``coords`` (if possible) and otherwise default to
309             ``['dim_0', ... 'dim_n']``.
310         name : str or None, optional
311             Name of this array.
312         attrs : dict_like or None, optional
313             Attributes to assign to the new instance. By default, an empty
314             attribute dictionary is initialized.
315         """
316         if encoding is not None:
317             warnings.warn(
318                 "The `encoding` argument to `DataArray` is deprecated, and . "
319                 "will be removed in 0.15. "
320                 "Instead, specify the encoding when writing to disk or "
321                 "set the `encoding` attribute directly.",
322                 FutureWarning,
323                 stacklevel=2,
324             )
325         if fastpath:
326             variable = data
327             assert dims is None
328             assert attrs is None
329             assert encoding is None
330         else:
331             # try to fill in arguments from data if they weren't supplied
332             if coords is None:
333 
334                 if isinstance(data, DataArray):
335                     coords = data.coords
336                 elif isinstance(data, pd.Series):
337                     coords = [data.index]
338                 elif isinstance(data, pd.DataFrame):
339                     coords = [data.index, data.columns]
340                 elif isinstance(data, (pd.Index, IndexVariable)):
341                     coords = [data]
342                 elif isinstance(data, pdcompat.Panel):
343                     coords = [data.items, data.major_axis, data.minor_axis]
344 
345             if dims is None:
346                 dims = getattr(data, "dims", getattr(coords, "dims", None))
347             if name is None:
348                 name = getattr(data, "name", None)
349             if attrs is None and not isinstance(data, PANDAS_TYPES):
350                 attrs = getattr(data, "attrs", None)
351             if encoding is None:
352                 encoding = getattr(data, "encoding", None)
353 
354             data = _check_data_shape(data, coords, dims)
355             data = as_compatible_data(data)
356             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
357             variable = Variable(dims, data, attrs, encoding, fastpath=True)
358             indexes = dict(
359                 _extract_indexes_from_coords(coords)
360             )  # needed for to_dataset
361 
362         # These fully describe a DataArray
363         self._variable = variable
364         assert isinstance(coords, dict)
365         self._coords = coords
366         self._name = name
367 
368         # TODO(shoyer): document this argument, once it becomes part of the
369         # public interface.
370         self._indexes = indexes
371 
372         self._file_obj = None
373 
374     def _replace(
375         self,
376         variable: Variable = None,
377         coords=None,
378         name: Union[Hashable, None, Default] = _default,
379         indexes=None,
380     ) -> "DataArray":
381         if variable is None:
382             variable = self.variable
383         if coords is None:
384             coords = self._coords
385         if name is _default:
386             name = self.name
387         return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)
388 
389     def _replace_maybe_drop_dims(
390         self, variable: Variable, name: Union[Hashable, None, Default] = _default
391     ) -> "DataArray":
392         if variable.dims == self.dims and variable.shape == self.shape:
393             coords = self._coords.copy()
394             indexes = self._indexes
395         elif variable.dims == self.dims:
396             # Shape has changed (e.g. from reduce(..., keepdims=True)
397             new_sizes = dict(zip(self.dims, variable.shape))
398             coords = {
399                 k: v
400                 for k, v in self._coords.items()
401                 if v.shape == tuple(new_sizes[d] for d in v.dims)
402             }
403             changed_dims = [
404                 k for k in variable.dims if variable.sizes[k] != self.sizes[k]
405             ]
406             indexes = propagate_indexes(self._indexes, exclude=changed_dims)
407         else:
408             allowed_dims = set(variable.dims)
409             coords = {
410                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
411             }
412             indexes = propagate_indexes(
413                 self._indexes, exclude=(set(self.dims) - allowed_dims)
414             )
415         return self._replace(variable, coords, name, indexes=indexes)
416 
417     def _overwrite_indexes(self, indexes: Mapping[Hashable, Any]) -> "DataArray":
418         if not len(indexes):
419             return self
420         coords = self._coords.copy()
421         for name, idx in indexes.items():
422             coords[name] = IndexVariable(name, idx)
423         obj = self._replace(coords=coords)
424 
425         # switch from dimension to level names, if necessary
426         dim_names: Dict[Any, str] = {}
427         for dim, idx in indexes.items():
428             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:
429                 dim_names[dim] = idx.name
430         if dim_names:
431             obj = obj.rename(dim_names)
432         return obj
433 
434     def _to_temp_dataset(self) -> Dataset:
435         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
436 
437     def _from_temp_dataset(
438         self, dataset: Dataset, name: Hashable = _default
439     ) -> "DataArray":
440         variable = dataset._variables.pop(_THIS_ARRAY)
441         coords = dataset._variables
442         indexes = dataset._indexes
443         return self._replace(variable, coords, name, indexes=indexes)
444 
445     def _to_dataset_split(self, dim: Hashable) -> Dataset:
446         """ splits dataarray along dimension 'dim' """
447 
448         def subset(dim, label):
449             array = self.loc[{dim: label}]
450             array.attrs = {}
451             return as_variable(array)
452 
453         variables = {label: subset(dim, label) for label in self.get_index(dim)}
454         variables.update({k: v for k, v in self._coords.items() if k != dim})
455         indexes = propagate_indexes(self._indexes, exclude=dim)
456         coord_names = set(self._coords) - set([dim])
457         dataset = Dataset._construct_direct(
458             variables, coord_names, indexes=indexes, attrs=self.attrs
459         )
460         return dataset
461 
462     def _to_dataset_whole(
463         self, name: Hashable = None, shallow_copy: bool = True
464     ) -> Dataset:
465         if name is None:
466             name = self.name
467         if name is None:
468             raise ValueError(
469                 "unable to convert unnamed DataArray to a "
470                 "Dataset without providing an explicit name"
471             )
472         if name in self.coords:
473             raise ValueError(
474                 "cannot create a Dataset from a DataArray with "
475                 "the same name as one of its coordinates"
476             )
477         # use private APIs for speed: this is called by _to_temp_dataset(),
478         # which is used in the guts of a lot of operations (e.g., reindex)
479         variables = self._coords.copy()
480         variables[name] = self.variable
481         if shallow_copy:
482             for k in variables:
483                 variables[k] = variables[k].copy(deep=False)
484         indexes = self._indexes
485 
486         coord_names = set(self._coords)
487         dataset = Dataset._construct_direct(variables, coord_names, indexes=indexes)
488         return dataset
489 
490     def to_dataset(self, dim: Hashable = None, *, name: Hashable = None) -> Dataset:
491         """Convert a DataArray to a Dataset.
492 
493         Parameters
494         ----------
495         dim : hashable, optional
496             Name of the dimension on this array along which to split this array
497             into separate variables. If not provided, this array is converted
498             into a Dataset of one variable.
499         name : hashable, optional
500             Name to substitute for this array's name. Only valid if ``dim`` is
501             not provided.
502 
503         Returns
504         -------
505         dataset : Dataset
506         """
507         if dim is not None and dim not in self.dims:
508             raise TypeError(
509                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
510             )
511 
512         if dim is not None:
513             if name is not None:
514                 raise TypeError("cannot supply both dim and name arguments")
515             return self._to_dataset_split(dim)
516         else:
517             return self._to_dataset_whole(name)
518 
519     @property
520     def name(self) -> Optional[Hashable]:
521         """The name of this array.
522         """
523         return self._name
524 
525     @name.setter
526     def name(self, value: Optional[Hashable]) -> None:
527         self._name = value
528 
529     @property
530     def variable(self) -> Variable:
531         """Low level interface to the Variable object for this DataArray."""
532         return self._variable
533 
534     @property
535     def dtype(self) -> np.dtype:
536         return self.variable.dtype
537 
538     @property
539     def shape(self) -> Tuple[int, ...]:
540         return self.variable.shape
541 
542     @property
543     def size(self) -> int:
544         return self.variable.size
545 
546     @property
547     def nbytes(self) -> int:
548         return self.variable.nbytes
549 
550     @property
551     def ndim(self) -> int:
552         return self.variable.ndim
553 
554     def __len__(self) -> int:
555         return len(self.variable)
556 
557     @property
558     def data(self) -> Any:
559         """The array's data as a dask or numpy array
560         """
561         return self.variable.data
562 
563     @data.setter
564     def data(self, value: Any) -> None:
565         self.variable.data = value
566 
567     @property
568     def values(self) -> np.ndarray:
569         """The array's data as a numpy.ndarray"""
570         return self.variable.values
571 
572     @values.setter
573     def values(self, value: Any) -> None:
574         self.variable.values = value
575 
576     @property
577     def _in_memory(self) -> bool:
578         return self.variable._in_memory
579 
580     def to_index(self) -> pd.Index:
581         """Convert this variable to a pandas.Index. Only possible for 1D
582         arrays.
583         """
584         return self.variable.to_index()
585 
586     @property
587     def dims(self) -> Tuple[Hashable, ...]:
588         """Tuple of dimension names associated with this array.
589 
590         Note that the type of this property is inconsistent with
591         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
592         consistently named properties.
593         """
594         return self.variable.dims
595 
596     @dims.setter
597     def dims(self, value):
598         raise AttributeError(
599             "you cannot assign dims on a DataArray. Use "
600             ".rename() or .swap_dims() instead."
601         )
602 
603     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
604         if utils.is_dict_like(key):
605             return key
606         else:
607             key = indexing.expanded_indexer(key, self.ndim)
608             return dict(zip(self.dims, key))
609 
610     @property
611     def _level_coords(self) -> Dict[Hashable, Hashable]:
612         """Return a mapping of all MultiIndex levels and their corresponding
613         coordinate name.
614         """
615         level_coords: Dict[Hashable, Hashable] = {}
616 
617         for cname, var in self._coords.items():
618             if var.ndim == 1 and isinstance(var, IndexVariable):
619                 level_names = var.level_names
620                 if level_names is not None:
621                     (dim,) = var.dims
622                     level_coords.update({lname: dim for lname in level_names})
623         return level_coords
624 
625     def _getitem_coord(self, key):
626         from .dataset import _get_virtual_variable
627 
628         try:
629             var = self._coords[key]
630         except KeyError:
631             dim_sizes = dict(zip(self.dims, self.shape))
632             _, key, var = _get_virtual_variable(
633                 self._coords, key, self._level_coords, dim_sizes
634             )
635 
636         return self._replace_maybe_drop_dims(var, name=key)
637 
638     def __getitem__(self, key: Any) -> "DataArray":
639         if isinstance(key, str):
640             return self._getitem_coord(key)
641         else:
642             # xarray-style array indexing
643             return self.isel(indexers=self._item_key_to_dict(key))
644 
645     def __setitem__(self, key: Any, value: Any) -> None:
646         if isinstance(key, str):
647             self.coords[key] = value
648         else:
649             # Coordinates in key, value and self[key] should be consistent.
650             # TODO Coordinate consistency in key is checked here, but it
651             # causes unnecessary indexing. It should be optimized.
652             obj = self[key]
653             if isinstance(value, DataArray):
654                 assert_coordinate_consistent(value, obj.coords.variables)
655             # DataArray key -> Variable key
656             key = {
657                 k: v.variable if isinstance(v, DataArray) else v
658                 for k, v in self._item_key_to_dict(key).items()
659             }
660             self.variable[key] = value
661 
662     def __delitem__(self, key: Any) -> None:
663         del self.coords[key]
664 
665     @property
666     def _attr_sources(self) -> List[Mapping[Hashable, Any]]:
667         """List of places to look-up items for attribute-style access
668         """
669         return self._item_sources + [self.attrs]
670 
671     @property
672     def _item_sources(self) -> List[Mapping[Hashable, Any]]:
673         """List of places to look-up items for key-completion
674         """
675         return [
676             self.coords,
677             {d: self.coords[d] for d in self.dims},
678             LevelCoordinatesSource(self),
679         ]
680 
681     def __contains__(self, key: Any) -> bool:
682         return key in self.data
683 
684     @property
685     def loc(self) -> _LocIndexer:
686         """Attribute for location based indexing like pandas.
687         """
688         return _LocIndexer(self)
689 
690     @property
691     def attrs(self) -> Dict[Hashable, Any]:
692         """Dictionary storing arbitrary metadata with this array."""
693         return self.variable.attrs
694 
695     @attrs.setter
696     def attrs(self, value: Mapping[Hashable, Any]) -> None:
697         # Disable type checking to work around mypy bug - see mypy#4167
698         self.variable.attrs = value  # type: ignore
699 
700     @property
701     def encoding(self) -> Dict[Hashable, Any]:
702         """Dictionary of format-specific settings for how this array should be
703         serialized."""
704         return self.variable.encoding
705 
706     @encoding.setter
707     def encoding(self, value: Mapping[Hashable, Any]) -> None:
708         self.variable.encoding = value
709 
710     @property
711     def indexes(self) -> Indexes:
712         """Mapping of pandas.Index objects used for label based indexing
713         """
714         if self._indexes is None:
715             self._indexes = default_indexes(self._coords, self.dims)
716         return Indexes(self._indexes)
717 
718     @property
719     def coords(self) -> DataArrayCoordinates:
720         """Dictionary-like container of coordinate arrays.
721         """
722         return DataArrayCoordinates(self)
723 
724     def reset_coords(
725         self,
726         names: Union[Iterable[Hashable], Hashable, None] = None,
727         drop: bool = False,
728         inplace: bool = None,
729     ) -> Union[None, "DataArray", Dataset]:
730         """Given names of coordinates, reset them to become variables.
731 
732         Parameters
733         ----------
734         names : hashable or iterable of hashables, optional
735             Name(s) of non-index coordinates in this dataset to reset into
736             variables. By default, all non-index coordinates are reset.
737         drop : bool, optional
738             If True, remove coordinates instead of converting them into
739             variables.
740 
741         Returns
742         -------
743         Dataset, or DataArray if ``drop == True``
744         """
745         _check_inplace(inplace)
746         if names is None:
747             names = set(self.coords) - set(self.dims)
748         dataset = self.coords.to_dataset().reset_coords(names, drop)
749         if drop:
750             return self._replace(coords=dataset._variables)
751         else:
752             if self.name is None:
753                 raise ValueError(
754                     "cannot reset_coords with drop=False on an unnamed DataArrray"
755                 )
756             dataset[self.name] = self.variable
757             return dataset
758 
759     def __dask_tokenize__(self):
760         from dask.base import normalize_token
761 
762         return normalize_token((type(self), self._variable, self._coords, self._name))
763 
764     def __dask_graph__(self):
765         return self._to_temp_dataset().__dask_graph__()
766 
767     def __dask_keys__(self):
768         return self._to_temp_dataset().__dask_keys__()
769 
770     def __dask_layers__(self):
771         return self._to_temp_dataset().__dask_layers__()
772 
773     @property
774     def __dask_optimize__(self):
775         return self._to_temp_dataset().__dask_optimize__
776 
777     @property
778     def __dask_scheduler__(self):
779         return self._to_temp_dataset().__dask_scheduler__
780 
781     def __dask_postcompute__(self):
782         func, args = self._to_temp_dataset().__dask_postcompute__()
783         return self._dask_finalize, (func, args, self.name)
784 
785     def __dask_postpersist__(self):
786         func, args = self._to_temp_dataset().__dask_postpersist__()
787         return self._dask_finalize, (func, args, self.name)
788 
789     @staticmethod
790     def _dask_finalize(results, func, args, name):
791         ds = func(results, *args)
792         variable = ds._variables.pop(_THIS_ARRAY)
793         coords = ds._variables
794         return DataArray(variable, coords, name=name, fastpath=True)
795 
796     def load(self, **kwargs) -> "DataArray":
797         """Manually trigger loading of this array's data from disk or a
798         remote source into memory and return this array.
799 
800         Normally, it should not be necessary to call this method in user code,
801         because all xarray functions should either work on deferred data or
802         load data automatically. However, this method can be necessary when
803         working with many file objects on disk.
804 
805         Parameters
806         ----------
807         **kwargs : dict
808             Additional keyword arguments passed on to ``dask.array.compute``.
809 
810         See Also
811         --------
812         dask.array.compute
813         """
814         ds = self._to_temp_dataset().load(**kwargs)
815         new = self._from_temp_dataset(ds)
816         self._variable = new._variable
817         self._coords = new._coords
818         return self
819 
820     def compute(self, **kwargs) -> "DataArray":
821         """Manually trigger loading of this array's data from disk or a
822         remote source into memory and return a new array. The original is
823         left unaltered.
824 
825         Normally, it should not be necessary to call this method in user code,
826         because all xarray functions should either work on deferred data or
827         load data automatically. However, this method can be necessary when
828         working with many file objects on disk.
829 
830         Parameters
831         ----------
832         **kwargs : dict
833             Additional keyword arguments passed on to ``dask.array.compute``.
834 
835         See Also
836         --------
837         dask.array.compute
838         """
839         new = self.copy(deep=False)
840         return new.load(**kwargs)
841 
842     def persist(self, **kwargs) -> "DataArray":
843         """ Trigger computation in constituent dask arrays
844 
845         This keeps them as dask arrays but encourages them to keep data in
846         memory.  This is particularly useful when on a distributed machine.
847         When on a single machine consider using ``.compute()`` instead.
848 
849         Parameters
850         ----------
851         **kwargs : dict
852             Additional keyword arguments passed on to ``dask.persist``.
853 
854         See Also
855         --------
856         dask.persist
857         """
858         ds = self._to_temp_dataset().persist(**kwargs)
859         return self._from_temp_dataset(ds)
860 
861     def copy(self, deep: bool = True, data: Any = None) -> "DataArray":
862         """Returns a copy of this array.
863 
864         If `deep=True`, a deep copy is made of the data array.
865         Otherwise, a shallow copy is made, so each variable in the new
866         array's dataset is also a variable in this array's dataset.
867 
868         Use `data` to create a new object with the same structure as
869         original but entirely new data.
870 
871         Parameters
872         ----------
873         deep : bool, optional
874             Whether the data array and its coordinates are loaded into memory
875             and copied onto the new object. Default is True.
876         data : array_like, optional
877             Data to use in the new object. Must have same shape as original.
878             When `data` is used, `deep` is ignored for all data variables,
879             and only used for coords.
880 
881         Returns
882         -------
883         object : DataArray
884             New object with dimensions, attributes, coordinates, name,
885             encoding, and optionally data copied from original.
886 
887         Examples
888         --------
889 
890         Shallow versus deep copy
891 
892         >>> array = xr.DataArray([1, 2, 3], dims='x',
893         ...                      coords={'x': ['a', 'b', 'c']})
894         >>> array.copy()
895         <xarray.DataArray (x: 3)>
896         array([1, 2, 3])
897         Coordinates:
898         * x        (x) <U1 'a' 'b' 'c'
899         >>> array_0 = array.copy(deep=False)
900         >>> array_0[0] = 7
901         >>> array_0
902         <xarray.DataArray (x: 3)>
903         array([7, 2, 3])
904         Coordinates:
905         * x        (x) <U1 'a' 'b' 'c'
906         >>> array
907         <xarray.DataArray (x: 3)>
908         array([7, 2, 3])
909         Coordinates:
910         * x        (x) <U1 'a' 'b' 'c'
911 
912         Changing the data using the ``data`` argument maintains the
913         structure of the original object, but with the new data. Original
914         object is unaffected.
915 
916         >>> array.copy(data=[0.1, 0.2, 0.3])
917         <xarray.DataArray (x: 3)>
918         array([ 0.1,  0.2,  0.3])
919         Coordinates:
920         * x        (x) <U1 'a' 'b' 'c'
921         >>> array
922         <xarray.DataArray (x: 3)>
923         array([1, 2, 3])
924         Coordinates:
925         * x        (x) <U1 'a' 'b' 'c'
926 
927         See Also
928         --------
929         pandas.DataFrame.copy
930         """
931         variable = self.variable.copy(deep=deep, data=data)
932         coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}
933         indexes = self._indexes
934         return self._replace(variable, coords, indexes=indexes)
935 
936     def __copy__(self) -> "DataArray":
937         return self.copy(deep=False)
938 
939     def __deepcopy__(self, memo=None) -> "DataArray":
940         # memo does nothing but is required for compatibility with
941         # copy.deepcopy
942         return self.copy(deep=True)
943 
944     # mutable objects should not be hashable
945     # https://github.com/python/mypy/issues/4266
946     __hash__ = None  # type: ignore
947 
948     @property
949     def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:
950         """Block dimensions for this array's data or None if it's not a dask
951         array.
952         """
953         return self.variable.chunks
954 
955     def chunk(
956         self,
957         chunks: Union[
958             None,
959             Number,
960             Tuple[Number, ...],
961             Tuple[Tuple[Number, ...], ...],
962             Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]],
963         ] = None,
964         name_prefix: str = "xarray-",
965         token: str = None,
966         lock: bool = False,
967     ) -> "DataArray":
968         """Coerce this array's data into a dask arrays with the given chunks.
969 
970         If this variable is a non-dask array, it will be converted to dask
971         array. If it's a dask array, it will be rechunked to the given chunk
972         sizes.
973 
974         If neither chunks is not provided for one or more dimensions, chunk
975         sizes along that dimension will not be updated; non-dask arrays will be
976         converted into dask arrays with a single block.
977 
978         Parameters
979         ----------
980         chunks : int, tuple or mapping, optional
981             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
982             ``{'x': 5, 'y': 5}``.
983         name_prefix : str, optional
984             Prefix for the name of the new dask array.
985         token : str, optional
986             Token uniquely identifying this array.
987         lock : optional
988             Passed on to :py:func:`dask.array.from_array`, if the array is not
989             already as dask array.
990 
991         Returns
992         -------
993         chunked : xarray.DataArray
994         """
995         if isinstance(chunks, (tuple, list)):
996             chunks = dict(zip(self.dims, chunks))
997 
998         ds = self._to_temp_dataset().chunk(
999             chunks, name_prefix=name_prefix, token=token, lock=lock
1000         )
1001         return self._from_temp_dataset(ds)
1002 
1003     def isel(
1004         self,
1005         indexers: Mapping[Hashable, Any] = None,
1006         drop: bool = False,
1007         **indexers_kwargs: Any,
1008     ) -> "DataArray":
1009         """Return a new DataArray whose data is given by integer indexing
1010         along the specified dimension(s).
1011 
1012         See Also
1013         --------
1014         Dataset.isel
1015         DataArray.sel
1016         """
1017         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1018         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1019             ds = self._to_temp_dataset()._isel_fancy(indexers, drop=drop)
1020             return self._from_temp_dataset(ds)
1021 
1022         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1023         # lists, or zero or one-dimensional np.ndarray's
1024 
1025         variable = self._variable.isel(indexers)
1026 
1027         coords = {}
1028         for coord_name, coord_value in self._coords.items():
1029             coord_indexers = {
1030                 k: v for k, v in indexers.items() if k in coord_value.dims
1031             }
1032             if coord_indexers:
1033                 coord_value = coord_value.isel(coord_indexers)
1034                 if drop and coord_value.ndim == 0:
1035                     continue
1036             coords[coord_name] = coord_value
1037 
1038         return self._replace(variable=variable, coords=coords)
1039 
1040     def sel(
1041         self,
1042         indexers: Mapping[Hashable, Any] = None,
1043         method: str = None,
1044         tolerance=None,
1045         drop: bool = False,
1046         **indexers_kwargs: Any,
1047     ) -> "DataArray":
1048         """Return a new DataArray whose data is given by selecting index
1049         labels along the specified dimension(s).
1050 
1051         .. warning::
1052 
1053           Do not try to assign values when using any of the indexing methods
1054           ``isel`` or ``sel``::
1055 
1056             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1057             # DO NOT do this
1058             da.isel(x=[0, 1, 2])[1] = -1
1059 
1060           Assigning values with the chained indexing using ``.sel`` or
1061           ``.isel`` fails silently.
1062 
1063         See Also
1064         --------
1065         Dataset.sel
1066         DataArray.isel
1067 
1068         """
1069         ds = self._to_temp_dataset().sel(
1070             indexers=indexers,
1071             drop=drop,
1072             method=method,
1073             tolerance=tolerance,
1074             **indexers_kwargs,
1075         )
1076         return self._from_temp_dataset(ds)
1077 
1078     def head(
1079         self,
1080         indexers: Union[Mapping[Hashable, int], int] = None,
1081         **indexers_kwargs: Any,
1082     ) -> "DataArray":
1083         """Return a new DataArray whose data is given by the the first `n`
1084         values along the specified dimension(s). Default `n` = 5
1085 
1086         See Also
1087         --------
1088         Dataset.head
1089         DataArray.tail
1090         DataArray.thin
1091         """
1092         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1093         return self._from_temp_dataset(ds)
1094 
1095     def tail(
1096         self,
1097         indexers: Union[Mapping[Hashable, int], int] = None,
1098         **indexers_kwargs: Any,
1099     ) -> "DataArray":
1100         """Return a new DataArray whose data is given by the the last `n`
1101         values along the specified dimension(s). Default `n` = 5
1102 
1103         See Also
1104         --------
1105         Dataset.tail
1106         DataArray.head
1107         DataArray.thin
1108         """
1109         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1110         return self._from_temp_dataset(ds)
1111 
1112     def thin(
1113         self,
1114         indexers: Union[Mapping[Hashable, int], int] = None,
1115         **indexers_kwargs: Any,
1116     ) -> "DataArray":
1117         """Return a new DataArray whose data is given by each `n` value
1118         along the specified dimension(s).
1119 
1120         See Also
1121         --------
1122         Dataset.thin
1123         DataArray.head
1124         DataArray.tail
1125         """
1126         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1127         return self._from_temp_dataset(ds)
1128 
1129     def broadcast_like(
1130         self, other: Union["DataArray", Dataset], exclude: Iterable[Hashable] = None
1131     ) -> "DataArray":
1132         """Broadcast this DataArray against another Dataset or DataArray.
1133 
1134         This is equivalent to xr.broadcast(other, self)[1]
1135 
1136         xarray objects are broadcast against each other in arithmetic
1137         operations, so this method is not be necessary for most uses.
1138 
1139         If no change is needed, the input data is returned to the output
1140         without being copied.
1141 
1142         If new coords are added by the broadcast, their values are
1143         NaN filled.
1144 
1145         Parameters
1146         ----------
1147         other : Dataset or DataArray
1148             Object against which to broadcast this array.
1149         exclude : iterable of hashable, optional
1150             Dimensions that must not be broadcasted
1151 
1152         Returns
1153         -------
1154         new_da: xr.DataArray
1155 
1156         Examples
1157         --------
1158 
1159         >>> arr1
1160         <xarray.DataArray (x: 2, y: 3)>
1161         array([[0.840235, 0.215216, 0.77917 ],
1162                [0.726351, 0.543824, 0.875115]])
1163         Coordinates:
1164           * x        (x) <U1 'a' 'b'
1165           * y        (y) <U1 'a' 'b' 'c'
1166         >>> arr2
1167         <xarray.DataArray (x: 3, y: 2)>
1168         array([[0.612611, 0.125753],
1169                [0.853181, 0.948818],
1170                [0.180885, 0.33363 ]])
1171         Coordinates:
1172           * x        (x) <U1 'a' 'b' 'c'
1173           * y        (y) <U1 'a' 'b'
1174         >>> arr1.broadcast_like(arr2)
1175         <xarray.DataArray (x: 3, y: 3)>
1176         array([[0.840235, 0.215216, 0.77917 ],
1177                [0.726351, 0.543824, 0.875115],
1178                [     nan,      nan,      nan]])
1179         Coordinates:
1180           * x        (x) object 'a' 'b' 'c'
1181           * y        (y) object 'a' 'b' 'c'
1182         """
1183         if exclude is None:
1184             exclude = set()
1185         else:
1186             exclude = set(exclude)
1187         args = align(other, self, join="outer", copy=False, exclude=exclude)
1188 
1189         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1190 
1191         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
1192 
1193     def reindex_like(
1194         self,
1195         other: Union["DataArray", Dataset],
1196         method: str = None,
1197         tolerance=None,
1198         copy: bool = True,
1199         fill_value=dtypes.NA,
1200     ) -> "DataArray":
1201         """Conform this object onto the indexes of another object, filling in
1202         missing values with ``fill_value``. The default fill value is NaN.
1203 
1204         Parameters
1205         ----------
1206         other : Dataset or DataArray
1207             Object with an 'indexes' attribute giving a mapping from dimension
1208             names to pandas.Index objects, which provides coordinates upon
1209             which to index the variables in this dataset. The indexes on this
1210             other object need not be the same as the indexes on this
1211             dataset. Any mis-matched index values will be filled in with
1212             NaN, and any mis-matched dimension names will simply be ignored.
1213         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1214             Method to use for filling index values from other not found on this
1215             data array:
1216 
1217             * None (default): don't fill gaps
1218             * pad / ffill: propagate last valid index value forward
1219             * backfill / bfill: propagate next valid index value backward
1220             * nearest: use nearest valid index value
1221         tolerance : optional
1222             Maximum distance between original and new labels for inexact
1223             matches. The values of the index at the matching locations must
1224             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1225         copy : bool, optional
1226             If ``copy=True``, data in the return value is always copied. If
1227             ``copy=False`` and reindexing is unnecessary, or can be performed
1228             with only slice operations, then the output may share memory with
1229             the input. In either case, a new xarray object is always returned.
1230         fill_value : scalar, optional
1231             Value to use for newly missing values
1232 
1233         Returns
1234         -------
1235         reindexed : DataArray
1236             Another dataset array, with this array's data but coordinates from
1237             the other object.
1238 
1239         See Also
1240         --------
1241         DataArray.reindex
1242         align
1243         """
1244         indexers = reindex_like_indexers(self, other)
1245         return self.reindex(
1246             indexers=indexers,
1247             method=method,
1248             tolerance=tolerance,
1249             copy=copy,
1250             fill_value=fill_value,
1251         )
1252 
1253     def reindex(
1254         self,
1255         indexers: Mapping[Hashable, Any] = None,
1256         method: str = None,
1257         tolerance=None,
1258         copy: bool = True,
1259         fill_value=dtypes.NA,
1260         **indexers_kwargs: Any,
1261     ) -> "DataArray":
1262         """Conform this object onto the indexes of another object, filling in
1263         missing values with ``fill_value``. The default fill value is NaN.
1264 
1265         Parameters
1266         ----------
1267         indexers : dict, optional
1268             Dictionary with keys given by dimension names and values given by
1269             arrays of coordinates tick labels. Any mis-matched coordinate
1270             values will be filled in with NaN, and any mis-matched dimension
1271             names will simply be ignored.
1272             One of indexers or indexers_kwargs must be provided.
1273         copy : bool, optional
1274             If ``copy=True``, data in the return value is always copied. If
1275             ``copy=False`` and reindexing is unnecessary, or can be performed
1276             with only slice operations, then the output may share memory with
1277             the input. In either case, a new xarray object is always returned.
1278         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1279             Method to use for filling index values in ``indexers`` not found on
1280             this data array:
1281 
1282             * None (default): don't fill gaps
1283             * pad / ffill: propagate last valid index value forward
1284             * backfill / bfill: propagate next valid index value backward
1285             * nearest: use nearest valid index value
1286         tolerance : optional
1287             Maximum distance between original and new labels for inexact
1288             matches. The values of the index at the matching locations must
1289             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1290         fill_value : scalar, optional
1291             Value to use for newly missing values
1292         **indexers_kwargs : {dim: indexer, ...}, optional
1293             The keyword arguments form of ``indexers``.
1294             One of indexers or indexers_kwargs must be provided.
1295 
1296         Returns
1297         -------
1298         reindexed : DataArray
1299             Another dataset array, with this array's data but replaced
1300             coordinates.
1301 
1302         See Also
1303         --------
1304         DataArray.reindex_like
1305         align
1306         """
1307         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
1308         ds = self._to_temp_dataset().reindex(
1309             indexers=indexers,
1310             method=method,
1311             tolerance=tolerance,
1312             copy=copy,
1313             fill_value=fill_value,
1314         )
1315         return self._from_temp_dataset(ds)
1316 
1317     def interp(
1318         self,
1319         coords: Mapping[Hashable, Any] = None,
1320         method: str = "linear",
1321         assume_sorted: bool = False,
1322         kwargs: Mapping[str, Any] = None,
1323         **coords_kwargs: Any,
1324     ) -> "DataArray":
1325         """ Multidimensional interpolation of variables.
1326 
1327         coords : dict, optional
1328             Mapping from dimension names to the new coordinates.
1329             new coordinate can be an scalar, array-like or DataArray.
1330             If DataArrays are passed as new coordates, their dimensions are
1331             used for the broadcasting.
1332         method: {'linear', 'nearest'} for multidimensional array,
1333             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
1334             for 1-dimensional array.
1335         assume_sorted: boolean, optional
1336             If False, values of x can be in any order and they are sorted
1337             first. If True, x has to be an array of monotonically increasing
1338             values.
1339         kwargs: dictionary
1340             Additional keyword passed to scipy's interpolator.
1341         ``**coords_kwargs`` : {dim: coordinate, ...}, optional
1342             The keyword arguments form of ``coords``.
1343             One of coords or coords_kwargs must be provided.
1344 
1345         Returns
1346         -------
1347         interpolated: xr.DataArray
1348             New dataarray on the new coordinates.
1349 
1350         Notes
1351         -----
1352         scipy is required.
1353 
1354         See Also
1355         --------
1356         scipy.interpolate.interp1d
1357         scipy.interpolate.interpn
1358 
1359         Examples
1360         --------
1361         >>> da = xr.DataArray([1, 3], [('x', np.arange(2))])
1362         >>> da.interp(x=0.5)
1363         <xarray.DataArray ()>
1364         array(2.0)
1365         Coordinates:
1366             x        float64 0.5
1367         """
1368         if self.dtype.kind not in "uifc":
1369             raise TypeError(
1370                 "interp only works for a numeric type array. "
1371                 "Given {}.".format(self.dtype)
1372             )
1373         ds = self._to_temp_dataset().interp(
1374             coords,
1375             method=method,
1376             kwargs=kwargs,
1377             assume_sorted=assume_sorted,
1378             **coords_kwargs,
1379         )
1380         return self._from_temp_dataset(ds)
1381 
1382     def interp_like(
1383         self,
1384         other: Union["DataArray", Dataset],
1385         method: str = "linear",
1386         assume_sorted: bool = False,
1387         kwargs: Mapping[str, Any] = None,
1388     ) -> "DataArray":
1389         """Interpolate this object onto the coordinates of another object,
1390         filling out of range values with NaN.
1391 
1392         Parameters
1393         ----------
1394         other : Dataset or DataArray
1395             Object with an 'indexes' attribute giving a mapping from dimension
1396             names to an 1d array-like, which provides coordinates upon
1397             which to index the variables in this dataset.
1398         method: string, optional.
1399             {'linear', 'nearest'} for multidimensional array,
1400             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
1401             for 1-dimensional array. 'linear' is used by default.
1402         assume_sorted: boolean, optional
1403             If False, values of coordinates that are interpolated over can be
1404             in any order and they are sorted first. If True, interpolated
1405             coordinates are assumed to be an array of monotonically increasing
1406             values.
1407         kwargs: dictionary, optional
1408             Additional keyword passed to scipy's interpolator.
1409 
1410         Returns
1411         -------
1412         interpolated: xr.DataArray
1413             Another dataarray by interpolating this dataarray's data along the
1414             coordinates of the other object.
1415 
1416         Notes
1417         -----
1418         scipy is required.
1419         If the dataarray has object-type coordinates, reindex is used for these
1420         coordinates instead of the interpolation.
1421 
1422         See Also
1423         --------
1424         DataArray.interp
1425         DataArray.reindex_like
1426         """
1427         if self.dtype.kind not in "uifc":
1428             raise TypeError(
1429                 "interp only works for a numeric type array. "
1430                 "Given {}.".format(self.dtype)
1431             )
1432         ds = self._to_temp_dataset().interp_like(
1433             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
1434         )
1435         return self._from_temp_dataset(ds)
1436 
1437     def rename(
1438         self,
1439         new_name_or_name_dict: Union[Hashable, Mapping[Hashable, Hashable]] = None,
1440         **names: Hashable,
1441     ) -> "DataArray":
1442         """Returns a new DataArray with renamed coordinates or a new name.
1443 
1444         Parameters
1445         ----------
1446         new_name_or_name_dict : str or dict-like, optional
1447             If the argument is dict-like, it used as a mapping from old
1448             names to new names for coordinates. Otherwise, use the argument
1449             as the new name for this array.
1450         **names: hashable, optional
1451             The keyword arguments form of a mapping from old names to
1452             new names for coordinates.
1453             One of new_name_or_name_dict or names must be provided.
1454 
1455         Returns
1456         -------
1457         renamed : DataArray
1458             Renamed array or array with renamed coordinates.
1459 
1460         See Also
1461         --------
1462         Dataset.rename
1463         DataArray.swap_dims
1464         """
1465         if names or utils.is_dict_like(new_name_or_name_dict):
1466             new_name_or_name_dict = cast(
1467                 Mapping[Hashable, Hashable], new_name_or_name_dict
1468             )
1469             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
1470             dataset = self._to_temp_dataset().rename(name_dict)
1471             return self._from_temp_dataset(dataset)
1472         else:
1473             new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)
1474             return self._replace(name=new_name_or_name_dict)
1475 
1476     def swap_dims(self, dims_dict: Mapping[Hashable, Hashable]) -> "DataArray":
1477         """Returns a new DataArray with swapped dimensions.
1478 
1479         Parameters
1480         ----------
1481         dims_dict : dict-like
1482             Dictionary whose keys are current dimension names and whose values
1483             are new names. Each value must already be a coordinate on this
1484             array.
1485 
1486         Returns
1487         -------
1488         swapped : DataArray
1489             DataArray with swapped dimensions.
1490 
1491         Examples
1492         --------
1493         >>> arr = xr.DataArray(data=[0, 1], dims="x",
1494                                coords={"x": ["a", "b"], "y": ("x", [0, 1])})
1495         >>> arr
1496         <xarray.DataArray (x: 2)>
1497         array([0, 1])
1498         Coordinates:
1499           * x        (x) <U1 'a' 'b'
1500             y        (x) int64 0 1
1501         >>> arr.swap_dims({"x": "y"})
1502         <xarray.DataArray (y: 2)>
1503         array([0, 1])
1504         Coordinates:
1505             x        (y) <U1 'a' 'b'
1506           * y        (y) int64 0 1
1507 
1508         See Also
1509         --------
1510 
1511         DataArray.rename
1512         Dataset.swap_dims
1513         """
1514         ds = self._to_temp_dataset().swap_dims(dims_dict)
1515         return self._from_temp_dataset(ds)
1516 
1517     def expand_dims(
1518         self,
1519         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,
1520         axis=None,
1521         **dim_kwargs: Any,
1522     ) -> "DataArray":
1523         """Return a new object with an additional axis (or axes) inserted at
1524         the corresponding position in the array shape. The new object is a
1525         view into the underlying array, not a copy.
1526 
1527 
1528         If dim is already a scalar coordinate, it will be promoted to a 1D
1529         coordinate consisting of a single value.
1530 
1531         Parameters
1532         ----------
1533         dim : hashable, sequence of hashable, dict, or None
1534             Dimensions to include on the new variable.
1535             If provided as str or sequence of str, then dimensions are inserted
1536             with length 1. If provided as a dict, then the keys are the new
1537             dimensions and the values are either integers (giving the length of
1538             the new dimensions) or sequence/ndarray (giving the coordinates of
1539             the new dimensions).
1540         axis : integer, list (or tuple) of integers, or None
1541             Axis position(s) where new axis is to be inserted (position(s) on
1542             the result array). If a list (or tuple) of integers is passed,
1543             multiple axes are inserted. In this case, dim arguments should be
1544             same length list. If axis=None is passed, all the axes will be
1545             inserted to the start of the result array.
1546         **dim_kwargs : int or sequence/ndarray
1547             The keywords are arbitrary dimensions being inserted and the values
1548             are either the lengths of the new dims (if int is given), or their
1549             coordinates. Note, this is an alternative to passing a dict to the
1550             dim kwarg and will only be used if dim is None.
1551 
1552         Returns
1553         -------
1554         expanded : same type as caller
1555             This object, but with an additional dimension(s).
1556         """
1557         if isinstance(dim, int):
1558             raise TypeError("dim should be hashable or sequence/mapping of hashables")
1559         elif isinstance(dim, Sequence) and not isinstance(dim, str):
1560             if len(dim) != len(set(dim)):
1561                 raise ValueError("dims should not contain duplicate values.")
1562             dim = dict.fromkeys(dim, 1)
1563         elif dim is not None and not isinstance(dim, Mapping):
1564             dim = {cast(Hashable, dim): 1}
1565 
1566         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
1567         ds = self._to_temp_dataset().expand_dims(dim, axis)
1568         return self._from_temp_dataset(ds)
1569 
1570     def set_index(
1571         self,
1572         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
1573         append: bool = False,
1574         inplace: bool = None,
1575         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],
1576     ) -> Optional["DataArray"]:
1577         """Set DataArray (multi-)indexes using one or more existing
1578         coordinates.
1579 
1580         Parameters
1581         ----------
1582         indexes : {dim: index, ...}
1583             Mapping from names matching dimensions and values given
1584             by (lists of) the names of existing coordinates or variables to set
1585             as new (multi-)index.
1586         append : bool, optional
1587             If True, append the supplied index(es) to the existing index(es).
1588             Otherwise replace the existing index(es) (default).
1589         **indexes_kwargs: optional
1590             The keyword arguments form of ``indexes``.
1591             One of indexes or indexes_kwargs must be provided.
1592 
1593         Returns
1594         -------
1595         obj : DataArray
1596             Another DataArray, with this data but replaced coordinates.
1597 
1598         Examples
1599         --------
1600         >>> arr = xr.DataArray(data=np.ones((2, 3)),
1601         ...                    dims=['x', 'y'],
1602         ...                    coords={'x':
1603         ...                        range(2), 'y':
1604         ...                        range(3), 'a': ('x', [3, 4])
1605         ...                    })
1606         >>> arr
1607         <xarray.DataArray (x: 2, y: 3)>
1608         array([[1., 1., 1.],
1609                [1., 1., 1.]])
1610         Coordinates:
1611           * x        (x) int64 0 1
1612           * y        (y) int64 0 1 2
1613             a        (x) int64 3 4
1614         >>> arr.set_index(x='a')
1615         <xarray.DataArray (x: 2, y: 3)>
1616         array([[1., 1., 1.],
1617                [1., 1., 1.]])
1618         Coordinates:
1619           * x        (x) int64 3 4
1620           * y        (y) int64 0 1 2
1621 
1622         See Also
1623         --------
1624         DataArray.reset_index
1625         """
1626         ds = self._to_temp_dataset().set_index(
1627             indexes, append=append, inplace=inplace, **indexes_kwargs
1628         )
1629         return self._from_temp_dataset(ds)
1630 
1631     def reset_index(
1632         self,
1633         dims_or_levels: Union[Hashable, Sequence[Hashable]],
1634         drop: bool = False,
1635         inplace: bool = None,
1636     ) -> Optional["DataArray"]:
1637         """Reset the specified index(es) or multi-index level(s).
1638 
1639         Parameters
1640         ----------
1641         dims_or_levels : hashable or sequence of hashables
1642             Name(s) of the dimension(s) and/or multi-index level(s) that will
1643             be reset.
1644         drop : bool, optional
1645             If True, remove the specified indexes and/or multi-index levels
1646             instead of extracting them as new coordinates (default: False).
1647 
1648         Returns
1649         -------
1650         obj : DataArray
1651             Another dataarray, with this dataarray's data but replaced
1652             coordinates.
1653 
1654         See Also
1655         --------
1656         DataArray.set_index
1657         """
1658         _check_inplace(inplace)
1659         coords, _ = split_indexes(
1660             dims_or_levels, self._coords, set(), self._level_coords, drop=drop
1661         )
1662         return self._replace(coords=coords)
1663 
1664     def reorder_levels(
1665         self,
1666         dim_order: Mapping[Hashable, Sequence[int]] = None,
1667         inplace: bool = None,
1668         **dim_order_kwargs: Sequence[int],
1669     ) -> "DataArray":
1670         """Rearrange index levels using input order.
1671 
1672         Parameters
1673         ----------
1674         dim_order : optional
1675             Mapping from names matching dimensions and values given
1676             by lists representing new level orders. Every given dimension
1677             must have a multi-index.
1678         **dim_order_kwargs: optional
1679             The keyword arguments form of ``dim_order``.
1680             One of dim_order or dim_order_kwargs must be provided.
1681 
1682         Returns
1683         -------
1684         obj : DataArray
1685             Another dataarray, with this dataarray's data but replaced
1686             coordinates.
1687         """
1688         _check_inplace(inplace)
1689         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
1690         replace_coords = {}
1691         for dim, order in dim_order.items():
1692             coord = self._coords[dim]
1693             index = coord.to_index()
1694             if not isinstance(index, pd.MultiIndex):
1695                 raise ValueError("coordinate %r has no MultiIndex" % dim)
1696             replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))
1697         coords = self._coords.copy()
1698         coords.update(replace_coords)
1699         return self._replace(coords=coords)
1700 
1701     def stack(
1702         self,
1703         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
1704         **dimensions_kwargs: Sequence[Hashable],
1705     ) -> "DataArray":
1706         """
1707         Stack any number of existing dimensions into a single new dimension.
1708 
1709         New dimensions will be added at the end, and the corresponding
1710         coordinate variables will be combined into a MultiIndex.
1711 
1712         Parameters
1713         ----------
1714         dimensions : Mapping of the form new_name=(dim1, dim2, ...)
1715             Names of new dimensions, and the existing dimensions that they
1716             replace.
1717         **dimensions_kwargs:
1718             The keyword arguments form of ``dimensions``.
1719             One of dimensions or dimensions_kwargs must be provided.
1720 
1721         Returns
1722         -------
1723         stacked : DataArray
1724             DataArray with stacked data.
1725 
1726         Examples
1727         --------
1728 
1729         >>> arr = DataArray(np.arange(6).reshape(2, 3),
1730         ...                 coords=[('x', ['a', 'b']), ('y', [0, 1, 2])])
1731         >>> arr
1732         <xarray.DataArray (x: 2, y: 3)>
1733         array([[0, 1, 2],
1734                [3, 4, 5]])
1735         Coordinates:
1736           * x        (x) |S1 'a' 'b'
1737           * y        (y) int64 0 1 2
1738         >>> stacked = arr.stack(z=('x', 'y'))
1739         >>> stacked.indexes['z']
1740         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1741                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],
1742                    names=['x', 'y'])
1743 
1744         See Also
1745         --------
1746         DataArray.unstack
1747         """
1748         ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)
1749         return self._from_temp_dataset(ds)
1750 
1751     def unstack(
1752         self,
1753         dim: Union[Hashable, Sequence[Hashable], None] = None,
1754         fill_value: Any = dtypes.NA,
1755         sparse: bool = False,
1756     ) -> "DataArray":
1757         """
1758         Unstack existing dimensions corresponding to MultiIndexes into
1759         multiple new dimensions.
1760 
1761         New dimensions will be added at the end.
1762 
1763         Parameters
1764         ----------
1765         dim : hashable or sequence of hashable, optional
1766             Dimension(s) over which to unstack. By default unstacks all
1767             MultiIndexes.
1768         fill_value: value to be filled. By default, np.nan
1769         sparse: use sparse-array if True
1770 
1771         Returns
1772         -------
1773         unstacked : DataArray
1774             Array with unstacked data.
1775 
1776         Examples
1777         --------
1778 
1779         >>> arr = DataArray(np.arange(6).reshape(2, 3),
1780         ...                 coords=[('x', ['a', 'b']), ('y', [0, 1, 2])])
1781         >>> arr
1782         <xarray.DataArray (x: 2, y: 3)>
1783         array([[0, 1, 2],
1784                [3, 4, 5]])
1785         Coordinates:
1786           * x        (x) |S1 'a' 'b'
1787           * y        (y) int64 0 1 2
1788         >>> stacked = arr.stack(z=('x', 'y'))
1789         >>> stacked.indexes['z']
1790         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1791                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],
1792                    names=['x', 'y'])
1793         >>> roundtripped = stacked.unstack()
1794         >>> arr.identical(roundtripped)
1795         True
1796 
1797         See Also
1798         --------
1799         DataArray.stack
1800         """
1801         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
1802         return self._from_temp_dataset(ds)
1803 
1804     def to_unstacked_dataset(self, dim, level=0):
1805         """Unstack DataArray expanding to Dataset along a given level of a
1806         stacked coordinate.
1807 
1808         This is the inverse operation of Dataset.to_stacked_array.
1809 
1810         Parameters
1811         ----------
1812         dim : str
1813             Name of existing dimension to unstack
1814         level : int or str
1815             The MultiIndex level to expand to a dataset along. Can either be
1816             the integer index of the level or its name.
1817         label : int, default 0
1818             Label of the level to expand dataset along. Overrides the label
1819             argument if given.
1820 
1821         Returns
1822         -------
1823         unstacked: Dataset
1824 
1825         Examples
1826         --------
1827         >>> import xarray as xr
1828         >>> arr = DataArray(np.arange(6).reshape(2, 3),
1829         ...                 coords=[('x', ['a', 'b']), ('y', [0, 1, 2])])
1830         >>> data = xr.Dataset({'a': arr, 'b': arr.isel(y=0)})
1831         >>> data
1832         <xarray.Dataset>
1833         Dimensions:  (x: 2, y: 3)
1834         Coordinates:
1835           * x        (x) <U1 'a' 'b'
1836           * y        (y) int64 0 1 2
1837         Data variables:
1838             a        (x, y) int64 0 1 2 3 4 5
1839             b        (x) int64 0 3
1840         >>> stacked = data.to_stacked_array("z", ['y'])
1841         >>> stacked.indexes['z']
1842         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1843                 labels=[[0, 0, 0, 1], [0, 1, 2, -1]],
1844                 names=['variable', 'y'])
1845         >>> roundtripped = stacked.to_unstacked_dataset(dim='z')
1846         >>> data.identical(roundtripped)
1847         True
1848 
1849         See Also
1850         --------
1851         Dataset.to_stacked_array
1852         """
1853 
1854         idx = self.indexes[dim]
1855         if not isinstance(idx, pd.MultiIndex):
1856             raise ValueError(f"'{dim}' is not a stacked coordinate")
1857 
1858         level_number = idx._get_level_number(level)
1859         variables = idx.levels[level_number]
1860         variable_dim = idx.names[level_number]
1861 
1862         # pull variables out of datarray
1863         data_dict = {}
1864         for k in variables:
1865             data_dict[k] = self.sel({variable_dim: k}).squeeze(drop=True)
1866 
1867         # unstacked dataset
1868         return Dataset(data_dict)
1869 
1870     def transpose(self, *dims: Hashable, transpose_coords: bool = None) -> "DataArray":
1871         """Return a new DataArray object with transposed dimensions.
1872 
1873         Parameters
1874         ----------
1875         *dims : hashable, optional
1876             By default, reverse the dimensions. Otherwise, reorder the
1877             dimensions to this order.
1878         transpose_coords : boolean, optional
1879             If True, also transpose the coordinates of this DataArray.
1880 
1881         Returns
1882         -------
1883         transposed : DataArray
1884             The returned DataArray's array is transposed.
1885 
1886         Notes
1887         -----
1888         This operation returns a view of this array's data. It is
1889         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
1890         -- the data will be fully loaded.
1891 
1892         See Also
1893         --------
1894         numpy.transpose
1895         Dataset.transpose
1896         """
1897         if dims:
1898             dims = tuple(utils.infix_dims(dims, self.dims))
1899         variable = self.variable.transpose(*dims)
1900         if transpose_coords:
1901             coords: Dict[Hashable, Variable] = {}
1902             for name, coord in self.coords.items():
1903                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
1904                 coords[name] = coord.variable.transpose(*coord_dims)
1905             return self._replace(variable, coords)
1906         else:
1907             if transpose_coords is None and any(self[c].ndim > 1 for c in self.coords):
1908                 warnings.warn(
1909                     "This DataArray contains multi-dimensional "
1910                     "coordinates. In the future, these coordinates "
1911                     "will be transposed as well unless you specify "
1912                     "transpose_coords=False.",
1913                     FutureWarning,
1914                     stacklevel=2,
1915                 )
1916             return self._replace(variable)
1917 
1918     @property
1919     def T(self) -> "DataArray":
1920         return self.transpose()
1921 
1922     def drop_vars(
1923         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
1924     ) -> "DataArray":
1925         """Drop variables from this DataArray.
1926 
1927         Parameters
1928         ----------
1929         names : hashable or iterable of hashables
1930             Name(s) of variables to drop.
1931         errors: {'raise', 'ignore'}, optional
1932             If 'raise' (default), raises a ValueError error if any of the variable
1933             passed are not in the dataset. If 'ignore', any given names that are in the
1934             DataArray are dropped and no error is raised.
1935 
1936         Returns
1937         -------
1938         dropped : Dataset
1939 
1940         """
1941         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
1942         return self._from_temp_dataset(ds)
1943 
1944     def drop(
1945         self,
1946         labels: Mapping = None,
1947         dim: Hashable = None,
1948         *,
1949         errors: str = "raise",
1950         **labels_kwargs,
1951     ) -> "DataArray":
1952         """Backward compatible method based on `drop_vars` and `drop_sel`
1953 
1954         Using either `drop_vars` or `drop_sel` is encouraged
1955 
1956         See Also
1957         --------
1958         DataArray.drop_vars
1959         DataArray.drop_sel
1960         """
1961         ds = self._to_temp_dataset().drop(labels, dim, errors=errors)
1962         return self._from_temp_dataset(ds)
1963 
1964     def drop_sel(
1965         self,
1966         labels: Mapping[Hashable, Any] = None,
1967         *,
1968         errors: str = "raise",
1969         **labels_kwargs,
1970     ) -> "DataArray":
1971         """Drop index labels from this DataArray.
1972 
1973         Parameters
1974         ----------
1975         labels : Mapping[Hashable, Any]
1976             Index labels to drop
1977         errors: {'raise', 'ignore'}, optional
1978             If 'raise' (default), raises a ValueError error if
1979             any of the index labels passed are not
1980             in the dataset. If 'ignore', any given labels that are in the
1981             dataset are dropped and no error is raised.
1982         **labels_kwargs : {dim: label, ...}, optional
1983             The keyword arguments form of ``dim`` and ``labels``
1984 
1985         Returns
1986         -------
1987         dropped : DataArray
1988         """
1989         if labels_kwargs or isinstance(labels, dict):
1990             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
1991 
1992         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
1993         return self._from_temp_dataset(ds)
1994 
1995     def dropna(
1996         self, dim: Hashable, how: str = "any", thresh: int = None
1997     ) -> "DataArray":
1998         """Returns a new array with dropped labels for missing values along
1999         the provided dimension.
2000 
2001         Parameters
2002         ----------
2003         dim : hashable
2004             Dimension along which to drop missing values. Dropping along
2005             multiple dimensions simultaneously is not yet supported.
2006         how : {'any', 'all'}, optional
2007             * any : if any NA values are present, drop that label
2008             * all : if all values are NA, drop that label
2009         thresh : int, default None
2010             If supplied, require this many non-NA values.
2011 
2012         Returns
2013         -------
2014         DataArray
2015         """
2016         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
2017         return self._from_temp_dataset(ds)
2018 
2019     def fillna(self, value: Any) -> "DataArray":
2020         """Fill missing values in this object.
2021 
2022         This operation follows the normal broadcasting and alignment rules that
2023         xarray uses for binary arithmetic, except the result is aligned to this
2024         object (``join='left'``) instead of aligned to the intersection of
2025         index coordinates (``join='inner'``).
2026 
2027         Parameters
2028         ----------
2029         value : scalar, ndarray or DataArray
2030             Used to fill all matching missing values in this array. If the
2031             argument is a DataArray, it is first aligned with (reindexed to)
2032             this array.
2033 
2034         Returns
2035         -------
2036         DataArray
2037         """
2038         if utils.is_dict_like(value):
2039             raise TypeError(
2040                 "cannot provide fill value as a dictionary with "
2041                 "fillna on a DataArray"
2042             )
2043         out = ops.fillna(self, value)
2044         return out
2045 
2046     def interpolate_na(
2047         self,
2048         dim: Hashable = None,
2049         method: str = "linear",
2050         limit: int = None,
2051         use_coordinate: Union[bool, str] = True,
2052         max_gap: Union[int, float, str, pd.Timedelta, np.timedelta64] = None,
2053         **kwargs: Any,
2054     ) -> "DataArray":
2055         """Fill in NaNs by interpolating according to different methods.
2056 
2057         Parameters
2058         ----------
2059         dim : str
2060             Specifies the dimension along which to interpolate.
2061         method : str, optional
2062             String indicating which method to use for interpolation:
2063 
2064             - 'linear': linear interpolation (Default). Additional keyword
2065               arguments are passed to :py:func:`numpy.interp`
2066             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
2067               are passed to :py:func:`scipy.interpolate.interp1d`. If
2068               ``method='polynomial'``, the ``order`` keyword argument must also be
2069               provided.
2070             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
2071               respective :py:class:`scipy.interpolate` classes.
2072 
2073         use_coordinate : bool, str, default True
2074             Specifies which index to use as the x values in the interpolation
2075             formulated as `y = f(x)`. If False, values are treated as if
2076             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
2077             used. If ``use_coordinate`` is a string, it specifies the name of a
2078             coordinate variariable to use as the index.
2079         limit : int, default None
2080             Maximum number of consecutive NaNs to fill. Must be greater than 0
2081             or None for no limit. This filling is done regardless of the size of
2082             the gap in the data. To only interpolate over gaps less than a given length,
2083             see ``max_gap``.
2084         max_gap: int, float, str, pandas.Timedelta, numpy.timedelta64, default None.
2085             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
2086             Use None for no limit. When interpolating along a datetime64 dimension
2087             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
2088 
2089             - a string that is valid input for pandas.to_timedelta
2090             - a :py:class:`numpy.timedelta64` object
2091             - a :py:class:`pandas.Timedelta` object
2092 
2093             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
2094             dimensions has not been implemented yet. Gap length is defined as the difference
2095             between coordinate values at the first data point after a gap and the last value
2096             before a gap. For gaps at the beginning (end), gap length is defined as the difference
2097             between coordinate values at the first (last) valid data point and the first (last) NaN.
2098             For example, consider::
2099 
2100                 <xarray.DataArray (x: 9)>
2101                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
2102                 Coordinates:
2103                   * x        (x) int64 0 1 2 3 4 5 6 7 8
2104 
2105             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
2106         kwargs : dict, optional
2107             parameters passed verbatim to the underlying interpolation function
2108 
2109         Returns
2110         -------
2111         interpolated: DataArray
2112             Filled in DataArray.
2113 
2114         See also
2115         --------
2116         numpy.interp
2117         scipy.interpolate
2118         """
2119         from .missing import interp_na
2120 
2121         return interp_na(
2122             self,
2123             dim=dim,
2124             method=method,
2125             limit=limit,
2126             use_coordinate=use_coordinate,
2127             max_gap=max_gap,
2128             **kwargs,
2129         )
2130 
2131     def ffill(self, dim: Hashable, limit: int = None) -> "DataArray":
2132         """Fill NaN values by propogating values forward
2133 
2134         *Requires bottleneck.*
2135 
2136         Parameters
2137         ----------
2138         dim : hashable
2139             Specifies the dimension along which to propagate values when
2140             filling.
2141         limit : int, default None
2142             The maximum number of consecutive NaN values to forward fill. In
2143             other words, if there is a gap with more than this number of
2144             consecutive NaNs, it will only be partially filled. Must be greater
2145             than 0 or None for no limit.
2146 
2147         Returns
2148         -------
2149         DataArray
2150         """
2151         from .missing import ffill
2152 
2153         return ffill(self, dim, limit=limit)
2154 
2155     def bfill(self, dim: Hashable, limit: int = None) -> "DataArray":
2156         """Fill NaN values by propogating values backward
2157 
2158         *Requires bottleneck.*
2159 
2160         Parameters
2161         ----------
2162         dim : str
2163             Specifies the dimension along which to propagate values when
2164             filling.
2165         limit : int, default None
2166             The maximum number of consecutive NaN values to backward fill. In
2167             other words, if there is a gap with more than this number of
2168             consecutive NaNs, it will only be partially filled. Must be greater
2169             than 0 or None for no limit.
2170 
2171         Returns
2172         -------
2173         DataArray
2174         """
2175         from .missing import bfill
2176 
2177         return bfill(self, dim, limit=limit)
2178 
2179     def combine_first(self, other: "DataArray") -> "DataArray":
2180         """Combine two DataArray objects, with union of coordinates.
2181 
2182         This operation follows the normal broadcasting and alignment rules of
2183         ``join='outer'``.  Default to non-null values of array calling the
2184         method.  Use np.nan to fill in vacant cells after alignment.
2185 
2186         Parameters
2187         ----------
2188         other : DataArray
2189             Used to fill all matching missing values in this array.
2190 
2191         Returns
2192         -------
2193         DataArray
2194         """
2195         return ops.fillna(self, other, join="outer")
2196 
2197     def reduce(
2198         self,
2199         func: Callable[..., Any],
2200         dim: Union[None, Hashable, Sequence[Hashable]] = None,
2201         axis: Union[None, int, Sequence[int]] = None,
2202         keep_attrs: bool = None,
2203         keepdims: bool = False,
2204         **kwargs: Any,
2205     ) -> "DataArray":
2206         """Reduce this array by applying `func` along some dimension(s).
2207 
2208         Parameters
2209         ----------
2210         func : function
2211             Function which can be called in the form
2212             `f(x, axis=axis, **kwargs)` to return the result of reducing an
2213             np.ndarray over an integer valued axis.
2214         dim : hashable or sequence of hashables, optional
2215             Dimension(s) over which to apply `func`.
2216         axis : int or sequence of int, optional
2217             Axis(es) over which to repeatedly apply `func`. Only one of the
2218             'dim' and 'axis' arguments can be supplied. If neither are
2219             supplied, then the reduction is calculated over the flattened array
2220             (by calling `f(x)` without an axis argument).
2221         keep_attrs : bool, optional
2222             If True, the variable's attributes (`attrs`) will be copied from
2223             the original object to the new one.  If False (default), the new
2224             object will be returned without attributes.
2225         keepdims : bool, default False
2226             If True, the dimensions which are reduced are left in the result
2227             as dimensions of size one. Coordinates that use these dimensions
2228             are removed.
2229         **kwargs : dict
2230             Additional keyword arguments passed on to `func`.
2231 
2232         Returns
2233         -------
2234         reduced : DataArray
2235             DataArray with this object's array replaced with an array with
2236             summarized data and the indicated dimension(s) removed.
2237         """
2238 
2239         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
2240         return self._replace_maybe_drop_dims(var)
2241 
2242     def to_pandas(self) -> Union["DataArray", pd.Series, pd.DataFrame]:
2243         """Convert this array into a pandas object with the same shape.
2244 
2245         The type of the returned object depends on the number of DataArray
2246         dimensions:
2247 
2248         * 0D -> `xarray.DataArray`
2249         * 1D -> `pandas.Series`
2250         * 2D -> `pandas.DataFrame`
2251         * 3D -> `pandas.Panel` *(deprecated)*
2252 
2253         Only works for arrays with 3 or fewer dimensions.
2254 
2255         The DataArray constructor performs the inverse transformation.
2256         """
2257         # TODO: consolidate the info about pandas constructors and the
2258         # attributes that correspond to their indexes into a separate module?
2259         constructors = {
2260             0: lambda x: x,
2261             1: pd.Series,
2262             2: pd.DataFrame,
2263             3: pdcompat.Panel,
2264         }
2265         try:
2266             constructor = constructors[self.ndim]
2267         except KeyError:
2268             raise ValueError(
2269                 "cannot convert arrays with %s dimensions into "
2270                 "pandas objects" % self.ndim
2271             )
2272         indexes = [self.get_index(dim) for dim in self.dims]
2273         return constructor(self.values, *indexes)
2274 
2275     def to_dataframe(self, name: Hashable = None) -> pd.DataFrame:
2276         """Convert this array and its coordinates into a tidy pandas.DataFrame.
2277 
2278         The DataFrame is indexed by the Cartesian product of index coordinates
2279         (in the form of a :py:class:`pandas.MultiIndex`).
2280 
2281         Other coordinates are included as columns in the DataFrame.
2282         """
2283         if name is None:
2284             name = self.name
2285         if name is None:
2286             raise ValueError(
2287                 "cannot convert an unnamed DataArray to a "
2288                 "DataFrame: use the ``name`` parameter"
2289             )
2290 
2291         dims = dict(zip(self.dims, self.shape))
2292         # By using a unique name, we can convert a DataArray into a DataFrame
2293         # even if it shares a name with one of its coordinates.
2294         # I would normally use unique_name = object() but that results in a
2295         # dataframe with columns in the wrong order, for reasons I have not
2296         # been able to debug (possibly a pandas bug?).
2297         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
2298         ds = self._to_dataset_whole(name=unique_name)
2299         df = ds._to_dataframe(dims)
2300         df.columns = [name if c == unique_name else c for c in df.columns]
2301         return df
2302 
2303     def to_series(self) -> pd.Series:
2304         """Convert this array into a pandas.Series.
2305 
2306         The Series is indexed by the Cartesian product of index coordinates
2307         (in the form of a :py:class:`pandas.MultiIndex`).
2308         """
2309         index = self.coords.to_index()
2310         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
2311 
2312     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
2313         """Convert this array into a numpy.ma.MaskedArray
2314 
2315         Parameters
2316         ----------
2317         copy : bool
2318             If True (default) make a copy of the array in the result. If False,
2319             a MaskedArray view of DataArray.values is returned.
2320 
2321         Returns
2322         -------
2323         result : MaskedArray
2324             Masked where invalid values (nan or inf) occur.
2325         """
2326         values = self.values  # only compute lazy arrays once
2327         isnull = pd.isnull(values)
2328         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
2329 
2330     def to_netcdf(self, *args, **kwargs) -> Union[bytes, "Delayed", None]:
2331         """Write DataArray contents to a netCDF file.
2332 
2333         All parameters are passed directly to `xarray.Dataset.to_netcdf`.
2334 
2335         Notes
2336         -----
2337         Only xarray.Dataset objects can be written to netCDF files, so
2338         the xarray.DataArray is converted to a xarray.Dataset object
2339         containing a single variable. If the DataArray has no name, or if the
2340         name is the same as a co-ordinate name, then it is given the name
2341         '__xarray_dataarray_variable__'.
2342         """
2343         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE
2344 
2345         if self.name is None:
2346             # If no name is set then use a generic xarray name
2347             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2348         elif self.name in self.coords or self.name in self.dims:
2349             # The name is the same as one of the coords names, which netCDF
2350             # doesn't support, so rename it but keep track of the old name
2351             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2352             dataset.attrs[DATAARRAY_NAME] = self.name
2353         else:
2354             # No problems with the name - so we're fine!
2355             dataset = self.to_dataset()
2356 
2357         return dataset.to_netcdf(*args, **kwargs)
2358 
2359     def to_dict(self, data: bool = True) -> dict:
2360         """
2361         Convert this xarray.DataArray into a dictionary following xarray
2362         naming conventions.
2363 
2364         Converts all variables and attributes to native Python objects.
2365         Useful for coverting to json. To avoid datetime incompatibility
2366         use decode_times=False kwarg in xarrray.open_dataset.
2367 
2368         Parameters
2369         ----------
2370         data : bool, optional
2371             Whether to include the actual data in the dictionary. When set to
2372             False, returns just the schema.
2373 
2374         See also
2375         --------
2376         DataArray.from_dict
2377         """
2378         d = self.variable.to_dict(data=data)
2379         d.update({"coords": {}, "name": self.name})
2380         for k in self.coords:
2381             d["coords"][k] = self.coords[k].variable.to_dict(data=data)
2382         return d
2383 
2384     @classmethod
2385     def from_dict(cls, d: dict) -> "DataArray":
2386         """
2387         Convert a dictionary into an xarray.DataArray
2388 
2389         Input dict can take several forms::
2390 
2391             d = {'dims': ('t'), 'data': x}
2392 
2393             d = {'coords': {'t': {'dims': 't', 'data': t,
2394                                   'attrs': {'units':'s'}}},
2395                  'attrs': {'title': 'air temperature'},
2396                  'dims': 't',
2397                  'data': x,
2398                  'name': 'a'}
2399 
2400         where 't' is the name of the dimesion, 'a' is the name of the array,
2401         and  x and t are lists, numpy.arrays, or pandas objects.
2402 
2403         Parameters
2404         ----------
2405         d : dict, with a minimum structure of {'dims': [..], 'data': [..]}
2406 
2407         Returns
2408         -------
2409         obj : xarray.DataArray
2410 
2411         See also
2412         --------
2413         DataArray.to_dict
2414         Dataset.from_dict
2415         """
2416         coords = None
2417         if "coords" in d:
2418             try:
2419                 coords = {
2420                     k: (v["dims"], v["data"], v.get("attrs"))
2421                     for k, v in d["coords"].items()
2422                 }
2423             except KeyError as e:
2424                 raise ValueError(
2425                     "cannot convert dict when coords are missing the key "
2426                     "'{dims_data}'".format(dims_data=str(e.args[0]))
2427                 )
2428         try:
2429             data = d["data"]
2430         except KeyError:
2431             raise ValueError("cannot convert dict without the key 'data''")
2432         else:
2433             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
2434         return obj
2435 
2436     @classmethod
2437     def from_series(cls, series: pd.Series, sparse: bool = False) -> "DataArray":
2438         """Convert a pandas.Series into an xarray.DataArray.
2439 
2440         If the series's index is a MultiIndex, it will be expanded into a
2441         tensor product of one-dimensional coordinates (filling in missing
2442         values with NaN). Thus this operation should be the inverse of the
2443         `to_series` method.
2444 
2445         If sparse=True, creates a sparse array instead of a dense NumPy array.
2446         Requires the pydata/sparse package.
2447 
2448         See also
2449         --------
2450         xarray.Dataset.from_dataframe
2451         """
2452         temp_name = "__temporary_name"
2453         df = pd.DataFrame({temp_name: series})
2454         ds = Dataset.from_dataframe(df, sparse=sparse)
2455         result = cast(DataArray, ds[temp_name])
2456         result.name = series.name
2457         return result
2458 
2459     def to_cdms2(self) -> "cdms2_Variable":
2460         """Convert this array into a cdms2.Variable
2461         """
2462         from ..convert import to_cdms2
2463 
2464         return to_cdms2(self)
2465 
2466     @classmethod
2467     def from_cdms2(cls, variable: "cdms2_Variable") -> "DataArray":
2468         """Convert a cdms2.Variable into an xarray.DataArray
2469         """
2470         from ..convert import from_cdms2
2471 
2472         return from_cdms2(variable)
2473 
2474     def to_iris(self) -> "iris_Cube":
2475         """Convert this array into a iris.cube.Cube
2476         """
2477         from ..convert import to_iris
2478 
2479         return to_iris(self)
2480 
2481     @classmethod
2482     def from_iris(cls, cube: "iris_Cube") -> "DataArray":
2483         """Convert a iris.cube.Cube into an xarray.DataArray
2484         """
2485         from ..convert import from_iris
2486 
2487         return from_iris(cube)
2488 
2489     def _all_compat(self, other: "DataArray", compat_str: str) -> bool:
2490         """Helper function for equals, broadcast_equals, and identical
2491         """
2492 
2493         def compat(x, y):
2494             return getattr(x.variable, compat_str)(y.variable)
2495 
2496         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
2497             self, other
2498         )
2499 
2500     def broadcast_equals(self, other: "DataArray") -> bool:
2501         """Two DataArrays are broadcast equal if they are equal after
2502         broadcasting them against each other such that they have the same
2503         dimensions.
2504 
2505         See Also
2506         --------
2507         DataArray.equals
2508         DataArray.identical
2509         """
2510         try:
2511             return self._all_compat(other, "broadcast_equals")
2512         except (TypeError, AttributeError):
2513             return False
2514 
2515     def equals(self, other: "DataArray") -> bool:
2516         """True if two DataArrays have the same dimensions, coordinates and
2517         values; otherwise False.
2518 
2519         DataArrays can still be equal (like pandas objects) if they have NaN
2520         values in the same locations.
2521 
2522         This method is necessary because `v1 == v2` for ``DataArray``
2523         does element-wise comparisons (like numpy.ndarrays).
2524 
2525         See Also
2526         --------
2527         DataArray.broadcast_equals
2528         DataArray.identical
2529         """
2530         try:
2531             return self._all_compat(other, "equals")
2532         except (TypeError, AttributeError):
2533             return False
2534 
2535     def identical(self, other: "DataArray") -> bool:
2536         """Like equals, but also checks the array name and attributes, and
2537         attributes on all coordinates.
2538 
2539         See Also
2540         --------
2541         DataArray.broadcast_equals
2542         DataArray.equal
2543         """
2544         try:
2545             return self.name == other.name and self._all_compat(other, "identical")
2546         except (TypeError, AttributeError):
2547             return False
2548 
2549     def _result_name(self, other: Any = None) -> Optional[Hashable]:
2550         # use the same naming heuristics as pandas:
2551         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
2552         other_name = getattr(other, "name", _default)
2553         if other_name is _default or other_name == self.name:
2554             return self.name
2555         else:
2556             return None
2557 
2558     def __array_wrap__(self, obj, context=None) -> "DataArray":
2559         new_var = self.variable.__array_wrap__(obj, context)
2560         return self._replace(new_var)
2561 
2562     def __matmul__(self, obj):
2563         return self.dot(obj)
2564 
2565     def __rmatmul__(self, other):
2566         # currently somewhat duplicative, as only other DataArrays are
2567         # compatible with matmul
2568         return computation.dot(other, self)
2569 
2570     @staticmethod
2571     def _unary_op(f: Callable[..., Any]) -> Callable[..., "DataArray"]:
2572         @functools.wraps(f)
2573         def func(self, *args, **kwargs):
2574             with np.errstate(all="ignore"):
2575                 return self.__array_wrap__(f(self.variable.data, *args, **kwargs))
2576 
2577         return func
2578 
2579     @staticmethod
2580     def _binary_op(
2581         f: Callable[..., Any],
2582         reflexive: bool = False,
2583         join: str = None,  # see xarray.align
2584         **ignored_kwargs,
2585     ) -> Callable[..., "DataArray"]:
2586         @functools.wraps(f)
2587         def func(self, other):
2588             if isinstance(other, (Dataset, groupby.GroupBy)):
2589                 return NotImplemented
2590             if isinstance(other, DataArray):
2591                 align_type = OPTIONS["arithmetic_join"] if join is None else join
2592                 self, other = align(self, other, join=align_type, copy=False)
2593             other_variable = getattr(other, "variable", other)
2594             other_coords = getattr(other, "coords", None)
2595 
2596             variable = (
2597                 f(self.variable, other_variable)
2598                 if not reflexive
2599                 else f(other_variable, self.variable)
2600             )
2601             coords, indexes = self.coords._merge_raw(other_coords)
2602             name = self._result_name(other)
2603 
2604             return self._replace(variable, coords, name, indexes=indexes)
2605 
2606         return func
2607 
2608     @staticmethod
2609     def _inplace_binary_op(f: Callable) -> Callable[..., "DataArray"]:
2610         @functools.wraps(f)
2611         def func(self, other):
2612             if isinstance(other, groupby.GroupBy):
2613                 raise TypeError(
2614                     "in-place operations between a DataArray and "
2615                     "a grouped object are not permitted"
2616                 )
2617             # n.b. we can't align other to self (with other.reindex_like(self))
2618             # because `other` may be converted into floats, which would cause
2619             # in-place arithmetic to fail unpredictably. Instead, we simply
2620             # don't support automatic alignment with in-place arithmetic.
2621             other_coords = getattr(other, "coords", None)
2622             other_variable = getattr(other, "variable", other)
2623             with self.coords._merge_inplace(other_coords):
2624                 f(self.variable, other_variable)
2625             return self
2626 
2627         return func
2628 
2629     def _copy_attrs_from(self, other: Union["DataArray", Dataset, Variable]) -> None:
2630         self.attrs = other.attrs
2631 
2632     @property
2633     def plot(self) -> _PlotMethods:
2634         """
2635         Access plotting functions for DataArray's
2636 
2637         >>> d = DataArray([[1, 2], [3, 4]])
2638 
2639         For convenience just call this directly
2640 
2641         >>> d.plot()
2642 
2643         Or use it as a namespace to use xarray.plot functions as
2644         DataArray methods
2645 
2646         >>> d.plot.imshow()  # equivalent to xarray.plot.imshow(d)
2647 
2648         """
2649         return _PlotMethods(self)
2650 
2651     def _title_for_slice(self, truncate: int = 50) -> str:
2652         """
2653         If the dataarray has 1 dimensional coordinates or comes from a slice
2654         we can show that info in the title
2655 
2656         Parameters
2657         ----------
2658         truncate : integer
2659             maximum number of characters for title
2660 
2661         Returns
2662         -------
2663         title : string
2664             Can be used for plot titles
2665 
2666         """
2667         one_dims = []
2668         for dim, coord in self.coords.items():
2669             if coord.size == 1:
2670                 one_dims.append(
2671                     "{dim} = {v}".format(dim=dim, v=format_item(coord.values))
2672                 )
2673 
2674         title = ", ".join(one_dims)
2675         if len(title) > truncate:
2676             title = title[: (truncate - 3)] + "..."
2677 
2678         return title
2679 
2680     def diff(self, dim: Hashable, n: int = 1, label: Hashable = "upper") -> "DataArray":
2681         """Calculate the n-th order discrete difference along given axis.
2682 
2683         Parameters
2684         ----------
2685         dim : hashable, optional
2686             Dimension over which to calculate the finite difference.
2687         n : int, optional
2688             The number of times values are differenced.
2689         label : hashable, optional
2690             The new coordinate in dimension ``dim`` will have the
2691             values of either the minuend's or subtrahend's coordinate
2692             for values 'upper' and 'lower', respectively.  Other
2693             values are not supported.
2694 
2695         Returns
2696         -------
2697         difference : same type as caller
2698             The n-th order finite difference of this object.
2699 
2700         Examples
2701         --------
2702         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ['x'])
2703         >>> arr.diff('x')
2704         <xarray.DataArray (x: 3)>
2705         array([0, 1, 0])
2706         Coordinates:
2707         * x        (x) int64 2 3 4
2708         >>> arr.diff('x', 2)
2709         <xarray.DataArray (x: 2)>
2710         array([ 1, -1])
2711         Coordinates:
2712         * x        (x) int64 3 4
2713 
2714         See Also
2715         --------
2716         DataArray.differentiate
2717         """
2718         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
2719         return self._from_temp_dataset(ds)
2720 
2721     def shift(
2722         self,
2723         shifts: Mapping[Hashable, int] = None,
2724         fill_value: Any = dtypes.NA,
2725         **shifts_kwargs: int,
2726     ) -> "DataArray":
2727         """Shift this array by an offset along one or more dimensions.
2728 
2729         Only the data is moved; coordinates stay in place. Values shifted from
2730         beyond array bounds are replaced by NaN. This is consistent with the
2731         behavior of ``shift`` in pandas.
2732 
2733         Parameters
2734         ----------
2735         shifts : Mapping with the form of {dim: offset}
2736             Integer offset to shift along each of the given dimensions.
2737             Positive offsets shift to the right; negative offsets shift to the
2738             left.
2739         fill_value: scalar, optional
2740             Value to use for newly missing values
2741         **shifts_kwargs:
2742             The keyword arguments form of ``shifts``.
2743             One of shifts or shifts_kwargs must be provided.
2744 
2745         Returns
2746         -------
2747         shifted : DataArray
2748             DataArray with the same coordinates and attributes but shifted
2749             data.
2750 
2751         See also
2752         --------
2753         roll
2754 
2755         Examples
2756         --------
2757 
2758         >>> arr = xr.DataArray([5, 6, 7], dims='x')
2759         >>> arr.shift(x=1)
2760         <xarray.DataArray (x: 3)>
2761         array([ nan,   5.,   6.])
2762         Coordinates:
2763           * x        (x) int64 0 1 2
2764         """
2765         variable = self.variable.shift(
2766             shifts=shifts, fill_value=fill_value, **shifts_kwargs
2767         )
2768         return self._replace(variable=variable)
2769 
2770     def roll(
2771         self,
2772         shifts: Mapping[Hashable, int] = None,
2773         roll_coords: bool = None,
2774         **shifts_kwargs: int,
2775     ) -> "DataArray":
2776         """Roll this array by an offset along one or more dimensions.
2777 
2778         Unlike shift, roll may rotate all variables, including coordinates
2779         if specified. The direction of rotation is consistent with
2780         :py:func:`numpy.roll`.
2781 
2782         Parameters
2783         ----------
2784         shifts : Mapping with the form of {dim: offset}
2785             Integer offset to rotate each of the given dimensions.
2786             Positive offsets roll to the right; negative offsets roll to the
2787             left.
2788         roll_coords : bool
2789             Indicates whether to  roll the coordinates by the offset
2790             The current default of roll_coords (None, equivalent to True) is
2791             deprecated and will change to False in a future version.
2792             Explicitly pass roll_coords to silence the warning.
2793         **shifts_kwargs : The keyword arguments form of ``shifts``.
2794             One of shifts or shifts_kwargs must be provided.
2795 
2796         Returns
2797         -------
2798         rolled : DataArray
2799             DataArray with the same attributes but rolled data and coordinates.
2800 
2801         See also
2802         --------
2803         shift
2804 
2805         Examples
2806         --------
2807 
2808         >>> arr = xr.DataArray([5, 6, 7], dims='x')
2809         >>> arr.roll(x=1)
2810         <xarray.DataArray (x: 3)>
2811         array([7, 5, 6])
2812         Coordinates:
2813           * x        (x) int64 2 0 1
2814         """
2815         ds = self._to_temp_dataset().roll(
2816             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
2817         )
2818         return self._from_temp_dataset(ds)
2819 
2820     @property
2821     def real(self) -> "DataArray":
2822         return self._replace(self.variable.real)
2823 
2824     @property
2825     def imag(self) -> "DataArray":
2826         return self._replace(self.variable.imag)
2827 
2828     def dot(
2829         self, other: "DataArray", dims: Union[Hashable, Sequence[Hashable], None] = None
2830     ) -> "DataArray":
2831         """Perform dot product of two DataArrays along their shared dims.
2832 
2833         Equivalent to taking taking tensordot over all shared dims.
2834 
2835         Parameters
2836         ----------
2837         other : DataArray
2838             The other array with which the dot product is performed.
2839         dims: '...', hashable or sequence of hashables, optional
2840             Which dimensions to sum over. Ellipsis ('...') sums over all dimensions.
2841             If not specified, then all the common dimensions are summed over.
2842 
2843         Returns
2844         -------
2845         result : DataArray
2846             Array resulting from the dot product over all shared dimensions.
2847 
2848         See also
2849         --------
2850         dot
2851         numpy.tensordot
2852 
2853         Examples
2854         --------
2855 
2856         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
2857         >>> da = DataArray(da_vals, dims=['x', 'y', 'z'])
2858         >>> dm_vals = np.arange(4)
2859         >>> dm = DataArray(dm_vals, dims=['z'])
2860 
2861         >>> dm.dims
2862         ('z')
2863         >>> da.dims
2864         ('x', 'y', 'z')
2865 
2866         >>> dot_result = da.dot(dm)
2867         >>> dot_result.dims
2868         ('x', 'y')
2869         """
2870         if isinstance(other, Dataset):
2871             raise NotImplementedError(
2872                 "dot products are not yet supported with Dataset objects."
2873             )
2874         if not isinstance(other, DataArray):
2875             raise TypeError("dot only operates on DataArrays.")
2876 
2877         return computation.dot(self, other, dims=dims)
2878 
2879     def sortby(
2880         self,
2881         variables: Union[Hashable, "DataArray", Sequence[Union[Hashable, "DataArray"]]],
2882         ascending: bool = True,
2883     ) -> "DataArray":
2884         """Sort object by labels or values (along an axis).
2885 
2886         Sorts the dataarray, either along specified dimensions,
2887         or according to values of 1-D dataarrays that share dimension
2888         with calling object.
2889 
2890         If the input variables are dataarrays, then the dataarrays are aligned
2891         (via left-join) to the calling object prior to sorting by cell values.
2892         NaNs are sorted to the end, following Numpy convention.
2893 
2894         If multiple sorts along the same dimension is
2895         given, numpy's lexsort is performed along that dimension:
2896         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
2897         and the FIRST key in the sequence is used as the primary sort key,
2898         followed by the 2nd key, etc.
2899 
2900         Parameters
2901         ----------
2902         variables: hashable, DataArray, or sequence of either
2903             1D DataArray objects or name(s) of 1D variable(s) in
2904             coords whose values are used to sort this array.
2905         ascending: boolean, optional
2906             Whether to sort by ascending or descending order.
2907 
2908         Returns
2909         -------
2910         sorted: DataArray
2911             A new dataarray where all the specified dims are sorted by dim
2912             labels.
2913 
2914         Examples
2915         --------
2916 
2917         >>> da = xr.DataArray(np.random.rand(5),
2918         ...                   coords=[pd.date_range('1/1/2000', periods=5)],
2919         ...                   dims='time')
2920         >>> da
2921         <xarray.DataArray (time: 5)>
2922         array([ 0.965471,  0.615637,  0.26532 ,  0.270962,  0.552878])
2923         Coordinates:
2924           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...
2925 
2926         >>> da.sortby(da)
2927         <xarray.DataArray (time: 5)>
2928         array([ 0.26532 ,  0.270962,  0.552878,  0.615637,  0.965471])
2929         Coordinates:
2930           * time     (time) datetime64[ns] 2000-01-03 2000-01-04 2000-01-05 ...
2931         """
2932         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
2933         return self._from_temp_dataset(ds)
2934 
2935     def quantile(
2936         self,
2937         q: Any,
2938         dim: Union[Hashable, Sequence[Hashable], None] = None,
2939         interpolation: str = "linear",
2940         keep_attrs: bool = None,
2941     ) -> "DataArray":
2942         """Compute the qth quantile of the data along the specified dimension.
2943 
2944         Returns the qth quantiles(s) of the array elements.
2945 
2946         Parameters
2947         ----------
2948         q : float in range of [0,1] or array-like of floats
2949             Quantile to compute, which must be between 0 and 1 inclusive.
2950         dim : hashable or sequence of hashable, optional
2951             Dimension(s) over which to apply quantile.
2952         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
2953             This optional parameter specifies the interpolation method to
2954             use when the desired quantile lies between two data points
2955             ``i < j``:
2956 
2957                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is
2958                   the fractional part of the index surrounded by ``i`` and
2959                   ``j``.
2960                 - lower: ``i``.
2961                 - higher: ``j``.
2962                 - nearest: ``i`` or ``j``, whichever is nearest.
2963                 - midpoint: ``(i + j) / 2``.
2964         keep_attrs : bool, optional
2965             If True, the dataset's attributes (`attrs`) will be copied from
2966             the original object to the new one.  If False (default), the new
2967             object will be returned without attributes.
2968 
2969         Returns
2970         -------
2971         quantiles : DataArray
2972             If `q` is a single quantile, then the result
2973             is a scalar. If multiple percentiles are given, first axis of
2974             the result corresponds to the quantile and a quantile dimension
2975             is added to the return array. The other dimensions are the
2976             dimensions that remain after the reduction of the array.
2977 
2978         See Also
2979         --------
2980         numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile
2981 
2982         Examples
2983         --------
2984 
2985         >>> da = xr.DataArray(
2986         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
2987         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
2988         ...     dims=("x", "y"),
2989         ... )
2990         >>> da.quantile(0)  # or da.quantile(0, dim=...)
2991         <xarray.DataArray ()>
2992         array(0.7)
2993         Coordinates:
2994             quantile  float64 0.0
2995         >>> da.quantile(0, dim="x")
2996         <xarray.DataArray (y: 4)>
2997         array([0.7, 4.2, 2.6, 1.5])
2998         Coordinates:
2999           * y         (y) float64 1.0 1.5 2.0 2.5
3000             quantile  float64 0.0
3001         >>> da.quantile([0, 0.5, 1])
3002         <xarray.DataArray (quantile: 3)>
3003         array([0.7, 3.4, 9.4])
3004         Coordinates:
3005           * quantile  (quantile) float64 0.0 0.5 1.0
3006         >>> da.quantile([0, 0.5, 1], dim="x")
3007         <xarray.DataArray (quantile: 3, y: 4)>
3008         array([[0.7 , 4.2 , 2.6 , 1.5 ],
3009                [3.6 , 5.75, 6.  , 1.7 ],
3010                [6.5 , 7.3 , 9.4 , 1.9 ]])
3011         Coordinates:
3012           * y         (y) float64 1.0 1.5 2.0 2.5
3013           * quantile  (quantile) float64 0.0 0.5 1.0
3014         """
3015 
3016         ds = self._to_temp_dataset().quantile(
3017             q, dim=dim, keep_attrs=keep_attrs, interpolation=interpolation
3018         )
3019         return self._from_temp_dataset(ds)
3020 
3021     def rank(
3022         self, dim: Hashable, pct: bool = False, keep_attrs: bool = None
3023     ) -> "DataArray":
3024         """Ranks the data.
3025 
3026         Equal values are assigned a rank that is the average of the ranks that
3027         would have been otherwise assigned to all of the values within that
3028         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
3029 
3030         NaNs in the input array are returned as NaNs.
3031 
3032         The `bottleneck` library is required.
3033 
3034         Parameters
3035         ----------
3036         dim : hashable
3037             Dimension over which to compute rank.
3038         pct : bool, optional
3039             If True, compute percentage ranks, otherwise compute integer ranks.
3040         keep_attrs : bool, optional
3041             If True, the dataset's attributes (`attrs`) will be copied from
3042             the original object to the new one.  If False (default), the new
3043             object will be returned without attributes.
3044 
3045         Returns
3046         -------
3047         ranked : DataArray
3048             DataArray with the same coordinates and dtype 'float64'.
3049 
3050         Examples
3051         --------
3052 
3053         >>> arr = xr.DataArray([5, 6, 7], dims='x')
3054         >>> arr.rank('x')
3055         <xarray.DataArray (x: 3)>
3056         array([ 1.,   2.,   3.])
3057         Dimensions without coordinates: x
3058         """
3059 
3060         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
3061         return self._from_temp_dataset(ds)
3062 
3063     def differentiate(
3064         self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None
3065     ) -> "DataArray":
3066         """ Differentiate the array with the second order accurate central
3067         differences.
3068 
3069         .. note::
3070             This feature is limited to simple cartesian geometry, i.e. coord
3071             must be one dimensional.
3072 
3073         Parameters
3074         ----------
3075         coord: hashable
3076             The coordinate to be used to compute the gradient.
3077         edge_order: 1 or 2. Default 1
3078             N-th order accurate differences at the boundaries.
3079         datetime_unit: None or any of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms',
3080             'us', 'ns', 'ps', 'fs', 'as'}
3081             Unit to compute gradient. Only valid for datetime coordinate.
3082 
3083         Returns
3084         -------
3085         differentiated: DataArray
3086 
3087         See also
3088         --------
3089         numpy.gradient: corresponding numpy function
3090 
3091         Examples
3092         --------
3093 
3094         >>> da = xr.DataArray(np.arange(12).reshape(4, 3), dims=['x', 'y'],
3095         ...                   coords={'x': [0, 0.1, 1.1, 1.2]})
3096         >>> da
3097         <xarray.DataArray (x: 4, y: 3)>
3098         array([[ 0,  1,  2],
3099                [ 3,  4,  5],
3100                [ 6,  7,  8],
3101                [ 9, 10, 11]])
3102         Coordinates:
3103           * x        (x) float64 0.0 0.1 1.1 1.2
3104         Dimensions without coordinates: y
3105         >>>
3106         >>> da.differentiate('x')
3107         <xarray.DataArray (x: 4, y: 3)>
3108         array([[30.      , 30.      , 30.      ],
3109                [27.545455, 27.545455, 27.545455],
3110                [27.545455, 27.545455, 27.545455],
3111                [30.      , 30.      , 30.      ]])
3112         Coordinates:
3113           * x        (x) float64 0.0 0.1 1.1 1.2
3114         Dimensions without coordinates: y
3115         """
3116         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
3117         return self._from_temp_dataset(ds)
3118 
3119     def integrate(
3120         self, dim: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None
3121     ) -> "DataArray":
3122         """ integrate the array with the trapezoidal rule.
3123 
3124         .. note::
3125             This feature is limited to simple cartesian geometry, i.e. dim
3126             must be one dimensional.
3127 
3128         Parameters
3129         ----------
3130         dim: hashable, or a sequence of hashable
3131             Coordinate(s) used for the integration.
3132         datetime_unit: str, optional
3133             Can be used to specify the unit if datetime coordinate is used.
3134             One of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', 'ps',
3135             'fs', 'as'}
3136 
3137         Returns
3138         -------
3139         integrated: DataArray
3140 
3141         See also
3142         --------
3143         numpy.trapz: corresponding numpy function
3144 
3145         Examples
3146         --------
3147 
3148         >>> da = xr.DataArray(np.arange(12).reshape(4, 3), dims=['x', 'y'],
3149         ...                   coords={'x': [0, 0.1, 1.1, 1.2]})
3150         >>> da
3151         <xarray.DataArray (x: 4, y: 3)>
3152         array([[ 0,  1,  2],
3153                [ 3,  4,  5],
3154                [ 6,  7,  8],
3155                [ 9, 10, 11]])
3156         Coordinates:
3157           * x        (x) float64 0.0 0.1 1.1 1.2
3158         Dimensions without coordinates: y
3159         >>>
3160         >>> da.integrate('x')
3161         <xarray.DataArray (y: 3)>
3162         array([5.4, 6.6, 7.8])
3163         Dimensions without coordinates: y
3164         """
3165         ds = self._to_temp_dataset().integrate(dim, datetime_unit)
3166         return self._from_temp_dataset(ds)
3167 
3168     def unify_chunks(self) -> "DataArray":
3169         """ Unify chunk size along all chunked dimensions of this DataArray.
3170 
3171         Returns
3172         -------
3173 
3174         DataArray with consistent chunk sizes for all dask-array variables
3175 
3176         See Also
3177         --------
3178 
3179         dask.array.core.unify_chunks
3180         """
3181         ds = self._to_temp_dataset().unify_chunks()
3182         return self._from_temp_dataset(ds)
3183 
3184     def map_blocks(
3185         self,
3186         func: "Callable[..., T_DSorDA]",
3187         args: Sequence[Any] = (),
3188         kwargs: Mapping[str, Any] = None,
3189     ) -> "T_DSorDA":
3190         """
3191         Apply a function to each chunk of this DataArray. This method is experimental
3192         and its signature may change.
3193 
3194         Parameters
3195         ----------
3196         func: callable
3197             User-provided function that accepts a DataArray as its first parameter. The
3198             function will receive a subset of this DataArray, corresponding to one chunk
3199             along each chunked dimension. ``func`` will be executed as
3200             ``func(obj_subset, *args, **kwargs)``.
3201 
3202             The function will be first run on mocked-up data, that looks like this array
3203             but has sizes 0, to determine properties of the returned object such as
3204             dtype, variable names, new dimensions and new indexes (if any).
3205 
3206             This function must return either a single DataArray or a single Dataset.
3207 
3208             This function cannot change size of existing dimensions, or add new chunked
3209             dimensions.
3210         args: Sequence
3211             Passed verbatim to func after unpacking, after the sliced DataArray. xarray
3212             objects, if any, will not be split by chunks. Passing dask collections is
3213             not allowed.
3214         kwargs: Mapping
3215             Passed verbatim to func after unpacking. xarray objects, if any, will not be
3216             split by chunks. Passing dask collections is not allowed.
3217 
3218         Returns
3219         -------
3220         A single DataArray or Dataset with dask backend, reassembled from the outputs of
3221         the function.
3222 
3223         Notes
3224         -----
3225         This method is designed for when one needs to manipulate a whole xarray object
3226         within each chunk. In the more common case where one can work on numpy arrays,
3227         it is recommended to use apply_ufunc.
3228 
3229         If none of the variables in this DataArray is backed by dask, calling this
3230         method is equivalent to calling ``func(self, *args, **kwargs)``.
3231 
3232         See Also
3233         --------
3234         dask.array.map_blocks, xarray.apply_ufunc, xarray.map_blocks,
3235         xarray.Dataset.map_blocks
3236         """
3237         from .parallel import map_blocks
3238 
3239         return map_blocks(func, self, args, kwargs)
3240 
3241     # this needs to be at the end, or mypy will confuse with `str`
3242     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
3243     str = property(StringAccessor)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>xarray/core/variable.py</file>
<class>Variable</class>
<method>quantile</method>
<code>
1683     def quantile(self, q, dim=None, interpolation="linear", keep_attrs=None):
1684         """Compute the qth quantile of the data along the specified dimension.
1685 
1686         Returns the qth quantiles(s) of the array elements.
1687 
1688         Parameters
1689         ----------
1690         q : float in range of [0,1] (or sequence of floats)
1691             Quantile to compute, which must be between 0 and 1
1692             inclusive.
1693         dim : str or sequence of str, optional
1694             Dimension(s) over which to apply quantile.
1695         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
1696             This optional parameter specifies the interpolation method to
1697             use when the desired quantile lies between two data points
1698             ``i < j``:
1699                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is
1700                   the fractional part of the index surrounded by ``i`` and
1701                   ``j``.
1702                 * lower: ``i``.
1703                 * higher: ``j``.
1704                 * nearest: ``i`` or ``j``, whichever is nearest.
1705                 * midpoint: ``(i + j) / 2``.
1706         keep_attrs : bool, optional
1707             If True, the variable's attributes (`attrs`) will be copied from
1708             the original object to the new one.  If False (default), the new
1709             object will be returned without attributes.
1710 
1711         Returns
1712         -------
1713         quantiles : Variable
1714             If `q` is a single quantile, then the result
1715             is a scalar. If multiple percentiles are given, first axis of
1716             the result corresponds to the quantile and a quantile dimension
1717             is added to the return array. The other dimensions are the
1718             dimensions that remain after the reduction of the array.
1719 
1720         See Also
1721         --------
1722         numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile,
1723         DataArray.quantile
1724         """
1725 
1726         from .computation import apply_ufunc
1727 
1728         if keep_attrs is None:
1729             keep_attrs = _get_keep_attrs(default=False)
1730 
1731         scalar = utils.is_scalar(q)
1732         q = np.atleast_1d(np.asarray(q, dtype=np.float64))
1733 
1734         if dim is None:
1735             dim = self.dims
1736 
1737         if utils.is_scalar(dim):
1738             dim = [dim]
1739 
1740         def _wrapper(npa, **kwargs):
1741             # move quantile axis to end. required for apply_ufunc
1742             return np.moveaxis(np.nanpercentile(npa, **kwargs), 0, -1)
1743 
1744         axis = np.arange(-1, -1 * len(dim) - 1, -1)
1745         result = apply_ufunc(
1746             _wrapper,
1747             self,
1748             input_core_dims=[dim],
1749             exclude_dims=set(dim),
1750             output_core_dims=[["quantile"]],
1751             output_dtypes=[np.float64],
1752             output_sizes={"quantile": len(q)},
1753             dask="parallelized",
1754             kwargs={"q": q * 100, "axis": axis, "interpolation": interpolation},
1755         )
1756 
1757         # for backward compatibility
1758         result = result.transpose("quantile", ...)
1759         if scalar:
1760             result = result.squeeze("quantile")
1761         if keep_attrs:
1762             result.attrs = self._attrs
1763         return result

</code><intended_behavior>Although the direct issue doesn't lie here, reviewing how errors are propagated when `np.nanpercentile` is called could be beneficial. Ensure that any error messages about input range exceptions are consistent with the quantile range [0, 1]. If `Variable.quantile` is where the numpy function calls are made, it should either handle the error and rethrow it with a message consistent with xarray's expected input range, or it should contain preemptive input validation to provide a clear and correct error message before deference to numpy's functions.</intended_behavior>, <file>xarray/core/variable.py</file>
<class>Variable</class>
<method>None</method>
<code>
263 class Variable(
264     common.AbstractArray, arithmetic.SupportsArithmetic, utils.NdimSizeLenMixin
265 ):
266     """A netcdf-like variable consisting of dimensions, data and attributes
267     which describe a single Array. A single Variable object is not fully
268     described outside the context of its parent Dataset (if you want such a
269     fully described object, use a DataArray instead).
270 
271     The main functional difference between Variables and numpy arrays is that
272     numerical operations on Variables implement array broadcasting by dimension
273     name. For example, adding an Variable with dimensions `('time',)` to
274     another Variable with dimensions `('space',)` results in a new Variable
275     with dimensions `('time', 'space')`. Furthermore, numpy reduce operations
276     like ``mean`` or ``sum`` are overwritten to take a "dimension" argument
277     instead of an "axis".
278 
279     Variables are light-weight objects used as the building block for datasets.
280     They are more primitive objects, so operations with them provide marginally
281     higher performance than using DataArrays. However, manipulating data in the
282     form of a Dataset or DataArray should almost always be preferred, because
283     they can use more complete metadata in context of coordinate labels.
284     """
285 
286     __slots__ = ("_dims", "_data", "_attrs", "_encoding")
287 
288     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):
289         """
290         Parameters
291         ----------
292         dims : str or sequence of str
293             Name(s) of the the data dimension(s). Must be either a string (only
294             for 1D data) or a sequence of strings with length equal to the
295             number of dimensions.
296         data : array_like
297             Data array which supports numpy-like data access.
298         attrs : dict_like or None, optional
299             Attributes to assign to the new variable. If None (default), an
300             empty attribute dictionary is initialized.
301         encoding : dict_like or None, optional
302             Dictionary specifying how to encode this array's data into a
303             serialized format like netCDF4. Currently used keys (for netCDF)
304             include '_FillValue', 'scale_factor', 'add_offset' and 'dtype'.
305             Well-behaved code to serialize a Variable should ignore
306             unrecognized encoding items.
307         """
308         self._data = as_compatible_data(data, fastpath=fastpath)
309         self._dims = self._parse_dimensions(dims)
310         self._attrs = None
311         self._encoding = None
312         if attrs is not None:
313             self.attrs = attrs
314         if encoding is not None:
315             self.encoding = encoding
316 
317     @property
318     def dtype(self):
319         return self._data.dtype
320 
321     @property
322     def shape(self):
323         return self._data.shape
324 
325     @property
326     def nbytes(self):
327         return self.size * self.dtype.itemsize
328 
329     @property
330     def _in_memory(self):
331         return isinstance(self._data, (np.ndarray, np.number, PandasIndexAdapter)) or (
332             isinstance(self._data, indexing.MemoryCachedArray)
333             and isinstance(self._data.array, indexing.NumpyIndexingAdapter)
334         )
335 
336     @property
337     def data(self):
338         if hasattr(self._data, "__array_function__") or isinstance(
339             self._data, dask_array_type
340         ):
341             return self._data
342         else:
343             return self.values
344 
345     @data.setter
346     def data(self, data):
347         data = as_compatible_data(data)
348         if data.shape != self.shape:
349             raise ValueError(
350                 f"replacement data must match the Variable's shape. "
351                 f"replacement data has shape {data.shape}; Variable has shape {self.shape}"
352             )
353         self._data = data
354 
355     def load(self, **kwargs):
356         """Manually trigger loading of this variable's data from disk or a
357         remote source into memory and return this variable.
358 
359         Normally, it should not be necessary to call this method in user code,
360         because all xarray functions should either work on deferred data or
361         load data automatically.
362 
363         Parameters
364         ----------
365         **kwargs : dict
366             Additional keyword arguments passed on to ``dask.array.compute``.
367 
368         See Also
369         --------
370         dask.array.compute
371         """
372         if isinstance(self._data, dask_array_type):
373             self._data = as_compatible_data(self._data.compute(**kwargs))
374         elif not hasattr(self._data, "__array_function__"):
375             self._data = np.asarray(self._data)
376         return self
377 
378     def compute(self, **kwargs):
379         """Manually trigger loading of this variable's data from disk or a
380         remote source into memory and return a new variable. The original is
381         left unaltered.
382 
383         Normally, it should not be necessary to call this method in user code,
384         because all xarray functions should either work on deferred data or
385         load data automatically.
386 
387         Parameters
388         ----------
389         **kwargs : dict
390             Additional keyword arguments passed on to ``dask.array.compute``.
391 
392         See Also
393         --------
394         dask.array.compute
395         """
396         new = self.copy(deep=False)
397         return new.load(**kwargs)
398 
399     def __dask_tokenize__(self):
400         # Use v.data, instead of v._data, in order to cope with the wrappers
401         # around NetCDF and the like
402         from dask.base import normalize_token
403 
404         return normalize_token((type(self), self._dims, self.data, self._attrs))
405 
406     def __dask_graph__(self):
407         if isinstance(self._data, dask_array_type):
408             return self._data.__dask_graph__()
409         else:
410             return None
411 
412     def __dask_keys__(self):
413         return self._data.__dask_keys__()
414 
415     def __dask_layers__(self):
416         return self._data.__dask_layers__()
417 
418     @property
419     def __dask_optimize__(self):
420         return self._data.__dask_optimize__
421 
422     @property
423     def __dask_scheduler__(self):
424         return self._data.__dask_scheduler__
425 
426     def __dask_postcompute__(self):
427         array_func, array_args = self._data.__dask_postcompute__()
428         return (
429             self._dask_finalize,
430             (array_func, array_args, self._dims, self._attrs, self._encoding),
431         )
432 
433     def __dask_postpersist__(self):
434         array_func, array_args = self._data.__dask_postpersist__()
435         return (
436             self._dask_finalize,
437             (array_func, array_args, self._dims, self._attrs, self._encoding),
438         )
439 
440     @staticmethod
441     def _dask_finalize(results, array_func, array_args, dims, attrs, encoding):
442         if isinstance(results, dict):  # persist case
443             name = array_args[0]
444             results = {k: v for k, v in results.items() if k[0] == name}
445         data = array_func(results, *array_args)
446         return Variable(dims, data, attrs=attrs, encoding=encoding)
447 
448     @property
449     def values(self):
450         """The variable's data as a numpy.ndarray"""
451         return _as_array_or_item(self._data)
452 
453     @values.setter
454     def values(self, values):
455         self.data = values
456 
457     def to_base_variable(self):
458         """Return this variable as a base xarray.Variable"""
459         return Variable(
460             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True
461         )
462 
463     to_variable = utils.alias(to_base_variable, "to_variable")
464 
465     def to_index_variable(self):
466         """Return this variable as an xarray.IndexVariable"""
467         return IndexVariable(
468             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True
469         )
470 
471     to_coord = utils.alias(to_index_variable, "to_coord")
472 
473     def to_index(self):
474         """Convert this variable to a pandas.Index"""
475         return self.to_index_variable().to_index()
476 
477     def to_dict(self, data=True):
478         """Dictionary representation of variable."""
479         item = {"dims": self.dims, "attrs": decode_numpy_dict_values(self.attrs)}
480         if data:
481             item["data"] = ensure_us_time_resolution(self.values).tolist()
482         else:
483             item.update({"dtype": str(self.dtype), "shape": self.shape})
484         return item
485 
486     @property
487     def dims(self):
488         """Tuple of dimension names with which this variable is associated.
489         """
490         return self._dims
491 
492     @dims.setter
493     def dims(self, value):
494         self._dims = self._parse_dimensions(value)
495 
496     def _parse_dimensions(self, dims):
497         if isinstance(dims, str):
498             dims = (dims,)
499         dims = tuple(dims)
500         if len(dims) != self.ndim:
501             raise ValueError(
502                 "dimensions %s must have the same length as the "
503                 "number of data dimensions, ndim=%s" % (dims, self.ndim)
504             )
505         return dims
506 
507     def _item_key_to_tuple(self, key):
508         if utils.is_dict_like(key):
509             return tuple(key.get(dim, slice(None)) for dim in self.dims)
510         else:
511             return key
512 
513     def _broadcast_indexes(self, key):
514         """Prepare an indexing key for an indexing operation.
515 
516         Parameters
517         -----------
518         key: int, slice, array, dict or tuple of integer, slices and arrays
519             Any valid input for indexing.
520 
521         Returns
522         -------
523         dims: tuple
524             Dimension of the resultant variable.
525         indexers: IndexingTuple subclass
526             Tuple of integer, array-like, or slices to use when indexing
527             self._data. The type of this argument indicates the type of
528             indexing to perform, either basic, outer or vectorized.
529         new_order : Optional[Sequence[int]]
530             Optional reordering to do on the result of indexing. If not None,
531             the first len(new_order) indexing should be moved to these
532             positions.
533         """
534         key = self._item_key_to_tuple(key)  # key is a tuple
535         # key is a tuple of full size
536         key = indexing.expanded_indexer(key, self.ndim)
537         # Convert a scalar Variable to an integer
538         key = tuple(
539             k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k for k in key
540         )
541         # Convert a 0d-array to an integer
542         key = tuple(
543             k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key
544         )
545 
546         if all(isinstance(k, BASIC_INDEXING_TYPES) for k in key):
547             return self._broadcast_indexes_basic(key)
548 
549         self._validate_indexers(key)
550         # Detect it can be mapped as an outer indexer
551         # If all key is unlabeled, or
552         # key can be mapped as an OuterIndexer.
553         if all(not isinstance(k, Variable) for k in key):
554             return self._broadcast_indexes_outer(key)
555 
556         # If all key is 1-dimensional and there are no duplicate labels,
557         # key can be mapped as an OuterIndexer.
558         dims = []
559         for k, d in zip(key, self.dims):
560             if isinstance(k, Variable):
561                 if len(k.dims) > 1:
562                     return self._broadcast_indexes_vectorized(key)
563                 dims.append(k.dims[0])
564             elif not isinstance(k, integer_types):
565                 dims.append(d)
566         if len(set(dims)) == len(dims):
567             return self._broadcast_indexes_outer(key)
568 
569         return self._broadcast_indexes_vectorized(key)
570 
571     def _broadcast_indexes_basic(self, key):
572         dims = tuple(
573             dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)
574         )
575         return dims, BasicIndexer(key), None
576 
577     def _validate_indexers(self, key):
578         """ Make sanity checks """
579         for dim, k in zip(self.dims, key):
580             if isinstance(k, BASIC_INDEXING_TYPES):
581                 pass
582             else:
583                 if not isinstance(k, Variable):
584                     k = np.asarray(k)
585                     if k.ndim > 1:
586                         raise IndexError(
587                             "Unlabeled multi-dimensional array cannot be "
588                             "used for indexing: {}".format(k)
589                         )
590                 if k.dtype.kind == "b":
591                     if self.shape[self.get_axis_num(dim)] != len(k):
592                         raise IndexError(
593                             "Boolean array size {:d} is used to index array "
594                             "with shape {:s}.".format(len(k), str(self.shape))
595                         )
596                     if k.ndim > 1:
597                         raise IndexError(
598                             "{}-dimensional boolean indexing is "
599                             "not supported. ".format(k.ndim)
600                         )
601                     if getattr(k, "dims", (dim,)) != (dim,):
602                         raise IndexError(
603                             "Boolean indexer should be unlabeled or on the "
604                             "same dimension to the indexed array. Indexer is "
605                             "on {:s} but the target dimension is {:s}.".format(
606                                 str(k.dims), dim
607                             )
608                         )
609 
610     def _broadcast_indexes_outer(self, key):
611         dims = tuple(
612             k.dims[0] if isinstance(k, Variable) else dim
613             for k, dim in zip(key, self.dims)
614             if not isinstance(k, integer_types)
615         )
616 
617         new_key = []
618         for k in key:
619             if isinstance(k, Variable):
620                 k = k.data
621             if not isinstance(k, BASIC_INDEXING_TYPES):
622                 k = np.asarray(k)
623                 if k.size == 0:
624                     # Slice by empty list; numpy could not infer the dtype
625                     k = k.astype(int)
626                 elif k.dtype.kind == "b":
627                     (k,) = np.nonzero(k)
628             new_key.append(k)
629 
630         return dims, OuterIndexer(tuple(new_key)), None
631 
632     def _nonzero(self):
633         """ Equivalent numpy's nonzero but returns a tuple of Varibles. """
634         # TODO we should replace dask's native nonzero
635         # after https://github.com/dask/dask/issues/1076 is implemented.
636         nonzeros = np.nonzero(self.data)
637         return tuple(Variable((dim), nz) for nz, dim in zip(nonzeros, self.dims))
638 
639     def _broadcast_indexes_vectorized(self, key):
640         variables = []
641         out_dims_set = OrderedSet()
642         for dim, value in zip(self.dims, key):
643             if isinstance(value, slice):
644                 out_dims_set.add(dim)
645             else:
646                 variable = (
647                     value
648                     if isinstance(value, Variable)
649                     else as_variable(value, name=dim)
650                 )
651                 if variable.dtype.kind == "b":  # boolean indexing case
652                     (variable,) = variable._nonzero()
653 
654                 variables.append(variable)
655                 out_dims_set.update(variable.dims)
656 
657         variable_dims = set()
658         for variable in variables:
659             variable_dims.update(variable.dims)
660 
661         slices = []
662         for i, (dim, value) in enumerate(zip(self.dims, key)):
663             if isinstance(value, slice):
664                 if dim in variable_dims:
665                     # We only convert slice objects to variables if they share
666                     # a dimension with at least one other variable. Otherwise,
667                     # we can equivalently leave them as slices aknd transpose
668                     # the result. This is significantly faster/more efficient
669                     # for most array backends.
670                     values = np.arange(*value.indices(self.sizes[dim]))
671                     variables.insert(i - len(slices), Variable((dim,), values))
672                 else:
673                     slices.append((i, value))
674 
675         try:
676             variables = _broadcast_compat_variables(*variables)
677         except ValueError:
678             raise IndexError(f"Dimensions of indexers mismatch: {key}")
679 
680         out_key = [variable.data for variable in variables]
681         out_dims = tuple(out_dims_set)
682         slice_positions = set()
683         for i, value in slices:
684             out_key.insert(i, value)
685             new_position = out_dims.index(self.dims[i])
686             slice_positions.add(new_position)
687 
688         if slice_positions:
689             new_order = [i for i in range(len(out_dims)) if i not in slice_positions]
690         else:
691             new_order = None
692 
693         return out_dims, VectorizedIndexer(tuple(out_key)), new_order
694 
695     def __getitem__(self: VariableType, key) -> VariableType:
696         """Return a new Variable object whose contents are consistent with
697         getting the provided key from the underlying data.
698 
699         NB. __getitem__ and __setitem__ implement xarray-style indexing,
700         where if keys are unlabeled arrays, we index the array orthogonally
701         with them. If keys are labeled array (such as Variables), they are
702         broadcasted with our usual scheme and then the array is indexed with
703         the broadcasted key, like numpy's fancy indexing.
704 
705         If you really want to do indexing like `x[x > 0]`, manipulate the numpy
706         array `x.values` directly.
707         """
708         dims, indexer, new_order = self._broadcast_indexes(key)
709         data = as_indexable(self._data)[indexer]
710         if new_order:
711             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)
712         return self._finalize_indexing_result(dims, data)
713 
714     def _finalize_indexing_result(self: VariableType, dims, data) -> VariableType:
715         """Used by IndexVariable to return IndexVariable objects when possible.
716         """
717         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)
718 
719     def _getitem_with_mask(self, key, fill_value=dtypes.NA):
720         """Index this Variable with -1 remapped to fill_value."""
721         # TODO(shoyer): expose this method in public API somewhere (isel?) and
722         # use it for reindex.
723         # TODO(shoyer): add a sanity check that all other integers are
724         # non-negative
725         # TODO(shoyer): add an optimization, remapping -1 to an adjacent value
726         # that is actually indexed rather than mapping it to the last value
727         # along each axis.
728 
729         if fill_value is dtypes.NA:
730             fill_value = dtypes.get_fill_value(self.dtype)
731 
732         dims, indexer, new_order = self._broadcast_indexes(key)
733 
734         if self.size:
735             if isinstance(self._data, dask_array_type):
736                 # dask's indexing is faster this way; also vindex does not
737                 # support negative indices yet:
738                 # https://github.com/dask/dask/pull/2967
739                 actual_indexer = indexing.posify_mask_indexer(indexer)
740             else:
741                 actual_indexer = indexer
742 
743             data = as_indexable(self._data)[actual_indexer]
744             mask = indexing.create_mask(indexer, self.shape, data)
745             data = duck_array_ops.where(mask, fill_value, data)
746         else:
747             # array cannot be indexed along dimensions of size 0, so just
748             # build the mask directly instead.
749             mask = indexing.create_mask(indexer, self.shape)
750             data = np.broadcast_to(fill_value, getattr(mask, "shape", ()))
751 
752         if new_order:
753             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)
754         return self._finalize_indexing_result(dims, data)
755 
756     def __setitem__(self, key, value):
757         """__setitem__ is overloaded to access the underlying numpy values with
758         orthogonal indexing.
759 
760         See __getitem__ for more details.
761         """
762         dims, index_tuple, new_order = self._broadcast_indexes(key)
763 
764         if not isinstance(value, Variable):
765             value = as_compatible_data(value)
766             if value.ndim > len(dims):
767                 raise ValueError(
768                     "shape mismatch: value array of shape %s could not be "
769                     "broadcast to indexing result with %s dimensions"
770                     % (value.shape, len(dims))
771                 )
772             if value.ndim == 0:
773                 value = Variable((), value)
774             else:
775                 value = Variable(dims[-value.ndim :], value)
776         # broadcast to become assignable
777         value = value.set_dims(dims).data
778 
779         if new_order:
780             value = duck_array_ops.asarray(value)
781             value = value[(len(dims) - value.ndim) * (np.newaxis,) + (Ellipsis,)]
782             value = duck_array_ops.moveaxis(value, new_order, range(len(new_order)))
783 
784         indexable = as_indexable(self._data)
785         indexable[index_tuple] = value
786 
787     @property
788     def attrs(self) -> Dict[Hashable, Any]:
789         """Dictionary of local attributes on this variable.
790         """
791         if self._attrs is None:
792             self._attrs = {}
793         return self._attrs
794 
795     @attrs.setter
796     def attrs(self, value: Mapping[Hashable, Any]) -> None:
797         self._attrs = dict(value)
798 
799     @property
800     def encoding(self):
801         """Dictionary of encodings on this variable.
802         """
803         if self._encoding is None:
804             self._encoding = {}
805         return self._encoding
806 
807     @encoding.setter
808     def encoding(self, value):
809         try:
810             self._encoding = dict(value)
811         except ValueError:
812             raise ValueError("encoding must be castable to a dictionary")
813 
814     def copy(self, deep=True, data=None):
815         """Returns a copy of this object.
816 
817         If `deep=True`, the data array is loaded into memory and copied onto
818         the new object. Dimensions, attributes and encodings are always copied.
819 
820         Use `data` to create a new object with the same structure as
821         original but entirely new data.
822 
823         Parameters
824         ----------
825         deep : bool, optional
826             Whether the data array is loaded into memory and copied onto
827             the new object. Default is True.
828         data : array_like, optional
829             Data to use in the new object. Must have same shape as original.
830             When `data` is used, `deep` is ignored.
831 
832         Returns
833         -------
834         object : Variable
835             New object with dimensions, attributes, encodings, and optionally
836             data copied from original.
837 
838         Examples
839         --------
840 
841         Shallow copy versus deep copy
842 
843         >>> var = xr.Variable(data=[1, 2, 3], dims='x')
844         >>> var.copy()
845         <xarray.Variable (x: 3)>
846         array([1, 2, 3])
847         >>> var_0 = var.copy(deep=False)
848         >>> var_0[0] = 7
849         >>> var_0
850         <xarray.Variable (x: 3)>
851         array([7, 2, 3])
852         >>> var
853         <xarray.Variable (x: 3)>
854         array([7, 2, 3])
855 
856         Changing the data using the ``data`` argument maintains the
857         structure of the original object, but with the new data. Original
858         object is unaffected.
859 
860         >>> var.copy(data=[0.1, 0.2, 0.3])
861         <xarray.Variable (x: 3)>
862         array([ 0.1,  0.2,  0.3])
863         >>> var
864         <xarray.Variable (x: 3)>
865         array([7, 2, 3])
866 
867         See Also
868         --------
869         pandas.DataFrame.copy
870         """
871         if data is None:
872             data = self._data
873 
874             if isinstance(data, indexing.MemoryCachedArray):
875                 # don't share caching between copies
876                 data = indexing.MemoryCachedArray(data.array)
877 
878             if deep:
879                 if hasattr(data, "__array_function__") or isinstance(
880                     data, dask_array_type
881                 ):
882                     data = data.copy()
883                 elif not isinstance(data, PandasIndexAdapter):
884                     # pandas.Index is immutable
885                     data = np.array(data)
886         else:
887             data = as_compatible_data(data)
888             if self.shape != data.shape:
889                 raise ValueError(
890                     "Data shape {} must match shape of object {}".format(
891                         data.shape, self.shape
892                     )
893                 )
894 
895         # note:
896         # dims is already an immutable tuple
897         # attributes and encoding will be copied when the new Array is created
898         return self._replace(data=data)
899 
900     def _replace(
901         self, dims=_default, data=_default, attrs=_default, encoding=_default
902     ) -> "Variable":
903         if dims is _default:
904             dims = copy.copy(self._dims)
905         if data is _default:
906             data = copy.copy(self.data)
907         if attrs is _default:
908             attrs = copy.copy(self._attrs)
909         if encoding is _default:
910             encoding = copy.copy(self._encoding)
911         return type(self)(dims, data, attrs, encoding, fastpath=True)
912 
913     def __copy__(self):
914         return self.copy(deep=False)
915 
916     def __deepcopy__(self, memo=None):
917         # memo does nothing but is required for compatibility with
918         # copy.deepcopy
919         return self.copy(deep=True)
920 
921     # mutable objects should not be hashable
922     # https://github.com/python/mypy/issues/4266
923     __hash__ = None  # type: ignore
924 
925     @property
926     def chunks(self):
927         """Block dimensions for this array's data or None if it's not a dask
928         array.
929         """
930         return getattr(self._data, "chunks", None)
931 
932     _array_counter = itertools.count()
933 
934     def chunk(self, chunks=None, name=None, lock=False):
935         """Coerce this array's data into a dask arrays with the given chunks.
936 
937         If this variable is a non-dask array, it will be converted to dask
938         array. If it's a dask array, it will be rechunked to the given chunk
939         sizes.
940 
941         If neither chunks is not provided for one or more dimensions, chunk
942         sizes along that dimension will not be updated; non-dask arrays will be
943         converted into dask arrays with a single block.
944 
945         Parameters
946         ----------
947         chunks : int, tuple or dict, optional
948             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
949             ``{'x': 5, 'y': 5}``.
950         name : str, optional
951             Used to generate the name for this array in the internal dask
952             graph. Does not need not be unique.
953         lock : optional
954             Passed on to :py:func:`dask.array.from_array`, if the array is not
955             already as dask array.
956 
957         Returns
958         -------
959         chunked : xarray.Variable
960         """
961         import dask
962         import dask.array as da
963 
964         if utils.is_dict_like(chunks):
965             chunks = {self.get_axis_num(dim): chunk for dim, chunk in chunks.items()}
966 
967         if chunks is None:
968             chunks = self.chunks or self.shape
969 
970         data = self._data
971         if isinstance(data, da.Array):
972             data = data.rechunk(chunks)
973         else:
974             if isinstance(data, indexing.ExplicitlyIndexed):
975                 # Unambiguously handle array storage backends (like NetCDF4 and h5py)
976                 # that can't handle general array indexing. For example, in netCDF4 you
977                 # can do "outer" indexing along two dimensions independent, which works
978                 # differently from how NumPy handles it.
979                 # da.from_array works by using lazy indexing with a tuple of slices.
980                 # Using OuterIndexer is a pragmatic choice: dask does not yet handle
981                 # different indexing types in an explicit way:
982                 # https://github.com/dask/dask/issues/2883
983                 data = indexing.ImplicitToExplicitIndexingAdapter(
984                     data, indexing.OuterIndexer
985                 )
986                 if LooseVersion(dask.__version__) < "2.0.0":
987                     kwargs = {}
988                 else:
989                     # All of our lazily loaded backend array classes should use NumPy
990                     # array operations.
991                     kwargs = {"meta": np.ndarray}
992             else:
993                 kwargs = {}
994 
995             if utils.is_dict_like(chunks):
996                 chunks = tuple(chunks.get(n, s) for n, s in enumerate(self.shape))
997 
998             data = da.from_array(data, chunks, name=name, lock=lock, **kwargs)
999 
1000         return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)
1001 
1002     def _as_sparse(self, sparse_format=_default, fill_value=dtypes.NA):
1003         """
1004         use sparse-array as backend.
1005         """
1006         import sparse
1007 
1008         # TODO  what to do if dask-backended?
1009         if fill_value is dtypes.NA:
1010             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1011         else:
1012             dtype = dtypes.result_type(self.dtype, fill_value)
1013 
1014         if sparse_format is _default:
1015             sparse_format = "coo"
1016         try:
1017             as_sparse = getattr(sparse, "as_{}".format(sparse_format.lower()))
1018         except AttributeError:
1019             raise ValueError("{} is not a valid sparse format".format(sparse_format))
1020 
1021         data = as_sparse(self.data.astype(dtype), fill_value=fill_value)
1022         return self._replace(data=data)
1023 
1024     def _to_dense(self):
1025         """
1026         Change backend from sparse to np.array
1027         """
1028         if hasattr(self._data, "todense"):
1029             return self._replace(data=self._data.todense())
1030         return self.copy(deep=False)
1031 
1032     def isel(
1033         self: VariableType,
1034         indexers: Mapping[Hashable, Any] = None,
1035         **indexers_kwargs: Any,
1036     ) -> VariableType:
1037         """Return a new array indexed along the specified dimension(s).
1038 
1039         Parameters
1040         ----------
1041         **indexers : {dim: indexer, ...}
1042             Keyword arguments with names matching dimensions and values given
1043             by integers, slice objects or arrays.
1044 
1045         Returns
1046         -------
1047         obj : Array object
1048             A new Array with the selected data and dimensions. In general,
1049             the new variable's data will be a view of this variable's data,
1050             unless numpy fancy indexing was triggered by using an array
1051             indexer, in which case the data will be a copy.
1052         """
1053         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1054 
1055         invalid = indexers.keys() - set(self.dims)
1056         if invalid:
1057             raise ValueError("dimensions %r do not exist" % invalid)
1058 
1059         key = tuple(indexers.get(dim, slice(None)) for dim in self.dims)
1060         return self[key]
1061 
1062     def squeeze(self, dim=None):
1063         """Return a new object with squeezed data.
1064 
1065         Parameters
1066         ----------
1067         dim : None or str or tuple of str, optional
1068             Selects a subset of the length one dimensions. If a dimension is
1069             selected with length greater than one, an error is raised. If
1070             None, all length one dimensions are squeezed.
1071 
1072         Returns
1073         -------
1074         squeezed : same type as caller
1075             This object, but with with all or a subset of the dimensions of
1076             length 1 removed.
1077 
1078         See Also
1079         --------
1080         numpy.squeeze
1081         """
1082         dims = common.get_squeeze_dims(self, dim)
1083         return self.isel({d: 0 for d in dims})
1084 
1085     def _shift_one_dim(self, dim, count, fill_value=dtypes.NA):
1086         axis = self.get_axis_num(dim)
1087 
1088         if count > 0:
1089             keep = slice(None, -count)
1090         elif count < 0:
1091             keep = slice(-count, None)
1092         else:
1093             keep = slice(None)
1094 
1095         trimmed_data = self[(slice(None),) * axis + (keep,)].data
1096 
1097         if fill_value is dtypes.NA:
1098             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1099         else:
1100             dtype = self.dtype
1101 
1102         shape = list(self.shape)
1103         shape[axis] = min(abs(count), shape[axis])
1104 
1105         if isinstance(trimmed_data, dask_array_type):
1106             chunks = list(trimmed_data.chunks)
1107             chunks[axis] = (shape[axis],)
1108             full = functools.partial(da.full, chunks=chunks)
1109         else:
1110             full = np.full
1111 
1112         filler = full(shape, fill_value, dtype=dtype)
1113 
1114         if count > 0:
1115             arrays = [filler, trimmed_data]
1116         else:
1117             arrays = [trimmed_data, filler]
1118 
1119         data = duck_array_ops.concatenate(arrays, axis)
1120 
1121         if isinstance(data, dask_array_type):
1122             # chunked data should come out with the same chunks; this makes
1123             # it feasible to combine shifted and unshifted data
1124             # TODO: remove this once dask.array automatically aligns chunks
1125             data = data.rechunk(self.data.chunks)
1126 
1127         return type(self)(self.dims, data, self._attrs, fastpath=True)
1128 
1129     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):
1130         """
1131         Return a new Variable with shifted data.
1132 
1133         Parameters
1134         ----------
1135         shifts : mapping of the form {dim: offset}
1136             Integer offset to shift along each of the given dimensions.
1137             Positive offsets shift to the right; negative offsets shift to the
1138             left.
1139         fill_value: scalar, optional
1140             Value to use for newly missing values
1141         **shifts_kwargs:
1142             The keyword arguments form of ``shifts``.
1143             One of shifts or shifts_kwargs must be provided.
1144 
1145         Returns
1146         -------
1147         shifted : Variable
1148             Variable with the same dimensions and attributes but shifted data.
1149         """
1150         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
1151         result = self
1152         for dim, count in shifts.items():
1153             result = result._shift_one_dim(dim, count, fill_value=fill_value)
1154         return result
1155 
1156     def pad_with_fill_value(
1157         self, pad_widths=None, fill_value=dtypes.NA, **pad_widths_kwargs
1158     ):
1159         """
1160         Return a new Variable with paddings.
1161 
1162         Parameters
1163         ----------
1164         pad_width: Mapping of the form {dim: (before, after)}
1165             Number of values padded to the edges of each dimension.
1166         **pad_widths_kwargs:
1167             Keyword argument for pad_widths
1168         """
1169         pad_widths = either_dict_or_kwargs(pad_widths, pad_widths_kwargs, "pad")
1170 
1171         if fill_value is dtypes.NA:
1172             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1173         else:
1174             dtype = self.dtype
1175 
1176         if isinstance(self.data, dask_array_type):
1177             array = self.data
1178 
1179             # Dask does not yet support pad. We manually implement it.
1180             # https://github.com/dask/dask/issues/1926
1181             for d, pad in pad_widths.items():
1182                 axis = self.get_axis_num(d)
1183                 before_shape = list(array.shape)
1184                 before_shape[axis] = pad[0]
1185                 before_chunks = list(array.chunks)
1186                 before_chunks[axis] = (pad[0],)
1187                 after_shape = list(array.shape)
1188                 after_shape[axis] = pad[1]
1189                 after_chunks = list(array.chunks)
1190                 after_chunks[axis] = (pad[1],)
1191 
1192                 arrays = []
1193                 if pad[0] > 0:
1194                     arrays.append(
1195                         da.full(
1196                             before_shape, fill_value, dtype=dtype, chunks=before_chunks
1197                         )
1198                     )
1199                 arrays.append(array)
1200                 if pad[1] > 0:
1201                     arrays.append(
1202                         da.full(
1203                             after_shape, fill_value, dtype=dtype, chunks=after_chunks
1204                         )
1205                     )
1206                 if len(arrays) > 1:
1207                     array = da.concatenate(arrays, axis=axis)
1208         else:
1209             pads = [(0, 0) if d not in pad_widths else pad_widths[d] for d in self.dims]
1210             array = np.pad(
1211                 self.data.astype(dtype, copy=False),
1212                 pads,
1213                 mode="constant",
1214                 constant_values=fill_value,
1215             )
1216         return type(self)(self.dims, array)
1217 
1218     def _roll_one_dim(self, dim, count):
1219         axis = self.get_axis_num(dim)
1220 
1221         count %= self.shape[axis]
1222         if count != 0:
1223             indices = [slice(-count, None), slice(None, -count)]
1224         else:
1225             indices = [slice(None)]
1226 
1227         arrays = [self[(slice(None),) * axis + (idx,)].data for idx in indices]
1228 
1229         data = duck_array_ops.concatenate(arrays, axis)
1230 
1231         if isinstance(data, dask_array_type):
1232             # chunked data should come out with the same chunks; this makes
1233             # it feasible to combine shifted and unshifted data
1234             # TODO: remove this once dask.array automatically aligns chunks
1235             data = data.rechunk(self.data.chunks)
1236 
1237         return type(self)(self.dims, data, self._attrs, fastpath=True)
1238 
1239     def roll(self, shifts=None, **shifts_kwargs):
1240         """
1241         Return a new Variable with rolld data.
1242 
1243         Parameters
1244         ----------
1245         shifts : mapping of the form {dim: offset}
1246             Integer offset to roll along each of the given dimensions.
1247             Positive offsets roll to the right; negative offsets roll to the
1248             left.
1249         **shifts_kwargs:
1250             The keyword arguments form of ``shifts``.
1251             One of shifts or shifts_kwargs must be provided.
1252 
1253         Returns
1254         -------
1255         shifted : Variable
1256             Variable with the same dimensions and attributes but rolled data.
1257         """
1258         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
1259 
1260         result = self
1261         for dim, count in shifts.items():
1262             result = result._roll_one_dim(dim, count)
1263         return result
1264 
1265     def transpose(self, *dims) -> "Variable":
1266         """Return a new Variable object with transposed dimensions.
1267 
1268         Parameters
1269         ----------
1270         *dims : str, optional
1271             By default, reverse the dimensions. Otherwise, reorder the
1272             dimensions to this order.
1273 
1274         Returns
1275         -------
1276         transposed : Variable
1277             The returned object has transposed data and dimensions with the
1278             same attributes as the original.
1279 
1280         Notes
1281         -----
1282         This operation returns a view of this variable's data. It is
1283         lazy for dask-backed Variables but not for numpy-backed Variables.
1284 
1285         See Also
1286         --------
1287         numpy.transpose
1288         """
1289         if len(dims) == 0:
1290             dims = self.dims[::-1]
1291         dims = tuple(infix_dims(dims, self.dims))
1292         axes = self.get_axis_num(dims)
1293         if len(dims) < 2 or dims == self.dims:
1294             # no need to transpose if only one dimension
1295             # or dims are in same order
1296             return self.copy(deep=False)
1297 
1298         data = as_indexable(self._data).transpose(axes)
1299         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)
1300 
1301     @property
1302     def T(self) -> "Variable":
1303         return self.transpose()
1304 
1305     def set_dims(self, dims, shape=None):
1306         """Return a new variable with given set of dimensions.
1307         This method might be used to attach new dimension(s) to variable.
1308 
1309         When possible, this operation does not copy this variable's data.
1310 
1311         Parameters
1312         ----------
1313         dims : str or sequence of str or dict
1314             Dimensions to include on the new variable. If a dict, values are
1315             used to provide the sizes of new dimensions; otherwise, new
1316             dimensions are inserted with length 1.
1317 
1318         Returns
1319         -------
1320         Variable
1321         """
1322         if isinstance(dims, str):
1323             dims = [dims]
1324 
1325         if shape is None and utils.is_dict_like(dims):
1326             shape = dims.values()
1327 
1328         missing_dims = set(self.dims) - set(dims)
1329         if missing_dims:
1330             raise ValueError(
1331                 "new dimensions %r must be a superset of "
1332                 "existing dimensions %r" % (dims, self.dims)
1333             )
1334 
1335         self_dims = set(self.dims)
1336         expanded_dims = tuple(d for d in dims if d not in self_dims) + self.dims
1337 
1338         if self.dims == expanded_dims:
1339             # don't use broadcast_to unless necessary so the result remains
1340             # writeable if possible
1341             expanded_data = self.data
1342         elif shape is not None:
1343             dims_map = dict(zip(dims, shape))
1344             tmp_shape = tuple(dims_map[d] for d in expanded_dims)
1345             expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)
1346         else:
1347             expanded_data = self.data[(None,) * (len(expanded_dims) - self.ndim)]
1348 
1349         expanded_var = Variable(
1350             expanded_dims, expanded_data, self._attrs, self._encoding, fastpath=True
1351         )
1352         return expanded_var.transpose(*dims)
1353 
1354     def _stack_once(self, dims, new_dim):
1355         if not set(dims) <= set(self.dims):
1356             raise ValueError("invalid existing dimensions: %s" % dims)
1357 
1358         if new_dim in self.dims:
1359             raise ValueError(
1360                 "cannot create a new dimension with the same "
1361                 "name as an existing dimension"
1362             )
1363 
1364         if len(dims) == 0:
1365             # don't stack
1366             return self.copy(deep=False)
1367 
1368         other_dims = [d for d in self.dims if d not in dims]
1369         dim_order = other_dims + list(dims)
1370         reordered = self.transpose(*dim_order)
1371 
1372         new_shape = reordered.shape[: len(other_dims)] + (-1,)
1373         new_data = reordered.data.reshape(new_shape)
1374         new_dims = reordered.dims[: len(other_dims)] + (new_dim,)
1375 
1376         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)
1377 
1378     def stack(self, dimensions=None, **dimensions_kwargs):
1379         """
1380         Stack any number of existing dimensions into a single new dimension.
1381 
1382         New dimensions will be added at the end, and the order of the data
1383         along each new dimension will be in contiguous (C) order.
1384 
1385         Parameters
1386         ----------
1387         dimensions : Mapping of form new_name=(dim1, dim2, ...)
1388             Names of new dimensions, and the existing dimensions that they
1389             replace.
1390         **dimensions_kwargs:
1391             The keyword arguments form of ``dimensions``.
1392             One of dimensions or dimensions_kwargs must be provided.
1393 
1394         Returns
1395         -------
1396         stacked : Variable
1397             Variable with the same attributes but stacked data.
1398 
1399         See also
1400         --------
1401         Variable.unstack
1402         """
1403         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
1404         result = self
1405         for new_dim, dims in dimensions.items():
1406             result = result._stack_once(dims, new_dim)
1407         return result
1408 
1409     def _unstack_once(self, dims, old_dim):
1410         new_dim_names = tuple(dims.keys())
1411         new_dim_sizes = tuple(dims.values())
1412 
1413         if old_dim not in self.dims:
1414             raise ValueError("invalid existing dimension: %s" % old_dim)
1415 
1416         if set(new_dim_names).intersection(self.dims):
1417             raise ValueError(
1418                 "cannot create a new dimension with the same "
1419                 "name as an existing dimension"
1420             )
1421 
1422         if np.prod(new_dim_sizes) != self.sizes[old_dim]:
1423             raise ValueError(
1424                 "the product of the new dimension sizes must "
1425                 "equal the size of the old dimension"
1426             )
1427 
1428         other_dims = [d for d in self.dims if d != old_dim]
1429         dim_order = other_dims + [old_dim]
1430         reordered = self.transpose(*dim_order)
1431 
1432         new_shape = reordered.shape[: len(other_dims)] + new_dim_sizes
1433         new_data = reordered.data.reshape(new_shape)
1434         new_dims = reordered.dims[: len(other_dims)] + new_dim_names
1435 
1436         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)
1437 
1438     def unstack(self, dimensions=None, **dimensions_kwargs):
1439         """
1440         Unstack an existing dimension into multiple new dimensions.
1441 
1442         New dimensions will be added at the end, and the order of the data
1443         along each new dimension will be in contiguous (C) order.
1444 
1445         Parameters
1446         ----------
1447         dimensions : mapping of the form old_dim={dim1: size1, ...}
1448             Names of existing dimensions, and the new dimensions and sizes
1449             that they map to.
1450         **dimensions_kwargs:
1451             The keyword arguments form of ``dimensions``.
1452             One of dimensions or dimensions_kwargs must be provided.
1453 
1454         Returns
1455         -------
1456         unstacked : Variable
1457             Variable with the same attributes but unstacked data.
1458 
1459         See also
1460         --------
1461         Variable.stack
1462         """
1463         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "unstack")
1464         result = self
1465         for old_dim, dims in dimensions.items():
1466             result = result._unstack_once(dims, old_dim)
1467         return result
1468 
1469     def fillna(self, value):
1470         return ops.fillna(self, value)
1471 
1472     def where(self, cond, other=dtypes.NA):
1473         return ops.where_method(self, cond, other)
1474 
1475     def reduce(
1476         self,
1477         func,
1478         dim=None,
1479         axis=None,
1480         keep_attrs=None,
1481         keepdims=False,
1482         allow_lazy=None,
1483         **kwargs,
1484     ):
1485         """Reduce this array by applying `func` along some dimension(s).
1486 
1487         Parameters
1488         ----------
1489         func : function
1490             Function which can be called in the form
1491             `func(x, axis=axis, **kwargs)` to return the result of reducing an
1492             np.ndarray over an integer valued axis.
1493         dim : str or sequence of str, optional
1494             Dimension(s) over which to apply `func`.
1495         axis : int or sequence of int, optional
1496             Axis(es) over which to apply `func`. Only one of the 'dim'
1497             and 'axis' arguments can be supplied. If neither are supplied, then
1498             the reduction is calculated over the flattened array (by calling
1499             `func(x)` without an axis argument).
1500         keep_attrs : bool, optional
1501             If True, the variable's attributes (`attrs`) will be copied from
1502             the original object to the new one.  If False (default), the new
1503             object will be returned without attributes.
1504         keepdims : bool, default False
1505             If True, the dimensions which are reduced are left in the result
1506             as dimensions of size one
1507         **kwargs : dict
1508             Additional keyword arguments passed on to `func`.
1509 
1510         Returns
1511         -------
1512         reduced : Array
1513             Array with summarized data and the indicated dimension(s)
1514             removed.
1515         """
1516         if dim == ...:
1517             dim = None
1518         if dim is not None and axis is not None:
1519             raise ValueError("cannot supply both 'axis' and 'dim' arguments")
1520 
1521         if dim is not None:
1522             axis = self.get_axis_num(dim)
1523 
1524         if allow_lazy is not None:
1525             warnings.warn(
1526                 "allow_lazy is deprecated and will be removed in version 0.16.0. It is now True by default.",
1527                 DeprecationWarning,
1528             )
1529         else:
1530             allow_lazy = True
1531 
1532         input_data = self.data if allow_lazy else self.values
1533 
1534         if axis is not None:
1535             data = func(input_data, axis=axis, **kwargs)
1536         else:
1537             data = func(input_data, **kwargs)
1538 
1539         if getattr(data, "shape", ()) == self.shape:
1540             dims = self.dims
1541         else:
1542             removed_axes = (
1543                 range(self.ndim) if axis is None else np.atleast_1d(axis) % self.ndim
1544             )
1545             if keepdims:
1546                 # Insert np.newaxis for removed dims
1547                 slices = tuple(
1548                     np.newaxis if i in removed_axes else slice(None, None)
1549                     for i in range(self.ndim)
1550                 )
1551                 if getattr(data, "shape", None) is None:
1552                     # Reduce has produced a scalar value, not an array-like
1553                     data = np.asanyarray(data)[slices]
1554                 else:
1555                     data = data[slices]
1556                 dims = self.dims
1557             else:
1558                 dims = [
1559                     adim for n, adim in enumerate(self.dims) if n not in removed_axes
1560                 ]
1561 
1562         if keep_attrs is None:
1563             keep_attrs = _get_keep_attrs(default=False)
1564         attrs = self._attrs if keep_attrs else None
1565 
1566         return Variable(dims, data, attrs=attrs)
1567 
1568     @classmethod
1569     def concat(cls, variables, dim="concat_dim", positions=None, shortcut=False):
1570         """Concatenate variables along a new or existing dimension.
1571 
1572         Parameters
1573         ----------
1574         variables : iterable of Array
1575             Arrays to stack together. Each variable is expected to have
1576             matching dimensions and shape except for along the stacked
1577             dimension.
1578         dim : str or DataArray, optional
1579             Name of the dimension to stack along. This can either be a new
1580             dimension name, in which case it is added along axis=0, or an
1581             existing dimension name, in which case the location of the
1582             dimension is unchanged. Where to insert the new dimension is
1583             determined by the first variable.
1584         positions : None or list of integer arrays, optional
1585             List of integer arrays which specifies the integer positions to
1586             which to assign each dataset along the concatenated dimension.
1587             If not supplied, objects are concatenated in the provided order.
1588         shortcut : bool, optional
1589             This option is used internally to speed-up groupby operations.
1590             If `shortcut` is True, some checks of internal consistency between
1591             arrays to concatenate are skipped.
1592 
1593         Returns
1594         -------
1595         stacked : Variable
1596             Concatenated Variable formed by stacking all the supplied variables
1597             along the given dimension.
1598         """
1599         if not isinstance(dim, str):
1600             (dim,) = dim.dims
1601 
1602         # can't do this lazily: we need to loop through variables at least
1603         # twice
1604         variables = list(variables)
1605         first_var = variables[0]
1606 
1607         arrays = [v.data for v in variables]
1608 
1609         if dim in first_var.dims:
1610             axis = first_var.get_axis_num(dim)
1611             dims = first_var.dims
1612             data = duck_array_ops.concatenate(arrays, axis=axis)
1613             if positions is not None:
1614                 # TODO: deprecate this option -- we don't need it for groupby
1615                 # any more.
1616                 indices = nputils.inverse_permutation(np.concatenate(positions))
1617                 data = duck_array_ops.take(data, indices, axis=axis)
1618         else:
1619             axis = 0
1620             dims = (dim,) + first_var.dims
1621             data = duck_array_ops.stack(arrays, axis=axis)
1622 
1623         attrs = dict(first_var.attrs)
1624         encoding = dict(first_var.encoding)
1625         if not shortcut:
1626             for var in variables:
1627                 if var.dims != first_var.dims:
1628                     raise ValueError("inconsistent dimensions")
1629                 utils.remove_incompatible_items(attrs, var.attrs)
1630 
1631         return cls(dims, data, attrs, encoding)
1632 
1633     def equals(self, other, equiv=duck_array_ops.array_equiv):
1634         """True if two Variables have the same dimensions and values;
1635         otherwise False.
1636 
1637         Variables can still be equal (like pandas objects) if they have NaN
1638         values in the same locations.
1639 
1640         This method is necessary because `v1 == v2` for Variables
1641         does element-wise comparisons (like numpy.ndarrays).
1642         """
1643         other = getattr(other, "variable", other)
1644         try:
1645             return self.dims == other.dims and (
1646                 self._data is other._data or equiv(self.data, other.data)
1647             )
1648         except (TypeError, AttributeError):
1649             return False
1650 
1651     def broadcast_equals(self, other, equiv=duck_array_ops.array_equiv):
1652         """True if two Variables have the values after being broadcast against
1653         each other; otherwise False.
1654 
1655         Variables can still be equal (like pandas objects) if they have NaN
1656         values in the same locations.
1657         """
1658         try:
1659             self, other = broadcast_variables(self, other)
1660         except (ValueError, AttributeError):
1661             return False
1662         return self.equals(other, equiv=equiv)
1663 
1664     def identical(self, other, equiv=duck_array_ops.array_equiv):
1665         """Like equals, but also checks attributes.
1666         """
1667         try:
1668             return utils.dict_equiv(self.attrs, other.attrs) and self.equals(
1669                 other, equiv=equiv
1670             )
1671         except (TypeError, AttributeError):
1672             return False
1673 
1674     def no_conflicts(self, other, equiv=duck_array_ops.array_notnull_equiv):
1675         """True if the intersection of two Variable's non-null data is
1676         equal; otherwise false.
1677 
1678         Variables can thus still be equal if there are locations where either,
1679         or both, contain NaN values.
1680         """
1681         return self.broadcast_equals(other, equiv=equiv)
1682 
1683     def quantile(self, q, dim=None, interpolation="linear", keep_attrs=None):
1684         """Compute the qth quantile of the data along the specified dimension.
1685 
1686         Returns the qth quantiles(s) of the array elements.
1687 
1688         Parameters
1689         ----------
1690         q : float in range of [0,1] (or sequence of floats)
1691             Quantile to compute, which must be between 0 and 1
1692             inclusive.
1693         dim : str or sequence of str, optional
1694             Dimension(s) over which to apply quantile.
1695         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
1696             This optional parameter specifies the interpolation method to
1697             use when the desired quantile lies between two data points
1698             ``i < j``:
1699                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is
1700                   the fractional part of the index surrounded by ``i`` and
1701                   ``j``.
1702                 * lower: ``i``.
1703                 * higher: ``j``.
1704                 * nearest: ``i`` or ``j``, whichever is nearest.
1705                 * midpoint: ``(i + j) / 2``.
1706         keep_attrs : bool, optional
1707             If True, the variable's attributes (`attrs`) will be copied from
1708             the original object to the new one.  If False (default), the new
1709             object will be returned without attributes.
1710 
1711         Returns
1712         -------
1713         quantiles : Variable
1714             If `q` is a single quantile, then the result
1715             is a scalar. If multiple percentiles are given, first axis of
1716             the result corresponds to the quantile and a quantile dimension
1717             is added to the return array. The other dimensions are the
1718             dimensions that remain after the reduction of the array.
1719 
1720         See Also
1721         --------
1722         numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile,
1723         DataArray.quantile
1724         """
1725 
1726         from .computation import apply_ufunc
1727 
1728         if keep_attrs is None:
1729             keep_attrs = _get_keep_attrs(default=False)
1730 
1731         scalar = utils.is_scalar(q)
1732         q = np.atleast_1d(np.asarray(q, dtype=np.float64))
1733 
1734         if dim is None:
1735             dim = self.dims
1736 
1737         if utils.is_scalar(dim):
1738             dim = [dim]
1739 
1740         def _wrapper(npa, **kwargs):
1741             # move quantile axis to end. required for apply_ufunc
1742             return np.moveaxis(np.nanpercentile(npa, **kwargs), 0, -1)
1743 
1744         axis = np.arange(-1, -1 * len(dim) - 1, -1)
1745         result = apply_ufunc(
1746             _wrapper,
1747             self,
1748             input_core_dims=[dim],
1749             exclude_dims=set(dim),
1750             output_core_dims=[["quantile"]],
1751             output_dtypes=[np.float64],
1752             output_sizes={"quantile": len(q)},
1753             dask="parallelized",
1754             kwargs={"q": q * 100, "axis": axis, "interpolation": interpolation},
1755         )
1756 
1757         # for backward compatibility
1758         result = result.transpose("quantile", ...)
1759         if scalar:
1760             result = result.squeeze("quantile")
1761         if keep_attrs:
1762             result.attrs = self._attrs
1763         return result
1764 
1765     def rank(self, dim, pct=False):
1766         """Ranks the data.
1767 
1768         Equal values are assigned a rank that is the average of the ranks that
1769         would have been otherwise assigned to all of the values within that
1770         set.  Ranks begin at 1, not 0. If `pct`, computes percentage ranks.
1771 
1772         NaNs in the input array are returned as NaNs.
1773 
1774         The `bottleneck` library is required.
1775 
1776         Parameters
1777         ----------
1778         dim : str
1779             Dimension over which to compute rank.
1780         pct : bool, optional
1781             If True, compute percentage ranks, otherwise compute integer ranks.
1782 
1783         Returns
1784         -------
1785         ranked : Variable
1786 
1787         See Also
1788         --------
1789         Dataset.rank, DataArray.rank
1790         """
1791         import bottleneck as bn
1792 
1793         data = self.data
1794 
1795         if isinstance(data, dask_array_type):
1796             raise TypeError(
1797                 "rank does not work for arrays stored as dask "
1798                 "arrays. Load the data via .compute() or .load() "
1799                 "prior to calling this method."
1800             )
1801         elif not isinstance(data, np.ndarray):
1802             raise TypeError(
1803                 "rank is not implemented for {} objects.".format(type(data))
1804             )
1805 
1806         axis = self.get_axis_num(dim)
1807         func = bn.nanrankdata if self.dtype.kind == "f" else bn.rankdata
1808         ranked = func(data, axis=axis)
1809         if pct:
1810             count = np.sum(~np.isnan(data), axis=axis, keepdims=True)
1811             ranked /= count
1812         return Variable(self.dims, ranked)
1813 
1814     def rolling_window(
1815         self, dim, window, window_dim, center=False, fill_value=dtypes.NA
1816     ):
1817         """
1818         Make a rolling_window along dim and add a new_dim to the last place.
1819 
1820         Parameters
1821         ----------
1822         dim: str
1823             Dimension over which to compute rolling_window
1824         window: int
1825             Window size of the rolling
1826         window_dim: str
1827             New name of the window dimension.
1828         center: boolean. default False.
1829             If True, pad fill_value for both ends. Otherwise, pad in the head
1830             of the axis.
1831         fill_value:
1832             value to be filled.
1833 
1834         Returns
1835         -------
1836         Variable that is a view of the original array with a added dimension of
1837         size w.
1838         The return dim: self.dims + (window_dim, )
1839         The return shape: self.shape + (window, )
1840 
1841         Examples
1842         --------
1843         >>> v=Variable(('a', 'b'), np.arange(8).reshape((2,4)))
1844         >>> v.rolling_window(x, 'b', 3, 'window_dim')
1845         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
1846         array([[[nan, nan, 0], [nan, 0, 1], [0, 1, 2], [1, 2, 3]],
1847                [[nan, nan, 4], [nan, 4, 5], [4, 5, 6], [5, 6, 7]]])
1848 
1849         >>> v.rolling_window(x, 'b', 3, 'window_dim', center=True)
1850         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
1851         array([[[nan, 0, 1], [0, 1, 2], [1, 2, 3], [2, 3, nan]],
1852                [[nan, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, nan]]])
1853         """
1854         if fill_value is dtypes.NA:  # np.nan is passed
1855             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1856             array = self.astype(dtype, copy=False).data
1857         else:
1858             dtype = self.dtype
1859             array = self.data
1860 
1861         new_dims = self.dims + (window_dim,)
1862         return Variable(
1863             new_dims,
1864             duck_array_ops.rolling_window(
1865                 array,
1866                 axis=self.get_axis_num(dim),
1867                 window=window,
1868                 center=center,
1869                 fill_value=fill_value,
1870             ),
1871         )
1872 
1873     def coarsen(self, windows, func, boundary="exact", side="left", **kwargs):
1874         """
1875         Apply reduction function.
1876         """
1877         windows = {k: v for k, v in windows.items() if k in self.dims}
1878         if not windows:
1879             return self.copy()
1880 
1881         reshaped, axes = self._coarsen_reshape(windows, boundary, side)
1882         if isinstance(func, str):
1883             name = func
1884             func = getattr(duck_array_ops, name, None)
1885             if func is None:
1886                 raise NameError(f"{name} is not a valid method.")
1887         return self._replace(data=func(reshaped, axis=axes, **kwargs))
1888 
1889     def _coarsen_reshape(self, windows, boundary, side):
1890         """
1891         Construct a reshaped-array for coarsen
1892         """
1893         if not utils.is_dict_like(boundary):
1894             boundary = {d: boundary for d in windows.keys()}
1895 
1896         if not utils.is_dict_like(side):
1897             side = {d: side for d in windows.keys()}
1898 
1899         # remove unrelated dimensions
1900         boundary = {k: v for k, v in boundary.items() if k in windows}
1901         side = {k: v for k, v in side.items() if k in windows}
1902 
1903         for d, window in windows.items():
1904             if window <= 0:
1905                 raise ValueError(f"window must be > 0. Given {window}")
1906 
1907         variable = self
1908         for d, window in windows.items():
1909             # trim or pad the object
1910             size = variable.shape[self._get_axis_num(d)]
1911             n = int(size / window)
1912             if boundary[d] == "exact":
1913                 if n * window != size:
1914                     raise ValueError(
1915                         "Could not coarsen a dimension of size {} with "
1916                         "window {}".format(size, window)
1917                     )
1918             elif boundary[d] == "trim":
1919                 if side[d] == "left":
1920                     variable = variable.isel({d: slice(0, window * n)})
1921                 else:
1922                     excess = size - window * n
1923                     variable = variable.isel({d: slice(excess, None)})
1924             elif boundary[d] == "pad":  # pad
1925                 pad = window * n - size
1926                 if pad < 0:
1927                     pad += window
1928                 if side[d] == "left":
1929                     pad_widths = {d: (0, pad)}
1930                 else:
1931                     pad_widths = {d: (pad, 0)}
1932                 variable = variable.pad_with_fill_value(pad_widths)
1933             else:
1934                 raise TypeError(
1935                     "{} is invalid for boundary. Valid option is 'exact', "
1936                     "'trim' and 'pad'".format(boundary[d])
1937                 )
1938 
1939         shape = []
1940         axes = []
1941         axis_count = 0
1942         for i, d in enumerate(variable.dims):
1943             if d in windows:
1944                 size = variable.shape[i]
1945                 shape.append(int(size / windows[d]))
1946                 shape.append(windows[d])
1947                 axis_count += 1
1948                 axes.append(i + axis_count)
1949             else:
1950                 shape.append(variable.shape[i])
1951 
1952         return variable.data.reshape(shape), tuple(axes)
1953 
1954     @property
1955     def real(self):
1956         return type(self)(self.dims, self.data.real, self._attrs)
1957 
1958     @property
1959     def imag(self):
1960         return type(self)(self.dims, self.data.imag, self._attrs)
1961 
1962     def __array_wrap__(self, obj, context=None):
1963         return Variable(self.dims, obj)
1964 
1965     @staticmethod
1966     def _unary_op(f):
1967         @functools.wraps(f)
1968         def func(self, *args, **kwargs):
1969             with np.errstate(all="ignore"):
1970                 return self.__array_wrap__(f(self.data, *args, **kwargs))
1971 
1972         return func
1973 
1974     @staticmethod
1975     def _binary_op(f, reflexive=False, **ignored_kwargs):
1976         @functools.wraps(f)
1977         def func(self, other):
1978             if isinstance(other, (xr.DataArray, xr.Dataset)):
1979                 return NotImplemented
1980             self_data, other_data, dims = _broadcast_compat_data(self, other)
1981             keep_attrs = _get_keep_attrs(default=False)
1982             attrs = self._attrs if keep_attrs else None
1983             with np.errstate(all="ignore"):
1984                 new_data = (
1985                     f(self_data, other_data)
1986                     if not reflexive
1987                     else f(other_data, self_data)
1988                 )
1989             result = Variable(dims, new_data, attrs=attrs)
1990             return result
1991 
1992         return func
1993 
1994     @staticmethod
1995     def _inplace_binary_op(f):
1996         @functools.wraps(f)
1997         def func(self, other):
1998             if isinstance(other, xr.Dataset):
1999                 raise TypeError("cannot add a Dataset to a Variable in-place")
2000             self_data, other_data, dims = _broadcast_compat_data(self, other)
2001             if dims != self.dims:
2002                 raise ValueError("dimensions cannot change for in-place " "operations")
2003             with np.errstate(all="ignore"):
2004                 self.values = f(self_data, other_data)
2005             return self
2006 
2007         return func
2008 
2009     def _to_numeric(self, offset=None, datetime_unit=None, dtype=float):
2010         """ A (private) method to convert datetime array to numeric dtype
2011         See duck_array_ops.datetime_to_numeric
2012         """
2013         numeric_array = duck_array_ops.datetime_to_numeric(
2014             self.data, offset, datetime_unit, dtype
2015         )
2016         return type(self)(self.dims, numeric_array, self._attrs)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-17 15:36:05.262 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-17 15:36:05.262 | INFO     | Search-only: skipping patch selection
2025-10-17 15:36:05.328 | INFO     | Task pydata__xarray-3635 completed successfully.
2025-10-17 15:36:05.329 | INFO     | No patch generated. You can try running ACR again.
