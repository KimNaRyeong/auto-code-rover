[
    {
        "rel_file_path": "xarray/core/dataarray.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/dataarray.py",
        "start": 2935,
        "end": 3019,
        "class_name": "DataArray",
        "method_name": "quantile",
        "code": "2935     def quantile(\n2936         self,\n2937         q: Any,\n2938         dim: Union[Hashable, Sequence[Hashable], None] = None,\n2939         interpolation: str = \"linear\",\n2940         keep_attrs: bool = None,\n2941     ) -> \"DataArray\":\n2942         \"\"\"Compute the qth quantile of the data along the specified dimension.\n2943 \n2944         Returns the qth quantiles(s) of the array elements.\n2945 \n2946         Parameters\n2947         ----------\n2948         q : float in range of [0,1] or array-like of floats\n2949             Quantile to compute, which must be between 0 and 1 inclusive.\n2950         dim : hashable or sequence of hashable, optional\n2951             Dimension(s) over which to apply quantile.\n2952         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n2953             This optional parameter specifies the interpolation method to\n2954             use when the desired quantile lies between two data points\n2955             ``i < j``:\n2956 \n2957                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is\n2958                   the fractional part of the index surrounded by ``i`` and\n2959                   ``j``.\n2960                 - lower: ``i``.\n2961                 - higher: ``j``.\n2962                 - nearest: ``i`` or ``j``, whichever is nearest.\n2963                 - midpoint: ``(i + j) / 2``.\n2964         keep_attrs : bool, optional\n2965             If True, the dataset's attributes (`attrs`) will be copied from\n2966             the original object to the new one.  If False (default), the new\n2967             object will be returned without attributes.\n2968 \n2969         Returns\n2970         -------\n2971         quantiles : DataArray\n2972             If `q` is a single quantile, then the result\n2973             is a scalar. If multiple percentiles are given, first axis of\n2974             the result corresponds to the quantile and a quantile dimension\n2975             is added to the return array. The other dimensions are the\n2976             dimensions that remain after the reduction of the array.\n2977 \n2978         See Also\n2979         --------\n2980         numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile\n2981 \n2982         Examples\n2983         --------\n2984 \n2985         >>> da = xr.DataArray(\n2986         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],\n2987         ...     coords={\"x\": [7, 9], \"y\": [1, 1.5, 2, 2.5]},\n2988         ...     dims=(\"x\", \"y\"),\n2989         ... )\n2990         >>> da.quantile(0)  # or da.quantile(0, dim=...)\n2991         <xarray.DataArray ()>\n2992         array(0.7)\n2993         Coordinates:\n2994             quantile  float64 0.0\n2995         >>> da.quantile(0, dim=\"x\")\n2996         <xarray.DataArray (y: 4)>\n2997         array([0.7, 4.2, 2.6, 1.5])\n2998         Coordinates:\n2999           * y         (y) float64 1.0 1.5 2.0 2.5\n3000             quantile  float64 0.0\n3001         >>> da.quantile([0, 0.5, 1])\n3002         <xarray.DataArray (quantile: 3)>\n3003         array([0.7, 3.4, 9.4])\n3004         Coordinates:\n3005           * quantile  (quantile) float64 0.0 0.5 1.0\n3006         >>> da.quantile([0, 0.5, 1], dim=\"x\")\n3007         <xarray.DataArray (quantile: 3, y: 4)>\n3008         array([[0.7 , 4.2 , 2.6 , 1.5 ],\n3009                [3.6 , 5.75, 6.  , 1.7 ],\n3010                [6.5 , 7.3 , 9.4 , 1.9 ]])\n3011         Coordinates:\n3012           * y         (y) float64 1.0 1.5 2.0 2.5\n3013           * quantile  (quantile) float64 0.0 0.5 1.0\n3014         \"\"\"\n3015 \n3016         ds = self._to_temp_dataset().quantile(\n3017             q, dim=dim, keep_attrs=keep_attrs, interpolation=interpolation\n3018         )\n3019         return self._from_temp_dataset(ds)\n",
        "intended_behavior": "The `quantile` method should validate the `q` parameter to ensure it falls within the 0 to 1 range, corresponding to quantiles, not percentiles. If a value outside this range is detected, the method should raise a `ValueError` with a corrected message: `\"Quantiles must be in the range [0, 1]\"`. This adjustment ensures users receive accurate feedback aligned with the method's expected inputs, preserving the existing functionality while correcting the misleading error message."
    },
    {
        "rel_file_path": "xarray/core/dataarray.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/dataarray.py",
        "start": 214,
        "end": 3243,
        "class_name": "DataArray",
        "method_name": null,
        "code": "214 class DataArray(AbstractArray, DataWithCoords):\n215     \"\"\"N-dimensional array with labeled coordinates and dimensions.\n216 \n217     DataArray provides a wrapper around numpy ndarrays that uses labeled\n218     dimensions and coordinates to support metadata aware operations. The API is\n219     similar to that for the pandas Series or DataFrame, but DataArray objects\n220     can have any number of dimensions, and their contents have fixed data\n221     types.\n222 \n223     Additional features over raw numpy arrays:\n224 \n225     - Apply operations over dimensions by name: ``x.sum('time')``.\n226     - Select or assign values by integer location (like numpy): ``x[:10]``\n227       or by label (like pandas): ``x.loc['2014-01-01']`` or\n228       ``x.sel(time='2014-01-01')``.\n229     - Mathematical operations (e.g., ``x - y``) vectorize across multiple\n230       dimensions (known in numpy as \"broadcasting\") based on dimension names,\n231       regardless of their original order.\n232     - Keep track of arbitrary metadata in the form of a Python dictionary:\n233       ``x.attrs``\n234     - Convert to a pandas Series: ``x.to_series()``.\n235 \n236     Getting items from or doing mathematical operations with a DataArray\n237     always returns another DataArray.\n238     \"\"\"\n239 \n240     _cache: Dict[str, Any]\n241     _coords: Dict[Any, Variable]\n242     _indexes: Optional[Dict[Hashable, pd.Index]]\n243     _name: Optional[Hashable]\n244     _variable: Variable\n245 \n246     __slots__ = (\n247         \"_cache\",\n248         \"_coords\",\n249         \"_file_obj\",\n250         \"_indexes\",\n251         \"_name\",\n252         \"_variable\",\n253         \"__weakref__\",\n254     )\n255 \n256     _groupby_cls = groupby.DataArrayGroupBy\n257     _rolling_cls = rolling.DataArrayRolling\n258     _coarsen_cls = rolling.DataArrayCoarsen\n259     _resample_cls = resample.DataArrayResample\n260 \n261     dt = property(DatetimeAccessor)\n262 \n263     def __init__(\n264         self,\n265         data: Any = dtypes.NA,\n266         coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None] = None,\n267         dims: Union[Hashable, Sequence[Hashable], None] = None,\n268         name: Hashable = None,\n269         attrs: Mapping = None,\n270         # deprecated parameters\n271         encoding=None,\n272         # internal parameters\n273         indexes: Dict[Hashable, pd.Index] = None,\n274         fastpath: bool = False,\n275     ):\n276         \"\"\"\n277         Parameters\n278         ----------\n279         data : array_like\n280             Values for this array. Must be an ``numpy.ndarray``, ndarray like,\n281             or castable to an ``ndarray``. If a self-described xarray or pandas\n282             object, attempts are made to use this array's metadata to fill in\n283             other unspecified arguments. A view of the array's data is used\n284             instead of a copy if possible.\n285         coords : sequence or dict of array_like objects, optional\n286             Coordinates (tick labels) to use for indexing along each dimension.\n287             The following notations are accepted:\n288 \n289             - mapping {dimension name: array-like}\n290             - sequence of tuples that are valid arguments for xarray.Variable()\n291               - (dims, data)\n292               - (dims, data, attrs)\n293               - (dims, data, attrs, encoding)\n294 \n295             Additionally, it is possible to define a coord whose name\n296             does not match the dimension name, or a coord based on multiple\n297             dimensions, with one of the following notations:\n298 \n299             - mapping {coord name: DataArray}\n300             - mapping {coord name: Variable}\n301             - mapping {coord name: (dimension name, array-like)}\n302             - mapping {coord name: (tuple of dimension names, array-like)}\n303 \n304         dims : hashable or sequence of hashable, optional\n305             Name(s) of the data dimension(s). Must be either a hashable (only\n306             for 1D data) or a sequence of hashables with length equal to the\n307             number of dimensions. If this argument is omitted, dimension names\n308             are taken from ``coords`` (if possible) and otherwise default to\n309             ``['dim_0', ... 'dim_n']``.\n310         name : str or None, optional\n311             Name of this array.\n312         attrs : dict_like or None, optional\n313             Attributes to assign to the new instance. By default, an empty\n314             attribute dictionary is initialized.\n315         \"\"\"\n316         if encoding is not None:\n317             warnings.warn(\n318                 \"The `encoding` argument to `DataArray` is deprecated, and . \"\n319                 \"will be removed in 0.15. \"\n320                 \"Instead, specify the encoding when writing to disk or \"\n321                 \"set the `encoding` attribute directly.\",\n322                 FutureWarning,\n323                 stacklevel=2,\n324             )\n325         if fastpath:\n326             variable = data\n327             assert dims is None\n328             assert attrs is None\n329             assert encoding is None\n330         else:\n331             # try to fill in arguments from data if they weren't supplied\n332             if coords is None:\n333 \n334                 if isinstance(data, DataArray):\n335                     coords = data.coords\n336                 elif isinstance(data, pd.Series):\n337                     coords = [data.index]\n338                 elif isinstance(data, pd.DataFrame):\n339                     coords = [data.index, data.columns]\n340                 elif isinstance(data, (pd.Index, IndexVariable)):\n341                     coords = [data]\n342                 elif isinstance(data, pdcompat.Panel):\n343                     coords = [data.items, data.major_axis, data.minor_axis]\n344 \n345             if dims is None:\n346                 dims = getattr(data, \"dims\", getattr(coords, \"dims\", None))\n347             if name is None:\n348                 name = getattr(data, \"name\", None)\n349             if attrs is None and not isinstance(data, PANDAS_TYPES):\n350                 attrs = getattr(data, \"attrs\", None)\n351             if encoding is None:\n352                 encoding = getattr(data, \"encoding\", None)\n353 \n354             data = _check_data_shape(data, coords, dims)\n355             data = as_compatible_data(data)\n356             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)\n357             variable = Variable(dims, data, attrs, encoding, fastpath=True)\n358             indexes = dict(\n359                 _extract_indexes_from_coords(coords)\n360             )  # needed for to_dataset\n361 \n362         # These fully describe a DataArray\n363         self._variable = variable\n364         assert isinstance(coords, dict)\n365         self._coords = coords\n366         self._name = name\n367 \n368         # TODO(shoyer): document this argument, once it becomes part of the\n369         # public interface.\n370         self._indexes = indexes\n371 \n372         self._file_obj = None\n373 \n374     def _replace(\n375         self,\n376         variable: Variable = None,\n377         coords=None,\n378         name: Union[Hashable, None, Default] = _default,\n379         indexes=None,\n380     ) -> \"DataArray\":\n381         if variable is None:\n382             variable = self.variable\n383         if coords is None:\n384             coords = self._coords\n385         if name is _default:\n386             name = self.name\n387         return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)\n388 \n389     def _replace_maybe_drop_dims(\n390         self, variable: Variable, name: Union[Hashable, None, Default] = _default\n391     ) -> \"DataArray\":\n392         if variable.dims == self.dims and variable.shape == self.shape:\n393             coords = self._coords.copy()\n394             indexes = self._indexes\n395         elif variable.dims == self.dims:\n396             # Shape has changed (e.g. from reduce(..., keepdims=True)\n397             new_sizes = dict(zip(self.dims, variable.shape))\n398             coords = {\n399                 k: v\n400                 for k, v in self._coords.items()\n401                 if v.shape == tuple(new_sizes[d] for d in v.dims)\n402             }\n403             changed_dims = [\n404                 k for k in variable.dims if variable.sizes[k] != self.sizes[k]\n405             ]\n406             indexes = propagate_indexes(self._indexes, exclude=changed_dims)\n407         else:\n408             allowed_dims = set(variable.dims)\n409             coords = {\n410                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims\n411             }\n412             indexes = propagate_indexes(\n413                 self._indexes, exclude=(set(self.dims) - allowed_dims)\n414             )\n415         return self._replace(variable, coords, name, indexes=indexes)\n416 \n417     def _overwrite_indexes(self, indexes: Mapping[Hashable, Any]) -> \"DataArray\":\n418         if not len(indexes):\n419             return self\n420         coords = self._coords.copy()\n421         for name, idx in indexes.items():\n422             coords[name] = IndexVariable(name, idx)\n423         obj = self._replace(coords=coords)\n424 \n425         # switch from dimension to level names, if necessary\n426         dim_names: Dict[Any, str] = {}\n427         for dim, idx in indexes.items():\n428             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:\n429                 dim_names[dim] = idx.name\n430         if dim_names:\n431             obj = obj.rename(dim_names)\n432         return obj\n433 \n434     def _to_temp_dataset(self) -> Dataset:\n435         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)\n436 \n437     def _from_temp_dataset(\n438         self, dataset: Dataset, name: Hashable = _default\n439     ) -> \"DataArray\":\n440         variable = dataset._variables.pop(_THIS_ARRAY)\n441         coords = dataset._variables\n442         indexes = dataset._indexes\n443         return self._replace(variable, coords, name, indexes=indexes)\n444 \n445     def _to_dataset_split(self, dim: Hashable) -> Dataset:\n446         \"\"\" splits dataarray along dimension 'dim' \"\"\"\n447 \n448         def subset(dim, label):\n449             array = self.loc[{dim: label}]\n450             array.attrs = {}\n451             return as_variable(array)\n452 \n453         variables = {label: subset(dim, label) for label in self.get_index(dim)}\n454         variables.update({k: v for k, v in self._coords.items() if k != dim})\n455         indexes = propagate_indexes(self._indexes, exclude=dim)\n456         coord_names = set(self._coords) - set([dim])\n457         dataset = Dataset._construct_direct(\n458             variables, coord_names, indexes=indexes, attrs=self.attrs\n459         )\n460         return dataset\n461 \n462     def _to_dataset_whole(\n463         self, name: Hashable = None, shallow_copy: bool = True\n464     ) -> Dataset:\n465         if name is None:\n466             name = self.name\n467         if name is None:\n468             raise ValueError(\n469                 \"unable to convert unnamed DataArray to a \"\n470                 \"Dataset without providing an explicit name\"\n471             )\n472         if name in self.coords:\n473             raise ValueError(\n474                 \"cannot create a Dataset from a DataArray with \"\n475                 \"the same name as one of its coordinates\"\n476             )\n477         # use private APIs for speed: this is called by _to_temp_dataset(),\n478         # which is used in the guts of a lot of operations (e.g., reindex)\n479         variables = self._coords.copy()\n480         variables[name] = self.variable\n481         if shallow_copy:\n482             for k in variables:\n483                 variables[k] = variables[k].copy(deep=False)\n484         indexes = self._indexes\n485 \n486         coord_names = set(self._coords)\n487         dataset = Dataset._construct_direct(variables, coord_names, indexes=indexes)\n488         return dataset\n489 \n490     def to_dataset(self, dim: Hashable = None, *, name: Hashable = None) -> Dataset:\n491         \"\"\"Convert a DataArray to a Dataset.\n492 \n493         Parameters\n494         ----------\n495         dim : hashable, optional\n496             Name of the dimension on this array along which to split this array\n497             into separate variables. If not provided, this array is converted\n498             into a Dataset of one variable.\n499         name : hashable, optional\n500             Name to substitute for this array's name. Only valid if ``dim`` is\n501             not provided.\n502 \n503         Returns\n504         -------\n505         dataset : Dataset\n506         \"\"\"\n507         if dim is not None and dim not in self.dims:\n508             raise TypeError(\n509                 f\"{dim} is not a dim. If supplying a ``name``, pass as a kwarg.\"\n510             )\n511 \n512         if dim is not None:\n513             if name is not None:\n514                 raise TypeError(\"cannot supply both dim and name arguments\")\n515             return self._to_dataset_split(dim)\n516         else:\n517             return self._to_dataset_whole(name)\n518 \n519     @property\n520     def name(self) -> Optional[Hashable]:\n521         \"\"\"The name of this array.\n522         \"\"\"\n523         return self._name\n524 \n525     @name.setter\n526     def name(self, value: Optional[Hashable]) -> None:\n527         self._name = value\n528 \n529     @property\n530     def variable(self) -> Variable:\n531         \"\"\"Low level interface to the Variable object for this DataArray.\"\"\"\n532         return self._variable\n533 \n534     @property\n535     def dtype(self) -> np.dtype:\n536         return self.variable.dtype\n537 \n538     @property\n539     def shape(self) -> Tuple[int, ...]:\n540         return self.variable.shape\n541 \n542     @property\n543     def size(self) -> int:\n544         return self.variable.size\n545 \n546     @property\n547     def nbytes(self) -> int:\n548         return self.variable.nbytes\n549 \n550     @property\n551     def ndim(self) -> int:\n552         return self.variable.ndim\n553 \n554     def __len__(self) -> int:\n555         return len(self.variable)\n556 \n557     @property\n558     def data(self) -> Any:\n559         \"\"\"The array's data as a dask or numpy array\n560         \"\"\"\n561         return self.variable.data\n562 \n563     @data.setter\n564     def data(self, value: Any) -> None:\n565         self.variable.data = value\n566 \n567     @property\n568     def values(self) -> np.ndarray:\n569         \"\"\"The array's data as a numpy.ndarray\"\"\"\n570         return self.variable.values\n571 \n572     @values.setter\n573     def values(self, value: Any) -> None:\n574         self.variable.values = value\n575 \n576     @property\n577     def _in_memory(self) -> bool:\n578         return self.variable._in_memory\n579 \n580     def to_index(self) -> pd.Index:\n581         \"\"\"Convert this variable to a pandas.Index. Only possible for 1D\n582         arrays.\n583         \"\"\"\n584         return self.variable.to_index()\n585 \n586     @property\n587     def dims(self) -> Tuple[Hashable, ...]:\n588         \"\"\"Tuple of dimension names associated with this array.\n589 \n590         Note that the type of this property is inconsistent with\n591         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for\n592         consistently named properties.\n593         \"\"\"\n594         return self.variable.dims\n595 \n596     @dims.setter\n597     def dims(self, value):\n598         raise AttributeError(\n599             \"you cannot assign dims on a DataArray. Use \"\n600             \".rename() or .swap_dims() instead.\"\n601         )\n602 \n603     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:\n604         if utils.is_dict_like(key):\n605             return key\n606         else:\n607             key = indexing.expanded_indexer(key, self.ndim)\n608             return dict(zip(self.dims, key))\n609 \n610     @property\n611     def _level_coords(self) -> Dict[Hashable, Hashable]:\n612         \"\"\"Return a mapping of all MultiIndex levels and their corresponding\n613         coordinate name.\n614         \"\"\"\n615         level_coords: Dict[Hashable, Hashable] = {}\n616 \n617         for cname, var in self._coords.items():\n618             if var.ndim == 1 and isinstance(var, IndexVariable):\n619                 level_names = var.level_names\n620                 if level_names is not None:\n621                     (dim,) = var.dims\n622                     level_coords.update({lname: dim for lname in level_names})\n623         return level_coords\n624 \n625     def _getitem_coord(self, key):\n626         from .dataset import _get_virtual_variable\n627 \n628         try:\n629             var = self._coords[key]\n630         except KeyError:\n631             dim_sizes = dict(zip(self.dims, self.shape))\n632             _, key, var = _get_virtual_variable(\n633                 self._coords, key, self._level_coords, dim_sizes\n634             )\n635 \n636         return self._replace_maybe_drop_dims(var, name=key)\n637 \n638     def __getitem__(self, key: Any) -> \"DataArray\":\n639         if isinstance(key, str):\n640             return self._getitem_coord(key)\n641         else:\n642             # xarray-style array indexing\n643             return self.isel(indexers=self._item_key_to_dict(key))\n644 \n645     def __setitem__(self, key: Any, value: Any) -> None:\n646         if isinstance(key, str):\n647             self.coords[key] = value\n648         else:\n649             # Coordinates in key, value and self[key] should be consistent.\n650             # TODO Coordinate consistency in key is checked here, but it\n651             # causes unnecessary indexing. It should be optimized.\n652             obj = self[key]\n653             if isinstance(value, DataArray):\n654                 assert_coordinate_consistent(value, obj.coords.variables)\n655             # DataArray key -> Variable key\n656             key = {\n657                 k: v.variable if isinstance(v, DataArray) else v\n658                 for k, v in self._item_key_to_dict(key).items()\n659             }\n660             self.variable[key] = value\n661 \n662     def __delitem__(self, key: Any) -> None:\n663         del self.coords[key]\n664 \n665     @property\n666     def _attr_sources(self) -> List[Mapping[Hashable, Any]]:\n667         \"\"\"List of places to look-up items for attribute-style access\n668         \"\"\"\n669         return self._item_sources + [self.attrs]\n670 \n671     @property\n672     def _item_sources(self) -> List[Mapping[Hashable, Any]]:\n673         \"\"\"List of places to look-up items for key-completion\n674         \"\"\"\n675         return [\n676             self.coords,\n677             {d: self.coords[d] for d in self.dims},\n678             LevelCoordinatesSource(self),\n679         ]\n680 \n681     def __contains__(self, key: Any) -> bool:\n682         return key in self.data\n683 \n684     @property\n685     def loc(self) -> _LocIndexer:\n686         \"\"\"Attribute for location based indexing like pandas.\n687         \"\"\"\n688         return _LocIndexer(self)\n689 \n690     @property\n691     def attrs(self) -> Dict[Hashable, Any]:\n692         \"\"\"Dictionary storing arbitrary metadata with this array.\"\"\"\n693         return self.variable.attrs\n694 \n695     @attrs.setter\n696     def attrs(self, value: Mapping[Hashable, Any]) -> None:\n697         # Disable type checking to work around mypy bug - see mypy#4167\n698         self.variable.attrs = value  # type: ignore\n699 \n700     @property\n701     def encoding(self) -> Dict[Hashable, Any]:\n702         \"\"\"Dictionary of format-specific settings for how this array should be\n703         serialized.\"\"\"\n704         return self.variable.encoding\n705 \n706     @encoding.setter\n707     def encoding(self, value: Mapping[Hashable, Any]) -> None:\n708         self.variable.encoding = value\n709 \n710     @property\n711     def indexes(self) -> Indexes:\n712         \"\"\"Mapping of pandas.Index objects used for label based indexing\n713         \"\"\"\n714         if self._indexes is None:\n715             self._indexes = default_indexes(self._coords, self.dims)\n716         return Indexes(self._indexes)\n717 \n718     @property\n719     def coords(self) -> DataArrayCoordinates:\n720         \"\"\"Dictionary-like container of coordinate arrays.\n721         \"\"\"\n722         return DataArrayCoordinates(self)\n723 \n724     def reset_coords(\n725         self,\n726         names: Union[Iterable[Hashable], Hashable, None] = None,\n727         drop: bool = False,\n728         inplace: bool = None,\n729     ) -> Union[None, \"DataArray\", Dataset]:\n730         \"\"\"Given names of coordinates, reset them to become variables.\n731 \n732         Parameters\n733         ----------\n734         names : hashable or iterable of hashables, optional\n735             Name(s) of non-index coordinates in this dataset to reset into\n736             variables. By default, all non-index coordinates are reset.\n737         drop : bool, optional\n738             If True, remove coordinates instead of converting them into\n739             variables.\n740 \n741         Returns\n742         -------\n743         Dataset, or DataArray if ``drop == True``\n744         \"\"\"\n745         _check_inplace(inplace)\n746         if names is None:\n747             names = set(self.coords) - set(self.dims)\n748         dataset = self.coords.to_dataset().reset_coords(names, drop)\n749         if drop:\n750             return self._replace(coords=dataset._variables)\n751         else:\n752             if self.name is None:\n753                 raise ValueError(\n754                     \"cannot reset_coords with drop=False on an unnamed DataArrray\"\n755                 )\n756             dataset[self.name] = self.variable\n757             return dataset\n758 \n759     def __dask_tokenize__(self):\n760         from dask.base import normalize_token\n761 \n762         return normalize_token((type(self), self._variable, self._coords, self._name))\n763 \n764     def __dask_graph__(self):\n765         return self._to_temp_dataset().__dask_graph__()\n766 \n767     def __dask_keys__(self):\n768         return self._to_temp_dataset().__dask_keys__()\n769 \n770     def __dask_layers__(self):\n771         return self._to_temp_dataset().__dask_layers__()\n772 \n773     @property\n774     def __dask_optimize__(self):\n775         return self._to_temp_dataset().__dask_optimize__\n776 \n777     @property\n778     def __dask_scheduler__(self):\n779         return self._to_temp_dataset().__dask_scheduler__\n780 \n781     def __dask_postcompute__(self):\n782         func, args = self._to_temp_dataset().__dask_postcompute__()\n783         return self._dask_finalize, (func, args, self.name)\n784 \n785     def __dask_postpersist__(self):\n786         func, args = self._to_temp_dataset().__dask_postpersist__()\n787         return self._dask_finalize, (func, args, self.name)\n788 \n789     @staticmethod\n790     def _dask_finalize(results, func, args, name):\n791         ds = func(results, *args)\n792         variable = ds._variables.pop(_THIS_ARRAY)\n793         coords = ds._variables\n794         return DataArray(variable, coords, name=name, fastpath=True)\n795 \n796     def load(self, **kwargs) -> \"DataArray\":\n797         \"\"\"Manually trigger loading of this array's data from disk or a\n798         remote source into memory and return this array.\n799 \n800         Normally, it should not be necessary to call this method in user code,\n801         because all xarray functions should either work on deferred data or\n802         load data automatically. However, this method can be necessary when\n803         working with many file objects on disk.\n804 \n805         Parameters\n806         ----------\n807         **kwargs : dict\n808             Additional keyword arguments passed on to ``dask.array.compute``.\n809 \n810         See Also\n811         --------\n812         dask.array.compute\n813         \"\"\"\n814         ds = self._to_temp_dataset().load(**kwargs)\n815         new = self._from_temp_dataset(ds)\n816         self._variable = new._variable\n817         self._coords = new._coords\n818         return self\n819 \n820     def compute(self, **kwargs) -> \"DataArray\":\n821         \"\"\"Manually trigger loading of this array's data from disk or a\n822         remote source into memory and return a new array. The original is\n823         left unaltered.\n824 \n825         Normally, it should not be necessary to call this method in user code,\n826         because all xarray functions should either work on deferred data or\n827         load data automatically. However, this method can be necessary when\n828         working with many file objects on disk.\n829 \n830         Parameters\n831         ----------\n832         **kwargs : dict\n833             Additional keyword arguments passed on to ``dask.array.compute``.\n834 \n835         See Also\n836         --------\n837         dask.array.compute\n838         \"\"\"\n839         new = self.copy(deep=False)\n840         return new.load(**kwargs)\n841 \n842     def persist(self, **kwargs) -> \"DataArray\":\n843         \"\"\" Trigger computation in constituent dask arrays\n844 \n845         This keeps them as dask arrays but encourages them to keep data in\n846         memory.  This is particularly useful when on a distributed machine.\n847         When on a single machine consider using ``.compute()`` instead.\n848 \n849         Parameters\n850         ----------\n851         **kwargs : dict\n852             Additional keyword arguments passed on to ``dask.persist``.\n853 \n854         See Also\n855         --------\n856         dask.persist\n857         \"\"\"\n858         ds = self._to_temp_dataset().persist(**kwargs)\n859         return self._from_temp_dataset(ds)\n860 \n861     def copy(self, deep: bool = True, data: Any = None) -> \"DataArray\":\n862         \"\"\"Returns a copy of this array.\n863 \n864         If `deep=True`, a deep copy is made of the data array.\n865         Otherwise, a shallow copy is made, so each variable in the new\n866         array's dataset is also a variable in this array's dataset.\n867 \n868         Use `data` to create a new object with the same structure as\n869         original but entirely new data.\n870 \n871         Parameters\n872         ----------\n873         deep : bool, optional\n874             Whether the data array and its coordinates are loaded into memory\n875             and copied onto the new object. Default is True.\n876         data : array_like, optional\n877             Data to use in the new object. Must have same shape as original.\n878             When `data` is used, `deep` is ignored for all data variables,\n879             and only used for coords.\n880 \n881         Returns\n882         -------\n883         object : DataArray\n884             New object with dimensions, attributes, coordinates, name,\n885             encoding, and optionally data copied from original.\n886 \n887         Examples\n888         --------\n889 \n890         Shallow versus deep copy\n891 \n892         >>> array = xr.DataArray([1, 2, 3], dims='x',\n893         ...                      coords={'x': ['a', 'b', 'c']})\n894         >>> array.copy()\n895         <xarray.DataArray (x: 3)>\n896         array([1, 2, 3])\n897         Coordinates:\n898         * x        (x) <U1 'a' 'b' 'c'\n899         >>> array_0 = array.copy(deep=False)\n900         >>> array_0[0] = 7\n901         >>> array_0\n902         <xarray.DataArray (x: 3)>\n903         array([7, 2, 3])\n904         Coordinates:\n905         * x        (x) <U1 'a' 'b' 'c'\n906         >>> array\n907         <xarray.DataArray (x: 3)>\n908         array([7, 2, 3])\n909         Coordinates:\n910         * x        (x) <U1 'a' 'b' 'c'\n911 \n912         Changing the data using the ``data`` argument maintains the\n913         structure of the original object, but with the new data. Original\n914         object is unaffected.\n915 \n916         >>> array.copy(data=[0.1, 0.2, 0.3])\n917         <xarray.DataArray (x: 3)>\n918         array([ 0.1,  0.2,  0.3])\n919         Coordinates:\n920         * x        (x) <U1 'a' 'b' 'c'\n921         >>> array\n922         <xarray.DataArray (x: 3)>\n923         array([1, 2, 3])\n924         Coordinates:\n925         * x        (x) <U1 'a' 'b' 'c'\n926 \n927         See Also\n928         --------\n929         pandas.DataFrame.copy\n930         \"\"\"\n931         variable = self.variable.copy(deep=deep, data=data)\n932         coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}\n933         indexes = self._indexes\n934         return self._replace(variable, coords, indexes=indexes)\n935 \n936     def __copy__(self) -> \"DataArray\":\n937         return self.copy(deep=False)\n938 \n939     def __deepcopy__(self, memo=None) -> \"DataArray\":\n940         # memo does nothing but is required for compatibility with\n941         # copy.deepcopy\n942         return self.copy(deep=True)\n943 \n944     # mutable objects should not be hashable\n945     # https://github.com/python/mypy/issues/4266\n946     __hash__ = None  # type: ignore\n947 \n948     @property\n949     def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:\n950         \"\"\"Block dimensions for this array's data or None if it's not a dask\n951         array.\n952         \"\"\"\n953         return self.variable.chunks\n954 \n955     def chunk(\n956         self,\n957         chunks: Union[\n958             None,\n959             Number,\n960             Tuple[Number, ...],\n961             Tuple[Tuple[Number, ...], ...],\n962             Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]],\n963         ] = None,\n964         name_prefix: str = \"xarray-\",\n965         token: str = None,\n966         lock: bool = False,\n967     ) -> \"DataArray\":\n968         \"\"\"Coerce this array's data into a dask arrays with the given chunks.\n969 \n970         If this variable is a non-dask array, it will be converted to dask\n971         array. If it's a dask array, it will be rechunked to the given chunk\n972         sizes.\n973 \n974         If neither chunks is not provided for one or more dimensions, chunk\n975         sizes along that dimension will not be updated; non-dask arrays will be\n976         converted into dask arrays with a single block.\n977 \n978         Parameters\n979         ----------\n980         chunks : int, tuple or mapping, optional\n981             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or\n982             ``{'x': 5, 'y': 5}``.\n983         name_prefix : str, optional\n984             Prefix for the name of the new dask array.\n985         token : str, optional\n986             Token uniquely identifying this array.\n987         lock : optional\n988             Passed on to :py:func:`dask.array.from_array`, if the array is not\n989             already as dask array.\n990 \n991         Returns\n992         -------\n993         chunked : xarray.DataArray\n994         \"\"\"\n995         if isinstance(chunks, (tuple, list)):\n996             chunks = dict(zip(self.dims, chunks))\n997 \n998         ds = self._to_temp_dataset().chunk(\n999             chunks, name_prefix=name_prefix, token=token, lock=lock\n1000         )\n1001         return self._from_temp_dataset(ds)\n1002 \n1003     def isel(\n1004         self,\n1005         indexers: Mapping[Hashable, Any] = None,\n1006         drop: bool = False,\n1007         **indexers_kwargs: Any,\n1008     ) -> \"DataArray\":\n1009         \"\"\"Return a new DataArray whose data is given by integer indexing\n1010         along the specified dimension(s).\n1011 \n1012         See Also\n1013         --------\n1014         Dataset.isel\n1015         DataArray.sel\n1016         \"\"\"\n1017         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"isel\")\n1018         if any(is_fancy_indexer(idx) for idx in indexers.values()):\n1019             ds = self._to_temp_dataset()._isel_fancy(indexers, drop=drop)\n1020             return self._from_temp_dataset(ds)\n1021 \n1022         # Much faster algorithm for when all indexers are ints, slices, one-dimensional\n1023         # lists, or zero or one-dimensional np.ndarray's\n1024 \n1025         variable = self._variable.isel(indexers)\n1026 \n1027         coords = {}\n1028         for coord_name, coord_value in self._coords.items():\n1029             coord_indexers = {\n1030                 k: v for k, v in indexers.items() if k in coord_value.dims\n1031             }\n1032             if coord_indexers:\n1033                 coord_value = coord_value.isel(coord_indexers)\n1034                 if drop and coord_value.ndim == 0:\n1035                     continue\n1036             coords[coord_name] = coord_value\n1037 \n1038         return self._replace(variable=variable, coords=coords)\n1039 \n1040     def sel(\n1041         self,\n1042         indexers: Mapping[Hashable, Any] = None,\n1043         method: str = None,\n1044         tolerance=None,\n1045         drop: bool = False,\n1046         **indexers_kwargs: Any,\n1047     ) -> \"DataArray\":\n1048         \"\"\"Return a new DataArray whose data is given by selecting index\n1049         labels along the specified dimension(s).\n1050 \n1051         .. warning::\n1052 \n1053           Do not try to assign values when using any of the indexing methods\n1054           ``isel`` or ``sel``::\n1055 \n1056             da = xr.DataArray([0, 1, 2, 3], dims=['x'])\n1057             # DO NOT do this\n1058             da.isel(x=[0, 1, 2])[1] = -1\n1059 \n1060           Assigning values with the chained indexing using ``.sel`` or\n1061           ``.isel`` fails silently.\n1062 \n1063         See Also\n1064         --------\n1065         Dataset.sel\n1066         DataArray.isel\n1067 \n1068         \"\"\"\n1069         ds = self._to_temp_dataset().sel(\n1070             indexers=indexers,\n1071             drop=drop,\n1072             method=method,\n1073             tolerance=tolerance,\n1074             **indexers_kwargs,\n1075         )\n1076         return self._from_temp_dataset(ds)\n1077 \n1078     def head(\n1079         self,\n1080         indexers: Union[Mapping[Hashable, int], int] = None,\n1081         **indexers_kwargs: Any,\n1082     ) -> \"DataArray\":\n1083         \"\"\"Return a new DataArray whose data is given by the the first `n`\n1084         values along the specified dimension(s). Default `n` = 5\n1085 \n1086         See Also\n1087         --------\n1088         Dataset.head\n1089         DataArray.tail\n1090         DataArray.thin\n1091         \"\"\"\n1092         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)\n1093         return self._from_temp_dataset(ds)\n1094 \n1095     def tail(\n1096         self,\n1097         indexers: Union[Mapping[Hashable, int], int] = None,\n1098         **indexers_kwargs: Any,\n1099     ) -> \"DataArray\":\n1100         \"\"\"Return a new DataArray whose data is given by the the last `n`\n1101         values along the specified dimension(s). Default `n` = 5\n1102 \n1103         See Also\n1104         --------\n1105         Dataset.tail\n1106         DataArray.head\n1107         DataArray.thin\n1108         \"\"\"\n1109         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)\n1110         return self._from_temp_dataset(ds)\n1111 \n1112     def thin(\n1113         self,\n1114         indexers: Union[Mapping[Hashable, int], int] = None,\n1115         **indexers_kwargs: Any,\n1116     ) -> \"DataArray\":\n1117         \"\"\"Return a new DataArray whose data is given by each `n` value\n1118         along the specified dimension(s).\n1119 \n1120         See Also\n1121         --------\n1122         Dataset.thin\n1123         DataArray.head\n1124         DataArray.tail\n1125         \"\"\"\n1126         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)\n1127         return self._from_temp_dataset(ds)\n1128 \n1129     def broadcast_like(\n1130         self, other: Union[\"DataArray\", Dataset], exclude: Iterable[Hashable] = None\n1131     ) -> \"DataArray\":\n1132         \"\"\"Broadcast this DataArray against another Dataset or DataArray.\n1133 \n1134         This is equivalent to xr.broadcast(other, self)[1]\n1135 \n1136         xarray objects are broadcast against each other in arithmetic\n1137         operations, so this method is not be necessary for most uses.\n1138 \n1139         If no change is needed, the input data is returned to the output\n1140         without being copied.\n1141 \n1142         If new coords are added by the broadcast, their values are\n1143         NaN filled.\n1144 \n1145         Parameters\n1146         ----------\n1147         other : Dataset or DataArray\n1148             Object against which to broadcast this array.\n1149         exclude : iterable of hashable, optional\n1150             Dimensions that must not be broadcasted\n1151 \n1152         Returns\n1153         -------\n1154         new_da: xr.DataArray\n1155 \n1156         Examples\n1157         --------\n1158 \n1159         >>> arr1\n1160         <xarray.DataArray (x: 2, y: 3)>\n1161         array([[0.840235, 0.215216, 0.77917 ],\n1162                [0.726351, 0.543824, 0.875115]])\n1163         Coordinates:\n1164           * x        (x) <U1 'a' 'b'\n1165           * y        (y) <U1 'a' 'b' 'c'\n1166         >>> arr2\n1167         <xarray.DataArray (x: 3, y: 2)>\n1168         array([[0.612611, 0.125753],\n1169                [0.853181, 0.948818],\n1170                [0.180885, 0.33363 ]])\n1171         Coordinates:\n1172           * x        (x) <U1 'a' 'b' 'c'\n1173           * y        (y) <U1 'a' 'b'\n1174         >>> arr1.broadcast_like(arr2)\n1175         <xarray.DataArray (x: 3, y: 3)>\n1176         array([[0.840235, 0.215216, 0.77917 ],\n1177                [0.726351, 0.543824, 0.875115],\n1178                [     nan,      nan,      nan]])\n1179         Coordinates:\n1180           * x        (x) object 'a' 'b' 'c'\n1181           * y        (y) object 'a' 'b' 'c'\n1182         \"\"\"\n1183         if exclude is None:\n1184             exclude = set()\n1185         else:\n1186             exclude = set(exclude)\n1187         args = align(other, self, join=\"outer\", copy=False, exclude=exclude)\n1188 \n1189         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)\n1190 \n1191         return _broadcast_helper(args[1], exclude, dims_map, common_coords)\n1192 \n1193     def reindex_like(\n1194         self,\n1195         other: Union[\"DataArray\", Dataset],\n1196         method: str = None,\n1197         tolerance=None,\n1198         copy: bool = True,\n1199         fill_value=dtypes.NA,\n1200     ) -> \"DataArray\":\n1201         \"\"\"Conform this object onto the indexes of another object, filling in\n1202         missing values with ``fill_value``. The default fill value is NaN.\n1203 \n1204         Parameters\n1205         ----------\n1206         other : Dataset or DataArray\n1207             Object with an 'indexes' attribute giving a mapping from dimension\n1208             names to pandas.Index objects, which provides coordinates upon\n1209             which to index the variables in this dataset. The indexes on this\n1210             other object need not be the same as the indexes on this\n1211             dataset. Any mis-matched index values will be filled in with\n1212             NaN, and any mis-matched dimension names will simply be ignored.\n1213         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n1214             Method to use for filling index values from other not found on this\n1215             data array:\n1216 \n1217             * None (default): don't fill gaps\n1218             * pad / ffill: propagate last valid index value forward\n1219             * backfill / bfill: propagate next valid index value backward\n1220             * nearest: use nearest valid index value\n1221         tolerance : optional\n1222             Maximum distance between original and new labels for inexact\n1223             matches. The values of the index at the matching locations must\n1224             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1225         copy : bool, optional\n1226             If ``copy=True``, data in the return value is always copied. If\n1227             ``copy=False`` and reindexing is unnecessary, or can be performed\n1228             with only slice operations, then the output may share memory with\n1229             the input. In either case, a new xarray object is always returned.\n1230         fill_value : scalar, optional\n1231             Value to use for newly missing values\n1232 \n1233         Returns\n1234         -------\n1235         reindexed : DataArray\n1236             Another dataset array, with this array's data but coordinates from\n1237             the other object.\n1238 \n1239         See Also\n1240         --------\n1241         DataArray.reindex\n1242         align\n1243         \"\"\"\n1244         indexers = reindex_like_indexers(self, other)\n1245         return self.reindex(\n1246             indexers=indexers,\n1247             method=method,\n1248             tolerance=tolerance,\n1249             copy=copy,\n1250             fill_value=fill_value,\n1251         )\n1252 \n1253     def reindex(\n1254         self,\n1255         indexers: Mapping[Hashable, Any] = None,\n1256         method: str = None,\n1257         tolerance=None,\n1258         copy: bool = True,\n1259         fill_value=dtypes.NA,\n1260         **indexers_kwargs: Any,\n1261     ) -> \"DataArray\":\n1262         \"\"\"Conform this object onto the indexes of another object, filling in\n1263         missing values with ``fill_value``. The default fill value is NaN.\n1264 \n1265         Parameters\n1266         ----------\n1267         indexers : dict, optional\n1268             Dictionary with keys given by dimension names and values given by\n1269             arrays of coordinates tick labels. Any mis-matched coordinate\n1270             values will be filled in with NaN, and any mis-matched dimension\n1271             names will simply be ignored.\n1272             One of indexers or indexers_kwargs must be provided.\n1273         copy : bool, optional\n1274             If ``copy=True``, data in the return value is always copied. If\n1275             ``copy=False`` and reindexing is unnecessary, or can be performed\n1276             with only slice operations, then the output may share memory with\n1277             the input. In either case, a new xarray object is always returned.\n1278         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n1279             Method to use for filling index values in ``indexers`` not found on\n1280             this data array:\n1281 \n1282             * None (default): don't fill gaps\n1283             * pad / ffill: propagate last valid index value forward\n1284             * backfill / bfill: propagate next valid index value backward\n1285             * nearest: use nearest valid index value\n1286         tolerance : optional\n1287             Maximum distance between original and new labels for inexact\n1288             matches. The values of the index at the matching locations must\n1289             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1290         fill_value : scalar, optional\n1291             Value to use for newly missing values\n1292         **indexers_kwargs : {dim: indexer, ...}, optional\n1293             The keyword arguments form of ``indexers``.\n1294             One of indexers or indexers_kwargs must be provided.\n1295 \n1296         Returns\n1297         -------\n1298         reindexed : DataArray\n1299             Another dataset array, with this array's data but replaced\n1300             coordinates.\n1301 \n1302         See Also\n1303         --------\n1304         DataArray.reindex_like\n1305         align\n1306         \"\"\"\n1307         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"reindex\")\n1308         ds = self._to_temp_dataset().reindex(\n1309             indexers=indexers,\n1310             method=method,\n1311             tolerance=tolerance,\n1312             copy=copy,\n1313             fill_value=fill_value,\n1314         )\n1315         return self._from_temp_dataset(ds)\n1316 \n1317     def interp(\n1318         self,\n1319         coords: Mapping[Hashable, Any] = None,\n1320         method: str = \"linear\",\n1321         assume_sorted: bool = False,\n1322         kwargs: Mapping[str, Any] = None,\n1323         **coords_kwargs: Any,\n1324     ) -> \"DataArray\":\n1325         \"\"\" Multidimensional interpolation of variables.\n1326 \n1327         coords : dict, optional\n1328             Mapping from dimension names to the new coordinates.\n1329             new coordinate can be an scalar, array-like or DataArray.\n1330             If DataArrays are passed as new coordates, their dimensions are\n1331             used for the broadcasting.\n1332         method: {'linear', 'nearest'} for multidimensional array,\n1333             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}\n1334             for 1-dimensional array.\n1335         assume_sorted: boolean, optional\n1336             If False, values of x can be in any order and they are sorted\n1337             first. If True, x has to be an array of monotonically increasing\n1338             values.\n1339         kwargs: dictionary\n1340             Additional keyword passed to scipy's interpolator.\n1341         ``**coords_kwargs`` : {dim: coordinate, ...}, optional\n1342             The keyword arguments form of ``coords``.\n1343             One of coords or coords_kwargs must be provided.\n1344 \n1345         Returns\n1346         -------\n1347         interpolated: xr.DataArray\n1348             New dataarray on the new coordinates.\n1349 \n1350         Notes\n1351         -----\n1352         scipy is required.\n1353 \n1354         See Also\n1355         --------\n1356         scipy.interpolate.interp1d\n1357         scipy.interpolate.interpn\n1358 \n1359         Examples\n1360         --------\n1361         >>> da = xr.DataArray([1, 3], [('x', np.arange(2))])\n1362         >>> da.interp(x=0.5)\n1363         <xarray.DataArray ()>\n1364         array(2.0)\n1365         Coordinates:\n1366             x        float64 0.5\n1367         \"\"\"\n1368         if self.dtype.kind not in \"uifc\":\n1369             raise TypeError(\n1370                 \"interp only works for a numeric type array. \"\n1371                 \"Given {}.\".format(self.dtype)\n1372             )\n1373         ds = self._to_temp_dataset().interp(\n1374             coords,\n1375             method=method,\n1376             kwargs=kwargs,\n1377             assume_sorted=assume_sorted,\n1378             **coords_kwargs,\n1379         )\n1380         return self._from_temp_dataset(ds)\n1381 \n1382     def interp_like(\n1383         self,\n1384         other: Union[\"DataArray\", Dataset],\n1385         method: str = \"linear\",\n1386         assume_sorted: bool = False,\n1387         kwargs: Mapping[str, Any] = None,\n1388     ) -> \"DataArray\":\n1389         \"\"\"Interpolate this object onto the coordinates of another object,\n1390         filling out of range values with NaN.\n1391 \n1392         Parameters\n1393         ----------\n1394         other : Dataset or DataArray\n1395             Object with an 'indexes' attribute giving a mapping from dimension\n1396             names to an 1d array-like, which provides coordinates upon\n1397             which to index the variables in this dataset.\n1398         method: string, optional.\n1399             {'linear', 'nearest'} for multidimensional array,\n1400             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}\n1401             for 1-dimensional array. 'linear' is used by default.\n1402         assume_sorted: boolean, optional\n1403             If False, values of coordinates that are interpolated over can be\n1404             in any order and they are sorted first. If True, interpolated\n1405             coordinates are assumed to be an array of monotonically increasing\n1406             values.\n1407         kwargs: dictionary, optional\n1408             Additional keyword passed to scipy's interpolator.\n1409 \n1410         Returns\n1411         -------\n1412         interpolated: xr.DataArray\n1413             Another dataarray by interpolating this dataarray's data along the\n1414             coordinates of the other object.\n1415 \n1416         Notes\n1417         -----\n1418         scipy is required.\n1419         If the dataarray has object-type coordinates, reindex is used for these\n1420         coordinates instead of the interpolation.\n1421 \n1422         See Also\n1423         --------\n1424         DataArray.interp\n1425         DataArray.reindex_like\n1426         \"\"\"\n1427         if self.dtype.kind not in \"uifc\":\n1428             raise TypeError(\n1429                 \"interp only works for a numeric type array. \"\n1430                 \"Given {}.\".format(self.dtype)\n1431             )\n1432         ds = self._to_temp_dataset().interp_like(\n1433             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted\n1434         )\n1435         return self._from_temp_dataset(ds)\n1436 \n1437     def rename(\n1438         self,\n1439         new_name_or_name_dict: Union[Hashable, Mapping[Hashable, Hashable]] = None,\n1440         **names: Hashable,\n1441     ) -> \"DataArray\":\n1442         \"\"\"Returns a new DataArray with renamed coordinates or a new name.\n1443 \n1444         Parameters\n1445         ----------\n1446         new_name_or_name_dict : str or dict-like, optional\n1447             If the argument is dict-like, it used as a mapping from old\n1448             names to new names for coordinates. Otherwise, use the argument\n1449             as the new name for this array.\n1450         **names: hashable, optional\n1451             The keyword arguments form of a mapping from old names to\n1452             new names for coordinates.\n1453             One of new_name_or_name_dict or names must be provided.\n1454 \n1455         Returns\n1456         -------\n1457         renamed : DataArray\n1458             Renamed array or array with renamed coordinates.\n1459 \n1460         See Also\n1461         --------\n1462         Dataset.rename\n1463         DataArray.swap_dims\n1464         \"\"\"\n1465         if names or utils.is_dict_like(new_name_or_name_dict):\n1466             new_name_or_name_dict = cast(\n1467                 Mapping[Hashable, Hashable], new_name_or_name_dict\n1468             )\n1469             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, \"rename\")\n1470             dataset = self._to_temp_dataset().rename(name_dict)\n1471             return self._from_temp_dataset(dataset)\n1472         else:\n1473             new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)\n1474             return self._replace(name=new_name_or_name_dict)\n1475 \n1476     def swap_dims(self, dims_dict: Mapping[Hashable, Hashable]) -> \"DataArray\":\n1477         \"\"\"Returns a new DataArray with swapped dimensions.\n1478 \n1479         Parameters\n1480         ----------\n1481         dims_dict : dict-like\n1482             Dictionary whose keys are current dimension names and whose values\n1483             are new names. Each value must already be a coordinate on this\n1484             array.\n1485 \n1486         Returns\n1487         -------\n1488         swapped : DataArray\n1489             DataArray with swapped dimensions.\n1490 \n1491         Examples\n1492         --------\n1493         >>> arr = xr.DataArray(data=[0, 1], dims=\"x\",\n1494                                coords={\"x\": [\"a\", \"b\"], \"y\": (\"x\", [0, 1])})\n1495         >>> arr\n1496         <xarray.DataArray (x: 2)>\n1497         array([0, 1])\n1498         Coordinates:\n1499           * x        (x) <U1 'a' 'b'\n1500             y        (x) int64 0 1\n1501         >>> arr.swap_dims({\"x\": \"y\"})\n1502         <xarray.DataArray (y: 2)>\n1503         array([0, 1])\n1504         Coordinates:\n1505             x        (y) <U1 'a' 'b'\n1506           * y        (y) int64 0 1\n1507 \n1508         See Also\n1509         --------\n1510 \n1511         DataArray.rename\n1512         Dataset.swap_dims\n1513         \"\"\"\n1514         ds = self._to_temp_dataset().swap_dims(dims_dict)\n1515         return self._from_temp_dataset(ds)\n1516 \n1517     def expand_dims(\n1518         self,\n1519         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,\n1520         axis=None,\n1521         **dim_kwargs: Any,\n1522     ) -> \"DataArray\":\n1523         \"\"\"Return a new object with an additional axis (or axes) inserted at\n1524         the corresponding position in the array shape. The new object is a\n1525         view into the underlying array, not a copy.\n1526 \n1527 \n1528         If dim is already a scalar coordinate, it will be promoted to a 1D\n1529         coordinate consisting of a single value.\n1530 \n1531         Parameters\n1532         ----------\n1533         dim : hashable, sequence of hashable, dict, or None\n1534             Dimensions to include on the new variable.\n1535             If provided as str or sequence of str, then dimensions are inserted\n1536             with length 1. If provided as a dict, then the keys are the new\n1537             dimensions and the values are either integers (giving the length of\n1538             the new dimensions) or sequence/ndarray (giving the coordinates of\n1539             the new dimensions).\n1540         axis : integer, list (or tuple) of integers, or None\n1541             Axis position(s) where new axis is to be inserted (position(s) on\n1542             the result array). If a list (or tuple) of integers is passed,\n1543             multiple axes are inserted. In this case, dim arguments should be\n1544             same length list. If axis=None is passed, all the axes will be\n1545             inserted to the start of the result array.\n1546         **dim_kwargs : int or sequence/ndarray\n1547             The keywords are arbitrary dimensions being inserted and the values\n1548             are either the lengths of the new dims (if int is given), or their\n1549             coordinates. Note, this is an alternative to passing a dict to the\n1550             dim kwarg and will only be used if dim is None.\n1551 \n1552         Returns\n1553         -------\n1554         expanded : same type as caller\n1555             This object, but with an additional dimension(s).\n1556         \"\"\"\n1557         if isinstance(dim, int):\n1558             raise TypeError(\"dim should be hashable or sequence/mapping of hashables\")\n1559         elif isinstance(dim, Sequence) and not isinstance(dim, str):\n1560             if len(dim) != len(set(dim)):\n1561                 raise ValueError(\"dims should not contain duplicate values.\")\n1562             dim = dict.fromkeys(dim, 1)\n1563         elif dim is not None and not isinstance(dim, Mapping):\n1564             dim = {cast(Hashable, dim): 1}\n1565 \n1566         dim = either_dict_or_kwargs(dim, dim_kwargs, \"expand_dims\")\n1567         ds = self._to_temp_dataset().expand_dims(dim, axis)\n1568         return self._from_temp_dataset(ds)\n1569 \n1570     def set_index(\n1571         self,\n1572         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,\n1573         append: bool = False,\n1574         inplace: bool = None,\n1575         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],\n1576     ) -> Optional[\"DataArray\"]:\n1577         \"\"\"Set DataArray (multi-)indexes using one or more existing\n1578         coordinates.\n1579 \n1580         Parameters\n1581         ----------\n1582         indexes : {dim: index, ...}\n1583             Mapping from names matching dimensions and values given\n1584             by (lists of) the names of existing coordinates or variables to set\n1585             as new (multi-)index.\n1586         append : bool, optional\n1587             If True, append the supplied index(es) to the existing index(es).\n1588             Otherwise replace the existing index(es) (default).\n1589         **indexes_kwargs: optional\n1590             The keyword arguments form of ``indexes``.\n1591             One of indexes or indexes_kwargs must be provided.\n1592 \n1593         Returns\n1594         -------\n1595         obj : DataArray\n1596             Another DataArray, with this data but replaced coordinates.\n1597 \n1598         Examples\n1599         --------\n1600         >>> arr = xr.DataArray(data=np.ones((2, 3)),\n1601         ...                    dims=['x', 'y'],\n1602         ...                    coords={'x':\n1603         ...                        range(2), 'y':\n1604         ...                        range(3), 'a': ('x', [3, 4])\n1605         ...                    })\n1606         >>> arr\n1607         <xarray.DataArray (x: 2, y: 3)>\n1608         array([[1., 1., 1.],\n1609                [1., 1., 1.]])\n1610         Coordinates:\n1611           * x        (x) int64 0 1\n1612           * y        (y) int64 0 1 2\n1613             a        (x) int64 3 4\n1614         >>> arr.set_index(x='a')\n1615         <xarray.DataArray (x: 2, y: 3)>\n1616         array([[1., 1., 1.],\n1617                [1., 1., 1.]])\n1618         Coordinates:\n1619           * x        (x) int64 3 4\n1620           * y        (y) int64 0 1 2\n1621 \n1622         See Also\n1623         --------\n1624         DataArray.reset_index\n1625         \"\"\"\n1626         ds = self._to_temp_dataset().set_index(\n1627             indexes, append=append, inplace=inplace, **indexes_kwargs\n1628         )\n1629         return self._from_temp_dataset(ds)\n1630 \n1631     def reset_index(\n1632         self,\n1633         dims_or_levels: Union[Hashable, Sequence[Hashable]],\n1634         drop: bool = False,\n1635         inplace: bool = None,\n1636     ) -> Optional[\"DataArray\"]:\n1637         \"\"\"Reset the specified index(es) or multi-index level(s).\n1638 \n1639         Parameters\n1640         ----------\n1641         dims_or_levels : hashable or sequence of hashables\n1642             Name(s) of the dimension(s) and/or multi-index level(s) that will\n1643             be reset.\n1644         drop : bool, optional\n1645             If True, remove the specified indexes and/or multi-index levels\n1646             instead of extracting them as new coordinates (default: False).\n1647 \n1648         Returns\n1649         -------\n1650         obj : DataArray\n1651             Another dataarray, with this dataarray's data but replaced\n1652             coordinates.\n1653 \n1654         See Also\n1655         --------\n1656         DataArray.set_index\n1657         \"\"\"\n1658         _check_inplace(inplace)\n1659         coords, _ = split_indexes(\n1660             dims_or_levels, self._coords, set(), self._level_coords, drop=drop\n1661         )\n1662         return self._replace(coords=coords)\n1663 \n1664     def reorder_levels(\n1665         self,\n1666         dim_order: Mapping[Hashable, Sequence[int]] = None,\n1667         inplace: bool = None,\n1668         **dim_order_kwargs: Sequence[int],\n1669     ) -> \"DataArray\":\n1670         \"\"\"Rearrange index levels using input order.\n1671 \n1672         Parameters\n1673         ----------\n1674         dim_order : optional\n1675             Mapping from names matching dimensions and values given\n1676             by lists representing new level orders. Every given dimension\n1677             must have a multi-index.\n1678         **dim_order_kwargs: optional\n1679             The keyword arguments form of ``dim_order``.\n1680             One of dim_order or dim_order_kwargs must be provided.\n1681 \n1682         Returns\n1683         -------\n1684         obj : DataArray\n1685             Another dataarray, with this dataarray's data but replaced\n1686             coordinates.\n1687         \"\"\"\n1688         _check_inplace(inplace)\n1689         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, \"reorder_levels\")\n1690         replace_coords = {}\n1691         for dim, order in dim_order.items():\n1692             coord = self._coords[dim]\n1693             index = coord.to_index()\n1694             if not isinstance(index, pd.MultiIndex):\n1695                 raise ValueError(\"coordinate %r has no MultiIndex\" % dim)\n1696             replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))\n1697         coords = self._coords.copy()\n1698         coords.update(replace_coords)\n1699         return self._replace(coords=coords)\n1700 \n1701     def stack(\n1702         self,\n1703         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,\n1704         **dimensions_kwargs: Sequence[Hashable],\n1705     ) -> \"DataArray\":\n1706         \"\"\"\n1707         Stack any number of existing dimensions into a single new dimension.\n1708 \n1709         New dimensions will be added at the end, and the corresponding\n1710         coordinate variables will be combined into a MultiIndex.\n1711 \n1712         Parameters\n1713         ----------\n1714         dimensions : Mapping of the form new_name=(dim1, dim2, ...)\n1715             Names of new dimensions, and the existing dimensions that they\n1716             replace.\n1717         **dimensions_kwargs:\n1718             The keyword arguments form of ``dimensions``.\n1719             One of dimensions or dimensions_kwargs must be provided.\n1720 \n1721         Returns\n1722         -------\n1723         stacked : DataArray\n1724             DataArray with stacked data.\n1725 \n1726         Examples\n1727         --------\n1728 \n1729         >>> arr = DataArray(np.arange(6).reshape(2, 3),\n1730         ...                 coords=[('x', ['a', 'b']), ('y', [0, 1, 2])])\n1731         >>> arr\n1732         <xarray.DataArray (x: 2, y: 3)>\n1733         array([[0, 1, 2],\n1734                [3, 4, 5]])\n1735         Coordinates:\n1736           * x        (x) |S1 'a' 'b'\n1737           * y        (y) int64 0 1 2\n1738         >>> stacked = arr.stack(z=('x', 'y'))\n1739         >>> stacked.indexes['z']\n1740         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],\n1741                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],\n1742                    names=['x', 'y'])\n1743 \n1744         See Also\n1745         --------\n1746         DataArray.unstack\n1747         \"\"\"\n1748         ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)\n1749         return self._from_temp_dataset(ds)\n1750 \n1751     def unstack(\n1752         self,\n1753         dim: Union[Hashable, Sequence[Hashable], None] = None,\n1754         fill_value: Any = dtypes.NA,\n1755         sparse: bool = False,\n1756     ) -> \"DataArray\":\n1757         \"\"\"\n1758         Unstack existing dimensions corresponding to MultiIndexes into\n1759         multiple new dimensions.\n1760 \n1761         New dimensions will be added at the end.\n1762 \n1763         Parameters\n1764         ----------\n1765         dim : hashable or sequence of hashable, optional\n1766             Dimension(s) over which to unstack. By default unstacks all\n1767             MultiIndexes.\n1768         fill_value: value to be filled. By default, np.nan\n1769         sparse: use sparse-array if True\n1770 \n1771         Returns\n1772         -------\n1773         unstacked : DataArray\n1774             Array with unstacked data.\n1775 \n1776         Examples\n1777         --------\n1778 \n1779         >>> arr = DataArray(np.arange(6).reshape(2, 3),\n1780         ...                 coords=[('x', ['a', 'b']), ('y', [0, 1, 2])])\n1781         >>> arr\n1782         <xarray.DataArray (x: 2, y: 3)>\n1783         array([[0, 1, 2],\n1784                [3, 4, 5]])\n1785         Coordinates:\n1786           * x        (x) |S1 'a' 'b'\n1787           * y        (y) int64 0 1 2\n1788         >>> stacked = arr.stack(z=('x', 'y'))\n1789         >>> stacked.indexes['z']\n1790         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],\n1791                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],\n1792                    names=['x', 'y'])\n1793         >>> roundtripped = stacked.unstack()\n1794         >>> arr.identical(roundtripped)\n1795         True\n1796 \n1797         See Also\n1798         --------\n1799         DataArray.stack\n1800         \"\"\"\n1801         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)\n1802         return self._from_temp_dataset(ds)\n1803 \n1804     def to_unstacked_dataset(self, dim, level=0):\n1805         \"\"\"Unstack DataArray expanding to Dataset along a given level of a\n1806         stacked coordinate.\n1807 \n1808         This is the inverse operation of Dataset.to_stacked_array.\n1809 \n1810         Parameters\n1811         ----------\n1812         dim : str\n1813             Name of existing dimension to unstack\n1814         level : int or str\n1815             The MultiIndex level to expand to a dataset along. Can either be\n1816             the integer index of the level or its name.\n1817         label : int, default 0\n1818             Label of the level to expand dataset along. Overrides the label\n1819             argument if given.\n1820 \n1821         Returns\n1822         -------\n1823         unstacked: Dataset\n1824 \n1825         Examples\n1826         --------\n1827         >>> import xarray as xr\n1828         >>> arr = DataArray(np.arange(6).reshape(2, 3),\n1829         ...                 coords=[('x', ['a', 'b']), ('y', [0, 1, 2])])\n1830         >>> data = xr.Dataset({'a': arr, 'b': arr.isel(y=0)})\n1831         >>> data\n1832         <xarray.Dataset>\n1833         Dimensions:  (x: 2, y: 3)\n1834         Coordinates:\n1835           * x        (x) <U1 'a' 'b'\n1836           * y        (y) int64 0 1 2\n1837         Data variables:\n1838             a        (x, y) int64 0 1 2 3 4 5\n1839             b        (x) int64 0 3\n1840         >>> stacked = data.to_stacked_array(\"z\", ['y'])\n1841         >>> stacked.indexes['z']\n1842         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],\n1843                 labels=[[0, 0, 0, 1], [0, 1, 2, -1]],\n1844                 names=['variable', 'y'])\n1845         >>> roundtripped = stacked.to_unstacked_dataset(dim='z')\n1846         >>> data.identical(roundtripped)\n1847         True\n1848 \n1849         See Also\n1850         --------\n1851         Dataset.to_stacked_array\n1852         \"\"\"\n1853 \n1854         idx = self.indexes[dim]\n1855         if not isinstance(idx, pd.MultiIndex):\n1856             raise ValueError(f\"'{dim}' is not a stacked coordinate\")\n1857 \n1858         level_number = idx._get_level_number(level)\n1859         variables = idx.levels[level_number]\n1860         variable_dim = idx.names[level_number]\n1861 \n1862         # pull variables out of datarray\n1863         data_dict = {}\n1864         for k in variables:\n1865             data_dict[k] = self.sel({variable_dim: k}).squeeze(drop=True)\n1866 \n1867         # unstacked dataset\n1868         return Dataset(data_dict)\n1869 \n1870     def transpose(self, *dims: Hashable, transpose_coords: bool = None) -> \"DataArray\":\n1871         \"\"\"Return a new DataArray object with transposed dimensions.\n1872 \n1873         Parameters\n1874         ----------\n1875         *dims : hashable, optional\n1876             By default, reverse the dimensions. Otherwise, reorder the\n1877             dimensions to this order.\n1878         transpose_coords : boolean, optional\n1879             If True, also transpose the coordinates of this DataArray.\n1880 \n1881         Returns\n1882         -------\n1883         transposed : DataArray\n1884             The returned DataArray's array is transposed.\n1885 \n1886         Notes\n1887         -----\n1888         This operation returns a view of this array's data. It is\n1889         lazy for dask-backed DataArrays but not for numpy-backed DataArrays\n1890         -- the data will be fully loaded.\n1891 \n1892         See Also\n1893         --------\n1894         numpy.transpose\n1895         Dataset.transpose\n1896         \"\"\"\n1897         if dims:\n1898             dims = tuple(utils.infix_dims(dims, self.dims))\n1899         variable = self.variable.transpose(*dims)\n1900         if transpose_coords:\n1901             coords: Dict[Hashable, Variable] = {}\n1902             for name, coord in self.coords.items():\n1903                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)\n1904                 coords[name] = coord.variable.transpose(*coord_dims)\n1905             return self._replace(variable, coords)\n1906         else:\n1907             if transpose_coords is None and any(self[c].ndim > 1 for c in self.coords):\n1908                 warnings.warn(\n1909                     \"This DataArray contains multi-dimensional \"\n1910                     \"coordinates. In the future, these coordinates \"\n1911                     \"will be transposed as well unless you specify \"\n1912                     \"transpose_coords=False.\",\n1913                     FutureWarning,\n1914                     stacklevel=2,\n1915                 )\n1916             return self._replace(variable)\n1917 \n1918     @property\n1919     def T(self) -> \"DataArray\":\n1920         return self.transpose()\n1921 \n1922     def drop_vars(\n1923         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = \"raise\"\n1924     ) -> \"DataArray\":\n1925         \"\"\"Drop variables from this DataArray.\n1926 \n1927         Parameters\n1928         ----------\n1929         names : hashable or iterable of hashables\n1930             Name(s) of variables to drop.\n1931         errors: {'raise', 'ignore'}, optional\n1932             If 'raise' (default), raises a ValueError error if any of the variable\n1933             passed are not in the dataset. If 'ignore', any given names that are in the\n1934             DataArray are dropped and no error is raised.\n1935 \n1936         Returns\n1937         -------\n1938         dropped : Dataset\n1939 \n1940         \"\"\"\n1941         ds = self._to_temp_dataset().drop_vars(names, errors=errors)\n1942         return self._from_temp_dataset(ds)\n1943 \n1944     def drop(\n1945         self,\n1946         labels: Mapping = None,\n1947         dim: Hashable = None,\n1948         *,\n1949         errors: str = \"raise\",\n1950         **labels_kwargs,\n1951     ) -> \"DataArray\":\n1952         \"\"\"Backward compatible method based on `drop_vars` and `drop_sel`\n1953 \n1954         Using either `drop_vars` or `drop_sel` is encouraged\n1955 \n1956         See Also\n1957         --------\n1958         DataArray.drop_vars\n1959         DataArray.drop_sel\n1960         \"\"\"\n1961         ds = self._to_temp_dataset().drop(labels, dim, errors=errors)\n1962         return self._from_temp_dataset(ds)\n1963 \n1964     def drop_sel(\n1965         self,\n1966         labels: Mapping[Hashable, Any] = None,\n1967         *,\n1968         errors: str = \"raise\",\n1969         **labels_kwargs,\n1970     ) -> \"DataArray\":\n1971         \"\"\"Drop index labels from this DataArray.\n1972 \n1973         Parameters\n1974         ----------\n1975         labels : Mapping[Hashable, Any]\n1976             Index labels to drop\n1977         errors: {'raise', 'ignore'}, optional\n1978             If 'raise' (default), raises a ValueError error if\n1979             any of the index labels passed are not\n1980             in the dataset. If 'ignore', any given labels that are in the\n1981             dataset are dropped and no error is raised.\n1982         **labels_kwargs : {dim: label, ...}, optional\n1983             The keyword arguments form of ``dim`` and ``labels``\n1984 \n1985         Returns\n1986         -------\n1987         dropped : DataArray\n1988         \"\"\"\n1989         if labels_kwargs or isinstance(labels, dict):\n1990             labels = either_dict_or_kwargs(labels, labels_kwargs, \"drop\")\n1991 \n1992         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)\n1993         return self._from_temp_dataset(ds)\n1994 \n1995     def dropna(\n1996         self, dim: Hashable, how: str = \"any\", thresh: int = None\n1997     ) -> \"DataArray\":\n1998         \"\"\"Returns a new array with dropped labels for missing values along\n1999         the provided dimension.\n2000 \n2001         Parameters\n2002         ----------\n2003         dim : hashable\n2004             Dimension along which to drop missing values. Dropping along\n2005             multiple dimensions simultaneously is not yet supported.\n2006         how : {'any', 'all'}, optional\n2007             * any : if any NA values are present, drop that label\n2008             * all : if all values are NA, drop that label\n2009         thresh : int, default None\n2010             If supplied, require this many non-NA values.\n2011 \n2012         Returns\n2013         -------\n2014         DataArray\n2015         \"\"\"\n2016         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)\n2017         return self._from_temp_dataset(ds)\n2018 \n2019     def fillna(self, value: Any) -> \"DataArray\":\n2020         \"\"\"Fill missing values in this object.\n2021 \n2022         This operation follows the normal broadcasting and alignment rules that\n2023         xarray uses for binary arithmetic, except the result is aligned to this\n2024         object (``join='left'``) instead of aligned to the intersection of\n2025         index coordinates (``join='inner'``).\n2026 \n2027         Parameters\n2028         ----------\n2029         value : scalar, ndarray or DataArray\n2030             Used to fill all matching missing values in this array. If the\n2031             argument is a DataArray, it is first aligned with (reindexed to)\n2032             this array.\n2033 \n2034         Returns\n2035         -------\n2036         DataArray\n2037         \"\"\"\n2038         if utils.is_dict_like(value):\n2039             raise TypeError(\n2040                 \"cannot provide fill value as a dictionary with \"\n2041                 \"fillna on a DataArray\"\n2042             )\n2043         out = ops.fillna(self, value)\n2044         return out\n2045 \n2046     def interpolate_na(\n2047         self,\n2048         dim: Hashable = None,\n2049         method: str = \"linear\",\n2050         limit: int = None,\n2051         use_coordinate: Union[bool, str] = True,\n2052         max_gap: Union[int, float, str, pd.Timedelta, np.timedelta64] = None,\n2053         **kwargs: Any,\n2054     ) -> \"DataArray\":\n2055         \"\"\"Fill in NaNs by interpolating according to different methods.\n2056 \n2057         Parameters\n2058         ----------\n2059         dim : str\n2060             Specifies the dimension along which to interpolate.\n2061         method : str, optional\n2062             String indicating which method to use for interpolation:\n2063 \n2064             - 'linear': linear interpolation (Default). Additional keyword\n2065               arguments are passed to :py:func:`numpy.interp`\n2066             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n2067               are passed to :py:func:`scipy.interpolate.interp1d`. If\n2068               ``method='polynomial'``, the ``order`` keyword argument must also be\n2069               provided.\n2070             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n2071               respective :py:class:`scipy.interpolate` classes.\n2072 \n2073         use_coordinate : bool, str, default True\n2074             Specifies which index to use as the x values in the interpolation\n2075             formulated as `y = f(x)`. If False, values are treated as if\n2076             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is\n2077             used. If ``use_coordinate`` is a string, it specifies the name of a\n2078             coordinate variariable to use as the index.\n2079         limit : int, default None\n2080             Maximum number of consecutive NaNs to fill. Must be greater than 0\n2081             or None for no limit. This filling is done regardless of the size of\n2082             the gap in the data. To only interpolate over gaps less than a given length,\n2083             see ``max_gap``.\n2084         max_gap: int, float, str, pandas.Timedelta, numpy.timedelta64, default None.\n2085             Maximum size of gap, a continuous sequence of NaNs, that will be filled.\n2086             Use None for no limit. When interpolating along a datetime64 dimension\n2087             and ``use_coordinate=True``, ``max_gap`` can be one of the following:\n2088 \n2089             - a string that is valid input for pandas.to_timedelta\n2090             - a :py:class:`numpy.timedelta64` object\n2091             - a :py:class:`pandas.Timedelta` object\n2092 \n2093             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled\n2094             dimensions has not been implemented yet. Gap length is defined as the difference\n2095             between coordinate values at the first data point after a gap and the last value\n2096             before a gap. For gaps at the beginning (end), gap length is defined as the difference\n2097             between coordinate values at the first (last) valid data point and the first (last) NaN.\n2098             For example, consider::\n2099 \n2100                 <xarray.DataArray (x: 9)>\n2101                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])\n2102                 Coordinates:\n2103                   * x        (x) int64 0 1 2 3 4 5 6 7 8\n2104 \n2105             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively\n2106         kwargs : dict, optional\n2107             parameters passed verbatim to the underlying interpolation function\n2108 \n2109         Returns\n2110         -------\n2111         interpolated: DataArray\n2112             Filled in DataArray.\n2113 \n2114         See also\n2115         --------\n2116         numpy.interp\n2117         scipy.interpolate\n2118         \"\"\"\n2119         from .missing import interp_na\n2120 \n2121         return interp_na(\n2122             self,\n2123             dim=dim,\n2124             method=method,\n2125             limit=limit,\n2126             use_coordinate=use_coordinate,\n2127             max_gap=max_gap,\n2128             **kwargs,\n2129         )\n2130 \n2131     def ffill(self, dim: Hashable, limit: int = None) -> \"DataArray\":\n2132         \"\"\"Fill NaN values by propogating values forward\n2133 \n2134         *Requires bottleneck.*\n2135 \n2136         Parameters\n2137         ----------\n2138         dim : hashable\n2139             Specifies the dimension along which to propagate values when\n2140             filling.\n2141         limit : int, default None\n2142             The maximum number of consecutive NaN values to forward fill. In\n2143             other words, if there is a gap with more than this number of\n2144             consecutive NaNs, it will only be partially filled. Must be greater\n2145             than 0 or None for no limit.\n2146 \n2147         Returns\n2148         -------\n2149         DataArray\n2150         \"\"\"\n2151         from .missing import ffill\n2152 \n2153         return ffill(self, dim, limit=limit)\n2154 \n2155     def bfill(self, dim: Hashable, limit: int = None) -> \"DataArray\":\n2156         \"\"\"Fill NaN values by propogating values backward\n2157 \n2158         *Requires bottleneck.*\n2159 \n2160         Parameters\n2161         ----------\n2162         dim : str\n2163             Specifies the dimension along which to propagate values when\n2164             filling.\n2165         limit : int, default None\n2166             The maximum number of consecutive NaN values to backward fill. In\n2167             other words, if there is a gap with more than this number of\n2168             consecutive NaNs, it will only be partially filled. Must be greater\n2169             than 0 or None for no limit.\n2170 \n2171         Returns\n2172         -------\n2173         DataArray\n2174         \"\"\"\n2175         from .missing import bfill\n2176 \n2177         return bfill(self, dim, limit=limit)\n2178 \n2179     def combine_first(self, other: \"DataArray\") -> \"DataArray\":\n2180         \"\"\"Combine two DataArray objects, with union of coordinates.\n2181 \n2182         This operation follows the normal broadcasting and alignment rules of\n2183         ``join='outer'``.  Default to non-null values of array calling the\n2184         method.  Use np.nan to fill in vacant cells after alignment.\n2185 \n2186         Parameters\n2187         ----------\n2188         other : DataArray\n2189             Used to fill all matching missing values in this array.\n2190 \n2191         Returns\n2192         -------\n2193         DataArray\n2194         \"\"\"\n2195         return ops.fillna(self, other, join=\"outer\")\n2196 \n2197     def reduce(\n2198         self,\n2199         func: Callable[..., Any],\n2200         dim: Union[None, Hashable, Sequence[Hashable]] = None,\n2201         axis: Union[None, int, Sequence[int]] = None,\n2202         keep_attrs: bool = None,\n2203         keepdims: bool = False,\n2204         **kwargs: Any,\n2205     ) -> \"DataArray\":\n2206         \"\"\"Reduce this array by applying `func` along some dimension(s).\n2207 \n2208         Parameters\n2209         ----------\n2210         func : function\n2211             Function which can be called in the form\n2212             `f(x, axis=axis, **kwargs)` to return the result of reducing an\n2213             np.ndarray over an integer valued axis.\n2214         dim : hashable or sequence of hashables, optional\n2215             Dimension(s) over which to apply `func`.\n2216         axis : int or sequence of int, optional\n2217             Axis(es) over which to repeatedly apply `func`. Only one of the\n2218             'dim' and 'axis' arguments can be supplied. If neither are\n2219             supplied, then the reduction is calculated over the flattened array\n2220             (by calling `f(x)` without an axis argument).\n2221         keep_attrs : bool, optional\n2222             If True, the variable's attributes (`attrs`) will be copied from\n2223             the original object to the new one.  If False (default), the new\n2224             object will be returned without attributes.\n2225         keepdims : bool, default False\n2226             If True, the dimensions which are reduced are left in the result\n2227             as dimensions of size one. Coordinates that use these dimensions\n2228             are removed.\n2229         **kwargs : dict\n2230             Additional keyword arguments passed on to `func`.\n2231 \n2232         Returns\n2233         -------\n2234         reduced : DataArray\n2235             DataArray with this object's array replaced with an array with\n2236             summarized data and the indicated dimension(s) removed.\n2237         \"\"\"\n2238 \n2239         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)\n2240         return self._replace_maybe_drop_dims(var)\n2241 \n2242     def to_pandas(self) -> Union[\"DataArray\", pd.Series, pd.DataFrame]:\n2243         \"\"\"Convert this array into a pandas object with the same shape.\n2244 \n2245         The type of the returned object depends on the number of DataArray\n2246         dimensions:\n2247 \n2248         * 0D -> `xarray.DataArray`\n2249         * 1D -> `pandas.Series`\n2250         * 2D -> `pandas.DataFrame`\n2251         * 3D -> `pandas.Panel` *(deprecated)*\n2252 \n2253         Only works for arrays with 3 or fewer dimensions.\n2254 \n2255         The DataArray constructor performs the inverse transformation.\n2256         \"\"\"\n2257         # TODO: consolidate the info about pandas constructors and the\n2258         # attributes that correspond to their indexes into a separate module?\n2259         constructors = {\n2260             0: lambda x: x,\n2261             1: pd.Series,\n2262             2: pd.DataFrame,\n2263             3: pdcompat.Panel,\n2264         }\n2265         try:\n2266             constructor = constructors[self.ndim]\n2267         except KeyError:\n2268             raise ValueError(\n2269                 \"cannot convert arrays with %s dimensions into \"\n2270                 \"pandas objects\" % self.ndim\n2271             )\n2272         indexes = [self.get_index(dim) for dim in self.dims]\n2273         return constructor(self.values, *indexes)\n2274 \n2275     def to_dataframe(self, name: Hashable = None) -> pd.DataFrame:\n2276         \"\"\"Convert this array and its coordinates into a tidy pandas.DataFrame.\n2277 \n2278         The DataFrame is indexed by the Cartesian product of index coordinates\n2279         (in the form of a :py:class:`pandas.MultiIndex`).\n2280 \n2281         Other coordinates are included as columns in the DataFrame.\n2282         \"\"\"\n2283         if name is None:\n2284             name = self.name\n2285         if name is None:\n2286             raise ValueError(\n2287                 \"cannot convert an unnamed DataArray to a \"\n2288                 \"DataFrame: use the ``name`` parameter\"\n2289             )\n2290 \n2291         dims = dict(zip(self.dims, self.shape))\n2292         # By using a unique name, we can convert a DataArray into a DataFrame\n2293         # even if it shares a name with one of its coordinates.\n2294         # I would normally use unique_name = object() but that results in a\n2295         # dataframe with columns in the wrong order, for reasons I have not\n2296         # been able to debug (possibly a pandas bug?).\n2297         unique_name = \"__unique_name_identifier_z98xfz98xugfg73ho__\"\n2298         ds = self._to_dataset_whole(name=unique_name)\n2299         df = ds._to_dataframe(dims)\n2300         df.columns = [name if c == unique_name else c for c in df.columns]\n2301         return df\n2302 \n2303     def to_series(self) -> pd.Series:\n2304         \"\"\"Convert this array into a pandas.Series.\n2305 \n2306         The Series is indexed by the Cartesian product of index coordinates\n2307         (in the form of a :py:class:`pandas.MultiIndex`).\n2308         \"\"\"\n2309         index = self.coords.to_index()\n2310         return pd.Series(self.values.reshape(-1), index=index, name=self.name)\n2311 \n2312     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:\n2313         \"\"\"Convert this array into a numpy.ma.MaskedArray\n2314 \n2315         Parameters\n2316         ----------\n2317         copy : bool\n2318             If True (default) make a copy of the array in the result. If False,\n2319             a MaskedArray view of DataArray.values is returned.\n2320 \n2321         Returns\n2322         -------\n2323         result : MaskedArray\n2324             Masked where invalid values (nan or inf) occur.\n2325         \"\"\"\n2326         values = self.values  # only compute lazy arrays once\n2327         isnull = pd.isnull(values)\n2328         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)\n2329 \n2330     def to_netcdf(self, *args, **kwargs) -> Union[bytes, \"Delayed\", None]:\n2331         \"\"\"Write DataArray contents to a netCDF file.\n2332 \n2333         All parameters are passed directly to `xarray.Dataset.to_netcdf`.\n2334 \n2335         Notes\n2336         -----\n2337         Only xarray.Dataset objects can be written to netCDF files, so\n2338         the xarray.DataArray is converted to a xarray.Dataset object\n2339         containing a single variable. If the DataArray has no name, or if the\n2340         name is the same as a co-ordinate name, then it is given the name\n2341         '__xarray_dataarray_variable__'.\n2342         \"\"\"\n2343         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE\n2344 \n2345         if self.name is None:\n2346             # If no name is set then use a generic xarray name\n2347             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n2348         elif self.name in self.coords or self.name in self.dims:\n2349             # The name is the same as one of the coords names, which netCDF\n2350             # doesn't support, so rename it but keep track of the old name\n2351             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n2352             dataset.attrs[DATAARRAY_NAME] = self.name\n2353         else:\n2354             # No problems with the name - so we're fine!\n2355             dataset = self.to_dataset()\n2356 \n2357         return dataset.to_netcdf(*args, **kwargs)\n2358 \n2359     def to_dict(self, data: bool = True) -> dict:\n2360         \"\"\"\n2361         Convert this xarray.DataArray into a dictionary following xarray\n2362         naming conventions.\n2363 \n2364         Converts all variables and attributes to native Python objects.\n2365         Useful for coverting to json. To avoid datetime incompatibility\n2366         use decode_times=False kwarg in xarrray.open_dataset.\n2367 \n2368         Parameters\n2369         ----------\n2370         data : bool, optional\n2371             Whether to include the actual data in the dictionary. When set to\n2372             False, returns just the schema.\n2373 \n2374         See also\n2375         --------\n2376         DataArray.from_dict\n2377         \"\"\"\n2378         d = self.variable.to_dict(data=data)\n2379         d.update({\"coords\": {}, \"name\": self.name})\n2380         for k in self.coords:\n2381             d[\"coords\"][k] = self.coords[k].variable.to_dict(data=data)\n2382         return d\n2383 \n2384     @classmethod\n2385     def from_dict(cls, d: dict) -> \"DataArray\":\n2386         \"\"\"\n2387         Convert a dictionary into an xarray.DataArray\n2388 \n2389         Input dict can take several forms::\n2390 \n2391             d = {'dims': ('t'), 'data': x}\n2392 \n2393             d = {'coords': {'t': {'dims': 't', 'data': t,\n2394                                   'attrs': {'units':'s'}}},\n2395                  'attrs': {'title': 'air temperature'},\n2396                  'dims': 't',\n2397                  'data': x,\n2398                  'name': 'a'}\n2399 \n2400         where 't' is the name of the dimesion, 'a' is the name of the array,\n2401         and  x and t are lists, numpy.arrays, or pandas objects.\n2402 \n2403         Parameters\n2404         ----------\n2405         d : dict, with a minimum structure of {'dims': [..], 'data': [..]}\n2406 \n2407         Returns\n2408         -------\n2409         obj : xarray.DataArray\n2410 \n2411         See also\n2412         --------\n2413         DataArray.to_dict\n2414         Dataset.from_dict\n2415         \"\"\"\n2416         coords = None\n2417         if \"coords\" in d:\n2418             try:\n2419                 coords = {\n2420                     k: (v[\"dims\"], v[\"data\"], v.get(\"attrs\"))\n2421                     for k, v in d[\"coords\"].items()\n2422                 }\n2423             except KeyError as e:\n2424                 raise ValueError(\n2425                     \"cannot convert dict when coords are missing the key \"\n2426                     \"'{dims_data}'\".format(dims_data=str(e.args[0]))\n2427                 )\n2428         try:\n2429             data = d[\"data\"]\n2430         except KeyError:\n2431             raise ValueError(\"cannot convert dict without the key 'data''\")\n2432         else:\n2433             obj = cls(data, coords, d.get(\"dims\"), d.get(\"name\"), d.get(\"attrs\"))\n2434         return obj\n2435 \n2436     @classmethod\n2437     def from_series(cls, series: pd.Series, sparse: bool = False) -> \"DataArray\":\n2438         \"\"\"Convert a pandas.Series into an xarray.DataArray.\n2439 \n2440         If the series's index is a MultiIndex, it will be expanded into a\n2441         tensor product of one-dimensional coordinates (filling in missing\n2442         values with NaN). Thus this operation should be the inverse of the\n2443         `to_series` method.\n2444 \n2445         If sparse=True, creates a sparse array instead of a dense NumPy array.\n2446         Requires the pydata/sparse package.\n2447 \n2448         See also\n2449         --------\n2450         xarray.Dataset.from_dataframe\n2451         \"\"\"\n2452         temp_name = \"__temporary_name\"\n2453         df = pd.DataFrame({temp_name: series})\n2454         ds = Dataset.from_dataframe(df, sparse=sparse)\n2455         result = cast(DataArray, ds[temp_name])\n2456         result.name = series.name\n2457         return result\n2458 \n2459     def to_cdms2(self) -> \"cdms2_Variable\":\n2460         \"\"\"Convert this array into a cdms2.Variable\n2461         \"\"\"\n2462         from ..convert import to_cdms2\n2463 \n2464         return to_cdms2(self)\n2465 \n2466     @classmethod\n2467     def from_cdms2(cls, variable: \"cdms2_Variable\") -> \"DataArray\":\n2468         \"\"\"Convert a cdms2.Variable into an xarray.DataArray\n2469         \"\"\"\n2470         from ..convert import from_cdms2\n2471 \n2472         return from_cdms2(variable)\n2473 \n2474     def to_iris(self) -> \"iris_Cube\":\n2475         \"\"\"Convert this array into a iris.cube.Cube\n2476         \"\"\"\n2477         from ..convert import to_iris\n2478 \n2479         return to_iris(self)\n2480 \n2481     @classmethod\n2482     def from_iris(cls, cube: \"iris_Cube\") -> \"DataArray\":\n2483         \"\"\"Convert a iris.cube.Cube into an xarray.DataArray\n2484         \"\"\"\n2485         from ..convert import from_iris\n2486 \n2487         return from_iris(cube)\n2488 \n2489     def _all_compat(self, other: \"DataArray\", compat_str: str) -> bool:\n2490         \"\"\"Helper function for equals, broadcast_equals, and identical\n2491         \"\"\"\n2492 \n2493         def compat(x, y):\n2494             return getattr(x.variable, compat_str)(y.variable)\n2495 \n2496         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(\n2497             self, other\n2498         )\n2499 \n2500     def broadcast_equals(self, other: \"DataArray\") -> bool:\n2501         \"\"\"Two DataArrays are broadcast equal if they are equal after\n2502         broadcasting them against each other such that they have the same\n2503         dimensions.\n2504 \n2505         See Also\n2506         --------\n2507         DataArray.equals\n2508         DataArray.identical\n2509         \"\"\"\n2510         try:\n2511             return self._all_compat(other, \"broadcast_equals\")\n2512         except (TypeError, AttributeError):\n2513             return False\n2514 \n2515     def equals(self, other: \"DataArray\") -> bool:\n2516         \"\"\"True if two DataArrays have the same dimensions, coordinates and\n2517         values; otherwise False.\n2518 \n2519         DataArrays can still be equal (like pandas objects) if they have NaN\n2520         values in the same locations.\n2521 \n2522         This method is necessary because `v1 == v2` for ``DataArray``\n2523         does element-wise comparisons (like numpy.ndarrays).\n2524 \n2525         See Also\n2526         --------\n2527         DataArray.broadcast_equals\n2528         DataArray.identical\n2529         \"\"\"\n2530         try:\n2531             return self._all_compat(other, \"equals\")\n2532         except (TypeError, AttributeError):\n2533             return False\n2534 \n2535     def identical(self, other: \"DataArray\") -> bool:\n2536         \"\"\"Like equals, but also checks the array name and attributes, and\n2537         attributes on all coordinates.\n2538 \n2539         See Also\n2540         --------\n2541         DataArray.broadcast_equals\n2542         DataArray.equal\n2543         \"\"\"\n2544         try:\n2545             return self.name == other.name and self._all_compat(other, \"identical\")\n2546         except (TypeError, AttributeError):\n2547             return False\n2548 \n2549     def _result_name(self, other: Any = None) -> Optional[Hashable]:\n2550         # use the same naming heuristics as pandas:\n2551         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356\n2552         other_name = getattr(other, \"name\", _default)\n2553         if other_name is _default or other_name == self.name:\n2554             return self.name\n2555         else:\n2556             return None\n2557 \n2558     def __array_wrap__(self, obj, context=None) -> \"DataArray\":\n2559         new_var = self.variable.__array_wrap__(obj, context)\n2560         return self._replace(new_var)\n2561 \n2562     def __matmul__(self, obj):\n2563         return self.dot(obj)\n2564 \n2565     def __rmatmul__(self, other):\n2566         # currently somewhat duplicative, as only other DataArrays are\n2567         # compatible with matmul\n2568         return computation.dot(other, self)\n2569 \n2570     @staticmethod\n2571     def _unary_op(f: Callable[..., Any]) -> Callable[..., \"DataArray\"]:\n2572         @functools.wraps(f)\n2573         def func(self, *args, **kwargs):\n2574             with np.errstate(all=\"ignore\"):\n2575                 return self.__array_wrap__(f(self.variable.data, *args, **kwargs))\n2576 \n2577         return func\n2578 \n2579     @staticmethod\n2580     def _binary_op(\n2581         f: Callable[..., Any],\n2582         reflexive: bool = False,\n2583         join: str = None,  # see xarray.align\n2584         **ignored_kwargs,\n2585     ) -> Callable[..., \"DataArray\"]:\n2586         @functools.wraps(f)\n2587         def func(self, other):\n2588             if isinstance(other, (Dataset, groupby.GroupBy)):\n2589                 return NotImplemented\n2590             if isinstance(other, DataArray):\n2591                 align_type = OPTIONS[\"arithmetic_join\"] if join is None else join\n2592                 self, other = align(self, other, join=align_type, copy=False)\n2593             other_variable = getattr(other, \"variable\", other)\n2594             other_coords = getattr(other, \"coords\", None)\n2595 \n2596             variable = (\n2597                 f(self.variable, other_variable)\n2598                 if not reflexive\n2599                 else f(other_variable, self.variable)\n2600             )\n2601             coords, indexes = self.coords._merge_raw(other_coords)\n2602             name = self._result_name(other)\n2603 \n2604             return self._replace(variable, coords, name, indexes=indexes)\n2605 \n2606         return func\n2607 \n2608     @staticmethod\n2609     def _inplace_binary_op(f: Callable) -> Callable[..., \"DataArray\"]:\n2610         @functools.wraps(f)\n2611         def func(self, other):\n2612             if isinstance(other, groupby.GroupBy):\n2613                 raise TypeError(\n2614                     \"in-place operations between a DataArray and \"\n2615                     \"a grouped object are not permitted\"\n2616                 )\n2617             # n.b. we can't align other to self (with other.reindex_like(self))\n2618             # because `other` may be converted into floats, which would cause\n2619             # in-place arithmetic to fail unpredictably. Instead, we simply\n2620             # don't support automatic alignment with in-place arithmetic.\n2621             other_coords = getattr(other, \"coords\", None)\n2622             other_variable = getattr(other, \"variable\", other)\n2623             with self.coords._merge_inplace(other_coords):\n2624                 f(self.variable, other_variable)\n2625             return self\n2626 \n2627         return func\n2628 \n2629     def _copy_attrs_from(self, other: Union[\"DataArray\", Dataset, Variable]) -> None:\n2630         self.attrs = other.attrs\n2631 \n2632     @property\n2633     def plot(self) -> _PlotMethods:\n2634         \"\"\"\n2635         Access plotting functions for DataArray's\n2636 \n2637         >>> d = DataArray([[1, 2], [3, 4]])\n2638 \n2639         For convenience just call this directly\n2640 \n2641         >>> d.plot()\n2642 \n2643         Or use it as a namespace to use xarray.plot functions as\n2644         DataArray methods\n2645 \n2646         >>> d.plot.imshow()  # equivalent to xarray.plot.imshow(d)\n2647 \n2648         \"\"\"\n2649         return _PlotMethods(self)\n2650 \n2651     def _title_for_slice(self, truncate: int = 50) -> str:\n2652         \"\"\"\n2653         If the dataarray has 1 dimensional coordinates or comes from a slice\n2654         we can show that info in the title\n2655 \n2656         Parameters\n2657         ----------\n2658         truncate : integer\n2659             maximum number of characters for title\n2660 \n2661         Returns\n2662         -------\n2663         title : string\n2664             Can be used for plot titles\n2665 \n2666         \"\"\"\n2667         one_dims = []\n2668         for dim, coord in self.coords.items():\n2669             if coord.size == 1:\n2670                 one_dims.append(\n2671                     \"{dim} = {v}\".format(dim=dim, v=format_item(coord.values))\n2672                 )\n2673 \n2674         title = \", \".join(one_dims)\n2675         if len(title) > truncate:\n2676             title = title[: (truncate - 3)] + \"...\"\n2677 \n2678         return title\n2679 \n2680     def diff(self, dim: Hashable, n: int = 1, label: Hashable = \"upper\") -> \"DataArray\":\n2681         \"\"\"Calculate the n-th order discrete difference along given axis.\n2682 \n2683         Parameters\n2684         ----------\n2685         dim : hashable, optional\n2686             Dimension over which to calculate the finite difference.\n2687         n : int, optional\n2688             The number of times values are differenced.\n2689         label : hashable, optional\n2690             The new coordinate in dimension ``dim`` will have the\n2691             values of either the minuend's or subtrahend's coordinate\n2692             for values 'upper' and 'lower', respectively.  Other\n2693             values are not supported.\n2694 \n2695         Returns\n2696         -------\n2697         difference : same type as caller\n2698             The n-th order finite difference of this object.\n2699 \n2700         Examples\n2701         --------\n2702         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ['x'])\n2703         >>> arr.diff('x')\n2704         <xarray.DataArray (x: 3)>\n2705         array([0, 1, 0])\n2706         Coordinates:\n2707         * x        (x) int64 2 3 4\n2708         >>> arr.diff('x', 2)\n2709         <xarray.DataArray (x: 2)>\n2710         array([ 1, -1])\n2711         Coordinates:\n2712         * x        (x) int64 3 4\n2713 \n2714         See Also\n2715         --------\n2716         DataArray.differentiate\n2717         \"\"\"\n2718         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)\n2719         return self._from_temp_dataset(ds)\n2720 \n2721     def shift(\n2722         self,\n2723         shifts: Mapping[Hashable, int] = None,\n2724         fill_value: Any = dtypes.NA,\n2725         **shifts_kwargs: int,\n2726     ) -> \"DataArray\":\n2727         \"\"\"Shift this array by an offset along one or more dimensions.\n2728 \n2729         Only the data is moved; coordinates stay in place. Values shifted from\n2730         beyond array bounds are replaced by NaN. This is consistent with the\n2731         behavior of ``shift`` in pandas.\n2732 \n2733         Parameters\n2734         ----------\n2735         shifts : Mapping with the form of {dim: offset}\n2736             Integer offset to shift along each of the given dimensions.\n2737             Positive offsets shift to the right; negative offsets shift to the\n2738             left.\n2739         fill_value: scalar, optional\n2740             Value to use for newly missing values\n2741         **shifts_kwargs:\n2742             The keyword arguments form of ``shifts``.\n2743             One of shifts or shifts_kwargs must be provided.\n2744 \n2745         Returns\n2746         -------\n2747         shifted : DataArray\n2748             DataArray with the same coordinates and attributes but shifted\n2749             data.\n2750 \n2751         See also\n2752         --------\n2753         roll\n2754 \n2755         Examples\n2756         --------\n2757 \n2758         >>> arr = xr.DataArray([5, 6, 7], dims='x')\n2759         >>> arr.shift(x=1)\n2760         <xarray.DataArray (x: 3)>\n2761         array([ nan,   5.,   6.])\n2762         Coordinates:\n2763           * x        (x) int64 0 1 2\n2764         \"\"\"\n2765         variable = self.variable.shift(\n2766             shifts=shifts, fill_value=fill_value, **shifts_kwargs\n2767         )\n2768         return self._replace(variable=variable)\n2769 \n2770     def roll(\n2771         self,\n2772         shifts: Mapping[Hashable, int] = None,\n2773         roll_coords: bool = None,\n2774         **shifts_kwargs: int,\n2775     ) -> \"DataArray\":\n2776         \"\"\"Roll this array by an offset along one or more dimensions.\n2777 \n2778         Unlike shift, roll may rotate all variables, including coordinates\n2779         if specified. The direction of rotation is consistent with\n2780         :py:func:`numpy.roll`.\n2781 \n2782         Parameters\n2783         ----------\n2784         shifts : Mapping with the form of {dim: offset}\n2785             Integer offset to rotate each of the given dimensions.\n2786             Positive offsets roll to the right; negative offsets roll to the\n2787             left.\n2788         roll_coords : bool\n2789             Indicates whether to  roll the coordinates by the offset\n2790             The current default of roll_coords (None, equivalent to True) is\n2791             deprecated and will change to False in a future version.\n2792             Explicitly pass roll_coords to silence the warning.\n2793         **shifts_kwargs : The keyword arguments form of ``shifts``.\n2794             One of shifts or shifts_kwargs must be provided.\n2795 \n2796         Returns\n2797         -------\n2798         rolled : DataArray\n2799             DataArray with the same attributes but rolled data and coordinates.\n2800 \n2801         See also\n2802         --------\n2803         shift\n2804 \n2805         Examples\n2806         --------\n2807 \n2808         >>> arr = xr.DataArray([5, 6, 7], dims='x')\n2809         >>> arr.roll(x=1)\n2810         <xarray.DataArray (x: 3)>\n2811         array([7, 5, 6])\n2812         Coordinates:\n2813           * x        (x) int64 2 0 1\n2814         \"\"\"\n2815         ds = self._to_temp_dataset().roll(\n2816             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs\n2817         )\n2818         return self._from_temp_dataset(ds)\n2819 \n2820     @property\n2821     def real(self) -> \"DataArray\":\n2822         return self._replace(self.variable.real)\n2823 \n2824     @property\n2825     def imag(self) -> \"DataArray\":\n2826         return self._replace(self.variable.imag)\n2827 \n2828     def dot(\n2829         self, other: \"DataArray\", dims: Union[Hashable, Sequence[Hashable], None] = None\n2830     ) -> \"DataArray\":\n2831         \"\"\"Perform dot product of two DataArrays along their shared dims.\n2832 \n2833         Equivalent to taking taking tensordot over all shared dims.\n2834 \n2835         Parameters\n2836         ----------\n2837         other : DataArray\n2838             The other array with which the dot product is performed.\n2839         dims: '...', hashable or sequence of hashables, optional\n2840             Which dimensions to sum over. Ellipsis ('...') sums over all dimensions.\n2841             If not specified, then all the common dimensions are summed over.\n2842 \n2843         Returns\n2844         -------\n2845         result : DataArray\n2846             Array resulting from the dot product over all shared dimensions.\n2847 \n2848         See also\n2849         --------\n2850         dot\n2851         numpy.tensordot\n2852 \n2853         Examples\n2854         --------\n2855 \n2856         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))\n2857         >>> da = DataArray(da_vals, dims=['x', 'y', 'z'])\n2858         >>> dm_vals = np.arange(4)\n2859         >>> dm = DataArray(dm_vals, dims=['z'])\n2860 \n2861         >>> dm.dims\n2862         ('z')\n2863         >>> da.dims\n2864         ('x', 'y', 'z')\n2865 \n2866         >>> dot_result = da.dot(dm)\n2867         >>> dot_result.dims\n2868         ('x', 'y')\n2869         \"\"\"\n2870         if isinstance(other, Dataset):\n2871             raise NotImplementedError(\n2872                 \"dot products are not yet supported with Dataset objects.\"\n2873             )\n2874         if not isinstance(other, DataArray):\n2875             raise TypeError(\"dot only operates on DataArrays.\")\n2876 \n2877         return computation.dot(self, other, dims=dims)\n2878 \n2879     def sortby(\n2880         self,\n2881         variables: Union[Hashable, \"DataArray\", Sequence[Union[Hashable, \"DataArray\"]]],\n2882         ascending: bool = True,\n2883     ) -> \"DataArray\":\n2884         \"\"\"Sort object by labels or values (along an axis).\n2885 \n2886         Sorts the dataarray, either along specified dimensions,\n2887         or according to values of 1-D dataarrays that share dimension\n2888         with calling object.\n2889 \n2890         If the input variables are dataarrays, then the dataarrays are aligned\n2891         (via left-join) to the calling object prior to sorting by cell values.\n2892         NaNs are sorted to the end, following Numpy convention.\n2893 \n2894         If multiple sorts along the same dimension is\n2895         given, numpy's lexsort is performed along that dimension:\n2896         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html\n2897         and the FIRST key in the sequence is used as the primary sort key,\n2898         followed by the 2nd key, etc.\n2899 \n2900         Parameters\n2901         ----------\n2902         variables: hashable, DataArray, or sequence of either\n2903             1D DataArray objects or name(s) of 1D variable(s) in\n2904             coords whose values are used to sort this array.\n2905         ascending: boolean, optional\n2906             Whether to sort by ascending or descending order.\n2907 \n2908         Returns\n2909         -------\n2910         sorted: DataArray\n2911             A new dataarray where all the specified dims are sorted by dim\n2912             labels.\n2913 \n2914         Examples\n2915         --------\n2916 \n2917         >>> da = xr.DataArray(np.random.rand(5),\n2918         ...                   coords=[pd.date_range('1/1/2000', periods=5)],\n2919         ...                   dims='time')\n2920         >>> da\n2921         <xarray.DataArray (time: 5)>\n2922         array([ 0.965471,  0.615637,  0.26532 ,  0.270962,  0.552878])\n2923         Coordinates:\n2924           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\n2925 \n2926         >>> da.sortby(da)\n2927         <xarray.DataArray (time: 5)>\n2928         array([ 0.26532 ,  0.270962,  0.552878,  0.615637,  0.965471])\n2929         Coordinates:\n2930           * time     (time) datetime64[ns] 2000-01-03 2000-01-04 2000-01-05 ...\n2931         \"\"\"\n2932         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)\n2933         return self._from_temp_dataset(ds)\n2934 \n2935     def quantile(\n2936         self,\n2937         q: Any,\n2938         dim: Union[Hashable, Sequence[Hashable], None] = None,\n2939         interpolation: str = \"linear\",\n2940         keep_attrs: bool = None,\n2941     ) -> \"DataArray\":\n2942         \"\"\"Compute the qth quantile of the data along the specified dimension.\n2943 \n2944         Returns the qth quantiles(s) of the array elements.\n2945 \n2946         Parameters\n2947         ----------\n2948         q : float in range of [0,1] or array-like of floats\n2949             Quantile to compute, which must be between 0 and 1 inclusive.\n2950         dim : hashable or sequence of hashable, optional\n2951             Dimension(s) over which to apply quantile.\n2952         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n2953             This optional parameter specifies the interpolation method to\n2954             use when the desired quantile lies between two data points\n2955             ``i < j``:\n2956 \n2957                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is\n2958                   the fractional part of the index surrounded by ``i`` and\n2959                   ``j``.\n2960                 - lower: ``i``.\n2961                 - higher: ``j``.\n2962                 - nearest: ``i`` or ``j``, whichever is nearest.\n2963                 - midpoint: ``(i + j) / 2``.\n2964         keep_attrs : bool, optional\n2965             If True, the dataset's attributes (`attrs`) will be copied from\n2966             the original object to the new one.  If False (default), the new\n2967             object will be returned without attributes.\n2968 \n2969         Returns\n2970         -------\n2971         quantiles : DataArray\n2972             If `q` is a single quantile, then the result\n2973             is a scalar. If multiple percentiles are given, first axis of\n2974             the result corresponds to the quantile and a quantile dimension\n2975             is added to the return array. The other dimensions are the\n2976             dimensions that remain after the reduction of the array.\n2977 \n2978         See Also\n2979         --------\n2980         numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile\n2981 \n2982         Examples\n2983         --------\n2984 \n2985         >>> da = xr.DataArray(\n2986         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],\n2987         ...     coords={\"x\": [7, 9], \"y\": [1, 1.5, 2, 2.5]},\n2988         ...     dims=(\"x\", \"y\"),\n2989         ... )\n2990         >>> da.quantile(0)  # or da.quantile(0, dim=...)\n2991         <xarray.DataArray ()>\n2992         array(0.7)\n2993         Coordinates:\n2994             quantile  float64 0.0\n2995         >>> da.quantile(0, dim=\"x\")\n2996         <xarray.DataArray (y: 4)>\n2997         array([0.7, 4.2, 2.6, 1.5])\n2998         Coordinates:\n2999           * y         (y) float64 1.0 1.5 2.0 2.5\n3000             quantile  float64 0.0\n3001         >>> da.quantile([0, 0.5, 1])\n3002         <xarray.DataArray (quantile: 3)>\n3003         array([0.7, 3.4, 9.4])\n3004         Coordinates:\n3005           * quantile  (quantile) float64 0.0 0.5 1.0\n3006         >>> da.quantile([0, 0.5, 1], dim=\"x\")\n3007         <xarray.DataArray (quantile: 3, y: 4)>\n3008         array([[0.7 , 4.2 , 2.6 , 1.5 ],\n3009                [3.6 , 5.75, 6.  , 1.7 ],\n3010                [6.5 , 7.3 , 9.4 , 1.9 ]])\n3011         Coordinates:\n3012           * y         (y) float64 1.0 1.5 2.0 2.5\n3013           * quantile  (quantile) float64 0.0 0.5 1.0\n3014         \"\"\"\n3015 \n3016         ds = self._to_temp_dataset().quantile(\n3017             q, dim=dim, keep_attrs=keep_attrs, interpolation=interpolation\n3018         )\n3019         return self._from_temp_dataset(ds)\n3020 \n3021     def rank(\n3022         self, dim: Hashable, pct: bool = False, keep_attrs: bool = None\n3023     ) -> \"DataArray\":\n3024         \"\"\"Ranks the data.\n3025 \n3026         Equal values are assigned a rank that is the average of the ranks that\n3027         would have been otherwise assigned to all of the values within that\n3028         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.\n3029 \n3030         NaNs in the input array are returned as NaNs.\n3031 \n3032         The `bottleneck` library is required.\n3033 \n3034         Parameters\n3035         ----------\n3036         dim : hashable\n3037             Dimension over which to compute rank.\n3038         pct : bool, optional\n3039             If True, compute percentage ranks, otherwise compute integer ranks.\n3040         keep_attrs : bool, optional\n3041             If True, the dataset's attributes (`attrs`) will be copied from\n3042             the original object to the new one.  If False (default), the new\n3043             object will be returned without attributes.\n3044 \n3045         Returns\n3046         -------\n3047         ranked : DataArray\n3048             DataArray with the same coordinates and dtype 'float64'.\n3049 \n3050         Examples\n3051         --------\n3052 \n3053         >>> arr = xr.DataArray([5, 6, 7], dims='x')\n3054         >>> arr.rank('x')\n3055         <xarray.DataArray (x: 3)>\n3056         array([ 1.,   2.,   3.])\n3057         Dimensions without coordinates: x\n3058         \"\"\"\n3059 \n3060         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)\n3061         return self._from_temp_dataset(ds)\n3062 \n3063     def differentiate(\n3064         self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None\n3065     ) -> \"DataArray\":\n3066         \"\"\" Differentiate the array with the second order accurate central\n3067         differences.\n3068 \n3069         .. note::\n3070             This feature is limited to simple cartesian geometry, i.e. coord\n3071             must be one dimensional.\n3072 \n3073         Parameters\n3074         ----------\n3075         coord: hashable\n3076             The coordinate to be used to compute the gradient.\n3077         edge_order: 1 or 2. Default 1\n3078             N-th order accurate differences at the boundaries.\n3079         datetime_unit: None or any of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms',\n3080             'us', 'ns', 'ps', 'fs', 'as'}\n3081             Unit to compute gradient. Only valid for datetime coordinate.\n3082 \n3083         Returns\n3084         -------\n3085         differentiated: DataArray\n3086 \n3087         See also\n3088         --------\n3089         numpy.gradient: corresponding numpy function\n3090 \n3091         Examples\n3092         --------\n3093 \n3094         >>> da = xr.DataArray(np.arange(12).reshape(4, 3), dims=['x', 'y'],\n3095         ...                   coords={'x': [0, 0.1, 1.1, 1.2]})\n3096         >>> da\n3097         <xarray.DataArray (x: 4, y: 3)>\n3098         array([[ 0,  1,  2],\n3099                [ 3,  4,  5],\n3100                [ 6,  7,  8],\n3101                [ 9, 10, 11]])\n3102         Coordinates:\n3103           * x        (x) float64 0.0 0.1 1.1 1.2\n3104         Dimensions without coordinates: y\n3105         >>>\n3106         >>> da.differentiate('x')\n3107         <xarray.DataArray (x: 4, y: 3)>\n3108         array([[30.      , 30.      , 30.      ],\n3109                [27.545455, 27.545455, 27.545455],\n3110                [27.545455, 27.545455, 27.545455],\n3111                [30.      , 30.      , 30.      ]])\n3112         Coordinates:\n3113           * x        (x) float64 0.0 0.1 1.1 1.2\n3114         Dimensions without coordinates: y\n3115         \"\"\"\n3116         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)\n3117         return self._from_temp_dataset(ds)\n3118 \n3119     def integrate(\n3120         self, dim: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None\n3121     ) -> \"DataArray\":\n3122         \"\"\" integrate the array with the trapezoidal rule.\n3123 \n3124         .. note::\n3125             This feature is limited to simple cartesian geometry, i.e. dim\n3126             must be one dimensional.\n3127 \n3128         Parameters\n3129         ----------\n3130         dim: hashable, or a sequence of hashable\n3131             Coordinate(s) used for the integration.\n3132         datetime_unit: str, optional\n3133             Can be used to specify the unit if datetime coordinate is used.\n3134             One of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', 'ps',\n3135             'fs', 'as'}\n3136 \n3137         Returns\n3138         -------\n3139         integrated: DataArray\n3140 \n3141         See also\n3142         --------\n3143         numpy.trapz: corresponding numpy function\n3144 \n3145         Examples\n3146         --------\n3147 \n3148         >>> da = xr.DataArray(np.arange(12).reshape(4, 3), dims=['x', 'y'],\n3149         ...                   coords={'x': [0, 0.1, 1.1, 1.2]})\n3150         >>> da\n3151         <xarray.DataArray (x: 4, y: 3)>\n3152         array([[ 0,  1,  2],\n3153                [ 3,  4,  5],\n3154                [ 6,  7,  8],\n3155                [ 9, 10, 11]])\n3156         Coordinates:\n3157           * x        (x) float64 0.0 0.1 1.1 1.2\n3158         Dimensions without coordinates: y\n3159         >>>\n3160         >>> da.integrate('x')\n3161         <xarray.DataArray (y: 3)>\n3162         array([5.4, 6.6, 7.8])\n3163         Dimensions without coordinates: y\n3164         \"\"\"\n3165         ds = self._to_temp_dataset().integrate(dim, datetime_unit)\n3166         return self._from_temp_dataset(ds)\n3167 \n3168     def unify_chunks(self) -> \"DataArray\":\n3169         \"\"\" Unify chunk size along all chunked dimensions of this DataArray.\n3170 \n3171         Returns\n3172         -------\n3173 \n3174         DataArray with consistent chunk sizes for all dask-array variables\n3175 \n3176         See Also\n3177         --------\n3178 \n3179         dask.array.core.unify_chunks\n3180         \"\"\"\n3181         ds = self._to_temp_dataset().unify_chunks()\n3182         return self._from_temp_dataset(ds)\n3183 \n3184     def map_blocks(\n3185         self,\n3186         func: \"Callable[..., T_DSorDA]\",\n3187         args: Sequence[Any] = (),\n3188         kwargs: Mapping[str, Any] = None,\n3189     ) -> \"T_DSorDA\":\n3190         \"\"\"\n3191         Apply a function to each chunk of this DataArray. This method is experimental\n3192         and its signature may change.\n3193 \n3194         Parameters\n3195         ----------\n3196         func: callable\n3197             User-provided function that accepts a DataArray as its first parameter. The\n3198             function will receive a subset of this DataArray, corresponding to one chunk\n3199             along each chunked dimension. ``func`` will be executed as\n3200             ``func(obj_subset, *args, **kwargs)``.\n3201 \n3202             The function will be first run on mocked-up data, that looks like this array\n3203             but has sizes 0, to determine properties of the returned object such as\n3204             dtype, variable names, new dimensions and new indexes (if any).\n3205 \n3206             This function must return either a single DataArray or a single Dataset.\n3207 \n3208             This function cannot change size of existing dimensions, or add new chunked\n3209             dimensions.\n3210         args: Sequence\n3211             Passed verbatim to func after unpacking, after the sliced DataArray. xarray\n3212             objects, if any, will not be split by chunks. Passing dask collections is\n3213             not allowed.\n3214         kwargs: Mapping\n3215             Passed verbatim to func after unpacking. xarray objects, if any, will not be\n3216             split by chunks. Passing dask collections is not allowed.\n3217 \n3218         Returns\n3219         -------\n3220         A single DataArray or Dataset with dask backend, reassembled from the outputs of\n3221         the function.\n3222 \n3223         Notes\n3224         -----\n3225         This method is designed for when one needs to manipulate a whole xarray object\n3226         within each chunk. In the more common case where one can work on numpy arrays,\n3227         it is recommended to use apply_ufunc.\n3228 \n3229         If none of the variables in this DataArray is backed by dask, calling this\n3230         method is equivalent to calling ``func(self, *args, **kwargs)``.\n3231 \n3232         See Also\n3233         --------\n3234         dask.array.map_blocks, xarray.apply_ufunc, xarray.map_blocks,\n3235         xarray.Dataset.map_blocks\n3236         \"\"\"\n3237         from .parallel import map_blocks\n3238 \n3239         return map_blocks(func, self, args, kwargs)\n3240 \n3241     # this needs to be at the end, or mypy will confuse with `str`\n3242     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names\n3243     str = property(StringAccessor)\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "xarray/core/variable.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/variable.py",
        "start": 1683,
        "end": 1763,
        "class_name": "Variable",
        "method_name": "quantile",
        "code": "1683     def quantile(self, q, dim=None, interpolation=\"linear\", keep_attrs=None):\n1684         \"\"\"Compute the qth quantile of the data along the specified dimension.\n1685 \n1686         Returns the qth quantiles(s) of the array elements.\n1687 \n1688         Parameters\n1689         ----------\n1690         q : float in range of [0,1] (or sequence of floats)\n1691             Quantile to compute, which must be between 0 and 1\n1692             inclusive.\n1693         dim : str or sequence of str, optional\n1694             Dimension(s) over which to apply quantile.\n1695         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n1696             This optional parameter specifies the interpolation method to\n1697             use when the desired quantile lies between two data points\n1698             ``i < j``:\n1699                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is\n1700                   the fractional part of the index surrounded by ``i`` and\n1701                   ``j``.\n1702                 * lower: ``i``.\n1703                 * higher: ``j``.\n1704                 * nearest: ``i`` or ``j``, whichever is nearest.\n1705                 * midpoint: ``(i + j) / 2``.\n1706         keep_attrs : bool, optional\n1707             If True, the variable's attributes (`attrs`) will be copied from\n1708             the original object to the new one.  If False (default), the new\n1709             object will be returned without attributes.\n1710 \n1711         Returns\n1712         -------\n1713         quantiles : Variable\n1714             If `q` is a single quantile, then the result\n1715             is a scalar. If multiple percentiles are given, first axis of\n1716             the result corresponds to the quantile and a quantile dimension\n1717             is added to the return array. The other dimensions are the\n1718             dimensions that remain after the reduction of the array.\n1719 \n1720         See Also\n1721         --------\n1722         numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile,\n1723         DataArray.quantile\n1724         \"\"\"\n1725 \n1726         from .computation import apply_ufunc\n1727 \n1728         if keep_attrs is None:\n1729             keep_attrs = _get_keep_attrs(default=False)\n1730 \n1731         scalar = utils.is_scalar(q)\n1732         q = np.atleast_1d(np.asarray(q, dtype=np.float64))\n1733 \n1734         if dim is None:\n1735             dim = self.dims\n1736 \n1737         if utils.is_scalar(dim):\n1738             dim = [dim]\n1739 \n1740         def _wrapper(npa, **kwargs):\n1741             # move quantile axis to end. required for apply_ufunc\n1742             return np.moveaxis(np.nanpercentile(npa, **kwargs), 0, -1)\n1743 \n1744         axis = np.arange(-1, -1 * len(dim) - 1, -1)\n1745         result = apply_ufunc(\n1746             _wrapper,\n1747             self,\n1748             input_core_dims=[dim],\n1749             exclude_dims=set(dim),\n1750             output_core_dims=[[\"quantile\"]],\n1751             output_dtypes=[np.float64],\n1752             output_sizes={\"quantile\": len(q)},\n1753             dask=\"parallelized\",\n1754             kwargs={\"q\": q * 100, \"axis\": axis, \"interpolation\": interpolation},\n1755         )\n1756 \n1757         # for backward compatibility\n1758         result = result.transpose(\"quantile\", ...)\n1759         if scalar:\n1760             result = result.squeeze(\"quantile\")\n1761         if keep_attrs:\n1762             result.attrs = self._attrs\n1763         return result\n",
        "intended_behavior": "Although the direct issue doesn't lie here, reviewing how errors are propagated when `np.nanpercentile` is called could be beneficial. Ensure that any error messages about input range exceptions are consistent with the quantile range [0, 1]. If `Variable.quantile` is where the numpy function calls are made, it should either handle the error and rethrow it with a message consistent with xarray's expected input range, or it should contain preemptive input validation to provide a clear and correct error message before deference to numpy's functions."
    },
    {
        "rel_file_path": "xarray/core/variable.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/variable.py",
        "start": 263,
        "end": 2016,
        "class_name": "Variable",
        "method_name": null,
        "code": "263 class Variable(\n264     common.AbstractArray, arithmetic.SupportsArithmetic, utils.NdimSizeLenMixin\n265 ):\n266     \"\"\"A netcdf-like variable consisting of dimensions, data and attributes\n267     which describe a single Array. A single Variable object is not fully\n268     described outside the context of its parent Dataset (if you want such a\n269     fully described object, use a DataArray instead).\n270 \n271     The main functional difference between Variables and numpy arrays is that\n272     numerical operations on Variables implement array broadcasting by dimension\n273     name. For example, adding an Variable with dimensions `('time',)` to\n274     another Variable with dimensions `('space',)` results in a new Variable\n275     with dimensions `('time', 'space')`. Furthermore, numpy reduce operations\n276     like ``mean`` or ``sum`` are overwritten to take a \"dimension\" argument\n277     instead of an \"axis\".\n278 \n279     Variables are light-weight objects used as the building block for datasets.\n280     They are more primitive objects, so operations with them provide marginally\n281     higher performance than using DataArrays. However, manipulating data in the\n282     form of a Dataset or DataArray should almost always be preferred, because\n283     they can use more complete metadata in context of coordinate labels.\n284     \"\"\"\n285 \n286     __slots__ = (\"_dims\", \"_data\", \"_attrs\", \"_encoding\")\n287 \n288     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n289         \"\"\"\n290         Parameters\n291         ----------\n292         dims : str or sequence of str\n293             Name(s) of the the data dimension(s). Must be either a string (only\n294             for 1D data) or a sequence of strings with length equal to the\n295             number of dimensions.\n296         data : array_like\n297             Data array which supports numpy-like data access.\n298         attrs : dict_like or None, optional\n299             Attributes to assign to the new variable. If None (default), an\n300             empty attribute dictionary is initialized.\n301         encoding : dict_like or None, optional\n302             Dictionary specifying how to encode this array's data into a\n303             serialized format like netCDF4. Currently used keys (for netCDF)\n304             include '_FillValue', 'scale_factor', 'add_offset' and 'dtype'.\n305             Well-behaved code to serialize a Variable should ignore\n306             unrecognized encoding items.\n307         \"\"\"\n308         self._data = as_compatible_data(data, fastpath=fastpath)\n309         self._dims = self._parse_dimensions(dims)\n310         self._attrs = None\n311         self._encoding = None\n312         if attrs is not None:\n313             self.attrs = attrs\n314         if encoding is not None:\n315             self.encoding = encoding\n316 \n317     @property\n318     def dtype(self):\n319         return self._data.dtype\n320 \n321     @property\n322     def shape(self):\n323         return self._data.shape\n324 \n325     @property\n326     def nbytes(self):\n327         return self.size * self.dtype.itemsize\n328 \n329     @property\n330     def _in_memory(self):\n331         return isinstance(self._data, (np.ndarray, np.number, PandasIndexAdapter)) or (\n332             isinstance(self._data, indexing.MemoryCachedArray)\n333             and isinstance(self._data.array, indexing.NumpyIndexingAdapter)\n334         )\n335 \n336     @property\n337     def data(self):\n338         if hasattr(self._data, \"__array_function__\") or isinstance(\n339             self._data, dask_array_type\n340         ):\n341             return self._data\n342         else:\n343             return self.values\n344 \n345     @data.setter\n346     def data(self, data):\n347         data = as_compatible_data(data)\n348         if data.shape != self.shape:\n349             raise ValueError(\n350                 f\"replacement data must match the Variable's shape. \"\n351                 f\"replacement data has shape {data.shape}; Variable has shape {self.shape}\"\n352             )\n353         self._data = data\n354 \n355     def load(self, **kwargs):\n356         \"\"\"Manually trigger loading of this variable's data from disk or a\n357         remote source into memory and return this variable.\n358 \n359         Normally, it should not be necessary to call this method in user code,\n360         because all xarray functions should either work on deferred data or\n361         load data automatically.\n362 \n363         Parameters\n364         ----------\n365         **kwargs : dict\n366             Additional keyword arguments passed on to ``dask.array.compute``.\n367 \n368         See Also\n369         --------\n370         dask.array.compute\n371         \"\"\"\n372         if isinstance(self._data, dask_array_type):\n373             self._data = as_compatible_data(self._data.compute(**kwargs))\n374         elif not hasattr(self._data, \"__array_function__\"):\n375             self._data = np.asarray(self._data)\n376         return self\n377 \n378     def compute(self, **kwargs):\n379         \"\"\"Manually trigger loading of this variable's data from disk or a\n380         remote source into memory and return a new variable. The original is\n381         left unaltered.\n382 \n383         Normally, it should not be necessary to call this method in user code,\n384         because all xarray functions should either work on deferred data or\n385         load data automatically.\n386 \n387         Parameters\n388         ----------\n389         **kwargs : dict\n390             Additional keyword arguments passed on to ``dask.array.compute``.\n391 \n392         See Also\n393         --------\n394         dask.array.compute\n395         \"\"\"\n396         new = self.copy(deep=False)\n397         return new.load(**kwargs)\n398 \n399     def __dask_tokenize__(self):\n400         # Use v.data, instead of v._data, in order to cope with the wrappers\n401         # around NetCDF and the like\n402         from dask.base import normalize_token\n403 \n404         return normalize_token((type(self), self._dims, self.data, self._attrs))\n405 \n406     def __dask_graph__(self):\n407         if isinstance(self._data, dask_array_type):\n408             return self._data.__dask_graph__()\n409         else:\n410             return None\n411 \n412     def __dask_keys__(self):\n413         return self._data.__dask_keys__()\n414 \n415     def __dask_layers__(self):\n416         return self._data.__dask_layers__()\n417 \n418     @property\n419     def __dask_optimize__(self):\n420         return self._data.__dask_optimize__\n421 \n422     @property\n423     def __dask_scheduler__(self):\n424         return self._data.__dask_scheduler__\n425 \n426     def __dask_postcompute__(self):\n427         array_func, array_args = self._data.__dask_postcompute__()\n428         return (\n429             self._dask_finalize,\n430             (array_func, array_args, self._dims, self._attrs, self._encoding),\n431         )\n432 \n433     def __dask_postpersist__(self):\n434         array_func, array_args = self._data.__dask_postpersist__()\n435         return (\n436             self._dask_finalize,\n437             (array_func, array_args, self._dims, self._attrs, self._encoding),\n438         )\n439 \n440     @staticmethod\n441     def _dask_finalize(results, array_func, array_args, dims, attrs, encoding):\n442         if isinstance(results, dict):  # persist case\n443             name = array_args[0]\n444             results = {k: v for k, v in results.items() if k[0] == name}\n445         data = array_func(results, *array_args)\n446         return Variable(dims, data, attrs=attrs, encoding=encoding)\n447 \n448     @property\n449     def values(self):\n450         \"\"\"The variable's data as a numpy.ndarray\"\"\"\n451         return _as_array_or_item(self._data)\n452 \n453     @values.setter\n454     def values(self, values):\n455         self.data = values\n456 \n457     def to_base_variable(self):\n458         \"\"\"Return this variable as a base xarray.Variable\"\"\"\n459         return Variable(\n460             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True\n461         )\n462 \n463     to_variable = utils.alias(to_base_variable, \"to_variable\")\n464 \n465     def to_index_variable(self):\n466         \"\"\"Return this variable as an xarray.IndexVariable\"\"\"\n467         return IndexVariable(\n468             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True\n469         )\n470 \n471     to_coord = utils.alias(to_index_variable, \"to_coord\")\n472 \n473     def to_index(self):\n474         \"\"\"Convert this variable to a pandas.Index\"\"\"\n475         return self.to_index_variable().to_index()\n476 \n477     def to_dict(self, data=True):\n478         \"\"\"Dictionary representation of variable.\"\"\"\n479         item = {\"dims\": self.dims, \"attrs\": decode_numpy_dict_values(self.attrs)}\n480         if data:\n481             item[\"data\"] = ensure_us_time_resolution(self.values).tolist()\n482         else:\n483             item.update({\"dtype\": str(self.dtype), \"shape\": self.shape})\n484         return item\n485 \n486     @property\n487     def dims(self):\n488         \"\"\"Tuple of dimension names with which this variable is associated.\n489         \"\"\"\n490         return self._dims\n491 \n492     @dims.setter\n493     def dims(self, value):\n494         self._dims = self._parse_dimensions(value)\n495 \n496     def _parse_dimensions(self, dims):\n497         if isinstance(dims, str):\n498             dims = (dims,)\n499         dims = tuple(dims)\n500         if len(dims) != self.ndim:\n501             raise ValueError(\n502                 \"dimensions %s must have the same length as the \"\n503                 \"number of data dimensions, ndim=%s\" % (dims, self.ndim)\n504             )\n505         return dims\n506 \n507     def _item_key_to_tuple(self, key):\n508         if utils.is_dict_like(key):\n509             return tuple(key.get(dim, slice(None)) for dim in self.dims)\n510         else:\n511             return key\n512 \n513     def _broadcast_indexes(self, key):\n514         \"\"\"Prepare an indexing key for an indexing operation.\n515 \n516         Parameters\n517         -----------\n518         key: int, slice, array, dict or tuple of integer, slices and arrays\n519             Any valid input for indexing.\n520 \n521         Returns\n522         -------\n523         dims: tuple\n524             Dimension of the resultant variable.\n525         indexers: IndexingTuple subclass\n526             Tuple of integer, array-like, or slices to use when indexing\n527             self._data. The type of this argument indicates the type of\n528             indexing to perform, either basic, outer or vectorized.\n529         new_order : Optional[Sequence[int]]\n530             Optional reordering to do on the result of indexing. If not None,\n531             the first len(new_order) indexing should be moved to these\n532             positions.\n533         \"\"\"\n534         key = self._item_key_to_tuple(key)  # key is a tuple\n535         # key is a tuple of full size\n536         key = indexing.expanded_indexer(key, self.ndim)\n537         # Convert a scalar Variable to an integer\n538         key = tuple(\n539             k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k for k in key\n540         )\n541         # Convert a 0d-array to an integer\n542         key = tuple(\n543             k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key\n544         )\n545 \n546         if all(isinstance(k, BASIC_INDEXING_TYPES) for k in key):\n547             return self._broadcast_indexes_basic(key)\n548 \n549         self._validate_indexers(key)\n550         # Detect it can be mapped as an outer indexer\n551         # If all key is unlabeled, or\n552         # key can be mapped as an OuterIndexer.\n553         if all(not isinstance(k, Variable) for k in key):\n554             return self._broadcast_indexes_outer(key)\n555 \n556         # If all key is 1-dimensional and there are no duplicate labels,\n557         # key can be mapped as an OuterIndexer.\n558         dims = []\n559         for k, d in zip(key, self.dims):\n560             if isinstance(k, Variable):\n561                 if len(k.dims) > 1:\n562                     return self._broadcast_indexes_vectorized(key)\n563                 dims.append(k.dims[0])\n564             elif not isinstance(k, integer_types):\n565                 dims.append(d)\n566         if len(set(dims)) == len(dims):\n567             return self._broadcast_indexes_outer(key)\n568 \n569         return self._broadcast_indexes_vectorized(key)\n570 \n571     def _broadcast_indexes_basic(self, key):\n572         dims = tuple(\n573             dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)\n574         )\n575         return dims, BasicIndexer(key), None\n576 \n577     def _validate_indexers(self, key):\n578         \"\"\" Make sanity checks \"\"\"\n579         for dim, k in zip(self.dims, key):\n580             if isinstance(k, BASIC_INDEXING_TYPES):\n581                 pass\n582             else:\n583                 if not isinstance(k, Variable):\n584                     k = np.asarray(k)\n585                     if k.ndim > 1:\n586                         raise IndexError(\n587                             \"Unlabeled multi-dimensional array cannot be \"\n588                             \"used for indexing: {}\".format(k)\n589                         )\n590                 if k.dtype.kind == \"b\":\n591                     if self.shape[self.get_axis_num(dim)] != len(k):\n592                         raise IndexError(\n593                             \"Boolean array size {:d} is used to index array \"\n594                             \"with shape {:s}.\".format(len(k), str(self.shape))\n595                         )\n596                     if k.ndim > 1:\n597                         raise IndexError(\n598                             \"{}-dimensional boolean indexing is \"\n599                             \"not supported. \".format(k.ndim)\n600                         )\n601                     if getattr(k, \"dims\", (dim,)) != (dim,):\n602                         raise IndexError(\n603                             \"Boolean indexer should be unlabeled or on the \"\n604                             \"same dimension to the indexed array. Indexer is \"\n605                             \"on {:s} but the target dimension is {:s}.\".format(\n606                                 str(k.dims), dim\n607                             )\n608                         )\n609 \n610     def _broadcast_indexes_outer(self, key):\n611         dims = tuple(\n612             k.dims[0] if isinstance(k, Variable) else dim\n613             for k, dim in zip(key, self.dims)\n614             if not isinstance(k, integer_types)\n615         )\n616 \n617         new_key = []\n618         for k in key:\n619             if isinstance(k, Variable):\n620                 k = k.data\n621             if not isinstance(k, BASIC_INDEXING_TYPES):\n622                 k = np.asarray(k)\n623                 if k.size == 0:\n624                     # Slice by empty list; numpy could not infer the dtype\n625                     k = k.astype(int)\n626                 elif k.dtype.kind == \"b\":\n627                     (k,) = np.nonzero(k)\n628             new_key.append(k)\n629 \n630         return dims, OuterIndexer(tuple(new_key)), None\n631 \n632     def _nonzero(self):\n633         \"\"\" Equivalent numpy's nonzero but returns a tuple of Varibles. \"\"\"\n634         # TODO we should replace dask's native nonzero\n635         # after https://github.com/dask/dask/issues/1076 is implemented.\n636         nonzeros = np.nonzero(self.data)\n637         return tuple(Variable((dim), nz) for nz, dim in zip(nonzeros, self.dims))\n638 \n639     def _broadcast_indexes_vectorized(self, key):\n640         variables = []\n641         out_dims_set = OrderedSet()\n642         for dim, value in zip(self.dims, key):\n643             if isinstance(value, slice):\n644                 out_dims_set.add(dim)\n645             else:\n646                 variable = (\n647                     value\n648                     if isinstance(value, Variable)\n649                     else as_variable(value, name=dim)\n650                 )\n651                 if variable.dtype.kind == \"b\":  # boolean indexing case\n652                     (variable,) = variable._nonzero()\n653 \n654                 variables.append(variable)\n655                 out_dims_set.update(variable.dims)\n656 \n657         variable_dims = set()\n658         for variable in variables:\n659             variable_dims.update(variable.dims)\n660 \n661         slices = []\n662         for i, (dim, value) in enumerate(zip(self.dims, key)):\n663             if isinstance(value, slice):\n664                 if dim in variable_dims:\n665                     # We only convert slice objects to variables if they share\n666                     # a dimension with at least one other variable. Otherwise,\n667                     # we can equivalently leave them as slices aknd transpose\n668                     # the result. This is significantly faster/more efficient\n669                     # for most array backends.\n670                     values = np.arange(*value.indices(self.sizes[dim]))\n671                     variables.insert(i - len(slices), Variable((dim,), values))\n672                 else:\n673                     slices.append((i, value))\n674 \n675         try:\n676             variables = _broadcast_compat_variables(*variables)\n677         except ValueError:\n678             raise IndexError(f\"Dimensions of indexers mismatch: {key}\")\n679 \n680         out_key = [variable.data for variable in variables]\n681         out_dims = tuple(out_dims_set)\n682         slice_positions = set()\n683         for i, value in slices:\n684             out_key.insert(i, value)\n685             new_position = out_dims.index(self.dims[i])\n686             slice_positions.add(new_position)\n687 \n688         if slice_positions:\n689             new_order = [i for i in range(len(out_dims)) if i not in slice_positions]\n690         else:\n691             new_order = None\n692 \n693         return out_dims, VectorizedIndexer(tuple(out_key)), new_order\n694 \n695     def __getitem__(self: VariableType, key) -> VariableType:\n696         \"\"\"Return a new Variable object whose contents are consistent with\n697         getting the provided key from the underlying data.\n698 \n699         NB. __getitem__ and __setitem__ implement xarray-style indexing,\n700         where if keys are unlabeled arrays, we index the array orthogonally\n701         with them. If keys are labeled array (such as Variables), they are\n702         broadcasted with our usual scheme and then the array is indexed with\n703         the broadcasted key, like numpy's fancy indexing.\n704 \n705         If you really want to do indexing like `x[x > 0]`, manipulate the numpy\n706         array `x.values` directly.\n707         \"\"\"\n708         dims, indexer, new_order = self._broadcast_indexes(key)\n709         data = as_indexable(self._data)[indexer]\n710         if new_order:\n711             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)\n712         return self._finalize_indexing_result(dims, data)\n713 \n714     def _finalize_indexing_result(self: VariableType, dims, data) -> VariableType:\n715         \"\"\"Used by IndexVariable to return IndexVariable objects when possible.\n716         \"\"\"\n717         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)\n718 \n719     def _getitem_with_mask(self, key, fill_value=dtypes.NA):\n720         \"\"\"Index this Variable with -1 remapped to fill_value.\"\"\"\n721         # TODO(shoyer): expose this method in public API somewhere (isel?) and\n722         # use it for reindex.\n723         # TODO(shoyer): add a sanity check that all other integers are\n724         # non-negative\n725         # TODO(shoyer): add an optimization, remapping -1 to an adjacent value\n726         # that is actually indexed rather than mapping it to the last value\n727         # along each axis.\n728 \n729         if fill_value is dtypes.NA:\n730             fill_value = dtypes.get_fill_value(self.dtype)\n731 \n732         dims, indexer, new_order = self._broadcast_indexes(key)\n733 \n734         if self.size:\n735             if isinstance(self._data, dask_array_type):\n736                 # dask's indexing is faster this way; also vindex does not\n737                 # support negative indices yet:\n738                 # https://github.com/dask/dask/pull/2967\n739                 actual_indexer = indexing.posify_mask_indexer(indexer)\n740             else:\n741                 actual_indexer = indexer\n742 \n743             data = as_indexable(self._data)[actual_indexer]\n744             mask = indexing.create_mask(indexer, self.shape, data)\n745             data = duck_array_ops.where(mask, fill_value, data)\n746         else:\n747             # array cannot be indexed along dimensions of size 0, so just\n748             # build the mask directly instead.\n749             mask = indexing.create_mask(indexer, self.shape)\n750             data = np.broadcast_to(fill_value, getattr(mask, \"shape\", ()))\n751 \n752         if new_order:\n753             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)\n754         return self._finalize_indexing_result(dims, data)\n755 \n756     def __setitem__(self, key, value):\n757         \"\"\"__setitem__ is overloaded to access the underlying numpy values with\n758         orthogonal indexing.\n759 \n760         See __getitem__ for more details.\n761         \"\"\"\n762         dims, index_tuple, new_order = self._broadcast_indexes(key)\n763 \n764         if not isinstance(value, Variable):\n765             value = as_compatible_data(value)\n766             if value.ndim > len(dims):\n767                 raise ValueError(\n768                     \"shape mismatch: value array of shape %s could not be \"\n769                     \"broadcast to indexing result with %s dimensions\"\n770                     % (value.shape, len(dims))\n771                 )\n772             if value.ndim == 0:\n773                 value = Variable((), value)\n774             else:\n775                 value = Variable(dims[-value.ndim :], value)\n776         # broadcast to become assignable\n777         value = value.set_dims(dims).data\n778 \n779         if new_order:\n780             value = duck_array_ops.asarray(value)\n781             value = value[(len(dims) - value.ndim) * (np.newaxis,) + (Ellipsis,)]\n782             value = duck_array_ops.moveaxis(value, new_order, range(len(new_order)))\n783 \n784         indexable = as_indexable(self._data)\n785         indexable[index_tuple] = value\n786 \n787     @property\n788     def attrs(self) -> Dict[Hashable, Any]:\n789         \"\"\"Dictionary of local attributes on this variable.\n790         \"\"\"\n791         if self._attrs is None:\n792             self._attrs = {}\n793         return self._attrs\n794 \n795     @attrs.setter\n796     def attrs(self, value: Mapping[Hashable, Any]) -> None:\n797         self._attrs = dict(value)\n798 \n799     @property\n800     def encoding(self):\n801         \"\"\"Dictionary of encodings on this variable.\n802         \"\"\"\n803         if self._encoding is None:\n804             self._encoding = {}\n805         return self._encoding\n806 \n807     @encoding.setter\n808     def encoding(self, value):\n809         try:\n810             self._encoding = dict(value)\n811         except ValueError:\n812             raise ValueError(\"encoding must be castable to a dictionary\")\n813 \n814     def copy(self, deep=True, data=None):\n815         \"\"\"Returns a copy of this object.\n816 \n817         If `deep=True`, the data array is loaded into memory and copied onto\n818         the new object. Dimensions, attributes and encodings are always copied.\n819 \n820         Use `data` to create a new object with the same structure as\n821         original but entirely new data.\n822 \n823         Parameters\n824         ----------\n825         deep : bool, optional\n826             Whether the data array is loaded into memory and copied onto\n827             the new object. Default is True.\n828         data : array_like, optional\n829             Data to use in the new object. Must have same shape as original.\n830             When `data` is used, `deep` is ignored.\n831 \n832         Returns\n833         -------\n834         object : Variable\n835             New object with dimensions, attributes, encodings, and optionally\n836             data copied from original.\n837 \n838         Examples\n839         --------\n840 \n841         Shallow copy versus deep copy\n842 \n843         >>> var = xr.Variable(data=[1, 2, 3], dims='x')\n844         >>> var.copy()\n845         <xarray.Variable (x: 3)>\n846         array([1, 2, 3])\n847         >>> var_0 = var.copy(deep=False)\n848         >>> var_0[0] = 7\n849         >>> var_0\n850         <xarray.Variable (x: 3)>\n851         array([7, 2, 3])\n852         >>> var\n853         <xarray.Variable (x: 3)>\n854         array([7, 2, 3])\n855 \n856         Changing the data using the ``data`` argument maintains the\n857         structure of the original object, but with the new data. Original\n858         object is unaffected.\n859 \n860         >>> var.copy(data=[0.1, 0.2, 0.3])\n861         <xarray.Variable (x: 3)>\n862         array([ 0.1,  0.2,  0.3])\n863         >>> var\n864         <xarray.Variable (x: 3)>\n865         array([7, 2, 3])\n866 \n867         See Also\n868         --------\n869         pandas.DataFrame.copy\n870         \"\"\"\n871         if data is None:\n872             data = self._data\n873 \n874             if isinstance(data, indexing.MemoryCachedArray):\n875                 # don't share caching between copies\n876                 data = indexing.MemoryCachedArray(data.array)\n877 \n878             if deep:\n879                 if hasattr(data, \"__array_function__\") or isinstance(\n880                     data, dask_array_type\n881                 ):\n882                     data = data.copy()\n883                 elif not isinstance(data, PandasIndexAdapter):\n884                     # pandas.Index is immutable\n885                     data = np.array(data)\n886         else:\n887             data = as_compatible_data(data)\n888             if self.shape != data.shape:\n889                 raise ValueError(\n890                     \"Data shape {} must match shape of object {}\".format(\n891                         data.shape, self.shape\n892                     )\n893                 )\n894 \n895         # note:\n896         # dims is already an immutable tuple\n897         # attributes and encoding will be copied when the new Array is created\n898         return self._replace(data=data)\n899 \n900     def _replace(\n901         self, dims=_default, data=_default, attrs=_default, encoding=_default\n902     ) -> \"Variable\":\n903         if dims is _default:\n904             dims = copy.copy(self._dims)\n905         if data is _default:\n906             data = copy.copy(self.data)\n907         if attrs is _default:\n908             attrs = copy.copy(self._attrs)\n909         if encoding is _default:\n910             encoding = copy.copy(self._encoding)\n911         return type(self)(dims, data, attrs, encoding, fastpath=True)\n912 \n913     def __copy__(self):\n914         return self.copy(deep=False)\n915 \n916     def __deepcopy__(self, memo=None):\n917         # memo does nothing but is required for compatibility with\n918         # copy.deepcopy\n919         return self.copy(deep=True)\n920 \n921     # mutable objects should not be hashable\n922     # https://github.com/python/mypy/issues/4266\n923     __hash__ = None  # type: ignore\n924 \n925     @property\n926     def chunks(self):\n927         \"\"\"Block dimensions for this array's data or None if it's not a dask\n928         array.\n929         \"\"\"\n930         return getattr(self._data, \"chunks\", None)\n931 \n932     _array_counter = itertools.count()\n933 \n934     def chunk(self, chunks=None, name=None, lock=False):\n935         \"\"\"Coerce this array's data into a dask arrays with the given chunks.\n936 \n937         If this variable is a non-dask array, it will be converted to dask\n938         array. If it's a dask array, it will be rechunked to the given chunk\n939         sizes.\n940 \n941         If neither chunks is not provided for one or more dimensions, chunk\n942         sizes along that dimension will not be updated; non-dask arrays will be\n943         converted into dask arrays with a single block.\n944 \n945         Parameters\n946         ----------\n947         chunks : int, tuple or dict, optional\n948             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or\n949             ``{'x': 5, 'y': 5}``.\n950         name : str, optional\n951             Used to generate the name for this array in the internal dask\n952             graph. Does not need not be unique.\n953         lock : optional\n954             Passed on to :py:func:`dask.array.from_array`, if the array is not\n955             already as dask array.\n956 \n957         Returns\n958         -------\n959         chunked : xarray.Variable\n960         \"\"\"\n961         import dask\n962         import dask.array as da\n963 \n964         if utils.is_dict_like(chunks):\n965             chunks = {self.get_axis_num(dim): chunk for dim, chunk in chunks.items()}\n966 \n967         if chunks is None:\n968             chunks = self.chunks or self.shape\n969 \n970         data = self._data\n971         if isinstance(data, da.Array):\n972             data = data.rechunk(chunks)\n973         else:\n974             if isinstance(data, indexing.ExplicitlyIndexed):\n975                 # Unambiguously handle array storage backends (like NetCDF4 and h5py)\n976                 # that can't handle general array indexing. For example, in netCDF4 you\n977                 # can do \"outer\" indexing along two dimensions independent, which works\n978                 # differently from how NumPy handles it.\n979                 # da.from_array works by using lazy indexing with a tuple of slices.\n980                 # Using OuterIndexer is a pragmatic choice: dask does not yet handle\n981                 # different indexing types in an explicit way:\n982                 # https://github.com/dask/dask/issues/2883\n983                 data = indexing.ImplicitToExplicitIndexingAdapter(\n984                     data, indexing.OuterIndexer\n985                 )\n986                 if LooseVersion(dask.__version__) < \"2.0.0\":\n987                     kwargs = {}\n988                 else:\n989                     # All of our lazily loaded backend array classes should use NumPy\n990                     # array operations.\n991                     kwargs = {\"meta\": np.ndarray}\n992             else:\n993                 kwargs = {}\n994 \n995             if utils.is_dict_like(chunks):\n996                 chunks = tuple(chunks.get(n, s) for n, s in enumerate(self.shape))\n997 \n998             data = da.from_array(data, chunks, name=name, lock=lock, **kwargs)\n999 \n1000         return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)\n1001 \n1002     def _as_sparse(self, sparse_format=_default, fill_value=dtypes.NA):\n1003         \"\"\"\n1004         use sparse-array as backend.\n1005         \"\"\"\n1006         import sparse\n1007 \n1008         # TODO  what to do if dask-backended?\n1009         if fill_value is dtypes.NA:\n1010             dtype, fill_value = dtypes.maybe_promote(self.dtype)\n1011         else:\n1012             dtype = dtypes.result_type(self.dtype, fill_value)\n1013 \n1014         if sparse_format is _default:\n1015             sparse_format = \"coo\"\n1016         try:\n1017             as_sparse = getattr(sparse, \"as_{}\".format(sparse_format.lower()))\n1018         except AttributeError:\n1019             raise ValueError(\"{} is not a valid sparse format\".format(sparse_format))\n1020 \n1021         data = as_sparse(self.data.astype(dtype), fill_value=fill_value)\n1022         return self._replace(data=data)\n1023 \n1024     def _to_dense(self):\n1025         \"\"\"\n1026         Change backend from sparse to np.array\n1027         \"\"\"\n1028         if hasattr(self._data, \"todense\"):\n1029             return self._replace(data=self._data.todense())\n1030         return self.copy(deep=False)\n1031 \n1032     def isel(\n1033         self: VariableType,\n1034         indexers: Mapping[Hashable, Any] = None,\n1035         **indexers_kwargs: Any,\n1036     ) -> VariableType:\n1037         \"\"\"Return a new array indexed along the specified dimension(s).\n1038 \n1039         Parameters\n1040         ----------\n1041         **indexers : {dim: indexer, ...}\n1042             Keyword arguments with names matching dimensions and values given\n1043             by integers, slice objects or arrays.\n1044 \n1045         Returns\n1046         -------\n1047         obj : Array object\n1048             A new Array with the selected data and dimensions. In general,\n1049             the new variable's data will be a view of this variable's data,\n1050             unless numpy fancy indexing was triggered by using an array\n1051             indexer, in which case the data will be a copy.\n1052         \"\"\"\n1053         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"isel\")\n1054 \n1055         invalid = indexers.keys() - set(self.dims)\n1056         if invalid:\n1057             raise ValueError(\"dimensions %r do not exist\" % invalid)\n1058 \n1059         key = tuple(indexers.get(dim, slice(None)) for dim in self.dims)\n1060         return self[key]\n1061 \n1062     def squeeze(self, dim=None):\n1063         \"\"\"Return a new object with squeezed data.\n1064 \n1065         Parameters\n1066         ----------\n1067         dim : None or str or tuple of str, optional\n1068             Selects a subset of the length one dimensions. If a dimension is\n1069             selected with length greater than one, an error is raised. If\n1070             None, all length one dimensions are squeezed.\n1071 \n1072         Returns\n1073         -------\n1074         squeezed : same type as caller\n1075             This object, but with with all or a subset of the dimensions of\n1076             length 1 removed.\n1077 \n1078         See Also\n1079         --------\n1080         numpy.squeeze\n1081         \"\"\"\n1082         dims = common.get_squeeze_dims(self, dim)\n1083         return self.isel({d: 0 for d in dims})\n1084 \n1085     def _shift_one_dim(self, dim, count, fill_value=dtypes.NA):\n1086         axis = self.get_axis_num(dim)\n1087 \n1088         if count > 0:\n1089             keep = slice(None, -count)\n1090         elif count < 0:\n1091             keep = slice(-count, None)\n1092         else:\n1093             keep = slice(None)\n1094 \n1095         trimmed_data = self[(slice(None),) * axis + (keep,)].data\n1096 \n1097         if fill_value is dtypes.NA:\n1098             dtype, fill_value = dtypes.maybe_promote(self.dtype)\n1099         else:\n1100             dtype = self.dtype\n1101 \n1102         shape = list(self.shape)\n1103         shape[axis] = min(abs(count), shape[axis])\n1104 \n1105         if isinstance(trimmed_data, dask_array_type):\n1106             chunks = list(trimmed_data.chunks)\n1107             chunks[axis] = (shape[axis],)\n1108             full = functools.partial(da.full, chunks=chunks)\n1109         else:\n1110             full = np.full\n1111 \n1112         filler = full(shape, fill_value, dtype=dtype)\n1113 \n1114         if count > 0:\n1115             arrays = [filler, trimmed_data]\n1116         else:\n1117             arrays = [trimmed_data, filler]\n1118 \n1119         data = duck_array_ops.concatenate(arrays, axis)\n1120 \n1121         if isinstance(data, dask_array_type):\n1122             # chunked data should come out with the same chunks; this makes\n1123             # it feasible to combine shifted and unshifted data\n1124             # TODO: remove this once dask.array automatically aligns chunks\n1125             data = data.rechunk(self.data.chunks)\n1126 \n1127         return type(self)(self.dims, data, self._attrs, fastpath=True)\n1128 \n1129     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):\n1130         \"\"\"\n1131         Return a new Variable with shifted data.\n1132 \n1133         Parameters\n1134         ----------\n1135         shifts : mapping of the form {dim: offset}\n1136             Integer offset to shift along each of the given dimensions.\n1137             Positive offsets shift to the right; negative offsets shift to the\n1138             left.\n1139         fill_value: scalar, optional\n1140             Value to use for newly missing values\n1141         **shifts_kwargs:\n1142             The keyword arguments form of ``shifts``.\n1143             One of shifts or shifts_kwargs must be provided.\n1144 \n1145         Returns\n1146         -------\n1147         shifted : Variable\n1148             Variable with the same dimensions and attributes but shifted data.\n1149         \"\"\"\n1150         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"shift\")\n1151         result = self\n1152         for dim, count in shifts.items():\n1153             result = result._shift_one_dim(dim, count, fill_value=fill_value)\n1154         return result\n1155 \n1156     def pad_with_fill_value(\n1157         self, pad_widths=None, fill_value=dtypes.NA, **pad_widths_kwargs\n1158     ):\n1159         \"\"\"\n1160         Return a new Variable with paddings.\n1161 \n1162         Parameters\n1163         ----------\n1164         pad_width: Mapping of the form {dim: (before, after)}\n1165             Number of values padded to the edges of each dimension.\n1166         **pad_widths_kwargs:\n1167             Keyword argument for pad_widths\n1168         \"\"\"\n1169         pad_widths = either_dict_or_kwargs(pad_widths, pad_widths_kwargs, \"pad\")\n1170 \n1171         if fill_value is dtypes.NA:\n1172             dtype, fill_value = dtypes.maybe_promote(self.dtype)\n1173         else:\n1174             dtype = self.dtype\n1175 \n1176         if isinstance(self.data, dask_array_type):\n1177             array = self.data\n1178 \n1179             # Dask does not yet support pad. We manually implement it.\n1180             # https://github.com/dask/dask/issues/1926\n1181             for d, pad in pad_widths.items():\n1182                 axis = self.get_axis_num(d)\n1183                 before_shape = list(array.shape)\n1184                 before_shape[axis] = pad[0]\n1185                 before_chunks = list(array.chunks)\n1186                 before_chunks[axis] = (pad[0],)\n1187                 after_shape = list(array.shape)\n1188                 after_shape[axis] = pad[1]\n1189                 after_chunks = list(array.chunks)\n1190                 after_chunks[axis] = (pad[1],)\n1191 \n1192                 arrays = []\n1193                 if pad[0] > 0:\n1194                     arrays.append(\n1195                         da.full(\n1196                             before_shape, fill_value, dtype=dtype, chunks=before_chunks\n1197                         )\n1198                     )\n1199                 arrays.append(array)\n1200                 if pad[1] > 0:\n1201                     arrays.append(\n1202                         da.full(\n1203                             after_shape, fill_value, dtype=dtype, chunks=after_chunks\n1204                         )\n1205                     )\n1206                 if len(arrays) > 1:\n1207                     array = da.concatenate(arrays, axis=axis)\n1208         else:\n1209             pads = [(0, 0) if d not in pad_widths else pad_widths[d] for d in self.dims]\n1210             array = np.pad(\n1211                 self.data.astype(dtype, copy=False),\n1212                 pads,\n1213                 mode=\"constant\",\n1214                 constant_values=fill_value,\n1215             )\n1216         return type(self)(self.dims, array)\n1217 \n1218     def _roll_one_dim(self, dim, count):\n1219         axis = self.get_axis_num(dim)\n1220 \n1221         count %= self.shape[axis]\n1222         if count != 0:\n1223             indices = [slice(-count, None), slice(None, -count)]\n1224         else:\n1225             indices = [slice(None)]\n1226 \n1227         arrays = [self[(slice(None),) * axis + (idx,)].data for idx in indices]\n1228 \n1229         data = duck_array_ops.concatenate(arrays, axis)\n1230 \n1231         if isinstance(data, dask_array_type):\n1232             # chunked data should come out with the same chunks; this makes\n1233             # it feasible to combine shifted and unshifted data\n1234             # TODO: remove this once dask.array automatically aligns chunks\n1235             data = data.rechunk(self.data.chunks)\n1236 \n1237         return type(self)(self.dims, data, self._attrs, fastpath=True)\n1238 \n1239     def roll(self, shifts=None, **shifts_kwargs):\n1240         \"\"\"\n1241         Return a new Variable with rolld data.\n1242 \n1243         Parameters\n1244         ----------\n1245         shifts : mapping of the form {dim: offset}\n1246             Integer offset to roll along each of the given dimensions.\n1247             Positive offsets roll to the right; negative offsets roll to the\n1248             left.\n1249         **shifts_kwargs:\n1250             The keyword arguments form of ``shifts``.\n1251             One of shifts or shifts_kwargs must be provided.\n1252 \n1253         Returns\n1254         -------\n1255         shifted : Variable\n1256             Variable with the same dimensions and attributes but rolled data.\n1257         \"\"\"\n1258         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"roll\")\n1259 \n1260         result = self\n1261         for dim, count in shifts.items():\n1262             result = result._roll_one_dim(dim, count)\n1263         return result\n1264 \n1265     def transpose(self, *dims) -> \"Variable\":\n1266         \"\"\"Return a new Variable object with transposed dimensions.\n1267 \n1268         Parameters\n1269         ----------\n1270         *dims : str, optional\n1271             By default, reverse the dimensions. Otherwise, reorder the\n1272             dimensions to this order.\n1273 \n1274         Returns\n1275         -------\n1276         transposed : Variable\n1277             The returned object has transposed data and dimensions with the\n1278             same attributes as the original.\n1279 \n1280         Notes\n1281         -----\n1282         This operation returns a view of this variable's data. It is\n1283         lazy for dask-backed Variables but not for numpy-backed Variables.\n1284 \n1285         See Also\n1286         --------\n1287         numpy.transpose\n1288         \"\"\"\n1289         if len(dims) == 0:\n1290             dims = self.dims[::-1]\n1291         dims = tuple(infix_dims(dims, self.dims))\n1292         axes = self.get_axis_num(dims)\n1293         if len(dims) < 2 or dims == self.dims:\n1294             # no need to transpose if only one dimension\n1295             # or dims are in same order\n1296             return self.copy(deep=False)\n1297 \n1298         data = as_indexable(self._data).transpose(axes)\n1299         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)\n1300 \n1301     @property\n1302     def T(self) -> \"Variable\":\n1303         return self.transpose()\n1304 \n1305     def set_dims(self, dims, shape=None):\n1306         \"\"\"Return a new variable with given set of dimensions.\n1307         This method might be used to attach new dimension(s) to variable.\n1308 \n1309         When possible, this operation does not copy this variable's data.\n1310 \n1311         Parameters\n1312         ----------\n1313         dims : str or sequence of str or dict\n1314             Dimensions to include on the new variable. If a dict, values are\n1315             used to provide the sizes of new dimensions; otherwise, new\n1316             dimensions are inserted with length 1.\n1317 \n1318         Returns\n1319         -------\n1320         Variable\n1321         \"\"\"\n1322         if isinstance(dims, str):\n1323             dims = [dims]\n1324 \n1325         if shape is None and utils.is_dict_like(dims):\n1326             shape = dims.values()\n1327 \n1328         missing_dims = set(self.dims) - set(dims)\n1329         if missing_dims:\n1330             raise ValueError(\n1331                 \"new dimensions %r must be a superset of \"\n1332                 \"existing dimensions %r\" % (dims, self.dims)\n1333             )\n1334 \n1335         self_dims = set(self.dims)\n1336         expanded_dims = tuple(d for d in dims if d not in self_dims) + self.dims\n1337 \n1338         if self.dims == expanded_dims:\n1339             # don't use broadcast_to unless necessary so the result remains\n1340             # writeable if possible\n1341             expanded_data = self.data\n1342         elif shape is not None:\n1343             dims_map = dict(zip(dims, shape))\n1344             tmp_shape = tuple(dims_map[d] for d in expanded_dims)\n1345             expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)\n1346         else:\n1347             expanded_data = self.data[(None,) * (len(expanded_dims) - self.ndim)]\n1348 \n1349         expanded_var = Variable(\n1350             expanded_dims, expanded_data, self._attrs, self._encoding, fastpath=True\n1351         )\n1352         return expanded_var.transpose(*dims)\n1353 \n1354     def _stack_once(self, dims, new_dim):\n1355         if not set(dims) <= set(self.dims):\n1356             raise ValueError(\"invalid existing dimensions: %s\" % dims)\n1357 \n1358         if new_dim in self.dims:\n1359             raise ValueError(\n1360                 \"cannot create a new dimension with the same \"\n1361                 \"name as an existing dimension\"\n1362             )\n1363 \n1364         if len(dims) == 0:\n1365             # don't stack\n1366             return self.copy(deep=False)\n1367 \n1368         other_dims = [d for d in self.dims if d not in dims]\n1369         dim_order = other_dims + list(dims)\n1370         reordered = self.transpose(*dim_order)\n1371 \n1372         new_shape = reordered.shape[: len(other_dims)] + (-1,)\n1373         new_data = reordered.data.reshape(new_shape)\n1374         new_dims = reordered.dims[: len(other_dims)] + (new_dim,)\n1375 \n1376         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)\n1377 \n1378     def stack(self, dimensions=None, **dimensions_kwargs):\n1379         \"\"\"\n1380         Stack any number of existing dimensions into a single new dimension.\n1381 \n1382         New dimensions will be added at the end, and the order of the data\n1383         along each new dimension will be in contiguous (C) order.\n1384 \n1385         Parameters\n1386         ----------\n1387         dimensions : Mapping of form new_name=(dim1, dim2, ...)\n1388             Names of new dimensions, and the existing dimensions that they\n1389             replace.\n1390         **dimensions_kwargs:\n1391             The keyword arguments form of ``dimensions``.\n1392             One of dimensions or dimensions_kwargs must be provided.\n1393 \n1394         Returns\n1395         -------\n1396         stacked : Variable\n1397             Variable with the same attributes but stacked data.\n1398 \n1399         See also\n1400         --------\n1401         Variable.unstack\n1402         \"\"\"\n1403         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, \"stack\")\n1404         result = self\n1405         for new_dim, dims in dimensions.items():\n1406             result = result._stack_once(dims, new_dim)\n1407         return result\n1408 \n1409     def _unstack_once(self, dims, old_dim):\n1410         new_dim_names = tuple(dims.keys())\n1411         new_dim_sizes = tuple(dims.values())\n1412 \n1413         if old_dim not in self.dims:\n1414             raise ValueError(\"invalid existing dimension: %s\" % old_dim)\n1415 \n1416         if set(new_dim_names).intersection(self.dims):\n1417             raise ValueError(\n1418                 \"cannot create a new dimension with the same \"\n1419                 \"name as an existing dimension\"\n1420             )\n1421 \n1422         if np.prod(new_dim_sizes) != self.sizes[old_dim]:\n1423             raise ValueError(\n1424                 \"the product of the new dimension sizes must \"\n1425                 \"equal the size of the old dimension\"\n1426             )\n1427 \n1428         other_dims = [d for d in self.dims if d != old_dim]\n1429         dim_order = other_dims + [old_dim]\n1430         reordered = self.transpose(*dim_order)\n1431 \n1432         new_shape = reordered.shape[: len(other_dims)] + new_dim_sizes\n1433         new_data = reordered.data.reshape(new_shape)\n1434         new_dims = reordered.dims[: len(other_dims)] + new_dim_names\n1435 \n1436         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)\n1437 \n1438     def unstack(self, dimensions=None, **dimensions_kwargs):\n1439         \"\"\"\n1440         Unstack an existing dimension into multiple new dimensions.\n1441 \n1442         New dimensions will be added at the end, and the order of the data\n1443         along each new dimension will be in contiguous (C) order.\n1444 \n1445         Parameters\n1446         ----------\n1447         dimensions : mapping of the form old_dim={dim1: size1, ...}\n1448             Names of existing dimensions, and the new dimensions and sizes\n1449             that they map to.\n1450         **dimensions_kwargs:\n1451             The keyword arguments form of ``dimensions``.\n1452             One of dimensions or dimensions_kwargs must be provided.\n1453 \n1454         Returns\n1455         -------\n1456         unstacked : Variable\n1457             Variable with the same attributes but unstacked data.\n1458 \n1459         See also\n1460         --------\n1461         Variable.stack\n1462         \"\"\"\n1463         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, \"unstack\")\n1464         result = self\n1465         for old_dim, dims in dimensions.items():\n1466             result = result._unstack_once(dims, old_dim)\n1467         return result\n1468 \n1469     def fillna(self, value):\n1470         return ops.fillna(self, value)\n1471 \n1472     def where(self, cond, other=dtypes.NA):\n1473         return ops.where_method(self, cond, other)\n1474 \n1475     def reduce(\n1476         self,\n1477         func,\n1478         dim=None,\n1479         axis=None,\n1480         keep_attrs=None,\n1481         keepdims=False,\n1482         allow_lazy=None,\n1483         **kwargs,\n1484     ):\n1485         \"\"\"Reduce this array by applying `func` along some dimension(s).\n1486 \n1487         Parameters\n1488         ----------\n1489         func : function\n1490             Function which can be called in the form\n1491             `func(x, axis=axis, **kwargs)` to return the result of reducing an\n1492             np.ndarray over an integer valued axis.\n1493         dim : str or sequence of str, optional\n1494             Dimension(s) over which to apply `func`.\n1495         axis : int or sequence of int, optional\n1496             Axis(es) over which to apply `func`. Only one of the 'dim'\n1497             and 'axis' arguments can be supplied. If neither are supplied, then\n1498             the reduction is calculated over the flattened array (by calling\n1499             `func(x)` without an axis argument).\n1500         keep_attrs : bool, optional\n1501             If True, the variable's attributes (`attrs`) will be copied from\n1502             the original object to the new one.  If False (default), the new\n1503             object will be returned without attributes.\n1504         keepdims : bool, default False\n1505             If True, the dimensions which are reduced are left in the result\n1506             as dimensions of size one\n1507         **kwargs : dict\n1508             Additional keyword arguments passed on to `func`.\n1509 \n1510         Returns\n1511         -------\n1512         reduced : Array\n1513             Array with summarized data and the indicated dimension(s)\n1514             removed.\n1515         \"\"\"\n1516         if dim == ...:\n1517             dim = None\n1518         if dim is not None and axis is not None:\n1519             raise ValueError(\"cannot supply both 'axis' and 'dim' arguments\")\n1520 \n1521         if dim is not None:\n1522             axis = self.get_axis_num(dim)\n1523 \n1524         if allow_lazy is not None:\n1525             warnings.warn(\n1526                 \"allow_lazy is deprecated and will be removed in version 0.16.0. It is now True by default.\",\n1527                 DeprecationWarning,\n1528             )\n1529         else:\n1530             allow_lazy = True\n1531 \n1532         input_data = self.data if allow_lazy else self.values\n1533 \n1534         if axis is not None:\n1535             data = func(input_data, axis=axis, **kwargs)\n1536         else:\n1537             data = func(input_data, **kwargs)\n1538 \n1539         if getattr(data, \"shape\", ()) == self.shape:\n1540             dims = self.dims\n1541         else:\n1542             removed_axes = (\n1543                 range(self.ndim) if axis is None else np.atleast_1d(axis) % self.ndim\n1544             )\n1545             if keepdims:\n1546                 # Insert np.newaxis for removed dims\n1547                 slices = tuple(\n1548                     np.newaxis if i in removed_axes else slice(None, None)\n1549                     for i in range(self.ndim)\n1550                 )\n1551                 if getattr(data, \"shape\", None) is None:\n1552                     # Reduce has produced a scalar value, not an array-like\n1553                     data = np.asanyarray(data)[slices]\n1554                 else:\n1555                     data = data[slices]\n1556                 dims = self.dims\n1557             else:\n1558                 dims = [\n1559                     adim for n, adim in enumerate(self.dims) if n not in removed_axes\n1560                 ]\n1561 \n1562         if keep_attrs is None:\n1563             keep_attrs = _get_keep_attrs(default=False)\n1564         attrs = self._attrs if keep_attrs else None\n1565 \n1566         return Variable(dims, data, attrs=attrs)\n1567 \n1568     @classmethod\n1569     def concat(cls, variables, dim=\"concat_dim\", positions=None, shortcut=False):\n1570         \"\"\"Concatenate variables along a new or existing dimension.\n1571 \n1572         Parameters\n1573         ----------\n1574         variables : iterable of Array\n1575             Arrays to stack together. Each variable is expected to have\n1576             matching dimensions and shape except for along the stacked\n1577             dimension.\n1578         dim : str or DataArray, optional\n1579             Name of the dimension to stack along. This can either be a new\n1580             dimension name, in which case it is added along axis=0, or an\n1581             existing dimension name, in which case the location of the\n1582             dimension is unchanged. Where to insert the new dimension is\n1583             determined by the first variable.\n1584         positions : None or list of integer arrays, optional\n1585             List of integer arrays which specifies the integer positions to\n1586             which to assign each dataset along the concatenated dimension.\n1587             If not supplied, objects are concatenated in the provided order.\n1588         shortcut : bool, optional\n1589             This option is used internally to speed-up groupby operations.\n1590             If `shortcut` is True, some checks of internal consistency between\n1591             arrays to concatenate are skipped.\n1592 \n1593         Returns\n1594         -------\n1595         stacked : Variable\n1596             Concatenated Variable formed by stacking all the supplied variables\n1597             along the given dimension.\n1598         \"\"\"\n1599         if not isinstance(dim, str):\n1600             (dim,) = dim.dims\n1601 \n1602         # can't do this lazily: we need to loop through variables at least\n1603         # twice\n1604         variables = list(variables)\n1605         first_var = variables[0]\n1606 \n1607         arrays = [v.data for v in variables]\n1608 \n1609         if dim in first_var.dims:\n1610             axis = first_var.get_axis_num(dim)\n1611             dims = first_var.dims\n1612             data = duck_array_ops.concatenate(arrays, axis=axis)\n1613             if positions is not None:\n1614                 # TODO: deprecate this option -- we don't need it for groupby\n1615                 # any more.\n1616                 indices = nputils.inverse_permutation(np.concatenate(positions))\n1617                 data = duck_array_ops.take(data, indices, axis=axis)\n1618         else:\n1619             axis = 0\n1620             dims = (dim,) + first_var.dims\n1621             data = duck_array_ops.stack(arrays, axis=axis)\n1622 \n1623         attrs = dict(first_var.attrs)\n1624         encoding = dict(first_var.encoding)\n1625         if not shortcut:\n1626             for var in variables:\n1627                 if var.dims != first_var.dims:\n1628                     raise ValueError(\"inconsistent dimensions\")\n1629                 utils.remove_incompatible_items(attrs, var.attrs)\n1630 \n1631         return cls(dims, data, attrs, encoding)\n1632 \n1633     def equals(self, other, equiv=duck_array_ops.array_equiv):\n1634         \"\"\"True if two Variables have the same dimensions and values;\n1635         otherwise False.\n1636 \n1637         Variables can still be equal (like pandas objects) if they have NaN\n1638         values in the same locations.\n1639 \n1640         This method is necessary because `v1 == v2` for Variables\n1641         does element-wise comparisons (like numpy.ndarrays).\n1642         \"\"\"\n1643         other = getattr(other, \"variable\", other)\n1644         try:\n1645             return self.dims == other.dims and (\n1646                 self._data is other._data or equiv(self.data, other.data)\n1647             )\n1648         except (TypeError, AttributeError):\n1649             return False\n1650 \n1651     def broadcast_equals(self, other, equiv=duck_array_ops.array_equiv):\n1652         \"\"\"True if two Variables have the values after being broadcast against\n1653         each other; otherwise False.\n1654 \n1655         Variables can still be equal (like pandas objects) if they have NaN\n1656         values in the same locations.\n1657         \"\"\"\n1658         try:\n1659             self, other = broadcast_variables(self, other)\n1660         except (ValueError, AttributeError):\n1661             return False\n1662         return self.equals(other, equiv=equiv)\n1663 \n1664     def identical(self, other, equiv=duck_array_ops.array_equiv):\n1665         \"\"\"Like equals, but also checks attributes.\n1666         \"\"\"\n1667         try:\n1668             return utils.dict_equiv(self.attrs, other.attrs) and self.equals(\n1669                 other, equiv=equiv\n1670             )\n1671         except (TypeError, AttributeError):\n1672             return False\n1673 \n1674     def no_conflicts(self, other, equiv=duck_array_ops.array_notnull_equiv):\n1675         \"\"\"True if the intersection of two Variable's non-null data is\n1676         equal; otherwise false.\n1677 \n1678         Variables can thus still be equal if there are locations where either,\n1679         or both, contain NaN values.\n1680         \"\"\"\n1681         return self.broadcast_equals(other, equiv=equiv)\n1682 \n1683     def quantile(self, q, dim=None, interpolation=\"linear\", keep_attrs=None):\n1684         \"\"\"Compute the qth quantile of the data along the specified dimension.\n1685 \n1686         Returns the qth quantiles(s) of the array elements.\n1687 \n1688         Parameters\n1689         ----------\n1690         q : float in range of [0,1] (or sequence of floats)\n1691             Quantile to compute, which must be between 0 and 1\n1692             inclusive.\n1693         dim : str or sequence of str, optional\n1694             Dimension(s) over which to apply quantile.\n1695         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n1696             This optional parameter specifies the interpolation method to\n1697             use when the desired quantile lies between two data points\n1698             ``i < j``:\n1699                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is\n1700                   the fractional part of the index surrounded by ``i`` and\n1701                   ``j``.\n1702                 * lower: ``i``.\n1703                 * higher: ``j``.\n1704                 * nearest: ``i`` or ``j``, whichever is nearest.\n1705                 * midpoint: ``(i + j) / 2``.\n1706         keep_attrs : bool, optional\n1707             If True, the variable's attributes (`attrs`) will be copied from\n1708             the original object to the new one.  If False (default), the new\n1709             object will be returned without attributes.\n1710 \n1711         Returns\n1712         -------\n1713         quantiles : Variable\n1714             If `q` is a single quantile, then the result\n1715             is a scalar. If multiple percentiles are given, first axis of\n1716             the result corresponds to the quantile and a quantile dimension\n1717             is added to the return array. The other dimensions are the\n1718             dimensions that remain after the reduction of the array.\n1719 \n1720         See Also\n1721         --------\n1722         numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile,\n1723         DataArray.quantile\n1724         \"\"\"\n1725 \n1726         from .computation import apply_ufunc\n1727 \n1728         if keep_attrs is None:\n1729             keep_attrs = _get_keep_attrs(default=False)\n1730 \n1731         scalar = utils.is_scalar(q)\n1732         q = np.atleast_1d(np.asarray(q, dtype=np.float64))\n1733 \n1734         if dim is None:\n1735             dim = self.dims\n1736 \n1737         if utils.is_scalar(dim):\n1738             dim = [dim]\n1739 \n1740         def _wrapper(npa, **kwargs):\n1741             # move quantile axis to end. required for apply_ufunc\n1742             return np.moveaxis(np.nanpercentile(npa, **kwargs), 0, -1)\n1743 \n1744         axis = np.arange(-1, -1 * len(dim) - 1, -1)\n1745         result = apply_ufunc(\n1746             _wrapper,\n1747             self,\n1748             input_core_dims=[dim],\n1749             exclude_dims=set(dim),\n1750             output_core_dims=[[\"quantile\"]],\n1751             output_dtypes=[np.float64],\n1752             output_sizes={\"quantile\": len(q)},\n1753             dask=\"parallelized\",\n1754             kwargs={\"q\": q * 100, \"axis\": axis, \"interpolation\": interpolation},\n1755         )\n1756 \n1757         # for backward compatibility\n1758         result = result.transpose(\"quantile\", ...)\n1759         if scalar:\n1760             result = result.squeeze(\"quantile\")\n1761         if keep_attrs:\n1762             result.attrs = self._attrs\n1763         return result\n1764 \n1765     def rank(self, dim, pct=False):\n1766         \"\"\"Ranks the data.\n1767 \n1768         Equal values are assigned a rank that is the average of the ranks that\n1769         would have been otherwise assigned to all of the values within that\n1770         set.  Ranks begin at 1, not 0. If `pct`, computes percentage ranks.\n1771 \n1772         NaNs in the input array are returned as NaNs.\n1773 \n1774         The `bottleneck` library is required.\n1775 \n1776         Parameters\n1777         ----------\n1778         dim : str\n1779             Dimension over which to compute rank.\n1780         pct : bool, optional\n1781             If True, compute percentage ranks, otherwise compute integer ranks.\n1782 \n1783         Returns\n1784         -------\n1785         ranked : Variable\n1786 \n1787         See Also\n1788         --------\n1789         Dataset.rank, DataArray.rank\n1790         \"\"\"\n1791         import bottleneck as bn\n1792 \n1793         data = self.data\n1794 \n1795         if isinstance(data, dask_array_type):\n1796             raise TypeError(\n1797                 \"rank does not work for arrays stored as dask \"\n1798                 \"arrays. Load the data via .compute() or .load() \"\n1799                 \"prior to calling this method.\"\n1800             )\n1801         elif not isinstance(data, np.ndarray):\n1802             raise TypeError(\n1803                 \"rank is not implemented for {} objects.\".format(type(data))\n1804             )\n1805 \n1806         axis = self.get_axis_num(dim)\n1807         func = bn.nanrankdata if self.dtype.kind == \"f\" else bn.rankdata\n1808         ranked = func(data, axis=axis)\n1809         if pct:\n1810             count = np.sum(~np.isnan(data), axis=axis, keepdims=True)\n1811             ranked /= count\n1812         return Variable(self.dims, ranked)\n1813 \n1814     def rolling_window(\n1815         self, dim, window, window_dim, center=False, fill_value=dtypes.NA\n1816     ):\n1817         \"\"\"\n1818         Make a rolling_window along dim and add a new_dim to the last place.\n1819 \n1820         Parameters\n1821         ----------\n1822         dim: str\n1823             Dimension over which to compute rolling_window\n1824         window: int\n1825             Window size of the rolling\n1826         window_dim: str\n1827             New name of the window dimension.\n1828         center: boolean. default False.\n1829             If True, pad fill_value for both ends. Otherwise, pad in the head\n1830             of the axis.\n1831         fill_value:\n1832             value to be filled.\n1833 \n1834         Returns\n1835         -------\n1836         Variable that is a view of the original array with a added dimension of\n1837         size w.\n1838         The return dim: self.dims + (window_dim, )\n1839         The return shape: self.shape + (window, )\n1840 \n1841         Examples\n1842         --------\n1843         >>> v=Variable(('a', 'b'), np.arange(8).reshape((2,4)))\n1844         >>> v.rolling_window(x, 'b', 3, 'window_dim')\n1845         <xarray.Variable (a: 2, b: 4, window_dim: 3)>\n1846         array([[[nan, nan, 0], [nan, 0, 1], [0, 1, 2], [1, 2, 3]],\n1847                [[nan, nan, 4], [nan, 4, 5], [4, 5, 6], [5, 6, 7]]])\n1848 \n1849         >>> v.rolling_window(x, 'b', 3, 'window_dim', center=True)\n1850         <xarray.Variable (a: 2, b: 4, window_dim: 3)>\n1851         array([[[nan, 0, 1], [0, 1, 2], [1, 2, 3], [2, 3, nan]],\n1852                [[nan, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, nan]]])\n1853         \"\"\"\n1854         if fill_value is dtypes.NA:  # np.nan is passed\n1855             dtype, fill_value = dtypes.maybe_promote(self.dtype)\n1856             array = self.astype(dtype, copy=False).data\n1857         else:\n1858             dtype = self.dtype\n1859             array = self.data\n1860 \n1861         new_dims = self.dims + (window_dim,)\n1862         return Variable(\n1863             new_dims,\n1864             duck_array_ops.rolling_window(\n1865                 array,\n1866                 axis=self.get_axis_num(dim),\n1867                 window=window,\n1868                 center=center,\n1869                 fill_value=fill_value,\n1870             ),\n1871         )\n1872 \n1873     def coarsen(self, windows, func, boundary=\"exact\", side=\"left\", **kwargs):\n1874         \"\"\"\n1875         Apply reduction function.\n1876         \"\"\"\n1877         windows = {k: v for k, v in windows.items() if k in self.dims}\n1878         if not windows:\n1879             return self.copy()\n1880 \n1881         reshaped, axes = self._coarsen_reshape(windows, boundary, side)\n1882         if isinstance(func, str):\n1883             name = func\n1884             func = getattr(duck_array_ops, name, None)\n1885             if func is None:\n1886                 raise NameError(f\"{name} is not a valid method.\")\n1887         return self._replace(data=func(reshaped, axis=axes, **kwargs))\n1888 \n1889     def _coarsen_reshape(self, windows, boundary, side):\n1890         \"\"\"\n1891         Construct a reshaped-array for coarsen\n1892         \"\"\"\n1893         if not utils.is_dict_like(boundary):\n1894             boundary = {d: boundary for d in windows.keys()}\n1895 \n1896         if not utils.is_dict_like(side):\n1897             side = {d: side for d in windows.keys()}\n1898 \n1899         # remove unrelated dimensions\n1900         boundary = {k: v for k, v in boundary.items() if k in windows}\n1901         side = {k: v for k, v in side.items() if k in windows}\n1902 \n1903         for d, window in windows.items():\n1904             if window <= 0:\n1905                 raise ValueError(f\"window must be > 0. Given {window}\")\n1906 \n1907         variable = self\n1908         for d, window in windows.items():\n1909             # trim or pad the object\n1910             size = variable.shape[self._get_axis_num(d)]\n1911             n = int(size / window)\n1912             if boundary[d] == \"exact\":\n1913                 if n * window != size:\n1914                     raise ValueError(\n1915                         \"Could not coarsen a dimension of size {} with \"\n1916                         \"window {}\".format(size, window)\n1917                     )\n1918             elif boundary[d] == \"trim\":\n1919                 if side[d] == \"left\":\n1920                     variable = variable.isel({d: slice(0, window * n)})\n1921                 else:\n1922                     excess = size - window * n\n1923                     variable = variable.isel({d: slice(excess, None)})\n1924             elif boundary[d] == \"pad\":  # pad\n1925                 pad = window * n - size\n1926                 if pad < 0:\n1927                     pad += window\n1928                 if side[d] == \"left\":\n1929                     pad_widths = {d: (0, pad)}\n1930                 else:\n1931                     pad_widths = {d: (pad, 0)}\n1932                 variable = variable.pad_with_fill_value(pad_widths)\n1933             else:\n1934                 raise TypeError(\n1935                     \"{} is invalid for boundary. Valid option is 'exact', \"\n1936                     \"'trim' and 'pad'\".format(boundary[d])\n1937                 )\n1938 \n1939         shape = []\n1940         axes = []\n1941         axis_count = 0\n1942         for i, d in enumerate(variable.dims):\n1943             if d in windows:\n1944                 size = variable.shape[i]\n1945                 shape.append(int(size / windows[d]))\n1946                 shape.append(windows[d])\n1947                 axis_count += 1\n1948                 axes.append(i + axis_count)\n1949             else:\n1950                 shape.append(variable.shape[i])\n1951 \n1952         return variable.data.reshape(shape), tuple(axes)\n1953 \n1954     @property\n1955     def real(self):\n1956         return type(self)(self.dims, self.data.real, self._attrs)\n1957 \n1958     @property\n1959     def imag(self):\n1960         return type(self)(self.dims, self.data.imag, self._attrs)\n1961 \n1962     def __array_wrap__(self, obj, context=None):\n1963         return Variable(self.dims, obj)\n1964 \n1965     @staticmethod\n1966     def _unary_op(f):\n1967         @functools.wraps(f)\n1968         def func(self, *args, **kwargs):\n1969             with np.errstate(all=\"ignore\"):\n1970                 return self.__array_wrap__(f(self.data, *args, **kwargs))\n1971 \n1972         return func\n1973 \n1974     @staticmethod\n1975     def _binary_op(f, reflexive=False, **ignored_kwargs):\n1976         @functools.wraps(f)\n1977         def func(self, other):\n1978             if isinstance(other, (xr.DataArray, xr.Dataset)):\n1979                 return NotImplemented\n1980             self_data, other_data, dims = _broadcast_compat_data(self, other)\n1981             keep_attrs = _get_keep_attrs(default=False)\n1982             attrs = self._attrs if keep_attrs else None\n1983             with np.errstate(all=\"ignore\"):\n1984                 new_data = (\n1985                     f(self_data, other_data)\n1986                     if not reflexive\n1987                     else f(other_data, self_data)\n1988                 )\n1989             result = Variable(dims, new_data, attrs=attrs)\n1990             return result\n1991 \n1992         return func\n1993 \n1994     @staticmethod\n1995     def _inplace_binary_op(f):\n1996         @functools.wraps(f)\n1997         def func(self, other):\n1998             if isinstance(other, xr.Dataset):\n1999                 raise TypeError(\"cannot add a Dataset to a Variable in-place\")\n2000             self_data, other_data, dims = _broadcast_compat_data(self, other)\n2001             if dims != self.dims:\n2002                 raise ValueError(\"dimensions cannot change for in-place \" \"operations\")\n2003             with np.errstate(all=\"ignore\"):\n2004                 self.values = f(self_data, other_data)\n2005             return self\n2006 \n2007         return func\n2008 \n2009     def _to_numeric(self, offset=None, datetime_unit=None, dtype=float):\n2010         \"\"\" A (private) method to convert datetime array to numeric dtype\n2011         See duck_array_ops.datetime_to_numeric\n2012         \"\"\"\n2013         numeric_array = duck_array_ops.datetime_to_numeric(\n2014             self.data, offset, datetime_unit, dtype\n2015         )\n2016         return type(self)(self.dims, numeric_array, self._attrs)\n",
        "intended_behavior": "This class provides additional context to the issue."
    }
]