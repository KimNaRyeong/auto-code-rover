2025-10-17 18:09:15.029 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-17 18:09:18.949 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=479, output_tokens=12, cost=0.005150
2025-10-17 18:09:25.867 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=659, output_tokens=236, cost=0.013670
2025-10-17 18:09:26.849 | INFO     | registered non reproducing test 0
2025-10-17 18:09:38.915 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1754, output_tokens=471, cost=0.031670
2025-10-17 18:09:39.891 | INFO     | registered non reproducing test 1
2025-10-17 18:09:55.103 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1989, output_tokens=501, cost=0.034920
2025-10-17 18:09:56.078 | INFO     | registered non reproducing test 2
2025-10-17 18:09:56.078 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-17 18:09:56.085 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 18:10:00.793 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=781, output_tokens=175, cost=0.013060
2025-10-17 18:10:00.800 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 18:10:03.816 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=529, output_tokens=53, cost=0.006880
2025-10-17 18:10:03.817 | DEBUG    | Extracted a valid json.
2025-10-17 18:10:03.817 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"AdaBoostClassifier\")",
        "search_method_in_class(\"_validate_params\", \"AdaBoostClassifier\")",
        "search_method(\"validate_parameter_constraints\")"
    ],
    "bug_locations": []
}
2025-10-17 18:10:03.851 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-17 18:10:03.852 | DEBUG    | <Agent search> Analyzing search results.
2025-10-17 18:10:17.951 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2108, output_tokens=570, cost=0.038180
2025-10-17 18:10:17.964 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 18:10:27.103 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2894, output_tokens=375, cost=0.040190
2025-10-17 18:10:27.110 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 18:10:41.019 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=729, output_tokens=401, cost=0.019320
2025-10-17 18:10:41.020 | DEBUG    | Extracted a valid json.
2025-10-17 18:10:41.021 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "sklearn/utils/_param_validation.py",
            "class": "",
            "method": "validate_parameter_constraints",
            "intended_behavior": "The `validate_parameter_constraints` function is intended to verify that all provided parameter values satisfy the constraints established for them. To resolve the issue, this function needs to be updated or the constraints for `base_estimator` need adjustment to explicitly allow `None` as a valid value. Since `base_estimator` has been deprecated in favor of `estimator`, its validation must handle both the deprecation gracefully and support `None` to maintain backward compatibility. Providing a clear path for how `None` is treated (either as an acceptable value during the transition or by implementing a fallback mechanism when `base_estimator` is `None`) is crucial. This might include adjusting the parameter constraints definition for `base_estimator` to include `None` as a valid option or handling the deprecation case more explicitly within the validation logic."
        },
        {
            "file": "sklearn/ensemble/_weight_boosting.py",
            "class": "AdaBoostClassifier",
            "method": "",
            "intended_behavior": "The constructor of `AdaBoostClassifier` might need adjustments to ensure that when `base_estimator` is set to `None`, it is either treated as a valid setting, or an appropriate default estimator is set in place if `None` is not a desired final value. The key is to ensure that the transition from `base_estimator` to `estimator` does not break existing codebases that rely on the default behavior of `base_estimator=None`. This could involve adding logic within the constructor or a related method that checks for the presence of `base_estimator` and handles its deprecation and value of `None` appropriately, ensuring users who have not yet migrated to the new `estimator` parameter do not experience errors."
        }
    ]
}
2025-10-17 18:10:41.030 | DEBUG    | Bug location extracted successfully: [<file>sklearn/utils/_param_validation.py</file>
<class>None</class>
<method>validate_parameter_constraints</method>
<code>
28 def validate_parameter_constraints(parameter_constraints, params, caller_name):
29     """Validate types and values of given parameters.
30 
31     Parameters
32     ----------
33     parameter_constraints : dict or {"no_validation"}
34         If "no_validation", validation is skipped for this parameter.
35 
36         If a dict, it must be a dictionary `param_name: list of constraints`.
37         A parameter is valid if it satisfies one of the constraints from the list.
38         Constraints can be:
39         - an Interval object, representing a continuous or discrete range of numbers
40         - the string "array-like"
41         - the string "sparse matrix"
42         - the string "random_state"
43         - callable
44         - None, meaning that None is a valid value for the parameter
45         - any type, meaning that any instance of this type is valid
46         - an Options object, representing a set of elements of a given type
47         - a StrOptions object, representing a set of strings
48         - the string "boolean"
49         - the string "verbose"
50         - the string "cv_object"
51         - a MissingValues object representing markers for missing values
52         - a HasMethods object, representing method(s) an object must have
53         - a Hidden object, representing a constraint not meant to be exposed to the user
54 
55     params : dict
56         A dictionary `param_name: param_value`. The parameters to validate against the
57         constraints.
58 
59     caller_name : str
60         The name of the estimator or function or method that called this function.
61     """
62     for param_name, param_val in params.items():
63         # We allow parameters to not have a constraint so that third party estimators
64         # can inherit from sklearn estimators without having to necessarily use the
65         # validation tools.
66         if param_name not in parameter_constraints:
67             continue
68 
69         constraints = parameter_constraints[param_name]
70 
71         if constraints == "no_validation":
72             continue
73 
74         constraints = [make_constraint(constraint) for constraint in constraints]
75 
76         for constraint in constraints:
77             if constraint.is_satisfied_by(param_val):
78                 # this constraint is satisfied, no need to check further.
79                 break
80         else:
81             # No constraint is satisfied, raise with an informative message.
82 
83             # Ignore constraints that we don't want to expose in the error message,
84             # i.e. options that are for internal purpose or not officially supported.
85             constraints = [
86                 constraint for constraint in constraints if not constraint.hidden
87             ]
88 
89             if len(constraints) == 1:
90                 constraints_str = f"{constraints[0]}"
91             else:
92                 constraints_str = (
93                     f"{', '.join([str(c) for c in constraints[:-1]])} or"
94                     f" {constraints[-1]}"
95                 )
96 
97             raise InvalidParameterError(
98                 f"The {param_name!r} parameter of {caller_name} must be"
99                 f" {constraints_str}. Got {param_val!r} instead."
100             )

</code><intended_behavior>The `validate_parameter_constraints` function is intended to verify that all provided parameter values satisfy the constraints established for them. To resolve the issue, this function needs to be updated or the constraints for `base_estimator` need adjustment to explicitly allow `None` as a valid value. Since `base_estimator` has been deprecated in favor of `estimator`, its validation must handle both the deprecation gracefully and support `None` to maintain backward compatibility. Providing a clear path for how `None` is treated (either as an acceptable value during the transition or by implementing a fallback mechanism when `base_estimator` is `None`) is crucial. This might include adjusting the parameter constraints definition for `base_estimator` to include `None` as a valid option or handling the deprecation case more explicitly within the validation logic.</intended_behavior>, <file>sklearn/ensemble/_weight_boosting.py</file>
<class>AdaBoostClassifier</class>
<method>None</method>
<code>
331 class AdaBoostClassifier(ClassifierMixin, BaseWeightBoosting):
332     """An AdaBoost classifier.
333 
334     An AdaBoost [1] classifier is a meta-estimator that begins by fitting a
335     classifier on the original dataset and then fits additional copies of the
336     classifier on the same dataset but where the weights of incorrectly
337     classified instances are adjusted such that subsequent classifiers focus
338     more on difficult cases.
339 
340     This class implements the algorithm known as AdaBoost-SAMME [2].
341 
342     Read more in the :ref:`User Guide <adaboost>`.
343 
344     .. versionadded:: 0.14
345 
346     Parameters
347     ----------
348     estimator : object, default=None
349         The base estimator from which the boosted ensemble is built.
350         Support for sample weighting is required, as well as proper
351         ``classes_`` and ``n_classes_`` attributes. If ``None``, then
352         the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`
353         initialized with `max_depth=1`.
354 
355         .. versionadded:: 1.2
356            `base_estimator` was renamed to `estimator`.
357 
358     n_estimators : int, default=50
359         The maximum number of estimators at which boosting is terminated.
360         In case of perfect fit, the learning procedure is stopped early.
361         Values must be in the range `[1, inf)`.
362 
363     learning_rate : float, default=1.0
364         Weight applied to each classifier at each boosting iteration. A higher
365         learning rate increases the contribution of each classifier. There is
366         a trade-off between the `learning_rate` and `n_estimators` parameters.
367         Values must be in the range `(0.0, inf)`.
368 
369     algorithm : {'SAMME', 'SAMME.R'}, default='SAMME.R'
370         If 'SAMME.R' then use the SAMME.R real boosting algorithm.
371         ``estimator`` must support calculation of class probabilities.
372         If 'SAMME' then use the SAMME discrete boosting algorithm.
373         The SAMME.R algorithm typically converges faster than SAMME,
374         achieving a lower test error with fewer boosting iterations.
375 
376     random_state : int, RandomState instance or None, default=None
377         Controls the random seed given at each `estimator` at each
378         boosting iteration.
379         Thus, it is only used when `estimator` exposes a `random_state`.
380         Pass an int for reproducible output across multiple function calls.
381         See :term:`Glossary <random_state>`.
382 
383     base_estimator : object, default=None
384         The base estimator from which the boosted ensemble is built.
385         Support for sample weighting is required, as well as proper
386         ``classes_`` and ``n_classes_`` attributes. If ``None``, then
387         the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`
388         initialized with `max_depth=1`.
389 
390         .. deprecated:: 1.2
391             `base_estimator` is deprecated and will be removed in 1.4.
392             Use `estimator` instead.
393 
394     Attributes
395     ----------
396     estimator_ : estimator
397         The base estimator from which the ensemble is grown.
398 
399         .. versionadded:: 1.2
400            `base_estimator_` was renamed to `estimator_`.
401 
402     base_estimator_ : estimator
403         The base estimator from which the ensemble is grown.
404 
405         .. deprecated:: 1.2
406             `base_estimator_` is deprecated and will be removed in 1.4.
407             Use `estimator_` instead.
408 
409     estimators_ : list of classifiers
410         The collection of fitted sub-estimators.
411 
412     classes_ : ndarray of shape (n_classes,)
413         The classes labels.
414 
415     n_classes_ : int
416         The number of classes.
417 
418     estimator_weights_ : ndarray of floats
419         Weights for each estimator in the boosted ensemble.
420 
421     estimator_errors_ : ndarray of floats
422         Classification error for each estimator in the boosted
423         ensemble.
424 
425     feature_importances_ : ndarray of shape (n_features,)
426         The impurity-based feature importances if supported by the
427         ``estimator`` (when based on decision trees).
428 
429         Warning: impurity-based feature importances can be misleading for
430         high cardinality features (many unique values). See
431         :func:`sklearn.inspection.permutation_importance` as an alternative.
432 
433     n_features_in_ : int
434         Number of features seen during :term:`fit`.
435 
436         .. versionadded:: 0.24
437 
438     feature_names_in_ : ndarray of shape (`n_features_in_`,)
439         Names of features seen during :term:`fit`. Defined only when `X`
440         has feature names that are all strings.
441 
442         .. versionadded:: 1.0
443 
444     See Also
445     --------
446     AdaBoostRegressor : An AdaBoost regressor that begins by fitting a
447         regressor on the original dataset and then fits additional copies of
448         the regressor on the same dataset but where the weights of instances
449         are adjusted according to the error of the current prediction.
450 
451     GradientBoostingClassifier : GB builds an additive model in a forward
452         stage-wise fashion. Regression trees are fit on the negative gradient
453         of the binomial or multinomial deviance loss function. Binary
454         classification is a special case where only a single regression tree is
455         induced.
456 
457     sklearn.tree.DecisionTreeClassifier : A non-parametric supervised learning
458         method used for classification.
459         Creates a model that predicts the value of a target variable by
460         learning simple decision rules inferred from the data features.
461 
462     References
463     ----------
464     .. [1] Y. Freund, R. Schapire, "A Decision-Theoretic Generalization of
465            on-Line Learning and an Application to Boosting", 1995.
466 
467     .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, "Multi-class AdaBoost", 2009.
468 
469     Examples
470     --------
471     >>> from sklearn.ensemble import AdaBoostClassifier
472     >>> from sklearn.datasets import make_classification
473     >>> X, y = make_classification(n_samples=1000, n_features=4,
474     ...                            n_informative=2, n_redundant=0,
475     ...                            random_state=0, shuffle=False)
476     >>> clf = AdaBoostClassifier(n_estimators=100, random_state=0)
477     >>> clf.fit(X, y)
478     AdaBoostClassifier(n_estimators=100, random_state=0)
479     >>> clf.predict([[0, 0, 0, 0]])
480     array([1])
481     >>> clf.score(X, y)
482     0.983...
483     """
484 
485     _parameter_constraints: dict = {
486         **BaseWeightBoosting._parameter_constraints,
487         "algorithm": [StrOptions({"SAMME", "SAMME.R"})],
488     }
489 
490     def __init__(
491         self,
492         estimator=None,
493         *,
494         n_estimators=50,
495         learning_rate=1.0,
496         algorithm="SAMME.R",
497         random_state=None,
498         base_estimator="deprecated",
499     ):
500         super().__init__(
501             estimator=estimator,
502             n_estimators=n_estimators,
503             learning_rate=learning_rate,
504             random_state=random_state,
505             base_estimator=base_estimator,
506         )
507 
508         self.algorithm = algorithm
509 
510     def _validate_estimator(self):
511         """Check the estimator and set the estimator_ attribute."""
512         super()._validate_estimator(default=DecisionTreeClassifier(max_depth=1))
513 
514         #  SAMME-R requires predict_proba-enabled base estimators
515         if self.algorithm == "SAMME.R":
516             if not hasattr(self.estimator_, "predict_proba"):
517                 raise TypeError(
518                     "AdaBoostClassifier with algorithm='SAMME.R' requires "
519                     "that the weak learner supports the calculation of class "
520                     "probabilities with a predict_proba method.\n"
521                     "Please change the base estimator or set "
522                     "algorithm='SAMME' instead."
523                 )
524         if not has_fit_parameter(self.estimator_, "sample_weight"):
525             raise ValueError(
526                 f"{self.estimator.__class__.__name__} doesn't support sample_weight."
527             )
528 
529     def _boost(self, iboost, X, y, sample_weight, random_state):
530         """Implement a single boost.
531 
532         Perform a single boost according to the real multi-class SAMME.R
533         algorithm or to the discrete SAMME algorithm and return the updated
534         sample weights.
535 
536         Parameters
537         ----------
538         iboost : int
539             The index of the current boost iteration.
540 
541         X : {array-like, sparse matrix} of shape (n_samples, n_features)
542             The training input samples.
543 
544         y : array-like of shape (n_samples,)
545             The target values (class labels).
546 
547         sample_weight : array-like of shape (n_samples,)
548             The current sample weights.
549 
550         random_state : RandomState instance
551             The RandomState instance used if the base estimator accepts a
552             `random_state` attribute.
553 
554         Returns
555         -------
556         sample_weight : array-like of shape (n_samples,) or None
557             The reweighted sample weights.
558             If None then boosting has terminated early.
559 
560         estimator_weight : float
561             The weight for the current boost.
562             If None then boosting has terminated early.
563 
564         estimator_error : float
565             The classification error for the current boost.
566             If None then boosting has terminated early.
567         """
568         if self.algorithm == "SAMME.R":
569             return self._boost_real(iboost, X, y, sample_weight, random_state)
570 
571         else:  # elif self.algorithm == "SAMME":
572             return self._boost_discrete(iboost, X, y, sample_weight, random_state)
573 
574     def _boost_real(self, iboost, X, y, sample_weight, random_state):
575         """Implement a single boost using the SAMME.R real algorithm."""
576         estimator = self._make_estimator(random_state=random_state)
577 
578         estimator.fit(X, y, sample_weight=sample_weight)
579 
580         y_predict_proba = estimator.predict_proba(X)
581 
582         if iboost == 0:
583             self.classes_ = getattr(estimator, "classes_", None)
584             self.n_classes_ = len(self.classes_)
585 
586         y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1), axis=0)
587 
588         # Instances incorrectly classified
589         incorrect = y_predict != y
590 
591         # Error fraction
592         estimator_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))
593 
594         # Stop if classification is perfect
595         if estimator_error <= 0:
596             return sample_weight, 1.0, 0.0
597 
598         # Construct y coding as described in Zhu et al [2]:
599         #
600         #    y_k = 1 if c == k else -1 / (K - 1)
601         #
602         # where K == n_classes_ and c, k in [0, K) are indices along the second
603         # axis of the y coding with c being the index corresponding to the true
604         # class label.
605         n_classes = self.n_classes_
606         classes = self.classes_
607         y_codes = np.array([-1.0 / (n_classes - 1), 1.0])
608         y_coding = y_codes.take(classes == y[:, np.newaxis])
609 
610         # Displace zero probabilities so the log is defined.
611         # Also fix negative elements which may occur with
612         # negative sample weights.
613         proba = y_predict_proba  # alias for readability
614         np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)
615 
616         # Boost weight using multi-class AdaBoost SAMME.R alg
617         estimator_weight = (
618             -1.0
619             * self.learning_rate
620             * ((n_classes - 1.0) / n_classes)
621             * xlogy(y_coding, y_predict_proba).sum(axis=1)
622         )
623 
624         # Only boost the weights if it will fit again
625         if not iboost == self.n_estimators - 1:
626             # Only boost positive weights
627             sample_weight *= np.exp(
628                 estimator_weight * ((sample_weight > 0) | (estimator_weight < 0))
629             )
630 
631         return sample_weight, 1.0, estimator_error
632 
633     def _boost_discrete(self, iboost, X, y, sample_weight, random_state):
634         """Implement a single boost using the SAMME discrete algorithm."""
635         estimator = self._make_estimator(random_state=random_state)
636 
637         estimator.fit(X, y, sample_weight=sample_weight)
638 
639         y_predict = estimator.predict(X)
640 
641         if iboost == 0:
642             self.classes_ = getattr(estimator, "classes_", None)
643             self.n_classes_ = len(self.classes_)
644 
645         # Instances incorrectly classified
646         incorrect = y_predict != y
647 
648         # Error fraction
649         estimator_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))
650 
651         # Stop if classification is perfect
652         if estimator_error <= 0:
653             return sample_weight, 1.0, 0.0
654 
655         n_classes = self.n_classes_
656 
657         # Stop if the error is at least as bad as random guessing
658         if estimator_error >= 1.0 - (1.0 / n_classes):
659             self.estimators_.pop(-1)
660             if len(self.estimators_) == 0:
661                 raise ValueError(
662                     "BaseClassifier in AdaBoostClassifier "
663                     "ensemble is worse than random, ensemble "
664                     "can not be fit."
665                 )
666             return None, None, None
667 
668         # Boost weight using multi-class AdaBoost SAMME alg
669         estimator_weight = self.learning_rate * (
670             np.log((1.0 - estimator_error) / estimator_error) + np.log(n_classes - 1.0)
671         )
672 
673         # Only boost the weights if it will fit again
674         if not iboost == self.n_estimators - 1:
675             # Only boost positive weights
676             sample_weight = np.exp(
677                 np.log(sample_weight)
678                 + estimator_weight * incorrect * (sample_weight > 0)
679             )
680 
681         return sample_weight, estimator_weight, estimator_error
682 
683     def predict(self, X):
684         """Predict classes for X.
685 
686         The predicted class of an input sample is computed as the weighted mean
687         prediction of the classifiers in the ensemble.
688 
689         Parameters
690         ----------
691         X : {array-like, sparse matrix} of shape (n_samples, n_features)
692             The training input samples. Sparse matrix can be CSC, CSR, COO,
693             DOK, or LIL. COO, DOK, and LIL are converted to CSR.
694 
695         Returns
696         -------
697         y : ndarray of shape (n_samples,)
698             The predicted classes.
699         """
700         pred = self.decision_function(X)
701 
702         if self.n_classes_ == 2:
703             return self.classes_.take(pred > 0, axis=0)
704 
705         return self.classes_.take(np.argmax(pred, axis=1), axis=0)
706 
707     def staged_predict(self, X):
708         """Return staged predictions for X.
709 
710         The predicted class of an input sample is computed as the weighted mean
711         prediction of the classifiers in the ensemble.
712 
713         This generator method yields the ensemble prediction after each
714         iteration of boosting and therefore allows monitoring, such as to
715         determine the prediction on a test set after each boost.
716 
717         Parameters
718         ----------
719         X : array-like of shape (n_samples, n_features)
720             The input samples. Sparse matrix can be CSC, CSR, COO,
721             DOK, or LIL. COO, DOK, and LIL are converted to CSR.
722 
723         Yields
724         ------
725         y : generator of ndarray of shape (n_samples,)
726             The predicted classes.
727         """
728         X = self._check_X(X)
729 
730         n_classes = self.n_classes_
731         classes = self.classes_
732 
733         if n_classes == 2:
734             for pred in self.staged_decision_function(X):
735                 yield np.array(classes.take(pred > 0, axis=0))
736 
737         else:
738             for pred in self.staged_decision_function(X):
739                 yield np.array(classes.take(np.argmax(pred, axis=1), axis=0))
740 
741     def decision_function(self, X):
742         """Compute the decision function of ``X``.
743 
744         Parameters
745         ----------
746         X : {array-like, sparse matrix} of shape (n_samples, n_features)
747             The training input samples. Sparse matrix can be CSC, CSR, COO,
748             DOK, or LIL. COO, DOK, and LIL are converted to CSR.
749 
750         Returns
751         -------
752         score : ndarray of shape of (n_samples, k)
753             The decision function of the input samples. The order of
754             outputs is the same of that of the :term:`classes_` attribute.
755             Binary classification is a special cases with ``k == 1``,
756             otherwise ``k==n_classes``. For binary classification,
757             values closer to -1 or 1 mean more like the first or second
758             class in ``classes_``, respectively.
759         """
760         check_is_fitted(self)
761         X = self._check_X(X)
762 
763         n_classes = self.n_classes_
764         classes = self.classes_[:, np.newaxis]
765 
766         if self.algorithm == "SAMME.R":
767             # The weights are all 1. for SAMME.R
768             pred = sum(
769                 _samme_proba(estimator, n_classes, X) for estimator in self.estimators_
770             )
771         else:  # self.algorithm == "SAMME"
772             pred = sum(
773                 (estimator.predict(X) == classes).T * w
774                 for estimator, w in zip(self.estimators_, self.estimator_weights_)
775             )
776 
777         pred /= self.estimator_weights_.sum()
778         if n_classes == 2:
779             pred[:, 0] *= -1
780             return pred.sum(axis=1)
781         return pred
782 
783     def staged_decision_function(self, X):
784         """Compute decision function of ``X`` for each boosting iteration.
785 
786         This method allows monitoring (i.e. determine error on testing set)
787         after each boosting iteration.
788 
789         Parameters
790         ----------
791         X : {array-like, sparse matrix} of shape (n_samples, n_features)
792             The training input samples. Sparse matrix can be CSC, CSR, COO,
793             DOK, or LIL. COO, DOK, and LIL are converted to CSR.
794 
795         Yields
796         ------
797         score : generator of ndarray of shape (n_samples, k)
798             The decision function of the input samples. The order of
799             outputs is the same of that of the :term:`classes_` attribute.
800             Binary classification is a special cases with ``k == 1``,
801             otherwise ``k==n_classes``. For binary classification,
802             values closer to -1 or 1 mean more like the first or second
803             class in ``classes_``, respectively.
804         """
805         check_is_fitted(self)
806         X = self._check_X(X)
807 
808         n_classes = self.n_classes_
809         classes = self.classes_[:, np.newaxis]
810         pred = None
811         norm = 0.0
812 
813         for weight, estimator in zip(self.estimator_weights_, self.estimators_):
814             norm += weight
815 
816             if self.algorithm == "SAMME.R":
817                 # The weights are all 1. for SAMME.R
818                 current_pred = _samme_proba(estimator, n_classes, X)
819             else:  # elif self.algorithm == "SAMME":
820                 current_pred = estimator.predict(X)
821                 current_pred = (current_pred == classes).T * weight
822 
823             if pred is None:
824                 pred = current_pred
825             else:
826                 pred += current_pred
827 
828             if n_classes == 2:
829                 tmp_pred = np.copy(pred)
830                 tmp_pred[:, 0] *= -1
831                 yield (tmp_pred / norm).sum(axis=1)
832             else:
833                 yield pred / norm
834 
835     @staticmethod
836     def _compute_proba_from_decision(decision, n_classes):
837         """Compute probabilities from the decision function.
838 
839         This is based eq. (4) of [1] where:
840             p(y=c|X) = exp((1 / K-1) f_c(X)) / sum_k(exp((1 / K-1) f_k(X)))
841                      = softmax((1 / K-1) * f(X))
842 
843         References
844         ----------
845         .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, "Multi-class AdaBoost",
846                2009.
847         """
848         if n_classes == 2:
849             decision = np.vstack([-decision, decision]).T / 2
850         else:
851             decision /= n_classes - 1
852         return softmax(decision, copy=False)
853 
854     def predict_proba(self, X):
855         """Predict class probabilities for X.
856 
857         The predicted class probabilities of an input sample is computed as
858         the weighted mean predicted class probabilities of the classifiers
859         in the ensemble.
860 
861         Parameters
862         ----------
863         X : {array-like, sparse matrix} of shape (n_samples, n_features)
864             The training input samples. Sparse matrix can be CSC, CSR, COO,
865             DOK, or LIL. COO, DOK, and LIL are converted to CSR.
866 
867         Returns
868         -------
869         p : ndarray of shape (n_samples, n_classes)
870             The class probabilities of the input samples. The order of
871             outputs is the same of that of the :term:`classes_` attribute.
872         """
873         check_is_fitted(self)
874         n_classes = self.n_classes_
875 
876         if n_classes == 1:
877             return np.ones((_num_samples(X), 1))
878 
879         decision = self.decision_function(X)
880         return self._compute_proba_from_decision(decision, n_classes)
881 
882     def staged_predict_proba(self, X):
883         """Predict class probabilities for X.
884 
885         The predicted class probabilities of an input sample is computed as
886         the weighted mean predicted class probabilities of the classifiers
887         in the ensemble.
888 
889         This generator method yields the ensemble predicted class probabilities
890         after each iteration of boosting and therefore allows monitoring, such
891         as to determine the predicted class probabilities on a test set after
892         each boost.
893 
894         Parameters
895         ----------
896         X : {array-like, sparse matrix} of shape (n_samples, n_features)
897             The training input samples. Sparse matrix can be CSC, CSR, COO,
898             DOK, or LIL. COO, DOK, and LIL are converted to CSR.
899 
900         Yields
901         ------
902         p : generator of ndarray of shape (n_samples,)
903             The class probabilities of the input samples. The order of
904             outputs is the same of that of the :term:`classes_` attribute.
905         """
906 
907         n_classes = self.n_classes_
908 
909         for decision in self.staged_decision_function(X):
910             yield self._compute_proba_from_decision(decision, n_classes)
911 
912     def predict_log_proba(self, X):
913         """Predict class log-probabilities for X.
914 
915         The predicted class log-probabilities of an input sample is computed as
916         the weighted mean predicted class log-probabilities of the classifiers
917         in the ensemble.
918 
919         Parameters
920         ----------
921         X : {array-like, sparse matrix} of shape (n_samples, n_features)
922             The training input samples. Sparse matrix can be CSC, CSR, COO,
923             DOK, or LIL. COO, DOK, and LIL are converted to CSR.
924 
925         Returns
926         -------
927         p : ndarray of shape (n_samples, n_classes)
928             The class probabilities of the input samples. The order of
929             outputs is the same of that of the :term:`classes_` attribute.
930         """
931         return np.log(self.predict_proba(X))

</code><intended_behavior>The constructor of `AdaBoostClassifier` might need adjustments to ensure that when `base_estimator` is set to `None`, it is either treated as a valid setting, or an appropriate default estimator is set in place if `None` is not a desired final value. The key is to ensure that the transition from `base_estimator` to `estimator` does not break existing codebases that rely on the default behavior of `base_estimator=None`. This could involve adding logic within the constructor or a related method that checks for the presence of `base_estimator` and handles its deprecation and value of `None` appropriately, ensuring users who have not yet migrated to the new `estimator` parameter do not experience errors.</intended_behavior>]
2025-10-17 18:10:41.030 | INFO     | Search completed. Bug locations: [<file>sklearn/utils/_param_validation.py</file>
<class>None</class>
<method>validate_parameter_constraints</method>
<code>
28 def validate_parameter_constraints(parameter_constraints, params, caller_name):
29     """Validate types and values of given parameters.
30 
31     Parameters
32     ----------
33     parameter_constraints : dict or {"no_validation"}
34         If "no_validation", validation is skipped for this parameter.
35 
36         If a dict, it must be a dictionary `param_name: list of constraints`.
37         A parameter is valid if it satisfies one of the constraints from the list.
38         Constraints can be:
39         - an Interval object, representing a continuous or discrete range of numbers
40         - the string "array-like"
41         - the string "sparse matrix"
42         - the string "random_state"
43         - callable
44         - None, meaning that None is a valid value for the parameter
45         - any type, meaning that any instance of this type is valid
46         - an Options object, representing a set of elements of a given type
47         - a StrOptions object, representing a set of strings
48         - the string "boolean"
49         - the string "verbose"
50         - the string "cv_object"
51         - a MissingValues object representing markers for missing values
52         - a HasMethods object, representing method(s) an object must have
53         - a Hidden object, representing a constraint not meant to be exposed to the user
54 
55     params : dict
56         A dictionary `param_name: param_value`. The parameters to validate against the
57         constraints.
58 
59     caller_name : str
60         The name of the estimator or function or method that called this function.
61     """
62     for param_name, param_val in params.items():
63         # We allow parameters to not have a constraint so that third party estimators
64         # can inherit from sklearn estimators without having to necessarily use the
65         # validation tools.
66         if param_name not in parameter_constraints:
67             continue
68 
69         constraints = parameter_constraints[param_name]
70 
71         if constraints == "no_validation":
72             continue
73 
74         constraints = [make_constraint(constraint) for constraint in constraints]
75 
76         for constraint in constraints:
77             if constraint.is_satisfied_by(param_val):
78                 # this constraint is satisfied, no need to check further.
79                 break
80         else:
81             # No constraint is satisfied, raise with an informative message.
82 
83             # Ignore constraints that we don't want to expose in the error message,
84             # i.e. options that are for internal purpose or not officially supported.
85             constraints = [
86                 constraint for constraint in constraints if not constraint.hidden
87             ]
88 
89             if len(constraints) == 1:
90                 constraints_str = f"{constraints[0]}"
91             else:
92                 constraints_str = (
93                     f"{', '.join([str(c) for c in constraints[:-1]])} or"
94                     f" {constraints[-1]}"
95                 )
96 
97             raise InvalidParameterError(
98                 f"The {param_name!r} parameter of {caller_name} must be"
99                 f" {constraints_str}. Got {param_val!r} instead."
100             )

</code><intended_behavior>The `validate_parameter_constraints` function is intended to verify that all provided parameter values satisfy the constraints established for them. To resolve the issue, this function needs to be updated or the constraints for `base_estimator` need adjustment to explicitly allow `None` as a valid value. Since `base_estimator` has been deprecated in favor of `estimator`, its validation must handle both the deprecation gracefully and support `None` to maintain backward compatibility. Providing a clear path for how `None` is treated (either as an acceptable value during the transition or by implementing a fallback mechanism when `base_estimator` is `None`) is crucial. This might include adjusting the parameter constraints definition for `base_estimator` to include `None` as a valid option or handling the deprecation case more explicitly within the validation logic.</intended_behavior>, <file>sklearn/ensemble/_weight_boosting.py</file>
<class>AdaBoostClassifier</class>
<method>None</method>
<code>
331 class AdaBoostClassifier(ClassifierMixin, BaseWeightBoosting):
332     """An AdaBoost classifier.
333 
334     An AdaBoost [1] classifier is a meta-estimator that begins by fitting a
335     classifier on the original dataset and then fits additional copies of the
336     classifier on the same dataset but where the weights of incorrectly
337     classified instances are adjusted such that subsequent classifiers focus
338     more on difficult cases.
339 
340     This class implements the algorithm known as AdaBoost-SAMME [2].
341 
342     Read more in the :ref:`User Guide <adaboost>`.
343 
344     .. versionadded:: 0.14
345 
346     Parameters
347     ----------
348     estimator : object, default=None
349         The base estimator from which the boosted ensemble is built.
350         Support for sample weighting is required, as well as proper
351         ``classes_`` and ``n_classes_`` attributes. If ``None``, then
352         the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`
353         initialized with `max_depth=1`.
354 
355         .. versionadded:: 1.2
356            `base_estimator` was renamed to `estimator`.
357 
358     n_estimators : int, default=50
359         The maximum number of estimators at which boosting is terminated.
360         In case of perfect fit, the learning procedure is stopped early.
361         Values must be in the range `[1, inf)`.
362 
363     learning_rate : float, default=1.0
364         Weight applied to each classifier at each boosting iteration. A higher
365         learning rate increases the contribution of each classifier. There is
366         a trade-off between the `learning_rate` and `n_estimators` parameters.
367         Values must be in the range `(0.0, inf)`.
368 
369     algorithm : {'SAMME', 'SAMME.R'}, default='SAMME.R'
370         If 'SAMME.R' then use the SAMME.R real boosting algorithm.
371         ``estimator`` must support calculation of class probabilities.
372         If 'SAMME' then use the SAMME discrete boosting algorithm.
373         The SAMME.R algorithm typically converges faster than SAMME,
374         achieving a lower test error with fewer boosting iterations.
375 
376     random_state : int, RandomState instance or None, default=None
377         Controls the random seed given at each `estimator` at each
378         boosting iteration.
379         Thus, it is only used when `estimator` exposes a `random_state`.
380         Pass an int for reproducible output across multiple function calls.
381         See :term:`Glossary <random_state>`.
382 
383     base_estimator : object, default=None
384         The base estimator from which the boosted ensemble is built.
385         Support for sample weighting is required, as well as proper
386         ``classes_`` and ``n_classes_`` attributes. If ``None``, then
387         the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`
388         initialized with `max_depth=1`.
389 
390         .. deprecated:: 1.2
391             `base_estimator` is deprecated and will be removed in 1.4.
392             Use `estimator` instead.
393 
394     Attributes
395     ----------
396     estimator_ : estimator
397         The base estimator from which the ensemble is grown.
398 
399         .. versionadded:: 1.2
400            `base_estimator_` was renamed to `estimator_`.
401 
402     base_estimator_ : estimator
403         The base estimator from which the ensemble is grown.
404 
405         .. deprecated:: 1.2
406             `base_estimator_` is deprecated and will be removed in 1.4.
407             Use `estimator_` instead.
408 
409     estimators_ : list of classifiers
410         The collection of fitted sub-estimators.
411 
412     classes_ : ndarray of shape (n_classes,)
413         The classes labels.
414 
415     n_classes_ : int
416         The number of classes.
417 
418     estimator_weights_ : ndarray of floats
419         Weights for each estimator in the boosted ensemble.
420 
421     estimator_errors_ : ndarray of floats
422         Classification error for each estimator in the boosted
423         ensemble.
424 
425     feature_importances_ : ndarray of shape (n_features,)
426         The impurity-based feature importances if supported by the
427         ``estimator`` (when based on decision trees).
428 
429         Warning: impurity-based feature importances can be misleading for
430         high cardinality features (many unique values). See
431         :func:`sklearn.inspection.permutation_importance` as an alternative.
432 
433     n_features_in_ : int
434         Number of features seen during :term:`fit`.
435 
436         .. versionadded:: 0.24
437 
438     feature_names_in_ : ndarray of shape (`n_features_in_`,)
439         Names of features seen during :term:`fit`. Defined only when `X`
440         has feature names that are all strings.
441 
442         .. versionadded:: 1.0
443 
444     See Also
445     --------
446     AdaBoostRegressor : An AdaBoost regressor that begins by fitting a
447         regressor on the original dataset and then fits additional copies of
448         the regressor on the same dataset but where the weights of instances
449         are adjusted according to the error of the current prediction.
450 
451     GradientBoostingClassifier : GB builds an additive model in a forward
452         stage-wise fashion. Regression trees are fit on the negative gradient
453         of the binomial or multinomial deviance loss function. Binary
454         classification is a special case where only a single regression tree is
455         induced.
456 
457     sklearn.tree.DecisionTreeClassifier : A non-parametric supervised learning
458         method used for classification.
459         Creates a model that predicts the value of a target variable by
460         learning simple decision rules inferred from the data features.
461 
462     References
463     ----------
464     .. [1] Y. Freund, R. Schapire, "A Decision-Theoretic Generalization of
465            on-Line Learning and an Application to Boosting", 1995.
466 
467     .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, "Multi-class AdaBoost", 2009.
468 
469     Examples
470     --------
471     >>> from sklearn.ensemble import AdaBoostClassifier
472     >>> from sklearn.datasets import make_classification
473     >>> X, y = make_classification(n_samples=1000, n_features=4,
474     ...                            n_informative=2, n_redundant=0,
475     ...                            random_state=0, shuffle=False)
476     >>> clf = AdaBoostClassifier(n_estimators=100, random_state=0)
477     >>> clf.fit(X, y)
478     AdaBoostClassifier(n_estimators=100, random_state=0)
479     >>> clf.predict([[0, 0, 0, 0]])
480     array([1])
481     >>> clf.score(X, y)
482     0.983...
483     """
484 
485     _parameter_constraints: dict = {
486         **BaseWeightBoosting._parameter_constraints,
487         "algorithm": [StrOptions({"SAMME", "SAMME.R"})],
488     }
489 
490     def __init__(
491         self,
492         estimator=None,
493         *,
494         n_estimators=50,
495         learning_rate=1.0,
496         algorithm="SAMME.R",
497         random_state=None,
498         base_estimator="deprecated",
499     ):
500         super().__init__(
501             estimator=estimator,
502             n_estimators=n_estimators,
503             learning_rate=learning_rate,
504             random_state=random_state,
505             base_estimator=base_estimator,
506         )
507 
508         self.algorithm = algorithm
509 
510     def _validate_estimator(self):
511         """Check the estimator and set the estimator_ attribute."""
512         super()._validate_estimator(default=DecisionTreeClassifier(max_depth=1))
513 
514         #  SAMME-R requires predict_proba-enabled base estimators
515         if self.algorithm == "SAMME.R":
516             if not hasattr(self.estimator_, "predict_proba"):
517                 raise TypeError(
518                     "AdaBoostClassifier with algorithm='SAMME.R' requires "
519                     "that the weak learner supports the calculation of class "
520                     "probabilities with a predict_proba method.\n"
521                     "Please change the base estimator or set "
522                     "algorithm='SAMME' instead."
523                 )
524         if not has_fit_parameter(self.estimator_, "sample_weight"):
525             raise ValueError(
526                 f"{self.estimator.__class__.__name__} doesn't support sample_weight."
527             )
528 
529     def _boost(self, iboost, X, y, sample_weight, random_state):
530         """Implement a single boost.
531 
532         Perform a single boost according to the real multi-class SAMME.R
533         algorithm or to the discrete SAMME algorithm and return the updated
534         sample weights.
535 
536         Parameters
537         ----------
538         iboost : int
539             The index of the current boost iteration.
540 
541         X : {array-like, sparse matrix} of shape (n_samples, n_features)
542             The training input samples.
543 
544         y : array-like of shape (n_samples,)
545             The target values (class labels).
546 
547         sample_weight : array-like of shape (n_samples,)
548             The current sample weights.
549 
550         random_state : RandomState instance
551             The RandomState instance used if the base estimator accepts a
552             `random_state` attribute.
553 
554         Returns
555         -------
556         sample_weight : array-like of shape (n_samples,) or None
557             The reweighted sample weights.
558             If None then boosting has terminated early.
559 
560         estimator_weight : float
561             The weight for the current boost.
562             If None then boosting has terminated early.
563 
564         estimator_error : float
565             The classification error for the current boost.
566             If None then boosting has terminated early.
567         """
568         if self.algorithm == "SAMME.R":
569             return self._boost_real(iboost, X, y, sample_weight, random_state)
570 
571         else:  # elif self.algorithm == "SAMME":
572             return self._boost_discrete(iboost, X, y, sample_weight, random_state)
573 
574     def _boost_real(self, iboost, X, y, sample_weight, random_state):
575         """Implement a single boost using the SAMME.R real algorithm."""
576         estimator = self._make_estimator(random_state=random_state)
577 
578         estimator.fit(X, y, sample_weight=sample_weight)
579 
580         y_predict_proba = estimator.predict_proba(X)
581 
582         if iboost == 0:
583             self.classes_ = getattr(estimator, "classes_", None)
584             self.n_classes_ = len(self.classes_)
585 
586         y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1), axis=0)
587 
588         # Instances incorrectly classified
589         incorrect = y_predict != y
590 
591         # Error fraction
592         estimator_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))
593 
594         # Stop if classification is perfect
595         if estimator_error <= 0:
596             return sample_weight, 1.0, 0.0
597 
598         # Construct y coding as described in Zhu et al [2]:
599         #
600         #    y_k = 1 if c == k else -1 / (K - 1)
601         #
602         # where K == n_classes_ and c, k in [0, K) are indices along the second
603         # axis of the y coding with c being the index corresponding to the true
604         # class label.
605         n_classes = self.n_classes_
606         classes = self.classes_
607         y_codes = np.array([-1.0 / (n_classes - 1), 1.0])
608         y_coding = y_codes.take(classes == y[:, np.newaxis])
609 
610         # Displace zero probabilities so the log is defined.
611         # Also fix negative elements which may occur with
612         # negative sample weights.
613         proba = y_predict_proba  # alias for readability
614         np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)
615 
616         # Boost weight using multi-class AdaBoost SAMME.R alg
617         estimator_weight = (
618             -1.0
619             * self.learning_rate
620             * ((n_classes - 1.0) / n_classes)
621             * xlogy(y_coding, y_predict_proba).sum(axis=1)
622         )
623 
624         # Only boost the weights if it will fit again
625         if not iboost == self.n_estimators - 1:
626             # Only boost positive weights
627             sample_weight *= np.exp(
628                 estimator_weight * ((sample_weight > 0) | (estimator_weight < 0))
629             )
630 
631         return sample_weight, 1.0, estimator_error
632 
633     def _boost_discrete(self, iboost, X, y, sample_weight, random_state):
634         """Implement a single boost using the SAMME discrete algorithm."""
635         estimator = self._make_estimator(random_state=random_state)
636 
637         estimator.fit(X, y, sample_weight=sample_weight)
638 
639         y_predict = estimator.predict(X)
640 
641         if iboost == 0:
642             self.classes_ = getattr(estimator, "classes_", None)
643             self.n_classes_ = len(self.classes_)
644 
645         # Instances incorrectly classified
646         incorrect = y_predict != y
647 
648         # Error fraction
649         estimator_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))
650 
651         # Stop if classification is perfect
652         if estimator_error <= 0:
653             return sample_weight, 1.0, 0.0
654 
655         n_classes = self.n_classes_
656 
657         # Stop if the error is at least as bad as random guessing
658         if estimator_error >= 1.0 - (1.0 / n_classes):
659             self.estimators_.pop(-1)
660             if len(self.estimators_) == 0:
661                 raise ValueError(
662                     "BaseClassifier in AdaBoostClassifier "
663                     "ensemble is worse than random, ensemble "
664                     "can not be fit."
665                 )
666             return None, None, None
667 
668         # Boost weight using multi-class AdaBoost SAMME alg
669         estimator_weight = self.learning_rate * (
670             np.log((1.0 - estimator_error) / estimator_error) + np.log(n_classes - 1.0)
671         )
672 
673         # Only boost the weights if it will fit again
674         if not iboost == self.n_estimators - 1:
675             # Only boost positive weights
676             sample_weight = np.exp(
677                 np.log(sample_weight)
678                 + estimator_weight * incorrect * (sample_weight > 0)
679             )
680 
681         return sample_weight, estimator_weight, estimator_error
682 
683     def predict(self, X):
684         """Predict classes for X.
685 
686         The predicted class of an input sample is computed as the weighted mean
687         prediction of the classifiers in the ensemble.
688 
689         Parameters
690         ----------
691         X : {array-like, sparse matrix} of shape (n_samples, n_features)
692             The training input samples. Sparse matrix can be CSC, CSR, COO,
693             DOK, or LIL. COO, DOK, and LIL are converted to CSR.
694 
695         Returns
696         -------
697         y : ndarray of shape (n_samples,)
698             The predicted classes.
699         """
700         pred = self.decision_function(X)
701 
702         if self.n_classes_ == 2:
703             return self.classes_.take(pred > 0, axis=0)
704 
705         return self.classes_.take(np.argmax(pred, axis=1), axis=0)
706 
707     def staged_predict(self, X):
708         """Return staged predictions for X.
709 
710         The predicted class of an input sample is computed as the weighted mean
711         prediction of the classifiers in the ensemble.
712 
713         This generator method yields the ensemble prediction after each
714         iteration of boosting and therefore allows monitoring, such as to
715         determine the prediction on a test set after each boost.
716 
717         Parameters
718         ----------
719         X : array-like of shape (n_samples, n_features)
720             The input samples. Sparse matrix can be CSC, CSR, COO,
721             DOK, or LIL. COO, DOK, and LIL are converted to CSR.
722 
723         Yields
724         ------
725         y : generator of ndarray of shape (n_samples,)
726             The predicted classes.
727         """
728         X = self._check_X(X)
729 
730         n_classes = self.n_classes_
731         classes = self.classes_
732 
733         if n_classes == 2:
734             for pred in self.staged_decision_function(X):
735                 yield np.array(classes.take(pred > 0, axis=0))
736 
737         else:
738             for pred in self.staged_decision_function(X):
739                 yield np.array(classes.take(np.argmax(pred, axis=1), axis=0))
740 
741     def decision_function(self, X):
742         """Compute the decision function of ``X``.
743 
744         Parameters
745         ----------
746         X : {array-like, sparse matrix} of shape (n_samples, n_features)
747             The training input samples. Sparse matrix can be CSC, CSR, COO,
748             DOK, or LIL. COO, DOK, and LIL are converted to CSR.
749 
750         Returns
751         -------
752         score : ndarray of shape of (n_samples, k)
753             The decision function of the input samples. The order of
754             outputs is the same of that of the :term:`classes_` attribute.
755             Binary classification is a special cases with ``k == 1``,
756             otherwise ``k==n_classes``. For binary classification,
757             values closer to -1 or 1 mean more like the first or second
758             class in ``classes_``, respectively.
759         """
760         check_is_fitted(self)
761         X = self._check_X(X)
762 
763         n_classes = self.n_classes_
764         classes = self.classes_[:, np.newaxis]
765 
766         if self.algorithm == "SAMME.R":
767             # The weights are all 1. for SAMME.R
768             pred = sum(
769                 _samme_proba(estimator, n_classes, X) for estimator in self.estimators_
770             )
771         else:  # self.algorithm == "SAMME"
772             pred = sum(
773                 (estimator.predict(X) == classes).T * w
774                 for estimator, w in zip(self.estimators_, self.estimator_weights_)
775             )
776 
777         pred /= self.estimator_weights_.sum()
778         if n_classes == 2:
779             pred[:, 0] *= -1
780             return pred.sum(axis=1)
781         return pred
782 
783     def staged_decision_function(self, X):
784         """Compute decision function of ``X`` for each boosting iteration.
785 
786         This method allows monitoring (i.e. determine error on testing set)
787         after each boosting iteration.
788 
789         Parameters
790         ----------
791         X : {array-like, sparse matrix} of shape (n_samples, n_features)
792             The training input samples. Sparse matrix can be CSC, CSR, COO,
793             DOK, or LIL. COO, DOK, and LIL are converted to CSR.
794 
795         Yields
796         ------
797         score : generator of ndarray of shape (n_samples, k)
798             The decision function of the input samples. The order of
799             outputs is the same of that of the :term:`classes_` attribute.
800             Binary classification is a special cases with ``k == 1``,
801             otherwise ``k==n_classes``. For binary classification,
802             values closer to -1 or 1 mean more like the first or second
803             class in ``classes_``, respectively.
804         """
805         check_is_fitted(self)
806         X = self._check_X(X)
807 
808         n_classes = self.n_classes_
809         classes = self.classes_[:, np.newaxis]
810         pred = None
811         norm = 0.0
812 
813         for weight, estimator in zip(self.estimator_weights_, self.estimators_):
814             norm += weight
815 
816             if self.algorithm == "SAMME.R":
817                 # The weights are all 1. for SAMME.R
818                 current_pred = _samme_proba(estimator, n_classes, X)
819             else:  # elif self.algorithm == "SAMME":
820                 current_pred = estimator.predict(X)
821                 current_pred = (current_pred == classes).T * weight
822 
823             if pred is None:
824                 pred = current_pred
825             else:
826                 pred += current_pred
827 
828             if n_classes == 2:
829                 tmp_pred = np.copy(pred)
830                 tmp_pred[:, 0] *= -1
831                 yield (tmp_pred / norm).sum(axis=1)
832             else:
833                 yield pred / norm
834 
835     @staticmethod
836     def _compute_proba_from_decision(decision, n_classes):
837         """Compute probabilities from the decision function.
838 
839         This is based eq. (4) of [1] where:
840             p(y=c|X) = exp((1 / K-1) f_c(X)) / sum_k(exp((1 / K-1) f_k(X)))
841                      = softmax((1 / K-1) * f(X))
842 
843         References
844         ----------
845         .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, "Multi-class AdaBoost",
846                2009.
847         """
848         if n_classes == 2:
849             decision = np.vstack([-decision, decision]).T / 2
850         else:
851             decision /= n_classes - 1
852         return softmax(decision, copy=False)
853 
854     def predict_proba(self, X):
855         """Predict class probabilities for X.
856 
857         The predicted class probabilities of an input sample is computed as
858         the weighted mean predicted class probabilities of the classifiers
859         in the ensemble.
860 
861         Parameters
862         ----------
863         X : {array-like, sparse matrix} of shape (n_samples, n_features)
864             The training input samples. Sparse matrix can be CSC, CSR, COO,
865             DOK, or LIL. COO, DOK, and LIL are converted to CSR.
866 
867         Returns
868         -------
869         p : ndarray of shape (n_samples, n_classes)
870             The class probabilities of the input samples. The order of
871             outputs is the same of that of the :term:`classes_` attribute.
872         """
873         check_is_fitted(self)
874         n_classes = self.n_classes_
875 
876         if n_classes == 1:
877             return np.ones((_num_samples(X), 1))
878 
879         decision = self.decision_function(X)
880         return self._compute_proba_from_decision(decision, n_classes)
881 
882     def staged_predict_proba(self, X):
883         """Predict class probabilities for X.
884 
885         The predicted class probabilities of an input sample is computed as
886         the weighted mean predicted class probabilities of the classifiers
887         in the ensemble.
888 
889         This generator method yields the ensemble predicted class probabilities
890         after each iteration of boosting and therefore allows monitoring, such
891         as to determine the predicted class probabilities on a test set after
892         each boost.
893 
894         Parameters
895         ----------
896         X : {array-like, sparse matrix} of shape (n_samples, n_features)
897             The training input samples. Sparse matrix can be CSC, CSR, COO,
898             DOK, or LIL. COO, DOK, and LIL are converted to CSR.
899 
900         Yields
901         ------
902         p : generator of ndarray of shape (n_samples,)
903             The class probabilities of the input samples. The order of
904             outputs is the same of that of the :term:`classes_` attribute.
905         """
906 
907         n_classes = self.n_classes_
908 
909         for decision in self.staged_decision_function(X):
910             yield self._compute_proba_from_decision(decision, n_classes)
911 
912     def predict_log_proba(self, X):
913         """Predict class log-probabilities for X.
914 
915         The predicted class log-probabilities of an input sample is computed as
916         the weighted mean predicted class log-probabilities of the classifiers
917         in the ensemble.
918 
919         Parameters
920         ----------
921         X : {array-like, sparse matrix} of shape (n_samples, n_features)
922             The training input samples. Sparse matrix can be CSC, CSR, COO,
923             DOK, or LIL. COO, DOK, and LIL are converted to CSR.
924 
925         Returns
926         -------
927         p : ndarray of shape (n_samples, n_classes)
928             The class probabilities of the input samples. The order of
929             outputs is the same of that of the :term:`classes_` attribute.
930         """
931         return np.log(self.predict_proba(X))

</code><intended_behavior>The constructor of `AdaBoostClassifier` might need adjustments to ensure that when `base_estimator` is set to `None`, it is either treated as a valid setting, or an appropriate default estimator is set in place if `None` is not a desired final value. The key is to ensure that the transition from `base_estimator` to `estimator` does not break existing codebases that rely on the default behavior of `base_estimator=None`. This could involve adding logic within the constructor or a related method that checks for the presence of `base_estimator` and handles its deprecation and value of `None` appropriately, ensuring users who have not yet migrated to the new `estimator` parameter do not experience errors.</intended_behavior>]
2025-10-17 18:10:41.030 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-17 18:10:41.030 | INFO     | Search-only: skipping patch selection
2025-10-17 18:10:41.101 | INFO     | Task scikit-learn__scikit-learn-26242 completed successfully.
2025-10-17 18:10:41.102 | INFO     | No patch generated. You can try running ACR again.
