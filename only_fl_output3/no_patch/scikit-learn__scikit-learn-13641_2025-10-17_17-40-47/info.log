2025-10-17 17:40:47.455 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-17 17:40:50.091 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=160, output_tokens=12, cost=0.001960
2025-10-17 17:41:10.844 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=340, output_tokens=754, cost=0.026020
2025-10-17 17:41:11.854 | INFO     | registered non reproducing test 0
2025-10-17 17:41:27.379 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1925, output_tokens=594, cost=0.037070
2025-10-17 17:41:28.358 | INFO     | registered non reproducing test 1
2025-10-17 17:41:43.589 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1767, output_tokens=604, cost=0.035790
2025-10-17 17:41:44.567 | INFO     | registered non reproducing test 2
2025-10-17 17:41:44.567 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-17 17:41:44.570 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 17:41:51.057 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=469, output_tokens=223, cost=0.011380
2025-10-17 17:41:51.063 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 17:41:53.326 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=577, output_tokens=66, cost=0.007750
2025-10-17 17:41:53.328 | DEBUG    | Extracted a valid json.
2025-10-17 17:41:53.328 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"CountVectorizer\")",
        "search_method_in_class(\"fit\", \"CountVectorizer\")",
        "search_method_in_class(\"__init__\", \"CountVectorizer\")",
        "search_code(\"input='filename'\")"
    ],
    "bug_locations": []
}
2025-10-17 17:41:53.383 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-17 17:41:53.383 | DEBUG    | <Agent search> Analyzing search results.
2025-10-17 17:42:16.891 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1666, output_tokens=803, cost=0.040750
2025-10-17 17:42:16.905 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 17:42:29.886 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2685, output_tokens=370, cost=0.037950
2025-10-17 17:42:29.894 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 17:42:41.295 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=724, output_tokens=372, cost=0.018400
2025-10-17 17:42:41.296 | DEBUG    | Extracted a valid json.
2025-10-17 17:42:41.296 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "sklearn/feature_extraction/text.py",
            "class": "CountVectorizer",
            "method": "__init__",
            "intended_behavior": "The constructor correctly initializes the `CountVectorizer` object, handling the `input` parameter. However, to resolve the issue, within the scope of `__init__` or possibly in other subsequent methods (`fit`, `fit_transform`, etc.), logic needs to ensure that when `input='filename'` is specified, the files listed in the input iterable are correctly read and their contents passed to the custom analyzer if provided. This logic does not currently seem to handle the specific case of `input='filename'` especially in conjunction with a custom analyzer, and thus it should be updated to do so while preserving existing functionality for other `input` values."
        },
        {
            "file": "sklearn/feature_extraction/text.py",
            "class": "CountVectorizer",
            "method": "fit",
            "intended_behavior": "The 'fit' method should, depending on the `input` parameter and presence of a custom `analyzer`, adapt its behavior to handle file inputs specifically. If the `input` is set to 'filename', the method should ensure files are read, and their contents are passed through the custom analyzer before continuing with the rest of the fit process. This requires integrating a step to check the `input` type and process files as necessary right at the beginning of the `fit` or `fit_transform` method before the actual vectorization process starts. The intended behavior here is to accommodate file inputs seamlessly, just as it does for raw text inputs, thereby extending the versatility of the `CountVectorizer` to users providing filenames directly."
        }
    ]
}
2025-10-17 17:42:41.306 | DEBUG    | Bug location extracted successfully: [<file>sklearn/feature_extraction/text.py</file>
<class>CountVectorizer</class>
<method>__init__</method>
<code>
829     def __init__(self, input='content', encoding='utf-8',
830                  decode_error='strict', strip_accents=None,
831                  lowercase=True, preprocessor=None, tokenizer=None,
832                  stop_words=None, token_pattern=r"(?u)\b\w\w+\b",
833                  ngram_range=(1, 1), analyzer='word',
834                  max_df=1.0, min_df=1, max_features=None,
835                  vocabulary=None, binary=False, dtype=np.int64):
836         self.input = input
837         self.encoding = encoding
838         self.decode_error = decode_error
839         self.strip_accents = strip_accents
840         self.preprocessor = preprocessor
841         self.tokenizer = tokenizer
842         self.analyzer = analyzer
843         self.lowercase = lowercase
844         self.token_pattern = token_pattern
845         self.stop_words = stop_words
846         self.max_df = max_df
847         self.min_df = min_df
848         if max_df < 0 or min_df < 0:
849             raise ValueError("negative value for max_df or min_df")
850         self.max_features = max_features
851         if max_features is not None:
852             if (not isinstance(max_features, numbers.Integral) or
853                     max_features <= 0):
854                 raise ValueError(
855                     "max_features=%r, neither a positive integer nor None"
856                     % max_features)
857         self.ngram_range = ngram_range
858         self.vocabulary = vocabulary
859         self.binary = binary
860         self.dtype = dtype

</code><intended_behavior>The constructor correctly initializes the `CountVectorizer` object, handling the `input` parameter. However, to resolve the issue, within the scope of `__init__` or possibly in other subsequent methods (`fit`, `fit_transform`, etc.), logic needs to ensure that when `input='filename'` is specified, the files listed in the input iterable are correctly read and their contents passed to the custom analyzer if provided. This logic does not currently seem to handle the specific case of `input='filename'` especially in conjunction with a custom analyzer, and thus it should be updated to do so while preserving existing functionality for other `input` values.</intended_behavior>, <file>sklearn/feature_extraction/text.py</file>
<class>CountVectorizer</class>
<method>None</method>
<code>
658 class CountVectorizer(BaseEstimator, VectorizerMixin):
659     """Convert a collection of text documents to a matrix of token counts
660 
661     This implementation produces a sparse representation of the counts using
662     scipy.sparse.csr_matrix.
663 
664     If you do not provide an a-priori dictionary and you do not use an analyzer
665     that does some kind of feature selection then the number of features will
666     be equal to the vocabulary size found by analyzing the data.
667 
668     Read more in the :ref:`User Guide <text_feature_extraction>`.
669 
670     Parameters
671     ----------
672     input : string {'filename', 'file', 'content'}
673         If 'filename', the sequence passed as an argument to fit is
674         expected to be a list of filenames that need reading to fetch
675         the raw content to analyze.
676 
677         If 'file', the sequence items must have a 'read' method (file-like
678         object) that is called to fetch the bytes in memory.
679 
680         Otherwise the input is expected to be the sequence strings or
681         bytes items are expected to be analyzed directly.
682 
683     encoding : string, 'utf-8' by default.
684         If bytes or files are given to analyze, this encoding is used to
685         decode.
686 
687     decode_error : {'strict', 'ignore', 'replace'}
688         Instruction on what to do if a byte sequence is given to analyze that
689         contains characters not of the given `encoding`. By default, it is
690         'strict', meaning that a UnicodeDecodeError will be raised. Other
691         values are 'ignore' and 'replace'.
692 
693     strip_accents : {'ascii', 'unicode', None}
694         Remove accents and perform other character normalization
695         during the preprocessing step.
696         'ascii' is a fast method that only works on characters that have
697         an direct ASCII mapping.
698         'unicode' is a slightly slower method that works on any characters.
699         None (default) does nothing.
700 
701         Both 'ascii' and 'unicode' use NFKD normalization from
702         :func:`unicodedata.normalize`.
703 
704     lowercase : boolean, True by default
705         Convert all characters to lowercase before tokenizing.
706 
707     preprocessor : callable or None (default)
708         Override the preprocessing (string transformation) stage while
709         preserving the tokenizing and n-grams generation steps.
710 
711     tokenizer : callable or None (default)
712         Override the string tokenization step while preserving the
713         preprocessing and n-grams generation steps.
714         Only applies if ``analyzer == 'word'``.
715 
716     stop_words : string {'english'}, list, or None (default)
717         If 'english', a built-in stop word list for English is used.
718         There are several known issues with 'english' and you should
719         consider an alternative (see :ref:`stop_words`).
720 
721         If a list, that list is assumed to contain stop words, all of which
722         will be removed from the resulting tokens.
723         Only applies if ``analyzer == 'word'``.
724 
725         If None, no stop words will be used. max_df can be set to a value
726         in the range [0.7, 1.0) to automatically detect and filter stop
727         words based on intra corpus document frequency of terms.
728 
729     token_pattern : string
730         Regular expression denoting what constitutes a "token", only used
731         if ``analyzer == 'word'``. The default regexp select tokens of 2
732         or more alphanumeric characters (punctuation is completely ignored
733         and always treated as a token separator).
734 
735     ngram_range : tuple (min_n, max_n)
736         The lower and upper boundary of the range of n-values for different
737         n-grams to be extracted. All values of n such that min_n <= n <= max_n
738         will be used.
739 
740     analyzer : string, {'word', 'char', 'char_wb'} or callable
741         Whether the feature should be made of word or character n-grams.
742         Option 'char_wb' creates character n-grams only from text inside
743         word boundaries; n-grams at the edges of words are padded with space.
744 
745         If a callable is passed it is used to extract the sequence of features
746         out of the raw, unprocessed input.
747 
748     max_df : float in range [0.0, 1.0] or int, default=1.0
749         When building the vocabulary ignore terms that have a document
750         frequency strictly higher than the given threshold (corpus-specific
751         stop words).
752         If float, the parameter represents a proportion of documents, integer
753         absolute counts.
754         This parameter is ignored if vocabulary is not None.
755 
756     min_df : float in range [0.0, 1.0] or int, default=1
757         When building the vocabulary ignore terms that have a document
758         frequency strictly lower than the given threshold. This value is also
759         called cut-off in the literature.
760         If float, the parameter represents a proportion of documents, integer
761         absolute counts.
762         This parameter is ignored if vocabulary is not None.
763 
764     max_features : int or None, default=None
765         If not None, build a vocabulary that only consider the top
766         max_features ordered by term frequency across the corpus.
767 
768         This parameter is ignored if vocabulary is not None.
769 
770     vocabulary : Mapping or iterable, optional
771         Either a Mapping (e.g., a dict) where keys are terms and values are
772         indices in the feature matrix, or an iterable over terms. If not
773         given, a vocabulary is determined from the input documents. Indices
774         in the mapping should not be repeated and should not have any gap
775         between 0 and the largest index.
776 
777     binary : boolean, default=False
778         If True, all non zero counts are set to 1. This is useful for discrete
779         probabilistic models that model binary events rather than integer
780         counts.
781 
782     dtype : type, optional
783         Type of the matrix returned by fit_transform() or transform().
784 
785     Attributes
786     ----------
787     vocabulary_ : dict
788         A mapping of terms to feature indices.
789 
790     stop_words_ : set
791         Terms that were ignored because they either:
792 
793           - occurred in too many documents (`max_df`)
794           - occurred in too few documents (`min_df`)
795           - were cut off by feature selection (`max_features`).
796 
797         This is only available if no vocabulary was given.
798 
799     Examples
800     --------
801     >>> from sklearn.feature_extraction.text import CountVectorizer
802     >>> corpus = [
803     ...     'This is the first document.',
804     ...     'This document is the second document.',
805     ...     'And this is the third one.',
806     ...     'Is this the first document?',
807     ... ]
808     >>> vectorizer = CountVectorizer()
809     >>> X = vectorizer.fit_transform(corpus)
810     >>> print(vectorizer.get_feature_names())
811     ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
812     >>> print(X.toarray())  # doctest: +NORMALIZE_WHITESPACE
813     [[0 1 1 1 0 0 1 0 1]
814      [0 2 0 1 0 1 1 0 1]
815      [1 0 0 1 1 0 1 1 1]
816      [0 1 1 1 0 0 1 0 1]]
817 
818     See also
819     --------
820     HashingVectorizer, TfidfVectorizer
821 
822     Notes
823     -----
824     The ``stop_words_`` attribute can get large and increase the model size
825     when pickling. This attribute is provided only for introspection and can
826     be safely removed using delattr or set to None before pickling.
827     """
828 
829     def __init__(self, input='content', encoding='utf-8',
830                  decode_error='strict', strip_accents=None,
831                  lowercase=True, preprocessor=None, tokenizer=None,
832                  stop_words=None, token_pattern=r"(?u)\b\w\w+\b",
833                  ngram_range=(1, 1), analyzer='word',
834                  max_df=1.0, min_df=1, max_features=None,
835                  vocabulary=None, binary=False, dtype=np.int64):
836         self.input = input
837         self.encoding = encoding
838         self.decode_error = decode_error
839         self.strip_accents = strip_accents
840         self.preprocessor = preprocessor
841         self.tokenizer = tokenizer
842         self.analyzer = analyzer
843         self.lowercase = lowercase
844         self.token_pattern = token_pattern
845         self.stop_words = stop_words
846         self.max_df = max_df
847         self.min_df = min_df
848         if max_df < 0 or min_df < 0:
849             raise ValueError("negative value for max_df or min_df")
850         self.max_features = max_features
851         if max_features is not None:
852             if (not isinstance(max_features, numbers.Integral) or
853                     max_features <= 0):
854                 raise ValueError(
855                     "max_features=%r, neither a positive integer nor None"
856                     % max_features)
857         self.ngram_range = ngram_range
858         self.vocabulary = vocabulary
859         self.binary = binary
860         self.dtype = dtype
861 
862     def _sort_features(self, X, vocabulary):
863         """Sort features by name
864 
865         Returns a reordered matrix and modifies the vocabulary in place
866         """
867         sorted_features = sorted(vocabulary.items())
868         map_index = np.empty(len(sorted_features), dtype=X.indices.dtype)
869         for new_val, (term, old_val) in enumerate(sorted_features):
870             vocabulary[term] = new_val
871             map_index[old_val] = new_val
872 
873         X.indices = map_index.take(X.indices, mode='clip')
874         return X
875 
876     def _limit_features(self, X, vocabulary, high=None, low=None,
877                         limit=None):
878         """Remove too rare or too common features.
879 
880         Prune features that are non zero in more samples than high or less
881         documents than low, modifying the vocabulary, and restricting it to
882         at most the limit most frequent.
883 
884         This does not prune samples with zero features.
885         """
886         if high is None and low is None and limit is None:
887             return X, set()
888 
889         # Calculate a mask based on document frequencies
890         dfs = _document_frequency(X)
891         tfs = np.asarray(X.sum(axis=0)).ravel()
892         mask = np.ones(len(dfs), dtype=bool)
893         if high is not None:
894             mask &= dfs <= high
895         if low is not None:
896             mask &= dfs >= low
897         if limit is not None and mask.sum() > limit:
898             mask_inds = (-tfs[mask]).argsort()[:limit]
899             new_mask = np.zeros(len(dfs), dtype=bool)
900             new_mask[np.where(mask)[0][mask_inds]] = True
901             mask = new_mask
902 
903         new_indices = np.cumsum(mask) - 1  # maps old indices to new
904         removed_terms = set()
905         for term, old_index in list(vocabulary.items()):
906             if mask[old_index]:
907                 vocabulary[term] = new_indices[old_index]
908             else:
909                 del vocabulary[term]
910                 removed_terms.add(term)
911         kept_indices = np.where(mask)[0]
912         if len(kept_indices) == 0:
913             raise ValueError("After pruning, no terms remain. Try a lower"
914                              " min_df or a higher max_df.")
915         return X[:, kept_indices], removed_terms
916 
917     def _count_vocab(self, raw_documents, fixed_vocab):
918         """Create sparse feature matrix, and vocabulary where fixed_vocab=False
919         """
920         if fixed_vocab:
921             vocabulary = self.vocabulary_
922         else:
923             # Add a new value when a new vocabulary item is seen
924             vocabulary = defaultdict()
925             vocabulary.default_factory = vocabulary.__len__
926 
927         analyze = self.build_analyzer()
928         j_indices = []
929         indptr = []
930 
931         values = _make_int_array()
932         indptr.append(0)
933         for doc in raw_documents:
934             feature_counter = {}
935             for feature in analyze(doc):
936                 try:
937                     feature_idx = vocabulary[feature]
938                     if feature_idx not in feature_counter:
939                         feature_counter[feature_idx] = 1
940                     else:
941                         feature_counter[feature_idx] += 1
942                 except KeyError:
943                     # Ignore out-of-vocabulary items for fixed_vocab=True
944                     continue
945 
946             j_indices.extend(feature_counter.keys())
947             values.extend(feature_counter.values())
948             indptr.append(len(j_indices))
949 
950         if not fixed_vocab:
951             # disable defaultdict behaviour
952             vocabulary = dict(vocabulary)
953             if not vocabulary:
954                 raise ValueError("empty vocabulary; perhaps the documents only"
955                                  " contain stop words")
956 
957         if indptr[-1] > 2147483648:  # = 2**31 - 1
958             if _IS_32BIT:
959                 raise ValueError(('sparse CSR array has {} non-zero '
960                                   'elements and requires 64 bit indexing, '
961                                   'which is unsupported with 32 bit Python.')
962                                  .format(indptr[-1]))
963             indices_dtype = np.int64
964 
965         else:
966             indices_dtype = np.int32
967         j_indices = np.asarray(j_indices, dtype=indices_dtype)
968         indptr = np.asarray(indptr, dtype=indices_dtype)
969         values = np.frombuffer(values, dtype=np.intc)
970 
971         X = sp.csr_matrix((values, j_indices, indptr),
972                           shape=(len(indptr) - 1, len(vocabulary)),
973                           dtype=self.dtype)
974         X.sort_indices()
975         return vocabulary, X
976 
977     def fit(self, raw_documents, y=None):
978         """Learn a vocabulary dictionary of all tokens in the raw documents.
979 
980         Parameters
981         ----------
982         raw_documents : iterable
983             An iterable which yields either str, unicode or file objects.
984 
985         Returns
986         -------
987         self
988         """
989         self.fit_transform(raw_documents)
990         return self
991 
992     def fit_transform(self, raw_documents, y=None):
993         """Learn the vocabulary dictionary and return term-document matrix.
994 
995         This is equivalent to fit followed by transform, but more efficiently
996         implemented.
997 
998         Parameters
999         ----------
1000         raw_documents : iterable
1001             An iterable which yields either str, unicode or file objects.
1002 
1003         Returns
1004         -------
1005         X : array, [n_samples, n_features]
1006             Document-term matrix.
1007         """
1008         # We intentionally don't call the transform method to make
1009         # fit_transform overridable without unwanted side effects in
1010         # TfidfVectorizer.
1011         if isinstance(raw_documents, str):
1012             raise ValueError(
1013                 "Iterable over raw text documents expected, "
1014                 "string object received.")
1015 
1016         self._validate_params()
1017         self._validate_vocabulary()
1018         max_df = self.max_df
1019         min_df = self.min_df
1020         max_features = self.max_features
1021 
1022         vocabulary, X = self._count_vocab(raw_documents,
1023                                           self.fixed_vocabulary_)
1024 
1025         if self.binary:
1026             X.data.fill(1)
1027 
1028         if not self.fixed_vocabulary_:
1029             X = self._sort_features(X, vocabulary)
1030 
1031             n_doc = X.shape[0]
1032             max_doc_count = (max_df
1033                              if isinstance(max_df, numbers.Integral)
1034                              else max_df * n_doc)
1035             min_doc_count = (min_df
1036                              if isinstance(min_df, numbers.Integral)
1037                              else min_df * n_doc)
1038             if max_doc_count < min_doc_count:
1039                 raise ValueError(
1040                     "max_df corresponds to < documents than min_df")
1041             X, self.stop_words_ = self._limit_features(X, vocabulary,
1042                                                        max_doc_count,
1043                                                        min_doc_count,
1044                                                        max_features)
1045 
1046             self.vocabulary_ = vocabulary
1047 
1048         return X
1049 
1050     def transform(self, raw_documents):
1051         """Transform documents to document-term matrix.
1052 
1053         Extract token counts out of raw text documents using the vocabulary
1054         fitted with fit or the one provided to the constructor.
1055 
1056         Parameters
1057         ----------
1058         raw_documents : iterable
1059             An iterable which yields either str, unicode or file objects.
1060 
1061         Returns
1062         -------
1063         X : sparse matrix, [n_samples, n_features]
1064             Document-term matrix.
1065         """
1066         if isinstance(raw_documents, str):
1067             raise ValueError(
1068                 "Iterable over raw text documents expected, "
1069                 "string object received.")
1070 
1071         if not hasattr(self, 'vocabulary_'):
1072             self._validate_vocabulary()
1073 
1074         self._check_vocabulary()
1075 
1076         # use the same matrix-building strategy as fit_transform
1077         _, X = self._count_vocab(raw_documents, fixed_vocab=True)
1078         if self.binary:
1079             X.data.fill(1)
1080         return X
1081 
1082     def inverse_transform(self, X):
1083         """Return terms per document with nonzero entries in X.
1084 
1085         Parameters
1086         ----------
1087         X : {array, sparse matrix}, shape = [n_samples, n_features]
1088 
1089         Returns
1090         -------
1091         X_inv : list of arrays, len = n_samples
1092             List of arrays of terms.
1093         """
1094         self._check_vocabulary()
1095 
1096         if sp.issparse(X):
1097             # We need CSR format for fast row manipulations.
1098             X = X.tocsr()
1099         else:
1100             # We need to convert X to a matrix, so that the indexing
1101             # returns 2D objects
1102             X = np.asmatrix(X)
1103         n_samples = X.shape[0]
1104 
1105         terms = np.array(list(self.vocabulary_.keys()))
1106         indices = np.array(list(self.vocabulary_.values()))
1107         inverse_vocabulary = terms[np.argsort(indices)]
1108 
1109         return [inverse_vocabulary[X[i, :].nonzero()[1]].ravel()
1110                 for i in range(n_samples)]
1111 
1112     def get_feature_names(self):
1113         """Array mapping from feature integer indices to feature name"""
1114         if not hasattr(self, 'vocabulary_'):
1115             self._validate_vocabulary()
1116 
1117         self._check_vocabulary()
1118 
1119         return [t for t, i in sorted(self.vocabulary_.items(),
1120                                      key=itemgetter(1))]
1121 
1122     def _more_tags(self):
1123         return {'X_types': ['string']}

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/feature_extraction/text.py</file>
<class>CountVectorizer</class>
<method>fit</method>
<code>
977     def fit(self, raw_documents, y=None):
978         """Learn a vocabulary dictionary of all tokens in the raw documents.
979 
980         Parameters
981         ----------
982         raw_documents : iterable
983             An iterable which yields either str, unicode or file objects.
984 
985         Returns
986         -------
987         self
988         """
989         self.fit_transform(raw_documents)
990         return self

</code><intended_behavior>The 'fit' method should, depending on the `input` parameter and presence of a custom `analyzer`, adapt its behavior to handle file inputs specifically. If the `input` is set to 'filename', the method should ensure files are read, and their contents are passed through the custom analyzer before continuing with the rest of the fit process. This requires integrating a step to check the `input` type and process files as necessary right at the beginning of the `fit` or `fit_transform` method before the actual vectorization process starts. The intended behavior here is to accommodate file inputs seamlessly, just as it does for raw text inputs, thereby extending the versatility of the `CountVectorizer` to users providing filenames directly.</intended_behavior>, <file>sklearn/feature_extraction/text.py</file>
<class>CountVectorizer</class>
<method>None</method>
<code>
658 class CountVectorizer(BaseEstimator, VectorizerMixin):
659     """Convert a collection of text documents to a matrix of token counts
660 
661     This implementation produces a sparse representation of the counts using
662     scipy.sparse.csr_matrix.
663 
664     If you do not provide an a-priori dictionary and you do not use an analyzer
665     that does some kind of feature selection then the number of features will
666     be equal to the vocabulary size found by analyzing the data.
667 
668     Read more in the :ref:`User Guide <text_feature_extraction>`.
669 
670     Parameters
671     ----------
672     input : string {'filename', 'file', 'content'}
673         If 'filename', the sequence passed as an argument to fit is
674         expected to be a list of filenames that need reading to fetch
675         the raw content to analyze.
676 
677         If 'file', the sequence items must have a 'read' method (file-like
678         object) that is called to fetch the bytes in memory.
679 
680         Otherwise the input is expected to be the sequence strings or
681         bytes items are expected to be analyzed directly.
682 
683     encoding : string, 'utf-8' by default.
684         If bytes or files are given to analyze, this encoding is used to
685         decode.
686 
687     decode_error : {'strict', 'ignore', 'replace'}
688         Instruction on what to do if a byte sequence is given to analyze that
689         contains characters not of the given `encoding`. By default, it is
690         'strict', meaning that a UnicodeDecodeError will be raised. Other
691         values are 'ignore' and 'replace'.
692 
693     strip_accents : {'ascii', 'unicode', None}
694         Remove accents and perform other character normalization
695         during the preprocessing step.
696         'ascii' is a fast method that only works on characters that have
697         an direct ASCII mapping.
698         'unicode' is a slightly slower method that works on any characters.
699         None (default) does nothing.
700 
701         Both 'ascii' and 'unicode' use NFKD normalization from
702         :func:`unicodedata.normalize`.
703 
704     lowercase : boolean, True by default
705         Convert all characters to lowercase before tokenizing.
706 
707     preprocessor : callable or None (default)
708         Override the preprocessing (string transformation) stage while
709         preserving the tokenizing and n-grams generation steps.
710 
711     tokenizer : callable or None (default)
712         Override the string tokenization step while preserving the
713         preprocessing and n-grams generation steps.
714         Only applies if ``analyzer == 'word'``.
715 
716     stop_words : string {'english'}, list, or None (default)
717         If 'english', a built-in stop word list for English is used.
718         There are several known issues with 'english' and you should
719         consider an alternative (see :ref:`stop_words`).
720 
721         If a list, that list is assumed to contain stop words, all of which
722         will be removed from the resulting tokens.
723         Only applies if ``analyzer == 'word'``.
724 
725         If None, no stop words will be used. max_df can be set to a value
726         in the range [0.7, 1.0) to automatically detect and filter stop
727         words based on intra corpus document frequency of terms.
728 
729     token_pattern : string
730         Regular expression denoting what constitutes a "token", only used
731         if ``analyzer == 'word'``. The default regexp select tokens of 2
732         or more alphanumeric characters (punctuation is completely ignored
733         and always treated as a token separator).
734 
735     ngram_range : tuple (min_n, max_n)
736         The lower and upper boundary of the range of n-values for different
737         n-grams to be extracted. All values of n such that min_n <= n <= max_n
738         will be used.
739 
740     analyzer : string, {'word', 'char', 'char_wb'} or callable
741         Whether the feature should be made of word or character n-grams.
742         Option 'char_wb' creates character n-grams only from text inside
743         word boundaries; n-grams at the edges of words are padded with space.
744 
745         If a callable is passed it is used to extract the sequence of features
746         out of the raw, unprocessed input.
747 
748     max_df : float in range [0.0, 1.0] or int, default=1.0
749         When building the vocabulary ignore terms that have a document
750         frequency strictly higher than the given threshold (corpus-specific
751         stop words).
752         If float, the parameter represents a proportion of documents, integer
753         absolute counts.
754         This parameter is ignored if vocabulary is not None.
755 
756     min_df : float in range [0.0, 1.0] or int, default=1
757         When building the vocabulary ignore terms that have a document
758         frequency strictly lower than the given threshold. This value is also
759         called cut-off in the literature.
760         If float, the parameter represents a proportion of documents, integer
761         absolute counts.
762         This parameter is ignored if vocabulary is not None.
763 
764     max_features : int or None, default=None
765         If not None, build a vocabulary that only consider the top
766         max_features ordered by term frequency across the corpus.
767 
768         This parameter is ignored if vocabulary is not None.
769 
770     vocabulary : Mapping or iterable, optional
771         Either a Mapping (e.g., a dict) where keys are terms and values are
772         indices in the feature matrix, or an iterable over terms. If not
773         given, a vocabulary is determined from the input documents. Indices
774         in the mapping should not be repeated and should not have any gap
775         between 0 and the largest index.
776 
777     binary : boolean, default=False
778         If True, all non zero counts are set to 1. This is useful for discrete
779         probabilistic models that model binary events rather than integer
780         counts.
781 
782     dtype : type, optional
783         Type of the matrix returned by fit_transform() or transform().
784 
785     Attributes
786     ----------
787     vocabulary_ : dict
788         A mapping of terms to feature indices.
789 
790     stop_words_ : set
791         Terms that were ignored because they either:
792 
793           - occurred in too many documents (`max_df`)
794           - occurred in too few documents (`min_df`)
795           - were cut off by feature selection (`max_features`).
796 
797         This is only available if no vocabulary was given.
798 
799     Examples
800     --------
801     >>> from sklearn.feature_extraction.text import CountVectorizer
802     >>> corpus = [
803     ...     'This is the first document.',
804     ...     'This document is the second document.',
805     ...     'And this is the third one.',
806     ...     'Is this the first document?',
807     ... ]
808     >>> vectorizer = CountVectorizer()
809     >>> X = vectorizer.fit_transform(corpus)
810     >>> print(vectorizer.get_feature_names())
811     ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
812     >>> print(X.toarray())  # doctest: +NORMALIZE_WHITESPACE
813     [[0 1 1 1 0 0 1 0 1]
814      [0 2 0 1 0 1 1 0 1]
815      [1 0 0 1 1 0 1 1 1]
816      [0 1 1 1 0 0 1 0 1]]
817 
818     See also
819     --------
820     HashingVectorizer, TfidfVectorizer
821 
822     Notes
823     -----
824     The ``stop_words_`` attribute can get large and increase the model size
825     when pickling. This attribute is provided only for introspection and can
826     be safely removed using delattr or set to None before pickling.
827     """
828 
829     def __init__(self, input='content', encoding='utf-8',
830                  decode_error='strict', strip_accents=None,
831                  lowercase=True, preprocessor=None, tokenizer=None,
832                  stop_words=None, token_pattern=r"(?u)\b\w\w+\b",
833                  ngram_range=(1, 1), analyzer='word',
834                  max_df=1.0, min_df=1, max_features=None,
835                  vocabulary=None, binary=False, dtype=np.int64):
836         self.input = input
837         self.encoding = encoding
838         self.decode_error = decode_error
839         self.strip_accents = strip_accents
840         self.preprocessor = preprocessor
841         self.tokenizer = tokenizer
842         self.analyzer = analyzer
843         self.lowercase = lowercase
844         self.token_pattern = token_pattern
845         self.stop_words = stop_words
846         self.max_df = max_df
847         self.min_df = min_df
848         if max_df < 0 or min_df < 0:
849             raise ValueError("negative value for max_df or min_df")
850         self.max_features = max_features
851         if max_features is not None:
852             if (not isinstance(max_features, numbers.Integral) or
853                     max_features <= 0):
854                 raise ValueError(
855                     "max_features=%r, neither a positive integer nor None"
856                     % max_features)
857         self.ngram_range = ngram_range
858         self.vocabulary = vocabulary
859         self.binary = binary
860         self.dtype = dtype
861 
862     def _sort_features(self, X, vocabulary):
863         """Sort features by name
864 
865         Returns a reordered matrix and modifies the vocabulary in place
866         """
867         sorted_features = sorted(vocabulary.items())
868         map_index = np.empty(len(sorted_features), dtype=X.indices.dtype)
869         for new_val, (term, old_val) in enumerate(sorted_features):
870             vocabulary[term] = new_val
871             map_index[old_val] = new_val
872 
873         X.indices = map_index.take(X.indices, mode='clip')
874         return X
875 
876     def _limit_features(self, X, vocabulary, high=None, low=None,
877                         limit=None):
878         """Remove too rare or too common features.
879 
880         Prune features that are non zero in more samples than high or less
881         documents than low, modifying the vocabulary, and restricting it to
882         at most the limit most frequent.
883 
884         This does not prune samples with zero features.
885         """
886         if high is None and low is None and limit is None:
887             return X, set()
888 
889         # Calculate a mask based on document frequencies
890         dfs = _document_frequency(X)
891         tfs = np.asarray(X.sum(axis=0)).ravel()
892         mask = np.ones(len(dfs), dtype=bool)
893         if high is not None:
894             mask &= dfs <= high
895         if low is not None:
896             mask &= dfs >= low
897         if limit is not None and mask.sum() > limit:
898             mask_inds = (-tfs[mask]).argsort()[:limit]
899             new_mask = np.zeros(len(dfs), dtype=bool)
900             new_mask[np.where(mask)[0][mask_inds]] = True
901             mask = new_mask
902 
903         new_indices = np.cumsum(mask) - 1  # maps old indices to new
904         removed_terms = set()
905         for term, old_index in list(vocabulary.items()):
906             if mask[old_index]:
907                 vocabulary[term] = new_indices[old_index]
908             else:
909                 del vocabulary[term]
910                 removed_terms.add(term)
911         kept_indices = np.where(mask)[0]
912         if len(kept_indices) == 0:
913             raise ValueError("After pruning, no terms remain. Try a lower"
914                              " min_df or a higher max_df.")
915         return X[:, kept_indices], removed_terms
916 
917     def _count_vocab(self, raw_documents, fixed_vocab):
918         """Create sparse feature matrix, and vocabulary where fixed_vocab=False
919         """
920         if fixed_vocab:
921             vocabulary = self.vocabulary_
922         else:
923             # Add a new value when a new vocabulary item is seen
924             vocabulary = defaultdict()
925             vocabulary.default_factory = vocabulary.__len__
926 
927         analyze = self.build_analyzer()
928         j_indices = []
929         indptr = []
930 
931         values = _make_int_array()
932         indptr.append(0)
933         for doc in raw_documents:
934             feature_counter = {}
935             for feature in analyze(doc):
936                 try:
937                     feature_idx = vocabulary[feature]
938                     if feature_idx not in feature_counter:
939                         feature_counter[feature_idx] = 1
940                     else:
941                         feature_counter[feature_idx] += 1
942                 except KeyError:
943                     # Ignore out-of-vocabulary items for fixed_vocab=True
944                     continue
945 
946             j_indices.extend(feature_counter.keys())
947             values.extend(feature_counter.values())
948             indptr.append(len(j_indices))
949 
950         if not fixed_vocab:
951             # disable defaultdict behaviour
952             vocabulary = dict(vocabulary)
953             if not vocabulary:
954                 raise ValueError("empty vocabulary; perhaps the documents only"
955                                  " contain stop words")
956 
957         if indptr[-1] > 2147483648:  # = 2**31 - 1
958             if _IS_32BIT:
959                 raise ValueError(('sparse CSR array has {} non-zero '
960                                   'elements and requires 64 bit indexing, '
961                                   'which is unsupported with 32 bit Python.')
962                                  .format(indptr[-1]))
963             indices_dtype = np.int64
964 
965         else:
966             indices_dtype = np.int32
967         j_indices = np.asarray(j_indices, dtype=indices_dtype)
968         indptr = np.asarray(indptr, dtype=indices_dtype)
969         values = np.frombuffer(values, dtype=np.intc)
970 
971         X = sp.csr_matrix((values, j_indices, indptr),
972                           shape=(len(indptr) - 1, len(vocabulary)),
973                           dtype=self.dtype)
974         X.sort_indices()
975         return vocabulary, X
976 
977     def fit(self, raw_documents, y=None):
978         """Learn a vocabulary dictionary of all tokens in the raw documents.
979 
980         Parameters
981         ----------
982         raw_documents : iterable
983             An iterable which yields either str, unicode or file objects.
984 
985         Returns
986         -------
987         self
988         """
989         self.fit_transform(raw_documents)
990         return self
991 
992     def fit_transform(self, raw_documents, y=None):
993         """Learn the vocabulary dictionary and return term-document matrix.
994 
995         This is equivalent to fit followed by transform, but more efficiently
996         implemented.
997 
998         Parameters
999         ----------
1000         raw_documents : iterable
1001             An iterable which yields either str, unicode or file objects.
1002 
1003         Returns
1004         -------
1005         X : array, [n_samples, n_features]
1006             Document-term matrix.
1007         """
1008         # We intentionally don't call the transform method to make
1009         # fit_transform overridable without unwanted side effects in
1010         # TfidfVectorizer.
1011         if isinstance(raw_documents, str):
1012             raise ValueError(
1013                 "Iterable over raw text documents expected, "
1014                 "string object received.")
1015 
1016         self._validate_params()
1017         self._validate_vocabulary()
1018         max_df = self.max_df
1019         min_df = self.min_df
1020         max_features = self.max_features
1021 
1022         vocabulary, X = self._count_vocab(raw_documents,
1023                                           self.fixed_vocabulary_)
1024 
1025         if self.binary:
1026             X.data.fill(1)
1027 
1028         if not self.fixed_vocabulary_:
1029             X = self._sort_features(X, vocabulary)
1030 
1031             n_doc = X.shape[0]
1032             max_doc_count = (max_df
1033                              if isinstance(max_df, numbers.Integral)
1034                              else max_df * n_doc)
1035             min_doc_count = (min_df
1036                              if isinstance(min_df, numbers.Integral)
1037                              else min_df * n_doc)
1038             if max_doc_count < min_doc_count:
1039                 raise ValueError(
1040                     "max_df corresponds to < documents than min_df")
1041             X, self.stop_words_ = self._limit_features(X, vocabulary,
1042                                                        max_doc_count,
1043                                                        min_doc_count,
1044                                                        max_features)
1045 
1046             self.vocabulary_ = vocabulary
1047 
1048         return X
1049 
1050     def transform(self, raw_documents):
1051         """Transform documents to document-term matrix.
1052 
1053         Extract token counts out of raw text documents using the vocabulary
1054         fitted with fit or the one provided to the constructor.
1055 
1056         Parameters
1057         ----------
1058         raw_documents : iterable
1059             An iterable which yields either str, unicode or file objects.
1060 
1061         Returns
1062         -------
1063         X : sparse matrix, [n_samples, n_features]
1064             Document-term matrix.
1065         """
1066         if isinstance(raw_documents, str):
1067             raise ValueError(
1068                 "Iterable over raw text documents expected, "
1069                 "string object received.")
1070 
1071         if not hasattr(self, 'vocabulary_'):
1072             self._validate_vocabulary()
1073 
1074         self._check_vocabulary()
1075 
1076         # use the same matrix-building strategy as fit_transform
1077         _, X = self._count_vocab(raw_documents, fixed_vocab=True)
1078         if self.binary:
1079             X.data.fill(1)
1080         return X
1081 
1082     def inverse_transform(self, X):
1083         """Return terms per document with nonzero entries in X.
1084 
1085         Parameters
1086         ----------
1087         X : {array, sparse matrix}, shape = [n_samples, n_features]
1088 
1089         Returns
1090         -------
1091         X_inv : list of arrays, len = n_samples
1092             List of arrays of terms.
1093         """
1094         self._check_vocabulary()
1095 
1096         if sp.issparse(X):
1097             # We need CSR format for fast row manipulations.
1098             X = X.tocsr()
1099         else:
1100             # We need to convert X to a matrix, so that the indexing
1101             # returns 2D objects
1102             X = np.asmatrix(X)
1103         n_samples = X.shape[0]
1104 
1105         terms = np.array(list(self.vocabulary_.keys()))
1106         indices = np.array(list(self.vocabulary_.values()))
1107         inverse_vocabulary = terms[np.argsort(indices)]
1108 
1109         return [inverse_vocabulary[X[i, :].nonzero()[1]].ravel()
1110                 for i in range(n_samples)]
1111 
1112     def get_feature_names(self):
1113         """Array mapping from feature integer indices to feature name"""
1114         if not hasattr(self, 'vocabulary_'):
1115             self._validate_vocabulary()
1116 
1117         self._check_vocabulary()
1118 
1119         return [t for t, i in sorted(self.vocabulary_.items(),
1120                                      key=itemgetter(1))]
1121 
1122     def _more_tags(self):
1123         return {'X_types': ['string']}

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-17 17:42:41.306 | INFO     | Search completed. Bug locations: [<file>sklearn/feature_extraction/text.py</file>
<class>CountVectorizer</class>
<method>__init__</method>
<code>
829     def __init__(self, input='content', encoding='utf-8',
830                  decode_error='strict', strip_accents=None,
831                  lowercase=True, preprocessor=None, tokenizer=None,
832                  stop_words=None, token_pattern=r"(?u)\b\w\w+\b",
833                  ngram_range=(1, 1), analyzer='word',
834                  max_df=1.0, min_df=1, max_features=None,
835                  vocabulary=None, binary=False, dtype=np.int64):
836         self.input = input
837         self.encoding = encoding
838         self.decode_error = decode_error
839         self.strip_accents = strip_accents
840         self.preprocessor = preprocessor
841         self.tokenizer = tokenizer
842         self.analyzer = analyzer
843         self.lowercase = lowercase
844         self.token_pattern = token_pattern
845         self.stop_words = stop_words
846         self.max_df = max_df
847         self.min_df = min_df
848         if max_df < 0 or min_df < 0:
849             raise ValueError("negative value for max_df or min_df")
850         self.max_features = max_features
851         if max_features is not None:
852             if (not isinstance(max_features, numbers.Integral) or
853                     max_features <= 0):
854                 raise ValueError(
855                     "max_features=%r, neither a positive integer nor None"
856                     % max_features)
857         self.ngram_range = ngram_range
858         self.vocabulary = vocabulary
859         self.binary = binary
860         self.dtype = dtype

</code><intended_behavior>The constructor correctly initializes the `CountVectorizer` object, handling the `input` parameter. However, to resolve the issue, within the scope of `__init__` or possibly in other subsequent methods (`fit`, `fit_transform`, etc.), logic needs to ensure that when `input='filename'` is specified, the files listed in the input iterable are correctly read and their contents passed to the custom analyzer if provided. This logic does not currently seem to handle the specific case of `input='filename'` especially in conjunction with a custom analyzer, and thus it should be updated to do so while preserving existing functionality for other `input` values.</intended_behavior>, <file>sklearn/feature_extraction/text.py</file>
<class>CountVectorizer</class>
<method>None</method>
<code>
658 class CountVectorizer(BaseEstimator, VectorizerMixin):
659     """Convert a collection of text documents to a matrix of token counts
660 
661     This implementation produces a sparse representation of the counts using
662     scipy.sparse.csr_matrix.
663 
664     If you do not provide an a-priori dictionary and you do not use an analyzer
665     that does some kind of feature selection then the number of features will
666     be equal to the vocabulary size found by analyzing the data.
667 
668     Read more in the :ref:`User Guide <text_feature_extraction>`.
669 
670     Parameters
671     ----------
672     input : string {'filename', 'file', 'content'}
673         If 'filename', the sequence passed as an argument to fit is
674         expected to be a list of filenames that need reading to fetch
675         the raw content to analyze.
676 
677         If 'file', the sequence items must have a 'read' method (file-like
678         object) that is called to fetch the bytes in memory.
679 
680         Otherwise the input is expected to be the sequence strings or
681         bytes items are expected to be analyzed directly.
682 
683     encoding : string, 'utf-8' by default.
684         If bytes or files are given to analyze, this encoding is used to
685         decode.
686 
687     decode_error : {'strict', 'ignore', 'replace'}
688         Instruction on what to do if a byte sequence is given to analyze that
689         contains characters not of the given `encoding`. By default, it is
690         'strict', meaning that a UnicodeDecodeError will be raised. Other
691         values are 'ignore' and 'replace'.
692 
693     strip_accents : {'ascii', 'unicode', None}
694         Remove accents and perform other character normalization
695         during the preprocessing step.
696         'ascii' is a fast method that only works on characters that have
697         an direct ASCII mapping.
698         'unicode' is a slightly slower method that works on any characters.
699         None (default) does nothing.
700 
701         Both 'ascii' and 'unicode' use NFKD normalization from
702         :func:`unicodedata.normalize`.
703 
704     lowercase : boolean, True by default
705         Convert all characters to lowercase before tokenizing.
706 
707     preprocessor : callable or None (default)
708         Override the preprocessing (string transformation) stage while
709         preserving the tokenizing and n-grams generation steps.
710 
711     tokenizer : callable or None (default)
712         Override the string tokenization step while preserving the
713         preprocessing and n-grams generation steps.
714         Only applies if ``analyzer == 'word'``.
715 
716     stop_words : string {'english'}, list, or None (default)
717         If 'english', a built-in stop word list for English is used.
718         There are several known issues with 'english' and you should
719         consider an alternative (see :ref:`stop_words`).
720 
721         If a list, that list is assumed to contain stop words, all of which
722         will be removed from the resulting tokens.
723         Only applies if ``analyzer == 'word'``.
724 
725         If None, no stop words will be used. max_df can be set to a value
726         in the range [0.7, 1.0) to automatically detect and filter stop
727         words based on intra corpus document frequency of terms.
728 
729     token_pattern : string
730         Regular expression denoting what constitutes a "token", only used
731         if ``analyzer == 'word'``. The default regexp select tokens of 2
732         or more alphanumeric characters (punctuation is completely ignored
733         and always treated as a token separator).
734 
735     ngram_range : tuple (min_n, max_n)
736         The lower and upper boundary of the range of n-values for different
737         n-grams to be extracted. All values of n such that min_n <= n <= max_n
738         will be used.
739 
740     analyzer : string, {'word', 'char', 'char_wb'} or callable
741         Whether the feature should be made of word or character n-grams.
742         Option 'char_wb' creates character n-grams only from text inside
743         word boundaries; n-grams at the edges of words are padded with space.
744 
745         If a callable is passed it is used to extract the sequence of features
746         out of the raw, unprocessed input.
747 
748     max_df : float in range [0.0, 1.0] or int, default=1.0
749         When building the vocabulary ignore terms that have a document
750         frequency strictly higher than the given threshold (corpus-specific
751         stop words).
752         If float, the parameter represents a proportion of documents, integer
753         absolute counts.
754         This parameter is ignored if vocabulary is not None.
755 
756     min_df : float in range [0.0, 1.0] or int, default=1
757         When building the vocabulary ignore terms that have a document
758         frequency strictly lower than the given threshold. This value is also
759         called cut-off in the literature.
760         If float, the parameter represents a proportion of documents, integer
761         absolute counts.
762         This parameter is ignored if vocabulary is not None.
763 
764     max_features : int or None, default=None
765         If not None, build a vocabulary that only consider the top
766         max_features ordered by term frequency across the corpus.
767 
768         This parameter is ignored if vocabulary is not None.
769 
770     vocabulary : Mapping or iterable, optional
771         Either a Mapping (e.g., a dict) where keys are terms and values are
772         indices in the feature matrix, or an iterable over terms. If not
773         given, a vocabulary is determined from the input documents. Indices
774         in the mapping should not be repeated and should not have any gap
775         between 0 and the largest index.
776 
777     binary : boolean, default=False
778         If True, all non zero counts are set to 1. This is useful for discrete
779         probabilistic models that model binary events rather than integer
780         counts.
781 
782     dtype : type, optional
783         Type of the matrix returned by fit_transform() or transform().
784 
785     Attributes
786     ----------
787     vocabulary_ : dict
788         A mapping of terms to feature indices.
789 
790     stop_words_ : set
791         Terms that were ignored because they either:
792 
793           - occurred in too many documents (`max_df`)
794           - occurred in too few documents (`min_df`)
795           - were cut off by feature selection (`max_features`).
796 
797         This is only available if no vocabulary was given.
798 
799     Examples
800     --------
801     >>> from sklearn.feature_extraction.text import CountVectorizer
802     >>> corpus = [
803     ...     'This is the first document.',
804     ...     'This document is the second document.',
805     ...     'And this is the third one.',
806     ...     'Is this the first document?',
807     ... ]
808     >>> vectorizer = CountVectorizer()
809     >>> X = vectorizer.fit_transform(corpus)
810     >>> print(vectorizer.get_feature_names())
811     ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
812     >>> print(X.toarray())  # doctest: +NORMALIZE_WHITESPACE
813     [[0 1 1 1 0 0 1 0 1]
814      [0 2 0 1 0 1 1 0 1]
815      [1 0 0 1 1 0 1 1 1]
816      [0 1 1 1 0 0 1 0 1]]
817 
818     See also
819     --------
820     HashingVectorizer, TfidfVectorizer
821 
822     Notes
823     -----
824     The ``stop_words_`` attribute can get large and increase the model size
825     when pickling. This attribute is provided only for introspection and can
826     be safely removed using delattr or set to None before pickling.
827     """
828 
829     def __init__(self, input='content', encoding='utf-8',
830                  decode_error='strict', strip_accents=None,
831                  lowercase=True, preprocessor=None, tokenizer=None,
832                  stop_words=None, token_pattern=r"(?u)\b\w\w+\b",
833                  ngram_range=(1, 1), analyzer='word',
834                  max_df=1.0, min_df=1, max_features=None,
835                  vocabulary=None, binary=False, dtype=np.int64):
836         self.input = input
837         self.encoding = encoding
838         self.decode_error = decode_error
839         self.strip_accents = strip_accents
840         self.preprocessor = preprocessor
841         self.tokenizer = tokenizer
842         self.analyzer = analyzer
843         self.lowercase = lowercase
844         self.token_pattern = token_pattern
845         self.stop_words = stop_words
846         self.max_df = max_df
847         self.min_df = min_df
848         if max_df < 0 or min_df < 0:
849             raise ValueError("negative value for max_df or min_df")
850         self.max_features = max_features
851         if max_features is not None:
852             if (not isinstance(max_features, numbers.Integral) or
853                     max_features <= 0):
854                 raise ValueError(
855                     "max_features=%r, neither a positive integer nor None"
856                     % max_features)
857         self.ngram_range = ngram_range
858         self.vocabulary = vocabulary
859         self.binary = binary
860         self.dtype = dtype
861 
862     def _sort_features(self, X, vocabulary):
863         """Sort features by name
864 
865         Returns a reordered matrix and modifies the vocabulary in place
866         """
867         sorted_features = sorted(vocabulary.items())
868         map_index = np.empty(len(sorted_features), dtype=X.indices.dtype)
869         for new_val, (term, old_val) in enumerate(sorted_features):
870             vocabulary[term] = new_val
871             map_index[old_val] = new_val
872 
873         X.indices = map_index.take(X.indices, mode='clip')
874         return X
875 
876     def _limit_features(self, X, vocabulary, high=None, low=None,
877                         limit=None):
878         """Remove too rare or too common features.
879 
880         Prune features that are non zero in more samples than high or less
881         documents than low, modifying the vocabulary, and restricting it to
882         at most the limit most frequent.
883 
884         This does not prune samples with zero features.
885         """
886         if high is None and low is None and limit is None:
887             return X, set()
888 
889         # Calculate a mask based on document frequencies
890         dfs = _document_frequency(X)
891         tfs = np.asarray(X.sum(axis=0)).ravel()
892         mask = np.ones(len(dfs), dtype=bool)
893         if high is not None:
894             mask &= dfs <= high
895         if low is not None:
896             mask &= dfs >= low
897         if limit is not None and mask.sum() > limit:
898             mask_inds = (-tfs[mask]).argsort()[:limit]
899             new_mask = np.zeros(len(dfs), dtype=bool)
900             new_mask[np.where(mask)[0][mask_inds]] = True
901             mask = new_mask
902 
903         new_indices = np.cumsum(mask) - 1  # maps old indices to new
904         removed_terms = set()
905         for term, old_index in list(vocabulary.items()):
906             if mask[old_index]:
907                 vocabulary[term] = new_indices[old_index]
908             else:
909                 del vocabulary[term]
910                 removed_terms.add(term)
911         kept_indices = np.where(mask)[0]
912         if len(kept_indices) == 0:
913             raise ValueError("After pruning, no terms remain. Try a lower"
914                              " min_df or a higher max_df.")
915         return X[:, kept_indices], removed_terms
916 
917     def _count_vocab(self, raw_documents, fixed_vocab):
918         """Create sparse feature matrix, and vocabulary where fixed_vocab=False
919         """
920         if fixed_vocab:
921             vocabulary = self.vocabulary_
922         else:
923             # Add a new value when a new vocabulary item is seen
924             vocabulary = defaultdict()
925             vocabulary.default_factory = vocabulary.__len__
926 
927         analyze = self.build_analyzer()
928         j_indices = []
929         indptr = []
930 
931         values = _make_int_array()
932         indptr.append(0)
933         for doc in raw_documents:
934             feature_counter = {}
935             for feature in analyze(doc):
936                 try:
937                     feature_idx = vocabulary[feature]
938                     if feature_idx not in feature_counter:
939                         feature_counter[feature_idx] = 1
940                     else:
941                         feature_counter[feature_idx] += 1
942                 except KeyError:
943                     # Ignore out-of-vocabulary items for fixed_vocab=True
944                     continue
945 
946             j_indices.extend(feature_counter.keys())
947             values.extend(feature_counter.values())
948             indptr.append(len(j_indices))
949 
950         if not fixed_vocab:
951             # disable defaultdict behaviour
952             vocabulary = dict(vocabulary)
953             if not vocabulary:
954                 raise ValueError("empty vocabulary; perhaps the documents only"
955                                  " contain stop words")
956 
957         if indptr[-1] > 2147483648:  # = 2**31 - 1
958             if _IS_32BIT:
959                 raise ValueError(('sparse CSR array has {} non-zero '
960                                   'elements and requires 64 bit indexing, '
961                                   'which is unsupported with 32 bit Python.')
962                                  .format(indptr[-1]))
963             indices_dtype = np.int64
964 
965         else:
966             indices_dtype = np.int32
967         j_indices = np.asarray(j_indices, dtype=indices_dtype)
968         indptr = np.asarray(indptr, dtype=indices_dtype)
969         values = np.frombuffer(values, dtype=np.intc)
970 
971         X = sp.csr_matrix((values, j_indices, indptr),
972                           shape=(len(indptr) - 1, len(vocabulary)),
973                           dtype=self.dtype)
974         X.sort_indices()
975         return vocabulary, X
976 
977     def fit(self, raw_documents, y=None):
978         """Learn a vocabulary dictionary of all tokens in the raw documents.
979 
980         Parameters
981         ----------
982         raw_documents : iterable
983             An iterable which yields either str, unicode or file objects.
984 
985         Returns
986         -------
987         self
988         """
989         self.fit_transform(raw_documents)
990         return self
991 
992     def fit_transform(self, raw_documents, y=None):
993         """Learn the vocabulary dictionary and return term-document matrix.
994 
995         This is equivalent to fit followed by transform, but more efficiently
996         implemented.
997 
998         Parameters
999         ----------
1000         raw_documents : iterable
1001             An iterable which yields either str, unicode or file objects.
1002 
1003         Returns
1004         -------
1005         X : array, [n_samples, n_features]
1006             Document-term matrix.
1007         """
1008         # We intentionally don't call the transform method to make
1009         # fit_transform overridable without unwanted side effects in
1010         # TfidfVectorizer.
1011         if isinstance(raw_documents, str):
1012             raise ValueError(
1013                 "Iterable over raw text documents expected, "
1014                 "string object received.")
1015 
1016         self._validate_params()
1017         self._validate_vocabulary()
1018         max_df = self.max_df
1019         min_df = self.min_df
1020         max_features = self.max_features
1021 
1022         vocabulary, X = self._count_vocab(raw_documents,
1023                                           self.fixed_vocabulary_)
1024 
1025         if self.binary:
1026             X.data.fill(1)
1027 
1028         if not self.fixed_vocabulary_:
1029             X = self._sort_features(X, vocabulary)
1030 
1031             n_doc = X.shape[0]
1032             max_doc_count = (max_df
1033                              if isinstance(max_df, numbers.Integral)
1034                              else max_df * n_doc)
1035             min_doc_count = (min_df
1036                              if isinstance(min_df, numbers.Integral)
1037                              else min_df * n_doc)
1038             if max_doc_count < min_doc_count:
1039                 raise ValueError(
1040                     "max_df corresponds to < documents than min_df")
1041             X, self.stop_words_ = self._limit_features(X, vocabulary,
1042                                                        max_doc_count,
1043                                                        min_doc_count,
1044                                                        max_features)
1045 
1046             self.vocabulary_ = vocabulary
1047 
1048         return X
1049 
1050     def transform(self, raw_documents):
1051         """Transform documents to document-term matrix.
1052 
1053         Extract token counts out of raw text documents using the vocabulary
1054         fitted with fit or the one provided to the constructor.
1055 
1056         Parameters
1057         ----------
1058         raw_documents : iterable
1059             An iterable which yields either str, unicode or file objects.
1060 
1061         Returns
1062         -------
1063         X : sparse matrix, [n_samples, n_features]
1064             Document-term matrix.
1065         """
1066         if isinstance(raw_documents, str):
1067             raise ValueError(
1068                 "Iterable over raw text documents expected, "
1069                 "string object received.")
1070 
1071         if not hasattr(self, 'vocabulary_'):
1072             self._validate_vocabulary()
1073 
1074         self._check_vocabulary()
1075 
1076         # use the same matrix-building strategy as fit_transform
1077         _, X = self._count_vocab(raw_documents, fixed_vocab=True)
1078         if self.binary:
1079             X.data.fill(1)
1080         return X
1081 
1082     def inverse_transform(self, X):
1083         """Return terms per document with nonzero entries in X.
1084 
1085         Parameters
1086         ----------
1087         X : {array, sparse matrix}, shape = [n_samples, n_features]
1088 
1089         Returns
1090         -------
1091         X_inv : list of arrays, len = n_samples
1092             List of arrays of terms.
1093         """
1094         self._check_vocabulary()
1095 
1096         if sp.issparse(X):
1097             # We need CSR format for fast row manipulations.
1098             X = X.tocsr()
1099         else:
1100             # We need to convert X to a matrix, so that the indexing
1101             # returns 2D objects
1102             X = np.asmatrix(X)
1103         n_samples = X.shape[0]
1104 
1105         terms = np.array(list(self.vocabulary_.keys()))
1106         indices = np.array(list(self.vocabulary_.values()))
1107         inverse_vocabulary = terms[np.argsort(indices)]
1108 
1109         return [inverse_vocabulary[X[i, :].nonzero()[1]].ravel()
1110                 for i in range(n_samples)]
1111 
1112     def get_feature_names(self):
1113         """Array mapping from feature integer indices to feature name"""
1114         if not hasattr(self, 'vocabulary_'):
1115             self._validate_vocabulary()
1116 
1117         self._check_vocabulary()
1118 
1119         return [t for t, i in sorted(self.vocabulary_.items(),
1120                                      key=itemgetter(1))]
1121 
1122     def _more_tags(self):
1123         return {'X_types': ['string']}

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/feature_extraction/text.py</file>
<class>CountVectorizer</class>
<method>fit</method>
<code>
977     def fit(self, raw_documents, y=None):
978         """Learn a vocabulary dictionary of all tokens in the raw documents.
979 
980         Parameters
981         ----------
982         raw_documents : iterable
983             An iterable which yields either str, unicode or file objects.
984 
985         Returns
986         -------
987         self
988         """
989         self.fit_transform(raw_documents)
990         return self

</code><intended_behavior>The 'fit' method should, depending on the `input` parameter and presence of a custom `analyzer`, adapt its behavior to handle file inputs specifically. If the `input` is set to 'filename', the method should ensure files are read, and their contents are passed through the custom analyzer before continuing with the rest of the fit process. This requires integrating a step to check the `input` type and process files as necessary right at the beginning of the `fit` or `fit_transform` method before the actual vectorization process starts. The intended behavior here is to accommodate file inputs seamlessly, just as it does for raw text inputs, thereby extending the versatility of the `CountVectorizer` to users providing filenames directly.</intended_behavior>, <file>sklearn/feature_extraction/text.py</file>
<class>CountVectorizer</class>
<method>None</method>
<code>
658 class CountVectorizer(BaseEstimator, VectorizerMixin):
659     """Convert a collection of text documents to a matrix of token counts
660 
661     This implementation produces a sparse representation of the counts using
662     scipy.sparse.csr_matrix.
663 
664     If you do not provide an a-priori dictionary and you do not use an analyzer
665     that does some kind of feature selection then the number of features will
666     be equal to the vocabulary size found by analyzing the data.
667 
668     Read more in the :ref:`User Guide <text_feature_extraction>`.
669 
670     Parameters
671     ----------
672     input : string {'filename', 'file', 'content'}
673         If 'filename', the sequence passed as an argument to fit is
674         expected to be a list of filenames that need reading to fetch
675         the raw content to analyze.
676 
677         If 'file', the sequence items must have a 'read' method (file-like
678         object) that is called to fetch the bytes in memory.
679 
680         Otherwise the input is expected to be the sequence strings or
681         bytes items are expected to be analyzed directly.
682 
683     encoding : string, 'utf-8' by default.
684         If bytes or files are given to analyze, this encoding is used to
685         decode.
686 
687     decode_error : {'strict', 'ignore', 'replace'}
688         Instruction on what to do if a byte sequence is given to analyze that
689         contains characters not of the given `encoding`. By default, it is
690         'strict', meaning that a UnicodeDecodeError will be raised. Other
691         values are 'ignore' and 'replace'.
692 
693     strip_accents : {'ascii', 'unicode', None}
694         Remove accents and perform other character normalization
695         during the preprocessing step.
696         'ascii' is a fast method that only works on characters that have
697         an direct ASCII mapping.
698         'unicode' is a slightly slower method that works on any characters.
699         None (default) does nothing.
700 
701         Both 'ascii' and 'unicode' use NFKD normalization from
702         :func:`unicodedata.normalize`.
703 
704     lowercase : boolean, True by default
705         Convert all characters to lowercase before tokenizing.
706 
707     preprocessor : callable or None (default)
708         Override the preprocessing (string transformation) stage while
709         preserving the tokenizing and n-grams generation steps.
710 
711     tokenizer : callable or None (default)
712         Override the string tokenization step while preserving the
713         preprocessing and n-grams generation steps.
714         Only applies if ``analyzer == 'word'``.
715 
716     stop_words : string {'english'}, list, or None (default)
717         If 'english', a built-in stop word list for English is used.
718         There are several known issues with 'english' and you should
719         consider an alternative (see :ref:`stop_words`).
720 
721         If a list, that list is assumed to contain stop words, all of which
722         will be removed from the resulting tokens.
723         Only applies if ``analyzer == 'word'``.
724 
725         If None, no stop words will be used. max_df can be set to a value
726         in the range [0.7, 1.0) to automatically detect and filter stop
727         words based on intra corpus document frequency of terms.
728 
729     token_pattern : string
730         Regular expression denoting what constitutes a "token", only used
731         if ``analyzer == 'word'``. The default regexp select tokens of 2
732         or more alphanumeric characters (punctuation is completely ignored
733         and always treated as a token separator).
734 
735     ngram_range : tuple (min_n, max_n)
736         The lower and upper boundary of the range of n-values for different
737         n-grams to be extracted. All values of n such that min_n <= n <= max_n
738         will be used.
739 
740     analyzer : string, {'word', 'char', 'char_wb'} or callable
741         Whether the feature should be made of word or character n-grams.
742         Option 'char_wb' creates character n-grams only from text inside
743         word boundaries; n-grams at the edges of words are padded with space.
744 
745         If a callable is passed it is used to extract the sequence of features
746         out of the raw, unprocessed input.
747 
748     max_df : float in range [0.0, 1.0] or int, default=1.0
749         When building the vocabulary ignore terms that have a document
750         frequency strictly higher than the given threshold (corpus-specific
751         stop words).
752         If float, the parameter represents a proportion of documents, integer
753         absolute counts.
754         This parameter is ignored if vocabulary is not None.
755 
756     min_df : float in range [0.0, 1.0] or int, default=1
757         When building the vocabulary ignore terms that have a document
758         frequency strictly lower than the given threshold. This value is also
759         called cut-off in the literature.
760         If float, the parameter represents a proportion of documents, integer
761         absolute counts.
762         This parameter is ignored if vocabulary is not None.
763 
764     max_features : int or None, default=None
765         If not None, build a vocabulary that only consider the top
766         max_features ordered by term frequency across the corpus.
767 
768         This parameter is ignored if vocabulary is not None.
769 
770     vocabulary : Mapping or iterable, optional
771         Either a Mapping (e.g., a dict) where keys are terms and values are
772         indices in the feature matrix, or an iterable over terms. If not
773         given, a vocabulary is determined from the input documents. Indices
774         in the mapping should not be repeated and should not have any gap
775         between 0 and the largest index.
776 
777     binary : boolean, default=False
778         If True, all non zero counts are set to 1. This is useful for discrete
779         probabilistic models that model binary events rather than integer
780         counts.
781 
782     dtype : type, optional
783         Type of the matrix returned by fit_transform() or transform().
784 
785     Attributes
786     ----------
787     vocabulary_ : dict
788         A mapping of terms to feature indices.
789 
790     stop_words_ : set
791         Terms that were ignored because they either:
792 
793           - occurred in too many documents (`max_df`)
794           - occurred in too few documents (`min_df`)
795           - were cut off by feature selection (`max_features`).
796 
797         This is only available if no vocabulary was given.
798 
799     Examples
800     --------
801     >>> from sklearn.feature_extraction.text import CountVectorizer
802     >>> corpus = [
803     ...     'This is the first document.',
804     ...     'This document is the second document.',
805     ...     'And this is the third one.',
806     ...     'Is this the first document?',
807     ... ]
808     >>> vectorizer = CountVectorizer()
809     >>> X = vectorizer.fit_transform(corpus)
810     >>> print(vectorizer.get_feature_names())
811     ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
812     >>> print(X.toarray())  # doctest: +NORMALIZE_WHITESPACE
813     [[0 1 1 1 0 0 1 0 1]
814      [0 2 0 1 0 1 1 0 1]
815      [1 0 0 1 1 0 1 1 1]
816      [0 1 1 1 0 0 1 0 1]]
817 
818     See also
819     --------
820     HashingVectorizer, TfidfVectorizer
821 
822     Notes
823     -----
824     The ``stop_words_`` attribute can get large and increase the model size
825     when pickling. This attribute is provided only for introspection and can
826     be safely removed using delattr or set to None before pickling.
827     """
828 
829     def __init__(self, input='content', encoding='utf-8',
830                  decode_error='strict', strip_accents=None,
831                  lowercase=True, preprocessor=None, tokenizer=None,
832                  stop_words=None, token_pattern=r"(?u)\b\w\w+\b",
833                  ngram_range=(1, 1), analyzer='word',
834                  max_df=1.0, min_df=1, max_features=None,
835                  vocabulary=None, binary=False, dtype=np.int64):
836         self.input = input
837         self.encoding = encoding
838         self.decode_error = decode_error
839         self.strip_accents = strip_accents
840         self.preprocessor = preprocessor
841         self.tokenizer = tokenizer
842         self.analyzer = analyzer
843         self.lowercase = lowercase
844         self.token_pattern = token_pattern
845         self.stop_words = stop_words
846         self.max_df = max_df
847         self.min_df = min_df
848         if max_df < 0 or min_df < 0:
849             raise ValueError("negative value for max_df or min_df")
850         self.max_features = max_features
851         if max_features is not None:
852             if (not isinstance(max_features, numbers.Integral) or
853                     max_features <= 0):
854                 raise ValueError(
855                     "max_features=%r, neither a positive integer nor None"
856                     % max_features)
857         self.ngram_range = ngram_range
858         self.vocabulary = vocabulary
859         self.binary = binary
860         self.dtype = dtype
861 
862     def _sort_features(self, X, vocabulary):
863         """Sort features by name
864 
865         Returns a reordered matrix and modifies the vocabulary in place
866         """
867         sorted_features = sorted(vocabulary.items())
868         map_index = np.empty(len(sorted_features), dtype=X.indices.dtype)
869         for new_val, (term, old_val) in enumerate(sorted_features):
870             vocabulary[term] = new_val
871             map_index[old_val] = new_val
872 
873         X.indices = map_index.take(X.indices, mode='clip')
874         return X
875 
876     def _limit_features(self, X, vocabulary, high=None, low=None,
877                         limit=None):
878         """Remove too rare or too common features.
879 
880         Prune features that are non zero in more samples than high or less
881         documents than low, modifying the vocabulary, and restricting it to
882         at most the limit most frequent.
883 
884         This does not prune samples with zero features.
885         """
886         if high is None and low is None and limit is None:
887             return X, set()
888 
889         # Calculate a mask based on document frequencies
890         dfs = _document_frequency(X)
891         tfs = np.asarray(X.sum(axis=0)).ravel()
892         mask = np.ones(len(dfs), dtype=bool)
893         if high is not None:
894             mask &= dfs <= high
895         if low is not None:
896             mask &= dfs >= low
897         if limit is not None and mask.sum() > limit:
898             mask_inds = (-tfs[mask]).argsort()[:limit]
899             new_mask = np.zeros(len(dfs), dtype=bool)
900             new_mask[np.where(mask)[0][mask_inds]] = True
901             mask = new_mask
902 
903         new_indices = np.cumsum(mask) - 1  # maps old indices to new
904         removed_terms = set()
905         for term, old_index in list(vocabulary.items()):
906             if mask[old_index]:
907                 vocabulary[term] = new_indices[old_index]
908             else:
909                 del vocabulary[term]
910                 removed_terms.add(term)
911         kept_indices = np.where(mask)[0]
912         if len(kept_indices) == 0:
913             raise ValueError("After pruning, no terms remain. Try a lower"
914                              " min_df or a higher max_df.")
915         return X[:, kept_indices], removed_terms
916 
917     def _count_vocab(self, raw_documents, fixed_vocab):
918         """Create sparse feature matrix, and vocabulary where fixed_vocab=False
919         """
920         if fixed_vocab:
921             vocabulary = self.vocabulary_
922         else:
923             # Add a new value when a new vocabulary item is seen
924             vocabulary = defaultdict()
925             vocabulary.default_factory = vocabulary.__len__
926 
927         analyze = self.build_analyzer()
928         j_indices = []
929         indptr = []
930 
931         values = _make_int_array()
932         indptr.append(0)
933         for doc in raw_documents:
934             feature_counter = {}
935             for feature in analyze(doc):
936                 try:
937                     feature_idx = vocabulary[feature]
938                     if feature_idx not in feature_counter:
939                         feature_counter[feature_idx] = 1
940                     else:
941                         feature_counter[feature_idx] += 1
942                 except KeyError:
943                     # Ignore out-of-vocabulary items for fixed_vocab=True
944                     continue
945 
946             j_indices.extend(feature_counter.keys())
947             values.extend(feature_counter.values())
948             indptr.append(len(j_indices))
949 
950         if not fixed_vocab:
951             # disable defaultdict behaviour
952             vocabulary = dict(vocabulary)
953             if not vocabulary:
954                 raise ValueError("empty vocabulary; perhaps the documents only"
955                                  " contain stop words")
956 
957         if indptr[-1] > 2147483648:  # = 2**31 - 1
958             if _IS_32BIT:
959                 raise ValueError(('sparse CSR array has {} non-zero '
960                                   'elements and requires 64 bit indexing, '
961                                   'which is unsupported with 32 bit Python.')
962                                  .format(indptr[-1]))
963             indices_dtype = np.int64
964 
965         else:
966             indices_dtype = np.int32
967         j_indices = np.asarray(j_indices, dtype=indices_dtype)
968         indptr = np.asarray(indptr, dtype=indices_dtype)
969         values = np.frombuffer(values, dtype=np.intc)
970 
971         X = sp.csr_matrix((values, j_indices, indptr),
972                           shape=(len(indptr) - 1, len(vocabulary)),
973                           dtype=self.dtype)
974         X.sort_indices()
975         return vocabulary, X
976 
977     def fit(self, raw_documents, y=None):
978         """Learn a vocabulary dictionary of all tokens in the raw documents.
979 
980         Parameters
981         ----------
982         raw_documents : iterable
983             An iterable which yields either str, unicode or file objects.
984 
985         Returns
986         -------
987         self
988         """
989         self.fit_transform(raw_documents)
990         return self
991 
992     def fit_transform(self, raw_documents, y=None):
993         """Learn the vocabulary dictionary and return term-document matrix.
994 
995         This is equivalent to fit followed by transform, but more efficiently
996         implemented.
997 
998         Parameters
999         ----------
1000         raw_documents : iterable
1001             An iterable which yields either str, unicode or file objects.
1002 
1003         Returns
1004         -------
1005         X : array, [n_samples, n_features]
1006             Document-term matrix.
1007         """
1008         # We intentionally don't call the transform method to make
1009         # fit_transform overridable without unwanted side effects in
1010         # TfidfVectorizer.
1011         if isinstance(raw_documents, str):
1012             raise ValueError(
1013                 "Iterable over raw text documents expected, "
1014                 "string object received.")
1015 
1016         self._validate_params()
1017         self._validate_vocabulary()
1018         max_df = self.max_df
1019         min_df = self.min_df
1020         max_features = self.max_features
1021 
1022         vocabulary, X = self._count_vocab(raw_documents,
1023                                           self.fixed_vocabulary_)
1024 
1025         if self.binary:
1026             X.data.fill(1)
1027 
1028         if not self.fixed_vocabulary_:
1029             X = self._sort_features(X, vocabulary)
1030 
1031             n_doc = X.shape[0]
1032             max_doc_count = (max_df
1033                              if isinstance(max_df, numbers.Integral)
1034                              else max_df * n_doc)
1035             min_doc_count = (min_df
1036                              if isinstance(min_df, numbers.Integral)
1037                              else min_df * n_doc)
1038             if max_doc_count < min_doc_count:
1039                 raise ValueError(
1040                     "max_df corresponds to < documents than min_df")
1041             X, self.stop_words_ = self._limit_features(X, vocabulary,
1042                                                        max_doc_count,
1043                                                        min_doc_count,
1044                                                        max_features)
1045 
1046             self.vocabulary_ = vocabulary
1047 
1048         return X
1049 
1050     def transform(self, raw_documents):
1051         """Transform documents to document-term matrix.
1052 
1053         Extract token counts out of raw text documents using the vocabulary
1054         fitted with fit or the one provided to the constructor.
1055 
1056         Parameters
1057         ----------
1058         raw_documents : iterable
1059             An iterable which yields either str, unicode or file objects.
1060 
1061         Returns
1062         -------
1063         X : sparse matrix, [n_samples, n_features]
1064             Document-term matrix.
1065         """
1066         if isinstance(raw_documents, str):
1067             raise ValueError(
1068                 "Iterable over raw text documents expected, "
1069                 "string object received.")
1070 
1071         if not hasattr(self, 'vocabulary_'):
1072             self._validate_vocabulary()
1073 
1074         self._check_vocabulary()
1075 
1076         # use the same matrix-building strategy as fit_transform
1077         _, X = self._count_vocab(raw_documents, fixed_vocab=True)
1078         if self.binary:
1079             X.data.fill(1)
1080         return X
1081 
1082     def inverse_transform(self, X):
1083         """Return terms per document with nonzero entries in X.
1084 
1085         Parameters
1086         ----------
1087         X : {array, sparse matrix}, shape = [n_samples, n_features]
1088 
1089         Returns
1090         -------
1091         X_inv : list of arrays, len = n_samples
1092             List of arrays of terms.
1093         """
1094         self._check_vocabulary()
1095 
1096         if sp.issparse(X):
1097             # We need CSR format for fast row manipulations.
1098             X = X.tocsr()
1099         else:
1100             # We need to convert X to a matrix, so that the indexing
1101             # returns 2D objects
1102             X = np.asmatrix(X)
1103         n_samples = X.shape[0]
1104 
1105         terms = np.array(list(self.vocabulary_.keys()))
1106         indices = np.array(list(self.vocabulary_.values()))
1107         inverse_vocabulary = terms[np.argsort(indices)]
1108 
1109         return [inverse_vocabulary[X[i, :].nonzero()[1]].ravel()
1110                 for i in range(n_samples)]
1111 
1112     def get_feature_names(self):
1113         """Array mapping from feature integer indices to feature name"""
1114         if not hasattr(self, 'vocabulary_'):
1115             self._validate_vocabulary()
1116 
1117         self._check_vocabulary()
1118 
1119         return [t for t, i in sorted(self.vocabulary_.items(),
1120                                      key=itemgetter(1))]
1121 
1122     def _more_tags(self):
1123         return {'X_types': ['string']}

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-17 17:42:41.306 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-17 17:42:41.306 | INFO     | Search-only: skipping patch selection
2025-10-17 17:42:41.391 | INFO     | Task scikit-learn__scikit-learn-13641 completed successfully.
2025-10-17 17:42:41.392 | INFO     | No patch generated. You can try running ACR again.
