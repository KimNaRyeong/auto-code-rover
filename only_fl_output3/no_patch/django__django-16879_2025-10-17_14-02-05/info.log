2025-10-17 14:02:05.711 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-17 14:02:08.610 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=706, output_tokens=12, cost=0.007420
2025-10-17 14:02:40.134 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=886, output_tokens=994, cost=0.038680
2025-10-17 14:02:41.425 | INFO     | registered non reproducing test 0
2025-10-17 14:03:07.153 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3512, output_tokens=944, cost=0.063440
2025-10-17 14:03:08.317 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 14:03:16.419 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=921, output_tokens=402, cost=0.021270
2025-10-17 14:03:16.427 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 14:03:19.466 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=756, output_tokens=89, cost=0.010230
2025-10-17 14:03:19.467 | DEBUG    | Extracted a valid json.
2025-10-17 14:03:19.467 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"QuerySet\")",
        "search_method_in_class(\"annotate\", \"QuerySet\")",
        "search_method_in_class(\"aggregate\", \"QuerySet\")",
        "search_class(\"Sum\")",
        "search_class(\"Case\")",
        "search_class(\"When\")",
        "search_code(\"Cannot resolve keyword into field\")"
    ],
    "bug_locations": []
}
2025-10-17 14:03:19.559 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-17 14:03:19.560 | DEBUG    | <Agent search> Analyzing search results.
2025-10-17 14:03:36.781 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3660, output_tokens=687, cost=0.057210
2025-10-17 14:03:36.794 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 14:03:47.814 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=4563, output_tokens=515, cost=0.061080
2025-10-17 14:03:47.823 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 14:03:55.383 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=869, output_tokens=275, cost=0.016940
2025-10-17 14:03:55.384 | DEBUG    | Extracted a valid json.
2025-10-17 14:03:55.384 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "django/db/models/query.py",
            "class": "QuerySet",
            "method": "aggregate",
            "intended_behavior": "The `aggregate` method should be able to correctly resolve and use fields that have been added to a `QuerySet` through an annotation, especially when referenced in further aggregate or conditional expressions as seen in the provided issue report. This implies enhancing the mechanism within Django's ORM that interprets field references in such expressions, possibly requiring adjustments in how queries are chained or how field names are resolved during the aggregation phase. The behavior should ensure that aggregated fields can be referenced without causing a `FieldError`."
        },
        {
            "file": "django/db/models/expressions.py",
            "class": "Case",
            "method": "Case",
            "intended_behavior": "Given the error seems to relate to the handling of the `Case` expression, specifically in version 4.2, any adjustment or improvement in expression resolution or compilation that was changed from version 4.1 to 4.2 should be revisited. The `Case` expression should accurately handle aggregated fields derived from annotations, ensuring that these fields are correctly identified and utilized in the conditional logic without causing resolution errors."
        }
    ]
}
2025-10-17 14:03:55.400 | DEBUG    | Bug location extracted successfully: [<file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>aggregate</method>
<code>
561     def aggregate(self, *args, **kwargs):
562         """
563         Return a dictionary containing the calculations (aggregation)
564         over the current queryset.
565 
566         If args is present the expression is passed as a kwarg using
567         the Aggregate object's default alias.
568         """
569         if self.query.distinct_fields:
570             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
571         self._validate_values_are_expressions(
572             (*args, *kwargs.values()), method_name="aggregate"
573         )
574         for arg in args:
575             # The default_alias property raises TypeError if default_alias
576             # can't be set automatically or AttributeError if it isn't an
577             # attribute.
578             try:
579                 arg.default_alias
580             except (AttributeError, TypeError):
581                 raise TypeError("Complex aggregates require an alias")
582             kwargs[arg.default_alias] = arg
583 
584         return self.query.chain().get_aggregation(self.db, kwargs)

</code><intended_behavior>The `aggregate` method should be able to correctly resolve and use fields that have been added to a `QuerySet` through an annotation, especially when referenced in further aggregate or conditional expressions as seen in the provided issue report. This implies enhancing the mechanism within Django's ORM that interprets field references in such expressions, possibly requiring adjustments in how queries are chained or how field names are resolved during the aggregation phase. The behavior should ensure that aggregated fields can be referenced without causing a `FieldError`.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
290 class QuerySet(AltersData):
291     """Represent a lazy database lookup for a set of objects."""
292 
293     def __init__(self, model=None, query=None, using=None, hints=None):
294         self.model = model
295         self._db = using
296         self._hints = hints or {}
297         self._query = query or sql.Query(self.model)
298         self._result_cache = None
299         self._sticky_filter = False
300         self._for_write = False
301         self._prefetch_related_lookups = ()
302         self._prefetch_done = False
303         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
304         self._iterable_class = ModelIterable
305         self._fields = None
306         self._defer_next_filter = False
307         self._deferred_filter = None
308 
309     @property
310     def query(self):
311         if self._deferred_filter:
312             negate, args, kwargs = self._deferred_filter
313             self._filter_or_exclude_inplace(negate, args, kwargs)
314             self._deferred_filter = None
315         return self._query
316 
317     @query.setter
318     def query(self, value):
319         if value.values_select:
320             self._iterable_class = ValuesIterable
321         self._query = value
322 
323     def as_manager(cls):
324         # Address the circular dependency between `Queryset` and `Manager`.
325         from django.db.models.manager import Manager
326 
327         manager = Manager.from_queryset(cls)()
328         manager._built_with_as_manager = True
329         return manager
330 
331     as_manager.queryset_only = True
332     as_manager = classmethod(as_manager)
333 
334     ########################
335     # PYTHON MAGIC METHODS #
336     ########################
337 
338     def __deepcopy__(self, memo):
339         """Don't populate the QuerySet's cache."""
340         obj = self.__class__()
341         for k, v in self.__dict__.items():
342             if k == "_result_cache":
343                 obj.__dict__[k] = None
344             else:
345                 obj.__dict__[k] = copy.deepcopy(v, memo)
346         return obj
347 
348     def __getstate__(self):
349         # Force the cache to be fully populated.
350         self._fetch_all()
351         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
352 
353     def __setstate__(self, state):
354         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
355         if pickled_version:
356             if pickled_version != django.__version__:
357                 warnings.warn(
358                     "Pickled queryset instance's Django version %s does not "
359                     "match the current version %s."
360                     % (pickled_version, django.__version__),
361                     RuntimeWarning,
362                     stacklevel=2,
363                 )
364         else:
365             warnings.warn(
366                 "Pickled queryset instance's Django version is not specified.",
367                 RuntimeWarning,
368                 stacklevel=2,
369             )
370         self.__dict__.update(state)
371 
372     def __repr__(self):
373         data = list(self[: REPR_OUTPUT_SIZE + 1])
374         if len(data) > REPR_OUTPUT_SIZE:
375             data[-1] = "...(remaining elements truncated)..."
376         return "<%s %r>" % (self.__class__.__name__, data)
377 
378     def __len__(self):
379         self._fetch_all()
380         return len(self._result_cache)
381 
382     def __iter__(self):
383         """
384         The queryset iterator protocol uses three nested iterators in the
385         default case:
386             1. sql.compiler.execute_sql()
387                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
388                  using cursor.fetchmany(). This part is responsible for
389                  doing some column masking, and returning the rows in chunks.
390             2. sql.compiler.results_iter()
391                - Returns one row at time. At this point the rows are still just
392                  tuples. In some cases the return values are converted to
393                  Python values at this location.
394             3. self.iterator()
395                - Responsible for turning the rows into model objects.
396         """
397         self._fetch_all()
398         return iter(self._result_cache)
399 
400     def __aiter__(self):
401         # Remember, __aiter__ itself is synchronous, it's the thing it returns
402         # that is async!
403         async def generator():
404             await sync_to_async(self._fetch_all)()
405             for item in self._result_cache:
406                 yield item
407 
408         return generator()
409 
410     def __bool__(self):
411         self._fetch_all()
412         return bool(self._result_cache)
413 
414     def __getitem__(self, k):
415         """Retrieve an item or slice from the set of results."""
416         if not isinstance(k, (int, slice)):
417             raise TypeError(
418                 "QuerySet indices must be integers or slices, not %s."
419                 % type(k).__name__
420             )
421         if (isinstance(k, int) and k < 0) or (
422             isinstance(k, slice)
423             and (
424                 (k.start is not None and k.start < 0)
425                 or (k.stop is not None and k.stop < 0)
426             )
427         ):
428             raise ValueError("Negative indexing is not supported.")
429 
430         if self._result_cache is not None:
431             return self._result_cache[k]
432 
433         if isinstance(k, slice):
434             qs = self._chain()
435             if k.start is not None:
436                 start = int(k.start)
437             else:
438                 start = None
439             if k.stop is not None:
440                 stop = int(k.stop)
441             else:
442                 stop = None
443             qs.query.set_limits(start, stop)
444             return list(qs)[:: k.step] if k.step else qs
445 
446         qs = self._chain()
447         qs.query.set_limits(k, k + 1)
448         qs._fetch_all()
449         return qs._result_cache[0]
450 
451     def __class_getitem__(cls, *args, **kwargs):
452         return cls
453 
454     def __and__(self, other):
455         self._check_operator_queryset(other, "&")
456         self._merge_sanity_check(other)
457         if isinstance(other, EmptyQuerySet):
458             return other
459         if isinstance(self, EmptyQuerySet):
460             return self
461         combined = self._chain()
462         combined._merge_known_related_objects(other)
463         combined.query.combine(other.query, sql.AND)
464         return combined
465 
466     def __or__(self, other):
467         self._check_operator_queryset(other, "|")
468         self._merge_sanity_check(other)
469         if isinstance(self, EmptyQuerySet):
470             return other
471         if isinstance(other, EmptyQuerySet):
472             return self
473         query = (
474             self
475             if self.query.can_filter()
476             else self.model._base_manager.filter(pk__in=self.values("pk"))
477         )
478         combined = query._chain()
479         combined._merge_known_related_objects(other)
480         if not other.query.can_filter():
481             other = other.model._base_manager.filter(pk__in=other.values("pk"))
482         combined.query.combine(other.query, sql.OR)
483         return combined
484 
485     def __xor__(self, other):
486         self._check_operator_queryset(other, "^")
487         self._merge_sanity_check(other)
488         if isinstance(self, EmptyQuerySet):
489             return other
490         if isinstance(other, EmptyQuerySet):
491             return self
492         query = (
493             self
494             if self.query.can_filter()
495             else self.model._base_manager.filter(pk__in=self.values("pk"))
496         )
497         combined = query._chain()
498         combined._merge_known_related_objects(other)
499         if not other.query.can_filter():
500             other = other.model._base_manager.filter(pk__in=other.values("pk"))
501         combined.query.combine(other.query, sql.XOR)
502         return combined
503 
504     ####################################
505     # METHODS THAT DO DATABASE QUERIES #
506     ####################################
507 
508     def _iterator(self, use_chunked_fetch, chunk_size):
509         iterable = self._iterable_class(
510             self,
511             chunked_fetch=use_chunked_fetch,
512             chunk_size=chunk_size or 2000,
513         )
514         if not self._prefetch_related_lookups or chunk_size is None:
515             yield from iterable
516             return
517 
518         iterator = iter(iterable)
519         while results := list(islice(iterator, chunk_size)):
520             prefetch_related_objects(results, *self._prefetch_related_lookups)
521             yield from results
522 
523     def iterator(self, chunk_size=None):
524         """
525         An iterator over the results from applying this QuerySet to the
526         database. chunk_size must be provided for QuerySets that prefetch
527         related objects. Otherwise, a default chunk_size of 2000 is supplied.
528         """
529         if chunk_size is None:
530             if self._prefetch_related_lookups:
531                 raise ValueError(
532                     "chunk_size must be provided when using QuerySet.iterator() after "
533                     "prefetch_related()."
534                 )
535         elif chunk_size <= 0:
536             raise ValueError("Chunk size must be strictly positive.")
537         use_chunked_fetch = not connections[self.db].settings_dict.get(
538             "DISABLE_SERVER_SIDE_CURSORS"
539         )
540         return self._iterator(use_chunked_fetch, chunk_size)
541 
542     async def aiterator(self, chunk_size=2000):
543         """
544         An asynchronous iterator over the results from applying this QuerySet
545         to the database.
546         """
547         if self._prefetch_related_lookups:
548             raise NotSupportedError(
549                 "Using QuerySet.aiterator() after prefetch_related() is not supported."
550             )
551         if chunk_size <= 0:
552             raise ValueError("Chunk size must be strictly positive.")
553         use_chunked_fetch = not connections[self.db].settings_dict.get(
554             "DISABLE_SERVER_SIDE_CURSORS"
555         )
556         async for item in self._iterable_class(
557             self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size
558         ):
559             yield item
560 
561     def aggregate(self, *args, **kwargs):
562         """
563         Return a dictionary containing the calculations (aggregation)
564         over the current queryset.
565 
566         If args is present the expression is passed as a kwarg using
567         the Aggregate object's default alias.
568         """
569         if self.query.distinct_fields:
570             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
571         self._validate_values_are_expressions(
572             (*args, *kwargs.values()), method_name="aggregate"
573         )
574         for arg in args:
575             # The default_alias property raises TypeError if default_alias
576             # can't be set automatically or AttributeError if it isn't an
577             # attribute.
578             try:
579                 arg.default_alias
580             except (AttributeError, TypeError):
581                 raise TypeError("Complex aggregates require an alias")
582             kwargs[arg.default_alias] = arg
583 
584         return self.query.chain().get_aggregation(self.db, kwargs)
585 
586     async def aaggregate(self, *args, **kwargs):
587         return await sync_to_async(self.aggregate)(*args, **kwargs)
588 
589     def count(self):
590         """
591         Perform a SELECT COUNT() and return the number of records as an
592         integer.
593 
594         If the QuerySet is already fully cached, return the length of the
595         cached results set to avoid multiple SELECT COUNT(*) calls.
596         """
597         if self._result_cache is not None:
598             return len(self._result_cache)
599 
600         return self.query.get_count(using=self.db)
601 
602     async def acount(self):
603         return await sync_to_async(self.count)()
604 
605     def get(self, *args, **kwargs):
606         """
607         Perform the query and return a single object matching the given
608         keyword arguments.
609         """
610         if self.query.combinator and (args or kwargs):
611             raise NotSupportedError(
612                 "Calling QuerySet.get(...) with filters after %s() is not "
613                 "supported." % self.query.combinator
614             )
615         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
616         if self.query.can_filter() and not self.query.distinct_fields:
617             clone = clone.order_by()
618         limit = None
619         if (
620             not clone.query.select_for_update
621             or connections[clone.db].features.supports_select_for_update_with_limit
622         ):
623             limit = MAX_GET_RESULTS
624             clone.query.set_limits(high=limit)
625         num = len(clone)
626         if num == 1:
627             return clone._result_cache[0]
628         if not num:
629             raise self.model.DoesNotExist(
630                 "%s matching query does not exist." % self.model._meta.object_name
631             )
632         raise self.model.MultipleObjectsReturned(
633             "get() returned more than one %s -- it returned %s!"
634             % (
635                 self.model._meta.object_name,
636                 num if not limit or num < limit else "more than %s" % (limit - 1),
637             )
638         )
639 
640     async def aget(self, *args, **kwargs):
641         return await sync_to_async(self.get)(*args, **kwargs)
642 
643     def create(self, **kwargs):
644         """
645         Create a new object with the given kwargs, saving it to the database
646         and returning the created object.
647         """
648         obj = self.model(**kwargs)
649         self._for_write = True
650         obj.save(force_insert=True, using=self.db)
651         return obj
652 
653     async def acreate(self, **kwargs):
654         return await sync_to_async(self.create)(**kwargs)
655 
656     def _prepare_for_bulk_create(self, objs):
657         from django.db.models.expressions import DatabaseDefault
658 
659         connection = connections[self.db]
660         for obj in objs:
661             if obj.pk is None:
662                 # Populate new PK values.
663                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
664             if not connection.features.supports_default_keyword_in_bulk_insert:
665                 for field in obj._meta.fields:
666                     value = getattr(obj, field.attname)
667                     if isinstance(value, DatabaseDefault):
668                         setattr(obj, field.attname, field.db_default)
669 
670             obj._prepare_related_fields_for_save(operation_name="bulk_create")
671 
672     def _check_bulk_create_options(
673         self, ignore_conflicts, update_conflicts, update_fields, unique_fields
674     ):
675         if ignore_conflicts and update_conflicts:
676             raise ValueError(
677                 "ignore_conflicts and update_conflicts are mutually exclusive."
678             )
679         db_features = connections[self.db].features
680         if ignore_conflicts:
681             if not db_features.supports_ignore_conflicts:
682                 raise NotSupportedError(
683                     "This database backend does not support ignoring conflicts."
684                 )
685             return OnConflict.IGNORE
686         elif update_conflicts:
687             if not db_features.supports_update_conflicts:
688                 raise NotSupportedError(
689                     "This database backend does not support updating conflicts."
690                 )
691             if not update_fields:
692                 raise ValueError(
693                     "Fields that will be updated when a row insertion fails "
694                     "on conflicts must be provided."
695                 )
696             if unique_fields and not db_features.supports_update_conflicts_with_target:
697                 raise NotSupportedError(
698                     "This database backend does not support updating "
699                     "conflicts with specifying unique fields that can trigger "
700                     "the upsert."
701                 )
702             if not unique_fields and db_features.supports_update_conflicts_with_target:
703                 raise ValueError(
704                     "Unique fields that can trigger the upsert must be provided."
705                 )
706             # Updating primary keys and non-concrete fields is forbidden.
707             if any(not f.concrete or f.many_to_many for f in update_fields):
708                 raise ValueError(
709                     "bulk_create() can only be used with concrete fields in "
710                     "update_fields."
711                 )
712             if any(f.primary_key for f in update_fields):
713                 raise ValueError(
714                     "bulk_create() cannot be used with primary keys in "
715                     "update_fields."
716                 )
717             if unique_fields:
718                 if any(not f.concrete or f.many_to_many for f in unique_fields):
719                     raise ValueError(
720                         "bulk_create() can only be used with concrete fields "
721                         "in unique_fields."
722                     )
723             return OnConflict.UPDATE
724         return None
725 
726     def bulk_create(
727         self,
728         objs,
729         batch_size=None,
730         ignore_conflicts=False,
731         update_conflicts=False,
732         update_fields=None,
733         unique_fields=None,
734     ):
735         """
736         Insert each of the instances into the database. Do *not* call
737         save() on each of the instances, do not send any pre/post_save
738         signals, and do not set the primary key attribute if it is an
739         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
740         Multi-table models are not supported.
741         """
742         # When you bulk insert you don't get the primary keys back (if it's an
743         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
744         # you can't insert into the child tables which references this. There
745         # are two workarounds:
746         # 1) This could be implemented if you didn't have an autoincrement pk
747         # 2) You could do it by doing O(n) normal inserts into the parent
748         #    tables to get the primary keys back and then doing a single bulk
749         #    insert into the childmost table.
750         # We currently set the primary keys on the objects when using
751         # PostgreSQL via the RETURNING ID clause. It should be possible for
752         # Oracle as well, but the semantics for extracting the primary keys is
753         # trickier so it's not done yet.
754         if batch_size is not None and batch_size <= 0:
755             raise ValueError("Batch size must be a positive integer.")
756         # Check that the parents share the same concrete model with the our
757         # model to detect the inheritance pattern ConcreteGrandParent ->
758         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
759         # would not identify that case as involving multiple tables.
760         for parent in self.model._meta.get_parent_list():
761             if parent._meta.concrete_model is not self.model._meta.concrete_model:
762                 raise ValueError("Can't bulk create a multi-table inherited model")
763         if not objs:
764             return objs
765         opts = self.model._meta
766         if unique_fields:
767             # Primary key is allowed in unique_fields.
768             unique_fields = [
769                 self.model._meta.get_field(opts.pk.name if name == "pk" else name)
770                 for name in unique_fields
771             ]
772         if update_fields:
773             update_fields = [self.model._meta.get_field(name) for name in update_fields]
774         on_conflict = self._check_bulk_create_options(
775             ignore_conflicts,
776             update_conflicts,
777             update_fields,
778             unique_fields,
779         )
780         self._for_write = True
781         fields = opts.concrete_fields
782         objs = list(objs)
783         self._prepare_for_bulk_create(objs)
784         with transaction.atomic(using=self.db, savepoint=False):
785             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
786             if objs_with_pk:
787                 returned_columns = self._batched_insert(
788                     objs_with_pk,
789                     fields,
790                     batch_size,
791                     on_conflict=on_conflict,
792                     update_fields=update_fields,
793                     unique_fields=unique_fields,
794                 )
795                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
796                     for result, field in zip(results, opts.db_returning_fields):
797                         if field != opts.pk:
798                             setattr(obj_with_pk, field.attname, result)
799                 for obj_with_pk in objs_with_pk:
800                     obj_with_pk._state.adding = False
801                     obj_with_pk._state.db = self.db
802             if objs_without_pk:
803                 fields = [f for f in fields if not isinstance(f, AutoField)]
804                 returned_columns = self._batched_insert(
805                     objs_without_pk,
806                     fields,
807                     batch_size,
808                     on_conflict=on_conflict,
809                     update_fields=update_fields,
810                     unique_fields=unique_fields,
811                 )
812                 connection = connections[self.db]
813                 if (
814                     connection.features.can_return_rows_from_bulk_insert
815                     and on_conflict is None
816                 ):
817                     assert len(returned_columns) == len(objs_without_pk)
818                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
819                     for result, field in zip(results, opts.db_returning_fields):
820                         setattr(obj_without_pk, field.attname, result)
821                     obj_without_pk._state.adding = False
822                     obj_without_pk._state.db = self.db
823 
824         return objs
825 
826     async def abulk_create(
827         self,
828         objs,
829         batch_size=None,
830         ignore_conflicts=False,
831         update_conflicts=False,
832         update_fields=None,
833         unique_fields=None,
834     ):
835         return await sync_to_async(self.bulk_create)(
836             objs=objs,
837             batch_size=batch_size,
838             ignore_conflicts=ignore_conflicts,
839             update_conflicts=update_conflicts,
840             update_fields=update_fields,
841             unique_fields=unique_fields,
842         )
843 
844     def bulk_update(self, objs, fields, batch_size=None):
845         """
846         Update the given fields in each of the given objects in the database.
847         """
848         if batch_size is not None and batch_size <= 0:
849             raise ValueError("Batch size must be a positive integer.")
850         if not fields:
851             raise ValueError("Field names must be given to bulk_update().")
852         objs = tuple(objs)
853         if any(obj.pk is None for obj in objs):
854             raise ValueError("All bulk_update() objects must have a primary key set.")
855         fields = [self.model._meta.get_field(name) for name in fields]
856         if any(not f.concrete or f.many_to_many for f in fields):
857             raise ValueError("bulk_update() can only be used with concrete fields.")
858         if any(f.primary_key for f in fields):
859             raise ValueError("bulk_update() cannot be used with primary key fields.")
860         if not objs:
861             return 0
862         for obj in objs:
863             obj._prepare_related_fields_for_save(
864                 operation_name="bulk_update", fields=fields
865             )
866         # PK is used twice in the resulting update query, once in the filter
867         # and once in the WHEN. Each field will also have one CAST.
868         self._for_write = True
869         connection = connections[self.db]
870         max_batch_size = connection.ops.bulk_batch_size(["pk", "pk"] + fields, objs)
871         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
872         requires_casting = connection.features.requires_casted_case_in_updates
873         batches = (objs[i : i + batch_size] for i in range(0, len(objs), batch_size))
874         updates = []
875         for batch_objs in batches:
876             update_kwargs = {}
877             for field in fields:
878                 when_statements = []
879                 for obj in batch_objs:
880                     attr = getattr(obj, field.attname)
881                     if not hasattr(attr, "resolve_expression"):
882                         attr = Value(attr, output_field=field)
883                     when_statements.append(When(pk=obj.pk, then=attr))
884                 case_statement = Case(*when_statements, output_field=field)
885                 if requires_casting:
886                     case_statement = Cast(case_statement, output_field=field)
887                 update_kwargs[field.attname] = case_statement
888             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
889         rows_updated = 0
890         queryset = self.using(self.db)
891         with transaction.atomic(using=self.db, savepoint=False):
892             for pks, update_kwargs in updates:
893                 rows_updated += queryset.filter(pk__in=pks).update(**update_kwargs)
894         return rows_updated
895 
896     bulk_update.alters_data = True
897 
898     async def abulk_update(self, objs, fields, batch_size=None):
899         return await sync_to_async(self.bulk_update)(
900             objs=objs,
901             fields=fields,
902             batch_size=batch_size,
903         )
904 
905     abulk_update.alters_data = True
906 
907     def get_or_create(self, defaults=None, **kwargs):
908         """
909         Look up an object with the given kwargs, creating one if necessary.
910         Return a tuple of (object, created), where created is a boolean
911         specifying whether an object was created.
912         """
913         # The get() needs to be targeted at the write database in order
914         # to avoid potential transaction consistency problems.
915         self._for_write = True
916         try:
917             return self.get(**kwargs), False
918         except self.model.DoesNotExist:
919             params = self._extract_model_params(defaults, **kwargs)
920             # Try to create an object using passed params.
921             try:
922                 with transaction.atomic(using=self.db):
923                     params = dict(resolve_callables(params))
924                     return self.create(**params), True
925             except IntegrityError:
926                 try:
927                     return self.get(**kwargs), False
928                 except self.model.DoesNotExist:
929                     pass
930                 raise
931 
932     async def aget_or_create(self, defaults=None, **kwargs):
933         return await sync_to_async(self.get_or_create)(
934             defaults=defaults,
935             **kwargs,
936         )
937 
938     def update_or_create(self, defaults=None, create_defaults=None, **kwargs):
939         """
940         Look up an object with the given kwargs, updating one with defaults
941         if it exists, otherwise create a new one. Optionally, an object can
942         be created with different values than defaults by using
943         create_defaults.
944         Return a tuple (object, created), where created is a boolean
945         specifying whether an object was created.
946         """
947         if create_defaults is None:
948             update_defaults = create_defaults = defaults or {}
949         else:
950             update_defaults = defaults or {}
951         self._for_write = True
952         with transaction.atomic(using=self.db):
953             # Lock the row so that a concurrent update is blocked until
954             # update_or_create() has performed its save.
955             obj, created = self.select_for_update().get_or_create(
956                 create_defaults, **kwargs
957             )
958             if created:
959                 return obj, created
960             for k, v in resolve_callables(update_defaults):
961                 setattr(obj, k, v)
962 
963             update_fields = set(update_defaults)
964             concrete_field_names = self.model._meta._non_pk_concrete_field_names
965             # update_fields does not support non-concrete fields.
966             if concrete_field_names.issuperset(update_fields):
967                 # Add fields which are set on pre_save(), e.g. auto_now fields.
968                 # This is to maintain backward compatibility as these fields
969                 # are not updated unless explicitly specified in the
970                 # update_fields list.
971                 for field in self.model._meta.local_concrete_fields:
972                     if not (
973                         field.primary_key or field.__class__.pre_save is Field.pre_save
974                     ):
975                         update_fields.add(field.name)
976                         if field.name != field.attname:
977                             update_fields.add(field.attname)
978                 obj.save(using=self.db, update_fields=update_fields)
979             else:
980                 obj.save(using=self.db)
981         return obj, False
982 
983     async def aupdate_or_create(self, defaults=None, create_defaults=None, **kwargs):
984         return await sync_to_async(self.update_or_create)(
985             defaults=defaults,
986             create_defaults=create_defaults,
987             **kwargs,
988         )
989 
990     def _extract_model_params(self, defaults, **kwargs):
991         """
992         Prepare `params` for creating a model instance based on the given
993         kwargs; for use by get_or_create().
994         """
995         defaults = defaults or {}
996         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
997         params.update(defaults)
998         property_names = self.model._meta._property_names
999         invalid_params = []
1000         for param in params:
1001             try:
1002                 self.model._meta.get_field(param)
1003             except exceptions.FieldDoesNotExist:
1004                 # It's okay to use a model's property if it has a setter.
1005                 if not (param in property_names and getattr(self.model, param).fset):
1006                     invalid_params.append(param)
1007         if invalid_params:
1008             raise exceptions.FieldError(
1009                 "Invalid field name(s) for model %s: '%s'."
1010                 % (
1011                     self.model._meta.object_name,
1012                     "', '".join(sorted(invalid_params)),
1013                 )
1014             )
1015         return params
1016 
1017     def _earliest(self, *fields):
1018         """
1019         Return the earliest object according to fields (if given) or by the
1020         model's Meta.get_latest_by.
1021         """
1022         if fields:
1023             order_by = fields
1024         else:
1025             order_by = getattr(self.model._meta, "get_latest_by")
1026             if order_by and not isinstance(order_by, (tuple, list)):
1027                 order_by = (order_by,)
1028         if order_by is None:
1029             raise ValueError(
1030                 "earliest() and latest() require either fields as positional "
1031                 "arguments or 'get_latest_by' in the model's Meta."
1032             )
1033         obj = self._chain()
1034         obj.query.set_limits(high=1)
1035         obj.query.clear_ordering(force=True)
1036         obj.query.add_ordering(*order_by)
1037         return obj.get()
1038 
1039     def earliest(self, *fields):
1040         if self.query.is_sliced:
1041             raise TypeError("Cannot change a query once a slice has been taken.")
1042         return self._earliest(*fields)
1043 
1044     async def aearliest(self, *fields):
1045         return await sync_to_async(self.earliest)(*fields)
1046 
1047     def latest(self, *fields):
1048         """
1049         Return the latest object according to fields (if given) or by the
1050         model's Meta.get_latest_by.
1051         """
1052         if self.query.is_sliced:
1053             raise TypeError("Cannot change a query once a slice has been taken.")
1054         return self.reverse()._earliest(*fields)
1055 
1056     async def alatest(self, *fields):
1057         return await sync_to_async(self.latest)(*fields)
1058 
1059     def first(self):
1060         """Return the first object of a query or None if no match is found."""
1061         if self.ordered:
1062             queryset = self
1063         else:
1064             self._check_ordering_first_last_queryset_aggregation(method="first")
1065             queryset = self.order_by("pk")
1066         for obj in queryset[:1]:
1067             return obj
1068 
1069     async def afirst(self):
1070         return await sync_to_async(self.first)()
1071 
1072     def last(self):
1073         """Return the last object of a query or None if no match is found."""
1074         if self.ordered:
1075             queryset = self.reverse()
1076         else:
1077             self._check_ordering_first_last_queryset_aggregation(method="last")
1078             queryset = self.order_by("-pk")
1079         for obj in queryset[:1]:
1080             return obj
1081 
1082     async def alast(self):
1083         return await sync_to_async(self.last)()
1084 
1085     def in_bulk(self, id_list=None, *, field_name="pk"):
1086         """
1087         Return a dictionary mapping each of the given IDs to the object with
1088         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
1089         """
1090         if self.query.is_sliced:
1091             raise TypeError("Cannot use 'limit' or 'offset' with in_bulk().")
1092         opts = self.model._meta
1093         unique_fields = [
1094             constraint.fields[0]
1095             for constraint in opts.total_unique_constraints
1096             if len(constraint.fields) == 1
1097         ]
1098         if (
1099             field_name != "pk"
1100             and not opts.get_field(field_name).unique
1101             and field_name not in unique_fields
1102             and self.query.distinct_fields != (field_name,)
1103         ):
1104             raise ValueError(
1105                 "in_bulk()'s field_name must be a unique field but %r isn't."
1106                 % field_name
1107             )
1108         if id_list is not None:
1109             if not id_list:
1110                 return {}
1111             filter_key = "{}__in".format(field_name)
1112             batch_size = connections[self.db].features.max_query_params
1113             id_list = tuple(id_list)
1114             # If the database has a limit on the number of query parameters
1115             # (e.g. SQLite), retrieve objects in batches if necessary.
1116             if batch_size and batch_size < len(id_list):
1117                 qs = ()
1118                 for offset in range(0, len(id_list), batch_size):
1119                     batch = id_list[offset : offset + batch_size]
1120                     qs += tuple(self.filter(**{filter_key: batch}))
1121             else:
1122                 qs = self.filter(**{filter_key: id_list})
1123         else:
1124             qs = self._chain()
1125         return {getattr(obj, field_name): obj for obj in qs}
1126 
1127     async def ain_bulk(self, id_list=None, *, field_name="pk"):
1128         return await sync_to_async(self.in_bulk)(
1129             id_list=id_list,
1130             field_name=field_name,
1131         )
1132 
1133     def delete(self):
1134         """Delete the records in the current QuerySet."""
1135         self._not_support_combined_queries("delete")
1136         if self.query.is_sliced:
1137             raise TypeError("Cannot use 'limit' or 'offset' with delete().")
1138         if self.query.distinct or self.query.distinct_fields:
1139             raise TypeError("Cannot call delete() after .distinct().")
1140         if self._fields is not None:
1141             raise TypeError("Cannot call delete() after .values() or .values_list()")
1142 
1143         del_query = self._chain()
1144 
1145         # The delete is actually 2 queries - one to find related objects,
1146         # and one to delete. Make sure that the discovery of related
1147         # objects is performed on the same database as the deletion.
1148         del_query._for_write = True
1149 
1150         # Disable non-supported fields.
1151         del_query.query.select_for_update = False
1152         del_query.query.select_related = False
1153         del_query.query.clear_ordering(force=True)
1154 
1155         collector = Collector(using=del_query.db, origin=self)
1156         collector.collect(del_query)
1157         deleted, _rows_count = collector.delete()
1158 
1159         # Clear the result cache, in case this QuerySet gets reused.
1160         self._result_cache = None
1161         return deleted, _rows_count
1162 
1163     delete.alters_data = True
1164     delete.queryset_only = True
1165 
1166     async def adelete(self):
1167         return await sync_to_async(self.delete)()
1168 
1169     adelete.alters_data = True
1170     adelete.queryset_only = True
1171 
1172     def _raw_delete(self, using):
1173         """
1174         Delete objects found from the given queryset in single direct SQL
1175         query. No signals are sent and there is no protection for cascades.
1176         """
1177         query = self.query.clone()
1178         query.__class__ = sql.DeleteQuery
1179         cursor = query.get_compiler(using).execute_sql(CURSOR)
1180         if cursor:
1181             with cursor:
1182                 return cursor.rowcount
1183         return 0
1184 
1185     _raw_delete.alters_data = True
1186 
1187     def update(self, **kwargs):
1188         """
1189         Update all elements in the current QuerySet, setting all the given
1190         fields to the appropriate values.
1191         """
1192         self._not_support_combined_queries("update")
1193         if self.query.is_sliced:
1194             raise TypeError("Cannot update a query once a slice has been taken.")
1195         self._for_write = True
1196         query = self.query.chain(sql.UpdateQuery)
1197         query.add_update_values(kwargs)
1198 
1199         # Inline annotations in order_by(), if possible.
1200         new_order_by = []
1201         for col in query.order_by:
1202             alias = col
1203             descending = False
1204             if isinstance(alias, str) and alias.startswith("-"):
1205                 alias = alias.removeprefix("-")
1206                 descending = True
1207             if annotation := query.annotations.get(alias):
1208                 if getattr(annotation, "contains_aggregate", False):
1209                     raise exceptions.FieldError(
1210                         f"Cannot update when ordering by an aggregate: {annotation}"
1211                     )
1212                 if descending:
1213                     annotation = annotation.desc()
1214                 new_order_by.append(annotation)
1215             else:
1216                 new_order_by.append(col)
1217         query.order_by = tuple(new_order_by)
1218 
1219         # Clear any annotations so that they won't be present in subqueries.
1220         query.annotations = {}
1221         with transaction.mark_for_rollback_on_error(using=self.db):
1222             rows = query.get_compiler(self.db).execute_sql(CURSOR)
1223         self._result_cache = None
1224         return rows
1225 
1226     update.alters_data = True
1227 
1228     async def aupdate(self, **kwargs):
1229         return await sync_to_async(self.update)(**kwargs)
1230 
1231     aupdate.alters_data = True
1232 
1233     def _update(self, values):
1234         """
1235         A version of update() that accepts field objects instead of field names.
1236         Used primarily for model saving and not intended for use by general
1237         code (it requires too much poking around at model internals to be
1238         useful at that level).
1239         """
1240         if self.query.is_sliced:
1241             raise TypeError("Cannot update a query once a slice has been taken.")
1242         query = self.query.chain(sql.UpdateQuery)
1243         query.add_update_fields(values)
1244         # Clear any annotations so that they won't be present in subqueries.
1245         query.annotations = {}
1246         self._result_cache = None
1247         return query.get_compiler(self.db).execute_sql(CURSOR)
1248 
1249     _update.alters_data = True
1250     _update.queryset_only = False
1251 
1252     def exists(self):
1253         """
1254         Return True if the QuerySet would have any results, False otherwise.
1255         """
1256         if self._result_cache is None:
1257             return self.query.has_results(using=self.db)
1258         return bool(self._result_cache)
1259 
1260     async def aexists(self):
1261         return await sync_to_async(self.exists)()
1262 
1263     def contains(self, obj):
1264         """
1265         Return True if the QuerySet contains the provided obj,
1266         False otherwise.
1267         """
1268         self._not_support_combined_queries("contains")
1269         if self._fields is not None:
1270             raise TypeError(
1271                 "Cannot call QuerySet.contains() after .values() or .values_list()."
1272             )
1273         try:
1274             if obj._meta.concrete_model != self.model._meta.concrete_model:
1275                 return False
1276         except AttributeError:
1277             raise TypeError("'obj' must be a model instance.")
1278         if obj.pk is None:
1279             raise ValueError("QuerySet.contains() cannot be used on unsaved objects.")
1280         if self._result_cache is not None:
1281             return obj in self._result_cache
1282         return self.filter(pk=obj.pk).exists()
1283 
1284     async def acontains(self, obj):
1285         return await sync_to_async(self.contains)(obj=obj)
1286 
1287     def _prefetch_related_objects(self):
1288         # This method can only be called once the result cache has been filled.
1289         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
1290         self._prefetch_done = True
1291 
1292     def explain(self, *, format=None, **options):
1293         """
1294         Runs an EXPLAIN on the SQL query this QuerySet would perform, and
1295         returns the results.
1296         """
1297         return self.query.explain(using=self.db, format=format, **options)
1298 
1299     async def aexplain(self, *, format=None, **options):
1300         return await sync_to_async(self.explain)(format=format, **options)
1301 
1302     ##################################################
1303     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
1304     ##################################################
1305 
1306     def raw(self, raw_query, params=(), translations=None, using=None):
1307         if using is None:
1308             using = self.db
1309         qs = RawQuerySet(
1310             raw_query,
1311             model=self.model,
1312             params=params,
1313             translations=translations,
1314             using=using,
1315         )
1316         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
1317         return qs
1318 
1319     def _values(self, *fields, **expressions):
1320         clone = self._chain()
1321         if expressions:
1322             clone = clone.annotate(**expressions)
1323         clone._fields = fields
1324         clone.query.set_values(fields)
1325         return clone
1326 
1327     def values(self, *fields, **expressions):
1328         fields += tuple(expressions)
1329         clone = self._values(*fields, **expressions)
1330         clone._iterable_class = ValuesIterable
1331         return clone
1332 
1333     def values_list(self, *fields, flat=False, named=False):
1334         if flat and named:
1335             raise TypeError("'flat' and 'named' can't be used together.")
1336         if flat and len(fields) > 1:
1337             raise TypeError(
1338                 "'flat' is not valid when values_list is called with more than one "
1339                 "field."
1340             )
1341 
1342         field_names = {f for f in fields if not hasattr(f, "resolve_expression")}
1343         _fields = []
1344         expressions = {}
1345         counter = 1
1346         for field in fields:
1347             if hasattr(field, "resolve_expression"):
1348                 field_id_prefix = getattr(
1349                     field, "default_alias", field.__class__.__name__.lower()
1350                 )
1351                 while True:
1352                     field_id = field_id_prefix + str(counter)
1353                     counter += 1
1354                     if field_id not in field_names:
1355                         break
1356                 expressions[field_id] = field
1357                 _fields.append(field_id)
1358             else:
1359                 _fields.append(field)
1360 
1361         clone = self._values(*_fields, **expressions)
1362         clone._iterable_class = (
1363             NamedValuesListIterable
1364             if named
1365             else FlatValuesListIterable
1366             if flat
1367             else ValuesListIterable
1368         )
1369         return clone
1370 
1371     def dates(self, field_name, kind, order="ASC"):
1372         """
1373         Return a list of date objects representing all available dates for
1374         the given field_name, scoped to 'kind'.
1375         """
1376         if kind not in ("year", "month", "week", "day"):
1377             raise ValueError("'kind' must be one of 'year', 'month', 'week', or 'day'.")
1378         if order not in ("ASC", "DESC"):
1379             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
1380         return (
1381             self.annotate(
1382                 datefield=Trunc(field_name, kind, output_field=DateField()),
1383                 plain_field=F(field_name),
1384             )
1385             .values_list("datefield", flat=True)
1386             .distinct()
1387             .filter(plain_field__isnull=False)
1388             .order_by(("-" if order == "DESC" else "") + "datefield")
1389         )
1390 
1391     def datetimes(self, field_name, kind, order="ASC", tzinfo=None):
1392         """
1393         Return a list of datetime objects representing all available
1394         datetimes for the given field_name, scoped to 'kind'.
1395         """
1396         if kind not in ("year", "month", "week", "day", "hour", "minute", "second"):
1397             raise ValueError(
1398                 "'kind' must be one of 'year', 'month', 'week', 'day', "
1399                 "'hour', 'minute', or 'second'."
1400             )
1401         if order not in ("ASC", "DESC"):
1402             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
1403         if settings.USE_TZ:
1404             if tzinfo is None:
1405                 tzinfo = timezone.get_current_timezone()
1406         else:
1407             tzinfo = None
1408         return (
1409             self.annotate(
1410                 datetimefield=Trunc(
1411                     field_name,
1412                     kind,
1413                     output_field=DateTimeField(),
1414                     tzinfo=tzinfo,
1415                 ),
1416                 plain_field=F(field_name),
1417             )
1418             .values_list("datetimefield", flat=True)
1419             .distinct()
1420             .filter(plain_field__isnull=False)
1421             .order_by(("-" if order == "DESC" else "") + "datetimefield")
1422         )
1423 
1424     def none(self):
1425         """Return an empty QuerySet."""
1426         clone = self._chain()
1427         clone.query.set_empty()
1428         return clone
1429 
1430     ##################################################################
1431     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
1432     ##################################################################
1433 
1434     def all(self):
1435         """
1436         Return a new QuerySet that is a copy of the current one. This allows a
1437         QuerySet to proxy for a model manager in some cases.
1438         """
1439         return self._chain()
1440 
1441     def filter(self, *args, **kwargs):
1442         """
1443         Return a new QuerySet instance with the args ANDed to the existing
1444         set.
1445         """
1446         self._not_support_combined_queries("filter")
1447         return self._filter_or_exclude(False, args, kwargs)
1448 
1449     def exclude(self, *args, **kwargs):
1450         """
1451         Return a new QuerySet instance with NOT (args) ANDed to the existing
1452         set.
1453         """
1454         self._not_support_combined_queries("exclude")
1455         return self._filter_or_exclude(True, args, kwargs)
1456 
1457     def _filter_or_exclude(self, negate, args, kwargs):
1458         if (args or kwargs) and self.query.is_sliced:
1459             raise TypeError("Cannot filter a query once a slice has been taken.")
1460         clone = self._chain()
1461         if self._defer_next_filter:
1462             self._defer_next_filter = False
1463             clone._deferred_filter = negate, args, kwargs
1464         else:
1465             clone._filter_or_exclude_inplace(negate, args, kwargs)
1466         return clone
1467 
1468     def _filter_or_exclude_inplace(self, negate, args, kwargs):
1469         if negate:
1470             self._query.add_q(~Q(*args, **kwargs))
1471         else:
1472             self._query.add_q(Q(*args, **kwargs))
1473 
1474     def complex_filter(self, filter_obj):
1475         """
1476         Return a new QuerySet instance with filter_obj added to the filters.
1477 
1478         filter_obj can be a Q object or a dictionary of keyword lookup
1479         arguments.
1480 
1481         This exists to support framework features such as 'limit_choices_to',
1482         and usually it will be more natural to use other methods.
1483         """
1484         if isinstance(filter_obj, Q):
1485             clone = self._chain()
1486             clone.query.add_q(filter_obj)
1487             return clone
1488         else:
1489             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
1490 
1491     def _combinator_query(self, combinator, *other_qs, all=False):
1492         # Clone the query to inherit the select list and everything
1493         clone = self._chain()
1494         # Clear limits and ordering so they can be reapplied
1495         clone.query.clear_ordering(force=True)
1496         clone.query.clear_limits()
1497         clone.query.combined_queries = (self.query,) + tuple(
1498             qs.query for qs in other_qs
1499         )
1500         clone.query.combinator = combinator
1501         clone.query.combinator_all = all
1502         return clone
1503 
1504     def union(self, *other_qs, all=False):
1505         # If the query is an EmptyQuerySet, combine all nonempty querysets.
1506         if isinstance(self, EmptyQuerySet):
1507             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1508             if not qs:
1509                 return self
1510             if len(qs) == 1:
1511                 return qs[0]
1512             return qs[0]._combinator_query("union", *qs[1:], all=all)
1513         return self._combinator_query("union", *other_qs, all=all)
1514 
1515     def intersection(self, *other_qs):
1516         # If any query is an EmptyQuerySet, return it.
1517         if isinstance(self, EmptyQuerySet):
1518             return self
1519         for other in other_qs:
1520             if isinstance(other, EmptyQuerySet):
1521                 return other
1522         return self._combinator_query("intersection", *other_qs)
1523 
1524     def difference(self, *other_qs):
1525         # If the query is an EmptyQuerySet, return it.
1526         if isinstance(self, EmptyQuerySet):
1527             return self
1528         return self._combinator_query("difference", *other_qs)
1529 
1530     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1531         """
1532         Return a new QuerySet instance that will select objects with a
1533         FOR UPDATE lock.
1534         """
1535         if nowait and skip_locked:
1536             raise ValueError("The nowait option cannot be used with skip_locked.")
1537         obj = self._chain()
1538         obj._for_write = True
1539         obj.query.select_for_update = True
1540         obj.query.select_for_update_nowait = nowait
1541         obj.query.select_for_update_skip_locked = skip_locked
1542         obj.query.select_for_update_of = of
1543         obj.query.select_for_no_key_update = no_key
1544         return obj
1545 
1546     def select_related(self, *fields):
1547         """
1548         Return a new QuerySet instance that will select related objects.
1549 
1550         If fields are specified, they must be ForeignKey fields and only those
1551         related objects are included in the selection.
1552 
1553         If select_related(None) is called, clear the list.
1554         """
1555         self._not_support_combined_queries("select_related")
1556         if self._fields is not None:
1557             raise TypeError(
1558                 "Cannot call select_related() after .values() or .values_list()"
1559             )
1560 
1561         obj = self._chain()
1562         if fields == (None,):
1563             obj.query.select_related = False
1564         elif fields:
1565             obj.query.add_select_related(fields)
1566         else:
1567             obj.query.select_related = True
1568         return obj
1569 
1570     def prefetch_related(self, *lookups):
1571         """
1572         Return a new QuerySet instance that will prefetch the specified
1573         Many-To-One and Many-To-Many related objects when the QuerySet is
1574         evaluated.
1575 
1576         When prefetch_related() is called more than once, append to the list of
1577         prefetch lookups. If prefetch_related(None) is called, clear the list.
1578         """
1579         self._not_support_combined_queries("prefetch_related")
1580         clone = self._chain()
1581         if lookups == (None,):
1582             clone._prefetch_related_lookups = ()
1583         else:
1584             for lookup in lookups:
1585                 if isinstance(lookup, Prefetch):
1586                     lookup = lookup.prefetch_to
1587                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1588                 if lookup in self.query._filtered_relations:
1589                     raise ValueError(
1590                         "prefetch_related() is not supported with FilteredRelation."
1591                     )
1592             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1593         return clone
1594 
1595     def annotate(self, *args, **kwargs):
1596         """
1597         Return a query set in which the returned objects have been annotated
1598         with extra data or aggregations.
1599         """
1600         self._not_support_combined_queries("annotate")
1601         return self._annotate(args, kwargs, select=True)
1602 
1603     def alias(self, *args, **kwargs):
1604         """
1605         Return a query set with added aliases for extra data or aggregations.
1606         """
1607         self._not_support_combined_queries("alias")
1608         return self._annotate(args, kwargs, select=False)
1609 
1610     def _annotate(self, args, kwargs, select=True):
1611         self._validate_values_are_expressions(
1612             args + tuple(kwargs.values()), method_name="annotate"
1613         )
1614         annotations = {}
1615         for arg in args:
1616             # The default_alias property may raise a TypeError.
1617             try:
1618                 if arg.default_alias in kwargs:
1619                     raise ValueError(
1620                         "The named annotation '%s' conflicts with the "
1621                         "default name for another annotation." % arg.default_alias
1622                     )
1623             except TypeError:
1624                 raise TypeError("Complex annotations require an alias")
1625             annotations[arg.default_alias] = arg
1626         annotations.update(kwargs)
1627 
1628         clone = self._chain()
1629         names = self._fields
1630         if names is None:
1631             names = set(
1632                 chain.from_iterable(
1633                     (field.name, field.attname)
1634                     if hasattr(field, "attname")
1635                     else (field.name,)
1636                     for field in self.model._meta.get_fields()
1637                 )
1638             )
1639 
1640         for alias, annotation in annotations.items():
1641             if alias in names:
1642                 raise ValueError(
1643                     "The annotation '%s' conflicts with a field on "
1644                     "the model." % alias
1645                 )
1646             if isinstance(annotation, FilteredRelation):
1647                 clone.query.add_filtered_relation(annotation, alias)
1648             else:
1649                 clone.query.add_annotation(
1650                     annotation,
1651                     alias,
1652                     select=select,
1653                 )
1654         for alias, annotation in clone.query.annotations.items():
1655             if alias in annotations and annotation.contains_aggregate:
1656                 if clone._fields is None:
1657                     clone.query.group_by = True
1658                 else:
1659                     clone.query.set_group_by()
1660                 break
1661 
1662         return clone
1663 
1664     def order_by(self, *field_names):
1665         """Return a new QuerySet instance with the ordering changed."""
1666         if self.query.is_sliced:
1667             raise TypeError("Cannot reorder a query once a slice has been taken.")
1668         obj = self._chain()
1669         obj.query.clear_ordering(force=True, clear_default=False)
1670         obj.query.add_ordering(*field_names)
1671         return obj
1672 
1673     def distinct(self, *field_names):
1674         """
1675         Return a new QuerySet instance that will select only distinct results.
1676         """
1677         self._not_support_combined_queries("distinct")
1678         if self.query.is_sliced:
1679             raise TypeError(
1680                 "Cannot create distinct fields once a slice has been taken."
1681             )
1682         obj = self._chain()
1683         obj.query.add_distinct_fields(*field_names)
1684         return obj
1685 
1686     def extra(
1687         self,
1688         select=None,
1689         where=None,
1690         params=None,
1691         tables=None,
1692         order_by=None,
1693         select_params=None,
1694     ):
1695         """Add extra SQL fragments to the query."""
1696         self._not_support_combined_queries("extra")
1697         if self.query.is_sliced:
1698             raise TypeError("Cannot change a query once a slice has been taken.")
1699         clone = self._chain()
1700         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1701         return clone
1702 
1703     def reverse(self):
1704         """Reverse the ordering of the QuerySet."""
1705         if self.query.is_sliced:
1706             raise TypeError("Cannot reverse a query once a slice has been taken.")
1707         clone = self._chain()
1708         clone.query.standard_ordering = not clone.query.standard_ordering
1709         return clone
1710 
1711     def defer(self, *fields):
1712         """
1713         Defer the loading of data for certain fields until they are accessed.
1714         Add the set of deferred fields to any existing set of deferred fields.
1715         The only exception to this is if None is passed in as the only
1716         parameter, in which case removal all deferrals.
1717         """
1718         self._not_support_combined_queries("defer")
1719         if self._fields is not None:
1720             raise TypeError("Cannot call defer() after .values() or .values_list()")
1721         clone = self._chain()
1722         if fields == (None,):
1723             clone.query.clear_deferred_loading()
1724         else:
1725             clone.query.add_deferred_loading(fields)
1726         return clone
1727 
1728     def only(self, *fields):
1729         """
1730         Essentially, the opposite of defer(). Only the fields passed into this
1731         method and that are not already specified as deferred are loaded
1732         immediately when the queryset is evaluated.
1733         """
1734         self._not_support_combined_queries("only")
1735         if self._fields is not None:
1736             raise TypeError("Cannot call only() after .values() or .values_list()")
1737         if fields == (None,):
1738             # Can only pass None to defer(), not only(), as the rest option.
1739             # That won't stop people trying to do this, so let's be explicit.
1740             raise TypeError("Cannot pass None as an argument to only().")
1741         for field in fields:
1742             field = field.split(LOOKUP_SEP, 1)[0]
1743             if field in self.query._filtered_relations:
1744                 raise ValueError("only() is not supported with FilteredRelation.")
1745         clone = self._chain()
1746         clone.query.add_immediate_loading(fields)
1747         return clone
1748 
1749     def using(self, alias):
1750         """Select which database this QuerySet should execute against."""
1751         clone = self._chain()
1752         clone._db = alias
1753         return clone
1754 
1755     ###################################
1756     # PUBLIC INTROSPECTION ATTRIBUTES #
1757     ###################################
1758 
1759     @property
1760     def ordered(self):
1761         """
1762         Return True if the QuerySet is ordered -- i.e. has an order_by()
1763         clause or a default ordering on the model (or is empty).
1764         """
1765         if isinstance(self, EmptyQuerySet):
1766             return True
1767         if self.query.extra_order_by or self.query.order_by:
1768             return True
1769         elif (
1770             self.query.default_ordering
1771             and self.query.get_meta().ordering
1772             and
1773             # A default ordering doesn't affect GROUP BY queries.
1774             not self.query.group_by
1775         ):
1776             return True
1777         else:
1778             return False
1779 
1780     @property
1781     def db(self):
1782         """Return the database used if this query is executed now."""
1783         if self._for_write:
1784             return self._db or router.db_for_write(self.model, **self._hints)
1785         return self._db or router.db_for_read(self.model, **self._hints)
1786 
1787     ###################
1788     # PRIVATE METHODS #
1789     ###################
1790 
1791     def _insert(
1792         self,
1793         objs,
1794         fields,
1795         returning_fields=None,
1796         raw=False,
1797         using=None,
1798         on_conflict=None,
1799         update_fields=None,
1800         unique_fields=None,
1801     ):
1802         """
1803         Insert a new record for the given model. This provides an interface to
1804         the InsertQuery class and is how Model.save() is implemented.
1805         """
1806         self._for_write = True
1807         if using is None:
1808             using = self.db
1809         query = sql.InsertQuery(
1810             self.model,
1811             on_conflict=on_conflict,
1812             update_fields=update_fields,
1813             unique_fields=unique_fields,
1814         )
1815         query.insert_values(fields, objs, raw=raw)
1816         return query.get_compiler(using=using).execute_sql(returning_fields)
1817 
1818     _insert.alters_data = True
1819     _insert.queryset_only = False
1820 
1821     def _batched_insert(
1822         self,
1823         objs,
1824         fields,
1825         batch_size,
1826         on_conflict=None,
1827         update_fields=None,
1828         unique_fields=None,
1829     ):
1830         """
1831         Helper method for bulk_create() to insert objs one batch at a time.
1832         """
1833         connection = connections[self.db]
1834         ops = connection.ops
1835         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1836         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1837         inserted_rows = []
1838         bulk_return = connection.features.can_return_rows_from_bulk_insert
1839         for item in [objs[i : i + batch_size] for i in range(0, len(objs), batch_size)]:
1840             if bulk_return and on_conflict is None:
1841                 inserted_rows.extend(
1842                     self._insert(
1843                         item,
1844                         fields=fields,
1845                         using=self.db,
1846                         returning_fields=self.model._meta.db_returning_fields,
1847                     )
1848                 )
1849             else:
1850                 self._insert(
1851                     item,
1852                     fields=fields,
1853                     using=self.db,
1854                     on_conflict=on_conflict,
1855                     update_fields=update_fields,
1856                     unique_fields=unique_fields,
1857                 )
1858         return inserted_rows
1859 
1860     def _chain(self):
1861         """
1862         Return a copy of the current QuerySet that's ready for another
1863         operation.
1864         """
1865         obj = self._clone()
1866         if obj._sticky_filter:
1867             obj.query.filter_is_sticky = True
1868             obj._sticky_filter = False
1869         return obj
1870 
1871     def _clone(self):
1872         """
1873         Return a copy of the current QuerySet. A lightweight alternative
1874         to deepcopy().
1875         """
1876         c = self.__class__(
1877             model=self.model,
1878             query=self.query.chain(),
1879             using=self._db,
1880             hints=self._hints,
1881         )
1882         c._sticky_filter = self._sticky_filter
1883         c._for_write = self._for_write
1884         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1885         c._known_related_objects = self._known_related_objects
1886         c._iterable_class = self._iterable_class
1887         c._fields = self._fields
1888         return c
1889 
1890     def _fetch_all(self):
1891         if self._result_cache is None:
1892             self._result_cache = list(self._iterable_class(self))
1893         if self._prefetch_related_lookups and not self._prefetch_done:
1894             self._prefetch_related_objects()
1895 
1896     def _next_is_sticky(self):
1897         """
1898         Indicate that the next filter call and the one following that should
1899         be treated as a single filter. This is only important when it comes to
1900         determining when to reuse tables for many-to-many filters. Required so
1901         that we can filter naturally on the results of related managers.
1902 
1903         This doesn't return a clone of the current QuerySet (it returns
1904         "self"). The method is only used internally and should be immediately
1905         followed by a filter() that does create a clone.
1906         """
1907         self._sticky_filter = True
1908         return self
1909 
1910     def _merge_sanity_check(self, other):
1911         """Check that two QuerySet classes may be merged."""
1912         if self._fields is not None and (
1913             set(self.query.values_select) != set(other.query.values_select)
1914             or set(self.query.extra_select) != set(other.query.extra_select)
1915             or set(self.query.annotation_select) != set(other.query.annotation_select)
1916         ):
1917             raise TypeError(
1918                 "Merging '%s' classes must involve the same values in each case."
1919                 % self.__class__.__name__
1920             )
1921 
1922     def _merge_known_related_objects(self, other):
1923         """
1924         Keep track of all known related objects from either QuerySet instance.
1925         """
1926         for field, objects in other._known_related_objects.items():
1927             self._known_related_objects.setdefault(field, {}).update(objects)
1928 
1929     def resolve_expression(self, *args, **kwargs):
1930         if self._fields and len(self._fields) > 1:
1931             # values() queryset can only be used as nested queries
1932             # if they are set up to select only a single field.
1933             raise TypeError("Cannot use multi-field values as a filter value.")
1934         query = self.query.resolve_expression(*args, **kwargs)
1935         query._db = self._db
1936         return query
1937 
1938     resolve_expression.queryset_only = True
1939 
1940     def _add_hints(self, **hints):
1941         """
1942         Update hinting information for use by routers. Add new key/values or
1943         overwrite existing key/values.
1944         """
1945         self._hints.update(hints)
1946 
1947     def _has_filters(self):
1948         """
1949         Check if this QuerySet has any filtering going on. This isn't
1950         equivalent with checking if all objects are present in results, for
1951         example, qs[1:]._has_filters() -> False.
1952         """
1953         return self.query.has_filters()
1954 
1955     @staticmethod
1956     def _validate_values_are_expressions(values, method_name):
1957         invalid_args = sorted(
1958             str(arg) for arg in values if not hasattr(arg, "resolve_expression")
1959         )
1960         if invalid_args:
1961             raise TypeError(
1962                 "QuerySet.%s() received non-expression(s): %s."
1963                 % (
1964                     method_name,
1965                     ", ".join(invalid_args),
1966                 )
1967             )
1968 
1969     def _not_support_combined_queries(self, operation_name):
1970         if self.query.combinator:
1971             raise NotSupportedError(
1972                 "Calling QuerySet.%s() after %s() is not supported."
1973                 % (operation_name, self.query.combinator)
1974             )
1975 
1976     def _check_operator_queryset(self, other, operator_):
1977         if self.query.combinator or other.query.combinator:
1978             raise TypeError(f"Cannot use {operator_} operator with combined queryset.")
1979 
1980     def _check_ordering_first_last_queryset_aggregation(self, method):
1981         if isinstance(self.query.group_by, tuple) and not any(
1982             col.output_field is self.model._meta.pk for col in self.query.group_by
1983         ):
1984             raise TypeError(
1985                 f"Cannot use QuerySet.{method}() on an unordered queryset performing "
1986                 f"aggregation. Add an ordering with order_by()."
1987             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Case</class>
<method>None</method>
<code>
1433 class Case(SQLiteNumericMixin, Expression):
1434     """
1435     An SQL searched CASE expression:
1436 
1437         CASE
1438             WHEN n > 0
1439                 THEN 'positive'
1440             WHEN n < 0
1441                 THEN 'negative'
1442             ELSE 'zero'
1443         END
1444     """
1445 
1446     template = "CASE %(cases)s ELSE %(default)s END"
1447     case_joiner = " "
1448 
1449     def __init__(self, *cases, default=None, output_field=None, **extra):
1450         if not all(isinstance(case, When) for case in cases):
1451             raise TypeError("Positional arguments must all be When objects.")
1452         super().__init__(output_field)
1453         self.cases = list(cases)
1454         self.default = self._parse_expressions(default)[0]
1455         self.extra = extra
1456 
1457     def __str__(self):
1458         return "CASE %s, ELSE %r" % (
1459             ", ".join(str(c) for c in self.cases),
1460             self.default,
1461         )
1462 
1463     def __repr__(self):
1464         return "<%s: %s>" % (self.__class__.__name__, self)
1465 
1466     def get_source_expressions(self):
1467         return self.cases + [self.default]
1468 
1469     def set_source_expressions(self, exprs):
1470         *self.cases, self.default = exprs
1471 
1472     def resolve_expression(
1473         self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False
1474     ):
1475         c = self.copy()
1476         c.is_summary = summarize
1477         for pos, case in enumerate(c.cases):
1478             c.cases[pos] = case.resolve_expression(
1479                 query, allow_joins, reuse, summarize, for_save
1480             )
1481         c.default = c.default.resolve_expression(
1482             query, allow_joins, reuse, summarize, for_save
1483         )
1484         return c
1485 
1486     def copy(self):
1487         c = super().copy()
1488         c.cases = c.cases[:]
1489         return c
1490 
1491     def as_sql(
1492         self, compiler, connection, template=None, case_joiner=None, **extra_context
1493     ):
1494         connection.ops.check_expression_support(self)
1495         if not self.cases:
1496             return compiler.compile(self.default)
1497         template_params = {**self.extra, **extra_context}
1498         case_parts = []
1499         sql_params = []
1500         default_sql, default_params = compiler.compile(self.default)
1501         for case in self.cases:
1502             try:
1503                 case_sql, case_params = compiler.compile(case)
1504             except EmptyResultSet:
1505                 continue
1506             except FullResultSet:
1507                 default_sql, default_params = compiler.compile(case.result)
1508                 break
1509             case_parts.append(case_sql)
1510             sql_params.extend(case_params)
1511         if not case_parts:
1512             return default_sql, default_params
1513         case_joiner = case_joiner or self.case_joiner
1514         template_params["cases"] = case_joiner.join(case_parts)
1515         template_params["default"] = default_sql
1516         sql_params.extend(default_params)
1517         template = template or template_params.get("template", self.template)
1518         sql = template % template_params
1519         if self._output_field_or_none is not None:
1520             sql = connection.ops.unification_cast_sql(self.output_field) % sql
1521         return sql, sql_params
1522 
1523     def get_group_by_cols(self):
1524         if not self.cases:
1525             return self.default.get_group_by_cols()
1526         return super().get_group_by_cols()
1527 
1528     @cached_property
1529     def allowed_default(self):
1530         return self.default.allowed_default and all(
1531             case_.allowed_default for case_ in self.cases
1532         )

</code><intended_behavior>Given the error seems to relate to the handling of the `Case` expression, specifically in version 4.2, any adjustment or improvement in expression resolution or compilation that was changed from version 4.1 to 4.2 should be revisited. The `Case` expression should accurately handle aggregated fields derived from annotations, ensuring that these fields are correctly identified and utilized in the conditional logic without causing resolution errors.</intended_behavior>]
2025-10-17 14:03:55.401 | INFO     | Search completed. Bug locations: [<file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>aggregate</method>
<code>
561     def aggregate(self, *args, **kwargs):
562         """
563         Return a dictionary containing the calculations (aggregation)
564         over the current queryset.
565 
566         If args is present the expression is passed as a kwarg using
567         the Aggregate object's default alias.
568         """
569         if self.query.distinct_fields:
570             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
571         self._validate_values_are_expressions(
572             (*args, *kwargs.values()), method_name="aggregate"
573         )
574         for arg in args:
575             # The default_alias property raises TypeError if default_alias
576             # can't be set automatically or AttributeError if it isn't an
577             # attribute.
578             try:
579                 arg.default_alias
580             except (AttributeError, TypeError):
581                 raise TypeError("Complex aggregates require an alias")
582             kwargs[arg.default_alias] = arg
583 
584         return self.query.chain().get_aggregation(self.db, kwargs)

</code><intended_behavior>The `aggregate` method should be able to correctly resolve and use fields that have been added to a `QuerySet` through an annotation, especially when referenced in further aggregate or conditional expressions as seen in the provided issue report. This implies enhancing the mechanism within Django's ORM that interprets field references in such expressions, possibly requiring adjustments in how queries are chained or how field names are resolved during the aggregation phase. The behavior should ensure that aggregated fields can be referenced without causing a `FieldError`.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
290 class QuerySet(AltersData):
291     """Represent a lazy database lookup for a set of objects."""
292 
293     def __init__(self, model=None, query=None, using=None, hints=None):
294         self.model = model
295         self._db = using
296         self._hints = hints or {}
297         self._query = query or sql.Query(self.model)
298         self._result_cache = None
299         self._sticky_filter = False
300         self._for_write = False
301         self._prefetch_related_lookups = ()
302         self._prefetch_done = False
303         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
304         self._iterable_class = ModelIterable
305         self._fields = None
306         self._defer_next_filter = False
307         self._deferred_filter = None
308 
309     @property
310     def query(self):
311         if self._deferred_filter:
312             negate, args, kwargs = self._deferred_filter
313             self._filter_or_exclude_inplace(negate, args, kwargs)
314             self._deferred_filter = None
315         return self._query
316 
317     @query.setter
318     def query(self, value):
319         if value.values_select:
320             self._iterable_class = ValuesIterable
321         self._query = value
322 
323     def as_manager(cls):
324         # Address the circular dependency between `Queryset` and `Manager`.
325         from django.db.models.manager import Manager
326 
327         manager = Manager.from_queryset(cls)()
328         manager._built_with_as_manager = True
329         return manager
330 
331     as_manager.queryset_only = True
332     as_manager = classmethod(as_manager)
333 
334     ########################
335     # PYTHON MAGIC METHODS #
336     ########################
337 
338     def __deepcopy__(self, memo):
339         """Don't populate the QuerySet's cache."""
340         obj = self.__class__()
341         for k, v in self.__dict__.items():
342             if k == "_result_cache":
343                 obj.__dict__[k] = None
344             else:
345                 obj.__dict__[k] = copy.deepcopy(v, memo)
346         return obj
347 
348     def __getstate__(self):
349         # Force the cache to be fully populated.
350         self._fetch_all()
351         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
352 
353     def __setstate__(self, state):
354         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
355         if pickled_version:
356             if pickled_version != django.__version__:
357                 warnings.warn(
358                     "Pickled queryset instance's Django version %s does not "
359                     "match the current version %s."
360                     % (pickled_version, django.__version__),
361                     RuntimeWarning,
362                     stacklevel=2,
363                 )
364         else:
365             warnings.warn(
366                 "Pickled queryset instance's Django version is not specified.",
367                 RuntimeWarning,
368                 stacklevel=2,
369             )
370         self.__dict__.update(state)
371 
372     def __repr__(self):
373         data = list(self[: REPR_OUTPUT_SIZE + 1])
374         if len(data) > REPR_OUTPUT_SIZE:
375             data[-1] = "...(remaining elements truncated)..."
376         return "<%s %r>" % (self.__class__.__name__, data)
377 
378     def __len__(self):
379         self._fetch_all()
380         return len(self._result_cache)
381 
382     def __iter__(self):
383         """
384         The queryset iterator protocol uses three nested iterators in the
385         default case:
386             1. sql.compiler.execute_sql()
387                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
388                  using cursor.fetchmany(). This part is responsible for
389                  doing some column masking, and returning the rows in chunks.
390             2. sql.compiler.results_iter()
391                - Returns one row at time. At this point the rows are still just
392                  tuples. In some cases the return values are converted to
393                  Python values at this location.
394             3. self.iterator()
395                - Responsible for turning the rows into model objects.
396         """
397         self._fetch_all()
398         return iter(self._result_cache)
399 
400     def __aiter__(self):
401         # Remember, __aiter__ itself is synchronous, it's the thing it returns
402         # that is async!
403         async def generator():
404             await sync_to_async(self._fetch_all)()
405             for item in self._result_cache:
406                 yield item
407 
408         return generator()
409 
410     def __bool__(self):
411         self._fetch_all()
412         return bool(self._result_cache)
413 
414     def __getitem__(self, k):
415         """Retrieve an item or slice from the set of results."""
416         if not isinstance(k, (int, slice)):
417             raise TypeError(
418                 "QuerySet indices must be integers or slices, not %s."
419                 % type(k).__name__
420             )
421         if (isinstance(k, int) and k < 0) or (
422             isinstance(k, slice)
423             and (
424                 (k.start is not None and k.start < 0)
425                 or (k.stop is not None and k.stop < 0)
426             )
427         ):
428             raise ValueError("Negative indexing is not supported.")
429 
430         if self._result_cache is not None:
431             return self._result_cache[k]
432 
433         if isinstance(k, slice):
434             qs = self._chain()
435             if k.start is not None:
436                 start = int(k.start)
437             else:
438                 start = None
439             if k.stop is not None:
440                 stop = int(k.stop)
441             else:
442                 stop = None
443             qs.query.set_limits(start, stop)
444             return list(qs)[:: k.step] if k.step else qs
445 
446         qs = self._chain()
447         qs.query.set_limits(k, k + 1)
448         qs._fetch_all()
449         return qs._result_cache[0]
450 
451     def __class_getitem__(cls, *args, **kwargs):
452         return cls
453 
454     def __and__(self, other):
455         self._check_operator_queryset(other, "&")
456         self._merge_sanity_check(other)
457         if isinstance(other, EmptyQuerySet):
458             return other
459         if isinstance(self, EmptyQuerySet):
460             return self
461         combined = self._chain()
462         combined._merge_known_related_objects(other)
463         combined.query.combine(other.query, sql.AND)
464         return combined
465 
466     def __or__(self, other):
467         self._check_operator_queryset(other, "|")
468         self._merge_sanity_check(other)
469         if isinstance(self, EmptyQuerySet):
470             return other
471         if isinstance(other, EmptyQuerySet):
472             return self
473         query = (
474             self
475             if self.query.can_filter()
476             else self.model._base_manager.filter(pk__in=self.values("pk"))
477         )
478         combined = query._chain()
479         combined._merge_known_related_objects(other)
480         if not other.query.can_filter():
481             other = other.model._base_manager.filter(pk__in=other.values("pk"))
482         combined.query.combine(other.query, sql.OR)
483         return combined
484 
485     def __xor__(self, other):
486         self._check_operator_queryset(other, "^")
487         self._merge_sanity_check(other)
488         if isinstance(self, EmptyQuerySet):
489             return other
490         if isinstance(other, EmptyQuerySet):
491             return self
492         query = (
493             self
494             if self.query.can_filter()
495             else self.model._base_manager.filter(pk__in=self.values("pk"))
496         )
497         combined = query._chain()
498         combined._merge_known_related_objects(other)
499         if not other.query.can_filter():
500             other = other.model._base_manager.filter(pk__in=other.values("pk"))
501         combined.query.combine(other.query, sql.XOR)
502         return combined
503 
504     ####################################
505     # METHODS THAT DO DATABASE QUERIES #
506     ####################################
507 
508     def _iterator(self, use_chunked_fetch, chunk_size):
509         iterable = self._iterable_class(
510             self,
511             chunked_fetch=use_chunked_fetch,
512             chunk_size=chunk_size or 2000,
513         )
514         if not self._prefetch_related_lookups or chunk_size is None:
515             yield from iterable
516             return
517 
518         iterator = iter(iterable)
519         while results := list(islice(iterator, chunk_size)):
520             prefetch_related_objects(results, *self._prefetch_related_lookups)
521             yield from results
522 
523     def iterator(self, chunk_size=None):
524         """
525         An iterator over the results from applying this QuerySet to the
526         database. chunk_size must be provided for QuerySets that prefetch
527         related objects. Otherwise, a default chunk_size of 2000 is supplied.
528         """
529         if chunk_size is None:
530             if self._prefetch_related_lookups:
531                 raise ValueError(
532                     "chunk_size must be provided when using QuerySet.iterator() after "
533                     "prefetch_related()."
534                 )
535         elif chunk_size <= 0:
536             raise ValueError("Chunk size must be strictly positive.")
537         use_chunked_fetch = not connections[self.db].settings_dict.get(
538             "DISABLE_SERVER_SIDE_CURSORS"
539         )
540         return self._iterator(use_chunked_fetch, chunk_size)
541 
542     async def aiterator(self, chunk_size=2000):
543         """
544         An asynchronous iterator over the results from applying this QuerySet
545         to the database.
546         """
547         if self._prefetch_related_lookups:
548             raise NotSupportedError(
549                 "Using QuerySet.aiterator() after prefetch_related() is not supported."
550             )
551         if chunk_size <= 0:
552             raise ValueError("Chunk size must be strictly positive.")
553         use_chunked_fetch = not connections[self.db].settings_dict.get(
554             "DISABLE_SERVER_SIDE_CURSORS"
555         )
556         async for item in self._iterable_class(
557             self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size
558         ):
559             yield item
560 
561     def aggregate(self, *args, **kwargs):
562         """
563         Return a dictionary containing the calculations (aggregation)
564         over the current queryset.
565 
566         If args is present the expression is passed as a kwarg using
567         the Aggregate object's default alias.
568         """
569         if self.query.distinct_fields:
570             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
571         self._validate_values_are_expressions(
572             (*args, *kwargs.values()), method_name="aggregate"
573         )
574         for arg in args:
575             # The default_alias property raises TypeError if default_alias
576             # can't be set automatically or AttributeError if it isn't an
577             # attribute.
578             try:
579                 arg.default_alias
580             except (AttributeError, TypeError):
581                 raise TypeError("Complex aggregates require an alias")
582             kwargs[arg.default_alias] = arg
583 
584         return self.query.chain().get_aggregation(self.db, kwargs)
585 
586     async def aaggregate(self, *args, **kwargs):
587         return await sync_to_async(self.aggregate)(*args, **kwargs)
588 
589     def count(self):
590         """
591         Perform a SELECT COUNT() and return the number of records as an
592         integer.
593 
594         If the QuerySet is already fully cached, return the length of the
595         cached results set to avoid multiple SELECT COUNT(*) calls.
596         """
597         if self._result_cache is not None:
598             return len(self._result_cache)
599 
600         return self.query.get_count(using=self.db)
601 
602     async def acount(self):
603         return await sync_to_async(self.count)()
604 
605     def get(self, *args, **kwargs):
606         """
607         Perform the query and return a single object matching the given
608         keyword arguments.
609         """
610         if self.query.combinator and (args or kwargs):
611             raise NotSupportedError(
612                 "Calling QuerySet.get(...) with filters after %s() is not "
613                 "supported." % self.query.combinator
614             )
615         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
616         if self.query.can_filter() and not self.query.distinct_fields:
617             clone = clone.order_by()
618         limit = None
619         if (
620             not clone.query.select_for_update
621             or connections[clone.db].features.supports_select_for_update_with_limit
622         ):
623             limit = MAX_GET_RESULTS
624             clone.query.set_limits(high=limit)
625         num = len(clone)
626         if num == 1:
627             return clone._result_cache[0]
628         if not num:
629             raise self.model.DoesNotExist(
630                 "%s matching query does not exist." % self.model._meta.object_name
631             )
632         raise self.model.MultipleObjectsReturned(
633             "get() returned more than one %s -- it returned %s!"
634             % (
635                 self.model._meta.object_name,
636                 num if not limit or num < limit else "more than %s" % (limit - 1),
637             )
638         )
639 
640     async def aget(self, *args, **kwargs):
641         return await sync_to_async(self.get)(*args, **kwargs)
642 
643     def create(self, **kwargs):
644         """
645         Create a new object with the given kwargs, saving it to the database
646         and returning the created object.
647         """
648         obj = self.model(**kwargs)
649         self._for_write = True
650         obj.save(force_insert=True, using=self.db)
651         return obj
652 
653     async def acreate(self, **kwargs):
654         return await sync_to_async(self.create)(**kwargs)
655 
656     def _prepare_for_bulk_create(self, objs):
657         from django.db.models.expressions import DatabaseDefault
658 
659         connection = connections[self.db]
660         for obj in objs:
661             if obj.pk is None:
662                 # Populate new PK values.
663                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
664             if not connection.features.supports_default_keyword_in_bulk_insert:
665                 for field in obj._meta.fields:
666                     value = getattr(obj, field.attname)
667                     if isinstance(value, DatabaseDefault):
668                         setattr(obj, field.attname, field.db_default)
669 
670             obj._prepare_related_fields_for_save(operation_name="bulk_create")
671 
672     def _check_bulk_create_options(
673         self, ignore_conflicts, update_conflicts, update_fields, unique_fields
674     ):
675         if ignore_conflicts and update_conflicts:
676             raise ValueError(
677                 "ignore_conflicts and update_conflicts are mutually exclusive."
678             )
679         db_features = connections[self.db].features
680         if ignore_conflicts:
681             if not db_features.supports_ignore_conflicts:
682                 raise NotSupportedError(
683                     "This database backend does not support ignoring conflicts."
684                 )
685             return OnConflict.IGNORE
686         elif update_conflicts:
687             if not db_features.supports_update_conflicts:
688                 raise NotSupportedError(
689                     "This database backend does not support updating conflicts."
690                 )
691             if not update_fields:
692                 raise ValueError(
693                     "Fields that will be updated when a row insertion fails "
694                     "on conflicts must be provided."
695                 )
696             if unique_fields and not db_features.supports_update_conflicts_with_target:
697                 raise NotSupportedError(
698                     "This database backend does not support updating "
699                     "conflicts with specifying unique fields that can trigger "
700                     "the upsert."
701                 )
702             if not unique_fields and db_features.supports_update_conflicts_with_target:
703                 raise ValueError(
704                     "Unique fields that can trigger the upsert must be provided."
705                 )
706             # Updating primary keys and non-concrete fields is forbidden.
707             if any(not f.concrete or f.many_to_many for f in update_fields):
708                 raise ValueError(
709                     "bulk_create() can only be used with concrete fields in "
710                     "update_fields."
711                 )
712             if any(f.primary_key for f in update_fields):
713                 raise ValueError(
714                     "bulk_create() cannot be used with primary keys in "
715                     "update_fields."
716                 )
717             if unique_fields:
718                 if any(not f.concrete or f.many_to_many for f in unique_fields):
719                     raise ValueError(
720                         "bulk_create() can only be used with concrete fields "
721                         "in unique_fields."
722                     )
723             return OnConflict.UPDATE
724         return None
725 
726     def bulk_create(
727         self,
728         objs,
729         batch_size=None,
730         ignore_conflicts=False,
731         update_conflicts=False,
732         update_fields=None,
733         unique_fields=None,
734     ):
735         """
736         Insert each of the instances into the database. Do *not* call
737         save() on each of the instances, do not send any pre/post_save
738         signals, and do not set the primary key attribute if it is an
739         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
740         Multi-table models are not supported.
741         """
742         # When you bulk insert you don't get the primary keys back (if it's an
743         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
744         # you can't insert into the child tables which references this. There
745         # are two workarounds:
746         # 1) This could be implemented if you didn't have an autoincrement pk
747         # 2) You could do it by doing O(n) normal inserts into the parent
748         #    tables to get the primary keys back and then doing a single bulk
749         #    insert into the childmost table.
750         # We currently set the primary keys on the objects when using
751         # PostgreSQL via the RETURNING ID clause. It should be possible for
752         # Oracle as well, but the semantics for extracting the primary keys is
753         # trickier so it's not done yet.
754         if batch_size is not None and batch_size <= 0:
755             raise ValueError("Batch size must be a positive integer.")
756         # Check that the parents share the same concrete model with the our
757         # model to detect the inheritance pattern ConcreteGrandParent ->
758         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
759         # would not identify that case as involving multiple tables.
760         for parent in self.model._meta.get_parent_list():
761             if parent._meta.concrete_model is not self.model._meta.concrete_model:
762                 raise ValueError("Can't bulk create a multi-table inherited model")
763         if not objs:
764             return objs
765         opts = self.model._meta
766         if unique_fields:
767             # Primary key is allowed in unique_fields.
768             unique_fields = [
769                 self.model._meta.get_field(opts.pk.name if name == "pk" else name)
770                 for name in unique_fields
771             ]
772         if update_fields:
773             update_fields = [self.model._meta.get_field(name) for name in update_fields]
774         on_conflict = self._check_bulk_create_options(
775             ignore_conflicts,
776             update_conflicts,
777             update_fields,
778             unique_fields,
779         )
780         self._for_write = True
781         fields = opts.concrete_fields
782         objs = list(objs)
783         self._prepare_for_bulk_create(objs)
784         with transaction.atomic(using=self.db, savepoint=False):
785             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
786             if objs_with_pk:
787                 returned_columns = self._batched_insert(
788                     objs_with_pk,
789                     fields,
790                     batch_size,
791                     on_conflict=on_conflict,
792                     update_fields=update_fields,
793                     unique_fields=unique_fields,
794                 )
795                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
796                     for result, field in zip(results, opts.db_returning_fields):
797                         if field != opts.pk:
798                             setattr(obj_with_pk, field.attname, result)
799                 for obj_with_pk in objs_with_pk:
800                     obj_with_pk._state.adding = False
801                     obj_with_pk._state.db = self.db
802             if objs_without_pk:
803                 fields = [f for f in fields if not isinstance(f, AutoField)]
804                 returned_columns = self._batched_insert(
805                     objs_without_pk,
806                     fields,
807                     batch_size,
808                     on_conflict=on_conflict,
809                     update_fields=update_fields,
810                     unique_fields=unique_fields,
811                 )
812                 connection = connections[self.db]
813                 if (
814                     connection.features.can_return_rows_from_bulk_insert
815                     and on_conflict is None
816                 ):
817                     assert len(returned_columns) == len(objs_without_pk)
818                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
819                     for result, field in zip(results, opts.db_returning_fields):
820                         setattr(obj_without_pk, field.attname, result)
821                     obj_without_pk._state.adding = False
822                     obj_without_pk._state.db = self.db
823 
824         return objs
825 
826     async def abulk_create(
827         self,
828         objs,
829         batch_size=None,
830         ignore_conflicts=False,
831         update_conflicts=False,
832         update_fields=None,
833         unique_fields=None,
834     ):
835         return await sync_to_async(self.bulk_create)(
836             objs=objs,
837             batch_size=batch_size,
838             ignore_conflicts=ignore_conflicts,
839             update_conflicts=update_conflicts,
840             update_fields=update_fields,
841             unique_fields=unique_fields,
842         )
843 
844     def bulk_update(self, objs, fields, batch_size=None):
845         """
846         Update the given fields in each of the given objects in the database.
847         """
848         if batch_size is not None and batch_size <= 0:
849             raise ValueError("Batch size must be a positive integer.")
850         if not fields:
851             raise ValueError("Field names must be given to bulk_update().")
852         objs = tuple(objs)
853         if any(obj.pk is None for obj in objs):
854             raise ValueError("All bulk_update() objects must have a primary key set.")
855         fields = [self.model._meta.get_field(name) for name in fields]
856         if any(not f.concrete or f.many_to_many for f in fields):
857             raise ValueError("bulk_update() can only be used with concrete fields.")
858         if any(f.primary_key for f in fields):
859             raise ValueError("bulk_update() cannot be used with primary key fields.")
860         if not objs:
861             return 0
862         for obj in objs:
863             obj._prepare_related_fields_for_save(
864                 operation_name="bulk_update", fields=fields
865             )
866         # PK is used twice in the resulting update query, once in the filter
867         # and once in the WHEN. Each field will also have one CAST.
868         self._for_write = True
869         connection = connections[self.db]
870         max_batch_size = connection.ops.bulk_batch_size(["pk", "pk"] + fields, objs)
871         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
872         requires_casting = connection.features.requires_casted_case_in_updates
873         batches = (objs[i : i + batch_size] for i in range(0, len(objs), batch_size))
874         updates = []
875         for batch_objs in batches:
876             update_kwargs = {}
877             for field in fields:
878                 when_statements = []
879                 for obj in batch_objs:
880                     attr = getattr(obj, field.attname)
881                     if not hasattr(attr, "resolve_expression"):
882                         attr = Value(attr, output_field=field)
883                     when_statements.append(When(pk=obj.pk, then=attr))
884                 case_statement = Case(*when_statements, output_field=field)
885                 if requires_casting:
886                     case_statement = Cast(case_statement, output_field=field)
887                 update_kwargs[field.attname] = case_statement
888             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
889         rows_updated = 0
890         queryset = self.using(self.db)
891         with transaction.atomic(using=self.db, savepoint=False):
892             for pks, update_kwargs in updates:
893                 rows_updated += queryset.filter(pk__in=pks).update(**update_kwargs)
894         return rows_updated
895 
896     bulk_update.alters_data = True
897 
898     async def abulk_update(self, objs, fields, batch_size=None):
899         return await sync_to_async(self.bulk_update)(
900             objs=objs,
901             fields=fields,
902             batch_size=batch_size,
903         )
904 
905     abulk_update.alters_data = True
906 
907     def get_or_create(self, defaults=None, **kwargs):
908         """
909         Look up an object with the given kwargs, creating one if necessary.
910         Return a tuple of (object, created), where created is a boolean
911         specifying whether an object was created.
912         """
913         # The get() needs to be targeted at the write database in order
914         # to avoid potential transaction consistency problems.
915         self._for_write = True
916         try:
917             return self.get(**kwargs), False
918         except self.model.DoesNotExist:
919             params = self._extract_model_params(defaults, **kwargs)
920             # Try to create an object using passed params.
921             try:
922                 with transaction.atomic(using=self.db):
923                     params = dict(resolve_callables(params))
924                     return self.create(**params), True
925             except IntegrityError:
926                 try:
927                     return self.get(**kwargs), False
928                 except self.model.DoesNotExist:
929                     pass
930                 raise
931 
932     async def aget_or_create(self, defaults=None, **kwargs):
933         return await sync_to_async(self.get_or_create)(
934             defaults=defaults,
935             **kwargs,
936         )
937 
938     def update_or_create(self, defaults=None, create_defaults=None, **kwargs):
939         """
940         Look up an object with the given kwargs, updating one with defaults
941         if it exists, otherwise create a new one. Optionally, an object can
942         be created with different values than defaults by using
943         create_defaults.
944         Return a tuple (object, created), where created is a boolean
945         specifying whether an object was created.
946         """
947         if create_defaults is None:
948             update_defaults = create_defaults = defaults or {}
949         else:
950             update_defaults = defaults or {}
951         self._for_write = True
952         with transaction.atomic(using=self.db):
953             # Lock the row so that a concurrent update is blocked until
954             # update_or_create() has performed its save.
955             obj, created = self.select_for_update().get_or_create(
956                 create_defaults, **kwargs
957             )
958             if created:
959                 return obj, created
960             for k, v in resolve_callables(update_defaults):
961                 setattr(obj, k, v)
962 
963             update_fields = set(update_defaults)
964             concrete_field_names = self.model._meta._non_pk_concrete_field_names
965             # update_fields does not support non-concrete fields.
966             if concrete_field_names.issuperset(update_fields):
967                 # Add fields which are set on pre_save(), e.g. auto_now fields.
968                 # This is to maintain backward compatibility as these fields
969                 # are not updated unless explicitly specified in the
970                 # update_fields list.
971                 for field in self.model._meta.local_concrete_fields:
972                     if not (
973                         field.primary_key or field.__class__.pre_save is Field.pre_save
974                     ):
975                         update_fields.add(field.name)
976                         if field.name != field.attname:
977                             update_fields.add(field.attname)
978                 obj.save(using=self.db, update_fields=update_fields)
979             else:
980                 obj.save(using=self.db)
981         return obj, False
982 
983     async def aupdate_or_create(self, defaults=None, create_defaults=None, **kwargs):
984         return await sync_to_async(self.update_or_create)(
985             defaults=defaults,
986             create_defaults=create_defaults,
987             **kwargs,
988         )
989 
990     def _extract_model_params(self, defaults, **kwargs):
991         """
992         Prepare `params` for creating a model instance based on the given
993         kwargs; for use by get_or_create().
994         """
995         defaults = defaults or {}
996         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
997         params.update(defaults)
998         property_names = self.model._meta._property_names
999         invalid_params = []
1000         for param in params:
1001             try:
1002                 self.model._meta.get_field(param)
1003             except exceptions.FieldDoesNotExist:
1004                 # It's okay to use a model's property if it has a setter.
1005                 if not (param in property_names and getattr(self.model, param).fset):
1006                     invalid_params.append(param)
1007         if invalid_params:
1008             raise exceptions.FieldError(
1009                 "Invalid field name(s) for model %s: '%s'."
1010                 % (
1011                     self.model._meta.object_name,
1012                     "', '".join(sorted(invalid_params)),
1013                 )
1014             )
1015         return params
1016 
1017     def _earliest(self, *fields):
1018         """
1019         Return the earliest object according to fields (if given) or by the
1020         model's Meta.get_latest_by.
1021         """
1022         if fields:
1023             order_by = fields
1024         else:
1025             order_by = getattr(self.model._meta, "get_latest_by")
1026             if order_by and not isinstance(order_by, (tuple, list)):
1027                 order_by = (order_by,)
1028         if order_by is None:
1029             raise ValueError(
1030                 "earliest() and latest() require either fields as positional "
1031                 "arguments or 'get_latest_by' in the model's Meta."
1032             )
1033         obj = self._chain()
1034         obj.query.set_limits(high=1)
1035         obj.query.clear_ordering(force=True)
1036         obj.query.add_ordering(*order_by)
1037         return obj.get()
1038 
1039     def earliest(self, *fields):
1040         if self.query.is_sliced:
1041             raise TypeError("Cannot change a query once a slice has been taken.")
1042         return self._earliest(*fields)
1043 
1044     async def aearliest(self, *fields):
1045         return await sync_to_async(self.earliest)(*fields)
1046 
1047     def latest(self, *fields):
1048         """
1049         Return the latest object according to fields (if given) or by the
1050         model's Meta.get_latest_by.
1051         """
1052         if self.query.is_sliced:
1053             raise TypeError("Cannot change a query once a slice has been taken.")
1054         return self.reverse()._earliest(*fields)
1055 
1056     async def alatest(self, *fields):
1057         return await sync_to_async(self.latest)(*fields)
1058 
1059     def first(self):
1060         """Return the first object of a query or None if no match is found."""
1061         if self.ordered:
1062             queryset = self
1063         else:
1064             self._check_ordering_first_last_queryset_aggregation(method="first")
1065             queryset = self.order_by("pk")
1066         for obj in queryset[:1]:
1067             return obj
1068 
1069     async def afirst(self):
1070         return await sync_to_async(self.first)()
1071 
1072     def last(self):
1073         """Return the last object of a query or None if no match is found."""
1074         if self.ordered:
1075             queryset = self.reverse()
1076         else:
1077             self._check_ordering_first_last_queryset_aggregation(method="last")
1078             queryset = self.order_by("-pk")
1079         for obj in queryset[:1]:
1080             return obj
1081 
1082     async def alast(self):
1083         return await sync_to_async(self.last)()
1084 
1085     def in_bulk(self, id_list=None, *, field_name="pk"):
1086         """
1087         Return a dictionary mapping each of the given IDs to the object with
1088         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
1089         """
1090         if self.query.is_sliced:
1091             raise TypeError("Cannot use 'limit' or 'offset' with in_bulk().")
1092         opts = self.model._meta
1093         unique_fields = [
1094             constraint.fields[0]
1095             for constraint in opts.total_unique_constraints
1096             if len(constraint.fields) == 1
1097         ]
1098         if (
1099             field_name != "pk"
1100             and not opts.get_field(field_name).unique
1101             and field_name not in unique_fields
1102             and self.query.distinct_fields != (field_name,)
1103         ):
1104             raise ValueError(
1105                 "in_bulk()'s field_name must be a unique field but %r isn't."
1106                 % field_name
1107             )
1108         if id_list is not None:
1109             if not id_list:
1110                 return {}
1111             filter_key = "{}__in".format(field_name)
1112             batch_size = connections[self.db].features.max_query_params
1113             id_list = tuple(id_list)
1114             # If the database has a limit on the number of query parameters
1115             # (e.g. SQLite), retrieve objects in batches if necessary.
1116             if batch_size and batch_size < len(id_list):
1117                 qs = ()
1118                 for offset in range(0, len(id_list), batch_size):
1119                     batch = id_list[offset : offset + batch_size]
1120                     qs += tuple(self.filter(**{filter_key: batch}))
1121             else:
1122                 qs = self.filter(**{filter_key: id_list})
1123         else:
1124             qs = self._chain()
1125         return {getattr(obj, field_name): obj for obj in qs}
1126 
1127     async def ain_bulk(self, id_list=None, *, field_name="pk"):
1128         return await sync_to_async(self.in_bulk)(
1129             id_list=id_list,
1130             field_name=field_name,
1131         )
1132 
1133     def delete(self):
1134         """Delete the records in the current QuerySet."""
1135         self._not_support_combined_queries("delete")
1136         if self.query.is_sliced:
1137             raise TypeError("Cannot use 'limit' or 'offset' with delete().")
1138         if self.query.distinct or self.query.distinct_fields:
1139             raise TypeError("Cannot call delete() after .distinct().")
1140         if self._fields is not None:
1141             raise TypeError("Cannot call delete() after .values() or .values_list()")
1142 
1143         del_query = self._chain()
1144 
1145         # The delete is actually 2 queries - one to find related objects,
1146         # and one to delete. Make sure that the discovery of related
1147         # objects is performed on the same database as the deletion.
1148         del_query._for_write = True
1149 
1150         # Disable non-supported fields.
1151         del_query.query.select_for_update = False
1152         del_query.query.select_related = False
1153         del_query.query.clear_ordering(force=True)
1154 
1155         collector = Collector(using=del_query.db, origin=self)
1156         collector.collect(del_query)
1157         deleted, _rows_count = collector.delete()
1158 
1159         # Clear the result cache, in case this QuerySet gets reused.
1160         self._result_cache = None
1161         return deleted, _rows_count
1162 
1163     delete.alters_data = True
1164     delete.queryset_only = True
1165 
1166     async def adelete(self):
1167         return await sync_to_async(self.delete)()
1168 
1169     adelete.alters_data = True
1170     adelete.queryset_only = True
1171 
1172     def _raw_delete(self, using):
1173         """
1174         Delete objects found from the given queryset in single direct SQL
1175         query. No signals are sent and there is no protection for cascades.
1176         """
1177         query = self.query.clone()
1178         query.__class__ = sql.DeleteQuery
1179         cursor = query.get_compiler(using).execute_sql(CURSOR)
1180         if cursor:
1181             with cursor:
1182                 return cursor.rowcount
1183         return 0
1184 
1185     _raw_delete.alters_data = True
1186 
1187     def update(self, **kwargs):
1188         """
1189         Update all elements in the current QuerySet, setting all the given
1190         fields to the appropriate values.
1191         """
1192         self._not_support_combined_queries("update")
1193         if self.query.is_sliced:
1194             raise TypeError("Cannot update a query once a slice has been taken.")
1195         self._for_write = True
1196         query = self.query.chain(sql.UpdateQuery)
1197         query.add_update_values(kwargs)
1198 
1199         # Inline annotations in order_by(), if possible.
1200         new_order_by = []
1201         for col in query.order_by:
1202             alias = col
1203             descending = False
1204             if isinstance(alias, str) and alias.startswith("-"):
1205                 alias = alias.removeprefix("-")
1206                 descending = True
1207             if annotation := query.annotations.get(alias):
1208                 if getattr(annotation, "contains_aggregate", False):
1209                     raise exceptions.FieldError(
1210                         f"Cannot update when ordering by an aggregate: {annotation}"
1211                     )
1212                 if descending:
1213                     annotation = annotation.desc()
1214                 new_order_by.append(annotation)
1215             else:
1216                 new_order_by.append(col)
1217         query.order_by = tuple(new_order_by)
1218 
1219         # Clear any annotations so that they won't be present in subqueries.
1220         query.annotations = {}
1221         with transaction.mark_for_rollback_on_error(using=self.db):
1222             rows = query.get_compiler(self.db).execute_sql(CURSOR)
1223         self._result_cache = None
1224         return rows
1225 
1226     update.alters_data = True
1227 
1228     async def aupdate(self, **kwargs):
1229         return await sync_to_async(self.update)(**kwargs)
1230 
1231     aupdate.alters_data = True
1232 
1233     def _update(self, values):
1234         """
1235         A version of update() that accepts field objects instead of field names.
1236         Used primarily for model saving and not intended for use by general
1237         code (it requires too much poking around at model internals to be
1238         useful at that level).
1239         """
1240         if self.query.is_sliced:
1241             raise TypeError("Cannot update a query once a slice has been taken.")
1242         query = self.query.chain(sql.UpdateQuery)
1243         query.add_update_fields(values)
1244         # Clear any annotations so that they won't be present in subqueries.
1245         query.annotations = {}
1246         self._result_cache = None
1247         return query.get_compiler(self.db).execute_sql(CURSOR)
1248 
1249     _update.alters_data = True
1250     _update.queryset_only = False
1251 
1252     def exists(self):
1253         """
1254         Return True if the QuerySet would have any results, False otherwise.
1255         """
1256         if self._result_cache is None:
1257             return self.query.has_results(using=self.db)
1258         return bool(self._result_cache)
1259 
1260     async def aexists(self):
1261         return await sync_to_async(self.exists)()
1262 
1263     def contains(self, obj):
1264         """
1265         Return True if the QuerySet contains the provided obj,
1266         False otherwise.
1267         """
1268         self._not_support_combined_queries("contains")
1269         if self._fields is not None:
1270             raise TypeError(
1271                 "Cannot call QuerySet.contains() after .values() or .values_list()."
1272             )
1273         try:
1274             if obj._meta.concrete_model != self.model._meta.concrete_model:
1275                 return False
1276         except AttributeError:
1277             raise TypeError("'obj' must be a model instance.")
1278         if obj.pk is None:
1279             raise ValueError("QuerySet.contains() cannot be used on unsaved objects.")
1280         if self._result_cache is not None:
1281             return obj in self._result_cache
1282         return self.filter(pk=obj.pk).exists()
1283 
1284     async def acontains(self, obj):
1285         return await sync_to_async(self.contains)(obj=obj)
1286 
1287     def _prefetch_related_objects(self):
1288         # This method can only be called once the result cache has been filled.
1289         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
1290         self._prefetch_done = True
1291 
1292     def explain(self, *, format=None, **options):
1293         """
1294         Runs an EXPLAIN on the SQL query this QuerySet would perform, and
1295         returns the results.
1296         """
1297         return self.query.explain(using=self.db, format=format, **options)
1298 
1299     async def aexplain(self, *, format=None, **options):
1300         return await sync_to_async(self.explain)(format=format, **options)
1301 
1302     ##################################################
1303     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
1304     ##################################################
1305 
1306     def raw(self, raw_query, params=(), translations=None, using=None):
1307         if using is None:
1308             using = self.db
1309         qs = RawQuerySet(
1310             raw_query,
1311             model=self.model,
1312             params=params,
1313             translations=translations,
1314             using=using,
1315         )
1316         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
1317         return qs
1318 
1319     def _values(self, *fields, **expressions):
1320         clone = self._chain()
1321         if expressions:
1322             clone = clone.annotate(**expressions)
1323         clone._fields = fields
1324         clone.query.set_values(fields)
1325         return clone
1326 
1327     def values(self, *fields, **expressions):
1328         fields += tuple(expressions)
1329         clone = self._values(*fields, **expressions)
1330         clone._iterable_class = ValuesIterable
1331         return clone
1332 
1333     def values_list(self, *fields, flat=False, named=False):
1334         if flat and named:
1335             raise TypeError("'flat' and 'named' can't be used together.")
1336         if flat and len(fields) > 1:
1337             raise TypeError(
1338                 "'flat' is not valid when values_list is called with more than one "
1339                 "field."
1340             )
1341 
1342         field_names = {f for f in fields if not hasattr(f, "resolve_expression")}
1343         _fields = []
1344         expressions = {}
1345         counter = 1
1346         for field in fields:
1347             if hasattr(field, "resolve_expression"):
1348                 field_id_prefix = getattr(
1349                     field, "default_alias", field.__class__.__name__.lower()
1350                 )
1351                 while True:
1352                     field_id = field_id_prefix + str(counter)
1353                     counter += 1
1354                     if field_id not in field_names:
1355                         break
1356                 expressions[field_id] = field
1357                 _fields.append(field_id)
1358             else:
1359                 _fields.append(field)
1360 
1361         clone = self._values(*_fields, **expressions)
1362         clone._iterable_class = (
1363             NamedValuesListIterable
1364             if named
1365             else FlatValuesListIterable
1366             if flat
1367             else ValuesListIterable
1368         )
1369         return clone
1370 
1371     def dates(self, field_name, kind, order="ASC"):
1372         """
1373         Return a list of date objects representing all available dates for
1374         the given field_name, scoped to 'kind'.
1375         """
1376         if kind not in ("year", "month", "week", "day"):
1377             raise ValueError("'kind' must be one of 'year', 'month', 'week', or 'day'.")
1378         if order not in ("ASC", "DESC"):
1379             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
1380         return (
1381             self.annotate(
1382                 datefield=Trunc(field_name, kind, output_field=DateField()),
1383                 plain_field=F(field_name),
1384             )
1385             .values_list("datefield", flat=True)
1386             .distinct()
1387             .filter(plain_field__isnull=False)
1388             .order_by(("-" if order == "DESC" else "") + "datefield")
1389         )
1390 
1391     def datetimes(self, field_name, kind, order="ASC", tzinfo=None):
1392         """
1393         Return a list of datetime objects representing all available
1394         datetimes for the given field_name, scoped to 'kind'.
1395         """
1396         if kind not in ("year", "month", "week", "day", "hour", "minute", "second"):
1397             raise ValueError(
1398                 "'kind' must be one of 'year', 'month', 'week', 'day', "
1399                 "'hour', 'minute', or 'second'."
1400             )
1401         if order not in ("ASC", "DESC"):
1402             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
1403         if settings.USE_TZ:
1404             if tzinfo is None:
1405                 tzinfo = timezone.get_current_timezone()
1406         else:
1407             tzinfo = None
1408         return (
1409             self.annotate(
1410                 datetimefield=Trunc(
1411                     field_name,
1412                     kind,
1413                     output_field=DateTimeField(),
1414                     tzinfo=tzinfo,
1415                 ),
1416                 plain_field=F(field_name),
1417             )
1418             .values_list("datetimefield", flat=True)
1419             .distinct()
1420             .filter(plain_field__isnull=False)
1421             .order_by(("-" if order == "DESC" else "") + "datetimefield")
1422         )
1423 
1424     def none(self):
1425         """Return an empty QuerySet."""
1426         clone = self._chain()
1427         clone.query.set_empty()
1428         return clone
1429 
1430     ##################################################################
1431     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
1432     ##################################################################
1433 
1434     def all(self):
1435         """
1436         Return a new QuerySet that is a copy of the current one. This allows a
1437         QuerySet to proxy for a model manager in some cases.
1438         """
1439         return self._chain()
1440 
1441     def filter(self, *args, **kwargs):
1442         """
1443         Return a new QuerySet instance with the args ANDed to the existing
1444         set.
1445         """
1446         self._not_support_combined_queries("filter")
1447         return self._filter_or_exclude(False, args, kwargs)
1448 
1449     def exclude(self, *args, **kwargs):
1450         """
1451         Return a new QuerySet instance with NOT (args) ANDed to the existing
1452         set.
1453         """
1454         self._not_support_combined_queries("exclude")
1455         return self._filter_or_exclude(True, args, kwargs)
1456 
1457     def _filter_or_exclude(self, negate, args, kwargs):
1458         if (args or kwargs) and self.query.is_sliced:
1459             raise TypeError("Cannot filter a query once a slice has been taken.")
1460         clone = self._chain()
1461         if self._defer_next_filter:
1462             self._defer_next_filter = False
1463             clone._deferred_filter = negate, args, kwargs
1464         else:
1465             clone._filter_or_exclude_inplace(negate, args, kwargs)
1466         return clone
1467 
1468     def _filter_or_exclude_inplace(self, negate, args, kwargs):
1469         if negate:
1470             self._query.add_q(~Q(*args, **kwargs))
1471         else:
1472             self._query.add_q(Q(*args, **kwargs))
1473 
1474     def complex_filter(self, filter_obj):
1475         """
1476         Return a new QuerySet instance with filter_obj added to the filters.
1477 
1478         filter_obj can be a Q object or a dictionary of keyword lookup
1479         arguments.
1480 
1481         This exists to support framework features such as 'limit_choices_to',
1482         and usually it will be more natural to use other methods.
1483         """
1484         if isinstance(filter_obj, Q):
1485             clone = self._chain()
1486             clone.query.add_q(filter_obj)
1487             return clone
1488         else:
1489             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
1490 
1491     def _combinator_query(self, combinator, *other_qs, all=False):
1492         # Clone the query to inherit the select list and everything
1493         clone = self._chain()
1494         # Clear limits and ordering so they can be reapplied
1495         clone.query.clear_ordering(force=True)
1496         clone.query.clear_limits()
1497         clone.query.combined_queries = (self.query,) + tuple(
1498             qs.query for qs in other_qs
1499         )
1500         clone.query.combinator = combinator
1501         clone.query.combinator_all = all
1502         return clone
1503 
1504     def union(self, *other_qs, all=False):
1505         # If the query is an EmptyQuerySet, combine all nonempty querysets.
1506         if isinstance(self, EmptyQuerySet):
1507             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1508             if not qs:
1509                 return self
1510             if len(qs) == 1:
1511                 return qs[0]
1512             return qs[0]._combinator_query("union", *qs[1:], all=all)
1513         return self._combinator_query("union", *other_qs, all=all)
1514 
1515     def intersection(self, *other_qs):
1516         # If any query is an EmptyQuerySet, return it.
1517         if isinstance(self, EmptyQuerySet):
1518             return self
1519         for other in other_qs:
1520             if isinstance(other, EmptyQuerySet):
1521                 return other
1522         return self._combinator_query("intersection", *other_qs)
1523 
1524     def difference(self, *other_qs):
1525         # If the query is an EmptyQuerySet, return it.
1526         if isinstance(self, EmptyQuerySet):
1527             return self
1528         return self._combinator_query("difference", *other_qs)
1529 
1530     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1531         """
1532         Return a new QuerySet instance that will select objects with a
1533         FOR UPDATE lock.
1534         """
1535         if nowait and skip_locked:
1536             raise ValueError("The nowait option cannot be used with skip_locked.")
1537         obj = self._chain()
1538         obj._for_write = True
1539         obj.query.select_for_update = True
1540         obj.query.select_for_update_nowait = nowait
1541         obj.query.select_for_update_skip_locked = skip_locked
1542         obj.query.select_for_update_of = of
1543         obj.query.select_for_no_key_update = no_key
1544         return obj
1545 
1546     def select_related(self, *fields):
1547         """
1548         Return a new QuerySet instance that will select related objects.
1549 
1550         If fields are specified, they must be ForeignKey fields and only those
1551         related objects are included in the selection.
1552 
1553         If select_related(None) is called, clear the list.
1554         """
1555         self._not_support_combined_queries("select_related")
1556         if self._fields is not None:
1557             raise TypeError(
1558                 "Cannot call select_related() after .values() or .values_list()"
1559             )
1560 
1561         obj = self._chain()
1562         if fields == (None,):
1563             obj.query.select_related = False
1564         elif fields:
1565             obj.query.add_select_related(fields)
1566         else:
1567             obj.query.select_related = True
1568         return obj
1569 
1570     def prefetch_related(self, *lookups):
1571         """
1572         Return a new QuerySet instance that will prefetch the specified
1573         Many-To-One and Many-To-Many related objects when the QuerySet is
1574         evaluated.
1575 
1576         When prefetch_related() is called more than once, append to the list of
1577         prefetch lookups. If prefetch_related(None) is called, clear the list.
1578         """
1579         self._not_support_combined_queries("prefetch_related")
1580         clone = self._chain()
1581         if lookups == (None,):
1582             clone._prefetch_related_lookups = ()
1583         else:
1584             for lookup in lookups:
1585                 if isinstance(lookup, Prefetch):
1586                     lookup = lookup.prefetch_to
1587                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1588                 if lookup in self.query._filtered_relations:
1589                     raise ValueError(
1590                         "prefetch_related() is not supported with FilteredRelation."
1591                     )
1592             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1593         return clone
1594 
1595     def annotate(self, *args, **kwargs):
1596         """
1597         Return a query set in which the returned objects have been annotated
1598         with extra data or aggregations.
1599         """
1600         self._not_support_combined_queries("annotate")
1601         return self._annotate(args, kwargs, select=True)
1602 
1603     def alias(self, *args, **kwargs):
1604         """
1605         Return a query set with added aliases for extra data or aggregations.
1606         """
1607         self._not_support_combined_queries("alias")
1608         return self._annotate(args, kwargs, select=False)
1609 
1610     def _annotate(self, args, kwargs, select=True):
1611         self._validate_values_are_expressions(
1612             args + tuple(kwargs.values()), method_name="annotate"
1613         )
1614         annotations = {}
1615         for arg in args:
1616             # The default_alias property may raise a TypeError.
1617             try:
1618                 if arg.default_alias in kwargs:
1619                     raise ValueError(
1620                         "The named annotation '%s' conflicts with the "
1621                         "default name for another annotation." % arg.default_alias
1622                     )
1623             except TypeError:
1624                 raise TypeError("Complex annotations require an alias")
1625             annotations[arg.default_alias] = arg
1626         annotations.update(kwargs)
1627 
1628         clone = self._chain()
1629         names = self._fields
1630         if names is None:
1631             names = set(
1632                 chain.from_iterable(
1633                     (field.name, field.attname)
1634                     if hasattr(field, "attname")
1635                     else (field.name,)
1636                     for field in self.model._meta.get_fields()
1637                 )
1638             )
1639 
1640         for alias, annotation in annotations.items():
1641             if alias in names:
1642                 raise ValueError(
1643                     "The annotation '%s' conflicts with a field on "
1644                     "the model." % alias
1645                 )
1646             if isinstance(annotation, FilteredRelation):
1647                 clone.query.add_filtered_relation(annotation, alias)
1648             else:
1649                 clone.query.add_annotation(
1650                     annotation,
1651                     alias,
1652                     select=select,
1653                 )
1654         for alias, annotation in clone.query.annotations.items():
1655             if alias in annotations and annotation.contains_aggregate:
1656                 if clone._fields is None:
1657                     clone.query.group_by = True
1658                 else:
1659                     clone.query.set_group_by()
1660                 break
1661 
1662         return clone
1663 
1664     def order_by(self, *field_names):
1665         """Return a new QuerySet instance with the ordering changed."""
1666         if self.query.is_sliced:
1667             raise TypeError("Cannot reorder a query once a slice has been taken.")
1668         obj = self._chain()
1669         obj.query.clear_ordering(force=True, clear_default=False)
1670         obj.query.add_ordering(*field_names)
1671         return obj
1672 
1673     def distinct(self, *field_names):
1674         """
1675         Return a new QuerySet instance that will select only distinct results.
1676         """
1677         self._not_support_combined_queries("distinct")
1678         if self.query.is_sliced:
1679             raise TypeError(
1680                 "Cannot create distinct fields once a slice has been taken."
1681             )
1682         obj = self._chain()
1683         obj.query.add_distinct_fields(*field_names)
1684         return obj
1685 
1686     def extra(
1687         self,
1688         select=None,
1689         where=None,
1690         params=None,
1691         tables=None,
1692         order_by=None,
1693         select_params=None,
1694     ):
1695         """Add extra SQL fragments to the query."""
1696         self._not_support_combined_queries("extra")
1697         if self.query.is_sliced:
1698             raise TypeError("Cannot change a query once a slice has been taken.")
1699         clone = self._chain()
1700         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1701         return clone
1702 
1703     def reverse(self):
1704         """Reverse the ordering of the QuerySet."""
1705         if self.query.is_sliced:
1706             raise TypeError("Cannot reverse a query once a slice has been taken.")
1707         clone = self._chain()
1708         clone.query.standard_ordering = not clone.query.standard_ordering
1709         return clone
1710 
1711     def defer(self, *fields):
1712         """
1713         Defer the loading of data for certain fields until they are accessed.
1714         Add the set of deferred fields to any existing set of deferred fields.
1715         The only exception to this is if None is passed in as the only
1716         parameter, in which case removal all deferrals.
1717         """
1718         self._not_support_combined_queries("defer")
1719         if self._fields is not None:
1720             raise TypeError("Cannot call defer() after .values() or .values_list()")
1721         clone = self._chain()
1722         if fields == (None,):
1723             clone.query.clear_deferred_loading()
1724         else:
1725             clone.query.add_deferred_loading(fields)
1726         return clone
1727 
1728     def only(self, *fields):
1729         """
1730         Essentially, the opposite of defer(). Only the fields passed into this
1731         method and that are not already specified as deferred are loaded
1732         immediately when the queryset is evaluated.
1733         """
1734         self._not_support_combined_queries("only")
1735         if self._fields is not None:
1736             raise TypeError("Cannot call only() after .values() or .values_list()")
1737         if fields == (None,):
1738             # Can only pass None to defer(), not only(), as the rest option.
1739             # That won't stop people trying to do this, so let's be explicit.
1740             raise TypeError("Cannot pass None as an argument to only().")
1741         for field in fields:
1742             field = field.split(LOOKUP_SEP, 1)[0]
1743             if field in self.query._filtered_relations:
1744                 raise ValueError("only() is not supported with FilteredRelation.")
1745         clone = self._chain()
1746         clone.query.add_immediate_loading(fields)
1747         return clone
1748 
1749     def using(self, alias):
1750         """Select which database this QuerySet should execute against."""
1751         clone = self._chain()
1752         clone._db = alias
1753         return clone
1754 
1755     ###################################
1756     # PUBLIC INTROSPECTION ATTRIBUTES #
1757     ###################################
1758 
1759     @property
1760     def ordered(self):
1761         """
1762         Return True if the QuerySet is ordered -- i.e. has an order_by()
1763         clause or a default ordering on the model (or is empty).
1764         """
1765         if isinstance(self, EmptyQuerySet):
1766             return True
1767         if self.query.extra_order_by or self.query.order_by:
1768             return True
1769         elif (
1770             self.query.default_ordering
1771             and self.query.get_meta().ordering
1772             and
1773             # A default ordering doesn't affect GROUP BY queries.
1774             not self.query.group_by
1775         ):
1776             return True
1777         else:
1778             return False
1779 
1780     @property
1781     def db(self):
1782         """Return the database used if this query is executed now."""
1783         if self._for_write:
1784             return self._db or router.db_for_write(self.model, **self._hints)
1785         return self._db or router.db_for_read(self.model, **self._hints)
1786 
1787     ###################
1788     # PRIVATE METHODS #
1789     ###################
1790 
1791     def _insert(
1792         self,
1793         objs,
1794         fields,
1795         returning_fields=None,
1796         raw=False,
1797         using=None,
1798         on_conflict=None,
1799         update_fields=None,
1800         unique_fields=None,
1801     ):
1802         """
1803         Insert a new record for the given model. This provides an interface to
1804         the InsertQuery class and is how Model.save() is implemented.
1805         """
1806         self._for_write = True
1807         if using is None:
1808             using = self.db
1809         query = sql.InsertQuery(
1810             self.model,
1811             on_conflict=on_conflict,
1812             update_fields=update_fields,
1813             unique_fields=unique_fields,
1814         )
1815         query.insert_values(fields, objs, raw=raw)
1816         return query.get_compiler(using=using).execute_sql(returning_fields)
1817 
1818     _insert.alters_data = True
1819     _insert.queryset_only = False
1820 
1821     def _batched_insert(
1822         self,
1823         objs,
1824         fields,
1825         batch_size,
1826         on_conflict=None,
1827         update_fields=None,
1828         unique_fields=None,
1829     ):
1830         """
1831         Helper method for bulk_create() to insert objs one batch at a time.
1832         """
1833         connection = connections[self.db]
1834         ops = connection.ops
1835         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1836         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1837         inserted_rows = []
1838         bulk_return = connection.features.can_return_rows_from_bulk_insert
1839         for item in [objs[i : i + batch_size] for i in range(0, len(objs), batch_size)]:
1840             if bulk_return and on_conflict is None:
1841                 inserted_rows.extend(
1842                     self._insert(
1843                         item,
1844                         fields=fields,
1845                         using=self.db,
1846                         returning_fields=self.model._meta.db_returning_fields,
1847                     )
1848                 )
1849             else:
1850                 self._insert(
1851                     item,
1852                     fields=fields,
1853                     using=self.db,
1854                     on_conflict=on_conflict,
1855                     update_fields=update_fields,
1856                     unique_fields=unique_fields,
1857                 )
1858         return inserted_rows
1859 
1860     def _chain(self):
1861         """
1862         Return a copy of the current QuerySet that's ready for another
1863         operation.
1864         """
1865         obj = self._clone()
1866         if obj._sticky_filter:
1867             obj.query.filter_is_sticky = True
1868             obj._sticky_filter = False
1869         return obj
1870 
1871     def _clone(self):
1872         """
1873         Return a copy of the current QuerySet. A lightweight alternative
1874         to deepcopy().
1875         """
1876         c = self.__class__(
1877             model=self.model,
1878             query=self.query.chain(),
1879             using=self._db,
1880             hints=self._hints,
1881         )
1882         c._sticky_filter = self._sticky_filter
1883         c._for_write = self._for_write
1884         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1885         c._known_related_objects = self._known_related_objects
1886         c._iterable_class = self._iterable_class
1887         c._fields = self._fields
1888         return c
1889 
1890     def _fetch_all(self):
1891         if self._result_cache is None:
1892             self._result_cache = list(self._iterable_class(self))
1893         if self._prefetch_related_lookups and not self._prefetch_done:
1894             self._prefetch_related_objects()
1895 
1896     def _next_is_sticky(self):
1897         """
1898         Indicate that the next filter call and the one following that should
1899         be treated as a single filter. This is only important when it comes to
1900         determining when to reuse tables for many-to-many filters. Required so
1901         that we can filter naturally on the results of related managers.
1902 
1903         This doesn't return a clone of the current QuerySet (it returns
1904         "self"). The method is only used internally and should be immediately
1905         followed by a filter() that does create a clone.
1906         """
1907         self._sticky_filter = True
1908         return self
1909 
1910     def _merge_sanity_check(self, other):
1911         """Check that two QuerySet classes may be merged."""
1912         if self._fields is not None and (
1913             set(self.query.values_select) != set(other.query.values_select)
1914             or set(self.query.extra_select) != set(other.query.extra_select)
1915             or set(self.query.annotation_select) != set(other.query.annotation_select)
1916         ):
1917             raise TypeError(
1918                 "Merging '%s' classes must involve the same values in each case."
1919                 % self.__class__.__name__
1920             )
1921 
1922     def _merge_known_related_objects(self, other):
1923         """
1924         Keep track of all known related objects from either QuerySet instance.
1925         """
1926         for field, objects in other._known_related_objects.items():
1927             self._known_related_objects.setdefault(field, {}).update(objects)
1928 
1929     def resolve_expression(self, *args, **kwargs):
1930         if self._fields and len(self._fields) > 1:
1931             # values() queryset can only be used as nested queries
1932             # if they are set up to select only a single field.
1933             raise TypeError("Cannot use multi-field values as a filter value.")
1934         query = self.query.resolve_expression(*args, **kwargs)
1935         query._db = self._db
1936         return query
1937 
1938     resolve_expression.queryset_only = True
1939 
1940     def _add_hints(self, **hints):
1941         """
1942         Update hinting information for use by routers. Add new key/values or
1943         overwrite existing key/values.
1944         """
1945         self._hints.update(hints)
1946 
1947     def _has_filters(self):
1948         """
1949         Check if this QuerySet has any filtering going on. This isn't
1950         equivalent with checking if all objects are present in results, for
1951         example, qs[1:]._has_filters() -> False.
1952         """
1953         return self.query.has_filters()
1954 
1955     @staticmethod
1956     def _validate_values_are_expressions(values, method_name):
1957         invalid_args = sorted(
1958             str(arg) for arg in values if not hasattr(arg, "resolve_expression")
1959         )
1960         if invalid_args:
1961             raise TypeError(
1962                 "QuerySet.%s() received non-expression(s): %s."
1963                 % (
1964                     method_name,
1965                     ", ".join(invalid_args),
1966                 )
1967             )
1968 
1969     def _not_support_combined_queries(self, operation_name):
1970         if self.query.combinator:
1971             raise NotSupportedError(
1972                 "Calling QuerySet.%s() after %s() is not supported."
1973                 % (operation_name, self.query.combinator)
1974             )
1975 
1976     def _check_operator_queryset(self, other, operator_):
1977         if self.query.combinator or other.query.combinator:
1978             raise TypeError(f"Cannot use {operator_} operator with combined queryset.")
1979 
1980     def _check_ordering_first_last_queryset_aggregation(self, method):
1981         if isinstance(self.query.group_by, tuple) and not any(
1982             col.output_field is self.model._meta.pk for col in self.query.group_by
1983         ):
1984             raise TypeError(
1985                 f"Cannot use QuerySet.{method}() on an unordered queryset performing "
1986                 f"aggregation. Add an ordering with order_by()."
1987             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Case</class>
<method>None</method>
<code>
1433 class Case(SQLiteNumericMixin, Expression):
1434     """
1435     An SQL searched CASE expression:
1436 
1437         CASE
1438             WHEN n > 0
1439                 THEN 'positive'
1440             WHEN n < 0
1441                 THEN 'negative'
1442             ELSE 'zero'
1443         END
1444     """
1445 
1446     template = "CASE %(cases)s ELSE %(default)s END"
1447     case_joiner = " "
1448 
1449     def __init__(self, *cases, default=None, output_field=None, **extra):
1450         if not all(isinstance(case, When) for case in cases):
1451             raise TypeError("Positional arguments must all be When objects.")
1452         super().__init__(output_field)
1453         self.cases = list(cases)
1454         self.default = self._parse_expressions(default)[0]
1455         self.extra = extra
1456 
1457     def __str__(self):
1458         return "CASE %s, ELSE %r" % (
1459             ", ".join(str(c) for c in self.cases),
1460             self.default,
1461         )
1462 
1463     def __repr__(self):
1464         return "<%s: %s>" % (self.__class__.__name__, self)
1465 
1466     def get_source_expressions(self):
1467         return self.cases + [self.default]
1468 
1469     def set_source_expressions(self, exprs):
1470         *self.cases, self.default = exprs
1471 
1472     def resolve_expression(
1473         self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False
1474     ):
1475         c = self.copy()
1476         c.is_summary = summarize
1477         for pos, case in enumerate(c.cases):
1478             c.cases[pos] = case.resolve_expression(
1479                 query, allow_joins, reuse, summarize, for_save
1480             )
1481         c.default = c.default.resolve_expression(
1482             query, allow_joins, reuse, summarize, for_save
1483         )
1484         return c
1485 
1486     def copy(self):
1487         c = super().copy()
1488         c.cases = c.cases[:]
1489         return c
1490 
1491     def as_sql(
1492         self, compiler, connection, template=None, case_joiner=None, **extra_context
1493     ):
1494         connection.ops.check_expression_support(self)
1495         if not self.cases:
1496             return compiler.compile(self.default)
1497         template_params = {**self.extra, **extra_context}
1498         case_parts = []
1499         sql_params = []
1500         default_sql, default_params = compiler.compile(self.default)
1501         for case in self.cases:
1502             try:
1503                 case_sql, case_params = compiler.compile(case)
1504             except EmptyResultSet:
1505                 continue
1506             except FullResultSet:
1507                 default_sql, default_params = compiler.compile(case.result)
1508                 break
1509             case_parts.append(case_sql)
1510             sql_params.extend(case_params)
1511         if not case_parts:
1512             return default_sql, default_params
1513         case_joiner = case_joiner or self.case_joiner
1514         template_params["cases"] = case_joiner.join(case_parts)
1515         template_params["default"] = default_sql
1516         sql_params.extend(default_params)
1517         template = template or template_params.get("template", self.template)
1518         sql = template % template_params
1519         if self._output_field_or_none is not None:
1520             sql = connection.ops.unification_cast_sql(self.output_field) % sql
1521         return sql, sql_params
1522 
1523     def get_group_by_cols(self):
1524         if not self.cases:
1525             return self.default.get_group_by_cols()
1526         return super().get_group_by_cols()
1527 
1528     @cached_property
1529     def allowed_default(self):
1530         return self.default.allowed_default and all(
1531             case_.allowed_default for case_ in self.cases
1532         )

</code><intended_behavior>Given the error seems to relate to the handling of the `Case` expression, specifically in version 4.2, any adjustment or improvement in expression resolution or compilation that was changed from version 4.1 to 4.2 should be revisited. The `Case` expression should accurately handle aggregated fields derived from annotations, ensuring that these fields are correctly identified and utilized in the conditional logic without causing resolution errors.</intended_behavior>]
2025-10-17 14:03:55.401 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-17 14:03:55.401 | INFO     | Search-only: skipping patch selection
2025-10-17 14:03:55.546 | INFO     | Task django__django-16879 completed successfully.
2025-10-17 14:03:55.547 | INFO     | No patch generated. You can try running ACR again.
