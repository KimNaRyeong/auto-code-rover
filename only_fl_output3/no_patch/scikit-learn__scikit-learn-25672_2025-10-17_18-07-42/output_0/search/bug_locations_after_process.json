[
    {
        "rel_file_path": "sklearn/metrics/_ranking.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__1.3/sklearn/metrics/_ranking.py",
        "start": 1632,
        "end": 1742,
        "class_name": null,
        "method_name": "ndcg_score",
        "code": "1632 def ndcg_score(y_true, y_score, *, k=None, sample_weight=None, ignore_ties=False):\n1633     \"\"\"Compute Normalized Discounted Cumulative Gain.\n1634 \n1635     Sum the true scores ranked in the order induced by the predicted scores,\n1636     after applying a logarithmic discount. Then divide by the best possible\n1637     score (Ideal DCG, obtained for a perfect ranking) to obtain a score between\n1638     0 and 1.\n1639 \n1640     This ranking metric returns a high value if true labels are ranked high by\n1641     ``y_score``.\n1642 \n1643     Parameters\n1644     ----------\n1645     y_true : array-like of shape (n_samples, n_labels)\n1646         True targets of multilabel classification, or true scores of entities\n1647         to be ranked. Negative values in `y_true` may result in an output\n1648         that is not between 0 and 1.\n1649 \n1650         .. versionchanged:: 1.2\n1651             These negative values are deprecated, and will raise an error in v1.4.\n1652 \n1653     y_score : array-like of shape (n_samples, n_labels)\n1654         Target scores, can either be probability estimates, confidence values,\n1655         or non-thresholded measure of decisions (as returned by\n1656         \"decision_function\" on some classifiers).\n1657 \n1658     k : int, default=None\n1659         Only consider the highest k scores in the ranking. If `None`, use all\n1660         outputs.\n1661 \n1662     sample_weight : array-like of shape (n_samples,), default=None\n1663         Sample weights. If `None`, all samples are given the same weight.\n1664 \n1665     ignore_ties : bool, default=False\n1666         Assume that there are no ties in y_score (which is likely to be the\n1667         case if y_score is continuous) for efficiency gains.\n1668 \n1669     Returns\n1670     -------\n1671     normalized_discounted_cumulative_gain : float in [0., 1.]\n1672         The averaged NDCG scores for all samples.\n1673 \n1674     See Also\n1675     --------\n1676     dcg_score : Discounted Cumulative Gain (not normalized).\n1677 \n1678     References\n1679     ----------\n1680     `Wikipedia entry for Discounted Cumulative Gain\n1681     <https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_\n1682 \n1683     Jarvelin, K., & Kekalainen, J. (2002).\n1684     Cumulated gain-based evaluation of IR techniques. ACM Transactions on\n1685     Information Systems (TOIS), 20(4), 422-446.\n1686 \n1687     Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).\n1688     A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th\n1689     Annual Conference on Learning Theory (COLT 2013)\n1690 \n1691     McSherry, F., & Najork, M. (2008, March). Computing information retrieval\n1692     performance measures efficiently in the presence of tied scores. In\n1693     European conference on information retrieval (pp. 414-421). Springer,\n1694     Berlin, Heidelberg.\n1695 \n1696     Examples\n1697     --------\n1698     >>> import numpy as np\n1699     >>> from sklearn.metrics import ndcg_score\n1700     >>> # we have groud-truth relevance of some answers to a query:\n1701     >>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])\n1702     >>> # we predict some scores (relevance) for the answers\n1703     >>> scores = np.asarray([[.1, .2, .3, 4, 70]])\n1704     >>> ndcg_score(true_relevance, scores)\n1705     0.69...\n1706     >>> scores = np.asarray([[.05, 1.1, 1., .5, .0]])\n1707     >>> ndcg_score(true_relevance, scores)\n1708     0.49...\n1709     >>> # we can set k to truncate the sum; only top k answers contribute.\n1710     >>> ndcg_score(true_relevance, scores, k=4)\n1711     0.35...\n1712     >>> # the normalization takes k into account so a perfect answer\n1713     >>> # would still get 1.0\n1714     >>> ndcg_score(true_relevance, true_relevance, k=4)\n1715     1.0...\n1716     >>> # now we have some ties in our prediction\n1717     >>> scores = np.asarray([[1, 0, 0, 0, 1]])\n1718     >>> # by default ties are averaged, so here we get the average (normalized)\n1719     >>> # true relevance of our top predictions: (10 / 10 + 5 / 10) / 2 = .75\n1720     >>> ndcg_score(true_relevance, scores, k=1)\n1721     0.75...\n1722     >>> # we can choose to ignore ties for faster results, but only\n1723     >>> # if we know there aren't ties in our scores, otherwise we get\n1724     >>> # wrong results:\n1725     >>> ndcg_score(true_relevance,\n1726     ...           scores, k=1, ignore_ties=True)\n1727     0.5...\n1728     \"\"\"\n1729     y_true = check_array(y_true, ensure_2d=False)\n1730     y_score = check_array(y_score, ensure_2d=False)\n1731     check_consistent_length(y_true, y_score, sample_weight)\n1732 \n1733     if y_true.min() < 0:\n1734         # TODO(1.4): Replace warning w/ ValueError\n1735         warnings.warn(\n1736             \"ndcg_score should not be used on negative y_true values. ndcg_score will\"\n1737             \" raise a ValueError on negative y_true values starting from version 1.4.\",\n1738             FutureWarning,\n1739         )\n1740     _check_dcg_target_type(y_true)\n1741     gain = _ndcg_sample_scores(y_true, y_score, k=k, ignore_ties=ignore_ties)\n1742     return np.average(gain, weights=sample_weight)\n",
        "intended_behavior": "Before proceeding to the computation of NDCG or calling `_check_dcg_target_type(y_true)`, add a check for binary relevance with single-element lists, since NDCG does not conceptually apply to such scenarios. If a single-element, binary relevance is detected, the method should raise a ValueError with a clear and user-friendly error message explaining why such input is not supported. This should be done in a way that preserves existing functionality for all other valid inputs (e.g., non-single-element or non-binary relevance cases). This involves introducing a condition that checks for the shape and content of `y_true` to determine if it's binary with a length of 1. This behavior is important to resolve the issue as it directly addresses the user's confusion and the lack of clarity the current error message provides. It ensures that users will have a better understanding of why their input is not accepted and what conditions are supported by the `ndcg_score` method."
    }
]