[
    {
        "rel_file_path": "sklearn/dummy.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/dummy.py",
        "start": 101,
        "end": 176,
        "class_name": "DummyClassifier",
        "method_name": "fit",
        "code": "101     def fit(self, X, y, sample_weight=None):\n102         \"\"\"Fit the random classifier.\n103 \n104         Parameters\n105         ----------\n106         X : {array-like, object with finite length or shape}\n107             Training data, requires length = n_samples\n108 \n109         y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n110             Target values.\n111 \n112         sample_weight : array-like of shape (n_samples,), default=None\n113             Sample weights.\n114 \n115         Returns\n116         -------\n117         self : object\n118         \"\"\"\n119         allowed_strategies = (\"most_frequent\", \"stratified\", \"uniform\",\n120                               \"constant\", \"prior\")\n121         if self.strategy not in allowed_strategies:\n122             raise ValueError(\"Unknown strategy type: %s, expected one of %s.\"\n123                              % (self.strategy, allowed_strategies))\n124 \n125         if self.strategy == \"uniform\" and sp.issparse(y):\n126             y = y.toarray()\n127             warnings.warn('A local copy of the target data has been converted '\n128                           'to a numpy array. Predicting on sparse target data '\n129                           'with the uniform strategy would not save memory '\n130                           'and would be slower.',\n131                           UserWarning)\n132 \n133         self.sparse_output_ = sp.issparse(y)\n134 \n135         if not self.sparse_output_:\n136             y = np.asarray(y)\n137             y = np.atleast_1d(y)\n138 \n139         if y.ndim == 1:\n140             y = np.reshape(y, (-1, 1))\n141 \n142         self.n_outputs_ = y.shape[1]\n143 \n144         check_consistent_length(X, y, sample_weight)\n145 \n146         if self.strategy == \"constant\":\n147             if self.constant is None:\n148                 raise ValueError(\"Constant target value has to be specified \"\n149                                  \"when the constant strategy is used.\")\n150             else:\n151                 constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))\n152                 if constant.shape[0] != self.n_outputs_:\n153                     raise ValueError(\"Constant target value should have \"\n154                                      \"shape (%d, 1).\" % self.n_outputs_)\n155 \n156         (self.classes_,\n157          self.n_classes_,\n158          self.class_prior_) = class_distribution(y, sample_weight)\n159 \n160         if self.strategy == \"constant\":\n161             for k in range(self.n_outputs_):\n162                 if not any(constant[k][0] == c for c in self.classes_[k]):\n163                     # Checking in case of constant strategy if the constant\n164                     # provided by the user is in y.\n165                     err_msg = (\"The constant target value must be present in \"\n166                                \"the training data. You provided constant={}. \"\n167                                \"Possible values are: {}.\"\n168                                .format(self.constant, list(self.classes_[k])))\n169                     raise ValueError(err_msg)\n170 \n171         if self.n_outputs_ == 1:\n172             self.n_classes_ = self.n_classes_[0]\n173             self.classes_ = self.classes_[0]\n174             self.class_prior_ = self.class_prior_[0]\n175 \n176         return self\n",
        "intended_behavior": "This should be replaced with a call to sample_weight = _check_sample_weight(sample_weight, X) right after the method signature to ensure sample_weight is validated correctly and consistently, preserving the intended functionality of handling sample weights in classification tasks by DummyClassifier. This change centralizes the sample weight validation logic, enhancing maintainability."
    },
    {
        "rel_file_path": "sklearn/dummy.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/dummy.py",
        "start": 23,
        "end": 379,
        "class_name": "DummyClassifier",
        "method_name": null,
        "code": "23 class DummyClassifier(MultiOutputMixin, ClassifierMixin, BaseEstimator):\n24     \"\"\"\n25     DummyClassifier is a classifier that makes predictions using simple rules.\n26 \n27     This classifier is useful as a simple baseline to compare with other\n28     (real) classifiers. Do not use it for real problems.\n29 \n30     Read more in the :ref:`User Guide <dummy_estimators>`.\n31 \n32     Parameters\n33     ----------\n34     strategy : str, default=\"stratified\"\n35         Strategy to use to generate predictions.\n36 \n37         * \"stratified\": generates predictions by respecting the training\n38           set's class distribution.\n39         * \"most_frequent\": always predicts the most frequent label in the\n40           training set.\n41         * \"prior\": always predicts the class that maximizes the class prior\n42           (like \"most_frequent\") and ``predict_proba`` returns the class prior.\n43         * \"uniform\": generates predictions uniformly at random.\n44         * \"constant\": always predicts a constant label that is provided by\n45           the user. This is useful for metrics that evaluate a non-majority\n46           class\n47 \n48           .. versionadded:: 0.17\n49              Dummy Classifier now supports prior fitting strategy using\n50              parameter *prior*.\n51 \n52     random_state : int, RandomState instance or None, optional, default=None\n53         If int, random_state is the seed used by the random number generator;\n54         If RandomState instance, random_state is the random number generator;\n55         If None, the random number generator is the RandomState instance used\n56         by `np.random`.\n57 \n58     constant : int or str or array-like of shape (n_outputs,)\n59         The explicit constant as predicted by the \"constant\" strategy. This\n60         parameter is useful only for the \"constant\" strategy.\n61 \n62     Attributes\n63     ----------\n64     classes_ : array or list of array of shape (n_classes,)\n65         Class labels for each output.\n66 \n67     n_classes_ : array or list of array of shape (n_classes,)\n68         Number of label for each output.\n69 \n70     class_prior_ : array or list of array of shape (n_classes,)\n71         Probability of each class for each output.\n72 \n73     n_outputs_ : int,\n74         Number of outputs.\n75 \n76     sparse_output_ : bool,\n77         True if the array returned from predict is to be in sparse CSC format.\n78         Is automatically set to True if the input y is passed in sparse format.\n79 \n80     Examples\n81     --------\n82     >>> import numpy as np\n83     >>> from sklearn.dummy import DummyClassifier\n84     >>> X = np.array([-1, 1, 1, 1])\n85     >>> y = np.array([0, 1, 1, 1])\n86     >>> dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n87     >>> dummy_clf.fit(X, y)\n88     DummyClassifier(strategy='most_frequent')\n89     >>> dummy_clf.predict(X)\n90     array([1, 1, 1, 1])\n91     >>> dummy_clf.score(X, y)\n92     0.75\n93     \"\"\"\n94 \n95     def __init__(self, strategy=\"stratified\", random_state=None,\n96                  constant=None):\n97         self.strategy = strategy\n98         self.random_state = random_state\n99         self.constant = constant\n100 \n101     def fit(self, X, y, sample_weight=None):\n102         \"\"\"Fit the random classifier.\n103 \n104         Parameters\n105         ----------\n106         X : {array-like, object with finite length or shape}\n107             Training data, requires length = n_samples\n108 \n109         y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n110             Target values.\n111 \n112         sample_weight : array-like of shape (n_samples,), default=None\n113             Sample weights.\n114 \n115         Returns\n116         -------\n117         self : object\n118         \"\"\"\n119         allowed_strategies = (\"most_frequent\", \"stratified\", \"uniform\",\n120                               \"constant\", \"prior\")\n121         if self.strategy not in allowed_strategies:\n122             raise ValueError(\"Unknown strategy type: %s, expected one of %s.\"\n123                              % (self.strategy, allowed_strategies))\n124 \n125         if self.strategy == \"uniform\" and sp.issparse(y):\n126             y = y.toarray()\n127             warnings.warn('A local copy of the target data has been converted '\n128                           'to a numpy array. Predicting on sparse target data '\n129                           'with the uniform strategy would not save memory '\n130                           'and would be slower.',\n131                           UserWarning)\n132 \n133         self.sparse_output_ = sp.issparse(y)\n134 \n135         if not self.sparse_output_:\n136             y = np.asarray(y)\n137             y = np.atleast_1d(y)\n138 \n139         if y.ndim == 1:\n140             y = np.reshape(y, (-1, 1))\n141 \n142         self.n_outputs_ = y.shape[1]\n143 \n144         check_consistent_length(X, y, sample_weight)\n145 \n146         if self.strategy == \"constant\":\n147             if self.constant is None:\n148                 raise ValueError(\"Constant target value has to be specified \"\n149                                  \"when the constant strategy is used.\")\n150             else:\n151                 constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))\n152                 if constant.shape[0] != self.n_outputs_:\n153                     raise ValueError(\"Constant target value should have \"\n154                                      \"shape (%d, 1).\" % self.n_outputs_)\n155 \n156         (self.classes_,\n157          self.n_classes_,\n158          self.class_prior_) = class_distribution(y, sample_weight)\n159 \n160         if self.strategy == \"constant\":\n161             for k in range(self.n_outputs_):\n162                 if not any(constant[k][0] == c for c in self.classes_[k]):\n163                     # Checking in case of constant strategy if the constant\n164                     # provided by the user is in y.\n165                     err_msg = (\"The constant target value must be present in \"\n166                                \"the training data. You provided constant={}. \"\n167                                \"Possible values are: {}.\"\n168                                .format(self.constant, list(self.classes_[k])))\n169                     raise ValueError(err_msg)\n170 \n171         if self.n_outputs_ == 1:\n172             self.n_classes_ = self.n_classes_[0]\n173             self.classes_ = self.classes_[0]\n174             self.class_prior_ = self.class_prior_[0]\n175 \n176         return self\n177 \n178     def predict(self, X):\n179         \"\"\"Perform classification on test vectors X.\n180 \n181         Parameters\n182         ----------\n183         X : {array-like, object with finite length or shape}\n184             Training data, requires length = n_samples\n185 \n186         Returns\n187         -------\n188         y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n189             Predicted target values for X.\n190         \"\"\"\n191         check_is_fitted(self)\n192 \n193         # numpy random_state expects Python int and not long as size argument\n194         # under Windows\n195         n_samples = _num_samples(X)\n196         rs = check_random_state(self.random_state)\n197 \n198         n_classes_ = self.n_classes_\n199         classes_ = self.classes_\n200         class_prior_ = self.class_prior_\n201         constant = self.constant\n202         if self.n_outputs_ == 1:\n203             # Get same type even for self.n_outputs_ == 1\n204             n_classes_ = [n_classes_]\n205             classes_ = [classes_]\n206             class_prior_ = [class_prior_]\n207             constant = [constant]\n208         # Compute probability only once\n209         if self.strategy == \"stratified\":\n210             proba = self.predict_proba(X)\n211             if self.n_outputs_ == 1:\n212                 proba = [proba]\n213 \n214         if self.sparse_output_:\n215             class_prob = None\n216             if self.strategy in (\"most_frequent\", \"prior\"):\n217                 classes_ = [np.array([cp.argmax()]) for cp in class_prior_]\n218 \n219             elif self.strategy == \"stratified\":\n220                 class_prob = class_prior_\n221 \n222             elif self.strategy == \"uniform\":\n223                 raise ValueError(\"Sparse target prediction is not \"\n224                                  \"supported with the uniform strategy\")\n225 \n226             elif self.strategy == \"constant\":\n227                 classes_ = [np.array([c]) for c in constant]\n228 \n229             y = _random_choice_csc(n_samples, classes_, class_prob,\n230                                   self.random_state)\n231         else:\n232             if self.strategy in (\"most_frequent\", \"prior\"):\n233                 y = np.tile([classes_[k][class_prior_[k].argmax()] for\n234                              k in range(self.n_outputs_)], [n_samples, 1])\n235 \n236             elif self.strategy == \"stratified\":\n237                 y = np.vstack([classes_[k][proba[k].argmax(axis=1)] for\n238                                k in range(self.n_outputs_)]).T\n239 \n240             elif self.strategy == \"uniform\":\n241                 ret = [classes_[k][rs.randint(n_classes_[k], size=n_samples)]\n242                        for k in range(self.n_outputs_)]\n243                 y = np.vstack(ret).T\n244 \n245             elif self.strategy == \"constant\":\n246                 y = np.tile(self.constant, (n_samples, 1))\n247 \n248             if self.n_outputs_ == 1:\n249                 y = np.ravel(y)\n250 \n251         return y\n252 \n253     def predict_proba(self, X):\n254         \"\"\"\n255         Return probability estimates for the test vectors X.\n256 \n257         Parameters\n258         ----------\n259         X : {array-like, object with finite length or shape}\n260             Training data, requires length = n_samples\n261 \n262         Returns\n263         -------\n264         P : array-like or list of array-lke of shape (n_samples, n_classes)\n265             Returns the probability of the sample for each class in\n266             the model, where classes are ordered arithmetically, for each\n267             output.\n268         \"\"\"\n269         check_is_fitted(self)\n270 \n271         # numpy random_state expects Python int and not long as size argument\n272         # under Windows\n273         n_samples = _num_samples(X)\n274         rs = check_random_state(self.random_state)\n275 \n276         n_classes_ = self.n_classes_\n277         classes_ = self.classes_\n278         class_prior_ = self.class_prior_\n279         constant = self.constant\n280         if self.n_outputs_ == 1:\n281             # Get same type even for self.n_outputs_ == 1\n282             n_classes_ = [n_classes_]\n283             classes_ = [classes_]\n284             class_prior_ = [class_prior_]\n285             constant = [constant]\n286 \n287         P = []\n288         for k in range(self.n_outputs_):\n289             if self.strategy == \"most_frequent\":\n290                 ind = class_prior_[k].argmax()\n291                 out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)\n292                 out[:, ind] = 1.0\n293             elif self.strategy == \"prior\":\n294                 out = np.ones((n_samples, 1)) * class_prior_[k]\n295 \n296             elif self.strategy == \"stratified\":\n297                 out = rs.multinomial(1, class_prior_[k], size=n_samples)\n298                 out = out.astype(np.float64)\n299 \n300             elif self.strategy == \"uniform\":\n301                 out = np.ones((n_samples, n_classes_[k]), dtype=np.float64)\n302                 out /= n_classes_[k]\n303 \n304             elif self.strategy == \"constant\":\n305                 ind = np.where(classes_[k] == constant[k])\n306                 out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)\n307                 out[:, ind] = 1.0\n308 \n309             P.append(out)\n310 \n311         if self.n_outputs_ == 1:\n312             P = P[0]\n313 \n314         return P\n315 \n316     def predict_log_proba(self, X):\n317         \"\"\"\n318         Return log probability estimates for the test vectors X.\n319 \n320         Parameters\n321         ----------\n322         X : {array-like, object with finite length or shape}\n323             Training data, requires length = n_samples\n324 \n325         Returns\n326         -------\n327         P : array-like or list of array-like of shape (n_samples, n_classes)\n328             Returns the log probability of the sample for each class in\n329             the model, where classes are ordered arithmetically for each\n330             output.\n331         \"\"\"\n332         proba = self.predict_proba(X)\n333         if self.n_outputs_ == 1:\n334             return np.log(proba)\n335         else:\n336             return [np.log(p) for p in proba]\n337 \n338     def _more_tags(self):\n339         return {'poor_score': True, 'no_validation': True}\n340 \n341     def score(self, X, y, sample_weight=None):\n342         \"\"\"Returns the mean accuracy on the given test data and labels.\n343 \n344         In multi-label classification, this is the subset accuracy\n345         which is a harsh metric since you require for each sample that\n346         each label set be correctly predicted.\n347 \n348         Parameters\n349         ----------\n350         X : {array-like, None}\n351             Test samples with shape = (n_samples, n_features) or\n352             None. Passing None as test samples gives the same result\n353             as passing real test samples, since DummyClassifier\n354             operates independently of the sampled observations.\n355 \n356         y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n357             True labels for X.\n358 \n359         sample_weight : array-like of shape (n_samples,), default=None\n360             Sample weights.\n361 \n362         Returns\n363         -------\n364         score : float\n365             Mean accuracy of self.predict(X) wrt. y.\n366 \n367         \"\"\"\n368         if X is None:\n369             X = np.zeros(shape=(len(y), 1))\n370         return super().score(X, y, sample_weight)\n371 \n372     @deprecated(\n373         \"The outputs_2d_ attribute is deprecated in version 0.22 \"\n374         \"and will be removed in version 0.24. It is equivalent to \"\n375         \"n_outputs_ > 1.\"\n376     )\n377     @property\n378     def outputs_2d_(self):\n379         return self.n_outputs_ != 1\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "sklearn/cluster/_dbscan.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/cluster/_dbscan.py",
        "start": 284,
        "end": 361,
        "class_name": "DBSCAN",
        "method_name": "fit",
        "code": "284     def fit(self, X, y=None, sample_weight=None):\n285         \"\"\"Perform DBSCAN clustering from features, or distance matrix.\n286 \n287         Parameters\n288         ----------\n289         X : array-like or sparse matrix, shape (n_samples, n_features), or \\\n290             (n_samples, n_samples)\n291             Training instances to cluster, or distances between instances if\n292             ``metric='precomputed'``. If a sparse matrix is provided, it will\n293             be converted into a sparse ``csr_matrix``.\n294 \n295         sample_weight : array, shape (n_samples,), optional\n296             Weight of each sample, such that a sample with a weight of at least\n297             ``min_samples`` is by itself a core sample; a sample with a\n298             negative weight may inhibit its eps-neighbor from being core.\n299             Note that weights are absolute, and default to 1.\n300 \n301         y : Ignored\n302             Not used, present here for API consistency by convention.\n303 \n304         Returns\n305         -------\n306         self\n307 \n308         \"\"\"\n309         X = check_array(X, accept_sparse='csr')\n310 \n311         if not self.eps > 0.0:\n312             raise ValueError(\"eps must be positive.\")\n313 \n314         if sample_weight is not None:\n315             sample_weight = np.asarray(sample_weight)\n316             check_consistent_length(X, sample_weight)\n317 \n318         # Calculate neighborhood for all samples. This leaves the original\n319         # point in, which needs to be considered later (i.e. point i is in the\n320         # neighborhood of point i. While True, its useless information)\n321         if self.metric == 'precomputed' and sparse.issparse(X):\n322             # set the diagonal to explicit values, as a point is its own\n323             # neighbor\n324             with warnings.catch_warnings():\n325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)\n326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place\n327 \n328         neighbors_model = NearestNeighbors(\n329             radius=self.eps, algorithm=self.algorithm,\n330             leaf_size=self.leaf_size, metric=self.metric,\n331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)\n332         neighbors_model.fit(X)\n333         # This has worst case O(n^2) memory complexity\n334         neighborhoods = neighbors_model.radius_neighbors(X,\n335                                                          return_distance=False)\n336 \n337         if sample_weight is None:\n338             n_neighbors = np.array([len(neighbors)\n339                                     for neighbors in neighborhoods])\n340         else:\n341             n_neighbors = np.array([np.sum(sample_weight[neighbors])\n342                                     for neighbors in neighborhoods])\n343 \n344         # Initially, all samples are noise.\n345         labels = np.full(X.shape[0], -1, dtype=np.intp)\n346 \n347         # A list of all core samples found.\n348         core_samples = np.asarray(n_neighbors >= self.min_samples,\n349                                   dtype=np.uint8)\n350         dbscan_inner(core_samples, neighborhoods, labels)\n351 \n352         self.core_sample_indices_ = np.where(core_samples)[0]\n353         self.labels_ = labels\n354 \n355         if len(self.core_sample_indices_):\n356             # fix for scipy sparse indexing issue\n357             self.components_ = X[self.core_sample_indices_].copy()\n358         else:\n359             # no core samples\n360             self.components_ = np.empty((0, X.shape[1]))\n361         return self\n",
        "intended_behavior": "Incorporate _check_sample_weight(sample_weight, X) to not only validate sample_weight but also standardize its dtype and shape, ensuring that sample_weight adheres to the requirements for DBSCAN clustering operations. This adjustment facilitates a standardized approach to handling sample weights across clustering operations."
    },
    {
        "rel_file_path": "sklearn/cluster/_dbscan.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/cluster/_dbscan.py",
        "start": 147,
        "end": 390,
        "class_name": "DBSCAN",
        "method_name": null,
        "code": "147 class DBSCAN(ClusterMixin, BaseEstimator):\n148     \"\"\"Perform DBSCAN clustering from vector array or distance matrix.\n149 \n150     DBSCAN - Density-Based Spatial Clustering of Applications with Noise.\n151     Finds core samples of high density and expands clusters from them.\n152     Good for data which contains clusters of similar density.\n153 \n154     Read more in the :ref:`User Guide <dbscan>`.\n155 \n156     Parameters\n157     ----------\n158     eps : float, optional\n159         The maximum distance between two samples for one to be considered\n160         as in the neighborhood of the other. This is not a maximum bound\n161         on the distances of points within a cluster. This is the most\n162         important DBSCAN parameter to choose appropriately for your data set\n163         and distance function.\n164 \n165     min_samples : int, optional\n166         The number of samples (or total weight) in a neighborhood for a point\n167         to be considered as a core point. This includes the point itself.\n168 \n169     metric : string, or callable\n170         The metric to use when calculating distance between instances in a\n171         feature array. If metric is a string or callable, it must be one of\n172         the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n173         its metric parameter.\n174         If metric is \"precomputed\", X is assumed to be a distance matrix and\n175         must be square. X may be a :term:`Glossary <sparse graph>`, in which\n176         case only \"nonzero\" elements may be considered neighbors for DBSCAN.\n177 \n178         .. versionadded:: 0.17\n179            metric *precomputed* to accept precomputed sparse matrix.\n180 \n181     metric_params : dict, optional\n182         Additional keyword arguments for the metric function.\n183 \n184         .. versionadded:: 0.19\n185 \n186     algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n187         The algorithm to be used by the NearestNeighbors module\n188         to compute pointwise distances and find nearest neighbors.\n189         See NearestNeighbors module documentation for details.\n190 \n191     leaf_size : int, optional (default = 30)\n192         Leaf size passed to BallTree or cKDTree. This can affect the speed\n193         of the construction and query, as well as the memory required\n194         to store the tree. The optimal value depends\n195         on the nature of the problem.\n196 \n197     p : float, optional\n198         The power of the Minkowski metric to be used to calculate distance\n199         between points.\n200 \n201     n_jobs : int or None, optional (default=None)\n202         The number of parallel jobs to run.\n203         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n204         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n205         for more details.\n206 \n207     Attributes\n208     ----------\n209     core_sample_indices_ : array, shape = [n_core_samples]\n210         Indices of core samples.\n211 \n212     components_ : array, shape = [n_core_samples, n_features]\n213         Copy of each core sample found by training.\n214 \n215     labels_ : array, shape = [n_samples]\n216         Cluster labels for each point in the dataset given to fit().\n217         Noisy samples are given the label -1.\n218 \n219     Examples\n220     --------\n221     >>> from sklearn.cluster import DBSCAN\n222     >>> import numpy as np\n223     >>> X = np.array([[1, 2], [2, 2], [2, 3],\n224     ...               [8, 7], [8, 8], [25, 80]])\n225     >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n226     >>> clustering.labels_\n227     array([ 0,  0,  0,  1,  1, -1])\n228     >>> clustering\n229     DBSCAN(eps=3, min_samples=2)\n230 \n231     See also\n232     --------\n233     OPTICS\n234         A similar clustering at multiple values of eps. Our implementation\n235         is optimized for memory usage.\n236 \n237     Notes\n238     -----\n239     For an example, see :ref:`examples/cluster/plot_dbscan.py\n240     <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.\n241 \n242     This implementation bulk-computes all neighborhood queries, which increases\n243     the memory complexity to O(n.d) where d is the average number of neighbors,\n244     while original DBSCAN had memory complexity O(n). It may attract a higher\n245     memory complexity when querying these nearest neighborhoods, depending\n246     on the ``algorithm``.\n247 \n248     One way to avoid the query complexity is to pre-compute sparse\n249     neighborhoods in chunks using\n250     :func:`NearestNeighbors.radius_neighbors_graph\n251     <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n252     ``mode='distance'``, then using ``metric='precomputed'`` here.\n253 \n254     Another way to reduce memory and computation time is to remove\n255     (near-)duplicate points and use ``sample_weight`` instead.\n256 \n257     :class:`cluster.OPTICS` provides a similar clustering with lower memory\n258     usage.\n259 \n260     References\n261     ----------\n262     Ester, M., H. P. Kriegel, J. Sander, and X. Xu, \"A Density-Based\n263     Algorithm for Discovering Clusters in Large Spatial Databases with Noise\".\n264     In: Proceedings of the 2nd International Conference on Knowledge Discovery\n265     and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n266 \n267     Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n268     DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\n269     ACM Transactions on Database Systems (TODS), 42(3), 19.\n270     \"\"\"\n271 \n272     def __init__(self, eps=0.5, min_samples=5, metric='euclidean',\n273                  metric_params=None, algorithm='auto', leaf_size=30, p=None,\n274                  n_jobs=None):\n275         self.eps = eps\n276         self.min_samples = min_samples\n277         self.metric = metric\n278         self.metric_params = metric_params\n279         self.algorithm = algorithm\n280         self.leaf_size = leaf_size\n281         self.p = p\n282         self.n_jobs = n_jobs\n283 \n284     def fit(self, X, y=None, sample_weight=None):\n285         \"\"\"Perform DBSCAN clustering from features, or distance matrix.\n286 \n287         Parameters\n288         ----------\n289         X : array-like or sparse matrix, shape (n_samples, n_features), or \\\n290             (n_samples, n_samples)\n291             Training instances to cluster, or distances between instances if\n292             ``metric='precomputed'``. If a sparse matrix is provided, it will\n293             be converted into a sparse ``csr_matrix``.\n294 \n295         sample_weight : array, shape (n_samples,), optional\n296             Weight of each sample, such that a sample with a weight of at least\n297             ``min_samples`` is by itself a core sample; a sample with a\n298             negative weight may inhibit its eps-neighbor from being core.\n299             Note that weights are absolute, and default to 1.\n300 \n301         y : Ignored\n302             Not used, present here for API consistency by convention.\n303 \n304         Returns\n305         -------\n306         self\n307 \n308         \"\"\"\n309         X = check_array(X, accept_sparse='csr')\n310 \n311         if not self.eps > 0.0:\n312             raise ValueError(\"eps must be positive.\")\n313 \n314         if sample_weight is not None:\n315             sample_weight = np.asarray(sample_weight)\n316             check_consistent_length(X, sample_weight)\n317 \n318         # Calculate neighborhood for all samples. This leaves the original\n319         # point in, which needs to be considered later (i.e. point i is in the\n320         # neighborhood of point i. While True, its useless information)\n321         if self.metric == 'precomputed' and sparse.issparse(X):\n322             # set the diagonal to explicit values, as a point is its own\n323             # neighbor\n324             with warnings.catch_warnings():\n325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)\n326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place\n327 \n328         neighbors_model = NearestNeighbors(\n329             radius=self.eps, algorithm=self.algorithm,\n330             leaf_size=self.leaf_size, metric=self.metric,\n331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)\n332         neighbors_model.fit(X)\n333         # This has worst case O(n^2) memory complexity\n334         neighborhoods = neighbors_model.radius_neighbors(X,\n335                                                          return_distance=False)\n336 \n337         if sample_weight is None:\n338             n_neighbors = np.array([len(neighbors)\n339                                     for neighbors in neighborhoods])\n340         else:\n341             n_neighbors = np.array([np.sum(sample_weight[neighbors])\n342                                     for neighbors in neighborhoods])\n343 \n344         # Initially, all samples are noise.\n345         labels = np.full(X.shape[0], -1, dtype=np.intp)\n346 \n347         # A list of all core samples found.\n348         core_samples = np.asarray(n_neighbors >= self.min_samples,\n349                                   dtype=np.uint8)\n350         dbscan_inner(core_samples, neighborhoods, labels)\n351 \n352         self.core_sample_indices_ = np.where(core_samples)[0]\n353         self.labels_ = labels\n354 \n355         if len(self.core_sample_indices_):\n356             # fix for scipy sparse indexing issue\n357             self.components_ = X[self.core_sample_indices_].copy()\n358         else:\n359             # no core samples\n360             self.components_ = np.empty((0, X.shape[1]))\n361         return self\n362 \n363     def fit_predict(self, X, y=None, sample_weight=None):\n364         \"\"\"Perform DBSCAN clustering from features or distance matrix,\n365         and return cluster labels.\n366 \n367         Parameters\n368         ----------\n369         X : array-like or sparse matrix, shape (n_samples, n_features), or \\\n370             (n_samples, n_samples)\n371             Training instances to cluster, or distances between instances if\n372             ``metric='precomputed'``. If a sparse matrix is provided, it will\n373             be converted into a sparse ``csr_matrix``.\n374 \n375         sample_weight : array, shape (n_samples,), optional\n376             Weight of each sample, such that a sample with a weight of at least\n377             ``min_samples`` is by itself a core sample; a sample with a\n378             negative weight may inhibit its eps-neighbor from being core.\n379             Note that weights are absolute, and default to 1.\n380 \n381         y : Ignored\n382             Not used, present here for API consistency by convention.\n383 \n384         Returns\n385         -------\n386         labels : ndarray, shape (n_samples,)\n387             Cluster labels. Noisy samples are given the label -1.\n388         \"\"\"\n389         self.fit(X, sample_weight=sample_weight)\n390         return self.labels_\n",
        "intended_behavior": "This class provides additional context to the issue."
    }
]