2025-10-17 17:57:28.906 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-17 17:57:33.406 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=339, output_tokens=12, cost=0.003750
2025-10-17 17:57:33.407 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-17 17:57:33.411 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 17:57:42.635 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=647, output_tokens=381, cost=0.017900
2025-10-17 17:57:42.642 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 17:57:49.229 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=735, output_tokens=145, cost=0.011700
2025-10-17 17:57:49.231 | DEBUG    | Extracted a valid json.
2025-10-17 17:57:49.231 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_method(\"_check_sample_weight\")",
        "search_method_in_class(\"fit\", \"DummyClassifier\")",
        "search_method_in_class(\"fit\", \"DBSCAN\")",
        "search_code_in_file(\"check_consistent_length\", \"<path_to_DummyClassifier_fit_method>\")",
        "search_code_in_file(\"check_array\", \"<path_to_DummyClassifier_fit_method>\")",
        "search_code_in_file(\"check_consistent_length\", \"<path_to_DBSCAN_fit_method>\")",
        "search_code_in_file(\"check_array\", \"<path_to_DBSCAN_fit_method>\")"
    ],
    "bug_locations": []
}
2025-10-17 17:57:49.290 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-17 17:57:49.291 | DEBUG    | <Agent search> Analyzing search results.
2025-10-17 17:58:13.520 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3477, output_tokens=846, cost=0.060150
2025-10-17 17:58:13.536 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 17:58:24.914 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=4539, output_tokens=422, cost=0.058050
2025-10-17 17:58:24.923 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 17:58:30.250 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=776, output_tokens=214, cost=0.014180
2025-10-17 17:58:30.251 | DEBUG    | Extracted a valid json.
2025-10-17 17:58:30.251 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "sklearn/dummy.py",
            "class": "DummyClassifier",
            "method": "fit",
            "intended_behavior": "This should be replaced with a call to sample_weight = _check_sample_weight(sample_weight, X) right after the method signature to ensure sample_weight is validated correctly and consistently, preserving the intended functionality of handling sample weights in classification tasks by DummyClassifier. This change centralizes the sample weight validation logic, enhancing maintainability."
        },
        {
            "file": "sklearn/cluster/_dbscan.py",
            "class": "DBSCAN",
            "method": "fit",
            "intended_behavior": "Incorporate _check_sample_weight(sample_weight, X) to not only validate sample_weight but also standardize its dtype and shape, ensuring that sample_weight adheres to the requirements for DBSCAN clustering operations. This adjustment facilitates a standardized approach to handling sample weights across clustering operations."
        }
    ]
}
2025-10-17 17:58:30.257 | DEBUG    | Bug location extracted successfully: [<file>sklearn/dummy.py</file>
<class>DummyClassifier</class>
<method>fit</method>
<code>
101     def fit(self, X, y, sample_weight=None):
102         """Fit the random classifier.
103 
104         Parameters
105         ----------
106         X : {array-like, object with finite length or shape}
107             Training data, requires length = n_samples
108 
109         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
110             Target values.
111 
112         sample_weight : array-like of shape (n_samples,), default=None
113             Sample weights.
114 
115         Returns
116         -------
117         self : object
118         """
119         allowed_strategies = ("most_frequent", "stratified", "uniform",
120                               "constant", "prior")
121         if self.strategy not in allowed_strategies:
122             raise ValueError("Unknown strategy type: %s, expected one of %s."
123                              % (self.strategy, allowed_strategies))
124 
125         if self.strategy == "uniform" and sp.issparse(y):
126             y = y.toarray()
127             warnings.warn('A local copy of the target data has been converted '
128                           'to a numpy array. Predicting on sparse target data '
129                           'with the uniform strategy would not save memory '
130                           'and would be slower.',
131                           UserWarning)
132 
133         self.sparse_output_ = sp.issparse(y)
134 
135         if not self.sparse_output_:
136             y = np.asarray(y)
137             y = np.atleast_1d(y)
138 
139         if y.ndim == 1:
140             y = np.reshape(y, (-1, 1))
141 
142         self.n_outputs_ = y.shape[1]
143 
144         check_consistent_length(X, y, sample_weight)
145 
146         if self.strategy == "constant":
147             if self.constant is None:
148                 raise ValueError("Constant target value has to be specified "
149                                  "when the constant strategy is used.")
150             else:
151                 constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))
152                 if constant.shape[0] != self.n_outputs_:
153                     raise ValueError("Constant target value should have "
154                                      "shape (%d, 1)." % self.n_outputs_)
155 
156         (self.classes_,
157          self.n_classes_,
158          self.class_prior_) = class_distribution(y, sample_weight)
159 
160         if self.strategy == "constant":
161             for k in range(self.n_outputs_):
162                 if not any(constant[k][0] == c for c in self.classes_[k]):
163                     # Checking in case of constant strategy if the constant
164                     # provided by the user is in y.
165                     err_msg = ("The constant target value must be present in "
166                                "the training data. You provided constant={}. "
167                                "Possible values are: {}."
168                                .format(self.constant, list(self.classes_[k])))
169                     raise ValueError(err_msg)
170 
171         if self.n_outputs_ == 1:
172             self.n_classes_ = self.n_classes_[0]
173             self.classes_ = self.classes_[0]
174             self.class_prior_ = self.class_prior_[0]
175 
176         return self

</code><intended_behavior>This should be replaced with a call to sample_weight = _check_sample_weight(sample_weight, X) right after the method signature to ensure sample_weight is validated correctly and consistently, preserving the intended functionality of handling sample weights in classification tasks by DummyClassifier. This change centralizes the sample weight validation logic, enhancing maintainability.</intended_behavior>, <file>sklearn/dummy.py</file>
<class>DummyClassifier</class>
<method>None</method>
<code>
23 class DummyClassifier(MultiOutputMixin, ClassifierMixin, BaseEstimator):
24     """
25     DummyClassifier is a classifier that makes predictions using simple rules.
26 
27     This classifier is useful as a simple baseline to compare with other
28     (real) classifiers. Do not use it for real problems.
29 
30     Read more in the :ref:`User Guide <dummy_estimators>`.
31 
32     Parameters
33     ----------
34     strategy : str, default="stratified"
35         Strategy to use to generate predictions.
36 
37         * "stratified": generates predictions by respecting the training
38           set's class distribution.
39         * "most_frequent": always predicts the most frequent label in the
40           training set.
41         * "prior": always predicts the class that maximizes the class prior
42           (like "most_frequent") and ``predict_proba`` returns the class prior.
43         * "uniform": generates predictions uniformly at random.
44         * "constant": always predicts a constant label that is provided by
45           the user. This is useful for metrics that evaluate a non-majority
46           class
47 
48           .. versionadded:: 0.17
49              Dummy Classifier now supports prior fitting strategy using
50              parameter *prior*.
51 
52     random_state : int, RandomState instance or None, optional, default=None
53         If int, random_state is the seed used by the random number generator;
54         If RandomState instance, random_state is the random number generator;
55         If None, the random number generator is the RandomState instance used
56         by `np.random`.
57 
58     constant : int or str or array-like of shape (n_outputs,)
59         The explicit constant as predicted by the "constant" strategy. This
60         parameter is useful only for the "constant" strategy.
61 
62     Attributes
63     ----------
64     classes_ : array or list of array of shape (n_classes,)
65         Class labels for each output.
66 
67     n_classes_ : array or list of array of shape (n_classes,)
68         Number of label for each output.
69 
70     class_prior_ : array or list of array of shape (n_classes,)
71         Probability of each class for each output.
72 
73     n_outputs_ : int,
74         Number of outputs.
75 
76     sparse_output_ : bool,
77         True if the array returned from predict is to be in sparse CSC format.
78         Is automatically set to True if the input y is passed in sparse format.
79 
80     Examples
81     --------
82     >>> import numpy as np
83     >>> from sklearn.dummy import DummyClassifier
84     >>> X = np.array([-1, 1, 1, 1])
85     >>> y = np.array([0, 1, 1, 1])
86     >>> dummy_clf = DummyClassifier(strategy="most_frequent")
87     >>> dummy_clf.fit(X, y)
88     DummyClassifier(strategy='most_frequent')
89     >>> dummy_clf.predict(X)
90     array([1, 1, 1, 1])
91     >>> dummy_clf.score(X, y)
92     0.75
93     """
94 
95     def __init__(self, strategy="stratified", random_state=None,
96                  constant=None):
97         self.strategy = strategy
98         self.random_state = random_state
99         self.constant = constant
100 
101     def fit(self, X, y, sample_weight=None):
102         """Fit the random classifier.
103 
104         Parameters
105         ----------
106         X : {array-like, object with finite length or shape}
107             Training data, requires length = n_samples
108 
109         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
110             Target values.
111 
112         sample_weight : array-like of shape (n_samples,), default=None
113             Sample weights.
114 
115         Returns
116         -------
117         self : object
118         """
119         allowed_strategies = ("most_frequent", "stratified", "uniform",
120                               "constant", "prior")
121         if self.strategy not in allowed_strategies:
122             raise ValueError("Unknown strategy type: %s, expected one of %s."
123                              % (self.strategy, allowed_strategies))
124 
125         if self.strategy == "uniform" and sp.issparse(y):
126             y = y.toarray()
127             warnings.warn('A local copy of the target data has been converted '
128                           'to a numpy array. Predicting on sparse target data '
129                           'with the uniform strategy would not save memory '
130                           'and would be slower.',
131                           UserWarning)
132 
133         self.sparse_output_ = sp.issparse(y)
134 
135         if not self.sparse_output_:
136             y = np.asarray(y)
137             y = np.atleast_1d(y)
138 
139         if y.ndim == 1:
140             y = np.reshape(y, (-1, 1))
141 
142         self.n_outputs_ = y.shape[1]
143 
144         check_consistent_length(X, y, sample_weight)
145 
146         if self.strategy == "constant":
147             if self.constant is None:
148                 raise ValueError("Constant target value has to be specified "
149                                  "when the constant strategy is used.")
150             else:
151                 constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))
152                 if constant.shape[0] != self.n_outputs_:
153                     raise ValueError("Constant target value should have "
154                                      "shape (%d, 1)." % self.n_outputs_)
155 
156         (self.classes_,
157          self.n_classes_,
158          self.class_prior_) = class_distribution(y, sample_weight)
159 
160         if self.strategy == "constant":
161             for k in range(self.n_outputs_):
162                 if not any(constant[k][0] == c for c in self.classes_[k]):
163                     # Checking in case of constant strategy if the constant
164                     # provided by the user is in y.
165                     err_msg = ("The constant target value must be present in "
166                                "the training data. You provided constant={}. "
167                                "Possible values are: {}."
168                                .format(self.constant, list(self.classes_[k])))
169                     raise ValueError(err_msg)
170 
171         if self.n_outputs_ == 1:
172             self.n_classes_ = self.n_classes_[0]
173             self.classes_ = self.classes_[0]
174             self.class_prior_ = self.class_prior_[0]
175 
176         return self
177 
178     def predict(self, X):
179         """Perform classification on test vectors X.
180 
181         Parameters
182         ----------
183         X : {array-like, object with finite length or shape}
184             Training data, requires length = n_samples
185 
186         Returns
187         -------
188         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
189             Predicted target values for X.
190         """
191         check_is_fitted(self)
192 
193         # numpy random_state expects Python int and not long as size argument
194         # under Windows
195         n_samples = _num_samples(X)
196         rs = check_random_state(self.random_state)
197 
198         n_classes_ = self.n_classes_
199         classes_ = self.classes_
200         class_prior_ = self.class_prior_
201         constant = self.constant
202         if self.n_outputs_ == 1:
203             # Get same type even for self.n_outputs_ == 1
204             n_classes_ = [n_classes_]
205             classes_ = [classes_]
206             class_prior_ = [class_prior_]
207             constant = [constant]
208         # Compute probability only once
209         if self.strategy == "stratified":
210             proba = self.predict_proba(X)
211             if self.n_outputs_ == 1:
212                 proba = [proba]
213 
214         if self.sparse_output_:
215             class_prob = None
216             if self.strategy in ("most_frequent", "prior"):
217                 classes_ = [np.array([cp.argmax()]) for cp in class_prior_]
218 
219             elif self.strategy == "stratified":
220                 class_prob = class_prior_
221 
222             elif self.strategy == "uniform":
223                 raise ValueError("Sparse target prediction is not "
224                                  "supported with the uniform strategy")
225 
226             elif self.strategy == "constant":
227                 classes_ = [np.array([c]) for c in constant]
228 
229             y = _random_choice_csc(n_samples, classes_, class_prob,
230                                   self.random_state)
231         else:
232             if self.strategy in ("most_frequent", "prior"):
233                 y = np.tile([classes_[k][class_prior_[k].argmax()] for
234                              k in range(self.n_outputs_)], [n_samples, 1])
235 
236             elif self.strategy == "stratified":
237                 y = np.vstack([classes_[k][proba[k].argmax(axis=1)] for
238                                k in range(self.n_outputs_)]).T
239 
240             elif self.strategy == "uniform":
241                 ret = [classes_[k][rs.randint(n_classes_[k], size=n_samples)]
242                        for k in range(self.n_outputs_)]
243                 y = np.vstack(ret).T
244 
245             elif self.strategy == "constant":
246                 y = np.tile(self.constant, (n_samples, 1))
247 
248             if self.n_outputs_ == 1:
249                 y = np.ravel(y)
250 
251         return y
252 
253     def predict_proba(self, X):
254         """
255         Return probability estimates for the test vectors X.
256 
257         Parameters
258         ----------
259         X : {array-like, object with finite length or shape}
260             Training data, requires length = n_samples
261 
262         Returns
263         -------
264         P : array-like or list of array-lke of shape (n_samples, n_classes)
265             Returns the probability of the sample for each class in
266             the model, where classes are ordered arithmetically, for each
267             output.
268         """
269         check_is_fitted(self)
270 
271         # numpy random_state expects Python int and not long as size argument
272         # under Windows
273         n_samples = _num_samples(X)
274         rs = check_random_state(self.random_state)
275 
276         n_classes_ = self.n_classes_
277         classes_ = self.classes_
278         class_prior_ = self.class_prior_
279         constant = self.constant
280         if self.n_outputs_ == 1:
281             # Get same type even for self.n_outputs_ == 1
282             n_classes_ = [n_classes_]
283             classes_ = [classes_]
284             class_prior_ = [class_prior_]
285             constant = [constant]
286 
287         P = []
288         for k in range(self.n_outputs_):
289             if self.strategy == "most_frequent":
290                 ind = class_prior_[k].argmax()
291                 out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)
292                 out[:, ind] = 1.0
293             elif self.strategy == "prior":
294                 out = np.ones((n_samples, 1)) * class_prior_[k]
295 
296             elif self.strategy == "stratified":
297                 out = rs.multinomial(1, class_prior_[k], size=n_samples)
298                 out = out.astype(np.float64)
299 
300             elif self.strategy == "uniform":
301                 out = np.ones((n_samples, n_classes_[k]), dtype=np.float64)
302                 out /= n_classes_[k]
303 
304             elif self.strategy == "constant":
305                 ind = np.where(classes_[k] == constant[k])
306                 out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)
307                 out[:, ind] = 1.0
308 
309             P.append(out)
310 
311         if self.n_outputs_ == 1:
312             P = P[0]
313 
314         return P
315 
316     def predict_log_proba(self, X):
317         """
318         Return log probability estimates for the test vectors X.
319 
320         Parameters
321         ----------
322         X : {array-like, object with finite length or shape}
323             Training data, requires length = n_samples
324 
325         Returns
326         -------
327         P : array-like or list of array-like of shape (n_samples, n_classes)
328             Returns the log probability of the sample for each class in
329             the model, where classes are ordered arithmetically for each
330             output.
331         """
332         proba = self.predict_proba(X)
333         if self.n_outputs_ == 1:
334             return np.log(proba)
335         else:
336             return [np.log(p) for p in proba]
337 
338     def _more_tags(self):
339         return {'poor_score': True, 'no_validation': True}
340 
341     def score(self, X, y, sample_weight=None):
342         """Returns the mean accuracy on the given test data and labels.
343 
344         In multi-label classification, this is the subset accuracy
345         which is a harsh metric since you require for each sample that
346         each label set be correctly predicted.
347 
348         Parameters
349         ----------
350         X : {array-like, None}
351             Test samples with shape = (n_samples, n_features) or
352             None. Passing None as test samples gives the same result
353             as passing real test samples, since DummyClassifier
354             operates independently of the sampled observations.
355 
356         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
357             True labels for X.
358 
359         sample_weight : array-like of shape (n_samples,), default=None
360             Sample weights.
361 
362         Returns
363         -------
364         score : float
365             Mean accuracy of self.predict(X) wrt. y.
366 
367         """
368         if X is None:
369             X = np.zeros(shape=(len(y), 1))
370         return super().score(X, y, sample_weight)
371 
372     @deprecated(
373         "The outputs_2d_ attribute is deprecated in version 0.22 "
374         "and will be removed in version 0.24. It is equivalent to "
375         "n_outputs_ > 1."
376     )
377     @property
378     def outputs_2d_(self):
379         return self.n_outputs_ != 1

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/cluster/_dbscan.py</file>
<class>DBSCAN</class>
<method>fit</method>
<code>
284     def fit(self, X, y=None, sample_weight=None):
285         """Perform DBSCAN clustering from features, or distance matrix.
286 
287         Parameters
288         ----------
289         X : array-like or sparse matrix, shape (n_samples, n_features), or \
290             (n_samples, n_samples)
291             Training instances to cluster, or distances between instances if
292             ``metric='precomputed'``. If a sparse matrix is provided, it will
293             be converted into a sparse ``csr_matrix``.
294 
295         sample_weight : array, shape (n_samples,), optional
296             Weight of each sample, such that a sample with a weight of at least
297             ``min_samples`` is by itself a core sample; a sample with a
298             negative weight may inhibit its eps-neighbor from being core.
299             Note that weights are absolute, and default to 1.
300 
301         y : Ignored
302             Not used, present here for API consistency by convention.
303 
304         Returns
305         -------
306         self
307 
308         """
309         X = check_array(X, accept_sparse='csr')
310 
311         if not self.eps > 0.0:
312             raise ValueError("eps must be positive.")
313 
314         if sample_weight is not None:
315             sample_weight = np.asarray(sample_weight)
316             check_consistent_length(X, sample_weight)
317 
318         # Calculate neighborhood for all samples. This leaves the original
319         # point in, which needs to be considered later (i.e. point i is in the
320         # neighborhood of point i. While True, its useless information)
321         if self.metric == 'precomputed' and sparse.issparse(X):
322             # set the diagonal to explicit values, as a point is its own
323             # neighbor
324             with warnings.catch_warnings():
325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)
326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place
327 
328         neighbors_model = NearestNeighbors(
329             radius=self.eps, algorithm=self.algorithm,
330             leaf_size=self.leaf_size, metric=self.metric,
331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)
332         neighbors_model.fit(X)
333         # This has worst case O(n^2) memory complexity
334         neighborhoods = neighbors_model.radius_neighbors(X,
335                                                          return_distance=False)
336 
337         if sample_weight is None:
338             n_neighbors = np.array([len(neighbors)
339                                     for neighbors in neighborhoods])
340         else:
341             n_neighbors = np.array([np.sum(sample_weight[neighbors])
342                                     for neighbors in neighborhoods])
343 
344         # Initially, all samples are noise.
345         labels = np.full(X.shape[0], -1, dtype=np.intp)
346 
347         # A list of all core samples found.
348         core_samples = np.asarray(n_neighbors >= self.min_samples,
349                                   dtype=np.uint8)
350         dbscan_inner(core_samples, neighborhoods, labels)
351 
352         self.core_sample_indices_ = np.where(core_samples)[0]
353         self.labels_ = labels
354 
355         if len(self.core_sample_indices_):
356             # fix for scipy sparse indexing issue
357             self.components_ = X[self.core_sample_indices_].copy()
358         else:
359             # no core samples
360             self.components_ = np.empty((0, X.shape[1]))
361         return self

</code><intended_behavior>Incorporate _check_sample_weight(sample_weight, X) to not only validate sample_weight but also standardize its dtype and shape, ensuring that sample_weight adheres to the requirements for DBSCAN clustering operations. This adjustment facilitates a standardized approach to handling sample weights across clustering operations.</intended_behavior>, <file>sklearn/cluster/_dbscan.py</file>
<class>DBSCAN</class>
<method>None</method>
<code>
147 class DBSCAN(ClusterMixin, BaseEstimator):
148     """Perform DBSCAN clustering from vector array or distance matrix.
149 
150     DBSCAN - Density-Based Spatial Clustering of Applications with Noise.
151     Finds core samples of high density and expands clusters from them.
152     Good for data which contains clusters of similar density.
153 
154     Read more in the :ref:`User Guide <dbscan>`.
155 
156     Parameters
157     ----------
158     eps : float, optional
159         The maximum distance between two samples for one to be considered
160         as in the neighborhood of the other. This is not a maximum bound
161         on the distances of points within a cluster. This is the most
162         important DBSCAN parameter to choose appropriately for your data set
163         and distance function.
164 
165     min_samples : int, optional
166         The number of samples (or total weight) in a neighborhood for a point
167         to be considered as a core point. This includes the point itself.
168 
169     metric : string, or callable
170         The metric to use when calculating distance between instances in a
171         feature array. If metric is a string or callable, it must be one of
172         the options allowed by :func:`sklearn.metrics.pairwise_distances` for
173         its metric parameter.
174         If metric is "precomputed", X is assumed to be a distance matrix and
175         must be square. X may be a :term:`Glossary <sparse graph>`, in which
176         case only "nonzero" elements may be considered neighbors for DBSCAN.
177 
178         .. versionadded:: 0.17
179            metric *precomputed* to accept precomputed sparse matrix.
180 
181     metric_params : dict, optional
182         Additional keyword arguments for the metric function.
183 
184         .. versionadded:: 0.19
185 
186     algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional
187         The algorithm to be used by the NearestNeighbors module
188         to compute pointwise distances and find nearest neighbors.
189         See NearestNeighbors module documentation for details.
190 
191     leaf_size : int, optional (default = 30)
192         Leaf size passed to BallTree or cKDTree. This can affect the speed
193         of the construction and query, as well as the memory required
194         to store the tree. The optimal value depends
195         on the nature of the problem.
196 
197     p : float, optional
198         The power of the Minkowski metric to be used to calculate distance
199         between points.
200 
201     n_jobs : int or None, optional (default=None)
202         The number of parallel jobs to run.
203         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
204         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
205         for more details.
206 
207     Attributes
208     ----------
209     core_sample_indices_ : array, shape = [n_core_samples]
210         Indices of core samples.
211 
212     components_ : array, shape = [n_core_samples, n_features]
213         Copy of each core sample found by training.
214 
215     labels_ : array, shape = [n_samples]
216         Cluster labels for each point in the dataset given to fit().
217         Noisy samples are given the label -1.
218 
219     Examples
220     --------
221     >>> from sklearn.cluster import DBSCAN
222     >>> import numpy as np
223     >>> X = np.array([[1, 2], [2, 2], [2, 3],
224     ...               [8, 7], [8, 8], [25, 80]])
225     >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)
226     >>> clustering.labels_
227     array([ 0,  0,  0,  1,  1, -1])
228     >>> clustering
229     DBSCAN(eps=3, min_samples=2)
230 
231     See also
232     --------
233     OPTICS
234         A similar clustering at multiple values of eps. Our implementation
235         is optimized for memory usage.
236 
237     Notes
238     -----
239     For an example, see :ref:`examples/cluster/plot_dbscan.py
240     <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.
241 
242     This implementation bulk-computes all neighborhood queries, which increases
243     the memory complexity to O(n.d) where d is the average number of neighbors,
244     while original DBSCAN had memory complexity O(n). It may attract a higher
245     memory complexity when querying these nearest neighborhoods, depending
246     on the ``algorithm``.
247 
248     One way to avoid the query complexity is to pre-compute sparse
249     neighborhoods in chunks using
250     :func:`NearestNeighbors.radius_neighbors_graph
251     <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with
252     ``mode='distance'``, then using ``metric='precomputed'`` here.
253 
254     Another way to reduce memory and computation time is to remove
255     (near-)duplicate points and use ``sample_weight`` instead.
256 
257     :class:`cluster.OPTICS` provides a similar clustering with lower memory
258     usage.
259 
260     References
261     ----------
262     Ester, M., H. P. Kriegel, J. Sander, and X. Xu, "A Density-Based
263     Algorithm for Discovering Clusters in Large Spatial Databases with Noise".
264     In: Proceedings of the 2nd International Conference on Knowledge Discovery
265     and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996
266 
267     Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).
268     DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.
269     ACM Transactions on Database Systems (TODS), 42(3), 19.
270     """
271 
272     def __init__(self, eps=0.5, min_samples=5, metric='euclidean',
273                  metric_params=None, algorithm='auto', leaf_size=30, p=None,
274                  n_jobs=None):
275         self.eps = eps
276         self.min_samples = min_samples
277         self.metric = metric
278         self.metric_params = metric_params
279         self.algorithm = algorithm
280         self.leaf_size = leaf_size
281         self.p = p
282         self.n_jobs = n_jobs
283 
284     def fit(self, X, y=None, sample_weight=None):
285         """Perform DBSCAN clustering from features, or distance matrix.
286 
287         Parameters
288         ----------
289         X : array-like or sparse matrix, shape (n_samples, n_features), or \
290             (n_samples, n_samples)
291             Training instances to cluster, or distances between instances if
292             ``metric='precomputed'``. If a sparse matrix is provided, it will
293             be converted into a sparse ``csr_matrix``.
294 
295         sample_weight : array, shape (n_samples,), optional
296             Weight of each sample, such that a sample with a weight of at least
297             ``min_samples`` is by itself a core sample; a sample with a
298             negative weight may inhibit its eps-neighbor from being core.
299             Note that weights are absolute, and default to 1.
300 
301         y : Ignored
302             Not used, present here for API consistency by convention.
303 
304         Returns
305         -------
306         self
307 
308         """
309         X = check_array(X, accept_sparse='csr')
310 
311         if not self.eps > 0.0:
312             raise ValueError("eps must be positive.")
313 
314         if sample_weight is not None:
315             sample_weight = np.asarray(sample_weight)
316             check_consistent_length(X, sample_weight)
317 
318         # Calculate neighborhood for all samples. This leaves the original
319         # point in, which needs to be considered later (i.e. point i is in the
320         # neighborhood of point i. While True, its useless information)
321         if self.metric == 'precomputed' and sparse.issparse(X):
322             # set the diagonal to explicit values, as a point is its own
323             # neighbor
324             with warnings.catch_warnings():
325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)
326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place
327 
328         neighbors_model = NearestNeighbors(
329             radius=self.eps, algorithm=self.algorithm,
330             leaf_size=self.leaf_size, metric=self.metric,
331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)
332         neighbors_model.fit(X)
333         # This has worst case O(n^2) memory complexity
334         neighborhoods = neighbors_model.radius_neighbors(X,
335                                                          return_distance=False)
336 
337         if sample_weight is None:
338             n_neighbors = np.array([len(neighbors)
339                                     for neighbors in neighborhoods])
340         else:
341             n_neighbors = np.array([np.sum(sample_weight[neighbors])
342                                     for neighbors in neighborhoods])
343 
344         # Initially, all samples are noise.
345         labels = np.full(X.shape[0], -1, dtype=np.intp)
346 
347         # A list of all core samples found.
348         core_samples = np.asarray(n_neighbors >= self.min_samples,
349                                   dtype=np.uint8)
350         dbscan_inner(core_samples, neighborhoods, labels)
351 
352         self.core_sample_indices_ = np.where(core_samples)[0]
353         self.labels_ = labels
354 
355         if len(self.core_sample_indices_):
356             # fix for scipy sparse indexing issue
357             self.components_ = X[self.core_sample_indices_].copy()
358         else:
359             # no core samples
360             self.components_ = np.empty((0, X.shape[1]))
361         return self
362 
363     def fit_predict(self, X, y=None, sample_weight=None):
364         """Perform DBSCAN clustering from features or distance matrix,
365         and return cluster labels.
366 
367         Parameters
368         ----------
369         X : array-like or sparse matrix, shape (n_samples, n_features), or \
370             (n_samples, n_samples)
371             Training instances to cluster, or distances between instances if
372             ``metric='precomputed'``. If a sparse matrix is provided, it will
373             be converted into a sparse ``csr_matrix``.
374 
375         sample_weight : array, shape (n_samples,), optional
376             Weight of each sample, such that a sample with a weight of at least
377             ``min_samples`` is by itself a core sample; a sample with a
378             negative weight may inhibit its eps-neighbor from being core.
379             Note that weights are absolute, and default to 1.
380 
381         y : Ignored
382             Not used, present here for API consistency by convention.
383 
384         Returns
385         -------
386         labels : ndarray, shape (n_samples,)
387             Cluster labels. Noisy samples are given the label -1.
388         """
389         self.fit(X, sample_weight=sample_weight)
390         return self.labels_

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-17 17:58:30.258 | INFO     | Search completed. Bug locations: [<file>sklearn/dummy.py</file>
<class>DummyClassifier</class>
<method>fit</method>
<code>
101     def fit(self, X, y, sample_weight=None):
102         """Fit the random classifier.
103 
104         Parameters
105         ----------
106         X : {array-like, object with finite length or shape}
107             Training data, requires length = n_samples
108 
109         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
110             Target values.
111 
112         sample_weight : array-like of shape (n_samples,), default=None
113             Sample weights.
114 
115         Returns
116         -------
117         self : object
118         """
119         allowed_strategies = ("most_frequent", "stratified", "uniform",
120                               "constant", "prior")
121         if self.strategy not in allowed_strategies:
122             raise ValueError("Unknown strategy type: %s, expected one of %s."
123                              % (self.strategy, allowed_strategies))
124 
125         if self.strategy == "uniform" and sp.issparse(y):
126             y = y.toarray()
127             warnings.warn('A local copy of the target data has been converted '
128                           'to a numpy array. Predicting on sparse target data '
129                           'with the uniform strategy would not save memory '
130                           'and would be slower.',
131                           UserWarning)
132 
133         self.sparse_output_ = sp.issparse(y)
134 
135         if not self.sparse_output_:
136             y = np.asarray(y)
137             y = np.atleast_1d(y)
138 
139         if y.ndim == 1:
140             y = np.reshape(y, (-1, 1))
141 
142         self.n_outputs_ = y.shape[1]
143 
144         check_consistent_length(X, y, sample_weight)
145 
146         if self.strategy == "constant":
147             if self.constant is None:
148                 raise ValueError("Constant target value has to be specified "
149                                  "when the constant strategy is used.")
150             else:
151                 constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))
152                 if constant.shape[0] != self.n_outputs_:
153                     raise ValueError("Constant target value should have "
154                                      "shape (%d, 1)." % self.n_outputs_)
155 
156         (self.classes_,
157          self.n_classes_,
158          self.class_prior_) = class_distribution(y, sample_weight)
159 
160         if self.strategy == "constant":
161             for k in range(self.n_outputs_):
162                 if not any(constant[k][0] == c for c in self.classes_[k]):
163                     # Checking in case of constant strategy if the constant
164                     # provided by the user is in y.
165                     err_msg = ("The constant target value must be present in "
166                                "the training data. You provided constant={}. "
167                                "Possible values are: {}."
168                                .format(self.constant, list(self.classes_[k])))
169                     raise ValueError(err_msg)
170 
171         if self.n_outputs_ == 1:
172             self.n_classes_ = self.n_classes_[0]
173             self.classes_ = self.classes_[0]
174             self.class_prior_ = self.class_prior_[0]
175 
176         return self

</code><intended_behavior>This should be replaced with a call to sample_weight = _check_sample_weight(sample_weight, X) right after the method signature to ensure sample_weight is validated correctly and consistently, preserving the intended functionality of handling sample weights in classification tasks by DummyClassifier. This change centralizes the sample weight validation logic, enhancing maintainability.</intended_behavior>, <file>sklearn/dummy.py</file>
<class>DummyClassifier</class>
<method>None</method>
<code>
23 class DummyClassifier(MultiOutputMixin, ClassifierMixin, BaseEstimator):
24     """
25     DummyClassifier is a classifier that makes predictions using simple rules.
26 
27     This classifier is useful as a simple baseline to compare with other
28     (real) classifiers. Do not use it for real problems.
29 
30     Read more in the :ref:`User Guide <dummy_estimators>`.
31 
32     Parameters
33     ----------
34     strategy : str, default="stratified"
35         Strategy to use to generate predictions.
36 
37         * "stratified": generates predictions by respecting the training
38           set's class distribution.
39         * "most_frequent": always predicts the most frequent label in the
40           training set.
41         * "prior": always predicts the class that maximizes the class prior
42           (like "most_frequent") and ``predict_proba`` returns the class prior.
43         * "uniform": generates predictions uniformly at random.
44         * "constant": always predicts a constant label that is provided by
45           the user. This is useful for metrics that evaluate a non-majority
46           class
47 
48           .. versionadded:: 0.17
49              Dummy Classifier now supports prior fitting strategy using
50              parameter *prior*.
51 
52     random_state : int, RandomState instance or None, optional, default=None
53         If int, random_state is the seed used by the random number generator;
54         If RandomState instance, random_state is the random number generator;
55         If None, the random number generator is the RandomState instance used
56         by `np.random`.
57 
58     constant : int or str or array-like of shape (n_outputs,)
59         The explicit constant as predicted by the "constant" strategy. This
60         parameter is useful only for the "constant" strategy.
61 
62     Attributes
63     ----------
64     classes_ : array or list of array of shape (n_classes,)
65         Class labels for each output.
66 
67     n_classes_ : array or list of array of shape (n_classes,)
68         Number of label for each output.
69 
70     class_prior_ : array or list of array of shape (n_classes,)
71         Probability of each class for each output.
72 
73     n_outputs_ : int,
74         Number of outputs.
75 
76     sparse_output_ : bool,
77         True if the array returned from predict is to be in sparse CSC format.
78         Is automatically set to True if the input y is passed in sparse format.
79 
80     Examples
81     --------
82     >>> import numpy as np
83     >>> from sklearn.dummy import DummyClassifier
84     >>> X = np.array([-1, 1, 1, 1])
85     >>> y = np.array([0, 1, 1, 1])
86     >>> dummy_clf = DummyClassifier(strategy="most_frequent")
87     >>> dummy_clf.fit(X, y)
88     DummyClassifier(strategy='most_frequent')
89     >>> dummy_clf.predict(X)
90     array([1, 1, 1, 1])
91     >>> dummy_clf.score(X, y)
92     0.75
93     """
94 
95     def __init__(self, strategy="stratified", random_state=None,
96                  constant=None):
97         self.strategy = strategy
98         self.random_state = random_state
99         self.constant = constant
100 
101     def fit(self, X, y, sample_weight=None):
102         """Fit the random classifier.
103 
104         Parameters
105         ----------
106         X : {array-like, object with finite length or shape}
107             Training data, requires length = n_samples
108 
109         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
110             Target values.
111 
112         sample_weight : array-like of shape (n_samples,), default=None
113             Sample weights.
114 
115         Returns
116         -------
117         self : object
118         """
119         allowed_strategies = ("most_frequent", "stratified", "uniform",
120                               "constant", "prior")
121         if self.strategy not in allowed_strategies:
122             raise ValueError("Unknown strategy type: %s, expected one of %s."
123                              % (self.strategy, allowed_strategies))
124 
125         if self.strategy == "uniform" and sp.issparse(y):
126             y = y.toarray()
127             warnings.warn('A local copy of the target data has been converted '
128                           'to a numpy array. Predicting on sparse target data '
129                           'with the uniform strategy would not save memory '
130                           'and would be slower.',
131                           UserWarning)
132 
133         self.sparse_output_ = sp.issparse(y)
134 
135         if not self.sparse_output_:
136             y = np.asarray(y)
137             y = np.atleast_1d(y)
138 
139         if y.ndim == 1:
140             y = np.reshape(y, (-1, 1))
141 
142         self.n_outputs_ = y.shape[1]
143 
144         check_consistent_length(X, y, sample_weight)
145 
146         if self.strategy == "constant":
147             if self.constant is None:
148                 raise ValueError("Constant target value has to be specified "
149                                  "when the constant strategy is used.")
150             else:
151                 constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))
152                 if constant.shape[0] != self.n_outputs_:
153                     raise ValueError("Constant target value should have "
154                                      "shape (%d, 1)." % self.n_outputs_)
155 
156         (self.classes_,
157          self.n_classes_,
158          self.class_prior_) = class_distribution(y, sample_weight)
159 
160         if self.strategy == "constant":
161             for k in range(self.n_outputs_):
162                 if not any(constant[k][0] == c for c in self.classes_[k]):
163                     # Checking in case of constant strategy if the constant
164                     # provided by the user is in y.
165                     err_msg = ("The constant target value must be present in "
166                                "the training data. You provided constant={}. "
167                                "Possible values are: {}."
168                                .format(self.constant, list(self.classes_[k])))
169                     raise ValueError(err_msg)
170 
171         if self.n_outputs_ == 1:
172             self.n_classes_ = self.n_classes_[0]
173             self.classes_ = self.classes_[0]
174             self.class_prior_ = self.class_prior_[0]
175 
176         return self
177 
178     def predict(self, X):
179         """Perform classification on test vectors X.
180 
181         Parameters
182         ----------
183         X : {array-like, object with finite length or shape}
184             Training data, requires length = n_samples
185 
186         Returns
187         -------
188         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
189             Predicted target values for X.
190         """
191         check_is_fitted(self)
192 
193         # numpy random_state expects Python int and not long as size argument
194         # under Windows
195         n_samples = _num_samples(X)
196         rs = check_random_state(self.random_state)
197 
198         n_classes_ = self.n_classes_
199         classes_ = self.classes_
200         class_prior_ = self.class_prior_
201         constant = self.constant
202         if self.n_outputs_ == 1:
203             # Get same type even for self.n_outputs_ == 1
204             n_classes_ = [n_classes_]
205             classes_ = [classes_]
206             class_prior_ = [class_prior_]
207             constant = [constant]
208         # Compute probability only once
209         if self.strategy == "stratified":
210             proba = self.predict_proba(X)
211             if self.n_outputs_ == 1:
212                 proba = [proba]
213 
214         if self.sparse_output_:
215             class_prob = None
216             if self.strategy in ("most_frequent", "prior"):
217                 classes_ = [np.array([cp.argmax()]) for cp in class_prior_]
218 
219             elif self.strategy == "stratified":
220                 class_prob = class_prior_
221 
222             elif self.strategy == "uniform":
223                 raise ValueError("Sparse target prediction is not "
224                                  "supported with the uniform strategy")
225 
226             elif self.strategy == "constant":
227                 classes_ = [np.array([c]) for c in constant]
228 
229             y = _random_choice_csc(n_samples, classes_, class_prob,
230                                   self.random_state)
231         else:
232             if self.strategy in ("most_frequent", "prior"):
233                 y = np.tile([classes_[k][class_prior_[k].argmax()] for
234                              k in range(self.n_outputs_)], [n_samples, 1])
235 
236             elif self.strategy == "stratified":
237                 y = np.vstack([classes_[k][proba[k].argmax(axis=1)] for
238                                k in range(self.n_outputs_)]).T
239 
240             elif self.strategy == "uniform":
241                 ret = [classes_[k][rs.randint(n_classes_[k], size=n_samples)]
242                        for k in range(self.n_outputs_)]
243                 y = np.vstack(ret).T
244 
245             elif self.strategy == "constant":
246                 y = np.tile(self.constant, (n_samples, 1))
247 
248             if self.n_outputs_ == 1:
249                 y = np.ravel(y)
250 
251         return y
252 
253     def predict_proba(self, X):
254         """
255         Return probability estimates for the test vectors X.
256 
257         Parameters
258         ----------
259         X : {array-like, object with finite length or shape}
260             Training data, requires length = n_samples
261 
262         Returns
263         -------
264         P : array-like or list of array-lke of shape (n_samples, n_classes)
265             Returns the probability of the sample for each class in
266             the model, where classes are ordered arithmetically, for each
267             output.
268         """
269         check_is_fitted(self)
270 
271         # numpy random_state expects Python int and not long as size argument
272         # under Windows
273         n_samples = _num_samples(X)
274         rs = check_random_state(self.random_state)
275 
276         n_classes_ = self.n_classes_
277         classes_ = self.classes_
278         class_prior_ = self.class_prior_
279         constant = self.constant
280         if self.n_outputs_ == 1:
281             # Get same type even for self.n_outputs_ == 1
282             n_classes_ = [n_classes_]
283             classes_ = [classes_]
284             class_prior_ = [class_prior_]
285             constant = [constant]
286 
287         P = []
288         for k in range(self.n_outputs_):
289             if self.strategy == "most_frequent":
290                 ind = class_prior_[k].argmax()
291                 out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)
292                 out[:, ind] = 1.0
293             elif self.strategy == "prior":
294                 out = np.ones((n_samples, 1)) * class_prior_[k]
295 
296             elif self.strategy == "stratified":
297                 out = rs.multinomial(1, class_prior_[k], size=n_samples)
298                 out = out.astype(np.float64)
299 
300             elif self.strategy == "uniform":
301                 out = np.ones((n_samples, n_classes_[k]), dtype=np.float64)
302                 out /= n_classes_[k]
303 
304             elif self.strategy == "constant":
305                 ind = np.where(classes_[k] == constant[k])
306                 out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)
307                 out[:, ind] = 1.0
308 
309             P.append(out)
310 
311         if self.n_outputs_ == 1:
312             P = P[0]
313 
314         return P
315 
316     def predict_log_proba(self, X):
317         """
318         Return log probability estimates for the test vectors X.
319 
320         Parameters
321         ----------
322         X : {array-like, object with finite length or shape}
323             Training data, requires length = n_samples
324 
325         Returns
326         -------
327         P : array-like or list of array-like of shape (n_samples, n_classes)
328             Returns the log probability of the sample for each class in
329             the model, where classes are ordered arithmetically for each
330             output.
331         """
332         proba = self.predict_proba(X)
333         if self.n_outputs_ == 1:
334             return np.log(proba)
335         else:
336             return [np.log(p) for p in proba]
337 
338     def _more_tags(self):
339         return {'poor_score': True, 'no_validation': True}
340 
341     def score(self, X, y, sample_weight=None):
342         """Returns the mean accuracy on the given test data and labels.
343 
344         In multi-label classification, this is the subset accuracy
345         which is a harsh metric since you require for each sample that
346         each label set be correctly predicted.
347 
348         Parameters
349         ----------
350         X : {array-like, None}
351             Test samples with shape = (n_samples, n_features) or
352             None. Passing None as test samples gives the same result
353             as passing real test samples, since DummyClassifier
354             operates independently of the sampled observations.
355 
356         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
357             True labels for X.
358 
359         sample_weight : array-like of shape (n_samples,), default=None
360             Sample weights.
361 
362         Returns
363         -------
364         score : float
365             Mean accuracy of self.predict(X) wrt. y.
366 
367         """
368         if X is None:
369             X = np.zeros(shape=(len(y), 1))
370         return super().score(X, y, sample_weight)
371 
372     @deprecated(
373         "The outputs_2d_ attribute is deprecated in version 0.22 "
374         "and will be removed in version 0.24. It is equivalent to "
375         "n_outputs_ > 1."
376     )
377     @property
378     def outputs_2d_(self):
379         return self.n_outputs_ != 1

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/cluster/_dbscan.py</file>
<class>DBSCAN</class>
<method>fit</method>
<code>
284     def fit(self, X, y=None, sample_weight=None):
285         """Perform DBSCAN clustering from features, or distance matrix.
286 
287         Parameters
288         ----------
289         X : array-like or sparse matrix, shape (n_samples, n_features), or \
290             (n_samples, n_samples)
291             Training instances to cluster, or distances between instances if
292             ``metric='precomputed'``. If a sparse matrix is provided, it will
293             be converted into a sparse ``csr_matrix``.
294 
295         sample_weight : array, shape (n_samples,), optional
296             Weight of each sample, such that a sample with a weight of at least
297             ``min_samples`` is by itself a core sample; a sample with a
298             negative weight may inhibit its eps-neighbor from being core.
299             Note that weights are absolute, and default to 1.
300 
301         y : Ignored
302             Not used, present here for API consistency by convention.
303 
304         Returns
305         -------
306         self
307 
308         """
309         X = check_array(X, accept_sparse='csr')
310 
311         if not self.eps > 0.0:
312             raise ValueError("eps must be positive.")
313 
314         if sample_weight is not None:
315             sample_weight = np.asarray(sample_weight)
316             check_consistent_length(X, sample_weight)
317 
318         # Calculate neighborhood for all samples. This leaves the original
319         # point in, which needs to be considered later (i.e. point i is in the
320         # neighborhood of point i. While True, its useless information)
321         if self.metric == 'precomputed' and sparse.issparse(X):
322             # set the diagonal to explicit values, as a point is its own
323             # neighbor
324             with warnings.catch_warnings():
325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)
326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place
327 
328         neighbors_model = NearestNeighbors(
329             radius=self.eps, algorithm=self.algorithm,
330             leaf_size=self.leaf_size, metric=self.metric,
331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)
332         neighbors_model.fit(X)
333         # This has worst case O(n^2) memory complexity
334         neighborhoods = neighbors_model.radius_neighbors(X,
335                                                          return_distance=False)
336 
337         if sample_weight is None:
338             n_neighbors = np.array([len(neighbors)
339                                     for neighbors in neighborhoods])
340         else:
341             n_neighbors = np.array([np.sum(sample_weight[neighbors])
342                                     for neighbors in neighborhoods])
343 
344         # Initially, all samples are noise.
345         labels = np.full(X.shape[0], -1, dtype=np.intp)
346 
347         # A list of all core samples found.
348         core_samples = np.asarray(n_neighbors >= self.min_samples,
349                                   dtype=np.uint8)
350         dbscan_inner(core_samples, neighborhoods, labels)
351 
352         self.core_sample_indices_ = np.where(core_samples)[0]
353         self.labels_ = labels
354 
355         if len(self.core_sample_indices_):
356             # fix for scipy sparse indexing issue
357             self.components_ = X[self.core_sample_indices_].copy()
358         else:
359             # no core samples
360             self.components_ = np.empty((0, X.shape[1]))
361         return self

</code><intended_behavior>Incorporate _check_sample_weight(sample_weight, X) to not only validate sample_weight but also standardize its dtype and shape, ensuring that sample_weight adheres to the requirements for DBSCAN clustering operations. This adjustment facilitates a standardized approach to handling sample weights across clustering operations.</intended_behavior>, <file>sklearn/cluster/_dbscan.py</file>
<class>DBSCAN</class>
<method>None</method>
<code>
147 class DBSCAN(ClusterMixin, BaseEstimator):
148     """Perform DBSCAN clustering from vector array or distance matrix.
149 
150     DBSCAN - Density-Based Spatial Clustering of Applications with Noise.
151     Finds core samples of high density and expands clusters from them.
152     Good for data which contains clusters of similar density.
153 
154     Read more in the :ref:`User Guide <dbscan>`.
155 
156     Parameters
157     ----------
158     eps : float, optional
159         The maximum distance between two samples for one to be considered
160         as in the neighborhood of the other. This is not a maximum bound
161         on the distances of points within a cluster. This is the most
162         important DBSCAN parameter to choose appropriately for your data set
163         and distance function.
164 
165     min_samples : int, optional
166         The number of samples (or total weight) in a neighborhood for a point
167         to be considered as a core point. This includes the point itself.
168 
169     metric : string, or callable
170         The metric to use when calculating distance between instances in a
171         feature array. If metric is a string or callable, it must be one of
172         the options allowed by :func:`sklearn.metrics.pairwise_distances` for
173         its metric parameter.
174         If metric is "precomputed", X is assumed to be a distance matrix and
175         must be square. X may be a :term:`Glossary <sparse graph>`, in which
176         case only "nonzero" elements may be considered neighbors for DBSCAN.
177 
178         .. versionadded:: 0.17
179            metric *precomputed* to accept precomputed sparse matrix.
180 
181     metric_params : dict, optional
182         Additional keyword arguments for the metric function.
183 
184         .. versionadded:: 0.19
185 
186     algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional
187         The algorithm to be used by the NearestNeighbors module
188         to compute pointwise distances and find nearest neighbors.
189         See NearestNeighbors module documentation for details.
190 
191     leaf_size : int, optional (default = 30)
192         Leaf size passed to BallTree or cKDTree. This can affect the speed
193         of the construction and query, as well as the memory required
194         to store the tree. The optimal value depends
195         on the nature of the problem.
196 
197     p : float, optional
198         The power of the Minkowski metric to be used to calculate distance
199         between points.
200 
201     n_jobs : int or None, optional (default=None)
202         The number of parallel jobs to run.
203         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
204         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
205         for more details.
206 
207     Attributes
208     ----------
209     core_sample_indices_ : array, shape = [n_core_samples]
210         Indices of core samples.
211 
212     components_ : array, shape = [n_core_samples, n_features]
213         Copy of each core sample found by training.
214 
215     labels_ : array, shape = [n_samples]
216         Cluster labels for each point in the dataset given to fit().
217         Noisy samples are given the label -1.
218 
219     Examples
220     --------
221     >>> from sklearn.cluster import DBSCAN
222     >>> import numpy as np
223     >>> X = np.array([[1, 2], [2, 2], [2, 3],
224     ...               [8, 7], [8, 8], [25, 80]])
225     >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)
226     >>> clustering.labels_
227     array([ 0,  0,  0,  1,  1, -1])
228     >>> clustering
229     DBSCAN(eps=3, min_samples=2)
230 
231     See also
232     --------
233     OPTICS
234         A similar clustering at multiple values of eps. Our implementation
235         is optimized for memory usage.
236 
237     Notes
238     -----
239     For an example, see :ref:`examples/cluster/plot_dbscan.py
240     <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.
241 
242     This implementation bulk-computes all neighborhood queries, which increases
243     the memory complexity to O(n.d) where d is the average number of neighbors,
244     while original DBSCAN had memory complexity O(n). It may attract a higher
245     memory complexity when querying these nearest neighborhoods, depending
246     on the ``algorithm``.
247 
248     One way to avoid the query complexity is to pre-compute sparse
249     neighborhoods in chunks using
250     :func:`NearestNeighbors.radius_neighbors_graph
251     <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with
252     ``mode='distance'``, then using ``metric='precomputed'`` here.
253 
254     Another way to reduce memory and computation time is to remove
255     (near-)duplicate points and use ``sample_weight`` instead.
256 
257     :class:`cluster.OPTICS` provides a similar clustering with lower memory
258     usage.
259 
260     References
261     ----------
262     Ester, M., H. P. Kriegel, J. Sander, and X. Xu, "A Density-Based
263     Algorithm for Discovering Clusters in Large Spatial Databases with Noise".
264     In: Proceedings of the 2nd International Conference on Knowledge Discovery
265     and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996
266 
267     Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).
268     DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.
269     ACM Transactions on Database Systems (TODS), 42(3), 19.
270     """
271 
272     def __init__(self, eps=0.5, min_samples=5, metric='euclidean',
273                  metric_params=None, algorithm='auto', leaf_size=30, p=None,
274                  n_jobs=None):
275         self.eps = eps
276         self.min_samples = min_samples
277         self.metric = metric
278         self.metric_params = metric_params
279         self.algorithm = algorithm
280         self.leaf_size = leaf_size
281         self.p = p
282         self.n_jobs = n_jobs
283 
284     def fit(self, X, y=None, sample_weight=None):
285         """Perform DBSCAN clustering from features, or distance matrix.
286 
287         Parameters
288         ----------
289         X : array-like or sparse matrix, shape (n_samples, n_features), or \
290             (n_samples, n_samples)
291             Training instances to cluster, or distances between instances if
292             ``metric='precomputed'``. If a sparse matrix is provided, it will
293             be converted into a sparse ``csr_matrix``.
294 
295         sample_weight : array, shape (n_samples,), optional
296             Weight of each sample, such that a sample with a weight of at least
297             ``min_samples`` is by itself a core sample; a sample with a
298             negative weight may inhibit its eps-neighbor from being core.
299             Note that weights are absolute, and default to 1.
300 
301         y : Ignored
302             Not used, present here for API consistency by convention.
303 
304         Returns
305         -------
306         self
307 
308         """
309         X = check_array(X, accept_sparse='csr')
310 
311         if not self.eps > 0.0:
312             raise ValueError("eps must be positive.")
313 
314         if sample_weight is not None:
315             sample_weight = np.asarray(sample_weight)
316             check_consistent_length(X, sample_weight)
317 
318         # Calculate neighborhood for all samples. This leaves the original
319         # point in, which needs to be considered later (i.e. point i is in the
320         # neighborhood of point i. While True, its useless information)
321         if self.metric == 'precomputed' and sparse.issparse(X):
322             # set the diagonal to explicit values, as a point is its own
323             # neighbor
324             with warnings.catch_warnings():
325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)
326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place
327 
328         neighbors_model = NearestNeighbors(
329             radius=self.eps, algorithm=self.algorithm,
330             leaf_size=self.leaf_size, metric=self.metric,
331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)
332         neighbors_model.fit(X)
333         # This has worst case O(n^2) memory complexity
334         neighborhoods = neighbors_model.radius_neighbors(X,
335                                                          return_distance=False)
336 
337         if sample_weight is None:
338             n_neighbors = np.array([len(neighbors)
339                                     for neighbors in neighborhoods])
340         else:
341             n_neighbors = np.array([np.sum(sample_weight[neighbors])
342                                     for neighbors in neighborhoods])
343 
344         # Initially, all samples are noise.
345         labels = np.full(X.shape[0], -1, dtype=np.intp)
346 
347         # A list of all core samples found.
348         core_samples = np.asarray(n_neighbors >= self.min_samples,
349                                   dtype=np.uint8)
350         dbscan_inner(core_samples, neighborhoods, labels)
351 
352         self.core_sample_indices_ = np.where(core_samples)[0]
353         self.labels_ = labels
354 
355         if len(self.core_sample_indices_):
356             # fix for scipy sparse indexing issue
357             self.components_ = X[self.core_sample_indices_].copy()
358         else:
359             # no core samples
360             self.components_ = np.empty((0, X.shape[1]))
361         return self
362 
363     def fit_predict(self, X, y=None, sample_weight=None):
364         """Perform DBSCAN clustering from features or distance matrix,
365         and return cluster labels.
366 
367         Parameters
368         ----------
369         X : array-like or sparse matrix, shape (n_samples, n_features), or \
370             (n_samples, n_samples)
371             Training instances to cluster, or distances between instances if
372             ``metric='precomputed'``. If a sparse matrix is provided, it will
373             be converted into a sparse ``csr_matrix``.
374 
375         sample_weight : array, shape (n_samples,), optional
376             Weight of each sample, such that a sample with a weight of at least
377             ``min_samples`` is by itself a core sample; a sample with a
378             negative weight may inhibit its eps-neighbor from being core.
379             Note that weights are absolute, and default to 1.
380 
381         y : Ignored
382             Not used, present here for API consistency by convention.
383 
384         Returns
385         -------
386         labels : ndarray, shape (n_samples,)
387             Cluster labels. Noisy samples are given the label -1.
388         """
389         self.fit(X, sample_weight=sample_weight)
390         return self.labels_

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-17 17:58:30.258 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-17 17:58:30.258 | INFO     | Search-only: skipping patch selection
2025-10-17 17:58:30.348 | INFO     | Task scikit-learn__scikit-learn-15495 completed successfully.
2025-10-17 17:58:30.348 | INFO     | No patch generated. You can try running ACR again.
