[
    {
        "rel_file_path": "sklearn/feature_selection/mutual_info_.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/feature_selection/mutual_info_.py",
        "start": 195,
        "end": 290,
        "class_name": null,
        "method_name": "_estimate_mi",
        "code": "195 def _estimate_mi(X, y, discrete_features='auto', discrete_target=False,\n196                  n_neighbors=3, copy=True, random_state=None):\n197     \"\"\"Estimate mutual information between the features and the target.\n198 \n199     Parameters\n200     ----------\n201     X : array_like or sparse matrix, shape (n_samples, n_features)\n202         Feature matrix.\n203 \n204     y : array_like, shape (n_samples,)\n205         Target vector.\n206 \n207     discrete_features : {'auto', bool, array_like}, default 'auto'\n208         If bool, then determines whether to consider all features discrete\n209         or continuous. If array, then it should be either a boolean mask\n210         with shape (n_features,) or array with indices of discrete features.\n211         If 'auto', it is assigned to False for dense `X` and to True for\n212         sparse `X`.\n213 \n214     discrete_target : bool, default False\n215         Whether to consider `y` as a discrete variable.\n216 \n217     n_neighbors : int, default 3\n218         Number of neighbors to use for MI estimation for continuous variables,\n219         see [1]_ and [2]_. Higher values reduce variance of the estimation, but\n220         could introduce a bias.\n221 \n222     copy : bool, default True\n223         Whether to make a copy of the given data. If set to False, the initial\n224         data will be overwritten.\n225 \n226     random_state : int, RandomState instance or None, optional, default None\n227         The seed of the pseudo random number generator for adding small noise\n228         to continuous variables in order to remove repeated values.  If int,\n229         random_state is the seed used by the random number generator; If\n230         RandomState instance, random_state is the random number generator; If\n231         None, the random number generator is the RandomState instance used by\n232         `np.random`.\n233 \n234     Returns\n235     -------\n236     mi : ndarray, shape (n_features,)\n237         Estimated mutual information between each feature and the target.\n238         A negative value will be replaced by 0.\n239 \n240     References\n241     ----------\n242     .. [1] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual\n243            information\". Phys. Rev. E 69, 2004.\n244     .. [2] B. C. Ross \"Mutual Information between Discrete and Continuous\n245            Data Sets\". PLoS ONE 9(2), 2014.\n246     \"\"\"\n247     X, y = check_X_y(X, y, accept_sparse='csc', y_numeric=not discrete_target)\n248     n_samples, n_features = X.shape\n249 \n250     if discrete_features == 'auto':\n251         discrete_features = issparse(X)\n252 \n253     if isinstance(discrete_features, bool):\n254         discrete_mask = np.empty(n_features, dtype=bool)\n255         discrete_mask.fill(discrete_features)\n256     else:\n257         discrete_features = np.asarray(discrete_features)\n258         if discrete_features.dtype != 'bool':\n259             discrete_mask = np.zeros(n_features, dtype=bool)\n260             discrete_mask[discrete_features] = True\n261         else:\n262             discrete_mask = discrete_features\n263 \n264     continuous_mask = ~discrete_mask\n265     if np.any(continuous_mask) and issparse(X):\n266         raise ValueError(\"Sparse matrix `X` can't have continuous features.\")\n267 \n268     rng = check_random_state(random_state)\n269     if np.any(continuous_mask):\n270         if copy:\n271             X = X.copy()\n272 \n273         if not discrete_target:\n274             X[:, continuous_mask] = scale(X[:, continuous_mask],\n275                                           with_mean=False, copy=False)\n276 \n277         # Add small noise to continuous features as advised in Kraskov et. al.\n278         X = X.astype(float, **_astype_copy_false(X))\n279         means = np.maximum(1, np.mean(np.abs(X[:, continuous_mask]), axis=0))\n280         X[:, continuous_mask] += 1e-10 * means * rng.randn(\n281                 n_samples, np.sum(continuous_mask))\n282 \n283     if not discrete_target:\n284         y = scale(y, with_mean=False)\n285         y += 1e-10 * np.maximum(1, np.mean(np.abs(y))) * rng.randn(n_samples)\n286 \n287     mi = [_compute_mi(x, y, discrete_feature, discrete_target, n_neighbors) for\n288           x, discrete_feature in zip(_iterate_columns(X), discrete_mask)]\n289 \n290     return np.array(mi)\n",
        "intended_behavior": "The `_estimate_mi` function should accurately handle the `discrete_features` parameter in all its potential forms: as a string (specifically 'auto'), a boolean value, or an array (either an array of boolean masks or an array of indices).\n- When `discrete_features` is 'auto', the function should check if `X` is sparse to determine whether features are discrete or not.\n- If `discrete_features` is a boolean, it should directly influence the processing of features as either all discrete or all continuous.\n- Most importantly, when `discrete_features` is an array, the function needs to incorporate logic to treat specified features as discrete based on their indices (if the array is of indices) or based on their boolean mask (if the array is of booleans). This detailed handling will ensure the function's future compatibility with numpy and enhance its robustness in feature selection tasks."
    }
]