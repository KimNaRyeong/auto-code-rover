2025-10-17 11:55:16.771 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-17 11:55:20.469 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=412, output_tokens=12, cost=0.004480
2025-10-17 11:55:37.451 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=592, output_tokens=744, cost=0.028240
2025-10-17 11:55:38.640 | INFO     | registered non reproducing test 0
2025-10-17 11:55:58.059 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2384, output_tokens=876, cost=0.050120
2025-10-17 11:56:14.321 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2384, output_tokens=772, cost=0.047000
2025-10-17 11:56:15.425 | INFO     | registered non reproducing test 2
2025-10-17 11:56:15.426 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-17 11:56:15.432 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 11:56:23.315 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=718, output_tokens=348, cost=0.017620
2025-10-17 11:56:23.323 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 11:56:25.878 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=702, output_tokens=41, cost=0.008250
2025-10-17 11:56:25.880 | DEBUG    | Extracted a valid json.
2025-10-17 11:56:25.880 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"ExpressionWrapper\")",
        "search_class(\"Value\")",
        "search_method(\"annotate\")"
    ],
    "bug_locations": []
}
2025-10-17 11:56:25.936 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-17 11:56:25.937 | DEBUG    | <Agent search> Analyzing search results.
2025-10-17 11:56:40.656 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2391, output_tokens=696, cost=0.044790
2025-10-17 11:56:40.671 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 11:56:49.569 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3303, output_tokens=386, cost=0.044610
2025-10-17 11:56:49.578 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 11:56:57.124 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=740, output_tokens=299, cost=0.016370
2025-10-17 11:56:57.125 | DEBUG    | Extracted a valid json.
2025-10-17 11:56:57.125 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "django/db/models/expressions.py",
            "class": "ExpressionWrapper",
            "method": "get_group_by_cols",
            "intended_behavior": "The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards."
        },
        {
            "file": "django/db/models/query.py",
            "class": "QuerySet",
            "method": "annotate",
            "intended_behavior": "Although not intrinsically faulty, a refined examination within the annotate method might be necessary to ensure it correctly handles the inclusion (or exclusion) of expressions wrapped by ExpressionWrapper in GROUP BY clauses. This does not suggest the annotate method itself is misbehaving, but a deeper integration or handling check with ExpressionWrapper's adjusted behavior (as proposed above) might be required to holistically resolve the issue. The annotate method should seamlessly support the refined logic of ExpressionWrapper.get_group_by_cols to exclude constant expressions from the GROUP BY clause where applicable, ensuring PostgreSQL compatibility."
        }
    ]
}
2025-10-17 11:56:57.150 | DEBUG    | Bug location extracted successfully: [<file>django/db/models/expressions.py</file>
<class>None</class>
<method>get_group_by_cols</method>
<code>
350     def get_group_by_cols(self, alias=None):
351         if not self.contains_aggregate:
352             return [self]
353         cols = []
354         for source in self.get_source_expressions():
355             cols.extend(source.get_group_by_cols())
356         return cols

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>None</class>
<method>get_group_by_cols</method>
<code>
578     def get_group_by_cols(self, alias=None):
579         return []

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>None</class>
<method>get_group_by_cols</method>
<code>
706     def get_group_by_cols(self, alias=None):
707         return []

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>None</class>
<method>get_group_by_cols</method>
<code>
731     def get_group_by_cols(self, alias=None):
732         return [self]

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>None</class>
<method>get_group_by_cols</method>
<code>
790     def get_group_by_cols(self, alias=None):
791         return [self]

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>None</class>
<method>get_group_by_cols</method>
<code>
829     def get_group_by_cols(self, alias=None):
830         return [self]

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>None</class>
<method>get_group_by_cols</method>
<code>
932     def get_group_by_cols(self, alias=None):
933         # This is not a complete expression and cannot be used in GROUP BY.
934         cols = []
935         for source in self.get_source_expressions():
936             cols.extend(source.get_group_by_cols())
937         return cols

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>None</class>
<method>get_group_by_cols</method>
<code>
1069     def get_group_by_cols(self, alias=None):
1070         if alias:
1071             return [Ref(alias, self)]
1072         external_cols = self.query.get_external_cols()
1073         if any(col.possibly_multivalued for col in external_cols):
1074             return [self]
1075         return external_cols

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>None</class>
<method>get_group_by_cols</method>
<code>
1173     def get_group_by_cols(self, alias=None):
1174         cols = []
1175         for source in self.get_source_expressions():
1176             cols.extend(source.get_group_by_cols())
1177         return cols

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>None</class>
<method>get_group_by_cols</method>
<code>
1283     def get_group_by_cols(self, alias=None):
1284         return []

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>None</class>
<method>get_group_by_cols</method>
<code>
1319     def get_group_by_cols(self, alias=None):
1320         return []

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>BaseExpression</class>
<method>get_group_by_cols</method>
<code>
350     def get_group_by_cols(self, alias=None):
351         if not self.contains_aggregate:
352             return [self]
353         cols = []
354         for source in self.get_source_expressions():
355             cols.extend(source.get_group_by_cols())
356         return cols

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>ResolvedOuterRef</class>
<method>get_group_by_cols</method>
<code>
578     def get_group_by_cols(self, alias=None):
579         return []

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Value</class>
<method>get_group_by_cols</method>
<code>
706     def get_group_by_cols(self, alias=None):
707         return []

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>RawSQL</class>
<method>get_group_by_cols</method>
<code>
731     def get_group_by_cols(self, alias=None):
732         return [self]

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Col</class>
<method>get_group_by_cols</method>
<code>
790     def get_group_by_cols(self, alias=None):
791         return [self]

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Ref</class>
<method>get_group_by_cols</method>
<code>
829     def get_group_by_cols(self, alias=None):
830         return [self]

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>When</class>
<method>get_group_by_cols</method>
<code>
932     def get_group_by_cols(self, alias=None):
933         # This is not a complete expression and cannot be used in GROUP BY.
934         cols = []
935         for source in self.get_source_expressions():
936             cols.extend(source.get_group_by_cols())
937         return cols

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Subquery</class>
<method>get_group_by_cols</method>
<code>
1069     def get_group_by_cols(self, alias=None):
1070         if alias:
1071             return [Ref(alias, self)]
1072         external_cols = self.query.get_external_cols()
1073         if any(col.possibly_multivalued for col in external_cols):
1074             return [self]
1075         return external_cols

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>OrderBy</class>
<method>get_group_by_cols</method>
<code>
1173     def get_group_by_cols(self, alias=None):
1174         cols = []
1175         for source in self.get_source_expressions():
1176             cols.extend(source.get_group_by_cols())
1177         return cols

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Window</class>
<method>get_group_by_cols</method>
<code>
1283     def get_group_by_cols(self, alias=None):
1284         return []

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>WindowFrame</class>
<method>get_group_by_cols</method>
<code>
1319     def get_group_by_cols(self, alias=None):
1320         return []

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>annotate</method>
<code>
1082     def annotate(self, *args, **kwargs):
1083         """
1084         Return a query set in which the returned objects have been annotated
1085         with extra data or aggregations.
1086         """
1087         self._not_support_combined_queries('annotate')
1088         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1089         annotations = {}
1090         for arg in args:
1091             # The default_alias property may raise a TypeError.
1092             try:
1093                 if arg.default_alias in kwargs:
1094                     raise ValueError("The named annotation '%s' conflicts with the "
1095                                      "default name for another annotation."
1096                                      % arg.default_alias)
1097             except TypeError:
1098                 raise TypeError("Complex annotations require an alias")
1099             annotations[arg.default_alias] = arg
1100         annotations.update(kwargs)
1101 
1102         clone = self._chain()
1103         names = self._fields
1104         if names is None:
1105             names = set(chain.from_iterable(
1106                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1107                 for field in self.model._meta.get_fields()
1108             ))
1109 
1110         for alias, annotation in annotations.items():
1111             if alias in names:
1112                 raise ValueError("The annotation '%s' conflicts with a field on "
1113                                  "the model." % alias)
1114             if isinstance(annotation, FilteredRelation):
1115                 clone.query.add_filtered_relation(annotation, alias)
1116             else:
1117                 clone.query.add_annotation(annotation, alias, is_summary=False)
1118 
1119         for alias, annotation in clone.query.annotations.items():
1120             if alias in annotations and annotation.contains_aggregate:
1121                 if clone._fields is None:
1122                     clone.query.group_by = True
1123                 else:
1124                     clone.query.set_group_by()
1125                 break
1126 
1127         return clone

</code><intended_behavior>Although not intrinsically faulty, a refined examination within the annotate method might be necessary to ensure it correctly handles the inclusion (or exclusion) of expressions wrapped by ExpressionWrapper in GROUP BY clauses. This does not suggest the annotate method itself is misbehaving, but a deeper integration or handling check with ExpressionWrapper's adjusted behavior (as proposed above) might be required to holistically resolve the issue. The annotate method should seamlessly support the refined logic of ExpressionWrapper.get_group_by_cols to exclude constant expressions from the GROUP BY clause where applicable, ensuring PostgreSQL compatibility.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
184 class QuerySet:
185     """Represent a lazy database lookup for a set of objects."""
186 
187     def __init__(self, model=None, query=None, using=None, hints=None):
188         self.model = model
189         self._db = using
190         self._hints = hints or {}
191         self._query = query or sql.Query(self.model)
192         self._result_cache = None
193         self._sticky_filter = False
194         self._for_write = False
195         self._prefetch_related_lookups = ()
196         self._prefetch_done = False
197         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
198         self._iterable_class = ModelIterable
199         self._fields = None
200         self._defer_next_filter = False
201         self._deferred_filter = None
202 
203     @property
204     def query(self):
205         if self._deferred_filter:
206             negate, args, kwargs = self._deferred_filter
207             self._filter_or_exclude_inplace(negate, *args, **kwargs)
208             self._deferred_filter = None
209         return self._query
210 
211     @query.setter
212     def query(self, value):
213         self._query = value
214 
215     def as_manager(cls):
216         # Address the circular dependency between `Queryset` and `Manager`.
217         from django.db.models.manager import Manager
218         manager = Manager.from_queryset(cls)()
219         manager._built_with_as_manager = True
220         return manager
221     as_manager.queryset_only = True
222     as_manager = classmethod(as_manager)
223 
224     ########################
225     # PYTHON MAGIC METHODS #
226     ########################
227 
228     def __deepcopy__(self, memo):
229         """Don't populate the QuerySet's cache."""
230         obj = self.__class__()
231         for k, v in self.__dict__.items():
232             if k == '_result_cache':
233                 obj.__dict__[k] = None
234             else:
235                 obj.__dict__[k] = copy.deepcopy(v, memo)
236         return obj
237 
238     def __getstate__(self):
239         # Force the cache to be fully populated.
240         self._fetch_all()
241         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
242 
243     def __setstate__(self, state):
244         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
245         if pickled_version:
246             if pickled_version != django.__version__:
247                 warnings.warn(
248                     "Pickled queryset instance's Django version %s does not "
249                     "match the current version %s."
250                     % (pickled_version, django.__version__),
251                     RuntimeWarning,
252                     stacklevel=2,
253                 )
254         else:
255             warnings.warn(
256                 "Pickled queryset instance's Django version is not specified.",
257                 RuntimeWarning,
258                 stacklevel=2,
259             )
260         self.__dict__.update(state)
261 
262     def __repr__(self):
263         data = list(self[:REPR_OUTPUT_SIZE + 1])
264         if len(data) > REPR_OUTPUT_SIZE:
265             data[-1] = "...(remaining elements truncated)..."
266         return '<%s %r>' % (self.__class__.__name__, data)
267 
268     def __len__(self):
269         self._fetch_all()
270         return len(self._result_cache)
271 
272     def __iter__(self):
273         """
274         The queryset iterator protocol uses three nested iterators in the
275         default case:
276             1. sql.compiler.execute_sql()
277                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
278                  using cursor.fetchmany(). This part is responsible for
279                  doing some column masking, and returning the rows in chunks.
280             2. sql.compiler.results_iter()
281                - Returns one row at time. At this point the rows are still just
282                  tuples. In some cases the return values are converted to
283                  Python values at this location.
284             3. self.iterator()
285                - Responsible for turning the rows into model objects.
286         """
287         self._fetch_all()
288         return iter(self._result_cache)
289 
290     def __bool__(self):
291         self._fetch_all()
292         return bool(self._result_cache)
293 
294     def __getitem__(self, k):
295         """Retrieve an item or slice from the set of results."""
296         if not isinstance(k, (int, slice)):
297             raise TypeError(
298                 'QuerySet indices must be integers or slices, not %s.'
299                 % type(k).__name__
300             )
301         assert ((not isinstance(k, slice) and (k >= 0)) or
302                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
303                  (k.stop is None or k.stop >= 0))), \
304             "Negative indexing is not supported."
305 
306         if self._result_cache is not None:
307             return self._result_cache[k]
308 
309         if isinstance(k, slice):
310             qs = self._chain()
311             if k.start is not None:
312                 start = int(k.start)
313             else:
314                 start = None
315             if k.stop is not None:
316                 stop = int(k.stop)
317             else:
318                 stop = None
319             qs.query.set_limits(start, stop)
320             return list(qs)[::k.step] if k.step else qs
321 
322         qs = self._chain()
323         qs.query.set_limits(k, k + 1)
324         qs._fetch_all()
325         return qs._result_cache[0]
326 
327     def __class_getitem__(cls, *args, **kwargs):
328         return cls
329 
330     def __and__(self, other):
331         self._merge_sanity_check(other)
332         if isinstance(other, EmptyQuerySet):
333             return other
334         if isinstance(self, EmptyQuerySet):
335             return self
336         combined = self._chain()
337         combined._merge_known_related_objects(other)
338         combined.query.combine(other.query, sql.AND)
339         return combined
340 
341     def __or__(self, other):
342         self._merge_sanity_check(other)
343         if isinstance(self, EmptyQuerySet):
344             return other
345         if isinstance(other, EmptyQuerySet):
346             return self
347         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
348         combined = query._chain()
349         combined._merge_known_related_objects(other)
350         if not other.query.can_filter():
351             other = other.model._base_manager.filter(pk__in=other.values('pk'))
352         combined.query.combine(other.query, sql.OR)
353         return combined
354 
355     ####################################
356     # METHODS THAT DO DATABASE QUERIES #
357     ####################################
358 
359     def _iterator(self, use_chunked_fetch, chunk_size):
360         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
361 
362     def iterator(self, chunk_size=2000):
363         """
364         An iterator over the results from applying this QuerySet to the
365         database.
366         """
367         if chunk_size <= 0:
368             raise ValueError('Chunk size must be strictly positive.')
369         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
370         return self._iterator(use_chunked_fetch, chunk_size)
371 
372     def aggregate(self, *args, **kwargs):
373         """
374         Return a dictionary containing the calculations (aggregation)
375         over the current queryset.
376 
377         If args is present the expression is passed as a kwarg using
378         the Aggregate object's default alias.
379         """
380         if self.query.distinct_fields:
381             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
382         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
383         for arg in args:
384             # The default_alias property raises TypeError if default_alias
385             # can't be set automatically or AttributeError if it isn't an
386             # attribute.
387             try:
388                 arg.default_alias
389             except (AttributeError, TypeError):
390                 raise TypeError("Complex aggregates require an alias")
391             kwargs[arg.default_alias] = arg
392 
393         query = self.query.chain()
394         for (alias, aggregate_expr) in kwargs.items():
395             query.add_annotation(aggregate_expr, alias, is_summary=True)
396             if not query.annotations[alias].contains_aggregate:
397                 raise TypeError("%s is not an aggregate expression" % alias)
398         return query.get_aggregation(self.db, kwargs)
399 
400     def count(self):
401         """
402         Perform a SELECT COUNT() and return the number of records as an
403         integer.
404 
405         If the QuerySet is already fully cached, return the length of the
406         cached results set to avoid multiple SELECT COUNT(*) calls.
407         """
408         if self._result_cache is not None:
409             return len(self._result_cache)
410 
411         return self.query.get_count(using=self.db)
412 
413     def get(self, *args, **kwargs):
414         """
415         Perform the query and return a single object matching the given
416         keyword arguments.
417         """
418         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
419         if self.query.can_filter() and not self.query.distinct_fields:
420             clone = clone.order_by()
421         limit = None
422         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
423             limit = MAX_GET_RESULTS
424             clone.query.set_limits(high=limit)
425         num = len(clone)
426         if num == 1:
427             return clone._result_cache[0]
428         if not num:
429             raise self.model.DoesNotExist(
430                 "%s matching query does not exist." %
431                 self.model._meta.object_name
432             )
433         raise self.model.MultipleObjectsReturned(
434             'get() returned more than one %s -- it returned %s!' % (
435                 self.model._meta.object_name,
436                 num if not limit or num < limit else 'more than %s' % (limit - 1),
437             )
438         )
439 
440     def create(self, **kwargs):
441         """
442         Create a new object with the given kwargs, saving it to the database
443         and returning the created object.
444         """
445         obj = self.model(**kwargs)
446         self._for_write = True
447         obj.save(force_insert=True, using=self.db)
448         return obj
449 
450     def _populate_pk_values(self, objs):
451         for obj in objs:
452             if obj.pk is None:
453                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
454 
455     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
456         """
457         Insert each of the instances into the database. Do *not* call
458         save() on each of the instances, do not send any pre/post_save
459         signals, and do not set the primary key attribute if it is an
460         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
461         Multi-table models are not supported.
462         """
463         # When you bulk insert you don't get the primary keys back (if it's an
464         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
465         # you can't insert into the child tables which references this. There
466         # are two workarounds:
467         # 1) This could be implemented if you didn't have an autoincrement pk
468         # 2) You could do it by doing O(n) normal inserts into the parent
469         #    tables to get the primary keys back and then doing a single bulk
470         #    insert into the childmost table.
471         # We currently set the primary keys on the objects when using
472         # PostgreSQL via the RETURNING ID clause. It should be possible for
473         # Oracle as well, but the semantics for extracting the primary keys is
474         # trickier so it's not done yet.
475         assert batch_size is None or batch_size > 0
476         # Check that the parents share the same concrete model with the our
477         # model to detect the inheritance pattern ConcreteGrandParent ->
478         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
479         # would not identify that case as involving multiple tables.
480         for parent in self.model._meta.get_parent_list():
481             if parent._meta.concrete_model is not self.model._meta.concrete_model:
482                 raise ValueError("Can't bulk create a multi-table inherited model")
483         if not objs:
484             return objs
485         self._for_write = True
486         connection = connections[self.db]
487         opts = self.model._meta
488         fields = opts.concrete_fields
489         objs = list(objs)
490         self._populate_pk_values(objs)
491         with transaction.atomic(using=self.db, savepoint=False):
492             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
493             if objs_with_pk:
494                 returned_columns = self._batched_insert(
495                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
496                 )
497                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
498                     for result, field in zip(results, opts.db_returning_fields):
499                         if field != opts.pk:
500                             setattr(obj_with_pk, field.attname, result)
501                 for obj_with_pk in objs_with_pk:
502                     obj_with_pk._state.adding = False
503                     obj_with_pk._state.db = self.db
504             if objs_without_pk:
505                 fields = [f for f in fields if not isinstance(f, AutoField)]
506                 returned_columns = self._batched_insert(
507                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
508                 )
509                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
510                     assert len(returned_columns) == len(objs_without_pk)
511                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
512                     for result, field in zip(results, opts.db_returning_fields):
513                         setattr(obj_without_pk, field.attname, result)
514                     obj_without_pk._state.adding = False
515                     obj_without_pk._state.db = self.db
516 
517         return objs
518 
519     def bulk_update(self, objs, fields, batch_size=None):
520         """
521         Update the given fields in each of the given objects in the database.
522         """
523         if batch_size is not None and batch_size < 0:
524             raise ValueError('Batch size must be a positive integer.')
525         if not fields:
526             raise ValueError('Field names must be given to bulk_update().')
527         objs = tuple(objs)
528         if any(obj.pk is None for obj in objs):
529             raise ValueError('All bulk_update() objects must have a primary key set.')
530         fields = [self.model._meta.get_field(name) for name in fields]
531         if any(not f.concrete or f.many_to_many for f in fields):
532             raise ValueError('bulk_update() can only be used with concrete fields.')
533         if any(f.primary_key for f in fields):
534             raise ValueError('bulk_update() cannot be used with primary key fields.')
535         if not objs:
536             return
537         # PK is used twice in the resulting update query, once in the filter
538         # and once in the WHEN. Each field will also have one CAST.
539         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
540         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
541         requires_casting = connections[self.db].features.requires_casted_case_in_updates
542         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
543         updates = []
544         for batch_objs in batches:
545             update_kwargs = {}
546             for field in fields:
547                 when_statements = []
548                 for obj in batch_objs:
549                     attr = getattr(obj, field.attname)
550                     if not isinstance(attr, Expression):
551                         attr = Value(attr, output_field=field)
552                     when_statements.append(When(pk=obj.pk, then=attr))
553                 case_statement = Case(*when_statements, output_field=field)
554                 if requires_casting:
555                     case_statement = Cast(case_statement, output_field=field)
556                 update_kwargs[field.attname] = case_statement
557             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
558         with transaction.atomic(using=self.db, savepoint=False):
559             for pks, update_kwargs in updates:
560                 self.filter(pk__in=pks).update(**update_kwargs)
561     bulk_update.alters_data = True
562 
563     def get_or_create(self, defaults=None, **kwargs):
564         """
565         Look up an object with the given kwargs, creating one if necessary.
566         Return a tuple of (object, created), where created is a boolean
567         specifying whether an object was created.
568         """
569         # The get() needs to be targeted at the write database in order
570         # to avoid potential transaction consistency problems.
571         self._for_write = True
572         try:
573             return self.get(**kwargs), False
574         except self.model.DoesNotExist:
575             params = self._extract_model_params(defaults, **kwargs)
576             return self._create_object_from_params(kwargs, params)
577 
578     def update_or_create(self, defaults=None, **kwargs):
579         """
580         Look up an object with the given kwargs, updating one with defaults
581         if it exists, otherwise create a new one.
582         Return a tuple (object, created), where created is a boolean
583         specifying whether an object was created.
584         """
585         defaults = defaults or {}
586         self._for_write = True
587         with transaction.atomic(using=self.db):
588             try:
589                 obj = self.select_for_update().get(**kwargs)
590             except self.model.DoesNotExist:
591                 params = self._extract_model_params(defaults, **kwargs)
592                 # Lock the row so that a concurrent update is blocked until
593                 # after update_or_create() has performed its save.
594                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
595                 if created:
596                     return obj, created
597             for k, v in resolve_callables(defaults):
598                 setattr(obj, k, v)
599             obj.save(using=self.db)
600         return obj, False
601 
602     def _create_object_from_params(self, lookup, params, lock=False):
603         """
604         Try to create an object using passed params. Used by get_or_create()
605         and update_or_create().
606         """
607         try:
608             with transaction.atomic(using=self.db):
609                 params = dict(resolve_callables(params))
610                 obj = self.create(**params)
611             return obj, True
612         except IntegrityError:
613             try:
614                 qs = self.select_for_update() if lock else self
615                 return qs.get(**lookup), False
616             except self.model.DoesNotExist:
617                 pass
618             raise
619 
620     def _extract_model_params(self, defaults, **kwargs):
621         """
622         Prepare `params` for creating a model instance based on the given
623         kwargs; for use by get_or_create() and update_or_create().
624         """
625         defaults = defaults or {}
626         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
627         params.update(defaults)
628         property_names = self.model._meta._property_names
629         invalid_params = []
630         for param in params:
631             try:
632                 self.model._meta.get_field(param)
633             except exceptions.FieldDoesNotExist:
634                 # It's okay to use a model's property if it has a setter.
635                 if not (param in property_names and getattr(self.model, param).fset):
636                     invalid_params.append(param)
637         if invalid_params:
638             raise exceptions.FieldError(
639                 "Invalid field name(s) for model %s: '%s'." % (
640                     self.model._meta.object_name,
641                     "', '".join(sorted(invalid_params)),
642                 ))
643         return params
644 
645     def _earliest(self, *fields):
646         """
647         Return the earliest object according to fields (if given) or by the
648         model's Meta.get_latest_by.
649         """
650         if fields:
651             order_by = fields
652         else:
653             order_by = getattr(self.model._meta, 'get_latest_by')
654             if order_by and not isinstance(order_by, (tuple, list)):
655                 order_by = (order_by,)
656         if order_by is None:
657             raise ValueError(
658                 "earliest() and latest() require either fields as positional "
659                 "arguments or 'get_latest_by' in the model's Meta."
660             )
661 
662         assert not self.query.is_sliced, \
663             "Cannot change a query once a slice has been taken."
664         obj = self._chain()
665         obj.query.set_limits(high=1)
666         obj.query.clear_ordering(force_empty=True)
667         obj.query.add_ordering(*order_by)
668         return obj.get()
669 
670     def earliest(self, *fields):
671         return self._earliest(*fields)
672 
673     def latest(self, *fields):
674         return self.reverse()._earliest(*fields)
675 
676     def first(self):
677         """Return the first object of a query or None if no match is found."""
678         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
679             return obj
680 
681     def last(self):
682         """Return the last object of a query or None if no match is found."""
683         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
684             return obj
685 
686     def in_bulk(self, id_list=None, *, field_name='pk'):
687         """
688         Return a dictionary mapping each of the given IDs to the object with
689         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
690         """
691         assert not self.query.is_sliced, \
692             "Cannot use 'limit' or 'offset' with in_bulk"
693         opts = self.model._meta
694         unique_fields = [
695             constraint.fields[0]
696             for constraint in opts.total_unique_constraints
697             if len(constraint.fields) == 1
698         ]
699         if (
700             field_name != 'pk' and
701             not opts.get_field(field_name).unique and
702             field_name not in unique_fields
703         ):
704             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
705         if id_list is not None:
706             if not id_list:
707                 return {}
708             filter_key = '{}__in'.format(field_name)
709             batch_size = connections[self.db].features.max_query_params
710             id_list = tuple(id_list)
711             # If the database has a limit on the number of query parameters
712             # (e.g. SQLite), retrieve objects in batches if necessary.
713             if batch_size and batch_size < len(id_list):
714                 qs = ()
715                 for offset in range(0, len(id_list), batch_size):
716                     batch = id_list[offset:offset + batch_size]
717                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
718             else:
719                 qs = self.filter(**{filter_key: id_list}).order_by()
720         else:
721             qs = self._chain()
722         return {getattr(obj, field_name): obj for obj in qs}
723 
724     def delete(self):
725         """Delete the records in the current QuerySet."""
726         self._not_support_combined_queries('delete')
727         assert not self.query.is_sliced, \
728             "Cannot use 'limit' or 'offset' with delete."
729 
730         if self._fields is not None:
731             raise TypeError("Cannot call delete() after .values() or .values_list()")
732 
733         del_query = self._chain()
734 
735         # The delete is actually 2 queries - one to find related objects,
736         # and one to delete. Make sure that the discovery of related
737         # objects is performed on the same database as the deletion.
738         del_query._for_write = True
739 
740         # Disable non-supported fields.
741         del_query.query.select_for_update = False
742         del_query.query.select_related = False
743         del_query.query.clear_ordering(force_empty=True)
744 
745         collector = Collector(using=del_query.db)
746         collector.collect(del_query)
747         deleted, _rows_count = collector.delete()
748 
749         # Clear the result cache, in case this QuerySet gets reused.
750         self._result_cache = None
751         return deleted, _rows_count
752 
753     delete.alters_data = True
754     delete.queryset_only = True
755 
756     def _raw_delete(self, using):
757         """
758         Delete objects found from the given queryset in single direct SQL
759         query. No signals are sent and there is no protection for cascades.
760         """
761         query = self.query.clone()
762         query.__class__ = sql.DeleteQuery
763         cursor = query.get_compiler(using).execute_sql(CURSOR)
764         if cursor:
765             with cursor:
766                 return cursor.rowcount
767         return 0
768     _raw_delete.alters_data = True
769 
770     def update(self, **kwargs):
771         """
772         Update all elements in the current QuerySet, setting all the given
773         fields to the appropriate values.
774         """
775         self._not_support_combined_queries('update')
776         assert not self.query.is_sliced, \
777             "Cannot update a query once a slice has been taken."
778         self._for_write = True
779         query = self.query.chain(sql.UpdateQuery)
780         query.add_update_values(kwargs)
781         # Clear any annotations so that they won't be present in subqueries.
782         query.annotations = {}
783         with transaction.mark_for_rollback_on_error(using=self.db):
784             rows = query.get_compiler(self.db).execute_sql(CURSOR)
785         self._result_cache = None
786         return rows
787     update.alters_data = True
788 
789     def _update(self, values):
790         """
791         A version of update() that accepts field objects instead of field names.
792         Used primarily for model saving and not intended for use by general
793         code (it requires too much poking around at model internals to be
794         useful at that level).
795         """
796         assert not self.query.is_sliced, \
797             "Cannot update a query once a slice has been taken."
798         query = self.query.chain(sql.UpdateQuery)
799         query.add_update_fields(values)
800         # Clear any annotations so that they won't be present in subqueries.
801         query.annotations = {}
802         self._result_cache = None
803         return query.get_compiler(self.db).execute_sql(CURSOR)
804     _update.alters_data = True
805     _update.queryset_only = False
806 
807     def exists(self):
808         if self._result_cache is None:
809             return self.query.has_results(using=self.db)
810         return bool(self._result_cache)
811 
812     def _prefetch_related_objects(self):
813         # This method can only be called once the result cache has been filled.
814         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
815         self._prefetch_done = True
816 
817     def explain(self, *, format=None, **options):
818         return self.query.explain(using=self.db, format=format, **options)
819 
820     ##################################################
821     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
822     ##################################################
823 
824     def raw(self, raw_query, params=None, translations=None, using=None):
825         if using is None:
826             using = self.db
827         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
828         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
829         return qs
830 
831     def _values(self, *fields, **expressions):
832         clone = self._chain()
833         if expressions:
834             clone = clone.annotate(**expressions)
835         clone._fields = fields
836         clone.query.set_values(fields)
837         return clone
838 
839     def values(self, *fields, **expressions):
840         fields += tuple(expressions)
841         clone = self._values(*fields, **expressions)
842         clone._iterable_class = ValuesIterable
843         return clone
844 
845     def values_list(self, *fields, flat=False, named=False):
846         if flat and named:
847             raise TypeError("'flat' and 'named' can't be used together.")
848         if flat and len(fields) > 1:
849             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
850 
851         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
852         _fields = []
853         expressions = {}
854         counter = 1
855         for field in fields:
856             if hasattr(field, 'resolve_expression'):
857                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
858                 while True:
859                     field_id = field_id_prefix + str(counter)
860                     counter += 1
861                     if field_id not in field_names:
862                         break
863                 expressions[field_id] = field
864                 _fields.append(field_id)
865             else:
866                 _fields.append(field)
867 
868         clone = self._values(*_fields, **expressions)
869         clone._iterable_class = (
870             NamedValuesListIterable if named
871             else FlatValuesListIterable if flat
872             else ValuesListIterable
873         )
874         return clone
875 
876     def dates(self, field_name, kind, order='ASC'):
877         """
878         Return a list of date objects representing all available dates for
879         the given field_name, scoped to 'kind'.
880         """
881         assert kind in ('year', 'month', 'week', 'day'), \
882             "'kind' must be one of 'year', 'month', 'week', or 'day'."
883         assert order in ('ASC', 'DESC'), \
884             "'order' must be either 'ASC' or 'DESC'."
885         return self.annotate(
886             datefield=Trunc(field_name, kind, output_field=DateField()),
887             plain_field=F(field_name)
888         ).values_list(
889             'datefield', flat=True
890         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
891 
892     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):
893         """
894         Return a list of datetime objects representing all available
895         datetimes for the given field_name, scoped to 'kind'.
896         """
897         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
898             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
899         assert order in ('ASC', 'DESC'), \
900             "'order' must be either 'ASC' or 'DESC'."
901         if settings.USE_TZ:
902             if tzinfo is None:
903                 tzinfo = timezone.get_current_timezone()
904         else:
905             tzinfo = None
906         return self.annotate(
907             datetimefield=Trunc(
908                 field_name,
909                 kind,
910                 output_field=DateTimeField(),
911                 tzinfo=tzinfo,
912                 is_dst=is_dst,
913             ),
914             plain_field=F(field_name)
915         ).values_list(
916             'datetimefield', flat=True
917         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
918 
919     def none(self):
920         """Return an empty QuerySet."""
921         clone = self._chain()
922         clone.query.set_empty()
923         return clone
924 
925     ##################################################################
926     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
927     ##################################################################
928 
929     def all(self):
930         """
931         Return a new QuerySet that is a copy of the current one. This allows a
932         QuerySet to proxy for a model manager in some cases.
933         """
934         return self._chain()
935 
936     def filter(self, *args, **kwargs):
937         """
938         Return a new QuerySet instance with the args ANDed to the existing
939         set.
940         """
941         self._not_support_combined_queries('filter')
942         return self._filter_or_exclude(False, *args, **kwargs)
943 
944     def exclude(self, *args, **kwargs):
945         """
946         Return a new QuerySet instance with NOT (args) ANDed to the existing
947         set.
948         """
949         self._not_support_combined_queries('exclude')
950         return self._filter_or_exclude(True, *args, **kwargs)
951 
952     def _filter_or_exclude(self, negate, *args, **kwargs):
953         if args or kwargs:
954             assert not self.query.is_sliced, \
955                 "Cannot filter a query once a slice has been taken."
956 
957         clone = self._chain()
958         if self._defer_next_filter:
959             self._defer_next_filter = False
960             clone._deferred_filter = negate, args, kwargs
961         else:
962             clone._filter_or_exclude_inplace(negate, *args, **kwargs)
963         return clone
964 
965     def _filter_or_exclude_inplace(self, negate, *args, **kwargs):
966         if negate:
967             self._query.add_q(~Q(*args, **kwargs))
968         else:
969             self._query.add_q(Q(*args, **kwargs))
970 
971     def complex_filter(self, filter_obj):
972         """
973         Return a new QuerySet instance with filter_obj added to the filters.
974 
975         filter_obj can be a Q object or a dictionary of keyword lookup
976         arguments.
977 
978         This exists to support framework features such as 'limit_choices_to',
979         and usually it will be more natural to use other methods.
980         """
981         if isinstance(filter_obj, Q):
982             clone = self._chain()
983             clone.query.add_q(filter_obj)
984             return clone
985         else:
986             return self._filter_or_exclude(False, **filter_obj)
987 
988     def _combinator_query(self, combinator, *other_qs, all=False):
989         # Clone the query to inherit the select list and everything
990         clone = self._chain()
991         # Clear limits and ordering so they can be reapplied
992         clone.query.clear_ordering(True)
993         clone.query.clear_limits()
994         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
995         clone.query.combinator = combinator
996         clone.query.combinator_all = all
997         return clone
998 
999     def union(self, *other_qs, all=False):
1000         # If the query is an EmptyQuerySet, combine all nonempty querysets.
1001         if isinstance(self, EmptyQuerySet):
1002             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1003             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
1004         return self._combinator_query('union', *other_qs, all=all)
1005 
1006     def intersection(self, *other_qs):
1007         # If any query is an EmptyQuerySet, return it.
1008         if isinstance(self, EmptyQuerySet):
1009             return self
1010         for other in other_qs:
1011             if isinstance(other, EmptyQuerySet):
1012                 return other
1013         return self._combinator_query('intersection', *other_qs)
1014 
1015     def difference(self, *other_qs):
1016         # If the query is an EmptyQuerySet, return it.
1017         if isinstance(self, EmptyQuerySet):
1018             return self
1019         return self._combinator_query('difference', *other_qs)
1020 
1021     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1022         """
1023         Return a new QuerySet instance that will select objects with a
1024         FOR UPDATE lock.
1025         """
1026         if nowait and skip_locked:
1027             raise ValueError('The nowait option cannot be used with skip_locked.')
1028         obj = self._chain()
1029         obj._for_write = True
1030         obj.query.select_for_update = True
1031         obj.query.select_for_update_nowait = nowait
1032         obj.query.select_for_update_skip_locked = skip_locked
1033         obj.query.select_for_update_of = of
1034         obj.query.select_for_no_key_update = no_key
1035         return obj
1036 
1037     def select_related(self, *fields):
1038         """
1039         Return a new QuerySet instance that will select related objects.
1040 
1041         If fields are specified, they must be ForeignKey fields and only those
1042         related objects are included in the selection.
1043 
1044         If select_related(None) is called, clear the list.
1045         """
1046         self._not_support_combined_queries('select_related')
1047         if self._fields is not None:
1048             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1049 
1050         obj = self._chain()
1051         if fields == (None,):
1052             obj.query.select_related = False
1053         elif fields:
1054             obj.query.add_select_related(fields)
1055         else:
1056             obj.query.select_related = True
1057         return obj
1058 
1059     def prefetch_related(self, *lookups):
1060         """
1061         Return a new QuerySet instance that will prefetch the specified
1062         Many-To-One and Many-To-Many related objects when the QuerySet is
1063         evaluated.
1064 
1065         When prefetch_related() is called more than once, append to the list of
1066         prefetch lookups. If prefetch_related(None) is called, clear the list.
1067         """
1068         self._not_support_combined_queries('prefetch_related')
1069         clone = self._chain()
1070         if lookups == (None,):
1071             clone._prefetch_related_lookups = ()
1072         else:
1073             for lookup in lookups:
1074                 if isinstance(lookup, Prefetch):
1075                     lookup = lookup.prefetch_to
1076                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1077                 if lookup in self.query._filtered_relations:
1078                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1079             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1080         return clone
1081 
1082     def annotate(self, *args, **kwargs):
1083         """
1084         Return a query set in which the returned objects have been annotated
1085         with extra data or aggregations.
1086         """
1087         self._not_support_combined_queries('annotate')
1088         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1089         annotations = {}
1090         for arg in args:
1091             # The default_alias property may raise a TypeError.
1092             try:
1093                 if arg.default_alias in kwargs:
1094                     raise ValueError("The named annotation '%s' conflicts with the "
1095                                      "default name for another annotation."
1096                                      % arg.default_alias)
1097             except TypeError:
1098                 raise TypeError("Complex annotations require an alias")
1099             annotations[arg.default_alias] = arg
1100         annotations.update(kwargs)
1101 
1102         clone = self._chain()
1103         names = self._fields
1104         if names is None:
1105             names = set(chain.from_iterable(
1106                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1107                 for field in self.model._meta.get_fields()
1108             ))
1109 
1110         for alias, annotation in annotations.items():
1111             if alias in names:
1112                 raise ValueError("The annotation '%s' conflicts with a field on "
1113                                  "the model." % alias)
1114             if isinstance(annotation, FilteredRelation):
1115                 clone.query.add_filtered_relation(annotation, alias)
1116             else:
1117                 clone.query.add_annotation(annotation, alias, is_summary=False)
1118 
1119         for alias, annotation in clone.query.annotations.items():
1120             if alias in annotations and annotation.contains_aggregate:
1121                 if clone._fields is None:
1122                     clone.query.group_by = True
1123                 else:
1124                     clone.query.set_group_by()
1125                 break
1126 
1127         return clone
1128 
1129     def order_by(self, *field_names):
1130         """Return a new QuerySet instance with the ordering changed."""
1131         assert not self.query.is_sliced, \
1132             "Cannot reorder a query once a slice has been taken."
1133         obj = self._chain()
1134         obj.query.clear_ordering(force_empty=False)
1135         obj.query.add_ordering(*field_names)
1136         return obj
1137 
1138     def distinct(self, *field_names):
1139         """
1140         Return a new QuerySet instance that will select only distinct results.
1141         """
1142         self._not_support_combined_queries('distinct')
1143         assert not self.query.is_sliced, \
1144             "Cannot create distinct fields once a slice has been taken."
1145         obj = self._chain()
1146         obj.query.add_distinct_fields(*field_names)
1147         return obj
1148 
1149     def extra(self, select=None, where=None, params=None, tables=None,
1150               order_by=None, select_params=None):
1151         """Add extra SQL fragments to the query."""
1152         self._not_support_combined_queries('extra')
1153         assert not self.query.is_sliced, \
1154             "Cannot change a query once a slice has been taken"
1155         clone = self._chain()
1156         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1157         return clone
1158 
1159     def reverse(self):
1160         """Reverse the ordering of the QuerySet."""
1161         if self.query.is_sliced:
1162             raise TypeError('Cannot reverse a query once a slice has been taken.')
1163         clone = self._chain()
1164         clone.query.standard_ordering = not clone.query.standard_ordering
1165         return clone
1166 
1167     def defer(self, *fields):
1168         """
1169         Defer the loading of data for certain fields until they are accessed.
1170         Add the set of deferred fields to any existing set of deferred fields.
1171         The only exception to this is if None is passed in as the only
1172         parameter, in which case removal all deferrals.
1173         """
1174         self._not_support_combined_queries('defer')
1175         if self._fields is not None:
1176             raise TypeError("Cannot call defer() after .values() or .values_list()")
1177         clone = self._chain()
1178         if fields == (None,):
1179             clone.query.clear_deferred_loading()
1180         else:
1181             clone.query.add_deferred_loading(fields)
1182         return clone
1183 
1184     def only(self, *fields):
1185         """
1186         Essentially, the opposite of defer(). Only the fields passed into this
1187         method and that are not already specified as deferred are loaded
1188         immediately when the queryset is evaluated.
1189         """
1190         self._not_support_combined_queries('only')
1191         if self._fields is not None:
1192             raise TypeError("Cannot call only() after .values() or .values_list()")
1193         if fields == (None,):
1194             # Can only pass None to defer(), not only(), as the rest option.
1195             # That won't stop people trying to do this, so let's be explicit.
1196             raise TypeError("Cannot pass None as an argument to only().")
1197         for field in fields:
1198             field = field.split(LOOKUP_SEP, 1)[0]
1199             if field in self.query._filtered_relations:
1200                 raise ValueError('only() is not supported with FilteredRelation.')
1201         clone = self._chain()
1202         clone.query.add_immediate_loading(fields)
1203         return clone
1204 
1205     def using(self, alias):
1206         """Select which database this QuerySet should execute against."""
1207         clone = self._chain()
1208         clone._db = alias
1209         return clone
1210 
1211     ###################################
1212     # PUBLIC INTROSPECTION ATTRIBUTES #
1213     ###################################
1214 
1215     @property
1216     def ordered(self):
1217         """
1218         Return True if the QuerySet is ordered -- i.e. has an order_by()
1219         clause or a default ordering on the model (or is empty).
1220         """
1221         if isinstance(self, EmptyQuerySet):
1222             return True
1223         if self.query.extra_order_by or self.query.order_by:
1224             return True
1225         elif self.query.default_ordering and self.query.get_meta().ordering:
1226             return True
1227         else:
1228             return False
1229 
1230     @property
1231     def db(self):
1232         """Return the database used if this query is executed now."""
1233         if self._for_write:
1234             return self._db or router.db_for_write(self.model, **self._hints)
1235         return self._db or router.db_for_read(self.model, **self._hints)
1236 
1237     ###################
1238     # PRIVATE METHODS #
1239     ###################
1240 
1241     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1242         """
1243         Insert a new record for the given model. This provides an interface to
1244         the InsertQuery class and is how Model.save() is implemented.
1245         """
1246         self._for_write = True
1247         if using is None:
1248             using = self.db
1249         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1250         query.insert_values(fields, objs, raw=raw)
1251         return query.get_compiler(using=using).execute_sql(returning_fields)
1252     _insert.alters_data = True
1253     _insert.queryset_only = False
1254 
1255     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1256         """
1257         Helper method for bulk_create() to insert objs one batch at a time.
1258         """
1259         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1260             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1261         ops = connections[self.db].ops
1262         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1263         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1264         inserted_rows = []
1265         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1266         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1267             if bulk_return and not ignore_conflicts:
1268                 inserted_rows.extend(self._insert(
1269                     item, fields=fields, using=self.db,
1270                     returning_fields=self.model._meta.db_returning_fields,
1271                     ignore_conflicts=ignore_conflicts,
1272                 ))
1273             else:
1274                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1275         return inserted_rows
1276 
1277     def _chain(self, **kwargs):
1278         """
1279         Return a copy of the current QuerySet that's ready for another
1280         operation.
1281         """
1282         obj = self._clone()
1283         if obj._sticky_filter:
1284             obj.query.filter_is_sticky = True
1285             obj._sticky_filter = False
1286         obj.__dict__.update(kwargs)
1287         return obj
1288 
1289     def _clone(self):
1290         """
1291         Return a copy of the current QuerySet. A lightweight alternative
1292         to deepcopy().
1293         """
1294         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1295         c._sticky_filter = self._sticky_filter
1296         c._for_write = self._for_write
1297         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1298         c._known_related_objects = self._known_related_objects
1299         c._iterable_class = self._iterable_class
1300         c._fields = self._fields
1301         return c
1302 
1303     def _fetch_all(self):
1304         if self._result_cache is None:
1305             self._result_cache = list(self._iterable_class(self))
1306         if self._prefetch_related_lookups and not self._prefetch_done:
1307             self._prefetch_related_objects()
1308 
1309     def _next_is_sticky(self):
1310         """
1311         Indicate that the next filter call and the one following that should
1312         be treated as a single filter. This is only important when it comes to
1313         determining when to reuse tables for many-to-many filters. Required so
1314         that we can filter naturally on the results of related managers.
1315 
1316         This doesn't return a clone of the current QuerySet (it returns
1317         "self"). The method is only used internally and should be immediately
1318         followed by a filter() that does create a clone.
1319         """
1320         self._sticky_filter = True
1321         return self
1322 
1323     def _merge_sanity_check(self, other):
1324         """Check that two QuerySet classes may be merged."""
1325         if self._fields is not None and (
1326                 set(self.query.values_select) != set(other.query.values_select) or
1327                 set(self.query.extra_select) != set(other.query.extra_select) or
1328                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1329             raise TypeError(
1330                 "Merging '%s' classes must involve the same values in each case."
1331                 % self.__class__.__name__
1332             )
1333 
1334     def _merge_known_related_objects(self, other):
1335         """
1336         Keep track of all known related objects from either QuerySet instance.
1337         """
1338         for field, objects in other._known_related_objects.items():
1339             self._known_related_objects.setdefault(field, {}).update(objects)
1340 
1341     def resolve_expression(self, *args, **kwargs):
1342         if self._fields and len(self._fields) > 1:
1343             # values() queryset can only be used as nested queries
1344             # if they are set up to select only a single field.
1345             raise TypeError('Cannot use multi-field values as a filter value.')
1346         query = self.query.resolve_expression(*args, **kwargs)
1347         query._db = self._db
1348         return query
1349     resolve_expression.queryset_only = True
1350 
1351     def _add_hints(self, **hints):
1352         """
1353         Update hinting information for use by routers. Add new key/values or
1354         overwrite existing key/values.
1355         """
1356         self._hints.update(hints)
1357 
1358     def _has_filters(self):
1359         """
1360         Check if this QuerySet has any filtering going on. This isn't
1361         equivalent with checking if all objects are present in results, for
1362         example, qs[1:]._has_filters() -> False.
1363         """
1364         return self.query.has_filters()
1365 
1366     @staticmethod
1367     def _validate_values_are_expressions(values, method_name):
1368         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1369         if invalid_args:
1370             raise TypeError(
1371                 'QuerySet.%s() received non-expression(s): %s.' % (
1372                     method_name,
1373                     ', '.join(invalid_args),
1374                 )
1375             )
1376 
1377     def _not_support_combined_queries(self, operation_name):
1378         if self.query.combinator:
1379             raise NotSupportedError(
1380                 'Calling QuerySet.%s() after %s() is not supported.'
1381                 % (operation_name, self.query.combinator)
1382             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-17 11:56:57.151 | INFO     | Search completed. Bug locations: [<file>django/db/models/expressions.py</file>
<class>None</class>
<method>get_group_by_cols</method>
<code>
350     def get_group_by_cols(self, alias=None):
351         if not self.contains_aggregate:
352             return [self]
353         cols = []
354         for source in self.get_source_expressions():
355             cols.extend(source.get_group_by_cols())
356         return cols

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>None</class>
<method>get_group_by_cols</method>
<code>
578     def get_group_by_cols(self, alias=None):
579         return []

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>None</class>
<method>get_group_by_cols</method>
<code>
706     def get_group_by_cols(self, alias=None):
707         return []

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>None</class>
<method>get_group_by_cols</method>
<code>
731     def get_group_by_cols(self, alias=None):
732         return [self]

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>None</class>
<method>get_group_by_cols</method>
<code>
790     def get_group_by_cols(self, alias=None):
791         return [self]

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>None</class>
<method>get_group_by_cols</method>
<code>
829     def get_group_by_cols(self, alias=None):
830         return [self]

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>None</class>
<method>get_group_by_cols</method>
<code>
932     def get_group_by_cols(self, alias=None):
933         # This is not a complete expression and cannot be used in GROUP BY.
934         cols = []
935         for source in self.get_source_expressions():
936             cols.extend(source.get_group_by_cols())
937         return cols

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>None</class>
<method>get_group_by_cols</method>
<code>
1069     def get_group_by_cols(self, alias=None):
1070         if alias:
1071             return [Ref(alias, self)]
1072         external_cols = self.query.get_external_cols()
1073         if any(col.possibly_multivalued for col in external_cols):
1074             return [self]
1075         return external_cols

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>None</class>
<method>get_group_by_cols</method>
<code>
1173     def get_group_by_cols(self, alias=None):
1174         cols = []
1175         for source in self.get_source_expressions():
1176             cols.extend(source.get_group_by_cols())
1177         return cols

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>None</class>
<method>get_group_by_cols</method>
<code>
1283     def get_group_by_cols(self, alias=None):
1284         return []

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>None</class>
<method>get_group_by_cols</method>
<code>
1319     def get_group_by_cols(self, alias=None):
1320         return []

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>BaseExpression</class>
<method>get_group_by_cols</method>
<code>
350     def get_group_by_cols(self, alias=None):
351         if not self.contains_aggregate:
352             return [self]
353         cols = []
354         for source in self.get_source_expressions():
355             cols.extend(source.get_group_by_cols())
356         return cols

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>ResolvedOuterRef</class>
<method>get_group_by_cols</method>
<code>
578     def get_group_by_cols(self, alias=None):
579         return []

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Value</class>
<method>get_group_by_cols</method>
<code>
706     def get_group_by_cols(self, alias=None):
707         return []

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>RawSQL</class>
<method>get_group_by_cols</method>
<code>
731     def get_group_by_cols(self, alias=None):
732         return [self]

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Col</class>
<method>get_group_by_cols</method>
<code>
790     def get_group_by_cols(self, alias=None):
791         return [self]

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Ref</class>
<method>get_group_by_cols</method>
<code>
829     def get_group_by_cols(self, alias=None):
830         return [self]

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>When</class>
<method>get_group_by_cols</method>
<code>
932     def get_group_by_cols(self, alias=None):
933         # This is not a complete expression and cannot be used in GROUP BY.
934         cols = []
935         for source in self.get_source_expressions():
936             cols.extend(source.get_group_by_cols())
937         return cols

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Subquery</class>
<method>get_group_by_cols</method>
<code>
1069     def get_group_by_cols(self, alias=None):
1070         if alias:
1071             return [Ref(alias, self)]
1072         external_cols = self.query.get_external_cols()
1073         if any(col.possibly_multivalued for col in external_cols):
1074             return [self]
1075         return external_cols

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>OrderBy</class>
<method>get_group_by_cols</method>
<code>
1173     def get_group_by_cols(self, alias=None):
1174         cols = []
1175         for source in self.get_source_expressions():
1176             cols.extend(source.get_group_by_cols())
1177         return cols

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Window</class>
<method>get_group_by_cols</method>
<code>
1283     def get_group_by_cols(self, alias=None):
1284         return []

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>WindowFrame</class>
<method>get_group_by_cols</method>
<code>
1319     def get_group_by_cols(self, alias=None):
1320         return []

</code><intended_behavior>The method get_group_by_cols for ExpressionWrapper should be updated or added if it doesn’t exist. It must be engineered to intelligently exclude constant expressions from being considered in the GROUP BY clause. This could involve checking if the wrapped expression is an instance of Value and, if so, not including it in the group by columns list. This adjustment would ensure that constant values don't inadvertently contribute to the GROUP BY clause in PostgreSQL, aligning with SQL standards.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>annotate</method>
<code>
1082     def annotate(self, *args, **kwargs):
1083         """
1084         Return a query set in which the returned objects have been annotated
1085         with extra data or aggregations.
1086         """
1087         self._not_support_combined_queries('annotate')
1088         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1089         annotations = {}
1090         for arg in args:
1091             # The default_alias property may raise a TypeError.
1092             try:
1093                 if arg.default_alias in kwargs:
1094                     raise ValueError("The named annotation '%s' conflicts with the "
1095                                      "default name for another annotation."
1096                                      % arg.default_alias)
1097             except TypeError:
1098                 raise TypeError("Complex annotations require an alias")
1099             annotations[arg.default_alias] = arg
1100         annotations.update(kwargs)
1101 
1102         clone = self._chain()
1103         names = self._fields
1104         if names is None:
1105             names = set(chain.from_iterable(
1106                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1107                 for field in self.model._meta.get_fields()
1108             ))
1109 
1110         for alias, annotation in annotations.items():
1111             if alias in names:
1112                 raise ValueError("The annotation '%s' conflicts with a field on "
1113                                  "the model." % alias)
1114             if isinstance(annotation, FilteredRelation):
1115                 clone.query.add_filtered_relation(annotation, alias)
1116             else:
1117                 clone.query.add_annotation(annotation, alias, is_summary=False)
1118 
1119         for alias, annotation in clone.query.annotations.items():
1120             if alias in annotations and annotation.contains_aggregate:
1121                 if clone._fields is None:
1122                     clone.query.group_by = True
1123                 else:
1124                     clone.query.set_group_by()
1125                 break
1126 
1127         return clone

</code><intended_behavior>Although not intrinsically faulty, a refined examination within the annotate method might be necessary to ensure it correctly handles the inclusion (or exclusion) of expressions wrapped by ExpressionWrapper in GROUP BY clauses. This does not suggest the annotate method itself is misbehaving, but a deeper integration or handling check with ExpressionWrapper's adjusted behavior (as proposed above) might be required to holistically resolve the issue. The annotate method should seamlessly support the refined logic of ExpressionWrapper.get_group_by_cols to exclude constant expressions from the GROUP BY clause where applicable, ensuring PostgreSQL compatibility.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
184 class QuerySet:
185     """Represent a lazy database lookup for a set of objects."""
186 
187     def __init__(self, model=None, query=None, using=None, hints=None):
188         self.model = model
189         self._db = using
190         self._hints = hints or {}
191         self._query = query or sql.Query(self.model)
192         self._result_cache = None
193         self._sticky_filter = False
194         self._for_write = False
195         self._prefetch_related_lookups = ()
196         self._prefetch_done = False
197         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
198         self._iterable_class = ModelIterable
199         self._fields = None
200         self._defer_next_filter = False
201         self._deferred_filter = None
202 
203     @property
204     def query(self):
205         if self._deferred_filter:
206             negate, args, kwargs = self._deferred_filter
207             self._filter_or_exclude_inplace(negate, *args, **kwargs)
208             self._deferred_filter = None
209         return self._query
210 
211     @query.setter
212     def query(self, value):
213         self._query = value
214 
215     def as_manager(cls):
216         # Address the circular dependency between `Queryset` and `Manager`.
217         from django.db.models.manager import Manager
218         manager = Manager.from_queryset(cls)()
219         manager._built_with_as_manager = True
220         return manager
221     as_manager.queryset_only = True
222     as_manager = classmethod(as_manager)
223 
224     ########################
225     # PYTHON MAGIC METHODS #
226     ########################
227 
228     def __deepcopy__(self, memo):
229         """Don't populate the QuerySet's cache."""
230         obj = self.__class__()
231         for k, v in self.__dict__.items():
232             if k == '_result_cache':
233                 obj.__dict__[k] = None
234             else:
235                 obj.__dict__[k] = copy.deepcopy(v, memo)
236         return obj
237 
238     def __getstate__(self):
239         # Force the cache to be fully populated.
240         self._fetch_all()
241         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
242 
243     def __setstate__(self, state):
244         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
245         if pickled_version:
246             if pickled_version != django.__version__:
247                 warnings.warn(
248                     "Pickled queryset instance's Django version %s does not "
249                     "match the current version %s."
250                     % (pickled_version, django.__version__),
251                     RuntimeWarning,
252                     stacklevel=2,
253                 )
254         else:
255             warnings.warn(
256                 "Pickled queryset instance's Django version is not specified.",
257                 RuntimeWarning,
258                 stacklevel=2,
259             )
260         self.__dict__.update(state)
261 
262     def __repr__(self):
263         data = list(self[:REPR_OUTPUT_SIZE + 1])
264         if len(data) > REPR_OUTPUT_SIZE:
265             data[-1] = "...(remaining elements truncated)..."
266         return '<%s %r>' % (self.__class__.__name__, data)
267 
268     def __len__(self):
269         self._fetch_all()
270         return len(self._result_cache)
271 
272     def __iter__(self):
273         """
274         The queryset iterator protocol uses three nested iterators in the
275         default case:
276             1. sql.compiler.execute_sql()
277                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
278                  using cursor.fetchmany(). This part is responsible for
279                  doing some column masking, and returning the rows in chunks.
280             2. sql.compiler.results_iter()
281                - Returns one row at time. At this point the rows are still just
282                  tuples. In some cases the return values are converted to
283                  Python values at this location.
284             3. self.iterator()
285                - Responsible for turning the rows into model objects.
286         """
287         self._fetch_all()
288         return iter(self._result_cache)
289 
290     def __bool__(self):
291         self._fetch_all()
292         return bool(self._result_cache)
293 
294     def __getitem__(self, k):
295         """Retrieve an item or slice from the set of results."""
296         if not isinstance(k, (int, slice)):
297             raise TypeError(
298                 'QuerySet indices must be integers or slices, not %s.'
299                 % type(k).__name__
300             )
301         assert ((not isinstance(k, slice) and (k >= 0)) or
302                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
303                  (k.stop is None or k.stop >= 0))), \
304             "Negative indexing is not supported."
305 
306         if self._result_cache is not None:
307             return self._result_cache[k]
308 
309         if isinstance(k, slice):
310             qs = self._chain()
311             if k.start is not None:
312                 start = int(k.start)
313             else:
314                 start = None
315             if k.stop is not None:
316                 stop = int(k.stop)
317             else:
318                 stop = None
319             qs.query.set_limits(start, stop)
320             return list(qs)[::k.step] if k.step else qs
321 
322         qs = self._chain()
323         qs.query.set_limits(k, k + 1)
324         qs._fetch_all()
325         return qs._result_cache[0]
326 
327     def __class_getitem__(cls, *args, **kwargs):
328         return cls
329 
330     def __and__(self, other):
331         self._merge_sanity_check(other)
332         if isinstance(other, EmptyQuerySet):
333             return other
334         if isinstance(self, EmptyQuerySet):
335             return self
336         combined = self._chain()
337         combined._merge_known_related_objects(other)
338         combined.query.combine(other.query, sql.AND)
339         return combined
340 
341     def __or__(self, other):
342         self._merge_sanity_check(other)
343         if isinstance(self, EmptyQuerySet):
344             return other
345         if isinstance(other, EmptyQuerySet):
346             return self
347         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
348         combined = query._chain()
349         combined._merge_known_related_objects(other)
350         if not other.query.can_filter():
351             other = other.model._base_manager.filter(pk__in=other.values('pk'))
352         combined.query.combine(other.query, sql.OR)
353         return combined
354 
355     ####################################
356     # METHODS THAT DO DATABASE QUERIES #
357     ####################################
358 
359     def _iterator(self, use_chunked_fetch, chunk_size):
360         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
361 
362     def iterator(self, chunk_size=2000):
363         """
364         An iterator over the results from applying this QuerySet to the
365         database.
366         """
367         if chunk_size <= 0:
368             raise ValueError('Chunk size must be strictly positive.')
369         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
370         return self._iterator(use_chunked_fetch, chunk_size)
371 
372     def aggregate(self, *args, **kwargs):
373         """
374         Return a dictionary containing the calculations (aggregation)
375         over the current queryset.
376 
377         If args is present the expression is passed as a kwarg using
378         the Aggregate object's default alias.
379         """
380         if self.query.distinct_fields:
381             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
382         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
383         for arg in args:
384             # The default_alias property raises TypeError if default_alias
385             # can't be set automatically or AttributeError if it isn't an
386             # attribute.
387             try:
388                 arg.default_alias
389             except (AttributeError, TypeError):
390                 raise TypeError("Complex aggregates require an alias")
391             kwargs[arg.default_alias] = arg
392 
393         query = self.query.chain()
394         for (alias, aggregate_expr) in kwargs.items():
395             query.add_annotation(aggregate_expr, alias, is_summary=True)
396             if not query.annotations[alias].contains_aggregate:
397                 raise TypeError("%s is not an aggregate expression" % alias)
398         return query.get_aggregation(self.db, kwargs)
399 
400     def count(self):
401         """
402         Perform a SELECT COUNT() and return the number of records as an
403         integer.
404 
405         If the QuerySet is already fully cached, return the length of the
406         cached results set to avoid multiple SELECT COUNT(*) calls.
407         """
408         if self._result_cache is not None:
409             return len(self._result_cache)
410 
411         return self.query.get_count(using=self.db)
412 
413     def get(self, *args, **kwargs):
414         """
415         Perform the query and return a single object matching the given
416         keyword arguments.
417         """
418         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
419         if self.query.can_filter() and not self.query.distinct_fields:
420             clone = clone.order_by()
421         limit = None
422         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
423             limit = MAX_GET_RESULTS
424             clone.query.set_limits(high=limit)
425         num = len(clone)
426         if num == 1:
427             return clone._result_cache[0]
428         if not num:
429             raise self.model.DoesNotExist(
430                 "%s matching query does not exist." %
431                 self.model._meta.object_name
432             )
433         raise self.model.MultipleObjectsReturned(
434             'get() returned more than one %s -- it returned %s!' % (
435                 self.model._meta.object_name,
436                 num if not limit or num < limit else 'more than %s' % (limit - 1),
437             )
438         )
439 
440     def create(self, **kwargs):
441         """
442         Create a new object with the given kwargs, saving it to the database
443         and returning the created object.
444         """
445         obj = self.model(**kwargs)
446         self._for_write = True
447         obj.save(force_insert=True, using=self.db)
448         return obj
449 
450     def _populate_pk_values(self, objs):
451         for obj in objs:
452             if obj.pk is None:
453                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
454 
455     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
456         """
457         Insert each of the instances into the database. Do *not* call
458         save() on each of the instances, do not send any pre/post_save
459         signals, and do not set the primary key attribute if it is an
460         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
461         Multi-table models are not supported.
462         """
463         # When you bulk insert you don't get the primary keys back (if it's an
464         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
465         # you can't insert into the child tables which references this. There
466         # are two workarounds:
467         # 1) This could be implemented if you didn't have an autoincrement pk
468         # 2) You could do it by doing O(n) normal inserts into the parent
469         #    tables to get the primary keys back and then doing a single bulk
470         #    insert into the childmost table.
471         # We currently set the primary keys on the objects when using
472         # PostgreSQL via the RETURNING ID clause. It should be possible for
473         # Oracle as well, but the semantics for extracting the primary keys is
474         # trickier so it's not done yet.
475         assert batch_size is None or batch_size > 0
476         # Check that the parents share the same concrete model with the our
477         # model to detect the inheritance pattern ConcreteGrandParent ->
478         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
479         # would not identify that case as involving multiple tables.
480         for parent in self.model._meta.get_parent_list():
481             if parent._meta.concrete_model is not self.model._meta.concrete_model:
482                 raise ValueError("Can't bulk create a multi-table inherited model")
483         if not objs:
484             return objs
485         self._for_write = True
486         connection = connections[self.db]
487         opts = self.model._meta
488         fields = opts.concrete_fields
489         objs = list(objs)
490         self._populate_pk_values(objs)
491         with transaction.atomic(using=self.db, savepoint=False):
492             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
493             if objs_with_pk:
494                 returned_columns = self._batched_insert(
495                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
496                 )
497                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
498                     for result, field in zip(results, opts.db_returning_fields):
499                         if field != opts.pk:
500                             setattr(obj_with_pk, field.attname, result)
501                 for obj_with_pk in objs_with_pk:
502                     obj_with_pk._state.adding = False
503                     obj_with_pk._state.db = self.db
504             if objs_without_pk:
505                 fields = [f for f in fields if not isinstance(f, AutoField)]
506                 returned_columns = self._batched_insert(
507                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
508                 )
509                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
510                     assert len(returned_columns) == len(objs_without_pk)
511                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
512                     for result, field in zip(results, opts.db_returning_fields):
513                         setattr(obj_without_pk, field.attname, result)
514                     obj_without_pk._state.adding = False
515                     obj_without_pk._state.db = self.db
516 
517         return objs
518 
519     def bulk_update(self, objs, fields, batch_size=None):
520         """
521         Update the given fields in each of the given objects in the database.
522         """
523         if batch_size is not None and batch_size < 0:
524             raise ValueError('Batch size must be a positive integer.')
525         if not fields:
526             raise ValueError('Field names must be given to bulk_update().')
527         objs = tuple(objs)
528         if any(obj.pk is None for obj in objs):
529             raise ValueError('All bulk_update() objects must have a primary key set.')
530         fields = [self.model._meta.get_field(name) for name in fields]
531         if any(not f.concrete or f.many_to_many for f in fields):
532             raise ValueError('bulk_update() can only be used with concrete fields.')
533         if any(f.primary_key for f in fields):
534             raise ValueError('bulk_update() cannot be used with primary key fields.')
535         if not objs:
536             return
537         # PK is used twice in the resulting update query, once in the filter
538         # and once in the WHEN. Each field will also have one CAST.
539         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
540         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
541         requires_casting = connections[self.db].features.requires_casted_case_in_updates
542         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
543         updates = []
544         for batch_objs in batches:
545             update_kwargs = {}
546             for field in fields:
547                 when_statements = []
548                 for obj in batch_objs:
549                     attr = getattr(obj, field.attname)
550                     if not isinstance(attr, Expression):
551                         attr = Value(attr, output_field=field)
552                     when_statements.append(When(pk=obj.pk, then=attr))
553                 case_statement = Case(*when_statements, output_field=field)
554                 if requires_casting:
555                     case_statement = Cast(case_statement, output_field=field)
556                 update_kwargs[field.attname] = case_statement
557             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
558         with transaction.atomic(using=self.db, savepoint=False):
559             for pks, update_kwargs in updates:
560                 self.filter(pk__in=pks).update(**update_kwargs)
561     bulk_update.alters_data = True
562 
563     def get_or_create(self, defaults=None, **kwargs):
564         """
565         Look up an object with the given kwargs, creating one if necessary.
566         Return a tuple of (object, created), where created is a boolean
567         specifying whether an object was created.
568         """
569         # The get() needs to be targeted at the write database in order
570         # to avoid potential transaction consistency problems.
571         self._for_write = True
572         try:
573             return self.get(**kwargs), False
574         except self.model.DoesNotExist:
575             params = self._extract_model_params(defaults, **kwargs)
576             return self._create_object_from_params(kwargs, params)
577 
578     def update_or_create(self, defaults=None, **kwargs):
579         """
580         Look up an object with the given kwargs, updating one with defaults
581         if it exists, otherwise create a new one.
582         Return a tuple (object, created), where created is a boolean
583         specifying whether an object was created.
584         """
585         defaults = defaults or {}
586         self._for_write = True
587         with transaction.atomic(using=self.db):
588             try:
589                 obj = self.select_for_update().get(**kwargs)
590             except self.model.DoesNotExist:
591                 params = self._extract_model_params(defaults, **kwargs)
592                 # Lock the row so that a concurrent update is blocked until
593                 # after update_or_create() has performed its save.
594                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
595                 if created:
596                     return obj, created
597             for k, v in resolve_callables(defaults):
598                 setattr(obj, k, v)
599             obj.save(using=self.db)
600         return obj, False
601 
602     def _create_object_from_params(self, lookup, params, lock=False):
603         """
604         Try to create an object using passed params. Used by get_or_create()
605         and update_or_create().
606         """
607         try:
608             with transaction.atomic(using=self.db):
609                 params = dict(resolve_callables(params))
610                 obj = self.create(**params)
611             return obj, True
612         except IntegrityError:
613             try:
614                 qs = self.select_for_update() if lock else self
615                 return qs.get(**lookup), False
616             except self.model.DoesNotExist:
617                 pass
618             raise
619 
620     def _extract_model_params(self, defaults, **kwargs):
621         """
622         Prepare `params` for creating a model instance based on the given
623         kwargs; for use by get_or_create() and update_or_create().
624         """
625         defaults = defaults or {}
626         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
627         params.update(defaults)
628         property_names = self.model._meta._property_names
629         invalid_params = []
630         for param in params:
631             try:
632                 self.model._meta.get_field(param)
633             except exceptions.FieldDoesNotExist:
634                 # It's okay to use a model's property if it has a setter.
635                 if not (param in property_names and getattr(self.model, param).fset):
636                     invalid_params.append(param)
637         if invalid_params:
638             raise exceptions.FieldError(
639                 "Invalid field name(s) for model %s: '%s'." % (
640                     self.model._meta.object_name,
641                     "', '".join(sorted(invalid_params)),
642                 ))
643         return params
644 
645     def _earliest(self, *fields):
646         """
647         Return the earliest object according to fields (if given) or by the
648         model's Meta.get_latest_by.
649         """
650         if fields:
651             order_by = fields
652         else:
653             order_by = getattr(self.model._meta, 'get_latest_by')
654             if order_by and not isinstance(order_by, (tuple, list)):
655                 order_by = (order_by,)
656         if order_by is None:
657             raise ValueError(
658                 "earliest() and latest() require either fields as positional "
659                 "arguments or 'get_latest_by' in the model's Meta."
660             )
661 
662         assert not self.query.is_sliced, \
663             "Cannot change a query once a slice has been taken."
664         obj = self._chain()
665         obj.query.set_limits(high=1)
666         obj.query.clear_ordering(force_empty=True)
667         obj.query.add_ordering(*order_by)
668         return obj.get()
669 
670     def earliest(self, *fields):
671         return self._earliest(*fields)
672 
673     def latest(self, *fields):
674         return self.reverse()._earliest(*fields)
675 
676     def first(self):
677         """Return the first object of a query or None if no match is found."""
678         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
679             return obj
680 
681     def last(self):
682         """Return the last object of a query or None if no match is found."""
683         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
684             return obj
685 
686     def in_bulk(self, id_list=None, *, field_name='pk'):
687         """
688         Return a dictionary mapping each of the given IDs to the object with
689         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
690         """
691         assert not self.query.is_sliced, \
692             "Cannot use 'limit' or 'offset' with in_bulk"
693         opts = self.model._meta
694         unique_fields = [
695             constraint.fields[0]
696             for constraint in opts.total_unique_constraints
697             if len(constraint.fields) == 1
698         ]
699         if (
700             field_name != 'pk' and
701             not opts.get_field(field_name).unique and
702             field_name not in unique_fields
703         ):
704             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
705         if id_list is not None:
706             if not id_list:
707                 return {}
708             filter_key = '{}__in'.format(field_name)
709             batch_size = connections[self.db].features.max_query_params
710             id_list = tuple(id_list)
711             # If the database has a limit on the number of query parameters
712             # (e.g. SQLite), retrieve objects in batches if necessary.
713             if batch_size and batch_size < len(id_list):
714                 qs = ()
715                 for offset in range(0, len(id_list), batch_size):
716                     batch = id_list[offset:offset + batch_size]
717                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
718             else:
719                 qs = self.filter(**{filter_key: id_list}).order_by()
720         else:
721             qs = self._chain()
722         return {getattr(obj, field_name): obj for obj in qs}
723 
724     def delete(self):
725         """Delete the records in the current QuerySet."""
726         self._not_support_combined_queries('delete')
727         assert not self.query.is_sliced, \
728             "Cannot use 'limit' or 'offset' with delete."
729 
730         if self._fields is not None:
731             raise TypeError("Cannot call delete() after .values() or .values_list()")
732 
733         del_query = self._chain()
734 
735         # The delete is actually 2 queries - one to find related objects,
736         # and one to delete. Make sure that the discovery of related
737         # objects is performed on the same database as the deletion.
738         del_query._for_write = True
739 
740         # Disable non-supported fields.
741         del_query.query.select_for_update = False
742         del_query.query.select_related = False
743         del_query.query.clear_ordering(force_empty=True)
744 
745         collector = Collector(using=del_query.db)
746         collector.collect(del_query)
747         deleted, _rows_count = collector.delete()
748 
749         # Clear the result cache, in case this QuerySet gets reused.
750         self._result_cache = None
751         return deleted, _rows_count
752 
753     delete.alters_data = True
754     delete.queryset_only = True
755 
756     def _raw_delete(self, using):
757         """
758         Delete objects found from the given queryset in single direct SQL
759         query. No signals are sent and there is no protection for cascades.
760         """
761         query = self.query.clone()
762         query.__class__ = sql.DeleteQuery
763         cursor = query.get_compiler(using).execute_sql(CURSOR)
764         if cursor:
765             with cursor:
766                 return cursor.rowcount
767         return 0
768     _raw_delete.alters_data = True
769 
770     def update(self, **kwargs):
771         """
772         Update all elements in the current QuerySet, setting all the given
773         fields to the appropriate values.
774         """
775         self._not_support_combined_queries('update')
776         assert not self.query.is_sliced, \
777             "Cannot update a query once a slice has been taken."
778         self._for_write = True
779         query = self.query.chain(sql.UpdateQuery)
780         query.add_update_values(kwargs)
781         # Clear any annotations so that they won't be present in subqueries.
782         query.annotations = {}
783         with transaction.mark_for_rollback_on_error(using=self.db):
784             rows = query.get_compiler(self.db).execute_sql(CURSOR)
785         self._result_cache = None
786         return rows
787     update.alters_data = True
788 
789     def _update(self, values):
790         """
791         A version of update() that accepts field objects instead of field names.
792         Used primarily for model saving and not intended for use by general
793         code (it requires too much poking around at model internals to be
794         useful at that level).
795         """
796         assert not self.query.is_sliced, \
797             "Cannot update a query once a slice has been taken."
798         query = self.query.chain(sql.UpdateQuery)
799         query.add_update_fields(values)
800         # Clear any annotations so that they won't be present in subqueries.
801         query.annotations = {}
802         self._result_cache = None
803         return query.get_compiler(self.db).execute_sql(CURSOR)
804     _update.alters_data = True
805     _update.queryset_only = False
806 
807     def exists(self):
808         if self._result_cache is None:
809             return self.query.has_results(using=self.db)
810         return bool(self._result_cache)
811 
812     def _prefetch_related_objects(self):
813         # This method can only be called once the result cache has been filled.
814         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
815         self._prefetch_done = True
816 
817     def explain(self, *, format=None, **options):
818         return self.query.explain(using=self.db, format=format, **options)
819 
820     ##################################################
821     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
822     ##################################################
823 
824     def raw(self, raw_query, params=None, translations=None, using=None):
825         if using is None:
826             using = self.db
827         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
828         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
829         return qs
830 
831     def _values(self, *fields, **expressions):
832         clone = self._chain()
833         if expressions:
834             clone = clone.annotate(**expressions)
835         clone._fields = fields
836         clone.query.set_values(fields)
837         return clone
838 
839     def values(self, *fields, **expressions):
840         fields += tuple(expressions)
841         clone = self._values(*fields, **expressions)
842         clone._iterable_class = ValuesIterable
843         return clone
844 
845     def values_list(self, *fields, flat=False, named=False):
846         if flat and named:
847             raise TypeError("'flat' and 'named' can't be used together.")
848         if flat and len(fields) > 1:
849             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
850 
851         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
852         _fields = []
853         expressions = {}
854         counter = 1
855         for field in fields:
856             if hasattr(field, 'resolve_expression'):
857                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
858                 while True:
859                     field_id = field_id_prefix + str(counter)
860                     counter += 1
861                     if field_id not in field_names:
862                         break
863                 expressions[field_id] = field
864                 _fields.append(field_id)
865             else:
866                 _fields.append(field)
867 
868         clone = self._values(*_fields, **expressions)
869         clone._iterable_class = (
870             NamedValuesListIterable if named
871             else FlatValuesListIterable if flat
872             else ValuesListIterable
873         )
874         return clone
875 
876     def dates(self, field_name, kind, order='ASC'):
877         """
878         Return a list of date objects representing all available dates for
879         the given field_name, scoped to 'kind'.
880         """
881         assert kind in ('year', 'month', 'week', 'day'), \
882             "'kind' must be one of 'year', 'month', 'week', or 'day'."
883         assert order in ('ASC', 'DESC'), \
884             "'order' must be either 'ASC' or 'DESC'."
885         return self.annotate(
886             datefield=Trunc(field_name, kind, output_field=DateField()),
887             plain_field=F(field_name)
888         ).values_list(
889             'datefield', flat=True
890         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
891 
892     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):
893         """
894         Return a list of datetime objects representing all available
895         datetimes for the given field_name, scoped to 'kind'.
896         """
897         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
898             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
899         assert order in ('ASC', 'DESC'), \
900             "'order' must be either 'ASC' or 'DESC'."
901         if settings.USE_TZ:
902             if tzinfo is None:
903                 tzinfo = timezone.get_current_timezone()
904         else:
905             tzinfo = None
906         return self.annotate(
907             datetimefield=Trunc(
908                 field_name,
909                 kind,
910                 output_field=DateTimeField(),
911                 tzinfo=tzinfo,
912                 is_dst=is_dst,
913             ),
914             plain_field=F(field_name)
915         ).values_list(
916             'datetimefield', flat=True
917         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
918 
919     def none(self):
920         """Return an empty QuerySet."""
921         clone = self._chain()
922         clone.query.set_empty()
923         return clone
924 
925     ##################################################################
926     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
927     ##################################################################
928 
929     def all(self):
930         """
931         Return a new QuerySet that is a copy of the current one. This allows a
932         QuerySet to proxy for a model manager in some cases.
933         """
934         return self._chain()
935 
936     def filter(self, *args, **kwargs):
937         """
938         Return a new QuerySet instance with the args ANDed to the existing
939         set.
940         """
941         self._not_support_combined_queries('filter')
942         return self._filter_or_exclude(False, *args, **kwargs)
943 
944     def exclude(self, *args, **kwargs):
945         """
946         Return a new QuerySet instance with NOT (args) ANDed to the existing
947         set.
948         """
949         self._not_support_combined_queries('exclude')
950         return self._filter_or_exclude(True, *args, **kwargs)
951 
952     def _filter_or_exclude(self, negate, *args, **kwargs):
953         if args or kwargs:
954             assert not self.query.is_sliced, \
955                 "Cannot filter a query once a slice has been taken."
956 
957         clone = self._chain()
958         if self._defer_next_filter:
959             self._defer_next_filter = False
960             clone._deferred_filter = negate, args, kwargs
961         else:
962             clone._filter_or_exclude_inplace(negate, *args, **kwargs)
963         return clone
964 
965     def _filter_or_exclude_inplace(self, negate, *args, **kwargs):
966         if negate:
967             self._query.add_q(~Q(*args, **kwargs))
968         else:
969             self._query.add_q(Q(*args, **kwargs))
970 
971     def complex_filter(self, filter_obj):
972         """
973         Return a new QuerySet instance with filter_obj added to the filters.
974 
975         filter_obj can be a Q object or a dictionary of keyword lookup
976         arguments.
977 
978         This exists to support framework features such as 'limit_choices_to',
979         and usually it will be more natural to use other methods.
980         """
981         if isinstance(filter_obj, Q):
982             clone = self._chain()
983             clone.query.add_q(filter_obj)
984             return clone
985         else:
986             return self._filter_or_exclude(False, **filter_obj)
987 
988     def _combinator_query(self, combinator, *other_qs, all=False):
989         # Clone the query to inherit the select list and everything
990         clone = self._chain()
991         # Clear limits and ordering so they can be reapplied
992         clone.query.clear_ordering(True)
993         clone.query.clear_limits()
994         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
995         clone.query.combinator = combinator
996         clone.query.combinator_all = all
997         return clone
998 
999     def union(self, *other_qs, all=False):
1000         # If the query is an EmptyQuerySet, combine all nonempty querysets.
1001         if isinstance(self, EmptyQuerySet):
1002             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1003             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
1004         return self._combinator_query('union', *other_qs, all=all)
1005 
1006     def intersection(self, *other_qs):
1007         # If any query is an EmptyQuerySet, return it.
1008         if isinstance(self, EmptyQuerySet):
1009             return self
1010         for other in other_qs:
1011             if isinstance(other, EmptyQuerySet):
1012                 return other
1013         return self._combinator_query('intersection', *other_qs)
1014 
1015     def difference(self, *other_qs):
1016         # If the query is an EmptyQuerySet, return it.
1017         if isinstance(self, EmptyQuerySet):
1018             return self
1019         return self._combinator_query('difference', *other_qs)
1020 
1021     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1022         """
1023         Return a new QuerySet instance that will select objects with a
1024         FOR UPDATE lock.
1025         """
1026         if nowait and skip_locked:
1027             raise ValueError('The nowait option cannot be used with skip_locked.')
1028         obj = self._chain()
1029         obj._for_write = True
1030         obj.query.select_for_update = True
1031         obj.query.select_for_update_nowait = nowait
1032         obj.query.select_for_update_skip_locked = skip_locked
1033         obj.query.select_for_update_of = of
1034         obj.query.select_for_no_key_update = no_key
1035         return obj
1036 
1037     def select_related(self, *fields):
1038         """
1039         Return a new QuerySet instance that will select related objects.
1040 
1041         If fields are specified, they must be ForeignKey fields and only those
1042         related objects are included in the selection.
1043 
1044         If select_related(None) is called, clear the list.
1045         """
1046         self._not_support_combined_queries('select_related')
1047         if self._fields is not None:
1048             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1049 
1050         obj = self._chain()
1051         if fields == (None,):
1052             obj.query.select_related = False
1053         elif fields:
1054             obj.query.add_select_related(fields)
1055         else:
1056             obj.query.select_related = True
1057         return obj
1058 
1059     def prefetch_related(self, *lookups):
1060         """
1061         Return a new QuerySet instance that will prefetch the specified
1062         Many-To-One and Many-To-Many related objects when the QuerySet is
1063         evaluated.
1064 
1065         When prefetch_related() is called more than once, append to the list of
1066         prefetch lookups. If prefetch_related(None) is called, clear the list.
1067         """
1068         self._not_support_combined_queries('prefetch_related')
1069         clone = self._chain()
1070         if lookups == (None,):
1071             clone._prefetch_related_lookups = ()
1072         else:
1073             for lookup in lookups:
1074                 if isinstance(lookup, Prefetch):
1075                     lookup = lookup.prefetch_to
1076                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1077                 if lookup in self.query._filtered_relations:
1078                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1079             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1080         return clone
1081 
1082     def annotate(self, *args, **kwargs):
1083         """
1084         Return a query set in which the returned objects have been annotated
1085         with extra data or aggregations.
1086         """
1087         self._not_support_combined_queries('annotate')
1088         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1089         annotations = {}
1090         for arg in args:
1091             # The default_alias property may raise a TypeError.
1092             try:
1093                 if arg.default_alias in kwargs:
1094                     raise ValueError("The named annotation '%s' conflicts with the "
1095                                      "default name for another annotation."
1096                                      % arg.default_alias)
1097             except TypeError:
1098                 raise TypeError("Complex annotations require an alias")
1099             annotations[arg.default_alias] = arg
1100         annotations.update(kwargs)
1101 
1102         clone = self._chain()
1103         names = self._fields
1104         if names is None:
1105             names = set(chain.from_iterable(
1106                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1107                 for field in self.model._meta.get_fields()
1108             ))
1109 
1110         for alias, annotation in annotations.items():
1111             if alias in names:
1112                 raise ValueError("The annotation '%s' conflicts with a field on "
1113                                  "the model." % alias)
1114             if isinstance(annotation, FilteredRelation):
1115                 clone.query.add_filtered_relation(annotation, alias)
1116             else:
1117                 clone.query.add_annotation(annotation, alias, is_summary=False)
1118 
1119         for alias, annotation in clone.query.annotations.items():
1120             if alias in annotations and annotation.contains_aggregate:
1121                 if clone._fields is None:
1122                     clone.query.group_by = True
1123                 else:
1124                     clone.query.set_group_by()
1125                 break
1126 
1127         return clone
1128 
1129     def order_by(self, *field_names):
1130         """Return a new QuerySet instance with the ordering changed."""
1131         assert not self.query.is_sliced, \
1132             "Cannot reorder a query once a slice has been taken."
1133         obj = self._chain()
1134         obj.query.clear_ordering(force_empty=False)
1135         obj.query.add_ordering(*field_names)
1136         return obj
1137 
1138     def distinct(self, *field_names):
1139         """
1140         Return a new QuerySet instance that will select only distinct results.
1141         """
1142         self._not_support_combined_queries('distinct')
1143         assert not self.query.is_sliced, \
1144             "Cannot create distinct fields once a slice has been taken."
1145         obj = self._chain()
1146         obj.query.add_distinct_fields(*field_names)
1147         return obj
1148 
1149     def extra(self, select=None, where=None, params=None, tables=None,
1150               order_by=None, select_params=None):
1151         """Add extra SQL fragments to the query."""
1152         self._not_support_combined_queries('extra')
1153         assert not self.query.is_sliced, \
1154             "Cannot change a query once a slice has been taken"
1155         clone = self._chain()
1156         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1157         return clone
1158 
1159     def reverse(self):
1160         """Reverse the ordering of the QuerySet."""
1161         if self.query.is_sliced:
1162             raise TypeError('Cannot reverse a query once a slice has been taken.')
1163         clone = self._chain()
1164         clone.query.standard_ordering = not clone.query.standard_ordering
1165         return clone
1166 
1167     def defer(self, *fields):
1168         """
1169         Defer the loading of data for certain fields until they are accessed.
1170         Add the set of deferred fields to any existing set of deferred fields.
1171         The only exception to this is if None is passed in as the only
1172         parameter, in which case removal all deferrals.
1173         """
1174         self._not_support_combined_queries('defer')
1175         if self._fields is not None:
1176             raise TypeError("Cannot call defer() after .values() or .values_list()")
1177         clone = self._chain()
1178         if fields == (None,):
1179             clone.query.clear_deferred_loading()
1180         else:
1181             clone.query.add_deferred_loading(fields)
1182         return clone
1183 
1184     def only(self, *fields):
1185         """
1186         Essentially, the opposite of defer(). Only the fields passed into this
1187         method and that are not already specified as deferred are loaded
1188         immediately when the queryset is evaluated.
1189         """
1190         self._not_support_combined_queries('only')
1191         if self._fields is not None:
1192             raise TypeError("Cannot call only() after .values() or .values_list()")
1193         if fields == (None,):
1194             # Can only pass None to defer(), not only(), as the rest option.
1195             # That won't stop people trying to do this, so let's be explicit.
1196             raise TypeError("Cannot pass None as an argument to only().")
1197         for field in fields:
1198             field = field.split(LOOKUP_SEP, 1)[0]
1199             if field in self.query._filtered_relations:
1200                 raise ValueError('only() is not supported with FilteredRelation.')
1201         clone = self._chain()
1202         clone.query.add_immediate_loading(fields)
1203         return clone
1204 
1205     def using(self, alias):
1206         """Select which database this QuerySet should execute against."""
1207         clone = self._chain()
1208         clone._db = alias
1209         return clone
1210 
1211     ###################################
1212     # PUBLIC INTROSPECTION ATTRIBUTES #
1213     ###################################
1214 
1215     @property
1216     def ordered(self):
1217         """
1218         Return True if the QuerySet is ordered -- i.e. has an order_by()
1219         clause or a default ordering on the model (or is empty).
1220         """
1221         if isinstance(self, EmptyQuerySet):
1222             return True
1223         if self.query.extra_order_by or self.query.order_by:
1224             return True
1225         elif self.query.default_ordering and self.query.get_meta().ordering:
1226             return True
1227         else:
1228             return False
1229 
1230     @property
1231     def db(self):
1232         """Return the database used if this query is executed now."""
1233         if self._for_write:
1234             return self._db or router.db_for_write(self.model, **self._hints)
1235         return self._db or router.db_for_read(self.model, **self._hints)
1236 
1237     ###################
1238     # PRIVATE METHODS #
1239     ###################
1240 
1241     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1242         """
1243         Insert a new record for the given model. This provides an interface to
1244         the InsertQuery class and is how Model.save() is implemented.
1245         """
1246         self._for_write = True
1247         if using is None:
1248             using = self.db
1249         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1250         query.insert_values(fields, objs, raw=raw)
1251         return query.get_compiler(using=using).execute_sql(returning_fields)
1252     _insert.alters_data = True
1253     _insert.queryset_only = False
1254 
1255     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1256         """
1257         Helper method for bulk_create() to insert objs one batch at a time.
1258         """
1259         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1260             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1261         ops = connections[self.db].ops
1262         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1263         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1264         inserted_rows = []
1265         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1266         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1267             if bulk_return and not ignore_conflicts:
1268                 inserted_rows.extend(self._insert(
1269                     item, fields=fields, using=self.db,
1270                     returning_fields=self.model._meta.db_returning_fields,
1271                     ignore_conflicts=ignore_conflicts,
1272                 ))
1273             else:
1274                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1275         return inserted_rows
1276 
1277     def _chain(self, **kwargs):
1278         """
1279         Return a copy of the current QuerySet that's ready for another
1280         operation.
1281         """
1282         obj = self._clone()
1283         if obj._sticky_filter:
1284             obj.query.filter_is_sticky = True
1285             obj._sticky_filter = False
1286         obj.__dict__.update(kwargs)
1287         return obj
1288 
1289     def _clone(self):
1290         """
1291         Return a copy of the current QuerySet. A lightweight alternative
1292         to deepcopy().
1293         """
1294         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1295         c._sticky_filter = self._sticky_filter
1296         c._for_write = self._for_write
1297         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1298         c._known_related_objects = self._known_related_objects
1299         c._iterable_class = self._iterable_class
1300         c._fields = self._fields
1301         return c
1302 
1303     def _fetch_all(self):
1304         if self._result_cache is None:
1305             self._result_cache = list(self._iterable_class(self))
1306         if self._prefetch_related_lookups and not self._prefetch_done:
1307             self._prefetch_related_objects()
1308 
1309     def _next_is_sticky(self):
1310         """
1311         Indicate that the next filter call and the one following that should
1312         be treated as a single filter. This is only important when it comes to
1313         determining when to reuse tables for many-to-many filters. Required so
1314         that we can filter naturally on the results of related managers.
1315 
1316         This doesn't return a clone of the current QuerySet (it returns
1317         "self"). The method is only used internally and should be immediately
1318         followed by a filter() that does create a clone.
1319         """
1320         self._sticky_filter = True
1321         return self
1322 
1323     def _merge_sanity_check(self, other):
1324         """Check that two QuerySet classes may be merged."""
1325         if self._fields is not None and (
1326                 set(self.query.values_select) != set(other.query.values_select) or
1327                 set(self.query.extra_select) != set(other.query.extra_select) or
1328                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1329             raise TypeError(
1330                 "Merging '%s' classes must involve the same values in each case."
1331                 % self.__class__.__name__
1332             )
1333 
1334     def _merge_known_related_objects(self, other):
1335         """
1336         Keep track of all known related objects from either QuerySet instance.
1337         """
1338         for field, objects in other._known_related_objects.items():
1339             self._known_related_objects.setdefault(field, {}).update(objects)
1340 
1341     def resolve_expression(self, *args, **kwargs):
1342         if self._fields and len(self._fields) > 1:
1343             # values() queryset can only be used as nested queries
1344             # if they are set up to select only a single field.
1345             raise TypeError('Cannot use multi-field values as a filter value.')
1346         query = self.query.resolve_expression(*args, **kwargs)
1347         query._db = self._db
1348         return query
1349     resolve_expression.queryset_only = True
1350 
1351     def _add_hints(self, **hints):
1352         """
1353         Update hinting information for use by routers. Add new key/values or
1354         overwrite existing key/values.
1355         """
1356         self._hints.update(hints)
1357 
1358     def _has_filters(self):
1359         """
1360         Check if this QuerySet has any filtering going on. This isn't
1361         equivalent with checking if all objects are present in results, for
1362         example, qs[1:]._has_filters() -> False.
1363         """
1364         return self.query.has_filters()
1365 
1366     @staticmethod
1367     def _validate_values_are_expressions(values, method_name):
1368         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1369         if invalid_args:
1370             raise TypeError(
1371                 'QuerySet.%s() received non-expression(s): %s.' % (
1372                     method_name,
1373                     ', '.join(invalid_args),
1374                 )
1375             )
1376 
1377     def _not_support_combined_queries(self, operation_name):
1378         if self.query.combinator:
1379             raise NotSupportedError(
1380                 'Calling QuerySet.%s() after %s() is not supported.'
1381                 % (operation_name, self.query.combinator)
1382             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-17 11:56:57.151 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-17 11:56:57.151 | INFO     | Search-only: skipping patch selection
2025-10-17 11:56:57.299 | INFO     | Task django__django-13012 completed successfully.
2025-10-17 11:56:57.300 | INFO     | No patch generated. You can try running ACR again.
