[
    {
        "rel_file_path": "sklearn/model_selection/_search.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/model_selection/_search.py",
        "start": 605,
        "end": 749,
        "class_name": null,
        "method_name": "fit",
        "code": "605     def fit(self, X, y=None, groups=None, **fit_params):\n606         \"\"\"Run fit with all sets of parameters.\n607 \n608         Parameters\n609         ----------\n610 \n611         X : array-like, shape = [n_samples, n_features]\n612             Training vector, where n_samples is the number of samples and\n613             n_features is the number of features.\n614 \n615         y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n616             Target relative to X for classification or regression;\n617             None for unsupervised learning.\n618 \n619         groups : array-like, with shape (n_samples,), optional\n620             Group labels for the samples used while splitting the dataset into\n621             train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n622             instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).\n623 \n624         **fit_params : dict of string -> object\n625             Parameters passed to the ``fit`` method of the estimator\n626         \"\"\"\n627         estimator = self.estimator\n628         cv = check_cv(self.cv, y, classifier=is_classifier(estimator))\n629 \n630         scorers, self.multimetric_ = _check_multimetric_scoring(\n631             self.estimator, scoring=self.scoring)\n632 \n633         if self.multimetric_:\n634             if self.refit is not False and (\n635                     not isinstance(self.refit, str) or\n636                     # This will work for both dict / list (tuple)\n637                     self.refit not in scorers) and not callable(self.refit):\n638                 raise ValueError(\"For multi-metric scoring, the parameter \"\n639                                  \"refit must be set to a scorer key or a \"\n640                                  \"callable to refit an estimator with the \"\n641                                  \"best parameter setting on the whole \"\n642                                  \"data and make the best_* attributes \"\n643                                  \"available for that metric. If this is \"\n644                                  \"not needed, refit should be set to \"\n645                                  \"False explicitly. %r was passed.\"\n646                                  % self.refit)\n647             else:\n648                 refit_metric = self.refit\n649         else:\n650             refit_metric = 'score'\n651 \n652         X, y, groups = indexable(X, y, groups)\n653         n_splits = cv.get_n_splits(X, y, groups)\n654 \n655         base_estimator = clone(self.estimator)\n656 \n657         parallel = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n658                             pre_dispatch=self.pre_dispatch)\n659 \n660         fit_and_score_kwargs = dict(scorer=scorers,\n661                                     fit_params=fit_params,\n662                                     return_train_score=self.return_train_score,\n663                                     return_n_test_samples=True,\n664                                     return_times=True,\n665                                     return_parameters=False,\n666                                     error_score=self.error_score,\n667                                     verbose=self.verbose)\n668         results = {}\n669         with parallel:\n670             all_candidate_params = []\n671             all_out = []\n672 \n673             def evaluate_candidates(candidate_params):\n674                 candidate_params = list(candidate_params)\n675                 n_candidates = len(candidate_params)\n676 \n677                 if self.verbose > 0:\n678                     print(\"Fitting {0} folds for each of {1} candidates,\"\n679                           \" totalling {2} fits\".format(\n680                               n_splits, n_candidates, n_candidates * n_splits))\n681 \n682                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n683                                                        X, y,\n684                                                        train=train, test=test,\n685                                                        parameters=parameters,\n686                                                        **fit_and_score_kwargs)\n687                                for parameters, (train, test)\n688                                in product(candidate_params,\n689                                           cv.split(X, y, groups)))\n690 \n691                 if len(out) < 1:\n692                     raise ValueError('No fits were performed. '\n693                                      'Was the CV iterator empty? '\n694                                      'Were there no candidates?')\n695                 elif len(out) != n_candidates * n_splits:\n696                     raise ValueError('cv.split and cv.get_n_splits returned '\n697                                      'inconsistent results. Expected {} '\n698                                      'splits, got {}'\n699                                      .format(n_splits,\n700                                              len(out) // n_candidates))\n701 \n702                 all_candidate_params.extend(candidate_params)\n703                 all_out.extend(out)\n704 \n705                 nonlocal results\n706                 results = self._format_results(\n707                     all_candidate_params, scorers, n_splits, all_out)\n708                 return results\n709 \n710             self._run_search(evaluate_candidates)\n711 \n712         # For multi-metric evaluation, store the best_index_, best_params_ and\n713         # best_score_ iff refit is one of the scorer names\n714         # In single metric evaluation, refit_metric is \"score\"\n715         if self.refit or not self.multimetric_:\n716             # If callable, refit is expected to return the index of the best\n717             # parameter set.\n718             if callable(self.refit):\n719                 self.best_index_ = self.refit(results)\n720                 if not isinstance(self.best_index_, numbers.Integral):\n721                     raise TypeError('best_index_ returned is not an integer')\n722                 if (self.best_index_ < 0 or\n723                    self.best_index_ >= len(results[\"params\"])):\n724                     raise IndexError('best_index_ index out of range')\n725             else:\n726                 self.best_index_ = results[\"rank_test_%s\"\n727                                            % refit_metric].argmin()\n728                 self.best_score_ = results[\"mean_test_%s\" % refit_metric][\n729                                            self.best_index_]\n730             self.best_params_ = results[\"params\"][self.best_index_]\n731 \n732         if self.refit:\n733             self.best_estimator_ = clone(base_estimator).set_params(\n734                 **self.best_params_)\n735             refit_start_time = time.time()\n736             if y is not None:\n737                 self.best_estimator_.fit(X, y, **fit_params)\n738             else:\n739                 self.best_estimator_.fit(X, **fit_params)\n740             refit_end_time = time.time()\n741             self.refit_time_ = refit_end_time - refit_start_time\n742 \n743         # Store the only scorer not as a dict for single metric evaluation\n744         self.scorer_ = scorers if self.multimetric_ else scorers['score']\n745 \n746         self.cv_results_ = results\n747         self.n_splits_ = n_splits\n748 \n749         return self\n",
        "intended_behavior": "The `fit` method should ensure that the parameter settings stored in `cv_results_['params']` and associated `cv_results_['param_*']` strictly contain the parameter values and do not inadvertently hold references to fitted estimator instances. This implies enhancing the mechanism that extracts parameters from the estimator for storage in `cv_results_` to ensure it decouples the parameter data from the actual estimator instances which might be stateful after fitting. A safe and deep copy of parameter values should be made when collecting the results for `cv_results_`, ensuring that only immutable, serializable data reflecting the configuration\u2014not the state\u2014of used estimators are stored. This ensures the serialized `cv_results_` is compact and does not unexpectedly increase in size due to storing complex objects like fitted estimators."
    },
    {
        "rel_file_path": "sklearn/model_selection/_search.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/model_selection/_search.py",
        "start": 605,
        "end": 749,
        "class_name": "BaseSearchCV",
        "method_name": "fit",
        "code": "605     def fit(self, X, y=None, groups=None, **fit_params):\n606         \"\"\"Run fit with all sets of parameters.\n607 \n608         Parameters\n609         ----------\n610 \n611         X : array-like, shape = [n_samples, n_features]\n612             Training vector, where n_samples is the number of samples and\n613             n_features is the number of features.\n614 \n615         y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n616             Target relative to X for classification or regression;\n617             None for unsupervised learning.\n618 \n619         groups : array-like, with shape (n_samples,), optional\n620             Group labels for the samples used while splitting the dataset into\n621             train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n622             instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).\n623 \n624         **fit_params : dict of string -> object\n625             Parameters passed to the ``fit`` method of the estimator\n626         \"\"\"\n627         estimator = self.estimator\n628         cv = check_cv(self.cv, y, classifier=is_classifier(estimator))\n629 \n630         scorers, self.multimetric_ = _check_multimetric_scoring(\n631             self.estimator, scoring=self.scoring)\n632 \n633         if self.multimetric_:\n634             if self.refit is not False and (\n635                     not isinstance(self.refit, str) or\n636                     # This will work for both dict / list (tuple)\n637                     self.refit not in scorers) and not callable(self.refit):\n638                 raise ValueError(\"For multi-metric scoring, the parameter \"\n639                                  \"refit must be set to a scorer key or a \"\n640                                  \"callable to refit an estimator with the \"\n641                                  \"best parameter setting on the whole \"\n642                                  \"data and make the best_* attributes \"\n643                                  \"available for that metric. If this is \"\n644                                  \"not needed, refit should be set to \"\n645                                  \"False explicitly. %r was passed.\"\n646                                  % self.refit)\n647             else:\n648                 refit_metric = self.refit\n649         else:\n650             refit_metric = 'score'\n651 \n652         X, y, groups = indexable(X, y, groups)\n653         n_splits = cv.get_n_splits(X, y, groups)\n654 \n655         base_estimator = clone(self.estimator)\n656 \n657         parallel = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n658                             pre_dispatch=self.pre_dispatch)\n659 \n660         fit_and_score_kwargs = dict(scorer=scorers,\n661                                     fit_params=fit_params,\n662                                     return_train_score=self.return_train_score,\n663                                     return_n_test_samples=True,\n664                                     return_times=True,\n665                                     return_parameters=False,\n666                                     error_score=self.error_score,\n667                                     verbose=self.verbose)\n668         results = {}\n669         with parallel:\n670             all_candidate_params = []\n671             all_out = []\n672 \n673             def evaluate_candidates(candidate_params):\n674                 candidate_params = list(candidate_params)\n675                 n_candidates = len(candidate_params)\n676 \n677                 if self.verbose > 0:\n678                     print(\"Fitting {0} folds for each of {1} candidates,\"\n679                           \" totalling {2} fits\".format(\n680                               n_splits, n_candidates, n_candidates * n_splits))\n681 \n682                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n683                                                        X, y,\n684                                                        train=train, test=test,\n685                                                        parameters=parameters,\n686                                                        **fit_and_score_kwargs)\n687                                for parameters, (train, test)\n688                                in product(candidate_params,\n689                                           cv.split(X, y, groups)))\n690 \n691                 if len(out) < 1:\n692                     raise ValueError('No fits were performed. '\n693                                      'Was the CV iterator empty? '\n694                                      'Were there no candidates?')\n695                 elif len(out) != n_candidates * n_splits:\n696                     raise ValueError('cv.split and cv.get_n_splits returned '\n697                                      'inconsistent results. Expected {} '\n698                                      'splits, got {}'\n699                                      .format(n_splits,\n700                                              len(out) // n_candidates))\n701 \n702                 all_candidate_params.extend(candidate_params)\n703                 all_out.extend(out)\n704 \n705                 nonlocal results\n706                 results = self._format_results(\n707                     all_candidate_params, scorers, n_splits, all_out)\n708                 return results\n709 \n710             self._run_search(evaluate_candidates)\n711 \n712         # For multi-metric evaluation, store the best_index_, best_params_ and\n713         # best_score_ iff refit is one of the scorer names\n714         # In single metric evaluation, refit_metric is \"score\"\n715         if self.refit or not self.multimetric_:\n716             # If callable, refit is expected to return the index of the best\n717             # parameter set.\n718             if callable(self.refit):\n719                 self.best_index_ = self.refit(results)\n720                 if not isinstance(self.best_index_, numbers.Integral):\n721                     raise TypeError('best_index_ returned is not an integer')\n722                 if (self.best_index_ < 0 or\n723                    self.best_index_ >= len(results[\"params\"])):\n724                     raise IndexError('best_index_ index out of range')\n725             else:\n726                 self.best_index_ = results[\"rank_test_%s\"\n727                                            % refit_metric].argmin()\n728                 self.best_score_ = results[\"mean_test_%s\" % refit_metric][\n729                                            self.best_index_]\n730             self.best_params_ = results[\"params\"][self.best_index_]\n731 \n732         if self.refit:\n733             self.best_estimator_ = clone(base_estimator).set_params(\n734                 **self.best_params_)\n735             refit_start_time = time.time()\n736             if y is not None:\n737                 self.best_estimator_.fit(X, y, **fit_params)\n738             else:\n739                 self.best_estimator_.fit(X, **fit_params)\n740             refit_end_time = time.time()\n741             self.refit_time_ = refit_end_time - refit_start_time\n742 \n743         # Store the only scorer not as a dict for single metric evaluation\n744         self.scorer_ = scorers if self.multimetric_ else scorers['score']\n745 \n746         self.cv_results_ = results\n747         self.n_splits_ = n_splits\n748 \n749         return self\n",
        "intended_behavior": "The `fit` method should ensure that the parameter settings stored in `cv_results_['params']` and associated `cv_results_['param_*']` strictly contain the parameter values and do not inadvertently hold references to fitted estimator instances. This implies enhancing the mechanism that extracts parameters from the estimator for storage in `cv_results_` to ensure it decouples the parameter data from the actual estimator instances which might be stateful after fitting. A safe and deep copy of parameter values should be made when collecting the results for `cv_results_`, ensuring that only immutable, serializable data reflecting the configuration\u2014not the state\u2014of used estimators are stored. This ensures the serialized `cv_results_` is compact and does not unexpectedly increase in size due to storing complex objects like fitted estimators."
    }
]