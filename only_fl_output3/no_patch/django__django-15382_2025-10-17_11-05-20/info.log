2025-10-17 11:05:20.413 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-17 11:05:24.116 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=178, output_tokens=12, cost=0.002140
2025-10-17 11:05:33.953 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=358, output_tokens=417, cost=0.016090
2025-10-17 11:05:35.126 | INFO     | registered non reproducing test 0
2025-10-17 11:05:53.826 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1486, output_tokens=743, cost=0.037150
2025-10-17 11:05:54.945 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 11:06:01.444 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=483, output_tokens=170, cost=0.009930
2025-10-17 11:06:01.450 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 11:06:05.139 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=524, output_tokens=78, cost=0.007580
2025-10-17 11:06:05.140 | DEBUG    | Extracted a valid json.
2025-10-17 11:06:05.141 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"QuerySet\")",
        "search_class(\"Exists\")",
        "search_method(\"filter\")",
        "search_code(\"MyModel.objects.filter(~models.Exists(MyModel.objects.none()), name='test')\")",
        "search_method_in_class(\"as_sql\", \"Exists\")"
    ],
    "bug_locations": []
}
2025-10-17 11:06:05.224 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-17 11:06:05.225 | DEBUG    | <Agent search> Analyzing search results.
2025-10-17 11:06:25.412 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2374, output_tokens=855, cost=0.049390
2025-10-17 11:06:25.428 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 11:06:34.943 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3445, output_tokens=389, cost=0.046120
2025-10-17 11:06:34.951 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 11:06:46.140 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=743, output_tokens=397, cost=0.019340
2025-10-17 11:06:46.141 | DEBUG    | Extracted a valid json.
2025-10-17 11:06:46.141 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "django/db/models/query.py",
            "class": "QuerySet",
            "method": "filter",
            "intended_behavior": "The `filter` method (or `_filter_or_exclude` if it’s more directly involved in constructing the query) should ensure that when constructing a query with an `Exists` subquery, even if the queryset inside `Exists` is empty, the WHERE clause in the generated SQL is not omitted entirely. Special handling may need to be added to recognize when an `Exists` subquery will produce no results and to ensure that the resulting SQL properly reflects a negation or presence check without discarding the WHERE clause entirely. This preservation should occur even when the queryset is explicitly empty, as in the case of `MyModel.objects.none()`, ensuring that the logical negation (`~`) or presence indicated by `Exists` in Django ORM queries is correctly translated to SQL."
        },
        {
            "file": "django/db/models/expressions.py",
            "class": "Exists",
            "method": "as_sql",
            "intended_behavior": "The `as_sql` method within the `Exists` class needs to ensure that it handles the scenario of an empty queryset in a way that accurately reflects the intended logical operation in the generated SQL. When `Exists` is constructed with an empty queryset, especially when negated, this method should generate SQL that properly captures the 'not exists' condition without resulting in the omission of crucial parts of the SQL statement. This might involve adjusting the SQL generation logic to explicitly handle empty querysets in a way that still allows for meaningful WHERE conditions to be generated, rather than leading to an `EmptyResultSet` scenario that removes the WHERE block. This adjustment should ensure that the SQL output remains logical and functional even when faced with negated empty `Exists` subqueries."
        }
    ]
}
2025-10-17 11:06:46.154 | DEBUG    | Found override of as_sql in Subquery
2025-10-17 11:06:46.155 | ERROR    | SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
Traceback (most recent call last):

  File "/home/kimnal0/auto-code-rover/app/main.py", line 656, in <module>
    main()
    └ <function main at 0x7ea0c4f22c00>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 120, in main
    run_task_groups(groups, num_processes, organize_output=True)
    │               │       └ 1
    │               └ {'setup_astropy__astropy__4.3': [<app.raw_tasks.RawSweTask object at 0x7e9fd295fb00>, <app.raw_tasks.RawSweTask object at 0x7...
    └ <function run_task_groups at 0x7e9fd4b7b240>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 396, in run_task_groups
    run_tasks_serial(all_tasks)
    │                └ [<app.raw_tasks.RawSweTask object at 0x7e9fd295fb00>, <app.raw_tasks.RawSweTask object at 0x7e9fd295fb30>, <app.raw_tasks.Raw...
    └ <function run_tasks_serial at 0x7e9fd4b7b2e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 414, in run_tasks_serial
    run_task_in_subprocess(task)
    │                      └ <app.raw_tasks.RawSweTask object at 0x7e9fd26e34a0>
    └ <function run_task_in_subprocess at 0x7e9fd4b7b4c0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 462, in run_task_in_subprocess
    executor.submit(run_raw_task, task)
    │        │      │             └ <app.raw_tasks.RawSweTask object at 0x7e9fd26e34a0>
    │        │      └ <function run_raw_task at 0x7e9fd4b7b560>
    │        └ <function ProcessPoolExecutor.submit at 0x7ea0c4842020>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7e9fd5a2e120>

  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 831, in submit
    self._start_executor_manager_thread()
    │    └ <function ProcessPoolExecutor._start_executor_manager_thread at 0x7ea0c4841da0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7e9fd5a2e120>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 770, in _start_executor_manager_thread
    self._launch_processes()
    │    └ <function ProcessPoolExecutor._launch_processes at 0x7ea0c4841ee0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7e9fd5a2e120>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 797, in _launch_processes
    self._spawn_process()
    │    └ <function ProcessPoolExecutor._spawn_process at 0x7ea0c4841f80>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7e9fd5a2e120>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 807, in _spawn_process
    p.start()
    │ └ <function BaseProcess.start at 0x7ea0c4a04ea0>
    └ <ForkProcess name='ForkProcess-71' parent=307479 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
    │    │        │    │      └ <ForkProcess name='ForkProcess-71' parent=307479 started>
    │    │        │    └ <staticmethod(<function ForkProcess._Popen at 0x7ea0c4a760c0>)>
    │    │        └ <ForkProcess name='ForkProcess-71' parent=307479 started>
    │    └ None
    └ <ForkProcess name='ForkProcess-71' parent=307479 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/context.py", line 282, in _Popen
    return Popen(process_obj)
           │     └ <ForkProcess name='ForkProcess-71' parent=307479 started>
           └ <class 'multiprocessing.popen_fork.Popen'>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
    │    │       └ <ForkProcess name='ForkProcess-71' parent=307479 started>
    │    └ <function Popen._launch at 0x7e9fd2ad4ea0>
    └ <multiprocessing.popen_fork.Popen object at 0x7e9fd314ed50>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
           │           │                          └ 13
           │           └ <function BaseProcess._bootstrap at 0x7ea0c4a058a0>
           └ <ForkProcess name='ForkProcess-71' parent=307479 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x7ea0c4a04e00>
    └ <ForkProcess name='ForkProcess-71' parent=307479 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {}
    │    │        │    │        └ <ForkProcess name='ForkProcess-71' parent=307479 started>
    │    │        │    └ (<concurrent.futures.process._SafeQueue object at 0x7e9fd2b91310>, <multiprocessing.queues.SimpleQueue object at 0x7e9fd38674...
    │    │        └ <ForkProcess name='ForkProcess-71' parent=307479 started>
    │    └ <function _process_worker at 0x7ea0c48411c0>
    └ <ForkProcess name='ForkProcess-71' parent=307479 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 263, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
        │         │   │         │       │         └ {}
        │         │   │         │       └ <concurrent.futures.process._CallItem object at 0x7e9fd4fdd670>
        │         │   │         └ (<app.raw_tasks.RawSweTask object at 0x7e9fd314c650>,)
        │         │   └ <concurrent.futures.process._CallItem object at 0x7e9fd4fdd670>
        │         └ <function run_raw_task at 0x7e9fd4b7b560>
        └ <concurrent.futures.process._CallItem object at 0x7e9fd4fdd670>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 494, in run_raw_task
    run_ok = do_inference(task.to_task(), task_output_dir)
             │            │    │          └ '/home/kimnal0/auto-code-rover/only_fl_output3/django__django-15382_2025-10-17_11-05-20'
             │            │    └ <function RawSweTask.to_task at 0x7e9fd4b7a520>
             │            └ <app.raw_tasks.RawSweTask object at 0x7e9fd314c650>
             └ <function do_inference at 0x7e9fd4b7b740>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 590, in do_inference
    run_ok = inference.run_one_task(
             │         └ <function run_one_task at 0x7e9fd4bd5080>
             └ <module 'app.inference' from '/home/kimnal0/auto-code-rover/app/inference.py'>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 128, in run_one_task
    if _run_one_task(str(out_dir), api_manager, task.get_issue_statement()):
       │                 │         │            │    └ <function SweTask.get_issue_statement at 0x7e9fd5360ae0>
       │                 │         │            └ SweTask(task_id='django__django-15382', problem_statement="filter on exists-subquery with empty queryset removes whole WHERE ...
       │                 │         └ <app.manage.ProjectApiManager object at 0x7e9fd314fd10>
       │                 └ Path('/home/kimnal0/auto-code-rover/only_fl_output3/django__django-15382_2025-10-17_11-05-20/output_0')
       └ <function _run_one_task at 0x7e9fd4bd6660>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 303, in _run_one_task
    bug_locs, search_msg_thread = api_manager.search_manager.search_iterative(
                                  │           │              └ <function SearchManager.search_iterative at 0x7e9fd4fea840>
                                  │           └ <app.search.search_manage.SearchManager object at 0x7e9fd314ff80>
                                  └ <app.manage.ProjectApiManager object at 0x7e9fd314fd10>

  File "/home/kimnal0/auto-code-rover/app/search/search_manage.py", line 125, in search_iterative
    new_bug_locations.extend(self.backend.get_bug_loc_snippets_new(loc))
    │                 │      │    │       │                        └ {'file': 'django/db/models/expressions.py', 'class': 'Exists', 'method': 'as_sql', 'intended_behavior': "The `as_sql` method ...
    │                 │      │    │       └ <function SearchBackend.get_bug_loc_snippets_new at 0x7e9fd4fea7a0>
    │                 │      │    └ <app.search.search_backend.SearchBackend object at 0x7e9fd532bf50>
    │                 │      └ <app.search.search_manage.SearchManager object at 0x7e9fd314ff80>
    │                 └ <method 'extend' of 'list' objects>
    └ [<file>django/db/models/query.py</file>
      <class>QuerySet</class>
      <method>filter</method>
      <code>
      1078     def filter(self, *arg...

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 825, in get_bug_loc_snippets_new
    self._get_inherited_methods(res.class_name, res.func_name)
    │    │                      │   │           │   └ 'as_sql'
    │    │                      │   │           └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__4.1/django/db/mo...
    │    │                      │   └ 'Exists'
    │    │                      └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__4.1/django/db/mo...
    │    └ <function SearchBackend._get_inherited_methods at 0x7e9fd4fea700>
    └ <app.search.search_backend.SearchBackend object at 0x7e9fd532bf50>

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 748, in _get_inherited_methods
    output, search_res, call_ok = self.search_method_in_class(super_call)
                                  │    │                      └ {'class_name': 'Subquery', 'method_name': 'as_sql'}
                                  │    └ <function SearchBackend.search_method_in_class at 0x7e9fd4fe9e40>
                                  └ <app.search.search_backend.SearchBackend object at 0x7e9fd532bf50>

> File "/home/kimnal0/auto-code-rover/app/utils.py", line 343, in wrapper
    return func(*args, **kwargs)
           │     │       └ {}
           │     └ (<app.search.search_backend.SearchBackend object at 0x7e9fd532bf50>, {'class_name': 'Subquery', 'method_name': 'as_sql'})
           └ <function SearchBackend.search_method_in_class at 0x7e9fd4fe9da0>

TypeError: SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
2025-10-17 11:06:46.161 | DEBUG    | Bug location extracted successfully: [<file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>filter</method>
<code>
1078     def filter(self, *args, **kwargs):
1079         """
1080         Return a new QuerySet instance with the args ANDed to the existing
1081         set.
1082         """
1083         self._not_support_combined_queries('filter')
1084         return self._filter_or_exclude(False, args, kwargs)

</code><intended_behavior>The `filter` method (or `_filter_or_exclude` if it’s more directly involved in constructing the query) should ensure that when constructing a query with an `Exists` subquery, even if the queryset inside `Exists` is empty, the WHERE clause in the generated SQL is not omitted entirely. Special handling may need to be added to recognize when an `Exists` subquery will produce no results and to ensure that the resulting SQL properly reflects a negation or presence check without discarding the WHERE clause entirely. This preservation should occur even when the queryset is explicitly empty, as in the case of `MyModel.objects.none()`, ensuring that the logical negation (`~`) or presence indicated by `Exists` in Django ORM queries is correctly translated to SQL.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
176 class QuerySet:
177     """Represent a lazy database lookup for a set of objects."""
178 
179     def __init__(self, model=None, query=None, using=None, hints=None):
180         self.model = model
181         self._db = using
182         self._hints = hints or {}
183         self._query = query or sql.Query(self.model)
184         self._result_cache = None
185         self._sticky_filter = False
186         self._for_write = False
187         self._prefetch_related_lookups = ()
188         self._prefetch_done = False
189         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
190         self._iterable_class = ModelIterable
191         self._fields = None
192         self._defer_next_filter = False
193         self._deferred_filter = None
194 
195     @property
196     def query(self):
197         if self._deferred_filter:
198             negate, args, kwargs = self._deferred_filter
199             self._filter_or_exclude_inplace(negate, args, kwargs)
200             self._deferred_filter = None
201         return self._query
202 
203     @query.setter
204     def query(self, value):
205         if value.values_select:
206             self._iterable_class = ValuesIterable
207         self._query = value
208 
209     def as_manager(cls):
210         # Address the circular dependency between `Queryset` and `Manager`.
211         from django.db.models.manager import Manager
212         manager = Manager.from_queryset(cls)()
213         manager._built_with_as_manager = True
214         return manager
215     as_manager.queryset_only = True
216     as_manager = classmethod(as_manager)
217 
218     ########################
219     # PYTHON MAGIC METHODS #
220     ########################
221 
222     def __deepcopy__(self, memo):
223         """Don't populate the QuerySet's cache."""
224         obj = self.__class__()
225         for k, v in self.__dict__.items():
226             if k == '_result_cache':
227                 obj.__dict__[k] = None
228             else:
229                 obj.__dict__[k] = copy.deepcopy(v, memo)
230         return obj
231 
232     def __getstate__(self):
233         # Force the cache to be fully populated.
234         self._fetch_all()
235         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
236 
237     def __setstate__(self, state):
238         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
239         if pickled_version:
240             if pickled_version != django.__version__:
241                 warnings.warn(
242                     "Pickled queryset instance's Django version %s does not "
243                     "match the current version %s."
244                     % (pickled_version, django.__version__),
245                     RuntimeWarning,
246                     stacklevel=2,
247                 )
248         else:
249             warnings.warn(
250                 "Pickled queryset instance's Django version is not specified.",
251                 RuntimeWarning,
252                 stacklevel=2,
253             )
254         self.__dict__.update(state)
255 
256     def __repr__(self):
257         data = list(self[:REPR_OUTPUT_SIZE + 1])
258         if len(data) > REPR_OUTPUT_SIZE:
259             data[-1] = "...(remaining elements truncated)..."
260         return '<%s %r>' % (self.__class__.__name__, data)
261 
262     def __len__(self):
263         self._fetch_all()
264         return len(self._result_cache)
265 
266     def __iter__(self):
267         """
268         The queryset iterator protocol uses three nested iterators in the
269         default case:
270             1. sql.compiler.execute_sql()
271                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
272                  using cursor.fetchmany(). This part is responsible for
273                  doing some column masking, and returning the rows in chunks.
274             2. sql.compiler.results_iter()
275                - Returns one row at time. At this point the rows are still just
276                  tuples. In some cases the return values are converted to
277                  Python values at this location.
278             3. self.iterator()
279                - Responsible for turning the rows into model objects.
280         """
281         self._fetch_all()
282         return iter(self._result_cache)
283 
284     def __bool__(self):
285         self._fetch_all()
286         return bool(self._result_cache)
287 
288     def __getitem__(self, k):
289         """Retrieve an item or slice from the set of results."""
290         if not isinstance(k, (int, slice)):
291             raise TypeError(
292                 'QuerySet indices must be integers or slices, not %s.'
293                 % type(k).__name__
294             )
295         if (
296             (isinstance(k, int) and k < 0) or
297             (isinstance(k, slice) and (
298                 (k.start is not None and k.start < 0) or
299                 (k.stop is not None and k.stop < 0)
300             ))
301         ):
302             raise ValueError('Negative indexing is not supported.')
303 
304         if self._result_cache is not None:
305             return self._result_cache[k]
306 
307         if isinstance(k, slice):
308             qs = self._chain()
309             if k.start is not None:
310                 start = int(k.start)
311             else:
312                 start = None
313             if k.stop is not None:
314                 stop = int(k.stop)
315             else:
316                 stop = None
317             qs.query.set_limits(start, stop)
318             return list(qs)[::k.step] if k.step else qs
319 
320         qs = self._chain()
321         qs.query.set_limits(k, k + 1)
322         qs._fetch_all()
323         return qs._result_cache[0]
324 
325     def __class_getitem__(cls, *args, **kwargs):
326         return cls
327 
328     def __and__(self, other):
329         self._check_operator_queryset(other, '&')
330         self._merge_sanity_check(other)
331         if isinstance(other, EmptyQuerySet):
332             return other
333         if isinstance(self, EmptyQuerySet):
334             return self
335         combined = self._chain()
336         combined._merge_known_related_objects(other)
337         combined.query.combine(other.query, sql.AND)
338         return combined
339 
340     def __or__(self, other):
341         self._check_operator_queryset(other, '|')
342         self._merge_sanity_check(other)
343         if isinstance(self, EmptyQuerySet):
344             return other
345         if isinstance(other, EmptyQuerySet):
346             return self
347         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
348         combined = query._chain()
349         combined._merge_known_related_objects(other)
350         if not other.query.can_filter():
351             other = other.model._base_manager.filter(pk__in=other.values('pk'))
352         combined.query.combine(other.query, sql.OR)
353         return combined
354 
355     ####################################
356     # METHODS THAT DO DATABASE QUERIES #
357     ####################################
358 
359     def _iterator(self, use_chunked_fetch, chunk_size):
360         iterable = self._iterable_class(
361             self,
362             chunked_fetch=use_chunked_fetch,
363             chunk_size=chunk_size or 2000,
364         )
365         if not self._prefetch_related_lookups or chunk_size is None:
366             yield from iterable
367             return
368 
369         iterator = iter(iterable)
370         while results := list(islice(iterator, chunk_size)):
371             prefetch_related_objects(results, *self._prefetch_related_lookups)
372             yield from results
373 
374     def iterator(self, chunk_size=None):
375         """
376         An iterator over the results from applying this QuerySet to the
377         database. chunk_size must be provided for QuerySets that prefetch
378         related objects. Otherwise, a default chunk_size of 2000 is supplied.
379         """
380         if chunk_size is None:
381             if self._prefetch_related_lookups:
382                 # When the deprecation ends, replace with:
383                 # raise ValueError(
384                 #     'chunk_size must be provided when using '
385                 #     'QuerySet.iterator() after prefetch_related().'
386                 # )
387                 warnings.warn(
388                     'Using QuerySet.iterator() after prefetch_related() '
389                     'without specifying chunk_size is deprecated.',
390                     category=RemovedInDjango50Warning,
391                     stacklevel=2,
392                 )
393         elif chunk_size <= 0:
394             raise ValueError('Chunk size must be strictly positive.')
395         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
396         return self._iterator(use_chunked_fetch, chunk_size)
397 
398     def aggregate(self, *args, **kwargs):
399         """
400         Return a dictionary containing the calculations (aggregation)
401         over the current queryset.
402 
403         If args is present the expression is passed as a kwarg using
404         the Aggregate object's default alias.
405         """
406         if self.query.distinct_fields:
407             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
408         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
409         for arg in args:
410             # The default_alias property raises TypeError if default_alias
411             # can't be set automatically or AttributeError if it isn't an
412             # attribute.
413             try:
414                 arg.default_alias
415             except (AttributeError, TypeError):
416                 raise TypeError("Complex aggregates require an alias")
417             kwargs[arg.default_alias] = arg
418 
419         query = self.query.chain()
420         for (alias, aggregate_expr) in kwargs.items():
421             query.add_annotation(aggregate_expr, alias, is_summary=True)
422             annotation = query.annotations[alias]
423             if not annotation.contains_aggregate:
424                 raise TypeError("%s is not an aggregate expression" % alias)
425             for expr in annotation.get_source_expressions():
426                 if expr.contains_aggregate and isinstance(expr, Ref) and expr.refs in kwargs:
427                     name = expr.refs
428                     raise exceptions.FieldError(
429                         "Cannot compute %s('%s'): '%s' is an aggregate"
430                         % (annotation.name, name, name)
431                     )
432         return query.get_aggregation(self.db, kwargs)
433 
434     def count(self):
435         """
436         Perform a SELECT COUNT() and return the number of records as an
437         integer.
438 
439         If the QuerySet is already fully cached, return the length of the
440         cached results set to avoid multiple SELECT COUNT(*) calls.
441         """
442         if self._result_cache is not None:
443             return len(self._result_cache)
444 
445         return self.query.get_count(using=self.db)
446 
447     def get(self, *args, **kwargs):
448         """
449         Perform the query and return a single object matching the given
450         keyword arguments.
451         """
452         if self.query.combinator and (args or kwargs):
453             raise NotSupportedError(
454                 'Calling QuerySet.get(...) with filters after %s() is not '
455                 'supported.' % self.query.combinator
456             )
457         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
458         if self.query.can_filter() and not self.query.distinct_fields:
459             clone = clone.order_by()
460         limit = None
461         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
462             limit = MAX_GET_RESULTS
463             clone.query.set_limits(high=limit)
464         num = len(clone)
465         if num == 1:
466             return clone._result_cache[0]
467         if not num:
468             raise self.model.DoesNotExist(
469                 "%s matching query does not exist." %
470                 self.model._meta.object_name
471             )
472         raise self.model.MultipleObjectsReturned(
473             'get() returned more than one %s -- it returned %s!' % (
474                 self.model._meta.object_name,
475                 num if not limit or num < limit else 'more than %s' % (limit - 1),
476             )
477         )
478 
479     def create(self, **kwargs):
480         """
481         Create a new object with the given kwargs, saving it to the database
482         and returning the created object.
483         """
484         obj = self.model(**kwargs)
485         self._for_write = True
486         obj.save(force_insert=True, using=self.db)
487         return obj
488 
489     def _prepare_for_bulk_create(self, objs):
490         for obj in objs:
491             if obj.pk is None:
492                 # Populate new PK values.
493                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
494             obj._prepare_related_fields_for_save(operation_name='bulk_create')
495 
496     def _check_bulk_create_options(self, ignore_conflicts, update_conflicts, update_fields, unique_fields):
497         if ignore_conflicts and update_conflicts:
498             raise ValueError(
499                 'ignore_conflicts and update_conflicts are mutually exclusive.'
500             )
501         db_features = connections[self.db].features
502         if ignore_conflicts:
503             if not db_features.supports_ignore_conflicts:
504                 raise NotSupportedError(
505                     'This database backend does not support ignoring conflicts.'
506                 )
507             return OnConflict.IGNORE
508         elif update_conflicts:
509             if not db_features.supports_update_conflicts:
510                 raise NotSupportedError(
511                     'This database backend does not support updating conflicts.'
512                 )
513             if not update_fields:
514                 raise ValueError(
515                     'Fields that will be updated when a row insertion fails '
516                     'on conflicts must be provided.'
517                 )
518             if unique_fields and not db_features.supports_update_conflicts_with_target:
519                 raise NotSupportedError(
520                     'This database backend does not support updating '
521                     'conflicts with specifying unique fields that can trigger '
522                     'the upsert.'
523                 )
524             if not unique_fields and db_features.supports_update_conflicts_with_target:
525                 raise ValueError(
526                     'Unique fields that can trigger the upsert must be '
527                     'provided.'
528                 )
529             # Updating primary keys and non-concrete fields is forbidden.
530             update_fields = [self.model._meta.get_field(name) for name in update_fields]
531             if any(not f.concrete or f.many_to_many for f in update_fields):
532                 raise ValueError(
533                     'bulk_create() can only be used with concrete fields in '
534                     'update_fields.'
535                 )
536             if any(f.primary_key for f in update_fields):
537                 raise ValueError(
538                     'bulk_create() cannot be used with primary keys in '
539                     'update_fields.'
540                 )
541             if unique_fields:
542                 # Primary key is allowed in unique_fields.
543                 unique_fields = [
544                     self.model._meta.get_field(name)
545                     for name in unique_fields if name != 'pk'
546                 ]
547                 if any(not f.concrete or f.many_to_many for f in unique_fields):
548                     raise ValueError(
549                         'bulk_create() can only be used with concrete fields '
550                         'in unique_fields.'
551                     )
552             return OnConflict.UPDATE
553         return None
554 
555     def bulk_create(
556         self, objs, batch_size=None, ignore_conflicts=False,
557         update_conflicts=False, update_fields=None, unique_fields=None,
558     ):
559         """
560         Insert each of the instances into the database. Do *not* call
561         save() on each of the instances, do not send any pre/post_save
562         signals, and do not set the primary key attribute if it is an
563         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
564         Multi-table models are not supported.
565         """
566         # When you bulk insert you don't get the primary keys back (if it's an
567         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
568         # you can't insert into the child tables which references this. There
569         # are two workarounds:
570         # 1) This could be implemented if you didn't have an autoincrement pk
571         # 2) You could do it by doing O(n) normal inserts into the parent
572         #    tables to get the primary keys back and then doing a single bulk
573         #    insert into the childmost table.
574         # We currently set the primary keys on the objects when using
575         # PostgreSQL via the RETURNING ID clause. It should be possible for
576         # Oracle as well, but the semantics for extracting the primary keys is
577         # trickier so it's not done yet.
578         if batch_size is not None and batch_size <= 0:
579             raise ValueError('Batch size must be a positive integer.')
580         # Check that the parents share the same concrete model with the our
581         # model to detect the inheritance pattern ConcreteGrandParent ->
582         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
583         # would not identify that case as involving multiple tables.
584         for parent in self.model._meta.get_parent_list():
585             if parent._meta.concrete_model is not self.model._meta.concrete_model:
586                 raise ValueError("Can't bulk create a multi-table inherited model")
587         if not objs:
588             return objs
589         on_conflict = self._check_bulk_create_options(
590             ignore_conflicts,
591             update_conflicts,
592             update_fields,
593             unique_fields,
594         )
595         self._for_write = True
596         opts = self.model._meta
597         fields = opts.concrete_fields
598         objs = list(objs)
599         self._prepare_for_bulk_create(objs)
600         with transaction.atomic(using=self.db, savepoint=False):
601             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
602             if objs_with_pk:
603                 returned_columns = self._batched_insert(
604                     objs_with_pk,
605                     fields,
606                     batch_size,
607                     on_conflict=on_conflict,
608                     update_fields=update_fields,
609                     unique_fields=unique_fields,
610                 )
611                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
612                     for result, field in zip(results, opts.db_returning_fields):
613                         if field != opts.pk:
614                             setattr(obj_with_pk, field.attname, result)
615                 for obj_with_pk in objs_with_pk:
616                     obj_with_pk._state.adding = False
617                     obj_with_pk._state.db = self.db
618             if objs_without_pk:
619                 fields = [f for f in fields if not isinstance(f, AutoField)]
620                 returned_columns = self._batched_insert(
621                     objs_without_pk,
622                     fields,
623                     batch_size,
624                     on_conflict=on_conflict,
625                     update_fields=update_fields,
626                     unique_fields=unique_fields,
627                 )
628                 connection = connections[self.db]
629                 if connection.features.can_return_rows_from_bulk_insert and on_conflict is None:
630                     assert len(returned_columns) == len(objs_without_pk)
631                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
632                     for result, field in zip(results, opts.db_returning_fields):
633                         setattr(obj_without_pk, field.attname, result)
634                     obj_without_pk._state.adding = False
635                     obj_without_pk._state.db = self.db
636 
637         return objs
638 
639     def bulk_update(self, objs, fields, batch_size=None):
640         """
641         Update the given fields in each of the given objects in the database.
642         """
643         if batch_size is not None and batch_size < 0:
644             raise ValueError('Batch size must be a positive integer.')
645         if not fields:
646             raise ValueError('Field names must be given to bulk_update().')
647         objs = tuple(objs)
648         if any(obj.pk is None for obj in objs):
649             raise ValueError('All bulk_update() objects must have a primary key set.')
650         fields = [self.model._meta.get_field(name) for name in fields]
651         if any(not f.concrete or f.many_to_many for f in fields):
652             raise ValueError('bulk_update() can only be used with concrete fields.')
653         if any(f.primary_key for f in fields):
654             raise ValueError('bulk_update() cannot be used with primary key fields.')
655         if not objs:
656             return 0
657         for obj in objs:
658             obj._prepare_related_fields_for_save(operation_name='bulk_update', fields=fields)
659         # PK is used twice in the resulting update query, once in the filter
660         # and once in the WHEN. Each field will also have one CAST.
661         connection = connections[self.db]
662         max_batch_size = connection.ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
663         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
664         requires_casting = connection.features.requires_casted_case_in_updates
665         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
666         updates = []
667         for batch_objs in batches:
668             update_kwargs = {}
669             for field in fields:
670                 when_statements = []
671                 for obj in batch_objs:
672                     attr = getattr(obj, field.attname)
673                     if not hasattr(attr, 'resolve_expression'):
674                         attr = Value(attr, output_field=field)
675                     when_statements.append(When(pk=obj.pk, then=attr))
676                 case_statement = Case(*when_statements, output_field=field)
677                 if requires_casting:
678                     case_statement = Cast(case_statement, output_field=field)
679                 update_kwargs[field.attname] = case_statement
680             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
681         rows_updated = 0
682         with transaction.atomic(using=self.db, savepoint=False):
683             for pks, update_kwargs in updates:
684                 rows_updated += self.filter(pk__in=pks).update(**update_kwargs)
685         return rows_updated
686     bulk_update.alters_data = True
687 
688     def get_or_create(self, defaults=None, **kwargs):
689         """
690         Look up an object with the given kwargs, creating one if necessary.
691         Return a tuple of (object, created), where created is a boolean
692         specifying whether an object was created.
693         """
694         # The get() needs to be targeted at the write database in order
695         # to avoid potential transaction consistency problems.
696         self._for_write = True
697         try:
698             return self.get(**kwargs), False
699         except self.model.DoesNotExist:
700             params = self._extract_model_params(defaults, **kwargs)
701             # Try to create an object using passed params.
702             try:
703                 with transaction.atomic(using=self.db):
704                     params = dict(resolve_callables(params))
705                     return self.create(**params), True
706             except IntegrityError:
707                 try:
708                     return self.get(**kwargs), False
709                 except self.model.DoesNotExist:
710                     pass
711                 raise
712 
713     def update_or_create(self, defaults=None, **kwargs):
714         """
715         Look up an object with the given kwargs, updating one with defaults
716         if it exists, otherwise create a new one.
717         Return a tuple (object, created), where created is a boolean
718         specifying whether an object was created.
719         """
720         defaults = defaults or {}
721         self._for_write = True
722         with transaction.atomic(using=self.db):
723             # Lock the row so that a concurrent update is blocked until
724             # update_or_create() has performed its save.
725             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)
726             if created:
727                 return obj, created
728             for k, v in resolve_callables(defaults):
729                 setattr(obj, k, v)
730             obj.save(using=self.db)
731         return obj, False
732 
733     def _extract_model_params(self, defaults, **kwargs):
734         """
735         Prepare `params` for creating a model instance based on the given
736         kwargs; for use by get_or_create().
737         """
738         defaults = defaults or {}
739         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
740         params.update(defaults)
741         property_names = self.model._meta._property_names
742         invalid_params = []
743         for param in params:
744             try:
745                 self.model._meta.get_field(param)
746             except exceptions.FieldDoesNotExist:
747                 # It's okay to use a model's property if it has a setter.
748                 if not (param in property_names and getattr(self.model, param).fset):
749                     invalid_params.append(param)
750         if invalid_params:
751             raise exceptions.FieldError(
752                 "Invalid field name(s) for model %s: '%s'." % (
753                     self.model._meta.object_name,
754                     "', '".join(sorted(invalid_params)),
755                 ))
756         return params
757 
758     def _earliest(self, *fields):
759         """
760         Return the earliest object according to fields (if given) or by the
761         model's Meta.get_latest_by.
762         """
763         if fields:
764             order_by = fields
765         else:
766             order_by = getattr(self.model._meta, 'get_latest_by')
767             if order_by and not isinstance(order_by, (tuple, list)):
768                 order_by = (order_by,)
769         if order_by is None:
770             raise ValueError(
771                 "earliest() and latest() require either fields as positional "
772                 "arguments or 'get_latest_by' in the model's Meta."
773             )
774         obj = self._chain()
775         obj.query.set_limits(high=1)
776         obj.query.clear_ordering(force=True)
777         obj.query.add_ordering(*order_by)
778         return obj.get()
779 
780     def earliest(self, *fields):
781         if self.query.is_sliced:
782             raise TypeError('Cannot change a query once a slice has been taken.')
783         return self._earliest(*fields)
784 
785     def latest(self, *fields):
786         if self.query.is_sliced:
787             raise TypeError('Cannot change a query once a slice has been taken.')
788         return self.reverse()._earliest(*fields)
789 
790     def first(self):
791         """Return the first object of a query or None if no match is found."""
792         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
793             return obj
794 
795     def last(self):
796         """Return the last object of a query or None if no match is found."""
797         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
798             return obj
799 
800     def in_bulk(self, id_list=None, *, field_name='pk'):
801         """
802         Return a dictionary mapping each of the given IDs to the object with
803         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
804         """
805         if self.query.is_sliced:
806             raise TypeError("Cannot use 'limit' or 'offset' with in_bulk().")
807         opts = self.model._meta
808         unique_fields = [
809             constraint.fields[0]
810             for constraint in opts.total_unique_constraints
811             if len(constraint.fields) == 1
812         ]
813         if (
814             field_name != 'pk' and
815             not opts.get_field(field_name).unique and
816             field_name not in unique_fields and
817             self.query.distinct_fields != (field_name,)
818         ):
819             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
820         if id_list is not None:
821             if not id_list:
822                 return {}
823             filter_key = '{}__in'.format(field_name)
824             batch_size = connections[self.db].features.max_query_params
825             id_list = tuple(id_list)
826             # If the database has a limit on the number of query parameters
827             # (e.g. SQLite), retrieve objects in batches if necessary.
828             if batch_size and batch_size < len(id_list):
829                 qs = ()
830                 for offset in range(0, len(id_list), batch_size):
831                     batch = id_list[offset:offset + batch_size]
832                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
833             else:
834                 qs = self.filter(**{filter_key: id_list}).order_by()
835         else:
836             qs = self._chain()
837         return {getattr(obj, field_name): obj for obj in qs}
838 
839     def delete(self):
840         """Delete the records in the current QuerySet."""
841         self._not_support_combined_queries('delete')
842         if self.query.is_sliced:
843             raise TypeError("Cannot use 'limit' or 'offset' with delete().")
844         if self.query.distinct or self.query.distinct_fields:
845             raise TypeError('Cannot call delete() after .distinct().')
846         if self._fields is not None:
847             raise TypeError("Cannot call delete() after .values() or .values_list()")
848 
849         del_query = self._chain()
850 
851         # The delete is actually 2 queries - one to find related objects,
852         # and one to delete. Make sure that the discovery of related
853         # objects is performed on the same database as the deletion.
854         del_query._for_write = True
855 
856         # Disable non-supported fields.
857         del_query.query.select_for_update = False
858         del_query.query.select_related = False
859         del_query.query.clear_ordering(force=True)
860 
861         collector = Collector(using=del_query.db, origin=self)
862         collector.collect(del_query)
863         deleted, _rows_count = collector.delete()
864 
865         # Clear the result cache, in case this QuerySet gets reused.
866         self._result_cache = None
867         return deleted, _rows_count
868 
869     delete.alters_data = True
870     delete.queryset_only = True
871 
872     def _raw_delete(self, using):
873         """
874         Delete objects found from the given queryset in single direct SQL
875         query. No signals are sent and there is no protection for cascades.
876         """
877         query = self.query.clone()
878         query.__class__ = sql.DeleteQuery
879         cursor = query.get_compiler(using).execute_sql(CURSOR)
880         if cursor:
881             with cursor:
882                 return cursor.rowcount
883         return 0
884     _raw_delete.alters_data = True
885 
886     def update(self, **kwargs):
887         """
888         Update all elements in the current QuerySet, setting all the given
889         fields to the appropriate values.
890         """
891         self._not_support_combined_queries('update')
892         if self.query.is_sliced:
893             raise TypeError('Cannot update a query once a slice has been taken.')
894         self._for_write = True
895         query = self.query.chain(sql.UpdateQuery)
896         query.add_update_values(kwargs)
897         # Clear any annotations so that they won't be present in subqueries.
898         query.annotations = {}
899         with transaction.mark_for_rollback_on_error(using=self.db):
900             rows = query.get_compiler(self.db).execute_sql(CURSOR)
901         self._result_cache = None
902         return rows
903     update.alters_data = True
904 
905     def _update(self, values):
906         """
907         A version of update() that accepts field objects instead of field names.
908         Used primarily for model saving and not intended for use by general
909         code (it requires too much poking around at model internals to be
910         useful at that level).
911         """
912         if self.query.is_sliced:
913             raise TypeError('Cannot update a query once a slice has been taken.')
914         query = self.query.chain(sql.UpdateQuery)
915         query.add_update_fields(values)
916         # Clear any annotations so that they won't be present in subqueries.
917         query.annotations = {}
918         self._result_cache = None
919         return query.get_compiler(self.db).execute_sql(CURSOR)
920     _update.alters_data = True
921     _update.queryset_only = False
922 
923     def exists(self):
924         if self._result_cache is None:
925             return self.query.has_results(using=self.db)
926         return bool(self._result_cache)
927 
928     def contains(self, obj):
929         """Return True if the queryset contains an object."""
930         self._not_support_combined_queries('contains')
931         if self._fields is not None:
932             raise TypeError(
933                 'Cannot call QuerySet.contains() after .values() or '
934                 '.values_list().'
935             )
936         try:
937             if obj._meta.concrete_model != self.model._meta.concrete_model:
938                 return False
939         except AttributeError:
940             raise TypeError("'obj' must be a model instance.")
941         if obj.pk is None:
942             raise ValueError(
943                 'QuerySet.contains() cannot be used on unsaved objects.'
944             )
945         if self._result_cache is not None:
946             return obj in self._result_cache
947         return self.filter(pk=obj.pk).exists()
948 
949     def _prefetch_related_objects(self):
950         # This method can only be called once the result cache has been filled.
951         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
952         self._prefetch_done = True
953 
954     def explain(self, *, format=None, **options):
955         return self.query.explain(using=self.db, format=format, **options)
956 
957     ##################################################
958     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
959     ##################################################
960 
961     def raw(self, raw_query, params=(), translations=None, using=None):
962         if using is None:
963             using = self.db
964         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
965         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
966         return qs
967 
968     def _values(self, *fields, **expressions):
969         clone = self._chain()
970         if expressions:
971             clone = clone.annotate(**expressions)
972         clone._fields = fields
973         clone.query.set_values(fields)
974         return clone
975 
976     def values(self, *fields, **expressions):
977         fields += tuple(expressions)
978         clone = self._values(*fields, **expressions)
979         clone._iterable_class = ValuesIterable
980         return clone
981 
982     def values_list(self, *fields, flat=False, named=False):
983         if flat and named:
984             raise TypeError("'flat' and 'named' can't be used together.")
985         if flat and len(fields) > 1:
986             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
987 
988         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
989         _fields = []
990         expressions = {}
991         counter = 1
992         for field in fields:
993             if hasattr(field, 'resolve_expression'):
994                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
995                 while True:
996                     field_id = field_id_prefix + str(counter)
997                     counter += 1
998                     if field_id not in field_names:
999                         break
1000                 expressions[field_id] = field
1001                 _fields.append(field_id)
1002             else:
1003                 _fields.append(field)
1004 
1005         clone = self._values(*_fields, **expressions)
1006         clone._iterable_class = (
1007             NamedValuesListIterable if named
1008             else FlatValuesListIterable if flat
1009             else ValuesListIterable
1010         )
1011         return clone
1012 
1013     def dates(self, field_name, kind, order='ASC'):
1014         """
1015         Return a list of date objects representing all available dates for
1016         the given field_name, scoped to 'kind'.
1017         """
1018         if kind not in ('year', 'month', 'week', 'day'):
1019             raise ValueError("'kind' must be one of 'year', 'month', 'week', or 'day'.")
1020         if order not in ('ASC', 'DESC'):
1021             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
1022         return self.annotate(
1023             datefield=Trunc(field_name, kind, output_field=DateField()),
1024             plain_field=F(field_name)
1025         ).values_list(
1026             'datefield', flat=True
1027         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
1028 
1029     # RemovedInDjango50Warning: when the deprecation ends, remove is_dst
1030     # argument.
1031     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=timezone.NOT_PASSED):
1032         """
1033         Return a list of datetime objects representing all available
1034         datetimes for the given field_name, scoped to 'kind'.
1035         """
1036         if kind not in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'):
1037             raise ValueError(
1038                 "'kind' must be one of 'year', 'month', 'week', 'day', "
1039                 "'hour', 'minute', or 'second'."
1040             )
1041         if order not in ('ASC', 'DESC'):
1042             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
1043         if settings.USE_TZ:
1044             if tzinfo is None:
1045                 tzinfo = timezone.get_current_timezone()
1046         else:
1047             tzinfo = None
1048         return self.annotate(
1049             datetimefield=Trunc(
1050                 field_name,
1051                 kind,
1052                 output_field=DateTimeField(),
1053                 tzinfo=tzinfo,
1054                 is_dst=is_dst,
1055             ),
1056             plain_field=F(field_name)
1057         ).values_list(
1058             'datetimefield', flat=True
1059         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
1060 
1061     def none(self):
1062         """Return an empty QuerySet."""
1063         clone = self._chain()
1064         clone.query.set_empty()
1065         return clone
1066 
1067     ##################################################################
1068     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
1069     ##################################################################
1070 
1071     def all(self):
1072         """
1073         Return a new QuerySet that is a copy of the current one. This allows a
1074         QuerySet to proxy for a model manager in some cases.
1075         """
1076         return self._chain()
1077 
1078     def filter(self, *args, **kwargs):
1079         """
1080         Return a new QuerySet instance with the args ANDed to the existing
1081         set.
1082         """
1083         self._not_support_combined_queries('filter')
1084         return self._filter_or_exclude(False, args, kwargs)
1085 
1086     def exclude(self, *args, **kwargs):
1087         """
1088         Return a new QuerySet instance with NOT (args) ANDed to the existing
1089         set.
1090         """
1091         self._not_support_combined_queries('exclude')
1092         return self._filter_or_exclude(True, args, kwargs)
1093 
1094     def _filter_or_exclude(self, negate, args, kwargs):
1095         if (args or kwargs) and self.query.is_sliced:
1096             raise TypeError('Cannot filter a query once a slice has been taken.')
1097         clone = self._chain()
1098         if self._defer_next_filter:
1099             self._defer_next_filter = False
1100             clone._deferred_filter = negate, args, kwargs
1101         else:
1102             clone._filter_or_exclude_inplace(negate, args, kwargs)
1103         return clone
1104 
1105     def _filter_or_exclude_inplace(self, negate, args, kwargs):
1106         if negate:
1107             self._query.add_q(~Q(*args, **kwargs))
1108         else:
1109             self._query.add_q(Q(*args, **kwargs))
1110 
1111     def complex_filter(self, filter_obj):
1112         """
1113         Return a new QuerySet instance with filter_obj added to the filters.
1114 
1115         filter_obj can be a Q object or a dictionary of keyword lookup
1116         arguments.
1117 
1118         This exists to support framework features such as 'limit_choices_to',
1119         and usually it will be more natural to use other methods.
1120         """
1121         if isinstance(filter_obj, Q):
1122             clone = self._chain()
1123             clone.query.add_q(filter_obj)
1124             return clone
1125         else:
1126             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
1127 
1128     def _combinator_query(self, combinator, *other_qs, all=False):
1129         # Clone the query to inherit the select list and everything
1130         clone = self._chain()
1131         # Clear limits and ordering so they can be reapplied
1132         clone.query.clear_ordering(force=True)
1133         clone.query.clear_limits()
1134         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
1135         clone.query.combinator = combinator
1136         clone.query.combinator_all = all
1137         return clone
1138 
1139     def union(self, *other_qs, all=False):
1140         # If the query is an EmptyQuerySet, combine all nonempty querysets.
1141         if isinstance(self, EmptyQuerySet):
1142             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1143             if not qs:
1144                 return self
1145             if len(qs) == 1:
1146                 return qs[0]
1147             return qs[0]._combinator_query('union', *qs[1:], all=all)
1148         return self._combinator_query('union', *other_qs, all=all)
1149 
1150     def intersection(self, *other_qs):
1151         # If any query is an EmptyQuerySet, return it.
1152         if isinstance(self, EmptyQuerySet):
1153             return self
1154         for other in other_qs:
1155             if isinstance(other, EmptyQuerySet):
1156                 return other
1157         return self._combinator_query('intersection', *other_qs)
1158 
1159     def difference(self, *other_qs):
1160         # If the query is an EmptyQuerySet, return it.
1161         if isinstance(self, EmptyQuerySet):
1162             return self
1163         return self._combinator_query('difference', *other_qs)
1164 
1165     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1166         """
1167         Return a new QuerySet instance that will select objects with a
1168         FOR UPDATE lock.
1169         """
1170         if nowait and skip_locked:
1171             raise ValueError('The nowait option cannot be used with skip_locked.')
1172         obj = self._chain()
1173         obj._for_write = True
1174         obj.query.select_for_update = True
1175         obj.query.select_for_update_nowait = nowait
1176         obj.query.select_for_update_skip_locked = skip_locked
1177         obj.query.select_for_update_of = of
1178         obj.query.select_for_no_key_update = no_key
1179         return obj
1180 
1181     def select_related(self, *fields):
1182         """
1183         Return a new QuerySet instance that will select related objects.
1184 
1185         If fields are specified, they must be ForeignKey fields and only those
1186         related objects are included in the selection.
1187 
1188         If select_related(None) is called, clear the list.
1189         """
1190         self._not_support_combined_queries('select_related')
1191         if self._fields is not None:
1192             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1193 
1194         obj = self._chain()
1195         if fields == (None,):
1196             obj.query.select_related = False
1197         elif fields:
1198             obj.query.add_select_related(fields)
1199         else:
1200             obj.query.select_related = True
1201         return obj
1202 
1203     def prefetch_related(self, *lookups):
1204         """
1205         Return a new QuerySet instance that will prefetch the specified
1206         Many-To-One and Many-To-Many related objects when the QuerySet is
1207         evaluated.
1208 
1209         When prefetch_related() is called more than once, append to the list of
1210         prefetch lookups. If prefetch_related(None) is called, clear the list.
1211         """
1212         self._not_support_combined_queries('prefetch_related')
1213         clone = self._chain()
1214         if lookups == (None,):
1215             clone._prefetch_related_lookups = ()
1216         else:
1217             for lookup in lookups:
1218                 if isinstance(lookup, Prefetch):
1219                     lookup = lookup.prefetch_to
1220                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1221                 if lookup in self.query._filtered_relations:
1222                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1223             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1224         return clone
1225 
1226     def annotate(self, *args, **kwargs):
1227         """
1228         Return a query set in which the returned objects have been annotated
1229         with extra data or aggregations.
1230         """
1231         self._not_support_combined_queries('annotate')
1232         return self._annotate(args, kwargs, select=True)
1233 
1234     def alias(self, *args, **kwargs):
1235         """
1236         Return a query set with added aliases for extra data or aggregations.
1237         """
1238         self._not_support_combined_queries('alias')
1239         return self._annotate(args, kwargs, select=False)
1240 
1241     def _annotate(self, args, kwargs, select=True):
1242         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1243         annotations = {}
1244         for arg in args:
1245             # The default_alias property may raise a TypeError.
1246             try:
1247                 if arg.default_alias in kwargs:
1248                     raise ValueError("The named annotation '%s' conflicts with the "
1249                                      "default name for another annotation."
1250                                      % arg.default_alias)
1251             except TypeError:
1252                 raise TypeError("Complex annotations require an alias")
1253             annotations[arg.default_alias] = arg
1254         annotations.update(kwargs)
1255 
1256         clone = self._chain()
1257         names = self._fields
1258         if names is None:
1259             names = set(chain.from_iterable(
1260                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1261                 for field in self.model._meta.get_fields()
1262             ))
1263 
1264         for alias, annotation in annotations.items():
1265             if alias in names:
1266                 raise ValueError("The annotation '%s' conflicts with a field on "
1267                                  "the model." % alias)
1268             if isinstance(annotation, FilteredRelation):
1269                 clone.query.add_filtered_relation(annotation, alias)
1270             else:
1271                 clone.query.add_annotation(
1272                     annotation, alias, is_summary=False, select=select,
1273                 )
1274         for alias, annotation in clone.query.annotations.items():
1275             if alias in annotations and annotation.contains_aggregate:
1276                 if clone._fields is None:
1277                     clone.query.group_by = True
1278                 else:
1279                     clone.query.set_group_by()
1280                 break
1281 
1282         return clone
1283 
1284     def order_by(self, *field_names):
1285         """Return a new QuerySet instance with the ordering changed."""
1286         if self.query.is_sliced:
1287             raise TypeError('Cannot reorder a query once a slice has been taken.')
1288         obj = self._chain()
1289         obj.query.clear_ordering(force=True, clear_default=False)
1290         obj.query.add_ordering(*field_names)
1291         return obj
1292 
1293     def distinct(self, *field_names):
1294         """
1295         Return a new QuerySet instance that will select only distinct results.
1296         """
1297         self._not_support_combined_queries('distinct')
1298         if self.query.is_sliced:
1299             raise TypeError('Cannot create distinct fields once a slice has been taken.')
1300         obj = self._chain()
1301         obj.query.add_distinct_fields(*field_names)
1302         return obj
1303 
1304     def extra(self, select=None, where=None, params=None, tables=None,
1305               order_by=None, select_params=None):
1306         """Add extra SQL fragments to the query."""
1307         self._not_support_combined_queries('extra')
1308         if self.query.is_sliced:
1309             raise TypeError('Cannot change a query once a slice has been taken.')
1310         clone = self._chain()
1311         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1312         return clone
1313 
1314     def reverse(self):
1315         """Reverse the ordering of the QuerySet."""
1316         if self.query.is_sliced:
1317             raise TypeError('Cannot reverse a query once a slice has been taken.')
1318         clone = self._chain()
1319         clone.query.standard_ordering = not clone.query.standard_ordering
1320         return clone
1321 
1322     def defer(self, *fields):
1323         """
1324         Defer the loading of data for certain fields until they are accessed.
1325         Add the set of deferred fields to any existing set of deferred fields.
1326         The only exception to this is if None is passed in as the only
1327         parameter, in which case removal all deferrals.
1328         """
1329         self._not_support_combined_queries('defer')
1330         if self._fields is not None:
1331             raise TypeError("Cannot call defer() after .values() or .values_list()")
1332         clone = self._chain()
1333         if fields == (None,):
1334             clone.query.clear_deferred_loading()
1335         else:
1336             clone.query.add_deferred_loading(fields)
1337         return clone
1338 
1339     def only(self, *fields):
1340         """
1341         Essentially, the opposite of defer(). Only the fields passed into this
1342         method and that are not already specified as deferred are loaded
1343         immediately when the queryset is evaluated.
1344         """
1345         self._not_support_combined_queries('only')
1346         if self._fields is not None:
1347             raise TypeError("Cannot call only() after .values() or .values_list()")
1348         if fields == (None,):
1349             # Can only pass None to defer(), not only(), as the rest option.
1350             # That won't stop people trying to do this, so let's be explicit.
1351             raise TypeError("Cannot pass None as an argument to only().")
1352         for field in fields:
1353             field = field.split(LOOKUP_SEP, 1)[0]
1354             if field in self.query._filtered_relations:
1355                 raise ValueError('only() is not supported with FilteredRelation.')
1356         clone = self._chain()
1357         clone.query.add_immediate_loading(fields)
1358         return clone
1359 
1360     def using(self, alias):
1361         """Select which database this QuerySet should execute against."""
1362         clone = self._chain()
1363         clone._db = alias
1364         return clone
1365 
1366     ###################################
1367     # PUBLIC INTROSPECTION ATTRIBUTES #
1368     ###################################
1369 
1370     @property
1371     def ordered(self):
1372         """
1373         Return True if the QuerySet is ordered -- i.e. has an order_by()
1374         clause or a default ordering on the model (or is empty).
1375         """
1376         if isinstance(self, EmptyQuerySet):
1377             return True
1378         if self.query.extra_order_by or self.query.order_by:
1379             return True
1380         elif (
1381             self.query.default_ordering and
1382             self.query.get_meta().ordering and
1383             # A default ordering doesn't affect GROUP BY queries.
1384             not self.query.group_by
1385         ):
1386             return True
1387         else:
1388             return False
1389 
1390     @property
1391     def db(self):
1392         """Return the database used if this query is executed now."""
1393         if self._for_write:
1394             return self._db or router.db_for_write(self.model, **self._hints)
1395         return self._db or router.db_for_read(self.model, **self._hints)
1396 
1397     ###################
1398     # PRIVATE METHODS #
1399     ###################
1400 
1401     def _insert(
1402         self, objs, fields, returning_fields=None, raw=False, using=None,
1403         on_conflict=None, update_fields=None, unique_fields=None,
1404     ):
1405         """
1406         Insert a new record for the given model. This provides an interface to
1407         the InsertQuery class and is how Model.save() is implemented.
1408         """
1409         self._for_write = True
1410         if using is None:
1411             using = self.db
1412         query = sql.InsertQuery(
1413             self.model,
1414             on_conflict=on_conflict,
1415             update_fields=update_fields,
1416             unique_fields=unique_fields,
1417         )
1418         query.insert_values(fields, objs, raw=raw)
1419         return query.get_compiler(using=using).execute_sql(returning_fields)
1420     _insert.alters_data = True
1421     _insert.queryset_only = False
1422 
1423     def _batched_insert(
1424         self, objs, fields, batch_size, on_conflict=None, update_fields=None,
1425         unique_fields=None,
1426     ):
1427         """
1428         Helper method for bulk_create() to insert objs one batch at a time.
1429         """
1430         connection = connections[self.db]
1431         ops = connection.ops
1432         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1433         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1434         inserted_rows = []
1435         bulk_return = connection.features.can_return_rows_from_bulk_insert
1436         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1437             if bulk_return and on_conflict is None:
1438                 inserted_rows.extend(self._insert(
1439                     item, fields=fields, using=self.db,
1440                     returning_fields=self.model._meta.db_returning_fields,
1441                 ))
1442             else:
1443                 self._insert(
1444                     item,
1445                     fields=fields,
1446                     using=self.db,
1447                     on_conflict=on_conflict,
1448                     update_fields=update_fields,
1449                     unique_fields=unique_fields,
1450                 )
1451         return inserted_rows
1452 
1453     def _chain(self):
1454         """
1455         Return a copy of the current QuerySet that's ready for another
1456         operation.
1457         """
1458         obj = self._clone()
1459         if obj._sticky_filter:
1460             obj.query.filter_is_sticky = True
1461             obj._sticky_filter = False
1462         return obj
1463 
1464     def _clone(self):
1465         """
1466         Return a copy of the current QuerySet. A lightweight alternative
1467         to deepcopy().
1468         """
1469         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1470         c._sticky_filter = self._sticky_filter
1471         c._for_write = self._for_write
1472         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1473         c._known_related_objects = self._known_related_objects
1474         c._iterable_class = self._iterable_class
1475         c._fields = self._fields
1476         return c
1477 
1478     def _fetch_all(self):
1479         if self._result_cache is None:
1480             self._result_cache = list(self._iterable_class(self))
1481         if self._prefetch_related_lookups and not self._prefetch_done:
1482             self._prefetch_related_objects()
1483 
1484     def _next_is_sticky(self):
1485         """
1486         Indicate that the next filter call and the one following that should
1487         be treated as a single filter. This is only important when it comes to
1488         determining when to reuse tables for many-to-many filters. Required so
1489         that we can filter naturally on the results of related managers.
1490 
1491         This doesn't return a clone of the current QuerySet (it returns
1492         "self"). The method is only used internally and should be immediately
1493         followed by a filter() that does create a clone.
1494         """
1495         self._sticky_filter = True
1496         return self
1497 
1498     def _merge_sanity_check(self, other):
1499         """Check that two QuerySet classes may be merged."""
1500         if self._fields is not None and (
1501                 set(self.query.values_select) != set(other.query.values_select) or
1502                 set(self.query.extra_select) != set(other.query.extra_select) or
1503                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1504             raise TypeError(
1505                 "Merging '%s' classes must involve the same values in each case."
1506                 % self.__class__.__name__
1507             )
1508 
1509     def _merge_known_related_objects(self, other):
1510         """
1511         Keep track of all known related objects from either QuerySet instance.
1512         """
1513         for field, objects in other._known_related_objects.items():
1514             self._known_related_objects.setdefault(field, {}).update(objects)
1515 
1516     def resolve_expression(self, *args, **kwargs):
1517         if self._fields and len(self._fields) > 1:
1518             # values() queryset can only be used as nested queries
1519             # if they are set up to select only a single field.
1520             raise TypeError('Cannot use multi-field values as a filter value.')
1521         query = self.query.resolve_expression(*args, **kwargs)
1522         query._db = self._db
1523         return query
1524     resolve_expression.queryset_only = True
1525 
1526     def _add_hints(self, **hints):
1527         """
1528         Update hinting information for use by routers. Add new key/values or
1529         overwrite existing key/values.
1530         """
1531         self._hints.update(hints)
1532 
1533     def _has_filters(self):
1534         """
1535         Check if this QuerySet has any filtering going on. This isn't
1536         equivalent with checking if all objects are present in results, for
1537         example, qs[1:]._has_filters() -> False.
1538         """
1539         return self.query.has_filters()
1540 
1541     @staticmethod
1542     def _validate_values_are_expressions(values, method_name):
1543         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1544         if invalid_args:
1545             raise TypeError(
1546                 'QuerySet.%s() received non-expression(s): %s.' % (
1547                     method_name,
1548                     ', '.join(invalid_args),
1549                 )
1550             )
1551 
1552     def _not_support_combined_queries(self, operation_name):
1553         if self.query.combinator:
1554             raise NotSupportedError(
1555                 'Calling QuerySet.%s() after %s() is not supported.'
1556                 % (operation_name, self.query.combinator)
1557             )
1558 
1559     def _check_operator_queryset(self, other, operator_):
1560         if self.query.combinator or other.query.combinator:
1561             raise TypeError(f'Cannot use {operator_} operator with combined queryset.')

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Exists</class>
<method>as_sql</method>
<code>
1212     def as_sql(self, compiler, connection, template=None, **extra_context):
1213         query = self.query.exists(using=connection.alias)
1214         sql, params = super().as_sql(
1215             compiler,
1216             connection,
1217             template=template,
1218             query=query,
1219             **extra_context,
1220         )
1221         if self.negated:
1222             sql = 'NOT {}'.format(sql)
1223         return sql, params

</code><intended_behavior>The `as_sql` method within the `Exists` class needs to ensure that it handles the scenario of an empty queryset in a way that accurately reflects the intended logical operation in the generated SQL. When `Exists` is constructed with an empty queryset, especially when negated, this method should generate SQL that properly captures the 'not exists' condition without resulting in the omission of crucial parts of the SQL statement. This might involve adjusting the SQL generation logic to explicitly handle empty querysets in a way that still allows for meaningful WHERE conditions to be generated, rather than leading to an `EmptyResultSet` scenario that removes the WHERE block. This adjustment should ensure that the SQL output remains logical and functional even when faced with negated empty `Exists` subqueries.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Exists</class>
<method>None</method>
<code>
1199 class Exists(Subquery):
1200     template = 'EXISTS(%(subquery)s)'
1201     output_field = fields.BooleanField()
1202 
1203     def __init__(self, queryset, negated=False, **kwargs):
1204         self.negated = negated
1205         super().__init__(queryset, **kwargs)
1206 
1207     def __invert__(self):
1208         clone = self.copy()
1209         clone.negated = not self.negated
1210         return clone
1211 
1212     def as_sql(self, compiler, connection, template=None, **extra_context):
1213         query = self.query.exists(using=connection.alias)
1214         sql, params = super().as_sql(
1215             compiler,
1216             connection,
1217             template=template,
1218             query=query,
1219             **extra_context,
1220         )
1221         if self.negated:
1222             sql = 'NOT {}'.format(sql)
1223         return sql, params
1224 
1225     def select_format(self, compiler, sql, params):
1226         # Wrap EXISTS() with a CASE WHEN expression if a database backend
1227         # (e.g. Oracle) doesn't support boolean expression in SELECT or GROUP
1228         # BY list.
1229         if not compiler.connection.features.supports_boolean_expr_in_select_clause:
1230             sql = 'CASE WHEN {} THEN 1 ELSE 0 END'.format(sql)
1231         return sql, params

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-17 11:06:46.162 | INFO     | Search completed. Bug locations: [<file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>filter</method>
<code>
1078     def filter(self, *args, **kwargs):
1079         """
1080         Return a new QuerySet instance with the args ANDed to the existing
1081         set.
1082         """
1083         self._not_support_combined_queries('filter')
1084         return self._filter_or_exclude(False, args, kwargs)

</code><intended_behavior>The `filter` method (or `_filter_or_exclude` if it’s more directly involved in constructing the query) should ensure that when constructing a query with an `Exists` subquery, even if the queryset inside `Exists` is empty, the WHERE clause in the generated SQL is not omitted entirely. Special handling may need to be added to recognize when an `Exists` subquery will produce no results and to ensure that the resulting SQL properly reflects a negation or presence check without discarding the WHERE clause entirely. This preservation should occur even when the queryset is explicitly empty, as in the case of `MyModel.objects.none()`, ensuring that the logical negation (`~`) or presence indicated by `Exists` in Django ORM queries is correctly translated to SQL.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
176 class QuerySet:
177     """Represent a lazy database lookup for a set of objects."""
178 
179     def __init__(self, model=None, query=None, using=None, hints=None):
180         self.model = model
181         self._db = using
182         self._hints = hints or {}
183         self._query = query or sql.Query(self.model)
184         self._result_cache = None
185         self._sticky_filter = False
186         self._for_write = False
187         self._prefetch_related_lookups = ()
188         self._prefetch_done = False
189         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
190         self._iterable_class = ModelIterable
191         self._fields = None
192         self._defer_next_filter = False
193         self._deferred_filter = None
194 
195     @property
196     def query(self):
197         if self._deferred_filter:
198             negate, args, kwargs = self._deferred_filter
199             self._filter_or_exclude_inplace(negate, args, kwargs)
200             self._deferred_filter = None
201         return self._query
202 
203     @query.setter
204     def query(self, value):
205         if value.values_select:
206             self._iterable_class = ValuesIterable
207         self._query = value
208 
209     def as_manager(cls):
210         # Address the circular dependency between `Queryset` and `Manager`.
211         from django.db.models.manager import Manager
212         manager = Manager.from_queryset(cls)()
213         manager._built_with_as_manager = True
214         return manager
215     as_manager.queryset_only = True
216     as_manager = classmethod(as_manager)
217 
218     ########################
219     # PYTHON MAGIC METHODS #
220     ########################
221 
222     def __deepcopy__(self, memo):
223         """Don't populate the QuerySet's cache."""
224         obj = self.__class__()
225         for k, v in self.__dict__.items():
226             if k == '_result_cache':
227                 obj.__dict__[k] = None
228             else:
229                 obj.__dict__[k] = copy.deepcopy(v, memo)
230         return obj
231 
232     def __getstate__(self):
233         # Force the cache to be fully populated.
234         self._fetch_all()
235         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
236 
237     def __setstate__(self, state):
238         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
239         if pickled_version:
240             if pickled_version != django.__version__:
241                 warnings.warn(
242                     "Pickled queryset instance's Django version %s does not "
243                     "match the current version %s."
244                     % (pickled_version, django.__version__),
245                     RuntimeWarning,
246                     stacklevel=2,
247                 )
248         else:
249             warnings.warn(
250                 "Pickled queryset instance's Django version is not specified.",
251                 RuntimeWarning,
252                 stacklevel=2,
253             )
254         self.__dict__.update(state)
255 
256     def __repr__(self):
257         data = list(self[:REPR_OUTPUT_SIZE + 1])
258         if len(data) > REPR_OUTPUT_SIZE:
259             data[-1] = "...(remaining elements truncated)..."
260         return '<%s %r>' % (self.__class__.__name__, data)
261 
262     def __len__(self):
263         self._fetch_all()
264         return len(self._result_cache)
265 
266     def __iter__(self):
267         """
268         The queryset iterator protocol uses three nested iterators in the
269         default case:
270             1. sql.compiler.execute_sql()
271                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
272                  using cursor.fetchmany(). This part is responsible for
273                  doing some column masking, and returning the rows in chunks.
274             2. sql.compiler.results_iter()
275                - Returns one row at time. At this point the rows are still just
276                  tuples. In some cases the return values are converted to
277                  Python values at this location.
278             3. self.iterator()
279                - Responsible for turning the rows into model objects.
280         """
281         self._fetch_all()
282         return iter(self._result_cache)
283 
284     def __bool__(self):
285         self._fetch_all()
286         return bool(self._result_cache)
287 
288     def __getitem__(self, k):
289         """Retrieve an item or slice from the set of results."""
290         if not isinstance(k, (int, slice)):
291             raise TypeError(
292                 'QuerySet indices must be integers or slices, not %s.'
293                 % type(k).__name__
294             )
295         if (
296             (isinstance(k, int) and k < 0) or
297             (isinstance(k, slice) and (
298                 (k.start is not None and k.start < 0) or
299                 (k.stop is not None and k.stop < 0)
300             ))
301         ):
302             raise ValueError('Negative indexing is not supported.')
303 
304         if self._result_cache is not None:
305             return self._result_cache[k]
306 
307         if isinstance(k, slice):
308             qs = self._chain()
309             if k.start is not None:
310                 start = int(k.start)
311             else:
312                 start = None
313             if k.stop is not None:
314                 stop = int(k.stop)
315             else:
316                 stop = None
317             qs.query.set_limits(start, stop)
318             return list(qs)[::k.step] if k.step else qs
319 
320         qs = self._chain()
321         qs.query.set_limits(k, k + 1)
322         qs._fetch_all()
323         return qs._result_cache[0]
324 
325     def __class_getitem__(cls, *args, **kwargs):
326         return cls
327 
328     def __and__(self, other):
329         self._check_operator_queryset(other, '&')
330         self._merge_sanity_check(other)
331         if isinstance(other, EmptyQuerySet):
332             return other
333         if isinstance(self, EmptyQuerySet):
334             return self
335         combined = self._chain()
336         combined._merge_known_related_objects(other)
337         combined.query.combine(other.query, sql.AND)
338         return combined
339 
340     def __or__(self, other):
341         self._check_operator_queryset(other, '|')
342         self._merge_sanity_check(other)
343         if isinstance(self, EmptyQuerySet):
344             return other
345         if isinstance(other, EmptyQuerySet):
346             return self
347         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
348         combined = query._chain()
349         combined._merge_known_related_objects(other)
350         if not other.query.can_filter():
351             other = other.model._base_manager.filter(pk__in=other.values('pk'))
352         combined.query.combine(other.query, sql.OR)
353         return combined
354 
355     ####################################
356     # METHODS THAT DO DATABASE QUERIES #
357     ####################################
358 
359     def _iterator(self, use_chunked_fetch, chunk_size):
360         iterable = self._iterable_class(
361             self,
362             chunked_fetch=use_chunked_fetch,
363             chunk_size=chunk_size or 2000,
364         )
365         if not self._prefetch_related_lookups or chunk_size is None:
366             yield from iterable
367             return
368 
369         iterator = iter(iterable)
370         while results := list(islice(iterator, chunk_size)):
371             prefetch_related_objects(results, *self._prefetch_related_lookups)
372             yield from results
373 
374     def iterator(self, chunk_size=None):
375         """
376         An iterator over the results from applying this QuerySet to the
377         database. chunk_size must be provided for QuerySets that prefetch
378         related objects. Otherwise, a default chunk_size of 2000 is supplied.
379         """
380         if chunk_size is None:
381             if self._prefetch_related_lookups:
382                 # When the deprecation ends, replace with:
383                 # raise ValueError(
384                 #     'chunk_size must be provided when using '
385                 #     'QuerySet.iterator() after prefetch_related().'
386                 # )
387                 warnings.warn(
388                     'Using QuerySet.iterator() after prefetch_related() '
389                     'without specifying chunk_size is deprecated.',
390                     category=RemovedInDjango50Warning,
391                     stacklevel=2,
392                 )
393         elif chunk_size <= 0:
394             raise ValueError('Chunk size must be strictly positive.')
395         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
396         return self._iterator(use_chunked_fetch, chunk_size)
397 
398     def aggregate(self, *args, **kwargs):
399         """
400         Return a dictionary containing the calculations (aggregation)
401         over the current queryset.
402 
403         If args is present the expression is passed as a kwarg using
404         the Aggregate object's default alias.
405         """
406         if self.query.distinct_fields:
407             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
408         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
409         for arg in args:
410             # The default_alias property raises TypeError if default_alias
411             # can't be set automatically or AttributeError if it isn't an
412             # attribute.
413             try:
414                 arg.default_alias
415             except (AttributeError, TypeError):
416                 raise TypeError("Complex aggregates require an alias")
417             kwargs[arg.default_alias] = arg
418 
419         query = self.query.chain()
420         for (alias, aggregate_expr) in kwargs.items():
421             query.add_annotation(aggregate_expr, alias, is_summary=True)
422             annotation = query.annotations[alias]
423             if not annotation.contains_aggregate:
424                 raise TypeError("%s is not an aggregate expression" % alias)
425             for expr in annotation.get_source_expressions():
426                 if expr.contains_aggregate and isinstance(expr, Ref) and expr.refs in kwargs:
427                     name = expr.refs
428                     raise exceptions.FieldError(
429                         "Cannot compute %s('%s'): '%s' is an aggregate"
430                         % (annotation.name, name, name)
431                     )
432         return query.get_aggregation(self.db, kwargs)
433 
434     def count(self):
435         """
436         Perform a SELECT COUNT() and return the number of records as an
437         integer.
438 
439         If the QuerySet is already fully cached, return the length of the
440         cached results set to avoid multiple SELECT COUNT(*) calls.
441         """
442         if self._result_cache is not None:
443             return len(self._result_cache)
444 
445         return self.query.get_count(using=self.db)
446 
447     def get(self, *args, **kwargs):
448         """
449         Perform the query and return a single object matching the given
450         keyword arguments.
451         """
452         if self.query.combinator and (args or kwargs):
453             raise NotSupportedError(
454                 'Calling QuerySet.get(...) with filters after %s() is not '
455                 'supported.' % self.query.combinator
456             )
457         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
458         if self.query.can_filter() and not self.query.distinct_fields:
459             clone = clone.order_by()
460         limit = None
461         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
462             limit = MAX_GET_RESULTS
463             clone.query.set_limits(high=limit)
464         num = len(clone)
465         if num == 1:
466             return clone._result_cache[0]
467         if not num:
468             raise self.model.DoesNotExist(
469                 "%s matching query does not exist." %
470                 self.model._meta.object_name
471             )
472         raise self.model.MultipleObjectsReturned(
473             'get() returned more than one %s -- it returned %s!' % (
474                 self.model._meta.object_name,
475                 num if not limit or num < limit else 'more than %s' % (limit - 1),
476             )
477         )
478 
479     def create(self, **kwargs):
480         """
481         Create a new object with the given kwargs, saving it to the database
482         and returning the created object.
483         """
484         obj = self.model(**kwargs)
485         self._for_write = True
486         obj.save(force_insert=True, using=self.db)
487         return obj
488 
489     def _prepare_for_bulk_create(self, objs):
490         for obj in objs:
491             if obj.pk is None:
492                 # Populate new PK values.
493                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
494             obj._prepare_related_fields_for_save(operation_name='bulk_create')
495 
496     def _check_bulk_create_options(self, ignore_conflicts, update_conflicts, update_fields, unique_fields):
497         if ignore_conflicts and update_conflicts:
498             raise ValueError(
499                 'ignore_conflicts and update_conflicts are mutually exclusive.'
500             )
501         db_features = connections[self.db].features
502         if ignore_conflicts:
503             if not db_features.supports_ignore_conflicts:
504                 raise NotSupportedError(
505                     'This database backend does not support ignoring conflicts.'
506                 )
507             return OnConflict.IGNORE
508         elif update_conflicts:
509             if not db_features.supports_update_conflicts:
510                 raise NotSupportedError(
511                     'This database backend does not support updating conflicts.'
512                 )
513             if not update_fields:
514                 raise ValueError(
515                     'Fields that will be updated when a row insertion fails '
516                     'on conflicts must be provided.'
517                 )
518             if unique_fields and not db_features.supports_update_conflicts_with_target:
519                 raise NotSupportedError(
520                     'This database backend does not support updating '
521                     'conflicts with specifying unique fields that can trigger '
522                     'the upsert.'
523                 )
524             if not unique_fields and db_features.supports_update_conflicts_with_target:
525                 raise ValueError(
526                     'Unique fields that can trigger the upsert must be '
527                     'provided.'
528                 )
529             # Updating primary keys and non-concrete fields is forbidden.
530             update_fields = [self.model._meta.get_field(name) for name in update_fields]
531             if any(not f.concrete or f.many_to_many for f in update_fields):
532                 raise ValueError(
533                     'bulk_create() can only be used with concrete fields in '
534                     'update_fields.'
535                 )
536             if any(f.primary_key for f in update_fields):
537                 raise ValueError(
538                     'bulk_create() cannot be used with primary keys in '
539                     'update_fields.'
540                 )
541             if unique_fields:
542                 # Primary key is allowed in unique_fields.
543                 unique_fields = [
544                     self.model._meta.get_field(name)
545                     for name in unique_fields if name != 'pk'
546                 ]
547                 if any(not f.concrete or f.many_to_many for f in unique_fields):
548                     raise ValueError(
549                         'bulk_create() can only be used with concrete fields '
550                         'in unique_fields.'
551                     )
552             return OnConflict.UPDATE
553         return None
554 
555     def bulk_create(
556         self, objs, batch_size=None, ignore_conflicts=False,
557         update_conflicts=False, update_fields=None, unique_fields=None,
558     ):
559         """
560         Insert each of the instances into the database. Do *not* call
561         save() on each of the instances, do not send any pre/post_save
562         signals, and do not set the primary key attribute if it is an
563         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
564         Multi-table models are not supported.
565         """
566         # When you bulk insert you don't get the primary keys back (if it's an
567         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
568         # you can't insert into the child tables which references this. There
569         # are two workarounds:
570         # 1) This could be implemented if you didn't have an autoincrement pk
571         # 2) You could do it by doing O(n) normal inserts into the parent
572         #    tables to get the primary keys back and then doing a single bulk
573         #    insert into the childmost table.
574         # We currently set the primary keys on the objects when using
575         # PostgreSQL via the RETURNING ID clause. It should be possible for
576         # Oracle as well, but the semantics for extracting the primary keys is
577         # trickier so it's not done yet.
578         if batch_size is not None and batch_size <= 0:
579             raise ValueError('Batch size must be a positive integer.')
580         # Check that the parents share the same concrete model with the our
581         # model to detect the inheritance pattern ConcreteGrandParent ->
582         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
583         # would not identify that case as involving multiple tables.
584         for parent in self.model._meta.get_parent_list():
585             if parent._meta.concrete_model is not self.model._meta.concrete_model:
586                 raise ValueError("Can't bulk create a multi-table inherited model")
587         if not objs:
588             return objs
589         on_conflict = self._check_bulk_create_options(
590             ignore_conflicts,
591             update_conflicts,
592             update_fields,
593             unique_fields,
594         )
595         self._for_write = True
596         opts = self.model._meta
597         fields = opts.concrete_fields
598         objs = list(objs)
599         self._prepare_for_bulk_create(objs)
600         with transaction.atomic(using=self.db, savepoint=False):
601             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
602             if objs_with_pk:
603                 returned_columns = self._batched_insert(
604                     objs_with_pk,
605                     fields,
606                     batch_size,
607                     on_conflict=on_conflict,
608                     update_fields=update_fields,
609                     unique_fields=unique_fields,
610                 )
611                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
612                     for result, field in zip(results, opts.db_returning_fields):
613                         if field != opts.pk:
614                             setattr(obj_with_pk, field.attname, result)
615                 for obj_with_pk in objs_with_pk:
616                     obj_with_pk._state.adding = False
617                     obj_with_pk._state.db = self.db
618             if objs_without_pk:
619                 fields = [f for f in fields if not isinstance(f, AutoField)]
620                 returned_columns = self._batched_insert(
621                     objs_without_pk,
622                     fields,
623                     batch_size,
624                     on_conflict=on_conflict,
625                     update_fields=update_fields,
626                     unique_fields=unique_fields,
627                 )
628                 connection = connections[self.db]
629                 if connection.features.can_return_rows_from_bulk_insert and on_conflict is None:
630                     assert len(returned_columns) == len(objs_without_pk)
631                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
632                     for result, field in zip(results, opts.db_returning_fields):
633                         setattr(obj_without_pk, field.attname, result)
634                     obj_without_pk._state.adding = False
635                     obj_without_pk._state.db = self.db
636 
637         return objs
638 
639     def bulk_update(self, objs, fields, batch_size=None):
640         """
641         Update the given fields in each of the given objects in the database.
642         """
643         if batch_size is not None and batch_size < 0:
644             raise ValueError('Batch size must be a positive integer.')
645         if not fields:
646             raise ValueError('Field names must be given to bulk_update().')
647         objs = tuple(objs)
648         if any(obj.pk is None for obj in objs):
649             raise ValueError('All bulk_update() objects must have a primary key set.')
650         fields = [self.model._meta.get_field(name) for name in fields]
651         if any(not f.concrete or f.many_to_many for f in fields):
652             raise ValueError('bulk_update() can only be used with concrete fields.')
653         if any(f.primary_key for f in fields):
654             raise ValueError('bulk_update() cannot be used with primary key fields.')
655         if not objs:
656             return 0
657         for obj in objs:
658             obj._prepare_related_fields_for_save(operation_name='bulk_update', fields=fields)
659         # PK is used twice in the resulting update query, once in the filter
660         # and once in the WHEN. Each field will also have one CAST.
661         connection = connections[self.db]
662         max_batch_size = connection.ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
663         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
664         requires_casting = connection.features.requires_casted_case_in_updates
665         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
666         updates = []
667         for batch_objs in batches:
668             update_kwargs = {}
669             for field in fields:
670                 when_statements = []
671                 for obj in batch_objs:
672                     attr = getattr(obj, field.attname)
673                     if not hasattr(attr, 'resolve_expression'):
674                         attr = Value(attr, output_field=field)
675                     when_statements.append(When(pk=obj.pk, then=attr))
676                 case_statement = Case(*when_statements, output_field=field)
677                 if requires_casting:
678                     case_statement = Cast(case_statement, output_field=field)
679                 update_kwargs[field.attname] = case_statement
680             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
681         rows_updated = 0
682         with transaction.atomic(using=self.db, savepoint=False):
683             for pks, update_kwargs in updates:
684                 rows_updated += self.filter(pk__in=pks).update(**update_kwargs)
685         return rows_updated
686     bulk_update.alters_data = True
687 
688     def get_or_create(self, defaults=None, **kwargs):
689         """
690         Look up an object with the given kwargs, creating one if necessary.
691         Return a tuple of (object, created), where created is a boolean
692         specifying whether an object was created.
693         """
694         # The get() needs to be targeted at the write database in order
695         # to avoid potential transaction consistency problems.
696         self._for_write = True
697         try:
698             return self.get(**kwargs), False
699         except self.model.DoesNotExist:
700             params = self._extract_model_params(defaults, **kwargs)
701             # Try to create an object using passed params.
702             try:
703                 with transaction.atomic(using=self.db):
704                     params = dict(resolve_callables(params))
705                     return self.create(**params), True
706             except IntegrityError:
707                 try:
708                     return self.get(**kwargs), False
709                 except self.model.DoesNotExist:
710                     pass
711                 raise
712 
713     def update_or_create(self, defaults=None, **kwargs):
714         """
715         Look up an object with the given kwargs, updating one with defaults
716         if it exists, otherwise create a new one.
717         Return a tuple (object, created), where created is a boolean
718         specifying whether an object was created.
719         """
720         defaults = defaults or {}
721         self._for_write = True
722         with transaction.atomic(using=self.db):
723             # Lock the row so that a concurrent update is blocked until
724             # update_or_create() has performed its save.
725             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)
726             if created:
727                 return obj, created
728             for k, v in resolve_callables(defaults):
729                 setattr(obj, k, v)
730             obj.save(using=self.db)
731         return obj, False
732 
733     def _extract_model_params(self, defaults, **kwargs):
734         """
735         Prepare `params` for creating a model instance based on the given
736         kwargs; for use by get_or_create().
737         """
738         defaults = defaults or {}
739         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
740         params.update(defaults)
741         property_names = self.model._meta._property_names
742         invalid_params = []
743         for param in params:
744             try:
745                 self.model._meta.get_field(param)
746             except exceptions.FieldDoesNotExist:
747                 # It's okay to use a model's property if it has a setter.
748                 if not (param in property_names and getattr(self.model, param).fset):
749                     invalid_params.append(param)
750         if invalid_params:
751             raise exceptions.FieldError(
752                 "Invalid field name(s) for model %s: '%s'." % (
753                     self.model._meta.object_name,
754                     "', '".join(sorted(invalid_params)),
755                 ))
756         return params
757 
758     def _earliest(self, *fields):
759         """
760         Return the earliest object according to fields (if given) or by the
761         model's Meta.get_latest_by.
762         """
763         if fields:
764             order_by = fields
765         else:
766             order_by = getattr(self.model._meta, 'get_latest_by')
767             if order_by and not isinstance(order_by, (tuple, list)):
768                 order_by = (order_by,)
769         if order_by is None:
770             raise ValueError(
771                 "earliest() and latest() require either fields as positional "
772                 "arguments or 'get_latest_by' in the model's Meta."
773             )
774         obj = self._chain()
775         obj.query.set_limits(high=1)
776         obj.query.clear_ordering(force=True)
777         obj.query.add_ordering(*order_by)
778         return obj.get()
779 
780     def earliest(self, *fields):
781         if self.query.is_sliced:
782             raise TypeError('Cannot change a query once a slice has been taken.')
783         return self._earliest(*fields)
784 
785     def latest(self, *fields):
786         if self.query.is_sliced:
787             raise TypeError('Cannot change a query once a slice has been taken.')
788         return self.reverse()._earliest(*fields)
789 
790     def first(self):
791         """Return the first object of a query or None if no match is found."""
792         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
793             return obj
794 
795     def last(self):
796         """Return the last object of a query or None if no match is found."""
797         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
798             return obj
799 
800     def in_bulk(self, id_list=None, *, field_name='pk'):
801         """
802         Return a dictionary mapping each of the given IDs to the object with
803         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
804         """
805         if self.query.is_sliced:
806             raise TypeError("Cannot use 'limit' or 'offset' with in_bulk().")
807         opts = self.model._meta
808         unique_fields = [
809             constraint.fields[0]
810             for constraint in opts.total_unique_constraints
811             if len(constraint.fields) == 1
812         ]
813         if (
814             field_name != 'pk' and
815             not opts.get_field(field_name).unique and
816             field_name not in unique_fields and
817             self.query.distinct_fields != (field_name,)
818         ):
819             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
820         if id_list is not None:
821             if not id_list:
822                 return {}
823             filter_key = '{}__in'.format(field_name)
824             batch_size = connections[self.db].features.max_query_params
825             id_list = tuple(id_list)
826             # If the database has a limit on the number of query parameters
827             # (e.g. SQLite), retrieve objects in batches if necessary.
828             if batch_size and batch_size < len(id_list):
829                 qs = ()
830                 for offset in range(0, len(id_list), batch_size):
831                     batch = id_list[offset:offset + batch_size]
832                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
833             else:
834                 qs = self.filter(**{filter_key: id_list}).order_by()
835         else:
836             qs = self._chain()
837         return {getattr(obj, field_name): obj for obj in qs}
838 
839     def delete(self):
840         """Delete the records in the current QuerySet."""
841         self._not_support_combined_queries('delete')
842         if self.query.is_sliced:
843             raise TypeError("Cannot use 'limit' or 'offset' with delete().")
844         if self.query.distinct or self.query.distinct_fields:
845             raise TypeError('Cannot call delete() after .distinct().')
846         if self._fields is not None:
847             raise TypeError("Cannot call delete() after .values() or .values_list()")
848 
849         del_query = self._chain()
850 
851         # The delete is actually 2 queries - one to find related objects,
852         # and one to delete. Make sure that the discovery of related
853         # objects is performed on the same database as the deletion.
854         del_query._for_write = True
855 
856         # Disable non-supported fields.
857         del_query.query.select_for_update = False
858         del_query.query.select_related = False
859         del_query.query.clear_ordering(force=True)
860 
861         collector = Collector(using=del_query.db, origin=self)
862         collector.collect(del_query)
863         deleted, _rows_count = collector.delete()
864 
865         # Clear the result cache, in case this QuerySet gets reused.
866         self._result_cache = None
867         return deleted, _rows_count
868 
869     delete.alters_data = True
870     delete.queryset_only = True
871 
872     def _raw_delete(self, using):
873         """
874         Delete objects found from the given queryset in single direct SQL
875         query. No signals are sent and there is no protection for cascades.
876         """
877         query = self.query.clone()
878         query.__class__ = sql.DeleteQuery
879         cursor = query.get_compiler(using).execute_sql(CURSOR)
880         if cursor:
881             with cursor:
882                 return cursor.rowcount
883         return 0
884     _raw_delete.alters_data = True
885 
886     def update(self, **kwargs):
887         """
888         Update all elements in the current QuerySet, setting all the given
889         fields to the appropriate values.
890         """
891         self._not_support_combined_queries('update')
892         if self.query.is_sliced:
893             raise TypeError('Cannot update a query once a slice has been taken.')
894         self._for_write = True
895         query = self.query.chain(sql.UpdateQuery)
896         query.add_update_values(kwargs)
897         # Clear any annotations so that they won't be present in subqueries.
898         query.annotations = {}
899         with transaction.mark_for_rollback_on_error(using=self.db):
900             rows = query.get_compiler(self.db).execute_sql(CURSOR)
901         self._result_cache = None
902         return rows
903     update.alters_data = True
904 
905     def _update(self, values):
906         """
907         A version of update() that accepts field objects instead of field names.
908         Used primarily for model saving and not intended for use by general
909         code (it requires too much poking around at model internals to be
910         useful at that level).
911         """
912         if self.query.is_sliced:
913             raise TypeError('Cannot update a query once a slice has been taken.')
914         query = self.query.chain(sql.UpdateQuery)
915         query.add_update_fields(values)
916         # Clear any annotations so that they won't be present in subqueries.
917         query.annotations = {}
918         self._result_cache = None
919         return query.get_compiler(self.db).execute_sql(CURSOR)
920     _update.alters_data = True
921     _update.queryset_only = False
922 
923     def exists(self):
924         if self._result_cache is None:
925             return self.query.has_results(using=self.db)
926         return bool(self._result_cache)
927 
928     def contains(self, obj):
929         """Return True if the queryset contains an object."""
930         self._not_support_combined_queries('contains')
931         if self._fields is not None:
932             raise TypeError(
933                 'Cannot call QuerySet.contains() after .values() or '
934                 '.values_list().'
935             )
936         try:
937             if obj._meta.concrete_model != self.model._meta.concrete_model:
938                 return False
939         except AttributeError:
940             raise TypeError("'obj' must be a model instance.")
941         if obj.pk is None:
942             raise ValueError(
943                 'QuerySet.contains() cannot be used on unsaved objects.'
944             )
945         if self._result_cache is not None:
946             return obj in self._result_cache
947         return self.filter(pk=obj.pk).exists()
948 
949     def _prefetch_related_objects(self):
950         # This method can only be called once the result cache has been filled.
951         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
952         self._prefetch_done = True
953 
954     def explain(self, *, format=None, **options):
955         return self.query.explain(using=self.db, format=format, **options)
956 
957     ##################################################
958     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
959     ##################################################
960 
961     def raw(self, raw_query, params=(), translations=None, using=None):
962         if using is None:
963             using = self.db
964         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
965         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
966         return qs
967 
968     def _values(self, *fields, **expressions):
969         clone = self._chain()
970         if expressions:
971             clone = clone.annotate(**expressions)
972         clone._fields = fields
973         clone.query.set_values(fields)
974         return clone
975 
976     def values(self, *fields, **expressions):
977         fields += tuple(expressions)
978         clone = self._values(*fields, **expressions)
979         clone._iterable_class = ValuesIterable
980         return clone
981 
982     def values_list(self, *fields, flat=False, named=False):
983         if flat and named:
984             raise TypeError("'flat' and 'named' can't be used together.")
985         if flat and len(fields) > 1:
986             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
987 
988         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
989         _fields = []
990         expressions = {}
991         counter = 1
992         for field in fields:
993             if hasattr(field, 'resolve_expression'):
994                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
995                 while True:
996                     field_id = field_id_prefix + str(counter)
997                     counter += 1
998                     if field_id not in field_names:
999                         break
1000                 expressions[field_id] = field
1001                 _fields.append(field_id)
1002             else:
1003                 _fields.append(field)
1004 
1005         clone = self._values(*_fields, **expressions)
1006         clone._iterable_class = (
1007             NamedValuesListIterable if named
1008             else FlatValuesListIterable if flat
1009             else ValuesListIterable
1010         )
1011         return clone
1012 
1013     def dates(self, field_name, kind, order='ASC'):
1014         """
1015         Return a list of date objects representing all available dates for
1016         the given field_name, scoped to 'kind'.
1017         """
1018         if kind not in ('year', 'month', 'week', 'day'):
1019             raise ValueError("'kind' must be one of 'year', 'month', 'week', or 'day'.")
1020         if order not in ('ASC', 'DESC'):
1021             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
1022         return self.annotate(
1023             datefield=Trunc(field_name, kind, output_field=DateField()),
1024             plain_field=F(field_name)
1025         ).values_list(
1026             'datefield', flat=True
1027         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
1028 
1029     # RemovedInDjango50Warning: when the deprecation ends, remove is_dst
1030     # argument.
1031     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=timezone.NOT_PASSED):
1032         """
1033         Return a list of datetime objects representing all available
1034         datetimes for the given field_name, scoped to 'kind'.
1035         """
1036         if kind not in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'):
1037             raise ValueError(
1038                 "'kind' must be one of 'year', 'month', 'week', 'day', "
1039                 "'hour', 'minute', or 'second'."
1040             )
1041         if order not in ('ASC', 'DESC'):
1042             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
1043         if settings.USE_TZ:
1044             if tzinfo is None:
1045                 tzinfo = timezone.get_current_timezone()
1046         else:
1047             tzinfo = None
1048         return self.annotate(
1049             datetimefield=Trunc(
1050                 field_name,
1051                 kind,
1052                 output_field=DateTimeField(),
1053                 tzinfo=tzinfo,
1054                 is_dst=is_dst,
1055             ),
1056             plain_field=F(field_name)
1057         ).values_list(
1058             'datetimefield', flat=True
1059         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
1060 
1061     def none(self):
1062         """Return an empty QuerySet."""
1063         clone = self._chain()
1064         clone.query.set_empty()
1065         return clone
1066 
1067     ##################################################################
1068     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
1069     ##################################################################
1070 
1071     def all(self):
1072         """
1073         Return a new QuerySet that is a copy of the current one. This allows a
1074         QuerySet to proxy for a model manager in some cases.
1075         """
1076         return self._chain()
1077 
1078     def filter(self, *args, **kwargs):
1079         """
1080         Return a new QuerySet instance with the args ANDed to the existing
1081         set.
1082         """
1083         self._not_support_combined_queries('filter')
1084         return self._filter_or_exclude(False, args, kwargs)
1085 
1086     def exclude(self, *args, **kwargs):
1087         """
1088         Return a new QuerySet instance with NOT (args) ANDed to the existing
1089         set.
1090         """
1091         self._not_support_combined_queries('exclude')
1092         return self._filter_or_exclude(True, args, kwargs)
1093 
1094     def _filter_or_exclude(self, negate, args, kwargs):
1095         if (args or kwargs) and self.query.is_sliced:
1096             raise TypeError('Cannot filter a query once a slice has been taken.')
1097         clone = self._chain()
1098         if self._defer_next_filter:
1099             self._defer_next_filter = False
1100             clone._deferred_filter = negate, args, kwargs
1101         else:
1102             clone._filter_or_exclude_inplace(negate, args, kwargs)
1103         return clone
1104 
1105     def _filter_or_exclude_inplace(self, negate, args, kwargs):
1106         if negate:
1107             self._query.add_q(~Q(*args, **kwargs))
1108         else:
1109             self._query.add_q(Q(*args, **kwargs))
1110 
1111     def complex_filter(self, filter_obj):
1112         """
1113         Return a new QuerySet instance with filter_obj added to the filters.
1114 
1115         filter_obj can be a Q object or a dictionary of keyword lookup
1116         arguments.
1117 
1118         This exists to support framework features such as 'limit_choices_to',
1119         and usually it will be more natural to use other methods.
1120         """
1121         if isinstance(filter_obj, Q):
1122             clone = self._chain()
1123             clone.query.add_q(filter_obj)
1124             return clone
1125         else:
1126             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
1127 
1128     def _combinator_query(self, combinator, *other_qs, all=False):
1129         # Clone the query to inherit the select list and everything
1130         clone = self._chain()
1131         # Clear limits and ordering so they can be reapplied
1132         clone.query.clear_ordering(force=True)
1133         clone.query.clear_limits()
1134         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
1135         clone.query.combinator = combinator
1136         clone.query.combinator_all = all
1137         return clone
1138 
1139     def union(self, *other_qs, all=False):
1140         # If the query is an EmptyQuerySet, combine all nonempty querysets.
1141         if isinstance(self, EmptyQuerySet):
1142             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1143             if not qs:
1144                 return self
1145             if len(qs) == 1:
1146                 return qs[0]
1147             return qs[0]._combinator_query('union', *qs[1:], all=all)
1148         return self._combinator_query('union', *other_qs, all=all)
1149 
1150     def intersection(self, *other_qs):
1151         # If any query is an EmptyQuerySet, return it.
1152         if isinstance(self, EmptyQuerySet):
1153             return self
1154         for other in other_qs:
1155             if isinstance(other, EmptyQuerySet):
1156                 return other
1157         return self._combinator_query('intersection', *other_qs)
1158 
1159     def difference(self, *other_qs):
1160         # If the query is an EmptyQuerySet, return it.
1161         if isinstance(self, EmptyQuerySet):
1162             return self
1163         return self._combinator_query('difference', *other_qs)
1164 
1165     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1166         """
1167         Return a new QuerySet instance that will select objects with a
1168         FOR UPDATE lock.
1169         """
1170         if nowait and skip_locked:
1171             raise ValueError('The nowait option cannot be used with skip_locked.')
1172         obj = self._chain()
1173         obj._for_write = True
1174         obj.query.select_for_update = True
1175         obj.query.select_for_update_nowait = nowait
1176         obj.query.select_for_update_skip_locked = skip_locked
1177         obj.query.select_for_update_of = of
1178         obj.query.select_for_no_key_update = no_key
1179         return obj
1180 
1181     def select_related(self, *fields):
1182         """
1183         Return a new QuerySet instance that will select related objects.
1184 
1185         If fields are specified, they must be ForeignKey fields and only those
1186         related objects are included in the selection.
1187 
1188         If select_related(None) is called, clear the list.
1189         """
1190         self._not_support_combined_queries('select_related')
1191         if self._fields is not None:
1192             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1193 
1194         obj = self._chain()
1195         if fields == (None,):
1196             obj.query.select_related = False
1197         elif fields:
1198             obj.query.add_select_related(fields)
1199         else:
1200             obj.query.select_related = True
1201         return obj
1202 
1203     def prefetch_related(self, *lookups):
1204         """
1205         Return a new QuerySet instance that will prefetch the specified
1206         Many-To-One and Many-To-Many related objects when the QuerySet is
1207         evaluated.
1208 
1209         When prefetch_related() is called more than once, append to the list of
1210         prefetch lookups. If prefetch_related(None) is called, clear the list.
1211         """
1212         self._not_support_combined_queries('prefetch_related')
1213         clone = self._chain()
1214         if lookups == (None,):
1215             clone._prefetch_related_lookups = ()
1216         else:
1217             for lookup in lookups:
1218                 if isinstance(lookup, Prefetch):
1219                     lookup = lookup.prefetch_to
1220                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1221                 if lookup in self.query._filtered_relations:
1222                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1223             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1224         return clone
1225 
1226     def annotate(self, *args, **kwargs):
1227         """
1228         Return a query set in which the returned objects have been annotated
1229         with extra data or aggregations.
1230         """
1231         self._not_support_combined_queries('annotate')
1232         return self._annotate(args, kwargs, select=True)
1233 
1234     def alias(self, *args, **kwargs):
1235         """
1236         Return a query set with added aliases for extra data or aggregations.
1237         """
1238         self._not_support_combined_queries('alias')
1239         return self._annotate(args, kwargs, select=False)
1240 
1241     def _annotate(self, args, kwargs, select=True):
1242         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1243         annotations = {}
1244         for arg in args:
1245             # The default_alias property may raise a TypeError.
1246             try:
1247                 if arg.default_alias in kwargs:
1248                     raise ValueError("The named annotation '%s' conflicts with the "
1249                                      "default name for another annotation."
1250                                      % arg.default_alias)
1251             except TypeError:
1252                 raise TypeError("Complex annotations require an alias")
1253             annotations[arg.default_alias] = arg
1254         annotations.update(kwargs)
1255 
1256         clone = self._chain()
1257         names = self._fields
1258         if names is None:
1259             names = set(chain.from_iterable(
1260                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1261                 for field in self.model._meta.get_fields()
1262             ))
1263 
1264         for alias, annotation in annotations.items():
1265             if alias in names:
1266                 raise ValueError("The annotation '%s' conflicts with a field on "
1267                                  "the model." % alias)
1268             if isinstance(annotation, FilteredRelation):
1269                 clone.query.add_filtered_relation(annotation, alias)
1270             else:
1271                 clone.query.add_annotation(
1272                     annotation, alias, is_summary=False, select=select,
1273                 )
1274         for alias, annotation in clone.query.annotations.items():
1275             if alias in annotations and annotation.contains_aggregate:
1276                 if clone._fields is None:
1277                     clone.query.group_by = True
1278                 else:
1279                     clone.query.set_group_by()
1280                 break
1281 
1282         return clone
1283 
1284     def order_by(self, *field_names):
1285         """Return a new QuerySet instance with the ordering changed."""
1286         if self.query.is_sliced:
1287             raise TypeError('Cannot reorder a query once a slice has been taken.')
1288         obj = self._chain()
1289         obj.query.clear_ordering(force=True, clear_default=False)
1290         obj.query.add_ordering(*field_names)
1291         return obj
1292 
1293     def distinct(self, *field_names):
1294         """
1295         Return a new QuerySet instance that will select only distinct results.
1296         """
1297         self._not_support_combined_queries('distinct')
1298         if self.query.is_sliced:
1299             raise TypeError('Cannot create distinct fields once a slice has been taken.')
1300         obj = self._chain()
1301         obj.query.add_distinct_fields(*field_names)
1302         return obj
1303 
1304     def extra(self, select=None, where=None, params=None, tables=None,
1305               order_by=None, select_params=None):
1306         """Add extra SQL fragments to the query."""
1307         self._not_support_combined_queries('extra')
1308         if self.query.is_sliced:
1309             raise TypeError('Cannot change a query once a slice has been taken.')
1310         clone = self._chain()
1311         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1312         return clone
1313 
1314     def reverse(self):
1315         """Reverse the ordering of the QuerySet."""
1316         if self.query.is_sliced:
1317             raise TypeError('Cannot reverse a query once a slice has been taken.')
1318         clone = self._chain()
1319         clone.query.standard_ordering = not clone.query.standard_ordering
1320         return clone
1321 
1322     def defer(self, *fields):
1323         """
1324         Defer the loading of data for certain fields until they are accessed.
1325         Add the set of deferred fields to any existing set of deferred fields.
1326         The only exception to this is if None is passed in as the only
1327         parameter, in which case removal all deferrals.
1328         """
1329         self._not_support_combined_queries('defer')
1330         if self._fields is not None:
1331             raise TypeError("Cannot call defer() after .values() or .values_list()")
1332         clone = self._chain()
1333         if fields == (None,):
1334             clone.query.clear_deferred_loading()
1335         else:
1336             clone.query.add_deferred_loading(fields)
1337         return clone
1338 
1339     def only(self, *fields):
1340         """
1341         Essentially, the opposite of defer(). Only the fields passed into this
1342         method and that are not already specified as deferred are loaded
1343         immediately when the queryset is evaluated.
1344         """
1345         self._not_support_combined_queries('only')
1346         if self._fields is not None:
1347             raise TypeError("Cannot call only() after .values() or .values_list()")
1348         if fields == (None,):
1349             # Can only pass None to defer(), not only(), as the rest option.
1350             # That won't stop people trying to do this, so let's be explicit.
1351             raise TypeError("Cannot pass None as an argument to only().")
1352         for field in fields:
1353             field = field.split(LOOKUP_SEP, 1)[0]
1354             if field in self.query._filtered_relations:
1355                 raise ValueError('only() is not supported with FilteredRelation.')
1356         clone = self._chain()
1357         clone.query.add_immediate_loading(fields)
1358         return clone
1359 
1360     def using(self, alias):
1361         """Select which database this QuerySet should execute against."""
1362         clone = self._chain()
1363         clone._db = alias
1364         return clone
1365 
1366     ###################################
1367     # PUBLIC INTROSPECTION ATTRIBUTES #
1368     ###################################
1369 
1370     @property
1371     def ordered(self):
1372         """
1373         Return True if the QuerySet is ordered -- i.e. has an order_by()
1374         clause or a default ordering on the model (or is empty).
1375         """
1376         if isinstance(self, EmptyQuerySet):
1377             return True
1378         if self.query.extra_order_by or self.query.order_by:
1379             return True
1380         elif (
1381             self.query.default_ordering and
1382             self.query.get_meta().ordering and
1383             # A default ordering doesn't affect GROUP BY queries.
1384             not self.query.group_by
1385         ):
1386             return True
1387         else:
1388             return False
1389 
1390     @property
1391     def db(self):
1392         """Return the database used if this query is executed now."""
1393         if self._for_write:
1394             return self._db or router.db_for_write(self.model, **self._hints)
1395         return self._db or router.db_for_read(self.model, **self._hints)
1396 
1397     ###################
1398     # PRIVATE METHODS #
1399     ###################
1400 
1401     def _insert(
1402         self, objs, fields, returning_fields=None, raw=False, using=None,
1403         on_conflict=None, update_fields=None, unique_fields=None,
1404     ):
1405         """
1406         Insert a new record for the given model. This provides an interface to
1407         the InsertQuery class and is how Model.save() is implemented.
1408         """
1409         self._for_write = True
1410         if using is None:
1411             using = self.db
1412         query = sql.InsertQuery(
1413             self.model,
1414             on_conflict=on_conflict,
1415             update_fields=update_fields,
1416             unique_fields=unique_fields,
1417         )
1418         query.insert_values(fields, objs, raw=raw)
1419         return query.get_compiler(using=using).execute_sql(returning_fields)
1420     _insert.alters_data = True
1421     _insert.queryset_only = False
1422 
1423     def _batched_insert(
1424         self, objs, fields, batch_size, on_conflict=None, update_fields=None,
1425         unique_fields=None,
1426     ):
1427         """
1428         Helper method for bulk_create() to insert objs one batch at a time.
1429         """
1430         connection = connections[self.db]
1431         ops = connection.ops
1432         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1433         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1434         inserted_rows = []
1435         bulk_return = connection.features.can_return_rows_from_bulk_insert
1436         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1437             if bulk_return and on_conflict is None:
1438                 inserted_rows.extend(self._insert(
1439                     item, fields=fields, using=self.db,
1440                     returning_fields=self.model._meta.db_returning_fields,
1441                 ))
1442             else:
1443                 self._insert(
1444                     item,
1445                     fields=fields,
1446                     using=self.db,
1447                     on_conflict=on_conflict,
1448                     update_fields=update_fields,
1449                     unique_fields=unique_fields,
1450                 )
1451         return inserted_rows
1452 
1453     def _chain(self):
1454         """
1455         Return a copy of the current QuerySet that's ready for another
1456         operation.
1457         """
1458         obj = self._clone()
1459         if obj._sticky_filter:
1460             obj.query.filter_is_sticky = True
1461             obj._sticky_filter = False
1462         return obj
1463 
1464     def _clone(self):
1465         """
1466         Return a copy of the current QuerySet. A lightweight alternative
1467         to deepcopy().
1468         """
1469         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1470         c._sticky_filter = self._sticky_filter
1471         c._for_write = self._for_write
1472         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1473         c._known_related_objects = self._known_related_objects
1474         c._iterable_class = self._iterable_class
1475         c._fields = self._fields
1476         return c
1477 
1478     def _fetch_all(self):
1479         if self._result_cache is None:
1480             self._result_cache = list(self._iterable_class(self))
1481         if self._prefetch_related_lookups and not self._prefetch_done:
1482             self._prefetch_related_objects()
1483 
1484     def _next_is_sticky(self):
1485         """
1486         Indicate that the next filter call and the one following that should
1487         be treated as a single filter. This is only important when it comes to
1488         determining when to reuse tables for many-to-many filters. Required so
1489         that we can filter naturally on the results of related managers.
1490 
1491         This doesn't return a clone of the current QuerySet (it returns
1492         "self"). The method is only used internally and should be immediately
1493         followed by a filter() that does create a clone.
1494         """
1495         self._sticky_filter = True
1496         return self
1497 
1498     def _merge_sanity_check(self, other):
1499         """Check that two QuerySet classes may be merged."""
1500         if self._fields is not None and (
1501                 set(self.query.values_select) != set(other.query.values_select) or
1502                 set(self.query.extra_select) != set(other.query.extra_select) or
1503                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1504             raise TypeError(
1505                 "Merging '%s' classes must involve the same values in each case."
1506                 % self.__class__.__name__
1507             )
1508 
1509     def _merge_known_related_objects(self, other):
1510         """
1511         Keep track of all known related objects from either QuerySet instance.
1512         """
1513         for field, objects in other._known_related_objects.items():
1514             self._known_related_objects.setdefault(field, {}).update(objects)
1515 
1516     def resolve_expression(self, *args, **kwargs):
1517         if self._fields and len(self._fields) > 1:
1518             # values() queryset can only be used as nested queries
1519             # if they are set up to select only a single field.
1520             raise TypeError('Cannot use multi-field values as a filter value.')
1521         query = self.query.resolve_expression(*args, **kwargs)
1522         query._db = self._db
1523         return query
1524     resolve_expression.queryset_only = True
1525 
1526     def _add_hints(self, **hints):
1527         """
1528         Update hinting information for use by routers. Add new key/values or
1529         overwrite existing key/values.
1530         """
1531         self._hints.update(hints)
1532 
1533     def _has_filters(self):
1534         """
1535         Check if this QuerySet has any filtering going on. This isn't
1536         equivalent with checking if all objects are present in results, for
1537         example, qs[1:]._has_filters() -> False.
1538         """
1539         return self.query.has_filters()
1540 
1541     @staticmethod
1542     def _validate_values_are_expressions(values, method_name):
1543         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1544         if invalid_args:
1545             raise TypeError(
1546                 'QuerySet.%s() received non-expression(s): %s.' % (
1547                     method_name,
1548                     ', '.join(invalid_args),
1549                 )
1550             )
1551 
1552     def _not_support_combined_queries(self, operation_name):
1553         if self.query.combinator:
1554             raise NotSupportedError(
1555                 'Calling QuerySet.%s() after %s() is not supported.'
1556                 % (operation_name, self.query.combinator)
1557             )
1558 
1559     def _check_operator_queryset(self, other, operator_):
1560         if self.query.combinator or other.query.combinator:
1561             raise TypeError(f'Cannot use {operator_} operator with combined queryset.')

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Exists</class>
<method>as_sql</method>
<code>
1212     def as_sql(self, compiler, connection, template=None, **extra_context):
1213         query = self.query.exists(using=connection.alias)
1214         sql, params = super().as_sql(
1215             compiler,
1216             connection,
1217             template=template,
1218             query=query,
1219             **extra_context,
1220         )
1221         if self.negated:
1222             sql = 'NOT {}'.format(sql)
1223         return sql, params

</code><intended_behavior>The `as_sql` method within the `Exists` class needs to ensure that it handles the scenario of an empty queryset in a way that accurately reflects the intended logical operation in the generated SQL. When `Exists` is constructed with an empty queryset, especially when negated, this method should generate SQL that properly captures the 'not exists' condition without resulting in the omission of crucial parts of the SQL statement. This might involve adjusting the SQL generation logic to explicitly handle empty querysets in a way that still allows for meaningful WHERE conditions to be generated, rather than leading to an `EmptyResultSet` scenario that removes the WHERE block. This adjustment should ensure that the SQL output remains logical and functional even when faced with negated empty `Exists` subqueries.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Exists</class>
<method>None</method>
<code>
1199 class Exists(Subquery):
1200     template = 'EXISTS(%(subquery)s)'
1201     output_field = fields.BooleanField()
1202 
1203     def __init__(self, queryset, negated=False, **kwargs):
1204         self.negated = negated
1205         super().__init__(queryset, **kwargs)
1206 
1207     def __invert__(self):
1208         clone = self.copy()
1209         clone.negated = not self.negated
1210         return clone
1211 
1212     def as_sql(self, compiler, connection, template=None, **extra_context):
1213         query = self.query.exists(using=connection.alias)
1214         sql, params = super().as_sql(
1215             compiler,
1216             connection,
1217             template=template,
1218             query=query,
1219             **extra_context,
1220         )
1221         if self.negated:
1222             sql = 'NOT {}'.format(sql)
1223         return sql, params
1224 
1225     def select_format(self, compiler, sql, params):
1226         # Wrap EXISTS() with a CASE WHEN expression if a database backend
1227         # (e.g. Oracle) doesn't support boolean expression in SELECT or GROUP
1228         # BY list.
1229         if not compiler.connection.features.supports_boolean_expr_in_select_clause:
1230             sql = 'CASE WHEN {} THEN 1 ELSE 0 END'.format(sql)
1231         return sql, params

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-17 11:06:46.162 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-17 11:06:46.162 | INFO     | Search-only: skipping patch selection
2025-10-17 11:06:46.299 | INFO     | Task django__django-15382 completed successfully.
2025-10-17 11:06:46.300 | INFO     | No patch generated. You can try running ACR again.
