2025-10-17 16:00:44.439 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-17 16:00:49.535 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=756, output_tokens=12, cost=0.007920
2025-10-17 16:00:49.536 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-17 16:00:49.540 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 16:01:11.103 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1063, output_tokens=514, cost=0.026050
2025-10-17 16:01:11.302 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 16:01:20.094 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=868, output_tokens=69, cost=0.010750
2025-10-17 16:01:20.095 | DEBUG    | Extracted a valid json.
2025-10-17 16:01:20.095 | DEBUG    | Agent proxy return the following json: {
    "API_calls": ["search_code(\"dask.array\")", "search_method(\"chunk\")", "search_method(\"compute\")", "search_code(\"map_blocks\")", "search_code(\"blockwise\")", "search_code(\"rechunk\")", "search_code(\"dask.config\")"],
    "bug_locations": []
}
2025-10-17 16:01:20.296 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-17 16:01:20.297 | DEBUG    | <Agent search> Analyzing search results.
2025-10-17 16:01:54.304 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2823, output_tokens=741, cost=0.050460
2025-10-17 16:01:54.311 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 16:02:14.656 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3780, output_tokens=536, cost=0.053880
2025-10-17 16:02:14.662 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 16:02:30.566 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=890, output_tokens=331, cost=0.018830
2025-10-17 16:02:30.567 | DEBUG    | Extracted a valid json.
2025-10-17 16:02:30.567 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "xarray/core/dataarray.py",
            "class": "",
            "method": "",
            "intended_behavior": "These methods should be refactored to an interface that can dynamically dispatch the calls to the appropriate backend based on user configuration or an attribute within the DataArray/Dataset. The existing functionality for dask should be preserved, allowing for a seamless transition between backends without altering the user's experience."
        },
        {
            "file": "xarray/core/dataset.py",
            "class": "",
            "method": "",
            "intended_behavior": "Implement a backend-agnostic interface such that the Dataset class can operate with different parallel computing frameworks beyond dask. Maintain current dask functionalities and ensure new parallel backends can be introduced with minimal friction."
        },
        {
            "file": "xarray/core/variable.py",
            "class": "",
            "method": "",
            "intended_behavior": "Adapt these methods to utilize a generalized computational backend system that decides at runtime which backend to use for operations, based on user configuration or data attributes. Ensure that the behavior for dask remains unaffected for current users."
        },
        {
            "file": "xarray/core/computation.py",
            "class": "",
            "method": "",
            "intended_behavior": "Abstract the operation handling to be compatible with multiple array backends, enabling the support for cubed and other potential future backends. This might involve creating helper functions or classes responsible for executing backend-specific code."
        }
    ]
}
2025-10-17 16:02:30.609 | DEBUG    | Bug location extracted successfully: [<file>xarray/core/dataarray.py</file>
<class>None</class>
<method>None</method>
<code>
1 from __future__ import annotations
2 
3 import datetime
4 import warnings
5 from collections.abc import Hashable, Iterable, Mapping, MutableMapping, Sequence
6 from os import PathLike
7 from typing import TYPE_CHECKING, Any, Callable, Literal, NoReturn, cast, overload
8 
9 import numpy as np
10 import pandas as pd
11 
12 from xarray.coding.calendar_ops import convert_calendar, interp_calendar
13 from xarray.coding.cftimeindex import CFTimeIndex
14 from xarray.core import alignment, computation, dtypes, indexing, ops, utils
15 from xarray.core._aggregations import DataArrayAggregations
16 from xarray.core.accessor_dt import CombinedDatetimelikeAccessor
17 from xarray.core.accessor_str import StringAccessor
18 from xarray.core.alignment import (
19     _broadcast_helper,
20     _get_broadcast_dims_map_common_coords,
21     align,
22 )
23 from xarray.core.arithmetic import DataArrayArithmetic
24 from xarray.core.common import AbstractArray, DataWithCoords, get_chunksizes
25 from xarray.core.computation import unify_chunks
26 from xarray.core.coordinates import DataArrayCoordinates, assert_coordinate_consistent
27 from xarray.core.dataset import Dataset
28 from xarray.core.formatting import format_item
29 from xarray.core.indexes import (
30     Index,
31     Indexes,
32     PandasMultiIndex,
33     filter_indexes_from_coords,
34     isel_indexes,
35 )
36 from xarray.core.indexing import is_fancy_indexer, map_index_queries
37 from xarray.core.merge import PANDAS_TYPES, MergeError, _create_indexes_from_coords
38 from xarray.core.options import OPTIONS, _get_keep_attrs
39 from xarray.core.utils import (
40     Default,
41     HybridMappingProxy,
42     ReprObject,
43     _default,
44     either_dict_or_kwargs,
45 )
46 from xarray.core.variable import (
47     IndexVariable,
48     Variable,
49     as_compatible_data,
50     as_variable,
51 )
52 from xarray.plot.accessor import DataArrayPlotAccessor
53 from xarray.plot.utils import _get_units_from_attrs
54 
55 if TYPE_CHECKING:
56     from typing import TypeVar, Union
57 
58     from numpy.typing import ArrayLike
59 
60     try:
61         from dask.dataframe import DataFrame as DaskDataFrame
62     except ImportError:
63         DaskDataFrame = None  # type: ignore
64     try:
65         from dask.delayed import Delayed
66     except ImportError:
67         Delayed = None  # type: ignore
68     try:
69         from cdms2 import Variable as cdms2_Variable
70     except ImportError:
71         cdms2_Variable = None
72     try:
73         from iris.cube import Cube as iris_Cube
74     except ImportError:
75         iris_Cube = None
76 
77     from xarray.backends import ZarrStore
78     from xarray.backends.api import T_NetcdfEngine, T_NetcdfTypes
79     from xarray.core.groupby import DataArrayGroupBy
80     from xarray.core.resample import DataArrayResample
81     from xarray.core.rolling import DataArrayCoarsen, DataArrayRolling
82     from xarray.core.types import (
83         CoarsenBoundaryOptions,
84         DatetimeLike,
85         DatetimeUnitOptions,
86         Dims,
87         ErrorOptions,
88         ErrorOptionsWithWarn,
89         InterpOptions,
90         PadModeOptions,
91         PadReflectOptions,
92         QuantileMethods,
93         QueryEngineOptions,
94         QueryParserOptions,
95         ReindexMethodOptions,
96         SideOptions,
97         T_DataArray,
98         T_Xarray,
99     )
100     from xarray.core.weighted import DataArrayWeighted
101 
102     T_XarrayOther = TypeVar("T_XarrayOther", bound=Union["DataArray", Dataset])
103 
104 
105 def _infer_coords_and_dims(
106     shape, coords, dims
107 ) -> tuple[dict[Hashable, Variable], tuple[Hashable, ...]]:
108     """All the logic for creating a new DataArray"""
109 
110     if (
111         coords is not None
112         and not utils.is_dict_like(coords)
113         and len(coords) != len(shape)
114     ):
115         raise ValueError(
116             f"coords is not dict-like, but it has {len(coords)} items, "
117             f"which does not match the {len(shape)} dimensions of the "
118             "data"
119         )
120 
121     if isinstance(dims, str):
122         dims = (dims,)
123 
124     if dims is None:
125         dims = [f"dim_{n}" for n in range(len(shape))]
126         if coords is not None and len(coords) == len(shape):
127             # try to infer dimensions from coords
128             if utils.is_dict_like(coords):
129                 dims = list(coords.keys())
130             else:
131                 for n, (dim, coord) in enumerate(zip(dims, coords)):
132                     coord = as_variable(coord, name=dims[n]).to_index_variable()
133                     dims[n] = coord.name
134         dims = tuple(dims)
135     elif len(dims) != len(shape):
136         raise ValueError(
137             "different number of dimensions on data "
138             f"and dims: {len(shape)} vs {len(dims)}"
139         )
140     else:
141         for d in dims:
142             if not isinstance(d, str):
143                 raise TypeError(f"dimension {d} is not a string")
144 
145     new_coords: dict[Hashable, Variable] = {}
146 
147     if utils.is_dict_like(coords):
148         for k, v in coords.items():
149             new_coords[k] = as_variable(v, name=k)
150     elif coords is not None:
151         for dim, coord in zip(dims, coords):
152             var = as_variable(coord, name=dim)
153             var.dims = (dim,)
154             new_coords[dim] = var.to_index_variable()
155 
156     sizes = dict(zip(dims, shape))
157     for k, v in new_coords.items():
158         if any(d not in dims for d in v.dims):
159             raise ValueError(
160                 f"coordinate {k} has dimensions {v.dims}, but these "
161                 "are not a subset of the DataArray "
162                 f"dimensions {dims}"
163             )
164 
165         for d, s in zip(v.dims, v.shape):
166             if s != sizes[d]:
167                 raise ValueError(
168                     f"conflicting sizes for dimension {d!r}: "
169                     f"length {sizes[d]} on the data but length {s} on "
170                     f"coordinate {k!r}"
171                 )
172 
173         if k in sizes and v.shape != (sizes[k],):
174             raise ValueError(
175                 f"coordinate {k!r} is a DataArray dimension, but "
176                 f"it has shape {v.shape!r} rather than expected shape {sizes[k]!r} "
177                 "matching the dimension size"
178             )
179 
180     return new_coords, dims
181 
182 
183 def _check_data_shape(data, coords, dims):
184     if data is dtypes.NA:
185         data = np.nan
186     if coords is not None and utils.is_scalar(data, include_0d=False):
187         if utils.is_dict_like(coords):
188             if dims is None:
189                 return data
190             else:
191                 data_shape = tuple(
192                     as_variable(coords[k], k).size if k in coords.keys() else 1
193                     for k in dims
194                 )
195         else:
196             data_shape = tuple(as_variable(coord, "foo").size for coord in coords)
197         data = np.full(data_shape, data)
198     return data
199 
200 
201 class _LocIndexer:
202     __slots__ = ("data_array",)
203 
204     def __init__(self, data_array: DataArray):
205         self.data_array = data_array
206 
207     def __getitem__(self, key) -> DataArray:
208         if not utils.is_dict_like(key):
209             # expand the indexer so we can handle Ellipsis
210             labels = indexing.expanded_indexer(key, self.data_array.ndim)
211             key = dict(zip(self.data_array.dims, labels))
212         return self.data_array.sel(key)
213 
214     def __setitem__(self, key, value) -> None:
215         if not utils.is_dict_like(key):
216             # expand the indexer so we can handle Ellipsis
217             labels = indexing.expanded_indexer(key, self.data_array.ndim)
218             key = dict(zip(self.data_array.dims, labels))
219 
220         dim_indexers = map_index_queries(self.data_array, key).dim_indexers
221         self.data_array[dim_indexers] = value
222 
223 
224 # Used as the key corresponding to a DataArray's variable when converting
225 # arbitrary DataArray objects to datasets
226 _THIS_ARRAY = ReprObject("<this-array>")
227 
228 
229 class DataArray(
230     AbstractArray,
231     DataWithCoords,
232     DataArrayArithmetic,
233     DataArrayAggregations,
234 ):
235     """N-dimensional array with labeled coordinates and dimensions.
236 
237     DataArray provides a wrapper around numpy ndarrays that uses
238     labeled dimensions and coordinates to support metadata aware
239     operations. The API is similar to that for the pandas Series or
240     DataFrame, but DataArray objects can have any number of dimensions,
241     and their contents have fixed data types.
242 
243     Additional features over raw numpy arrays:
244 
245     - Apply operations over dimensions by name: ``x.sum('time')``.
246     - Select or assign values by integer location (like numpy):
247       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or
248       ``x.sel(time='2014-01-01')``.
249     - Mathematical operations (e.g., ``x - y``) vectorize across
250       multiple dimensions (known in numpy as "broadcasting") based on
251       dimension names, regardless of their original order.
252     - Keep track of arbitrary metadata in the form of a Python
253       dictionary: ``x.attrs``
254     - Convert to a pandas Series: ``x.to_series()``.
255 
256     Getting items from or doing mathematical operations with a
257     DataArray always returns another DataArray.
258 
259     Parameters
260     ----------
261     data : array_like
262         Values for this array. Must be an ``numpy.ndarray``, ndarray
263         like, or castable to an ``ndarray``. If a self-described xarray
264         or pandas object, attempts are made to use this array's
265         metadata to fill in other unspecified arguments. A view of the
266         array's data is used instead of a copy if possible.
267     coords : sequence or dict of array_like, optional
268         Coordinates (tick labels) to use for indexing along each
269         dimension. The following notations are accepted:
270 
271         - mapping {dimension name: array-like}
272         - sequence of tuples that are valid arguments for
273           ``xarray.Variable()``
274           - (dims, data)
275           - (dims, data, attrs)
276           - (dims, data, attrs, encoding)
277 
278         Additionally, it is possible to define a coord whose name
279         does not match the dimension name, or a coord based on multiple
280         dimensions, with one of the following notations:
281 
282         - mapping {coord name: DataArray}
283         - mapping {coord name: Variable}
284         - mapping {coord name: (dimension name, array-like)}
285         - mapping {coord name: (tuple of dimension names, array-like)}
286 
287     dims : Hashable or sequence of Hashable, optional
288         Name(s) of the data dimension(s). Must be either a Hashable
289         (only for 1D data) or a sequence of Hashables with length equal
290         to the number of dimensions. If this argument is omitted,
291         dimension names are taken from ``coords`` (if possible) and
292         otherwise default to ``['dim_0', ... 'dim_n']``.
293     name : str or None, optional
294         Name of this array.
295     attrs : dict_like or None, optional
296         Attributes to assign to the new instance. By default, an empty
297         attribute dictionary is initialized.
298 
299     Examples
300     --------
301     Create data:
302 
303     >>> np.random.seed(0)
304     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
305     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
306     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
307     >>> time = pd.date_range("2014-09-06", periods=3)
308     >>> reference_time = pd.Timestamp("2014-09-05")
309 
310     Initialize a dataarray with multiple dimensions:
311 
312     >>> da = xr.DataArray(
313     ...     data=temperature,
314     ...     dims=["x", "y", "time"],
315     ...     coords=dict(
316     ...         lon=(["x", "y"], lon),
317     ...         lat=(["x", "y"], lat),
318     ...         time=time,
319     ...         reference_time=reference_time,
320     ...     ),
321     ...     attrs=dict(
322     ...         description="Ambient temperature.",
323     ...         units="degC",
324     ...     ),
325     ... )
326     >>> da
327     <xarray.DataArray (x: 2, y: 2, time: 3)>
328     array([[[29.11241877, 18.20125767, 22.82990387],
329             [32.92714559, 29.94046392,  7.18177696]],
330     <BLANKLINE>
331            [[22.60070734, 13.78914233, 14.17424919],
332             [18.28478802, 16.15234857, 26.63418806]]])
333     Coordinates:
334         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
335         lat             (x, y) float64 42.25 42.21 42.63 42.59
336       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
337         reference_time  datetime64[ns] 2014-09-05
338     Dimensions without coordinates: x, y
339     Attributes:
340         description:  Ambient temperature.
341         units:        degC
342 
343     Find out where the coldest temperature was:
344 
345     >>> da.isel(da.argmin(...))
346     <xarray.DataArray ()>
347     array(7.18177696)
348     Coordinates:
349         lon             float64 -99.32
350         lat             float64 42.21
351         time            datetime64[ns] 2014-09-08
352         reference_time  datetime64[ns] 2014-09-05
353     Attributes:
354         description:  Ambient temperature.
355         units:        degC
356     """
357 
358     _cache: dict[str, Any]
359     _coords: dict[Any, Variable]
360     _close: Callable[[], None] | None
361     _indexes: dict[Hashable, Index]
362     _name: Hashable | None
363     _variable: Variable
364 
365     __slots__ = (
366         "_cache",
367         "_coords",
368         "_close",
369         "_indexes",
370         "_name",
371         "_variable",
372         "__weakref__",
373     )
374 
375     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor["DataArray"])
376 
377     def __init__(
378         self,
379         data: Any = dtypes.NA,
380         coords: Sequence[Sequence[Any] | pd.Index | DataArray]
381         | Mapping[Any, Any]
382         | None = None,
383         dims: Hashable | Sequence[Hashable] | None = None,
384         name: Hashable | None = None,
385         attrs: Mapping | None = None,
386         # internal parameters
387         indexes: dict[Hashable, Index] | None = None,
388         fastpath: bool = False,
389     ) -> None:
390         if fastpath:
391             variable = data
392             assert dims is None
393             assert attrs is None
394             assert indexes is not None
395         else:
396             # TODO: (benbovy - explicit indexes) remove
397             # once it becomes part of the public interface
398             if indexes is not None:
399                 raise ValueError("Providing explicit indexes is not supported yet")
400 
401             # try to fill in arguments from data if they weren't supplied
402             if coords is None:
403                 if isinstance(data, DataArray):
404                     coords = data.coords
405                 elif isinstance(data, pd.Series):
406                     coords = [data.index]
407                 elif isinstance(data, pd.DataFrame):
408                     coords = [data.index, data.columns]
409                 elif isinstance(data, (pd.Index, IndexVariable)):
410                     coords = [data]
411 
412             if dims is None:
413                 dims = getattr(data, "dims", getattr(coords, "dims", None))
414             if name is None:
415                 name = getattr(data, "name", None)
416             if attrs is None and not isinstance(data, PANDAS_TYPES):
417                 attrs = getattr(data, "attrs", None)
418 
419             data = _check_data_shape(data, coords, dims)
420             data = as_compatible_data(data)
421             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
422             variable = Variable(dims, data, attrs, fastpath=True)
423             indexes, coords = _create_indexes_from_coords(coords)
424 
425         # These fully describe a DataArray
426         self._variable = variable
427         assert isinstance(coords, dict)
428         self._coords = coords
429         self._name = name
430 
431         # TODO(shoyer): document this argument, once it becomes part of the
432         # public interface.
433         self._indexes = indexes
434 
435         self._close = None
436 
437     @classmethod
438     def _construct_direct(
439         cls: type[T_DataArray],
440         variable: Variable,
441         coords: dict[Any, Variable],
442         name: Hashable,
443         indexes: dict[Hashable, Index],
444     ) -> T_DataArray:
445         """Shortcut around __init__ for internal use when we want to skip
446         costly validation
447         """
448         obj = object.__new__(cls)
449         obj._variable = variable
450         obj._coords = coords
451         obj._name = name
452         obj._indexes = indexes
453         obj._close = None
454         return obj
455 
456     def _replace(
457         self: T_DataArray,
458         variable: Variable | None = None,
459         coords=None,
460         name: Hashable | None | Default = _default,
461         indexes=None,
462     ) -> T_DataArray:
463         if variable is None:
464             variable = self.variable
465         if coords is None:
466             coords = self._coords
467         if indexes is None:
468             indexes = self._indexes
469         if name is _default:
470             name = self.name
471         return type(self)(variable, coords, name=name, indexes=indexes, fastpath=True)
472 
473     def _replace_maybe_drop_dims(
474         self: T_DataArray,
475         variable: Variable,
476         name: Hashable | None | Default = _default,
477     ) -> T_DataArray:
478         if variable.dims == self.dims and variable.shape == self.shape:
479             coords = self._coords.copy()
480             indexes = self._indexes
481         elif variable.dims == self.dims:
482             # Shape has changed (e.g. from reduce(..., keepdims=True)
483             new_sizes = dict(zip(self.dims, variable.shape))
484             coords = {
485                 k: v
486                 for k, v in self._coords.items()
487                 if v.shape == tuple(new_sizes[d] for d in v.dims)
488             }
489             indexes = filter_indexes_from_coords(self._indexes, set(coords))
490         else:
491             allowed_dims = set(variable.dims)
492             coords = {
493                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
494             }
495             indexes = filter_indexes_from_coords(self._indexes, set(coords))
496         return self._replace(variable, coords, name, indexes=indexes)
497 
498     def _overwrite_indexes(
499         self: T_DataArray,
500         indexes: Mapping[Any, Index],
501         coords: Mapping[Any, Variable] | None = None,
502         drop_coords: list[Hashable] | None = None,
503         rename_dims: Mapping[Any, Any] | None = None,
504     ) -> T_DataArray:
505         """Maybe replace indexes and their corresponding coordinates."""
506         if not indexes:
507             return self
508 
509         if coords is None:
510             coords = {}
511         if drop_coords is None:
512             drop_coords = []
513 
514         new_variable = self.variable.copy()
515         new_coords = self._coords.copy()
516         new_indexes = dict(self._indexes)
517 
518         for name in indexes:
519             new_coords[name] = coords[name]
520             new_indexes[name] = indexes[name]
521 
522         for name in drop_coords:
523             new_coords.pop(name)
524             new_indexes.pop(name)
525 
526         if rename_dims:
527             new_variable.dims = tuple(rename_dims.get(d, d) for d in new_variable.dims)
528 
529         return self._replace(
530             variable=new_variable, coords=new_coords, indexes=new_indexes
531         )
532 
533     def _to_temp_dataset(self) -> Dataset:
534         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
535 
536     def _from_temp_dataset(
537         self: T_DataArray, dataset: Dataset, name: Hashable | None | Default = _default
538     ) -> T_DataArray:
539         variable = dataset._variables.pop(_THIS_ARRAY)
540         coords = dataset._variables
541         indexes = dataset._indexes
542         return self._replace(variable, coords, name, indexes=indexes)
543 
544     def _to_dataset_split(self, dim: Hashable) -> Dataset:
545         """splits dataarray along dimension 'dim'"""
546 
547         def subset(dim, label):
548             array = self.loc[{dim: label}]
549             array.attrs = {}
550             return as_variable(array)
551 
552         variables = {label: subset(dim, label) for label in self.get_index(dim)}
553         variables.update({k: v for k, v in self._coords.items() if k != dim})
554         coord_names = set(self._coords) - {dim}
555         indexes = filter_indexes_from_coords(self._indexes, coord_names)
556         dataset = Dataset._construct_direct(
557             variables, coord_names, indexes=indexes, attrs=self.attrs
558         )
559         return dataset
560 
561     def _to_dataset_whole(
562         self, name: Hashable = None, shallow_copy: bool = True
563     ) -> Dataset:
564         if name is None:
565             name = self.name
566         if name is None:
567             raise ValueError(
568                 "unable to convert unnamed DataArray to a "
569                 "Dataset without providing an explicit name"
570             )
571         if name in self.coords:
572             raise ValueError(
573                 "cannot create a Dataset from a DataArray with "
574                 "the same name as one of its coordinates"
575             )
576         # use private APIs for speed: this is called by _to_temp_dataset(),
577         # which is used in the guts of a lot of operations (e.g., reindex)
578         variables = self._coords.copy()
579         variables[name] = self.variable
580         if shallow_copy:
581             for k in variables:
582                 variables[k] = variables[k].copy(deep=False)
583         indexes = self._indexes
584 
585         coord_names = set(self._coords)
586         return Dataset._construct_direct(variables, coord_names, indexes=indexes)
587 
588     def to_dataset(
589         self,
590         dim: Hashable = None,
591         *,
592         name: Hashable = None,
593         promote_attrs: bool = False,
594     ) -> Dataset:
595         """Convert a DataArray to a Dataset.
596 
597         Parameters
598         ----------
599         dim : Hashable, optional
600             Name of the dimension on this array along which to split this array
601             into separate variables. If not provided, this array is converted
602             into a Dataset of one variable.
603         name : Hashable, optional
604             Name to substitute for this array's name. Only valid if ``dim`` is
605             not provided.
606         promote_attrs : bool, default: False
607             Set to True to shallow copy attrs of DataArray to returned Dataset.
608 
609         Returns
610         -------
611         dataset : Dataset
612         """
613         if dim is not None and dim not in self.dims:
614             raise TypeError(
615                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
616             )
617 
618         if dim is not None:
619             if name is not None:
620                 raise TypeError("cannot supply both dim and name arguments")
621             result = self._to_dataset_split(dim)
622         else:
623             result = self._to_dataset_whole(name)
624 
625         if promote_attrs:
626             result.attrs = dict(self.attrs)
627 
628         return result
629 
630     @property
631     def name(self) -> Hashable | None:
632         """The name of this array."""
633         return self._name
634 
635     @name.setter
636     def name(self, value: Hashable | None) -> None:
637         self._name = value
638 
639     @property
640     def variable(self) -> Variable:
641         """Low level interface to the Variable object for this DataArray."""
642         return self._variable
643 
644     @property
645     def dtype(self) -> np.dtype:
646         """
647         Data-type of the array’s elements.
648 
649         See Also
650         --------
651         ndarray.dtype
652         numpy.dtype
653         """
654         return self.variable.dtype
655 
656     @property
657     def shape(self) -> tuple[int, ...]:
658         """
659         Tuple of array dimensions.
660 
661         See Also
662         --------
663         numpy.ndarray.shape
664         """
665         return self.variable.shape
666 
667     @property
668     def size(self) -> int:
669         """
670         Number of elements in the array.
671 
672         Equal to ``np.prod(a.shape)``, i.e., the product of the array’s dimensions.
673 
674         See Also
675         --------
676         numpy.ndarray.size
677         """
678         return self.variable.size
679 
680     @property
681     def nbytes(self) -> int:
682         """
683         Total bytes consumed by the elements of this DataArray's data.
684 
685         If the underlying data array does not include ``nbytes``, estimates
686         the bytes consumed based on the ``size`` and ``dtype``.
687         """
688         return self.variable.nbytes
689 
690     @property
691     def ndim(self) -> int:
692         """
693         Number of array dimensions.
694 
695         See Also
696         --------
697         numpy.ndarray.ndim
698         """
699         return self.variable.ndim
700 
701     def __len__(self) -> int:
702         return len(self.variable)
703 
704     @property
705     def data(self) -> Any:
706         """
707         The DataArray's data as an array. The underlying array type
708         (e.g. dask, sparse, pint) is preserved.
709 
710         See Also
711         --------
712         DataArray.to_numpy
713         DataArray.as_numpy
714         DataArray.values
715         """
716         return self.variable.data
717 
718     @data.setter
719     def data(self, value: Any) -> None:
720         self.variable.data = value
721 
722     @property
723     def values(self) -> np.ndarray:
724         """
725         The array's data as a numpy.ndarray.
726 
727         If the array's data is not a numpy.ndarray this will attempt to convert
728         it naively using np.array(), which will raise an error if the array
729         type does not support coercion like this (e.g. cupy).
730         """
731         return self.variable.values
732 
733     @values.setter
734     def values(self, value: Any) -> None:
735         self.variable.values = value
736 
737     def to_numpy(self) -> np.ndarray:
738         """
739         Coerces wrapped data to numpy and returns a numpy.ndarray.
740 
741         See Also
742         --------
743         DataArray.as_numpy : Same but returns the surrounding DataArray instead.
744         Dataset.as_numpy
745         DataArray.values
746         DataArray.data
747         """
748         return self.variable.to_numpy()
749 
750     def as_numpy(self: T_DataArray) -> T_DataArray:
751         """
752         Coerces wrapped data and coordinates into numpy arrays, returning a DataArray.
753 
754         See Also
755         --------
756         DataArray.to_numpy : Same but returns only the data as a numpy.ndarray object.
757         Dataset.as_numpy : Converts all variables in a Dataset.
758         DataArray.values
759         DataArray.data
760         """
761         coords = {k: v.as_numpy() for k, v in self._coords.items()}
762         return self._replace(self.variable.as_numpy(), coords, indexes=self._indexes)
763 
764     @property
765     def _in_memory(self) -> bool:
766         return self.variable._in_memory
767 
768     def _to_index(self) -> pd.Index:
769         return self.variable._to_index()
770 
771     def to_index(self) -> pd.Index:
772         """Convert this variable to a pandas.Index. Only possible for 1D
773         arrays.
774         """
775         return self.variable.to_index()
776 
777     @property
778     def dims(self) -> tuple[Hashable, ...]:
779         """Tuple of dimension names associated with this array.
780 
781         Note that the type of this property is inconsistent with
782         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
783         consistently named properties.
784 
785         See Also
786         --------
787         DataArray.sizes
788         Dataset.dims
789         """
790         return self.variable.dims
791 
792     @dims.setter
793     def dims(self, value: Any) -> NoReturn:
794         raise AttributeError(
795             "you cannot assign dims on a DataArray. Use "
796             ".rename() or .swap_dims() instead."
797         )
798 
799     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
800         if utils.is_dict_like(key):
801             return key
802         key = indexing.expanded_indexer(key, self.ndim)
803         return dict(zip(self.dims, key))
804 
805     def _getitem_coord(self: T_DataArray, key: Any) -> T_DataArray:
806         from xarray.core.dataset import _get_virtual_variable
807 
808         try:
809             var = self._coords[key]
810         except KeyError:
811             dim_sizes = dict(zip(self.dims, self.shape))
812             _, key, var = _get_virtual_variable(self._coords, key, dim_sizes)
813 
814         return self._replace_maybe_drop_dims(var, name=key)
815 
816     def __getitem__(self: T_DataArray, key: Any) -> T_DataArray:
817         if isinstance(key, str):
818             return self._getitem_coord(key)
819         else:
820             # xarray-style array indexing
821             return self.isel(indexers=self._item_key_to_dict(key))
822 
823     def __setitem__(self, key: Any, value: Any) -> None:
824         if isinstance(key, str):
825             self.coords[key] = value
826         else:
827             # Coordinates in key, value and self[key] should be consistent.
828             # TODO Coordinate consistency in key is checked here, but it
829             # causes unnecessary indexing. It should be optimized.
830             obj = self[key]
831             if isinstance(value, DataArray):
832                 assert_coordinate_consistent(value, obj.coords.variables)
833             # DataArray key -> Variable key
834             key = {
835                 k: v.variable if isinstance(v, DataArray) else v
836                 for k, v in self._item_key_to_dict(key).items()
837             }
838             self.variable[key] = value
839 
840     def __delitem__(self, key: Any) -> None:
841         del self.coords[key]
842 
843     @property
844     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
845         """Places to look-up items for attribute-style access"""
846         yield from self._item_sources
847         yield self.attrs
848 
849     @property
850     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
851         """Places to look-up items for key-completion"""
852         yield HybridMappingProxy(keys=self._coords, mapping=self.coords)
853 
854         # virtual coordinates
855         # uses empty dict -- everything here can already be found in self.coords.
856         yield HybridMappingProxy(keys=self.dims, mapping={})
857 
858     def __contains__(self, key: Any) -> bool:
859         return key in self.data
860 
861     @property
862     def loc(self) -> _LocIndexer:
863         """Attribute for location based indexing like pandas."""
864         return _LocIndexer(self)
865 
866     @property
867     def attrs(self) -> dict[Any, Any]:
868         """Dictionary storing arbitrary metadata with this array."""
869         return self.variable.attrs
870 
871     @attrs.setter
872     def attrs(self, value: Mapping[Any, Any]) -> None:
873         self.variable.attrs = dict(value)
874 
875     @property
876     def encoding(self) -> dict[Any, Any]:
877         """Dictionary of format-specific settings for how this array should be
878         serialized."""
879         return self.variable.encoding
880 
881     @encoding.setter
882     def encoding(self, value: Mapping[Any, Any]) -> None:
883         self.variable.encoding = dict(value)
884 
885     def reset_encoding(self: T_DataArray) -> T_DataArray:
886         """Return a new DataArray without encoding on the array or any attached
887         coords."""
888         ds = self._to_temp_dataset().reset_encoding()
889         return self._from_temp_dataset(ds)
890 
891     @property
892     def indexes(self) -> Indexes:
893         """Mapping of pandas.Index objects used for label based indexing.
894 
895         Raises an error if this Dataset has indexes that cannot be coerced
896         to pandas.Index objects.
897 
898         See Also
899         --------
900         DataArray.xindexes
901 
902         """
903         return self.xindexes.to_pandas_indexes()
904 
905     @property
906     def xindexes(self) -> Indexes:
907         """Mapping of xarray Index objects used for label based indexing."""
908         return Indexes(self._indexes, {k: self._coords[k] for k in self._indexes})
909 
910     @property
911     def coords(self) -> DataArrayCoordinates:
912         """Dictionary-like container of coordinate arrays."""
913         return DataArrayCoordinates(self)
914 
915     @overload
916     def reset_coords(
917         self: T_DataArray,
918         names: Dims = None,
919         drop: Literal[False] = False,
920     ) -> Dataset:
921         ...
922 
923     @overload
924     def reset_coords(
925         self: T_DataArray,
926         names: Dims = None,
927         *,
928         drop: Literal[True],
929     ) -> T_DataArray:
930         ...
931 
932     def reset_coords(
933         self: T_DataArray,
934         names: Dims = None,
935         drop: bool = False,
936     ) -> T_DataArray | Dataset:
937         """Given names of coordinates, reset them to become variables.
938 
939         Parameters
940         ----------
941         names : str, Iterable of Hashable or None, optional
942             Name(s) of non-index coordinates in this dataset to reset into
943             variables. By default, all non-index coordinates are reset.
944         drop : bool, default: False
945             If True, remove coordinates instead of converting them into
946             variables.
947 
948         Returns
949         -------
950         Dataset, or DataArray if ``drop == True``
951 
952         Examples
953         --------
954         >>> temperature = np.arange(25).reshape(5, 5)
955         >>> pressure = np.arange(50, 75).reshape(5, 5)
956         >>> da = xr.DataArray(
957         ...     data=temperature,
958         ...     dims=["x", "y"],
959         ...     coords=dict(
960         ...         lon=("x", np.arange(10, 15)),
961         ...         lat=("y", np.arange(20, 25)),
962         ...         Pressure=(["x", "y"], pressure),
963         ...     ),
964         ...     name="Temperature",
965         ... )
966         >>> da
967         <xarray.DataArray 'Temperature' (x: 5, y: 5)>
968         array([[ 0,  1,  2,  3,  4],
969                [ 5,  6,  7,  8,  9],
970                [10, 11, 12, 13, 14],
971                [15, 16, 17, 18, 19],
972                [20, 21, 22, 23, 24]])
973         Coordinates:
974             lon       (x) int64 10 11 12 13 14
975             lat       (y) int64 20 21 22 23 24
976             Pressure  (x, y) int64 50 51 52 53 54 55 56 57 ... 67 68 69 70 71 72 73 74
977         Dimensions without coordinates: x, y
978 
979         Return Dataset with target coordinate as a data variable rather than a coordinate variable:
980 
981         >>> da.reset_coords(names="Pressure")
982         <xarray.Dataset>
983         Dimensions:      (x: 5, y: 5)
984         Coordinates:
985             lon          (x) int64 10 11 12 13 14
986             lat          (y) int64 20 21 22 23 24
987         Dimensions without coordinates: x, y
988         Data variables:
989             Pressure     (x, y) int64 50 51 52 53 54 55 56 57 ... 68 69 70 71 72 73 74
990             Temperature  (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24
991 
992         Return DataArray without targeted coordinate:
993 
994         >>> da.reset_coords(names="Pressure", drop=True)
995         <xarray.DataArray 'Temperature' (x: 5, y: 5)>
996         array([[ 0,  1,  2,  3,  4],
997                [ 5,  6,  7,  8,  9],
998                [10, 11, 12, 13, 14],
999                [15, 16, 17, 18, 19],
1000                [20, 21, 22, 23, 24]])
1001         Coordinates:
1002             lon      (x) int64 10 11 12 13 14
1003             lat      (y) int64 20 21 22 23 24
1004         Dimensions without coordinates: x, y
1005         """
1006         if names is None:
1007             names = set(self.coords) - set(self._indexes)
1008         dataset = self.coords.to_dataset().reset_coords(names, drop)
1009         if drop:
1010             return self._replace(coords=dataset._variables)
1011         if self.name is None:
1012             raise ValueError(
1013                 "cannot reset_coords with drop=False on an unnamed DataArrray"
1014             )
1015         dataset[self.name] = self.variable
1016         return dataset
1017 
1018     def __dask_tokenize__(self):
1019         from dask.base import normalize_token
1020 
1021         return normalize_token((type(self), self._variable, self._coords, self._name))
1022 
1023     def __dask_graph__(self):
1024         return self._to_temp_dataset().__dask_graph__()
1025 
1026     def __dask_keys__(self):
1027         return self._to_temp_dataset().__dask_keys__()
1028 
1029     def __dask_layers__(self):
1030         return self._to_temp_dataset().__dask_layers__()
1031 
1032     @property
1033     def __dask_optimize__(self):
1034         return self._to_temp_dataset().__dask_optimize__
1035 
1036     @property
1037     def __dask_scheduler__(self):
1038         return self._to_temp_dataset().__dask_scheduler__
1039 
1040     def __dask_postcompute__(self):
1041         func, args = self._to_temp_dataset().__dask_postcompute__()
1042         return self._dask_finalize, (self.name, func) + args
1043 
1044     def __dask_postpersist__(self):
1045         func, args = self._to_temp_dataset().__dask_postpersist__()
1046         return self._dask_finalize, (self.name, func) + args
1047 
1048     @staticmethod
1049     def _dask_finalize(results, name, func, *args, **kwargs) -> DataArray:
1050         ds = func(results, *args, **kwargs)
1051         variable = ds._variables.pop(_THIS_ARRAY)
1052         coords = ds._variables
1053         indexes = ds._indexes
1054         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1055 
1056     def load(self: T_DataArray, **kwargs) -> T_DataArray:
1057         """Manually trigger loading of this array's data from disk or a
1058         remote source into memory and return this array.
1059 
1060         Normally, it should not be necessary to call this method in user code,
1061         because all xarray functions should either work on deferred data or
1062         load data automatically. However, this method can be necessary when
1063         working with many file objects on disk.
1064 
1065         Parameters
1066         ----------
1067         **kwargs : dict
1068             Additional keyword arguments passed on to ``dask.compute``.
1069 
1070         See Also
1071         --------
1072         dask.compute
1073         """
1074         ds = self._to_temp_dataset().load(**kwargs)
1075         new = self._from_temp_dataset(ds)
1076         self._variable = new._variable
1077         self._coords = new._coords
1078         return self
1079 
1080     def compute(self: T_DataArray, **kwargs) -> T_DataArray:
1081         """Manually trigger loading of this array's data from disk or a
1082         remote source into memory and return a new array. The original is
1083         left unaltered.
1084 
1085         Normally, it should not be necessary to call this method in user code,
1086         because all xarray functions should either work on deferred data or
1087         load data automatically. However, this method can be necessary when
1088         working with many file objects on disk.
1089 
1090         Parameters
1091         ----------
1092         **kwargs : dict
1093             Additional keyword arguments passed on to ``dask.compute``.
1094 
1095         See Also
1096         --------
1097         dask.compute
1098         """
1099         new = self.copy(deep=False)
1100         return new.load(**kwargs)
1101 
1102     def persist(self: T_DataArray, **kwargs) -> T_DataArray:
1103         """Trigger computation in constituent dask arrays
1104 
1105         This keeps them as dask arrays but encourages them to keep data in
1106         memory.  This is particularly useful when on a distributed machine.
1107         When on a single machine consider using ``.compute()`` instead.
1108 
1109         Parameters
1110         ----------
1111         **kwargs : dict
1112             Additional keyword arguments passed on to ``dask.persist``.
1113 
1114         See Also
1115         --------
1116         dask.persist
1117         """
1118         ds = self._to_temp_dataset().persist(**kwargs)
1119         return self._from_temp_dataset(ds)
1120 
1121     def copy(self: T_DataArray, deep: bool = True, data: Any = None) -> T_DataArray:
1122         """Returns a copy of this array.
1123 
1124         If `deep=True`, a deep copy is made of the data array.
1125         Otherwise, a shallow copy is made, and the returned data array's
1126         values are a new view of this data array's values.
1127 
1128         Use `data` to create a new object with the same structure as
1129         original but entirely new data.
1130 
1131         Parameters
1132         ----------
1133         deep : bool, optional
1134             Whether the data array and its coordinates are loaded into memory
1135             and copied onto the new object. Default is True.
1136         data : array_like, optional
1137             Data to use in the new object. Must have same shape as original.
1138             When `data` is used, `deep` is ignored for all data variables,
1139             and only used for coords.
1140 
1141         Returns
1142         -------
1143         copy : DataArray
1144             New object with dimensions, attributes, coordinates, name,
1145             encoding, and optionally data copied from original.
1146 
1147         Examples
1148         --------
1149         Shallow versus deep copy
1150 
1151         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
1152         >>> array.copy()
1153         <xarray.DataArray (x: 3)>
1154         array([1, 2, 3])
1155         Coordinates:
1156           * x        (x) <U1 'a' 'b' 'c'
1157         >>> array_0 = array.copy(deep=False)
1158         >>> array_0[0] = 7
1159         >>> array_0
1160         <xarray.DataArray (x: 3)>
1161         array([7, 2, 3])
1162         Coordinates:
1163           * x        (x) <U1 'a' 'b' 'c'
1164         >>> array
1165         <xarray.DataArray (x: 3)>
1166         array([7, 2, 3])
1167         Coordinates:
1168           * x        (x) <U1 'a' 'b' 'c'
1169 
1170         Changing the data using the ``data`` argument maintains the
1171         structure of the original object, but with the new data. Original
1172         object is unaffected.
1173 
1174         >>> array.copy(data=[0.1, 0.2, 0.3])
1175         <xarray.DataArray (x: 3)>
1176         array([0.1, 0.2, 0.3])
1177         Coordinates:
1178           * x        (x) <U1 'a' 'b' 'c'
1179         >>> array
1180         <xarray.DataArray (x: 3)>
1181         array([7, 2, 3])
1182         Coordinates:
1183           * x        (x) <U1 'a' 'b' 'c'
1184 
1185         See Also
1186         --------
1187         pandas.DataFrame.copy
1188         """
1189         return self._copy(deep=deep, data=data)
1190 
1191     def _copy(
1192         self: T_DataArray,
1193         deep: bool = True,
1194         data: Any = None,
1195         memo: dict[int, Any] | None = None,
1196     ) -> T_DataArray:
1197         variable = self.variable._copy(deep=deep, data=data, memo=memo)
1198         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1199 
1200         coords = {}
1201         for k, v in self._coords.items():
1202             if k in index_vars:
1203                 coords[k] = index_vars[k]
1204             else:
1205                 coords[k] = v._copy(deep=deep, memo=memo)
1206 
1207         return self._replace(variable, coords, indexes=indexes)
1208 
1209     def __copy__(self: T_DataArray) -> T_DataArray:
1210         return self._copy(deep=False)
1211 
1212     def __deepcopy__(
1213         self: T_DataArray, memo: dict[int, Any] | None = None
1214     ) -> T_DataArray:
1215         return self._copy(deep=True, memo=memo)
1216 
1217     # mutable objects should not be Hashable
1218     # https://github.com/python/mypy/issues/4266
1219     __hash__ = None  # type: ignore[assignment]
1220 
1221     @property
1222     def chunks(self) -> tuple[tuple[int, ...], ...] | None:
1223         """
1224         Tuple of block lengths for this dataarray's data, in order of dimensions, or None if
1225         the underlying data is not a dask array.
1226 
1227         See Also
1228         --------
1229         DataArray.chunk
1230         DataArray.chunksizes
1231         xarray.unify_chunks
1232         """
1233         return self.variable.chunks
1234 
1235     @property
1236     def chunksizes(self) -> Mapping[Any, tuple[int, ...]]:
1237         """
1238         Mapping from dimension names to block lengths for this dataarray's data, or None if
1239         the underlying data is not a dask array.
1240         Cannot be modified directly, but can be modified by calling .chunk().
1241 
1242         Differs from DataArray.chunks because it returns a mapping of dimensions to chunk shapes
1243         instead of a tuple of chunk shapes.
1244 
1245         See Also
1246         --------
1247         DataArray.chunk
1248         DataArray.chunks
1249         xarray.unify_chunks
1250         """
1251         all_variables = [self.variable] + [c.variable for c in self.coords.values()]
1252         return get_chunksizes(all_variables)
1253 
1254     def chunk(
1255         self: T_DataArray,
1256         chunks: (
1257             int
1258             | Literal["auto"]
1259             | tuple[int, ...]
1260             | tuple[tuple[int, ...], ...]
1261             | Mapping[Any, None | int | tuple[int, ...]]
1262         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1263         name_prefix: str = "xarray-",
1264         token: str | None = None,
1265         lock: bool = False,
1266         inline_array: bool = False,
1267         **chunks_kwargs: Any,
1268     ) -> T_DataArray:
1269         """Coerce this array's data into a dask arrays with the given chunks.
1270 
1271         If this variable is a non-dask array, it will be converted to dask
1272         array. If it's a dask array, it will be rechunked to the given chunk
1273         sizes.
1274 
1275         If neither chunks is not provided for one or more dimensions, chunk
1276         sizes along that dimension will not be updated; non-dask arrays will be
1277         converted into dask arrays with a single block.
1278 
1279         Parameters
1280         ----------
1281         chunks : int, "auto", tuple of int or mapping of Hashable to int, optional
1282             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, ``(5, 5)`` or
1283             ``{"x": 5, "y": 5}``.
1284         name_prefix : str, optional
1285             Prefix for the name of the new dask array.
1286         token : str, optional
1287             Token uniquely identifying this array.
1288         lock : optional
1289             Passed on to :py:func:`dask.array.from_array`, if the array is not
1290             already as dask array.
1291         inline_array: optional
1292             Passed on to :py:func:`dask.array.from_array`, if the array is not
1293             already as dask array.
1294         **chunks_kwargs : {dim: chunks, ...}, optional
1295             The keyword arguments form of ``chunks``.
1296             One of chunks or chunks_kwargs must be provided.
1297 
1298         Returns
1299         -------
1300         chunked : xarray.DataArray
1301 
1302         See Also
1303         --------
1304         DataArray.chunks
1305         DataArray.chunksizes
1306         xarray.unify_chunks
1307         dask.array.from_array
1308         """
1309         if chunks is None:
1310             warnings.warn(
1311                 "None value for 'chunks' is deprecated. "
1312                 "It will raise an error in the future. Use instead '{}'",
1313                 category=FutureWarning,
1314             )
1315             chunks = {}
1316 
1317         if isinstance(chunks, (float, str, int)):
1318             # ignoring type; unclear why it won't accept a Literal into the value.
1319             chunks = dict.fromkeys(self.dims, chunks)  # type: ignore
1320         elif isinstance(chunks, (tuple, list)):
1321             chunks = dict(zip(self.dims, chunks))
1322         else:
1323             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
1324 
1325         ds = self._to_temp_dataset().chunk(
1326             chunks,
1327             name_prefix=name_prefix,
1328             token=token,
1329             lock=lock,
1330             inline_array=inline_array,
1331         )
1332         return self._from_temp_dataset(ds)
1333 
1334     def isel(
1335         self: T_DataArray,
1336         indexers: Mapping[Any, Any] | None = None,
1337         drop: bool = False,
1338         missing_dims: ErrorOptionsWithWarn = "raise",
1339         **indexers_kwargs: Any,
1340     ) -> T_DataArray:
1341         """Return a new DataArray whose data is given by selecting indexes
1342         along the specified dimension(s).
1343 
1344         Parameters
1345         ----------
1346         indexers : dict, optional
1347             A dict with keys matching dimensions and values given
1348             by integers, slice objects or arrays.
1349             indexer can be a integer, slice, array-like or DataArray.
1350             If DataArrays are passed as indexers, xarray-style indexing will be
1351             carried out. See :ref:`indexing` for the details.
1352             One of indexers or indexers_kwargs must be provided.
1353         drop : bool, default: False
1354             If ``drop=True``, drop coordinates variables indexed by integers
1355             instead of making them scalar.
1356         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1357             What to do if dimensions that should be selected from are not present in the
1358             DataArray:
1359             - "raise": raise an exception
1360             - "warn": raise a warning, and ignore the missing dimensions
1361             - "ignore": ignore the missing dimensions
1362         **indexers_kwargs : {dim: indexer, ...}, optional
1363             The keyword arguments form of ``indexers``.
1364 
1365         Returns
1366         -------
1367         indexed : xarray.DataArray
1368 
1369         See Also
1370         --------
1371         Dataset.isel
1372         DataArray.sel
1373 
1374         Examples
1375         --------
1376         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("x", "y"))
1377         >>> da
1378         <xarray.DataArray (x: 5, y: 5)>
1379         array([[ 0,  1,  2,  3,  4],
1380                [ 5,  6,  7,  8,  9],
1381                [10, 11, 12, 13, 14],
1382                [15, 16, 17, 18, 19],
1383                [20, 21, 22, 23, 24]])
1384         Dimensions without coordinates: x, y
1385 
1386         >>> tgt_x = xr.DataArray(np.arange(0, 5), dims="points")
1387         >>> tgt_y = xr.DataArray(np.arange(0, 5), dims="points")
1388         >>> da = da.isel(x=tgt_x, y=tgt_y)
1389         >>> da
1390         <xarray.DataArray (points: 5)>
1391         array([ 0,  6, 12, 18, 24])
1392         Dimensions without coordinates: points
1393         """
1394 
1395         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1396 
1397         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1398             ds = self._to_temp_dataset()._isel_fancy(
1399                 indexers, drop=drop, missing_dims=missing_dims
1400             )
1401             return self._from_temp_dataset(ds)
1402 
1403         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1404         # lists, or zero or one-dimensional np.ndarray's
1405 
1406         variable = self._variable.isel(indexers, missing_dims=missing_dims)
1407         indexes, index_variables = isel_indexes(self.xindexes, indexers)
1408 
1409         coords = {}
1410         for coord_name, coord_value in self._coords.items():
1411             if coord_name in index_variables:
1412                 coord_value = index_variables[coord_name]
1413             else:
1414                 coord_indexers = {
1415                     k: v for k, v in indexers.items() if k in coord_value.dims
1416                 }
1417                 if coord_indexers:
1418                     coord_value = coord_value.isel(coord_indexers)
1419                     if drop and coord_value.ndim == 0:
1420                         continue
1421             coords[coord_name] = coord_value
1422 
1423         return self._replace(variable=variable, coords=coords, indexes=indexes)
1424 
1425     def sel(
1426         self: T_DataArray,
1427         indexers: Mapping[Any, Any] | None = None,
1428         method: str | None = None,
1429         tolerance=None,
1430         drop: bool = False,
1431         **indexers_kwargs: Any,
1432     ) -> T_DataArray:
1433         """Return a new DataArray whose data is given by selecting index
1434         labels along the specified dimension(s).
1435 
1436         In contrast to `DataArray.isel`, indexers for this method should use
1437         labels instead of integers.
1438 
1439         Under the hood, this method is powered by using pandas's powerful Index
1440         objects. This makes label based indexing essentially just as fast as
1441         using integer indexing.
1442 
1443         It also means this method uses pandas's (well documented) logic for
1444         indexing. This means you can use string shortcuts for datetime indexes
1445         (e.g., '2000-01' to select all values in January 2000). It also means
1446         that slices are treated as inclusive of both the start and stop values,
1447         unlike normal Python indexing.
1448 
1449         .. warning::
1450 
1451           Do not try to assign values when using any of the indexing methods
1452           ``isel`` or ``sel``::
1453 
1454             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1455             # DO NOT do this
1456             da.isel(x=[0, 1, 2])[1] = -1
1457 
1458           Assigning values with the chained indexing using ``.sel`` or
1459           ``.isel`` fails silently.
1460 
1461         Parameters
1462         ----------
1463         indexers : dict, optional
1464             A dict with keys matching dimensions and values given
1465             by scalars, slices or arrays of tick labels. For dimensions with
1466             multi-index, the indexer may also be a dict-like object with keys
1467             matching index level names.
1468             If DataArrays are passed as indexers, xarray-style indexing will be
1469             carried out. See :ref:`indexing` for the details.
1470             One of indexers or indexers_kwargs must be provided.
1471         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1472             Method to use for inexact matches:
1473 
1474             - None (default): only exact matches
1475             - pad / ffill: propagate last valid index value forward
1476             - backfill / bfill: propagate next valid index value backward
1477             - nearest: use nearest valid index value
1478 
1479         tolerance : optional
1480             Maximum distance between original and new labels for inexact
1481             matches. The values of the index at the matching locations must
1482             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1483         drop : bool, optional
1484             If ``drop=True``, drop coordinates variables in `indexers` instead
1485             of making them scalar.
1486         **indexers_kwargs : {dim: indexer, ...}, optional
1487             The keyword arguments form of ``indexers``.
1488             One of indexers or indexers_kwargs must be provided.
1489 
1490         Returns
1491         -------
1492         obj : DataArray
1493             A new DataArray with the same contents as this DataArray, except the
1494             data and each dimension is indexed by the appropriate indexers.
1495             If indexer DataArrays have coordinates that do not conflict with
1496             this object, then these coordinates will be attached.
1497             In general, each array's data will be a view of the array's data
1498             in this DataArray, unless vectorized indexing was triggered by using
1499             an array indexer, in which case the data will be a copy.
1500 
1501         See Also
1502         --------
1503         Dataset.sel
1504         DataArray.isel
1505 
1506         Examples
1507         --------
1508         >>> da = xr.DataArray(
1509         ...     np.arange(25).reshape(5, 5),
1510         ...     coords={"x": np.arange(5), "y": np.arange(5)},
1511         ...     dims=("x", "y"),
1512         ... )
1513         >>> da
1514         <xarray.DataArray (x: 5, y: 5)>
1515         array([[ 0,  1,  2,  3,  4],
1516                [ 5,  6,  7,  8,  9],
1517                [10, 11, 12, 13, 14],
1518                [15, 16, 17, 18, 19],
1519                [20, 21, 22, 23, 24]])
1520         Coordinates:
1521           * x        (x) int64 0 1 2 3 4
1522           * y        (y) int64 0 1 2 3 4
1523 
1524         >>> tgt_x = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1525         >>> tgt_y = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1526         >>> da = da.sel(x=tgt_x, y=tgt_y, method="nearest")
1527         >>> da
1528         <xarray.DataArray (points: 5)>
1529         array([ 0,  6, 12, 18, 24])
1530         Coordinates:
1531             x        (points) int64 0 1 2 3 4
1532             y        (points) int64 0 1 2 3 4
1533         Dimensions without coordinates: points
1534         """
1535         ds = self._to_temp_dataset().sel(
1536             indexers=indexers,
1537             drop=drop,
1538             method=method,
1539             tolerance=tolerance,
1540             **indexers_kwargs,
1541         )
1542         return self._from_temp_dataset(ds)
1543 
1544     def head(
1545         self: T_DataArray,
1546         indexers: Mapping[Any, int] | int | None = None,
1547         **indexers_kwargs: Any,
1548     ) -> T_DataArray:
1549         """Return a new DataArray whose data is given by the the first `n`
1550         values along the specified dimension(s). Default `n` = 5
1551 
1552         See Also
1553         --------
1554         Dataset.head
1555         DataArray.tail
1556         DataArray.thin
1557 
1558         Examples
1559         --------
1560         >>> da = xr.DataArray(
1561         ...     np.arange(25).reshape(5, 5),
1562         ...     dims=("x", "y"),
1563         ... )
1564         >>> da
1565         <xarray.DataArray (x: 5, y: 5)>
1566         array([[ 0,  1,  2,  3,  4],
1567                [ 5,  6,  7,  8,  9],
1568                [10, 11, 12, 13, 14],
1569                [15, 16, 17, 18, 19],
1570                [20, 21, 22, 23, 24]])
1571         Dimensions without coordinates: x, y
1572 
1573         >>> da.head(x=1)
1574         <xarray.DataArray (x: 1, y: 5)>
1575         array([[0, 1, 2, 3, 4]])
1576         Dimensions without coordinates: x, y
1577 
1578         >>> da.head({"x": 2, "y": 2})
1579         <xarray.DataArray (x: 2, y: 2)>
1580         array([[0, 1],
1581                [5, 6]])
1582         Dimensions without coordinates: x, y
1583         """
1584         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1585         return self._from_temp_dataset(ds)
1586 
1587     def tail(
1588         self: T_DataArray,
1589         indexers: Mapping[Any, int] | int | None = None,
1590         **indexers_kwargs: Any,
1591     ) -> T_DataArray:
1592         """Return a new DataArray whose data is given by the the last `n`
1593         values along the specified dimension(s). Default `n` = 5
1594 
1595         See Also
1596         --------
1597         Dataset.tail
1598         DataArray.head
1599         DataArray.thin
1600 
1601         Examples
1602         --------
1603         >>> da = xr.DataArray(
1604         ...     np.arange(25).reshape(5, 5),
1605         ...     dims=("x", "y"),
1606         ... )
1607         >>> da
1608         <xarray.DataArray (x: 5, y: 5)>
1609         array([[ 0,  1,  2,  3,  4],
1610                [ 5,  6,  7,  8,  9],
1611                [10, 11, 12, 13, 14],
1612                [15, 16, 17, 18, 19],
1613                [20, 21, 22, 23, 24]])
1614         Dimensions without coordinates: x, y
1615 
1616         >>> da.tail(y=1)
1617         <xarray.DataArray (x: 5, y: 1)>
1618         array([[ 4],
1619                [ 9],
1620                [14],
1621                [19],
1622                [24]])
1623         Dimensions without coordinates: x, y
1624 
1625         >>> da.tail({"x": 2, "y": 2})
1626         <xarray.DataArray (x: 2, y: 2)>
1627         array([[18, 19],
1628                [23, 24]])
1629         Dimensions without coordinates: x, y
1630         """
1631         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1632         return self._from_temp_dataset(ds)
1633 
1634     def thin(
1635         self: T_DataArray,
1636         indexers: Mapping[Any, int] | int | None = None,
1637         **indexers_kwargs: Any,
1638     ) -> T_DataArray:
1639         """Return a new DataArray whose data is given by each `n` value
1640         along the specified dimension(s).
1641 
1642         Examples
1643         --------
1644         >>> x_arr = np.arange(0, 26)
1645         >>> x_arr
1646         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
1647                17, 18, 19, 20, 21, 22, 23, 24, 25])
1648         >>> x = xr.DataArray(
1649         ...     np.reshape(x_arr, (2, 13)),
1650         ...     dims=("x", "y"),
1651         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
1652         ... )
1653         >>> x
1654         <xarray.DataArray (x: 2, y: 13)>
1655         array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],
1656                [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]])
1657         Coordinates:
1658           * x        (x) int64 0 1
1659           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
1660 
1661         >>>
1662         >>> x.thin(3)
1663         <xarray.DataArray (x: 1, y: 5)>
1664         array([[ 0,  3,  6,  9, 12]])
1665         Coordinates:
1666           * x        (x) int64 0
1667           * y        (y) int64 0 3 6 9 12
1668         >>> x.thin({"x": 2, "y": 5})
1669         <xarray.DataArray (x: 1, y: 3)>
1670         array([[ 0,  5, 10]])
1671         Coordinates:
1672           * x        (x) int64 0
1673           * y        (y) int64 0 5 10
1674 
1675         See Also
1676         --------
1677         Dataset.thin
1678         DataArray.head
1679         DataArray.tail
1680         """
1681         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1682         return self._from_temp_dataset(ds)
1683 
1684     def broadcast_like(
1685         self: T_DataArray,
1686         other: DataArray | Dataset,
1687         exclude: Iterable[Hashable] | None = None,
1688     ) -> T_DataArray:
1689         """Broadcast this DataArray against another Dataset or DataArray.
1690 
1691         This is equivalent to xr.broadcast(other, self)[1]
1692 
1693         xarray objects are broadcast against each other in arithmetic
1694         operations, so this method is not be necessary for most uses.
1695 
1696         If no change is needed, the input data is returned to the output
1697         without being copied.
1698 
1699         If new coords are added by the broadcast, their values are
1700         NaN filled.
1701 
1702         Parameters
1703         ----------
1704         other : Dataset or DataArray
1705             Object against which to broadcast this array.
1706         exclude : iterable of Hashable, optional
1707             Dimensions that must not be broadcasted
1708 
1709         Returns
1710         -------
1711         new_da : DataArray
1712             The caller broadcasted against ``other``.
1713 
1714         Examples
1715         --------
1716         >>> arr1 = xr.DataArray(
1717         ...     np.random.randn(2, 3),
1718         ...     dims=("x", "y"),
1719         ...     coords={"x": ["a", "b"], "y": ["a", "b", "c"]},
1720         ... )
1721         >>> arr2 = xr.DataArray(
1722         ...     np.random.randn(3, 2),
1723         ...     dims=("x", "y"),
1724         ...     coords={"x": ["a", "b", "c"], "y": ["a", "b"]},
1725         ... )
1726         >>> arr1
1727         <xarray.DataArray (x: 2, y: 3)>
1728         array([[ 1.76405235,  0.40015721,  0.97873798],
1729                [ 2.2408932 ,  1.86755799, -0.97727788]])
1730         Coordinates:
1731           * x        (x) <U1 'a' 'b'
1732           * y        (y) <U1 'a' 'b' 'c'
1733         >>> arr2
1734         <xarray.DataArray (x: 3, y: 2)>
1735         array([[ 0.95008842, -0.15135721],
1736                [-0.10321885,  0.4105985 ],
1737                [ 0.14404357,  1.45427351]])
1738         Coordinates:
1739           * x        (x) <U1 'a' 'b' 'c'
1740           * y        (y) <U1 'a' 'b'
1741         >>> arr1.broadcast_like(arr2)
1742         <xarray.DataArray (x: 3, y: 3)>
1743         array([[ 1.76405235,  0.40015721,  0.97873798],
1744                [ 2.2408932 ,  1.86755799, -0.97727788],
1745                [        nan,         nan,         nan]])
1746         Coordinates:
1747           * x        (x) <U1 'a' 'b' 'c'
1748           * y        (y) <U1 'a' 'b' 'c'
1749         """
1750         if exclude is None:
1751             exclude = set()
1752         else:
1753             exclude = set(exclude)
1754         args = align(other, self, join="outer", copy=False, exclude=exclude)
1755 
1756         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1757 
1758         return _broadcast_helper(
1759             cast("T_DataArray", args[1]), exclude, dims_map, common_coords
1760         )
1761 
1762     def _reindex_callback(
1763         self: T_DataArray,
1764         aligner: alignment.Aligner,
1765         dim_pos_indexers: dict[Hashable, Any],
1766         variables: dict[Hashable, Variable],
1767         indexes: dict[Hashable, Index],
1768         fill_value: Any,
1769         exclude_dims: frozenset[Hashable],
1770         exclude_vars: frozenset[Hashable],
1771     ) -> T_DataArray:
1772         """Callback called from ``Aligner`` to create a new reindexed DataArray."""
1773 
1774         if isinstance(fill_value, dict):
1775             fill_value = fill_value.copy()
1776             sentinel = object()
1777             value = fill_value.pop(self.name, sentinel)
1778             if value is not sentinel:
1779                 fill_value[_THIS_ARRAY] = value
1780 
1781         ds = self._to_temp_dataset()
1782         reindexed = ds._reindex_callback(
1783             aligner,
1784             dim_pos_indexers,
1785             variables,
1786             indexes,
1787             fill_value,
1788             exclude_dims,
1789             exclude_vars,
1790         )
1791         return self._from_temp_dataset(reindexed)
1792 
1793     def reindex_like(
1794         self: T_DataArray,
1795         other: DataArray | Dataset,
1796         method: ReindexMethodOptions = None,
1797         tolerance: int | float | Iterable[int | float] | None = None,
1798         copy: bool = True,
1799         fill_value=dtypes.NA,
1800     ) -> T_DataArray:
1801         """Conform this object onto the indexes of another object, filling in
1802         missing values with ``fill_value``. The default fill value is NaN.
1803 
1804         Parameters
1805         ----------
1806         other : Dataset or DataArray
1807             Object with an 'indexes' attribute giving a mapping from dimension
1808             names to pandas.Index objects, which provides coordinates upon
1809             which to index the variables in this dataset. The indexes on this
1810             other object need not be the same as the indexes on this
1811             dataset. Any mis-matched index values will be filled in with
1812             NaN, and any mis-matched dimension names will simply be ignored.
1813         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1814             Method to use for filling index values from other not found on this
1815             data array:
1816 
1817             - None (default): don't fill gaps
1818             - pad / ffill: propagate last valid index value forward
1819             - backfill / bfill: propagate next valid index value backward
1820             - nearest: use nearest valid index value
1821 
1822         tolerance : optional
1823             Maximum distance between original and new labels for inexact
1824             matches. The values of the index at the matching locations must
1825             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1826             Tolerance may be a scalar value, which applies the same tolerance
1827             to all values, or list-like, which applies variable tolerance per
1828             element. List-like must be the same size as the index and its dtype
1829             must exactly match the index’s type.
1830         copy : bool, default: True
1831             If ``copy=True``, data in the return value is always copied. If
1832             ``copy=False`` and reindexing is unnecessary, or can be performed
1833             with only slice operations, then the output may share memory with
1834             the input. In either case, a new xarray object is always returned.
1835         fill_value : scalar or dict-like, optional
1836             Value to use for newly missing values. If a dict-like, maps
1837             variable names (including coordinates) to fill values. Use this
1838             data array's name to refer to the data array's values.
1839 
1840         Returns
1841         -------
1842         reindexed : DataArray
1843             Another dataset array, with this array's data but coordinates from
1844             the other object.
1845 
1846         Examples
1847         --------
1848         >>> data = np.arange(12).reshape(4, 3)
1849         >>> da1 = xr.DataArray(
1850         ...     data=data,
1851         ...     dims=["x", "y"],
1852         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
1853         ... )
1854         >>> da1
1855         <xarray.DataArray (x: 4, y: 3)>
1856         array([[ 0,  1,  2],
1857                [ 3,  4,  5],
1858                [ 6,  7,  8],
1859                [ 9, 10, 11]])
1860         Coordinates:
1861           * x        (x) int64 10 20 30 40
1862           * y        (y) int64 70 80 90
1863         >>> da2 = xr.DataArray(
1864         ...     data=data,
1865         ...     dims=["x", "y"],
1866         ...     coords={"x": [40, 30, 20, 10], "y": [90, 80, 70]},
1867         ... )
1868         >>> da2
1869         <xarray.DataArray (x: 4, y: 3)>
1870         array([[ 0,  1,  2],
1871                [ 3,  4,  5],
1872                [ 6,  7,  8],
1873                [ 9, 10, 11]])
1874         Coordinates:
1875           * x        (x) int64 40 30 20 10
1876           * y        (y) int64 90 80 70
1877 
1878         Reindexing with both DataArrays having the same coordinates set, but in different order:
1879 
1880         >>> da1.reindex_like(da2)
1881         <xarray.DataArray (x: 4, y: 3)>
1882         array([[11, 10,  9],
1883                [ 8,  7,  6],
1884                [ 5,  4,  3],
1885                [ 2,  1,  0]])
1886         Coordinates:
1887           * x        (x) int64 40 30 20 10
1888           * y        (y) int64 90 80 70
1889 
1890         Reindexing with the other array having coordinates which the source array doesn't have:
1891 
1892         >>> data = np.arange(12).reshape(4, 3)
1893         >>> da1 = xr.DataArray(
1894         ...     data=data,
1895         ...     dims=["x", "y"],
1896         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
1897         ... )
1898         >>> da2 = xr.DataArray(
1899         ...     data=data,
1900         ...     dims=["x", "y"],
1901         ...     coords={"x": [20, 10, 29, 39], "y": [70, 80, 90]},
1902         ... )
1903         >>> da1.reindex_like(da2)
1904         <xarray.DataArray (x: 4, y: 3)>
1905         array([[ 3.,  4.,  5.],
1906                [ 0.,  1.,  2.],
1907                [nan, nan, nan],
1908                [nan, nan, nan]])
1909         Coordinates:
1910           * x        (x) int64 20 10 29 39
1911           * y        (y) int64 70 80 90
1912 
1913         Filling missing values with the previous valid index with respect to the coordinates' value:
1914 
1915         >>> da1.reindex_like(da2, method="ffill")
1916         <xarray.DataArray (x: 4, y: 3)>
1917         array([[3, 4, 5],
1918                [0, 1, 2],
1919                [3, 4, 5],
1920                [6, 7, 8]])
1921         Coordinates:
1922           * x        (x) int64 20 10 29 39
1923           * y        (y) int64 70 80 90
1924 
1925         Filling missing values while tolerating specified error for inexact matches:
1926 
1927         >>> da1.reindex_like(da2, method="ffill", tolerance=5)
1928         <xarray.DataArray (x: 4, y: 3)>
1929         array([[ 3.,  4.,  5.],
1930                [ 0.,  1.,  2.],
1931                [nan, nan, nan],
1932                [nan, nan, nan]])
1933         Coordinates:
1934           * x        (x) int64 20 10 29 39
1935           * y        (y) int64 70 80 90
1936 
1937         Filling missing values with manually specified values:
1938 
1939         >>> da1.reindex_like(da2, fill_value=19)
1940         <xarray.DataArray (x: 4, y: 3)>
1941         array([[ 3,  4,  5],
1942                [ 0,  1,  2],
1943                [19, 19, 19],
1944                [19, 19, 19]])
1945         Coordinates:
1946           * x        (x) int64 20 10 29 39
1947           * y        (y) int64 70 80 90
1948 
1949         See Also
1950         --------
1951         DataArray.reindex
1952         align
1953         """
1954         return alignment.reindex_like(
1955             self,
1956             other=other,
1957             method=method,
1958             tolerance=tolerance,
1959             copy=copy,
1960             fill_value=fill_value,
1961         )
1962 
1963     def reindex(
1964         self: T_DataArray,
1965         indexers: Mapping[Any, Any] | None = None,
1966         method: ReindexMethodOptions = None,
1967         tolerance: float | Iterable[float] | None = None,
1968         copy: bool = True,
1969         fill_value=dtypes.NA,
1970         **indexers_kwargs: Any,
1971     ) -> T_DataArray:
1972         """Conform this object onto the indexes of another object, filling in
1973         missing values with ``fill_value``. The default fill value is NaN.
1974 
1975         Parameters
1976         ----------
1977         indexers : dict, optional
1978             Dictionary with keys given by dimension names and values given by
1979             arrays of coordinates tick labels. Any mis-matched coordinate
1980             values will be filled in with NaN, and any mis-matched dimension
1981             names will simply be ignored.
1982             One of indexers or indexers_kwargs must be provided.
1983         copy : bool, optional
1984             If ``copy=True``, data in the return value is always copied. If
1985             ``copy=False`` and reindexing is unnecessary, or can be performed
1986             with only slice operations, then the output may share memory with
1987             the input. In either case, a new xarray object is always returned.
1988         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1989             Method to use for filling index values in ``indexers`` not found on
1990             this data array:
1991 
1992             - None (default): don't fill gaps
1993             - pad / ffill: propagate last valid index value forward
1994             - backfill / bfill: propagate next valid index value backward
1995             - nearest: use nearest valid index value
1996 
1997         tolerance : float | Iterable[float] | None, default: None
1998             Maximum distance between original and new labels for inexact
1999             matches. The values of the index at the matching locations must
2000             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2001             Tolerance may be a scalar value, which applies the same tolerance
2002             to all values, or list-like, which applies variable tolerance per
2003             element. List-like must be the same size as the index and its dtype
2004             must exactly match the index’s type.
2005         fill_value : scalar or dict-like, optional
2006             Value to use for newly missing values. If a dict-like, maps
2007             variable names (including coordinates) to fill values. Use this
2008             data array's name to refer to the data array's values.
2009         **indexers_kwargs : {dim: indexer, ...}, optional
2010             The keyword arguments form of ``indexers``.
2011             One of indexers or indexers_kwargs must be provided.
2012 
2013         Returns
2014         -------
2015         reindexed : DataArray
2016             Another dataset array, with this array's data but replaced
2017             coordinates.
2018 
2019         Examples
2020         --------
2021         Reverse latitude:
2022 
2023         >>> da = xr.DataArray(
2024         ...     np.arange(4),
2025         ...     coords=[np.array([90, 89, 88, 87])],
2026         ...     dims="lat",
2027         ... )
2028         >>> da
2029         <xarray.DataArray (lat: 4)>
2030         array([0, 1, 2, 3])
2031         Coordinates:
2032           * lat      (lat) int64 90 89 88 87
2033         >>> da.reindex(lat=da.lat[::-1])
2034         <xarray.DataArray (lat: 4)>
2035         array([3, 2, 1, 0])
2036         Coordinates:
2037           * lat      (lat) int64 87 88 89 90
2038 
2039         See Also
2040         --------
2041         DataArray.reindex_like
2042         align
2043         """
2044         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
2045         return alignment.reindex(
2046             self,
2047             indexers=indexers,
2048             method=method,
2049             tolerance=tolerance,
2050             copy=copy,
2051             fill_value=fill_value,
2052         )
2053 
2054     def interp(
2055         self: T_DataArray,
2056         coords: Mapping[Any, Any] | None = None,
2057         method: InterpOptions = "linear",
2058         assume_sorted: bool = False,
2059         kwargs: Mapping[str, Any] | None = None,
2060         **coords_kwargs: Any,
2061     ) -> T_DataArray:
2062         """Interpolate a DataArray onto new coordinates
2063 
2064         Performs univariate or multivariate interpolation of a DataArray onto
2065         new coordinates using scipy's interpolation routines. If interpolating
2066         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
2067         called. When interpolating along multiple existing dimensions, an
2068         attempt is made to decompose the interpolation into multiple
2069         1-dimensional interpolations. If this is possible,
2070         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
2071         :py:func:`scipy.interpolate.interpn` is called.
2072 
2073         Parameters
2074         ----------
2075         coords : dict, optional
2076             Mapping from dimension names to the new coordinates.
2077             New coordinate can be a scalar, array-like or DataArray.
2078             If DataArrays are passed as new coordinates, their dimensions are
2079             used for the broadcasting. Missing values are skipped.
2080         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
2081             The method used to interpolate. The method should be supported by
2082             the scipy interpolator:
2083 
2084             - ``interp1d``: {"linear", "nearest", "zero", "slinear",
2085               "quadratic", "cubic", "polynomial"}
2086             - ``interpn``: {"linear", "nearest"}
2087 
2088             If ``"polynomial"`` is passed, the ``order`` keyword argument must
2089             also be provided.
2090         assume_sorted : bool, default: False
2091             If False, values of x can be in any order and they are sorted
2092             first. If True, x has to be an array of monotonically increasing
2093             values.
2094         kwargs : dict-like or None, default: None
2095             Additional keyword arguments passed to scipy's interpolator. Valid
2096             options and their behavior depend whether ``interp1d`` or
2097             ``interpn`` is used.
2098         **coords_kwargs : {dim: coordinate, ...}, optional
2099             The keyword arguments form of ``coords``.
2100             One of coords or coords_kwargs must be provided.
2101 
2102         Returns
2103         -------
2104         interpolated : DataArray
2105             New dataarray on the new coordinates.
2106 
2107         Notes
2108         -----
2109         scipy is required.
2110 
2111         See Also
2112         --------
2113         scipy.interpolate.interp1d
2114         scipy.interpolate.interpn
2115 
2116         Examples
2117         --------
2118         >>> da = xr.DataArray(
2119         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
2120         ...     dims=("x", "y"),
2121         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
2122         ... )
2123         >>> da
2124         <xarray.DataArray (x: 3, y: 4)>
2125         array([[ 1.,  4.,  2.,  9.],
2126                [ 2.,  7.,  6., nan],
2127                [ 6., nan,  5.,  8.]])
2128         Coordinates:
2129           * x        (x) int64 0 1 2
2130           * y        (y) int64 10 12 14 16
2131 
2132         1D linear interpolation (the default):
2133 
2134         >>> da.interp(x=[0, 0.75, 1.25, 1.75])
2135         <xarray.DataArray (x: 4, y: 4)>
2136         array([[1.  , 4.  , 2.  ,  nan],
2137                [1.75, 6.25, 5.  ,  nan],
2138                [3.  ,  nan, 5.75,  nan],
2139                [5.  ,  nan, 5.25,  nan]])
2140         Coordinates:
2141           * y        (y) int64 10 12 14 16
2142           * x        (x) float64 0.0 0.75 1.25 1.75
2143 
2144         1D nearest interpolation:
2145 
2146         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
2147         <xarray.DataArray (x: 4, y: 4)>
2148         array([[ 1.,  4.,  2.,  9.],
2149                [ 2.,  7.,  6., nan],
2150                [ 2.,  7.,  6., nan],
2151                [ 6., nan,  5.,  8.]])
2152         Coordinates:
2153           * y        (y) int64 10 12 14 16
2154           * x        (x) float64 0.0 0.75 1.25 1.75
2155 
2156         1D linear extrapolation:
2157 
2158         >>> da.interp(
2159         ...     x=[1, 1.5, 2.5, 3.5],
2160         ...     method="linear",
2161         ...     kwargs={"fill_value": "extrapolate"},
2162         ... )
2163         <xarray.DataArray (x: 4, y: 4)>
2164         array([[ 2. ,  7. ,  6. ,  nan],
2165                [ 4. ,  nan,  5.5,  nan],
2166                [ 8. ,  nan,  4.5,  nan],
2167                [12. ,  nan,  3.5,  nan]])
2168         Coordinates:
2169           * y        (y) int64 10 12 14 16
2170           * x        (x) float64 1.0 1.5 2.5 3.5
2171 
2172         2D linear interpolation:
2173 
2174         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
2175         <xarray.DataArray (x: 4, y: 3)>
2176         array([[2.5  , 3.   ,   nan],
2177                [4.   , 5.625,   nan],
2178                [  nan,   nan,   nan],
2179                [  nan,   nan,   nan]])
2180         Coordinates:
2181           * x        (x) float64 0.0 0.75 1.25 1.75
2182           * y        (y) int64 11 13 15
2183         """
2184         if self.dtype.kind not in "uifc":
2185             raise TypeError(
2186                 "interp only works for a numeric type array. "
2187                 "Given {}.".format(self.dtype)
2188             )
2189         ds = self._to_temp_dataset().interp(
2190             coords,
2191             method=method,
2192             kwargs=kwargs,
2193             assume_sorted=assume_sorted,
2194             **coords_kwargs,
2195         )
2196         return self._from_temp_dataset(ds)
2197 
2198     def interp_like(
2199         self: T_DataArray,
2200         other: DataArray | Dataset,
2201         method: InterpOptions = "linear",
2202         assume_sorted: bool = False,
2203         kwargs: Mapping[str, Any] | None = None,
2204     ) -> T_DataArray:
2205         """Interpolate this object onto the coordinates of another object,
2206         filling out of range values with NaN.
2207 
2208         If interpolating along a single existing dimension,
2209         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
2210         along multiple existing dimensions, an attempt is made to decompose the
2211         interpolation into multiple 1-dimensional interpolations. If this is
2212         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
2213         :py:func:`scipy.interpolate.interpn` is called.
2214 
2215         Parameters
2216         ----------
2217         other : Dataset or DataArray
2218             Object with an 'indexes' attribute giving a mapping from dimension
2219             names to an 1d array-like, which provides coordinates upon
2220             which to index the variables in this dataset. Missing values are skipped.
2221         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
2222             The method used to interpolate. The method should be supported by
2223             the scipy interpolator:
2224 
2225             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic",
2226               "polynomial"} when ``interp1d`` is called.
2227             - {"linear", "nearest"} when ``interpn`` is called.
2228 
2229             If ``"polynomial"`` is passed, the ``order`` keyword argument must
2230             also be provided.
2231         assume_sorted : bool, default: False
2232             If False, values of coordinates that are interpolated over can be
2233             in any order and they are sorted first. If True, interpolated
2234             coordinates are assumed to be an array of monotonically increasing
2235             values.
2236         kwargs : dict, optional
2237             Additional keyword passed to scipy's interpolator.
2238 
2239         Returns
2240         -------
2241         interpolated : DataArray
2242             Another dataarray by interpolating this dataarray's data along the
2243             coordinates of the other object.
2244 
2245         Examples
2246         --------
2247         >>> data = np.arange(12).reshape(4, 3)
2248         >>> da1 = xr.DataArray(
2249         ...     data=data,
2250         ...     dims=["x", "y"],
2251         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
2252         ... )
2253         >>> da1
2254         <xarray.DataArray (x: 4, y: 3)>
2255         array([[ 0,  1,  2],
2256                [ 3,  4,  5],
2257                [ 6,  7,  8],
2258                [ 9, 10, 11]])
2259         Coordinates:
2260           * x        (x) int64 10 20 30 40
2261           * y        (y) int64 70 80 90
2262         >>> da2 = xr.DataArray(
2263         ...     data=data,
2264         ...     dims=["x", "y"],
2265         ...     coords={"x": [10, 20, 29, 39], "y": [70, 80, 90]},
2266         ... )
2267         >>> da2
2268         <xarray.DataArray (x: 4, y: 3)>
2269         array([[ 0,  1,  2],
2270                [ 3,  4,  5],
2271                [ 6,  7,  8],
2272                [ 9, 10, 11]])
2273         Coordinates:
2274           * x        (x) int64 10 20 29 39
2275           * y        (y) int64 70 80 90
2276 
2277         Interpolate the values in the coordinates of the other DataArray with respect to the source's values:
2278 
2279         >>> da2.interp_like(da1)
2280         <xarray.DataArray (x: 4, y: 3)>
2281         array([[0. , 1. , 2. ],
2282                [3. , 4. , 5. ],
2283                [6.3, 7.3, 8.3],
2284                [nan, nan, nan]])
2285         Coordinates:
2286           * x        (x) int64 10 20 30 40
2287           * y        (y) int64 70 80 90
2288 
2289         Could also extrapolate missing values:
2290 
2291         >>> da2.interp_like(da1, kwargs={"fill_value": "extrapolate"})
2292         <xarray.DataArray (x: 4, y: 3)>
2293         array([[ 0. ,  1. ,  2. ],
2294                [ 3. ,  4. ,  5. ],
2295                [ 6.3,  7.3,  8.3],
2296                [ 9.3, 10.3, 11.3]])
2297         Coordinates:
2298           * x        (x) int64 10 20 30 40
2299           * y        (y) int64 70 80 90
2300 
2301         Notes
2302         -----
2303         scipy is required.
2304         If the dataarray has object-type coordinates, reindex is used for these
2305         coordinates instead of the interpolation.
2306 
2307         See Also
2308         --------
2309         DataArray.interp
2310         DataArray.reindex_like
2311         """
2312         if self.dtype.kind not in "uifc":
2313             raise TypeError(
2314                 "interp only works for a numeric type array. "
2315                 "Given {}.".format(self.dtype)
2316             )
2317         ds = self._to_temp_dataset().interp_like(
2318             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
2319         )
2320         return self._from_temp_dataset(ds)
2321 
2322     # change type of self and return to T_DataArray once
2323     # https://github.com/python/mypy/issues/12846 is resolved
2324     def rename(
2325         self,
2326         new_name_or_name_dict: Hashable | Mapping[Any, Hashable] | None = None,
2327         **names: Hashable,
2328     ) -> DataArray:
2329         """Returns a new DataArray with renamed coordinates, dimensions or a new name.
2330 
2331         Parameters
2332         ----------
2333         new_name_or_name_dict : str or dict-like, optional
2334             If the argument is dict-like, it used as a mapping from old
2335             names to new names for coordinates or dimensions. Otherwise,
2336             use the argument as the new name for this array.
2337         **names : Hashable, optional
2338             The keyword arguments form of a mapping from old names to
2339             new names for coordinates or dimensions.
2340             One of new_name_or_name_dict or names must be provided.
2341 
2342         Returns
2343         -------
2344         renamed : DataArray
2345             Renamed array or array with renamed coordinates.
2346 
2347         See Also
2348         --------
2349         Dataset.rename
2350         DataArray.swap_dims
2351         """
2352         if new_name_or_name_dict is None and not names:
2353             # change name to None?
2354             return self._replace(name=None)
2355         if utils.is_dict_like(new_name_or_name_dict) or new_name_or_name_dict is None:
2356             # change dims/coords
2357             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
2358             dataset = self._to_temp_dataset()._rename(name_dict)
2359             return self._from_temp_dataset(dataset)
2360         if utils.hashable(new_name_or_name_dict) and names:
2361             # change name + dims/coords
2362             dataset = self._to_temp_dataset()._rename(names)
2363             dataarray = self._from_temp_dataset(dataset)
2364             return dataarray._replace(name=new_name_or_name_dict)
2365         # only change name
2366         return self._replace(name=new_name_or_name_dict)
2367 
2368     def swap_dims(
2369         self: T_DataArray,
2370         dims_dict: Mapping[Any, Hashable] | None = None,
2371         **dims_kwargs,
2372     ) -> T_DataArray:
2373         """Returns a new DataArray with swapped dimensions.
2374 
2375         Parameters
2376         ----------
2377         dims_dict : dict-like
2378             Dictionary whose keys are current dimension names and whose values
2379             are new names.
2380         **dims_kwargs : {existing_dim: new_dim, ...}, optional
2381             The keyword arguments form of ``dims_dict``.
2382             One of dims_dict or dims_kwargs must be provided.
2383 
2384         Returns
2385         -------
2386         swapped : DataArray
2387             DataArray with swapped dimensions.
2388 
2389         Examples
2390         --------
2391         >>> arr = xr.DataArray(
2392         ...     data=[0, 1],
2393         ...     dims="x",
2394         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
2395         ... )
2396         >>> arr
2397         <xarray.DataArray (x: 2)>
2398         array([0, 1])
2399         Coordinates:
2400           * x        (x) <U1 'a' 'b'
2401             y        (x) int64 0 1
2402 
2403         >>> arr.swap_dims({"x": "y"})
2404         <xarray.DataArray (y: 2)>
2405         array([0, 1])
2406         Coordinates:
2407             x        (y) <U1 'a' 'b'
2408           * y        (y) int64 0 1
2409 
2410         >>> arr.swap_dims({"x": "z"})
2411         <xarray.DataArray (z: 2)>
2412         array([0, 1])
2413         Coordinates:
2414             x        (z) <U1 'a' 'b'
2415             y        (z) int64 0 1
2416         Dimensions without coordinates: z
2417 
2418         See Also
2419         --------
2420         DataArray.rename
2421         Dataset.swap_dims
2422         """
2423         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
2424         ds = self._to_temp_dataset().swap_dims(dims_dict)
2425         return self._from_temp_dataset(ds)
2426 
2427     # change type of self and return to T_DataArray once
2428     # https://github.com/python/mypy/issues/12846 is resolved
2429     def expand_dims(
2430         self,
2431         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
2432         axis: None | int | Sequence[int] = None,
2433         **dim_kwargs: Any,
2434     ) -> DataArray:
2435         """Return a new object with an additional axis (or axes) inserted at
2436         the corresponding position in the array shape. The new object is a
2437         view into the underlying array, not a copy.
2438 
2439         If dim is already a scalar coordinate, it will be promoted to a 1D
2440         coordinate consisting of a single value.
2441 
2442         Parameters
2443         ----------
2444         dim : Hashable, sequence of Hashable, dict, or None, optional
2445             Dimensions to include on the new variable.
2446             If provided as str or sequence of str, then dimensions are inserted
2447             with length 1. If provided as a dict, then the keys are the new
2448             dimensions and the values are either integers (giving the length of
2449             the new dimensions) or sequence/ndarray (giving the coordinates of
2450             the new dimensions).
2451         axis : int, sequence of int, or None, default: None
2452             Axis position(s) where new axis is to be inserted (position(s) on
2453             the result array). If a sequence of integers is passed,
2454             multiple axes are inserted. In this case, dim arguments should be
2455             same length list. If axis=None is passed, all the axes will be
2456             inserted to the start of the result array.
2457         **dim_kwargs : int or sequence or ndarray
2458             The keywords are arbitrary dimensions being inserted and the values
2459             are either the lengths of the new dims (if int is given), or their
2460             coordinates. Note, this is an alternative to passing a dict to the
2461             dim kwarg and will only be used if dim is None.
2462 
2463         Returns
2464         -------
2465         expanded : DataArray
2466             This object, but with additional dimension(s).
2467 
2468         See Also
2469         --------
2470         Dataset.expand_dims
2471 
2472         Examples
2473         --------
2474         >>> da = xr.DataArray(np.arange(5), dims=("x"))
2475         >>> da
2476         <xarray.DataArray (x: 5)>
2477         array([0, 1, 2, 3, 4])
2478         Dimensions without coordinates: x
2479 
2480         Add new dimension of length 2:
2481 
2482         >>> da.expand_dims(dim={"y": 2})
2483         <xarray.DataArray (y: 2, x: 5)>
2484         array([[0, 1, 2, 3, 4],
2485                [0, 1, 2, 3, 4]])
2486         Dimensions without coordinates: y, x
2487 
2488         >>> da.expand_dims(dim={"y": 2}, axis=1)
2489         <xarray.DataArray (x: 5, y: 2)>
2490         array([[0, 0],
2491                [1, 1],
2492                [2, 2],
2493                [3, 3],
2494                [4, 4]])
2495         Dimensions without coordinates: x, y
2496 
2497         Add a new dimension with coordinates from array:
2498 
2499         >>> da.expand_dims(dim={"y": np.arange(5)}, axis=0)
2500         <xarray.DataArray (y: 5, x: 5)>
2501         array([[0, 1, 2, 3, 4],
2502                [0, 1, 2, 3, 4],
2503                [0, 1, 2, 3, 4],
2504                [0, 1, 2, 3, 4],
2505                [0, 1, 2, 3, 4]])
2506         Coordinates:
2507           * y        (y) int64 0 1 2 3 4
2508         Dimensions without coordinates: x
2509         """
2510         if isinstance(dim, int):
2511             raise TypeError("dim should be Hashable or sequence/mapping of Hashables")
2512         elif isinstance(dim, Sequence) and not isinstance(dim, str):
2513             if len(dim) != len(set(dim)):
2514                 raise ValueError("dims should not contain duplicate values.")
2515             dim = dict.fromkeys(dim, 1)
2516         elif dim is not None and not isinstance(dim, Mapping):
2517             dim = {cast(Hashable, dim): 1}
2518 
2519         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
2520         ds = self._to_temp_dataset().expand_dims(dim, axis)
2521         return self._from_temp_dataset(ds)
2522 
2523     # change type of self and return to T_DataArray once
2524     # https://github.com/python/mypy/issues/12846 is resolved
2525     def set_index(
2526         self,
2527         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,
2528         append: bool = False,
2529         **indexes_kwargs: Hashable | Sequence[Hashable],
2530     ) -> DataArray:
2531         """Set DataArray (multi-)indexes using one or more existing
2532         coordinates.
2533 
2534         This legacy method is limited to pandas (multi-)indexes and
2535         1-dimensional "dimension" coordinates. See
2536         :py:meth:`~DataArray.set_xindex` for setting a pandas or a custom
2537         Xarray-compatible index from one or more arbitrary coordinates.
2538 
2539         Parameters
2540         ----------
2541         indexes : {dim: index, ...}
2542             Mapping from names matching dimensions and values given
2543             by (lists of) the names of existing coordinates or variables to set
2544             as new (multi-)index.
2545         append : bool, default: False
2546             If True, append the supplied index(es) to the existing index(es).
2547             Otherwise replace the existing index(es).
2548         **indexes_kwargs : optional
2549             The keyword arguments form of ``indexes``.
2550             One of indexes or indexes_kwargs must be provided.
2551 
2552         Returns
2553         -------
2554         obj : DataArray
2555             Another DataArray, with this data but replaced coordinates.
2556 
2557         Examples
2558         --------
2559         >>> arr = xr.DataArray(
2560         ...     data=np.ones((2, 3)),
2561         ...     dims=["x", "y"],
2562         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
2563         ... )
2564         >>> arr
2565         <xarray.DataArray (x: 2, y: 3)>
2566         array([[1., 1., 1.],
2567                [1., 1., 1.]])
2568         Coordinates:
2569           * x        (x) int64 0 1
2570           * y        (y) int64 0 1 2
2571             a        (x) int64 3 4
2572         >>> arr.set_index(x="a")
2573         <xarray.DataArray (x: 2, y: 3)>
2574         array([[1., 1., 1.],
2575                [1., 1., 1.]])
2576         Coordinates:
2577           * x        (x) int64 3 4
2578           * y        (y) int64 0 1 2
2579 
2580         See Also
2581         --------
2582         DataArray.reset_index
2583         DataArray.set_xindex
2584         """
2585         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)
2586         return self._from_temp_dataset(ds)
2587 
2588     # change type of self and return to T_DataArray once
2589     # https://github.com/python/mypy/issues/12846 is resolved
2590     def reset_index(
2591         self,
2592         dims_or_levels: Hashable | Sequence[Hashable],
2593         drop: bool = False,
2594     ) -> DataArray:
2595         """Reset the specified index(es) or multi-index level(s).
2596 
2597         This legacy method is specific to pandas (multi-)indexes and
2598         1-dimensional "dimension" coordinates. See the more generic
2599         :py:meth:`~DataArray.drop_indexes` and :py:meth:`~DataArray.set_xindex`
2600         method to respectively drop and set pandas or custom indexes for
2601         arbitrary coordinates.
2602 
2603         Parameters
2604         ----------
2605         dims_or_levels : Hashable or sequence of Hashable
2606             Name(s) of the dimension(s) and/or multi-index level(s) that will
2607             be reset.
2608         drop : bool, default: False
2609             If True, remove the specified indexes and/or multi-index levels
2610             instead of extracting them as new coordinates (default: False).
2611 
2612         Returns
2613         -------
2614         obj : DataArray
2615             Another dataarray, with this dataarray's data but replaced
2616             coordinates.
2617 
2618         See Also
2619         --------
2620         DataArray.set_index
2621         DataArray.set_xindex
2622         DataArray.drop_indexes
2623         """
2624         ds = self._to_temp_dataset().reset_index(dims_or_levels, drop=drop)
2625         return self._from_temp_dataset(ds)
2626 
2627     def set_xindex(
2628         self: T_DataArray,
2629         coord_names: str | Sequence[Hashable],
2630         index_cls: type[Index] | None = None,
2631         **options,
2632     ) -> T_DataArray:
2633         """Set a new, Xarray-compatible index from one or more existing
2634         coordinate(s).
2635 
2636         Parameters
2637         ----------
2638         coord_names : str or list
2639             Name(s) of the coordinate(s) used to build the index.
2640             If several names are given, their order matters.
2641         index_cls : subclass of :class:`~xarray.indexes.Index`
2642             The type of index to create. By default, try setting
2643             a pandas (multi-)index from the supplied coordinates.
2644         **options
2645             Options passed to the index constructor.
2646 
2647         Returns
2648         -------
2649         obj : DataArray
2650             Another dataarray, with this dataarray's data and with a new index.
2651 
2652         """
2653         ds = self._to_temp_dataset().set_xindex(coord_names, index_cls, **options)
2654         return self._from_temp_dataset(ds)
2655 
2656     def reorder_levels(
2657         self: T_DataArray,
2658         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
2659         **dim_order_kwargs: Sequence[int | Hashable],
2660     ) -> T_DataArray:
2661         """Rearrange index levels using input order.
2662 
2663         Parameters
2664         ----------
2665         dim_order dict-like of Hashable to int or Hashable: optional
2666             Mapping from names matching dimensions and values given
2667             by lists representing new level orders. Every given dimension
2668             must have a multi-index.
2669         **dim_order_kwargs : optional
2670             The keyword arguments form of ``dim_order``.
2671             One of dim_order or dim_order_kwargs must be provided.
2672 
2673         Returns
2674         -------
2675         obj : DataArray
2676             Another dataarray, with this dataarray's data but replaced
2677             coordinates.
2678         """
2679         ds = self._to_temp_dataset().reorder_levels(dim_order, **dim_order_kwargs)
2680         return self._from_temp_dataset(ds)
2681 
2682     def stack(
2683         self: T_DataArray,
2684         dimensions: Mapping[Any, Sequence[Hashable]] | None = None,
2685         create_index: bool | None = True,
2686         index_cls: type[Index] = PandasMultiIndex,
2687         **dimensions_kwargs: Sequence[Hashable],
2688     ) -> T_DataArray:
2689         """
2690         Stack any number of existing dimensions into a single new dimension.
2691 
2692         New dimensions will be added at the end, and the corresponding
2693         coordinate variables will be combined into a MultiIndex.
2694 
2695         Parameters
2696         ----------
2697         dimensions : mapping of Hashable to sequence of Hashable
2698             Mapping of the form `new_name=(dim1, dim2, ...)`.
2699             Names of new dimensions, and the existing dimensions that they
2700             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
2701             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
2702             all dimensions.
2703         create_index : bool or None, default: True
2704             If True, create a multi-index for each of the stacked dimensions.
2705             If False, don't create any index.
2706             If None, create a multi-index only if exactly one single (1-d) coordinate
2707             index is found for every dimension to stack.
2708         index_cls: class, optional
2709             Can be used to pass a custom multi-index type. Must be an Xarray index that
2710             implements `.stack()`. By default, a pandas multi-index wrapper is used.
2711         **dimensions_kwargs
2712             The keyword arguments form of ``dimensions``.
2713             One of dimensions or dimensions_kwargs must be provided.
2714 
2715         Returns
2716         -------
2717         stacked : DataArray
2718             DataArray with stacked data.
2719 
2720         Examples
2721         --------
2722         >>> arr = xr.DataArray(
2723         ...     np.arange(6).reshape(2, 3),
2724         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2725         ... )
2726         >>> arr
2727         <xarray.DataArray (x: 2, y: 3)>
2728         array([[0, 1, 2],
2729                [3, 4, 5]])
2730         Coordinates:
2731           * x        (x) <U1 'a' 'b'
2732           * y        (y) int64 0 1 2
2733         >>> stacked = arr.stack(z=("x", "y"))
2734         >>> stacked.indexes["z"]
2735         MultiIndex([('a', 0),
2736                     ('a', 1),
2737                     ('a', 2),
2738                     ('b', 0),
2739                     ('b', 1),
2740                     ('b', 2)],
2741                    name='z')
2742 
2743         See Also
2744         --------
2745         DataArray.unstack
2746         """
2747         ds = self._to_temp_dataset().stack(
2748             dimensions,
2749             create_index=create_index,
2750             index_cls=index_cls,
2751             **dimensions_kwargs,
2752         )
2753         return self._from_temp_dataset(ds)
2754 
2755     # change type of self and return to T_DataArray once
2756     # https://github.com/python/mypy/issues/12846 is resolved
2757     def unstack(
2758         self,
2759         dim: Dims = None,
2760         fill_value: Any = dtypes.NA,
2761         sparse: bool = False,
2762     ) -> DataArray:
2763         """
2764         Unstack existing dimensions corresponding to MultiIndexes into
2765         multiple new dimensions.
2766 
2767         New dimensions will be added at the end.
2768 
2769         Parameters
2770         ----------
2771         dim : str, Iterable of Hashable or None, optional
2772             Dimension(s) over which to unstack. By default unstacks all
2773             MultiIndexes.
2774         fill_value : scalar or dict-like, default: nan
2775             Value to be filled. If a dict-like, maps variable names to
2776             fill values. Use the data array's name to refer to its
2777             name. If not provided or if the dict-like does not contain
2778             all variables, the dtype's NA value will be used.
2779         sparse : bool, default: False
2780             Use sparse-array if True
2781 
2782         Returns
2783         -------
2784         unstacked : DataArray
2785             Array with unstacked data.
2786 
2787         Examples
2788         --------
2789         >>> arr = xr.DataArray(
2790         ...     np.arange(6).reshape(2, 3),
2791         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2792         ... )
2793         >>> arr
2794         <xarray.DataArray (x: 2, y: 3)>
2795         array([[0, 1, 2],
2796                [3, 4, 5]])
2797         Coordinates:
2798           * x        (x) <U1 'a' 'b'
2799           * y        (y) int64 0 1 2
2800         >>> stacked = arr.stack(z=("x", "y"))
2801         >>> stacked.indexes["z"]
2802         MultiIndex([('a', 0),
2803                     ('a', 1),
2804                     ('a', 2),
2805                     ('b', 0),
2806                     ('b', 1),
2807                     ('b', 2)],
2808                    name='z')
2809         >>> roundtripped = stacked.unstack()
2810         >>> arr.identical(roundtripped)
2811         True
2812 
2813         See Also
2814         --------
2815         DataArray.stack
2816         """
2817         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
2818         return self._from_temp_dataset(ds)
2819 
2820     def to_unstacked_dataset(self, dim: Hashable, level: int | Hashable = 0) -> Dataset:
2821         """Unstack DataArray expanding to Dataset along a given level of a
2822         stacked coordinate.
2823 
2824         This is the inverse operation of Dataset.to_stacked_array.
2825 
2826         Parameters
2827         ----------
2828         dim : Hashable
2829             Name of existing dimension to unstack
2830         level : int or Hashable, default: 0
2831             The MultiIndex level to expand to a dataset along. Can either be
2832             the integer index of the level or its name.
2833 
2834         Returns
2835         -------
2836         unstacked: Dataset
2837 
2838         Examples
2839         --------
2840         >>> arr = xr.DataArray(
2841         ...     np.arange(6).reshape(2, 3),
2842         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2843         ... )
2844         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
2845         >>> data
2846         <xarray.Dataset>
2847         Dimensions:  (x: 2, y: 3)
2848         Coordinates:
2849           * x        (x) <U1 'a' 'b'
2850           * y        (y) int64 0 1 2
2851         Data variables:
2852             a        (x, y) int64 0 1 2 3 4 5
2853             b        (x) int64 0 3
2854         >>> stacked = data.to_stacked_array("z", ["x"])
2855         >>> stacked.indexes["z"]
2856         MultiIndex([('a', 0.0),
2857                     ('a', 1.0),
2858                     ('a', 2.0),
2859                     ('b', nan)],
2860                    name='z')
2861         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
2862         >>> data.identical(roundtripped)
2863         True
2864 
2865         See Also
2866         --------
2867         Dataset.to_stacked_array
2868         """
2869         idx = self._indexes[dim].to_pandas_index()
2870         if not isinstance(idx, pd.MultiIndex):
2871             raise ValueError(f"'{dim}' is not a stacked coordinate")
2872 
2873         level_number = idx._get_level_number(level)
2874         variables = idx.levels[level_number]
2875         variable_dim = idx.names[level_number]
2876 
2877         # pull variables out of datarray
2878         data_dict = {}
2879         for k in variables:
2880             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)
2881 
2882         # unstacked dataset
2883         return Dataset(data_dict)
2884 
2885     def transpose(
2886         self: T_DataArray,
2887         *dims: Hashable,
2888         transpose_coords: bool = True,
2889         missing_dims: ErrorOptionsWithWarn = "raise",
2890     ) -> T_DataArray:
2891         """Return a new DataArray object with transposed dimensions.
2892 
2893         Parameters
2894         ----------
2895         *dims : Hashable, optional
2896             By default, reverse the dimensions. Otherwise, reorder the
2897             dimensions to this order.
2898         transpose_coords : bool, default: True
2899             If True, also transpose the coordinates of this DataArray.
2900         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2901             What to do if dimensions that should be selected from are not present in the
2902             DataArray:
2903             - "raise": raise an exception
2904             - "warn": raise a warning, and ignore the missing dimensions
2905             - "ignore": ignore the missing dimensions
2906 
2907         Returns
2908         -------
2909         transposed : DataArray
2910             The returned DataArray's array is transposed.
2911 
2912         Notes
2913         -----
2914         This operation returns a view of this array's data. It is
2915         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
2916         -- the data will be fully loaded.
2917 
2918         See Also
2919         --------
2920         numpy.transpose
2921         Dataset.transpose
2922         """
2923         if dims:
2924             dims = tuple(utils.infix_dims(dims, self.dims, missing_dims))
2925         variable = self.variable.transpose(*dims)
2926         if transpose_coords:
2927             coords: dict[Hashable, Variable] = {}
2928             for name, coord in self.coords.items():
2929                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
2930                 coords[name] = coord.variable.transpose(*coord_dims)
2931             return self._replace(variable, coords)
2932         else:
2933             return self._replace(variable)
2934 
2935     @property
2936     def T(self: T_DataArray) -> T_DataArray:
2937         return self.transpose()
2938 
2939     # change type of self and return to T_DataArray once
2940     # https://github.com/python/mypy/issues/12846 is resolved
2941     def drop_vars(
2942         self,
2943         names: Hashable | Iterable[Hashable],
2944         *,
2945         errors: ErrorOptions = "raise",
2946     ) -> DataArray:
2947         """Returns an array with dropped variables.
2948 
2949         Parameters
2950         ----------
2951         names : Hashable or iterable of Hashable
2952             Name(s) of variables to drop.
2953         errors : {"raise", "ignore"}, default: "raise"
2954             If 'raise', raises a ValueError error if any of the variable
2955             passed are not in the dataset. If 'ignore', any given names that are in the
2956             DataArray are dropped and no error is raised.
2957 
2958         Returns
2959         -------
2960         dropped : Dataset
2961             New Dataset copied from `self` with variables removed.
2962 
2963         Examples
2964         -------
2965         >>> data = np.arange(12).reshape(4, 3)
2966         >>> da = xr.DataArray(
2967         ...     data=data,
2968         ...     dims=["x", "y"],
2969         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
2970         ... )
2971         >>> da
2972         <xarray.DataArray (x: 4, y: 3)>
2973         array([[ 0,  1,  2],
2974                [ 3,  4,  5],
2975                [ 6,  7,  8],
2976                [ 9, 10, 11]])
2977         Coordinates:
2978           * x        (x) int64 10 20 30 40
2979           * y        (y) int64 70 80 90
2980 
2981         Removing a single variable:
2982 
2983         >>> da.drop_vars("x")
2984         <xarray.DataArray (x: 4, y: 3)>
2985         array([[ 0,  1,  2],
2986                [ 3,  4,  5],
2987                [ 6,  7,  8],
2988                [ 9, 10, 11]])
2989         Coordinates:
2990           * y        (y) int64 70 80 90
2991         Dimensions without coordinates: x
2992 
2993         Removing a list of variables:
2994 
2995         >>> da.drop_vars(["x", "y"])
2996         <xarray.DataArray (x: 4, y: 3)>
2997         array([[ 0,  1,  2],
2998                [ 3,  4,  5],
2999                [ 6,  7,  8],
3000                [ 9, 10, 11]])
3001         Dimensions without coordinates: x, y
3002         """
3003         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
3004         return self._from_temp_dataset(ds)
3005 
3006     def drop_indexes(
3007         self: T_DataArray,
3008         coord_names: Hashable | Iterable[Hashable],
3009         *,
3010         errors: ErrorOptions = "raise",
3011     ) -> T_DataArray:
3012         """Drop the indexes assigned to the given coordinates.
3013 
3014         Parameters
3015         ----------
3016         coord_names : hashable or iterable of hashable
3017             Name(s) of the coordinate(s) for which to drop the index.
3018         errors : {"raise", "ignore"}, default: "raise"
3019             If 'raise', raises a ValueError error if any of the coordinates
3020             passed have no index or are not in the dataset.
3021             If 'ignore', no error is raised.
3022 
3023         Returns
3024         -------
3025         dropped : DataArray
3026             A new dataarray with dropped indexes.
3027         """
3028         ds = self._to_temp_dataset().drop_indexes(coord_names, errors=errors)
3029         return self._from_temp_dataset(ds)
3030 
3031     def drop(
3032         self: T_DataArray,
3033         labels: Mapping[Any, Any] | None = None,
3034         dim: Hashable | None = None,
3035         *,
3036         errors: ErrorOptions = "raise",
3037         **labels_kwargs,
3038     ) -> T_DataArray:
3039         """Backward compatible method based on `drop_vars` and `drop_sel`
3040 
3041         Using either `drop_vars` or `drop_sel` is encouraged
3042 
3043         See Also
3044         --------
3045         DataArray.drop_vars
3046         DataArray.drop_sel
3047         """
3048         ds = self._to_temp_dataset().drop(labels, dim, errors=errors, **labels_kwargs)
3049         return self._from_temp_dataset(ds)
3050 
3051     def drop_sel(
3052         self: T_DataArray,
3053         labels: Mapping[Any, Any] | None = None,
3054         *,
3055         errors: ErrorOptions = "raise",
3056         **labels_kwargs,
3057     ) -> T_DataArray:
3058         """Drop index labels from this DataArray.
3059 
3060         Parameters
3061         ----------
3062         labels : mapping of Hashable to Any
3063             Index labels to drop
3064         errors : {"raise", "ignore"}, default: "raise"
3065             If 'raise', raises a ValueError error if
3066             any of the index labels passed are not
3067             in the dataset. If 'ignore', any given labels that are in the
3068             dataset are dropped and no error is raised.
3069         **labels_kwargs : {dim: label, ...}, optional
3070             The keyword arguments form of ``dim`` and ``labels``
3071 
3072         Returns
3073         -------
3074         dropped : DataArray
3075 
3076         Examples
3077         --------
3078         >>> da = xr.DataArray(
3079         ...     np.arange(25).reshape(5, 5),
3080         ...     coords={"x": np.arange(0, 9, 2), "y": np.arange(0, 13, 3)},
3081         ...     dims=("x", "y"),
3082         ... )
3083         >>> da
3084         <xarray.DataArray (x: 5, y: 5)>
3085         array([[ 0,  1,  2,  3,  4],
3086                [ 5,  6,  7,  8,  9],
3087                [10, 11, 12, 13, 14],
3088                [15, 16, 17, 18, 19],
3089                [20, 21, 22, 23, 24]])
3090         Coordinates:
3091           * x        (x) int64 0 2 4 6 8
3092           * y        (y) int64 0 3 6 9 12
3093 
3094         >>> da.drop_sel(x=[0, 2], y=9)
3095         <xarray.DataArray (x: 3, y: 4)>
3096         array([[10, 11, 12, 14],
3097                [15, 16, 17, 19],
3098                [20, 21, 22, 24]])
3099         Coordinates:
3100           * x        (x) int64 4 6 8
3101           * y        (y) int64 0 3 6 12
3102 
3103         >>> da.drop_sel({"x": 6, "y": [0, 3]})
3104         <xarray.DataArray (x: 4, y: 3)>
3105         array([[ 2,  3,  4],
3106                [ 7,  8,  9],
3107                [12, 13, 14],
3108                [22, 23, 24]])
3109         Coordinates:
3110           * x        (x) int64 0 2 4 8
3111           * y        (y) int64 6 9 12
3112         """
3113         if labels_kwargs or isinstance(labels, dict):
3114             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
3115 
3116         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
3117         return self._from_temp_dataset(ds)
3118 
3119     def drop_isel(
3120         self: T_DataArray, indexers: Mapping[Any, Any] | None = None, **indexers_kwargs
3121     ) -> T_DataArray:
3122         """Drop index positions from this DataArray.
3123 
3124         Parameters
3125         ----------
3126         indexers : mapping of Hashable to Any or None, default: None
3127             Index locations to drop
3128         **indexers_kwargs : {dim: position, ...}, optional
3129             The keyword arguments form of ``dim`` and ``positions``
3130 
3131         Returns
3132         -------
3133         dropped : DataArray
3134 
3135         Raises
3136         ------
3137         IndexError
3138 
3139         Examples
3140         --------
3141         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("X", "Y"))
3142         >>> da
3143         <xarray.DataArray (X: 5, Y: 5)>
3144         array([[ 0,  1,  2,  3,  4],
3145                [ 5,  6,  7,  8,  9],
3146                [10, 11, 12, 13, 14],
3147                [15, 16, 17, 18, 19],
3148                [20, 21, 22, 23, 24]])
3149         Dimensions without coordinates: X, Y
3150 
3151         >>> da.drop_isel(X=[0, 4], Y=2)
3152         <xarray.DataArray (X: 3, Y: 4)>
3153         array([[ 5,  6,  8,  9],
3154                [10, 11, 13, 14],
3155                [15, 16, 18, 19]])
3156         Dimensions without coordinates: X, Y
3157 
3158         >>> da.drop_isel({"X": 3, "Y": 3})
3159         <xarray.DataArray (X: 4, Y: 4)>
3160         array([[ 0,  1,  2,  4],
3161                [ 5,  6,  7,  9],
3162                [10, 11, 12, 14],
3163                [20, 21, 22, 24]])
3164         Dimensions without coordinates: X, Y
3165         """
3166         dataset = self._to_temp_dataset()
3167         dataset = dataset.drop_isel(indexers=indexers, **indexers_kwargs)
3168         return self._from_temp_dataset(dataset)
3169 
3170     def dropna(
3171         self: T_DataArray,
3172         dim: Hashable,
3173         how: Literal["any", "all"] = "any",
3174         thresh: int | None = None,
3175     ) -> T_DataArray:
3176         """Returns a new array with dropped labels for missing values along
3177         the provided dimension.
3178 
3179         Parameters
3180         ----------
3181         dim : Hashable
3182             Dimension along which to drop missing values. Dropping along
3183             multiple dimensions simultaneously is not yet supported.
3184         how : {"any", "all"}, default: "any"
3185             - any : if any NA values are present, drop that label
3186             - all : if all values are NA, drop that label
3187 
3188         thresh : int or None, default: None
3189             If supplied, require this many non-NA values.
3190 
3191         Returns
3192         -------
3193         dropped : DataArray
3194 
3195         Examples
3196         --------
3197         >>> temperature = [
3198         ...     [0, 4, 2, 9],
3199         ...     [np.nan, np.nan, np.nan, np.nan],
3200         ...     [np.nan, 4, 2, 0],
3201         ...     [3, 1, 0, 0],
3202         ... ]
3203         >>> da = xr.DataArray(
3204         ...     data=temperature,
3205         ...     dims=["Y", "X"],
3206         ...     coords=dict(
3207         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75])),
3208         ...         lon=("X", np.array([10.0, 10.25, 10.5, 10.75])),
3209         ...     ),
3210         ... )
3211         >>> da
3212         <xarray.DataArray (Y: 4, X: 4)>
3213         array([[ 0.,  4.,  2.,  9.],
3214                [nan, nan, nan, nan],
3215                [nan,  4.,  2.,  0.],
3216                [ 3.,  1.,  0.,  0.]])
3217         Coordinates:
3218             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75
3219             lon      (X) float64 10.0 10.25 10.5 10.75
3220         Dimensions without coordinates: Y, X
3221 
3222         >>> da.dropna(dim="Y", how="any")
3223         <xarray.DataArray (Y: 2, X: 4)>
3224         array([[0., 4., 2., 9.],
3225                [3., 1., 0., 0.]])
3226         Coordinates:
3227             lat      (Y) float64 -20.0 -20.75
3228             lon      (X) float64 10.0 10.25 10.5 10.75
3229         Dimensions without coordinates: Y, X
3230 
3231         Drop values only if all values along the dimension are NaN:
3232 
3233         >>> da.dropna(dim="Y", how="all")
3234         <xarray.DataArray (Y: 3, X: 4)>
3235         array([[ 0.,  4.,  2.,  9.],
3236                [nan,  4.,  2.,  0.],
3237                [ 3.,  1.,  0.,  0.]])
3238         Coordinates:
3239             lat      (Y) float64 -20.0 -20.5 -20.75
3240             lon      (X) float64 10.0 10.25 10.5 10.75
3241         Dimensions without coordinates: Y, X
3242         """
3243         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
3244         return self._from_temp_dataset(ds)
3245 
3246     def fillna(self: T_DataArray, value: Any) -> T_DataArray:
3247         """Fill missing values in this object.
3248 
3249         This operation follows the normal broadcasting and alignment rules that
3250         xarray uses for binary arithmetic, except the result is aligned to this
3251         object (``join='left'``) instead of aligned to the intersection of
3252         index coordinates (``join='inner'``).
3253 
3254         Parameters
3255         ----------
3256         value : scalar, ndarray or DataArray
3257             Used to fill all matching missing values in this array. If the
3258             argument is a DataArray, it is first aligned with (reindexed to)
3259             this array.
3260 
3261         Returns
3262         -------
3263         filled : DataArray
3264 
3265         Examples
3266         --------
3267         >>> da = xr.DataArray(
3268         ...     np.array([1, 4, np.nan, 0, 3, np.nan]),
3269         ...     dims="Z",
3270         ...     coords=dict(
3271         ...         Z=("Z", np.arange(6)),
3272         ...         height=("Z", np.array([0, 10, 20, 30, 40, 50])),
3273         ...     ),
3274         ... )
3275         >>> da
3276         <xarray.DataArray (Z: 6)>
3277         array([ 1.,  4., nan,  0.,  3., nan])
3278         Coordinates:
3279           * Z        (Z) int64 0 1 2 3 4 5
3280             height   (Z) int64 0 10 20 30 40 50
3281 
3282         Fill all NaN values with 0:
3283 
3284         >>> da.fillna(0)
3285         <xarray.DataArray (Z: 6)>
3286         array([1., 4., 0., 0., 3., 0.])
3287         Coordinates:
3288           * Z        (Z) int64 0 1 2 3 4 5
3289             height   (Z) int64 0 10 20 30 40 50
3290 
3291         Fill NaN values with corresponding values in array:
3292 
3293         >>> da.fillna(np.array([2, 9, 4, 2, 8, 9]))
3294         <xarray.DataArray (Z: 6)>
3295         array([1., 4., 4., 0., 3., 9.])
3296         Coordinates:
3297           * Z        (Z) int64 0 1 2 3 4 5
3298             height   (Z) int64 0 10 20 30 40 50
3299         """
3300         if utils.is_dict_like(value):
3301             raise TypeError(
3302                 "cannot provide fill value as a dictionary with "
3303                 "fillna on a DataArray"
3304             )
3305         out = ops.fillna(self, value)
3306         return out
3307 
3308     def interpolate_na(
3309         self: T_DataArray,
3310         dim: Hashable | None = None,
3311         method: InterpOptions = "linear",
3312         limit: int | None = None,
3313         use_coordinate: bool | str = True,
3314         max_gap: (
3315             None
3316             | int
3317             | float
3318             | str
3319             | pd.Timedelta
3320             | np.timedelta64
3321             | datetime.timedelta
3322         ) = None,
3323         keep_attrs: bool | None = None,
3324         **kwargs: Any,
3325     ) -> T_DataArray:
3326         """Fill in NaNs by interpolating according to different methods.
3327 
3328         Parameters
3329         ----------
3330         dim : Hashable or None, optional
3331             Specifies the dimension along which to interpolate.
3332         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3333             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3334             String indicating which method to use for interpolation:
3335 
3336             - 'linear': linear interpolation. Additional keyword
3337               arguments are passed to :py:func:`numpy.interp`
3338             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3339               are passed to :py:func:`scipy.interpolate.interp1d`. If
3340               ``method='polynomial'``, the ``order`` keyword argument must also be
3341               provided.
3342             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3343               respective :py:class:`scipy.interpolate` classes.
3344 
3345         use_coordinate : bool or str, default: True
3346             Specifies which index to use as the x values in the interpolation
3347             formulated as `y = f(x)`. If False, values are treated as if
3348             equally-spaced along ``dim``. If True, the IndexVariable `dim` is
3349             used. If ``use_coordinate`` is a string, it specifies the name of a
3350             coordinate variable to use as the index.
3351         limit : int or None, default: None
3352             Maximum number of consecutive NaNs to fill. Must be greater than 0
3353             or None for no limit. This filling is done regardless of the size of
3354             the gap in the data. To only interpolate over gaps less than a given length,
3355             see ``max_gap``.
3356         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
3357             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
3358             Use None for no limit. When interpolating along a datetime64 dimension
3359             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
3360 
3361             - a string that is valid input for pandas.to_timedelta
3362             - a :py:class:`numpy.timedelta64` object
3363             - a :py:class:`pandas.Timedelta` object
3364             - a :py:class:`datetime.timedelta` object
3365 
3366             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
3367             dimensions has not been implemented yet. Gap length is defined as the difference
3368             between coordinate values at the first data point after a gap and the last value
3369             before a gap. For gaps at the beginning (end), gap length is defined as the difference
3370             between coordinate values at the first (last) valid data point and the first (last) NaN.
3371             For example, consider::
3372 
3373                 <xarray.DataArray (x: 9)>
3374                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
3375                 Coordinates:
3376                   * x        (x) int64 0 1 2 3 4 5 6 7 8
3377 
3378             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
3379         keep_attrs : bool or None, default: None
3380             If True, the dataarray's attributes (`attrs`) will be copied from
3381             the original object to the new one.  If False, the new
3382             object will be returned without attributes.
3383         **kwargs : dict, optional
3384             parameters passed verbatim to the underlying interpolation function
3385 
3386         Returns
3387         -------
3388         interpolated: DataArray
3389             Filled in DataArray.
3390 
3391         See Also
3392         --------
3393         numpy.interp
3394         scipy.interpolate
3395 
3396         Examples
3397         --------
3398         >>> da = xr.DataArray(
3399         ...     [np.nan, 2, 3, np.nan, 0], dims="x", coords={"x": [0, 1, 2, 3, 4]}
3400         ... )
3401         >>> da
3402         <xarray.DataArray (x: 5)>
3403         array([nan,  2.,  3., nan,  0.])
3404         Coordinates:
3405           * x        (x) int64 0 1 2 3 4
3406 
3407         >>> da.interpolate_na(dim="x", method="linear")
3408         <xarray.DataArray (x: 5)>
3409         array([nan, 2. , 3. , 1.5, 0. ])
3410         Coordinates:
3411           * x        (x) int64 0 1 2 3 4
3412 
3413         >>> da.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
3414         <xarray.DataArray (x: 5)>
3415         array([1. , 2. , 3. , 1.5, 0. ])
3416         Coordinates:
3417           * x        (x) int64 0 1 2 3 4
3418         """
3419         from xarray.core.missing import interp_na
3420 
3421         return interp_na(
3422             self,
3423             dim=dim,
3424             method=method,
3425             limit=limit,
3426             use_coordinate=use_coordinate,
3427             max_gap=max_gap,
3428             keep_attrs=keep_attrs,
3429             **kwargs,
3430         )
3431 
3432     def ffill(
3433         self: T_DataArray, dim: Hashable, limit: int | None = None
3434     ) -> T_DataArray:
3435         """Fill NaN values by propagating values forward
3436 
3437         *Requires bottleneck.*
3438 
3439         Parameters
3440         ----------
3441         dim : Hashable
3442             Specifies the dimension along which to propagate values when
3443             filling.
3444         limit : int or None, default: None
3445             The maximum number of consecutive NaN values to forward fill. In
3446             other words, if there is a gap with more than this number of
3447             consecutive NaNs, it will only be partially filled. Must be greater
3448             than 0 or None for no limit. Must be None or greater than or equal
3449             to axis length if filling along chunked axes (dimensions).
3450 
3451         Returns
3452         -------
3453         filled : DataArray
3454 
3455         Examples
3456         --------
3457         >>> temperature = np.array(
3458         ...     [
3459         ...         [np.nan, 1, 3],
3460         ...         [0, np.nan, 5],
3461         ...         [5, np.nan, np.nan],
3462         ...         [3, np.nan, np.nan],
3463         ...         [0, 2, 0],
3464         ...     ]
3465         ... )
3466         >>> da = xr.DataArray(
3467         ...     data=temperature,
3468         ...     dims=["Y", "X"],
3469         ...     coords=dict(
3470         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75, -21.0])),
3471         ...         lon=("X", np.array([10.0, 10.25, 10.5])),
3472         ...     ),
3473         ... )
3474         >>> da
3475         <xarray.DataArray (Y: 5, X: 3)>
3476         array([[nan,  1.,  3.],
3477                [ 0., nan,  5.],
3478                [ 5., nan, nan],
3479                [ 3., nan, nan],
3480                [ 0.,  2.,  0.]])
3481         Coordinates:
3482             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3483             lon      (X) float64 10.0 10.25 10.5
3484         Dimensions without coordinates: Y, X
3485 
3486         Fill all NaN values:
3487 
3488         >>> da.ffill(dim="Y", limit=None)
3489         <xarray.DataArray (Y: 5, X: 3)>
3490         array([[nan,  1.,  3.],
3491                [ 0.,  1.,  5.],
3492                [ 5.,  1.,  5.],
3493                [ 3.,  1.,  5.],
3494                [ 0.,  2.,  0.]])
3495         Coordinates:
3496             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3497             lon      (X) float64 10.0 10.25 10.5
3498         Dimensions without coordinates: Y, X
3499 
3500         Fill only the first of consecutive NaN values:
3501 
3502         >>> da.ffill(dim="Y", limit=1)
3503         <xarray.DataArray (Y: 5, X: 3)>
3504         array([[nan,  1.,  3.],
3505                [ 0.,  1.,  5.],
3506                [ 5., nan,  5.],
3507                [ 3., nan, nan],
3508                [ 0.,  2.,  0.]])
3509         Coordinates:
3510             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3511             lon      (X) float64 10.0 10.25 10.5
3512         Dimensions without coordinates: Y, X
3513         """
3514         from xarray.core.missing import ffill
3515 
3516         return ffill(self, dim, limit=limit)
3517 
3518     def bfill(
3519         self: T_DataArray, dim: Hashable, limit: int | None = None
3520     ) -> T_DataArray:
3521         """Fill NaN values by propagating values backward
3522 
3523         *Requires bottleneck.*
3524 
3525         Parameters
3526         ----------
3527         dim : str
3528             Specifies the dimension along which to propagate values when
3529             filling.
3530         limit : int or None, default: None
3531             The maximum number of consecutive NaN values to backward fill. In
3532             other words, if there is a gap with more than this number of
3533             consecutive NaNs, it will only be partially filled. Must be greater
3534             than 0 or None for no limit. Must be None or greater than or equal
3535             to axis length if filling along chunked axes (dimensions).
3536 
3537         Returns
3538         -------
3539         filled : DataArray
3540 
3541         Examples
3542         --------
3543         >>> temperature = np.array(
3544         ...     [
3545         ...         [0, 1, 3],
3546         ...         [0, np.nan, 5],
3547         ...         [5, np.nan, np.nan],
3548         ...         [3, np.nan, np.nan],
3549         ...         [np.nan, 2, 0],
3550         ...     ]
3551         ... )
3552         >>> da = xr.DataArray(
3553         ...     data=temperature,
3554         ...     dims=["Y", "X"],
3555         ...     coords=dict(
3556         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75, -21.0])),
3557         ...         lon=("X", np.array([10.0, 10.25, 10.5])),
3558         ...     ),
3559         ... )
3560         >>> da
3561         <xarray.DataArray (Y: 5, X: 3)>
3562         array([[ 0.,  1.,  3.],
3563                [ 0., nan,  5.],
3564                [ 5., nan, nan],
3565                [ 3., nan, nan],
3566                [nan,  2.,  0.]])
3567         Coordinates:
3568             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3569             lon      (X) float64 10.0 10.25 10.5
3570         Dimensions without coordinates: Y, X
3571 
3572         Fill all NaN values:
3573 
3574         >>> da.bfill(dim="Y", limit=None)
3575         <xarray.DataArray (Y: 5, X: 3)>
3576         array([[ 0.,  1.,  3.],
3577                [ 0.,  2.,  5.],
3578                [ 5.,  2.,  0.],
3579                [ 3.,  2.,  0.],
3580                [nan,  2.,  0.]])
3581         Coordinates:
3582             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3583             lon      (X) float64 10.0 10.25 10.5
3584         Dimensions without coordinates: Y, X
3585 
3586         Fill only the first of consecutive NaN values:
3587 
3588         >>> da.bfill(dim="Y", limit=1)
3589         <xarray.DataArray (Y: 5, X: 3)>
3590         array([[ 0.,  1.,  3.],
3591                [ 0., nan,  5.],
3592                [ 5., nan, nan],
3593                [ 3.,  2.,  0.],
3594                [nan,  2.,  0.]])
3595         Coordinates:
3596             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3597             lon      (X) float64 10.0 10.25 10.5
3598         Dimensions without coordinates: Y, X
3599         """
3600         from xarray.core.missing import bfill
3601 
3602         return bfill(self, dim, limit=limit)
3603 
3604     def combine_first(self: T_DataArray, other: T_DataArray) -> T_DataArray:
3605         """Combine two DataArray objects, with union of coordinates.
3606 
3607         This operation follows the normal broadcasting and alignment rules of
3608         ``join='outer'``.  Default to non-null values of array calling the
3609         method.  Use np.nan to fill in vacant cells after alignment.
3610 
3611         Parameters
3612         ----------
3613         other : DataArray
3614             Used to fill all matching missing values in this array.
3615 
3616         Returns
3617         -------
3618         DataArray
3619         """
3620         return ops.fillna(self, other, join="outer")
3621 
3622     def reduce(
3623         self: T_DataArray,
3624         func: Callable[..., Any],
3625         dim: Dims = None,
3626         *,
3627         axis: int | Sequence[int] | None = None,
3628         keep_attrs: bool | None = None,
3629         keepdims: bool = False,
3630         **kwargs: Any,
3631     ) -> T_DataArray:
3632         """Reduce this array by applying `func` along some dimension(s).
3633 
3634         Parameters
3635         ----------
3636         func : callable
3637             Function which can be called in the form
3638             `f(x, axis=axis, **kwargs)` to return the result of reducing an
3639             np.ndarray over an integer valued axis.
3640         dim : "...", str, Iterable of Hashable or None, optional
3641             Dimension(s) over which to apply `func`. By default `func` is
3642             applied over all dimensions.
3643         axis : int or sequence of int, optional
3644             Axis(es) over which to repeatedly apply `func`. Only one of the
3645             'dim' and 'axis' arguments can be supplied. If neither are
3646             supplied, then the reduction is calculated over the flattened array
3647             (by calling `f(x)` without an axis argument).
3648         keep_attrs : bool or None, optional
3649             If True, the variable's attributes (`attrs`) will be copied from
3650             the original object to the new one.  If False (default), the new
3651             object will be returned without attributes.
3652         keepdims : bool, default: False
3653             If True, the dimensions which are reduced are left in the result
3654             as dimensions of size one. Coordinates that use these dimensions
3655             are removed.
3656         **kwargs : dict
3657             Additional keyword arguments passed on to `func`.
3658 
3659         Returns
3660         -------
3661         reduced : DataArray
3662             DataArray with this object's array replaced with an array with
3663             summarized data and the indicated dimension(s) removed.
3664         """
3665 
3666         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
3667         return self._replace_maybe_drop_dims(var)
3668 
3669     def to_pandas(self) -> DataArray | pd.Series | pd.DataFrame:
3670         """Convert this array into a pandas object with the same shape.
3671 
3672         The type of the returned object depends on the number of DataArray
3673         dimensions:
3674 
3675         * 0D -> `xarray.DataArray`
3676         * 1D -> `pandas.Series`
3677         * 2D -> `pandas.DataFrame`
3678 
3679         Only works for arrays with 2 or fewer dimensions.
3680 
3681         The DataArray constructor performs the inverse transformation.
3682 
3683         Returns
3684         -------
3685         result : DataArray | Series | DataFrame
3686             DataArray, pandas Series or pandas DataFrame.
3687         """
3688         # TODO: consolidate the info about pandas constructors and the
3689         # attributes that correspond to their indexes into a separate module?
3690         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
3691         try:
3692             constructor = constructors[self.ndim]
3693         except KeyError:
3694             raise ValueError(
3695                 f"Cannot convert arrays with {self.ndim} dimensions into "
3696                 "pandas objects. Requires 2 or fewer dimensions."
3697             )
3698         indexes = [self.get_index(dim) for dim in self.dims]
3699         return constructor(self.values, *indexes)
3700 
3701     def to_dataframe(
3702         self, name: Hashable | None = None, dim_order: Sequence[Hashable] | None = None
3703     ) -> pd.DataFrame:
3704         """Convert this array and its coordinates into a tidy pandas.DataFrame.
3705 
3706         The DataFrame is indexed by the Cartesian product of index coordinates
3707         (in the form of a :py:class:`pandas.MultiIndex`). Other coordinates are
3708         included as columns in the DataFrame.
3709 
3710         For 1D and 2D DataArrays, see also :py:func:`DataArray.to_pandas` which
3711         doesn't rely on a MultiIndex to build the DataFrame.
3712 
3713         Parameters
3714         ----------
3715         name: Hashable or None, optional
3716             Name to give to this array (required if unnamed).
3717         dim_order: Sequence of Hashable or None, optional
3718             Hierarchical dimension order for the resulting dataframe.
3719             Array content is transposed to this order and then written out as flat
3720             vectors in contiguous order, so the last dimension in this list
3721             will be contiguous in the resulting DataFrame. This has a major
3722             influence on which operations are efficient on the resulting
3723             dataframe.
3724 
3725             If provided, must include all dimensions of this DataArray. By default,
3726             dimensions are sorted according to the DataArray dimensions order.
3727 
3728         Returns
3729         -------
3730         result: DataFrame
3731             DataArray as a pandas DataFrame.
3732 
3733         See also
3734         --------
3735         DataArray.to_pandas
3736         DataArray.to_series
3737         """
3738         if name is None:
3739             name = self.name
3740         if name is None:
3741             raise ValueError(
3742                 "cannot convert an unnamed DataArray to a "
3743                 "DataFrame: use the ``name`` parameter"
3744             )
3745         if self.ndim == 0:
3746             raise ValueError("cannot convert a scalar to a DataFrame")
3747 
3748         # By using a unique name, we can convert a DataArray into a DataFrame
3749         # even if it shares a name with one of its coordinates.
3750         # I would normally use unique_name = object() but that results in a
3751         # dataframe with columns in the wrong order, for reasons I have not
3752         # been able to debug (possibly a pandas bug?).
3753         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
3754         ds = self._to_dataset_whole(name=unique_name)
3755 
3756         if dim_order is None:
3757             ordered_dims = dict(zip(self.dims, self.shape))
3758         else:
3759             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)
3760 
3761         df = ds._to_dataframe(ordered_dims)
3762         df.columns = [name if c == unique_name else c for c in df.columns]
3763         return df
3764 
3765     def to_series(self) -> pd.Series:
3766         """Convert this array into a pandas.Series.
3767 
3768         The Series is indexed by the Cartesian product of index coordinates
3769         (in the form of a :py:class:`pandas.MultiIndex`).
3770 
3771         Returns
3772         -------
3773         result : Series
3774             DataArray as a pandas Series.
3775 
3776         See also
3777         --------
3778         DataArray.to_pandas
3779         DataArray.to_dataframe
3780         """
3781         index = self.coords.to_index()
3782         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
3783 
3784     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
3785         """Convert this array into a numpy.ma.MaskedArray
3786 
3787         Parameters
3788         ----------
3789         copy : bool, default: True
3790             If True make a copy of the array in the result. If False,
3791             a MaskedArray view of DataArray.values is returned.
3792 
3793         Returns
3794         -------
3795         result : MaskedArray
3796             Masked where invalid values (nan or inf) occur.
3797         """
3798         values = self.to_numpy()  # only compute lazy arrays once
3799         isnull = pd.isnull(values)
3800         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
3801 
3802     # path=None writes to bytes
3803     @overload
3804     def to_netcdf(
3805         self,
3806         path: None = None,
3807         mode: Literal["w", "a"] = "w",
3808         format: T_NetcdfTypes | None = None,
3809         group: str | None = None,
3810         engine: T_NetcdfEngine | None = None,
3811         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3812         unlimited_dims: Iterable[Hashable] | None = None,
3813         compute: bool = True,
3814         invalid_netcdf: bool = False,
3815     ) -> bytes:
3816         ...
3817 
3818     # default return None
3819     @overload
3820     def to_netcdf(
3821         self,
3822         path: str | PathLike,
3823         mode: Literal["w", "a"] = "w",
3824         format: T_NetcdfTypes | None = None,
3825         group: str | None = None,
3826         engine: T_NetcdfEngine | None = None,
3827         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3828         unlimited_dims: Iterable[Hashable] | None = None,
3829         compute: Literal[True] = True,
3830         invalid_netcdf: bool = False,
3831     ) -> None:
3832         ...
3833 
3834     # compute=False returns dask.Delayed
3835     @overload
3836     def to_netcdf(
3837         self,
3838         path: str | PathLike,
3839         mode: Literal["w", "a"] = "w",
3840         format: T_NetcdfTypes | None = None,
3841         group: str | None = None,
3842         engine: T_NetcdfEngine | None = None,
3843         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3844         unlimited_dims: Iterable[Hashable] | None = None,
3845         *,
3846         compute: Literal[False],
3847         invalid_netcdf: bool = False,
3848     ) -> Delayed:
3849         ...
3850 
3851     def to_netcdf(
3852         self,
3853         path: str | PathLike | None = None,
3854         mode: Literal["w", "a"] = "w",
3855         format: T_NetcdfTypes | None = None,
3856         group: str | None = None,
3857         engine: T_NetcdfEngine | None = None,
3858         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3859         unlimited_dims: Iterable[Hashable] | None = None,
3860         compute: bool = True,
3861         invalid_netcdf: bool = False,
3862     ) -> bytes | Delayed | None:
3863         """Write DataArray contents to a netCDF file.
3864 
3865         Parameters
3866         ----------
3867         path : str, path-like or None, optional
3868             Path to which to save this dataset. File-like objects are only
3869             supported by the scipy engine. If no path is provided, this
3870             function returns the resulting netCDF file as bytes; in this case,
3871             we need to use scipy, which does not support netCDF version 4 (the
3872             default format becomes NETCDF3_64BIT).
3873         mode : {"w", "a"}, default: "w"
3874             Write ('w') or append ('a') mode. If mode='w', any existing file at
3875             this location will be overwritten. If mode='a', existing variables
3876             will be overwritten.
3877         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
3878                   "NETCDF3_CLASSIC"}, optional
3879             File format for the resulting netCDF file:
3880 
3881             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
3882               features.
3883             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
3884               netCDF 3 compatible API features.
3885             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
3886               which fully supports 2+ GB files, but is only compatible with
3887               clients linked against netCDF version 3.6.0 or later.
3888             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
3889               handle 2+ GB files very well.
3890 
3891             All formats are supported by the netCDF4-python library.
3892             scipy.io.netcdf only supports the last two formats.
3893 
3894             The default format is NETCDF4 if you are saving a file to disk and
3895             have the netCDF4-python library available. Otherwise, xarray falls
3896             back to using scipy to write netCDF files and defaults to the
3897             NETCDF3_64BIT format (scipy does not support netCDF4).
3898         group : str, optional
3899             Path to the netCDF4 group in the given file to open (only works for
3900             format='NETCDF4'). The group(s) will be created if necessary.
3901         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
3902             Engine to use when writing netCDF files. If not provided, the
3903             default engine is chosen based on available dependencies, with a
3904             preference for 'netcdf4' if writing to a file on disk.
3905         encoding : dict, optional
3906             Nested dictionary with variable names as keys and dictionaries of
3907             variable specific encodings as values, e.g.,
3908             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
3909             "zlib": True}, ...}``
3910 
3911             The `h5netcdf` engine supports both the NetCDF4-style compression
3912             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
3913             ones ``{"compression": "gzip", "compression_opts": 9}``.
3914             This allows using any compression plugin installed in the HDF5
3915             library, e.g. LZF.
3916 
3917         unlimited_dims : iterable of Hashable, optional
3918             Dimension(s) that should be serialized as unlimited dimensions.
3919             By default, no dimensions are treated as unlimited dimensions.
3920             Note that unlimited_dims may also be set via
3921             ``dataset.encoding["unlimited_dims"]``.
3922         compute: bool, default: True
3923             If true compute immediately, otherwise return a
3924             ``dask.delayed.Delayed`` object that can be computed later.
3925         invalid_netcdf: bool, default: False
3926             Only valid along with ``engine="h5netcdf"``. If True, allow writing
3927             hdf5 files which are invalid netcdf as described in
3928             https://github.com/h5netcdf/h5netcdf.
3929 
3930         Returns
3931         -------
3932         store: bytes or Delayed or None
3933             * ``bytes`` if path is None
3934             * ``dask.delayed.Delayed`` if compute is False
3935             * None otherwise
3936 
3937         Notes
3938         -----
3939         Only xarray.Dataset objects can be written to netCDF files, so
3940         the xarray.DataArray is converted to a xarray.Dataset object
3941         containing a single variable. If the DataArray has no name, or if the
3942         name is the same as a coordinate name, then it is given the name
3943         ``"__xarray_dataarray_variable__"``.
3944 
3945         See Also
3946         --------
3947         Dataset.to_netcdf
3948         """
3949         from xarray.backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_netcdf
3950 
3951         if self.name is None:
3952             # If no name is set then use a generic xarray name
3953             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3954         elif self.name in self.coords or self.name in self.dims:
3955             # The name is the same as one of the coords names, which netCDF
3956             # doesn't support, so rename it but keep track of the old name
3957             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3958             dataset.attrs[DATAARRAY_NAME] = self.name
3959         else:
3960             # No problems with the name - so we're fine!
3961             dataset = self.to_dataset()
3962 
3963         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
3964             dataset,
3965             path,
3966             mode=mode,
3967             format=format,
3968             group=group,
3969             engine=engine,
3970             encoding=encoding,
3971             unlimited_dims=unlimited_dims,
3972             compute=compute,
3973             multifile=False,
3974             invalid_netcdf=invalid_netcdf,
3975         )
3976 
3977     # compute=True (default) returns ZarrStore
3978     @overload
3979     def to_zarr(
3980         self,
3981         store: MutableMapping | str | PathLike[str] | None = None,
3982         chunk_store: MutableMapping | str | PathLike | None = None,
3983         mode: Literal["w", "w-", "a", "r+", None] = None,
3984         synchronizer=None,
3985         group: str | None = None,
3986         encoding: Mapping | None = None,
3987         compute: Literal[True] = True,
3988         consolidated: bool | None = None,
3989         append_dim: Hashable | None = None,
3990         region: Mapping[str, slice] | None = None,
3991         safe_chunks: bool = True,
3992         storage_options: dict[str, str] | None = None,
3993         zarr_version: int | None = None,
3994     ) -> ZarrStore:
3995         ...
3996 
3997     # compute=False returns dask.Delayed
3998     @overload
3999     def to_zarr(
4000         self,
4001         store: MutableMapping | str | PathLike[str] | None = None,
4002         chunk_store: MutableMapping | str | PathLike | None = None,
4003         mode: Literal["w", "w-", "a", "r+", None] = None,
4004         synchronizer=None,
4005         group: str | None = None,
4006         encoding: Mapping | None = None,
4007         *,
4008         compute: Literal[False],
4009         consolidated: bool | None = None,
4010         append_dim: Hashable | None = None,
4011         region: Mapping[str, slice] | None = None,
4012         safe_chunks: bool = True,
4013         storage_options: dict[str, str] | None = None,
4014         zarr_version: int | None = None,
4015     ) -> Delayed:
4016         ...
4017 
4018     def to_zarr(
4019         self,
4020         store: MutableMapping | str | PathLike[str] | None = None,
4021         chunk_store: MutableMapping | str | PathLike | None = None,
4022         mode: Literal["w", "w-", "a", "r+", None] = None,
4023         synchronizer=None,
4024         group: str | None = None,
4025         encoding: Mapping | None = None,
4026         compute: bool = True,
4027         consolidated: bool | None = None,
4028         append_dim: Hashable | None = None,
4029         region: Mapping[str, slice] | None = None,
4030         safe_chunks: bool = True,
4031         storage_options: dict[str, str] | None = None,
4032         zarr_version: int | None = None,
4033     ) -> ZarrStore | Delayed:
4034         """Write DataArray contents to a Zarr store
4035 
4036         Zarr chunks are determined in the following way:
4037 
4038         - From the ``chunks`` attribute in each variable's ``encoding``
4039           (can be set via `DataArray.chunk`).
4040         - If the variable is a Dask array, from the dask chunks
4041         - If neither Dask chunks nor encoding chunks are present, chunks will
4042           be determined automatically by Zarr
4043         - If both Dask chunks and encoding chunks are present, encoding chunks
4044           will be used, provided that there is a many-to-one relationship between
4045           encoding chunks and dask chunks (i.e. Dask chunks are bigger than and
4046           evenly divide encoding chunks); otherwise raise a ``ValueError``.
4047           This restriction ensures that no synchronization / locks are required
4048           when writing. To disable this restriction, use ``safe_chunks=False``.
4049 
4050         Parameters
4051         ----------
4052         store : MutableMapping, str or path-like, optional
4053             Store or path to directory in local or remote file system.
4054         chunk_store : MutableMapping, str or path-like, optional
4055             Store or path to directory in local or remote file system only for Zarr
4056             array chunks. Requires zarr-python v2.4.0 or later.
4057         mode : {"w", "w-", "a", "r+", None}, optional
4058             Persistence mode: "w" means create (overwrite if exists);
4059             "w-" means create (fail if exists);
4060             "a" means override existing variables (create if does not exist);
4061             "r+" means modify existing array *values* only (raise an error if
4062             any metadata or shapes would change).
4063             The default mode is "a" if ``append_dim`` is set. Otherwise, it is
4064             "r+" if ``region`` is set and ``w-`` otherwise.
4065         synchronizer : object, optional
4066             Zarr array synchronizer.
4067         group : str, optional
4068             Group path. (a.k.a. `path` in zarr terminology.)
4069         encoding : dict, optional
4070             Nested dictionary with variable names as keys and dictionaries of
4071             variable specific encodings as values, e.g.,
4072             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,}, ...}``
4073         compute : bool, default: True
4074             If True write array data immediately, otherwise return a
4075             ``dask.delayed.Delayed`` object that can be computed to write
4076             array data later. Metadata is always updated eagerly.
4077         consolidated : bool, optional
4078             If True, apply zarr's `consolidate_metadata` function to the store
4079             after writing metadata and read existing stores with consolidated
4080             metadata; if False, do not. The default (`consolidated=None`) means
4081             write consolidated metadata and attempt to read consolidated
4082             metadata for existing stores (falling back to non-consolidated).
4083 
4084             When the experimental ``zarr_version=3``, ``consolidated`` must be
4085             either be ``None`` or ``False``.
4086         append_dim : hashable, optional
4087             If set, the dimension along which the data will be appended. All
4088             other dimensions on overridden variables must remain the same size.
4089         region : dict, optional
4090             Optional mapping from dimension names to integer slices along
4091             dataarray dimensions to indicate the region of existing zarr array(s)
4092             in which to write this datarray's data. For example,
4093             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate
4094             that values should be written to the region ``0:1000`` along ``x``
4095             and ``10000:11000`` along ``y``.
4096 
4097             Two restrictions apply to the use of ``region``:
4098 
4099             - If ``region`` is set, _all_ variables in a dataarray must have at
4100               least one dimension in common with the region. Other variables
4101               should be written in a separate call to ``to_zarr()``.
4102             - Dimensions cannot be included in both ``region`` and
4103               ``append_dim`` at the same time. To create empty arrays to fill
4104               in with ``region``, use a separate call to ``to_zarr()`` with
4105               ``compute=False``. See "Appending to existing Zarr stores" in
4106               the reference documentation for full details.
4107         safe_chunks : bool, default: True
4108             If True, only allow writes to when there is a many-to-one relationship
4109             between Zarr chunks (specified in encoding) and Dask chunks.
4110             Set False to override this restriction; however, data may become corrupted
4111             if Zarr arrays are written in parallel. This option may be useful in combination
4112             with ``compute=False`` to initialize a Zarr store from an existing
4113             DataArray with arbitrary chunk structure.
4114         storage_options : dict, optional
4115             Any additional parameters for the storage backend (ignored for local
4116             paths).
4117         zarr_version : int or None, optional
4118             The desired zarr spec version to target (currently 2 or 3). The
4119             default of None will attempt to determine the zarr version from
4120             ``store`` when possible, otherwise defaulting to 2.
4121 
4122         Returns
4123         -------
4124             * ``dask.delayed.Delayed`` if compute is False
4125             * ZarrStore otherwise
4126 
4127         References
4128         ----------
4129         https://zarr.readthedocs.io/
4130 
4131         Notes
4132         -----
4133         Zarr chunking behavior:
4134             If chunks are found in the encoding argument or attribute
4135             corresponding to any DataArray, those chunks are used.
4136             If a DataArray is a dask array, it is written with those chunks.
4137             If not other chunks are found, Zarr uses its own heuristics to
4138             choose automatic chunk sizes.
4139 
4140         encoding:
4141             The encoding attribute (if exists) of the DataArray(s) will be
4142             used. Override any existing encodings by providing the ``encoding`` kwarg.
4143 
4144         See Also
4145         --------
4146         Dataset.to_zarr
4147         :ref:`io.zarr`
4148             The I/O user guide, with more details and examples.
4149         """
4150         from xarray.backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_zarr
4151 
4152         if self.name is None:
4153             # If no name is set then use a generic xarray name
4154             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
4155         elif self.name in self.coords or self.name in self.dims:
4156             # The name is the same as one of the coords names, which the netCDF data model
4157             # does not support, so rename it but keep track of the old name
4158             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
4159             dataset.attrs[DATAARRAY_NAME] = self.name
4160         else:
4161             # No problems with the name - so we're fine!
4162             dataset = self.to_dataset()
4163 
4164         return to_zarr(  # type: ignore[call-overload,misc]
4165             dataset,
4166             store=store,
4167             chunk_store=chunk_store,
4168             mode=mode,
4169             synchronizer=synchronizer,
4170             group=group,
4171             encoding=encoding,
4172             compute=compute,
4173             consolidated=consolidated,
4174             append_dim=append_dim,
4175             region=region,
4176             safe_chunks=safe_chunks,
4177             storage_options=storage_options,
4178             zarr_version=zarr_version,
4179         )
4180 
4181     def to_dict(
4182         self, data: bool | Literal["list", "array"] = "list", encoding: bool = False
4183     ) -> dict[str, Any]:
4184         """
4185         Convert this xarray.DataArray into a dictionary following xarray
4186         naming conventions.
4187 
4188         Converts all variables and attributes to native Python objects.
4189         Useful for converting to json. To avoid datetime incompatibility
4190         use decode_times=False kwarg in xarray.open_dataset.
4191 
4192         Parameters
4193         ----------
4194         data : bool or {"list", "array"}, default: "list"
4195             Whether to include the actual data in the dictionary. When set to
4196             False, returns just the schema. If set to "array", returns data as
4197             underlying array type. If set to "list" (or True for backwards
4198             compatibility), returns data in lists of Python data types. Note
4199             that for obtaining the "list" output efficiently, use
4200             `da.compute().to_dict(data="list")`.
4201 
4202         encoding : bool, default: False
4203             Whether to include the Dataset's encoding in the dictionary.
4204 
4205         Returns
4206         -------
4207         dict: dict
4208 
4209         See Also
4210         --------
4211         DataArray.from_dict
4212         Dataset.to_dict
4213         """
4214         d = self.variable.to_dict(data=data)
4215         d.update({"coords": {}, "name": self.name})
4216         for k, coord in self.coords.items():
4217             d["coords"][k] = coord.variable.to_dict(data=data)
4218         if encoding:
4219             d["encoding"] = dict(self.encoding)
4220         return d
4221 
4222     @classmethod
4223     def from_dict(cls: type[T_DataArray], d: Mapping[str, Any]) -> T_DataArray:
4224         """Convert a dictionary into an xarray.DataArray
4225 
4226         Parameters
4227         ----------
4228         d : dict
4229             Mapping with a minimum structure of {"dims": [...], "data": [...]}
4230 
4231         Returns
4232         -------
4233         obj : xarray.DataArray
4234 
4235         See Also
4236         --------
4237         DataArray.to_dict
4238         Dataset.from_dict
4239 
4240         Examples
4241         --------
4242         >>> d = {"dims": "t", "data": [1, 2, 3]}
4243         >>> da = xr.DataArray.from_dict(d)
4244         >>> da
4245         <xarray.DataArray (t: 3)>
4246         array([1, 2, 3])
4247         Dimensions without coordinates: t
4248 
4249         >>> d = {
4250         ...     "coords": {
4251         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
4252         ...     },
4253         ...     "attrs": {"title": "air temperature"},
4254         ...     "dims": "t",
4255         ...     "data": [10, 20, 30],
4256         ...     "name": "a",
4257         ... }
4258         >>> da = xr.DataArray.from_dict(d)
4259         >>> da
4260         <xarray.DataArray 'a' (t: 3)>
4261         array([10, 20, 30])
4262         Coordinates:
4263           * t        (t) int64 0 1 2
4264         Attributes:
4265             title:    air temperature
4266         """
4267         coords = None
4268         if "coords" in d:
4269             try:
4270                 coords = {
4271                     k: (v["dims"], v["data"], v.get("attrs"))
4272                     for k, v in d["coords"].items()
4273                 }
4274             except KeyError as e:
4275                 raise ValueError(
4276                     "cannot convert dict when coords are missing the key "
4277                     "'{dims_data}'".format(dims_data=str(e.args[0]))
4278                 )
4279         try:
4280             data = d["data"]
4281         except KeyError:
4282             raise ValueError("cannot convert dict without the key 'data''")
4283         else:
4284             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
4285 
4286         obj.encoding.update(d.get("encoding", {}))
4287 
4288         return obj
4289 
4290     @classmethod
4291     def from_series(cls, series: pd.Series, sparse: bool = False) -> DataArray:
4292         """Convert a pandas.Series into an xarray.DataArray.
4293 
4294         If the series's index is a MultiIndex, it will be expanded into a
4295         tensor product of one-dimensional coordinates (filling in missing
4296         values with NaN). Thus this operation should be the inverse of the
4297         `to_series` method.
4298 
4299         Parameters
4300         ----------
4301         series : Series
4302             Pandas Series object to convert.
4303         sparse : bool, default: False
4304             If sparse=True, creates a sparse array instead of a dense NumPy array.
4305             Requires the pydata/sparse package.
4306 
4307         See Also
4308         --------
4309         DataArray.to_series
4310         Dataset.from_dataframe
4311         """
4312         temp_name = "__temporary_name"
4313         df = pd.DataFrame({temp_name: series})
4314         ds = Dataset.from_dataframe(df, sparse=sparse)
4315         result = cast(DataArray, ds[temp_name])
4316         result.name = series.name
4317         return result
4318 
4319     def to_cdms2(self) -> cdms2_Variable:
4320         """Convert this array into a cdms2.Variable"""
4321         from xarray.convert import to_cdms2
4322 
4323         return to_cdms2(self)
4324 
4325     @classmethod
4326     def from_cdms2(cls, variable: cdms2_Variable) -> DataArray:
4327         """Convert a cdms2.Variable into an xarray.DataArray"""
4328         from xarray.convert import from_cdms2
4329 
4330         return from_cdms2(variable)
4331 
4332     def to_iris(self) -> iris_Cube:
4333         """Convert this array into a iris.cube.Cube"""
4334         from xarray.convert import to_iris
4335 
4336         return to_iris(self)
4337 
4338     @classmethod
4339     def from_iris(cls, cube: iris_Cube) -> DataArray:
4340         """Convert a iris.cube.Cube into an xarray.DataArray"""
4341         from xarray.convert import from_iris
4342 
4343         return from_iris(cube)
4344 
4345     def _all_compat(self: T_DataArray, other: T_DataArray, compat_str: str) -> bool:
4346         """Helper function for equals, broadcast_equals, and identical"""
4347 
4348         def compat(x, y):
4349             return getattr(x.variable, compat_str)(y.variable)
4350 
4351         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
4352             self, other
4353         )
4354 
4355     def broadcast_equals(self: T_DataArray, other: T_DataArray) -> bool:
4356         """Two DataArrays are broadcast equal if they are equal after
4357         broadcasting them against each other such that they have the same
4358         dimensions.
4359 
4360         Parameters
4361         ----------
4362         other : DataArray
4363             DataArray to compare to.
4364 
4365         Returns
4366         ----------
4367         equal : bool
4368             True if the two DataArrays are broadcast equal.
4369 
4370         See Also
4371         --------
4372         DataArray.equals
4373         DataArray.identical
4374 
4375         Examples
4376         --------
4377         >>> a = xr.DataArray([1, 2], dims="X")
4378         >>> b = xr.DataArray([[1, 1], [2, 2]], dims=["X", "Y"])
4379         >>> a
4380         <xarray.DataArray (X: 2)>
4381         array([1, 2])
4382         Dimensions without coordinates: X
4383         >>> b
4384         <xarray.DataArray (X: 2, Y: 2)>
4385         array([[1, 1],
4386                [2, 2]])
4387         Dimensions without coordinates: X, Y
4388 
4389         .equals returns True if two DataArrays have the same values, dimensions, and coordinates. .broadcast_equals returns True if the results of broadcasting two DataArrays against eachother have the same values, dimensions, and coordinates.
4390 
4391         >>> a.equals(b)
4392         False
4393         >>> a2, b2 = xr.broadcast(a, b)
4394         >>> a2.equals(b2)
4395         True
4396         >>> a.broadcast_equals(b)
4397         True
4398         """
4399         try:
4400             return self._all_compat(other, "broadcast_equals")
4401         except (TypeError, AttributeError):
4402             return False
4403 
4404     def equals(self: T_DataArray, other: T_DataArray) -> bool:
4405         """True if two DataArrays have the same dimensions, coordinates and
4406         values; otherwise False.
4407 
4408         DataArrays can still be equal (like pandas objects) if they have NaN
4409         values in the same locations.
4410 
4411         This method is necessary because `v1 == v2` for ``DataArray``
4412         does element-wise comparisons (like numpy.ndarrays).
4413 
4414         Parameters
4415         ----------
4416         other : DataArray
4417             DataArray to compare to.
4418 
4419         Returns
4420         ----------
4421         equal : bool
4422             True if the two DataArrays are equal.
4423 
4424         See Also
4425         --------
4426         DataArray.broadcast_equals
4427         DataArray.identical
4428 
4429         Examples
4430         --------
4431         >>> a = xr.DataArray([1, 2, 3], dims="X")
4432         >>> b = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"))
4433         >>> c = xr.DataArray([1, 2, 3], dims="Y")
4434         >>> d = xr.DataArray([3, 2, 1], dims="X")
4435         >>> a
4436         <xarray.DataArray (X: 3)>
4437         array([1, 2, 3])
4438         Dimensions without coordinates: X
4439         >>> b
4440         <xarray.DataArray (X: 3)>
4441         array([1, 2, 3])
4442         Dimensions without coordinates: X
4443         Attributes:
4444             units:    m
4445         >>> c
4446         <xarray.DataArray (Y: 3)>
4447         array([1, 2, 3])
4448         Dimensions without coordinates: Y
4449         >>> d
4450         <xarray.DataArray (X: 3)>
4451         array([3, 2, 1])
4452         Dimensions without coordinates: X
4453 
4454         >>> a.equals(b)
4455         True
4456         >>> a.equals(c)
4457         False
4458         >>> a.equals(d)
4459         False
4460         """
4461         try:
4462             return self._all_compat(other, "equals")
4463         except (TypeError, AttributeError):
4464             return False
4465 
4466     def identical(self: T_DataArray, other: T_DataArray) -> bool:
4467         """Like equals, but also checks the array name and attributes, and
4468         attributes on all coordinates.
4469 
4470         Parameters
4471         ----------
4472         other : DataArray
4473             DataArray to compare to.
4474 
4475         Returns
4476         ----------
4477         equal : bool
4478             True if the two DataArrays are identical.
4479 
4480         See Also
4481         --------
4482         DataArray.broadcast_equals
4483         DataArray.equals
4484 
4485         Examples
4486         --------
4487         >>> a = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"), name="Width")
4488         >>> b = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"), name="Width")
4489         >>> c = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="ft"), name="Width")
4490         >>> a
4491         <xarray.DataArray 'Width' (X: 3)>
4492         array([1, 2, 3])
4493         Dimensions without coordinates: X
4494         Attributes:
4495             units:    m
4496         >>> b
4497         <xarray.DataArray 'Width' (X: 3)>
4498         array([1, 2, 3])
4499         Dimensions without coordinates: X
4500         Attributes:
4501             units:    m
4502         >>> c
4503         <xarray.DataArray 'Width' (X: 3)>
4504         array([1, 2, 3])
4505         Dimensions without coordinates: X
4506         Attributes:
4507             units:    ft
4508 
4509         >>> a.equals(b)
4510         True
4511         >>> a.identical(b)
4512         True
4513 
4514         >>> a.equals(c)
4515         True
4516         >>> a.identical(c)
4517         False
4518         """
4519         try:
4520             return self.name == other.name and self._all_compat(other, "identical")
4521         except (TypeError, AttributeError):
4522             return False
4523 
4524     def _result_name(self, other: Any = None) -> Hashable | None:
4525         # use the same naming heuristics as pandas:
4526         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
4527         other_name = getattr(other, "name", _default)
4528         if other_name is _default or other_name == self.name:
4529             return self.name
4530         else:
4531             return None
4532 
4533     def __array_wrap__(self: T_DataArray, obj, context=None) -> T_DataArray:
4534         new_var = self.variable.__array_wrap__(obj, context)
4535         return self._replace(new_var)
4536 
4537     def __matmul__(self: T_DataArray, obj: T_DataArray) -> T_DataArray:
4538         return self.dot(obj)
4539 
4540     def __rmatmul__(self: T_DataArray, other: T_DataArray) -> T_DataArray:
4541         # currently somewhat duplicative, as only other DataArrays are
4542         # compatible with matmul
4543         return computation.dot(other, self)
4544 
4545     def _unary_op(self: T_DataArray, f: Callable, *args, **kwargs) -> T_DataArray:
4546         keep_attrs = kwargs.pop("keep_attrs", None)
4547         if keep_attrs is None:
4548             keep_attrs = _get_keep_attrs(default=True)
4549         with warnings.catch_warnings():
4550             warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
4551             warnings.filterwarnings(
4552                 "ignore", r"Mean of empty slice", category=RuntimeWarning
4553             )
4554             with np.errstate(all="ignore"):
4555                 da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))
4556             if keep_attrs:
4557                 da.attrs = self.attrs
4558             return da
4559 
4560     def _binary_op(
4561         self: T_DataArray,
4562         other: Any,
4563         f: Callable,
4564         reflexive: bool = False,
4565     ) -> T_DataArray:
4566         from xarray.core.groupby import GroupBy
4567 
4568         if isinstance(other, (Dataset, GroupBy)):
4569             return NotImplemented
4570         if isinstance(other, DataArray):
4571             align_type = OPTIONS["arithmetic_join"]
4572             self, other = align(self, other, join=align_type, copy=False)  # type: ignore
4573         other_variable = getattr(other, "variable", other)
4574         other_coords = getattr(other, "coords", None)
4575 
4576         variable = (
4577             f(self.variable, other_variable)
4578             if not reflexive
4579             else f(other_variable, self.variable)
4580         )
4581         coords, indexes = self.coords._merge_raw(other_coords, reflexive)
4582         name = self._result_name(other)
4583 
4584         return self._replace(variable, coords, name, indexes=indexes)
4585 
4586     def _inplace_binary_op(self: T_DataArray, other: Any, f: Callable) -> T_DataArray:
4587         from xarray.core.groupby import GroupBy
4588 
4589         if isinstance(other, GroupBy):
4590             raise TypeError(
4591                 "in-place operations between a DataArray and "
4592                 "a grouped object are not permitted"
4593             )
4594         # n.b. we can't align other to self (with other.reindex_like(self))
4595         # because `other` may be converted into floats, which would cause
4596         # in-place arithmetic to fail unpredictably. Instead, we simply
4597         # don't support automatic alignment with in-place arithmetic.
4598         other_coords = getattr(other, "coords", None)
4599         other_variable = getattr(other, "variable", other)
4600         try:
4601             with self.coords._merge_inplace(other_coords):
4602                 f(self.variable, other_variable)
4603         except MergeError as exc:
4604             raise MergeError(
4605                 "Automatic alignment is not supported for in-place operations.\n"
4606                 "Consider aligning the indices manually or using a not-in-place operation.\n"
4607                 "See https://github.com/pydata/xarray/issues/3910 for more explanations."
4608             ) from exc
4609         return self
4610 
4611     def _copy_attrs_from(self, other: DataArray | Dataset | Variable) -> None:
4612         self.attrs = other.attrs
4613 
4614     plot = utils.UncachedAccessor(DataArrayPlotAccessor)
4615 
4616     def _title_for_slice(self, truncate: int = 50) -> str:
4617         """
4618         If the dataarray has 1 dimensional coordinates or comes from a slice
4619         we can show that info in the title
4620 
4621         Parameters
4622         ----------
4623         truncate : int, default: 50
4624             maximum number of characters for title
4625 
4626         Returns
4627         -------
4628         title : string
4629             Can be used for plot titles
4630 
4631         """
4632         one_dims = []
4633         for dim, coord in self.coords.items():
4634             if coord.size == 1:
4635                 one_dims.append(
4636                     "{dim} = {v}{unit}".format(
4637                         dim=dim,
4638                         v=format_item(coord.values),
4639                         unit=_get_units_from_attrs(coord),
4640                     )
4641                 )
4642 
4643         title = ", ".join(one_dims)
4644         if len(title) > truncate:
4645             title = title[: (truncate - 3)] + "..."
4646 
4647         return title
4648 
4649     def diff(
4650         self: T_DataArray,
4651         dim: Hashable,
4652         n: int = 1,
4653         label: Literal["upper", "lower"] = "upper",
4654     ) -> T_DataArray:
4655         """Calculate the n-th order discrete difference along given axis.
4656 
4657         Parameters
4658         ----------
4659         dim : Hashable
4660             Dimension over which to calculate the finite difference.
4661         n : int, default: 1
4662             The number of times values are differenced.
4663         label : {"upper", "lower"}, default: "upper"
4664             The new coordinate in dimension ``dim`` will have the
4665             values of either the minuend's or subtrahend's coordinate
4666             for values 'upper' and 'lower', respectively.
4667 
4668         Returns
4669         -------
4670         difference : DataArray
4671             The n-th order finite difference of this object.
4672 
4673         Notes
4674         -----
4675         `n` matches numpy's behavior and is different from pandas' first argument named
4676         `periods`.
4677 
4678         Examples
4679         --------
4680         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
4681         >>> arr.diff("x")
4682         <xarray.DataArray (x: 3)>
4683         array([0, 1, 0])
4684         Coordinates:
4685           * x        (x) int64 2 3 4
4686         >>> arr.diff("x", 2)
4687         <xarray.DataArray (x: 2)>
4688         array([ 1, -1])
4689         Coordinates:
4690           * x        (x) int64 3 4
4691 
4692         See Also
4693         --------
4694         DataArray.differentiate
4695         """
4696         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
4697         return self._from_temp_dataset(ds)
4698 
4699     def shift(
4700         self: T_DataArray,
4701         shifts: Mapping[Any, int] | None = None,
4702         fill_value: Any = dtypes.NA,
4703         **shifts_kwargs: int,
4704     ) -> T_DataArray:
4705         """Shift this DataArray by an offset along one or more dimensions.
4706 
4707         Only the data is moved; coordinates stay in place. This is consistent
4708         with the behavior of ``shift`` in pandas.
4709 
4710         Values shifted from beyond array bounds will appear at one end of
4711         each dimension, which are filled according to `fill_value`. For periodic
4712         offsets instead see `roll`.
4713 
4714         Parameters
4715         ----------
4716         shifts : mapping of Hashable to int or None, optional
4717             Integer offset to shift along each of the given dimensions.
4718             Positive offsets shift to the right; negative offsets shift to the
4719             left.
4720         fill_value : scalar, optional
4721             Value to use for newly missing values
4722         **shifts_kwargs
4723             The keyword arguments form of ``shifts``.
4724             One of shifts or shifts_kwargs must be provided.
4725 
4726         Returns
4727         -------
4728         shifted : DataArray
4729             DataArray with the same coordinates and attributes but shifted
4730             data.
4731 
4732         See Also
4733         --------
4734         roll
4735 
4736         Examples
4737         --------
4738         >>> arr = xr.DataArray([5, 6, 7], dims="x")
4739         >>> arr.shift(x=1)
4740         <xarray.DataArray (x: 3)>
4741         array([nan,  5.,  6.])
4742         Dimensions without coordinates: x
4743         """
4744         variable = self.variable.shift(
4745             shifts=shifts, fill_value=fill_value, **shifts_kwargs
4746         )
4747         return self._replace(variable=variable)
4748 
4749     def roll(
4750         self: T_DataArray,
4751         shifts: Mapping[Hashable, int] | None = None,
4752         roll_coords: bool = False,
4753         **shifts_kwargs: int,
4754     ) -> T_DataArray:
4755         """Roll this array by an offset along one or more dimensions.
4756 
4757         Unlike shift, roll treats the given dimensions as periodic, so will not
4758         create any missing values to be filled.
4759 
4760         Unlike shift, roll may rotate all variables, including coordinates
4761         if specified. The direction of rotation is consistent with
4762         :py:func:`numpy.roll`.
4763 
4764         Parameters
4765         ----------
4766         shifts : mapping of Hashable to int, optional
4767             Integer offset to rotate each of the given dimensions.
4768             Positive offsets roll to the right; negative offsets roll to the
4769             left.
4770         roll_coords : bool, default: False
4771             Indicates whether to roll the coordinates by the offset too.
4772         **shifts_kwargs : {dim: offset, ...}, optional
4773             The keyword arguments form of ``shifts``.
4774             One of shifts or shifts_kwargs must be provided.
4775 
4776         Returns
4777         -------
4778         rolled : DataArray
4779             DataArray with the same attributes but rolled data and coordinates.
4780 
4781         See Also
4782         --------
4783         shift
4784 
4785         Examples
4786         --------
4787         >>> arr = xr.DataArray([5, 6, 7], dims="x")
4788         >>> arr.roll(x=1)
4789         <xarray.DataArray (x: 3)>
4790         array([7, 5, 6])
4791         Dimensions without coordinates: x
4792         """
4793         ds = self._to_temp_dataset().roll(
4794             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
4795         )
4796         return self._from_temp_dataset(ds)
4797 
4798     @property
4799     def real(self: T_DataArray) -> T_DataArray:
4800         """
4801         The real part of the array.
4802 
4803         See Also
4804         --------
4805         numpy.ndarray.real
4806         """
4807         return self._replace(self.variable.real)
4808 
4809     @property
4810     def imag(self: T_DataArray) -> T_DataArray:
4811         """
4812         The imaginary part of the array.
4813 
4814         See Also
4815         --------
4816         numpy.ndarray.imag
4817         """
4818         return self._replace(self.variable.imag)
4819 
4820     def dot(
4821         self: T_DataArray,
4822         other: T_DataArray,
4823         dims: Dims = None,
4824     ) -> T_DataArray:
4825         """Perform dot product of two DataArrays along their shared dims.
4826 
4827         Equivalent to taking taking tensordot over all shared dims.
4828 
4829         Parameters
4830         ----------
4831         other : DataArray
4832             The other array with which the dot product is performed.
4833         dims : ..., str, Iterable of Hashable or None, optional
4834             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.
4835             If not specified, then all the common dimensions are summed over.
4836 
4837         Returns
4838         -------
4839         result : DataArray
4840             Array resulting from the dot product over all shared dimensions.
4841 
4842         See Also
4843         --------
4844         dot
4845         numpy.tensordot
4846 
4847         Examples
4848         --------
4849         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
4850         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
4851         >>> dm_vals = np.arange(4)
4852         >>> dm = xr.DataArray(dm_vals, dims=["z"])
4853 
4854         >>> dm.dims
4855         ('z',)
4856 
4857         >>> da.dims
4858         ('x', 'y', 'z')
4859 
4860         >>> dot_result = da.dot(dm)
4861         >>> dot_result.dims
4862         ('x', 'y')
4863 
4864         """
4865         if isinstance(other, Dataset):
4866             raise NotImplementedError(
4867                 "dot products are not yet supported with Dataset objects."
4868             )
4869         if not isinstance(other, DataArray):
4870             raise TypeError("dot only operates on DataArrays.")
4871 
4872         return computation.dot(self, other, dims=dims)
4873 
4874     # change type of self and return to T_DataArray once
4875     # https://github.com/python/mypy/issues/12846 is resolved
4876     def sortby(
4877         self,
4878         variables: Hashable | DataArray | Sequence[Hashable | DataArray],
4879         ascending: bool = True,
4880     ) -> DataArray:
4881         """Sort object by labels or values (along an axis).
4882 
4883         Sorts the dataarray, either along specified dimensions,
4884         or according to values of 1-D dataarrays that share dimension
4885         with calling object.
4886 
4887         If the input variables are dataarrays, then the dataarrays are aligned
4888         (via left-join) to the calling object prior to sorting by cell values.
4889         NaNs are sorted to the end, following Numpy convention.
4890 
4891         If multiple sorts along the same dimension is
4892         given, numpy's lexsort is performed along that dimension:
4893         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
4894         and the FIRST key in the sequence is used as the primary sort key,
4895         followed by the 2nd key, etc.
4896 
4897         Parameters
4898         ----------
4899         variables : Hashable, DataArray, or sequence of Hashable or DataArray
4900             1D DataArray objects or name(s) of 1D variable(s) in
4901             coords whose values are used to sort this array.
4902         ascending : bool, default: True
4903             Whether to sort by ascending or descending order.
4904 
4905         Returns
4906         -------
4907         sorted : DataArray
4908             A new dataarray where all the specified dims are sorted by dim
4909             labels.
4910 
4911         See Also
4912         --------
4913         Dataset.sortby
4914         numpy.sort
4915         pandas.sort_values
4916         pandas.sort_index
4917 
4918         Examples
4919         --------
4920         >>> da = xr.DataArray(
4921         ...     np.random.rand(5),
4922         ...     coords=[pd.date_range("1/1/2000", periods=5)],
4923         ...     dims="time",
4924         ... )
4925         >>> da
4926         <xarray.DataArray (time: 5)>
4927         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])
4928         Coordinates:
4929           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05
4930 
4931         >>> da.sortby(da)
4932         <xarray.DataArray (time: 5)>
4933         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])
4934         Coordinates:
4935           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02
4936         """
4937         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
4938         return self._from_temp_dataset(ds)
4939 
4940     def quantile(
4941         self: T_DataArray,
4942         q: ArrayLike,
4943         dim: Dims = None,
4944         method: QuantileMethods = "linear",
4945         keep_attrs: bool | None = None,
4946         skipna: bool | None = None,
4947         interpolation: QuantileMethods | None = None,
4948     ) -> T_DataArray:
4949         """Compute the qth quantile of the data along the specified dimension.
4950 
4951         Returns the qth quantiles(s) of the array elements.
4952 
4953         Parameters
4954         ----------
4955         q : float or array-like of float
4956             Quantile to compute, which must be between 0 and 1 inclusive.
4957         dim : str or Iterable of Hashable, optional
4958             Dimension(s) over which to apply quantile.
4959         method : str, default: "linear"
4960             This optional parameter specifies the interpolation method to use when the
4961             desired quantile lies between two data points. The options sorted by their R
4962             type as summarized in the H&F paper [1]_ are:
4963 
4964                 1. "inverted_cdf" (*)
4965                 2. "averaged_inverted_cdf" (*)
4966                 3. "closest_observation" (*)
4967                 4. "interpolated_inverted_cdf" (*)
4968                 5. "hazen" (*)
4969                 6. "weibull" (*)
4970                 7. "linear"  (default)
4971                 8. "median_unbiased" (*)
4972                 9. "normal_unbiased" (*)
4973 
4974             The first three methods are discontiuous. The following discontinuous
4975             variations of the default "linear" (7.) option are also available:
4976 
4977                 * "lower"
4978                 * "higher"
4979                 * "midpoint"
4980                 * "nearest"
4981 
4982             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
4983             was previously called "interpolation", renamed in accordance with numpy
4984             version 1.22.0.
4985 
4986             (*) These methods require numpy version 1.22 or newer.
4987 
4988         keep_attrs : bool or None, optional
4989             If True, the dataset's attributes (`attrs`) will be copied from
4990             the original object to the new one.  If False (default), the new
4991             object will be returned without attributes.
4992         skipna : bool or None, optional
4993             If True, skip missing values (as marked by NaN). By default, only
4994             skips missing values for float dtypes; other dtypes either do not
4995             have a sentinel missing value (int) or skipna=True has not been
4996             implemented (object, datetime64 or timedelta64).
4997 
4998         Returns
4999         -------
5000         quantiles : DataArray
5001             If `q` is a single quantile, then the result
5002             is a scalar. If multiple percentiles are given, first axis of
5003             the result corresponds to the quantile and a quantile dimension
5004             is added to the return array. The other dimensions are the
5005             dimensions that remain after the reduction of the array.
5006 
5007         See Also
5008         --------
5009         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
5010 
5011         Examples
5012         --------
5013         >>> da = xr.DataArray(
5014         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
5015         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
5016         ...     dims=("x", "y"),
5017         ... )
5018         >>> da.quantile(0)  # or da.quantile(0, dim=...)
5019         <xarray.DataArray ()>
5020         array(0.7)
5021         Coordinates:
5022             quantile  float64 0.0
5023         >>> da.quantile(0, dim="x")
5024         <xarray.DataArray (y: 4)>
5025         array([0.7, 4.2, 2.6, 1.5])
5026         Coordinates:
5027           * y         (y) float64 1.0 1.5 2.0 2.5
5028             quantile  float64 0.0
5029         >>> da.quantile([0, 0.5, 1])
5030         <xarray.DataArray (quantile: 3)>
5031         array([0.7, 3.4, 9.4])
5032         Coordinates:
5033           * quantile  (quantile) float64 0.0 0.5 1.0
5034         >>> da.quantile([0, 0.5, 1], dim="x")
5035         <xarray.DataArray (quantile: 3, y: 4)>
5036         array([[0.7 , 4.2 , 2.6 , 1.5 ],
5037                [3.6 , 5.75, 6.  , 1.7 ],
5038                [6.5 , 7.3 , 9.4 , 1.9 ]])
5039         Coordinates:
5040           * y         (y) float64 1.0 1.5 2.0 2.5
5041           * quantile  (quantile) float64 0.0 0.5 1.0
5042 
5043         References
5044         ----------
5045         .. [1] R. J. Hyndman and Y. Fan,
5046            "Sample quantiles in statistical packages,"
5047            The American Statistician, 50(4), pp. 361-365, 1996
5048         """
5049 
5050         ds = self._to_temp_dataset().quantile(
5051             q,
5052             dim=dim,
5053             keep_attrs=keep_attrs,
5054             method=method,
5055             skipna=skipna,
5056             interpolation=interpolation,
5057         )
5058         return self._from_temp_dataset(ds)
5059 
5060     def rank(
5061         self: T_DataArray,
5062         dim: Hashable,
5063         pct: bool = False,
5064         keep_attrs: bool | None = None,
5065     ) -> T_DataArray:
5066         """Ranks the data.
5067 
5068         Equal values are assigned a rank that is the average of the ranks that
5069         would have been otherwise assigned to all of the values within that
5070         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
5071 
5072         NaNs in the input array are returned as NaNs.
5073 
5074         The `bottleneck` library is required.
5075 
5076         Parameters
5077         ----------
5078         dim : Hashable
5079             Dimension over which to compute rank.
5080         pct : bool, default: False
5081             If True, compute percentage ranks, otherwise compute integer ranks.
5082         keep_attrs : bool or None, optional
5083             If True, the dataset's attributes (`attrs`) will be copied from
5084             the original object to the new one.  If False (default), the new
5085             object will be returned without attributes.
5086 
5087         Returns
5088         -------
5089         ranked : DataArray
5090             DataArray with the same coordinates and dtype 'float64'.
5091 
5092         Examples
5093         --------
5094         >>> arr = xr.DataArray([5, 6, 7], dims="x")
5095         >>> arr.rank("x")
5096         <xarray.DataArray (x: 3)>
5097         array([1., 2., 3.])
5098         Dimensions without coordinates: x
5099         """
5100 
5101         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
5102         return self._from_temp_dataset(ds)
5103 
5104     def differentiate(
5105         self: T_DataArray,
5106         coord: Hashable,
5107         edge_order: Literal[1, 2] = 1,
5108         datetime_unit: DatetimeUnitOptions = None,
5109     ) -> T_DataArray:
5110         """ Differentiate the array with the second order accurate central
5111         differences.
5112 
5113         .. note::
5114             This feature is limited to simple cartesian geometry, i.e. coord
5115             must be one dimensional.
5116 
5117         Parameters
5118         ----------
5119         coord : Hashable
5120             The coordinate to be used to compute the gradient.
5121         edge_order : {1, 2}, default: 1
5122             N-th order accurate differences at the boundaries.
5123         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", \
5124                          "us", "ns", "ps", "fs", "as", None}, optional
5125             Unit to compute gradient. Only valid for datetime coordinate.
5126 
5127         Returns
5128         -------
5129         differentiated: DataArray
5130 
5131         See also
5132         --------
5133         numpy.gradient: corresponding numpy function
5134 
5135         Examples
5136         --------
5137 
5138         >>> da = xr.DataArray(
5139         ...     np.arange(12).reshape(4, 3),
5140         ...     dims=["x", "y"],
5141         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
5142         ... )
5143         >>> da
5144         <xarray.DataArray (x: 4, y: 3)>
5145         array([[ 0,  1,  2],
5146                [ 3,  4,  5],
5147                [ 6,  7,  8],
5148                [ 9, 10, 11]])
5149         Coordinates:
5150           * x        (x) float64 0.0 0.1 1.1 1.2
5151         Dimensions without coordinates: y
5152         >>>
5153         >>> da.differentiate("x")
5154         <xarray.DataArray (x: 4, y: 3)>
5155         array([[30.        , 30.        , 30.        ],
5156                [27.54545455, 27.54545455, 27.54545455],
5157                [27.54545455, 27.54545455, 27.54545455],
5158                [30.        , 30.        , 30.        ]])
5159         Coordinates:
5160           * x        (x) float64 0.0 0.1 1.1 1.2
5161         Dimensions without coordinates: y
5162         """
5163         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
5164         return self._from_temp_dataset(ds)
5165 
5166     # change type of self and return to T_DataArray once
5167     # https://github.com/python/mypy/issues/12846 is resolved
5168     def integrate(
5169         self,
5170         coord: Hashable | Sequence[Hashable] = None,
5171         datetime_unit: DatetimeUnitOptions = None,
5172     ) -> DataArray:
5173         """Integrate along the given coordinate using the trapezoidal rule.
5174 
5175         .. note::
5176             This feature is limited to simple cartesian geometry, i.e. coord
5177             must be one dimensional.
5178 
5179         Parameters
5180         ----------
5181         coord : Hashable, or sequence of Hashable
5182             Coordinate(s) used for the integration.
5183         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
5184                         'ps', 'fs', 'as', None}, optional
5185             Specify the unit if a datetime coordinate is used.
5186 
5187         Returns
5188         -------
5189         integrated : DataArray
5190 
5191         See also
5192         --------
5193         Dataset.integrate
5194         numpy.trapz : corresponding numpy function
5195 
5196         Examples
5197         --------
5198 
5199         >>> da = xr.DataArray(
5200         ...     np.arange(12).reshape(4, 3),
5201         ...     dims=["x", "y"],
5202         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
5203         ... )
5204         >>> da
5205         <xarray.DataArray (x: 4, y: 3)>
5206         array([[ 0,  1,  2],
5207                [ 3,  4,  5],
5208                [ 6,  7,  8],
5209                [ 9, 10, 11]])
5210         Coordinates:
5211           * x        (x) float64 0.0 0.1 1.1 1.2
5212         Dimensions without coordinates: y
5213         >>>
5214         >>> da.integrate("x")
5215         <xarray.DataArray (y: 3)>
5216         array([5.4, 6.6, 7.8])
5217         Dimensions without coordinates: y
5218         """
5219         ds = self._to_temp_dataset().integrate(coord, datetime_unit)
5220         return self._from_temp_dataset(ds)
5221 
5222     # change type of self and return to T_DataArray once
5223     # https://github.com/python/mypy/issues/12846 is resolved
5224     def cumulative_integrate(
5225         self,
5226         coord: Hashable | Sequence[Hashable] = None,
5227         datetime_unit: DatetimeUnitOptions = None,
5228     ) -> DataArray:
5229         """Integrate cumulatively along the given coordinate using the trapezoidal rule.
5230 
5231         .. note::
5232             This feature is limited to simple cartesian geometry, i.e. coord
5233             must be one dimensional.
5234 
5235             The first entry of the cumulative integral is always 0, in order to keep the
5236             length of the dimension unchanged between input and output.
5237 
5238         Parameters
5239         ----------
5240         coord : Hashable, or sequence of Hashable
5241             Coordinate(s) used for the integration.
5242         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
5243                         'ps', 'fs', 'as', None}, optional
5244             Specify the unit if a datetime coordinate is used.
5245 
5246         Returns
5247         -------
5248         integrated : DataArray
5249 
5250         See also
5251         --------
5252         Dataset.cumulative_integrate
5253         scipy.integrate.cumulative_trapezoid : corresponding scipy function
5254 
5255         Examples
5256         --------
5257 
5258         >>> da = xr.DataArray(
5259         ...     np.arange(12).reshape(4, 3),
5260         ...     dims=["x", "y"],
5261         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
5262         ... )
5263         >>> da
5264         <xarray.DataArray (x: 4, y: 3)>
5265         array([[ 0,  1,  2],
5266                [ 3,  4,  5],
5267                [ 6,  7,  8],
5268                [ 9, 10, 11]])
5269         Coordinates:
5270           * x        (x) float64 0.0 0.1 1.1 1.2
5271         Dimensions without coordinates: y
5272         >>>
5273         >>> da.cumulative_integrate("x")
5274         <xarray.DataArray (x: 4, y: 3)>
5275         array([[0.  , 0.  , 0.  ],
5276                [0.15, 0.25, 0.35],
5277                [4.65, 5.75, 6.85],
5278                [5.4 , 6.6 , 7.8 ]])
5279         Coordinates:
5280           * x        (x) float64 0.0 0.1 1.1 1.2
5281         Dimensions without coordinates: y
5282         """
5283         ds = self._to_temp_dataset().cumulative_integrate(coord, datetime_unit)
5284         return self._from_temp_dataset(ds)
5285 
5286     def unify_chunks(self) -> DataArray:
5287         """Unify chunk size along all chunked dimensions of this DataArray.
5288 
5289         Returns
5290         -------
5291         DataArray with consistent chunk sizes for all dask-array variables
5292 
5293         See Also
5294         --------
5295         dask.array.core.unify_chunks
5296         """
5297 
5298         return unify_chunks(self)[0]
5299 
5300     def map_blocks(
5301         self,
5302         func: Callable[..., T_Xarray],
5303         args: Sequence[Any] = (),
5304         kwargs: Mapping[str, Any] | None = None,
5305         template: DataArray | Dataset | None = None,
5306     ) -> T_Xarray:
5307         """
5308         Apply a function to each block of this DataArray.
5309 
5310         .. warning::
5311             This method is experimental and its signature may change.
5312 
5313         Parameters
5314         ----------
5315         func : callable
5316             User-provided function that accepts a DataArray as its first
5317             parameter. The function will receive a subset or 'block' of this DataArray (see below),
5318             corresponding to one chunk along each chunked dimension. ``func`` will be
5319             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.
5320 
5321             This function must return either a single DataArray or a single Dataset.
5322 
5323             This function cannot add a new chunked dimension.
5324         args : sequence
5325             Passed to func after unpacking and subsetting any xarray objects by blocks.
5326             xarray objects in args must be aligned with this object, otherwise an error is raised.
5327         kwargs : mapping
5328             Passed verbatim to func after unpacking. xarray objects, if any, will not be
5329             subset to blocks. Passing dask collections in kwargs is not allowed.
5330         template : DataArray or Dataset, optional
5331             xarray object representing the final result after compute is called. If not provided,
5332             the function will be first run on mocked-up data, that looks like this object but
5333             has sizes 0, to determine properties of the returned object such as dtype,
5334             variable names, attributes, new dimensions and new indexes (if any).
5335             ``template`` must be provided if the function changes the size of existing dimensions.
5336             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
5337             ``attrs`` set by ``func`` will be ignored.
5338 
5339         Returns
5340         -------
5341         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
5342         function.
5343 
5344         Notes
5345         -----
5346         This function is designed for when ``func`` needs to manipulate a whole xarray object
5347         subset to each block. Each block is loaded into memory. In the more common case where
5348         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
5349 
5350         If none of the variables in this object is backed by dask arrays, calling this function is
5351         equivalent to calling ``func(obj, *args, **kwargs)``.
5352 
5353         See Also
5354         --------
5355         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
5356         xarray.DataArray.map_blocks
5357 
5358         Examples
5359         --------
5360         Calculate an anomaly from climatology using ``.groupby()``. Using
5361         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
5362         its indices, and its methods like ``.groupby()``.
5363 
5364         >>> def calculate_anomaly(da, groupby_type="time.month"):
5365         ...     gb = da.groupby(groupby_type)
5366         ...     clim = gb.mean(dim="time")
5367         ...     return gb - clim
5368         ...
5369         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
5370         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
5371         >>> np.random.seed(123)
5372         >>> array = xr.DataArray(
5373         ...     np.random.rand(len(time)),
5374         ...     dims=["time"],
5375         ...     coords={"time": time, "month": month},
5376         ... ).chunk()
5377         >>> array.map_blocks(calculate_anomaly, template=array).compute()
5378         <xarray.DataArray (time: 24)>
5379         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,
5380                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,
5381                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,
5382                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,
5383                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])
5384         Coordinates:
5385           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
5386             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
5387 
5388         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
5389         to the function being applied in ``xr.map_blocks()``:
5390 
5391         >>> array.map_blocks(
5392         ...     calculate_anomaly, kwargs={"groupby_type": "time.year"}, template=array
5393         ... )  # doctest: +ELLIPSIS
5394         <xarray.DataArray (time: 24)>
5395         dask.array<<this-array>-calculate_anomaly, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>
5396         Coordinates:
5397           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
5398             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
5399         """
5400         from xarray.core.parallel import map_blocks
5401 
5402         return map_blocks(func, self, args, kwargs, template)
5403 
5404     def polyfit(
5405         self,
5406         dim: Hashable,
5407         deg: int,
5408         skipna: bool | None = None,
5409         rcond: float | None = None,
5410         w: Hashable | Any | None = None,
5411         full: bool = False,
5412         cov: bool | Literal["unscaled"] = False,
5413     ) -> Dataset:
5414         """
5415         Least squares polynomial fit.
5416 
5417         This replicates the behaviour of `numpy.polyfit` but differs by skipping
5418         invalid values when `skipna = True`.
5419 
5420         Parameters
5421         ----------
5422         dim : Hashable
5423             Coordinate along which to fit the polynomials.
5424         deg : int
5425             Degree of the fitting polynomial.
5426         skipna : bool or None, optional
5427             If True, removes all invalid values before fitting each 1D slices of the array.
5428             Default is True if data is stored in a dask.array or if there is any
5429             invalid values, False otherwise.
5430         rcond : float or None, optional
5431             Relative condition number to the fit.
5432         w : Hashable, array-like or None, optional
5433             Weights to apply to the y-coordinate of the sample points.
5434             Can be an array-like object or the name of a coordinate in the dataset.
5435         full : bool, default: False
5436             Whether to return the residuals, matrix rank and singular values in addition
5437             to the coefficients.
5438         cov : bool or "unscaled", default: False
5439             Whether to return to the covariance matrix in addition to the coefficients.
5440             The matrix is not scaled if `cov='unscaled'`.
5441 
5442         Returns
5443         -------
5444         polyfit_results : Dataset
5445             A single dataset which contains:
5446 
5447             polyfit_coefficients
5448                 The coefficients of the best fit.
5449             polyfit_residuals
5450                 The residuals of the least-square computation (only included if `full=True`).
5451                 When the matrix rank is deficient, np.nan is returned.
5452             [dim]_matrix_rank
5453                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
5454             [dim]_singular_value
5455                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
5456             polyfit_covariance
5457                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
5458 
5459         See Also
5460         --------
5461         numpy.polyfit
5462         numpy.polyval
5463         xarray.polyval
5464         """
5465         return self._to_temp_dataset().polyfit(
5466             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov
5467         )
5468 
5469     def pad(
5470         self: T_DataArray,
5471         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,
5472         mode: PadModeOptions = "constant",
5473         stat_length: int
5474         | tuple[int, int]
5475         | Mapping[Any, tuple[int, int]]
5476         | None = None,
5477         constant_values: float
5478         | tuple[float, float]
5479         | Mapping[Any, tuple[float, float]]
5480         | None = None,
5481         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
5482         reflect_type: PadReflectOptions = None,
5483         keep_attrs: bool | None = None,
5484         **pad_width_kwargs: Any,
5485     ) -> T_DataArray:
5486         """Pad this array along one or more dimensions.
5487 
5488         .. warning::
5489             This function is experimental and its behaviour is likely to change
5490             especially regarding padding of dimension coordinates (or IndexVariables).
5491 
5492         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
5493         coordinates will be padded with the same mode, otherwise coordinates
5494         are padded using the "constant" mode with fill_value dtypes.NA.
5495 
5496         Parameters
5497         ----------
5498         pad_width : mapping of Hashable to tuple of int
5499             Mapping with the form of {dim: (pad_before, pad_after)}
5500             describing the number of values padded along each dimension.
5501             {dim: pad} is a shortcut for pad_before = pad_after = pad
5502         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
5503             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
5504             How to pad the DataArray (taken from numpy docs):
5505 
5506             - "constant": Pads with a constant value.
5507             - "edge": Pads with the edge values of array.
5508             - "linear_ramp": Pads with the linear ramp between end_value and the
5509               array edge value.
5510             - "maximum": Pads with the maximum value of all or part of the
5511               vector along each axis.
5512             - "mean": Pads with the mean value of all or part of the
5513               vector along each axis.
5514             - "median": Pads with the median value of all or part of the
5515               vector along each axis.
5516             - "minimum": Pads with the minimum value of all or part of the
5517               vector along each axis.
5518             - "reflect": Pads with the reflection of the vector mirrored on
5519               the first and last values of the vector along each axis.
5520             - "symmetric": Pads with the reflection of the vector mirrored
5521               along the edge of the array.
5522             - "wrap": Pads with the wrap of the vector along the axis.
5523               The first values are used to pad the end and the
5524               end values are used to pad the beginning.
5525 
5526         stat_length : int, tuple or mapping of Hashable to tuple, default: None
5527             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
5528             values at edge of each axis used to calculate the statistic value.
5529             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
5530             statistic lengths along each dimension.
5531             ((before, after),) yields same before and after statistic lengths
5532             for each dimension.
5533             (stat_length,) or int is a shortcut for before = after = statistic
5534             length for all axes.
5535             Default is ``None``, to use the entire axis.
5536         constant_values : scalar, tuple or mapping of Hashable to tuple, default: 0
5537             Used in 'constant'.  The values to set the padded values for each
5538             axis.
5539             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
5540             pad constants along each dimension.
5541             ``((before, after),)`` yields same before and after constants for each
5542             dimension.
5543             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
5544             all dimensions.
5545             Default is 0.
5546         end_values : scalar, tuple or mapping of Hashable to tuple, default: 0
5547             Used in 'linear_ramp'.  The values used for the ending value of the
5548             linear_ramp and that will form the edge of the padded array.
5549             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
5550             end values along each dimension.
5551             ``((before, after),)`` yields same before and after end values for each
5552             axis.
5553             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
5554             all axes.
5555             Default is 0.
5556         reflect_type : {"even", "odd", None}, optional
5557             Used in "reflect", and "symmetric". The "even" style is the
5558             default with an unaltered reflection around the edge value. For
5559             the "odd" style, the extended part of the array is created by
5560             subtracting the reflected values from two times the edge value.
5561         keep_attrs : bool or None, optional
5562             If True, the attributes (``attrs``) will be copied from the
5563             original object to the new one. If False, the new object
5564             will be returned without attributes.
5565         **pad_width_kwargs
5566             The keyword arguments form of ``pad_width``.
5567             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
5568 
5569         Returns
5570         -------
5571         padded : DataArray
5572             DataArray with the padded coordinates and data.
5573 
5574         See Also
5575         --------
5576         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad
5577 
5578         Notes
5579         -----
5580         For ``mode="constant"`` and ``constant_values=None``, integer types will be
5581         promoted to ``float`` and padded with ``np.nan``.
5582 
5583         Padding coordinates will drop their corresponding index (if any) and will reset default
5584         indexes for dimension coordinates.
5585 
5586         Examples
5587         --------
5588         >>> arr = xr.DataArray([5, 6, 7], coords=[("x", [0, 1, 2])])
5589         >>> arr.pad(x=(1, 2), constant_values=0)
5590         <xarray.DataArray (x: 6)>
5591         array([0, 5, 6, 7, 0, 0])
5592         Coordinates:
5593           * x        (x) float64 nan 0.0 1.0 2.0 nan nan
5594 
5595         >>> da = xr.DataArray(
5596         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],
5597         ...     dims=["x", "y"],
5598         ...     coords={"x": [0, 1], "y": [10, 20, 30, 40], "z": ("x", [100, 200])},
5599         ... )
5600         >>> da.pad(x=1)
5601         <xarray.DataArray (x: 4, y: 4)>
5602         array([[nan, nan, nan, nan],
5603                [ 0.,  1.,  2.,  3.],
5604                [10., 11., 12., 13.],
5605                [nan, nan, nan, nan]])
5606         Coordinates:
5607           * x        (x) float64 nan 0.0 1.0 nan
5608           * y        (y) int64 10 20 30 40
5609             z        (x) float64 nan 100.0 200.0 nan
5610 
5611         Careful, ``constant_values`` are coerced to the data type of the array which may
5612         lead to a loss of precision:
5613 
5614         >>> da.pad(x=1, constant_values=1.23456789)
5615         <xarray.DataArray (x: 4, y: 4)>
5616         array([[ 1,  1,  1,  1],
5617                [ 0,  1,  2,  3],
5618                [10, 11, 12, 13],
5619                [ 1,  1,  1,  1]])
5620         Coordinates:
5621           * x        (x) float64 nan 0.0 1.0 nan
5622           * y        (y) int64 10 20 30 40
5623             z        (x) float64 nan 100.0 200.0 nan
5624         """
5625         ds = self._to_temp_dataset().pad(
5626             pad_width=pad_width,
5627             mode=mode,
5628             stat_length=stat_length,
5629             constant_values=constant_values,
5630             end_values=end_values,
5631             reflect_type=reflect_type,
5632             keep_attrs=keep_attrs,
5633             **pad_width_kwargs,
5634         )
5635         return self._from_temp_dataset(ds)
5636 
5637     def idxmin(
5638         self,
5639         dim: Hashable | None = None,
5640         skipna: bool | None = None,
5641         fill_value: Any = dtypes.NA,
5642         keep_attrs: bool | None = None,
5643     ) -> DataArray:
5644         """Return the coordinate label of the minimum value along a dimension.
5645 
5646         Returns a new `DataArray` named after the dimension with the values of
5647         the coordinate labels along that dimension corresponding to minimum
5648         values along that dimension.
5649 
5650         In comparison to :py:meth:`~DataArray.argmin`, this returns the
5651         coordinate label while :py:meth:`~DataArray.argmin` returns the index.
5652 
5653         Parameters
5654         ----------
5655         dim : str, optional
5656             Dimension over which to apply `idxmin`.  This is optional for 1D
5657             arrays, but required for arrays with 2 or more dimensions.
5658         skipna : bool or None, default: None
5659             If True, skip missing values (as marked by NaN). By default, only
5660             skips missing values for ``float``, ``complex``, and ``object``
5661             dtypes; other dtypes either do not have a sentinel missing value
5662             (``int``) or ``skipna=True`` has not been implemented
5663             (``datetime64`` or ``timedelta64``).
5664         fill_value : Any, default: NaN
5665             Value to be filled in case all of the values along a dimension are
5666             null.  By default this is NaN.  The fill value and result are
5667             automatically converted to a compatible dtype if possible.
5668             Ignored if ``skipna`` is False.
5669         keep_attrs : bool or None, optional
5670             If True, the attributes (``attrs``) will be copied from the
5671             original object to the new one. If False, the new object
5672             will be returned without attributes.
5673 
5674         Returns
5675         -------
5676         reduced : DataArray
5677             New `DataArray` object with `idxmin` applied to its data and the
5678             indicated dimension removed.
5679 
5680         See Also
5681         --------
5682         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin
5683 
5684         Examples
5685         --------
5686         >>> array = xr.DataArray(
5687         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
5688         ... )
5689         >>> array.min()
5690         <xarray.DataArray ()>
5691         array(-2)
5692         >>> array.argmin(...)
5693         {'x': <xarray.DataArray ()>
5694         array(4)}
5695         >>> array.idxmin()
5696         <xarray.DataArray 'x' ()>
5697         array('e', dtype='<U1')
5698 
5699         >>> array = xr.DataArray(
5700         ...     [
5701         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
5702         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
5703         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
5704         ...     ],
5705         ...     dims=["y", "x"],
5706         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
5707         ... )
5708         >>> array.min(dim="x")
5709         <xarray.DataArray (y: 3)>
5710         array([-2., -4.,  1.])
5711         Coordinates:
5712           * y        (y) int64 -1 0 1
5713         >>> array.argmin(dim="x")
5714         <xarray.DataArray (y: 3)>
5715         array([4, 0, 2])
5716         Coordinates:
5717           * y        (y) int64 -1 0 1
5718         >>> array.idxmin(dim="x")
5719         <xarray.DataArray 'x' (y: 3)>
5720         array([16.,  0.,  4.])
5721         Coordinates:
5722           * y        (y) int64 -1 0 1
5723         """
5724         return computation._calc_idxminmax(
5725             array=self,
5726             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),
5727             dim=dim,
5728             skipna=skipna,
5729             fill_value=fill_value,
5730             keep_attrs=keep_attrs,
5731         )
5732 
5733     def idxmax(
5734         self,
5735         dim: Hashable = None,
5736         skipna: bool | None = None,
5737         fill_value: Any = dtypes.NA,
5738         keep_attrs: bool | None = None,
5739     ) -> DataArray:
5740         """Return the coordinate label of the maximum value along a dimension.
5741 
5742         Returns a new `DataArray` named after the dimension with the values of
5743         the coordinate labels along that dimension corresponding to maximum
5744         values along that dimension.
5745 
5746         In comparison to :py:meth:`~DataArray.argmax`, this returns the
5747         coordinate label while :py:meth:`~DataArray.argmax` returns the index.
5748 
5749         Parameters
5750         ----------
5751         dim : Hashable, optional
5752             Dimension over which to apply `idxmax`.  This is optional for 1D
5753             arrays, but required for arrays with 2 or more dimensions.
5754         skipna : bool or None, default: None
5755             If True, skip missing values (as marked by NaN). By default, only
5756             skips missing values for ``float``, ``complex``, and ``object``
5757             dtypes; other dtypes either do not have a sentinel missing value
5758             (``int``) or ``skipna=True`` has not been implemented
5759             (``datetime64`` or ``timedelta64``).
5760         fill_value : Any, default: NaN
5761             Value to be filled in case all of the values along a dimension are
5762             null.  By default this is NaN.  The fill value and result are
5763             automatically converted to a compatible dtype if possible.
5764             Ignored if ``skipna`` is False.
5765         keep_attrs : bool or None, optional
5766             If True, the attributes (``attrs``) will be copied from the
5767             original object to the new one. If False, the new object
5768             will be returned without attributes.
5769 
5770         Returns
5771         -------
5772         reduced : DataArray
5773             New `DataArray` object with `idxmax` applied to its data and the
5774             indicated dimension removed.
5775 
5776         See Also
5777         --------
5778         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax
5779 
5780         Examples
5781         --------
5782         >>> array = xr.DataArray(
5783         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
5784         ... )
5785         >>> array.max()
5786         <xarray.DataArray ()>
5787         array(2)
5788         >>> array.argmax(...)
5789         {'x': <xarray.DataArray ()>
5790         array(1)}
5791         >>> array.idxmax()
5792         <xarray.DataArray 'x' ()>
5793         array('b', dtype='<U1')
5794 
5795         >>> array = xr.DataArray(
5796         ...     [
5797         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
5798         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
5799         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
5800         ...     ],
5801         ...     dims=["y", "x"],
5802         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
5803         ... )
5804         >>> array.max(dim="x")
5805         <xarray.DataArray (y: 3)>
5806         array([2., 2., 1.])
5807         Coordinates:
5808           * y        (y) int64 -1 0 1
5809         >>> array.argmax(dim="x")
5810         <xarray.DataArray (y: 3)>
5811         array([0, 2, 2])
5812         Coordinates:
5813           * y        (y) int64 -1 0 1
5814         >>> array.idxmax(dim="x")
5815         <xarray.DataArray 'x' (y: 3)>
5816         array([0., 4., 4.])
5817         Coordinates:
5818           * y        (y) int64 -1 0 1
5819         """
5820         return computation._calc_idxminmax(
5821             array=self,
5822             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),
5823             dim=dim,
5824             skipna=skipna,
5825             fill_value=fill_value,
5826             keep_attrs=keep_attrs,
5827         )
5828 
5829     # change type of self and return to T_DataArray once
5830     # https://github.com/python/mypy/issues/12846 is resolved
5831     def argmin(
5832         self,
5833         dim: Dims = None,
5834         axis: int | None = None,
5835         keep_attrs: bool | None = None,
5836         skipna: bool | None = None,
5837     ) -> DataArray | dict[Hashable, DataArray]:
5838         """Index or indices of the minimum of the DataArray over one or more dimensions.
5839 
5840         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
5841         which can be passed directly to isel(). If a single str is passed to 'dim' then
5842         returns a DataArray with dtype int.
5843 
5844         If there are multiple minima, the indices of the first one found will be
5845         returned.
5846 
5847         Parameters
5848         ----------
5849         dim : "...", str, Iterable of Hashable or None, optional
5850             The dimensions over which to find the minimum. By default, finds minimum over
5851             all dimensions - for now returning an int for backward compatibility, but
5852             this is deprecated, in future will return a dict with indices for all
5853             dimensions; to return a dict with all dimensions now, pass '...'.
5854         axis : int or None, optional
5855             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
5856             can be supplied.
5857         keep_attrs : bool or None, optional
5858             If True, the attributes (`attrs`) will be copied from the original
5859             object to the new one. If False, the new object will be
5860             returned without attributes.
5861         skipna : bool or None, optional
5862             If True, skip missing values (as marked by NaN). By default, only
5863             skips missing values for float dtypes; other dtypes either do not
5864             have a sentinel missing value (int) or skipna=True has not been
5865             implemented (object, datetime64 or timedelta64).
5866 
5867         Returns
5868         -------
5869         result : DataArray or dict of DataArray
5870 
5871         See Also
5872         --------
5873         Variable.argmin, DataArray.idxmin
5874 
5875         Examples
5876         --------
5877         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
5878         >>> array.min()
5879         <xarray.DataArray ()>
5880         array(-1)
5881         >>> array.argmin(...)
5882         {'x': <xarray.DataArray ()>
5883         array(2)}
5884         >>> array.isel(array.argmin(...))
5885         <xarray.DataArray ()>
5886         array(-1)
5887 
5888         >>> array = xr.DataArray(
5889         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],
5890         ...     dims=("x", "y", "z"),
5891         ... )
5892         >>> array.min(dim="x")
5893         <xarray.DataArray (y: 3, z: 3)>
5894         array([[ 1,  2,  1],
5895                [ 2, -5,  1],
5896                [ 2,  1,  1]])
5897         Dimensions without coordinates: y, z
5898         >>> array.argmin(dim="x")
5899         <xarray.DataArray (y: 3, z: 3)>
5900         array([[1, 0, 0],
5901                [1, 1, 1],
5902                [0, 0, 1]])
5903         Dimensions without coordinates: y, z
5904         >>> array.argmin(dim=["x"])
5905         {'x': <xarray.DataArray (y: 3, z: 3)>
5906         array([[1, 0, 0],
5907                [1, 1, 1],
5908                [0, 0, 1]])
5909         Dimensions without coordinates: y, z}
5910         >>> array.min(dim=("x", "z"))
5911         <xarray.DataArray (y: 3)>
5912         array([ 1, -5,  1])
5913         Dimensions without coordinates: y
5914         >>> array.argmin(dim=["x", "z"])
5915         {'x': <xarray.DataArray (y: 3)>
5916         array([0, 1, 0])
5917         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
5918         array([2, 1, 1])
5919         Dimensions without coordinates: y}
5920         >>> array.isel(array.argmin(dim=["x", "z"]))
5921         <xarray.DataArray (y: 3)>
5922         array([ 1, -5,  1])
5923         Dimensions without coordinates: y
5924         """
5925         result = self.variable.argmin(dim, axis, keep_attrs, skipna)
5926         if isinstance(result, dict):
5927             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
5928         else:
5929             return self._replace_maybe_drop_dims(result)
5930 
5931     # change type of self and return to T_DataArray once
5932     # https://github.com/python/mypy/issues/12846 is resolved
5933     def argmax(
5934         self,
5935         dim: Dims = None,
5936         axis: int | None = None,
5937         keep_attrs: bool | None = None,
5938         skipna: bool | None = None,
5939     ) -> DataArray | dict[Hashable, DataArray]:
5940         """Index or indices of the maximum of the DataArray over one or more dimensions.
5941 
5942         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
5943         which can be passed directly to isel(). If a single str is passed to 'dim' then
5944         returns a DataArray with dtype int.
5945 
5946         If there are multiple maxima, the indices of the first one found will be
5947         returned.
5948 
5949         Parameters
5950         ----------
5951         dim : "...", str, Iterable of Hashable or None, optional
5952             The dimensions over which to find the maximum. By default, finds maximum over
5953             all dimensions - for now returning an int for backward compatibility, but
5954             this is deprecated, in future will return a dict with indices for all
5955             dimensions; to return a dict with all dimensions now, pass '...'.
5956         axis : int or None, optional
5957             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
5958             can be supplied.
5959         keep_attrs : bool or None, optional
5960             If True, the attributes (`attrs`) will be copied from the original
5961             object to the new one. If False, the new object will be
5962             returned without attributes.
5963         skipna : bool or None, optional
5964             If True, skip missing values (as marked by NaN). By default, only
5965             skips missing values for float dtypes; other dtypes either do not
5966             have a sentinel missing value (int) or skipna=True has not been
5967             implemented (object, datetime64 or timedelta64).
5968 
5969         Returns
5970         -------
5971         result : DataArray or dict of DataArray
5972 
5973         See Also
5974         --------
5975         Variable.argmax, DataArray.idxmax
5976 
5977         Examples
5978         --------
5979         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
5980         >>> array.max()
5981         <xarray.DataArray ()>
5982         array(3)
5983         >>> array.argmax(...)
5984         {'x': <xarray.DataArray ()>
5985         array(3)}
5986         >>> array.isel(array.argmax(...))
5987         <xarray.DataArray ()>
5988         array(3)
5989 
5990         >>> array = xr.DataArray(
5991         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],
5992         ...     dims=("x", "y", "z"),
5993         ... )
5994         >>> array.max(dim="x")
5995         <xarray.DataArray (y: 3, z: 3)>
5996         array([[3, 3, 2],
5997                [3, 5, 2],
5998                [2, 3, 3]])
5999         Dimensions without coordinates: y, z
6000         >>> array.argmax(dim="x")
6001         <xarray.DataArray (y: 3, z: 3)>
6002         array([[0, 1, 1],
6003                [0, 1, 0],
6004                [0, 1, 0]])
6005         Dimensions without coordinates: y, z
6006         >>> array.argmax(dim=["x"])
6007         {'x': <xarray.DataArray (y: 3, z: 3)>
6008         array([[0, 1, 1],
6009                [0, 1, 0],
6010                [0, 1, 0]])
6011         Dimensions without coordinates: y, z}
6012         >>> array.max(dim=("x", "z"))
6013         <xarray.DataArray (y: 3)>
6014         array([3, 5, 3])
6015         Dimensions without coordinates: y
6016         >>> array.argmax(dim=["x", "z"])
6017         {'x': <xarray.DataArray (y: 3)>
6018         array([0, 1, 0])
6019         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
6020         array([0, 1, 2])
6021         Dimensions without coordinates: y}
6022         >>> array.isel(array.argmax(dim=["x", "z"]))
6023         <xarray.DataArray (y: 3)>
6024         array([3, 5, 3])
6025         Dimensions without coordinates: y
6026         """
6027         result = self.variable.argmax(dim, axis, keep_attrs, skipna)
6028         if isinstance(result, dict):
6029             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
6030         else:
6031             return self._replace_maybe_drop_dims(result)
6032 
6033     def query(
6034         self,
6035         queries: Mapping[Any, Any] | None = None,
6036         parser: QueryParserOptions = "pandas",
6037         engine: QueryEngineOptions = None,
6038         missing_dims: ErrorOptionsWithWarn = "raise",
6039         **queries_kwargs: Any,
6040     ) -> DataArray:
6041         """Return a new data array indexed along the specified
6042         dimension(s), where the indexers are given as strings containing
6043         Python expressions to be evaluated against the values in the array.
6044 
6045         Parameters
6046         ----------
6047         queries : dict-like or None, optional
6048             A dict-like with keys matching dimensions and values given by strings
6049             containing Python expressions to be evaluated against the data variables
6050             in the dataset. The expressions will be evaluated using the pandas
6051             eval() function, and can contain any valid Python expressions but cannot
6052             contain any Python statements.
6053         parser : {"pandas", "python"}, default: "pandas"
6054             The parser to use to construct the syntax tree from the expression.
6055             The default of 'pandas' parses code slightly different than standard
6056             Python. Alternatively, you can parse an expression using the 'python'
6057             parser to retain strict Python semantics.
6058         engine : {"python", "numexpr", None}, default: None
6059             The engine used to evaluate the expression. Supported engines are:
6060 
6061             - None: tries to use numexpr, falls back to python
6062             - "numexpr": evaluates expressions using numexpr
6063             - "python": performs operations as if you had eval’d in top level python
6064 
6065         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
6066             What to do if dimensions that should be selected from are not present in the
6067             DataArray:
6068 
6069             - "raise": raise an exception
6070             - "warn": raise a warning, and ignore the missing dimensions
6071             - "ignore": ignore the missing dimensions
6072 
6073         **queries_kwargs : {dim: query, ...}, optional
6074             The keyword arguments form of ``queries``.
6075             One of queries or queries_kwargs must be provided.
6076 
6077         Returns
6078         -------
6079         obj : DataArray
6080             A new DataArray with the same contents as this dataset, indexed by
6081             the results of the appropriate queries.
6082 
6083         See Also
6084         --------
6085         DataArray.isel
6086         Dataset.query
6087         pandas.eval
6088 
6089         Examples
6090         --------
6091         >>> da = xr.DataArray(np.arange(0, 5, 1), dims="x", name="a")
6092         >>> da
6093         <xarray.DataArray 'a' (x: 5)>
6094         array([0, 1, 2, 3, 4])
6095         Dimensions without coordinates: x
6096         >>> da.query(x="a > 2")
6097         <xarray.DataArray 'a' (x: 2)>
6098         array([3, 4])
6099         Dimensions without coordinates: x
6100         """
6101 
6102         ds = self._to_dataset_whole(shallow_copy=True)
6103         ds = ds.query(
6104             queries=queries,
6105             parser=parser,
6106             engine=engine,
6107             missing_dims=missing_dims,
6108             **queries_kwargs,
6109         )
6110         return ds[self.name]
6111 
6112     def curvefit(
6113         self,
6114         coords: str | DataArray | Iterable[str | DataArray],
6115         func: Callable[..., Any],
6116         reduce_dims: Dims = None,
6117         skipna: bool = True,
6118         p0: dict[str, Any] | None = None,
6119         bounds: dict[str, Any] | None = None,
6120         param_names: Sequence[str] | None = None,
6121         kwargs: dict[str, Any] | None = None,
6122     ) -> Dataset:
6123         """
6124         Curve fitting optimization for arbitrary functions.
6125 
6126         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
6127 
6128         Parameters
6129         ----------
6130         coords : Hashable, DataArray, or sequence of DataArray or Hashable
6131             Independent coordinate(s) over which to perform the curve fitting. Must share
6132             at least one dimension with the calling object. When fitting multi-dimensional
6133             functions, supply `coords` as a sequence in the same order as arguments in
6134             `func`. To fit along existing dimensions of the calling object, `coords` can
6135             also be specified as a str or sequence of strs.
6136         func : callable
6137             User specified function in the form `f(x, *params)` which returns a numpy
6138             array of length `len(x)`. `params` are the fittable parameters which are optimized
6139             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
6140             coordinates, e.g. `f((x0, x1), *params)`.
6141         reduce_dims : str, Iterable of Hashable or None, optional
6142             Additional dimension(s) over which to aggregate while fitting. For example,
6143             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
6144             aggregate all lat and lon points and fit the specified function along the
6145             time dimension.
6146         skipna : bool, default: True
6147             Whether to skip missing values when fitting. Default is True.
6148         p0 : dict-like or None, optional
6149             Optional dictionary of parameter names to initial guesses passed to the
6150             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
6151             be assigned initial values following the default scipy behavior.
6152         bounds : dict-like or None, optional
6153             Optional dictionary of parameter names to bounding values passed to the
6154             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
6155             will be unbounded following the default scipy behavior.
6156         param_names : sequence of Hashable or None, optional
6157             Sequence of names for the fittable parameters of `func`. If not supplied,
6158             this will be automatically determined by arguments of `func`. `param_names`
6159             should be manually supplied when fitting a function that takes a variable
6160             number of parameters.
6161         **kwargs : optional
6162             Additional keyword arguments to passed to scipy curve_fit.
6163 
6164         Returns
6165         -------
6166         curvefit_results : Dataset
6167             A single dataset which contains:
6168 
6169             [var]_curvefit_coefficients
6170                 The coefficients of the best fit.
6171             [var]_curvefit_covariance
6172                 The covariance matrix of the coefficient estimates.
6173 
6174         See Also
6175         --------
6176         DataArray.polyfit
6177         scipy.optimize.curve_fit
6178         """
6179         return self._to_temp_dataset().curvefit(
6180             coords,
6181             func,
6182             reduce_dims=reduce_dims,
6183             skipna=skipna,
6184             p0=p0,
6185             bounds=bounds,
6186             param_names=param_names,
6187             kwargs=kwargs,
6188         )
6189 
6190     def drop_duplicates(
6191         self: T_DataArray,
6192         dim: Hashable | Iterable[Hashable],
6193         keep: Literal["first", "last", False] = "first",
6194     ) -> T_DataArray:
6195         """Returns a new DataArray with duplicate dimension values removed.
6196 
6197         Parameters
6198         ----------
6199         dim : dimension label or labels
6200             Pass `...` to drop duplicates along all dimensions.
6201         keep : {"first", "last", False}, default: "first"
6202             Determines which duplicates (if any) to keep.
6203 
6204             - ``"first"`` : Drop duplicates except for the first occurrence.
6205             - ``"last"`` : Drop duplicates except for the last occurrence.
6206             - False : Drop all duplicates.
6207 
6208         Returns
6209         -------
6210         DataArray
6211 
6212         See Also
6213         --------
6214         Dataset.drop_duplicates
6215 
6216         Examples
6217         --------
6218         >>> da = xr.DataArray(
6219         ...     np.arange(25).reshape(5, 5),
6220         ...     dims=("x", "y"),
6221         ...     coords={"x": np.array([0, 0, 1, 2, 3]), "y": np.array([0, 1, 2, 3, 3])},
6222         ... )
6223         >>> da
6224         <xarray.DataArray (x: 5, y: 5)>
6225         array([[ 0,  1,  2,  3,  4],
6226                [ 5,  6,  7,  8,  9],
6227                [10, 11, 12, 13, 14],
6228                [15, 16, 17, 18, 19],
6229                [20, 21, 22, 23, 24]])
6230         Coordinates:
6231           * x        (x) int64 0 0 1 2 3
6232           * y        (y) int64 0 1 2 3 3
6233 
6234         >>> da.drop_duplicates(dim="x")
6235         <xarray.DataArray (x: 4, y: 5)>
6236         array([[ 0,  1,  2,  3,  4],
6237                [10, 11, 12, 13, 14],
6238                [15, 16, 17, 18, 19],
6239                [20, 21, 22, 23, 24]])
6240         Coordinates:
6241           * x        (x) int64 0 1 2 3
6242           * y        (y) int64 0 1 2 3 3
6243 
6244         >>> da.drop_duplicates(dim="x", keep="last")
6245         <xarray.DataArray (x: 4, y: 5)>
6246         array([[ 5,  6,  7,  8,  9],
6247                [10, 11, 12, 13, 14],
6248                [15, 16, 17, 18, 19],
6249                [20, 21, 22, 23, 24]])
6250         Coordinates:
6251           * x        (x) int64 0 1 2 3
6252           * y        (y) int64 0 1 2 3 3
6253 
6254         Drop all duplicate dimension values:
6255 
6256         >>> da.drop_duplicates(dim=...)
6257         <xarray.DataArray (x: 4, y: 4)>
6258         array([[ 0,  1,  2,  3],
6259                [10, 11, 12, 13],
6260                [15, 16, 17, 18],
6261                [20, 21, 22, 23]])
6262         Coordinates:
6263           * x        (x) int64 0 1 2 3
6264           * y        (y) int64 0 1 2 3
6265         """
6266         deduplicated = self._to_temp_dataset().drop_duplicates(dim, keep=keep)
6267         return self._from_temp_dataset(deduplicated)
6268 
6269     def convert_calendar(
6270         self,
6271         calendar: str,
6272         dim: str = "time",
6273         align_on: str | None = None,
6274         missing: Any | None = None,
6275         use_cftime: bool | None = None,
6276     ) -> DataArray:
6277         """Convert the DataArray to another calendar.
6278 
6279         Only converts the individual timestamps, does not modify any data except
6280         in dropping invalid/surplus dates or inserting missing dates.
6281 
6282         If the source and target calendars are either no_leap, all_leap or a
6283         standard type, only the type of the time array is modified.
6284         When converting to a leap year from a non-leap year, the 29th of February
6285         is removed from the array. In the other direction the 29th of February
6286         will be missing in the output, unless `missing` is specified,
6287         in which case that value is inserted.
6288 
6289         For conversions involving `360_day` calendars, see Notes.
6290 
6291         This method is safe to use with sub-daily data as it doesn't touch the
6292         time part of the timestamps.
6293 
6294         Parameters
6295         ---------
6296         calendar : str
6297             The target calendar name.
6298         dim : str
6299             Name of the time coordinate.
6300         align_on : {None, 'date', 'year'}
6301             Must be specified when either source or target is a `360_day` calendar,
6302            ignored otherwise. See Notes.
6303         missing : Optional[any]
6304             By default, i.e. if the value is None, this method will simply attempt
6305             to convert the dates in the source calendar to the same dates in the
6306             target calendar, and drop any of those that are not possible to
6307             represent.  If a value is provided, a new time coordinate will be
6308             created in the target calendar with the same frequency as the original
6309             time coordinate; for any dates that are not present in the source, the
6310             data will be filled with this value.  Note that using this mode requires
6311             that the source data have an inferable frequency; for more information
6312             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
6313             target calendar combinations, this could result in many missing values, see notes.
6314         use_cftime : boolean, optional
6315             Whether to use cftime objects in the output, only used if `calendar`
6316             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
6317             If True, the new time axis uses cftime objects.
6318             If None (default), it uses :py:class:`numpy.datetime64` values if the
6319             date range permits it, and :py:class:`cftime.datetime` objects if not.
6320             If False, it uses :py:class:`numpy.datetime64`  or fails.
6321 
6322         Returns
6323         -------
6324         DataArray
6325             Copy of the dataarray with the time coordinate converted to the
6326             target calendar. If 'missing' was None (default), invalid dates in
6327             the new calendar are dropped, but missing dates are not inserted.
6328             If `missing` was given, the new data is reindexed to have a time axis
6329             with the same frequency as the source, but in the new calendar; any
6330             missing datapoints are filled with `missing`.
6331 
6332         Notes
6333         -----
6334         Passing a value to `missing` is only usable if the source's time coordinate as an
6335         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
6336         if the target coordinate, generated from this frequency, has dates equivalent to the
6337         source. It is usually **not** appropriate to use this mode with:
6338 
6339         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
6340         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
6341             or 'mH' where 24 % m != 0).
6342 
6343         If one of the source or target calendars is `"360_day"`, `align_on` must
6344         be specified and two options are offered.
6345 
6346         - "year"
6347             The dates are translated according to their relative position in the year,
6348             ignoring their original month and day information, meaning that the
6349             missing/surplus days are added/removed at regular intervals.
6350 
6351             From a `360_day` to a standard calendar, the output will be missing the
6352             following dates (day of year in parentheses):
6353 
6354             To a leap year:
6355                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
6356                 September 31st (275) and November 30th (335).
6357             To a non-leap year:
6358                 February 6th (36), April 19th (109), July 2nd (183),
6359                 September 12th (255), November 25th (329).
6360 
6361             From a standard calendar to a `"360_day"`, the following dates in the
6362             source array will be dropped:
6363 
6364             From a leap year:
6365                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
6366                 September 31st (275), December 1st (336)
6367             From a non-leap year:
6368                 February 6th (37), April 20th (110), July 2nd (183),
6369                 September 13th (256), November 25th (329)
6370 
6371             This option is best used on daily and subdaily data.
6372 
6373         - "date"
6374             The month/day information is conserved and invalid dates are dropped
6375             from the output. This means that when converting from a `"360_day"` to a
6376             standard calendar, all 31st (Jan, March, May, July, August, October and
6377             December) will be missing as there is no equivalent dates in the
6378             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
6379             will be dropped as there are no equivalent dates in a standard calendar.
6380 
6381             This option is best used with data on a frequency coarser than daily.
6382         """
6383         return convert_calendar(
6384             self,
6385             calendar,
6386             dim=dim,
6387             align_on=align_on,
6388             missing=missing,
6389             use_cftime=use_cftime,
6390         )
6391 
6392     def interp_calendar(
6393         self,
6394         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
6395         dim: str = "time",
6396     ) -> DataArray:
6397         """Interpolates the DataArray to another calendar based on decimal year measure.
6398 
6399         Each timestamp in `source` and `target` are first converted to their decimal
6400         year equivalent then `source` is interpolated on the target coordinate.
6401         The decimal year of a timestamp is its year plus its sub-year component
6402         converted to the fraction of its year. For example "2000-03-01 12:00" is
6403         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
6404 
6405         This method should only be used when the time (HH:MM:SS) information of
6406         time coordinate is not important.
6407 
6408         Parameters
6409         ----------
6410         target: DataArray or DatetimeIndex or CFTimeIndex
6411             The target time coordinate of a valid dtype
6412             (np.datetime64 or cftime objects)
6413         dim : str
6414             The time coordinate name.
6415 
6416         Return
6417         ------
6418         DataArray
6419             The source interpolated on the decimal years of target,
6420         """
6421         return interp_calendar(self, target, dim=dim)
6422 
6423     def groupby(
6424         self,
6425         group: Hashable | DataArray | IndexVariable,
6426         squeeze: bool = True,
6427         restore_coord_dims: bool = False,
6428     ) -> DataArrayGroupBy:
6429         """Returns a DataArrayGroupBy object for performing grouped operations.
6430 
6431         Parameters
6432         ----------
6433         group : Hashable, DataArray or IndexVariable
6434             Array whose unique values should be used to group this array. If a
6435             Hashable, must be the name of a coordinate contained in this dataarray.
6436         squeeze : bool, default: True
6437             If "group" is a dimension of any arrays in this dataset, `squeeze`
6438             controls whether the subarrays have a dimension of length 1 along
6439             that dimension or if the dimension is squeezed out.
6440         restore_coord_dims : bool, default: False
6441             If True, also restore the dimension order of multi-dimensional
6442             coordinates.
6443 
6444         Returns
6445         -------
6446         grouped : DataArrayGroupBy
6447             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
6448             iterated over in the form of `(unique_value, grouped_array)` pairs.
6449 
6450         Examples
6451         --------
6452         Calculate daily anomalies for daily data:
6453 
6454         >>> da = xr.DataArray(
6455         ...     np.linspace(0, 1826, num=1827),
6456         ...     coords=[pd.date_range("2000-01-01", "2004-12-31", freq="D")],
6457         ...     dims="time",
6458         ... )
6459         >>> da
6460         <xarray.DataArray (time: 1827)>
6461         array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03,
6462                1.826e+03])
6463         Coordinates:
6464           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
6465         >>> da.groupby("time.dayofyear") - da.groupby("time.dayofyear").mean("time")
6466         <xarray.DataArray (time: 1827)>
6467         array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])
6468         Coordinates:
6469           * time       (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
6470             dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 359 360 361 362 363 364 365 366
6471 
6472         See Also
6473         --------
6474         :ref:`groupby`
6475             Users guide explanation of how to group and bin data.
6476         DataArray.groupby_bins
6477         Dataset.groupby
6478         core.groupby.DataArrayGroupBy
6479         pandas.DataFrame.groupby
6480         """
6481         from xarray.core.groupby import (
6482             DataArrayGroupBy,
6483             ResolvedUniqueGrouper,
6484             UniqueGrouper,
6485             _validate_groupby_squeeze,
6486         )
6487 
6488         _validate_groupby_squeeze(squeeze)
6489         rgrouper = ResolvedUniqueGrouper(UniqueGrouper(), group, self)
6490         return DataArrayGroupBy(
6491             self,
6492             (rgrouper,),
6493             squeeze=squeeze,
6494             restore_coord_dims=restore_coord_dims,
6495         )
6496 
6497     def groupby_bins(
6498         self,
6499         group: Hashable | DataArray | IndexVariable,
6500         bins: ArrayLike,
6501         right: bool = True,
6502         labels: ArrayLike | Literal[False] | None = None,
6503         precision: int = 3,
6504         include_lowest: bool = False,
6505         squeeze: bool = True,
6506         restore_coord_dims: bool = False,
6507     ) -> DataArrayGroupBy:
6508         """Returns a DataArrayGroupBy object for performing grouped operations.
6509 
6510         Rather than using all unique values of `group`, the values are discretized
6511         first by applying `pandas.cut` [1]_ to `group`.
6512 
6513         Parameters
6514         ----------
6515         group : Hashable, DataArray or IndexVariable
6516             Array whose binned values should be used to group this array. If a
6517             Hashable, must be the name of a coordinate contained in this dataarray.
6518         bins : int or array-like
6519             If bins is an int, it defines the number of equal-width bins in the
6520             range of x. However, in this case, the range of x is extended by .1%
6521             on each side to include the min or max values of x. If bins is a
6522             sequence it defines the bin edges allowing for non-uniform bin
6523             width. No extension of the range of x is done in this case.
6524         right : bool, default: True
6525             Indicates whether the bins include the rightmost edge or not. If
6526             right == True (the default), then the bins [1,2,3,4] indicate
6527             (1,2], (2,3], (3,4].
6528         labels : array-like, False or None, default: None
6529             Used as labels for the resulting bins. Must be of the same length as
6530             the resulting bins. If False, string bin labels are assigned by
6531             `pandas.cut`.
6532         precision : int, default: 3
6533             The precision at which to store and display the bins labels.
6534         include_lowest : bool, default: False
6535             Whether the first interval should be left-inclusive or not.
6536         squeeze : bool, default: True
6537             If "group" is a dimension of any arrays in this dataset, `squeeze`
6538             controls whether the subarrays have a dimension of length 1 along
6539             that dimension or if the dimension is squeezed out.
6540         restore_coord_dims : bool, default: False
6541             If True, also restore the dimension order of multi-dimensional
6542             coordinates.
6543 
6544         Returns
6545         -------
6546         grouped : DataArrayGroupBy
6547             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
6548             iterated over in the form of `(unique_value, grouped_array)` pairs.
6549             The name of the group has the added suffix `_bins` in order to
6550             distinguish it from the original variable.
6551 
6552         See Also
6553         --------
6554         :ref:`groupby`
6555             Users guide explanation of how to group and bin data.
6556         DataArray.groupby
6557         Dataset.groupby_bins
6558         core.groupby.DataArrayGroupBy
6559         pandas.DataFrame.groupby
6560 
6561         References
6562         ----------
6563         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
6564         """
6565         from xarray.core.groupby import (
6566             BinGrouper,
6567             DataArrayGroupBy,
6568             ResolvedBinGrouper,
6569             _validate_groupby_squeeze,
6570         )
6571 
6572         _validate_groupby_squeeze(squeeze)
6573         grouper = BinGrouper(
6574             bins=bins,
6575             cut_kwargs={
6576                 "right": right,
6577                 "labels": labels,
6578                 "precision": precision,
6579                 "include_lowest": include_lowest,
6580             },
6581         )
6582         rgrouper = ResolvedBinGrouper(grouper, group, self)
6583 
6584         return DataArrayGroupBy(
6585             self,
6586             (rgrouper,),
6587             squeeze=squeeze,
6588             restore_coord_dims=restore_coord_dims,
6589         )
6590 
6591     def weighted(self, weights: DataArray) -> DataArrayWeighted:
6592         """
6593         Weighted DataArray operations.
6594 
6595         Parameters
6596         ----------
6597         weights : DataArray
6598             An array of weights associated with the values in this Dataset.
6599             Each value in the data contributes to the reduction operation
6600             according to its associated weight.
6601 
6602         Notes
6603         -----
6604         ``weights`` must be a DataArray and cannot contain missing values.
6605         Missing values can be replaced by ``weights.fillna(0)``.
6606 
6607         Returns
6608         -------
6609         core.weighted.DataArrayWeighted
6610 
6611         See Also
6612         --------
6613         Dataset.weighted
6614         """
6615         from xarray.core.weighted import DataArrayWeighted
6616 
6617         return DataArrayWeighted(self, weights)
6618 
6619     def rolling(
6620         self,
6621         dim: Mapping[Any, int] | None = None,
6622         min_periods: int | None = None,
6623         center: bool | Mapping[Any, bool] = False,
6624         **window_kwargs: int,
6625     ) -> DataArrayRolling:
6626         """
6627         Rolling window object for DataArrays.
6628 
6629         Parameters
6630         ----------
6631         dim : dict, optional
6632             Mapping from the dimension name to create the rolling iterator
6633             along (e.g. `time`) to its moving window size.
6634         min_periods : int or None, default: None
6635             Minimum number of observations in window required to have a value
6636             (otherwise result is NA). The default, None, is equivalent to
6637             setting min_periods equal to the size of the window.
6638         center : bool or Mapping to int, default: False
6639             Set the labels at the center of the window.
6640         **window_kwargs : optional
6641             The keyword arguments form of ``dim``.
6642             One of dim or window_kwargs must be provided.
6643 
6644         Returns
6645         -------
6646         core.rolling.DataArrayRolling
6647 
6648         Examples
6649         --------
6650         Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:
6651 
6652         >>> da = xr.DataArray(
6653         ...     np.linspace(0, 11, num=12),
6654         ...     coords=[
6655         ...         pd.date_range(
6656         ...             "1999-12-15",
6657         ...             periods=12,
6658         ...             freq=pd.DateOffset(months=1),
6659         ...         )
6660         ...     ],
6661         ...     dims="time",
6662         ... )
6663         >>> da
6664         <xarray.DataArray (time: 12)>
6665         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
6666         Coordinates:
6667           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6668         >>> da.rolling(time=3, center=True).mean()
6669         <xarray.DataArray (time: 12)>
6670         array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])
6671         Coordinates:
6672           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6673 
6674         Remove the NaNs using ``dropna()``:
6675 
6676         >>> da.rolling(time=3, center=True).mean().dropna("time")
6677         <xarray.DataArray (time: 10)>
6678         array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])
6679         Coordinates:
6680           * time     (time) datetime64[ns] 2000-01-15 2000-02-15 ... 2000-10-15
6681 
6682         See Also
6683         --------
6684         core.rolling.DataArrayRolling
6685         Dataset.rolling
6686         """
6687         from xarray.core.rolling import DataArrayRolling
6688 
6689         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
6690         return DataArrayRolling(self, dim, min_periods=min_periods, center=center)
6691 
6692     def coarsen(
6693         self,
6694         dim: Mapping[Any, int] | None = None,
6695         boundary: CoarsenBoundaryOptions = "exact",
6696         side: SideOptions | Mapping[Any, SideOptions] = "left",
6697         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
6698         **window_kwargs: int,
6699     ) -> DataArrayCoarsen:
6700         """
6701         Coarsen object for DataArrays.
6702 
6703         Parameters
6704         ----------
6705         dim : mapping of hashable to int, optional
6706             Mapping from the dimension name to the window size.
6707         boundary : {"exact", "trim", "pad"}, default: "exact"
6708             If 'exact', a ValueError will be raised if dimension size is not a
6709             multiple of the window size. If 'trim', the excess entries are
6710             dropped. If 'pad', NA will be padded.
6711         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
6712         coord_func : str or mapping of hashable to str, default: "mean"
6713             function (name) that is applied to the coordinates,
6714             or a mapping from coordinate name to function (name).
6715 
6716         Returns
6717         -------
6718         core.rolling.DataArrayCoarsen
6719 
6720         Examples
6721         --------
6722         Coarsen the long time series by averaging over every three days.
6723 
6724         >>> da = xr.DataArray(
6725         ...     np.linspace(0, 364, num=364),
6726         ...     dims="time",
6727         ...     coords={"time": pd.date_range("1999-12-15", periods=364)},
6728         ... )
6729         >>> da  # +doctest: ELLIPSIS
6730         <xarray.DataArray (time: 364)>
6731         array([  0.        ,   1.00275482,   2.00550964,   3.00826446,
6732                  4.01101928,   5.0137741 ,   6.01652893,   7.01928375,
6733                  8.02203857,   9.02479339,  10.02754821,  11.03030303,
6734         ...
6735                356.98071625, 357.98347107, 358.9862259 , 359.98898072,
6736                360.99173554, 361.99449036, 362.99724518, 364.        ])
6737         Coordinates:
6738           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-12-12
6739         >>> da.coarsen(time=3, boundary="trim").mean()  # +doctest: ELLIPSIS
6740         <xarray.DataArray (time: 121)>
6741         array([  1.00275482,   4.01101928,   7.01928375,  10.02754821,
6742                 13.03581267,  16.04407713,  19.0523416 ,  22.06060606,
6743                 25.06887052,  28.07713499,  31.08539945,  34.09366391,
6744         ...
6745                349.96143251, 352.96969697, 355.97796143, 358.9862259 ,
6746                361.99449036])
6747         Coordinates:
6748           * time     (time) datetime64[ns] 1999-12-16 1999-12-19 ... 2000-12-10
6749         >>>
6750 
6751         See Also
6752         --------
6753         core.rolling.DataArrayCoarsen
6754         Dataset.coarsen
6755         """
6756         from xarray.core.rolling import DataArrayCoarsen
6757 
6758         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
6759         return DataArrayCoarsen(
6760             self,
6761             dim,
6762             boundary=boundary,
6763             side=side,
6764             coord_func=coord_func,
6765         )
6766 
6767     def resample(
6768         self,
6769         indexer: Mapping[Any, str] | None = None,
6770         skipna: bool | None = None,
6771         closed: SideOptions | None = None,
6772         label: SideOptions | None = None,
6773         base: int | None = None,
6774         offset: pd.Timedelta | datetime.timedelta | str | None = None,
6775         origin: str | DatetimeLike = "start_day",
6776         keep_attrs: bool | None = None,
6777         loffset: datetime.timedelta | str | None = None,
6778         restore_coord_dims: bool | None = None,
6779         **indexer_kwargs: str,
6780     ) -> DataArrayResample:
6781         """Returns a Resample object for performing resampling operations.
6782 
6783         Handles both downsampling and upsampling. The resampled
6784         dimension must be a datetime-like coordinate. If any intervals
6785         contain no values from the original object, they will be given
6786         the value ``NaN``.
6787 
6788         Parameters
6789         ----------
6790         indexer : Mapping of Hashable to str, optional
6791             Mapping from the dimension name to resample frequency [1]_. The
6792             dimension must be datetime-like.
6793         skipna : bool, optional
6794             Whether to skip missing values when aggregating in downsampling.
6795         closed : {"left", "right"}, optional
6796             Side of each interval to treat as closed.
6797         label : {"left", "right"}, optional
6798             Side of each interval to use for labeling.
6799         base : int, optional
6800             For frequencies that evenly subdivide 1 day, the "origin" of the
6801             aggregated intervals. For example, for "24H" frequency, base could
6802             range from 0 through 23.
6803         origin : {'epoch', 'start', 'start_day', 'end', 'end_day'}, pd.Timestamp, datetime.datetime, np.datetime64, or cftime.datetime, default 'start_day'
6804             The datetime on which to adjust the grouping. The timezone of origin
6805             must match the timezone of the index.
6806 
6807             If a datetime is not used, these values are also supported:
6808             - 'epoch': `origin` is 1970-01-01
6809             - 'start': `origin` is the first value of the timeseries
6810             - 'start_day': `origin` is the first day at midnight of the timeseries
6811             - 'end': `origin` is the last value of the timeseries
6812             - 'end_day': `origin` is the ceiling midnight of the last day
6813         offset : pd.Timedelta, datetime.timedelta, or str, default is None
6814             An offset timedelta added to the origin.
6815         loffset : timedelta or str, optional
6816             Offset used to adjust the resampled time labels. Some pandas date
6817             offset strings are supported.
6818         restore_coord_dims : bool, optional
6819             If True, also restore the dimension order of multi-dimensional
6820             coordinates.
6821         **indexer_kwargs : str
6822             The keyword arguments form of ``indexer``.
6823             One of indexer or indexer_kwargs must be provided.
6824 
6825         Returns
6826         -------
6827         resampled : core.resample.DataArrayResample
6828             This object resampled.
6829 
6830         Examples
6831         --------
6832         Downsample monthly time-series data to seasonal data:
6833 
6834         >>> da = xr.DataArray(
6835         ...     np.linspace(0, 11, num=12),
6836         ...     coords=[
6837         ...         pd.date_range(
6838         ...             "1999-12-15",
6839         ...             periods=12,
6840         ...             freq=pd.DateOffset(months=1),
6841         ...         )
6842         ...     ],
6843         ...     dims="time",
6844         ... )
6845         >>> da
6846         <xarray.DataArray (time: 12)>
6847         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
6848         Coordinates:
6849           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6850         >>> da.resample(time="QS-DEC").mean()
6851         <xarray.DataArray (time: 4)>
6852         array([ 1.,  4.,  7., 10.])
6853         Coordinates:
6854           * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01
6855 
6856         Upsample monthly time-series data to daily data:
6857 
6858         >>> da.resample(time="1D").interpolate("linear")  # +doctest: ELLIPSIS
6859         <xarray.DataArray (time: 337)>
6860         array([ 0.        ,  0.03225806,  0.06451613,  0.09677419,  0.12903226,
6861                 0.16129032,  0.19354839,  0.22580645,  0.25806452,  0.29032258,
6862                 0.32258065,  0.35483871,  0.38709677,  0.41935484,  0.4516129 ,
6863         ...
6864                10.80645161, 10.83870968, 10.87096774, 10.90322581, 10.93548387,
6865                10.96774194, 11.        ])
6866         Coordinates:
6867           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
6868 
6869         Limit scope of upsampling method
6870 
6871         >>> da.resample(time="1D").nearest(tolerance="1D")
6872         <xarray.DataArray (time: 337)>
6873         array([ 0.,  0., nan, ..., nan, 11., 11.])
6874         Coordinates:
6875           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
6876 
6877         See Also
6878         --------
6879         Dataset.resample
6880         pandas.Series.resample
6881         pandas.DataFrame.resample
6882 
6883         References
6884         ----------
6885         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
6886         """
6887         from xarray.core.resample import DataArrayResample
6888 
6889         return self._resample(
6890             resample_cls=DataArrayResample,
6891             indexer=indexer,
6892             skipna=skipna,
6893             closed=closed,
6894             label=label,
6895             base=base,
6896             offset=offset,
6897             origin=origin,
6898             keep_attrs=keep_attrs,
6899             loffset=loffset,
6900             restore_coord_dims=restore_coord_dims,
6901             **indexer_kwargs,
6902         )
6903 
6904     def to_dask_dataframe(
6905         self,
6906         dim_order: Sequence[Hashable] | None = None,
6907         set_index: bool = False,
6908     ) -> DaskDataFrame:
6909         """Convert this array into a dask.dataframe.DataFrame.
6910 
6911         Parameters
6912         ----------
6913         dim_order : Sequence of Hashable or None , optional
6914             Hierarchical dimension order for the resulting dataframe.
6915             Array content is transposed to this order and then written out as flat
6916             vectors in contiguous order, so the last dimension in this list
6917             will be contiguous in the resulting DataFrame. This has a major influence
6918             on which operations are efficient on the resulting dask dataframe.
6919         set_index : bool, default: False
6920             If set_index=True, the dask DataFrame is indexed by this dataset's
6921             coordinate. Since dask DataFrames do not support multi-indexes,
6922             set_index only works if the dataset only contains one dimension.
6923 
6924         Returns
6925         -------
6926         dask.dataframe.DataFrame
6927 
6928         Examples
6929         --------
6930         >>> da = xr.DataArray(
6931         ...     np.arange(4 * 2 * 2).reshape(4, 2, 2),
6932         ...     dims=("time", "lat", "lon"),
6933         ...     coords={
6934         ...         "time": np.arange(4),
6935         ...         "lat": [-30, -20],
6936         ...         "lon": [120, 130],
6937         ...     },
6938         ...     name="eg_dataarray",
6939         ...     attrs={"units": "Celsius", "description": "Random temperature data"},
6940         ... )
6941         >>> da.to_dask_dataframe(["lat", "lon", "time"]).compute()
6942             lat  lon  time  eg_dataarray
6943         0   -30  120     0             0
6944         1   -30  120     1             4
6945         2   -30  120     2             8
6946         3   -30  120     3            12
6947         4   -30  130     0             1
6948         5   -30  130     1             5
6949         6   -30  130     2             9
6950         7   -30  130     3            13
6951         8   -20  120     0             2
6952         9   -20  120     1             6
6953         10  -20  120     2            10
6954         11  -20  120     3            14
6955         12  -20  130     0             3
6956         13  -20  130     1             7
6957         14  -20  130     2            11
6958         15  -20  130     3            15
6959         """
6960         if self.name is None:
6961             raise ValueError(
6962                 "Cannot convert an unnamed DataArray to a "
6963                 "dask dataframe : use the ``.rename`` method to assign a name."
6964             )
6965         name = self.name
6966         ds = self._to_dataset_whole(name, shallow_copy=False)
6967         return ds.to_dask_dataframe(dim_order, set_index)
6968 
6969     # this needs to be at the end, or mypy will confuse with `str`
6970     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
6971     str = utils.UncachedAccessor(StringAccessor["DataArray"])

</code><intended_behavior>These methods should be refactored to an interface that can dynamically dispatch the calls to the appropriate backend based on user configuration or an attribute within the DataArray/Dataset. The existing functionality for dask should be preserved, allowing for a seamless transition between backends without altering the user's experience.</intended_behavior>, <file>xarray/core/dataset.py</file>
<class>None</class>
<method>None</method>
<code>
1 from __future__ import annotations
2 
3 import copy
4 import datetime
5 import inspect
6 import itertools
7 import math
8 import sys
9 import warnings
10 from collections import defaultdict
11 from collections.abc import (
12     Collection,
13     Hashable,
14     Iterable,
15     Iterator,
16     Mapping,
17     MutableMapping,
18     Sequence,
19 )
20 from html import escape
21 from numbers import Number
22 from operator import methodcaller
23 from os import PathLike
24 from typing import IO, TYPE_CHECKING, Any, Callable, Generic, Literal, cast, overload
25 
26 import numpy as np
27 import pandas as pd
28 
29 from xarray.coding.calendar_ops import convert_calendar, interp_calendar
30 from xarray.coding.cftimeindex import CFTimeIndex, _parse_array_of_cftime_strings
31 from xarray.core import (
32     alignment,
33     duck_array_ops,
34     formatting,
35     formatting_html,
36     ops,
37     utils,
38 )
39 from xarray.core import dtypes as xrdtypes
40 from xarray.core._aggregations import DatasetAggregations
41 from xarray.core.alignment import (
42     _broadcast_helper,
43     _get_broadcast_dims_map_common_coords,
44     align,
45 )
46 from xarray.core.arithmetic import DatasetArithmetic
47 from xarray.core.common import (
48     DataWithCoords,
49     _contains_datetime_like_objects,
50     get_chunksizes,
51 )
52 from xarray.core.computation import unify_chunks
53 from xarray.core.coordinates import DatasetCoordinates, assert_coordinate_consistent
54 from xarray.core.duck_array_ops import datetime_to_numeric
55 from xarray.core.indexes import (
56     Index,
57     Indexes,
58     PandasIndex,
59     PandasMultiIndex,
60     assert_no_index_corrupted,
61     create_default_index_implicit,
62     filter_indexes_from_coords,
63     isel_indexes,
64     remove_unused_levels_categories,
65     roll_indexes,
66 )
67 from xarray.core.indexing import is_fancy_indexer, map_index_queries
68 from xarray.core.merge import (
69     dataset_merge_method,
70     dataset_update_method,
71     merge_coordinates_without_align,
72     merge_data_and_coords,
73 )
74 from xarray.core.missing import get_clean_interp_index
75 from xarray.core.options import OPTIONS, _get_keep_attrs
76 from xarray.core.pycompat import array_type, is_duck_array, is_duck_dask_array
77 from xarray.core.types import QuantileMethods, T_Dataset
78 from xarray.core.utils import (
79     Default,
80     Frozen,
81     HybridMappingProxy,
82     OrderedSet,
83     _default,
84     decode_numpy_dict_values,
85     drop_dims_from_indexers,
86     either_dict_or_kwargs,
87     infix_dims,
88     is_dict_like,
89     is_scalar,
90     maybe_wrap_array,
91 )
92 from xarray.core.variable import (
93     IndexVariable,
94     Variable,
95     as_variable,
96     broadcast_variables,
97     calculate_dimensions,
98 )
99 from xarray.plot.accessor import DatasetPlotAccessor
100 
101 if TYPE_CHECKING:
102     from numpy.typing import ArrayLike
103 
104     from xarray.backends import AbstractDataStore, ZarrStore
105     from xarray.backends.api import T_NetcdfEngine, T_NetcdfTypes
106     from xarray.core.coordinates import Coordinates
107     from xarray.core.dataarray import DataArray
108     from xarray.core.groupby import DatasetGroupBy
109     from xarray.core.merge import CoercibleMapping
110     from xarray.core.resample import DatasetResample
111     from xarray.core.rolling import DatasetCoarsen, DatasetRolling
112     from xarray.core.types import (
113         CFCalendar,
114         CoarsenBoundaryOptions,
115         CombineAttrsOptions,
116         CompatOptions,
117         DatetimeLike,
118         DatetimeUnitOptions,
119         Dims,
120         ErrorOptions,
121         ErrorOptionsWithWarn,
122         InterpOptions,
123         JoinOptions,
124         PadModeOptions,
125         PadReflectOptions,
126         QueryEngineOptions,
127         QueryParserOptions,
128         ReindexMethodOptions,
129         SideOptions,
130         T_Xarray,
131     )
132     from xarray.core.weighted import DatasetWeighted
133 
134     try:
135         from dask.delayed import Delayed
136     except ImportError:
137         Delayed = None  # type: ignore
138     try:
139         from dask.dataframe import DataFrame as DaskDataFrame
140     except ImportError:
141         DaskDataFrame = None  # type: ignore
142 
143 
144 # list of attributes of pd.DatetimeIndex that are ndarrays of time info
145 _DATETIMEINDEX_COMPONENTS = [
146     "year",
147     "month",
148     "day",
149     "hour",
150     "minute",
151     "second",
152     "microsecond",
153     "nanosecond",
154     "date",
155     "time",
156     "dayofyear",
157     "weekofyear",
158     "dayofweek",
159     "quarter",
160 ]
161 
162 
163 def _get_virtual_variable(
164     variables, key: Hashable, dim_sizes: Mapping | None = None
165 ) -> tuple[Hashable, Hashable, Variable]:
166     """Get a virtual variable (e.g., 'time.year') from a dict of xarray.Variable
167     objects (if possible)
168 
169     """
170     from xarray.core.dataarray import DataArray
171 
172     if dim_sizes is None:
173         dim_sizes = {}
174 
175     if key in dim_sizes:
176         data = pd.Index(range(dim_sizes[key]), name=key)
177         variable = IndexVariable((key,), data)
178         return key, key, variable
179 
180     if not isinstance(key, str):
181         raise KeyError(key)
182 
183     split_key = key.split(".", 1)
184     if len(split_key) != 2:
185         raise KeyError(key)
186 
187     ref_name, var_name = split_key
188     ref_var = variables[ref_name]
189 
190     if _contains_datetime_like_objects(ref_var):
191         ref_var = DataArray(ref_var)
192         data = getattr(ref_var.dt, var_name).data
193     else:
194         data = getattr(ref_var, var_name).data
195     virtual_var = Variable(ref_var.dims, data)
196 
197     return ref_name, var_name, virtual_var
198 
199 
200 def _assert_empty(args: tuple, msg: str = "%s") -> None:
201     if args:
202         raise ValueError(msg % args)
203 
204 
205 def _get_chunk(var, chunks):
206     """
207     Return map from each dim to chunk sizes, accounting for backend's preferred chunks.
208     """
209 
210     import dask.array as da
211 
212     if isinstance(var, IndexVariable):
213         return {}
214     dims = var.dims
215     shape = var.shape
216 
217     # Determine the explicit requested chunks.
218     preferred_chunks = var.encoding.get("preferred_chunks", {})
219     preferred_chunk_shape = tuple(
220         preferred_chunks.get(dim, size) for dim, size in zip(dims, shape)
221     )
222     if isinstance(chunks, Number) or (chunks == "auto"):
223         chunks = dict.fromkeys(dims, chunks)
224     chunk_shape = tuple(
225         chunks.get(dim, None) or preferred_chunk_sizes
226         for dim, preferred_chunk_sizes in zip(dims, preferred_chunk_shape)
227     )
228     chunk_shape = da.core.normalize_chunks(
229         chunk_shape, shape=shape, dtype=var.dtype, previous_chunks=preferred_chunk_shape
230     )
231 
232     # Warn where requested chunks break preferred chunks, provided that the variable
233     # contains data.
234     if var.size:
235         for dim, size, chunk_sizes in zip(dims, shape, chunk_shape):
236             try:
237                 preferred_chunk_sizes = preferred_chunks[dim]
238             except KeyError:
239                 continue
240             # Determine the stop indices of the preferred chunks, but omit the last stop
241             # (equal to the dim size).  In particular, assume that when a sequence
242             # expresses the preferred chunks, the sequence sums to the size.
243             preferred_stops = (
244                 range(preferred_chunk_sizes, size, preferred_chunk_sizes)
245                 if isinstance(preferred_chunk_sizes, Number)
246                 else itertools.accumulate(preferred_chunk_sizes[:-1])
247             )
248             # Gather any stop indices of the specified chunks that are not a stop index
249             # of a preferred chunk.  Again, omit the last stop, assuming that it equals
250             # the dim size.
251             breaks = set(itertools.accumulate(chunk_sizes[:-1])).difference(
252                 preferred_stops
253             )
254             if breaks:
255                 warnings.warn(
256                     "The specified Dask chunks separate the stored chunks along "
257                     f'dimension "{dim}" starting at index {min(breaks)}. This could '
258                     "degrade performance. Instead, consider rechunking after loading."
259                 )
260 
261     return dict(zip(dims, chunk_shape))
262 
263 
264 def _maybe_chunk(
265     name,
266     var,
267     chunks,
268     token=None,
269     lock=None,
270     name_prefix="xarray-",
271     overwrite_encoded_chunks=False,
272     inline_array=False,
273 ):
274     from dask.base import tokenize
275 
276     if chunks is not None:
277         chunks = {dim: chunks[dim] for dim in var.dims if dim in chunks}
278     if var.ndim:
279         # when rechunking by different amounts, make sure dask names change
280         # by provinding chunks as an input to tokenize.
281         # subtle bugs result otherwise. see GH3350
282         token2 = tokenize(name, token if token else var._data, chunks)
283         name2 = f"{name_prefix}{name}-{token2}"
284         var = var.chunk(chunks, name=name2, lock=lock, inline_array=inline_array)
285 
286         if overwrite_encoded_chunks and var.chunks is not None:
287             var.encoding["chunks"] = tuple(x[0] for x in var.chunks)
288         return var
289     else:
290         return var
291 
292 
293 def as_dataset(obj: Any) -> Dataset:
294     """Cast the given object to a Dataset.
295 
296     Handles Datasets, DataArrays and dictionaries of variables. A new Dataset
297     object is only created if the provided object is not already one.
298     """
299     if hasattr(obj, "to_dataset"):
300         obj = obj.to_dataset()
301     if not isinstance(obj, Dataset):
302         obj = Dataset(obj)
303     return obj
304 
305 
306 def _get_func_args(func, param_names):
307     """Use `inspect.signature` to try accessing `func` args. Otherwise, ensure
308     they are provided by user.
309     """
310     try:
311         func_args = inspect.signature(func).parameters
312     except ValueError:
313         func_args = {}
314         if not param_names:
315             raise ValueError(
316                 "Unable to inspect `func` signature, and `param_names` was not provided."
317             )
318     if param_names:
319         params = param_names
320     else:
321         params = list(func_args)[1:]
322         if any(
323             [(p.kind in [p.VAR_POSITIONAL, p.VAR_KEYWORD]) for p in func_args.values()]
324         ):
325             raise ValueError(
326                 "`param_names` must be provided because `func` takes variable length arguments."
327             )
328     return params, func_args
329 
330 
331 def _initialize_curvefit_params(params, p0, bounds, func_args):
332     """Set initial guess and bounds for curvefit.
333     Priority: 1) passed args 2) func signature 3) scipy defaults
334     """
335 
336     def _initialize_feasible(lb, ub):
337         # Mimics functionality of scipy.optimize.minpack._initialize_feasible
338         lb_finite = np.isfinite(lb)
339         ub_finite = np.isfinite(ub)
340         p0 = np.nansum(
341             [
342                 0.5 * (lb + ub) * int(lb_finite & ub_finite),
343                 (lb + 1) * int(lb_finite & ~ub_finite),
344                 (ub - 1) * int(~lb_finite & ub_finite),
345             ]
346         )
347         return p0
348 
349     param_defaults = {p: 1 for p in params}
350     bounds_defaults = {p: (-np.inf, np.inf) for p in params}
351     for p in params:
352         if p in func_args and func_args[p].default is not func_args[p].empty:
353             param_defaults[p] = func_args[p].default
354         if p in bounds:
355             bounds_defaults[p] = tuple(bounds[p])
356             if param_defaults[p] < bounds[p][0] or param_defaults[p] > bounds[p][1]:
357                 param_defaults[p] = _initialize_feasible(bounds[p][0], bounds[p][1])
358         if p in p0:
359             param_defaults[p] = p0[p]
360     return param_defaults, bounds_defaults
361 
362 
363 class DataVariables(Mapping[Any, "DataArray"]):
364     __slots__ = ("_dataset",)
365 
366     def __init__(self, dataset: Dataset):
367         self._dataset = dataset
368 
369     def __iter__(self) -> Iterator[Hashable]:
370         return (
371             key
372             for key in self._dataset._variables
373             if key not in self._dataset._coord_names
374         )
375 
376     def __len__(self) -> int:
377         return len(self._dataset._variables) - len(self._dataset._coord_names)
378 
379     def __contains__(self, key: Hashable) -> bool:
380         return key in self._dataset._variables and key not in self._dataset._coord_names
381 
382     def __getitem__(self, key: Hashable) -> DataArray:
383         if key not in self._dataset._coord_names:
384             return cast("DataArray", self._dataset[key])
385         raise KeyError(key)
386 
387     def __repr__(self) -> str:
388         return formatting.data_vars_repr(self)
389 
390     @property
391     def variables(self) -> Mapping[Hashable, Variable]:
392         all_variables = self._dataset.variables
393         return Frozen({k: all_variables[k] for k in self})
394 
395     @property
396     def dtypes(self) -> Frozen[Hashable, np.dtype]:
397         """Mapping from data variable names to dtypes.
398 
399         Cannot be modified directly, but is updated when adding new variables.
400 
401         See Also
402         --------
403         Dataset.dtype
404         """
405         return self._dataset.dtypes
406 
407     def _ipython_key_completions_(self):
408         """Provide method for the key-autocompletions in IPython."""
409         return [
410             key
411             for key in self._dataset._ipython_key_completions_()
412             if key not in self._dataset._coord_names
413         ]
414 
415 
416 class _LocIndexer(Generic[T_Dataset]):
417     __slots__ = ("dataset",)
418 
419     def __init__(self, dataset: T_Dataset):
420         self.dataset = dataset
421 
422     def __getitem__(self, key: Mapping[Any, Any]) -> T_Dataset:
423         if not utils.is_dict_like(key):
424             raise TypeError("can only lookup dictionaries from Dataset.loc")
425         return self.dataset.sel(key)
426 
427     def __setitem__(self, key, value) -> None:
428         if not utils.is_dict_like(key):
429             raise TypeError(
430                 "can only set locations defined by dictionaries from Dataset.loc."
431                 f" Got: {key}"
432             )
433 
434         # set new values
435         dim_indexers = map_index_queries(self.dataset, key).dim_indexers
436         self.dataset[dim_indexers] = value
437 
438 
439 class Dataset(
440     DataWithCoords,
441     DatasetAggregations,
442     DatasetArithmetic,
443     Mapping[Hashable, "DataArray"],
444 ):
445     """A multi-dimensional, in memory, array database.
446 
447     A dataset resembles an in-memory representation of a NetCDF file,
448     and consists of variables, coordinates and attributes which
449     together form a self describing dataset.
450 
451     Dataset implements the mapping interface with keys given by variable
452     names and values given by DataArray objects for each variable name.
453 
454     One dimensional variables with name equal to their dimension are
455     index coordinates used for label based indexing.
456 
457     To load data from a file or file-like object, use the `open_dataset`
458     function.
459 
460     Parameters
461     ----------
462     data_vars : dict-like, optional
463         A mapping from variable names to :py:class:`~xarray.DataArray`
464         objects, :py:class:`~xarray.Variable` objects or to tuples of
465         the form ``(dims, data[, attrs])`` which can be used as
466         arguments to create a new ``Variable``. Each dimension must
467         have the same length in all variables in which it appears.
468 
469         The following notations are accepted:
470 
471         - mapping {var name: DataArray}
472         - mapping {var name: Variable}
473         - mapping {var name: (dimension name, array-like)}
474         - mapping {var name: (tuple of dimension names, array-like)}
475         - mapping {dimension name: array-like}
476           (it will be automatically moved to coords, see below)
477 
478         Each dimension must have the same length in all variables in
479         which it appears.
480     coords : dict-like, optional
481         Another mapping in similar form as the `data_vars` argument,
482         except the each item is saved on the dataset as a "coordinate".
483         These variables have an associated meaning: they describe
484         constant/fixed/independent quantities, unlike the
485         varying/measured/dependent quantities that belong in
486         `variables`. Coordinates values may be given by 1-dimensional
487         arrays or scalars, in which case `dims` do not need to be
488         supplied: 1D arrays will be assumed to give index values along
489         the dimension with the same name.
490 
491         The following notations are accepted:
492 
493         - mapping {coord name: DataArray}
494         - mapping {coord name: Variable}
495         - mapping {coord name: (dimension name, array-like)}
496         - mapping {coord name: (tuple of dimension names, array-like)}
497         - mapping {dimension name: array-like}
498           (the dimension name is implicitly set to be the same as the
499           coord name)
500 
501         The last notation implies that the coord name is the same as
502         the dimension name.
503 
504     attrs : dict-like, optional
505         Global attributes to save on this dataset.
506 
507     Examples
508     --------
509     Create data:
510 
511     >>> np.random.seed(0)
512     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
513     >>> precipitation = 10 * np.random.rand(2, 2, 3)
514     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
515     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
516     >>> time = pd.date_range("2014-09-06", periods=3)
517     >>> reference_time = pd.Timestamp("2014-09-05")
518 
519     Initialize a dataset with multiple dimensions:
520 
521     >>> ds = xr.Dataset(
522     ...     data_vars=dict(
523     ...         temperature=(["x", "y", "time"], temperature),
524     ...         precipitation=(["x", "y", "time"], precipitation),
525     ...     ),
526     ...     coords=dict(
527     ...         lon=(["x", "y"], lon),
528     ...         lat=(["x", "y"], lat),
529     ...         time=time,
530     ...         reference_time=reference_time,
531     ...     ),
532     ...     attrs=dict(description="Weather related data."),
533     ... )
534     >>> ds
535     <xarray.Dataset>
536     Dimensions:         (x: 2, y: 2, time: 3)
537     Coordinates:
538         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
539         lat             (x, y) float64 42.25 42.21 42.63 42.59
540       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
541         reference_time  datetime64[ns] 2014-09-05
542     Dimensions without coordinates: x, y
543     Data variables:
544         temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
545         precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
546     Attributes:
547         description:  Weather related data.
548 
549     Find out where the coldest temperature was and what values the
550     other variables had:
551 
552     >>> ds.isel(ds.temperature.argmin(...))
553     <xarray.Dataset>
554     Dimensions:         ()
555     Coordinates:
556         lon             float64 -99.32
557         lat             float64 42.21
558         time            datetime64[ns] 2014-09-08
559         reference_time  datetime64[ns] 2014-09-05
560     Data variables:
561         temperature     float64 7.182
562         precipitation   float64 8.326
563     Attributes:
564         description:  Weather related data.
565     """
566 
567     _attrs: dict[Hashable, Any] | None
568     _cache: dict[str, Any]
569     _coord_names: set[Hashable]
570     _dims: dict[Hashable, int]
571     _encoding: dict[Hashable, Any] | None
572     _close: Callable[[], None] | None
573     _indexes: dict[Hashable, Index]
574     _variables: dict[Hashable, Variable]
575 
576     __slots__ = (
577         "_attrs",
578         "_cache",
579         "_coord_names",
580         "_dims",
581         "_encoding",
582         "_close",
583         "_indexes",
584         "_variables",
585         "__weakref__",
586     )
587 
588     def __init__(
589         self,
590         # could make a VariableArgs to use more generally, and refine these
591         # categories
592         data_vars: Mapping[Any, Any] | None = None,
593         coords: Mapping[Any, Any] | None = None,
594         attrs: Mapping[Any, Any] | None = None,
595     ) -> None:
596         # TODO(shoyer): expose indexes as a public argument in __init__
597 
598         if data_vars is None:
599             data_vars = {}
600         if coords is None:
601             coords = {}
602 
603         both_data_and_coords = set(data_vars) & set(coords)
604         if both_data_and_coords:
605             raise ValueError(
606                 f"variables {both_data_and_coords!r} are found in both data_vars and coords"
607             )
608 
609         if isinstance(coords, Dataset):
610             coords = coords.variables
611 
612         variables, coord_names, dims, indexes, _ = merge_data_and_coords(
613             data_vars, coords, compat="broadcast_equals"
614         )
615 
616         self._attrs = dict(attrs) if attrs is not None else None
617         self._close = None
618         self._encoding = None
619         self._variables = variables
620         self._coord_names = coord_names
621         self._dims = dims
622         self._indexes = indexes
623 
624     @classmethod
625     def load_store(cls: type[T_Dataset], store, decoder=None) -> T_Dataset:
626         """Create a new dataset from the contents of a backends.*DataStore
627         object
628         """
629         variables, attributes = store.load()
630         if decoder:
631             variables, attributes = decoder(variables, attributes)
632         obj = cls(variables, attrs=attributes)
633         obj.set_close(store.close)
634         return obj
635 
636     @property
637     def variables(self) -> Frozen[Hashable, Variable]:
638         """Low level interface to Dataset contents as dict of Variable objects.
639 
640         This ordered dictionary is frozen to prevent mutation that could
641         violate Dataset invariants. It contains all variable objects
642         constituting the Dataset, including both data variables and
643         coordinates.
644         """
645         return Frozen(self._variables)
646 
647     @property
648     def attrs(self) -> dict[Any, Any]:
649         """Dictionary of global attributes on this dataset"""
650         if self._attrs is None:
651             self._attrs = {}
652         return self._attrs
653 
654     @attrs.setter
655     def attrs(self, value: Mapping[Any, Any]) -> None:
656         self._attrs = dict(value)
657 
658     @property
659     def encoding(self) -> dict[Any, Any]:
660         """Dictionary of global encoding attributes on this dataset"""
661         if self._encoding is None:
662             self._encoding = {}
663         return self._encoding
664 
665     @encoding.setter
666     def encoding(self, value: Mapping[Any, Any]) -> None:
667         self._encoding = dict(value)
668 
669     def reset_encoding(self: T_Dataset) -> T_Dataset:
670         """Return a new Dataset without encoding on the dataset or any of its
671         variables/coords."""
672         variables = {k: v.reset_encoding() for k, v in self.variables.items()}
673         return self._replace(variables=variables, encoding={})
674 
675     @property
676     def dims(self) -> Frozen[Hashable, int]:
677         """Mapping from dimension names to lengths.
678 
679         Cannot be modified directly, but is updated when adding new variables.
680 
681         Note that type of this object differs from `DataArray.dims`.
682         See `Dataset.sizes` and `DataArray.sizes` for consistently named
683         properties.
684 
685         See Also
686         --------
687         Dataset.sizes
688         DataArray.dims
689         """
690         return Frozen(self._dims)
691 
692     @property
693     def sizes(self) -> Frozen[Hashable, int]:
694         """Mapping from dimension names to lengths.
695 
696         Cannot be modified directly, but is updated when adding new variables.
697 
698         This is an alias for `Dataset.dims` provided for the benefit of
699         consistency with `DataArray.sizes`.
700 
701         See Also
702         --------
703         DataArray.sizes
704         """
705         return self.dims
706 
707     @property
708     def dtypes(self) -> Frozen[Hashable, np.dtype]:
709         """Mapping from data variable names to dtypes.
710 
711         Cannot be modified directly, but is updated when adding new variables.
712 
713         See Also
714         --------
715         DataArray.dtype
716         """
717         return Frozen(
718             {
719                 n: v.dtype
720                 for n, v in self._variables.items()
721                 if n not in self._coord_names
722             }
723         )
724 
725     def load(self: T_Dataset, **kwargs) -> T_Dataset:
726         """Manually trigger loading and/or computation of this dataset's data
727         from disk or a remote source into memory and return this dataset.
728         Unlike compute, the original dataset is modified and returned.
729 
730         Normally, it should not be necessary to call this method in user code,
731         because all xarray functions should either work on deferred data or
732         load data automatically. However, this method can be necessary when
733         working with many file objects on disk.
734 
735         Parameters
736         ----------
737         **kwargs : dict
738             Additional keyword arguments passed on to ``dask.compute``.
739 
740         See Also
741         --------
742         dask.compute
743         """
744         # access .data to coerce everything to numpy or dask arrays
745         lazy_data = {
746             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
747         }
748         if lazy_data:
749             import dask.array as da
750 
751             # evaluate all the dask arrays simultaneously
752             evaluated_data = da.compute(*lazy_data.values(), **kwargs)
753 
754             for k, data in zip(lazy_data, evaluated_data):
755                 self.variables[k].data = data
756 
757         # load everything else sequentially
758         for k, v in self.variables.items():
759             if k not in lazy_data:
760                 v.load()
761 
762         return self
763 
764     def __dask_tokenize__(self):
765         from dask.base import normalize_token
766 
767         return normalize_token(
768             (type(self), self._variables, self._coord_names, self._attrs)
769         )
770 
771     def __dask_graph__(self):
772         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}
773         graphs = {k: v for k, v in graphs.items() if v is not None}
774         if not graphs:
775             return None
776         else:
777             try:
778                 from dask.highlevelgraph import HighLevelGraph
779 
780                 return HighLevelGraph.merge(*graphs.values())
781             except ImportError:
782                 from dask import sharedict
783 
784                 return sharedict.merge(*graphs.values())
785 
786     def __dask_keys__(self):
787         import dask
788 
789         return [
790             v.__dask_keys__()
791             for v in self.variables.values()
792             if dask.is_dask_collection(v)
793         ]
794 
795     def __dask_layers__(self):
796         import dask
797 
798         return sum(
799             (
800                 v.__dask_layers__()
801                 for v in self.variables.values()
802                 if dask.is_dask_collection(v)
803             ),
804             (),
805         )
806 
807     @property
808     def __dask_optimize__(self):
809         import dask.array as da
810 
811         return da.Array.__dask_optimize__
812 
813     @property
814     def __dask_scheduler__(self):
815         import dask.array as da
816 
817         return da.Array.__dask_scheduler__
818 
819     def __dask_postcompute__(self):
820         return self._dask_postcompute, ()
821 
822     def __dask_postpersist__(self):
823         return self._dask_postpersist, ()
824 
825     def _dask_postcompute(self: T_Dataset, results: Iterable[Variable]) -> T_Dataset:
826         import dask
827 
828         variables = {}
829         results_iter = iter(results)
830 
831         for k, v in self._variables.items():
832             if dask.is_dask_collection(v):
833                 rebuild, args = v.__dask_postcompute__()
834                 v = rebuild(next(results_iter), *args)
835             variables[k] = v
836 
837         return type(self)._construct_direct(
838             variables,
839             self._coord_names,
840             self._dims,
841             self._attrs,
842             self._indexes,
843             self._encoding,
844             self._close,
845         )
846 
847     def _dask_postpersist(
848         self: T_Dataset, dsk: Mapping, *, rename: Mapping[str, str] | None = None
849     ) -> T_Dataset:
850         from dask import is_dask_collection
851         from dask.highlevelgraph import HighLevelGraph
852         from dask.optimization import cull
853 
854         variables = {}
855 
856         for k, v in self._variables.items():
857             if not is_dask_collection(v):
858                 variables[k] = v
859                 continue
860 
861             if isinstance(dsk, HighLevelGraph):
862                 # dask >= 2021.3
863                 # __dask_postpersist__() was called by dask.highlevelgraph.
864                 # Don't use dsk.cull(), as we need to prevent partial layers:
865                 # https://github.com/dask/dask/issues/7137
866                 layers = v.__dask_layers__()
867                 if rename:
868                     layers = [rename.get(k, k) for k in layers]
869                 dsk2 = dsk.cull_layers(layers)
870             elif rename:  # pragma: nocover
871                 # At the moment of writing, this is only for forward compatibility.
872                 # replace_name_in_key requires dask >= 2021.3.
873                 from dask.base import flatten, replace_name_in_key
874 
875                 keys = [
876                     replace_name_in_key(k, rename) for k in flatten(v.__dask_keys__())
877                 ]
878                 dsk2, _ = cull(dsk, keys)
879             else:
880                 # __dask_postpersist__() was called by dask.optimize or dask.persist
881                 dsk2, _ = cull(dsk, v.__dask_keys__())
882 
883             rebuild, args = v.__dask_postpersist__()
884             # rename was added in dask 2021.3
885             kwargs = {"rename": rename} if rename else {}
886             variables[k] = rebuild(dsk2, *args, **kwargs)
887 
888         return type(self)._construct_direct(
889             variables,
890             self._coord_names,
891             self._dims,
892             self._attrs,
893             self._indexes,
894             self._encoding,
895             self._close,
896         )
897 
898     def compute(self: T_Dataset, **kwargs) -> T_Dataset:
899         """Manually trigger loading and/or computation of this dataset's data
900         from disk or a remote source into memory and return a new dataset.
901         Unlike load, the original dataset is left unaltered.
902 
903         Normally, it should not be necessary to call this method in user code,
904         because all xarray functions should either work on deferred data or
905         load data automatically. However, this method can be necessary when
906         working with many file objects on disk.
907 
908         Parameters
909         ----------
910         **kwargs : dict
911             Additional keyword arguments passed on to ``dask.compute``.
912 
913         See Also
914         --------
915         dask.compute
916         """
917         new = self.copy(deep=False)
918         return new.load(**kwargs)
919 
920     def _persist_inplace(self: T_Dataset, **kwargs) -> T_Dataset:
921         """Persist all Dask arrays in memory"""
922         # access .data to coerce everything to numpy or dask arrays
923         lazy_data = {
924             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
925         }
926         if lazy_data:
927             import dask
928 
929             # evaluate all the dask arrays simultaneously
930             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)
931 
932             for k, data in zip(lazy_data, evaluated_data):
933                 self.variables[k].data = data
934 
935         return self
936 
937     def persist(self: T_Dataset, **kwargs) -> T_Dataset:
938         """Trigger computation, keeping data as dask arrays
939 
940         This operation can be used to trigger computation on underlying dask
941         arrays, similar to ``.compute()`` or ``.load()``.  However this
942         operation keeps the data as dask arrays. This is particularly useful
943         when using the dask.distributed scheduler and you want to load a large
944         amount of data into distributed memory.
945 
946         Parameters
947         ----------
948         **kwargs : dict
949             Additional keyword arguments passed on to ``dask.persist``.
950 
951         See Also
952         --------
953         dask.persist
954         """
955         new = self.copy(deep=False)
956         return new._persist_inplace(**kwargs)
957 
958     @classmethod
959     def _construct_direct(
960         cls: type[T_Dataset],
961         variables: dict[Any, Variable],
962         coord_names: set[Hashable],
963         dims: dict[Any, int] | None = None,
964         attrs: dict | None = None,
965         indexes: dict[Any, Index] | None = None,
966         encoding: dict | None = None,
967         close: Callable[[], None] | None = None,
968     ) -> T_Dataset:
969         """Shortcut around __init__ for internal use when we want to skip
970         costly validation
971         """
972         if dims is None:
973             dims = calculate_dimensions(variables)
974         if indexes is None:
975             indexes = {}
976         obj = object.__new__(cls)
977         obj._variables = variables
978         obj._coord_names = coord_names
979         obj._dims = dims
980         obj._indexes = indexes
981         obj._attrs = attrs
982         obj._close = close
983         obj._encoding = encoding
984         return obj
985 
986     def _replace(
987         self: T_Dataset,
988         variables: dict[Hashable, Variable] | None = None,
989         coord_names: set[Hashable] | None = None,
990         dims: dict[Any, int] | None = None,
991         attrs: dict[Hashable, Any] | None | Default = _default,
992         indexes: dict[Hashable, Index] | None = None,
993         encoding: dict | None | Default = _default,
994         inplace: bool = False,
995     ) -> T_Dataset:
996         """Fastpath constructor for internal use.
997 
998         Returns an object with optionally with replaced attributes.
999 
1000         Explicitly passed arguments are *not* copied when placed on the new
1001         dataset. It is up to the caller to ensure that they have the right type
1002         and are not used elsewhere.
1003         """
1004         if inplace:
1005             if variables is not None:
1006                 self._variables = variables
1007             if coord_names is not None:
1008                 self._coord_names = coord_names
1009             if dims is not None:
1010                 self._dims = dims
1011             if attrs is not _default:
1012                 self._attrs = attrs
1013             if indexes is not None:
1014                 self._indexes = indexes
1015             if encoding is not _default:
1016                 self._encoding = encoding
1017             obj = self
1018         else:
1019             if variables is None:
1020                 variables = self._variables.copy()
1021             if coord_names is None:
1022                 coord_names = self._coord_names.copy()
1023             if dims is None:
1024                 dims = self._dims.copy()
1025             if attrs is _default:
1026                 attrs = copy.copy(self._attrs)
1027             if indexes is None:
1028                 indexes = self._indexes.copy()
1029             if encoding is _default:
1030                 encoding = copy.copy(self._encoding)
1031             obj = self._construct_direct(
1032                 variables, coord_names, dims, attrs, indexes, encoding
1033             )
1034         return obj
1035 
1036     def _replace_with_new_dims(
1037         self: T_Dataset,
1038         variables: dict[Hashable, Variable],
1039         coord_names: set | None = None,
1040         attrs: dict[Hashable, Any] | None | Default = _default,
1041         indexes: dict[Hashable, Index] | None = None,
1042         inplace: bool = False,
1043     ) -> T_Dataset:
1044         """Replace variables with recalculated dimensions."""
1045         dims = calculate_dimensions(variables)
1046         return self._replace(
1047             variables, coord_names, dims, attrs, indexes, inplace=inplace
1048         )
1049 
1050     def _replace_vars_and_dims(
1051         self: T_Dataset,
1052         variables: dict[Hashable, Variable],
1053         coord_names: set | None = None,
1054         dims: dict[Hashable, int] | None = None,
1055         attrs: dict[Hashable, Any] | None | Default = _default,
1056         inplace: bool = False,
1057     ) -> T_Dataset:
1058         """Deprecated version of _replace_with_new_dims().
1059 
1060         Unlike _replace_with_new_dims(), this method always recalculates
1061         indexes from variables.
1062         """
1063         if dims is None:
1064             dims = calculate_dimensions(variables)
1065         return self._replace(
1066             variables, coord_names, dims, attrs, indexes=None, inplace=inplace
1067         )
1068 
1069     def _overwrite_indexes(
1070         self: T_Dataset,
1071         indexes: Mapping[Hashable, Index],
1072         variables: Mapping[Hashable, Variable] | None = None,
1073         drop_variables: list[Hashable] | None = None,
1074         drop_indexes: list[Hashable] | None = None,
1075         rename_dims: Mapping[Hashable, Hashable] | None = None,
1076     ) -> T_Dataset:
1077         """Maybe replace indexes.
1078 
1079         This function may do a lot more depending on index query
1080         results.
1081 
1082         """
1083         if not indexes:
1084             return self
1085 
1086         if variables is None:
1087             variables = {}
1088         if drop_variables is None:
1089             drop_variables = []
1090         if drop_indexes is None:
1091             drop_indexes = []
1092 
1093         new_variables = self._variables.copy()
1094         new_coord_names = self._coord_names.copy()
1095         new_indexes = dict(self._indexes)
1096 
1097         index_variables = {}
1098         no_index_variables = {}
1099         for name, var in variables.items():
1100             old_var = self._variables.get(name)
1101             if old_var is not None:
1102                 var.attrs.update(old_var.attrs)
1103                 var.encoding.update(old_var.encoding)
1104             if name in indexes:
1105                 index_variables[name] = var
1106             else:
1107                 no_index_variables[name] = var
1108 
1109         for name in indexes:
1110             new_indexes[name] = indexes[name]
1111 
1112         for name, var in index_variables.items():
1113             new_coord_names.add(name)
1114             new_variables[name] = var
1115 
1116         # append no-index variables at the end
1117         for k in no_index_variables:
1118             new_variables.pop(k)
1119         new_variables.update(no_index_variables)
1120 
1121         for name in drop_indexes:
1122             new_indexes.pop(name)
1123 
1124         for name in drop_variables:
1125             new_variables.pop(name)
1126             new_indexes.pop(name, None)
1127             new_coord_names.remove(name)
1128 
1129         replaced = self._replace(
1130             variables=new_variables, coord_names=new_coord_names, indexes=new_indexes
1131         )
1132 
1133         if rename_dims:
1134             # skip rename indexes: they should already have the right name(s)
1135             dims = replaced._rename_dims(rename_dims)
1136             new_variables, new_coord_names = replaced._rename_vars({}, rename_dims)
1137             return replaced._replace(
1138                 variables=new_variables, coord_names=new_coord_names, dims=dims
1139             )
1140         else:
1141             return replaced
1142 
1143     def copy(
1144         self: T_Dataset, deep: bool = False, data: Mapping[Any, ArrayLike] | None = None
1145     ) -> T_Dataset:
1146         """Returns a copy of this dataset.
1147 
1148         If `deep=True`, a deep copy is made of each of the component variables.
1149         Otherwise, a shallow copy of each of the component variable is made, so
1150         that the underlying memory region of the new dataset is the same as in
1151         the original dataset.
1152 
1153         Use `data` to create a new object with the same structure as
1154         original but entirely new data.
1155 
1156         Parameters
1157         ----------
1158         deep : bool, default: False
1159             Whether each component variable is loaded into memory and copied onto
1160             the new object. Default is False.
1161         data : dict-like or None, optional
1162             Data to use in the new object. Each item in `data` must have same
1163             shape as corresponding data variable in original. When `data` is
1164             used, `deep` is ignored for the data variables and only used for
1165             coords.
1166 
1167         Returns
1168         -------
1169         object : Dataset
1170             New object with dimensions, attributes, coordinates, name, encoding,
1171             and optionally data copied from original.
1172 
1173         Examples
1174         --------
1175         Shallow copy versus deep copy
1176 
1177         >>> da = xr.DataArray(np.random.randn(2, 3))
1178         >>> ds = xr.Dataset(
1179         ...     {"foo": da, "bar": ("x", [-1, 2])},
1180         ...     coords={"x": ["one", "two"]},
1181         ... )
1182         >>> ds.copy()
1183         <xarray.Dataset>
1184         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1185         Coordinates:
1186           * x        (x) <U3 'one' 'two'
1187         Dimensions without coordinates: dim_0, dim_1
1188         Data variables:
1189             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
1190             bar      (x) int64 -1 2
1191 
1192         >>> ds_0 = ds.copy(deep=False)
1193         >>> ds_0["foo"][0, 0] = 7
1194         >>> ds_0
1195         <xarray.Dataset>
1196         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1197         Coordinates:
1198           * x        (x) <U3 'one' 'two'
1199         Dimensions without coordinates: dim_0, dim_1
1200         Data variables:
1201             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1202             bar      (x) int64 -1 2
1203 
1204         >>> ds
1205         <xarray.Dataset>
1206         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1207         Coordinates:
1208           * x        (x) <U3 'one' 'two'
1209         Dimensions without coordinates: dim_0, dim_1
1210         Data variables:
1211             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1212             bar      (x) int64 -1 2
1213 
1214         Changing the data using the ``data`` argument maintains the
1215         structure of the original object, but with the new data. Original
1216         object is unaffected.
1217 
1218         >>> ds.copy(data={"foo": np.arange(6).reshape(2, 3), "bar": ["a", "b"]})
1219         <xarray.Dataset>
1220         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1221         Coordinates:
1222           * x        (x) <U3 'one' 'two'
1223         Dimensions without coordinates: dim_0, dim_1
1224         Data variables:
1225             foo      (dim_0, dim_1) int64 0 1 2 3 4 5
1226             bar      (x) <U1 'a' 'b'
1227 
1228         >>> ds
1229         <xarray.Dataset>
1230         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1231         Coordinates:
1232           * x        (x) <U3 'one' 'two'
1233         Dimensions without coordinates: dim_0, dim_1
1234         Data variables:
1235             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1236             bar      (x) int64 -1 2
1237 
1238         See Also
1239         --------
1240         pandas.DataFrame.copy
1241         """
1242         return self._copy(deep=deep, data=data)
1243 
1244     def _copy(
1245         self: T_Dataset,
1246         deep: bool = False,
1247         data: Mapping[Any, ArrayLike] | None = None,
1248         memo: dict[int, Any] | None = None,
1249     ) -> T_Dataset:
1250         if data is None:
1251             data = {}
1252         elif not utils.is_dict_like(data):
1253             raise ValueError("Data must be dict-like")
1254 
1255         if data:
1256             var_keys = set(self.data_vars.keys())
1257             data_keys = set(data.keys())
1258             keys_not_in_vars = data_keys - var_keys
1259             if keys_not_in_vars:
1260                 raise ValueError(
1261                     "Data must only contain variables in original "
1262                     "dataset. Extra variables: {}".format(keys_not_in_vars)
1263                 )
1264             keys_missing_from_data = var_keys - data_keys
1265             if keys_missing_from_data:
1266                 raise ValueError(
1267                     "Data must contain all variables in original "
1268                     "dataset. Data is missing {}".format(keys_missing_from_data)
1269                 )
1270 
1271         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1272 
1273         variables = {}
1274         for k, v in self._variables.items():
1275             if k in index_vars:
1276                 variables[k] = index_vars[k]
1277             else:
1278                 variables[k] = v._copy(deep=deep, data=data.get(k), memo=memo)
1279 
1280         attrs = copy.deepcopy(self._attrs, memo) if deep else copy.copy(self._attrs)
1281         encoding = (
1282             copy.deepcopy(self._encoding, memo) if deep else copy.copy(self._encoding)
1283         )
1284 
1285         return self._replace(variables, indexes=indexes, attrs=attrs, encoding=encoding)
1286 
1287     def __copy__(self: T_Dataset) -> T_Dataset:
1288         return self._copy(deep=False)
1289 
1290     def __deepcopy__(self: T_Dataset, memo: dict[int, Any] | None = None) -> T_Dataset:
1291         return self._copy(deep=True, memo=memo)
1292 
1293     def as_numpy(self: T_Dataset) -> T_Dataset:
1294         """
1295         Coerces wrapped data and coordinates into numpy arrays, returning a Dataset.
1296 
1297         See also
1298         --------
1299         DataArray.as_numpy
1300         DataArray.to_numpy : Returns only the data as a numpy.ndarray object.
1301         """
1302         numpy_variables = {k: v.as_numpy() for k, v in self.variables.items()}
1303         return self._replace(variables=numpy_variables)
1304 
1305     def _copy_listed(self: T_Dataset, names: Iterable[Hashable]) -> T_Dataset:
1306         """Create a new Dataset with the listed variables from this dataset and
1307         the all relevant coordinates. Skips all validation.
1308         """
1309         variables: dict[Hashable, Variable] = {}
1310         coord_names = set()
1311         indexes: dict[Hashable, Index] = {}
1312 
1313         for name in names:
1314             try:
1315                 variables[name] = self._variables[name]
1316             except KeyError:
1317                 ref_name, var_name, var = _get_virtual_variable(
1318                     self._variables, name, self.dims
1319                 )
1320                 variables[var_name] = var
1321                 if ref_name in self._coord_names or ref_name in self.dims:
1322                     coord_names.add(var_name)
1323                 if (var_name,) == var.dims:
1324                     index, index_vars = create_default_index_implicit(var, names)
1325                     indexes.update({k: index for k in index_vars})
1326                     variables.update(index_vars)
1327                     coord_names.update(index_vars)
1328 
1329         needed_dims: OrderedSet[Hashable] = OrderedSet()
1330         for v in variables.values():
1331             needed_dims.update(v.dims)
1332 
1333         dims = {k: self.dims[k] for k in needed_dims}
1334 
1335         # preserves ordering of coordinates
1336         for k in self._variables:
1337             if k not in self._coord_names:
1338                 continue
1339 
1340             if set(self.variables[k].dims) <= needed_dims:
1341                 variables[k] = self._variables[k]
1342                 coord_names.add(k)
1343 
1344         indexes.update(filter_indexes_from_coords(self._indexes, coord_names))
1345 
1346         return self._replace(variables, coord_names, dims, indexes=indexes)
1347 
1348     def _construct_dataarray(self, name: Hashable) -> DataArray:
1349         """Construct a DataArray by indexing this dataset"""
1350         from xarray.core.dataarray import DataArray
1351 
1352         try:
1353             variable = self._variables[name]
1354         except KeyError:
1355             _, name, variable = _get_virtual_variable(self._variables, name, self.dims)
1356 
1357         needed_dims = set(variable.dims)
1358 
1359         coords: dict[Hashable, Variable] = {}
1360         # preserve ordering
1361         for k in self._variables:
1362             if k in self._coord_names and set(self.variables[k].dims) <= needed_dims:
1363                 coords[k] = self.variables[k]
1364 
1365         indexes = filter_indexes_from_coords(self._indexes, set(coords))
1366 
1367         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1368 
1369     @property
1370     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1371         """Places to look-up items for attribute-style access"""
1372         yield from self._item_sources
1373         yield self.attrs
1374 
1375     @property
1376     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1377         """Places to look-up items for key-completion"""
1378         yield self.data_vars
1379         yield HybridMappingProxy(keys=self._coord_names, mapping=self.coords)
1380 
1381         # virtual coordinates
1382         yield HybridMappingProxy(keys=self.dims, mapping=self)
1383 
1384     def __contains__(self, key: object) -> bool:
1385         """The 'in' operator will return true or false depending on whether
1386         'key' is an array in the dataset or not.
1387         """
1388         return key in self._variables
1389 
1390     def __len__(self) -> int:
1391         return len(self.data_vars)
1392 
1393     def __bool__(self) -> bool:
1394         return bool(self.data_vars)
1395 
1396     def __iter__(self) -> Iterator[Hashable]:
1397         return iter(self.data_vars)
1398 
1399     def __array__(self, dtype=None):
1400         raise TypeError(
1401             "cannot directly convert an xarray.Dataset into a "
1402             "numpy array. Instead, create an xarray.DataArray "
1403             "first, either with indexing on the Dataset or by "
1404             "invoking the `to_array()` method."
1405         )
1406 
1407     @property
1408     def nbytes(self) -> int:
1409         """
1410         Total bytes consumed by the data arrays of all variables in this dataset.
1411 
1412         If the backend array for any variable does not include ``nbytes``, estimates
1413         the total bytes for that array based on the ``size`` and ``dtype``.
1414         """
1415         return sum(v.nbytes for v in self.variables.values())
1416 
1417     @property
1418     def loc(self: T_Dataset) -> _LocIndexer[T_Dataset]:
1419         """Attribute for location based indexing. Only supports __getitem__,
1420         and only when the key is a dict of the form {dim: labels}.
1421         """
1422         return _LocIndexer(self)
1423 
1424     @overload
1425     def __getitem__(self, key: Hashable) -> DataArray:
1426         ...
1427 
1428     # Mapping is Iterable
1429     @overload
1430     def __getitem__(self: T_Dataset, key: Iterable[Hashable]) -> T_Dataset:
1431         ...
1432 
1433     def __getitem__(
1434         self: T_Dataset, key: Mapping[Any, Any] | Hashable | Iterable[Hashable]
1435     ) -> T_Dataset | DataArray:
1436         """Access variables or coordinates of this dataset as a
1437         :py:class:`~xarray.DataArray` or a subset of variables or a indexed dataset.
1438 
1439         Indexing with a list of names will return a new ``Dataset`` object.
1440         """
1441         if utils.is_dict_like(key):
1442             return self.isel(**key)
1443         if utils.hashable(key):
1444             return self._construct_dataarray(key)
1445         if utils.iterable_of_hashable(key):
1446             return self._copy_listed(key)
1447         raise ValueError(f"Unsupported key-type {type(key)}")
1448 
1449     def __setitem__(
1450         self, key: Hashable | Iterable[Hashable] | Mapping, value: Any
1451     ) -> None:
1452         """Add an array to this dataset.
1453         Multiple arrays can be added at the same time, in which case each of
1454         the following operations is applied to the respective value.
1455 
1456         If key is dict-like, update all variables in the dataset
1457         one by one with the given value at the given location.
1458         If the given value is also a dataset, select corresponding variables
1459         in the given value and in the dataset to be changed.
1460 
1461         If value is a `
1462         from .dataarray import DataArray`, call its `select_vars()` method, rename it
1463         to `key` and merge the contents of the resulting dataset into this
1464         dataset.
1465 
1466         If value is a `Variable` object (or tuple of form
1467         ``(dims, data[, attrs])``), add it to this dataset as a new
1468         variable.
1469         """
1470         from xarray.core.dataarray import DataArray
1471 
1472         if utils.is_dict_like(key):
1473             # check for consistency and convert value to dataset
1474             value = self._setitem_check(key, value)
1475             # loop over dataset variables and set new values
1476             processed = []
1477             for name, var in self.items():
1478                 try:
1479                     var[key] = value[name]
1480                     processed.append(name)
1481                 except Exception as e:
1482                     if processed:
1483                         raise RuntimeError(
1484                             "An error occurred while setting values of the"
1485                             f" variable '{name}'. The following variables have"
1486                             f" been successfully updated:\n{processed}"
1487                         ) from e
1488                     else:
1489                         raise e
1490 
1491         elif utils.hashable(key):
1492             if isinstance(value, Dataset):
1493                 raise TypeError(
1494                     "Cannot assign a Dataset to a single key - only a DataArray or Variable "
1495                     "object can be stored under a single key."
1496                 )
1497             self.update({key: value})
1498 
1499         elif utils.iterable_of_hashable(key):
1500             keylist = list(key)
1501             if len(keylist) == 0:
1502                 raise ValueError("Empty list of variables to be set")
1503             if len(keylist) == 1:
1504                 self.update({keylist[0]: value})
1505             else:
1506                 if len(keylist) != len(value):
1507                     raise ValueError(
1508                         f"Different lengths of variables to be set "
1509                         f"({len(keylist)}) and data used as input for "
1510                         f"setting ({len(value)})"
1511                     )
1512                 if isinstance(value, Dataset):
1513                     self.update(dict(zip(keylist, value.data_vars.values())))
1514                 elif isinstance(value, DataArray):
1515                     raise ValueError("Cannot assign single DataArray to multiple keys")
1516                 else:
1517                     self.update(dict(zip(keylist, value)))
1518 
1519         else:
1520             raise ValueError(f"Unsupported key-type {type(key)}")
1521 
1522     def _setitem_check(self, key, value):
1523         """Consistency check for __setitem__
1524 
1525         When assigning values to a subset of a Dataset, do consistency check beforehand
1526         to avoid leaving the dataset in a partially updated state when an error occurs.
1527         """
1528         from xarray.core.alignment import align
1529         from xarray.core.dataarray import DataArray
1530 
1531         if isinstance(value, Dataset):
1532             missing_vars = [
1533                 name for name in value.data_vars if name not in self.data_vars
1534             ]
1535             if missing_vars:
1536                 raise ValueError(
1537                     f"Variables {missing_vars} in new values"
1538                     f" not available in original dataset:\n{self}"
1539                 )
1540         elif not any([isinstance(value, t) for t in [DataArray, Number, str]]):
1541             raise TypeError(
1542                 "Dataset assignment only accepts DataArrays, Datasets, and scalars."
1543             )
1544 
1545         new_value = Dataset()
1546         for name, var in self.items():
1547             # test indexing
1548             try:
1549                 var_k = var[key]
1550             except Exception as e:
1551                 raise ValueError(
1552                     f"Variable '{name}': indexer {key} not available"
1553                 ) from e
1554 
1555             if isinstance(value, Dataset):
1556                 val = value[name]
1557             else:
1558                 val = value
1559 
1560             if isinstance(val, DataArray):
1561                 # check consistency of dimensions
1562                 for dim in val.dims:
1563                     if dim not in var_k.dims:
1564                         raise KeyError(
1565                             f"Variable '{name}': dimension '{dim}' appears in new values "
1566                             f"but not in the indexed original data"
1567                         )
1568                 dims = tuple(dim for dim in var_k.dims if dim in val.dims)
1569                 if dims != val.dims:
1570                     raise ValueError(
1571                         f"Variable '{name}': dimension order differs between"
1572                         f" original and new data:\n{dims}\nvs.\n{val.dims}"
1573                     )
1574             else:
1575                 val = np.array(val)
1576 
1577             # type conversion
1578             new_value[name] = val.astype(var_k.dtype, copy=False)
1579 
1580         # check consistency of dimension sizes and dimension coordinates
1581         if isinstance(value, DataArray) or isinstance(value, Dataset):
1582             align(self[key], value, join="exact", copy=False)
1583 
1584         return new_value
1585 
1586     def __delitem__(self, key: Hashable) -> None:
1587         """Remove a variable from this dataset."""
1588         assert_no_index_corrupted(self.xindexes, {key})
1589 
1590         if key in self._indexes:
1591             del self._indexes[key]
1592         del self._variables[key]
1593         self._coord_names.discard(key)
1594         self._dims = calculate_dimensions(self._variables)
1595 
1596     # mutable objects should not be hashable
1597     # https://github.com/python/mypy/issues/4266
1598     __hash__ = None  # type: ignore[assignment]
1599 
1600     def _all_compat(self, other: Dataset, compat_str: str) -> bool:
1601         """Helper function for equals and identical"""
1602 
1603         # some stores (e.g., scipy) do not seem to preserve order, so don't
1604         # require matching order for equality
1605         def compat(x: Variable, y: Variable) -> bool:
1606             return getattr(x, compat_str)(y)
1607 
1608         return self._coord_names == other._coord_names and utils.dict_equiv(
1609             self._variables, other._variables, compat=compat
1610         )
1611 
1612     def broadcast_equals(self, other: Dataset) -> bool:
1613         """Two Datasets are broadcast equal if they are equal after
1614         broadcasting all variables against each other.
1615 
1616         For example, variables that are scalar in one dataset but non-scalar in
1617         the other dataset can still be broadcast equal if the the non-scalar
1618         variable is a constant.
1619 
1620         See Also
1621         --------
1622         Dataset.equals
1623         Dataset.identical
1624         """
1625         try:
1626             return self._all_compat(other, "broadcast_equals")
1627         except (TypeError, AttributeError):
1628             return False
1629 
1630     def equals(self, other: Dataset) -> bool:
1631         """Two Datasets are equal if they have matching variables and
1632         coordinates, all of which are equal.
1633 
1634         Datasets can still be equal (like pandas objects) if they have NaN
1635         values in the same locations.
1636 
1637         This method is necessary because `v1 == v2` for ``Dataset``
1638         does element-wise comparisons (like numpy.ndarrays).
1639 
1640         See Also
1641         --------
1642         Dataset.broadcast_equals
1643         Dataset.identical
1644         """
1645         try:
1646             return self._all_compat(other, "equals")
1647         except (TypeError, AttributeError):
1648             return False
1649 
1650     def identical(self, other: Dataset) -> bool:
1651         """Like equals, but also checks all dataset attributes and the
1652         attributes on all variables and coordinates.
1653 
1654         See Also
1655         --------
1656         Dataset.broadcast_equals
1657         Dataset.equals
1658         """
1659         try:
1660             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(
1661                 other, "identical"
1662             )
1663         except (TypeError, AttributeError):
1664             return False
1665 
1666     @property
1667     def indexes(self) -> Indexes[pd.Index]:
1668         """Mapping of pandas.Index objects used for label based indexing.
1669 
1670         Raises an error if this Dataset has indexes that cannot be coerced
1671         to pandas.Index objects.
1672 
1673         See Also
1674         --------
1675         Dataset.xindexes
1676 
1677         """
1678         return self.xindexes.to_pandas_indexes()
1679 
1680     @property
1681     def xindexes(self) -> Indexes[Index]:
1682         """Mapping of xarray Index objects used for label based indexing."""
1683         return Indexes(self._indexes, {k: self._variables[k] for k in self._indexes})
1684 
1685     @property
1686     def coords(self) -> DatasetCoordinates:
1687         """Dictionary of xarray.DataArray objects corresponding to coordinate
1688         variables
1689         """
1690         return DatasetCoordinates(self)
1691 
1692     @property
1693     def data_vars(self) -> DataVariables:
1694         """Dictionary of DataArray objects corresponding to data variables"""
1695         return DataVariables(self)
1696 
1697     def set_coords(self: T_Dataset, names: Hashable | Iterable[Hashable]) -> T_Dataset:
1698         """Given names of one or more variables, set them as coordinates
1699 
1700         Parameters
1701         ----------
1702         names : hashable or iterable of hashable
1703             Name(s) of variables in this dataset to convert into coordinates.
1704 
1705         Returns
1706         -------
1707         Dataset
1708 
1709         See Also
1710         --------
1711         Dataset.swap_dims
1712         Dataset.assign_coords
1713         """
1714         # TODO: allow inserting new coordinates with this method, like
1715         # DataFrame.set_index?
1716         # nb. check in self._variables, not self.data_vars to insure that the
1717         # operation is idempotent
1718         if isinstance(names, str) or not isinstance(names, Iterable):
1719             names = [names]
1720         else:
1721             names = list(names)
1722         self._assert_all_in_dataset(names)
1723         obj = self.copy()
1724         obj._coord_names.update(names)
1725         return obj
1726 
1727     def reset_coords(
1728         self: T_Dataset,
1729         names: Dims = None,
1730         drop: bool = False,
1731     ) -> T_Dataset:
1732         """Given names of coordinates, reset them to become variables
1733 
1734         Parameters
1735         ----------
1736         names : str, Iterable of Hashable or None, optional
1737             Name(s) of non-index coordinates in this dataset to reset into
1738             variables. By default, all non-index coordinates are reset.
1739         drop : bool, default: False
1740             If True, remove coordinates instead of converting them into
1741             variables.
1742 
1743         Returns
1744         -------
1745         Dataset
1746         """
1747         if names is None:
1748             names = self._coord_names - set(self._indexes)
1749         else:
1750             if isinstance(names, str) or not isinstance(names, Iterable):
1751                 names = [names]
1752             else:
1753                 names = list(names)
1754             self._assert_all_in_dataset(names)
1755             bad_coords = set(names) & set(self._indexes)
1756             if bad_coords:
1757                 raise ValueError(
1758                     f"cannot remove index coordinates with reset_coords: {bad_coords}"
1759                 )
1760         obj = self.copy()
1761         obj._coord_names.difference_update(names)
1762         if drop:
1763             for name in names:
1764                 del obj._variables[name]
1765         return obj
1766 
1767     def dump_to_store(self, store: AbstractDataStore, **kwargs) -> None:
1768         """Store dataset contents to a backends.*DataStore object."""
1769         from xarray.backends.api import dump_to_store
1770 
1771         # TODO: rename and/or cleanup this method to make it more consistent
1772         # with to_netcdf()
1773         dump_to_store(self, store, **kwargs)
1774 
1775     # path=None writes to bytes
1776     @overload
1777     def to_netcdf(
1778         self,
1779         path: None = None,
1780         mode: Literal["w", "a"] = "w",
1781         format: T_NetcdfTypes | None = None,
1782         group: str | None = None,
1783         engine: T_NetcdfEngine | None = None,
1784         encoding: Mapping[Any, Mapping[str, Any]] | None = None,
1785         unlimited_dims: Iterable[Hashable] | None = None,
1786         compute: bool = True,
1787         invalid_netcdf: bool = False,
1788     ) -> bytes:
1789         ...
1790 
1791     # default return None
1792     @overload
1793     def to_netcdf(
1794         self,
1795         path: str | PathLike,
1796         mode: Literal["w", "a"] = "w",
1797         format: T_NetcdfTypes | None = None,
1798         group: str | None = None,
1799         engine: T_NetcdfEngine | None = None,
1800         encoding: Mapping[Any, Mapping[str, Any]] | None = None,
1801         unlimited_dims: Iterable[Hashable] | None = None,
1802         compute: Literal[True] = True,
1803         invalid_netcdf: bool = False,
1804     ) -> None:
1805         ...
1806 
1807     # compute=False returns dask.Delayed
1808     @overload
1809     def to_netcdf(
1810         self,
1811         path: str | PathLike,
1812         mode: Literal["w", "a"] = "w",
1813         format: T_NetcdfTypes | None = None,
1814         group: str | None = None,
1815         engine: T_NetcdfEngine | None = None,
1816         encoding: Mapping[Any, Mapping[str, Any]] | None = None,
1817         unlimited_dims: Iterable[Hashable] | None = None,
1818         *,
1819         compute: Literal[False],
1820         invalid_netcdf: bool = False,
1821     ) -> Delayed:
1822         ...
1823 
1824     def to_netcdf(
1825         self,
1826         path: str | PathLike | None = None,
1827         mode: Literal["w", "a"] = "w",
1828         format: T_NetcdfTypes | None = None,
1829         group: str | None = None,
1830         engine: T_NetcdfEngine | None = None,
1831         encoding: Mapping[Any, Mapping[str, Any]] | None = None,
1832         unlimited_dims: Iterable[Hashable] | None = None,
1833         compute: bool = True,
1834         invalid_netcdf: bool = False,
1835     ) -> bytes | Delayed | None:
1836         """Write dataset contents to a netCDF file.
1837 
1838         Parameters
1839         ----------
1840         path : str, path-like or file-like, optional
1841             Path to which to save this dataset. File-like objects are only
1842             supported by the scipy engine. If no path is provided, this
1843             function returns the resulting netCDF file as bytes; in this case,
1844             we need to use scipy, which does not support netCDF version 4 (the
1845             default format becomes NETCDF3_64BIT).
1846         mode : {"w", "a"}, default: "w"
1847             Write ('w') or append ('a') mode. If mode='w', any existing file at
1848             this location will be overwritten. If mode='a', existing variables
1849             will be overwritten.
1850         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
1851                   "NETCDF3_CLASSIC"}, optional
1852             File format for the resulting netCDF file:
1853 
1854             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
1855               features.
1856             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
1857               netCDF 3 compatible API features.
1858             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
1859               which fully supports 2+ GB files, but is only compatible with
1860               clients linked against netCDF version 3.6.0 or later.
1861             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
1862               handle 2+ GB files very well.
1863 
1864             All formats are supported by the netCDF4-python library.
1865             scipy.io.netcdf only supports the last two formats.
1866 
1867             The default format is NETCDF4 if you are saving a file to disk and
1868             have the netCDF4-python library available. Otherwise, xarray falls
1869             back to using scipy to write netCDF files and defaults to the
1870             NETCDF3_64BIT format (scipy does not support netCDF4).
1871         group : str, optional
1872             Path to the netCDF4 group in the given file to open (only works for
1873             format='NETCDF4'). The group(s) will be created if necessary.
1874         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
1875             Engine to use when writing netCDF files. If not provided, the
1876             default engine is chosen based on available dependencies, with a
1877             preference for 'netcdf4' if writing to a file on disk.
1878         encoding : dict, optional
1879             Nested dictionary with variable names as keys and dictionaries of
1880             variable specific encodings as values, e.g.,
1881             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
1882             "zlib": True}, ...}``
1883 
1884             The `h5netcdf` engine supports both the NetCDF4-style compression
1885             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
1886             ones ``{"compression": "gzip", "compression_opts": 9}``.
1887             This allows using any compression plugin installed in the HDF5
1888             library, e.g. LZF.
1889 
1890         unlimited_dims : iterable of hashable, optional
1891             Dimension(s) that should be serialized as unlimited dimensions.
1892             By default, no dimensions are treated as unlimited dimensions.
1893             Note that unlimited_dims may also be set via
1894             ``dataset.encoding["unlimited_dims"]``.
1895         compute: bool, default: True
1896             If true compute immediately, otherwise return a
1897             ``dask.delayed.Delayed`` object that can be computed later.
1898         invalid_netcdf: bool, default: False
1899             Only valid along with ``engine="h5netcdf"``. If True, allow writing
1900             hdf5 files which are invalid netcdf as described in
1901             https://github.com/h5netcdf/h5netcdf.
1902 
1903         Returns
1904         -------
1905             * ``bytes`` if path is None
1906             * ``dask.delayed.Delayed`` if compute is False
1907             * None otherwise
1908 
1909         See Also
1910         --------
1911         DataArray.to_netcdf
1912         """
1913         if encoding is None:
1914             encoding = {}
1915         from xarray.backends.api import to_netcdf
1916 
1917         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
1918             self,
1919             path,
1920             mode=mode,
1921             format=format,
1922             group=group,
1923             engine=engine,
1924             encoding=encoding,
1925             unlimited_dims=unlimited_dims,
1926             compute=compute,
1927             multifile=False,
1928             invalid_netcdf=invalid_netcdf,
1929         )
1930 
1931     # compute=True (default) returns ZarrStore
1932     @overload
1933     def to_zarr(
1934         self,
1935         store: MutableMapping | str | PathLike[str] | None = None,
1936         chunk_store: MutableMapping | str | PathLike | None = None,
1937         mode: Literal["w", "w-", "a", "r+", None] = None,
1938         synchronizer=None,
1939         group: str | None = None,
1940         encoding: Mapping | None = None,
1941         compute: Literal[True] = True,
1942         consolidated: bool | None = None,
1943         append_dim: Hashable | None = None,
1944         region: Mapping[str, slice] | None = None,
1945         safe_chunks: bool = True,
1946         storage_options: dict[str, str] | None = None,
1947         zarr_version: int | None = None,
1948     ) -> ZarrStore:
1949         ...
1950 
1951     # compute=False returns dask.Delayed
1952     @overload
1953     def to_zarr(
1954         self,
1955         store: MutableMapping | str | PathLike[str] | None = None,
1956         chunk_store: MutableMapping | str | PathLike | None = None,
1957         mode: Literal["w", "w-", "a", "r+", None] = None,
1958         synchronizer=None,
1959         group: str | None = None,
1960         encoding: Mapping | None = None,
1961         *,
1962         compute: Literal[False],
1963         consolidated: bool | None = None,
1964         append_dim: Hashable | None = None,
1965         region: Mapping[str, slice] | None = None,
1966         safe_chunks: bool = True,
1967         storage_options: dict[str, str] | None = None,
1968         zarr_version: int | None = None,
1969     ) -> Delayed:
1970         ...
1971 
1972     def to_zarr(
1973         self,
1974         store: MutableMapping | str | PathLike[str] | None = None,
1975         chunk_store: MutableMapping | str | PathLike | None = None,
1976         mode: Literal["w", "w-", "a", "r+", None] = None,
1977         synchronizer=None,
1978         group: str | None = None,
1979         encoding: Mapping | None = None,
1980         compute: bool = True,
1981         consolidated: bool | None = None,
1982         append_dim: Hashable | None = None,
1983         region: Mapping[str, slice] | None = None,
1984         safe_chunks: bool = True,
1985         storage_options: dict[str, str] | None = None,
1986         zarr_version: int | None = None,
1987     ) -> ZarrStore | Delayed:
1988         """Write dataset contents to a zarr group.
1989 
1990         Zarr chunks are determined in the following way:
1991 
1992         - From the ``chunks`` attribute in each variable's ``encoding``
1993           (can be set via `Dataset.chunk`).
1994         - If the variable is a Dask array, from the dask chunks
1995         - If neither Dask chunks nor encoding chunks are present, chunks will
1996           be determined automatically by Zarr
1997         - If both Dask chunks and encoding chunks are present, encoding chunks
1998           will be used, provided that there is a many-to-one relationship between
1999           encoding chunks and dask chunks (i.e. Dask chunks are bigger than and
2000           evenly divide encoding chunks); otherwise raise a ``ValueError``.
2001           This restriction ensures that no synchronization / locks are required
2002           when writing. To disable this restriction, use ``safe_chunks=False``.
2003 
2004         Parameters
2005         ----------
2006         store : MutableMapping, str or path-like, optional
2007             Store or path to directory in local or remote file system.
2008         chunk_store : MutableMapping, str or path-like, optional
2009             Store or path to directory in local or remote file system only for Zarr
2010             array chunks. Requires zarr-python v2.4.0 or later.
2011         mode : {"w", "w-", "a", "r+", None}, optional
2012             Persistence mode: "w" means create (overwrite if exists);
2013             "w-" means create (fail if exists);
2014             "a" means override existing variables (create if does not exist);
2015             "r+" means modify existing array *values* only (raise an error if
2016             any metadata or shapes would change).
2017             The default mode is "a" if ``append_dim`` is set. Otherwise, it is
2018             "r+" if ``region`` is set and ``w-`` otherwise.
2019         synchronizer : object, optional
2020             Zarr array synchronizer.
2021         group : str, optional
2022             Group path. (a.k.a. `path` in zarr terminology.)
2023         encoding : dict, optional
2024             Nested dictionary with variable names as keys and dictionaries of
2025             variable specific encodings as values, e.g.,
2026             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,}, ...}``
2027         compute : bool, default: True
2028             If True write array data immediately, otherwise return a
2029             ``dask.delayed.Delayed`` object that can be computed to write
2030             array data later. Metadata is always updated eagerly.
2031         consolidated : bool, optional
2032             If True, apply zarr's `consolidate_metadata` function to the store
2033             after writing metadata and read existing stores with consolidated
2034             metadata; if False, do not. The default (`consolidated=None`) means
2035             write consolidated metadata and attempt to read consolidated
2036             metadata for existing stores (falling back to non-consolidated).
2037 
2038             When the experimental ``zarr_version=3``, ``consolidated`` must be
2039             either be ``None`` or ``False``.
2040         append_dim : hashable, optional
2041             If set, the dimension along which the data will be appended. All
2042             other dimensions on overridden variables must remain the same size.
2043         region : dict, optional
2044             Optional mapping from dimension names to integer slices along
2045             dataset dimensions to indicate the region of existing zarr array(s)
2046             in which to write this dataset's data. For example,
2047             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate
2048             that values should be written to the region ``0:1000`` along ``x``
2049             and ``10000:11000`` along ``y``.
2050 
2051             Two restrictions apply to the use of ``region``:
2052 
2053             - If ``region`` is set, _all_ variables in a dataset must have at
2054               least one dimension in common with the region. Other variables
2055               should be written in a separate call to ``to_zarr()``.
2056             - Dimensions cannot be included in both ``region`` and
2057               ``append_dim`` at the same time. To create empty arrays to fill
2058               in with ``region``, use a separate call to ``to_zarr()`` with
2059               ``compute=False``. See "Appending to existing Zarr stores" in
2060               the reference documentation for full details.
2061         safe_chunks : bool, default: True
2062             If True, only allow writes to when there is a many-to-one relationship
2063             between Zarr chunks (specified in encoding) and Dask chunks.
2064             Set False to override this restriction; however, data may become corrupted
2065             if Zarr arrays are written in parallel. This option may be useful in combination
2066             with ``compute=False`` to initialize a Zarr from an existing
2067             Dataset with arbitrary chunk structure.
2068         storage_options : dict, optional
2069             Any additional parameters for the storage backend (ignored for local
2070             paths).
2071         zarr_version : int or None, optional
2072             The desired zarr spec version to target (currently 2 or 3). The
2073             default of None will attempt to determine the zarr version from
2074             ``store`` when possible, otherwise defaulting to 2.
2075 
2076         Returns
2077         -------
2078             * ``dask.delayed.Delayed`` if compute is False
2079             * ZarrStore otherwise
2080 
2081         References
2082         ----------
2083         https://zarr.readthedocs.io/
2084 
2085         Notes
2086         -----
2087         Zarr chunking behavior:
2088             If chunks are found in the encoding argument or attribute
2089             corresponding to any DataArray, those chunks are used.
2090             If a DataArray is a dask array, it is written with those chunks.
2091             If not other chunks are found, Zarr uses its own heuristics to
2092             choose automatic chunk sizes.
2093 
2094         encoding:
2095             The encoding attribute (if exists) of the DataArray(s) will be
2096             used. Override any existing encodings by providing the ``encoding`` kwarg.
2097 
2098         See Also
2099         --------
2100         :ref:`io.zarr`
2101             The I/O user guide, with more details and examples.
2102         """
2103         from xarray.backends.api import to_zarr
2104 
2105         return to_zarr(  # type: ignore[call-overload,misc]
2106             self,
2107             store=store,
2108             chunk_store=chunk_store,
2109             storage_options=storage_options,
2110             mode=mode,
2111             synchronizer=synchronizer,
2112             group=group,
2113             encoding=encoding,
2114             compute=compute,
2115             consolidated=consolidated,
2116             append_dim=append_dim,
2117             region=region,
2118             safe_chunks=safe_chunks,
2119             zarr_version=zarr_version,
2120         )
2121 
2122     def __repr__(self) -> str:
2123         return formatting.dataset_repr(self)
2124 
2125     def _repr_html_(self) -> str:
2126         if OPTIONS["display_style"] == "text":
2127             return f"<pre>{escape(repr(self))}</pre>"
2128         return formatting_html.dataset_repr(self)
2129 
2130     def info(self, buf: IO | None = None) -> None:
2131         """
2132         Concise summary of a Dataset variables and attributes.
2133 
2134         Parameters
2135         ----------
2136         buf : file-like, default: sys.stdout
2137             writable buffer
2138 
2139         See Also
2140         --------
2141         pandas.DataFrame.assign
2142         ncdump : netCDF's ncdump
2143         """
2144         if buf is None:  # pragma: no cover
2145             buf = sys.stdout
2146 
2147         lines = []
2148         lines.append("xarray.Dataset {")
2149         lines.append("dimensions:")
2150         for name, size in self.dims.items():
2151             lines.append(f"\t{name} = {size} ;")
2152         lines.append("\nvariables:")
2153         for name, da in self.variables.items():
2154             dims = ", ".join(map(str, da.dims))
2155             lines.append(f"\t{da.dtype} {name}({dims}) ;")
2156             for k, v in da.attrs.items():
2157                 lines.append(f"\t\t{name}:{k} = {v} ;")
2158         lines.append("\n// global attributes:")
2159         for k, v in self.attrs.items():
2160             lines.append(f"\t:{k} = {v} ;")
2161         lines.append("}")
2162 
2163         buf.write("\n".join(lines))
2164 
2165     @property
2166     def chunks(self) -> Mapping[Hashable, tuple[int, ...]]:
2167         """
2168         Mapping from dimension names to block lengths for this dataset's data, or None if
2169         the underlying data is not a dask array.
2170         Cannot be modified directly, but can be modified by calling .chunk().
2171 
2172         Same as Dataset.chunksizes, but maintained for backwards compatibility.
2173 
2174         See Also
2175         --------
2176         Dataset.chunk
2177         Dataset.chunksizes
2178         xarray.unify_chunks
2179         """
2180         return get_chunksizes(self.variables.values())
2181 
2182     @property
2183     def chunksizes(self) -> Mapping[Hashable, tuple[int, ...]]:
2184         """
2185         Mapping from dimension names to block lengths for this dataset's data, or None if
2186         the underlying data is not a dask array.
2187         Cannot be modified directly, but can be modified by calling .chunk().
2188 
2189         Same as Dataset.chunks.
2190 
2191         See Also
2192         --------
2193         Dataset.chunk
2194         Dataset.chunks
2195         xarray.unify_chunks
2196         """
2197         return get_chunksizes(self.variables.values())
2198 
2199     def chunk(
2200         self: T_Dataset,
2201         chunks: (
2202             int | Literal["auto"] | Mapping[Any, None | int | str | tuple[int, ...]]
2203         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
2204         name_prefix: str = "xarray-",
2205         token: str | None = None,
2206         lock: bool = False,
2207         inline_array: bool = False,
2208         **chunks_kwargs: None | int | str | tuple[int, ...],
2209     ) -> T_Dataset:
2210         """Coerce all arrays in this dataset into dask arrays with the given
2211         chunks.
2212 
2213         Non-dask arrays in this dataset will be converted to dask arrays. Dask
2214         arrays will be rechunked to the given chunk sizes.
2215 
2216         If neither chunks is not provided for one or more dimensions, chunk
2217         sizes along that dimension will not be updated; non-dask arrays will be
2218         converted into dask arrays with a single block.
2219 
2220         Parameters
2221         ----------
2222         chunks : int, tuple of int, "auto" or mapping of hashable to int, optional
2223             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, or
2224             ``{"x": 5, "y": 5}``.
2225         name_prefix : str, default: "xarray-"
2226             Prefix for the name of any new dask arrays.
2227         token : str, optional
2228             Token uniquely identifying this dataset.
2229         lock : bool, default: False
2230             Passed on to :py:func:`dask.array.from_array`, if the array is not
2231             already as dask array.
2232         inline_array: bool, default: False
2233             Passed on to :py:func:`dask.array.from_array`, if the array is not
2234             already as dask array.
2235         **chunks_kwargs : {dim: chunks, ...}, optional
2236             The keyword arguments form of ``chunks``.
2237             One of chunks or chunks_kwargs must be provided
2238 
2239         Returns
2240         -------
2241         chunked : xarray.Dataset
2242 
2243         See Also
2244         --------
2245         Dataset.chunks
2246         Dataset.chunksizes
2247         xarray.unify_chunks
2248         dask.array.from_array
2249         """
2250         if chunks is None and chunks_kwargs is None:
2251             warnings.warn(
2252                 "None value for 'chunks' is deprecated. "
2253                 "It will raise an error in the future. Use instead '{}'",
2254                 category=FutureWarning,
2255             )
2256             chunks = {}
2257 
2258         if isinstance(chunks, (Number, str, int)):
2259             chunks = dict.fromkeys(self.dims, chunks)
2260         else:
2261             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
2262 
2263         bad_dims = chunks.keys() - self.dims.keys()
2264         if bad_dims:
2265             raise ValueError(
2266                 f"some chunks keys are not dimensions on this object: {bad_dims}"
2267             )
2268 
2269         variables = {
2270             k: _maybe_chunk(k, v, chunks, token, lock, name_prefix)
2271             for k, v in self.variables.items()
2272         }
2273         return self._replace(variables)
2274 
2275     def _validate_indexers(
2276         self, indexers: Mapping[Any, Any], missing_dims: ErrorOptionsWithWarn = "raise"
2277     ) -> Iterator[tuple[Hashable, int | slice | np.ndarray | Variable]]:
2278         """Here we make sure
2279         + indexer has a valid keys
2280         + indexer is in a valid data type
2281         + string indexers are cast to the appropriate date type if the
2282           associated index is a DatetimeIndex or CFTimeIndex
2283         """
2284         from xarray.coding.cftimeindex import CFTimeIndex
2285         from xarray.core.dataarray import DataArray
2286 
2287         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2288 
2289         # all indexers should be int, slice, np.ndarrays, or Variable
2290         for k, v in indexers.items():
2291             if isinstance(v, (int, slice, Variable)):
2292                 yield k, v
2293             elif isinstance(v, DataArray):
2294                 yield k, v.variable
2295             elif isinstance(v, tuple):
2296                 yield k, as_variable(v)
2297             elif isinstance(v, Dataset):
2298                 raise TypeError("cannot use a Dataset as an indexer")
2299             elif isinstance(v, Sequence) and len(v) == 0:
2300                 yield k, np.empty((0,), dtype="int64")
2301             else:
2302                 if not is_duck_array(v):
2303                     v = np.asarray(v)
2304 
2305                 if v.dtype.kind in "US":
2306                     index = self._indexes[k].to_pandas_index()
2307                     if isinstance(index, pd.DatetimeIndex):
2308                         v = v.astype("datetime64[ns]")
2309                     elif isinstance(index, CFTimeIndex):
2310                         v = _parse_array_of_cftime_strings(v, index.date_type)
2311 
2312                 if v.ndim > 1:
2313                     raise IndexError(
2314                         "Unlabeled multi-dimensional array cannot be "
2315                         "used for indexing: {}".format(k)
2316                     )
2317                 yield k, v
2318 
2319     def _validate_interp_indexers(
2320         self, indexers: Mapping[Any, Any]
2321     ) -> Iterator[tuple[Hashable, Variable]]:
2322         """Variant of _validate_indexers to be used for interpolation"""
2323         for k, v in self._validate_indexers(indexers):
2324             if isinstance(v, Variable):
2325                 if v.ndim == 1:
2326                     yield k, v.to_index_variable()
2327                 else:
2328                     yield k, v
2329             elif isinstance(v, int):
2330                 yield k, Variable((), v, attrs=self.coords[k].attrs)
2331             elif isinstance(v, np.ndarray):
2332                 if v.ndim == 0:
2333                     yield k, Variable((), v, attrs=self.coords[k].attrs)
2334                 elif v.ndim == 1:
2335                     yield k, IndexVariable((k,), v, attrs=self.coords[k].attrs)
2336                 else:
2337                     raise AssertionError()  # Already tested by _validate_indexers
2338             else:
2339                 raise TypeError(type(v))
2340 
2341     def _get_indexers_coords_and_indexes(self, indexers):
2342         """Extract coordinates and indexes from indexers.
2343 
2344         Only coordinate with a name different from any of self.variables will
2345         be attached.
2346         """
2347         from xarray.core.dataarray import DataArray
2348 
2349         coords_list = []
2350         for k, v in indexers.items():
2351             if isinstance(v, DataArray):
2352                 if v.dtype.kind == "b":
2353                     if v.ndim != 1:  # we only support 1-d boolean array
2354                         raise ValueError(
2355                             "{:d}d-boolean array is used for indexing along "
2356                             "dimension {!r}, but only 1d boolean arrays are "
2357                             "supported.".format(v.ndim, k)
2358                         )
2359                     # Make sure in case of boolean DataArray, its
2360                     # coordinate also should be indexed.
2361                     v_coords = v[v.values.nonzero()[0]].coords
2362                 else:
2363                     v_coords = v.coords
2364                 coords_list.append(v_coords)
2365 
2366         # we don't need to call align() explicitly or check indexes for
2367         # alignment, because merge_variables already checks for exact alignment
2368         # between dimension coordinates
2369         coords, indexes = merge_coordinates_without_align(coords_list)
2370         assert_coordinate_consistent(self, coords)
2371 
2372         # silently drop the conflicted variables.
2373         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}
2374         attached_indexes = {
2375             k: v for k, v in indexes.items() if k not in self._variables
2376         }
2377         return attached_coords, attached_indexes
2378 
2379     def isel(
2380         self: T_Dataset,
2381         indexers: Mapping[Any, Any] | None = None,
2382         drop: bool = False,
2383         missing_dims: ErrorOptionsWithWarn = "raise",
2384         **indexers_kwargs: Any,
2385     ) -> T_Dataset:
2386         """Returns a new dataset with each array indexed along the specified
2387         dimension(s).
2388 
2389         This method selects values from each array using its `__getitem__`
2390         method, except this method does not require knowing the order of
2391         each array's dimensions.
2392 
2393         Parameters
2394         ----------
2395         indexers : dict, optional
2396             A dict with keys matching dimensions and values given
2397             by integers, slice objects or arrays.
2398             indexer can be a integer, slice, array-like or DataArray.
2399             If DataArrays are passed as indexers, xarray-style indexing will be
2400             carried out. See :ref:`indexing` for the details.
2401             One of indexers or indexers_kwargs must be provided.
2402         drop : bool, default: False
2403             If ``drop=True``, drop coordinates variables indexed by integers
2404             instead of making them scalar.
2405         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2406             What to do if dimensions that should be selected from are not present in the
2407             Dataset:
2408             - "raise": raise an exception
2409             - "warn": raise a warning, and ignore the missing dimensions
2410             - "ignore": ignore the missing dimensions
2411 
2412         **indexers_kwargs : {dim: indexer, ...}, optional
2413             The keyword arguments form of ``indexers``.
2414             One of indexers or indexers_kwargs must be provided.
2415 
2416         Returns
2417         -------
2418         obj : Dataset
2419             A new Dataset with the same contents as this dataset, except each
2420             array and dimension is indexed by the appropriate indexers.
2421             If indexer DataArrays have coordinates that do not conflict with
2422             this object, then these coordinates will be attached.
2423             In general, each array's data will be a view of the array's data
2424             in this dataset, unless vectorized indexing was triggered by using
2425             an array indexer, in which case the data will be a copy.
2426 
2427         See Also
2428         --------
2429         Dataset.sel
2430         DataArray.isel
2431         """
2432         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
2433         if any(is_fancy_indexer(idx) for idx in indexers.values()):
2434             return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)
2435 
2436         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
2437         # lists, or zero or one-dimensional np.ndarray's
2438         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2439 
2440         variables = {}
2441         dims: dict[Hashable, int] = {}
2442         coord_names = self._coord_names.copy()
2443 
2444         indexes, index_variables = isel_indexes(self.xindexes, indexers)
2445 
2446         for name, var in self._variables.items():
2447             # preserve variable order
2448             if name in index_variables:
2449                 var = index_variables[name]
2450             else:
2451                 var_indexers = {k: v for k, v in indexers.items() if k in var.dims}
2452                 if var_indexers:
2453                     var = var.isel(var_indexers)
2454                     if drop and var.ndim == 0 and name in coord_names:
2455                         coord_names.remove(name)
2456                         continue
2457             variables[name] = var
2458             dims.update(zip(var.dims, var.shape))
2459 
2460         return self._construct_direct(
2461             variables=variables,
2462             coord_names=coord_names,
2463             dims=dims,
2464             attrs=self._attrs,
2465             indexes=indexes,
2466             encoding=self._encoding,
2467             close=self._close,
2468         )
2469 
2470     def _isel_fancy(
2471         self: T_Dataset,
2472         indexers: Mapping[Any, Any],
2473         *,
2474         drop: bool,
2475         missing_dims: ErrorOptionsWithWarn = "raise",
2476     ) -> T_Dataset:
2477         valid_indexers = dict(self._validate_indexers(indexers, missing_dims))
2478 
2479         variables: dict[Hashable, Variable] = {}
2480         indexes, index_variables = isel_indexes(self.xindexes, valid_indexers)
2481 
2482         for name, var in self.variables.items():
2483             if name in index_variables:
2484                 new_var = index_variables[name]
2485             else:
2486                 var_indexers = {
2487                     k: v for k, v in valid_indexers.items() if k in var.dims
2488                 }
2489                 if var_indexers:
2490                     new_var = var.isel(indexers=var_indexers)
2491                     # drop scalar coordinates
2492                     # https://github.com/pydata/xarray/issues/6554
2493                     if name in self.coords and drop and new_var.ndim == 0:
2494                         continue
2495                 else:
2496                     new_var = var.copy(deep=False)
2497                 if name not in indexes:
2498                     new_var = new_var.to_base_variable()
2499             variables[name] = new_var
2500 
2501         coord_names = self._coord_names & variables.keys()
2502         selected = self._replace_with_new_dims(variables, coord_names, indexes)
2503 
2504         # Extract coordinates from indexers
2505         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)
2506         variables.update(coord_vars)
2507         indexes.update(new_indexes)
2508         coord_names = self._coord_names & variables.keys() | coord_vars.keys()
2509         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2510 
2511     def sel(
2512         self: T_Dataset,
2513         indexers: Mapping[Any, Any] | None = None,
2514         method: str | None = None,
2515         tolerance: int | float | Iterable[int | float] | None = None,
2516         drop: bool = False,
2517         **indexers_kwargs: Any,
2518     ) -> T_Dataset:
2519         """Returns a new dataset with each array indexed by tick labels
2520         along the specified dimension(s).
2521 
2522         In contrast to `Dataset.isel`, indexers for this method should use
2523         labels instead of integers.
2524 
2525         Under the hood, this method is powered by using pandas's powerful Index
2526         objects. This makes label based indexing essentially just as fast as
2527         using integer indexing.
2528 
2529         It also means this method uses pandas's (well documented) logic for
2530         indexing. This means you can use string shortcuts for datetime indexes
2531         (e.g., '2000-01' to select all values in January 2000). It also means
2532         that slices are treated as inclusive of both the start and stop values,
2533         unlike normal Python indexing.
2534 
2535         Parameters
2536         ----------
2537         indexers : dict, optional
2538             A dict with keys matching dimensions and values given
2539             by scalars, slices or arrays of tick labels. For dimensions with
2540             multi-index, the indexer may also be a dict-like object with keys
2541             matching index level names.
2542             If DataArrays are passed as indexers, xarray-style indexing will be
2543             carried out. See :ref:`indexing` for the details.
2544             One of indexers or indexers_kwargs must be provided.
2545         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2546             Method to use for inexact matches:
2547 
2548             * None (default): only exact matches
2549             * pad / ffill: propagate last valid index value forward
2550             * backfill / bfill: propagate next valid index value backward
2551             * nearest: use nearest valid index value
2552         tolerance : optional
2553             Maximum distance between original and new labels for inexact
2554             matches. The values of the index at the matching locations must
2555             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2556         drop : bool, optional
2557             If ``drop=True``, drop coordinates variables in `indexers` instead
2558             of making them scalar.
2559         **indexers_kwargs : {dim: indexer, ...}, optional
2560             The keyword arguments form of ``indexers``.
2561             One of indexers or indexers_kwargs must be provided.
2562 
2563         Returns
2564         -------
2565         obj : Dataset
2566             A new Dataset with the same contents as this dataset, except each
2567             variable and dimension is indexed by the appropriate indexers.
2568             If indexer DataArrays have coordinates that do not conflict with
2569             this object, then these coordinates will be attached.
2570             In general, each array's data will be a view of the array's data
2571             in this dataset, unless vectorized indexing was triggered by using
2572             an array indexer, in which case the data will be a copy.
2573 
2574         See Also
2575         --------
2576         Dataset.isel
2577         DataArray.sel
2578         """
2579         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "sel")
2580         query_results = map_index_queries(
2581             self, indexers=indexers, method=method, tolerance=tolerance
2582         )
2583 
2584         if drop:
2585             no_scalar_variables = {}
2586             for k, v in query_results.variables.items():
2587                 if v.dims:
2588                     no_scalar_variables[k] = v
2589                 else:
2590                     if k in self._coord_names:
2591                         query_results.drop_coords.append(k)
2592             query_results.variables = no_scalar_variables
2593 
2594         result = self.isel(indexers=query_results.dim_indexers, drop=drop)
2595         return result._overwrite_indexes(*query_results.as_tuple()[1:])
2596 
2597     def head(
2598         self: T_Dataset,
2599         indexers: Mapping[Any, int] | int | None = None,
2600         **indexers_kwargs: Any,
2601     ) -> T_Dataset:
2602         """Returns a new dataset with the first `n` values of each array
2603         for the specified dimension(s).
2604 
2605         Parameters
2606         ----------
2607         indexers : dict or int, default: 5
2608             A dict with keys matching dimensions and integer values `n`
2609             or a single integer `n` applied over all dimensions.
2610             One of indexers or indexers_kwargs must be provided.
2611         **indexers_kwargs : {dim: n, ...}, optional
2612             The keyword arguments form of ``indexers``.
2613             One of indexers or indexers_kwargs must be provided.
2614 
2615         See Also
2616         --------
2617         Dataset.tail
2618         Dataset.thin
2619         DataArray.head
2620         """
2621         if not indexers_kwargs:
2622             if indexers is None:
2623                 indexers = 5
2624             if not isinstance(indexers, int) and not is_dict_like(indexers):
2625                 raise TypeError("indexers must be either dict-like or a single integer")
2626         if isinstance(indexers, int):
2627             indexers = {dim: indexers for dim in self.dims}
2628         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "head")
2629         for k, v in indexers.items():
2630             if not isinstance(v, int):
2631                 raise TypeError(
2632                     "expected integer type indexer for "
2633                     f"dimension {k!r}, found {type(v)!r}"
2634                 )
2635             elif v < 0:
2636                 raise ValueError(
2637                     "expected positive integer as indexer "
2638                     f"for dimension {k!r}, found {v}"
2639                 )
2640         indexers_slices = {k: slice(val) for k, val in indexers.items()}
2641         return self.isel(indexers_slices)
2642 
2643     def tail(
2644         self: T_Dataset,
2645         indexers: Mapping[Any, int] | int | None = None,
2646         **indexers_kwargs: Any,
2647     ) -> T_Dataset:
2648         """Returns a new dataset with the last `n` values of each array
2649         for the specified dimension(s).
2650 
2651         Parameters
2652         ----------
2653         indexers : dict or int, default: 5
2654             A dict with keys matching dimensions and integer values `n`
2655             or a single integer `n` applied over all dimensions.
2656             One of indexers or indexers_kwargs must be provided.
2657         **indexers_kwargs : {dim: n, ...}, optional
2658             The keyword arguments form of ``indexers``.
2659             One of indexers or indexers_kwargs must be provided.
2660 
2661         See Also
2662         --------
2663         Dataset.head
2664         Dataset.thin
2665         DataArray.tail
2666         """
2667         if not indexers_kwargs:
2668             if indexers is None:
2669                 indexers = 5
2670             if not isinstance(indexers, int) and not is_dict_like(indexers):
2671                 raise TypeError("indexers must be either dict-like or a single integer")
2672         if isinstance(indexers, int):
2673             indexers = {dim: indexers for dim in self.dims}
2674         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "tail")
2675         for k, v in indexers.items():
2676             if not isinstance(v, int):
2677                 raise TypeError(
2678                     "expected integer type indexer for "
2679                     f"dimension {k!r}, found {type(v)!r}"
2680                 )
2681             elif v < 0:
2682                 raise ValueError(
2683                     "expected positive integer as indexer "
2684                     f"for dimension {k!r}, found {v}"
2685                 )
2686         indexers_slices = {
2687             k: slice(-val, None) if val != 0 else slice(val)
2688             for k, val in indexers.items()
2689         }
2690         return self.isel(indexers_slices)
2691 
2692     def thin(
2693         self: T_Dataset,
2694         indexers: Mapping[Any, int] | int | None = None,
2695         **indexers_kwargs: Any,
2696     ) -> T_Dataset:
2697         """Returns a new dataset with each array indexed along every `n`-th
2698         value for the specified dimension(s)
2699 
2700         Parameters
2701         ----------
2702         indexers : dict or int
2703             A dict with keys matching dimensions and integer values `n`
2704             or a single integer `n` applied over all dimensions.
2705             One of indexers or indexers_kwargs must be provided.
2706         **indexers_kwargs : {dim: n, ...}, optional
2707             The keyword arguments form of ``indexers``.
2708             One of indexers or indexers_kwargs must be provided.
2709 
2710         Examples
2711         --------
2712         >>> x_arr = np.arange(0, 26)
2713         >>> x_arr
2714         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
2715                17, 18, 19, 20, 21, 22, 23, 24, 25])
2716         >>> x = xr.DataArray(
2717         ...     np.reshape(x_arr, (2, 13)),
2718         ...     dims=("x", "y"),
2719         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
2720         ... )
2721         >>> x_ds = xr.Dataset({"foo": x})
2722         >>> x_ds
2723         <xarray.Dataset>
2724         Dimensions:  (x: 2, y: 13)
2725         Coordinates:
2726           * x        (x) int64 0 1
2727           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
2728         Data variables:
2729             foo      (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24 25
2730 
2731         >>> x_ds.thin(3)
2732         <xarray.Dataset>
2733         Dimensions:  (x: 1, y: 5)
2734         Coordinates:
2735           * x        (x) int64 0
2736           * y        (y) int64 0 3 6 9 12
2737         Data variables:
2738             foo      (x, y) int64 0 3 6 9 12
2739         >>> x.thin({"x": 2, "y": 5})
2740         <xarray.DataArray (x: 1, y: 3)>
2741         array([[ 0,  5, 10]])
2742         Coordinates:
2743           * x        (x) int64 0
2744           * y        (y) int64 0 5 10
2745 
2746         See Also
2747         --------
2748         Dataset.head
2749         Dataset.tail
2750         DataArray.thin
2751         """
2752         if (
2753             not indexers_kwargs
2754             and not isinstance(indexers, int)
2755             and not is_dict_like(indexers)
2756         ):
2757             raise TypeError("indexers must be either dict-like or a single integer")
2758         if isinstance(indexers, int):
2759             indexers = {dim: indexers for dim in self.dims}
2760         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "thin")
2761         for k, v in indexers.items():
2762             if not isinstance(v, int):
2763                 raise TypeError(
2764                     "expected integer type indexer for "
2765                     f"dimension {k!r}, found {type(v)!r}"
2766                 )
2767             elif v < 0:
2768                 raise ValueError(
2769                     "expected positive integer as indexer "
2770                     f"for dimension {k!r}, found {v}"
2771                 )
2772             elif v == 0:
2773                 raise ValueError("step cannot be zero")
2774         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}
2775         return self.isel(indexers_slices)
2776 
2777     def broadcast_like(
2778         self: T_Dataset,
2779         other: Dataset | DataArray,
2780         exclude: Iterable[Hashable] | None = None,
2781     ) -> T_Dataset:
2782         """Broadcast this DataArray against another Dataset or DataArray.
2783         This is equivalent to xr.broadcast(other, self)[1]
2784 
2785         Parameters
2786         ----------
2787         other : Dataset or DataArray
2788             Object against which to broadcast this array.
2789         exclude : iterable of hashable, optional
2790             Dimensions that must not be broadcasted
2791 
2792         """
2793         if exclude is None:
2794             exclude = set()
2795         else:
2796             exclude = set(exclude)
2797         args = align(other, self, join="outer", copy=False, exclude=exclude)
2798 
2799         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
2800 
2801         return _broadcast_helper(
2802             cast("T_Dataset", args[1]), exclude, dims_map, common_coords
2803         )
2804 
2805     def _reindex_callback(
2806         self,
2807         aligner: alignment.Aligner,
2808         dim_pos_indexers: dict[Hashable, Any],
2809         variables: dict[Hashable, Variable],
2810         indexes: dict[Hashable, Index],
2811         fill_value: Any,
2812         exclude_dims: frozenset[Hashable],
2813         exclude_vars: frozenset[Hashable],
2814     ) -> Dataset:
2815         """Callback called from ``Aligner`` to create a new reindexed Dataset."""
2816 
2817         new_variables = variables.copy()
2818         new_indexes = indexes.copy()
2819 
2820         # re-assign variable metadata
2821         for name, new_var in new_variables.items():
2822             var = self._variables.get(name)
2823             if var is not None:
2824                 new_var.attrs = var.attrs
2825                 new_var.encoding = var.encoding
2826 
2827         # pass through indexes from excluded dimensions
2828         # no extra check needed for multi-coordinate indexes, potential conflicts
2829         # should already have been detected when aligning the indexes
2830         for name, idx in self._indexes.items():
2831             var = self._variables[name]
2832             if set(var.dims) <= exclude_dims:
2833                 new_indexes[name] = idx
2834                 new_variables[name] = var
2835 
2836         if not dim_pos_indexers:
2837             # fast path for no reindexing necessary
2838             if set(new_indexes) - set(self._indexes):
2839                 # this only adds new indexes and their coordinate variables
2840                 reindexed = self._overwrite_indexes(new_indexes, new_variables)
2841             else:
2842                 reindexed = self.copy(deep=aligner.copy)
2843         else:
2844             to_reindex = {
2845                 k: v
2846                 for k, v in self.variables.items()
2847                 if k not in variables and k not in exclude_vars
2848             }
2849             reindexed_vars = alignment.reindex_variables(
2850                 to_reindex,
2851                 dim_pos_indexers,
2852                 copy=aligner.copy,
2853                 fill_value=fill_value,
2854                 sparse=aligner.sparse,
2855             )
2856             new_variables.update(reindexed_vars)
2857             new_coord_names = self._coord_names | set(new_indexes)
2858             reindexed = self._replace_with_new_dims(
2859                 new_variables, new_coord_names, indexes=new_indexes
2860             )
2861 
2862         return reindexed
2863 
2864     def reindex_like(
2865         self: T_Dataset,
2866         other: Dataset | DataArray,
2867         method: ReindexMethodOptions = None,
2868         tolerance: int | float | Iterable[int | float] | None = None,
2869         copy: bool = True,
2870         fill_value: Any = xrdtypes.NA,
2871     ) -> T_Dataset:
2872         """Conform this object onto the indexes of another object, filling in
2873         missing values with ``fill_value``. The default fill value is NaN.
2874 
2875         Parameters
2876         ----------
2877         other : Dataset or DataArray
2878             Object with an 'indexes' attribute giving a mapping from dimension
2879             names to pandas.Index objects, which provides coordinates upon
2880             which to index the variables in this dataset. The indexes on this
2881             other object need not be the same as the indexes on this
2882             dataset. Any mis-matched index values will be filled in with
2883             NaN, and any mis-matched dimension names will simply be ignored.
2884         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2885             Method to use for filling index values from other not found in this
2886             dataset:
2887 
2888             - None (default): don't fill gaps
2889             - "pad" / "ffill": propagate last valid index value forward
2890             - "backfill" / "bfill": propagate next valid index value backward
2891             - "nearest": use nearest valid index value
2892 
2893         tolerance : optional
2894             Maximum distance between original and new labels for inexact
2895             matches. The values of the index at the matching locations must
2896             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2897             Tolerance may be a scalar value, which applies the same tolerance
2898             to all values, or list-like, which applies variable tolerance per
2899             element. List-like must be the same size as the index and its dtype
2900             must exactly match the index’s type.
2901         copy : bool, default: True
2902             If ``copy=True``, data in the return value is always copied. If
2903             ``copy=False`` and reindexing is unnecessary, or can be performed
2904             with only slice operations, then the output may share memory with
2905             the input. In either case, a new xarray object is always returned.
2906         fill_value : scalar or dict-like, optional
2907             Value to use for newly missing values. If a dict-like maps
2908             variable names to fill values.
2909 
2910         Returns
2911         -------
2912         reindexed : Dataset
2913             Another dataset, with this dataset's data but coordinates from the
2914             other object.
2915 
2916         See Also
2917         --------
2918         Dataset.reindex
2919         align
2920         """
2921         return alignment.reindex_like(
2922             self,
2923             other=other,
2924             method=method,
2925             tolerance=tolerance,
2926             copy=copy,
2927             fill_value=fill_value,
2928         )
2929 
2930     def reindex(
2931         self: T_Dataset,
2932         indexers: Mapping[Any, Any] | None = None,
2933         method: ReindexMethodOptions = None,
2934         tolerance: int | float | Iterable[int | float] | None = None,
2935         copy: bool = True,
2936         fill_value: Any = xrdtypes.NA,
2937         **indexers_kwargs: Any,
2938     ) -> T_Dataset:
2939         """Conform this object onto a new set of indexes, filling in
2940         missing values with ``fill_value``. The default fill value is NaN.
2941 
2942         Parameters
2943         ----------
2944         indexers : dict, optional
2945             Dictionary with keys given by dimension names and values given by
2946             arrays of coordinates tick labels. Any mis-matched coordinate
2947             values will be filled in with NaN, and any mis-matched dimension
2948             names will simply be ignored.
2949             One of indexers or indexers_kwargs must be provided.
2950         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2951             Method to use for filling index values in ``indexers`` not found in
2952             this dataset:
2953 
2954             - None (default): don't fill gaps
2955             - "pad" / "ffill": propagate last valid index value forward
2956             - "backfill" / "bfill": propagate next valid index value backward
2957             - "nearest": use nearest valid index value
2958 
2959         tolerance : optional
2960             Maximum distance between original and new labels for inexact
2961             matches. The values of the index at the matching locations must
2962             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2963             Tolerance may be a scalar value, which applies the same tolerance
2964             to all values, or list-like, which applies variable tolerance per
2965             element. List-like must be the same size as the index and its dtype
2966             must exactly match the index’s type.
2967         copy : bool, default: True
2968             If ``copy=True``, data in the return value is always copied. If
2969             ``copy=False`` and reindexing is unnecessary, or can be performed
2970             with only slice operations, then the output may share memory with
2971             the input. In either case, a new xarray object is always returned.
2972         fill_value : scalar or dict-like, optional
2973             Value to use for newly missing values. If a dict-like,
2974             maps variable names (including coordinates) to fill values.
2975         sparse : bool, default: False
2976             use sparse-array.
2977         **indexers_kwargs : {dim: indexer, ...}, optional
2978             Keyword arguments in the same form as ``indexers``.
2979             One of indexers or indexers_kwargs must be provided.
2980 
2981         Returns
2982         -------
2983         reindexed : Dataset
2984             Another dataset, with this dataset's data but replaced coordinates.
2985 
2986         See Also
2987         --------
2988         Dataset.reindex_like
2989         align
2990         pandas.Index.get_indexer
2991 
2992         Examples
2993         --------
2994         Create a dataset with some fictional data.
2995 
2996         >>> x = xr.Dataset(
2997         ...     {
2998         ...         "temperature": ("station", 20 * np.random.rand(4)),
2999         ...         "pressure": ("station", 500 * np.random.rand(4)),
3000         ...     },
3001         ...     coords={"station": ["boston", "nyc", "seattle", "denver"]},
3002         ... )
3003         >>> x
3004         <xarray.Dataset>
3005         Dimensions:      (station: 4)
3006         Coordinates:
3007           * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'
3008         Data variables:
3009             temperature  (station) float64 10.98 14.3 12.06 10.9
3010             pressure     (station) float64 211.8 322.9 218.8 445.9
3011         >>> x.indexes
3012         Indexes:
3013             station  Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')
3014 
3015         Create a new index and reindex the dataset. By default values in the new index that
3016         do not have corresponding records in the dataset are assigned `NaN`.
3017 
3018         >>> new_index = ["boston", "austin", "seattle", "lincoln"]
3019         >>> x.reindex({"station": new_index})
3020         <xarray.Dataset>
3021         Dimensions:      (station: 4)
3022         Coordinates:
3023           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
3024         Data variables:
3025             temperature  (station) float64 10.98 nan 12.06 nan
3026             pressure     (station) float64 211.8 nan 218.8 nan
3027 
3028         We can fill in the missing values by passing a value to the keyword `fill_value`.
3029 
3030         >>> x.reindex({"station": new_index}, fill_value=0)
3031         <xarray.Dataset>
3032         Dimensions:      (station: 4)
3033         Coordinates:
3034           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
3035         Data variables:
3036             temperature  (station) float64 10.98 0.0 12.06 0.0
3037             pressure     (station) float64 211.8 0.0 218.8 0.0
3038 
3039         We can also use different fill values for each variable.
3040 
3041         >>> x.reindex(
3042         ...     {"station": new_index}, fill_value={"temperature": 0, "pressure": 100}
3043         ... )
3044         <xarray.Dataset>
3045         Dimensions:      (station: 4)
3046         Coordinates:
3047           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
3048         Data variables:
3049             temperature  (station) float64 10.98 0.0 12.06 0.0
3050             pressure     (station) float64 211.8 100.0 218.8 100.0
3051 
3052         Because the index is not monotonically increasing or decreasing, we cannot use arguments
3053         to the keyword method to fill the `NaN` values.
3054 
3055         >>> x.reindex({"station": new_index}, method="nearest")
3056         Traceback (most recent call last):
3057         ...
3058             raise ValueError('index must be monotonic increasing or decreasing')
3059         ValueError: index must be monotonic increasing or decreasing
3060 
3061         To further illustrate the filling functionality in reindex, we will create a
3062         dataset with a monotonically increasing index (for example, a sequence of dates).
3063 
3064         >>> x2 = xr.Dataset(
3065         ...     {
3066         ...         "temperature": (
3067         ...             "time",
3068         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],
3069         ...         ),
3070         ...         "pressure": ("time", 500 * np.random.rand(6)),
3071         ...     },
3072         ...     coords={"time": pd.date_range("01/01/2019", periods=6, freq="D")},
3073         ... )
3074         >>> x2
3075         <xarray.Dataset>
3076         Dimensions:      (time: 6)
3077         Coordinates:
3078           * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06
3079         Data variables:
3080             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12
3081             pressure     (time) float64 481.8 191.7 395.9 264.4 284.0 462.8
3082 
3083         Suppose we decide to expand the dataset to cover a wider date range.
3084 
3085         >>> time_index2 = pd.date_range("12/29/2018", periods=10, freq="D")
3086         >>> x2.reindex({"time": time_index2})
3087         <xarray.Dataset>
3088         Dimensions:      (time: 10)
3089         Coordinates:
3090           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3091         Data variables:
3092             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan
3093             pressure     (time) float64 nan nan nan 481.8 ... 264.4 284.0 462.8 nan
3094 
3095         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)
3096         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.
3097 
3098         For example, to back-propagate the last valid value to fill the `NaN` values,
3099         pass `bfill` as an argument to the `method` keyword.
3100 
3101         >>> x3 = x2.reindex({"time": time_index2}, method="bfill")
3102         >>> x3
3103         <xarray.Dataset>
3104         Dimensions:      (time: 10)
3105         Coordinates:
3106           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3107         Data variables:
3108             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan
3109             pressure     (time) float64 481.8 481.8 481.8 481.8 ... 284.0 462.8 nan
3110 
3111         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)
3112         will not be filled by any of the value propagation schemes.
3113 
3114         >>> x2.where(x2.temperature.isnull(), drop=True)
3115         <xarray.Dataset>
3116         Dimensions:      (time: 1)
3117         Coordinates:
3118           * time         (time) datetime64[ns] 2019-01-03
3119         Data variables:
3120             temperature  (time) float64 nan
3121             pressure     (time) float64 395.9
3122         >>> x3.where(x3.temperature.isnull(), drop=True)
3123         <xarray.Dataset>
3124         Dimensions:      (time: 2)
3125         Coordinates:
3126           * time         (time) datetime64[ns] 2019-01-03 2019-01-07
3127         Data variables:
3128             temperature  (time) float64 nan nan
3129             pressure     (time) float64 395.9 nan
3130 
3131         This is because filling while reindexing does not look at dataset values, but only compares
3132         the original and desired indexes. If you do want to fill in the `NaN` values present in the
3133         original dataset, use the :py:meth:`~Dataset.fillna()` method.
3134 
3135         """
3136         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3137         return alignment.reindex(
3138             self,
3139             indexers=indexers,
3140             method=method,
3141             tolerance=tolerance,
3142             copy=copy,
3143             fill_value=fill_value,
3144         )
3145 
3146     def _reindex(
3147         self: T_Dataset,
3148         indexers: Mapping[Any, Any] | None = None,
3149         method: str | None = None,
3150         tolerance: int | float | Iterable[int | float] | None = None,
3151         copy: bool = True,
3152         fill_value: Any = xrdtypes.NA,
3153         sparse: bool = False,
3154         **indexers_kwargs: Any,
3155     ) -> T_Dataset:
3156         """
3157         Same as reindex but supports sparse option.
3158         """
3159         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3160         return alignment.reindex(
3161             self,
3162             indexers=indexers,
3163             method=method,
3164             tolerance=tolerance,
3165             copy=copy,
3166             fill_value=fill_value,
3167             sparse=sparse,
3168         )
3169 
3170     def interp(
3171         self: T_Dataset,
3172         coords: Mapping[Any, Any] | None = None,
3173         method: InterpOptions = "linear",
3174         assume_sorted: bool = False,
3175         kwargs: Mapping[str, Any] | None = None,
3176         method_non_numeric: str = "nearest",
3177         **coords_kwargs: Any,
3178     ) -> T_Dataset:
3179         """Interpolate a Dataset onto new coordinates
3180 
3181         Performs univariate or multivariate interpolation of a Dataset onto
3182         new coordinates using scipy's interpolation routines. If interpolating
3183         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
3184         called.  When interpolating along multiple existing dimensions, an
3185         attempt is made to decompose the interpolation into multiple
3186         1-dimensional interpolations. If this is possible,
3187         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3188         :py:func:`scipy.interpolate.interpn` is called.
3189 
3190         Parameters
3191         ----------
3192         coords : dict, optional
3193             Mapping from dimension names to the new coordinates.
3194             New coordinate can be a scalar, array-like or DataArray.
3195             If DataArrays are passed as new coordinates, their dimensions are
3196             used for the broadcasting. Missing values are skipped.
3197         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3198             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3199             String indicating which method to use for interpolation:
3200 
3201             - 'linear': linear interpolation. Additional keyword
3202               arguments are passed to :py:func:`numpy.interp`
3203             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3204               are passed to :py:func:`scipy.interpolate.interp1d`. If
3205               ``method='polynomial'``, the ``order`` keyword argument must also be
3206               provided.
3207             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3208               respective :py:class:`scipy.interpolate` classes.
3209 
3210         assume_sorted : bool, default: False
3211             If False, values of coordinates that are interpolated over can be
3212             in any order and they are sorted first. If True, interpolated
3213             coordinates are assumed to be an array of monotonically increasing
3214             values.
3215         kwargs : dict, optional
3216             Additional keyword arguments passed to scipy's interpolator. Valid
3217             options and their behavior depend whether ``interp1d`` or
3218             ``interpn`` is used.
3219         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3220             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3221             ``"nearest"`` is used by default.
3222         **coords_kwargs : {dim: coordinate, ...}, optional
3223             The keyword arguments form of ``coords``.
3224             One of coords or coords_kwargs must be provided.
3225 
3226         Returns
3227         -------
3228         interpolated : Dataset
3229             New dataset on the new coordinates.
3230 
3231         Notes
3232         -----
3233         scipy is required.
3234 
3235         See Also
3236         --------
3237         scipy.interpolate.interp1d
3238         scipy.interpolate.interpn
3239 
3240         Examples
3241         --------
3242         >>> ds = xr.Dataset(
3243         ...     data_vars={
3244         ...         "a": ("x", [5, 7, 4]),
3245         ...         "b": (
3246         ...             ("x", "y"),
3247         ...             [[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
3248         ...         ),
3249         ...     },
3250         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
3251         ... )
3252         >>> ds
3253         <xarray.Dataset>
3254         Dimensions:  (x: 3, y: 4)
3255         Coordinates:
3256           * x        (x) int64 0 1 2
3257           * y        (y) int64 10 12 14 16
3258         Data variables:
3259             a        (x) int64 5 7 4
3260             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 6.0 nan 6.0 nan 5.0 8.0
3261 
3262         1D interpolation with the default method (linear):
3263 
3264         >>> ds.interp(x=[0, 0.75, 1.25, 1.75])
3265         <xarray.Dataset>
3266         Dimensions:  (x: 4, y: 4)
3267         Coordinates:
3268           * y        (y) int64 10 12 14 16
3269           * x        (x) float64 0.0 0.75 1.25 1.75
3270         Data variables:
3271             a        (x) float64 5.0 6.5 6.25 4.75
3272             b        (x, y) float64 1.0 4.0 2.0 nan 1.75 6.25 ... nan 5.0 nan 5.25 nan
3273 
3274         1D interpolation with a different method:
3275 
3276         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
3277         <xarray.Dataset>
3278         Dimensions:  (x: 4, y: 4)
3279         Coordinates:
3280           * y        (y) int64 10 12 14 16
3281           * x        (x) float64 0.0 0.75 1.25 1.75
3282         Data variables:
3283             a        (x) float64 5.0 7.0 7.0 4.0
3284             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 ... 6.0 nan 6.0 nan 5.0 8.0
3285 
3286         1D extrapolation:
3287 
3288         >>> ds.interp(
3289         ...     x=[1, 1.5, 2.5, 3.5],
3290         ...     method="linear",
3291         ...     kwargs={"fill_value": "extrapolate"},
3292         ... )
3293         <xarray.Dataset>
3294         Dimensions:  (x: 4, y: 4)
3295         Coordinates:
3296           * y        (y) int64 10 12 14 16
3297           * x        (x) float64 1.0 1.5 2.5 3.5
3298         Data variables:
3299             a        (x) float64 7.0 5.5 2.5 -0.5
3300             b        (x, y) float64 2.0 7.0 6.0 nan 4.0 nan ... 4.5 nan 12.0 nan 3.5 nan
3301 
3302         2D interpolation:
3303 
3304         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
3305         <xarray.Dataset>
3306         Dimensions:  (x: 4, y: 3)
3307         Coordinates:
3308           * x        (x) float64 0.0 0.75 1.25 1.75
3309           * y        (y) int64 11 13 15
3310         Data variables:
3311             a        (x) float64 5.0 6.5 6.25 4.75
3312             b        (x, y) float64 2.5 3.0 nan 4.0 5.625 nan nan nan nan nan nan nan
3313         """
3314         from xarray.core import missing
3315 
3316         if kwargs is None:
3317             kwargs = {}
3318 
3319         coords = either_dict_or_kwargs(coords, coords_kwargs, "interp")
3320         indexers = dict(self._validate_interp_indexers(coords))
3321 
3322         if coords:
3323             # This avoids broadcasting over coordinates that are both in
3324             # the original array AND in the indexing array. It essentially
3325             # forces interpolation along the shared coordinates.
3326             sdims = (
3327                 set(self.dims)
3328                 .intersection(*[set(nx.dims) for nx in indexers.values()])
3329                 .difference(coords.keys())
3330             )
3331             indexers.update({d: self.variables[d] for d in sdims})
3332 
3333         obj = self if assume_sorted else self.sortby([k for k in coords])
3334 
3335         def maybe_variable(obj, k):
3336             # workaround to get variable for dimension without coordinate.
3337             try:
3338                 return obj._variables[k]
3339             except KeyError:
3340                 return as_variable((k, range(obj.dims[k])))
3341 
3342         def _validate_interp_indexer(x, new_x):
3343             # In the case of datetimes, the restrictions placed on indexers
3344             # used with interp are stronger than those which are placed on
3345             # isel, so we need an additional check after _validate_indexers.
3346             if _contains_datetime_like_objects(
3347                 x
3348             ) and not _contains_datetime_like_objects(new_x):
3349                 raise TypeError(
3350                     "When interpolating over a datetime-like "
3351                     "coordinate, the coordinates to "
3352                     "interpolate to must be either datetime "
3353                     "strings or datetimes. "
3354                     "Instead got\n{}".format(new_x)
3355                 )
3356             return x, new_x
3357 
3358         validated_indexers = {
3359             k: _validate_interp_indexer(maybe_variable(obj, k), v)
3360             for k, v in indexers.items()
3361         }
3362 
3363         # optimization: subset to coordinate range of the target index
3364         if method in ["linear", "nearest"]:
3365             for k, v in validated_indexers.items():
3366                 obj, newidx = missing._localize(obj, {k: v})
3367                 validated_indexers[k] = newidx[k]
3368 
3369         # optimization: create dask coordinate arrays once per Dataset
3370         # rather than once per Variable when dask.array.unify_chunks is called later
3371         # GH4739
3372         if obj.__dask_graph__():
3373             dask_indexers = {
3374                 k: (index.to_base_variable().chunk(), dest.to_base_variable().chunk())
3375                 for k, (index, dest) in validated_indexers.items()
3376             }
3377 
3378         variables: dict[Hashable, Variable] = {}
3379         reindex: bool = False
3380         for name, var in obj._variables.items():
3381             if name in indexers:
3382                 continue
3383 
3384             if is_duck_dask_array(var.data):
3385                 use_indexers = dask_indexers
3386             else:
3387                 use_indexers = validated_indexers
3388 
3389             dtype_kind = var.dtype.kind
3390             if dtype_kind in "uifc":
3391                 # For normal number types do the interpolation:
3392                 var_indexers = {k: v for k, v in use_indexers.items() if k in var.dims}
3393                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)
3394             elif dtype_kind in "ObU" and (use_indexers.keys() & var.dims):
3395                 # For types that we do not understand do stepwise
3396                 # interpolation to avoid modifying the elements.
3397                 # reindex the variable instead because it supports
3398                 # booleans and objects and retains the dtype but inside
3399                 # this loop there might be some duplicate code that slows it
3400                 # down, therefore collect these signals and run it later:
3401                 reindex = True
3402             elif all(d not in indexers for d in var.dims):
3403                 # For anything else we can only keep variables if they
3404                 # are not dependent on any coords that are being
3405                 # interpolated along:
3406                 variables[name] = var
3407 
3408         if reindex:
3409             reindex_indexers = {
3410                 k: v for k, (_, v) in validated_indexers.items() if v.dims == (k,)
3411             }
3412             reindexed = alignment.reindex(
3413                 obj,
3414                 indexers=reindex_indexers,
3415                 method=method_non_numeric,
3416                 exclude_vars=variables.keys(),
3417             )
3418             indexes = dict(reindexed._indexes)
3419             variables.update(reindexed.variables)
3420         else:
3421             # Get the indexes that are not being interpolated along
3422             indexes = {k: v for k, v in obj._indexes.items() if k not in indexers}
3423 
3424         # Get the coords that also exist in the variables:
3425         coord_names = obj._coord_names & variables.keys()
3426         selected = self._replace_with_new_dims(
3427             variables.copy(), coord_names, indexes=indexes
3428         )
3429 
3430         # Attach indexer as coordinate
3431         for k, v in indexers.items():
3432             assert isinstance(v, Variable)
3433             if v.dims == (k,):
3434                 index = PandasIndex(v, k, coord_dtype=v.dtype)
3435                 index_vars = index.create_variables({k: v})
3436                 indexes[k] = index
3437                 variables.update(index_vars)
3438             else:
3439                 variables[k] = v
3440 
3441         # Extract coordinates from indexers
3442         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)
3443         variables.update(coord_vars)
3444         indexes.update(new_indexes)
3445 
3446         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()
3447         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3448 
3449     def interp_like(
3450         self,
3451         other: Dataset | DataArray,
3452         method: InterpOptions = "linear",
3453         assume_sorted: bool = False,
3454         kwargs: Mapping[str, Any] | None = None,
3455         method_non_numeric: str = "nearest",
3456     ) -> Dataset:
3457         """Interpolate this object onto the coordinates of another object,
3458         filling the out of range values with NaN.
3459 
3460         If interpolating along a single existing dimension,
3461         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
3462         along multiple existing dimensions, an attempt is made to decompose the
3463         interpolation into multiple 1-dimensional interpolations. If this is
3464         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3465         :py:func:`scipy.interpolate.interpn` is called.
3466 
3467         Parameters
3468         ----------
3469         other : Dataset or DataArray
3470             Object with an 'indexes' attribute giving a mapping from dimension
3471             names to an 1d array-like, which provides coordinates upon
3472             which to index the variables in this dataset. Missing values are skipped.
3473         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3474             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3475             String indicating which method to use for interpolation:
3476 
3477             - 'linear': linear interpolation. Additional keyword
3478               arguments are passed to :py:func:`numpy.interp`
3479             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3480               are passed to :py:func:`scipy.interpolate.interp1d`. If
3481               ``method='polynomial'``, the ``order`` keyword argument must also be
3482               provided.
3483             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3484               respective :py:class:`scipy.interpolate` classes.
3485 
3486         assume_sorted : bool, default: False
3487             If False, values of coordinates that are interpolated over can be
3488             in any order and they are sorted first. If True, interpolated
3489             coordinates are assumed to be an array of monotonically increasing
3490             values.
3491         kwargs : dict, optional
3492             Additional keyword passed to scipy's interpolator.
3493         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3494             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3495             ``"nearest"`` is used by default.
3496 
3497         Returns
3498         -------
3499         interpolated : Dataset
3500             Another dataset by interpolating this dataset's data along the
3501             coordinates of the other object.
3502 
3503         Notes
3504         -----
3505         scipy is required.
3506         If the dataset has object-type coordinates, reindex is used for these
3507         coordinates instead of the interpolation.
3508 
3509         See Also
3510         --------
3511         Dataset.interp
3512         Dataset.reindex_like
3513         """
3514         if kwargs is None:
3515             kwargs = {}
3516 
3517         # pick only dimension coordinates with a single index
3518         coords = {}
3519         other_indexes = other.xindexes
3520         for dim in self.dims:
3521             other_dim_coords = other_indexes.get_all_coords(dim, errors="ignore")
3522             if len(other_dim_coords) == 1:
3523                 coords[dim] = other_dim_coords[dim]
3524 
3525         numeric_coords: dict[Hashable, pd.Index] = {}
3526         object_coords: dict[Hashable, pd.Index] = {}
3527         for k, v in coords.items():
3528             if v.dtype.kind in "uifcMm":
3529                 numeric_coords[k] = v
3530             else:
3531                 object_coords[k] = v
3532 
3533         ds = self
3534         if object_coords:
3535             # We do not support interpolation along object coordinate.
3536             # reindex instead.
3537             ds = self.reindex(object_coords)
3538         return ds.interp(
3539             coords=numeric_coords,
3540             method=method,
3541             assume_sorted=assume_sorted,
3542             kwargs=kwargs,
3543             method_non_numeric=method_non_numeric,
3544         )
3545 
3546     # Helper methods for rename()
3547     def _rename_vars(
3548         self, name_dict, dims_dict
3549     ) -> tuple[dict[Hashable, Variable], set[Hashable]]:
3550         variables = {}
3551         coord_names = set()
3552         for k, v in self.variables.items():
3553             var = v.copy(deep=False)
3554             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3555             name = name_dict.get(k, k)
3556             if name in variables:
3557                 raise ValueError(f"the new name {name!r} conflicts")
3558             variables[name] = var
3559             if k in self._coord_names:
3560                 coord_names.add(name)
3561         return variables, coord_names
3562 
3563     def _rename_dims(self, name_dict: Mapping[Any, Hashable]) -> dict[Hashable, int]:
3564         return {name_dict.get(k, k): v for k, v in self.dims.items()}
3565 
3566     def _rename_indexes(
3567         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3568     ) -> tuple[dict[Hashable, Index], dict[Hashable, Variable]]:
3569         if not self._indexes:
3570             return {}, {}
3571 
3572         indexes = {}
3573         variables = {}
3574 
3575         for index, coord_names in self.xindexes.group_by_index():
3576             new_index = index.rename(name_dict, dims_dict)
3577             new_coord_names = [name_dict.get(k, k) for k in coord_names]
3578             indexes.update({k: new_index for k in new_coord_names})
3579             new_index_vars = new_index.create_variables(
3580                 {
3581                     new: self._variables[old]
3582                     for old, new in zip(coord_names, new_coord_names)
3583                 }
3584             )
3585             variables.update(new_index_vars)
3586 
3587         return indexes, variables
3588 
3589     def _rename_all(
3590         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3591     ) -> tuple[
3592         dict[Hashable, Variable],
3593         set[Hashable],
3594         dict[Hashable, int],
3595         dict[Hashable, Index],
3596     ]:
3597         variables, coord_names = self._rename_vars(name_dict, dims_dict)
3598         dims = self._rename_dims(dims_dict)
3599 
3600         indexes, index_vars = self._rename_indexes(name_dict, dims_dict)
3601         variables = {k: index_vars.get(k, v) for k, v in variables.items()}
3602 
3603         return variables, coord_names, dims, indexes
3604 
3605     def _rename(
3606         self: T_Dataset,
3607         name_dict: Mapping[Any, Hashable] | None = None,
3608         **names: Hashable,
3609     ) -> T_Dataset:
3610         """Also used internally by DataArray so that the warning (if any)
3611         is raised at the right stack level.
3612         """
3613         name_dict = either_dict_or_kwargs(name_dict, names, "rename")
3614         for k in name_dict.keys():
3615             if k not in self and k not in self.dims:
3616                 raise ValueError(
3617                     f"cannot rename {k!r} because it is not a "
3618                     "variable or dimension in this dataset"
3619                 )
3620 
3621             create_dim_coord = False
3622             new_k = name_dict[k]
3623 
3624             if k in self.dims and new_k in self._coord_names:
3625                 coord_dims = self._variables[name_dict[k]].dims
3626                 if coord_dims == (k,):
3627                     create_dim_coord = True
3628             elif k in self._coord_names and new_k in self.dims:
3629                 coord_dims = self._variables[k].dims
3630                 if coord_dims == (new_k,):
3631                     create_dim_coord = True
3632 
3633             if create_dim_coord:
3634                 warnings.warn(
3635                     f"rename {k!r} to {name_dict[k]!r} does not create an index "
3636                     "anymore. Try using swap_dims instead or use set_index "
3637                     "after rename to create an indexed coordinate.",
3638                     UserWarning,
3639                     stacklevel=3,
3640                 )
3641 
3642         variables, coord_names, dims, indexes = self._rename_all(
3643             name_dict=name_dict, dims_dict=name_dict
3644         )
3645         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3646 
3647     def rename(
3648         self: T_Dataset,
3649         name_dict: Mapping[Any, Hashable] | None = None,
3650         **names: Hashable,
3651     ) -> T_Dataset:
3652         """Returns a new object with renamed variables, coordinates and dimensions.
3653 
3654         Parameters
3655         ----------
3656         name_dict : dict-like, optional
3657             Dictionary whose keys are current variable, coordinate or dimension names and
3658             whose values are the desired names.
3659         **names : optional
3660             Keyword form of ``name_dict``.
3661             One of name_dict or names must be provided.
3662 
3663         Returns
3664         -------
3665         renamed : Dataset
3666             Dataset with renamed variables, coordinates and dimensions.
3667 
3668         See Also
3669         --------
3670         Dataset.swap_dims
3671         Dataset.rename_vars
3672         Dataset.rename_dims
3673         DataArray.rename
3674         """
3675         return self._rename(name_dict=name_dict, **names)
3676 
3677     def rename_dims(
3678         self: T_Dataset,
3679         dims_dict: Mapping[Any, Hashable] | None = None,
3680         **dims: Hashable,
3681     ) -> T_Dataset:
3682         """Returns a new object with renamed dimensions only.
3683 
3684         Parameters
3685         ----------
3686         dims_dict : dict-like, optional
3687             Dictionary whose keys are current dimension names and
3688             whose values are the desired names. The desired names must
3689             not be the name of an existing dimension or Variable in the Dataset.
3690         **dims : optional
3691             Keyword form of ``dims_dict``.
3692             One of dims_dict or dims must be provided.
3693 
3694         Returns
3695         -------
3696         renamed : Dataset
3697             Dataset with renamed dimensions.
3698 
3699         See Also
3700         --------
3701         Dataset.swap_dims
3702         Dataset.rename
3703         Dataset.rename_vars
3704         DataArray.rename
3705         """
3706         dims_dict = either_dict_or_kwargs(dims_dict, dims, "rename_dims")
3707         for k, v in dims_dict.items():
3708             if k not in self.dims:
3709                 raise ValueError(
3710                     f"cannot rename {k!r} because it is not a "
3711                     "dimension in this dataset"
3712                 )
3713             if v in self.dims or v in self:
3714                 raise ValueError(
3715                     f"Cannot rename {k} to {v} because {v} already exists. "
3716                     "Try using swap_dims instead."
3717                 )
3718 
3719         variables, coord_names, sizes, indexes = self._rename_all(
3720             name_dict={}, dims_dict=dims_dict
3721         )
3722         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)
3723 
3724     def rename_vars(
3725         self: T_Dataset,
3726         name_dict: Mapping[Any, Hashable] | None = None,
3727         **names: Hashable,
3728     ) -> T_Dataset:
3729         """Returns a new object with renamed variables including coordinates
3730 
3731         Parameters
3732         ----------
3733         name_dict : dict-like, optional
3734             Dictionary whose keys are current variable or coordinate names and
3735             whose values are the desired names.
3736         **names : optional
3737             Keyword form of ``name_dict``.
3738             One of name_dict or names must be provided.
3739 
3740         Returns
3741         -------
3742         renamed : Dataset
3743             Dataset with renamed variables including coordinates
3744 
3745         See Also
3746         --------
3747         Dataset.swap_dims
3748         Dataset.rename
3749         Dataset.rename_dims
3750         DataArray.rename
3751         """
3752         name_dict = either_dict_or_kwargs(name_dict, names, "rename_vars")
3753         for k in name_dict:
3754             if k not in self:
3755                 raise ValueError(
3756                     f"cannot rename {k!r} because it is not a "
3757                     "variable or coordinate in this dataset"
3758                 )
3759         variables, coord_names, dims, indexes = self._rename_all(
3760             name_dict=name_dict, dims_dict={}
3761         )
3762         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3763 
3764     def swap_dims(
3765         self: T_Dataset, dims_dict: Mapping[Any, Hashable] | None = None, **dims_kwargs
3766     ) -> T_Dataset:
3767         """Returns a new object with swapped dimensions.
3768 
3769         Parameters
3770         ----------
3771         dims_dict : dict-like
3772             Dictionary whose keys are current dimension names and whose values
3773             are new names.
3774         **dims_kwargs : {existing_dim: new_dim, ...}, optional
3775             The keyword arguments form of ``dims_dict``.
3776             One of dims_dict or dims_kwargs must be provided.
3777 
3778         Returns
3779         -------
3780         swapped : Dataset
3781             Dataset with swapped dimensions.
3782 
3783         Examples
3784         --------
3785         >>> ds = xr.Dataset(
3786         ...     data_vars={"a": ("x", [5, 7]), "b": ("x", [0.1, 2.4])},
3787         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
3788         ... )
3789         >>> ds
3790         <xarray.Dataset>
3791         Dimensions:  (x: 2)
3792         Coordinates:
3793           * x        (x) <U1 'a' 'b'
3794             y        (x) int64 0 1
3795         Data variables:
3796             a        (x) int64 5 7
3797             b        (x) float64 0.1 2.4
3798 
3799         >>> ds.swap_dims({"x": "y"})
3800         <xarray.Dataset>
3801         Dimensions:  (y: 2)
3802         Coordinates:
3803             x        (y) <U1 'a' 'b'
3804           * y        (y) int64 0 1
3805         Data variables:
3806             a        (y) int64 5 7
3807             b        (y) float64 0.1 2.4
3808 
3809         >>> ds.swap_dims({"x": "z"})
3810         <xarray.Dataset>
3811         Dimensions:  (z: 2)
3812         Coordinates:
3813             x        (z) <U1 'a' 'b'
3814             y        (z) int64 0 1
3815         Dimensions without coordinates: z
3816         Data variables:
3817             a        (z) int64 5 7
3818             b        (z) float64 0.1 2.4
3819 
3820         See Also
3821         --------
3822         Dataset.rename
3823         DataArray.swap_dims
3824         """
3825         # TODO: deprecate this method in favor of a (less confusing)
3826         # rename_dims() method that only renames dimensions.
3827 
3828         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
3829         for k, v in dims_dict.items():
3830             if k not in self.dims:
3831                 raise ValueError(
3832                     f"cannot swap from dimension {k!r} because it is "
3833                     "not an existing dimension"
3834                 )
3835             if v in self.variables and self.variables[v].dims != (k,):
3836                 raise ValueError(
3837                     f"replacement dimension {v!r} is not a 1D "
3838                     f"variable along the old dimension {k!r}"
3839                 )
3840 
3841         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}
3842 
3843         coord_names = self._coord_names.copy()
3844         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})
3845 
3846         variables: dict[Hashable, Variable] = {}
3847         indexes: dict[Hashable, Index] = {}
3848         for k, v in self.variables.items():
3849             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3850             var: Variable
3851             if k in result_dims:
3852                 var = v.to_index_variable()
3853                 var.dims = dims
3854                 if k in self._indexes:
3855                     indexes[k] = self._indexes[k]
3856                     variables[k] = var
3857                 else:
3858                     index, index_vars = create_default_index_implicit(var)
3859                     indexes.update({name: index for name in index_vars})
3860                     variables.update(index_vars)
3861                     coord_names.update(index_vars)
3862             else:
3863                 var = v.to_base_variable()
3864                 var.dims = dims
3865                 variables[k] = var
3866 
3867         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3868 
3869     # change type of self and return to T_Dataset once
3870     # https://github.com/python/mypy/issues/12846 is resolved
3871     def expand_dims(
3872         self,
3873         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
3874         axis: None | int | Sequence[int] = None,
3875         **dim_kwargs: Any,
3876     ) -> Dataset:
3877         """Return a new object with an additional axis (or axes) inserted at
3878         the corresponding position in the array shape.  The new object is a
3879         view into the underlying array, not a copy.
3880 
3881         If dim is already a scalar coordinate, it will be promoted to a 1D
3882         coordinate consisting of a single value.
3883 
3884         Parameters
3885         ----------
3886         dim : hashable, sequence of hashable, mapping, or None
3887             Dimensions to include on the new variable. If provided as hashable
3888             or sequence of hashable, then dimensions are inserted with length
3889             1. If provided as a mapping, then the keys are the new dimensions
3890             and the values are either integers (giving the length of the new
3891             dimensions) or array-like (giving the coordinates of the new
3892             dimensions).
3893         axis : int, sequence of int, or None, default: None
3894             Axis position(s) where new axis is to be inserted (position(s) on
3895             the result array). If a sequence of integers is passed,
3896             multiple axes are inserted. In this case, dim arguments should be
3897             same length list. If axis=None is passed, all the axes will be
3898             inserted to the start of the result array.
3899         **dim_kwargs : int or sequence or ndarray
3900             The keywords are arbitrary dimensions being inserted and the values
3901             are either the lengths of the new dims (if int is given), or their
3902             coordinates. Note, this is an alternative to passing a dict to the
3903             dim kwarg and will only be used if dim is None.
3904 
3905         Returns
3906         -------
3907         expanded : Dataset
3908             This object, but with additional dimension(s).
3909 
3910         See Also
3911         --------
3912         DataArray.expand_dims
3913         """
3914         if dim is None:
3915             pass
3916         elif isinstance(dim, Mapping):
3917             # We're later going to modify dim in place; don't tamper with
3918             # the input
3919             dim = dict(dim)
3920         elif isinstance(dim, int):
3921             raise TypeError(
3922                 "dim should be hashable or sequence of hashables or mapping"
3923             )
3924         elif isinstance(dim, str) or not isinstance(dim, Sequence):
3925             dim = {dim: 1}
3926         elif isinstance(dim, Sequence):
3927             if len(dim) != len(set(dim)):
3928                 raise ValueError("dims should not contain duplicate values.")
3929             dim = {d: 1 for d in dim}
3930 
3931         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
3932         assert isinstance(dim, MutableMapping)
3933 
3934         if axis is None:
3935             axis = list(range(len(dim)))
3936         elif not isinstance(axis, Sequence):
3937             axis = [axis]
3938 
3939         if len(dim) != len(axis):
3940             raise ValueError("lengths of dim and axis should be identical.")
3941         for d in dim:
3942             if d in self.dims:
3943                 raise ValueError(f"Dimension {d} already exists.")
3944             if d in self._variables and not utils.is_scalar(self._variables[d]):
3945                 raise ValueError(
3946                     "{dim} already exists as coordinate or"
3947                     " variable name.".format(dim=d)
3948                 )
3949 
3950         variables: dict[Hashable, Variable] = {}
3951         indexes: dict[Hashable, Index] = dict(self._indexes)
3952         coord_names = self._coord_names.copy()
3953         # If dim is a dict, then ensure that the values are either integers
3954         # or iterables.
3955         for k, v in dim.items():
3956             if hasattr(v, "__iter__"):
3957                 # If the value for the new dimension is an iterable, then
3958                 # save the coordinates to the variables dict, and set the
3959                 # value within the dim dict to the length of the iterable
3960                 # for later use.
3961                 index = PandasIndex(v, k)
3962                 indexes[k] = index
3963                 variables.update(index.create_variables())
3964                 coord_names.add(k)
3965                 dim[k] = variables[k].size
3966             elif isinstance(v, int):
3967                 pass  # Do nothing if the dimensions value is just an int
3968             else:
3969                 raise TypeError(
3970                     "The value of new dimension {k} must be "
3971                     "an iterable or an int".format(k=k)
3972                 )
3973 
3974         for k, v in self._variables.items():
3975             if k not in dim:
3976                 if k in coord_names:  # Do not change coordinates
3977                     variables[k] = v
3978                 else:
3979                     result_ndim = len(v.dims) + len(axis)
3980                     for a in axis:
3981                         if a < -result_ndim or result_ndim - 1 < a:
3982                             raise IndexError(
3983                                 f"Axis {a} of variable {k} is out of bounds of the "
3984                                 f"expanded dimension size {result_ndim}"
3985                             )
3986 
3987                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]
3988                     if len(axis_pos) != len(set(axis_pos)):
3989                         raise ValueError("axis should not contain duplicate values")
3990                     # We need to sort them to make sure `axis` equals to the
3991                     # axis positions of the result array.
3992                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))
3993 
3994                     all_dims = list(zip(v.dims, v.shape))
3995                     for d, c in zip_axis_dim:
3996                         all_dims.insert(d, c)
3997                     variables[k] = v.set_dims(dict(all_dims))
3998             else:
3999                 if k not in variables:
4000                     # If dims includes a label of a non-dimension coordinate,
4001                     # it will be promoted to a 1D coordinate with a single value.
4002                     index, index_vars = create_default_index_implicit(v.set_dims(k))
4003                     indexes[k] = index
4004                     variables.update(index_vars)
4005 
4006         return self._replace_with_new_dims(
4007             variables, coord_names=coord_names, indexes=indexes
4008         )
4009 
4010     # change type of self and return to T_Dataset once
4011     # https://github.com/python/mypy/issues/12846 is resolved
4012     def set_index(
4013         self,
4014         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,
4015         append: bool = False,
4016         **indexes_kwargs: Hashable | Sequence[Hashable],
4017     ) -> Dataset:
4018         """Set Dataset (multi-)indexes using one or more existing coordinates
4019         or variables.
4020 
4021         This legacy method is limited to pandas (multi-)indexes and
4022         1-dimensional "dimension" coordinates. See
4023         :py:meth:`~Dataset.set_xindex` for setting a pandas or a custom
4024         Xarray-compatible index from one or more arbitrary coordinates.
4025 
4026         Parameters
4027         ----------
4028         indexes : {dim: index, ...}
4029             Mapping from names matching dimensions and values given
4030             by (lists of) the names of existing coordinates or variables to set
4031             as new (multi-)index.
4032         append : bool, default: False
4033             If True, append the supplied index(es) to the existing index(es).
4034             Otherwise replace the existing index(es) (default).
4035         **indexes_kwargs : optional
4036             The keyword arguments form of ``indexes``.
4037             One of indexes or indexes_kwargs must be provided.
4038 
4039         Returns
4040         -------
4041         obj : Dataset
4042             Another dataset, with this dataset's data but replaced coordinates.
4043 
4044         Examples
4045         --------
4046         >>> arr = xr.DataArray(
4047         ...     data=np.ones((2, 3)),
4048         ...     dims=["x", "y"],
4049         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
4050         ... )
4051         >>> ds = xr.Dataset({"v": arr})
4052         >>> ds
4053         <xarray.Dataset>
4054         Dimensions:  (x: 2, y: 3)
4055         Coordinates:
4056           * x        (x) int64 0 1
4057           * y        (y) int64 0 1 2
4058             a        (x) int64 3 4
4059         Data variables:
4060             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
4061         >>> ds.set_index(x="a")
4062         <xarray.Dataset>
4063         Dimensions:  (x: 2, y: 3)
4064         Coordinates:
4065           * x        (x) int64 3 4
4066           * y        (y) int64 0 1 2
4067         Data variables:
4068             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
4069 
4070         See Also
4071         --------
4072         Dataset.reset_index
4073         Dataset.set_xindex
4074         Dataset.swap_dims
4075         """
4076         dim_coords = either_dict_or_kwargs(indexes, indexes_kwargs, "set_index")
4077 
4078         new_indexes: dict[Hashable, Index] = {}
4079         new_variables: dict[Hashable, Variable] = {}
4080         drop_indexes: set[Hashable] = set()
4081         drop_variables: set[Hashable] = set()
4082         replace_dims: dict[Hashable, Hashable] = {}
4083         all_var_names: set[Hashable] = set()
4084 
4085         for dim, _var_names in dim_coords.items():
4086             if isinstance(_var_names, str) or not isinstance(_var_names, Sequence):
4087                 var_names = [_var_names]
4088             else:
4089                 var_names = list(_var_names)
4090 
4091             invalid_vars = set(var_names) - set(self._variables)
4092             if invalid_vars:
4093                 raise ValueError(
4094                     ", ".join([str(v) for v in invalid_vars])
4095                     + " variable(s) do not exist"
4096                 )
4097 
4098             all_var_names.update(var_names)
4099             drop_variables.update(var_names)
4100 
4101             # drop any pre-existing index involved and its corresponding coordinates
4102             index_coord_names = self.xindexes.get_all_coords(dim, errors="ignore")
4103             all_index_coord_names = set(index_coord_names)
4104             for k in var_names:
4105                 all_index_coord_names.update(
4106                     self.xindexes.get_all_coords(k, errors="ignore")
4107                 )
4108 
4109             drop_indexes.update(all_index_coord_names)
4110             drop_variables.update(all_index_coord_names)
4111 
4112             if len(var_names) == 1 and (not append or dim not in self._indexes):
4113                 var_name = var_names[0]
4114                 var = self._variables[var_name]
4115                 if var.dims != (dim,):
4116                     raise ValueError(
4117                         f"dimension mismatch: try setting an index for dimension {dim!r} with "
4118                         f"variable {var_name!r} that has dimensions {var.dims}"
4119                     )
4120                 idx = PandasIndex.from_variables({dim: var}, options={})
4121                 idx_vars = idx.create_variables({var_name: var})
4122 
4123                 # trick to preserve coordinate order in this case
4124                 if dim in self._coord_names:
4125                     drop_variables.remove(dim)
4126             else:
4127                 if append:
4128                     current_variables = {
4129                         k: self._variables[k] for k in index_coord_names
4130                     }
4131                 else:
4132                     current_variables = {}
4133                 idx, idx_vars = PandasMultiIndex.from_variables_maybe_expand(
4134                     dim,
4135                     current_variables,
4136                     {k: self._variables[k] for k in var_names},
4137                 )
4138                 for n in idx.index.names:
4139                     replace_dims[n] = dim
4140 
4141             new_indexes.update({k: idx for k in idx_vars})
4142             new_variables.update(idx_vars)
4143 
4144         # re-add deindexed coordinates (convert to base variables)
4145         for k in drop_variables:
4146             if (
4147                 k not in new_variables
4148                 and k not in all_var_names
4149                 and k in self._coord_names
4150             ):
4151                 new_variables[k] = self._variables[k].to_base_variable()
4152 
4153         indexes_: dict[Any, Index] = {
4154             k: v for k, v in self._indexes.items() if k not in drop_indexes
4155         }
4156         indexes_.update(new_indexes)
4157 
4158         variables = {
4159             k: v for k, v in self._variables.items() if k not in drop_variables
4160         }
4161         variables.update(new_variables)
4162 
4163         # update dimensions if necessary, GH: 3512
4164         for k, v in variables.items():
4165             if any(d in replace_dims for d in v.dims):
4166                 new_dims = [replace_dims.get(d, d) for d in v.dims]
4167                 variables[k] = v._replace(dims=new_dims)
4168 
4169         coord_names = self._coord_names - drop_variables | set(new_variables)
4170 
4171         return self._replace_with_new_dims(
4172             variables, coord_names=coord_names, indexes=indexes_
4173         )
4174 
4175     def reset_index(
4176         self: T_Dataset,
4177         dims_or_levels: Hashable | Sequence[Hashable],
4178         drop: bool = False,
4179     ) -> T_Dataset:
4180         """Reset the specified index(es) or multi-index level(s).
4181 
4182         This legacy method is specific to pandas (multi-)indexes and
4183         1-dimensional "dimension" coordinates. See the more generic
4184         :py:meth:`~Dataset.drop_indexes` and :py:meth:`~Dataset.set_xindex`
4185         method to respectively drop and set pandas or custom indexes for
4186         arbitrary coordinates.
4187 
4188         Parameters
4189         ----------
4190         dims_or_levels : Hashable or Sequence of Hashable
4191             Name(s) of the dimension(s) and/or multi-index level(s) that will
4192             be reset.
4193         drop : bool, default: False
4194             If True, remove the specified indexes and/or multi-index levels
4195             instead of extracting them as new coordinates (default: False).
4196 
4197         Returns
4198         -------
4199         obj : Dataset
4200             Another dataset, with this dataset's data but replaced coordinates.
4201 
4202         See Also
4203         --------
4204         Dataset.set_index
4205         Dataset.set_xindex
4206         Dataset.drop_indexes
4207         """
4208         if isinstance(dims_or_levels, str) or not isinstance(dims_or_levels, Sequence):
4209             dims_or_levels = [dims_or_levels]
4210 
4211         invalid_coords = set(dims_or_levels) - set(self._indexes)
4212         if invalid_coords:
4213             raise ValueError(
4214                 f"{tuple(invalid_coords)} are not coordinates with an index"
4215             )
4216 
4217         drop_indexes: set[Hashable] = set()
4218         drop_variables: set[Hashable] = set()
4219         seen: set[Index] = set()
4220         new_indexes: dict[Hashable, Index] = {}
4221         new_variables: dict[Hashable, Variable] = {}
4222 
4223         def drop_or_convert(var_names):
4224             if drop:
4225                 drop_variables.update(var_names)
4226             else:
4227                 base_vars = {
4228                     k: self._variables[k].to_base_variable() for k in var_names
4229                 }
4230                 new_variables.update(base_vars)
4231 
4232         for name in dims_or_levels:
4233             index = self._indexes[name]
4234 
4235             if index in seen:
4236                 continue
4237             seen.add(index)
4238 
4239             idx_var_names = set(self.xindexes.get_all_coords(name))
4240             drop_indexes.update(idx_var_names)
4241 
4242             if isinstance(index, PandasMultiIndex):
4243                 # special case for pd.MultiIndex
4244                 level_names = index.index.names
4245                 keep_level_vars = {
4246                     k: self._variables[k]
4247                     for k in level_names
4248                     if k not in dims_or_levels
4249                 }
4250 
4251                 if index.dim not in dims_or_levels and keep_level_vars:
4252                     # do not drop the multi-index completely
4253                     # instead replace it by a new (multi-)index with dropped level(s)
4254                     idx = index.keep_levels(keep_level_vars)
4255                     idx_vars = idx.create_variables(keep_level_vars)
4256                     new_indexes.update({k: idx for k in idx_vars})
4257                     new_variables.update(idx_vars)
4258                     if not isinstance(idx, PandasMultiIndex):
4259                         # multi-index reduced to single index
4260                         # backward compatibility: unique level coordinate renamed to dimension
4261                         drop_variables.update(keep_level_vars)
4262                     drop_or_convert(
4263                         [k for k in level_names if k not in keep_level_vars]
4264                     )
4265                 else:
4266                     # always drop the multi-index dimension variable
4267                     drop_variables.add(index.dim)
4268                     drop_or_convert(level_names)
4269             else:
4270                 drop_or_convert(idx_var_names)
4271 
4272         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4273         indexes.update(new_indexes)
4274 
4275         variables = {
4276             k: v for k, v in self._variables.items() if k not in drop_variables
4277         }
4278         variables.update(new_variables)
4279 
4280         coord_names = self._coord_names - drop_variables
4281 
4282         return self._replace_with_new_dims(
4283             variables, coord_names=coord_names, indexes=indexes
4284         )
4285 
4286     def set_xindex(
4287         self: T_Dataset,
4288         coord_names: str | Sequence[Hashable],
4289         index_cls: type[Index] | None = None,
4290         **options,
4291     ) -> T_Dataset:
4292         """Set a new, Xarray-compatible index from one or more existing
4293         coordinate(s).
4294 
4295         Parameters
4296         ----------
4297         coord_names : str or list
4298             Name(s) of the coordinate(s) used to build the index.
4299             If several names are given, their order matters.
4300         index_cls : subclass of :class:`~xarray.indexes.Index`, optional
4301             The type of index to create. By default, try setting
4302             a ``PandasIndex`` if ``len(coord_names) == 1``,
4303             otherwise a ``PandasMultiIndex``.
4304         **options
4305             Options passed to the index constructor.
4306 
4307         Returns
4308         -------
4309         obj : Dataset
4310             Another dataset, with this dataset's data and with a new index.
4311 
4312         """
4313         # the Sequence check is required for mypy
4314         if is_scalar(coord_names) or not isinstance(coord_names, Sequence):
4315             coord_names = [coord_names]
4316 
4317         if index_cls is None:
4318             if len(coord_names) == 1:
4319                 index_cls = PandasIndex
4320             else:
4321                 index_cls = PandasMultiIndex
4322         else:
4323             if not issubclass(index_cls, Index):
4324                 raise TypeError(f"{index_cls} is not a subclass of xarray.Index")
4325 
4326         invalid_coords = set(coord_names) - self._coord_names
4327 
4328         if invalid_coords:
4329             msg = ["invalid coordinate(s)"]
4330             no_vars = invalid_coords - set(self._variables)
4331             data_vars = invalid_coords - no_vars
4332             if no_vars:
4333                 msg.append(f"those variables don't exist: {no_vars}")
4334             if data_vars:
4335                 msg.append(
4336                     f"those variables are data variables: {data_vars}, use `set_coords` first"
4337                 )
4338             raise ValueError("\n".join(msg))
4339 
4340         # we could be more clever here (e.g., drop-in index replacement if index
4341         # coordinates do not conflict), but let's not allow this for now
4342         indexed_coords = set(coord_names) & set(self._indexes)
4343 
4344         if indexed_coords:
4345             raise ValueError(
4346                 f"those coordinates already have an index: {indexed_coords}"
4347             )
4348 
4349         coord_vars = {name: self._variables[name] for name in coord_names}
4350 
4351         index = index_cls.from_variables(coord_vars, options=options)
4352 
4353         new_coord_vars = index.create_variables(coord_vars)
4354 
4355         # special case for setting a pandas multi-index from level coordinates
4356         # TODO: remove it once we depreciate pandas multi-index dimension (tuple
4357         # elements) coordinate
4358         if isinstance(index, PandasMultiIndex):
4359             coord_names = [index.dim] + list(coord_names)
4360 
4361         variables: dict[Hashable, Variable]
4362         indexes: dict[Hashable, Index]
4363 
4364         if len(coord_names) == 1:
4365             variables = self._variables.copy()
4366             indexes = self._indexes.copy()
4367 
4368             name = list(coord_names).pop()
4369             if name in new_coord_vars:
4370                 variables[name] = new_coord_vars[name]
4371             indexes[name] = index
4372         else:
4373             # reorder variables and indexes so that coordinates having the same
4374             # index are next to each other
4375             variables = {}
4376             for name, var in self._variables.items():
4377                 if name not in coord_names:
4378                     variables[name] = var
4379 
4380             indexes = {}
4381             for name, idx in self._indexes.items():
4382                 if name not in coord_names:
4383                     indexes[name] = idx
4384 
4385             for name in coord_names:
4386                 try:
4387                     variables[name] = new_coord_vars[name]
4388                 except KeyError:
4389                     variables[name] = self._variables[name]
4390                 indexes[name] = index
4391 
4392         return self._replace(
4393             variables=variables,
4394             coord_names=self._coord_names | set(coord_names),
4395             indexes=indexes,
4396         )
4397 
4398     def reorder_levels(
4399         self: T_Dataset,
4400         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
4401         **dim_order_kwargs: Sequence[int | Hashable],
4402     ) -> T_Dataset:
4403         """Rearrange index levels using input order.
4404 
4405         Parameters
4406         ----------
4407         dim_order : dict-like of Hashable to Sequence of int or Hashable, optional
4408             Mapping from names matching dimensions and values given
4409             by lists representing new level orders. Every given dimension
4410             must have a multi-index.
4411         **dim_order_kwargs : Sequence of int or Hashable, optional
4412             The keyword arguments form of ``dim_order``.
4413             One of dim_order or dim_order_kwargs must be provided.
4414 
4415         Returns
4416         -------
4417         obj : Dataset
4418             Another dataset, with this dataset's data but replaced
4419             coordinates.
4420         """
4421         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
4422         variables = self._variables.copy()
4423         indexes = dict(self._indexes)
4424         new_indexes: dict[Hashable, Index] = {}
4425         new_variables: dict[Hashable, IndexVariable] = {}
4426 
4427         for dim, order in dim_order.items():
4428             index = self._indexes[dim]
4429 
4430             if not isinstance(index, PandasMultiIndex):
4431                 raise ValueError(f"coordinate {dim} has no MultiIndex")
4432 
4433             level_vars = {k: self._variables[k] for k in order}
4434             idx = index.reorder_levels(level_vars)
4435             idx_vars = idx.create_variables(level_vars)
4436             new_indexes.update({k: idx for k in idx_vars})
4437             new_variables.update(idx_vars)
4438 
4439         indexes = {k: v for k, v in self._indexes.items() if k not in new_indexes}
4440         indexes.update(new_indexes)
4441 
4442         variables = {k: v for k, v in self._variables.items() if k not in new_variables}
4443         variables.update(new_variables)
4444 
4445         return self._replace(variables, indexes=indexes)
4446 
4447     def _get_stack_index(
4448         self,
4449         dim,
4450         multi=False,
4451         create_index=False,
4452     ) -> tuple[Index | None, dict[Hashable, Variable]]:
4453         """Used by stack and unstack to get one pandas (multi-)index among
4454         the indexed coordinates along dimension `dim`.
4455 
4456         If exactly one index is found, return it with its corresponding
4457         coordinate variables(s), otherwise return None and an empty dict.
4458 
4459         If `create_index=True`, create a new index if none is found or raise
4460         an error if multiple indexes are found.
4461 
4462         """
4463         stack_index: Index | None = None
4464         stack_coords: dict[Hashable, Variable] = {}
4465 
4466         for name, index in self._indexes.items():
4467             var = self._variables[name]
4468             if (
4469                 var.ndim == 1
4470                 and var.dims[0] == dim
4471                 and (
4472                     # stack: must be a single coordinate index
4473                     not multi
4474                     and not self.xindexes.is_multi(name)
4475                     # unstack: must be an index that implements .unstack
4476                     or multi
4477                     and type(index).unstack is not Index.unstack
4478                 )
4479             ):
4480                 if stack_index is not None and index is not stack_index:
4481                     # more than one index found, stop
4482                     if create_index:
4483                         raise ValueError(
4484                             f"cannot stack dimension {dim!r} with `create_index=True` "
4485                             "and with more than one index found along that dimension"
4486                         )
4487                     return None, {}
4488                 stack_index = index
4489                 stack_coords[name] = var
4490 
4491         if create_index and stack_index is None:
4492             if dim in self._variables:
4493                 var = self._variables[dim]
4494             else:
4495                 _, _, var = _get_virtual_variable(self._variables, dim, self.dims)
4496             # dummy index (only `stack_coords` will be used to construct the multi-index)
4497             stack_index = PandasIndex([0], dim)
4498             stack_coords = {dim: var}
4499 
4500         return stack_index, stack_coords
4501 
4502     def _stack_once(
4503         self: T_Dataset,
4504         dims: Sequence[Hashable | ellipsis],
4505         new_dim: Hashable,
4506         index_cls: type[Index],
4507         create_index: bool | None = True,
4508     ) -> T_Dataset:
4509         if dims == ...:
4510             raise ValueError("Please use [...] for dims, rather than just ...")
4511         if ... in dims:
4512             dims = list(infix_dims(dims, self.dims))
4513 
4514         new_variables: dict[Hashable, Variable] = {}
4515         stacked_var_names: list[Hashable] = []
4516         drop_indexes: list[Hashable] = []
4517 
4518         for name, var in self.variables.items():
4519             if any(d in var.dims for d in dims):
4520                 add_dims = [d for d in dims if d not in var.dims]
4521                 vdims = list(var.dims) + add_dims
4522                 shape = [self.dims[d] for d in vdims]
4523                 exp_var = var.set_dims(vdims, shape)
4524                 stacked_var = exp_var.stack(**{new_dim: dims})
4525                 new_variables[name] = stacked_var
4526                 stacked_var_names.append(name)
4527             else:
4528                 new_variables[name] = var.copy(deep=False)
4529 
4530         # drop indexes of stacked coordinates (if any)
4531         for name in stacked_var_names:
4532             drop_indexes += list(self.xindexes.get_all_coords(name, errors="ignore"))
4533 
4534         new_indexes = {}
4535         new_coord_names = set(self._coord_names)
4536         if create_index or create_index is None:
4537             product_vars: dict[Any, Variable] = {}
4538             for dim in dims:
4539                 idx, idx_vars = self._get_stack_index(dim, create_index=create_index)
4540                 if idx is not None:
4541                     product_vars.update(idx_vars)
4542 
4543             if len(product_vars) == len(dims):
4544                 idx = index_cls.stack(product_vars, new_dim)
4545                 new_indexes[new_dim] = idx
4546                 new_indexes.update({k: idx for k in product_vars})
4547                 idx_vars = idx.create_variables(product_vars)
4548                 # keep consistent multi-index coordinate order
4549                 for k in idx_vars:
4550                     new_variables.pop(k, None)
4551                 new_variables.update(idx_vars)
4552                 new_coord_names.update(idx_vars)
4553 
4554         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4555         indexes.update(new_indexes)
4556 
4557         return self._replace_with_new_dims(
4558             new_variables, coord_names=new_coord_names, indexes=indexes
4559         )
4560 
4561     def stack(
4562         self: T_Dataset,
4563         dimensions: Mapping[Any, Sequence[Hashable | ellipsis]] | None = None,
4564         create_index: bool | None = True,
4565         index_cls: type[Index] = PandasMultiIndex,
4566         **dimensions_kwargs: Sequence[Hashable | ellipsis],
4567     ) -> T_Dataset:
4568         """
4569         Stack any number of existing dimensions into a single new dimension.
4570 
4571         New dimensions will be added at the end, and by default the corresponding
4572         coordinate variables will be combined into a MultiIndex.
4573 
4574         Parameters
4575         ----------
4576         dimensions : mapping of hashable to sequence of hashable
4577             Mapping of the form `new_name=(dim1, dim2, ...)`. Names of new
4578             dimensions, and the existing dimensions that they replace. An
4579             ellipsis (`...`) will be replaced by all unlisted dimensions.
4580             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
4581             all dimensions.
4582         create_index : bool or None, default: True
4583 
4584             - True: create a multi-index for each of the stacked dimensions.
4585             - False: don't create any index.
4586             - None. create a multi-index only if exactly one single (1-d) coordinate
4587               index is found for every dimension to stack.
4588 
4589         index_cls: Index-class, default: PandasMultiIndex
4590             Can be used to pass a custom multi-index type (must be an Xarray index that
4591             implements `.stack()`). By default, a pandas multi-index wrapper is used.
4592         **dimensions_kwargs
4593             The keyword arguments form of ``dimensions``.
4594             One of dimensions or dimensions_kwargs must be provided.
4595 
4596         Returns
4597         -------
4598         stacked : Dataset
4599             Dataset with stacked data.
4600 
4601         See Also
4602         --------
4603         Dataset.unstack
4604         """
4605         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
4606         result = self
4607         for new_dim, dims in dimensions.items():
4608             result = result._stack_once(dims, new_dim, index_cls, create_index)
4609         return result
4610 
4611     def to_stacked_array(
4612         self,
4613         new_dim: Hashable,
4614         sample_dims: Collection[Hashable],
4615         variable_dim: Hashable = "variable",
4616         name: Hashable | None = None,
4617     ) -> DataArray:
4618         """Combine variables of differing dimensionality into a DataArray
4619         without broadcasting.
4620 
4621         This method is similar to Dataset.to_array but does not broadcast the
4622         variables.
4623 
4624         Parameters
4625         ----------
4626         new_dim : hashable
4627             Name of the new stacked coordinate
4628         sample_dims : Collection of hashables
4629             List of dimensions that **will not** be stacked. Each array in the
4630             dataset must share these dimensions. For machine learning
4631             applications, these define the dimensions over which samples are
4632             drawn.
4633         variable_dim : hashable, default: "variable"
4634             Name of the level in the stacked coordinate which corresponds to
4635             the variables.
4636         name : hashable, optional
4637             Name of the new data array.
4638 
4639         Returns
4640         -------
4641         stacked : DataArray
4642             DataArray with the specified dimensions and data variables
4643             stacked together. The stacked coordinate is named ``new_dim``
4644             and represented by a MultiIndex object with a level containing the
4645             data variable names. The name of this level is controlled using
4646             the ``variable_dim`` argument.
4647 
4648         See Also
4649         --------
4650         Dataset.to_array
4651         Dataset.stack
4652         DataArray.to_unstacked_dataset
4653 
4654         Examples
4655         --------
4656         >>> data = xr.Dataset(
4657         ...     data_vars={
4658         ...         "a": (("x", "y"), [[0, 1, 2], [3, 4, 5]]),
4659         ...         "b": ("x", [6, 7]),
4660         ...     },
4661         ...     coords={"y": ["u", "v", "w"]},
4662         ... )
4663 
4664         >>> data
4665         <xarray.Dataset>
4666         Dimensions:  (x: 2, y: 3)
4667         Coordinates:
4668           * y        (y) <U1 'u' 'v' 'w'
4669         Dimensions without coordinates: x
4670         Data variables:
4671             a        (x, y) int64 0 1 2 3 4 5
4672             b        (x) int64 6 7
4673 
4674         >>> data.to_stacked_array("z", sample_dims=["x"])
4675         <xarray.DataArray 'a' (x: 2, z: 4)>
4676         array([[0, 1, 2, 6],
4677                [3, 4, 5, 7]])
4678         Coordinates:
4679           * z         (z) object MultiIndex
4680           * variable  (z) object 'a' 'a' 'a' 'b'
4681           * y         (z) object 'u' 'v' 'w' nan
4682         Dimensions without coordinates: x
4683 
4684         """
4685         from xarray.core.concat import concat
4686 
4687         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)
4688 
4689         for variable in self:
4690             dims = self[variable].dims
4691             dims_include_sample_dims = set(sample_dims) <= set(dims)
4692             if not dims_include_sample_dims:
4693                 raise ValueError(
4694                     "All variables in the dataset must contain the "
4695                     "dimensions {}.".format(dims)
4696                 )
4697 
4698         def ensure_stackable(val):
4699             assign_coords = {variable_dim: val.name}
4700             for dim in stacking_dims:
4701                 if dim not in val.dims:
4702                     assign_coords[dim] = None
4703 
4704             expand_dims = set(stacking_dims).difference(set(val.dims))
4705             expand_dims.add(variable_dim)
4706             # must be list for .expand_dims
4707             expand_dims = list(expand_dims)
4708 
4709             return (
4710                 val.assign_coords(**assign_coords)
4711                 .expand_dims(expand_dims)
4712                 .stack({new_dim: (variable_dim,) + stacking_dims})
4713             )
4714 
4715         # concatenate the arrays
4716         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]
4717         data_array = concat(stackable_vars, dim=new_dim)
4718 
4719         if name is not None:
4720             data_array.name = name
4721 
4722         return data_array
4723 
4724     def _unstack_once(
4725         self: T_Dataset,
4726         dim: Hashable,
4727         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4728         fill_value,
4729         sparse: bool = False,
4730     ) -> T_Dataset:
4731         index, index_vars = index_and_vars
4732         variables: dict[Hashable, Variable] = {}
4733         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4734 
4735         new_indexes, clean_index = index.unstack()
4736         indexes.update(new_indexes)
4737 
4738         for name, idx in new_indexes.items():
4739             variables.update(idx.create_variables(index_vars))
4740 
4741         for name, var in self.variables.items():
4742             if name not in index_vars:
4743                 if dim in var.dims:
4744                     if isinstance(fill_value, Mapping):
4745                         fill_value_ = fill_value[name]
4746                     else:
4747                         fill_value_ = fill_value
4748 
4749                     variables[name] = var._unstack_once(
4750                         index=clean_index,
4751                         dim=dim,
4752                         fill_value=fill_value_,
4753                         sparse=sparse,
4754                     )
4755                 else:
4756                     variables[name] = var
4757 
4758         coord_names = set(self._coord_names) - {dim} | set(new_indexes)
4759 
4760         return self._replace_with_new_dims(
4761             variables, coord_names=coord_names, indexes=indexes
4762         )
4763 
4764     def _unstack_full_reindex(
4765         self: T_Dataset,
4766         dim: Hashable,
4767         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4768         fill_value,
4769         sparse: bool,
4770     ) -> T_Dataset:
4771         index, index_vars = index_and_vars
4772         variables: dict[Hashable, Variable] = {}
4773         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4774 
4775         new_indexes, clean_index = index.unstack()
4776         indexes.update(new_indexes)
4777 
4778         new_index_variables = {}
4779         for name, idx in new_indexes.items():
4780             new_index_variables.update(idx.create_variables(index_vars))
4781 
4782         new_dim_sizes = {k: v.size for k, v in new_index_variables.items()}
4783         variables.update(new_index_variables)
4784 
4785         # take a shortcut in case the MultiIndex was not modified.
4786         full_idx = pd.MultiIndex.from_product(
4787             clean_index.levels, names=clean_index.names
4788         )
4789         if clean_index.equals(full_idx):
4790             obj = self
4791         else:
4792             # TODO: we may depreciate implicit re-indexing with a pandas.MultiIndex
4793             xr_full_idx = PandasMultiIndex(full_idx, dim)
4794             indexers = Indexes(
4795                 {k: xr_full_idx for k in index_vars},
4796                 xr_full_idx.create_variables(index_vars),
4797             )
4798             obj = self._reindex(
4799                 indexers, copy=False, fill_value=fill_value, sparse=sparse
4800             )
4801 
4802         for name, var in obj.variables.items():
4803             if name not in index_vars:
4804                 if dim in var.dims:
4805                     variables[name] = var.unstack({dim: new_dim_sizes})
4806                 else:
4807                     variables[name] = var
4808 
4809         coord_names = set(self._coord_names) - {dim} | set(new_dim_sizes)
4810 
4811         return self._replace_with_new_dims(
4812             variables, coord_names=coord_names, indexes=indexes
4813         )
4814 
4815     def unstack(
4816         self: T_Dataset,
4817         dim: Dims = None,
4818         fill_value: Any = xrdtypes.NA,
4819         sparse: bool = False,
4820     ) -> T_Dataset:
4821         """
4822         Unstack existing dimensions corresponding to MultiIndexes into
4823         multiple new dimensions.
4824 
4825         New dimensions will be added at the end.
4826 
4827         Parameters
4828         ----------
4829         dim : str, Iterable of Hashable or None, optional
4830             Dimension(s) over which to unstack. By default unstacks all
4831             MultiIndexes.
4832         fill_value : scalar or dict-like, default: nan
4833             value to be filled. If a dict-like, maps variable names to
4834             fill values. If not provided or if the dict-like does not
4835             contain all variables, the dtype's NA value will be used.
4836         sparse : bool, default: False
4837             use sparse-array if True
4838 
4839         Returns
4840         -------
4841         unstacked : Dataset
4842             Dataset with unstacked data.
4843 
4844         See Also
4845         --------
4846         Dataset.stack
4847         """
4848 
4849         if dim is None:
4850             dims = list(self.dims)
4851         else:
4852             if isinstance(dim, str) or not isinstance(dim, Iterable):
4853                 dims = [dim]
4854             else:
4855                 dims = list(dim)
4856 
4857             missing_dims = [d for d in dims if d not in self.dims]
4858             if missing_dims:
4859                 raise ValueError(
4860                     f"Dataset does not contain the dimensions: {missing_dims}"
4861                 )
4862 
4863         # each specified dimension must have exactly one multi-index
4864         stacked_indexes: dict[Any, tuple[Index, dict[Hashable, Variable]]] = {}
4865         for d in dims:
4866             idx, idx_vars = self._get_stack_index(d, multi=True)
4867             if idx is not None:
4868                 stacked_indexes[d] = idx, idx_vars
4869 
4870         if dim is None:
4871             dims = list(stacked_indexes)
4872         else:
4873             non_multi_dims = set(dims) - set(stacked_indexes)
4874             if non_multi_dims:
4875                 raise ValueError(
4876                     "cannot unstack dimensions that do not "
4877                     f"have exactly one multi-index: {tuple(non_multi_dims)}"
4878                 )
4879 
4880         result = self.copy(deep=False)
4881 
4882         # we want to avoid allocating an object-dtype ndarray for a MultiIndex,
4883         # so we can't just access self.variables[v].data for every variable.
4884         # We only check the non-index variables.
4885         # https://github.com/pydata/xarray/issues/5902
4886         nonindexes = [
4887             self.variables[k] for k in set(self.variables) - set(self._indexes)
4888         ]
4889         # Notes for each of these cases:
4890         # 1. Dask arrays don't support assignment by index, which the fast unstack
4891         #    function requires.
4892         #    https://github.com/pydata/xarray/pull/4746#issuecomment-753282125
4893         # 2. Sparse doesn't currently support (though we could special-case it)
4894         #    https://github.com/pydata/sparse/issues/422
4895         # 3. pint requires checking if it's a NumPy array until
4896         #    https://github.com/pydata/xarray/pull/4751 is resolved,
4897         #    Once that is resolved, explicitly exclude pint arrays.
4898         #    pint doesn't implement `np.full_like` in a way that's
4899         #    currently compatible.
4900         sparse_array_type = array_type("sparse")
4901         needs_full_reindex = any(
4902             is_duck_dask_array(v.data)
4903             or isinstance(v.data, sparse_array_type)
4904             or not isinstance(v.data, np.ndarray)
4905             for v in nonindexes
4906         )
4907 
4908         for d in dims:
4909             if needs_full_reindex:
4910                 result = result._unstack_full_reindex(
4911                     d, stacked_indexes[d], fill_value, sparse
4912                 )
4913             else:
4914                 result = result._unstack_once(d, stacked_indexes[d], fill_value, sparse)
4915         return result
4916 
4917     def update(self: T_Dataset, other: CoercibleMapping) -> T_Dataset:
4918         """Update this dataset's variables with those from another dataset.
4919 
4920         Just like :py:meth:`dict.update` this is a in-place operation.
4921         For a non-inplace version, see :py:meth:`Dataset.merge`.
4922 
4923         Parameters
4924         ----------
4925         other : Dataset or mapping
4926             Variables with which to update this dataset. One of:
4927 
4928             - Dataset
4929             - mapping {var name: DataArray}
4930             - mapping {var name: Variable}
4931             - mapping {var name: (dimension name, array-like)}
4932             - mapping {var name: (tuple of dimension names, array-like)}
4933 
4934         Returns
4935         -------
4936         updated : Dataset
4937             Updated dataset. Note that since the update is in-place this is the input
4938             dataset.
4939 
4940             It is deprecated since version 0.17 and scheduled to be removed in 0.21.
4941 
4942         Raises
4943         ------
4944         ValueError
4945             If any dimensions would have inconsistent sizes in the updated
4946             dataset.
4947 
4948         See Also
4949         --------
4950         Dataset.assign
4951         Dataset.merge
4952         """
4953         merge_result = dataset_update_method(self, other)
4954         return self._replace(inplace=True, **merge_result._asdict())
4955 
4956     def merge(
4957         self: T_Dataset,
4958         other: CoercibleMapping | DataArray,
4959         overwrite_vars: Hashable | Iterable[Hashable] = frozenset(),
4960         compat: CompatOptions = "no_conflicts",
4961         join: JoinOptions = "outer",
4962         fill_value: Any = xrdtypes.NA,
4963         combine_attrs: CombineAttrsOptions = "override",
4964     ) -> T_Dataset:
4965         """Merge the arrays of two datasets into a single dataset.
4966 
4967         This method generally does not allow for overriding data, with the
4968         exception of attributes, which are ignored on the second dataset.
4969         Variables with the same name are checked for conflicts via the equals
4970         or identical methods.
4971 
4972         Parameters
4973         ----------
4974         other : Dataset or mapping
4975             Dataset or variables to merge with this dataset.
4976         overwrite_vars : hashable or iterable of hashable, optional
4977             If provided, update variables of these name(s) without checking for
4978             conflicts in this dataset.
4979         compat : {"identical", "equals", "broadcast_equals", \
4980                   "no_conflicts", "override", "minimal"}, default: "no_conflicts"
4981             String indicating how to compare variables of the same name for
4982             potential conflicts:
4983 
4984             - 'identical': all values, dimensions and attributes must be the
4985               same.
4986             - 'equals': all values and dimensions must be the same.
4987             - 'broadcast_equals': all values must be equal when variables are
4988               broadcast against each other to ensure common dimensions.
4989             - 'no_conflicts': only values which are not null in both datasets
4990               must be equal. The returned dataset then contains the combination
4991               of all non-null values.
4992             - 'override': skip comparing and pick variable from first dataset
4993             - 'minimal': drop conflicting coordinates
4994 
4995         join : {"outer", "inner", "left", "right", "exact", "override"}, \
4996                default: "outer"
4997             Method for joining ``self`` and ``other`` along shared dimensions:
4998 
4999             - 'outer': use the union of the indexes
5000             - 'inner': use the intersection of the indexes
5001             - 'left': use indexes from ``self``
5002             - 'right': use indexes from ``other``
5003             - 'exact': error instead of aligning non-equal indexes
5004             - 'override': use indexes from ``self`` that are the same size
5005               as those of ``other`` in that dimension
5006 
5007         fill_value : scalar or dict-like, optional
5008             Value to use for newly missing values. If a dict-like, maps
5009             variable names (including coordinates) to fill values.
5010         combine_attrs : {"drop", "identical", "no_conflicts", "drop_conflicts", \
5011                          "override"} or callable, default: "override"
5012             A callable or a string indicating how to combine attrs of the objects being
5013             merged:
5014 
5015             - "drop": empty attrs on returned Dataset.
5016             - "identical": all attrs must be the same on every object.
5017             - "no_conflicts": attrs from all objects are combined, any that have
5018               the same name must also have the same value.
5019             - "drop_conflicts": attrs from all objects are combined, any that have
5020               the same name but different values are dropped.
5021             - "override": skip comparing and copy attrs from the first dataset to
5022               the result.
5023 
5024             If a callable, it must expect a sequence of ``attrs`` dicts and a context object
5025             as its only parameters.
5026 
5027         Returns
5028         -------
5029         merged : Dataset
5030             Merged dataset.
5031 
5032         Raises
5033         ------
5034         MergeError
5035             If any variables conflict (see ``compat``).
5036 
5037         See Also
5038         --------
5039         Dataset.update
5040         """
5041         from xarray.core.dataarray import DataArray
5042 
5043         other = other.to_dataset() if isinstance(other, DataArray) else other
5044         merge_result = dataset_merge_method(
5045             self,
5046             other,
5047             overwrite_vars=overwrite_vars,
5048             compat=compat,
5049             join=join,
5050             fill_value=fill_value,
5051             combine_attrs=combine_attrs,
5052         )
5053         return self._replace(**merge_result._asdict())
5054 
5055     def _assert_all_in_dataset(
5056         self, names: Iterable[Hashable], virtual_okay: bool = False
5057     ) -> None:
5058         bad_names = set(names) - set(self._variables)
5059         if virtual_okay:
5060             bad_names -= self.virtual_variables
5061         if bad_names:
5062             ordered_bad_names = [name for name in names if name in bad_names]
5063             raise ValueError(
5064                 f"These variables cannot be found in this dataset: {ordered_bad_names}"
5065             )
5066 
5067     def drop_vars(
5068         self: T_Dataset,
5069         names: Hashable | Iterable[Hashable],
5070         *,
5071         errors: ErrorOptions = "raise",
5072     ) -> T_Dataset:
5073         """Drop variables from this dataset.
5074 
5075         Parameters
5076         ----------
5077         names : hashable or iterable of hashable
5078             Name(s) of variables to drop.
5079         errors : {"raise", "ignore"}, default: "raise"
5080             If 'raise', raises a ValueError error if any of the variable
5081             passed are not in the dataset. If 'ignore', any given names that are in the
5082             dataset are dropped and no error is raised.
5083 
5084         Returns
5085         -------
5086         dropped : Dataset
5087 
5088         """
5089         # the Iterable check is required for mypy
5090         if is_scalar(names) or not isinstance(names, Iterable):
5091             names = {names}
5092         else:
5093             names = set(names)
5094         if errors == "raise":
5095             self._assert_all_in_dataset(names)
5096 
5097         # GH6505
5098         other_names = set()
5099         for var in names:
5100             maybe_midx = self._indexes.get(var, None)
5101             if isinstance(maybe_midx, PandasMultiIndex):
5102                 idx_coord_names = set(maybe_midx.index.names + [maybe_midx.dim])
5103                 idx_other_names = idx_coord_names - set(names)
5104                 other_names.update(idx_other_names)
5105         if other_names:
5106             names |= set(other_names)
5107             warnings.warn(
5108                 f"Deleting a single level of a MultiIndex is deprecated. Previously, this deleted all levels of a MultiIndex. "
5109                 f"Please also drop the following variables: {other_names!r} to avoid an error in the future.",
5110                 DeprecationWarning,
5111                 stacklevel=2,
5112             )
5113 
5114         assert_no_index_corrupted(self.xindexes, names)
5115 
5116         variables = {k: v for k, v in self._variables.items() if k not in names}
5117         coord_names = {k for k in self._coord_names if k in variables}
5118         indexes = {k: v for k, v in self._indexes.items() if k not in names}
5119         return self._replace_with_new_dims(
5120             variables, coord_names=coord_names, indexes=indexes
5121         )
5122 
5123     def drop_indexes(
5124         self: T_Dataset,
5125         coord_names: Hashable | Iterable[Hashable],
5126         *,
5127         errors: ErrorOptions = "raise",
5128     ) -> T_Dataset:
5129         """Drop the indexes assigned to the given coordinates.
5130 
5131         Parameters
5132         ----------
5133         coord_names : hashable or iterable of hashable
5134             Name(s) of the coordinate(s) for which to drop the index.
5135         errors : {"raise", "ignore"}, default: "raise"
5136             If 'raise', raises a ValueError error if any of the coordinates
5137             passed have no index or are not in the dataset.
5138             If 'ignore', no error is raised.
5139 
5140         Returns
5141         -------
5142         dropped : Dataset
5143             A new dataset with dropped indexes.
5144 
5145         """
5146         # the Iterable check is required for mypy
5147         if is_scalar(coord_names) or not isinstance(coord_names, Iterable):
5148             coord_names = {coord_names}
5149         else:
5150             coord_names = set(coord_names)
5151 
5152         if errors == "raise":
5153             invalid_coords = coord_names - self._coord_names
5154             if invalid_coords:
5155                 raise ValueError(f"those coordinates don't exist: {invalid_coords}")
5156 
5157             unindexed_coords = set(coord_names) - set(self._indexes)
5158             if unindexed_coords:
5159                 raise ValueError(
5160                     f"those coordinates do not have an index: {unindexed_coords}"
5161                 )
5162 
5163         assert_no_index_corrupted(self.xindexes, coord_names, action="remove index(es)")
5164 
5165         variables = {}
5166         for name, var in self._variables.items():
5167             if name in coord_names:
5168                 variables[name] = var.to_base_variable()
5169             else:
5170                 variables[name] = var
5171 
5172         indexes = {k: v for k, v in self._indexes.items() if k not in coord_names}
5173 
5174         return self._replace(variables=variables, indexes=indexes)
5175 
5176     def drop(
5177         self: T_Dataset,
5178         labels=None,
5179         dim=None,
5180         *,
5181         errors: ErrorOptions = "raise",
5182         **labels_kwargs,
5183     ) -> T_Dataset:
5184         """Backward compatible method based on `drop_vars` and `drop_sel`
5185 
5186         Using either `drop_vars` or `drop_sel` is encouraged
5187 
5188         See Also
5189         --------
5190         Dataset.drop_vars
5191         Dataset.drop_sel
5192         """
5193         if errors not in ["raise", "ignore"]:
5194             raise ValueError('errors must be either "raise" or "ignore"')
5195 
5196         if is_dict_like(labels) and not isinstance(labels, dict):
5197             warnings.warn(
5198                 "dropping coordinates using `drop` is be deprecated; use drop_vars.",
5199                 FutureWarning,
5200                 stacklevel=2,
5201             )
5202             return self.drop_vars(labels, errors=errors)
5203 
5204         if labels_kwargs or isinstance(labels, dict):
5205             if dim is not None:
5206                 raise ValueError("cannot specify dim and dict-like arguments.")
5207             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
5208 
5209         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):
5210             warnings.warn(
5211                 "dropping variables using `drop` will be deprecated; using drop_vars is encouraged.",
5212                 PendingDeprecationWarning,
5213                 stacklevel=2,
5214             )
5215             return self.drop_vars(labels, errors=errors)
5216         if dim is not None:
5217             warnings.warn(
5218                 "dropping labels using list-like labels is deprecated; using "
5219                 "dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).",
5220                 DeprecationWarning,
5221                 stacklevel=2,
5222             )
5223             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)
5224 
5225         warnings.warn(
5226             "dropping labels using `drop` will be deprecated; using drop_sel is encouraged.",
5227             PendingDeprecationWarning,
5228             stacklevel=2,
5229         )
5230         return self.drop_sel(labels, errors=errors)
5231 
5232     def drop_sel(
5233         self: T_Dataset, labels=None, *, errors: ErrorOptions = "raise", **labels_kwargs
5234     ) -> T_Dataset:
5235         """Drop index labels from this dataset.
5236 
5237         Parameters
5238         ----------
5239         labels : mapping of hashable to Any
5240             Index labels to drop
5241         errors : {"raise", "ignore"}, default: "raise"
5242             If 'raise', raises a ValueError error if
5243             any of the index labels passed are not
5244             in the dataset. If 'ignore', any given labels that are in the
5245             dataset are dropped and no error is raised.
5246         **labels_kwargs : {dim: label, ...}, optional
5247             The keyword arguments form of ``dim`` and ``labels``
5248 
5249         Returns
5250         -------
5251         dropped : Dataset
5252 
5253         Examples
5254         --------
5255         >>> data = np.arange(6).reshape(2, 3)
5256         >>> labels = ["a", "b", "c"]
5257         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
5258         >>> ds
5259         <xarray.Dataset>
5260         Dimensions:  (x: 2, y: 3)
5261         Coordinates:
5262           * y        (y) <U1 'a' 'b' 'c'
5263         Dimensions without coordinates: x
5264         Data variables:
5265             A        (x, y) int64 0 1 2 3 4 5
5266         >>> ds.drop_sel(y=["a", "c"])
5267         <xarray.Dataset>
5268         Dimensions:  (x: 2, y: 1)
5269         Coordinates:
5270           * y        (y) <U1 'b'
5271         Dimensions without coordinates: x
5272         Data variables:
5273             A        (x, y) int64 1 4
5274         >>> ds.drop_sel(y="b")
5275         <xarray.Dataset>
5276         Dimensions:  (x: 2, y: 2)
5277         Coordinates:
5278           * y        (y) <U1 'a' 'c'
5279         Dimensions without coordinates: x
5280         Data variables:
5281             A        (x, y) int64 0 2 3 5
5282         """
5283         if errors not in ["raise", "ignore"]:
5284             raise ValueError('errors must be either "raise" or "ignore"')
5285 
5286         labels = either_dict_or_kwargs(labels, labels_kwargs, "drop_sel")
5287 
5288         ds = self
5289         for dim, labels_for_dim in labels.items():
5290             # Don't cast to set, as it would harm performance when labels
5291             # is a large numpy array
5292             if utils.is_scalar(labels_for_dim):
5293                 labels_for_dim = [labels_for_dim]
5294             labels_for_dim = np.asarray(labels_for_dim)
5295             try:
5296                 index = self.get_index(dim)
5297             except KeyError:
5298                 raise ValueError(f"dimension {dim!r} does not have coordinate labels")
5299             new_index = index.drop(labels_for_dim, errors=errors)
5300             ds = ds.loc[{dim: new_index}]
5301         return ds
5302 
5303     def drop_isel(self: T_Dataset, indexers=None, **indexers_kwargs) -> T_Dataset:
5304         """Drop index positions from this Dataset.
5305 
5306         Parameters
5307         ----------
5308         indexers : mapping of hashable to Any
5309             Index locations to drop
5310         **indexers_kwargs : {dim: position, ...}, optional
5311             The keyword arguments form of ``dim`` and ``positions``
5312 
5313         Returns
5314         -------
5315         dropped : Dataset
5316 
5317         Raises
5318         ------
5319         IndexError
5320 
5321         Examples
5322         --------
5323         >>> data = np.arange(6).reshape(2, 3)
5324         >>> labels = ["a", "b", "c"]
5325         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
5326         >>> ds
5327         <xarray.Dataset>
5328         Dimensions:  (x: 2, y: 3)
5329         Coordinates:
5330           * y        (y) <U1 'a' 'b' 'c'
5331         Dimensions without coordinates: x
5332         Data variables:
5333             A        (x, y) int64 0 1 2 3 4 5
5334         >>> ds.drop_isel(y=[0, 2])
5335         <xarray.Dataset>
5336         Dimensions:  (x: 2, y: 1)
5337         Coordinates:
5338           * y        (y) <U1 'b'
5339         Dimensions without coordinates: x
5340         Data variables:
5341             A        (x, y) int64 1 4
5342         >>> ds.drop_isel(y=1)
5343         <xarray.Dataset>
5344         Dimensions:  (x: 2, y: 2)
5345         Coordinates:
5346           * y        (y) <U1 'a' 'c'
5347         Dimensions without coordinates: x
5348         Data variables:
5349             A        (x, y) int64 0 2 3 5
5350         """
5351 
5352         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "drop_isel")
5353 
5354         ds = self
5355         dimension_index = {}
5356         for dim, pos_for_dim in indexers.items():
5357             # Don't cast to set, as it would harm performance when labels
5358             # is a large numpy array
5359             if utils.is_scalar(pos_for_dim):
5360                 pos_for_dim = [pos_for_dim]
5361             pos_for_dim = np.asarray(pos_for_dim)
5362             index = self.get_index(dim)
5363             new_index = index.delete(pos_for_dim)
5364             dimension_index[dim] = new_index
5365         ds = ds.loc[dimension_index]
5366         return ds
5367 
5368     def drop_dims(
5369         self: T_Dataset,
5370         drop_dims: str | Iterable[Hashable],
5371         *,
5372         errors: ErrorOptions = "raise",
5373     ) -> T_Dataset:
5374         """Drop dimensions and associated variables from this dataset.
5375 
5376         Parameters
5377         ----------
5378         drop_dims : str or Iterable of Hashable
5379             Dimension or dimensions to drop.
5380         errors : {"raise", "ignore"}, default: "raise"
5381             If 'raise', raises a ValueError error if any of the
5382             dimensions passed are not in the dataset. If 'ignore', any given
5383             dimensions that are in the dataset are dropped and no error is raised.
5384 
5385         Returns
5386         -------
5387         obj : Dataset
5388             The dataset without the given dimensions (or any variables
5389             containing those dimensions).
5390         """
5391         if errors not in ["raise", "ignore"]:
5392             raise ValueError('errors must be either "raise" or "ignore"')
5393 
5394         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):
5395             drop_dims = {drop_dims}
5396         else:
5397             drop_dims = set(drop_dims)
5398 
5399         if errors == "raise":
5400             missing_dims = drop_dims - set(self.dims)
5401             if missing_dims:
5402                 raise ValueError(
5403                     f"Dataset does not contain the dimensions: {missing_dims}"
5404                 )
5405 
5406         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}
5407         return self.drop_vars(drop_vars)
5408 
5409     def transpose(
5410         self: T_Dataset,
5411         *dims: Hashable,
5412         missing_dims: ErrorOptionsWithWarn = "raise",
5413     ) -> T_Dataset:
5414         """Return a new Dataset object with all array dimensions transposed.
5415 
5416         Although the order of dimensions on each array will change, the dataset
5417         dimensions themselves will remain in fixed (sorted) order.
5418 
5419         Parameters
5420         ----------
5421         *dims : hashable, optional
5422             By default, reverse the dimensions on each array. Otherwise,
5423             reorder the dimensions to this order.
5424         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
5425             What to do if dimensions that should be selected from are not present in the
5426             Dataset:
5427             - "raise": raise an exception
5428             - "warn": raise a warning, and ignore the missing dimensions
5429             - "ignore": ignore the missing dimensions
5430 
5431         Returns
5432         -------
5433         transposed : Dataset
5434             Each array in the dataset (including) coordinates will be
5435             transposed to the given order.
5436 
5437         Notes
5438         -----
5439         This operation returns a view of each array's data. It is
5440         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
5441         -- the data will be fully loaded into memory.
5442 
5443         See Also
5444         --------
5445         numpy.transpose
5446         DataArray.transpose
5447         """
5448         # Raise error if list is passed as dims
5449         if (len(dims) > 0) and (isinstance(dims[0], list)):
5450             list_fix = [f"{repr(x)}" if isinstance(x, str) else f"{x}" for x in dims[0]]
5451             raise TypeError(
5452                 f'transpose requires dims to be passed as multiple arguments. Expected `{", ".join(list_fix)}`. Received `{dims[0]}` instead'
5453             )
5454 
5455         # Use infix_dims to check once for missing dimensions
5456         if len(dims) != 0:
5457             _ = list(infix_dims(dims, self.dims, missing_dims))
5458 
5459         ds = self.copy()
5460         for name, var in self._variables.items():
5461             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))
5462             ds._variables[name] = var.transpose(*var_dims)
5463         return ds
5464 
5465     def dropna(
5466         self: T_Dataset,
5467         dim: Hashable,
5468         how: Literal["any", "all"] = "any",
5469         thresh: int | None = None,
5470         subset: Iterable[Hashable] | None = None,
5471     ) -> T_Dataset:
5472         """Returns a new dataset with dropped labels for missing values along
5473         the provided dimension.
5474 
5475         Parameters
5476         ----------
5477         dim : hashable
5478             Dimension along which to drop missing values. Dropping along
5479             multiple dimensions simultaneously is not yet supported.
5480         how : {"any", "all"}, default: "any"
5481             - any : if any NA values are present, drop that label
5482             - all : if all values are NA, drop that label
5483 
5484         thresh : int or None, optional
5485             If supplied, require this many non-NA values (summed over all the subset variables).
5486         subset : iterable of hashable or None, optional
5487             Which variables to check for missing values. By default, all
5488             variables in the dataset are checked.
5489 
5490         Returns
5491         -------
5492         Dataset
5493         """
5494         # TODO: consider supporting multiple dimensions? Or not, given that
5495         # there are some ugly edge cases, e.g., pandas's dropna differs
5496         # depending on the order of the supplied axes.
5497 
5498         if dim not in self.dims:
5499             raise ValueError(f"{dim} must be a single dataset dimension")
5500 
5501         if subset is None:
5502             subset = iter(self.data_vars)
5503 
5504         count = np.zeros(self.dims[dim], dtype=np.int64)
5505         size = np.int_(0)  # for type checking
5506 
5507         for k in subset:
5508             array = self._variables[k]
5509             if dim in array.dims:
5510                 dims = [d for d in array.dims if d != dim]
5511                 count += np.asarray(array.count(dims))  # type: ignore[attr-defined]
5512                 size += math.prod([self.dims[d] for d in dims])
5513 
5514         if thresh is not None:
5515             mask = count >= thresh
5516         elif how == "any":
5517             mask = count == size
5518         elif how == "all":
5519             mask = count > 0
5520         elif how is not None:
5521             raise ValueError(f"invalid how option: {how}")
5522         else:
5523             raise TypeError("must specify how or thresh")
5524 
5525         return self.isel({dim: mask})
5526 
5527     def fillna(self: T_Dataset, value: Any) -> T_Dataset:
5528         """Fill missing values in this object.
5529 
5530         This operation follows the normal broadcasting and alignment rules that
5531         xarray uses for binary arithmetic, except the result is aligned to this
5532         object (``join='left'``) instead of aligned to the intersection of
5533         index coordinates (``join='inner'``).
5534 
5535         Parameters
5536         ----------
5537         value : scalar, ndarray, DataArray, dict or Dataset
5538             Used to fill all matching missing values in this dataset's data
5539             variables. Scalars, ndarrays or DataArrays arguments are used to
5540             fill all data with aligned coordinates (for DataArrays).
5541             Dictionaries or datasets match data variables and then align
5542             coordinates if necessary.
5543 
5544         Returns
5545         -------
5546         Dataset
5547 
5548         Examples
5549         --------
5550         >>> ds = xr.Dataset(
5551         ...     {
5552         ...         "A": ("x", [np.nan, 2, np.nan, 0]),
5553         ...         "B": ("x", [3, 4, np.nan, 1]),
5554         ...         "C": ("x", [np.nan, np.nan, np.nan, 5]),
5555         ...         "D": ("x", [np.nan, 3, np.nan, 4]),
5556         ...     },
5557         ...     coords={"x": [0, 1, 2, 3]},
5558         ... )
5559         >>> ds
5560         <xarray.Dataset>
5561         Dimensions:  (x: 4)
5562         Coordinates:
5563           * x        (x) int64 0 1 2 3
5564         Data variables:
5565             A        (x) float64 nan 2.0 nan 0.0
5566             B        (x) float64 3.0 4.0 nan 1.0
5567             C        (x) float64 nan nan nan 5.0
5568             D        (x) float64 nan 3.0 nan 4.0
5569 
5570         Replace all `NaN` values with 0s.
5571 
5572         >>> ds.fillna(0)
5573         <xarray.Dataset>
5574         Dimensions:  (x: 4)
5575         Coordinates:
5576           * x        (x) int64 0 1 2 3
5577         Data variables:
5578             A        (x) float64 0.0 2.0 0.0 0.0
5579             B        (x) float64 3.0 4.0 0.0 1.0
5580             C        (x) float64 0.0 0.0 0.0 5.0
5581             D        (x) float64 0.0 3.0 0.0 4.0
5582 
5583         Replace all `NaN` elements in column ‘A’, ‘B’, ‘C’, and ‘D’, with 0, 1, 2, and 3 respectively.
5584 
5585         >>> values = {"A": 0, "B": 1, "C": 2, "D": 3}
5586         >>> ds.fillna(value=values)
5587         <xarray.Dataset>
5588         Dimensions:  (x: 4)
5589         Coordinates:
5590           * x        (x) int64 0 1 2 3
5591         Data variables:
5592             A        (x) float64 0.0 2.0 0.0 0.0
5593             B        (x) float64 3.0 4.0 1.0 1.0
5594             C        (x) float64 2.0 2.0 2.0 5.0
5595             D        (x) float64 3.0 3.0 3.0 4.0
5596         """
5597         if utils.is_dict_like(value):
5598             value_keys = getattr(value, "data_vars", value).keys()
5599             if not set(value_keys) <= set(self.data_vars.keys()):
5600                 raise ValueError(
5601                     "all variables in the argument to `fillna` "
5602                     "must be contained in the original dataset"
5603                 )
5604         out = ops.fillna(self, value)
5605         return out
5606 
5607     def interpolate_na(
5608         self: T_Dataset,
5609         dim: Hashable | None = None,
5610         method: InterpOptions = "linear",
5611         limit: int | None = None,
5612         use_coordinate: bool | Hashable = True,
5613         max_gap: (
5614             int | float | str | pd.Timedelta | np.timedelta64 | datetime.timedelta
5615         ) = None,
5616         **kwargs: Any,
5617     ) -> T_Dataset:
5618         """Fill in NaNs by interpolating according to different methods.
5619 
5620         Parameters
5621         ----------
5622         dim : Hashable or None, optional
5623             Specifies the dimension along which to interpolate.
5624         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
5625             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
5626             String indicating which method to use for interpolation:
5627 
5628             - 'linear': linear interpolation. Additional keyword
5629               arguments are passed to :py:func:`numpy.interp`
5630             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
5631               are passed to :py:func:`scipy.interpolate.interp1d`. If
5632               ``method='polynomial'``, the ``order`` keyword argument must also be
5633               provided.
5634             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
5635               respective :py:class:`scipy.interpolate` classes.
5636 
5637         use_coordinate : bool or Hashable, default: True
5638             Specifies which index to use as the x values in the interpolation
5639             formulated as `y = f(x)`. If False, values are treated as if
5640             equally-spaced along ``dim``. If True, the IndexVariable `dim` is
5641             used. If ``use_coordinate`` is a string, it specifies the name of a
5642             coordinate variable to use as the index.
5643         limit : int, default: None
5644             Maximum number of consecutive NaNs to fill. Must be greater than 0
5645             or None for no limit. This filling is done regardless of the size of
5646             the gap in the data. To only interpolate over gaps less than a given length,
5647             see ``max_gap``.
5648         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
5649             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
5650             Use None for no limit. When interpolating along a datetime64 dimension
5651             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
5652 
5653             - a string that is valid input for pandas.to_timedelta
5654             - a :py:class:`numpy.timedelta64` object
5655             - a :py:class:`pandas.Timedelta` object
5656             - a :py:class:`datetime.timedelta` object
5657 
5658             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
5659             dimensions has not been implemented yet. Gap length is defined as the difference
5660             between coordinate values at the first data point after a gap and the last value
5661             before a gap. For gaps at the beginning (end), gap length is defined as the difference
5662             between coordinate values at the first (last) valid data point and the first (last) NaN.
5663             For example, consider::
5664 
5665                 <xarray.DataArray (x: 9)>
5666                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
5667                 Coordinates:
5668                   * x        (x) int64 0 1 2 3 4 5 6 7 8
5669 
5670             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
5671         **kwargs : dict, optional
5672             parameters passed verbatim to the underlying interpolation function
5673 
5674         Returns
5675         -------
5676         interpolated: Dataset
5677             Filled in Dataset.
5678 
5679         See Also
5680         --------
5681         numpy.interp
5682         scipy.interpolate
5683 
5684         Examples
5685         --------
5686         >>> ds = xr.Dataset(
5687         ...     {
5688         ...         "A": ("x", [np.nan, 2, 3, np.nan, 0]),
5689         ...         "B": ("x", [3, 4, np.nan, 1, 7]),
5690         ...         "C": ("x", [np.nan, np.nan, np.nan, 5, 0]),
5691         ...         "D": ("x", [np.nan, 3, np.nan, -1, 4]),
5692         ...     },
5693         ...     coords={"x": [0, 1, 2, 3, 4]},
5694         ... )
5695         >>> ds
5696         <xarray.Dataset>
5697         Dimensions:  (x: 5)
5698         Coordinates:
5699           * x        (x) int64 0 1 2 3 4
5700         Data variables:
5701             A        (x) float64 nan 2.0 3.0 nan 0.0
5702             B        (x) float64 3.0 4.0 nan 1.0 7.0
5703             C        (x) float64 nan nan nan 5.0 0.0
5704             D        (x) float64 nan 3.0 nan -1.0 4.0
5705 
5706         >>> ds.interpolate_na(dim="x", method="linear")
5707         <xarray.Dataset>
5708         Dimensions:  (x: 5)
5709         Coordinates:
5710           * x        (x) int64 0 1 2 3 4
5711         Data variables:
5712             A        (x) float64 nan 2.0 3.0 1.5 0.0
5713             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5714             C        (x) float64 nan nan nan 5.0 0.0
5715             D        (x) float64 nan 3.0 1.0 -1.0 4.0
5716 
5717         >>> ds.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
5718         <xarray.Dataset>
5719         Dimensions:  (x: 5)
5720         Coordinates:
5721           * x        (x) int64 0 1 2 3 4
5722         Data variables:
5723             A        (x) float64 1.0 2.0 3.0 1.5 0.0
5724             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5725             C        (x) float64 20.0 15.0 10.0 5.0 0.0
5726             D        (x) float64 5.0 3.0 1.0 -1.0 4.0
5727         """
5728         from xarray.core.missing import _apply_over_vars_with_dim, interp_na
5729 
5730         new = _apply_over_vars_with_dim(
5731             interp_na,
5732             self,
5733             dim=dim,
5734             method=method,
5735             limit=limit,
5736             use_coordinate=use_coordinate,
5737             max_gap=max_gap,
5738             **kwargs,
5739         )
5740         return new
5741 
5742     def ffill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5743         """Fill NaN values by propagating values forward
5744 
5745         *Requires bottleneck.*
5746 
5747         Parameters
5748         ----------
5749         dim : Hashable
5750             Specifies the dimension along which to propagate values when
5751             filling.
5752         limit : int or None, optional
5753             The maximum number of consecutive NaN values to forward fill. In
5754             other words, if there is a gap with more than this number of
5755             consecutive NaNs, it will only be partially filled. Must be greater
5756             than 0 or None for no limit. Must be None or greater than or equal
5757             to axis length if filling along chunked axes (dimensions).
5758 
5759         Returns
5760         -------
5761         Dataset
5762         """
5763         from xarray.core.missing import _apply_over_vars_with_dim, ffill
5764 
5765         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)
5766         return new
5767 
5768     def bfill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5769         """Fill NaN values by propagating values backward
5770 
5771         *Requires bottleneck.*
5772 
5773         Parameters
5774         ----------
5775         dim : Hashable
5776             Specifies the dimension along which to propagate values when
5777             filling.
5778         limit : int or None, optional
5779             The maximum number of consecutive NaN values to backward fill. In
5780             other words, if there is a gap with more than this number of
5781             consecutive NaNs, it will only be partially filled. Must be greater
5782             than 0 or None for no limit. Must be None or greater than or equal
5783             to axis length if filling along chunked axes (dimensions).
5784 
5785         Returns
5786         -------
5787         Dataset
5788         """
5789         from xarray.core.missing import _apply_over_vars_with_dim, bfill
5790 
5791         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)
5792         return new
5793 
5794     def combine_first(self: T_Dataset, other: T_Dataset) -> T_Dataset:
5795         """Combine two Datasets, default to data_vars of self.
5796 
5797         The new coordinates follow the normal broadcasting and alignment rules
5798         of ``join='outer'``.  Vacant cells in the expanded coordinates are
5799         filled with np.nan.
5800 
5801         Parameters
5802         ----------
5803         other : Dataset
5804             Used to fill all matching missing values in this array.
5805 
5806         Returns
5807         -------
5808         Dataset
5809         """
5810         out = ops.fillna(self, other, join="outer", dataset_join="outer")
5811         return out
5812 
5813     def reduce(
5814         self: T_Dataset,
5815         func: Callable,
5816         dim: Dims = None,
5817         *,
5818         keep_attrs: bool | None = None,
5819         keepdims: bool = False,
5820         numeric_only: bool = False,
5821         **kwargs: Any,
5822     ) -> T_Dataset:
5823         """Reduce this dataset by applying `func` along some dimension(s).
5824 
5825         Parameters
5826         ----------
5827         func : callable
5828             Function which can be called in the form
5829             `f(x, axis=axis, **kwargs)` to return the result of reducing an
5830             np.ndarray over an integer valued axis.
5831         dim : str, Iterable of Hashable or None, optional
5832             Dimension(s) over which to apply `func`. By default `func` is
5833             applied over all dimensions.
5834         keep_attrs : bool or None, optional
5835             If True, the dataset's attributes (`attrs`) will be copied from
5836             the original object to the new one.  If False (default), the new
5837             object will be returned without attributes.
5838         keepdims : bool, default: False
5839             If True, the dimensions which are reduced are left in the result
5840             as dimensions of size one. Coordinates that use these dimensions
5841             are removed.
5842         numeric_only : bool, default: False
5843             If True, only apply ``func`` to variables with a numeric dtype.
5844         **kwargs : Any
5845             Additional keyword arguments passed on to ``func``.
5846 
5847         Returns
5848         -------
5849         reduced : Dataset
5850             Dataset with this object's DataArrays replaced with new DataArrays
5851             of summarized data and the indicated dimension(s) removed.
5852         """
5853         if kwargs.get("axis", None) is not None:
5854             raise ValueError(
5855                 "passing 'axis' to Dataset reduce methods is ambiguous."
5856                 " Please use 'dim' instead."
5857             )
5858 
5859         if dim is None or dim is ...:
5860             dims = set(self.dims)
5861         elif isinstance(dim, str) or not isinstance(dim, Iterable):
5862             dims = {dim}
5863         else:
5864             dims = set(dim)
5865 
5866         missing_dimensions = [d for d in dims if d not in self.dims]
5867         if missing_dimensions:
5868             raise ValueError(
5869                 f"Dataset does not contain the dimensions: {missing_dimensions}"
5870             )
5871 
5872         if keep_attrs is None:
5873             keep_attrs = _get_keep_attrs(default=False)
5874 
5875         variables: dict[Hashable, Variable] = {}
5876         for name, var in self._variables.items():
5877             reduce_dims = [d for d in var.dims if d in dims]
5878             if name in self.coords:
5879                 if not reduce_dims:
5880                     variables[name] = var
5881             else:
5882                 if (
5883                     # Some reduction functions (e.g. std, var) need to run on variables
5884                     # that don't have the reduce dims: PR5393
5885                     not reduce_dims
5886                     or not numeric_only
5887                     or np.issubdtype(var.dtype, np.number)
5888                     or (var.dtype == np.bool_)
5889                 ):
5890                     # prefer to aggregate over axis=None rather than
5891                     # axis=(0, 1) if they will be equivalent, because
5892                     # the former is often more efficient
5893                     # keep single-element dims as list, to support Hashables
5894                     reduce_maybe_single = (
5895                         None
5896                         if len(reduce_dims) == var.ndim and var.ndim != 1
5897                         else reduce_dims
5898                     )
5899                     variables[name] = var.reduce(
5900                         func,
5901                         dim=reduce_maybe_single,
5902                         keep_attrs=keep_attrs,
5903                         keepdims=keepdims,
5904                         **kwargs,
5905                     )
5906 
5907         coord_names = {k for k in self.coords if k in variables}
5908         indexes = {k: v for k, v in self._indexes.items() if k in variables}
5909         attrs = self.attrs if keep_attrs else None
5910         return self._replace_with_new_dims(
5911             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
5912         )
5913 
5914     def map(
5915         self: T_Dataset,
5916         func: Callable,
5917         keep_attrs: bool | None = None,
5918         args: Iterable[Any] = (),
5919         **kwargs: Any,
5920     ) -> T_Dataset:
5921         """Apply a function to each data variable in this dataset
5922 
5923         Parameters
5924         ----------
5925         func : callable
5926             Function which can be called in the form `func(x, *args, **kwargs)`
5927             to transform each DataArray `x` in this dataset into another
5928             DataArray.
5929         keep_attrs : bool or None, optional
5930             If True, both the dataset's and variables' attributes (`attrs`) will be
5931             copied from the original objects to the new ones. If False, the new dataset
5932             and variables will be returned without copying the attributes.
5933         args : iterable, optional
5934             Positional arguments passed on to `func`.
5935         **kwargs : Any
5936             Keyword arguments passed on to `func`.
5937 
5938         Returns
5939         -------
5940         applied : Dataset
5941             Resulting dataset from applying ``func`` to each data variable.
5942 
5943         Examples
5944         --------
5945         >>> da = xr.DataArray(np.random.randn(2, 3))
5946         >>> ds = xr.Dataset({"foo": da, "bar": ("x", [-1, 2])})
5947         >>> ds
5948         <xarray.Dataset>
5949         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5950         Dimensions without coordinates: dim_0, dim_1, x
5951         Data variables:
5952             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
5953             bar      (x) int64 -1 2
5954         >>> ds.map(np.fabs)
5955         <xarray.Dataset>
5956         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5957         Dimensions without coordinates: dim_0, dim_1, x
5958         Data variables:
5959             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 0.9773
5960             bar      (x) float64 1.0 2.0
5961         """
5962         if keep_attrs is None:
5963             keep_attrs = _get_keep_attrs(default=False)
5964         variables = {
5965             k: maybe_wrap_array(v, func(v, *args, **kwargs))
5966             for k, v in self.data_vars.items()
5967         }
5968         if keep_attrs:
5969             for k, v in variables.items():
5970                 v._copy_attrs_from(self.data_vars[k])
5971         attrs = self.attrs if keep_attrs else None
5972         return type(self)(variables, attrs=attrs)
5973 
5974     def apply(
5975         self: T_Dataset,
5976         func: Callable,
5977         keep_attrs: bool | None = None,
5978         args: Iterable[Any] = (),
5979         **kwargs: Any,
5980     ) -> T_Dataset:
5981         """
5982         Backward compatible implementation of ``map``
5983 
5984         See Also
5985         --------
5986         Dataset.map
5987         """
5988         warnings.warn(
5989             "Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged",
5990             PendingDeprecationWarning,
5991             stacklevel=2,
5992         )
5993         return self.map(func, keep_attrs, args, **kwargs)
5994 
5995     def assign(
5996         self: T_Dataset,
5997         variables: Mapping[Any, Any] | None = None,
5998         **variables_kwargs: Any,
5999     ) -> T_Dataset:
6000         """Assign new data variables to a Dataset, returning a new object
6001         with all the original variables in addition to the new ones.
6002 
6003         Parameters
6004         ----------
6005         variables : mapping of hashable to Any
6006             Mapping from variables names to the new values. If the new values
6007             are callable, they are computed on the Dataset and assigned to new
6008             data variables. If the values are not callable, (e.g. a DataArray,
6009             scalar, or array), they are simply assigned.
6010         **variables_kwargs
6011             The keyword arguments form of ``variables``.
6012             One of variables or variables_kwargs must be provided.
6013 
6014         Returns
6015         -------
6016         ds : Dataset
6017             A new Dataset with the new variables in addition to all the
6018             existing variables.
6019 
6020         Notes
6021         -----
6022         Since ``kwargs`` is a dictionary, the order of your arguments may not
6023         be preserved, and so the order of the new variables is not well
6024         defined. Assigning multiple variables within the same ``assign`` is
6025         possible, but you cannot reference other variables created within the
6026         same ``assign`` call.
6027 
6028         See Also
6029         --------
6030         pandas.DataFrame.assign
6031 
6032         Examples
6033         --------
6034         >>> x = xr.Dataset(
6035         ...     {
6036         ...         "temperature_c": (
6037         ...             ("lat", "lon"),
6038         ...             20 * np.random.rand(4).reshape(2, 2),
6039         ...         ),
6040         ...         "precipitation": (("lat", "lon"), np.random.rand(4).reshape(2, 2)),
6041         ...     },
6042         ...     coords={"lat": [10, 20], "lon": [150, 160]},
6043         ... )
6044         >>> x
6045         <xarray.Dataset>
6046         Dimensions:        (lat: 2, lon: 2)
6047         Coordinates:
6048           * lat            (lat) int64 10 20
6049           * lon            (lon) int64 150 160
6050         Data variables:
6051             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6052             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6053 
6054         Where the value is a callable, evaluated on dataset:
6055 
6056         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)
6057         <xarray.Dataset>
6058         Dimensions:        (lat: 2, lon: 2)
6059         Coordinates:
6060           * lat            (lat) int64 10 20
6061           * lon            (lon) int64 150 160
6062         Data variables:
6063             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6064             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6065             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
6066 
6067         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:
6068 
6069         >>> x.assign(temperature_f=x["temperature_c"] * 9 / 5 + 32)
6070         <xarray.Dataset>
6071         Dimensions:        (lat: 2, lon: 2)
6072         Coordinates:
6073           * lat            (lat) int64 10 20
6074           * lon            (lon) int64 150 160
6075         Data variables:
6076             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6077             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6078             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
6079 
6080         """
6081         variables = either_dict_or_kwargs(variables, variables_kwargs, "assign")
6082         data = self.copy()
6083         # do all calculations first...
6084         results: CoercibleMapping = data._calc_assign_results(variables)
6085         data.coords._maybe_drop_multiindex_coords(set(results.keys()))
6086         # ... and then assign
6087         data.update(results)
6088         return data
6089 
6090     def to_array(
6091         self, dim: Hashable = "variable", name: Hashable | None = None
6092     ) -> DataArray:
6093         """Convert this dataset into an xarray.DataArray
6094 
6095         The data variables of this dataset will be broadcast against each other
6096         and stacked along the first axis of the new array. All coordinates of
6097         this dataset will remain coordinates.
6098 
6099         Parameters
6100         ----------
6101         dim : Hashable, default: "variable"
6102             Name of the new dimension.
6103         name : Hashable or None, optional
6104             Name of the new data array.
6105 
6106         Returns
6107         -------
6108         array : xarray.DataArray
6109         """
6110         from xarray.core.dataarray import DataArray
6111 
6112         data_vars = [self.variables[k] for k in self.data_vars]
6113         broadcast_vars = broadcast_variables(*data_vars)
6114         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)
6115 
6116         dims = (dim,) + broadcast_vars[0].dims
6117         variable = Variable(dims, data, self.attrs, fastpath=True)
6118 
6119         coords = {k: v.variable for k, v in self.coords.items()}
6120         indexes = filter_indexes_from_coords(self._indexes, set(coords))
6121         new_dim_index = PandasIndex(list(self.data_vars), dim)
6122         indexes[dim] = new_dim_index
6123         coords.update(new_dim_index.create_variables())
6124 
6125         return DataArray._construct_direct(variable, coords, name, indexes)
6126 
6127     def _normalize_dim_order(
6128         self, dim_order: Sequence[Hashable] | None = None
6129     ) -> dict[Hashable, int]:
6130         """
6131         Check the validity of the provided dimensions if any and return the mapping
6132         between dimension name and their size.
6133 
6134         Parameters
6135         ----------
6136         dim_order: Sequence of Hashable or None, optional
6137             Dimension order to validate (default to the alphabetical order if None).
6138 
6139         Returns
6140         -------
6141         result : dict[Hashable, int]
6142             Validated dimensions mapping.
6143 
6144         """
6145         if dim_order is None:
6146             dim_order = list(self.dims)
6147         elif set(dim_order) != set(self.dims):
6148             raise ValueError(
6149                 "dim_order {} does not match the set of dimensions of this "
6150                 "Dataset: {}".format(dim_order, list(self.dims))
6151             )
6152 
6153         ordered_dims = {k: self.dims[k] for k in dim_order}
6154 
6155         return ordered_dims
6156 
6157     def to_pandas(self) -> pd.Series | pd.DataFrame:
6158         """Convert this dataset into a pandas object without changing the number of dimensions.
6159 
6160         The type of the returned object depends on the number of Dataset
6161         dimensions:
6162 
6163         * 0D -> `pandas.Series`
6164         * 1D -> `pandas.DataFrame`
6165 
6166         Only works for Datasets with 1 or fewer dimensions.
6167         """
6168         if len(self.dims) == 0:
6169             return pd.Series({k: v.item() for k, v in self.items()})
6170         if len(self.dims) == 1:
6171             return self.to_dataframe()
6172         raise ValueError(
6173             "cannot convert Datasets with %s dimensions into "
6174             "pandas objects without changing the number of dimensions. "
6175             "Please use Dataset.to_dataframe() instead." % len(self.dims)
6176         )
6177 
6178     def _to_dataframe(self, ordered_dims: Mapping[Any, int]):
6179         columns = [k for k in self.variables if k not in self.dims]
6180         data = [
6181             self._variables[k].set_dims(ordered_dims).values.reshape(-1)
6182             for k in columns
6183         ]
6184         index = self.coords.to_index([*ordered_dims])
6185         return pd.DataFrame(dict(zip(columns, data)), index=index)
6186 
6187     def to_dataframe(self, dim_order: Sequence[Hashable] | None = None) -> pd.DataFrame:
6188         """Convert this dataset into a pandas.DataFrame.
6189 
6190         Non-index variables in this dataset form the columns of the
6191         DataFrame. The DataFrame is indexed by the Cartesian product of
6192         this dataset's indices.
6193 
6194         Parameters
6195         ----------
6196         dim_order: Sequence of Hashable or None, optional
6197             Hierarchical dimension order for the resulting dataframe. All
6198             arrays are transposed to this order and then written out as flat
6199             vectors in contiguous order, so the last dimension in this list
6200             will be contiguous in the resulting DataFrame. This has a major
6201             influence on which operations are efficient on the resulting
6202             dataframe.
6203 
6204             If provided, must include all dimensions of this dataset. By
6205             default, dimensions are sorted alphabetically.
6206 
6207         Returns
6208         -------
6209         result : DataFrame
6210             Dataset as a pandas DataFrame.
6211 
6212         """
6213 
6214         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
6215 
6216         return self._to_dataframe(ordered_dims=ordered_dims)
6217 
6218     def _set_sparse_data_from_dataframe(
6219         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
6220     ) -> None:
6221         from sparse import COO
6222 
6223         if isinstance(idx, pd.MultiIndex):
6224             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)
6225             is_sorted = idx.is_monotonic_increasing
6226             shape = tuple(lev.size for lev in idx.levels)
6227         else:
6228             coords = np.arange(idx.size).reshape(1, -1)
6229             is_sorted = True
6230             shape = (idx.size,)
6231 
6232         for name, values in arrays:
6233             # In virtually all real use cases, the sparse array will now have
6234             # missing values and needs a fill_value. For consistency, don't
6235             # special case the rare exceptions (e.g., dtype=int without a
6236             # MultiIndex).
6237             dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
6238             values = np.asarray(values, dtype=dtype)
6239 
6240             data = COO(
6241                 coords,
6242                 values,
6243                 shape,
6244                 has_duplicates=False,
6245                 sorted=is_sorted,
6246                 fill_value=fill_value,
6247             )
6248             self[name] = (dims, data)
6249 
6250     def _set_numpy_data_from_dataframe(
6251         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
6252     ) -> None:
6253         if not isinstance(idx, pd.MultiIndex):
6254             for name, values in arrays:
6255                 self[name] = (dims, values)
6256             return
6257 
6258         # NB: similar, more general logic, now exists in
6259         # variable.unstack_once; we could consider combining them at some
6260         # point.
6261 
6262         shape = tuple(lev.size for lev in idx.levels)
6263         indexer = tuple(idx.codes)
6264 
6265         # We already verified that the MultiIndex has all unique values, so
6266         # there are missing values if and only if the size of output arrays is
6267         # larger that the index.
6268         missing_values = math.prod(shape) > idx.shape[0]
6269 
6270         for name, values in arrays:
6271             # NumPy indexing is much faster than using DataFrame.reindex() to
6272             # fill in missing values:
6273             # https://stackoverflow.com/a/35049899/809705
6274             if missing_values:
6275                 dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
6276                 data = np.full(shape, fill_value, dtype)
6277             else:
6278                 # If there are no missing values, keep the existing dtype
6279                 # instead of promoting to support NA, e.g., keep integer
6280                 # columns as integers.
6281                 # TODO: consider removing this special case, which doesn't
6282                 # exist for sparse=True.
6283                 data = np.zeros(shape, values.dtype)
6284             data[indexer] = values
6285             self[name] = (dims, data)
6286 
6287     @classmethod
6288     def from_dataframe(
6289         cls: type[T_Dataset], dataframe: pd.DataFrame, sparse: bool = False
6290     ) -> T_Dataset:
6291         """Convert a pandas.DataFrame into an xarray.Dataset
6292 
6293         Each column will be converted into an independent variable in the
6294         Dataset. If the dataframe's index is a MultiIndex, it will be expanded
6295         into a tensor product of one-dimensional indices (filling in missing
6296         values with NaN). This method will produce a Dataset very similar to
6297         that on which the 'to_dataframe' method was called, except with
6298         possibly redundant dimensions (since all dataset variables will have
6299         the same dimensionality)
6300 
6301         Parameters
6302         ----------
6303         dataframe : DataFrame
6304             DataFrame from which to copy data and indices.
6305         sparse : bool, default: False
6306             If true, create a sparse arrays instead of dense numpy arrays. This
6307             can potentially save a large amount of memory if the DataFrame has
6308             a MultiIndex. Requires the sparse package (sparse.pydata.org).
6309 
6310         Returns
6311         -------
6312         New Dataset.
6313 
6314         See Also
6315         --------
6316         xarray.DataArray.from_series
6317         pandas.DataFrame.to_xarray
6318         """
6319         # TODO: Add an option to remove dimensions along which the variables
6320         # are constant, to enable consistent serialization to/from a dataframe,
6321         # even if some variables have different dimensionality.
6322 
6323         if not dataframe.columns.is_unique:
6324             raise ValueError("cannot convert DataFrame with non-unique columns")
6325 
6326         idx = remove_unused_levels_categories(dataframe.index)
6327 
6328         if isinstance(idx, pd.MultiIndex) and not idx.is_unique:
6329             raise ValueError(
6330                 "cannot convert a DataFrame with a non-unique MultiIndex into xarray"
6331             )
6332 
6333         # Cast to a NumPy array first, in case the Series is a pandas Extension
6334         # array (which doesn't have a valid NumPy dtype)
6335         # TODO: allow users to control how this casting happens, e.g., by
6336         # forwarding arguments to pandas.Series.to_numpy?
6337         arrays = [(k, np.asarray(v)) for k, v in dataframe.items()]
6338 
6339         indexes: dict[Hashable, Index] = {}
6340         index_vars: dict[Hashable, Variable] = {}
6341 
6342         if isinstance(idx, pd.MultiIndex):
6343             dims = tuple(
6344                 name if name is not None else "level_%i" % n
6345                 for n, name in enumerate(idx.names)
6346             )
6347             for dim, lev in zip(dims, idx.levels):
6348                 xr_idx = PandasIndex(lev, dim)
6349                 indexes[dim] = xr_idx
6350                 index_vars.update(xr_idx.create_variables())
6351         else:
6352             index_name = idx.name if idx.name is not None else "index"
6353             dims = (index_name,)
6354             xr_idx = PandasIndex(idx, index_name)
6355             indexes[index_name] = xr_idx
6356             index_vars.update(xr_idx.create_variables())
6357 
6358         obj = cls._construct_direct(index_vars, set(index_vars), indexes=indexes)
6359 
6360         if sparse:
6361             obj._set_sparse_data_from_dataframe(idx, arrays, dims)
6362         else:
6363             obj._set_numpy_data_from_dataframe(idx, arrays, dims)
6364         return obj
6365 
6366     def to_dask_dataframe(
6367         self, dim_order: Sequence[Hashable] | None = None, set_index: bool = False
6368     ) -> DaskDataFrame:
6369         """
6370         Convert this dataset into a dask.dataframe.DataFrame.
6371 
6372         The dimensions, coordinates and data variables in this dataset form
6373         the columns of the DataFrame.
6374 
6375         Parameters
6376         ----------
6377         dim_order : list, optional
6378             Hierarchical dimension order for the resulting dataframe. All
6379             arrays are transposed to this order and then written out as flat
6380             vectors in contiguous order, so the last dimension in this list
6381             will be contiguous in the resulting DataFrame. This has a major
6382             influence on which operations are efficient on the resulting dask
6383             dataframe.
6384 
6385             If provided, must include all dimensions of this dataset. By
6386             default, dimensions are sorted alphabetically.
6387         set_index : bool, default: False
6388             If set_index=True, the dask DataFrame is indexed by this dataset's
6389             coordinate. Since dask DataFrames do not support multi-indexes,
6390             set_index only works if the dataset only contains one dimension.
6391 
6392         Returns
6393         -------
6394         dask.dataframe.DataFrame
6395         """
6396 
6397         import dask.array as da
6398         import dask.dataframe as dd
6399 
6400         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
6401 
6402         columns = list(ordered_dims)
6403         columns.extend(k for k in self.coords if k not in self.dims)
6404         columns.extend(self.data_vars)
6405 
6406         series_list = []
6407         for name in columns:
6408             try:
6409                 var = self.variables[name]
6410             except KeyError:
6411                 # dimension without a matching coordinate
6412                 size = self.dims[name]
6413                 data = da.arange(size, chunks=size, dtype=np.int64)
6414                 var = Variable((name,), data)
6415 
6416             # IndexVariable objects have a dummy .chunk() method
6417             if isinstance(var, IndexVariable):
6418                 var = var.to_base_variable()
6419 
6420             # Make sure var is a dask array, otherwise the array can become too large
6421             # when it is broadcasted to several dimensions:
6422             if not is_duck_dask_array(var._data):
6423                 var = var.chunk()
6424 
6425             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data
6426             series = dd.from_array(dask_array.reshape(-1), columns=[name])
6427             series_list.append(series)
6428 
6429         df = dd.concat(series_list, axis=1)
6430 
6431         if set_index:
6432             dim_order = [*ordered_dims]
6433 
6434             if len(dim_order) == 1:
6435                 (dim,) = dim_order
6436                 df = df.set_index(dim)
6437             else:
6438                 # triggers an error about multi-indexes, even if only one
6439                 # dimension is passed
6440                 df = df.set_index(dim_order)
6441 
6442         return df
6443 
6444     def to_dict(
6445         self, data: bool | Literal["list", "array"] = "list", encoding: bool = False
6446     ) -> dict[str, Any]:
6447         """
6448         Convert this dataset to a dictionary following xarray naming
6449         conventions.
6450 
6451         Converts all variables and attributes to native Python objects
6452         Useful for converting to json. To avoid datetime incompatibility
6453         use decode_times=False kwarg in xarrray.open_dataset.
6454 
6455         Parameters
6456         ----------
6457         data : bool or {"list", "array"}, default: "list"
6458             Whether to include the actual data in the dictionary. When set to
6459             False, returns just the schema. If set to "array", returns data as
6460             underlying array type. If set to "list" (or True for backwards
6461             compatibility), returns data in lists of Python data types. Note
6462             that for obtaining the "list" output efficiently, use
6463             `ds.compute().to_dict(data="list")`.
6464 
6465         encoding : bool, default: False
6466             Whether to include the Dataset's encoding in the dictionary.
6467 
6468         Returns
6469         -------
6470         d : dict
6471             Dict with keys: "coords", "attrs", "dims", "data_vars" and optionally
6472             "encoding".
6473 
6474         See Also
6475         --------
6476         Dataset.from_dict
6477         DataArray.to_dict
6478         """
6479         d: dict = {
6480             "coords": {},
6481             "attrs": decode_numpy_dict_values(self.attrs),
6482             "dims": dict(self.dims),
6483             "data_vars": {},
6484         }
6485         for k in self.coords:
6486             d["coords"].update(
6487                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6488             )
6489         for k in self.data_vars:
6490             d["data_vars"].update(
6491                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6492             )
6493         if encoding:
6494             d["encoding"] = dict(self.encoding)
6495         return d
6496 
6497     @classmethod
6498     def from_dict(cls: type[T_Dataset], d: Mapping[Any, Any]) -> T_Dataset:
6499         """Convert a dictionary into an xarray.Dataset.
6500 
6501         Parameters
6502         ----------
6503         d : dict-like
6504             Mapping with a minimum structure of
6505                 ``{"var_0": {"dims": [..], "data": [..]}, \
6506                             ...}``
6507 
6508         Returns
6509         -------
6510         obj : Dataset
6511 
6512         See also
6513         --------
6514         Dataset.to_dict
6515         DataArray.from_dict
6516 
6517         Examples
6518         --------
6519         >>> d = {
6520         ...     "t": {"dims": ("t"), "data": [0, 1, 2]},
6521         ...     "a": {"dims": ("t"), "data": ["a", "b", "c"]},
6522         ...     "b": {"dims": ("t"), "data": [10, 20, 30]},
6523         ... }
6524         >>> ds = xr.Dataset.from_dict(d)
6525         >>> ds
6526         <xarray.Dataset>
6527         Dimensions:  (t: 3)
6528         Coordinates:
6529           * t        (t) int64 0 1 2
6530         Data variables:
6531             a        (t) <U1 'a' 'b' 'c'
6532             b        (t) int64 10 20 30
6533 
6534         >>> d = {
6535         ...     "coords": {
6536         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
6537         ...     },
6538         ...     "attrs": {"title": "air temperature"},
6539         ...     "dims": "t",
6540         ...     "data_vars": {
6541         ...         "a": {"dims": "t", "data": [10, 20, 30]},
6542         ...         "b": {"dims": "t", "data": ["a", "b", "c"]},
6543         ...     },
6544         ... }
6545         >>> ds = xr.Dataset.from_dict(d)
6546         >>> ds
6547         <xarray.Dataset>
6548         Dimensions:  (t: 3)
6549         Coordinates:
6550           * t        (t) int64 0 1 2
6551         Data variables:
6552             a        (t) int64 10 20 30
6553             b        (t) <U1 'a' 'b' 'c'
6554         Attributes:
6555             title:    air temperature
6556 
6557         """
6558 
6559         variables: Iterable[tuple[Hashable, Any]]
6560         if not {"coords", "data_vars"}.issubset(set(d)):
6561             variables = d.items()
6562         else:
6563             import itertools
6564 
6565             variables = itertools.chain(
6566                 d.get("coords", {}).items(), d.get("data_vars", {}).items()
6567             )
6568         try:
6569             variable_dict = {
6570                 k: (v["dims"], v["data"], v.get("attrs"), v.get("encoding"))
6571                 for k, v in variables
6572             }
6573         except KeyError as e:
6574             raise ValueError(
6575                 "cannot convert dict without the key "
6576                 "'{dims_data}'".format(dims_data=str(e.args[0]))
6577             )
6578         obj = cls(variable_dict)
6579 
6580         # what if coords aren't dims?
6581         coords = set(d.get("coords", {})) - set(d.get("dims", {}))
6582         obj = obj.set_coords(coords)
6583 
6584         obj.attrs.update(d.get("attrs", {}))
6585         obj.encoding.update(d.get("encoding", {}))
6586 
6587         return obj
6588 
6589     def _unary_op(self: T_Dataset, f, *args, **kwargs) -> T_Dataset:
6590         variables = {}
6591         keep_attrs = kwargs.pop("keep_attrs", None)
6592         if keep_attrs is None:
6593             keep_attrs = _get_keep_attrs(default=True)
6594         for k, v in self._variables.items():
6595             if k in self._coord_names:
6596                 variables[k] = v
6597             else:
6598                 variables[k] = f(v, *args, **kwargs)
6599                 if keep_attrs:
6600                     variables[k].attrs = v._attrs
6601         attrs = self._attrs if keep_attrs else None
6602         return self._replace_with_new_dims(variables, attrs=attrs)
6603 
6604     def _binary_op(self, other, f, reflexive=False, join=None) -> Dataset:
6605         from xarray.core.dataarray import DataArray
6606         from xarray.core.groupby import GroupBy
6607 
6608         if isinstance(other, GroupBy):
6609             return NotImplemented
6610         align_type = OPTIONS["arithmetic_join"] if join is None else join
6611         if isinstance(other, (DataArray, Dataset)):
6612             self, other = align(self, other, join=align_type, copy=False)  # type: ignore[assignment]
6613         g = f if not reflexive else lambda x, y: f(y, x)
6614         ds = self._calculate_binary_op(g, other, join=align_type)
6615         keep_attrs = _get_keep_attrs(default=False)
6616         if keep_attrs:
6617             ds.attrs = self.attrs
6618         return ds
6619 
6620     def _inplace_binary_op(self: T_Dataset, other, f) -> T_Dataset:
6621         from xarray.core.dataarray import DataArray
6622         from xarray.core.groupby import GroupBy
6623 
6624         if isinstance(other, GroupBy):
6625             raise TypeError(
6626                 "in-place operations between a Dataset and "
6627                 "a grouped object are not permitted"
6628             )
6629         # we don't actually modify arrays in-place with in-place Dataset
6630         # arithmetic -- this lets us automatically align things
6631         if isinstance(other, (DataArray, Dataset)):
6632             other = other.reindex_like(self, copy=False)
6633         g = ops.inplace_to_noninplace_op(f)
6634         ds = self._calculate_binary_op(g, other, inplace=True)
6635         self._replace_with_new_dims(
6636             ds._variables,
6637             ds._coord_names,
6638             attrs=ds._attrs,
6639             indexes=ds._indexes,
6640             inplace=True,
6641         )
6642         return self
6643 
6644     def _calculate_binary_op(
6645         self, f, other, join="inner", inplace: bool = False
6646     ) -> Dataset:
6647         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):
6648             if inplace and set(lhs_data_vars) != set(rhs_data_vars):
6649                 raise ValueError(
6650                     "datasets must have the same data variables "
6651                     f"for in-place arithmetic operations: {list(lhs_data_vars)}, {list(rhs_data_vars)}"
6652                 )
6653 
6654             dest_vars = {}
6655 
6656             for k in lhs_data_vars:
6657                 if k in rhs_data_vars:
6658                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])
6659                 elif join in ["left", "outer"]:
6660                     dest_vars[k] = f(lhs_vars[k], np.nan)
6661             for k in rhs_data_vars:
6662                 if k not in dest_vars and join in ["right", "outer"]:
6663                     dest_vars[k] = f(rhs_vars[k], np.nan)
6664             return dest_vars
6665 
6666         if utils.is_dict_like(other) and not isinstance(other, Dataset):
6667             # can't use our shortcut of doing the binary operation with
6668             # Variable objects, so apply over our data vars instead.
6669             new_data_vars = apply_over_both(
6670                 self.data_vars, other, self.data_vars, other
6671             )
6672             return type(self)(new_data_vars)
6673 
6674         other_coords: Coordinates | None = getattr(other, "coords", None)
6675         ds = self.coords.merge(other_coords)
6676 
6677         if isinstance(other, Dataset):
6678             new_vars = apply_over_both(
6679                 self.data_vars, other.data_vars, self.variables, other.variables
6680             )
6681         else:
6682             other_variable = getattr(other, "variable", other)
6683             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}
6684         ds._variables.update(new_vars)
6685         ds._dims = calculate_dimensions(ds._variables)
6686         return ds
6687 
6688     def _copy_attrs_from(self, other):
6689         self.attrs = other.attrs
6690         for v in other.variables:
6691             if v in self.variables:
6692                 self.variables[v].attrs = other.variables[v].attrs
6693 
6694     def diff(
6695         self: T_Dataset,
6696         dim: Hashable,
6697         n: int = 1,
6698         label: Literal["upper", "lower"] = "upper",
6699     ) -> T_Dataset:
6700         """Calculate the n-th order discrete difference along given axis.
6701 
6702         Parameters
6703         ----------
6704         dim : Hashable
6705             Dimension over which to calculate the finite difference.
6706         n : int, default: 1
6707             The number of times values are differenced.
6708         label : {"upper", "lower"}, default: "upper"
6709             The new coordinate in dimension ``dim`` will have the
6710             values of either the minuend's or subtrahend's coordinate
6711             for values 'upper' and 'lower', respectively.
6712 
6713         Returns
6714         -------
6715         difference : Dataset
6716             The n-th order finite difference of this object.
6717 
6718         Notes
6719         -----
6720         `n` matches numpy's behavior and is different from pandas' first argument named
6721         `periods`.
6722 
6723         Examples
6724         --------
6725         >>> ds = xr.Dataset({"foo": ("x", [5, 5, 6, 6])})
6726         >>> ds.diff("x")
6727         <xarray.Dataset>
6728         Dimensions:  (x: 3)
6729         Dimensions without coordinates: x
6730         Data variables:
6731             foo      (x) int64 0 1 0
6732         >>> ds.diff("x", 2)
6733         <xarray.Dataset>
6734         Dimensions:  (x: 2)
6735         Dimensions without coordinates: x
6736         Data variables:
6737             foo      (x) int64 1 -1
6738 
6739         See Also
6740         --------
6741         Dataset.differentiate
6742         """
6743         if n == 0:
6744             return self
6745         if n < 0:
6746             raise ValueError(f"order `n` must be non-negative but got {n}")
6747 
6748         # prepare slices
6749         slice_start = {dim: slice(None, -1)}
6750         slice_end = {dim: slice(1, None)}
6751 
6752         # prepare new coordinate
6753         if label == "upper":
6754             slice_new = slice_end
6755         elif label == "lower":
6756             slice_new = slice_start
6757         else:
6758             raise ValueError("The 'label' argument has to be either 'upper' or 'lower'")
6759 
6760         indexes, index_vars = isel_indexes(self.xindexes, slice_new)
6761         variables = {}
6762 
6763         for name, var in self.variables.items():
6764             if name in index_vars:
6765                 variables[name] = index_vars[name]
6766             elif dim in var.dims:
6767                 if name in self.data_vars:
6768                     variables[name] = var.isel(slice_end) - var.isel(slice_start)
6769                 else:
6770                     variables[name] = var.isel(slice_new)
6771             else:
6772                 variables[name] = var
6773 
6774         difference = self._replace_with_new_dims(variables, indexes=indexes)
6775 
6776         if n > 1:
6777             return difference.diff(dim, n - 1)
6778         else:
6779             return difference
6780 
6781     def shift(
6782         self: T_Dataset,
6783         shifts: Mapping[Any, int] | None = None,
6784         fill_value: Any = xrdtypes.NA,
6785         **shifts_kwargs: int,
6786     ) -> T_Dataset:
6787         """Shift this dataset by an offset along one or more dimensions.
6788 
6789         Only data variables are moved; coordinates stay in place. This is
6790         consistent with the behavior of ``shift`` in pandas.
6791 
6792         Values shifted from beyond array bounds will appear at one end of
6793         each dimension, which are filled according to `fill_value`. For periodic
6794         offsets instead see `roll`.
6795 
6796         Parameters
6797         ----------
6798         shifts : mapping of hashable to int
6799             Integer offset to shift along each of the given dimensions.
6800             Positive offsets shift to the right; negative offsets shift to the
6801             left.
6802         fill_value : scalar or dict-like, optional
6803             Value to use for newly missing values. If a dict-like, maps
6804             variable names (including coordinates) to fill values.
6805         **shifts_kwargs
6806             The keyword arguments form of ``shifts``.
6807             One of shifts or shifts_kwargs must be provided.
6808 
6809         Returns
6810         -------
6811         shifted : Dataset
6812             Dataset with the same coordinates and attributes but shifted data
6813             variables.
6814 
6815         See Also
6816         --------
6817         roll
6818 
6819         Examples
6820         --------
6821         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
6822         >>> ds.shift(x=2)
6823         <xarray.Dataset>
6824         Dimensions:  (x: 5)
6825         Dimensions without coordinates: x
6826         Data variables:
6827             foo      (x) object nan nan 'a' 'b' 'c'
6828         """
6829         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
6830         invalid = [k for k in shifts if k not in self.dims]
6831         if invalid:
6832             raise ValueError(f"dimensions {invalid!r} do not exist")
6833 
6834         variables = {}
6835         for name, var in self.variables.items():
6836             if name in self.data_vars:
6837                 fill_value_ = (
6838                     fill_value.get(name, xrdtypes.NA)
6839                     if isinstance(fill_value, dict)
6840                     else fill_value
6841                 )
6842 
6843                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}
6844                 variables[name] = var.shift(fill_value=fill_value_, shifts=var_shifts)
6845             else:
6846                 variables[name] = var
6847 
6848         return self._replace(variables)
6849 
6850     def roll(
6851         self: T_Dataset,
6852         shifts: Mapping[Any, int] | None = None,
6853         roll_coords: bool = False,
6854         **shifts_kwargs: int,
6855     ) -> T_Dataset:
6856         """Roll this dataset by an offset along one or more dimensions.
6857 
6858         Unlike shift, roll treats the given dimensions as periodic, so will not
6859         create any missing values to be filled.
6860 
6861         Also unlike shift, roll may rotate all variables, including coordinates
6862         if specified. The direction of rotation is consistent with
6863         :py:func:`numpy.roll`.
6864 
6865         Parameters
6866         ----------
6867         shifts : mapping of hashable to int, optional
6868             A dict with keys matching dimensions and values given
6869             by integers to rotate each of the given dimensions. Positive
6870             offsets roll to the right; negative offsets roll to the left.
6871         roll_coords : bool, default: False
6872             Indicates whether to roll the coordinates by the offset too.
6873         **shifts_kwargs : {dim: offset, ...}, optional
6874             The keyword arguments form of ``shifts``.
6875             One of shifts or shifts_kwargs must be provided.
6876 
6877         Returns
6878         -------
6879         rolled : Dataset
6880             Dataset with the same attributes but rolled data and coordinates.
6881 
6882         See Also
6883         --------
6884         shift
6885 
6886         Examples
6887         --------
6888         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))}, coords={"x": np.arange(5)})
6889         >>> ds.roll(x=2)
6890         <xarray.Dataset>
6891         Dimensions:  (x: 5)
6892         Coordinates:
6893           * x        (x) int64 0 1 2 3 4
6894         Data variables:
6895             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6896 
6897         >>> ds.roll(x=2, roll_coords=True)
6898         <xarray.Dataset>
6899         Dimensions:  (x: 5)
6900         Coordinates:
6901           * x        (x) int64 3 4 0 1 2
6902         Data variables:
6903             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6904 
6905         """
6906         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
6907         invalid = [k for k in shifts if k not in self.dims]
6908         if invalid:
6909             raise ValueError(f"dimensions {invalid!r} do not exist")
6910 
6911         unrolled_vars: tuple[Hashable, ...]
6912 
6913         if roll_coords:
6914             indexes, index_vars = roll_indexes(self.xindexes, shifts)
6915             unrolled_vars = ()
6916         else:
6917             indexes = dict(self._indexes)
6918             index_vars = dict(self.xindexes.variables)
6919             unrolled_vars = tuple(self.coords)
6920 
6921         variables = {}
6922         for k, var in self.variables.items():
6923             if k in index_vars:
6924                 variables[k] = index_vars[k]
6925             elif k not in unrolled_vars:
6926                 variables[k] = var.roll(
6927                     shifts={k: s for k, s in shifts.items() if k in var.dims}
6928                 )
6929             else:
6930                 variables[k] = var
6931 
6932         return self._replace(variables, indexes=indexes)
6933 
6934     def sortby(
6935         self: T_Dataset,
6936         variables: Hashable | DataArray | list[Hashable | DataArray],
6937         ascending: bool = True,
6938     ) -> T_Dataset:
6939         """
6940         Sort object by labels or values (along an axis).
6941 
6942         Sorts the dataset, either along specified dimensions,
6943         or according to values of 1-D dataarrays that share dimension
6944         with calling object.
6945 
6946         If the input variables are dataarrays, then the dataarrays are aligned
6947         (via left-join) to the calling object prior to sorting by cell values.
6948         NaNs are sorted to the end, following Numpy convention.
6949 
6950         If multiple sorts along the same dimension is
6951         given, numpy's lexsort is performed along that dimension:
6952         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
6953         and the FIRST key in the sequence is used as the primary sort key,
6954         followed by the 2nd key, etc.
6955 
6956         Parameters
6957         ----------
6958         variables : Hashable, DataArray, or list of hashable or DataArray
6959             1D DataArray objects or name(s) of 1D variable(s) in
6960             coords/data_vars whose values are used to sort the dataset.
6961         ascending : bool, default: True
6962             Whether to sort by ascending or descending order.
6963 
6964         Returns
6965         -------
6966         sorted : Dataset
6967             A new dataset where all the specified dims are sorted by dim
6968             labels.
6969 
6970         See Also
6971         --------
6972         DataArray.sortby
6973         numpy.sort
6974         pandas.sort_values
6975         pandas.sort_index
6976 
6977         Examples
6978         --------
6979         >>> ds = xr.Dataset(
6980         ...     {
6981         ...         "A": (("x", "y"), [[1, 2], [3, 4]]),
6982         ...         "B": (("x", "y"), [[5, 6], [7, 8]]),
6983         ...     },
6984         ...     coords={"x": ["b", "a"], "y": [1, 0]},
6985         ... )
6986         >>> ds = ds.sortby("x")
6987         >>> ds
6988         <xarray.Dataset>
6989         Dimensions:  (x: 2, y: 2)
6990         Coordinates:
6991           * x        (x) <U1 'a' 'b'
6992           * y        (y) int64 1 0
6993         Data variables:
6994             A        (x, y) int64 3 4 1 2
6995             B        (x, y) int64 7 8 5 6
6996         """
6997         from xarray.core.dataarray import DataArray
6998 
6999         if not isinstance(variables, list):
7000             variables = [variables]
7001         else:
7002             variables = variables
7003         arrays = [v if isinstance(v, DataArray) else self[v] for v in variables]
7004         aligned_vars = align(self, *arrays, join="left")  # type: ignore[type-var]
7005         aligned_self: T_Dataset = aligned_vars[0]  # type: ignore[assignment]
7006         aligned_other_vars: tuple[DataArray, ...] = aligned_vars[1:]  # type: ignore[assignment]
7007         vars_by_dim = defaultdict(list)
7008         for data_array in aligned_other_vars:
7009             if data_array.ndim != 1:
7010                 raise ValueError("Input DataArray is not 1-D.")
7011             (key,) = data_array.dims
7012             vars_by_dim[key].append(data_array)
7013 
7014         indices = {}
7015         for key, arrays in vars_by_dim.items():
7016             order = np.lexsort(tuple(reversed(arrays)))
7017             indices[key] = order if ascending else order[::-1]
7018         return aligned_self.isel(indices)
7019 
7020     def quantile(
7021         self: T_Dataset,
7022         q: ArrayLike,
7023         dim: Dims = None,
7024         method: QuantileMethods = "linear",
7025         numeric_only: bool = False,
7026         keep_attrs: bool | None = None,
7027         skipna: bool | None = None,
7028         interpolation: QuantileMethods | None = None,
7029     ) -> T_Dataset:
7030         """Compute the qth quantile of the data along the specified dimension.
7031 
7032         Returns the qth quantiles(s) of the array elements for each variable
7033         in the Dataset.
7034 
7035         Parameters
7036         ----------
7037         q : float or array-like of float
7038             Quantile to compute, which must be between 0 and 1 inclusive.
7039         dim : str or Iterable of Hashable, optional
7040             Dimension(s) over which to apply quantile.
7041         method : str, default: "linear"
7042             This optional parameter specifies the interpolation method to use when the
7043             desired quantile lies between two data points. The options sorted by their R
7044             type as summarized in the H&F paper [1]_ are:
7045 
7046                 1. "inverted_cdf" (*)
7047                 2. "averaged_inverted_cdf" (*)
7048                 3. "closest_observation" (*)
7049                 4. "interpolated_inverted_cdf" (*)
7050                 5. "hazen" (*)
7051                 6. "weibull" (*)
7052                 7. "linear"  (default)
7053                 8. "median_unbiased" (*)
7054                 9. "normal_unbiased" (*)
7055 
7056             The first three methods are discontiuous.  The following discontinuous
7057             variations of the default "linear" (7.) option are also available:
7058 
7059                 * "lower"
7060                 * "higher"
7061                 * "midpoint"
7062                 * "nearest"
7063 
7064             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
7065             was previously called "interpolation", renamed in accordance with numpy
7066             version 1.22.0.
7067 
7068             (*) These methods require numpy version 1.22 or newer.
7069 
7070         keep_attrs : bool, optional
7071             If True, the dataset's attributes (`attrs`) will be copied from
7072             the original object to the new one.  If False (default), the new
7073             object will be returned without attributes.
7074         numeric_only : bool, optional
7075             If True, only apply ``func`` to variables with a numeric dtype.
7076         skipna : bool, optional
7077             If True, skip missing values (as marked by NaN). By default, only
7078             skips missing values for float dtypes; other dtypes either do not
7079             have a sentinel missing value (int) or skipna=True has not been
7080             implemented (object, datetime64 or timedelta64).
7081 
7082         Returns
7083         -------
7084         quantiles : Dataset
7085             If `q` is a single quantile, then the result is a scalar for each
7086             variable in data_vars. If multiple percentiles are given, first
7087             axis of the result corresponds to the quantile and a quantile
7088             dimension is added to the return Dataset. The other dimensions are
7089             the dimensions that remain after the reduction of the array.
7090 
7091         See Also
7092         --------
7093         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile
7094 
7095         Examples
7096         --------
7097         >>> ds = xr.Dataset(
7098         ...     {"a": (("x", "y"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},
7099         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
7100         ... )
7101         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)
7102         <xarray.Dataset>
7103         Dimensions:   ()
7104         Coordinates:
7105             quantile  float64 0.0
7106         Data variables:
7107             a         float64 0.7
7108         >>> ds.quantile(0, dim="x")
7109         <xarray.Dataset>
7110         Dimensions:   (y: 4)
7111         Coordinates:
7112           * y         (y) float64 1.0 1.5 2.0 2.5
7113             quantile  float64 0.0
7114         Data variables:
7115             a         (y) float64 0.7 4.2 2.6 1.5
7116         >>> ds.quantile([0, 0.5, 1])
7117         <xarray.Dataset>
7118         Dimensions:   (quantile: 3)
7119         Coordinates:
7120           * quantile  (quantile) float64 0.0 0.5 1.0
7121         Data variables:
7122             a         (quantile) float64 0.7 3.4 9.4
7123         >>> ds.quantile([0, 0.5, 1], dim="x")
7124         <xarray.Dataset>
7125         Dimensions:   (quantile: 3, y: 4)
7126         Coordinates:
7127           * y         (y) float64 1.0 1.5 2.0 2.5
7128           * quantile  (quantile) float64 0.0 0.5 1.0
7129         Data variables:
7130             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9
7131 
7132         References
7133         ----------
7134         .. [1] R. J. Hyndman and Y. Fan,
7135            "Sample quantiles in statistical packages,"
7136            The American Statistician, 50(4), pp. 361-365, 1996
7137         """
7138 
7139         # interpolation renamed to method in version 0.21.0
7140         # check here and in variable to avoid repeated warnings
7141         if interpolation is not None:
7142             warnings.warn(
7143                 "The `interpolation` argument to quantile was renamed to `method`.",
7144                 FutureWarning,
7145             )
7146 
7147             if method != "linear":
7148                 raise TypeError("Cannot pass interpolation and method keywords!")
7149 
7150             method = interpolation
7151 
7152         dims: set[Hashable]
7153         if isinstance(dim, str):
7154             dims = {dim}
7155         elif dim is None or dim is ...:
7156             dims = set(self.dims)
7157         else:
7158             dims = set(dim)
7159 
7160         _assert_empty(
7161             tuple(d for d in dims if d not in self.dims),
7162             "Dataset does not contain the dimensions: %s",
7163         )
7164 
7165         q = np.asarray(q, dtype=np.float64)
7166 
7167         variables = {}
7168         for name, var in self.variables.items():
7169             reduce_dims = [d for d in var.dims if d in dims]
7170             if reduce_dims or not var.dims:
7171                 if name not in self.coords:
7172                     if (
7173                         not numeric_only
7174                         or np.issubdtype(var.dtype, np.number)
7175                         or var.dtype == np.bool_
7176                     ):
7177                         variables[name] = var.quantile(
7178                             q,
7179                             dim=reduce_dims,
7180                             method=method,
7181                             keep_attrs=keep_attrs,
7182                             skipna=skipna,
7183                         )
7184 
7185             else:
7186                 variables[name] = var
7187 
7188         # construct the new dataset
7189         coord_names = {k for k in self.coords if k in variables}
7190         indexes = {k: v for k, v in self._indexes.items() if k in variables}
7191         if keep_attrs is None:
7192             keep_attrs = _get_keep_attrs(default=False)
7193         attrs = self.attrs if keep_attrs else None
7194         new = self._replace_with_new_dims(
7195             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
7196         )
7197         return new.assign_coords(quantile=q)
7198 
7199     def rank(
7200         self: T_Dataset,
7201         dim: Hashable,
7202         pct: bool = False,
7203         keep_attrs: bool | None = None,
7204     ) -> T_Dataset:
7205         """Ranks the data.
7206 
7207         Equal values are assigned a rank that is the average of the ranks that
7208         would have been otherwise assigned to all of the values within
7209         that set.
7210         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.
7211 
7212         NaNs in the input array are returned as NaNs.
7213 
7214         The `bottleneck` library is required.
7215 
7216         Parameters
7217         ----------
7218         dim : Hashable
7219             Dimension over which to compute rank.
7220         pct : bool, default: False
7221             If True, compute percentage ranks, otherwise compute integer ranks.
7222         keep_attrs : bool or None, optional
7223             If True, the dataset's attributes (`attrs`) will be copied from
7224             the original object to the new one.  If False, the new
7225             object will be returned without attributes.
7226 
7227         Returns
7228         -------
7229         ranked : Dataset
7230             Variables that do not depend on `dim` are dropped.
7231         """
7232         if not OPTIONS["use_bottleneck"]:
7233             raise RuntimeError(
7234                 "rank requires bottleneck to be enabled."
7235                 " Call `xr.set_options(use_bottleneck=True)` to enable it."
7236             )
7237 
7238         if dim not in self.dims:
7239             raise ValueError(f"Dataset does not contain the dimension: {dim}")
7240 
7241         variables = {}
7242         for name, var in self.variables.items():
7243             if name in self.data_vars:
7244                 if dim in var.dims:
7245                     variables[name] = var.rank(dim, pct=pct)
7246             else:
7247                 variables[name] = var
7248 
7249         coord_names = set(self.coords)
7250         if keep_attrs is None:
7251             keep_attrs = _get_keep_attrs(default=False)
7252         attrs = self.attrs if keep_attrs else None
7253         return self._replace(variables, coord_names, attrs=attrs)
7254 
7255     def differentiate(
7256         self: T_Dataset,
7257         coord: Hashable,
7258         edge_order: Literal[1, 2] = 1,
7259         datetime_unit: DatetimeUnitOptions | None = None,
7260     ) -> T_Dataset:
7261         """ Differentiate with the second order accurate central
7262         differences.
7263 
7264         .. note::
7265             This feature is limited to simple cartesian geometry, i.e. coord
7266             must be one dimensional.
7267 
7268         Parameters
7269         ----------
7270         coord : Hashable
7271             The coordinate to be used to compute the gradient.
7272         edge_order : {1, 2}, default: 1
7273             N-th order accurate differences at the boundaries.
7274         datetime_unit : None or {"Y", "M", "W", "D", "h", "m", "s", "ms", \
7275             "us", "ns", "ps", "fs", "as", None}, default: None
7276             Unit to compute gradient. Only valid for datetime coordinate.
7277 
7278         Returns
7279         -------
7280         differentiated: Dataset
7281 
7282         See also
7283         --------
7284         numpy.gradient: corresponding numpy function
7285         """
7286         from xarray.core.variable import Variable
7287 
7288         if coord not in self.variables and coord not in self.dims:
7289             raise ValueError(f"Coordinate {coord} does not exist.")
7290 
7291         coord_var = self[coord].variable
7292         if coord_var.ndim != 1:
7293             raise ValueError(
7294                 "Coordinate {} must be 1 dimensional but is {}"
7295                 " dimensional".format(coord, coord_var.ndim)
7296             )
7297 
7298         dim = coord_var.dims[0]
7299         if _contains_datetime_like_objects(coord_var):
7300             if coord_var.dtype.kind in "mM" and datetime_unit is None:
7301                 datetime_unit = cast(
7302                     "DatetimeUnitOptions", np.datetime_data(coord_var.dtype)[0]
7303                 )
7304             elif datetime_unit is None:
7305                 datetime_unit = "s"  # Default to seconds for cftime objects
7306             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)
7307 
7308         variables = {}
7309         for k, v in self.variables.items():
7310             if k in self.data_vars and dim in v.dims and k not in self.coords:
7311                 if _contains_datetime_like_objects(v):
7312                     v = v._to_numeric(datetime_unit=datetime_unit)
7313                 grad = duck_array_ops.gradient(
7314                     v.data,
7315                     coord_var.data,
7316                     edge_order=edge_order,
7317                     axis=v.get_axis_num(dim),
7318                 )
7319                 variables[k] = Variable(v.dims, grad)
7320             else:
7321                 variables[k] = v
7322         return self._replace(variables)
7323 
7324     def integrate(
7325         self: T_Dataset,
7326         coord: Hashable | Sequence[Hashable],
7327         datetime_unit: DatetimeUnitOptions = None,
7328     ) -> T_Dataset:
7329         """Integrate along the given coordinate using the trapezoidal rule.
7330 
7331         .. note::
7332             This feature is limited to simple cartesian geometry, i.e. coord
7333             must be one dimensional.
7334 
7335         Parameters
7336         ----------
7337         coord : hashable, or sequence of hashable
7338             Coordinate(s) used for the integration.
7339         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7340                         'ps', 'fs', 'as', None}, optional
7341             Specify the unit if datetime coordinate is used.
7342 
7343         Returns
7344         -------
7345         integrated : Dataset
7346 
7347         See also
7348         --------
7349         DataArray.integrate
7350         numpy.trapz : corresponding numpy function
7351 
7352         Examples
7353         --------
7354         >>> ds = xr.Dataset(
7355         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7356         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7357         ... )
7358         >>> ds
7359         <xarray.Dataset>
7360         Dimensions:  (x: 4)
7361         Coordinates:
7362           * x        (x) int64 0 1 2 3
7363             y        (x) int64 1 7 3 5
7364         Data variables:
7365             a        (x) int64 5 5 6 6
7366             b        (x) int64 1 2 1 0
7367         >>> ds.integrate("x")
7368         <xarray.Dataset>
7369         Dimensions:  ()
7370         Data variables:
7371             a        float64 16.5
7372             b        float64 3.5
7373         >>> ds.integrate("y")
7374         <xarray.Dataset>
7375         Dimensions:  ()
7376         Data variables:
7377             a        float64 20.0
7378             b        float64 4.0
7379         """
7380         if not isinstance(coord, (list, tuple)):
7381             coord = (coord,)
7382         result = self
7383         for c in coord:
7384             result = result._integrate_one(c, datetime_unit=datetime_unit)
7385         return result
7386 
7387     def _integrate_one(self, coord, datetime_unit=None, cumulative=False):
7388         from xarray.core.variable import Variable
7389 
7390         if coord not in self.variables and coord not in self.dims:
7391             raise ValueError(f"Coordinate {coord} does not exist.")
7392 
7393         coord_var = self[coord].variable
7394         if coord_var.ndim != 1:
7395             raise ValueError(
7396                 "Coordinate {} must be 1 dimensional but is {}"
7397                 " dimensional".format(coord, coord_var.ndim)
7398             )
7399 
7400         dim = coord_var.dims[0]
7401         if _contains_datetime_like_objects(coord_var):
7402             if coord_var.dtype.kind in "mM" and datetime_unit is None:
7403                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
7404             elif datetime_unit is None:
7405                 datetime_unit = "s"  # Default to seconds for cftime objects
7406             coord_var = coord_var._replace(
7407                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)
7408             )
7409 
7410         variables = {}
7411         coord_names = set()
7412         for k, v in self.variables.items():
7413             if k in self.coords:
7414                 if dim not in v.dims or cumulative:
7415                     variables[k] = v
7416                     coord_names.add(k)
7417             else:
7418                 if k in self.data_vars and dim in v.dims:
7419                     if _contains_datetime_like_objects(v):
7420                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)
7421                     if cumulative:
7422                         integ = duck_array_ops.cumulative_trapezoid(
7423                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7424                         )
7425                         v_dims = v.dims
7426                     else:
7427                         integ = duck_array_ops.trapz(
7428                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7429                         )
7430                         v_dims = list(v.dims)
7431                         v_dims.remove(dim)
7432                     variables[k] = Variable(v_dims, integ)
7433                 else:
7434                     variables[k] = v
7435         indexes = {k: v for k, v in self._indexes.items() if k in variables}
7436         return self._replace_with_new_dims(
7437             variables, coord_names=coord_names, indexes=indexes
7438         )
7439 
7440     def cumulative_integrate(
7441         self: T_Dataset,
7442         coord: Hashable | Sequence[Hashable],
7443         datetime_unit: DatetimeUnitOptions = None,
7444     ) -> T_Dataset:
7445         """Integrate along the given coordinate using the trapezoidal rule.
7446 
7447         .. note::
7448             This feature is limited to simple cartesian geometry, i.e. coord
7449             must be one dimensional.
7450 
7451             The first entry of the cumulative integral of each variable is always 0, in
7452             order to keep the length of the dimension unchanged between input and
7453             output.
7454 
7455         Parameters
7456         ----------
7457         coord : hashable, or sequence of hashable
7458             Coordinate(s) used for the integration.
7459         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7460                         'ps', 'fs', 'as', None}, optional
7461             Specify the unit if datetime coordinate is used.
7462 
7463         Returns
7464         -------
7465         integrated : Dataset
7466 
7467         See also
7468         --------
7469         DataArray.cumulative_integrate
7470         scipy.integrate.cumulative_trapezoid : corresponding scipy function
7471 
7472         Examples
7473         --------
7474         >>> ds = xr.Dataset(
7475         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7476         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7477         ... )
7478         >>> ds
7479         <xarray.Dataset>
7480         Dimensions:  (x: 4)
7481         Coordinates:
7482           * x        (x) int64 0 1 2 3
7483             y        (x) int64 1 7 3 5
7484         Data variables:
7485             a        (x) int64 5 5 6 6
7486             b        (x) int64 1 2 1 0
7487         >>> ds.cumulative_integrate("x")
7488         <xarray.Dataset>
7489         Dimensions:  (x: 4)
7490         Coordinates:
7491           * x        (x) int64 0 1 2 3
7492             y        (x) int64 1 7 3 5
7493         Data variables:
7494             a        (x) float64 0.0 5.0 10.5 16.5
7495             b        (x) float64 0.0 1.5 3.0 3.5
7496         >>> ds.cumulative_integrate("y")
7497         <xarray.Dataset>
7498         Dimensions:  (x: 4)
7499         Coordinates:
7500           * x        (x) int64 0 1 2 3
7501             y        (x) int64 1 7 3 5
7502         Data variables:
7503             a        (x) float64 0.0 30.0 8.0 20.0
7504             b        (x) float64 0.0 9.0 3.0 4.0
7505         """
7506         if not isinstance(coord, (list, tuple)):
7507             coord = (coord,)
7508         result = self
7509         for c in coord:
7510             result = result._integrate_one(
7511                 c, datetime_unit=datetime_unit, cumulative=True
7512             )
7513         return result
7514 
7515     @property
7516     def real(self: T_Dataset) -> T_Dataset:
7517         """
7518         The real part of each data variable.
7519 
7520         See Also
7521         --------
7522         numpy.ndarray.real
7523         """
7524         return self.map(lambda x: x.real, keep_attrs=True)
7525 
7526     @property
7527     def imag(self: T_Dataset) -> T_Dataset:
7528         """
7529         The imaginary part of each data variable.
7530 
7531         See Also
7532         --------
7533         numpy.ndarray.imag
7534         """
7535         return self.map(lambda x: x.imag, keep_attrs=True)
7536 
7537     plot = utils.UncachedAccessor(DatasetPlotAccessor)
7538 
7539     def filter_by_attrs(self: T_Dataset, **kwargs) -> T_Dataset:
7540         """Returns a ``Dataset`` with variables that match specific conditions.
7541 
7542         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned
7543         containing only the variables for which all the filter tests pass.
7544         These tests are either ``key=value`` for which the attribute ``key``
7545         has the exact value ``value`` or the callable passed into
7546         ``key=callable`` returns True. The callable will be passed a single
7547         value, either the value of the attribute ``key`` or ``None`` if the
7548         DataArray does not have an attribute with the name ``key``.
7549 
7550         Parameters
7551         ----------
7552         **kwargs
7553             key : str
7554                 Attribute name.
7555             value : callable or obj
7556                 If value is a callable, it should return a boolean in the form
7557                 of bool = func(attr) where attr is da.attrs[key].
7558                 Otherwise, value will be compared to the each
7559                 DataArray's attrs[key].
7560 
7561         Returns
7562         -------
7563         new : Dataset
7564             New dataset with variables filtered by attribute.
7565 
7566         Examples
7567         --------
7568         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)
7569         >>> precip = 10 * np.random.rand(2, 2, 3)
7570         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
7571         >>> lat = [[42.25, 42.21], [42.63, 42.59]]
7572         >>> dims = ["x", "y", "time"]
7573         >>> temp_attr = dict(standard_name="air_potential_temperature")
7574         >>> precip_attr = dict(standard_name="convective_precipitation_flux")
7575 
7576         >>> ds = xr.Dataset(
7577         ...     dict(
7578         ...         temperature=(dims, temp, temp_attr),
7579         ...         precipitation=(dims, precip, precip_attr),
7580         ...     ),
7581         ...     coords=dict(
7582         ...         lon=(["x", "y"], lon),
7583         ...         lat=(["x", "y"], lat),
7584         ...         time=pd.date_range("2014-09-06", periods=3),
7585         ...         reference_time=pd.Timestamp("2014-09-05"),
7586         ...     ),
7587         ... )
7588 
7589         Get variables matching a specific standard_name:
7590 
7591         >>> ds.filter_by_attrs(standard_name="convective_precipitation_flux")
7592         <xarray.Dataset>
7593         Dimensions:         (x: 2, y: 2, time: 3)
7594         Coordinates:
7595             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7596             lat             (x, y) float64 42.25 42.21 42.63 42.59
7597           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7598             reference_time  datetime64[ns] 2014-09-05
7599         Dimensions without coordinates: x, y
7600         Data variables:
7601             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7602 
7603         Get all variables that have a standard_name attribute:
7604 
7605         >>> standard_name = lambda v: v is not None
7606         >>> ds.filter_by_attrs(standard_name=standard_name)
7607         <xarray.Dataset>
7608         Dimensions:         (x: 2, y: 2, time: 3)
7609         Coordinates:
7610             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7611             lat             (x, y) float64 42.25 42.21 42.63 42.59
7612           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7613             reference_time  datetime64[ns] 2014-09-05
7614         Dimensions without coordinates: x, y
7615         Data variables:
7616             temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
7617             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7618 
7619         """
7620         selection = []
7621         for var_name, variable in self.variables.items():
7622             has_value_flag = False
7623             for attr_name, pattern in kwargs.items():
7624                 attr_value = variable.attrs.get(attr_name)
7625                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:
7626                     has_value_flag = True
7627                 else:
7628                     has_value_flag = False
7629                     break
7630             if has_value_flag is True:
7631                 selection.append(var_name)
7632         return self[selection]
7633 
7634     def unify_chunks(self: T_Dataset) -> T_Dataset:
7635         """Unify chunk size along all chunked dimensions of this Dataset.
7636 
7637         Returns
7638         -------
7639         Dataset with consistent chunk sizes for all dask-array variables
7640 
7641         See Also
7642         --------
7643         dask.array.core.unify_chunks
7644         """
7645 
7646         return unify_chunks(self)[0]
7647 
7648     def map_blocks(
7649         self,
7650         func: Callable[..., T_Xarray],
7651         args: Sequence[Any] = (),
7652         kwargs: Mapping[str, Any] | None = None,
7653         template: DataArray | Dataset | None = None,
7654     ) -> T_Xarray:
7655         """
7656         Apply a function to each block of this Dataset.
7657 
7658         .. warning::
7659             This method is experimental and its signature may change.
7660 
7661         Parameters
7662         ----------
7663         func : callable
7664             User-provided function that accepts a Dataset as its first
7665             parameter. The function will receive a subset or 'block' of this Dataset (see below),
7666             corresponding to one chunk along each chunked dimension. ``func`` will be
7667             executed as ``func(subset_dataset, *subset_args, **kwargs)``.
7668 
7669             This function must return either a single DataArray or a single Dataset.
7670 
7671             This function cannot add a new chunked dimension.
7672         args : sequence
7673             Passed to func after unpacking and subsetting any xarray objects by blocks.
7674             xarray objects in args must be aligned with obj, otherwise an error is raised.
7675         kwargs : Mapping or None
7676             Passed verbatim to func after unpacking. xarray objects, if any, will not be
7677             subset to blocks. Passing dask collections in kwargs is not allowed.
7678         template : DataArray, Dataset or None, optional
7679             xarray object representing the final result after compute is called. If not provided,
7680             the function will be first run on mocked-up data, that looks like this object but
7681             has sizes 0, to determine properties of the returned object such as dtype,
7682             variable names, attributes, new dimensions and new indexes (if any).
7683             ``template`` must be provided if the function changes the size of existing dimensions.
7684             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
7685             ``attrs`` set by ``func`` will be ignored.
7686 
7687         Returns
7688         -------
7689         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
7690         function.
7691 
7692         Notes
7693         -----
7694         This function is designed for when ``func`` needs to manipulate a whole xarray object
7695         subset to each block. Each block is loaded into memory. In the more common case where
7696         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
7697 
7698         If none of the variables in this object is backed by dask arrays, calling this function is
7699         equivalent to calling ``func(obj, *args, **kwargs)``.
7700 
7701         See Also
7702         --------
7703         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
7704         xarray.DataArray.map_blocks
7705 
7706         Examples
7707         --------
7708         Calculate an anomaly from climatology using ``.groupby()``. Using
7709         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
7710         its indices, and its methods like ``.groupby()``.
7711 
7712         >>> def calculate_anomaly(da, groupby_type="time.month"):
7713         ...     gb = da.groupby(groupby_type)
7714         ...     clim = gb.mean(dim="time")
7715         ...     return gb - clim
7716         ...
7717         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
7718         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
7719         >>> np.random.seed(123)
7720         >>> array = xr.DataArray(
7721         ...     np.random.rand(len(time)),
7722         ...     dims=["time"],
7723         ...     coords={"time": time, "month": month},
7724         ... ).chunk()
7725         >>> ds = xr.Dataset({"a": array})
7726         >>> ds.map_blocks(calculate_anomaly, template=ds).compute()
7727         <xarray.Dataset>
7728         Dimensions:  (time: 24)
7729         Coordinates:
7730           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7731             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
7732         Data variables:
7733             a        (time) float64 0.1289 0.1132 -0.0856 ... 0.2287 0.1906 -0.05901
7734 
7735         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
7736         to the function being applied in ``xr.map_blocks()``:
7737 
7738         >>> ds.map_blocks(
7739         ...     calculate_anomaly,
7740         ...     kwargs={"groupby_type": "time.year"},
7741         ...     template=ds,
7742         ... )
7743         <xarray.Dataset>
7744         Dimensions:  (time: 24)
7745         Coordinates:
7746           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7747             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
7748         Data variables:
7749             a        (time) float64 dask.array<chunksize=(24,), meta=np.ndarray>
7750         """
7751         from xarray.core.parallel import map_blocks
7752 
7753         return map_blocks(func, self, args, kwargs, template)
7754 
7755     def polyfit(
7756         self: T_Dataset,
7757         dim: Hashable,
7758         deg: int,
7759         skipna: bool | None = None,
7760         rcond: float | None = None,
7761         w: Hashable | Any = None,
7762         full: bool = False,
7763         cov: bool | Literal["unscaled"] = False,
7764     ) -> T_Dataset:
7765         """
7766         Least squares polynomial fit.
7767 
7768         This replicates the behaviour of `numpy.polyfit` but differs by skipping
7769         invalid values when `skipna = True`.
7770 
7771         Parameters
7772         ----------
7773         dim : hashable
7774             Coordinate along which to fit the polynomials.
7775         deg : int
7776             Degree of the fitting polynomial.
7777         skipna : bool or None, optional
7778             If True, removes all invalid values before fitting each 1D slices of the array.
7779             Default is True if data is stored in a dask.array or if there is any
7780             invalid values, False otherwise.
7781         rcond : float or None, optional
7782             Relative condition number to the fit.
7783         w : hashable or Any, optional
7784             Weights to apply to the y-coordinate of the sample points.
7785             Can be an array-like object or the name of a coordinate in the dataset.
7786         full : bool, default: False
7787             Whether to return the residuals, matrix rank and singular values in addition
7788             to the coefficients.
7789         cov : bool or "unscaled", default: False
7790             Whether to return to the covariance matrix in addition to the coefficients.
7791             The matrix is not scaled if `cov='unscaled'`.
7792 
7793         Returns
7794         -------
7795         polyfit_results : Dataset
7796             A single dataset which contains (for each "var" in the input dataset):
7797 
7798             [var]_polyfit_coefficients
7799                 The coefficients of the best fit for each variable in this dataset.
7800             [var]_polyfit_residuals
7801                 The residuals of the least-square computation for each variable (only included if `full=True`)
7802                 When the matrix rank is deficient, np.nan is returned.
7803             [dim]_matrix_rank
7804                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7805                 The rank is computed ignoring the NaN values that might be skipped.
7806             [dim]_singular_values
7807                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7808             [var]_polyfit_covariance
7809                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
7810 
7811         Warns
7812         -----
7813         RankWarning
7814             The rank of the coefficient matrix in the least-squares fit is deficient.
7815             The warning is not raised with in-memory (not dask) data and `full=True`.
7816 
7817         See Also
7818         --------
7819         numpy.polyfit
7820         numpy.polyval
7821         xarray.polyval
7822         """
7823         from xarray.core.dataarray import DataArray
7824 
7825         variables = {}
7826         skipna_da = skipna
7827 
7828         x = get_clean_interp_index(self, dim, strict=False)
7829         xname = f"{self[dim].name}_"
7830         order = int(deg) + 1
7831         lhs = np.vander(x, order)
7832 
7833         if rcond is None:
7834             rcond = (
7835                 x.shape[0] * np.core.finfo(x.dtype).eps  # type: ignore[attr-defined]
7836             )
7837 
7838         # Weights:
7839         if w is not None:
7840             if isinstance(w, Hashable):
7841                 w = self.coords[w]
7842             w = np.asarray(w)
7843             if w.ndim != 1:
7844                 raise TypeError("Expected a 1-d array for weights.")
7845             if w.shape[0] != lhs.shape[0]:
7846                 raise TypeError(f"Expected w and {dim} to have the same length")
7847             lhs *= w[:, np.newaxis]
7848 
7849         # Scaling
7850         scale = np.sqrt((lhs * lhs).sum(axis=0))
7851         lhs /= scale
7852 
7853         degree_dim = utils.get_temp_dimname(self.dims, "degree")
7854 
7855         rank = np.linalg.matrix_rank(lhs)
7856 
7857         if full:
7858             rank = DataArray(rank, name=xname + "matrix_rank")
7859             variables[rank.name] = rank
7860             _sing = np.linalg.svd(lhs, compute_uv=False)
7861             sing = DataArray(
7862                 _sing,
7863                 dims=(degree_dim,),
7864                 coords={degree_dim: np.arange(rank - 1, -1, -1)},
7865                 name=xname + "singular_values",
7866             )
7867             variables[sing.name] = sing
7868 
7869         for name, da in self.data_vars.items():
7870             if dim not in da.dims:
7871                 continue
7872 
7873             if is_duck_dask_array(da.data) and (
7874                 rank != order or full or skipna is None
7875             ):
7876                 # Current algorithm with dask and skipna=False neither supports
7877                 # deficient ranks nor does it output the "full" info (issue dask/dask#6516)
7878                 skipna_da = True
7879             elif skipna is None:
7880                 skipna_da = bool(np.any(da.isnull()))
7881 
7882             dims_to_stack = [dimname for dimname in da.dims if dimname != dim]
7883             stacked_coords: dict[Hashable, DataArray] = {}
7884             if dims_to_stack:
7885                 stacked_dim = utils.get_temp_dimname(dims_to_stack, "stacked")
7886                 rhs = da.transpose(dim, *dims_to_stack).stack(
7887                     {stacked_dim: dims_to_stack}
7888                 )
7889                 stacked_coords = {stacked_dim: rhs[stacked_dim]}
7890                 scale_da = scale[:, np.newaxis]
7891             else:
7892                 rhs = da
7893                 scale_da = scale
7894 
7895             if w is not None:
7896                 rhs *= w[:, np.newaxis]
7897 
7898             with warnings.catch_warnings():
7899                 if full:  # Copy np.polyfit behavior
7900                     warnings.simplefilter("ignore", np.RankWarning)
7901                 else:  # Raise only once per variable
7902                     warnings.simplefilter("once", np.RankWarning)
7903 
7904                 coeffs, residuals = duck_array_ops.least_squares(
7905                     lhs, rhs.data, rcond=rcond, skipna=skipna_da
7906                 )
7907 
7908             if isinstance(name, str):
7909                 name = f"{name}_"
7910             else:
7911                 # Thus a ReprObject => polyfit was called on a DataArray
7912                 name = ""
7913 
7914             coeffs = DataArray(
7915                 coeffs / scale_da,
7916                 dims=[degree_dim] + list(stacked_coords.keys()),
7917                 coords={degree_dim: np.arange(order)[::-1], **stacked_coords},
7918                 name=name + "polyfit_coefficients",
7919             )
7920             if dims_to_stack:
7921                 coeffs = coeffs.unstack(stacked_dim)
7922             variables[coeffs.name] = coeffs
7923 
7924             if full or (cov is True):
7925                 residuals = DataArray(
7926                     residuals if dims_to_stack else residuals.squeeze(),
7927                     dims=list(stacked_coords.keys()),
7928                     coords=stacked_coords,
7929                     name=name + "polyfit_residuals",
7930                 )
7931                 if dims_to_stack:
7932                     residuals = residuals.unstack(stacked_dim)
7933                 variables[residuals.name] = residuals
7934 
7935             if cov:
7936                 Vbase = np.linalg.inv(np.dot(lhs.T, lhs))
7937                 Vbase /= np.outer(scale, scale)
7938                 if cov == "unscaled":
7939                     fac = 1
7940                 else:
7941                     if x.shape[0] <= order:
7942                         raise ValueError(
7943                             "The number of data points must exceed order to scale the covariance matrix."
7944                         )
7945                     fac = residuals / (x.shape[0] - order)
7946                 covariance = DataArray(Vbase, dims=("cov_i", "cov_j")) * fac
7947                 variables[name + "polyfit_covariance"] = covariance
7948 
7949         return type(self)(data_vars=variables, attrs=self.attrs.copy())
7950 
7951     def pad(
7952         self: T_Dataset,
7953         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,
7954         mode: PadModeOptions = "constant",
7955         stat_length: int
7956         | tuple[int, int]
7957         | Mapping[Any, tuple[int, int]]
7958         | None = None,
7959         constant_values: (
7960             float | tuple[float, float] | Mapping[Any, tuple[float, float]] | None
7961         ) = None,
7962         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
7963         reflect_type: PadReflectOptions = None,
7964         keep_attrs: bool | None = None,
7965         **pad_width_kwargs: Any,
7966     ) -> T_Dataset:
7967         """Pad this dataset along one or more dimensions.
7968 
7969         .. warning::
7970             This function is experimental and its behaviour is likely to change
7971             especially regarding padding of dimension coordinates (or IndexVariables).
7972 
7973         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
7974         coordinates will be padded with the same mode, otherwise coordinates
7975         are padded using the "constant" mode with fill_value dtypes.NA.
7976 
7977         Parameters
7978         ----------
7979         pad_width : mapping of hashable to tuple of int
7980             Mapping with the form of {dim: (pad_before, pad_after)}
7981             describing the number of values padded along each dimension.
7982             {dim: pad} is a shortcut for pad_before = pad_after = pad
7983         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
7984             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
7985             How to pad the DataArray (taken from numpy docs):
7986 
7987             - "constant": Pads with a constant value.
7988             - "edge": Pads with the edge values of array.
7989             - "linear_ramp": Pads with the linear ramp between end_value and the
7990               array edge value.
7991             - "maximum": Pads with the maximum value of all or part of the
7992               vector along each axis.
7993             - "mean": Pads with the mean value of all or part of the
7994               vector along each axis.
7995             - "median": Pads with the median value of all or part of the
7996               vector along each axis.
7997             - "minimum": Pads with the minimum value of all or part of the
7998               vector along each axis.
7999             - "reflect": Pads with the reflection of the vector mirrored on
8000               the first and last values of the vector along each axis.
8001             - "symmetric": Pads with the reflection of the vector mirrored
8002               along the edge of the array.
8003             - "wrap": Pads with the wrap of the vector along the axis.
8004               The first values are used to pad the end and the
8005               end values are used to pad the beginning.
8006 
8007         stat_length : int, tuple or mapping of hashable to tuple, default: None
8008             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
8009             values at edge of each axis used to calculate the statistic value.
8010             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
8011             statistic lengths along each dimension.
8012             ((before, after),) yields same before and after statistic lengths
8013             for each dimension.
8014             (stat_length,) or int is a shortcut for before = after = statistic
8015             length for all axes.
8016             Default is ``None``, to use the entire axis.
8017         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
8018             Used in 'constant'.  The values to set the padded values for each
8019             axis.
8020             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
8021             pad constants along each dimension.
8022             ``((before, after),)`` yields same before and after constants for each
8023             dimension.
8024             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
8025             all dimensions.
8026             Default is 0.
8027         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
8028             Used in 'linear_ramp'.  The values used for the ending value of the
8029             linear_ramp and that will form the edge of the padded array.
8030             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
8031             end values along each dimension.
8032             ``((before, after),)`` yields same before and after end values for each
8033             axis.
8034             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
8035             all axes.
8036             Default is 0.
8037         reflect_type : {"even", "odd", None}, optional
8038             Used in "reflect", and "symmetric".  The "even" style is the
8039             default with an unaltered reflection around the edge value.  For
8040             the "odd" style, the extended part of the array is created by
8041             subtracting the reflected values from two times the edge value.
8042         keep_attrs : bool or None, optional
8043             If True, the attributes (``attrs``) will be copied from the
8044             original object to the new one. If False, the new object
8045             will be returned without attributes.
8046         **pad_width_kwargs
8047             The keyword arguments form of ``pad_width``.
8048             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
8049 
8050         Returns
8051         -------
8052         padded : Dataset
8053             Dataset with the padded coordinates and data.
8054 
8055         See Also
8056         --------
8057         Dataset.shift, Dataset.roll, Dataset.bfill, Dataset.ffill, numpy.pad, dask.array.pad
8058 
8059         Notes
8060         -----
8061         By default when ``mode="constant"`` and ``constant_values=None``, integer types will be
8062         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion
8063         specify ``constant_values=np.nan``
8064 
8065         Padding coordinates will drop their corresponding index (if any) and will reset default
8066         indexes for dimension coordinates.
8067 
8068         Examples
8069         --------
8070         >>> ds = xr.Dataset({"foo": ("x", range(5))})
8071         >>> ds.pad(x=(1, 2))
8072         <xarray.Dataset>
8073         Dimensions:  (x: 8)
8074         Dimensions without coordinates: x
8075         Data variables:
8076             foo      (x) float64 nan 0.0 1.0 2.0 3.0 4.0 nan nan
8077         """
8078         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, "pad")
8079 
8080         if mode in ("edge", "reflect", "symmetric", "wrap"):
8081             coord_pad_mode = mode
8082             coord_pad_options = {
8083                 "stat_length": stat_length,
8084                 "constant_values": constant_values,
8085                 "end_values": end_values,
8086                 "reflect_type": reflect_type,
8087             }
8088         else:
8089             coord_pad_mode = "constant"
8090             coord_pad_options = {}
8091 
8092         if keep_attrs is None:
8093             keep_attrs = _get_keep_attrs(default=True)
8094 
8095         variables = {}
8096 
8097         # keep indexes that won't be affected by pad and drop all other indexes
8098         xindexes = self.xindexes
8099         pad_dims = set(pad_width)
8100         indexes = {}
8101         for k, idx in xindexes.items():
8102             if not pad_dims.intersection(xindexes.get_all_dims(k)):
8103                 indexes[k] = idx
8104 
8105         for name, var in self.variables.items():
8106             var_pad_width = {k: v for k, v in pad_width.items() if k in var.dims}
8107             if not var_pad_width:
8108                 variables[name] = var
8109             elif name in self.data_vars:
8110                 variables[name] = var.pad(
8111                     pad_width=var_pad_width,
8112                     mode=mode,
8113                     stat_length=stat_length,
8114                     constant_values=constant_values,
8115                     end_values=end_values,
8116                     reflect_type=reflect_type,
8117                     keep_attrs=keep_attrs,
8118                 )
8119             else:
8120                 variables[name] = var.pad(
8121                     pad_width=var_pad_width,
8122                     mode=coord_pad_mode,
8123                     keep_attrs=keep_attrs,
8124                     **coord_pad_options,  # type: ignore[arg-type]
8125                 )
8126                 # reset default index of dimension coordinates
8127                 if (name,) == var.dims:
8128                     dim_var = {name: variables[name]}
8129                     index = PandasIndex.from_variables(dim_var, options={})
8130                     index_vars = index.create_variables(dim_var)
8131                     indexes[name] = index
8132                     variables[name] = index_vars[name]
8133 
8134         attrs = self._attrs if keep_attrs else None
8135         return self._replace_with_new_dims(variables, indexes=indexes, attrs=attrs)
8136 
8137     def idxmin(
8138         self: T_Dataset,
8139         dim: Hashable | None = None,
8140         skipna: bool | None = None,
8141         fill_value: Any = xrdtypes.NA,
8142         keep_attrs: bool | None = None,
8143     ) -> T_Dataset:
8144         """Return the coordinate label of the minimum value along a dimension.
8145 
8146         Returns a new `Dataset` named after the dimension with the values of
8147         the coordinate labels along that dimension corresponding to minimum
8148         values along that dimension.
8149 
8150         In comparison to :py:meth:`~Dataset.argmin`, this returns the
8151         coordinate label while :py:meth:`~Dataset.argmin` returns the index.
8152 
8153         Parameters
8154         ----------
8155         dim : Hashable, optional
8156             Dimension over which to apply `idxmin`.  This is optional for 1D
8157             variables, but required for variables with 2 or more dimensions.
8158         skipna : bool or None, optional
8159             If True, skip missing values (as marked by NaN). By default, only
8160             skips missing values for ``float``, ``complex``, and ``object``
8161             dtypes; other dtypes either do not have a sentinel missing value
8162             (``int``) or ``skipna=True`` has not been implemented
8163             (``datetime64`` or ``timedelta64``).
8164         fill_value : Any, default: NaN
8165             Value to be filled in case all of the values along a dimension are
8166             null.  By default this is NaN.  The fill value and result are
8167             automatically converted to a compatible dtype if possible.
8168             Ignored if ``skipna`` is False.
8169         keep_attrs : bool or None, optional
8170             If True, the attributes (``attrs``) will be copied from the
8171             original object to the new one. If False, the new object
8172             will be returned without attributes.
8173 
8174         Returns
8175         -------
8176         reduced : Dataset
8177             New `Dataset` object with `idxmin` applied to its data and the
8178             indicated dimension removed.
8179 
8180         See Also
8181         --------
8182         DataArray.idxmin, Dataset.idxmax, Dataset.min, Dataset.argmin
8183 
8184         Examples
8185         --------
8186         >>> array1 = xr.DataArray(
8187         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
8188         ... )
8189         >>> array2 = xr.DataArray(
8190         ...     [
8191         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
8192         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
8193         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
8194         ...     ],
8195         ...     dims=["y", "x"],
8196         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
8197         ... )
8198         >>> ds = xr.Dataset({"int": array1, "float": array2})
8199         >>> ds.min(dim="x")
8200         <xarray.Dataset>
8201         Dimensions:  (y: 3)
8202         Coordinates:
8203           * y        (y) int64 -1 0 1
8204         Data variables:
8205             int      int64 -2
8206             float    (y) float64 -2.0 -4.0 1.0
8207         >>> ds.argmin(dim="x")
8208         <xarray.Dataset>
8209         Dimensions:  (y: 3)
8210         Coordinates:
8211           * y        (y) int64 -1 0 1
8212         Data variables:
8213             int      int64 4
8214             float    (y) int64 4 0 2
8215         >>> ds.idxmin(dim="x")
8216         <xarray.Dataset>
8217         Dimensions:  (y: 3)
8218         Coordinates:
8219           * y        (y) int64 -1 0 1
8220         Data variables:
8221             int      <U1 'e'
8222             float    (y) object 'e' 'a' 'c'
8223         """
8224         return self.map(
8225             methodcaller(
8226                 "idxmin",
8227                 dim=dim,
8228                 skipna=skipna,
8229                 fill_value=fill_value,
8230                 keep_attrs=keep_attrs,
8231             )
8232         )
8233 
8234     def idxmax(
8235         self: T_Dataset,
8236         dim: Hashable | None = None,
8237         skipna: bool | None = None,
8238         fill_value: Any = xrdtypes.NA,
8239         keep_attrs: bool | None = None,
8240     ) -> T_Dataset:
8241         """Return the coordinate label of the maximum value along a dimension.
8242 
8243         Returns a new `Dataset` named after the dimension with the values of
8244         the coordinate labels along that dimension corresponding to maximum
8245         values along that dimension.
8246 
8247         In comparison to :py:meth:`~Dataset.argmax`, this returns the
8248         coordinate label while :py:meth:`~Dataset.argmax` returns the index.
8249 
8250         Parameters
8251         ----------
8252         dim : str, optional
8253             Dimension over which to apply `idxmax`.  This is optional for 1D
8254             variables, but required for variables with 2 or more dimensions.
8255         skipna : bool or None, optional
8256             If True, skip missing values (as marked by NaN). By default, only
8257             skips missing values for ``float``, ``complex``, and ``object``
8258             dtypes; other dtypes either do not have a sentinel missing value
8259             (``int``) or ``skipna=True`` has not been implemented
8260             (``datetime64`` or ``timedelta64``).
8261         fill_value : Any, default: NaN
8262             Value to be filled in case all of the values along a dimension are
8263             null.  By default this is NaN.  The fill value and result are
8264             automatically converted to a compatible dtype if possible.
8265             Ignored if ``skipna`` is False.
8266         keep_attrs : bool or None, optional
8267             If True, the attributes (``attrs``) will be copied from the
8268             original object to the new one. If False, the new object
8269             will be returned without attributes.
8270 
8271         Returns
8272         -------
8273         reduced : Dataset
8274             New `Dataset` object with `idxmax` applied to its data and the
8275             indicated dimension removed.
8276 
8277         See Also
8278         --------
8279         DataArray.idxmax, Dataset.idxmin, Dataset.max, Dataset.argmax
8280 
8281         Examples
8282         --------
8283         >>> array1 = xr.DataArray(
8284         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
8285         ... )
8286         >>> array2 = xr.DataArray(
8287         ...     [
8288         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
8289         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
8290         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
8291         ...     ],
8292         ...     dims=["y", "x"],
8293         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
8294         ... )
8295         >>> ds = xr.Dataset({"int": array1, "float": array2})
8296         >>> ds.max(dim="x")
8297         <xarray.Dataset>
8298         Dimensions:  (y: 3)
8299         Coordinates:
8300           * y        (y) int64 -1 0 1
8301         Data variables:
8302             int      int64 2
8303             float    (y) float64 2.0 2.0 1.0
8304         >>> ds.argmax(dim="x")
8305         <xarray.Dataset>
8306         Dimensions:  (y: 3)
8307         Coordinates:
8308           * y        (y) int64 -1 0 1
8309         Data variables:
8310             int      int64 1
8311             float    (y) int64 0 2 2
8312         >>> ds.idxmax(dim="x")
8313         <xarray.Dataset>
8314         Dimensions:  (y: 3)
8315         Coordinates:
8316           * y        (y) int64 -1 0 1
8317         Data variables:
8318             int      <U1 'b'
8319             float    (y) object 'a' 'c' 'c'
8320         """
8321         return self.map(
8322             methodcaller(
8323                 "idxmax",
8324                 dim=dim,
8325                 skipna=skipna,
8326                 fill_value=fill_value,
8327                 keep_attrs=keep_attrs,
8328             )
8329         )
8330 
8331     def argmin(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
8332         """Indices of the minima of the member variables.
8333 
8334         If there are multiple minima, the indices of the first one found will be
8335         returned.
8336 
8337         Parameters
8338         ----------
8339         dim : Hashable, optional
8340             The dimension over which to find the minimum. By default, finds minimum over
8341             all dimensions - for now returning an int for backward compatibility, but
8342             this is deprecated, in future will be an error, since DataArray.argmin will
8343             return a dict with indices for all dimensions, which does not make sense for
8344             a Dataset.
8345         keep_attrs : bool, optional
8346             If True, the attributes (`attrs`) will be copied from the original
8347             object to the new one.  If False (default), the new object will be
8348             returned without attributes.
8349         skipna : bool, optional
8350             If True, skip missing values (as marked by NaN). By default, only
8351             skips missing values for float dtypes; other dtypes either do not
8352             have a sentinel missing value (int) or skipna=True has not been
8353             implemented (object, datetime64 or timedelta64).
8354 
8355         Returns
8356         -------
8357         result : Dataset
8358 
8359         See Also
8360         --------
8361         DataArray.argmin
8362         """
8363         if dim is None:
8364             warnings.warn(
8365                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8366                 "dim changes to return a dict of indices of each dimension, for "
8367                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8368                 "since we don't return a dict of Datasets.",
8369                 DeprecationWarning,
8370                 stacklevel=2,
8371             )
8372         if (
8373             dim is None
8374             or (not isinstance(dim, Sequence) and dim is not ...)
8375             or isinstance(dim, str)
8376         ):
8377             # Return int index if single dimension is passed, and is not part of a
8378             # sequence
8379             argmin_func = getattr(duck_array_ops, "argmin")
8380             return self.reduce(
8381                 argmin_func, dim=None if dim is None else [dim], **kwargs
8382             )
8383         else:
8384             raise ValueError(
8385                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8386                 "dicts cannot be contained in a Dataset, so cannot call "
8387                 "Dataset.argmin() with a sequence or ... for dim"
8388             )
8389 
8390     def argmax(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
8391         """Indices of the maxima of the member variables.
8392 
8393         If there are multiple maxima, the indices of the first one found will be
8394         returned.
8395 
8396         Parameters
8397         ----------
8398         dim : str, optional
8399             The dimension over which to find the maximum. By default, finds maximum over
8400             all dimensions - for now returning an int for backward compatibility, but
8401             this is deprecated, in future will be an error, since DataArray.argmax will
8402             return a dict with indices for all dimensions, which does not make sense for
8403             a Dataset.
8404         keep_attrs : bool, optional
8405             If True, the attributes (`attrs`) will be copied from the original
8406             object to the new one.  If False (default), the new object will be
8407             returned without attributes.
8408         skipna : bool, optional
8409             If True, skip missing values (as marked by NaN). By default, only
8410             skips missing values for float dtypes; other dtypes either do not
8411             have a sentinel missing value (int) or skipna=True has not been
8412             implemented (object, datetime64 or timedelta64).
8413 
8414         Returns
8415         -------
8416         result : Dataset
8417 
8418         See Also
8419         --------
8420         DataArray.argmax
8421 
8422         """
8423         if dim is None:
8424             warnings.warn(
8425                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8426                 "dim changes to return a dict of indices of each dimension, for "
8427                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8428                 "since we don't return a dict of Datasets.",
8429                 DeprecationWarning,
8430                 stacklevel=2,
8431             )
8432         if (
8433             dim is None
8434             or (not isinstance(dim, Sequence) and dim is not ...)
8435             or isinstance(dim, str)
8436         ):
8437             # Return int index if single dimension is passed, and is not part of a
8438             # sequence
8439             argmax_func = getattr(duck_array_ops, "argmax")
8440             return self.reduce(
8441                 argmax_func, dim=None if dim is None else [dim], **kwargs
8442             )
8443         else:
8444             raise ValueError(
8445                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8446                 "dicts cannot be contained in a Dataset, so cannot call "
8447                 "Dataset.argmin() with a sequence or ... for dim"
8448             )
8449 
8450     def query(
8451         self: T_Dataset,
8452         queries: Mapping[Any, Any] | None = None,
8453         parser: QueryParserOptions = "pandas",
8454         engine: QueryEngineOptions = None,
8455         missing_dims: ErrorOptionsWithWarn = "raise",
8456         **queries_kwargs: Any,
8457     ) -> T_Dataset:
8458         """Return a new dataset with each array indexed along the specified
8459         dimension(s), where the indexers are given as strings containing
8460         Python expressions to be evaluated against the data variables in the
8461         dataset.
8462 
8463         Parameters
8464         ----------
8465         queries : dict-like, optional
8466             A dict-like with keys matching dimensions and values given by strings
8467             containing Python expressions to be evaluated against the data variables
8468             in the dataset. The expressions will be evaluated using the pandas
8469             eval() function, and can contain any valid Python expressions but cannot
8470             contain any Python statements.
8471         parser : {"pandas", "python"}, default: "pandas"
8472             The parser to use to construct the syntax tree from the expression.
8473             The default of 'pandas' parses code slightly different than standard
8474             Python. Alternatively, you can parse an expression using the 'python'
8475             parser to retain strict Python semantics.
8476         engine : {"python", "numexpr", None}, default: None
8477             The engine used to evaluate the expression. Supported engines are:
8478 
8479             - None: tries to use numexpr, falls back to python
8480             - "numexpr": evaluates expressions using numexpr
8481             - "python": performs operations as if you had eval’d in top level python
8482 
8483         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
8484             What to do if dimensions that should be selected from are not present in the
8485             Dataset:
8486 
8487             - "raise": raise an exception
8488             - "warn": raise a warning, and ignore the missing dimensions
8489             - "ignore": ignore the missing dimensions
8490 
8491         **queries_kwargs : {dim: query, ...}, optional
8492             The keyword arguments form of ``queries``.
8493             One of queries or queries_kwargs must be provided.
8494 
8495         Returns
8496         -------
8497         obj : Dataset
8498             A new Dataset with the same contents as this dataset, except each
8499             array and dimension is indexed by the results of the appropriate
8500             queries.
8501 
8502         See Also
8503         --------
8504         Dataset.isel
8505         pandas.eval
8506 
8507         Examples
8508         --------
8509         >>> a = np.arange(0, 5, 1)
8510         >>> b = np.linspace(0, 1, 5)
8511         >>> ds = xr.Dataset({"a": ("x", a), "b": ("x", b)})
8512         >>> ds
8513         <xarray.Dataset>
8514         Dimensions:  (x: 5)
8515         Dimensions without coordinates: x
8516         Data variables:
8517             a        (x) int64 0 1 2 3 4
8518             b        (x) float64 0.0 0.25 0.5 0.75 1.0
8519         >>> ds.query(x="a > 2")
8520         <xarray.Dataset>
8521         Dimensions:  (x: 2)
8522         Dimensions without coordinates: x
8523         Data variables:
8524             a        (x) int64 3 4
8525             b        (x) float64 0.75 1.0
8526         """
8527 
8528         # allow queries to be given either as a dict or as kwargs
8529         queries = either_dict_or_kwargs(queries, queries_kwargs, "query")
8530 
8531         # check queries
8532         for dim, expr in queries.items():
8533             if not isinstance(expr, str):
8534                 msg = f"expr for dim {dim} must be a string to be evaluated, {type(expr)} given"
8535                 raise ValueError(msg)
8536 
8537         # evaluate the queries to create the indexers
8538         indexers = {
8539             dim: pd.eval(expr, resolvers=[self], parser=parser, engine=engine)
8540             for dim, expr in queries.items()
8541         }
8542 
8543         # apply the selection
8544         return self.isel(indexers, missing_dims=missing_dims)
8545 
8546     def curvefit(
8547         self: T_Dataset,
8548         coords: str | DataArray | Iterable[str | DataArray],
8549         func: Callable[..., Any],
8550         reduce_dims: Dims = None,
8551         skipna: bool = True,
8552         p0: dict[str, Any] | None = None,
8553         bounds: dict[str, Any] | None = None,
8554         param_names: Sequence[str] | None = None,
8555         kwargs: dict[str, Any] | None = None,
8556     ) -> T_Dataset:
8557         """
8558         Curve fitting optimization for arbitrary functions.
8559 
8560         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
8561 
8562         Parameters
8563         ----------
8564         coords : hashable, DataArray, or sequence of hashable or DataArray
8565             Independent coordinate(s) over which to perform the curve fitting. Must share
8566             at least one dimension with the calling object. When fitting multi-dimensional
8567             functions, supply `coords` as a sequence in the same order as arguments in
8568             `func`. To fit along existing dimensions of the calling object, `coords` can
8569             also be specified as a str or sequence of strs.
8570         func : callable
8571             User specified function in the form `f(x, *params)` which returns a numpy
8572             array of length `len(x)`. `params` are the fittable parameters which are optimized
8573             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
8574             coordinates, e.g. `f((x0, x1), *params)`.
8575         reduce_dims : str, Iterable of Hashable or None, optional
8576             Additional dimension(s) over which to aggregate while fitting. For example,
8577             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
8578             aggregate all lat and lon points and fit the specified function along the
8579             time dimension.
8580         skipna : bool, default: True
8581             Whether to skip missing values when fitting. Default is True.
8582         p0 : dict-like, optional
8583             Optional dictionary of parameter names to initial guesses passed to the
8584             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
8585             be assigned initial values following the default scipy behavior.
8586         bounds : dict-like, optional
8587             Optional dictionary of parameter names to bounding values passed to the
8588             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
8589             will be unbounded following the default scipy behavior.
8590         param_names : sequence of hashable, optional
8591             Sequence of names for the fittable parameters of `func`. If not supplied,
8592             this will be automatically determined by arguments of `func`. `param_names`
8593             should be manually supplied when fitting a function that takes a variable
8594             number of parameters.
8595         **kwargs : optional
8596             Additional keyword arguments to passed to scipy curve_fit.
8597 
8598         Returns
8599         -------
8600         curvefit_results : Dataset
8601             A single dataset which contains:
8602 
8603             [var]_curvefit_coefficients
8604                 The coefficients of the best fit.
8605             [var]_curvefit_covariance
8606                 The covariance matrix of the coefficient estimates.
8607 
8608         See Also
8609         --------
8610         Dataset.polyfit
8611         scipy.optimize.curve_fit
8612         """
8613         from scipy.optimize import curve_fit
8614 
8615         from xarray.core.alignment import broadcast
8616         from xarray.core.computation import apply_ufunc
8617         from xarray.core.dataarray import _THIS_ARRAY, DataArray
8618 
8619         if p0 is None:
8620             p0 = {}
8621         if bounds is None:
8622             bounds = {}
8623         if kwargs is None:
8624             kwargs = {}
8625 
8626         reduce_dims_: list[Hashable]
8627         if not reduce_dims:
8628             reduce_dims_ = []
8629         elif isinstance(reduce_dims, str) or not isinstance(reduce_dims, Iterable):
8630             reduce_dims_ = [reduce_dims]
8631         else:
8632             reduce_dims_ = list(reduce_dims)
8633 
8634         if (
8635             isinstance(coords, str)
8636             or isinstance(coords, DataArray)
8637             or not isinstance(coords, Iterable)
8638         ):
8639             coords = [coords]
8640         coords_: Sequence[DataArray] = [
8641             self[coord] if isinstance(coord, str) else coord for coord in coords
8642         ]
8643 
8644         # Determine whether any coords are dims on self
8645         for coord in coords_:
8646             reduce_dims_ += [c for c in self.dims if coord.equals(self[c])]
8647         reduce_dims_ = list(set(reduce_dims_))
8648         preserved_dims = list(set(self.dims) - set(reduce_dims_))
8649         if not reduce_dims_:
8650             raise ValueError(
8651                 "No arguments to `coords` were identified as a dimension on the calling "
8652                 "object, and no dims were supplied to `reduce_dims`. This would result "
8653                 "in fitting on scalar data."
8654             )
8655 
8656         # Broadcast all coords with each other
8657         coords_ = broadcast(*coords_)
8658         coords_ = [
8659             coord.broadcast_like(self, exclude=preserved_dims) for coord in coords_
8660         ]
8661 
8662         params, func_args = _get_func_args(func, param_names)
8663         param_defaults, bounds_defaults = _initialize_curvefit_params(
8664             params, p0, bounds, func_args
8665         )
8666         n_params = len(params)
8667         kwargs.setdefault("p0", [param_defaults[p] for p in params])
8668         kwargs.setdefault(
8669             "bounds",
8670             [
8671                 [bounds_defaults[p][0] for p in params],
8672                 [bounds_defaults[p][1] for p in params],
8673             ],
8674         )
8675 
8676         def _wrapper(Y, *coords_, **kwargs):
8677             # Wrap curve_fit with raveled coordinates and pointwise NaN handling
8678             x = np.vstack([c.ravel() for c in coords_])
8679             y = Y.ravel()
8680             if skipna:
8681                 mask = np.all([np.any(~np.isnan(x), axis=0), ~np.isnan(y)], axis=0)
8682                 x = x[:, mask]
8683                 y = y[mask]
8684                 if not len(y):
8685                     popt = np.full([n_params], np.nan)
8686                     pcov = np.full([n_params, n_params], np.nan)
8687                     return popt, pcov
8688             x = np.squeeze(x)
8689             popt, pcov = curve_fit(func, x, y, **kwargs)
8690             return popt, pcov
8691 
8692         result = type(self)()
8693         for name, da in self.data_vars.items():
8694             if name is _THIS_ARRAY:
8695                 name = ""
8696             else:
8697                 name = f"{str(name)}_"
8698 
8699             popt, pcov = apply_ufunc(
8700                 _wrapper,
8701                 da,
8702                 *coords_,
8703                 vectorize=True,
8704                 dask="parallelized",
8705                 input_core_dims=[reduce_dims_ for d in range(len(coords_) + 1)],
8706                 output_core_dims=[["param"], ["cov_i", "cov_j"]],
8707                 dask_gufunc_kwargs={
8708                     "output_sizes": {
8709                         "param": n_params,
8710                         "cov_i": n_params,
8711                         "cov_j": n_params,
8712                     },
8713                 },
8714                 output_dtypes=(np.float64, np.float64),
8715                 exclude_dims=set(reduce_dims_),
8716                 kwargs=kwargs,
8717             )
8718             result[name + "curvefit_coefficients"] = popt
8719             result[name + "curvefit_covariance"] = pcov
8720 
8721         result = result.assign_coords(
8722             {"param": params, "cov_i": params, "cov_j": params}
8723         )
8724         result.attrs = self.attrs.copy()
8725 
8726         return result
8727 
8728     def drop_duplicates(
8729         self: T_Dataset,
8730         dim: Hashable | Iterable[Hashable],
8731         keep: Literal["first", "last", False] = "first",
8732     ) -> T_Dataset:
8733         """Returns a new Dataset with duplicate dimension values removed.
8734 
8735         Parameters
8736         ----------
8737         dim : dimension label or labels
8738             Pass `...` to drop duplicates along all dimensions.
8739         keep : {"first", "last", False}, default: "first"
8740             Determines which duplicates (if any) to keep.
8741             - ``"first"`` : Drop duplicates except for the first occurrence.
8742             - ``"last"`` : Drop duplicates except for the last occurrence.
8743             - False : Drop all duplicates.
8744 
8745         Returns
8746         -------
8747         Dataset
8748 
8749         See Also
8750         --------
8751         DataArray.drop_duplicates
8752         """
8753         if isinstance(dim, str):
8754             dims: Iterable = (dim,)
8755         elif dim is ...:
8756             dims = self.dims
8757         elif not isinstance(dim, Iterable):
8758             dims = [dim]
8759         else:
8760             dims = dim
8761 
8762         missing_dims = set(dims) - set(self.dims)
8763         if missing_dims:
8764             raise ValueError(f"'{missing_dims}' not found in dimensions")
8765 
8766         indexes = {dim: ~self.get_index(dim).duplicated(keep=keep) for dim in dims}
8767         return self.isel(indexes)
8768 
8769     def convert_calendar(
8770         self: T_Dataset,
8771         calendar: CFCalendar,
8772         dim: Hashable = "time",
8773         align_on: Literal["date", "year", None] = None,
8774         missing: Any | None = None,
8775         use_cftime: bool | None = None,
8776     ) -> T_Dataset:
8777         """Convert the Dataset to another calendar.
8778 
8779         Only converts the individual timestamps, does not modify any data except
8780         in dropping invalid/surplus dates or inserting missing dates.
8781 
8782         If the source and target calendars are either no_leap, all_leap or a
8783         standard type, only the type of the time array is modified.
8784         When converting to a leap year from a non-leap year, the 29th of February
8785         is removed from the array. In the other direction the 29th of February
8786         will be missing in the output, unless `missing` is specified,
8787         in which case that value is inserted.
8788 
8789         For conversions involving `360_day` calendars, see Notes.
8790 
8791         This method is safe to use with sub-daily data as it doesn't touch the
8792         time part of the timestamps.
8793 
8794         Parameters
8795         ---------
8796         calendar : str
8797             The target calendar name.
8798         dim : Hashable, default: "time"
8799             Name of the time coordinate.
8800         align_on : {None, 'date', 'year'}, optional
8801             Must be specified when either source or target is a `360_day` calendar,
8802             ignored otherwise. See Notes.
8803         missing : Any or None, optional
8804             By default, i.e. if the value is None, this method will simply attempt
8805             to convert the dates in the source calendar to the same dates in the
8806             target calendar, and drop any of those that are not possible to
8807             represent.  If a value is provided, a new time coordinate will be
8808             created in the target calendar with the same frequency as the original
8809             time coordinate; for any dates that are not present in the source, the
8810             data will be filled with this value.  Note that using this mode requires
8811             that the source data have an inferable frequency; for more information
8812             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
8813             target calendar combinations, this could result in many missing values, see notes.
8814         use_cftime : bool or None, optional
8815             Whether to use cftime objects in the output, only used if `calendar`
8816             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
8817             If True, the new time axis uses cftime objects.
8818             If None (default), it uses :py:class:`numpy.datetime64` values if the
8819             date range permits it, and :py:class:`cftime.datetime` objects if not.
8820             If False, it uses :py:class:`numpy.datetime64`  or fails.
8821 
8822         Returns
8823         -------
8824         Dataset
8825             Copy of the dataarray with the time coordinate converted to the
8826             target calendar. If 'missing' was None (default), invalid dates in
8827             the new calendar are dropped, but missing dates are not inserted.
8828             If `missing` was given, the new data is reindexed to have a time axis
8829             with the same frequency as the source, but in the new calendar; any
8830             missing datapoints are filled with `missing`.
8831 
8832         Notes
8833         -----
8834         Passing a value to `missing` is only usable if the source's time coordinate as an
8835         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
8836         if the target coordinate, generated from this frequency, has dates equivalent to the
8837         source. It is usually **not** appropriate to use this mode with:
8838 
8839         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
8840         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
8841             or 'mH' where 24 % m != 0).
8842 
8843         If one of the source or target calendars is `"360_day"`, `align_on` must
8844         be specified and two options are offered.
8845 
8846         - "year"
8847             The dates are translated according to their relative position in the year,
8848             ignoring their original month and day information, meaning that the
8849             missing/surplus days are added/removed at regular intervals.
8850 
8851             From a `360_day` to a standard calendar, the output will be missing the
8852             following dates (day of year in parentheses):
8853 
8854             To a leap year:
8855                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
8856                 September 31st (275) and November 30th (335).
8857             To a non-leap year:
8858                 February 6th (36), April 19th (109), July 2nd (183),
8859                 September 12th (255), November 25th (329).
8860 
8861             From a standard calendar to a `"360_day"`, the following dates in the
8862             source array will be dropped:
8863 
8864             From a leap year:
8865                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
8866                 September 31st (275), December 1st (336)
8867             From a non-leap year:
8868                 February 6th (37), April 20th (110), July 2nd (183),
8869                 September 13th (256), November 25th (329)
8870 
8871             This option is best used on daily and subdaily data.
8872 
8873         - "date"
8874             The month/day information is conserved and invalid dates are dropped
8875             from the output. This means that when converting from a `"360_day"` to a
8876             standard calendar, all 31st (Jan, March, May, July, August, October and
8877             December) will be missing as there is no equivalent dates in the
8878             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
8879             will be dropped as there are no equivalent dates in a standard calendar.
8880 
8881             This option is best used with data on a frequency coarser than daily.
8882         """
8883         return convert_calendar(
8884             self,
8885             calendar,
8886             dim=dim,
8887             align_on=align_on,
8888             missing=missing,
8889             use_cftime=use_cftime,
8890         )
8891 
8892     def interp_calendar(
8893         self: T_Dataset,
8894         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
8895         dim: Hashable = "time",
8896     ) -> T_Dataset:
8897         """Interpolates the Dataset to another calendar based on decimal year measure.
8898 
8899         Each timestamp in `source` and `target` are first converted to their decimal
8900         year equivalent then `source` is interpolated on the target coordinate.
8901         The decimal year of a timestamp is its year plus its sub-year component
8902         converted to the fraction of its year. For example "2000-03-01 12:00" is
8903         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
8904 
8905         This method should only be used when the time (HH:MM:SS) information of
8906         time coordinate is not important.
8907 
8908         Parameters
8909         ----------
8910         target: DataArray or DatetimeIndex or CFTimeIndex
8911             The target time coordinate of a valid dtype
8912             (np.datetime64 or cftime objects)
8913         dim : Hashable, default: "time"
8914             The time coordinate name.
8915 
8916         Return
8917         ------
8918         DataArray
8919             The source interpolated on the decimal years of target,
8920         """
8921         return interp_calendar(self, target, dim=dim)
8922 
8923     def groupby(
8924         self,
8925         group: Hashable | DataArray | IndexVariable,
8926         squeeze: bool = True,
8927         restore_coord_dims: bool = False,
8928     ) -> DatasetGroupBy:
8929         """Returns a DatasetGroupBy object for performing grouped operations.
8930 
8931         Parameters
8932         ----------
8933         group : Hashable, DataArray or IndexVariable
8934             Array whose unique values should be used to group this array. If a
8935             string, must be the name of a variable contained in this dataset.
8936         squeeze : bool, default: True
8937             If "group" is a dimension of any arrays in this dataset, `squeeze`
8938             controls whether the subarrays have a dimension of length 1 along
8939             that dimension or if the dimension is squeezed out.
8940         restore_coord_dims : bool, default: False
8941             If True, also restore the dimension order of multi-dimensional
8942             coordinates.
8943 
8944         Returns
8945         -------
8946         grouped : DatasetGroupBy
8947             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
8948             iterated over in the form of `(unique_value, grouped_array)` pairs.
8949 
8950         See Also
8951         --------
8952         :ref:`groupby`
8953             Users guide explanation of how to group and bin data.
8954         Dataset.groupby_bins
8955         DataArray.groupby
8956         core.groupby.DatasetGroupBy
8957         pandas.DataFrame.groupby
8958         Dataset.resample
8959         DataArray.resample
8960         """
8961         from xarray.core.groupby import (
8962             DatasetGroupBy,
8963             ResolvedUniqueGrouper,
8964             UniqueGrouper,
8965             _validate_groupby_squeeze,
8966         )
8967 
8968         _validate_groupby_squeeze(squeeze)
8969         rgrouper = ResolvedUniqueGrouper(UniqueGrouper(), group, self)
8970 
8971         return DatasetGroupBy(
8972             self,
8973             (rgrouper,),
8974             squeeze=squeeze,
8975             restore_coord_dims=restore_coord_dims,
8976         )
8977 
8978     def groupby_bins(
8979         self,
8980         group: Hashable | DataArray | IndexVariable,
8981         bins: ArrayLike,
8982         right: bool = True,
8983         labels: ArrayLike | None = None,
8984         precision: int = 3,
8985         include_lowest: bool = False,
8986         squeeze: bool = True,
8987         restore_coord_dims: bool = False,
8988     ) -> DatasetGroupBy:
8989         """Returns a DatasetGroupBy object for performing grouped operations.
8990 
8991         Rather than using all unique values of `group`, the values are discretized
8992         first by applying `pandas.cut` [1]_ to `group`.
8993 
8994         Parameters
8995         ----------
8996         group : Hashable, DataArray or IndexVariable
8997             Array whose binned values should be used to group this array. If a
8998             string, must be the name of a variable contained in this dataset.
8999         bins : int or array-like
9000             If bins is an int, it defines the number of equal-width bins in the
9001             range of x. However, in this case, the range of x is extended by .1%
9002             on each side to include the min or max values of x. If bins is a
9003             sequence it defines the bin edges allowing for non-uniform bin
9004             width. No extension of the range of x is done in this case.
9005         right : bool, default: True
9006             Indicates whether the bins include the rightmost edge or not. If
9007             right == True (the default), then the bins [1,2,3,4] indicate
9008             (1,2], (2,3], (3,4].
9009         labels : array-like or bool, default: None
9010             Used as labels for the resulting bins. Must be of the same length as
9011             the resulting bins. If False, string bin labels are assigned by
9012             `pandas.cut`.
9013         precision : int, default: 3
9014             The precision at which to store and display the bins labels.
9015         include_lowest : bool, default: False
9016             Whether the first interval should be left-inclusive or not.
9017         squeeze : bool, default: True
9018             If "group" is a dimension of any arrays in this dataset, `squeeze`
9019             controls whether the subarrays have a dimension of length 1 along
9020             that dimension or if the dimension is squeezed out.
9021         restore_coord_dims : bool, default: False
9022             If True, also restore the dimension order of multi-dimensional
9023             coordinates.
9024 
9025         Returns
9026         -------
9027         grouped : DatasetGroupBy
9028             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
9029             iterated over in the form of `(unique_value, grouped_array)` pairs.
9030             The name of the group has the added suffix `_bins` in order to
9031             distinguish it from the original variable.
9032 
9033         See Also
9034         --------
9035         :ref:`groupby`
9036             Users guide explanation of how to group and bin data.
9037         Dataset.groupby
9038         DataArray.groupby_bins
9039         core.groupby.DatasetGroupBy
9040         pandas.DataFrame.groupby
9041 
9042         References
9043         ----------
9044         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
9045         """
9046         from xarray.core.groupby import (
9047             BinGrouper,
9048             DatasetGroupBy,
9049             ResolvedBinGrouper,
9050             _validate_groupby_squeeze,
9051         )
9052 
9053         _validate_groupby_squeeze(squeeze)
9054         grouper = BinGrouper(
9055             bins=bins,
9056             cut_kwargs={
9057                 "right": right,
9058                 "labels": labels,
9059                 "precision": precision,
9060                 "include_lowest": include_lowest,
9061             },
9062         )
9063         rgrouper = ResolvedBinGrouper(grouper, group, self)
9064 
9065         return DatasetGroupBy(
9066             self,
9067             (rgrouper,),
9068             squeeze=squeeze,
9069             restore_coord_dims=restore_coord_dims,
9070         )
9071 
9072     def weighted(self, weights: DataArray) -> DatasetWeighted:
9073         """
9074         Weighted Dataset operations.
9075 
9076         Parameters
9077         ----------
9078         weights : DataArray
9079             An array of weights associated with the values in this Dataset.
9080             Each value in the data contributes to the reduction operation
9081             according to its associated weight.
9082 
9083         Notes
9084         -----
9085         ``weights`` must be a DataArray and cannot contain missing values.
9086         Missing values can be replaced by ``weights.fillna(0)``.
9087 
9088         Returns
9089         -------
9090         core.weighted.DatasetWeighted
9091 
9092         See Also
9093         --------
9094         DataArray.weighted
9095         """
9096         from xarray.core.weighted import DatasetWeighted
9097 
9098         return DatasetWeighted(self, weights)
9099 
9100     def rolling(
9101         self,
9102         dim: Mapping[Any, int] | None = None,
9103         min_periods: int | None = None,
9104         center: bool | Mapping[Any, bool] = False,
9105         **window_kwargs: int,
9106     ) -> DatasetRolling:
9107         """
9108         Rolling window object for Datasets.
9109 
9110         Parameters
9111         ----------
9112         dim : dict, optional
9113             Mapping from the dimension name to create the rolling iterator
9114             along (e.g. `time`) to its moving window size.
9115         min_periods : int or None, default: None
9116             Minimum number of observations in window required to have a value
9117             (otherwise result is NA). The default, None, is equivalent to
9118             setting min_periods equal to the size of the window.
9119         center : bool or Mapping to int, default: False
9120             Set the labels at the center of the window.
9121         **window_kwargs : optional
9122             The keyword arguments form of ``dim``.
9123             One of dim or window_kwargs must be provided.
9124 
9125         Returns
9126         -------
9127         core.rolling.DatasetRolling
9128 
9129         See Also
9130         --------
9131         core.rolling.DatasetRolling
9132         DataArray.rolling
9133         """
9134         from xarray.core.rolling import DatasetRolling
9135 
9136         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
9137         return DatasetRolling(self, dim, min_periods=min_periods, center=center)
9138 
9139     def coarsen(
9140         self,
9141         dim: Mapping[Any, int] | None = None,
9142         boundary: CoarsenBoundaryOptions = "exact",
9143         side: SideOptions | Mapping[Any, SideOptions] = "left",
9144         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
9145         **window_kwargs: int,
9146     ) -> DatasetCoarsen:
9147         """
9148         Coarsen object for Datasets.
9149 
9150         Parameters
9151         ----------
9152         dim : mapping of hashable to int, optional
9153             Mapping from the dimension name to the window size.
9154         boundary : {"exact", "trim", "pad"}, default: "exact"
9155             If 'exact', a ValueError will be raised if dimension size is not a
9156             multiple of the window size. If 'trim', the excess entries are
9157             dropped. If 'pad', NA will be padded.
9158         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
9159         coord_func : str or mapping of hashable to str, default: "mean"
9160             function (name) that is applied to the coordinates,
9161             or a mapping from coordinate name to function (name).
9162 
9163         Returns
9164         -------
9165         core.rolling.DatasetCoarsen
9166 
9167         See Also
9168         --------
9169         core.rolling.DatasetCoarsen
9170         DataArray.coarsen
9171         """
9172         from xarray.core.rolling import DatasetCoarsen
9173 
9174         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
9175         return DatasetCoarsen(
9176             self,
9177             dim,
9178             boundary=boundary,
9179             side=side,
9180             coord_func=coord_func,
9181         )
9182 
9183     def resample(
9184         self,
9185         indexer: Mapping[Any, str] | None = None,
9186         skipna: bool | None = None,
9187         closed: SideOptions | None = None,
9188         label: SideOptions | None = None,
9189         base: int | None = None,
9190         offset: pd.Timedelta | datetime.timedelta | str | None = None,
9191         origin: str | DatetimeLike = "start_day",
9192         keep_attrs: bool | None = None,
9193         loffset: datetime.timedelta | str | None = None,
9194         restore_coord_dims: bool | None = None,
9195         **indexer_kwargs: str,
9196     ) -> DatasetResample:
9197         """Returns a Resample object for performing resampling operations.
9198 
9199         Handles both downsampling and upsampling. The resampled
9200         dimension must be a datetime-like coordinate. If any intervals
9201         contain no values from the original object, they will be given
9202         the value ``NaN``.
9203 
9204         Parameters
9205         ----------
9206         indexer : Mapping of Hashable to str, optional
9207             Mapping from the dimension name to resample frequency [1]_. The
9208             dimension must be datetime-like.
9209         skipna : bool, optional
9210             Whether to skip missing values when aggregating in downsampling.
9211         closed : {"left", "right"}, optional
9212             Side of each interval to treat as closed.
9213         label : {"left", "right"}, optional
9214             Side of each interval to use for labeling.
9215         base : int, optional
9216             For frequencies that evenly subdivide 1 day, the "origin" of the
9217             aggregated intervals. For example, for "24H" frequency, base could
9218             range from 0 through 23.
9219         origin : {'epoch', 'start', 'start_day', 'end', 'end_day'}, pd.Timestamp, datetime.datetime, np.datetime64, or cftime.datetime, default 'start_day'
9220             The datetime on which to adjust the grouping. The timezone of origin
9221             must match the timezone of the index.
9222 
9223             If a datetime is not used, these values are also supported:
9224             - 'epoch': `origin` is 1970-01-01
9225             - 'start': `origin` is the first value of the timeseries
9226             - 'start_day': `origin` is the first day at midnight of the timeseries
9227             - 'end': `origin` is the last value of the timeseries
9228             - 'end_day': `origin` is the ceiling midnight of the last day
9229         offset : pd.Timedelta, datetime.timedelta, or str, default is None
9230             An offset timedelta added to the origin.
9231         loffset : timedelta or str, optional
9232             Offset used to adjust the resampled time labels. Some pandas date
9233             offset strings are supported.
9234         restore_coord_dims : bool, optional
9235             If True, also restore the dimension order of multi-dimensional
9236             coordinates.
9237         **indexer_kwargs : str
9238             The keyword arguments form of ``indexer``.
9239             One of indexer or indexer_kwargs must be provided.
9240 
9241         Returns
9242         -------
9243         resampled : core.resample.DataArrayResample
9244             This object resampled.
9245 
9246         See Also
9247         --------
9248         DataArray.resample
9249         pandas.Series.resample
9250         pandas.DataFrame.resample
9251         Dataset.groupby
9252         DataArray.groupby
9253 
9254         References
9255         ----------
9256         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
9257         """
9258         from xarray.core.resample import DatasetResample
9259 
9260         return self._resample(
9261             resample_cls=DatasetResample,
9262             indexer=indexer,
9263             skipna=skipna,
9264             closed=closed,
9265             label=label,
9266             base=base,
9267             offset=offset,
9268             origin=origin,
9269             keep_attrs=keep_attrs,
9270             loffset=loffset,
9271             restore_coord_dims=restore_coord_dims,
9272             **indexer_kwargs,
9273         )

</code><intended_behavior>Implement a backend-agnostic interface such that the Dataset class can operate with different parallel computing frameworks beyond dask. Maintain current dask functionalities and ensure new parallel backends can be introduced with minimal friction.</intended_behavior>, <file>xarray/core/variable.py</file>
<class>None</class>
<method>None</method>
<code>
1 from __future__ import annotations
2 
3 import copy
4 import itertools
5 import math
6 import numbers
7 import warnings
8 from collections.abc import Hashable, Iterable, Mapping, Sequence
9 from datetime import timedelta
10 from typing import TYPE_CHECKING, Any, Callable, Literal, NoReturn
11 
12 import numpy as np
13 import pandas as pd
14 from numpy.typing import ArrayLike
15 from packaging.version import Version
16 
17 import xarray as xr  # only for Dataset and DataArray
18 from xarray.core import common, dtypes, duck_array_ops, indexing, nputils, ops, utils
19 from xarray.core.arithmetic import VariableArithmetic
20 from xarray.core.common import AbstractArray
21 from xarray.core.indexing import (
22     BasicIndexer,
23     OuterIndexer,
24     PandasIndexingAdapter,
25     VectorizedIndexer,
26     as_indexable,
27 )
28 from xarray.core.options import OPTIONS, _get_keep_attrs
29 from xarray.core.pycompat import (
30     array_type,
31     integer_types,
32     is_0d_dask_array,
33     is_duck_dask_array,
34 )
35 from xarray.core.utils import (
36     Frozen,
37     NdimSizeLenMixin,
38     OrderedSet,
39     _default,
40     decode_numpy_dict_values,
41     drop_dims_from_indexers,
42     either_dict_or_kwargs,
43     ensure_us_time_resolution,
44     infix_dims,
45     is_duck_array,
46     maybe_coerce_to_str,
47 )
48 
49 NON_NUMPY_SUPPORTED_ARRAY_TYPES = (
50     indexing.ExplicitlyIndexed,
51     pd.Index,
52 )
53 # https://github.com/python/mypy/issues/224
54 BASIC_INDEXING_TYPES = integer_types + (slice,)
55 
56 if TYPE_CHECKING:
57     from xarray.core.types import (
58         Dims,
59         ErrorOptionsWithWarn,
60         PadModeOptions,
61         PadReflectOptions,
62         QuantileMethods,
63         T_Variable,
64     )
65 
66 NON_NANOSECOND_WARNING = (
67     "Converting non-nanosecond precision {case} values to nanosecond precision. "
68     "This behavior can eventually be relaxed in xarray, as it is an artifact from "
69     "pandas which is now beginning to support non-nanosecond precision values. "
70     "This warning is caused by passing non-nanosecond np.datetime64 or "
71     "np.timedelta64 values to the DataArray or Variable constructor; it can be "
72     "silenced by converting the values to nanosecond precision ahead of time."
73 )
74 
75 
76 class MissingDimensionsError(ValueError):
77     """Error class used when we can't safely guess a dimension name."""
78 
79     # inherits from ValueError for backward compatibility
80     # TODO: move this to an xarray.exceptions module?
81 
82 
83 def as_variable(obj, name=None) -> Variable | IndexVariable:
84     """Convert an object into a Variable.
85 
86     Parameters
87     ----------
88     obj : object
89         Object to convert into a Variable.
90 
91         - If the object is already a Variable, return a shallow copy.
92         - Otherwise, if the object has 'dims' and 'data' attributes, convert
93           it into a new Variable.
94         - If all else fails, attempt to convert the object into a Variable by
95           unpacking it into the arguments for creating a new Variable.
96     name : str, optional
97         If provided:
98 
99         - `obj` can be a 1D array, which is assumed to label coordinate values
100           along a dimension of this given name.
101         - Variables with name matching one of their dimensions are converted
102           into `IndexVariable` objects.
103 
104     Returns
105     -------
106     var : Variable
107         The newly created variable.
108 
109     """
110     from xarray.core.dataarray import DataArray
111 
112     # TODO: consider extending this method to automatically handle Iris and
113     if isinstance(obj, DataArray):
114         # extract the primary Variable from DataArrays
115         obj = obj.variable
116 
117     if isinstance(obj, Variable):
118         obj = obj.copy(deep=False)
119     elif isinstance(obj, tuple):
120         if isinstance(obj[1], DataArray):
121             raise TypeError(
122                 "Using a DataArray object to construct a variable is"
123                 " ambiguous, please extract the data using the .data property."
124             )
125         try:
126             obj = Variable(*obj)
127         except (TypeError, ValueError) as error:
128             # use .format() instead of % because it handles tuples consistently
129             raise error.__class__(
130                 "Could not convert tuple of form "
131                 "(dims, data[, attrs, encoding]): "
132                 "{} to Variable.".format(obj)
133             )
134     elif utils.is_scalar(obj):
135         obj = Variable([], obj)
136     elif isinstance(obj, (pd.Index, IndexVariable)) and obj.name is not None:
137         obj = Variable(obj.name, obj)
138     elif isinstance(obj, (set, dict)):
139         raise TypeError(f"variable {name!r} has invalid type {type(obj)!r}")
140     elif name is not None:
141         data = as_compatible_data(obj)
142         if data.ndim != 1:
143             raise MissingDimensionsError(
144                 f"cannot set variable {name!r} with {data.ndim!r}-dimensional data "
145                 "without explicit dimension names. Pass a tuple of "
146                 "(dims, data) instead."
147             )
148         obj = Variable(name, data, fastpath=True)
149     else:
150         raise TypeError(
151             "unable to convert object into a variable without an "
152             f"explicit list of dimensions: {obj!r}"
153         )
154 
155     if name is not None and name in obj.dims:
156         # convert the Variable into an Index
157         if obj.ndim != 1:
158             raise MissingDimensionsError(
159                 f"{name!r} has more than 1-dimension and the same name as one of its "
160                 f"dimensions {obj.dims!r}. xarray disallows such variables because they "
161                 "conflict with the coordinates used to label dimensions."
162             )
163         obj = obj.to_index_variable()
164 
165     return obj
166 
167 
168 def _maybe_wrap_data(data):
169     """
170     Put pandas.Index and numpy.ndarray arguments in adapter objects to ensure
171     they can be indexed properly.
172 
173     NumpyArrayAdapter, PandasIndexingAdapter and LazilyIndexedArray should
174     all pass through unmodified.
175     """
176     if isinstance(data, pd.Index):
177         return PandasIndexingAdapter(data)
178     return data
179 
180 
181 def _as_nanosecond_precision(data):
182     dtype = data.dtype
183     non_ns_datetime64 = (
184         dtype.kind == "M"
185         and isinstance(dtype, np.dtype)
186         and dtype != np.dtype("datetime64[ns]")
187     )
188     non_ns_datetime_tz_dtype = (
189         isinstance(dtype, pd.DatetimeTZDtype) and dtype.unit != "ns"
190     )
191     if non_ns_datetime64 or non_ns_datetime_tz_dtype:
192         utils.emit_user_level_warning(NON_NANOSECOND_WARNING.format(case="datetime"))
193         if isinstance(dtype, pd.DatetimeTZDtype):
194             nanosecond_precision_dtype = pd.DatetimeTZDtype("ns", dtype.tz)
195         else:
196             nanosecond_precision_dtype = "datetime64[ns]"
197         return data.astype(nanosecond_precision_dtype)
198     elif dtype.kind == "m" and dtype != np.dtype("timedelta64[ns]"):
199         utils.emit_user_level_warning(NON_NANOSECOND_WARNING.format(case="timedelta"))
200         return data.astype("timedelta64[ns]")
201     else:
202         return data
203 
204 
205 def _possibly_convert_objects(values):
206     """Convert arrays of datetime.datetime and datetime.timedelta objects into
207     datetime64 and timedelta64, according to the pandas convention. For the time
208     being, convert any non-nanosecond precision DatetimeIndex or TimedeltaIndex
209     objects to nanosecond precision.  While pandas is relaxing this in version
210     2.0.0, in xarray we will need to make sure we are ready to handle
211     non-nanosecond precision datetimes or timedeltas in our code before allowing
212     such values to pass through unchanged.  Converting to nanosecond precision
213     through pandas.Series objects ensures that datetimes and timedeltas are
214     within the valid date range for ns precision, as pandas will raise an error
215     if they are not.
216     """
217     as_series = pd.Series(values.ravel(), copy=False)
218     if as_series.dtype.kind in "mM":
219         as_series = _as_nanosecond_precision(as_series)
220     return np.asarray(as_series).reshape(values.shape)
221 
222 
223 def _possibly_convert_datetime_or_timedelta_index(data):
224     """For the time being, convert any non-nanosecond precision DatetimeIndex or
225     TimedeltaIndex objects to nanosecond precision.  While pandas is relaxing
226     this in version 2.0.0, in xarray we will need to make sure we are ready to
227     handle non-nanosecond precision datetimes or timedeltas in our code
228     before allowing such values to pass through unchanged."""
229     if isinstance(data, (pd.DatetimeIndex, pd.TimedeltaIndex)):
230         return _as_nanosecond_precision(data)
231     else:
232         return data
233 
234 
235 def as_compatible_data(data, fastpath=False):
236     """Prepare and wrap data to put in a Variable.
237 
238     - If data does not have the necessary attributes, convert it to ndarray.
239     - If data has dtype=datetime64, ensure that it has ns precision. If it's a
240       pandas.Timestamp, convert it to datetime64.
241     - If data is already a pandas or xarray object (other than an Index), just
242       use the values.
243 
244     Finally, wrap it up with an adapter if necessary.
245     """
246     if fastpath and getattr(data, "ndim", 0) > 0:
247         # can't use fastpath (yet) for scalars
248         return _maybe_wrap_data(data)
249 
250     from xarray.core.dataarray import DataArray
251 
252     if isinstance(data, (Variable, DataArray)):
253         return data.data
254 
255     if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):
256         data = _possibly_convert_datetime_or_timedelta_index(data)
257         return _maybe_wrap_data(data)
258 
259     if isinstance(data, tuple):
260         data = utils.to_0d_object_array(data)
261 
262     if isinstance(data, pd.Timestamp):
263         # TODO: convert, handle datetime objects, too
264         data = np.datetime64(data.value, "ns")
265 
266     if isinstance(data, timedelta):
267         data = np.timedelta64(getattr(data, "value", data), "ns")
268 
269     # we don't want nested self-described arrays
270     if isinstance(data, (pd.Series, pd.Index, pd.DataFrame)):
271         data = data.values
272 
273     if isinstance(data, np.ma.MaskedArray):
274         mask = np.ma.getmaskarray(data)
275         if mask.any():
276             dtype, fill_value = dtypes.maybe_promote(data.dtype)
277             data = np.asarray(data, dtype=dtype)
278             data[mask] = fill_value
279         else:
280             data = np.asarray(data)
281 
282     if not isinstance(data, np.ndarray) and (
283         hasattr(data, "__array_function__") or hasattr(data, "__array_namespace__")
284     ):
285         return data
286 
287     # validate whether the data is valid data types.
288     data = np.asarray(data)
289 
290     if isinstance(data, np.ndarray) and data.dtype.kind in "OMm":
291         data = _possibly_convert_objects(data)
292     return _maybe_wrap_data(data)
293 
294 
295 def _as_array_or_item(data):
296     """Return the given values as a numpy array, or as an individual item if
297     it's a 0d datetime64 or timedelta64 array.
298 
299     Importantly, this function does not copy data if it is already an ndarray -
300     otherwise, it will not be possible to update Variable values in place.
301 
302     This function mostly exists because 0-dimensional ndarrays with
303     dtype=datetime64 are broken :(
304     https://github.com/numpy/numpy/issues/4337
305     https://github.com/numpy/numpy/issues/7619
306 
307     TODO: remove this (replace with np.asarray) once these issues are fixed
308     """
309     data = np.asarray(data)
310     if data.ndim == 0:
311         if data.dtype.kind == "M":
312             data = np.datetime64(data, "ns")
313         elif data.dtype.kind == "m":
314             data = np.timedelta64(data, "ns")
315     return data
316 
317 
318 class Variable(AbstractArray, NdimSizeLenMixin, VariableArithmetic):
319     """A netcdf-like variable consisting of dimensions, data and attributes
320     which describe a single Array. A single Variable object is not fully
321     described outside the context of its parent Dataset (if you want such a
322     fully described object, use a DataArray instead).
323 
324     The main functional difference between Variables and numpy arrays is that
325     numerical operations on Variables implement array broadcasting by dimension
326     name. For example, adding an Variable with dimensions `('time',)` to
327     another Variable with dimensions `('space',)` results in a new Variable
328     with dimensions `('time', 'space')`. Furthermore, numpy reduce operations
329     like ``mean`` or ``sum`` are overwritten to take a "dimension" argument
330     instead of an "axis".
331 
332     Variables are light-weight objects used as the building block for datasets.
333     They are more primitive objects, so operations with them provide marginally
334     higher performance than using DataArrays. However, manipulating data in the
335     form of a Dataset or DataArray should almost always be preferred, because
336     they can use more complete metadata in context of coordinate labels.
337     """
338 
339     __slots__ = ("_dims", "_data", "_attrs", "_encoding")
340 
341     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):
342         """
343         Parameters
344         ----------
345         dims : str or sequence of str
346             Name(s) of the the data dimension(s). Must be either a string (only
347             for 1D data) or a sequence of strings with length equal to the
348             number of dimensions.
349         data : array_like
350             Data array which supports numpy-like data access.
351         attrs : dict_like or None, optional
352             Attributes to assign to the new variable. If None (default), an
353             empty attribute dictionary is initialized.
354         encoding : dict_like or None, optional
355             Dictionary specifying how to encode this array's data into a
356             serialized format like netCDF4. Currently used keys (for netCDF)
357             include '_FillValue', 'scale_factor', 'add_offset' and 'dtype'.
358             Well-behaved code to serialize a Variable should ignore
359             unrecognized encoding items.
360         """
361         self._data = as_compatible_data(data, fastpath=fastpath)
362         self._dims = self._parse_dimensions(dims)
363         self._attrs = None
364         self._encoding = None
365         if attrs is not None:
366             self.attrs = attrs
367         if encoding is not None:
368             self.encoding = encoding
369 
370     @property
371     def dtype(self):
372         """
373         Data-type of the array’s elements.
374 
375         See Also
376         --------
377         ndarray.dtype
378         numpy.dtype
379         """
380         return self._data.dtype
381 
382     @property
383     def shape(self):
384         """
385         Tuple of array dimensions.
386 
387         See Also
388         --------
389         numpy.ndarray.shape
390         """
391         return self._data.shape
392 
393     @property
394     def nbytes(self) -> int:
395         """
396         Total bytes consumed by the elements of the data array.
397 
398         If the underlying data array does not include ``nbytes``, estimates
399         the bytes consumed based on the ``size`` and ``dtype``.
400         """
401         if hasattr(self._data, "nbytes"):
402             return self._data.nbytes
403         else:
404             return self.size * self.dtype.itemsize
405 
406     @property
407     def _in_memory(self):
408         return isinstance(
409             self._data, (np.ndarray, np.number, PandasIndexingAdapter)
410         ) or (
411             isinstance(self._data, indexing.MemoryCachedArray)
412             and isinstance(self._data.array, indexing.NumpyIndexingAdapter)
413         )
414 
415     @property
416     def data(self) -> Any:
417         """
418         The Variable's data as an array. The underlying array type
419         (e.g. dask, sparse, pint) is preserved.
420 
421         See Also
422         --------
423         Variable.to_numpy
424         Variable.as_numpy
425         Variable.values
426         """
427         if is_duck_array(self._data):
428             return self._data
429         elif isinstance(self._data, indexing.ExplicitlyIndexed):
430             return self._data.get_duck_array()
431         else:
432             return self.values
433 
434     @data.setter
435     def data(self, data):
436         data = as_compatible_data(data)
437         if data.shape != self.shape:
438             raise ValueError(
439                 f"replacement data must match the Variable's shape. "
440                 f"replacement data has shape {data.shape}; Variable has shape {self.shape}"
441             )
442         self._data = data
443 
444     def astype(
445         self: T_Variable,
446         dtype,
447         *,
448         order=None,
449         casting=None,
450         subok=None,
451         copy=None,
452         keep_attrs=True,
453     ) -> T_Variable:
454         """
455         Copy of the Variable object, with data cast to a specified type.
456 
457         Parameters
458         ----------
459         dtype : str or dtype
460             Typecode or data-type to which the array is cast.
461         order : {'C', 'F', 'A', 'K'}, optional
462             Controls the memory layout order of the result. ‘C’ means C order,
463             ‘F’ means Fortran order, ‘A’ means ‘F’ order if all the arrays are
464             Fortran contiguous, ‘C’ order otherwise, and ‘K’ means as close to
465             the order the array elements appear in memory as possible.
466         casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional
467             Controls what kind of data casting may occur.
468 
469             * 'no' means the data types should not be cast at all.
470             * 'equiv' means only byte-order changes are allowed.
471             * 'safe' means only casts which can preserve values are allowed.
472             * 'same_kind' means only safe casts or casts within a kind,
473               like float64 to float32, are allowed.
474             * 'unsafe' means any data conversions may be done.
475         subok : bool, optional
476             If True, then sub-classes will be passed-through, otherwise the
477             returned array will be forced to be a base-class array.
478         copy : bool, optional
479             By default, astype always returns a newly allocated array. If this
480             is set to False and the `dtype` requirement is satisfied, the input
481             array is returned instead of a copy.
482         keep_attrs : bool, optional
483             By default, astype keeps attributes. Set to False to remove
484             attributes in the returned object.
485 
486         Returns
487         -------
488         out : same as object
489             New object with data cast to the specified type.
490 
491         Notes
492         -----
493         The ``order``, ``casting``, ``subok`` and ``copy`` arguments are only passed
494         through to the ``astype`` method of the underlying array when a value
495         different than ``None`` is supplied.
496         Make sure to only supply these arguments if the underlying array class
497         supports them.
498 
499         See Also
500         --------
501         numpy.ndarray.astype
502         dask.array.Array.astype
503         sparse.COO.astype
504         """
505         from xarray.core.computation import apply_ufunc
506 
507         kwargs = dict(order=order, casting=casting, subok=subok, copy=copy)
508         kwargs = {k: v for k, v in kwargs.items() if v is not None}
509 
510         return apply_ufunc(
511             duck_array_ops.astype,
512             self,
513             dtype,
514             kwargs=kwargs,
515             keep_attrs=keep_attrs,
516             dask="allowed",
517         )
518 
519     def load(self, **kwargs):
520         """Manually trigger loading of this variable's data from disk or a
521         remote source into memory and return this variable.
522 
523         Normally, it should not be necessary to call this method in user code,
524         because all xarray functions should either work on deferred data or
525         load data automatically.
526 
527         Parameters
528         ----------
529         **kwargs : dict
530             Additional keyword arguments passed on to ``dask.array.compute``.
531 
532         See Also
533         --------
534         dask.array.compute
535         """
536         if is_duck_dask_array(self._data):
537             self._data = as_compatible_data(self._data.compute(**kwargs))
538         elif isinstance(self._data, indexing.ExplicitlyIndexed):
539             self._data = self._data.get_duck_array()
540         elif not is_duck_array(self._data):
541             self._data = np.asarray(self._data)
542         return self
543 
544     def compute(self, **kwargs):
545         """Manually trigger loading of this variable's data from disk or a
546         remote source into memory and return a new variable. The original is
547         left unaltered.
548 
549         Normally, it should not be necessary to call this method in user code,
550         because all xarray functions should either work on deferred data or
551         load data automatically.
552 
553         Parameters
554         ----------
555         **kwargs : dict
556             Additional keyword arguments passed on to ``dask.array.compute``.
557 
558         See Also
559         --------
560         dask.array.compute
561         """
562         new = self.copy(deep=False)
563         return new.load(**kwargs)
564 
565     def __dask_tokenize__(self):
566         # Use v.data, instead of v._data, in order to cope with the wrappers
567         # around NetCDF and the like
568         from dask.base import normalize_token
569 
570         return normalize_token((type(self), self._dims, self.data, self._attrs))
571 
572     def __dask_graph__(self):
573         if is_duck_dask_array(self._data):
574             return self._data.__dask_graph__()
575         else:
576             return None
577 
578     def __dask_keys__(self):
579         return self._data.__dask_keys__()
580 
581     def __dask_layers__(self):
582         return self._data.__dask_layers__()
583 
584     @property
585     def __dask_optimize__(self):
586         return self._data.__dask_optimize__
587 
588     @property
589     def __dask_scheduler__(self):
590         return self._data.__dask_scheduler__
591 
592     def __dask_postcompute__(self):
593         array_func, array_args = self._data.__dask_postcompute__()
594         return self._dask_finalize, (array_func,) + array_args
595 
596     def __dask_postpersist__(self):
597         array_func, array_args = self._data.__dask_postpersist__()
598         return self._dask_finalize, (array_func,) + array_args
599 
600     def _dask_finalize(self, results, array_func, *args, **kwargs):
601         data = array_func(results, *args, **kwargs)
602         return Variable(self._dims, data, attrs=self._attrs, encoding=self._encoding)
603 
604     @property
605     def values(self):
606         """The variable's data as a numpy.ndarray"""
607         return _as_array_or_item(self._data)
608 
609     @values.setter
610     def values(self, values):
611         self.data = values
612 
613     def to_base_variable(self) -> Variable:
614         """Return this variable as a base xarray.Variable"""
615         return Variable(
616             self._dims, self._data, self._attrs, encoding=self._encoding, fastpath=True
617         )
618 
619     to_variable = utils.alias(to_base_variable, "to_variable")
620 
621     def to_index_variable(self) -> IndexVariable:
622         """Return this variable as an xarray.IndexVariable"""
623         return IndexVariable(
624             self._dims, self._data, self._attrs, encoding=self._encoding, fastpath=True
625         )
626 
627     to_coord = utils.alias(to_index_variable, "to_coord")
628 
629     def _to_index(self) -> pd.Index:
630         return self.to_index_variable()._to_index()
631 
632     def to_index(self) -> pd.Index:
633         """Convert this variable to a pandas.Index"""
634         return self.to_index_variable().to_index()
635 
636     def to_dict(
637         self, data: bool | str = "list", encoding: bool = False
638     ) -> dict[str, Any]:
639         """Dictionary representation of variable."""
640         item: dict[str, Any] = {
641             "dims": self.dims,
642             "attrs": decode_numpy_dict_values(self.attrs),
643         }
644         if data is not False:
645             if data in [True, "list"]:
646                 item["data"] = ensure_us_time_resolution(self.to_numpy()).tolist()
647             elif data == "array":
648                 item["data"] = ensure_us_time_resolution(self.data)
649             else:
650                 msg = 'data argument must be bool, "list", or "array"'
651                 raise ValueError(msg)
652 
653         else:
654             item.update({"dtype": str(self.dtype), "shape": self.shape})
655 
656         if encoding:
657             item["encoding"] = dict(self.encoding)
658 
659         return item
660 
661     @property
662     def dims(self) -> tuple[Hashable, ...]:
663         """Tuple of dimension names with which this variable is associated."""
664         return self._dims
665 
666     @dims.setter
667     def dims(self, value: str | Iterable[Hashable]) -> None:
668         self._dims = self._parse_dimensions(value)
669 
670     def _parse_dimensions(self, dims: str | Iterable[Hashable]) -> tuple[Hashable, ...]:
671         if isinstance(dims, str):
672             dims = (dims,)
673         dims = tuple(dims)
674         if len(dims) != self.ndim:
675             raise ValueError(
676                 f"dimensions {dims} must have the same length as the "
677                 f"number of data dimensions, ndim={self.ndim}"
678             )
679         return dims
680 
681     def _item_key_to_tuple(self, key):
682         if utils.is_dict_like(key):
683             return tuple(key.get(dim, slice(None)) for dim in self.dims)
684         else:
685             return key
686 
687     def _broadcast_indexes(self, key):
688         """Prepare an indexing key for an indexing operation.
689 
690         Parameters
691         ----------
692         key : int, slice, array-like, dict or tuple of integer, slice and array-like
693             Any valid input for indexing.
694 
695         Returns
696         -------
697         dims : tuple
698             Dimension of the resultant variable.
699         indexers : IndexingTuple subclass
700             Tuple of integer, array-like, or slices to use when indexing
701             self._data. The type of this argument indicates the type of
702             indexing to perform, either basic, outer or vectorized.
703         new_order : Optional[Sequence[int]]
704             Optional reordering to do on the result of indexing. If not None,
705             the first len(new_order) indexing should be moved to these
706             positions.
707         """
708         key = self._item_key_to_tuple(key)  # key is a tuple
709         # key is a tuple of full size
710         key = indexing.expanded_indexer(key, self.ndim)
711         # Convert a scalar Variable to a 0d-array
712         key = tuple(
713             k.data if isinstance(k, Variable) and k.ndim == 0 else k for k in key
714         )
715         # Convert a 0d numpy arrays to an integer
716         # dask 0d arrays are passed through
717         key = tuple(
718             k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key
719         )
720 
721         if all(isinstance(k, BASIC_INDEXING_TYPES) for k in key):
722             return self._broadcast_indexes_basic(key)
723 
724         self._validate_indexers(key)
725         # Detect it can be mapped as an outer indexer
726         # If all key is unlabeled, or
727         # key can be mapped as an OuterIndexer.
728         if all(not isinstance(k, Variable) for k in key):
729             return self._broadcast_indexes_outer(key)
730 
731         # If all key is 1-dimensional and there are no duplicate labels,
732         # key can be mapped as an OuterIndexer.
733         dims = []
734         for k, d in zip(key, self.dims):
735             if isinstance(k, Variable):
736                 if len(k.dims) > 1:
737                     return self._broadcast_indexes_vectorized(key)
738                 dims.append(k.dims[0])
739             elif not isinstance(k, integer_types):
740                 dims.append(d)
741         if len(set(dims)) == len(dims):
742             return self._broadcast_indexes_outer(key)
743 
744         return self._broadcast_indexes_vectorized(key)
745 
746     def _broadcast_indexes_basic(self, key):
747         dims = tuple(
748             dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)
749         )
750         return dims, BasicIndexer(key), None
751 
752     def _validate_indexers(self, key):
753         """Make sanity checks"""
754         for dim, k in zip(self.dims, key):
755             if not isinstance(k, BASIC_INDEXING_TYPES):
756                 if not isinstance(k, Variable):
757                     if not is_duck_array(k):
758                         k = np.asarray(k)
759                     if k.ndim > 1:
760                         raise IndexError(
761                             "Unlabeled multi-dimensional array cannot be "
762                             "used for indexing: {}".format(k)
763                         )
764                 if k.dtype.kind == "b":
765                     if self.shape[self.get_axis_num(dim)] != len(k):
766                         raise IndexError(
767                             "Boolean array size {:d} is used to index array "
768                             "with shape {:s}.".format(len(k), str(self.shape))
769                         )
770                     if k.ndim > 1:
771                         raise IndexError(
772                             "{}-dimensional boolean indexing is "
773                             "not supported. ".format(k.ndim)
774                         )
775                     if is_duck_dask_array(k.data):
776                         raise KeyError(
777                             "Indexing with a boolean dask array is not allowed. "
778                             "This will result in a dask array of unknown shape. "
779                             "Such arrays are unsupported by Xarray."
780                             "Please compute the indexer first using .compute()"
781                         )
782                     if getattr(k, "dims", (dim,)) != (dim,):
783                         raise IndexError(
784                             "Boolean indexer should be unlabeled or on the "
785                             "same dimension to the indexed array. Indexer is "
786                             "on {:s} but the target dimension is {:s}.".format(
787                                 str(k.dims), dim
788                             )
789                         )
790 
791     def _broadcast_indexes_outer(self, key):
792         # drop dim if k is integer or if k is a 0d dask array
793         dims = tuple(
794             k.dims[0] if isinstance(k, Variable) else dim
795             for k, dim in zip(key, self.dims)
796             if (not isinstance(k, integer_types) and not is_0d_dask_array(k))
797         )
798 
799         new_key = []
800         for k in key:
801             if isinstance(k, Variable):
802                 k = k.data
803             if not isinstance(k, BASIC_INDEXING_TYPES):
804                 if not is_duck_array(k):
805                     k = np.asarray(k)
806                 if k.size == 0:
807                     # Slice by empty list; numpy could not infer the dtype
808                     k = k.astype(int)
809                 elif k.dtype.kind == "b":
810                     (k,) = np.nonzero(k)
811             new_key.append(k)
812 
813         return dims, OuterIndexer(tuple(new_key)), None
814 
815     def _nonzero(self):
816         """Equivalent numpy's nonzero but returns a tuple of Variables."""
817         # TODO we should replace dask's native nonzero
818         # after https://github.com/dask/dask/issues/1076 is implemented.
819         nonzeros = np.nonzero(self.data)
820         return tuple(Variable((dim), nz) for nz, dim in zip(nonzeros, self.dims))
821 
822     def _broadcast_indexes_vectorized(self, key):
823         variables = []
824         out_dims_set = OrderedSet()
825         for dim, value in zip(self.dims, key):
826             if isinstance(value, slice):
827                 out_dims_set.add(dim)
828             else:
829                 variable = (
830                     value
831                     if isinstance(value, Variable)
832                     else as_variable(value, name=dim)
833                 )
834                 if variable.dtype.kind == "b":  # boolean indexing case
835                     (variable,) = variable._nonzero()
836 
837                 variables.append(variable)
838                 out_dims_set.update(variable.dims)
839 
840         variable_dims = set()
841         for variable in variables:
842             variable_dims.update(variable.dims)
843 
844         slices = []
845         for i, (dim, value) in enumerate(zip(self.dims, key)):
846             if isinstance(value, slice):
847                 if dim in variable_dims:
848                     # We only convert slice objects to variables if they share
849                     # a dimension with at least one other variable. Otherwise,
850                     # we can equivalently leave them as slices aknd transpose
851                     # the result. This is significantly faster/more efficient
852                     # for most array backends.
853                     values = np.arange(*value.indices(self.sizes[dim]))
854                     variables.insert(i - len(slices), Variable((dim,), values))
855                 else:
856                     slices.append((i, value))
857 
858         try:
859             variables = _broadcast_compat_variables(*variables)
860         except ValueError:
861             raise IndexError(f"Dimensions of indexers mismatch: {key}")
862 
863         out_key = [variable.data for variable in variables]
864         out_dims = tuple(out_dims_set)
865         slice_positions = set()
866         for i, value in slices:
867             out_key.insert(i, value)
868             new_position = out_dims.index(self.dims[i])
869             slice_positions.add(new_position)
870 
871         if slice_positions:
872             new_order = [i for i in range(len(out_dims)) if i not in slice_positions]
873         else:
874             new_order = None
875 
876         return out_dims, VectorizedIndexer(tuple(out_key)), new_order
877 
878     def __getitem__(self: T_Variable, key) -> T_Variable:
879         """Return a new Variable object whose contents are consistent with
880         getting the provided key from the underlying data.
881 
882         NB. __getitem__ and __setitem__ implement xarray-style indexing,
883         where if keys are unlabeled arrays, we index the array orthogonally
884         with them. If keys are labeled array (such as Variables), they are
885         broadcasted with our usual scheme and then the array is indexed with
886         the broadcasted key, like numpy's fancy indexing.
887 
888         If you really want to do indexing like `x[x > 0]`, manipulate the numpy
889         array `x.values` directly.
890         """
891         dims, indexer, new_order = self._broadcast_indexes(key)
892         data = as_indexable(self._data)[indexer]
893         if new_order:
894             data = np.moveaxis(data, range(len(new_order)), new_order)
895         return self._finalize_indexing_result(dims, data)
896 
897     def _finalize_indexing_result(self: T_Variable, dims, data) -> T_Variable:
898         """Used by IndexVariable to return IndexVariable objects when possible."""
899         return self._replace(dims=dims, data=data)
900 
901     def _getitem_with_mask(self, key, fill_value=dtypes.NA):
902         """Index this Variable with -1 remapped to fill_value."""
903         # TODO(shoyer): expose this method in public API somewhere (isel?) and
904         # use it for reindex.
905         # TODO(shoyer): add a sanity check that all other integers are
906         # non-negative
907         # TODO(shoyer): add an optimization, remapping -1 to an adjacent value
908         # that is actually indexed rather than mapping it to the last value
909         # along each axis.
910 
911         if fill_value is dtypes.NA:
912             fill_value = dtypes.get_fill_value(self.dtype)
913 
914         dims, indexer, new_order = self._broadcast_indexes(key)
915 
916         if self.size:
917             if is_duck_dask_array(self._data):
918                 # dask's indexing is faster this way; also vindex does not
919                 # support negative indices yet:
920                 # https://github.com/dask/dask/pull/2967
921                 actual_indexer = indexing.posify_mask_indexer(indexer)
922             else:
923                 actual_indexer = indexer
924 
925             data = as_indexable(self._data)[actual_indexer]
926             mask = indexing.create_mask(indexer, self.shape, data)
927             # we need to invert the mask in order to pass data first. This helps
928             # pint to choose the correct unit
929             # TODO: revert after https://github.com/hgrecco/pint/issues/1019 is fixed
930             data = duck_array_ops.where(np.logical_not(mask), data, fill_value)
931         else:
932             # array cannot be indexed along dimensions of size 0, so just
933             # build the mask directly instead.
934             mask = indexing.create_mask(indexer, self.shape)
935             data = np.broadcast_to(fill_value, getattr(mask, "shape", ()))
936 
937         if new_order:
938             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)
939         return self._finalize_indexing_result(dims, data)
940 
941     def __setitem__(self, key, value):
942         """__setitem__ is overloaded to access the underlying numpy values with
943         orthogonal indexing.
944 
945         See __getitem__ for more details.
946         """
947         dims, index_tuple, new_order = self._broadcast_indexes(key)
948 
949         if not isinstance(value, Variable):
950             value = as_compatible_data(value)
951             if value.ndim > len(dims):
952                 raise ValueError(
953                     f"shape mismatch: value array of shape {value.shape} could not be "
954                     f"broadcast to indexing result with {len(dims)} dimensions"
955                 )
956             if value.ndim == 0:
957                 value = Variable((), value)
958             else:
959                 value = Variable(dims[-value.ndim :], value)
960         # broadcast to become assignable
961         value = value.set_dims(dims).data
962 
963         if new_order:
964             value = duck_array_ops.asarray(value)
965             value = value[(len(dims) - value.ndim) * (np.newaxis,) + (Ellipsis,)]
966             value = np.moveaxis(value, new_order, range(len(new_order)))
967 
968         indexable = as_indexable(self._data)
969         indexable[index_tuple] = value
970 
971     @property
972     def attrs(self) -> dict[Any, Any]:
973         """Dictionary of local attributes on this variable."""
974         if self._attrs is None:
975             self._attrs = {}
976         return self._attrs
977 
978     @attrs.setter
979     def attrs(self, value: Mapping[Any, Any]) -> None:
980         self._attrs = dict(value)
981 
982     @property
983     def encoding(self) -> dict[Any, Any]:
984         """Dictionary of encodings on this variable."""
985         if self._encoding is None:
986             self._encoding = {}
987         return self._encoding
988 
989     @encoding.setter
990     def encoding(self, value):
991         try:
992             self._encoding = dict(value)
993         except ValueError:
994             raise ValueError("encoding must be castable to a dictionary")
995 
996     def reset_encoding(self: T_Variable) -> T_Variable:
997         """Return a new Variable without encoding."""
998         return self._replace(encoding={})
999 
1000     def copy(
1001         self: T_Variable, deep: bool = True, data: ArrayLike | None = None
1002     ) -> T_Variable:
1003         """Returns a copy of this object.
1004 
1005         If `deep=True`, the data array is loaded into memory and copied onto
1006         the new object. Dimensions, attributes and encodings are always copied.
1007 
1008         Use `data` to create a new object with the same structure as
1009         original but entirely new data.
1010 
1011         Parameters
1012         ----------
1013         deep : bool, default: True
1014             Whether the data array is loaded into memory and copied onto
1015             the new object. Default is True.
1016         data : array_like, optional
1017             Data to use in the new object. Must have same shape as original.
1018             When `data` is used, `deep` is ignored.
1019 
1020         Returns
1021         -------
1022         object : Variable
1023             New object with dimensions, attributes, encodings, and optionally
1024             data copied from original.
1025 
1026         Examples
1027         --------
1028         Shallow copy versus deep copy
1029 
1030         >>> var = xr.Variable(data=[1, 2, 3], dims="x")
1031         >>> var.copy()
1032         <xarray.Variable (x: 3)>
1033         array([1, 2, 3])
1034         >>> var_0 = var.copy(deep=False)
1035         >>> var_0[0] = 7
1036         >>> var_0
1037         <xarray.Variable (x: 3)>
1038         array([7, 2, 3])
1039         >>> var
1040         <xarray.Variable (x: 3)>
1041         array([7, 2, 3])
1042 
1043         Changing the data using the ``data`` argument maintains the
1044         structure of the original object, but with the new data. Original
1045         object is unaffected.
1046 
1047         >>> var.copy(data=[0.1, 0.2, 0.3])
1048         <xarray.Variable (x: 3)>
1049         array([0.1, 0.2, 0.3])
1050         >>> var
1051         <xarray.Variable (x: 3)>
1052         array([7, 2, 3])
1053 
1054         See Also
1055         --------
1056         pandas.DataFrame.copy
1057         """
1058         return self._copy(deep=deep, data=data)
1059 
1060     def _copy(
1061         self: T_Variable,
1062         deep: bool = True,
1063         data: ArrayLike | None = None,
1064         memo: dict[int, Any] | None = None,
1065     ) -> T_Variable:
1066         if data is None:
1067             ndata = self._data
1068 
1069             if isinstance(ndata, indexing.MemoryCachedArray):
1070                 # don't share caching between copies
1071                 ndata = indexing.MemoryCachedArray(ndata.array)
1072 
1073             if deep:
1074                 ndata = copy.deepcopy(ndata, memo)
1075 
1076         else:
1077             ndata = as_compatible_data(data)
1078             if self.shape != ndata.shape:
1079                 raise ValueError(
1080                     "Data shape {} must match shape of object {}".format(
1081                         ndata.shape, self.shape
1082                     )
1083                 )
1084 
1085         attrs = copy.deepcopy(self._attrs, memo) if deep else copy.copy(self._attrs)
1086         encoding = (
1087             copy.deepcopy(self._encoding, memo) if deep else copy.copy(self._encoding)
1088         )
1089 
1090         # note: dims is already an immutable tuple
1091         return self._replace(data=ndata, attrs=attrs, encoding=encoding)
1092 
1093     def _replace(
1094         self: T_Variable,
1095         dims=_default,
1096         data=_default,
1097         attrs=_default,
1098         encoding=_default,
1099     ) -> T_Variable:
1100         if dims is _default:
1101             dims = copy.copy(self._dims)
1102         if data is _default:
1103             data = copy.copy(self.data)
1104         if attrs is _default:
1105             attrs = copy.copy(self._attrs)
1106         if encoding is _default:
1107             encoding = copy.copy(self._encoding)
1108         return type(self)(dims, data, attrs, encoding, fastpath=True)
1109 
1110     def __copy__(self: T_Variable) -> T_Variable:
1111         return self._copy(deep=False)
1112 
1113     def __deepcopy__(
1114         self: T_Variable, memo: dict[int, Any] | None = None
1115     ) -> T_Variable:
1116         return self._copy(deep=True, memo=memo)
1117 
1118     # mutable objects should not be hashable
1119     # https://github.com/python/mypy/issues/4266
1120     __hash__ = None  # type: ignore[assignment]
1121 
1122     @property
1123     def chunks(self) -> tuple[tuple[int, ...], ...] | None:
1124         """
1125         Tuple of block lengths for this dataarray's data, in order of dimensions, or None if
1126         the underlying data is not a dask array.
1127 
1128         See Also
1129         --------
1130         Variable.chunk
1131         Variable.chunksizes
1132         xarray.unify_chunks
1133         """
1134         return getattr(self._data, "chunks", None)
1135 
1136     @property
1137     def chunksizes(self) -> Mapping[Any, tuple[int, ...]]:
1138         """
1139         Mapping from dimension names to block lengths for this variable's data, or None if
1140         the underlying data is not a dask array.
1141         Cannot be modified directly, but can be modified by calling .chunk().
1142 
1143         Differs from variable.chunks because it returns a mapping of dimensions to chunk shapes
1144         instead of a tuple of chunk shapes.
1145 
1146         See Also
1147         --------
1148         Variable.chunk
1149         Variable.chunks
1150         xarray.unify_chunks
1151         """
1152         if hasattr(self._data, "chunks"):
1153             return Frozen({dim: c for dim, c in zip(self.dims, self.data.chunks)})
1154         else:
1155             return {}
1156 
1157     _array_counter = itertools.count()
1158 
1159     def chunk(
1160         self,
1161         chunks: (
1162             int
1163             | Literal["auto"]
1164             | tuple[int, ...]
1165             | tuple[tuple[int, ...], ...]
1166             | Mapping[Any, None | int | tuple[int, ...]]
1167         ) = {},
1168         name: str | None = None,
1169         lock: bool = False,
1170         inline_array: bool = False,
1171         **chunks_kwargs: Any,
1172     ) -> Variable:
1173         """Coerce this array's data into a dask array with the given chunks.
1174 
1175         If this variable is a non-dask array, it will be converted to dask
1176         array. If it's a dask array, it will be rechunked to the given chunk
1177         sizes.
1178 
1179         If neither chunks is not provided for one or more dimensions, chunk
1180         sizes along that dimension will not be updated; non-dask arrays will be
1181         converted into dask arrays with a single block.
1182 
1183         Parameters
1184         ----------
1185         chunks : int, tuple or dict, optional
1186             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
1187             ``{'x': 5, 'y': 5}``.
1188         name : str, optional
1189             Used to generate the name for this array in the internal dask
1190             graph. Does not need not be unique.
1191         lock : optional
1192             Passed on to :py:func:`dask.array.from_array`, if the array is not
1193             already as dask array.
1194         inline_array: optional
1195             Passed on to :py:func:`dask.array.from_array`, if the array is not
1196             already as dask array.
1197         **chunks_kwargs : {dim: chunks, ...}, optional
1198             The keyword arguments form of ``chunks``.
1199             One of chunks or chunks_kwargs must be provided.
1200 
1201         Returns
1202         -------
1203         chunked : xarray.Variable
1204 
1205         See Also
1206         --------
1207         Variable.chunks
1208         Variable.chunksizes
1209         xarray.unify_chunks
1210         dask.array.from_array
1211         """
1212         import dask.array as da
1213 
1214         if chunks is None:
1215             warnings.warn(
1216                 "None value for 'chunks' is deprecated. "
1217                 "It will raise an error in the future. Use instead '{}'",
1218                 category=FutureWarning,
1219             )
1220             chunks = {}
1221 
1222         if isinstance(chunks, (float, str, int, tuple, list)):
1223             pass  # dask.array.from_array can handle these directly
1224         else:
1225             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
1226 
1227         if utils.is_dict_like(chunks):
1228             chunks = {self.get_axis_num(dim): chunk for dim, chunk in chunks.items()}
1229 
1230         data = self._data
1231         if is_duck_dask_array(data):
1232             data = data.rechunk(chunks)
1233         else:
1234             if isinstance(data, indexing.ExplicitlyIndexed):
1235                 # Unambiguously handle array storage backends (like NetCDF4 and h5py)
1236                 # that can't handle general array indexing. For example, in netCDF4 you
1237                 # can do "outer" indexing along two dimensions independent, which works
1238                 # differently from how NumPy handles it.
1239                 # da.from_array works by using lazy indexing with a tuple of slices.
1240                 # Using OuterIndexer is a pragmatic choice: dask does not yet handle
1241                 # different indexing types in an explicit way:
1242                 # https://github.com/dask/dask/issues/2883
1243                 data = indexing.ImplicitToExplicitIndexingAdapter(
1244                     data, indexing.OuterIndexer
1245                 )
1246 
1247                 # All of our lazily loaded backend array classes should use NumPy
1248                 # array operations.
1249                 kwargs = {"meta": np.ndarray}
1250             else:
1251                 kwargs = {}
1252 
1253             if utils.is_dict_like(chunks):
1254                 chunks = tuple(chunks.get(n, s) for n, s in enumerate(self.shape))
1255 
1256             data = da.from_array(
1257                 data, chunks, name=name, lock=lock, inline_array=inline_array, **kwargs
1258             )
1259 
1260         return self._replace(data=data)
1261 
1262     def to_numpy(self) -> np.ndarray:
1263         """Coerces wrapped data to numpy and returns a numpy.ndarray"""
1264         # TODO an entrypoint so array libraries can choose coercion method?
1265         data = self.data
1266 
1267         # TODO first attempt to call .to_numpy() once some libraries implement it
1268         if hasattr(data, "chunks"):
1269             data = data.compute()
1270         if isinstance(data, array_type("cupy")):
1271             data = data.get()
1272         # pint has to be imported dynamically as pint imports xarray
1273         if isinstance(data, array_type("pint")):
1274             data = data.magnitude
1275         if isinstance(data, array_type("sparse")):
1276             data = data.todense()
1277         data = np.asarray(data)
1278 
1279         return data
1280 
1281     def as_numpy(self: T_Variable) -> T_Variable:
1282         """Coerces wrapped data into a numpy array, returning a Variable."""
1283         return self._replace(data=self.to_numpy())
1284 
1285     def _as_sparse(self, sparse_format=_default, fill_value=dtypes.NA):
1286         """
1287         use sparse-array as backend.
1288         """
1289         import sparse
1290 
1291         # TODO: what to do if dask-backended?
1292         if fill_value is dtypes.NA:
1293             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1294         else:
1295             dtype = dtypes.result_type(self.dtype, fill_value)
1296 
1297         if sparse_format is _default:
1298             sparse_format = "coo"
1299         try:
1300             as_sparse = getattr(sparse, f"as_{sparse_format.lower()}")
1301         except AttributeError:
1302             raise ValueError(f"{sparse_format} is not a valid sparse format")
1303 
1304         data = as_sparse(self.data.astype(dtype), fill_value=fill_value)
1305         return self._replace(data=data)
1306 
1307     def _to_dense(self):
1308         """
1309         Change backend from sparse to np.array
1310         """
1311         if hasattr(self._data, "todense"):
1312             return self._replace(data=self._data.todense())
1313         return self.copy(deep=False)
1314 
1315     def isel(
1316         self: T_Variable,
1317         indexers: Mapping[Any, Any] | None = None,
1318         missing_dims: ErrorOptionsWithWarn = "raise",
1319         **indexers_kwargs: Any,
1320     ) -> T_Variable:
1321         """Return a new array indexed along the specified dimension(s).
1322 
1323         Parameters
1324         ----------
1325         **indexers : {dim: indexer, ...}
1326             Keyword arguments with names matching dimensions and values given
1327             by integers, slice objects or arrays.
1328         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1329             What to do if dimensions that should be selected from are not present in the
1330             DataArray:
1331             - "raise": raise an exception
1332             - "warn": raise a warning, and ignore the missing dimensions
1333             - "ignore": ignore the missing dimensions
1334 
1335         Returns
1336         -------
1337         obj : Array object
1338             A new Array with the selected data and dimensions. In general,
1339             the new variable's data will be a view of this variable's data,
1340             unless numpy fancy indexing was triggered by using an array
1341             indexer, in which case the data will be a copy.
1342         """
1343         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1344 
1345         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
1346 
1347         key = tuple(indexers.get(dim, slice(None)) for dim in self.dims)
1348         return self[key]
1349 
1350     def squeeze(self, dim=None):
1351         """Return a new object with squeezed data.
1352 
1353         Parameters
1354         ----------
1355         dim : None or str or tuple of str, optional
1356             Selects a subset of the length one dimensions. If a dimension is
1357             selected with length greater than one, an error is raised. If
1358             None, all length one dimensions are squeezed.
1359 
1360         Returns
1361         -------
1362         squeezed : same type as caller
1363             This object, but with with all or a subset of the dimensions of
1364             length 1 removed.
1365 
1366         See Also
1367         --------
1368         numpy.squeeze
1369         """
1370         dims = common.get_squeeze_dims(self, dim)
1371         return self.isel({d: 0 for d in dims})
1372 
1373     def _shift_one_dim(self, dim, count, fill_value=dtypes.NA):
1374         axis = self.get_axis_num(dim)
1375 
1376         if count > 0:
1377             keep = slice(None, -count)
1378         elif count < 0:
1379             keep = slice(-count, None)
1380         else:
1381             keep = slice(None)
1382 
1383         trimmed_data = self[(slice(None),) * axis + (keep,)].data
1384 
1385         if fill_value is dtypes.NA:
1386             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1387         else:
1388             dtype = self.dtype
1389 
1390         width = min(abs(count), self.shape[axis])
1391         dim_pad = (width, 0) if count >= 0 else (0, width)
1392         pads = [(0, 0) if d != dim else dim_pad for d in self.dims]
1393 
1394         data = np.pad(
1395             trimmed_data.astype(dtype),
1396             pads,
1397             mode="constant",
1398             constant_values=fill_value,
1399         )
1400 
1401         if is_duck_dask_array(data):
1402             # chunked data should come out with the same chunks; this makes
1403             # it feasible to combine shifted and unshifted data
1404             # TODO: remove this once dask.array automatically aligns chunks
1405             data = data.rechunk(self.data.chunks)
1406 
1407         return self._replace(data=data)
1408 
1409     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):
1410         """
1411         Return a new Variable with shifted data.
1412 
1413         Parameters
1414         ----------
1415         shifts : mapping of the form {dim: offset}
1416             Integer offset to shift along each of the given dimensions.
1417             Positive offsets shift to the right; negative offsets shift to the
1418             left.
1419         fill_value : scalar, optional
1420             Value to use for newly missing values
1421         **shifts_kwargs
1422             The keyword arguments form of ``shifts``.
1423             One of shifts or shifts_kwargs must be provided.
1424 
1425         Returns
1426         -------
1427         shifted : Variable
1428             Variable with the same dimensions and attributes but shifted data.
1429         """
1430         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
1431         result = self
1432         for dim, count in shifts.items():
1433             result = result._shift_one_dim(dim, count, fill_value=fill_value)
1434         return result
1435 
1436     def _pad_options_dim_to_index(
1437         self,
1438         pad_option: Mapping[Any, int | tuple[int, int]],
1439         fill_with_shape=False,
1440     ):
1441         if fill_with_shape:
1442             return [
1443                 (n, n) if d not in pad_option else pad_option[d]
1444                 for d, n in zip(self.dims, self.data.shape)
1445             ]
1446         return [(0, 0) if d not in pad_option else pad_option[d] for d in self.dims]
1447 
1448     def pad(
1449         self,
1450         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,
1451         mode: PadModeOptions = "constant",
1452         stat_length: int
1453         | tuple[int, int]
1454         | Mapping[Any, tuple[int, int]]
1455         | None = None,
1456         constant_values: float
1457         | tuple[float, float]
1458         | Mapping[Any, tuple[float, float]]
1459         | None = None,
1460         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
1461         reflect_type: PadReflectOptions = None,
1462         keep_attrs: bool | None = None,
1463         **pad_width_kwargs: Any,
1464     ):
1465         """
1466         Return a new Variable with padded data.
1467 
1468         Parameters
1469         ----------
1470         pad_width : mapping of hashable to tuple of int
1471             Mapping with the form of {dim: (pad_before, pad_after)}
1472             describing the number of values padded along each dimension.
1473             {dim: pad} is a shortcut for pad_before = pad_after = pad
1474         mode : str, default: "constant"
1475             See numpy / Dask docs
1476         stat_length : int, tuple or mapping of hashable to tuple
1477             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
1478             values at edge of each axis used to calculate the statistic value.
1479         constant_values : scalar, tuple or mapping of hashable to tuple
1480             Used in 'constant'.  The values to set the padded values for each
1481             axis.
1482         end_values : scalar, tuple or mapping of hashable to tuple
1483             Used in 'linear_ramp'.  The values used for the ending value of the
1484             linear_ramp and that will form the edge of the padded array.
1485         reflect_type : {"even", "odd"}, optional
1486             Used in "reflect", and "symmetric".  The "even" style is the
1487             default with an unaltered reflection around the edge value.  For
1488             the "odd" style, the extended part of the array is created by
1489             subtracting the reflected values from two times the edge value.
1490         keep_attrs : bool, optional
1491             If True, the variable's attributes (`attrs`) will be copied from
1492             the original object to the new one.  If False (default), the new
1493             object will be returned without attributes.
1494         **pad_width_kwargs
1495             One of pad_width or pad_width_kwargs must be provided.
1496 
1497         Returns
1498         -------
1499         padded : Variable
1500             Variable with the same dimensions and attributes but padded data.
1501         """
1502         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, "pad")
1503 
1504         # change default behaviour of pad with mode constant
1505         if mode == "constant" and (
1506             constant_values is None or constant_values is dtypes.NA
1507         ):
1508             dtype, constant_values = dtypes.maybe_promote(self.dtype)
1509         else:
1510             dtype = self.dtype
1511 
1512         # create pad_options_kwargs, numpy requires only relevant kwargs to be nonempty
1513         if isinstance(stat_length, dict):
1514             stat_length = self._pad_options_dim_to_index(
1515                 stat_length, fill_with_shape=True
1516             )
1517         if isinstance(constant_values, dict):
1518             constant_values = self._pad_options_dim_to_index(constant_values)
1519         if isinstance(end_values, dict):
1520             end_values = self._pad_options_dim_to_index(end_values)
1521 
1522         # workaround for bug in Dask's default value of stat_length https://github.com/dask/dask/issues/5303
1523         if stat_length is None and mode in ["maximum", "mean", "median", "minimum"]:
1524             stat_length = [(n, n) for n in self.data.shape]  # type: ignore[assignment]
1525 
1526         # change integer values to a tuple of two of those values and change pad_width to index
1527         for k, v in pad_width.items():
1528             if isinstance(v, numbers.Number):
1529                 pad_width[k] = (v, v)
1530         pad_width_by_index = self._pad_options_dim_to_index(pad_width)
1531 
1532         # create pad_options_kwargs, numpy/dask requires only relevant kwargs to be nonempty
1533         pad_option_kwargs: dict[str, Any] = {}
1534         if stat_length is not None:
1535             pad_option_kwargs["stat_length"] = stat_length
1536         if constant_values is not None:
1537             pad_option_kwargs["constant_values"] = constant_values
1538         if end_values is not None:
1539             pad_option_kwargs["end_values"] = end_values
1540         if reflect_type is not None:
1541             pad_option_kwargs["reflect_type"] = reflect_type
1542 
1543         array = np.pad(
1544             self.data.astype(dtype, copy=False),
1545             pad_width_by_index,
1546             mode=mode,
1547             **pad_option_kwargs,
1548         )
1549 
1550         if keep_attrs is None:
1551             keep_attrs = _get_keep_attrs(default=True)
1552         attrs = self._attrs if keep_attrs else None
1553 
1554         return type(self)(self.dims, array, attrs=attrs)
1555 
1556     def _roll_one_dim(self, dim, count):
1557         axis = self.get_axis_num(dim)
1558 
1559         count %= self.shape[axis]
1560         if count != 0:
1561             indices = [slice(-count, None), slice(None, -count)]
1562         else:
1563             indices = [slice(None)]
1564 
1565         arrays = [self[(slice(None),) * axis + (idx,)].data for idx in indices]
1566 
1567         data = duck_array_ops.concatenate(arrays, axis)
1568 
1569         if is_duck_dask_array(data):
1570             # chunked data should come out with the same chunks; this makes
1571             # it feasible to combine shifted and unshifted data
1572             # TODO: remove this once dask.array automatically aligns chunks
1573             data = data.rechunk(self.data.chunks)
1574 
1575         return self._replace(data=data)
1576 
1577     def roll(self, shifts=None, **shifts_kwargs):
1578         """
1579         Return a new Variable with rolld data.
1580 
1581         Parameters
1582         ----------
1583         shifts : mapping of hashable to int
1584             Integer offset to roll along each of the given dimensions.
1585             Positive offsets roll to the right; negative offsets roll to the
1586             left.
1587         **shifts_kwargs
1588             The keyword arguments form of ``shifts``.
1589             One of shifts or shifts_kwargs must be provided.
1590 
1591         Returns
1592         -------
1593         shifted : Variable
1594             Variable with the same dimensions and attributes but rolled data.
1595         """
1596         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
1597 
1598         result = self
1599         for dim, count in shifts.items():
1600             result = result._roll_one_dim(dim, count)
1601         return result
1602 
1603     def transpose(
1604         self,
1605         *dims: Hashable | ellipsis,
1606         missing_dims: ErrorOptionsWithWarn = "raise",
1607     ) -> Variable:
1608         """Return a new Variable object with transposed dimensions.
1609 
1610         Parameters
1611         ----------
1612         *dims : Hashable, optional
1613             By default, reverse the dimensions. Otherwise, reorder the
1614             dimensions to this order.
1615         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1616             What to do if dimensions that should be selected from are not present in the
1617             Variable:
1618             - "raise": raise an exception
1619             - "warn": raise a warning, and ignore the missing dimensions
1620             - "ignore": ignore the missing dimensions
1621 
1622         Returns
1623         -------
1624         transposed : Variable
1625             The returned object has transposed data and dimensions with the
1626             same attributes as the original.
1627 
1628         Notes
1629         -----
1630         This operation returns a view of this variable's data. It is
1631         lazy for dask-backed Variables but not for numpy-backed Variables.
1632 
1633         See Also
1634         --------
1635         numpy.transpose
1636         """
1637         if len(dims) == 0:
1638             dims = self.dims[::-1]
1639         else:
1640             dims = tuple(infix_dims(dims, self.dims, missing_dims))
1641 
1642         if len(dims) < 2 or dims == self.dims:
1643             # no need to transpose if only one dimension
1644             # or dims are in same order
1645             return self.copy(deep=False)
1646 
1647         axes = self.get_axis_num(dims)
1648         data = as_indexable(self._data).transpose(axes)
1649         return self._replace(dims=dims, data=data)
1650 
1651     @property
1652     def T(self) -> Variable:
1653         return self.transpose()
1654 
1655     def set_dims(self, dims, shape=None):
1656         """Return a new variable with given set of dimensions.
1657         This method might be used to attach new dimension(s) to variable.
1658 
1659         When possible, this operation does not copy this variable's data.
1660 
1661         Parameters
1662         ----------
1663         dims : str or sequence of str or dict
1664             Dimensions to include on the new variable. If a dict, values are
1665             used to provide the sizes of new dimensions; otherwise, new
1666             dimensions are inserted with length 1.
1667 
1668         Returns
1669         -------
1670         Variable
1671         """
1672         if isinstance(dims, str):
1673             dims = [dims]
1674 
1675         if shape is None and utils.is_dict_like(dims):
1676             shape = dims.values()
1677 
1678         missing_dims = set(self.dims) - set(dims)
1679         if missing_dims:
1680             raise ValueError(
1681                 f"new dimensions {dims!r} must be a superset of "
1682                 f"existing dimensions {self.dims!r}"
1683             )
1684 
1685         self_dims = set(self.dims)
1686         expanded_dims = tuple(d for d in dims if d not in self_dims) + self.dims
1687 
1688         if self.dims == expanded_dims:
1689             # don't use broadcast_to unless necessary so the result remains
1690             # writeable if possible
1691             expanded_data = self.data
1692         elif shape is not None:
1693             dims_map = dict(zip(dims, shape))
1694             tmp_shape = tuple(dims_map[d] for d in expanded_dims)
1695             expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)
1696         else:
1697             expanded_data = self.data[(None,) * (len(expanded_dims) - self.ndim)]
1698 
1699         expanded_var = Variable(
1700             expanded_dims, expanded_data, self._attrs, self._encoding, fastpath=True
1701         )
1702         return expanded_var.transpose(*dims)
1703 
1704     def _stack_once(self, dims: list[Hashable], new_dim: Hashable):
1705         if not set(dims) <= set(self.dims):
1706             raise ValueError(f"invalid existing dimensions: {dims}")
1707 
1708         if new_dim in self.dims:
1709             raise ValueError(
1710                 "cannot create a new dimension with the same "
1711                 "name as an existing dimension"
1712             )
1713 
1714         if len(dims) == 0:
1715             # don't stack
1716             return self.copy(deep=False)
1717 
1718         other_dims = [d for d in self.dims if d not in dims]
1719         dim_order = other_dims + list(dims)
1720         reordered = self.transpose(*dim_order)
1721 
1722         new_shape = reordered.shape[: len(other_dims)] + (-1,)
1723         new_data = duck_array_ops.reshape(reordered.data, new_shape)
1724         new_dims = reordered.dims[: len(other_dims)] + (new_dim,)
1725 
1726         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)
1727 
1728     def stack(self, dimensions=None, **dimensions_kwargs):
1729         """
1730         Stack any number of existing dimensions into a single new dimension.
1731 
1732         New dimensions will be added at the end, and the order of the data
1733         along each new dimension will be in contiguous (C) order.
1734 
1735         Parameters
1736         ----------
1737         dimensions : mapping of hashable to tuple of hashable
1738             Mapping of form new_name=(dim1, dim2, ...) describing the
1739             names of new dimensions, and the existing dimensions that
1740             they replace.
1741         **dimensions_kwargs
1742             The keyword arguments form of ``dimensions``.
1743             One of dimensions or dimensions_kwargs must be provided.
1744 
1745         Returns
1746         -------
1747         stacked : Variable
1748             Variable with the same attributes but stacked data.
1749 
1750         See Also
1751         --------
1752         Variable.unstack
1753         """
1754         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
1755         result = self
1756         for new_dim, dims in dimensions.items():
1757             result = result._stack_once(dims, new_dim)
1758         return result
1759 
1760     def _unstack_once_full(
1761         self, dims: Mapping[Any, int], old_dim: Hashable
1762     ) -> Variable:
1763         """
1764         Unstacks the variable without needing an index.
1765 
1766         Unlike `_unstack_once`, this function requires the existing dimension to
1767         contain the full product of the new dimensions.
1768         """
1769         new_dim_names = tuple(dims.keys())
1770         new_dim_sizes = tuple(dims.values())
1771 
1772         if old_dim not in self.dims:
1773             raise ValueError(f"invalid existing dimension: {old_dim}")
1774 
1775         if set(new_dim_names).intersection(self.dims):
1776             raise ValueError(
1777                 "cannot create a new dimension with the same "
1778                 "name as an existing dimension"
1779             )
1780 
1781         if math.prod(new_dim_sizes) != self.sizes[old_dim]:
1782             raise ValueError(
1783                 "the product of the new dimension sizes must "
1784                 "equal the size of the old dimension"
1785             )
1786 
1787         other_dims = [d for d in self.dims if d != old_dim]
1788         dim_order = other_dims + [old_dim]
1789         reordered = self.transpose(*dim_order)
1790 
1791         new_shape = reordered.shape[: len(other_dims)] + new_dim_sizes
1792         new_data = reordered.data.reshape(new_shape)
1793         new_dims = reordered.dims[: len(other_dims)] + new_dim_names
1794 
1795         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)
1796 
1797     def _unstack_once(
1798         self,
1799         index: pd.MultiIndex,
1800         dim: Hashable,
1801         fill_value=dtypes.NA,
1802         sparse: bool = False,
1803     ) -> Variable:
1804         """
1805         Unstacks this variable given an index to unstack and the name of the
1806         dimension to which the index refers.
1807         """
1808 
1809         reordered = self.transpose(..., dim)
1810 
1811         new_dim_sizes = [lev.size for lev in index.levels]
1812         new_dim_names = index.names
1813         indexer = index.codes
1814 
1815         # Potentially we could replace `len(other_dims)` with just `-1`
1816         other_dims = [d for d in self.dims if d != dim]
1817         new_shape = tuple(list(reordered.shape[: len(other_dims)]) + new_dim_sizes)
1818         new_dims = reordered.dims[: len(other_dims)] + new_dim_names
1819 
1820         if fill_value is dtypes.NA:
1821             is_missing_values = math.prod(new_shape) > math.prod(self.shape)
1822             if is_missing_values:
1823                 dtype, fill_value = dtypes.maybe_promote(self.dtype)
1824             else:
1825                 dtype = self.dtype
1826                 fill_value = dtypes.get_fill_value(dtype)
1827         else:
1828             dtype = self.dtype
1829 
1830         if sparse:
1831             # unstacking a dense multitindexed array to a sparse array
1832             from sparse import COO
1833 
1834             codes = zip(*index.codes)
1835             if reordered.ndim == 1:
1836                 indexes = codes
1837             else:
1838                 sizes = itertools.product(*[range(s) for s in reordered.shape[:-1]])
1839                 tuple_indexes = itertools.product(sizes, codes)
1840                 indexes = map(lambda x: list(itertools.chain(*x)), tuple_indexes)  # type: ignore
1841 
1842             data = COO(
1843                 coords=np.array(list(indexes)).T,
1844                 data=self.data.astype(dtype).ravel(),
1845                 fill_value=fill_value,
1846                 shape=new_shape,
1847                 sorted=index.is_monotonic_increasing,
1848             )
1849 
1850         else:
1851             data = np.full_like(
1852                 self.data,
1853                 fill_value=fill_value,
1854                 shape=new_shape,
1855                 dtype=dtype,
1856             )
1857 
1858             # Indexer is a list of lists of locations. Each list is the locations
1859             # on the new dimension. This is robust to the data being sparse; in that
1860             # case the destinations will be NaN / zero.
1861             data[(..., *indexer)] = reordered
1862 
1863         return self._replace(dims=new_dims, data=data)
1864 
1865     def unstack(self, dimensions=None, **dimensions_kwargs):
1866         """
1867         Unstack an existing dimension into multiple new dimensions.
1868 
1869         New dimensions will be added at the end, and the order of the data
1870         along each new dimension will be in contiguous (C) order.
1871 
1872         Note that unlike ``DataArray.unstack`` and ``Dataset.unstack``, this
1873         method requires the existing dimension to contain the full product of
1874         the new dimensions.
1875 
1876         Parameters
1877         ----------
1878         dimensions : mapping of hashable to mapping of hashable to int
1879             Mapping of the form old_dim={dim1: size1, ...} describing the
1880             names of existing dimensions, and the new dimensions and sizes
1881             that they map to.
1882         **dimensions_kwargs
1883             The keyword arguments form of ``dimensions``.
1884             One of dimensions or dimensions_kwargs must be provided.
1885 
1886         Returns
1887         -------
1888         unstacked : Variable
1889             Variable with the same attributes but unstacked data.
1890 
1891         See Also
1892         --------
1893         Variable.stack
1894         DataArray.unstack
1895         Dataset.unstack
1896         """
1897         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "unstack")
1898         result = self
1899         for old_dim, dims in dimensions.items():
1900             result = result._unstack_once_full(dims, old_dim)
1901         return result
1902 
1903     def fillna(self, value):
1904         return ops.fillna(self, value)
1905 
1906     def where(self, cond, other=dtypes.NA):
1907         return ops.where_method(self, cond, other)
1908 
1909     def clip(self, min=None, max=None):
1910         """
1911         Return an array whose values are limited to ``[min, max]``.
1912         At least one of max or min must be given.
1913 
1914         Refer to `numpy.clip` for full documentation.
1915 
1916         See Also
1917         --------
1918         numpy.clip : equivalent function
1919         """
1920         from xarray.core.computation import apply_ufunc
1921 
1922         return apply_ufunc(np.clip, self, min, max, dask="allowed")
1923 
1924     def reduce(
1925         self,
1926         func: Callable[..., Any],
1927         dim: Dims = None,
1928         axis: int | Sequence[int] | None = None,
1929         keep_attrs: bool | None = None,
1930         keepdims: bool = False,
1931         **kwargs,
1932     ) -> Variable:
1933         """Reduce this array by applying `func` along some dimension(s).
1934 
1935         Parameters
1936         ----------
1937         func : callable
1938             Function which can be called in the form
1939             `func(x, axis=axis, **kwargs)` to return the result of reducing an
1940             np.ndarray over an integer valued axis.
1941         dim : "...", str, Iterable of Hashable or None, optional
1942             Dimension(s) over which to apply `func`. By default `func` is
1943             applied over all dimensions.
1944         axis : int or Sequence of int, optional
1945             Axis(es) over which to apply `func`. Only one of the 'dim'
1946             and 'axis' arguments can be supplied. If neither are supplied, then
1947             the reduction is calculated over the flattened array (by calling
1948             `func(x)` without an axis argument).
1949         keep_attrs : bool, optional
1950             If True, the variable's attributes (`attrs`) will be copied from
1951             the original object to the new one.  If False (default), the new
1952             object will be returned without attributes.
1953         keepdims : bool, default: False
1954             If True, the dimensions which are reduced are left in the result
1955             as dimensions of size one
1956         **kwargs : dict
1957             Additional keyword arguments passed on to `func`.
1958 
1959         Returns
1960         -------
1961         reduced : Array
1962             Array with summarized data and the indicated dimension(s)
1963             removed.
1964         """
1965         if dim == ...:
1966             dim = None
1967         if dim is not None and axis is not None:
1968             raise ValueError("cannot supply both 'axis' and 'dim' arguments")
1969 
1970         if dim is not None:
1971             axis = self.get_axis_num(dim)
1972 
1973         with warnings.catch_warnings():
1974             warnings.filterwarnings(
1975                 "ignore", r"Mean of empty slice", category=RuntimeWarning
1976             )
1977             if axis is not None:
1978                 if isinstance(axis, tuple) and len(axis) == 1:
1979                     # unpack axis for the benefit of functions
1980                     # like np.argmin which can't handle tuple arguments
1981                     axis = axis[0]
1982                 data = func(self.data, axis=axis, **kwargs)
1983             else:
1984                 data = func(self.data, **kwargs)
1985 
1986         if getattr(data, "shape", ()) == self.shape:
1987             dims = self.dims
1988         else:
1989             removed_axes: Iterable[int]
1990             if axis is None:
1991                 removed_axes = range(self.ndim)
1992             else:
1993                 removed_axes = np.atleast_1d(axis) % self.ndim
1994             if keepdims:
1995                 # Insert np.newaxis for removed dims
1996                 slices = tuple(
1997                     np.newaxis if i in removed_axes else slice(None, None)
1998                     for i in range(self.ndim)
1999                 )
2000                 if getattr(data, "shape", None) is None:
2001                     # Reduce has produced a scalar value, not an array-like
2002                     data = np.asanyarray(data)[slices]
2003                 else:
2004                     data = data[slices]
2005                 dims = self.dims
2006             else:
2007                 dims = tuple(
2008                     adim for n, adim in enumerate(self.dims) if n not in removed_axes
2009                 )
2010 
2011         if keep_attrs is None:
2012             keep_attrs = _get_keep_attrs(default=False)
2013         attrs = self._attrs if keep_attrs else None
2014 
2015         return Variable(dims, data, attrs=attrs)
2016 
2017     @classmethod
2018     def concat(
2019         cls,
2020         variables,
2021         dim="concat_dim",
2022         positions=None,
2023         shortcut=False,
2024         combine_attrs="override",
2025     ):
2026         """Concatenate variables along a new or existing dimension.
2027 
2028         Parameters
2029         ----------
2030         variables : iterable of Variable
2031             Arrays to stack together. Each variable is expected to have
2032             matching dimensions and shape except for along the stacked
2033             dimension.
2034         dim : str or DataArray, optional
2035             Name of the dimension to stack along. This can either be a new
2036             dimension name, in which case it is added along axis=0, or an
2037             existing dimension name, in which case the location of the
2038             dimension is unchanged. Where to insert the new dimension is
2039             determined by the first variable.
2040         positions : None or list of array-like, optional
2041             List of integer arrays which specifies the integer positions to
2042             which to assign each dataset along the concatenated dimension.
2043             If not supplied, objects are concatenated in the provided order.
2044         shortcut : bool, optional
2045             This option is used internally to speed-up groupby operations.
2046             If `shortcut` is True, some checks of internal consistency between
2047             arrays to concatenate are skipped.
2048         combine_attrs : {"drop", "identical", "no_conflicts", "drop_conflicts", \
2049                          "override"}, default: "override"
2050             String indicating how to combine attrs of the objects being merged:
2051 
2052             - "drop": empty attrs on returned Dataset.
2053             - "identical": all attrs must be the same on every object.
2054             - "no_conflicts": attrs from all objects are combined, any that have
2055               the same name must also have the same value.
2056             - "drop_conflicts": attrs from all objects are combined, any that have
2057               the same name but different values are dropped.
2058             - "override": skip comparing and copy attrs from the first dataset to
2059               the result.
2060 
2061         Returns
2062         -------
2063         stacked : Variable
2064             Concatenated Variable formed by stacking all the supplied variables
2065             along the given dimension.
2066         """
2067         from xarray.core.merge import merge_attrs
2068 
2069         if not isinstance(dim, str):
2070             (dim,) = dim.dims
2071 
2072         # can't do this lazily: we need to loop through variables at least
2073         # twice
2074         variables = list(variables)
2075         first_var = variables[0]
2076 
2077         arrays = [v.data for v in variables]
2078 
2079         if dim in first_var.dims:
2080             axis = first_var.get_axis_num(dim)
2081             dims = first_var.dims
2082             data = duck_array_ops.concatenate(arrays, axis=axis)
2083             if positions is not None:
2084                 # TODO: deprecate this option -- we don't need it for groupby
2085                 # any more.
2086                 indices = nputils.inverse_permutation(np.concatenate(positions))
2087                 data = duck_array_ops.take(data, indices, axis=axis)
2088         else:
2089             axis = 0
2090             dims = (dim,) + first_var.dims
2091             data = duck_array_ops.stack(arrays, axis=axis)
2092 
2093         attrs = merge_attrs(
2094             [var.attrs for var in variables], combine_attrs=combine_attrs
2095         )
2096         encoding = dict(first_var.encoding)
2097         if not shortcut:
2098             for var in variables:
2099                 if var.dims != first_var.dims:
2100                     raise ValueError(
2101                         f"Variable has dimensions {list(var.dims)} but first Variable has dimensions {list(first_var.dims)}"
2102                     )
2103 
2104         return cls(dims, data, attrs, encoding)
2105 
2106     def equals(self, other, equiv=duck_array_ops.array_equiv):
2107         """True if two Variables have the same dimensions and values;
2108         otherwise False.
2109 
2110         Variables can still be equal (like pandas objects) if they have NaN
2111         values in the same locations.
2112 
2113         This method is necessary because `v1 == v2` for Variables
2114         does element-wise comparisons (like numpy.ndarrays).
2115         """
2116         other = getattr(other, "variable", other)
2117         try:
2118             return self.dims == other.dims and (
2119                 self._data is other._data or equiv(self.data, other.data)
2120             )
2121         except (TypeError, AttributeError):
2122             return False
2123 
2124     def broadcast_equals(self, other, equiv=duck_array_ops.array_equiv):
2125         """True if two Variables have the values after being broadcast against
2126         each other; otherwise False.
2127 
2128         Variables can still be equal (like pandas objects) if they have NaN
2129         values in the same locations.
2130         """
2131         try:
2132             self, other = broadcast_variables(self, other)
2133         except (ValueError, AttributeError):
2134             return False
2135         return self.equals(other, equiv=equiv)
2136 
2137     def identical(self, other, equiv=duck_array_ops.array_equiv):
2138         """Like equals, but also checks attributes."""
2139         try:
2140             return utils.dict_equiv(self.attrs, other.attrs) and self.equals(
2141                 other, equiv=equiv
2142             )
2143         except (TypeError, AttributeError):
2144             return False
2145 
2146     def no_conflicts(self, other, equiv=duck_array_ops.array_notnull_equiv):
2147         """True if the intersection of two Variable's non-null data is
2148         equal; otherwise false.
2149 
2150         Variables can thus still be equal if there are locations where either,
2151         or both, contain NaN values.
2152         """
2153         return self.broadcast_equals(other, equiv=equiv)
2154 
2155     def quantile(
2156         self,
2157         q: ArrayLike,
2158         dim: str | Sequence[Hashable] | None = None,
2159         method: QuantileMethods = "linear",
2160         keep_attrs: bool | None = None,
2161         skipna: bool | None = None,
2162         interpolation: QuantileMethods | None = None,
2163     ) -> Variable:
2164         """Compute the qth quantile of the data along the specified dimension.
2165 
2166         Returns the qth quantiles(s) of the array elements.
2167 
2168         Parameters
2169         ----------
2170         q : float or sequence of float
2171             Quantile to compute, which must be between 0 and 1
2172             inclusive.
2173         dim : str or sequence of str, optional
2174             Dimension(s) over which to apply quantile.
2175         method : str, default: "linear"
2176             This optional parameter specifies the interpolation method to use when the
2177             desired quantile lies between two data points. The options sorted by their R
2178             type as summarized in the H&F paper [1]_ are:
2179 
2180                 1. "inverted_cdf" (*)
2181                 2. "averaged_inverted_cdf" (*)
2182                 3. "closest_observation" (*)
2183                 4. "interpolated_inverted_cdf" (*)
2184                 5. "hazen" (*)
2185                 6. "weibull" (*)
2186                 7. "linear"  (default)
2187                 8. "median_unbiased" (*)
2188                 9. "normal_unbiased" (*)
2189 
2190             The first three methods are discontiuous.  The following discontinuous
2191             variations of the default "linear" (7.) option are also available:
2192 
2193                 * "lower"
2194                 * "higher"
2195                 * "midpoint"
2196                 * "nearest"
2197 
2198             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
2199             was previously called "interpolation", renamed in accordance with numpy
2200             version 1.22.0.
2201 
2202             (*) These methods require numpy version 1.22 or newer.
2203 
2204         keep_attrs : bool, optional
2205             If True, the variable's attributes (`attrs`) will be copied from
2206             the original object to the new one.  If False (default), the new
2207             object will be returned without attributes.
2208         skipna : bool, optional
2209             If True, skip missing values (as marked by NaN). By default, only
2210             skips missing values for float dtypes; other dtypes either do not
2211             have a sentinel missing value (int) or skipna=True has not been
2212             implemented (object, datetime64 or timedelta64).
2213 
2214         Returns
2215         -------
2216         quantiles : Variable
2217             If `q` is a single quantile, then the result
2218             is a scalar. If multiple percentiles are given, first axis of
2219             the result corresponds to the quantile and a quantile dimension
2220             is added to the return array. The other dimensions are the
2221             dimensions that remain after the reduction of the array.
2222 
2223         See Also
2224         --------
2225         numpy.nanquantile, pandas.Series.quantile, Dataset.quantile
2226         DataArray.quantile
2227 
2228         References
2229         ----------
2230         .. [1] R. J. Hyndman and Y. Fan,
2231            "Sample quantiles in statistical packages,"
2232            The American Statistician, 50(4), pp. 361-365, 1996
2233         """
2234 
2235         from xarray.core.computation import apply_ufunc
2236 
2237         if interpolation is not None:
2238             warnings.warn(
2239                 "The `interpolation` argument to quantile was renamed to `method`.",
2240                 FutureWarning,
2241             )
2242 
2243             if method != "linear":
2244                 raise TypeError("Cannot pass interpolation and method keywords!")
2245 
2246             method = interpolation
2247 
2248         if skipna or (skipna is None and self.dtype.kind in "cfO"):
2249             _quantile_func = np.nanquantile
2250         else:
2251             _quantile_func = np.quantile
2252 
2253         if keep_attrs is None:
2254             keep_attrs = _get_keep_attrs(default=False)
2255 
2256         scalar = utils.is_scalar(q)
2257         q = np.atleast_1d(np.asarray(q, dtype=np.float64))
2258 
2259         if dim is None:
2260             dim = self.dims
2261 
2262         if utils.is_scalar(dim):
2263             dim = [dim]
2264 
2265         def _wrapper(npa, **kwargs):
2266             # move quantile axis to end. required for apply_ufunc
2267             return np.moveaxis(_quantile_func(npa, **kwargs), 0, -1)
2268 
2269         axis = np.arange(-1, -1 * len(dim) - 1, -1)
2270 
2271         if Version(np.__version__) >= Version("1.22.0"):
2272             kwargs = {"q": q, "axis": axis, "method": method}
2273         else:
2274             if method not in ("linear", "lower", "higher", "midpoint", "nearest"):
2275                 raise ValueError(
2276                     f"Interpolation method '{method}' requires numpy >= 1.22 or is not supported."
2277                 )
2278             kwargs = {"q": q, "axis": axis, "interpolation": method}
2279 
2280         result = apply_ufunc(
2281             _wrapper,
2282             self,
2283             input_core_dims=[dim],
2284             exclude_dims=set(dim),
2285             output_core_dims=[["quantile"]],
2286             output_dtypes=[np.float64],
2287             dask_gufunc_kwargs=dict(output_sizes={"quantile": len(q)}),
2288             dask="parallelized",
2289             kwargs=kwargs,
2290         )
2291 
2292         # for backward compatibility
2293         result = result.transpose("quantile", ...)
2294         if scalar:
2295             result = result.squeeze("quantile")
2296         if keep_attrs:
2297             result.attrs = self._attrs
2298         return result
2299 
2300     def rank(self, dim, pct=False):
2301         """Ranks the data.
2302 
2303         Equal values are assigned a rank that is the average of the ranks that
2304         would have been otherwise assigned to all of the values within that
2305         set.  Ranks begin at 1, not 0. If `pct`, computes percentage ranks.
2306 
2307         NaNs in the input array are returned as NaNs.
2308 
2309         The `bottleneck` library is required.
2310 
2311         Parameters
2312         ----------
2313         dim : str
2314             Dimension over which to compute rank.
2315         pct : bool, optional
2316             If True, compute percentage ranks, otherwise compute integer ranks.
2317 
2318         Returns
2319         -------
2320         ranked : Variable
2321 
2322         See Also
2323         --------
2324         Dataset.rank, DataArray.rank
2325         """
2326         if not OPTIONS["use_bottleneck"]:
2327             raise RuntimeError(
2328                 "rank requires bottleneck to be enabled."
2329                 " Call `xr.set_options(use_bottleneck=True)` to enable it."
2330             )
2331 
2332         import bottleneck as bn
2333 
2334         data = self.data
2335 
2336         if is_duck_dask_array(data):
2337             raise TypeError(
2338                 "rank does not work for arrays stored as dask "
2339                 "arrays. Load the data via .compute() or .load() "
2340                 "prior to calling this method."
2341             )
2342         elif not isinstance(data, np.ndarray):
2343             raise TypeError(f"rank is not implemented for {type(data)} objects.")
2344 
2345         axis = self.get_axis_num(dim)
2346         func = bn.nanrankdata if self.dtype.kind == "f" else bn.rankdata
2347         ranked = func(data, axis=axis)
2348         if pct:
2349             count = np.sum(~np.isnan(data), axis=axis, keepdims=True)
2350             ranked /= count
2351         return Variable(self.dims, ranked)
2352 
2353     def rolling_window(
2354         self, dim, window, window_dim, center=False, fill_value=dtypes.NA
2355     ):
2356         """
2357         Make a rolling_window along dim and add a new_dim to the last place.
2358 
2359         Parameters
2360         ----------
2361         dim : str
2362             Dimension over which to compute rolling_window.
2363             For nd-rolling, should be list of dimensions.
2364         window : int
2365             Window size of the rolling
2366             For nd-rolling, should be list of integers.
2367         window_dim : str
2368             New name of the window dimension.
2369             For nd-rolling, should be list of strings.
2370         center : bool, default: False
2371             If True, pad fill_value for both ends. Otherwise, pad in the head
2372             of the axis.
2373         fill_value
2374             value to be filled.
2375 
2376         Returns
2377         -------
2378         Variable that is a view of the original array with a added dimension of
2379         size w.
2380         The return dim: self.dims + (window_dim, )
2381         The return shape: self.shape + (window, )
2382 
2383         Examples
2384         --------
2385         >>> v = Variable(("a", "b"), np.arange(8).reshape((2, 4)))
2386         >>> v.rolling_window("b", 3, "window_dim")
2387         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
2388         array([[[nan, nan,  0.],
2389                 [nan,  0.,  1.],
2390                 [ 0.,  1.,  2.],
2391                 [ 1.,  2.,  3.]],
2392         <BLANKLINE>
2393                [[nan, nan,  4.],
2394                 [nan,  4.,  5.],
2395                 [ 4.,  5.,  6.],
2396                 [ 5.,  6.,  7.]]])
2397 
2398         >>> v.rolling_window("b", 3, "window_dim", center=True)
2399         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
2400         array([[[nan,  0.,  1.],
2401                 [ 0.,  1.,  2.],
2402                 [ 1.,  2.,  3.],
2403                 [ 2.,  3., nan]],
2404         <BLANKLINE>
2405                [[nan,  4.,  5.],
2406                 [ 4.,  5.,  6.],
2407                 [ 5.,  6.,  7.],
2408                 [ 6.,  7., nan]]])
2409         """
2410         if fill_value is dtypes.NA:  # np.nan is passed
2411             dtype, fill_value = dtypes.maybe_promote(self.dtype)
2412             var = self.astype(dtype, copy=False)
2413         else:
2414             dtype = self.dtype
2415             var = self
2416 
2417         if utils.is_scalar(dim):
2418             for name, arg in zip(
2419                 ["window", "window_dim", "center"], [window, window_dim, center]
2420             ):
2421                 if not utils.is_scalar(arg):
2422                     raise ValueError(
2423                         f"Expected {name}={arg!r} to be a scalar like 'dim'."
2424                     )
2425             dim = [dim]
2426 
2427         # dim is now a list
2428         nroll = len(dim)
2429         if utils.is_scalar(window):
2430             window = [window] * nroll
2431         if utils.is_scalar(window_dim):
2432             window_dim = [window_dim] * nroll
2433         if utils.is_scalar(center):
2434             center = [center] * nroll
2435         if (
2436             len(dim) != len(window)
2437             or len(dim) != len(window_dim)
2438             or len(dim) != len(center)
2439         ):
2440             raise ValueError(
2441                 "'dim', 'window', 'window_dim', and 'center' must be the same length. "
2442                 f"Received dim={dim!r}, window={window!r}, window_dim={window_dim!r},"
2443                 f" and center={center!r}."
2444             )
2445 
2446         pads = {}
2447         for d, win, cent in zip(dim, window, center):
2448             if cent:
2449                 start = win // 2  # 10 -> 5,  9 -> 4
2450                 end = win - 1 - start
2451                 pads[d] = (start, end)
2452             else:
2453                 pads[d] = (win - 1, 0)
2454 
2455         padded = var.pad(pads, mode="constant", constant_values=fill_value)
2456         axis = [self.get_axis_num(d) for d in dim]
2457         new_dims = self.dims + tuple(window_dim)
2458         return Variable(
2459             new_dims,
2460             duck_array_ops.sliding_window_view(
2461                 padded.data, window_shape=window, axis=axis
2462             ),
2463         )
2464 
2465     def coarsen(
2466         self, windows, func, boundary="exact", side="left", keep_attrs=None, **kwargs
2467     ):
2468         """
2469         Apply reduction function.
2470         """
2471         windows = {k: v for k, v in windows.items() if k in self.dims}
2472 
2473         if keep_attrs is None:
2474             keep_attrs = _get_keep_attrs(default=True)
2475 
2476         if keep_attrs:
2477             _attrs = self.attrs
2478         else:
2479             _attrs = None
2480 
2481         if not windows:
2482             return self._replace(attrs=_attrs)
2483 
2484         reshaped, axes = self.coarsen_reshape(windows, boundary, side)
2485         if isinstance(func, str):
2486             name = func
2487             func = getattr(duck_array_ops, name, None)
2488             if func is None:
2489                 raise NameError(f"{name} is not a valid method.")
2490 
2491         return self._replace(data=func(reshaped, axis=axes, **kwargs), attrs=_attrs)
2492 
2493     def coarsen_reshape(self, windows, boundary, side):
2494         """
2495         Construct a reshaped-array for coarsen
2496         """
2497         if not utils.is_dict_like(boundary):
2498             boundary = {d: boundary for d in windows.keys()}
2499 
2500         if not utils.is_dict_like(side):
2501             side = {d: side for d in windows.keys()}
2502 
2503         # remove unrelated dimensions
2504         boundary = {k: v for k, v in boundary.items() if k in windows}
2505         side = {k: v for k, v in side.items() if k in windows}
2506 
2507         for d, window in windows.items():
2508             if window <= 0:
2509                 raise ValueError(
2510                     f"window must be > 0. Given {window} for dimension {d}"
2511                 )
2512 
2513         variable = self
2514         for d, window in windows.items():
2515             # trim or pad the object
2516             size = variable.shape[self._get_axis_num(d)]
2517             n = int(size / window)
2518             if boundary[d] == "exact":
2519                 if n * window != size:
2520                     raise ValueError(
2521                         f"Could not coarsen a dimension of size {size} with "
2522                         f"window {window} and boundary='exact'. Try a different 'boundary' option."
2523                     )
2524             elif boundary[d] == "trim":
2525                 if side[d] == "left":
2526                     variable = variable.isel({d: slice(0, window * n)})
2527                 else:
2528                     excess = size - window * n
2529                     variable = variable.isel({d: slice(excess, None)})
2530             elif boundary[d] == "pad":  # pad
2531                 pad = window * n - size
2532                 if pad < 0:
2533                     pad += window
2534                 if side[d] == "left":
2535                     pad_width = {d: (0, pad)}
2536                 else:
2537                     pad_width = {d: (pad, 0)}
2538                 variable = variable.pad(pad_width, mode="constant")
2539             else:
2540                 raise TypeError(
2541                     "{} is invalid for boundary. Valid option is 'exact', "
2542                     "'trim' and 'pad'".format(boundary[d])
2543                 )
2544 
2545         shape = []
2546         axes = []
2547         axis_count = 0
2548         for i, d in enumerate(variable.dims):
2549             if d in windows:
2550                 size = variable.shape[i]
2551                 shape.append(int(size / windows[d]))
2552                 shape.append(windows[d])
2553                 axis_count += 1
2554                 axes.append(i + axis_count)
2555             else:
2556                 shape.append(variable.shape[i])
2557 
2558         return variable.data.reshape(shape), tuple(axes)
2559 
2560     def isnull(self, keep_attrs: bool | None = None):
2561         """Test each value in the array for whether it is a missing value.
2562 
2563         Returns
2564         -------
2565         isnull : Variable
2566             Same type and shape as object, but the dtype of the data is bool.
2567 
2568         See Also
2569         --------
2570         pandas.isnull
2571 
2572         Examples
2573         --------
2574         >>> var = xr.Variable("x", [1, np.nan, 3])
2575         >>> var
2576         <xarray.Variable (x: 3)>
2577         array([ 1., nan,  3.])
2578         >>> var.isnull()
2579         <xarray.Variable (x: 3)>
2580         array([False,  True, False])
2581         """
2582         from xarray.core.computation import apply_ufunc
2583 
2584         if keep_attrs is None:
2585             keep_attrs = _get_keep_attrs(default=False)
2586 
2587         return apply_ufunc(
2588             duck_array_ops.isnull,
2589             self,
2590             dask="allowed",
2591             keep_attrs=keep_attrs,
2592         )
2593 
2594     def notnull(self, keep_attrs: bool | None = None):
2595         """Test each value in the array for whether it is not a missing value.
2596 
2597         Returns
2598         -------
2599         notnull : Variable
2600             Same type and shape as object, but the dtype of the data is bool.
2601 
2602         See Also
2603         --------
2604         pandas.notnull
2605 
2606         Examples
2607         --------
2608         >>> var = xr.Variable("x", [1, np.nan, 3])
2609         >>> var
2610         <xarray.Variable (x: 3)>
2611         array([ 1., nan,  3.])
2612         >>> var.notnull()
2613         <xarray.Variable (x: 3)>
2614         array([ True, False,  True])
2615         """
2616         from xarray.core.computation import apply_ufunc
2617 
2618         if keep_attrs is None:
2619             keep_attrs = _get_keep_attrs(default=False)
2620 
2621         return apply_ufunc(
2622             duck_array_ops.notnull,
2623             self,
2624             dask="allowed",
2625             keep_attrs=keep_attrs,
2626         )
2627 
2628     @property
2629     def real(self):
2630         """
2631         The real part of the variable.
2632 
2633         See Also
2634         --------
2635         numpy.ndarray.real
2636         """
2637         return self._replace(data=self.data.real)
2638 
2639     @property
2640     def imag(self):
2641         """
2642         The imaginary part of the variable.
2643 
2644         See Also
2645         --------
2646         numpy.ndarray.imag
2647         """
2648         return self._replace(data=self.data.imag)
2649 
2650     def __array_wrap__(self, obj, context=None):
2651         return Variable(self.dims, obj)
2652 
2653     def _unary_op(self, f, *args, **kwargs):
2654         keep_attrs = kwargs.pop("keep_attrs", None)
2655         if keep_attrs is None:
2656             keep_attrs = _get_keep_attrs(default=True)
2657         with np.errstate(all="ignore"):
2658             result = self.__array_wrap__(f(self.data, *args, **kwargs))
2659             if keep_attrs:
2660                 result.attrs = self.attrs
2661             return result
2662 
2663     def _binary_op(self, other, f, reflexive=False):
2664         if isinstance(other, (xr.DataArray, xr.Dataset)):
2665             return NotImplemented
2666         if reflexive and issubclass(type(self), type(other)):
2667             other_data, self_data, dims = _broadcast_compat_data(other, self)
2668         else:
2669             self_data, other_data, dims = _broadcast_compat_data(self, other)
2670         keep_attrs = _get_keep_attrs(default=False)
2671         attrs = self._attrs if keep_attrs else None
2672         with np.errstate(all="ignore"):
2673             new_data = (
2674                 f(self_data, other_data) if not reflexive else f(other_data, self_data)
2675             )
2676         result = Variable(dims, new_data, attrs=attrs)
2677         return result
2678 
2679     def _inplace_binary_op(self, other, f):
2680         if isinstance(other, xr.Dataset):
2681             raise TypeError("cannot add a Dataset to a Variable in-place")
2682         self_data, other_data, dims = _broadcast_compat_data(self, other)
2683         if dims != self.dims:
2684             raise ValueError("dimensions cannot change for in-place operations")
2685         with np.errstate(all="ignore"):
2686             self.values = f(self_data, other_data)
2687         return self
2688 
2689     def _to_numeric(self, offset=None, datetime_unit=None, dtype=float):
2690         """A (private) method to convert datetime array to numeric dtype
2691         See duck_array_ops.datetime_to_numeric
2692         """
2693         numeric_array = duck_array_ops.datetime_to_numeric(
2694             self.data, offset, datetime_unit, dtype
2695         )
2696         return type(self)(self.dims, numeric_array, self._attrs)
2697 
2698     def _unravel_argminmax(
2699         self,
2700         argminmax: str,
2701         dim: Dims,
2702         axis: int | None,
2703         keep_attrs: bool | None,
2704         skipna: bool | None,
2705     ) -> Variable | dict[Hashable, Variable]:
2706         """Apply argmin or argmax over one or more dimensions, returning the result as a
2707         dict of DataArray that can be passed directly to isel.
2708         """
2709         if dim is None and axis is None:
2710             warnings.warn(
2711                 "Behaviour of argmin/argmax with neither dim nor axis argument will "
2712                 "change to return a dict of indices of each dimension. To get a "
2713                 "single, flat index, please use np.argmin(da.data) or "
2714                 "np.argmax(da.data) instead of da.argmin() or da.argmax().",
2715                 DeprecationWarning,
2716                 stacklevel=3,
2717             )
2718 
2719         argminmax_func = getattr(duck_array_ops, argminmax)
2720 
2721         if dim is ...:
2722             # In future, should do this also when (dim is None and axis is None)
2723             dim = self.dims
2724         if (
2725             dim is None
2726             or axis is not None
2727             or not isinstance(dim, Sequence)
2728             or isinstance(dim, str)
2729         ):
2730             # Return int index if single dimension is passed, and is not part of a
2731             # sequence
2732             return self.reduce(
2733                 argminmax_func, dim=dim, axis=axis, keep_attrs=keep_attrs, skipna=skipna
2734             )
2735 
2736         # Get a name for the new dimension that does not conflict with any existing
2737         # dimension
2738         newdimname = "_unravel_argminmax_dim_0"
2739         count = 1
2740         while newdimname in self.dims:
2741             newdimname = f"_unravel_argminmax_dim_{count}"
2742             count += 1
2743 
2744         stacked = self.stack({newdimname: dim})
2745 
2746         result_dims = stacked.dims[:-1]
2747         reduce_shape = tuple(self.sizes[d] for d in dim)
2748 
2749         result_flat_indices = stacked.reduce(argminmax_func, axis=-1, skipna=skipna)
2750 
2751         result_unravelled_indices = duck_array_ops.unravel_index(
2752             result_flat_indices.data, reduce_shape
2753         )
2754 
2755         result = {
2756             d: Variable(dims=result_dims, data=i)
2757             for d, i in zip(dim, result_unravelled_indices)
2758         }
2759 
2760         if keep_attrs is None:
2761             keep_attrs = _get_keep_attrs(default=False)
2762         if keep_attrs:
2763             for v in result.values():
2764                 v.attrs = self.attrs
2765 
2766         return result
2767 
2768     def argmin(
2769         self,
2770         dim: Dims = None,
2771         axis: int | None = None,
2772         keep_attrs: bool | None = None,
2773         skipna: bool | None = None,
2774     ) -> Variable | dict[Hashable, Variable]:
2775         """Index or indices of the minimum of the Variable over one or more dimensions.
2776         If a sequence is passed to 'dim', then result returned as dict of Variables,
2777         which can be passed directly to isel(). If a single str is passed to 'dim' then
2778         returns a Variable with dtype int.
2779 
2780         If there are multiple minima, the indices of the first one found will be
2781         returned.
2782 
2783         Parameters
2784         ----------
2785         dim : "...", str, Iterable of Hashable or None, optional
2786             The dimensions over which to find the minimum. By default, finds minimum over
2787             all dimensions - for now returning an int for backward compatibility, but
2788             this is deprecated, in future will return a dict with indices for all
2789             dimensions; to return a dict with all dimensions now, pass '...'.
2790         axis : int, optional
2791             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
2792             can be supplied.
2793         keep_attrs : bool, optional
2794             If True, the attributes (`attrs`) will be copied from the original
2795             object to the new one.  If False (default), the new object will be
2796             returned without attributes.
2797         skipna : bool, optional
2798             If True, skip missing values (as marked by NaN). By default, only
2799             skips missing values for float dtypes; other dtypes either do not
2800             have a sentinel missing value (int) or skipna=True has not been
2801             implemented (object, datetime64 or timedelta64).
2802 
2803         Returns
2804         -------
2805         result : Variable or dict of Variable
2806 
2807         See Also
2808         --------
2809         DataArray.argmin, DataArray.idxmin
2810         """
2811         return self._unravel_argminmax("argmin", dim, axis, keep_attrs, skipna)
2812 
2813     def argmax(
2814         self,
2815         dim: Dims = None,
2816         axis: int | None = None,
2817         keep_attrs: bool | None = None,
2818         skipna: bool | None = None,
2819     ) -> Variable | dict[Hashable, Variable]:
2820         """Index or indices of the maximum of the Variable over one or more dimensions.
2821         If a sequence is passed to 'dim', then result returned as dict of Variables,
2822         which can be passed directly to isel(). If a single str is passed to 'dim' then
2823         returns a Variable with dtype int.
2824 
2825         If there are multiple maxima, the indices of the first one found will be
2826         returned.
2827 
2828         Parameters
2829         ----------
2830         dim : "...", str, Iterable of Hashable or None, optional
2831             The dimensions over which to find the maximum. By default, finds maximum over
2832             all dimensions - for now returning an int for backward compatibility, but
2833             this is deprecated, in future will return a dict with indices for all
2834             dimensions; to return a dict with all dimensions now, pass '...'.
2835         axis : int, optional
2836             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
2837             can be supplied.
2838         keep_attrs : bool, optional
2839             If True, the attributes (`attrs`) will be copied from the original
2840             object to the new one.  If False (default), the new object will be
2841             returned without attributes.
2842         skipna : bool, optional
2843             If True, skip missing values (as marked by NaN). By default, only
2844             skips missing values for float dtypes; other dtypes either do not
2845             have a sentinel missing value (int) or skipna=True has not been
2846             implemented (object, datetime64 or timedelta64).
2847 
2848         Returns
2849         -------
2850         result : Variable or dict of Variable
2851 
2852         See Also
2853         --------
2854         DataArray.argmax, DataArray.idxmax
2855         """
2856         return self._unravel_argminmax("argmax", dim, axis, keep_attrs, skipna)
2857 
2858 
2859 class IndexVariable(Variable):
2860     """Wrapper for accommodating a pandas.Index in an xarray.Variable.
2861 
2862     IndexVariable preserve loaded values in the form of a pandas.Index instead
2863     of a NumPy array. Hence, their values are immutable and must always be one-
2864     dimensional.
2865 
2866     They also have a name property, which is the name of their sole dimension
2867     unless another name is given.
2868     """
2869 
2870     __slots__ = ()
2871 
2872     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):
2873         super().__init__(dims, data, attrs, encoding, fastpath)
2874         if self.ndim != 1:
2875             raise ValueError(f"{type(self).__name__} objects must be 1-dimensional")
2876 
2877         # Unlike in Variable, always eagerly load values into memory
2878         if not isinstance(self._data, PandasIndexingAdapter):
2879             self._data = PandasIndexingAdapter(self._data)
2880 
2881     def __dask_tokenize__(self):
2882         from dask.base import normalize_token
2883 
2884         # Don't waste time converting pd.Index to np.ndarray
2885         return normalize_token((type(self), self._dims, self._data.array, self._attrs))
2886 
2887     def load(self):
2888         # data is already loaded into memory for IndexVariable
2889         return self
2890 
2891     # https://github.com/python/mypy/issues/1465
2892     @Variable.data.setter  # type: ignore[attr-defined]
2893     def data(self, data):
2894         raise ValueError(
2895             f"Cannot assign to the .data attribute of dimension coordinate a.k.a IndexVariable {self.name!r}. "
2896             f"Please use DataArray.assign_coords, Dataset.assign_coords or Dataset.assign as appropriate."
2897         )
2898 
2899     @Variable.values.setter  # type: ignore[attr-defined]
2900     def values(self, values):
2901         raise ValueError(
2902             f"Cannot assign to the .values attribute of dimension coordinate a.k.a IndexVariable {self.name!r}. "
2903             f"Please use DataArray.assign_coords, Dataset.assign_coords or Dataset.assign as appropriate."
2904         )
2905 
2906     def chunk(self, chunks={}, name=None, lock=False, inline_array=False):
2907         # Dummy - do not chunk. This method is invoked e.g. by Dataset.chunk()
2908         return self.copy(deep=False)
2909 
2910     def _as_sparse(self, sparse_format=_default, fill_value=_default):
2911         # Dummy
2912         return self.copy(deep=False)
2913 
2914     def _to_dense(self):
2915         # Dummy
2916         return self.copy(deep=False)
2917 
2918     def _finalize_indexing_result(self, dims, data):
2919         if getattr(data, "ndim", 0) != 1:
2920             # returns Variable rather than IndexVariable if multi-dimensional
2921             return Variable(dims, data, self._attrs, self._encoding)
2922         else:
2923             return self._replace(dims=dims, data=data)
2924 
2925     def __setitem__(self, key, value):
2926         raise TypeError(f"{type(self).__name__} values cannot be modified")
2927 
2928     @classmethod
2929     def concat(
2930         cls,
2931         variables,
2932         dim="concat_dim",
2933         positions=None,
2934         shortcut=False,
2935         combine_attrs="override",
2936     ):
2937         """Specialized version of Variable.concat for IndexVariable objects.
2938 
2939         This exists because we want to avoid converting Index objects to NumPy
2940         arrays, if possible.
2941         """
2942         from xarray.core.merge import merge_attrs
2943 
2944         if not isinstance(dim, str):
2945             (dim,) = dim.dims
2946 
2947         variables = list(variables)
2948         first_var = variables[0]
2949 
2950         if any(not isinstance(v, cls) for v in variables):
2951             raise TypeError(
2952                 "IndexVariable.concat requires that all input "
2953                 "variables be IndexVariable objects"
2954             )
2955 
2956         indexes = [v._data.array for v in variables]
2957 
2958         if not indexes:
2959             data = []
2960         else:
2961             data = indexes[0].append(indexes[1:])
2962 
2963             if positions is not None:
2964                 indices = nputils.inverse_permutation(np.concatenate(positions))
2965                 data = data.take(indices)
2966 
2967         # keep as str if possible as pandas.Index uses object (converts to numpy array)
2968         data = maybe_coerce_to_str(data, variables)
2969 
2970         attrs = merge_attrs(
2971             [var.attrs for var in variables], combine_attrs=combine_attrs
2972         )
2973         if not shortcut:
2974             for var in variables:
2975                 if var.dims != first_var.dims:
2976                     raise ValueError("inconsistent dimensions")
2977 
2978         return cls(first_var.dims, data, attrs)
2979 
2980     def copy(self, deep: bool = True, data: ArrayLike | None = None):
2981         """Returns a copy of this object.
2982 
2983         `deep` is ignored since data is stored in the form of
2984         pandas.Index, which is already immutable. Dimensions, attributes
2985         and encodings are always copied.
2986 
2987         Use `data` to create a new object with the same structure as
2988         original but entirely new data.
2989 
2990         Parameters
2991         ----------
2992         deep : bool, default: True
2993             Deep is ignored when data is given. Whether the data array is
2994             loaded into memory and copied onto the new object. Default is True.
2995         data : array_like, optional
2996             Data to use in the new object. Must have same shape as original.
2997 
2998         Returns
2999         -------
3000         object : Variable
3001             New object with dimensions, attributes, encodings, and optionally
3002             data copied from original.
3003         """
3004         if data is None:
3005             ndata = self._data.copy(deep=deep)
3006         else:
3007             ndata = as_compatible_data(data)
3008             if self.shape != ndata.shape:
3009                 raise ValueError(
3010                     "Data shape {} must match shape of object {}".format(
3011                         ndata.shape, self.shape
3012                     )
3013                 )
3014 
3015         attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)
3016         encoding = copy.deepcopy(self._encoding) if deep else copy.copy(self._encoding)
3017 
3018         return self._replace(data=ndata, attrs=attrs, encoding=encoding)
3019 
3020     def equals(self, other, equiv=None):
3021         # if equiv is specified, super up
3022         if equiv is not None:
3023             return super().equals(other, equiv)
3024 
3025         # otherwise use the native index equals, rather than looking at _data
3026         other = getattr(other, "variable", other)
3027         try:
3028             return self.dims == other.dims and self._data_equals(other)
3029         except (TypeError, AttributeError):
3030             return False
3031 
3032     def _data_equals(self, other):
3033         return self._to_index().equals(other._to_index())
3034 
3035     def to_index_variable(self) -> IndexVariable:
3036         """Return this variable as an xarray.IndexVariable"""
3037         return self.copy(deep=False)
3038 
3039     to_coord = utils.alias(to_index_variable, "to_coord")
3040 
3041     def _to_index(self) -> pd.Index:
3042         # n.b. creating a new pandas.Index from an old pandas.Index is
3043         # basically free as pandas.Index objects are immutable.
3044         # n.b.2. this method returns the multi-index instance for
3045         # a pandas multi-index level variable.
3046         assert self.ndim == 1
3047         index = self._data.array
3048         if isinstance(index, pd.MultiIndex):
3049             # set default names for multi-index unnamed levels so that
3050             # we can safely rename dimension / coordinate later
3051             valid_level_names = [
3052                 name or f"{self.dims[0]}_level_{i}"
3053                 for i, name in enumerate(index.names)
3054             ]
3055             index = index.set_names(valid_level_names)
3056         else:
3057             index = index.set_names(self.name)
3058         return index
3059 
3060     def to_index(self) -> pd.Index:
3061         """Convert this variable to a pandas.Index"""
3062         index = self._to_index()
3063         level = getattr(self._data, "level", None)
3064         if level is not None:
3065             # return multi-index level converted to a single index
3066             return index.get_level_values(level)
3067         else:
3068             return index
3069 
3070     @property
3071     def level_names(self) -> list[str] | None:
3072         """Return MultiIndex level names or None if this IndexVariable has no
3073         MultiIndex.
3074         """
3075         index = self.to_index()
3076         if isinstance(index, pd.MultiIndex):
3077             return index.names
3078         else:
3079             return None
3080 
3081     def get_level_variable(self, level):
3082         """Return a new IndexVariable from a given MultiIndex level."""
3083         if self.level_names is None:
3084             raise ValueError(f"IndexVariable {self.name!r} has no MultiIndex")
3085         index = self.to_index()
3086         return type(self)(self.dims, index.get_level_values(level))
3087 
3088     @property
3089     def name(self) -> Hashable:
3090         return self.dims[0]
3091 
3092     @name.setter
3093     def name(self, value) -> NoReturn:
3094         raise AttributeError("cannot modify name of IndexVariable in-place")
3095 
3096     def _inplace_binary_op(self, other, f):
3097         raise TypeError(
3098             "Values of an IndexVariable are immutable and can not be modified inplace"
3099         )
3100 
3101 
3102 # for backwards compatibility
3103 Coordinate = utils.alias(IndexVariable, "Coordinate")
3104 
3105 
3106 def _unified_dims(variables):
3107     # validate dimensions
3108     all_dims = {}
3109     for var in variables:
3110         var_dims = var.dims
3111         if len(set(var_dims)) < len(var_dims):
3112             raise ValueError(
3113                 "broadcasting cannot handle duplicate "
3114                 f"dimensions: {list(var_dims)!r}"
3115             )
3116         for d, s in zip(var_dims, var.shape):
3117             if d not in all_dims:
3118                 all_dims[d] = s
3119             elif all_dims[d] != s:
3120                 raise ValueError(
3121                     "operands cannot be broadcast together "
3122                     f"with mismatched lengths for dimension {d!r}: {(all_dims[d], s)}"
3123                 )
3124     return all_dims
3125 
3126 
3127 def _broadcast_compat_variables(*variables):
3128     """Create broadcast compatible variables, with the same dimensions.
3129 
3130     Unlike the result of broadcast_variables(), some variables may have
3131     dimensions of size 1 instead of the size of the broadcast dimension.
3132     """
3133     dims = tuple(_unified_dims(variables))
3134     return tuple(var.set_dims(dims) if var.dims != dims else var for var in variables)
3135 
3136 
3137 def broadcast_variables(*variables: Variable) -> tuple[Variable, ...]:
3138     """Given any number of variables, return variables with matching dimensions
3139     and broadcast data.
3140 
3141     The data on the returned variables will be a view of the data on the
3142     corresponding original arrays, but dimensions will be reordered and
3143     inserted so that both broadcast arrays have the same dimensions. The new
3144     dimensions are sorted in order of appearance in the first variable's
3145     dimensions followed by the second variable's dimensions.
3146     """
3147     dims_map = _unified_dims(variables)
3148     dims_tuple = tuple(dims_map)
3149     return tuple(
3150         var.set_dims(dims_map) if var.dims != dims_tuple else var for var in variables
3151     )
3152 
3153 
3154 def _broadcast_compat_data(self, other):
3155     if all(hasattr(other, attr) for attr in ["dims", "data", "shape", "encoding"]):
3156         # `other` satisfies the necessary Variable API for broadcast_variables
3157         new_self, new_other = _broadcast_compat_variables(self, other)
3158         self_data = new_self.data
3159         other_data = new_other.data
3160         dims = new_self.dims
3161     else:
3162         # rely on numpy broadcasting rules
3163         self_data = self.data
3164         other_data = other
3165         dims = self.dims
3166     return self_data, other_data, dims
3167 
3168 
3169 def concat(
3170     variables,
3171     dim="concat_dim",
3172     positions=None,
3173     shortcut=False,
3174     combine_attrs="override",
3175 ):
3176     """Concatenate variables along a new or existing dimension.
3177 
3178     Parameters
3179     ----------
3180     variables : iterable of Variable
3181         Arrays to stack together. Each variable is expected to have
3182         matching dimensions and shape except for along the stacked
3183         dimension.
3184     dim : str or DataArray, optional
3185         Name of the dimension to stack along. This can either be a new
3186         dimension name, in which case it is added along axis=0, or an
3187         existing dimension name, in which case the location of the
3188         dimension is unchanged. Where to insert the new dimension is
3189         determined by the first variable.
3190     positions : None or list of array-like, optional
3191         List of integer arrays which specifies the integer positions to which
3192         to assign each dataset along the concatenated dimension. If not
3193         supplied, objects are concatenated in the provided order.
3194     shortcut : bool, optional
3195         This option is used internally to speed-up groupby operations.
3196         If `shortcut` is True, some checks of internal consistency between
3197         arrays to concatenate are skipped.
3198     combine_attrs : {"drop", "identical", "no_conflicts", "drop_conflicts", \
3199                      "override"}, default: "override"
3200         String indicating how to combine attrs of the objects being merged:
3201 
3202         - "drop": empty attrs on returned Dataset.
3203         - "identical": all attrs must be the same on every object.
3204         - "no_conflicts": attrs from all objects are combined, any that have
3205           the same name must also have the same value.
3206         - "drop_conflicts": attrs from all objects are combined, any that have
3207           the same name but different values are dropped.
3208         - "override": skip comparing and copy attrs from the first dataset to
3209           the result.
3210 
3211     Returns
3212     -------
3213     stacked : Variable
3214         Concatenated Variable formed by stacking all the supplied variables
3215         along the given dimension.
3216     """
3217     variables = list(variables)
3218     if all(isinstance(v, IndexVariable) for v in variables):
3219         return IndexVariable.concat(variables, dim, positions, shortcut, combine_attrs)
3220     else:
3221         return Variable.concat(variables, dim, positions, shortcut, combine_attrs)
3222 
3223 
3224 def calculate_dimensions(variables: Mapping[Any, Variable]) -> dict[Hashable, int]:
3225     """Calculate the dimensions corresponding to a set of variables.
3226 
3227     Returns dictionary mapping from dimension names to sizes. Raises ValueError
3228     if any of the dimension sizes conflict.
3229     """
3230     dims: dict[Hashable, int] = {}
3231     last_used = {}
3232     scalar_vars = {k for k, v in variables.items() if not v.dims}
3233     for k, var in variables.items():
3234         for dim, size in zip(var.dims, var.shape):
3235             if dim in scalar_vars:
3236                 raise ValueError(
3237                     f"dimension {dim!r} already exists as a scalar variable"
3238                 )
3239             if dim not in dims:
3240                 dims[dim] = size
3241                 last_used[dim] = k
3242             elif dims[dim] != size:
3243                 raise ValueError(
3244                     f"conflicting sizes for dimension {dim!r}: "
3245                     f"length {size} on {k!r} and length {dims[dim]} on {last_used!r}"
3246                 )
3247     return dims

</code><intended_behavior>Adapt these methods to utilize a generalized computational backend system that decides at runtime which backend to use for operations, based on user configuration or data attributes. Ensure that the behavior for dask remains unaffected for current users.</intended_behavior>, <file>xarray/core/computation.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 Functions for applying functions that act on arrays to xarray's labeled data.
3 """
4 from __future__ import annotations
5 
6 import functools
7 import itertools
8 import operator
9 import warnings
10 from collections import Counter
11 from collections.abc import Hashable, Iterable, Mapping, Sequence, Set
12 from typing import TYPE_CHECKING, Any, Callable, Literal, TypeVar, Union, overload
13 
14 import numpy as np
15 
16 from xarray.core import dtypes, duck_array_ops, utils
17 from xarray.core.alignment import align, deep_align
18 from xarray.core.common import zeros_like
19 from xarray.core.duck_array_ops import datetime_to_numeric
20 from xarray.core.indexes import Index, filter_indexes_from_coords
21 from xarray.core.merge import merge_attrs, merge_coordinates_without_align
22 from xarray.core.options import OPTIONS, _get_keep_attrs
23 from xarray.core.pycompat import is_duck_dask_array
24 from xarray.core.types import Dims, T_DataArray
25 from xarray.core.utils import is_dict_like, is_scalar
26 from xarray.core.variable import Variable
27 
28 if TYPE_CHECKING:
29     from xarray.core.coordinates import Coordinates
30     from xarray.core.dataarray import DataArray
31     from xarray.core.dataset import Dataset
32     from xarray.core.types import CombineAttrsOptions, JoinOptions
33 
34 _NO_FILL_VALUE = utils.ReprObject("<no-fill-value>")
35 _DEFAULT_NAME = utils.ReprObject("<default-name>")
36 _JOINS_WITHOUT_FILL_VALUES = frozenset({"inner", "exact"})
37 
38 
39 def _first_of_type(args, kind):
40     """Return either first object of type 'kind' or raise if not found."""
41     for arg in args:
42         if isinstance(arg, kind):
43             return arg
44     raise ValueError("This should be unreachable.")
45 
46 
47 def _all_of_type(args, kind):
48     """Return all objects of type 'kind'"""
49     return [arg for arg in args if isinstance(arg, kind)]
50 
51 
52 class _UFuncSignature:
53     """Core dimensions signature for a given function.
54 
55     Based on the signature provided by generalized ufuncs in NumPy.
56 
57     Attributes
58     ----------
59     input_core_dims : tuple[tuple]
60         Core dimension names on each input variable.
61     output_core_dims : tuple[tuple]
62         Core dimension names on each output variable.
63     """
64 
65     __slots__ = (
66         "input_core_dims",
67         "output_core_dims",
68         "_all_input_core_dims",
69         "_all_output_core_dims",
70         "_all_core_dims",
71     )
72 
73     def __init__(self, input_core_dims, output_core_dims=((),)):
74         self.input_core_dims = tuple(tuple(a) for a in input_core_dims)
75         self.output_core_dims = tuple(tuple(a) for a in output_core_dims)
76         self._all_input_core_dims = None
77         self._all_output_core_dims = None
78         self._all_core_dims = None
79 
80     @property
81     def all_input_core_dims(self):
82         if self._all_input_core_dims is None:
83             self._all_input_core_dims = frozenset(
84                 dim for dims in self.input_core_dims for dim in dims
85             )
86         return self._all_input_core_dims
87 
88     @property
89     def all_output_core_dims(self):
90         if self._all_output_core_dims is None:
91             self._all_output_core_dims = frozenset(
92                 dim for dims in self.output_core_dims for dim in dims
93             )
94         return self._all_output_core_dims
95 
96     @property
97     def all_core_dims(self):
98         if self._all_core_dims is None:
99             self._all_core_dims = self.all_input_core_dims | self.all_output_core_dims
100         return self._all_core_dims
101 
102     @property
103     def dims_map(self):
104         return {
105             core_dim: f"dim{n}" for n, core_dim in enumerate(sorted(self.all_core_dims))
106         }
107 
108     @property
109     def num_inputs(self):
110         return len(self.input_core_dims)
111 
112     @property
113     def num_outputs(self):
114         return len(self.output_core_dims)
115 
116     def __eq__(self, other):
117         try:
118             return (
119                 self.input_core_dims == other.input_core_dims
120                 and self.output_core_dims == other.output_core_dims
121             )
122         except AttributeError:
123             return False
124 
125     def __ne__(self, other):
126         return not self == other
127 
128     def __repr__(self):
129         return "{}({!r}, {!r})".format(
130             type(self).__name__,
131             list(self.input_core_dims),
132             list(self.output_core_dims),
133         )
134 
135     def __str__(self):
136         lhs = ",".join("({})".format(",".join(dims)) for dims in self.input_core_dims)
137         rhs = ",".join("({})".format(",".join(dims)) for dims in self.output_core_dims)
138         return f"{lhs}->{rhs}"
139 
140     def to_gufunc_string(self, exclude_dims=frozenset()):
141         """Create an equivalent signature string for a NumPy gufunc.
142 
143         Unlike __str__, handles dimensions that don't map to Python
144         identifiers.
145 
146         Also creates unique names for input_core_dims contained in exclude_dims.
147         """
148         input_core_dims = [
149             [self.dims_map[dim] for dim in core_dims]
150             for core_dims in self.input_core_dims
151         ]
152         output_core_dims = [
153             [self.dims_map[dim] for dim in core_dims]
154             for core_dims in self.output_core_dims
155         ]
156 
157         # enumerate input_core_dims contained in exclude_dims to make them unique
158         if exclude_dims:
159             exclude_dims = [self.dims_map[dim] for dim in exclude_dims]
160 
161             counter = Counter()
162 
163             def _enumerate(dim):
164                 if dim in exclude_dims:
165                     n = counter[dim]
166                     counter.update([dim])
167                     dim = f"{dim}_{n}"
168                 return dim
169 
170             input_core_dims = [
171                 [_enumerate(dim) for dim in arg] for arg in input_core_dims
172             ]
173 
174         alt_signature = type(self)(input_core_dims, output_core_dims)
175         return str(alt_signature)
176 
177 
178 def result_name(objects: Iterable[Any]) -> Any:
179     # use the same naming heuristics as pandas:
180     # https://github.com/blaze/blaze/issues/458#issuecomment-51936356
181     names = {getattr(obj, "name", _DEFAULT_NAME) for obj in objects}
182     names.discard(_DEFAULT_NAME)
183     if len(names) == 1:
184         (name,) = names
185     else:
186         name = None
187     return name
188 
189 
190 def _get_coords_list(args: Iterable[Any]) -> list[Coordinates]:
191     coords_list = []
192     for arg in args:
193         try:
194             coords = arg.coords
195         except AttributeError:
196             pass  # skip this argument
197         else:
198             coords_list.append(coords)
199     return coords_list
200 
201 
202 def build_output_coords_and_indexes(
203     args: Iterable[Any],
204     signature: _UFuncSignature,
205     exclude_dims: Set = frozenset(),
206     combine_attrs: CombineAttrsOptions = "override",
207 ) -> tuple[list[dict[Any, Variable]], list[dict[Any, Index]]]:
208     """Build output coordinates and indexes for an operation.
209 
210     Parameters
211     ----------
212     args : Iterable
213         List of raw operation arguments. Any valid types for xarray operations
214         are OK, e.g., scalars, Variable, DataArray, Dataset.
215     signature : _UfuncSignature
216         Core dimensions signature for the operation.
217     exclude_dims : set, optional
218         Dimensions excluded from the operation. Coordinates along these
219         dimensions are dropped.
220     combine_attrs : {"drop", "identical", "no_conflicts", "drop_conflicts", \
221                      "override"} or callable, default: "drop"
222         A callable or a string indicating how to combine attrs of the objects being
223         merged:
224 
225         - "drop": empty attrs on returned Dataset.
226         - "identical": all attrs must be the same on every object.
227         - "no_conflicts": attrs from all objects are combined, any that have
228           the same name must also have the same value.
229         - "drop_conflicts": attrs from all objects are combined, any that have
230           the same name but different values are dropped.
231         - "override": skip comparing and copy attrs from the first dataset to
232           the result.
233 
234         If a callable, it must expect a sequence of ``attrs`` dicts and a context object
235         as its only parameters.
236 
237     Returns
238     -------
239     Dictionaries of Variable and Index objects with merged coordinates.
240     """
241     coords_list = _get_coords_list(args)
242 
243     if len(coords_list) == 1 and not exclude_dims:
244         # we can skip the expensive merge
245         (unpacked_coords,) = coords_list
246         merged_vars = dict(unpacked_coords.variables)
247         merged_indexes = dict(unpacked_coords.xindexes)
248     else:
249         merged_vars, merged_indexes = merge_coordinates_without_align(
250             coords_list, exclude_dims=exclude_dims, combine_attrs=combine_attrs
251         )
252 
253     output_coords = []
254     output_indexes = []
255     for output_dims in signature.output_core_dims:
256         dropped_dims = signature.all_input_core_dims - set(output_dims)
257         if dropped_dims:
258             filtered_coords = {
259                 k: v for k, v in merged_vars.items() if dropped_dims.isdisjoint(v.dims)
260             }
261             filtered_indexes = filter_indexes_from_coords(
262                 merged_indexes, set(filtered_coords)
263             )
264         else:
265             filtered_coords = merged_vars
266             filtered_indexes = merged_indexes
267         output_coords.append(filtered_coords)
268         output_indexes.append(filtered_indexes)
269 
270     return output_coords, output_indexes
271 
272 
273 def apply_dataarray_vfunc(
274     func,
275     *args,
276     signature: _UFuncSignature,
277     join: JoinOptions = "inner",
278     exclude_dims=frozenset(),
279     keep_attrs="override",
280 ) -> tuple[DataArray, ...] | DataArray:
281     """Apply a variable level function over DataArray, Variable and/or ndarray
282     objects.
283     """
284     from xarray.core.dataarray import DataArray
285 
286     if len(args) > 1:
287         args = deep_align(
288             args, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False
289         )
290 
291     objs = _all_of_type(args, DataArray)
292 
293     if keep_attrs == "drop":
294         name = result_name(args)
295     else:
296         first_obj = _first_of_type(args, DataArray)
297         name = first_obj.name
298     result_coords, result_indexes = build_output_coords_and_indexes(
299         args, signature, exclude_dims, combine_attrs=keep_attrs
300     )
301 
302     data_vars = [getattr(a, "variable", a) for a in args]
303     result_var = func(*data_vars)
304 
305     out: tuple[DataArray, ...] | DataArray
306     if signature.num_outputs > 1:
307         out = tuple(
308             DataArray(
309                 variable, coords=coords, indexes=indexes, name=name, fastpath=True
310             )
311             for variable, coords, indexes in zip(
312                 result_var, result_coords, result_indexes
313             )
314         )
315     else:
316         (coords,) = result_coords
317         (indexes,) = result_indexes
318         out = DataArray(
319             result_var, coords=coords, indexes=indexes, name=name, fastpath=True
320         )
321 
322     attrs = merge_attrs([x.attrs for x in objs], combine_attrs=keep_attrs)
323     if isinstance(out, tuple):
324         for da in out:
325             da.attrs = attrs
326     else:
327         out.attrs = attrs
328 
329     return out
330 
331 
332 def ordered_set_union(all_keys: list[Iterable]) -> Iterable:
333     return {key: None for keys in all_keys for key in keys}.keys()
334 
335 
336 def ordered_set_intersection(all_keys: list[Iterable]) -> Iterable:
337     intersection = set(all_keys[0])
338     for keys in all_keys[1:]:
339         intersection.intersection_update(keys)
340     return [key for key in all_keys[0] if key in intersection]
341 
342 
343 def assert_and_return_exact_match(all_keys):
344     first_keys = all_keys[0]
345     for keys in all_keys[1:]:
346         if keys != first_keys:
347             raise ValueError(
348                 "exact match required for all data variable names, "
349                 f"but {keys!r} != {first_keys!r}"
350             )
351     return first_keys
352 
353 
354 _JOINERS: dict[str, Callable] = {
355     "inner": ordered_set_intersection,
356     "outer": ordered_set_union,
357     "left": operator.itemgetter(0),
358     "right": operator.itemgetter(-1),
359     "exact": assert_and_return_exact_match,
360 }
361 
362 
363 def join_dict_keys(objects: Iterable[Mapping | Any], how: str = "inner") -> Iterable:
364     joiner = _JOINERS[how]
365     all_keys = [obj.keys() for obj in objects if hasattr(obj, "keys")]
366     return joiner(all_keys)
367 
368 
369 def collect_dict_values(
370     objects: Iterable[Mapping | Any], keys: Iterable, fill_value: object = None
371 ) -> list[list]:
372     return [
373         [obj.get(key, fill_value) if is_dict_like(obj) else obj for obj in objects]
374         for key in keys
375     ]
376 
377 
378 def _as_variables_or_variable(arg):
379     try:
380         return arg.variables
381     except AttributeError:
382         try:
383             return arg.variable
384         except AttributeError:
385             return arg
386 
387 
388 def _unpack_dict_tuples(
389     result_vars: Mapping[Any, tuple[Variable, ...]], num_outputs: int
390 ) -> tuple[dict[Hashable, Variable], ...]:
391     out: tuple[dict[Hashable, Variable], ...] = tuple({} for _ in range(num_outputs))
392     for name, values in result_vars.items():
393         for value, results_dict in zip(values, out):
394             results_dict[name] = value
395     return out
396 
397 
398 def apply_dict_of_variables_vfunc(
399     func, *args, signature: _UFuncSignature, join="inner", fill_value=None
400 ):
401     """Apply a variable level function over dicts of DataArray, DataArray,
402     Variable and ndarray objects.
403     """
404     args = tuple(_as_variables_or_variable(arg) for arg in args)
405     names = join_dict_keys(args, how=join)
406     grouped_by_name = collect_dict_values(args, names, fill_value)
407 
408     result_vars = {}
409     for name, variable_args in zip(names, grouped_by_name):
410         result_vars[name] = func(*variable_args)
411 
412     if signature.num_outputs > 1:
413         return _unpack_dict_tuples(result_vars, signature.num_outputs)
414     else:
415         return result_vars
416 
417 
418 def _fast_dataset(
419     variables: dict[Hashable, Variable],
420     coord_variables: Mapping[Hashable, Variable],
421     indexes: dict[Hashable, Index],
422 ) -> Dataset:
423     """Create a dataset as quickly as possible.
424 
425     Beware: the `variables` dict is modified INPLACE.
426     """
427     from xarray.core.dataset import Dataset
428 
429     variables.update(coord_variables)
430     coord_names = set(coord_variables)
431     return Dataset._construct_direct(variables, coord_names, indexes=indexes)
432 
433 
434 def apply_dataset_vfunc(
435     func,
436     *args,
437     signature: _UFuncSignature,
438     join="inner",
439     dataset_join="exact",
440     fill_value=_NO_FILL_VALUE,
441     exclude_dims=frozenset(),
442     keep_attrs="override",
443 ) -> Dataset | tuple[Dataset, ...]:
444     """Apply a variable level function over Dataset, dict of DataArray,
445     DataArray, Variable and/or ndarray objects.
446     """
447     from xarray.core.dataset import Dataset
448 
449     if dataset_join not in _JOINS_WITHOUT_FILL_VALUES and fill_value is _NO_FILL_VALUE:
450         raise TypeError(
451             "to apply an operation to datasets with different "
452             "data variables with apply_ufunc, you must supply the "
453             "dataset_fill_value argument."
454         )
455 
456     objs = _all_of_type(args, Dataset)
457 
458     if len(args) > 1:
459         args = deep_align(
460             args, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False
461         )
462 
463     list_of_coords, list_of_indexes = build_output_coords_and_indexes(
464         args, signature, exclude_dims, combine_attrs=keep_attrs
465     )
466     args = tuple(getattr(arg, "data_vars", arg) for arg in args)
467 
468     result_vars = apply_dict_of_variables_vfunc(
469         func, *args, signature=signature, join=dataset_join, fill_value=fill_value
470     )
471 
472     out: Dataset | tuple[Dataset, ...]
473     if signature.num_outputs > 1:
474         out = tuple(
475             _fast_dataset(*args)
476             for args in zip(result_vars, list_of_coords, list_of_indexes)
477         )
478     else:
479         (coord_vars,) = list_of_coords
480         (indexes,) = list_of_indexes
481         out = _fast_dataset(result_vars, coord_vars, indexes=indexes)
482 
483     attrs = merge_attrs([x.attrs for x in objs], combine_attrs=keep_attrs)
484     if isinstance(out, tuple):
485         for ds in out:
486             ds.attrs = attrs
487     else:
488         out.attrs = attrs
489 
490     return out
491 
492 
493 def _iter_over_selections(obj, dim, values):
494     """Iterate over selections of an xarray object in the provided order."""
495     from xarray.core.groupby import _dummy_copy
496 
497     dummy = None
498     for value in values:
499         try:
500             obj_sel = obj.sel(**{dim: value})
501         except (KeyError, IndexError):
502             if dummy is None:
503                 dummy = _dummy_copy(obj)
504             obj_sel = dummy
505         yield obj_sel
506 
507 
508 def apply_groupby_func(func, *args):
509     """Apply a dataset or datarray level function over GroupBy, Dataset,
510     DataArray, Variable and/or ndarray objects.
511     """
512     from xarray.core.groupby import GroupBy, peek_at
513     from xarray.core.variable import Variable
514 
515     groupbys = [arg for arg in args if isinstance(arg, GroupBy)]
516     assert groupbys, "must have at least one groupby to iterate over"
517     first_groupby = groupbys[0]
518     (grouper,) = first_groupby.groupers
519     if any(not grouper.group.equals(gb.groupers[0].group) for gb in groupbys[1:]):
520         raise ValueError(
521             "apply_ufunc can only perform operations over "
522             "multiple GroupBy objects at once if they are all "
523             "grouped the same way"
524         )
525 
526     grouped_dim = grouper.name
527     unique_values = grouper.unique_coord.values
528 
529     iterators = []
530     for arg in args:
531         if isinstance(arg, GroupBy):
532             iterator = (value for _, value in arg)
533         elif hasattr(arg, "dims") and grouped_dim in arg.dims:
534             if isinstance(arg, Variable):
535                 raise ValueError(
536                     "groupby operations cannot be performed with "
537                     "xarray.Variable objects that share a dimension with "
538                     "the grouped dimension"
539                 )
540             iterator = _iter_over_selections(arg, grouped_dim, unique_values)
541         else:
542             iterator = itertools.repeat(arg)
543         iterators.append(iterator)
544 
545     applied = (func(*zipped_args) for zipped_args in zip(*iterators))
546     applied_example, applied = peek_at(applied)
547     combine = first_groupby._combine
548     if isinstance(applied_example, tuple):
549         combined = tuple(combine(output) for output in zip(*applied))
550     else:
551         combined = combine(applied)
552     return combined
553 
554 
555 def unified_dim_sizes(
556     variables: Iterable[Variable], exclude_dims: Set = frozenset()
557 ) -> dict[Hashable, int]:
558     dim_sizes: dict[Hashable, int] = {}
559 
560     for var in variables:
561         if len(set(var.dims)) < len(var.dims):
562             raise ValueError(
563                 "broadcasting cannot handle duplicate "
564                 f"dimensions on a variable: {list(var.dims)}"
565             )
566         for dim, size in zip(var.dims, var.shape):
567             if dim not in exclude_dims:
568                 if dim not in dim_sizes:
569                     dim_sizes[dim] = size
570                 elif dim_sizes[dim] != size:
571                     raise ValueError(
572                         "operands cannot be broadcast together "
573                         "with mismatched lengths for dimension "
574                         f"{dim}: {dim_sizes[dim]} vs {size}"
575                     )
576     return dim_sizes
577 
578 
579 SLICE_NONE = slice(None)
580 
581 
582 def broadcast_compat_data(
583     variable: Variable,
584     broadcast_dims: tuple[Hashable, ...],
585     core_dims: tuple[Hashable, ...],
586 ) -> Any:
587     data = variable.data
588 
589     old_dims = variable.dims
590     new_dims = broadcast_dims + core_dims
591 
592     if new_dims == old_dims:
593         # optimize for the typical case
594         return data
595 
596     set_old_dims = set(old_dims)
597     missing_core_dims = [d for d in core_dims if d not in set_old_dims]
598     if missing_core_dims:
599         raise ValueError(
600             "operand to apply_ufunc has required core dimensions {}, but "
601             "some of these dimensions are absent on an input variable: {}".format(
602                 list(core_dims), missing_core_dims
603             )
604         )
605 
606     set_new_dims = set(new_dims)
607     unexpected_dims = [d for d in old_dims if d not in set_new_dims]
608     if unexpected_dims:
609         raise ValueError(
610             "operand to apply_ufunc encountered unexpected "
611             f"dimensions {unexpected_dims!r} on an input variable: these are core "
612             "dimensions on other input or output variables"
613         )
614 
615     # for consistency with numpy, keep broadcast dimensions to the left
616     old_broadcast_dims = tuple(d for d in broadcast_dims if d in set_old_dims)
617     reordered_dims = old_broadcast_dims + core_dims
618     if reordered_dims != old_dims:
619         order = tuple(old_dims.index(d) for d in reordered_dims)
620         data = duck_array_ops.transpose(data, order)
621 
622     if new_dims != reordered_dims:
623         key_parts: list[slice | None] = []
624         for dim in new_dims:
625             if dim in set_old_dims:
626                 key_parts.append(SLICE_NONE)
627             elif key_parts:
628                 # no need to insert new axes at the beginning that are already
629                 # handled by broadcasting
630                 key_parts.append(np.newaxis)
631         data = data[tuple(key_parts)]
632 
633     return data
634 
635 
636 def _vectorize(func, signature, output_dtypes, exclude_dims):
637     if signature.all_core_dims:
638         func = np.vectorize(
639             func,
640             otypes=output_dtypes,
641             signature=signature.to_gufunc_string(exclude_dims),
642         )
643     else:
644         func = np.vectorize(func, otypes=output_dtypes)
645 
646     return func
647 
648 
649 def apply_variable_ufunc(
650     func,
651     *args,
652     signature: _UFuncSignature,
653     exclude_dims=frozenset(),
654     dask="forbidden",
655     output_dtypes=None,
656     vectorize=False,
657     keep_attrs="override",
658     dask_gufunc_kwargs=None,
659 ) -> Variable | tuple[Variable, ...]:
660     """Apply a ndarray level function over Variable and/or ndarray objects."""
661     from xarray.core.variable import Variable, as_compatible_data
662 
663     dim_sizes = unified_dim_sizes(
664         (a for a in args if hasattr(a, "dims")), exclude_dims=exclude_dims
665     )
666     broadcast_dims = tuple(
667         dim for dim in dim_sizes if dim not in signature.all_core_dims
668     )
669     output_dims = [broadcast_dims + out for out in signature.output_core_dims]
670 
671     input_data = [
672         broadcast_compat_data(arg, broadcast_dims, core_dims)
673         if isinstance(arg, Variable)
674         else arg
675         for arg, core_dims in zip(args, signature.input_core_dims)
676     ]
677 
678     if any(is_duck_dask_array(array) for array in input_data):
679         if dask == "forbidden":
680             raise ValueError(
681                 "apply_ufunc encountered a dask array on an "
682                 "argument, but handling for dask arrays has not "
683                 "been enabled. Either set the ``dask`` argument "
684                 "or load your data into memory first with "
685                 "``.load()`` or ``.compute()``"
686             )
687         elif dask == "parallelized":
688             numpy_func = func
689 
690             if dask_gufunc_kwargs is None:
691                 dask_gufunc_kwargs = {}
692             else:
693                 dask_gufunc_kwargs = dask_gufunc_kwargs.copy()
694 
695             allow_rechunk = dask_gufunc_kwargs.get("allow_rechunk", None)
696             if allow_rechunk is None:
697                 for n, (data, core_dims) in enumerate(
698                     zip(input_data, signature.input_core_dims)
699                 ):
700                     if is_duck_dask_array(data):
701                         # core dimensions cannot span multiple chunks
702                         for axis, dim in enumerate(core_dims, start=-len(core_dims)):
703                             if len(data.chunks[axis]) != 1:
704                                 raise ValueError(
705                                     f"dimension {dim} on {n}th function argument to "
706                                     "apply_ufunc with dask='parallelized' consists of "
707                                     "multiple chunks, but is also a core dimension. To "
708                                     "fix, either rechunk into a single dask array chunk along "
709                                     f"this dimension, i.e., ``.chunk(dict({dim}=-1))``, or "
710                                     "pass ``allow_rechunk=True`` in ``dask_gufunc_kwargs`` "
711                                     "but beware that this may significantly increase memory usage."
712                                 )
713                 dask_gufunc_kwargs["allow_rechunk"] = True
714 
715             output_sizes = dask_gufunc_kwargs.pop("output_sizes", {})
716             if output_sizes:
717                 output_sizes_renamed = {}
718                 for key, value in output_sizes.items():
719                     if key not in signature.all_output_core_dims:
720                         raise ValueError(
721                             f"dimension '{key}' in 'output_sizes' must correspond to output_core_dims"
722                         )
723                     output_sizes_renamed[signature.dims_map[key]] = value
724                 dask_gufunc_kwargs["output_sizes"] = output_sizes_renamed
725 
726             for key in signature.all_output_core_dims:
727                 if (
728                     key not in signature.all_input_core_dims or key in exclude_dims
729                 ) and key not in output_sizes:
730                     raise ValueError(
731                         f"dimension '{key}' in 'output_core_dims' needs corresponding (dim, size) in 'output_sizes'"
732                     )
733 
734             def func(*arrays):
735                 import dask.array as da
736 
737                 res = da.apply_gufunc(
738                     numpy_func,
739                     signature.to_gufunc_string(exclude_dims),
740                     *arrays,
741                     vectorize=vectorize,
742                     output_dtypes=output_dtypes,
743                     **dask_gufunc_kwargs,
744                 )
745 
746                 return res
747 
748         elif dask == "allowed":
749             pass
750         else:
751             raise ValueError(
752                 "unknown setting for dask array handling in "
753                 "apply_ufunc: {}".format(dask)
754             )
755     else:
756         if vectorize:
757             func = _vectorize(
758                 func, signature, output_dtypes=output_dtypes, exclude_dims=exclude_dims
759             )
760 
761     result_data = func(*input_data)
762 
763     if signature.num_outputs == 1:
764         result_data = (result_data,)
765     elif (
766         not isinstance(result_data, tuple) or len(result_data) != signature.num_outputs
767     ):
768         raise ValueError(
769             "applied function does not have the number of "
770             "outputs specified in the ufunc signature. "
771             "Result is not a tuple of {} elements: {!r}".format(
772                 signature.num_outputs, result_data
773             )
774         )
775 
776     objs = _all_of_type(args, Variable)
777     attrs = merge_attrs(
778         [obj.attrs for obj in objs],
779         combine_attrs=keep_attrs,
780     )
781 
782     output: list[Variable] = []
783     for dims, data in zip(output_dims, result_data):
784         data = as_compatible_data(data)
785         if data.ndim != len(dims):
786             raise ValueError(
787                 "applied function returned data with unexpected "
788                 f"number of dimensions. Received {data.ndim} dimension(s) but "
789                 f"expected {len(dims)} dimensions with names: {dims!r}"
790             )
791 
792         var = Variable(dims, data, fastpath=True)
793         for dim, new_size in var.sizes.items():
794             if dim in dim_sizes and new_size != dim_sizes[dim]:
795                 raise ValueError(
796                     "size of dimension {!r} on inputs was unexpectedly "
797                     "changed by applied function from {} to {}. Only "
798                     "dimensions specified in ``exclude_dims`` with "
799                     "xarray.apply_ufunc are allowed to change size.".format(
800                         dim, dim_sizes[dim], new_size
801                     )
802                 )
803 
804         var.attrs = attrs
805         output.append(var)
806 
807     if signature.num_outputs == 1:
808         return output[0]
809     else:
810         return tuple(output)
811 
812 
813 def apply_array_ufunc(func, *args, dask="forbidden"):
814     """Apply a ndarray level function over ndarray objects."""
815     if any(is_duck_dask_array(arg) for arg in args):
816         if dask == "forbidden":
817             raise ValueError(
818                 "apply_ufunc encountered a dask array on an "
819                 "argument, but handling for dask arrays has not "
820                 "been enabled. Either set the ``dask`` argument "
821                 "or load your data into memory first with "
822                 "``.load()`` or ``.compute()``"
823             )
824         elif dask == "parallelized":
825             raise ValueError(
826                 "cannot use dask='parallelized' for apply_ufunc "
827                 "unless at least one input is an xarray object"
828             )
829         elif dask == "allowed":
830             pass
831         else:
832             raise ValueError(f"unknown setting for dask array handling: {dask}")
833     return func(*args)
834 
835 
836 def apply_ufunc(
837     func: Callable,
838     *args: Any,
839     input_core_dims: Sequence[Sequence] | None = None,
840     output_core_dims: Sequence[Sequence] | None = ((),),
841     exclude_dims: Set = frozenset(),
842     vectorize: bool = False,
843     join: JoinOptions = "exact",
844     dataset_join: str = "exact",
845     dataset_fill_value: object = _NO_FILL_VALUE,
846     keep_attrs: bool | str | None = None,
847     kwargs: Mapping | None = None,
848     dask: str = "forbidden",
849     output_dtypes: Sequence | None = None,
850     output_sizes: Mapping[Any, int] | None = None,
851     meta: Any = None,
852     dask_gufunc_kwargs: dict[str, Any] | None = None,
853 ) -> Any:
854     """Apply a vectorized function for unlabeled arrays on xarray objects.
855 
856     The function will be mapped over the data variable(s) of the input
857     arguments using xarray's standard rules for labeled computation, including
858     alignment, broadcasting, looping over GroupBy/Dataset variables, and
859     merging of coordinates.
860 
861     Parameters
862     ----------
863     func : callable
864         Function to call like ``func(*args, **kwargs)`` on unlabeled arrays
865         (``.data``) that returns an array or tuple of arrays. If multiple
866         arguments with non-matching dimensions are supplied, this function is
867         expected to vectorize (broadcast) over axes of positional arguments in
868         the style of NumPy universal functions [1]_ (if this is not the case,
869         set ``vectorize=True``). If this function returns multiple outputs, you
870         must set ``output_core_dims`` as well.
871     *args : Dataset, DataArray, DataArrayGroupBy, DatasetGroupBy, Variable, \
872         numpy.ndarray, dask.array.Array or scalar
873         Mix of labeled and/or unlabeled arrays to which to apply the function.
874     input_core_dims : sequence of sequence, optional
875         List of the same length as ``args`` giving the list of core dimensions
876         on each input argument that should not be broadcast. By default, we
877         assume there are no core dimensions on any input arguments.
878 
879         For example, ``input_core_dims=[[], ['time']]`` indicates that all
880         dimensions on the first argument and all dimensions other than 'time'
881         on the second argument should be broadcast.
882 
883         Core dimensions are automatically moved to the last axes of input
884         variables before applying ``func``, which facilitates using NumPy style
885         generalized ufuncs [2]_.
886     output_core_dims : list of tuple, optional
887         List of the same length as the number of output arguments from
888         ``func``, giving the list of core dimensions on each output that were
889         not broadcast on the inputs. By default, we assume that ``func``
890         outputs exactly one array, with axes corresponding to each broadcast
891         dimension.
892 
893         Core dimensions are assumed to appear as the last dimensions of each
894         output in the provided order.
895     exclude_dims : set, optional
896         Core dimensions on the inputs to exclude from alignment and
897         broadcasting entirely. Any input coordinates along these dimensions
898         will be dropped. Each excluded dimension must also appear in
899         ``input_core_dims`` for at least one argument. Only dimensions listed
900         here are allowed to change size between input and output objects.
901     vectorize : bool, optional
902         If True, then assume ``func`` only takes arrays defined over core
903         dimensions as input and vectorize it automatically with
904         :py:func:`numpy.vectorize`. This option exists for convenience, but is
905         almost always slower than supplying a pre-vectorized function.
906     join : {"outer", "inner", "left", "right", "exact"}, default: "exact"
907         Method for joining the indexes of the passed objects along each
908         dimension, and the variables of Dataset objects with mismatched
909         data variables:
910 
911         - 'outer': use the union of object indexes
912         - 'inner': use the intersection of object indexes
913         - 'left': use indexes from the first object with each dimension
914         - 'right': use indexes from the last object with each dimension
915         - 'exact': raise `ValueError` instead of aligning when indexes to be
916           aligned are not equal
917     dataset_join : {"outer", "inner", "left", "right", "exact"}, default: "exact"
918         Method for joining variables of Dataset objects with mismatched
919         data variables.
920 
921         - 'outer': take variables from both Dataset objects
922         - 'inner': take only overlapped variables
923         - 'left': take only variables from the first object
924         - 'right': take only variables from the last object
925         - 'exact': data variables on all Dataset objects must match exactly
926     dataset_fill_value : optional
927         Value used in place of missing variables on Dataset inputs when the
928         datasets do not share the exact same ``data_vars``. Required if
929         ``dataset_join not in {'inner', 'exact'}``, otherwise ignored.
930     keep_attrs : {"drop", "identical", "no_conflicts", "drop_conflicts", "override"} or bool, optional
931         - 'drop' or False: empty attrs on returned xarray object.
932         - 'identical': all attrs must be the same on every object.
933         - 'no_conflicts': attrs from all objects are combined, any that have the same name must also have the same value.
934         - 'drop_conflicts': attrs from all objects are combined, any that have the same name but different values are dropped.
935         - 'override' or True: skip comparing and copy attrs from the first object to the result.
936     kwargs : dict, optional
937         Optional keyword arguments passed directly on to call ``func``.
938     dask : {"forbidden", "allowed", "parallelized"}, default: "forbidden"
939         How to handle applying to objects containing lazy data in the form of
940         dask arrays:
941 
942         - 'forbidden' (default): raise an error if a dask array is encountered.
943         - 'allowed': pass dask arrays directly on to ``func``. Prefer this option if
944           ``func`` natively supports dask arrays.
945         - 'parallelized': automatically parallelize ``func`` if any of the
946           inputs are a dask array by using :py:func:`dask.array.apply_gufunc`. Multiple output
947           arguments are supported. Only use this option if ``func`` does not natively
948           support dask arrays (e.g. converts them to numpy arrays).
949     dask_gufunc_kwargs : dict, optional
950         Optional keyword arguments passed to :py:func:`dask.array.apply_gufunc` if
951         dask='parallelized'. Possible keywords are ``output_sizes``, ``allow_rechunk``
952         and ``meta``.
953     output_dtypes : list of dtype, optional
954         Optional list of output dtypes. Only used if ``dask='parallelized'`` or
955         ``vectorize=True``.
956     output_sizes : dict, optional
957         Optional mapping from dimension names to sizes for outputs. Only used
958         if dask='parallelized' and new dimensions (not found on inputs) appear
959         on outputs. ``output_sizes`` should be given in the ``dask_gufunc_kwargs``
960         parameter. It will be removed as direct parameter in a future version.
961     meta : optional
962         Size-0 object representing the type of array wrapped by dask array. Passed on to
963         :py:func:`dask.array.apply_gufunc`. ``meta`` should be given in the
964         ``dask_gufunc_kwargs`` parameter . It will be removed as direct parameter
965         a future version.
966 
967     Returns
968     -------
969     Single value or tuple of Dataset, DataArray, Variable, dask.array.Array or
970     numpy.ndarray, the first type on that list to appear on an input.
971 
972     Notes
973     -----
974     This function is designed for the more common case where ``func`` can work on numpy
975     arrays. If ``func`` needs to manipulate a whole xarray object subset to each block
976     it is possible to use :py:func:`xarray.map_blocks`.
977 
978     Note that due to the overhead :py:func:`xarray.map_blocks` is considerably slower than ``apply_ufunc``.
979 
980     Examples
981     --------
982     Calculate the vector magnitude of two arguments:
983 
984     >>> def magnitude(a, b):
985     ...     func = lambda x, y: np.sqrt(x**2 + y**2)
986     ...     return xr.apply_ufunc(func, a, b)
987     ...
988 
989     You can now apply ``magnitude()`` to :py:class:`DataArray` and :py:class:`Dataset`
990     objects, with automatically preserved dimensions and coordinates, e.g.,
991 
992     >>> array = xr.DataArray([1, 2, 3], coords=[("x", [0.1, 0.2, 0.3])])
993     >>> magnitude(array, -array)
994     <xarray.DataArray (x: 3)>
995     array([1.41421356, 2.82842712, 4.24264069])
996     Coordinates:
997       * x        (x) float64 0.1 0.2 0.3
998 
999     Plain scalars, numpy arrays and a mix of these with xarray objects is also
1000     supported:
1001 
1002     >>> magnitude(3, 4)
1003     5.0
1004     >>> magnitude(3, np.array([0, 4]))
1005     array([3., 5.])
1006     >>> magnitude(array, 0)
1007     <xarray.DataArray (x: 3)>
1008     array([1., 2., 3.])
1009     Coordinates:
1010       * x        (x) float64 0.1 0.2 0.3
1011 
1012     Other examples of how you could use ``apply_ufunc`` to write functions to
1013     (very nearly) replicate existing xarray functionality:
1014 
1015     Compute the mean (``.mean``) over one dimension:
1016 
1017     >>> def mean(obj, dim):
1018     ...     # note: apply always moves core dimensions to the end
1019     ...     return apply_ufunc(
1020     ...         np.mean, obj, input_core_dims=[[dim]], kwargs={"axis": -1}
1021     ...     )
1022     ...
1023 
1024     Inner product over a specific dimension (like :py:func:`dot`):
1025 
1026     >>> def _inner(x, y):
1027     ...     result = np.matmul(x[..., np.newaxis, :], y[..., :, np.newaxis])
1028     ...     return result[..., 0, 0]
1029     ...
1030     >>> def inner_product(a, b, dim):
1031     ...     return apply_ufunc(_inner, a, b, input_core_dims=[[dim], [dim]])
1032     ...
1033 
1034     Stack objects along a new dimension (like :py:func:`concat`):
1035 
1036     >>> def stack(objects, dim, new_coord):
1037     ...     # note: this version does not stack coordinates
1038     ...     func = lambda *x: np.stack(x, axis=-1)
1039     ...     result = apply_ufunc(
1040     ...         func,
1041     ...         *objects,
1042     ...         output_core_dims=[[dim]],
1043     ...         join="outer",
1044     ...         dataset_fill_value=np.nan
1045     ...     )
1046     ...     result[dim] = new_coord
1047     ...     return result
1048     ...
1049 
1050     If your function is not vectorized but can be applied only to core
1051     dimensions, you can use ``vectorize=True`` to turn into a vectorized
1052     function. This wraps :py:func:`numpy.vectorize`, so the operation isn't
1053     terribly fast. Here we'll use it to calculate the distance between
1054     empirical samples from two probability distributions, using a scipy
1055     function that needs to be applied to vectors:
1056 
1057     >>> import scipy.stats
1058     >>> def earth_mover_distance(first_samples, second_samples, dim="ensemble"):
1059     ...     return apply_ufunc(
1060     ...         scipy.stats.wasserstein_distance,
1061     ...         first_samples,
1062     ...         second_samples,
1063     ...         input_core_dims=[[dim], [dim]],
1064     ...         vectorize=True,
1065     ...     )
1066     ...
1067 
1068     Most of NumPy's builtin functions already broadcast their inputs
1069     appropriately for use in ``apply_ufunc``. You may find helper functions such as
1070     :py:func:`numpy.broadcast_arrays` helpful in writing your function. ``apply_ufunc`` also
1071     works well with :py:func:`numba.vectorize` and :py:func:`numba.guvectorize`.
1072 
1073     See Also
1074     --------
1075     numpy.broadcast_arrays
1076     numba.vectorize
1077     numba.guvectorize
1078     dask.array.apply_gufunc
1079     xarray.map_blocks
1080     :ref:`dask.automatic-parallelization`
1081         User guide describing :py:func:`apply_ufunc` and :py:func:`map_blocks`.
1082 
1083     References
1084     ----------
1085     .. [1] https://numpy.org/doc/stable/reference/ufuncs.html
1086     .. [2] https://numpy.org/doc/stable/reference/c-api/generalized-ufuncs.html
1087     """
1088     from xarray.core.dataarray import DataArray
1089     from xarray.core.groupby import GroupBy
1090     from xarray.core.variable import Variable
1091 
1092     if input_core_dims is None:
1093         input_core_dims = ((),) * (len(args))
1094     elif len(input_core_dims) != len(args):
1095         raise ValueError(
1096             f"input_core_dims must be None or a tuple with the length same to "
1097             f"the number of arguments. "
1098             f"Given {len(input_core_dims)} input_core_dims: {input_core_dims}, "
1099             f" but number of args is {len(args)}."
1100         )
1101 
1102     if kwargs is None:
1103         kwargs = {}
1104 
1105     signature = _UFuncSignature(input_core_dims, output_core_dims)
1106 
1107     if exclude_dims:
1108         if not isinstance(exclude_dims, set):
1109             raise TypeError(
1110                 f"Expected exclude_dims to be a 'set'. Received '{type(exclude_dims).__name__}' instead."
1111             )
1112         if not exclude_dims <= signature.all_core_dims:
1113             raise ValueError(
1114                 f"each dimension in `exclude_dims` must also be a "
1115                 f"core dimension in the function signature. "
1116                 f"Please make {(exclude_dims - signature.all_core_dims)} a core dimension"
1117             )
1118 
1119     # handle dask_gufunc_kwargs
1120     if dask == "parallelized":
1121         if dask_gufunc_kwargs is None:
1122             dask_gufunc_kwargs = {}
1123         else:
1124             dask_gufunc_kwargs = dask_gufunc_kwargs.copy()
1125         # todo: remove warnings after deprecation cycle
1126         if meta is not None:
1127             warnings.warn(
1128                 "``meta`` should be given in the ``dask_gufunc_kwargs`` parameter."
1129                 " It will be removed as direct parameter in a future version.",
1130                 FutureWarning,
1131                 stacklevel=2,
1132             )
1133             dask_gufunc_kwargs.setdefault("meta", meta)
1134         if output_sizes is not None:
1135             warnings.warn(
1136                 "``output_sizes`` should be given in the ``dask_gufunc_kwargs`` "
1137                 "parameter. It will be removed as direct parameter in a future "
1138                 "version.",
1139                 FutureWarning,
1140                 stacklevel=2,
1141             )
1142             dask_gufunc_kwargs.setdefault("output_sizes", output_sizes)
1143 
1144     if kwargs:
1145         func = functools.partial(func, **kwargs)
1146 
1147     if keep_attrs is None:
1148         keep_attrs = _get_keep_attrs(default=False)
1149 
1150     if isinstance(keep_attrs, bool):
1151         keep_attrs = "override" if keep_attrs else "drop"
1152 
1153     variables_vfunc = functools.partial(
1154         apply_variable_ufunc,
1155         func,
1156         signature=signature,
1157         exclude_dims=exclude_dims,
1158         keep_attrs=keep_attrs,
1159         dask=dask,
1160         vectorize=vectorize,
1161         output_dtypes=output_dtypes,
1162         dask_gufunc_kwargs=dask_gufunc_kwargs,
1163     )
1164 
1165     # feed groupby-apply_ufunc through apply_groupby_func
1166     if any(isinstance(a, GroupBy) for a in args):
1167         this_apply = functools.partial(
1168             apply_ufunc,
1169             func,
1170             input_core_dims=input_core_dims,
1171             output_core_dims=output_core_dims,
1172             exclude_dims=exclude_dims,
1173             join=join,
1174             dataset_join=dataset_join,
1175             dataset_fill_value=dataset_fill_value,
1176             keep_attrs=keep_attrs,
1177             dask=dask,
1178             vectorize=vectorize,
1179             output_dtypes=output_dtypes,
1180             dask_gufunc_kwargs=dask_gufunc_kwargs,
1181         )
1182         return apply_groupby_func(this_apply, *args)
1183     # feed datasets apply_variable_ufunc through apply_dataset_vfunc
1184     elif any(is_dict_like(a) for a in args):
1185         return apply_dataset_vfunc(
1186             variables_vfunc,
1187             *args,
1188             signature=signature,
1189             join=join,
1190             exclude_dims=exclude_dims,
1191             dataset_join=dataset_join,
1192             fill_value=dataset_fill_value,
1193             keep_attrs=keep_attrs,
1194         )
1195     # feed DataArray apply_variable_ufunc through apply_dataarray_vfunc
1196     elif any(isinstance(a, DataArray) for a in args):
1197         return apply_dataarray_vfunc(
1198             variables_vfunc,
1199             *args,
1200             signature=signature,
1201             join=join,
1202             exclude_dims=exclude_dims,
1203             keep_attrs=keep_attrs,
1204         )
1205     # feed Variables directly through apply_variable_ufunc
1206     elif any(isinstance(a, Variable) for a in args):
1207         return variables_vfunc(*args)
1208     else:
1209         # feed anything else through apply_array_ufunc
1210         return apply_array_ufunc(func, *args, dask=dask)
1211 
1212 
1213 def cov(
1214     da_a: T_DataArray, da_b: T_DataArray, dim: Dims = None, ddof: int = 1
1215 ) -> T_DataArray:
1216     """
1217     Compute covariance between two DataArray objects along a shared dimension.
1218 
1219     Parameters
1220     ----------
1221     da_a : DataArray
1222         Array to compute.
1223     da_b : DataArray
1224         Array to compute.
1225     dim : str, iterable of hashable, "..." or None, optional
1226         The dimension along which the covariance will be computed
1227     ddof : int, default: 1
1228         If ddof=1, covariance is normalized by N-1, giving an unbiased estimate,
1229         else normalization is by N.
1230 
1231     Returns
1232     -------
1233     covariance : DataArray
1234 
1235     See Also
1236     --------
1237     pandas.Series.cov : corresponding pandas function
1238     xarray.corr : respective function to calculate correlation
1239 
1240     Examples
1241     --------
1242     >>> from xarray import DataArray
1243     >>> da_a = DataArray(
1244     ...     np.array([[1, 2, 3], [0.1, 0.2, 0.3], [3.2, 0.6, 1.8]]),
1245     ...     dims=("space", "time"),
1246     ...     coords=[
1247     ...         ("space", ["IA", "IL", "IN"]),
1248     ...         ("time", pd.date_range("2000-01-01", freq="1D", periods=3)),
1249     ...     ],
1250     ... )
1251     >>> da_a
1252     <xarray.DataArray (space: 3, time: 3)>
1253     array([[1. , 2. , 3. ],
1254            [0.1, 0.2, 0.3],
1255            [3.2, 0.6, 1.8]])
1256     Coordinates:
1257       * space    (space) <U2 'IA' 'IL' 'IN'
1258       * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03
1259     >>> da_b = DataArray(
1260     ...     np.array([[0.2, 0.4, 0.6], [15, 10, 5], [3.2, 0.6, 1.8]]),
1261     ...     dims=("space", "time"),
1262     ...     coords=[
1263     ...         ("space", ["IA", "IL", "IN"]),
1264     ...         ("time", pd.date_range("2000-01-01", freq="1D", periods=3)),
1265     ...     ],
1266     ... )
1267     >>> da_b
1268     <xarray.DataArray (space: 3, time: 3)>
1269     array([[ 0.2,  0.4,  0.6],
1270            [15. , 10. ,  5. ],
1271            [ 3.2,  0.6,  1.8]])
1272     Coordinates:
1273       * space    (space) <U2 'IA' 'IL' 'IN'
1274       * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03
1275     >>> xr.cov(da_a, da_b)
1276     <xarray.DataArray ()>
1277     array(-3.53055556)
1278     >>> xr.cov(da_a, da_b, dim="time")
1279     <xarray.DataArray (space: 3)>
1280     array([ 0.2       , -0.5       ,  1.69333333])
1281     Coordinates:
1282       * space    (space) <U2 'IA' 'IL' 'IN'
1283     """
1284     from xarray.core.dataarray import DataArray
1285 
1286     if any(not isinstance(arr, DataArray) for arr in [da_a, da_b]):
1287         raise TypeError(
1288             "Only xr.DataArray is supported."
1289             "Given {}.".format([type(arr) for arr in [da_a, da_b]])
1290         )
1291 
1292     return _cov_corr(da_a, da_b, dim=dim, ddof=ddof, method="cov")
1293 
1294 
1295 def corr(da_a: T_DataArray, da_b: T_DataArray, dim: Dims = None) -> T_DataArray:
1296     """
1297     Compute the Pearson correlation coefficient between
1298     two DataArray objects along a shared dimension.
1299 
1300     Parameters
1301     ----------
1302     da_a : DataArray
1303         Array to compute.
1304     da_b : DataArray
1305         Array to compute.
1306     dim : str, iterable of hashable, "..." or None, optional
1307         The dimension along which the correlation will be computed
1308 
1309     Returns
1310     -------
1311     correlation: DataArray
1312 
1313     See Also
1314     --------
1315     pandas.Series.corr : corresponding pandas function
1316     xarray.cov : underlying covariance function
1317 
1318     Examples
1319     --------
1320     >>> from xarray import DataArray
1321     >>> da_a = DataArray(
1322     ...     np.array([[1, 2, 3], [0.1, 0.2, 0.3], [3.2, 0.6, 1.8]]),
1323     ...     dims=("space", "time"),
1324     ...     coords=[
1325     ...         ("space", ["IA", "IL", "IN"]),
1326     ...         ("time", pd.date_range("2000-01-01", freq="1D", periods=3)),
1327     ...     ],
1328     ... )
1329     >>> da_a
1330     <xarray.DataArray (space: 3, time: 3)>
1331     array([[1. , 2. , 3. ],
1332            [0.1, 0.2, 0.3],
1333            [3.2, 0.6, 1.8]])
1334     Coordinates:
1335       * space    (space) <U2 'IA' 'IL' 'IN'
1336       * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03
1337     >>> da_b = DataArray(
1338     ...     np.array([[0.2, 0.4, 0.6], [15, 10, 5], [3.2, 0.6, 1.8]]),
1339     ...     dims=("space", "time"),
1340     ...     coords=[
1341     ...         ("space", ["IA", "IL", "IN"]),
1342     ...         ("time", pd.date_range("2000-01-01", freq="1D", periods=3)),
1343     ...     ],
1344     ... )
1345     >>> da_b
1346     <xarray.DataArray (space: 3, time: 3)>
1347     array([[ 0.2,  0.4,  0.6],
1348            [15. , 10. ,  5. ],
1349            [ 3.2,  0.6,  1.8]])
1350     Coordinates:
1351       * space    (space) <U2 'IA' 'IL' 'IN'
1352       * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03
1353     >>> xr.corr(da_a, da_b)
1354     <xarray.DataArray ()>
1355     array(-0.57087777)
1356     >>> xr.corr(da_a, da_b, dim="time")
1357     <xarray.DataArray (space: 3)>
1358     array([ 1., -1.,  1.])
1359     Coordinates:
1360       * space    (space) <U2 'IA' 'IL' 'IN'
1361     """
1362     from xarray.core.dataarray import DataArray
1363 
1364     if any(not isinstance(arr, DataArray) for arr in [da_a, da_b]):
1365         raise TypeError(
1366             "Only xr.DataArray is supported."
1367             "Given {}.".format([type(arr) for arr in [da_a, da_b]])
1368         )
1369 
1370     return _cov_corr(da_a, da_b, dim=dim, method="corr")
1371 
1372 
1373 def _cov_corr(
1374     da_a: T_DataArray,
1375     da_b: T_DataArray,
1376     dim: Dims = None,
1377     ddof: int = 0,
1378     method: Literal["cov", "corr", None] = None,
1379 ) -> T_DataArray:
1380     """
1381     Internal method for xr.cov() and xr.corr() so only have to
1382     sanitize the input arrays once and we don't repeat code.
1383     """
1384     # 1. Broadcast the two arrays
1385     da_a, da_b = align(da_a, da_b, join="inner", copy=False)
1386 
1387     # 2. Ignore the nans
1388     valid_values = da_a.notnull() & da_b.notnull()
1389     da_a = da_a.where(valid_values)
1390     da_b = da_b.where(valid_values)
1391     valid_count = valid_values.sum(dim) - ddof
1392 
1393     # 3. Detrend along the given dim
1394     demeaned_da_a = da_a - da_a.mean(dim=dim)
1395     demeaned_da_b = da_b - da_b.mean(dim=dim)
1396 
1397     # 4. Compute covariance along the given dim
1398     # N.B. `skipna=True` is required or auto-covariance is computed incorrectly. E.g.
1399     # Try xr.cov(da,da) for da = xr.DataArray([[1, 2], [1, np.nan]], dims=["x", "time"])
1400     cov = (demeaned_da_a.conj() * demeaned_da_b).sum(
1401         dim=dim, skipna=True, min_count=1
1402     ) / (valid_count)
1403 
1404     if method == "cov":
1405         return cov  # type: ignore[return-value]
1406 
1407     else:
1408         # compute std + corr
1409         da_a_std = da_a.std(dim=dim)
1410         da_b_std = da_b.std(dim=dim)
1411         corr = cov / (da_a_std * da_b_std)
1412         return corr  # type: ignore[return-value]
1413 
1414 
1415 def cross(
1416     a: DataArray | Variable, b: DataArray | Variable, *, dim: Hashable
1417 ) -> DataArray | Variable:
1418     """
1419     Compute the cross product of two (arrays of) vectors.
1420 
1421     The cross product of `a` and `b` in :math:`R^3` is a vector
1422     perpendicular to both `a` and `b`. The vectors in `a` and `b` are
1423     defined by the values along the dimension `dim` and can have sizes
1424     1, 2 or 3. Where the size of either `a` or `b` is
1425     1 or 2, the remaining components of the input vector is assumed to
1426     be zero and the cross product calculated accordingly. In cases where
1427     both input vectors have dimension 2, the z-component of the cross
1428     product is returned.
1429 
1430     Parameters
1431     ----------
1432     a, b : DataArray or Variable
1433         Components of the first and second vector(s).
1434     dim : hashable
1435         The dimension along which the cross product will be computed.
1436         Must be available in both vectors.
1437 
1438     Examples
1439     --------
1440     Vector cross-product with 3 dimensions:
1441 
1442     >>> a = xr.DataArray([1, 2, 3])
1443     >>> b = xr.DataArray([4, 5, 6])
1444     >>> xr.cross(a, b, dim="dim_0")
1445     <xarray.DataArray (dim_0: 3)>
1446     array([-3,  6, -3])
1447     Dimensions without coordinates: dim_0
1448 
1449     Vector cross-product with 2 dimensions, returns in the perpendicular
1450     direction:
1451 
1452     >>> a = xr.DataArray([1, 2])
1453     >>> b = xr.DataArray([4, 5])
1454     >>> xr.cross(a, b, dim="dim_0")
1455     <xarray.DataArray ()>
1456     array(-3)
1457 
1458     Vector cross-product with 3 dimensions but zeros at the last axis
1459     yields the same results as with 2 dimensions:
1460 
1461     >>> a = xr.DataArray([1, 2, 0])
1462     >>> b = xr.DataArray([4, 5, 0])
1463     >>> xr.cross(a, b, dim="dim_0")
1464     <xarray.DataArray (dim_0: 3)>
1465     array([ 0,  0, -3])
1466     Dimensions without coordinates: dim_0
1467 
1468     One vector with dimension 2:
1469 
1470     >>> a = xr.DataArray(
1471     ...     [1, 2],
1472     ...     dims=["cartesian"],
1473     ...     coords=dict(cartesian=(["cartesian"], ["x", "y"])),
1474     ... )
1475     >>> b = xr.DataArray(
1476     ...     [4, 5, 6],
1477     ...     dims=["cartesian"],
1478     ...     coords=dict(cartesian=(["cartesian"], ["x", "y", "z"])),
1479     ... )
1480     >>> xr.cross(a, b, dim="cartesian")
1481     <xarray.DataArray (cartesian: 3)>
1482     array([12, -6, -3])
1483     Coordinates:
1484       * cartesian  (cartesian) <U1 'x' 'y' 'z'
1485 
1486     One vector with dimension 2 but coords in other positions:
1487 
1488     >>> a = xr.DataArray(
1489     ...     [1, 2],
1490     ...     dims=["cartesian"],
1491     ...     coords=dict(cartesian=(["cartesian"], ["x", "z"])),
1492     ... )
1493     >>> b = xr.DataArray(
1494     ...     [4, 5, 6],
1495     ...     dims=["cartesian"],
1496     ...     coords=dict(cartesian=(["cartesian"], ["x", "y", "z"])),
1497     ... )
1498     >>> xr.cross(a, b, dim="cartesian")
1499     <xarray.DataArray (cartesian: 3)>
1500     array([-10,   2,   5])
1501     Coordinates:
1502       * cartesian  (cartesian) <U1 'x' 'y' 'z'
1503 
1504     Multiple vector cross-products. Note that the direction of the
1505     cross product vector is defined by the right-hand rule:
1506 
1507     >>> a = xr.DataArray(
1508     ...     [[1, 2, 3], [4, 5, 6]],
1509     ...     dims=("time", "cartesian"),
1510     ...     coords=dict(
1511     ...         time=(["time"], [0, 1]),
1512     ...         cartesian=(["cartesian"], ["x", "y", "z"]),
1513     ...     ),
1514     ... )
1515     >>> b = xr.DataArray(
1516     ...     [[4, 5, 6], [1, 2, 3]],
1517     ...     dims=("time", "cartesian"),
1518     ...     coords=dict(
1519     ...         time=(["time"], [0, 1]),
1520     ...         cartesian=(["cartesian"], ["x", "y", "z"]),
1521     ...     ),
1522     ... )
1523     >>> xr.cross(a, b, dim="cartesian")
1524     <xarray.DataArray (time: 2, cartesian: 3)>
1525     array([[-3,  6, -3],
1526            [ 3, -6,  3]])
1527     Coordinates:
1528       * time       (time) int64 0 1
1529       * cartesian  (cartesian) <U1 'x' 'y' 'z'
1530 
1531     Cross can be called on Datasets by converting to DataArrays and later
1532     back to a Dataset:
1533 
1534     >>> ds_a = xr.Dataset(dict(x=("dim_0", [1]), y=("dim_0", [2]), z=("dim_0", [3])))
1535     >>> ds_b = xr.Dataset(dict(x=("dim_0", [4]), y=("dim_0", [5]), z=("dim_0", [6])))
1536     >>> c = xr.cross(
1537     ...     ds_a.to_array("cartesian"), ds_b.to_array("cartesian"), dim="cartesian"
1538     ... )
1539     >>> c.to_dataset(dim="cartesian")
1540     <xarray.Dataset>
1541     Dimensions:  (dim_0: 1)
1542     Dimensions without coordinates: dim_0
1543     Data variables:
1544         x        (dim_0) int64 -3
1545         y        (dim_0) int64 6
1546         z        (dim_0) int64 -3
1547 
1548     See Also
1549     --------
1550     numpy.cross : Corresponding numpy function
1551     """
1552 
1553     if dim not in a.dims:
1554         raise ValueError(f"Dimension {dim!r} not on a")
1555     elif dim not in b.dims:
1556         raise ValueError(f"Dimension {dim!r} not on b")
1557 
1558     if not 1 <= a.sizes[dim] <= 3:
1559         raise ValueError(
1560             f"The size of {dim!r} on a must be 1, 2, or 3 to be "
1561             f"compatible with a cross product but is {a.sizes[dim]}"
1562         )
1563     elif not 1 <= b.sizes[dim] <= 3:
1564         raise ValueError(
1565             f"The size of {dim!r} on b must be 1, 2, or 3 to be "
1566             f"compatible with a cross product but is {b.sizes[dim]}"
1567         )
1568 
1569     all_dims = list(dict.fromkeys(a.dims + b.dims))
1570 
1571     if a.sizes[dim] != b.sizes[dim]:
1572         # Arrays have different sizes. Append zeros where the smaller
1573         # array is missing a value, zeros will not affect np.cross:
1574 
1575         if (
1576             not isinstance(a, Variable)  # Only used to make mypy happy.
1577             and dim in getattr(a, "coords", {})
1578             and not isinstance(b, Variable)  # Only used to make mypy happy.
1579             and dim in getattr(b, "coords", {})
1580         ):
1581             # If the arrays have coords we know which indexes to fill
1582             # with zeros:
1583             a, b = align(
1584                 a,
1585                 b,
1586                 fill_value=0,
1587                 join="outer",
1588                 exclude=set(all_dims) - {dim},
1589             )
1590         elif min(a.sizes[dim], b.sizes[dim]) == 2:
1591             # If the array doesn't have coords we can only infer
1592             # that it has composite values if the size is at least 2.
1593             # Once padded, rechunk the padded array because apply_ufunc
1594             # requires core dimensions not to be chunked:
1595             if a.sizes[dim] < b.sizes[dim]:
1596                 a = a.pad({dim: (0, 1)}, constant_values=0)
1597                 # TODO: Should pad or apply_ufunc handle correct chunking?
1598                 a = a.chunk({dim: -1}) if is_duck_dask_array(a.data) else a
1599             else:
1600                 b = b.pad({dim: (0, 1)}, constant_values=0)
1601                 # TODO: Should pad or apply_ufunc handle correct chunking?
1602                 b = b.chunk({dim: -1}) if is_duck_dask_array(b.data) else b
1603         else:
1604             raise ValueError(
1605                 f"{dim!r} on {'a' if a.sizes[dim] == 1 else 'b'} is incompatible:"
1606                 " dimensions without coordinates must have have a length of 2 or 3"
1607             )
1608 
1609     c = apply_ufunc(
1610         np.cross,
1611         a,
1612         b,
1613         input_core_dims=[[dim], [dim]],
1614         output_core_dims=[[dim] if a.sizes[dim] == 3 else []],
1615         dask="parallelized",
1616         output_dtypes=[np.result_type(a, b)],
1617     )
1618     c = c.transpose(*all_dims, missing_dims="ignore")
1619 
1620     return c
1621 
1622 
1623 def dot(
1624     *arrays,
1625     dims: Dims = None,
1626     **kwargs: Any,
1627 ):
1628     """Generalized dot product for xarray objects. Like np.einsum, but
1629     provides a simpler interface based on array dimensions.
1630 
1631     Parameters
1632     ----------
1633     *arrays : DataArray or Variable
1634         Arrays to compute.
1635     dims : str, iterable of hashable, "..." or None, optional
1636         Which dimensions to sum over. Ellipsis ('...') sums over all dimensions.
1637         If not specified, then all the common dimensions are summed over.
1638     **kwargs : dict
1639         Additional keyword arguments passed to numpy.einsum or
1640         dask.array.einsum
1641 
1642     Returns
1643     -------
1644     DataArray
1645 
1646     Examples
1647     --------
1648     >>> da_a = xr.DataArray(np.arange(3 * 2).reshape(3, 2), dims=["a", "b"])
1649     >>> da_b = xr.DataArray(np.arange(3 * 2 * 2).reshape(3, 2, 2), dims=["a", "b", "c"])
1650     >>> da_c = xr.DataArray(np.arange(2 * 3).reshape(2, 3), dims=["c", "d"])
1651 
1652     >>> da_a
1653     <xarray.DataArray (a: 3, b: 2)>
1654     array([[0, 1],
1655            [2, 3],
1656            [4, 5]])
1657     Dimensions without coordinates: a, b
1658 
1659     >>> da_b
1660     <xarray.DataArray (a: 3, b: 2, c: 2)>
1661     array([[[ 0,  1],
1662             [ 2,  3]],
1663     <BLANKLINE>
1664            [[ 4,  5],
1665             [ 6,  7]],
1666     <BLANKLINE>
1667            [[ 8,  9],
1668             [10, 11]]])
1669     Dimensions without coordinates: a, b, c
1670 
1671     >>> da_c
1672     <xarray.DataArray (c: 2, d: 3)>
1673     array([[0, 1, 2],
1674            [3, 4, 5]])
1675     Dimensions without coordinates: c, d
1676 
1677     >>> xr.dot(da_a, da_b, dims=["a", "b"])
1678     <xarray.DataArray (c: 2)>
1679     array([110, 125])
1680     Dimensions without coordinates: c
1681 
1682     >>> xr.dot(da_a, da_b, dims=["a"])
1683     <xarray.DataArray (b: 2, c: 2)>
1684     array([[40, 46],
1685            [70, 79]])
1686     Dimensions without coordinates: b, c
1687 
1688     >>> xr.dot(da_a, da_b, da_c, dims=["b", "c"])
1689     <xarray.DataArray (a: 3, d: 3)>
1690     array([[  9,  14,  19],
1691            [ 93, 150, 207],
1692            [273, 446, 619]])
1693     Dimensions without coordinates: a, d
1694 
1695     >>> xr.dot(da_a, da_b)
1696     <xarray.DataArray (c: 2)>
1697     array([110, 125])
1698     Dimensions without coordinates: c
1699 
1700     >>> xr.dot(da_a, da_b, dims=...)
1701     <xarray.DataArray ()>
1702     array(235)
1703     """
1704     from xarray.core.dataarray import DataArray
1705     from xarray.core.variable import Variable
1706 
1707     if any(not isinstance(arr, (Variable, DataArray)) for arr in arrays):
1708         raise TypeError(
1709             "Only xr.DataArray and xr.Variable are supported."
1710             "Given {}.".format([type(arr) for arr in arrays])
1711         )
1712 
1713     if len(arrays) == 0:
1714         raise TypeError("At least one array should be given.")
1715 
1716     common_dims: set[Hashable] = set.intersection(*(set(arr.dims) for arr in arrays))
1717     all_dims = []
1718     for arr in arrays:
1719         all_dims += [d for d in arr.dims if d not in all_dims]
1720 
1721     einsum_axes = "abcdefghijklmnopqrstuvwxyz"
1722     dim_map = {d: einsum_axes[i] for i, d in enumerate(all_dims)}
1723 
1724     if dims is ...:
1725         dims = all_dims
1726     elif isinstance(dims, str):
1727         dims = (dims,)
1728     elif dims is None:
1729         # find dimensions that occur more than one times
1730         dim_counts: Counter = Counter()
1731         for arr in arrays:
1732             dim_counts.update(arr.dims)
1733         dims = tuple(d for d, c in dim_counts.items() if c > 1)
1734 
1735     dot_dims: set[Hashable] = set(dims)
1736 
1737     # dimensions to be parallelized
1738     broadcast_dims = common_dims - dot_dims
1739     input_core_dims = [
1740         [d for d in arr.dims if d not in broadcast_dims] for arr in arrays
1741     ]
1742     output_core_dims = [
1743         [d for d in all_dims if d not in dot_dims and d not in broadcast_dims]
1744     ]
1745 
1746     # construct einsum subscripts, such as '...abc,...ab->...c'
1747     # Note: input_core_dims are always moved to the last position
1748     subscripts_list = [
1749         "..." + "".join(dim_map[d] for d in ds) for ds in input_core_dims
1750     ]
1751     subscripts = ",".join(subscripts_list)
1752     subscripts += "->..." + "".join(dim_map[d] for d in output_core_dims[0])
1753 
1754     join = OPTIONS["arithmetic_join"]
1755     # using "inner" emulates `(a * b).sum()` for all joins (except "exact")
1756     if join != "exact":
1757         join = "inner"
1758 
1759     # subscripts should be passed to np.einsum as arg, not as kwargs. We need
1760     # to construct a partial function for apply_ufunc to work.
1761     func = functools.partial(duck_array_ops.einsum, subscripts, **kwargs)
1762     result = apply_ufunc(
1763         func,
1764         *arrays,
1765         input_core_dims=input_core_dims,
1766         output_core_dims=output_core_dims,
1767         join=join,
1768         dask="allowed",
1769     )
1770     return result.transpose(*all_dims, missing_dims="ignore")
1771 
1772 
1773 def where(cond, x, y, keep_attrs=None):
1774     """Return elements from `x` or `y` depending on `cond`.
1775 
1776     Performs xarray-like broadcasting across input arguments.
1777 
1778     All dimension coordinates on `x` and `y`  must be aligned with each
1779     other and with `cond`.
1780 
1781     Parameters
1782     ----------
1783     cond : scalar, array, Variable, DataArray or Dataset
1784         When True, return values from `x`, otherwise returns values from `y`.
1785     x : scalar, array, Variable, DataArray or Dataset
1786         values to choose from where `cond` is True
1787     y : scalar, array, Variable, DataArray or Dataset
1788         values to choose from where `cond` is False
1789     keep_attrs : bool or str or callable, optional
1790         How to treat attrs. If True, keep the attrs of `x`.
1791 
1792     Returns
1793     -------
1794     Dataset, DataArray, Variable or array
1795         In priority order: Dataset, DataArray, Variable or array, whichever
1796         type appears as an input argument.
1797 
1798     Examples
1799     --------
1800     >>> x = xr.DataArray(
1801     ...     0.1 * np.arange(10),
1802     ...     dims=["lat"],
1803     ...     coords={"lat": np.arange(10)},
1804     ...     name="sst",
1805     ... )
1806     >>> x
1807     <xarray.DataArray 'sst' (lat: 10)>
1808     array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])
1809     Coordinates:
1810       * lat      (lat) int64 0 1 2 3 4 5 6 7 8 9
1811 
1812     >>> xr.where(x < 0.5, x, x * 100)
1813     <xarray.DataArray 'sst' (lat: 10)>
1814     array([ 0. ,  0.1,  0.2,  0.3,  0.4, 50. , 60. , 70. , 80. , 90. ])
1815     Coordinates:
1816       * lat      (lat) int64 0 1 2 3 4 5 6 7 8 9
1817 
1818     >>> y = xr.DataArray(
1819     ...     0.1 * np.arange(9).reshape(3, 3),
1820     ...     dims=["lat", "lon"],
1821     ...     coords={"lat": np.arange(3), "lon": 10 + np.arange(3)},
1822     ...     name="sst",
1823     ... )
1824     >>> y
1825     <xarray.DataArray 'sst' (lat: 3, lon: 3)>
1826     array([[0. , 0.1, 0.2],
1827            [0.3, 0.4, 0.5],
1828            [0.6, 0.7, 0.8]])
1829     Coordinates:
1830       * lat      (lat) int64 0 1 2
1831       * lon      (lon) int64 10 11 12
1832 
1833     >>> xr.where(y.lat < 1, y, -1)
1834     <xarray.DataArray (lat: 3, lon: 3)>
1835     array([[ 0. ,  0.1,  0.2],
1836            [-1. , -1. , -1. ],
1837            [-1. , -1. , -1. ]])
1838     Coordinates:
1839       * lat      (lat) int64 0 1 2
1840       * lon      (lon) int64 10 11 12
1841 
1842     >>> cond = xr.DataArray([True, False], dims=["x"])
1843     >>> x = xr.DataArray([1, 2], dims=["y"])
1844     >>> xr.where(cond, x, 0)
1845     <xarray.DataArray (x: 2, y: 2)>
1846     array([[1, 2],
1847            [0, 0]])
1848     Dimensions without coordinates: x, y
1849 
1850     See Also
1851     --------
1852     numpy.where : corresponding numpy function
1853     Dataset.where, DataArray.where :
1854         equivalent methods
1855     """
1856     from xarray.core.dataset import Dataset
1857 
1858     if keep_attrs is None:
1859         keep_attrs = _get_keep_attrs(default=False)
1860 
1861     # alignment for three arguments is complicated, so don't support it yet
1862     result = apply_ufunc(
1863         duck_array_ops.where,
1864         cond,
1865         x,
1866         y,
1867         join="exact",
1868         dataset_join="exact",
1869         dask="allowed",
1870         keep_attrs=keep_attrs,
1871     )
1872 
1873     # keep the attributes of x, the second parameter, by default to
1874     # be consistent with the `where` method of `DataArray` and `Dataset`
1875     # rebuild the attrs from x at each level of the output, which could be
1876     # Dataset, DataArray, or Variable, and also handle coords
1877     if keep_attrs is True and hasattr(result, "attrs"):
1878         if isinstance(y, Dataset) and not isinstance(x, Dataset):
1879             # handle special case where x gets promoted to Dataset
1880             result.attrs = {}
1881             if getattr(x, "name", None) in result.data_vars:
1882                 result[x.name].attrs = getattr(x, "attrs", {})
1883         else:
1884             # otherwise, fill in global attrs and variable attrs (if they exist)
1885             result.attrs = getattr(x, "attrs", {})
1886             for v in getattr(result, "data_vars", []):
1887                 result[v].attrs = getattr(getattr(x, v, None), "attrs", {})
1888         for c in getattr(result, "coords", []):
1889             # always fill coord attrs of x
1890             result[c].attrs = getattr(getattr(x, c, None), "attrs", {})
1891 
1892     return result
1893 
1894 
1895 @overload
1896 def polyval(
1897     coord: DataArray, coeffs: DataArray, degree_dim: Hashable = "degree"
1898 ) -> DataArray:
1899     ...
1900 
1901 
1902 @overload
1903 def polyval(
1904     coord: DataArray, coeffs: Dataset, degree_dim: Hashable = "degree"
1905 ) -> Dataset:
1906     ...
1907 
1908 
1909 @overload
1910 def polyval(
1911     coord: Dataset, coeffs: DataArray, degree_dim: Hashable = "degree"
1912 ) -> Dataset:
1913     ...
1914 
1915 
1916 @overload
1917 def polyval(
1918     coord: Dataset, coeffs: Dataset, degree_dim: Hashable = "degree"
1919 ) -> Dataset:
1920     ...
1921 
1922 
1923 @overload
1924 def polyval(
1925     coord: Dataset | DataArray,
1926     coeffs: Dataset | DataArray,
1927     degree_dim: Hashable = "degree",
1928 ) -> Dataset | DataArray:
1929     ...
1930 
1931 
1932 def polyval(
1933     coord: Dataset | DataArray,
1934     coeffs: Dataset | DataArray,
1935     degree_dim: Hashable = "degree",
1936 ) -> Dataset | DataArray:
1937     """Evaluate a polynomial at specific values
1938 
1939     Parameters
1940     ----------
1941     coord : DataArray or Dataset
1942         Values at which to evaluate the polynomial.
1943     coeffs : DataArray or Dataset
1944         Coefficients of the polynomial.
1945     degree_dim : Hashable, default: "degree"
1946         Name of the polynomial degree dimension in `coeffs`.
1947 
1948     Returns
1949     -------
1950     DataArray or Dataset
1951         Evaluated polynomial.
1952 
1953     See Also
1954     --------
1955     xarray.DataArray.polyfit
1956     numpy.polynomial.polynomial.polyval
1957     """
1958 
1959     if degree_dim not in coeffs._indexes:
1960         raise ValueError(
1961             f"Dimension `{degree_dim}` should be a coordinate variable with labels."
1962         )
1963     if not np.issubdtype(coeffs[degree_dim].dtype, np.integer):
1964         raise ValueError(
1965             f"Dimension `{degree_dim}` should be of integer dtype. Received {coeffs[degree_dim].dtype} instead."
1966         )
1967     max_deg = coeffs[degree_dim].max().item()
1968     coeffs = coeffs.reindex(
1969         {degree_dim: np.arange(max_deg + 1)}, fill_value=0, copy=False
1970     )
1971     coord = _ensure_numeric(coord)
1972 
1973     # using Horner's method
1974     # https://en.wikipedia.org/wiki/Horner%27s_method
1975     res = zeros_like(coord) + coeffs.isel({degree_dim: max_deg}, drop=True)
1976     for deg in range(max_deg - 1, -1, -1):
1977         res *= coord
1978         res += coeffs.isel({degree_dim: deg}, drop=True)
1979 
1980     return res
1981 
1982 
1983 def _ensure_numeric(data: Dataset | DataArray) -> Dataset | DataArray:
1984     """Converts all datetime64 variables to float64
1985 
1986     Parameters
1987     ----------
1988     data : DataArray or Dataset
1989         Variables with possible datetime dtypes.
1990 
1991     Returns
1992     -------
1993     DataArray or Dataset
1994         Variables with datetime64 dtypes converted to float64.
1995     """
1996     from xarray.core.dataset import Dataset
1997 
1998     def _cfoffset(x: DataArray) -> Any:
1999         scalar = x.compute().data[0]
2000         if not is_scalar(scalar):
2001             # we do not get a scalar back on dask == 2021.04.1
2002             scalar = scalar.item()
2003         return type(scalar)(1970, 1, 1)
2004 
2005     def to_floatable(x: DataArray) -> DataArray:
2006         if x.dtype.kind in "MO":
2007             # datetimes (CFIndexes are object type)
2008             offset = (
2009                 np.datetime64("1970-01-01") if x.dtype.kind == "M" else _cfoffset(x)
2010             )
2011             return x.copy(
2012                 data=datetime_to_numeric(x.data, offset=offset, datetime_unit="ns"),
2013             )
2014         elif x.dtype.kind == "m":
2015             # timedeltas
2016             return x.astype(float)
2017         return x
2018 
2019     if isinstance(data, Dataset):
2020         return data.map(to_floatable)
2021     else:
2022         return to_floatable(data)
2023 
2024 
2025 def _calc_idxminmax(
2026     *,
2027     array,
2028     func: Callable,
2029     dim: Hashable | None = None,
2030     skipna: bool | None = None,
2031     fill_value: Any = dtypes.NA,
2032     keep_attrs: bool | None = None,
2033 ):
2034     """Apply common operations for idxmin and idxmax."""
2035     # This function doesn't make sense for scalars so don't try
2036     if not array.ndim:
2037         raise ValueError("This function does not apply for scalars")
2038 
2039     if dim is not None:
2040         pass  # Use the dim if available
2041     elif array.ndim == 1:
2042         # it is okay to guess the dim if there is only 1
2043         dim = array.dims[0]
2044     else:
2045         # The dim is not specified and ambiguous.  Don't guess.
2046         raise ValueError("Must supply 'dim' argument for multidimensional arrays")
2047 
2048     if dim not in array.dims:
2049         raise KeyError(f'Dimension "{dim}" not in dimension')
2050     if dim not in array.coords:
2051         raise KeyError(f'Dimension "{dim}" does not have coordinates')
2052 
2053     # These are dtypes with NaN values argmin and argmax can handle
2054     na_dtypes = "cfO"
2055 
2056     if skipna or (skipna is None and array.dtype.kind in na_dtypes):
2057         # Need to skip NaN values since argmin and argmax can't handle them
2058         allna = array.isnull().all(dim)
2059         array = array.where(~allna, 0)
2060 
2061     # This will run argmin or argmax.
2062     indx = func(array, dim=dim, axis=None, keep_attrs=keep_attrs, skipna=skipna)
2063 
2064     # Handle dask arrays.
2065     if is_duck_dask_array(array.data):
2066         import dask.array
2067 
2068         chunks = dict(zip(array.dims, array.chunks))
2069         dask_coord = dask.array.from_array(array[dim].data, chunks=chunks[dim])
2070         res = indx.copy(data=dask_coord[indx.data.ravel()].reshape(indx.shape))
2071         # we need to attach back the dim name
2072         res.name = dim
2073     else:
2074         res = array[dim][(indx,)]
2075         # The dim is gone but we need to remove the corresponding coordinate.
2076         del res.coords[dim]
2077 
2078     if skipna or (skipna is None and array.dtype.kind in na_dtypes):
2079         # Put the NaN values back in after removing them
2080         res = res.where(~allna, fill_value)
2081 
2082     # Copy attributes from argmin/argmax, if any
2083     res.attrs = indx.attrs
2084 
2085     return res
2086 
2087 
2088 _T = TypeVar("_T", bound=Union["Dataset", "DataArray"])
2089 _U = TypeVar("_U", bound=Union["Dataset", "DataArray"])
2090 _V = TypeVar("_V", bound=Union["Dataset", "DataArray"])
2091 
2092 
2093 @overload
2094 def unify_chunks(__obj: _T) -> tuple[_T]:
2095     ...
2096 
2097 
2098 @overload
2099 def unify_chunks(__obj1: _T, __obj2: _U) -> tuple[_T, _U]:
2100     ...
2101 
2102 
2103 @overload
2104 def unify_chunks(__obj1: _T, __obj2: _U, __obj3: _V) -> tuple[_T, _U, _V]:
2105     ...
2106 
2107 
2108 @overload
2109 def unify_chunks(*objects: Dataset | DataArray) -> tuple[Dataset | DataArray, ...]:
2110     ...
2111 
2112 
2113 def unify_chunks(*objects: Dataset | DataArray) -> tuple[Dataset | DataArray, ...]:
2114     """
2115     Given any number of Dataset and/or DataArray objects, returns
2116     new objects with unified chunk size along all chunked dimensions.
2117 
2118     Returns
2119     -------
2120     unified (DataArray or Dataset) – Tuple of objects with the same type as
2121     *objects with consistent chunk sizes for all dask-array variables
2122 
2123     See Also
2124     --------
2125     dask.array.core.unify_chunks
2126     """
2127     from xarray.core.dataarray import DataArray
2128 
2129     # Convert all objects to datasets
2130     datasets = [
2131         obj._to_temp_dataset() if isinstance(obj, DataArray) else obj.copy()
2132         for obj in objects
2133     ]
2134 
2135     # Get arguments to pass into dask.array.core.unify_chunks
2136     unify_chunks_args = []
2137     sizes: dict[Hashable, int] = {}
2138     for ds in datasets:
2139         for v in ds._variables.values():
2140             if v.chunks is not None:
2141                 # Check that sizes match across different datasets
2142                 for dim, size in v.sizes.items():
2143                     try:
2144                         if sizes[dim] != size:
2145                             raise ValueError(
2146                                 f"Dimension {dim!r} size mismatch: {sizes[dim]} != {size}"
2147                             )
2148                     except KeyError:
2149                         sizes[dim] = size
2150                 unify_chunks_args += [v._data, v._dims]
2151 
2152     # No dask arrays: Return inputs
2153     if not unify_chunks_args:
2154         return objects
2155 
2156     # Run dask.array.core.unify_chunks
2157     from dask.array.core import unify_chunks
2158 
2159     _, dask_data = unify_chunks(*unify_chunks_args)
2160     dask_data_iter = iter(dask_data)
2161     out: list[Dataset | DataArray] = []
2162     for obj, ds in zip(objects, datasets):
2163         for k, v in ds._variables.items():
2164             if v.chunks is not None:
2165                 ds._variables[k] = v.copy(data=next(dask_data_iter))
2166         out.append(obj._from_temp_dataset(ds) if isinstance(obj, DataArray) else ds)
2167 
2168     return tuple(out)

</code><intended_behavior>Abstract the operation handling to be compatible with multiple array backends, enabling the support for cubed and other potential future backends. This might involve creating helper functions or classes responsible for executing backend-specific code.</intended_behavior>]
2025-10-17 16:02:30.614 | INFO     | Search completed. Bug locations: [<file>xarray/core/dataarray.py</file>
<class>None</class>
<method>None</method>
<code>
1 from __future__ import annotations
2 
3 import datetime
4 import warnings
5 from collections.abc import Hashable, Iterable, Mapping, MutableMapping, Sequence
6 from os import PathLike
7 from typing import TYPE_CHECKING, Any, Callable, Literal, NoReturn, cast, overload
8 
9 import numpy as np
10 import pandas as pd
11 
12 from xarray.coding.calendar_ops import convert_calendar, interp_calendar
13 from xarray.coding.cftimeindex import CFTimeIndex
14 from xarray.core import alignment, computation, dtypes, indexing, ops, utils
15 from xarray.core._aggregations import DataArrayAggregations
16 from xarray.core.accessor_dt import CombinedDatetimelikeAccessor
17 from xarray.core.accessor_str import StringAccessor
18 from xarray.core.alignment import (
19     _broadcast_helper,
20     _get_broadcast_dims_map_common_coords,
21     align,
22 )
23 from xarray.core.arithmetic import DataArrayArithmetic
24 from xarray.core.common import AbstractArray, DataWithCoords, get_chunksizes
25 from xarray.core.computation import unify_chunks
26 from xarray.core.coordinates import DataArrayCoordinates, assert_coordinate_consistent
27 from xarray.core.dataset import Dataset
28 from xarray.core.formatting import format_item
29 from xarray.core.indexes import (
30     Index,
31     Indexes,
32     PandasMultiIndex,
33     filter_indexes_from_coords,
34     isel_indexes,
35 )
36 from xarray.core.indexing import is_fancy_indexer, map_index_queries
37 from xarray.core.merge import PANDAS_TYPES, MergeError, _create_indexes_from_coords
38 from xarray.core.options import OPTIONS, _get_keep_attrs
39 from xarray.core.utils import (
40     Default,
41     HybridMappingProxy,
42     ReprObject,
43     _default,
44     either_dict_or_kwargs,
45 )
46 from xarray.core.variable import (
47     IndexVariable,
48     Variable,
49     as_compatible_data,
50     as_variable,
51 )
52 from xarray.plot.accessor import DataArrayPlotAccessor
53 from xarray.plot.utils import _get_units_from_attrs
54 
55 if TYPE_CHECKING:
56     from typing import TypeVar, Union
57 
58     from numpy.typing import ArrayLike
59 
60     try:
61         from dask.dataframe import DataFrame as DaskDataFrame
62     except ImportError:
63         DaskDataFrame = None  # type: ignore
64     try:
65         from dask.delayed import Delayed
66     except ImportError:
67         Delayed = None  # type: ignore
68     try:
69         from cdms2 import Variable as cdms2_Variable
70     except ImportError:
71         cdms2_Variable = None
72     try:
73         from iris.cube import Cube as iris_Cube
74     except ImportError:
75         iris_Cube = None
76 
77     from xarray.backends import ZarrStore
78     from xarray.backends.api import T_NetcdfEngine, T_NetcdfTypes
79     from xarray.core.groupby import DataArrayGroupBy
80     from xarray.core.resample import DataArrayResample
81     from xarray.core.rolling import DataArrayCoarsen, DataArrayRolling
82     from xarray.core.types import (
83         CoarsenBoundaryOptions,
84         DatetimeLike,
85         DatetimeUnitOptions,
86         Dims,
87         ErrorOptions,
88         ErrorOptionsWithWarn,
89         InterpOptions,
90         PadModeOptions,
91         PadReflectOptions,
92         QuantileMethods,
93         QueryEngineOptions,
94         QueryParserOptions,
95         ReindexMethodOptions,
96         SideOptions,
97         T_DataArray,
98         T_Xarray,
99     )
100     from xarray.core.weighted import DataArrayWeighted
101 
102     T_XarrayOther = TypeVar("T_XarrayOther", bound=Union["DataArray", Dataset])
103 
104 
105 def _infer_coords_and_dims(
106     shape, coords, dims
107 ) -> tuple[dict[Hashable, Variable], tuple[Hashable, ...]]:
108     """All the logic for creating a new DataArray"""
109 
110     if (
111         coords is not None
112         and not utils.is_dict_like(coords)
113         and len(coords) != len(shape)
114     ):
115         raise ValueError(
116             f"coords is not dict-like, but it has {len(coords)} items, "
117             f"which does not match the {len(shape)} dimensions of the "
118             "data"
119         )
120 
121     if isinstance(dims, str):
122         dims = (dims,)
123 
124     if dims is None:
125         dims = [f"dim_{n}" for n in range(len(shape))]
126         if coords is not None and len(coords) == len(shape):
127             # try to infer dimensions from coords
128             if utils.is_dict_like(coords):
129                 dims = list(coords.keys())
130             else:
131                 for n, (dim, coord) in enumerate(zip(dims, coords)):
132                     coord = as_variable(coord, name=dims[n]).to_index_variable()
133                     dims[n] = coord.name
134         dims = tuple(dims)
135     elif len(dims) != len(shape):
136         raise ValueError(
137             "different number of dimensions on data "
138             f"and dims: {len(shape)} vs {len(dims)}"
139         )
140     else:
141         for d in dims:
142             if not isinstance(d, str):
143                 raise TypeError(f"dimension {d} is not a string")
144 
145     new_coords: dict[Hashable, Variable] = {}
146 
147     if utils.is_dict_like(coords):
148         for k, v in coords.items():
149             new_coords[k] = as_variable(v, name=k)
150     elif coords is not None:
151         for dim, coord in zip(dims, coords):
152             var = as_variable(coord, name=dim)
153             var.dims = (dim,)
154             new_coords[dim] = var.to_index_variable()
155 
156     sizes = dict(zip(dims, shape))
157     for k, v in new_coords.items():
158         if any(d not in dims for d in v.dims):
159             raise ValueError(
160                 f"coordinate {k} has dimensions {v.dims}, but these "
161                 "are not a subset of the DataArray "
162                 f"dimensions {dims}"
163             )
164 
165         for d, s in zip(v.dims, v.shape):
166             if s != sizes[d]:
167                 raise ValueError(
168                     f"conflicting sizes for dimension {d!r}: "
169                     f"length {sizes[d]} on the data but length {s} on "
170                     f"coordinate {k!r}"
171                 )
172 
173         if k in sizes and v.shape != (sizes[k],):
174             raise ValueError(
175                 f"coordinate {k!r} is a DataArray dimension, but "
176                 f"it has shape {v.shape!r} rather than expected shape {sizes[k]!r} "
177                 "matching the dimension size"
178             )
179 
180     return new_coords, dims
181 
182 
183 def _check_data_shape(data, coords, dims):
184     if data is dtypes.NA:
185         data = np.nan
186     if coords is not None and utils.is_scalar(data, include_0d=False):
187         if utils.is_dict_like(coords):
188             if dims is None:
189                 return data
190             else:
191                 data_shape = tuple(
192                     as_variable(coords[k], k).size if k in coords.keys() else 1
193                     for k in dims
194                 )
195         else:
196             data_shape = tuple(as_variable(coord, "foo").size for coord in coords)
197         data = np.full(data_shape, data)
198     return data
199 
200 
201 class _LocIndexer:
202     __slots__ = ("data_array",)
203 
204     def __init__(self, data_array: DataArray):
205         self.data_array = data_array
206 
207     def __getitem__(self, key) -> DataArray:
208         if not utils.is_dict_like(key):
209             # expand the indexer so we can handle Ellipsis
210             labels = indexing.expanded_indexer(key, self.data_array.ndim)
211             key = dict(zip(self.data_array.dims, labels))
212         return self.data_array.sel(key)
213 
214     def __setitem__(self, key, value) -> None:
215         if not utils.is_dict_like(key):
216             # expand the indexer so we can handle Ellipsis
217             labels = indexing.expanded_indexer(key, self.data_array.ndim)
218             key = dict(zip(self.data_array.dims, labels))
219 
220         dim_indexers = map_index_queries(self.data_array, key).dim_indexers
221         self.data_array[dim_indexers] = value
222 
223 
224 # Used as the key corresponding to a DataArray's variable when converting
225 # arbitrary DataArray objects to datasets
226 _THIS_ARRAY = ReprObject("<this-array>")
227 
228 
229 class DataArray(
230     AbstractArray,
231     DataWithCoords,
232     DataArrayArithmetic,
233     DataArrayAggregations,
234 ):
235     """N-dimensional array with labeled coordinates and dimensions.
236 
237     DataArray provides a wrapper around numpy ndarrays that uses
238     labeled dimensions and coordinates to support metadata aware
239     operations. The API is similar to that for the pandas Series or
240     DataFrame, but DataArray objects can have any number of dimensions,
241     and their contents have fixed data types.
242 
243     Additional features over raw numpy arrays:
244 
245     - Apply operations over dimensions by name: ``x.sum('time')``.
246     - Select or assign values by integer location (like numpy):
247       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or
248       ``x.sel(time='2014-01-01')``.
249     - Mathematical operations (e.g., ``x - y``) vectorize across
250       multiple dimensions (known in numpy as "broadcasting") based on
251       dimension names, regardless of their original order.
252     - Keep track of arbitrary metadata in the form of a Python
253       dictionary: ``x.attrs``
254     - Convert to a pandas Series: ``x.to_series()``.
255 
256     Getting items from or doing mathematical operations with a
257     DataArray always returns another DataArray.
258 
259     Parameters
260     ----------
261     data : array_like
262         Values for this array. Must be an ``numpy.ndarray``, ndarray
263         like, or castable to an ``ndarray``. If a self-described xarray
264         or pandas object, attempts are made to use this array's
265         metadata to fill in other unspecified arguments. A view of the
266         array's data is used instead of a copy if possible.
267     coords : sequence or dict of array_like, optional
268         Coordinates (tick labels) to use for indexing along each
269         dimension. The following notations are accepted:
270 
271         - mapping {dimension name: array-like}
272         - sequence of tuples that are valid arguments for
273           ``xarray.Variable()``
274           - (dims, data)
275           - (dims, data, attrs)
276           - (dims, data, attrs, encoding)
277 
278         Additionally, it is possible to define a coord whose name
279         does not match the dimension name, or a coord based on multiple
280         dimensions, with one of the following notations:
281 
282         - mapping {coord name: DataArray}
283         - mapping {coord name: Variable}
284         - mapping {coord name: (dimension name, array-like)}
285         - mapping {coord name: (tuple of dimension names, array-like)}
286 
287     dims : Hashable or sequence of Hashable, optional
288         Name(s) of the data dimension(s). Must be either a Hashable
289         (only for 1D data) or a sequence of Hashables with length equal
290         to the number of dimensions. If this argument is omitted,
291         dimension names are taken from ``coords`` (if possible) and
292         otherwise default to ``['dim_0', ... 'dim_n']``.
293     name : str or None, optional
294         Name of this array.
295     attrs : dict_like or None, optional
296         Attributes to assign to the new instance. By default, an empty
297         attribute dictionary is initialized.
298 
299     Examples
300     --------
301     Create data:
302 
303     >>> np.random.seed(0)
304     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
305     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
306     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
307     >>> time = pd.date_range("2014-09-06", periods=3)
308     >>> reference_time = pd.Timestamp("2014-09-05")
309 
310     Initialize a dataarray with multiple dimensions:
311 
312     >>> da = xr.DataArray(
313     ...     data=temperature,
314     ...     dims=["x", "y", "time"],
315     ...     coords=dict(
316     ...         lon=(["x", "y"], lon),
317     ...         lat=(["x", "y"], lat),
318     ...         time=time,
319     ...         reference_time=reference_time,
320     ...     ),
321     ...     attrs=dict(
322     ...         description="Ambient temperature.",
323     ...         units="degC",
324     ...     ),
325     ... )
326     >>> da
327     <xarray.DataArray (x: 2, y: 2, time: 3)>
328     array([[[29.11241877, 18.20125767, 22.82990387],
329             [32.92714559, 29.94046392,  7.18177696]],
330     <BLANKLINE>
331            [[22.60070734, 13.78914233, 14.17424919],
332             [18.28478802, 16.15234857, 26.63418806]]])
333     Coordinates:
334         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
335         lat             (x, y) float64 42.25 42.21 42.63 42.59
336       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
337         reference_time  datetime64[ns] 2014-09-05
338     Dimensions without coordinates: x, y
339     Attributes:
340         description:  Ambient temperature.
341         units:        degC
342 
343     Find out where the coldest temperature was:
344 
345     >>> da.isel(da.argmin(...))
346     <xarray.DataArray ()>
347     array(7.18177696)
348     Coordinates:
349         lon             float64 -99.32
350         lat             float64 42.21
351         time            datetime64[ns] 2014-09-08
352         reference_time  datetime64[ns] 2014-09-05
353     Attributes:
354         description:  Ambient temperature.
355         units:        degC
356     """
357 
358     _cache: dict[str, Any]
359     _coords: dict[Any, Variable]
360     _close: Callable[[], None] | None
361     _indexes: dict[Hashable, Index]
362     _name: Hashable | None
363     _variable: Variable
364 
365     __slots__ = (
366         "_cache",
367         "_coords",
368         "_close",
369         "_indexes",
370         "_name",
371         "_variable",
372         "__weakref__",
373     )
374 
375     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor["DataArray"])
376 
377     def __init__(
378         self,
379         data: Any = dtypes.NA,
380         coords: Sequence[Sequence[Any] | pd.Index | DataArray]
381         | Mapping[Any, Any]
382         | None = None,
383         dims: Hashable | Sequence[Hashable] | None = None,
384         name: Hashable | None = None,
385         attrs: Mapping | None = None,
386         # internal parameters
387         indexes: dict[Hashable, Index] | None = None,
388         fastpath: bool = False,
389     ) -> None:
390         if fastpath:
391             variable = data
392             assert dims is None
393             assert attrs is None
394             assert indexes is not None
395         else:
396             # TODO: (benbovy - explicit indexes) remove
397             # once it becomes part of the public interface
398             if indexes is not None:
399                 raise ValueError("Providing explicit indexes is not supported yet")
400 
401             # try to fill in arguments from data if they weren't supplied
402             if coords is None:
403                 if isinstance(data, DataArray):
404                     coords = data.coords
405                 elif isinstance(data, pd.Series):
406                     coords = [data.index]
407                 elif isinstance(data, pd.DataFrame):
408                     coords = [data.index, data.columns]
409                 elif isinstance(data, (pd.Index, IndexVariable)):
410                     coords = [data]
411 
412             if dims is None:
413                 dims = getattr(data, "dims", getattr(coords, "dims", None))
414             if name is None:
415                 name = getattr(data, "name", None)
416             if attrs is None and not isinstance(data, PANDAS_TYPES):
417                 attrs = getattr(data, "attrs", None)
418 
419             data = _check_data_shape(data, coords, dims)
420             data = as_compatible_data(data)
421             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
422             variable = Variable(dims, data, attrs, fastpath=True)
423             indexes, coords = _create_indexes_from_coords(coords)
424 
425         # These fully describe a DataArray
426         self._variable = variable
427         assert isinstance(coords, dict)
428         self._coords = coords
429         self._name = name
430 
431         # TODO(shoyer): document this argument, once it becomes part of the
432         # public interface.
433         self._indexes = indexes
434 
435         self._close = None
436 
437     @classmethod
438     def _construct_direct(
439         cls: type[T_DataArray],
440         variable: Variable,
441         coords: dict[Any, Variable],
442         name: Hashable,
443         indexes: dict[Hashable, Index],
444     ) -> T_DataArray:
445         """Shortcut around __init__ for internal use when we want to skip
446         costly validation
447         """
448         obj = object.__new__(cls)
449         obj._variable = variable
450         obj._coords = coords
451         obj._name = name
452         obj._indexes = indexes
453         obj._close = None
454         return obj
455 
456     def _replace(
457         self: T_DataArray,
458         variable: Variable | None = None,
459         coords=None,
460         name: Hashable | None | Default = _default,
461         indexes=None,
462     ) -> T_DataArray:
463         if variable is None:
464             variable = self.variable
465         if coords is None:
466             coords = self._coords
467         if indexes is None:
468             indexes = self._indexes
469         if name is _default:
470             name = self.name
471         return type(self)(variable, coords, name=name, indexes=indexes, fastpath=True)
472 
473     def _replace_maybe_drop_dims(
474         self: T_DataArray,
475         variable: Variable,
476         name: Hashable | None | Default = _default,
477     ) -> T_DataArray:
478         if variable.dims == self.dims and variable.shape == self.shape:
479             coords = self._coords.copy()
480             indexes = self._indexes
481         elif variable.dims == self.dims:
482             # Shape has changed (e.g. from reduce(..., keepdims=True)
483             new_sizes = dict(zip(self.dims, variable.shape))
484             coords = {
485                 k: v
486                 for k, v in self._coords.items()
487                 if v.shape == tuple(new_sizes[d] for d in v.dims)
488             }
489             indexes = filter_indexes_from_coords(self._indexes, set(coords))
490         else:
491             allowed_dims = set(variable.dims)
492             coords = {
493                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
494             }
495             indexes = filter_indexes_from_coords(self._indexes, set(coords))
496         return self._replace(variable, coords, name, indexes=indexes)
497 
498     def _overwrite_indexes(
499         self: T_DataArray,
500         indexes: Mapping[Any, Index],
501         coords: Mapping[Any, Variable] | None = None,
502         drop_coords: list[Hashable] | None = None,
503         rename_dims: Mapping[Any, Any] | None = None,
504     ) -> T_DataArray:
505         """Maybe replace indexes and their corresponding coordinates."""
506         if not indexes:
507             return self
508 
509         if coords is None:
510             coords = {}
511         if drop_coords is None:
512             drop_coords = []
513 
514         new_variable = self.variable.copy()
515         new_coords = self._coords.copy()
516         new_indexes = dict(self._indexes)
517 
518         for name in indexes:
519             new_coords[name] = coords[name]
520             new_indexes[name] = indexes[name]
521 
522         for name in drop_coords:
523             new_coords.pop(name)
524             new_indexes.pop(name)
525 
526         if rename_dims:
527             new_variable.dims = tuple(rename_dims.get(d, d) for d in new_variable.dims)
528 
529         return self._replace(
530             variable=new_variable, coords=new_coords, indexes=new_indexes
531         )
532 
533     def _to_temp_dataset(self) -> Dataset:
534         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
535 
536     def _from_temp_dataset(
537         self: T_DataArray, dataset: Dataset, name: Hashable | None | Default = _default
538     ) -> T_DataArray:
539         variable = dataset._variables.pop(_THIS_ARRAY)
540         coords = dataset._variables
541         indexes = dataset._indexes
542         return self._replace(variable, coords, name, indexes=indexes)
543 
544     def _to_dataset_split(self, dim: Hashable) -> Dataset:
545         """splits dataarray along dimension 'dim'"""
546 
547         def subset(dim, label):
548             array = self.loc[{dim: label}]
549             array.attrs = {}
550             return as_variable(array)
551 
552         variables = {label: subset(dim, label) for label in self.get_index(dim)}
553         variables.update({k: v for k, v in self._coords.items() if k != dim})
554         coord_names = set(self._coords) - {dim}
555         indexes = filter_indexes_from_coords(self._indexes, coord_names)
556         dataset = Dataset._construct_direct(
557             variables, coord_names, indexes=indexes, attrs=self.attrs
558         )
559         return dataset
560 
561     def _to_dataset_whole(
562         self, name: Hashable = None, shallow_copy: bool = True
563     ) -> Dataset:
564         if name is None:
565             name = self.name
566         if name is None:
567             raise ValueError(
568                 "unable to convert unnamed DataArray to a "
569                 "Dataset without providing an explicit name"
570             )
571         if name in self.coords:
572             raise ValueError(
573                 "cannot create a Dataset from a DataArray with "
574                 "the same name as one of its coordinates"
575             )
576         # use private APIs for speed: this is called by _to_temp_dataset(),
577         # which is used in the guts of a lot of operations (e.g., reindex)
578         variables = self._coords.copy()
579         variables[name] = self.variable
580         if shallow_copy:
581             for k in variables:
582                 variables[k] = variables[k].copy(deep=False)
583         indexes = self._indexes
584 
585         coord_names = set(self._coords)
586         return Dataset._construct_direct(variables, coord_names, indexes=indexes)
587 
588     def to_dataset(
589         self,
590         dim: Hashable = None,
591         *,
592         name: Hashable = None,
593         promote_attrs: bool = False,
594     ) -> Dataset:
595         """Convert a DataArray to a Dataset.
596 
597         Parameters
598         ----------
599         dim : Hashable, optional
600             Name of the dimension on this array along which to split this array
601             into separate variables. If not provided, this array is converted
602             into a Dataset of one variable.
603         name : Hashable, optional
604             Name to substitute for this array's name. Only valid if ``dim`` is
605             not provided.
606         promote_attrs : bool, default: False
607             Set to True to shallow copy attrs of DataArray to returned Dataset.
608 
609         Returns
610         -------
611         dataset : Dataset
612         """
613         if dim is not None and dim not in self.dims:
614             raise TypeError(
615                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
616             )
617 
618         if dim is not None:
619             if name is not None:
620                 raise TypeError("cannot supply both dim and name arguments")
621             result = self._to_dataset_split(dim)
622         else:
623             result = self._to_dataset_whole(name)
624 
625         if promote_attrs:
626             result.attrs = dict(self.attrs)
627 
628         return result
629 
630     @property
631     def name(self) -> Hashable | None:
632         """The name of this array."""
633         return self._name
634 
635     @name.setter
636     def name(self, value: Hashable | None) -> None:
637         self._name = value
638 
639     @property
640     def variable(self) -> Variable:
641         """Low level interface to the Variable object for this DataArray."""
642         return self._variable
643 
644     @property
645     def dtype(self) -> np.dtype:
646         """
647         Data-type of the array’s elements.
648 
649         See Also
650         --------
651         ndarray.dtype
652         numpy.dtype
653         """
654         return self.variable.dtype
655 
656     @property
657     def shape(self) -> tuple[int, ...]:
658         """
659         Tuple of array dimensions.
660 
661         See Also
662         --------
663         numpy.ndarray.shape
664         """
665         return self.variable.shape
666 
667     @property
668     def size(self) -> int:
669         """
670         Number of elements in the array.
671 
672         Equal to ``np.prod(a.shape)``, i.e., the product of the array’s dimensions.
673 
674         See Also
675         --------
676         numpy.ndarray.size
677         """
678         return self.variable.size
679 
680     @property
681     def nbytes(self) -> int:
682         """
683         Total bytes consumed by the elements of this DataArray's data.
684 
685         If the underlying data array does not include ``nbytes``, estimates
686         the bytes consumed based on the ``size`` and ``dtype``.
687         """
688         return self.variable.nbytes
689 
690     @property
691     def ndim(self) -> int:
692         """
693         Number of array dimensions.
694 
695         See Also
696         --------
697         numpy.ndarray.ndim
698         """
699         return self.variable.ndim
700 
701     def __len__(self) -> int:
702         return len(self.variable)
703 
704     @property
705     def data(self) -> Any:
706         """
707         The DataArray's data as an array. The underlying array type
708         (e.g. dask, sparse, pint) is preserved.
709 
710         See Also
711         --------
712         DataArray.to_numpy
713         DataArray.as_numpy
714         DataArray.values
715         """
716         return self.variable.data
717 
718     @data.setter
719     def data(self, value: Any) -> None:
720         self.variable.data = value
721 
722     @property
723     def values(self) -> np.ndarray:
724         """
725         The array's data as a numpy.ndarray.
726 
727         If the array's data is not a numpy.ndarray this will attempt to convert
728         it naively using np.array(), which will raise an error if the array
729         type does not support coercion like this (e.g. cupy).
730         """
731         return self.variable.values
732 
733     @values.setter
734     def values(self, value: Any) -> None:
735         self.variable.values = value
736 
737     def to_numpy(self) -> np.ndarray:
738         """
739         Coerces wrapped data to numpy and returns a numpy.ndarray.
740 
741         See Also
742         --------
743         DataArray.as_numpy : Same but returns the surrounding DataArray instead.
744         Dataset.as_numpy
745         DataArray.values
746         DataArray.data
747         """
748         return self.variable.to_numpy()
749 
750     def as_numpy(self: T_DataArray) -> T_DataArray:
751         """
752         Coerces wrapped data and coordinates into numpy arrays, returning a DataArray.
753 
754         See Also
755         --------
756         DataArray.to_numpy : Same but returns only the data as a numpy.ndarray object.
757         Dataset.as_numpy : Converts all variables in a Dataset.
758         DataArray.values
759         DataArray.data
760         """
761         coords = {k: v.as_numpy() for k, v in self._coords.items()}
762         return self._replace(self.variable.as_numpy(), coords, indexes=self._indexes)
763 
764     @property
765     def _in_memory(self) -> bool:
766         return self.variable._in_memory
767 
768     def _to_index(self) -> pd.Index:
769         return self.variable._to_index()
770 
771     def to_index(self) -> pd.Index:
772         """Convert this variable to a pandas.Index. Only possible for 1D
773         arrays.
774         """
775         return self.variable.to_index()
776 
777     @property
778     def dims(self) -> tuple[Hashable, ...]:
779         """Tuple of dimension names associated with this array.
780 
781         Note that the type of this property is inconsistent with
782         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
783         consistently named properties.
784 
785         See Also
786         --------
787         DataArray.sizes
788         Dataset.dims
789         """
790         return self.variable.dims
791 
792     @dims.setter
793     def dims(self, value: Any) -> NoReturn:
794         raise AttributeError(
795             "you cannot assign dims on a DataArray. Use "
796             ".rename() or .swap_dims() instead."
797         )
798 
799     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
800         if utils.is_dict_like(key):
801             return key
802         key = indexing.expanded_indexer(key, self.ndim)
803         return dict(zip(self.dims, key))
804 
805     def _getitem_coord(self: T_DataArray, key: Any) -> T_DataArray:
806         from xarray.core.dataset import _get_virtual_variable
807 
808         try:
809             var = self._coords[key]
810         except KeyError:
811             dim_sizes = dict(zip(self.dims, self.shape))
812             _, key, var = _get_virtual_variable(self._coords, key, dim_sizes)
813 
814         return self._replace_maybe_drop_dims(var, name=key)
815 
816     def __getitem__(self: T_DataArray, key: Any) -> T_DataArray:
817         if isinstance(key, str):
818             return self._getitem_coord(key)
819         else:
820             # xarray-style array indexing
821             return self.isel(indexers=self._item_key_to_dict(key))
822 
823     def __setitem__(self, key: Any, value: Any) -> None:
824         if isinstance(key, str):
825             self.coords[key] = value
826         else:
827             # Coordinates in key, value and self[key] should be consistent.
828             # TODO Coordinate consistency in key is checked here, but it
829             # causes unnecessary indexing. It should be optimized.
830             obj = self[key]
831             if isinstance(value, DataArray):
832                 assert_coordinate_consistent(value, obj.coords.variables)
833             # DataArray key -> Variable key
834             key = {
835                 k: v.variable if isinstance(v, DataArray) else v
836                 for k, v in self._item_key_to_dict(key).items()
837             }
838             self.variable[key] = value
839 
840     def __delitem__(self, key: Any) -> None:
841         del self.coords[key]
842 
843     @property
844     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
845         """Places to look-up items for attribute-style access"""
846         yield from self._item_sources
847         yield self.attrs
848 
849     @property
850     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
851         """Places to look-up items for key-completion"""
852         yield HybridMappingProxy(keys=self._coords, mapping=self.coords)
853 
854         # virtual coordinates
855         # uses empty dict -- everything here can already be found in self.coords.
856         yield HybridMappingProxy(keys=self.dims, mapping={})
857 
858     def __contains__(self, key: Any) -> bool:
859         return key in self.data
860 
861     @property
862     def loc(self) -> _LocIndexer:
863         """Attribute for location based indexing like pandas."""
864         return _LocIndexer(self)
865 
866     @property
867     def attrs(self) -> dict[Any, Any]:
868         """Dictionary storing arbitrary metadata with this array."""
869         return self.variable.attrs
870 
871     @attrs.setter
872     def attrs(self, value: Mapping[Any, Any]) -> None:
873         self.variable.attrs = dict(value)
874 
875     @property
876     def encoding(self) -> dict[Any, Any]:
877         """Dictionary of format-specific settings for how this array should be
878         serialized."""
879         return self.variable.encoding
880 
881     @encoding.setter
882     def encoding(self, value: Mapping[Any, Any]) -> None:
883         self.variable.encoding = dict(value)
884 
885     def reset_encoding(self: T_DataArray) -> T_DataArray:
886         """Return a new DataArray without encoding on the array or any attached
887         coords."""
888         ds = self._to_temp_dataset().reset_encoding()
889         return self._from_temp_dataset(ds)
890 
891     @property
892     def indexes(self) -> Indexes:
893         """Mapping of pandas.Index objects used for label based indexing.
894 
895         Raises an error if this Dataset has indexes that cannot be coerced
896         to pandas.Index objects.
897 
898         See Also
899         --------
900         DataArray.xindexes
901 
902         """
903         return self.xindexes.to_pandas_indexes()
904 
905     @property
906     def xindexes(self) -> Indexes:
907         """Mapping of xarray Index objects used for label based indexing."""
908         return Indexes(self._indexes, {k: self._coords[k] for k in self._indexes})
909 
910     @property
911     def coords(self) -> DataArrayCoordinates:
912         """Dictionary-like container of coordinate arrays."""
913         return DataArrayCoordinates(self)
914 
915     @overload
916     def reset_coords(
917         self: T_DataArray,
918         names: Dims = None,
919         drop: Literal[False] = False,
920     ) -> Dataset:
921         ...
922 
923     @overload
924     def reset_coords(
925         self: T_DataArray,
926         names: Dims = None,
927         *,
928         drop: Literal[True],
929     ) -> T_DataArray:
930         ...
931 
932     def reset_coords(
933         self: T_DataArray,
934         names: Dims = None,
935         drop: bool = False,
936     ) -> T_DataArray | Dataset:
937         """Given names of coordinates, reset them to become variables.
938 
939         Parameters
940         ----------
941         names : str, Iterable of Hashable or None, optional
942             Name(s) of non-index coordinates in this dataset to reset into
943             variables. By default, all non-index coordinates are reset.
944         drop : bool, default: False
945             If True, remove coordinates instead of converting them into
946             variables.
947 
948         Returns
949         -------
950         Dataset, or DataArray if ``drop == True``
951 
952         Examples
953         --------
954         >>> temperature = np.arange(25).reshape(5, 5)
955         >>> pressure = np.arange(50, 75).reshape(5, 5)
956         >>> da = xr.DataArray(
957         ...     data=temperature,
958         ...     dims=["x", "y"],
959         ...     coords=dict(
960         ...         lon=("x", np.arange(10, 15)),
961         ...         lat=("y", np.arange(20, 25)),
962         ...         Pressure=(["x", "y"], pressure),
963         ...     ),
964         ...     name="Temperature",
965         ... )
966         >>> da
967         <xarray.DataArray 'Temperature' (x: 5, y: 5)>
968         array([[ 0,  1,  2,  3,  4],
969                [ 5,  6,  7,  8,  9],
970                [10, 11, 12, 13, 14],
971                [15, 16, 17, 18, 19],
972                [20, 21, 22, 23, 24]])
973         Coordinates:
974             lon       (x) int64 10 11 12 13 14
975             lat       (y) int64 20 21 22 23 24
976             Pressure  (x, y) int64 50 51 52 53 54 55 56 57 ... 67 68 69 70 71 72 73 74
977         Dimensions without coordinates: x, y
978 
979         Return Dataset with target coordinate as a data variable rather than a coordinate variable:
980 
981         >>> da.reset_coords(names="Pressure")
982         <xarray.Dataset>
983         Dimensions:      (x: 5, y: 5)
984         Coordinates:
985             lon          (x) int64 10 11 12 13 14
986             lat          (y) int64 20 21 22 23 24
987         Dimensions without coordinates: x, y
988         Data variables:
989             Pressure     (x, y) int64 50 51 52 53 54 55 56 57 ... 68 69 70 71 72 73 74
990             Temperature  (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24
991 
992         Return DataArray without targeted coordinate:
993 
994         >>> da.reset_coords(names="Pressure", drop=True)
995         <xarray.DataArray 'Temperature' (x: 5, y: 5)>
996         array([[ 0,  1,  2,  3,  4],
997                [ 5,  6,  7,  8,  9],
998                [10, 11, 12, 13, 14],
999                [15, 16, 17, 18, 19],
1000                [20, 21, 22, 23, 24]])
1001         Coordinates:
1002             lon      (x) int64 10 11 12 13 14
1003             lat      (y) int64 20 21 22 23 24
1004         Dimensions without coordinates: x, y
1005         """
1006         if names is None:
1007             names = set(self.coords) - set(self._indexes)
1008         dataset = self.coords.to_dataset().reset_coords(names, drop)
1009         if drop:
1010             return self._replace(coords=dataset._variables)
1011         if self.name is None:
1012             raise ValueError(
1013                 "cannot reset_coords with drop=False on an unnamed DataArrray"
1014             )
1015         dataset[self.name] = self.variable
1016         return dataset
1017 
1018     def __dask_tokenize__(self):
1019         from dask.base import normalize_token
1020 
1021         return normalize_token((type(self), self._variable, self._coords, self._name))
1022 
1023     def __dask_graph__(self):
1024         return self._to_temp_dataset().__dask_graph__()
1025 
1026     def __dask_keys__(self):
1027         return self._to_temp_dataset().__dask_keys__()
1028 
1029     def __dask_layers__(self):
1030         return self._to_temp_dataset().__dask_layers__()
1031 
1032     @property
1033     def __dask_optimize__(self):
1034         return self._to_temp_dataset().__dask_optimize__
1035 
1036     @property
1037     def __dask_scheduler__(self):
1038         return self._to_temp_dataset().__dask_scheduler__
1039 
1040     def __dask_postcompute__(self):
1041         func, args = self._to_temp_dataset().__dask_postcompute__()
1042         return self._dask_finalize, (self.name, func) + args
1043 
1044     def __dask_postpersist__(self):
1045         func, args = self._to_temp_dataset().__dask_postpersist__()
1046         return self._dask_finalize, (self.name, func) + args
1047 
1048     @staticmethod
1049     def _dask_finalize(results, name, func, *args, **kwargs) -> DataArray:
1050         ds = func(results, *args, **kwargs)
1051         variable = ds._variables.pop(_THIS_ARRAY)
1052         coords = ds._variables
1053         indexes = ds._indexes
1054         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1055 
1056     def load(self: T_DataArray, **kwargs) -> T_DataArray:
1057         """Manually trigger loading of this array's data from disk or a
1058         remote source into memory and return this array.
1059 
1060         Normally, it should not be necessary to call this method in user code,
1061         because all xarray functions should either work on deferred data or
1062         load data automatically. However, this method can be necessary when
1063         working with many file objects on disk.
1064 
1065         Parameters
1066         ----------
1067         **kwargs : dict
1068             Additional keyword arguments passed on to ``dask.compute``.
1069 
1070         See Also
1071         --------
1072         dask.compute
1073         """
1074         ds = self._to_temp_dataset().load(**kwargs)
1075         new = self._from_temp_dataset(ds)
1076         self._variable = new._variable
1077         self._coords = new._coords
1078         return self
1079 
1080     def compute(self: T_DataArray, **kwargs) -> T_DataArray:
1081         """Manually trigger loading of this array's data from disk or a
1082         remote source into memory and return a new array. The original is
1083         left unaltered.
1084 
1085         Normally, it should not be necessary to call this method in user code,
1086         because all xarray functions should either work on deferred data or
1087         load data automatically. However, this method can be necessary when
1088         working with many file objects on disk.
1089 
1090         Parameters
1091         ----------
1092         **kwargs : dict
1093             Additional keyword arguments passed on to ``dask.compute``.
1094 
1095         See Also
1096         --------
1097         dask.compute
1098         """
1099         new = self.copy(deep=False)
1100         return new.load(**kwargs)
1101 
1102     def persist(self: T_DataArray, **kwargs) -> T_DataArray:
1103         """Trigger computation in constituent dask arrays
1104 
1105         This keeps them as dask arrays but encourages them to keep data in
1106         memory.  This is particularly useful when on a distributed machine.
1107         When on a single machine consider using ``.compute()`` instead.
1108 
1109         Parameters
1110         ----------
1111         **kwargs : dict
1112             Additional keyword arguments passed on to ``dask.persist``.
1113 
1114         See Also
1115         --------
1116         dask.persist
1117         """
1118         ds = self._to_temp_dataset().persist(**kwargs)
1119         return self._from_temp_dataset(ds)
1120 
1121     def copy(self: T_DataArray, deep: bool = True, data: Any = None) -> T_DataArray:
1122         """Returns a copy of this array.
1123 
1124         If `deep=True`, a deep copy is made of the data array.
1125         Otherwise, a shallow copy is made, and the returned data array's
1126         values are a new view of this data array's values.
1127 
1128         Use `data` to create a new object with the same structure as
1129         original but entirely new data.
1130 
1131         Parameters
1132         ----------
1133         deep : bool, optional
1134             Whether the data array and its coordinates are loaded into memory
1135             and copied onto the new object. Default is True.
1136         data : array_like, optional
1137             Data to use in the new object. Must have same shape as original.
1138             When `data` is used, `deep` is ignored for all data variables,
1139             and only used for coords.
1140 
1141         Returns
1142         -------
1143         copy : DataArray
1144             New object with dimensions, attributes, coordinates, name,
1145             encoding, and optionally data copied from original.
1146 
1147         Examples
1148         --------
1149         Shallow versus deep copy
1150 
1151         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
1152         >>> array.copy()
1153         <xarray.DataArray (x: 3)>
1154         array([1, 2, 3])
1155         Coordinates:
1156           * x        (x) <U1 'a' 'b' 'c'
1157         >>> array_0 = array.copy(deep=False)
1158         >>> array_0[0] = 7
1159         >>> array_0
1160         <xarray.DataArray (x: 3)>
1161         array([7, 2, 3])
1162         Coordinates:
1163           * x        (x) <U1 'a' 'b' 'c'
1164         >>> array
1165         <xarray.DataArray (x: 3)>
1166         array([7, 2, 3])
1167         Coordinates:
1168           * x        (x) <U1 'a' 'b' 'c'
1169 
1170         Changing the data using the ``data`` argument maintains the
1171         structure of the original object, but with the new data. Original
1172         object is unaffected.
1173 
1174         >>> array.copy(data=[0.1, 0.2, 0.3])
1175         <xarray.DataArray (x: 3)>
1176         array([0.1, 0.2, 0.3])
1177         Coordinates:
1178           * x        (x) <U1 'a' 'b' 'c'
1179         >>> array
1180         <xarray.DataArray (x: 3)>
1181         array([7, 2, 3])
1182         Coordinates:
1183           * x        (x) <U1 'a' 'b' 'c'
1184 
1185         See Also
1186         --------
1187         pandas.DataFrame.copy
1188         """
1189         return self._copy(deep=deep, data=data)
1190 
1191     def _copy(
1192         self: T_DataArray,
1193         deep: bool = True,
1194         data: Any = None,
1195         memo: dict[int, Any] | None = None,
1196     ) -> T_DataArray:
1197         variable = self.variable._copy(deep=deep, data=data, memo=memo)
1198         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1199 
1200         coords = {}
1201         for k, v in self._coords.items():
1202             if k in index_vars:
1203                 coords[k] = index_vars[k]
1204             else:
1205                 coords[k] = v._copy(deep=deep, memo=memo)
1206 
1207         return self._replace(variable, coords, indexes=indexes)
1208 
1209     def __copy__(self: T_DataArray) -> T_DataArray:
1210         return self._copy(deep=False)
1211 
1212     def __deepcopy__(
1213         self: T_DataArray, memo: dict[int, Any] | None = None
1214     ) -> T_DataArray:
1215         return self._copy(deep=True, memo=memo)
1216 
1217     # mutable objects should not be Hashable
1218     # https://github.com/python/mypy/issues/4266
1219     __hash__ = None  # type: ignore[assignment]
1220 
1221     @property
1222     def chunks(self) -> tuple[tuple[int, ...], ...] | None:
1223         """
1224         Tuple of block lengths for this dataarray's data, in order of dimensions, or None if
1225         the underlying data is not a dask array.
1226 
1227         See Also
1228         --------
1229         DataArray.chunk
1230         DataArray.chunksizes
1231         xarray.unify_chunks
1232         """
1233         return self.variable.chunks
1234 
1235     @property
1236     def chunksizes(self) -> Mapping[Any, tuple[int, ...]]:
1237         """
1238         Mapping from dimension names to block lengths for this dataarray's data, or None if
1239         the underlying data is not a dask array.
1240         Cannot be modified directly, but can be modified by calling .chunk().
1241 
1242         Differs from DataArray.chunks because it returns a mapping of dimensions to chunk shapes
1243         instead of a tuple of chunk shapes.
1244 
1245         See Also
1246         --------
1247         DataArray.chunk
1248         DataArray.chunks
1249         xarray.unify_chunks
1250         """
1251         all_variables = [self.variable] + [c.variable for c in self.coords.values()]
1252         return get_chunksizes(all_variables)
1253 
1254     def chunk(
1255         self: T_DataArray,
1256         chunks: (
1257             int
1258             | Literal["auto"]
1259             | tuple[int, ...]
1260             | tuple[tuple[int, ...], ...]
1261             | Mapping[Any, None | int | tuple[int, ...]]
1262         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1263         name_prefix: str = "xarray-",
1264         token: str | None = None,
1265         lock: bool = False,
1266         inline_array: bool = False,
1267         **chunks_kwargs: Any,
1268     ) -> T_DataArray:
1269         """Coerce this array's data into a dask arrays with the given chunks.
1270 
1271         If this variable is a non-dask array, it will be converted to dask
1272         array. If it's a dask array, it will be rechunked to the given chunk
1273         sizes.
1274 
1275         If neither chunks is not provided for one or more dimensions, chunk
1276         sizes along that dimension will not be updated; non-dask arrays will be
1277         converted into dask arrays with a single block.
1278 
1279         Parameters
1280         ----------
1281         chunks : int, "auto", tuple of int or mapping of Hashable to int, optional
1282             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, ``(5, 5)`` or
1283             ``{"x": 5, "y": 5}``.
1284         name_prefix : str, optional
1285             Prefix for the name of the new dask array.
1286         token : str, optional
1287             Token uniquely identifying this array.
1288         lock : optional
1289             Passed on to :py:func:`dask.array.from_array`, if the array is not
1290             already as dask array.
1291         inline_array: optional
1292             Passed on to :py:func:`dask.array.from_array`, if the array is not
1293             already as dask array.
1294         **chunks_kwargs : {dim: chunks, ...}, optional
1295             The keyword arguments form of ``chunks``.
1296             One of chunks or chunks_kwargs must be provided.
1297 
1298         Returns
1299         -------
1300         chunked : xarray.DataArray
1301 
1302         See Also
1303         --------
1304         DataArray.chunks
1305         DataArray.chunksizes
1306         xarray.unify_chunks
1307         dask.array.from_array
1308         """
1309         if chunks is None:
1310             warnings.warn(
1311                 "None value for 'chunks' is deprecated. "
1312                 "It will raise an error in the future. Use instead '{}'",
1313                 category=FutureWarning,
1314             )
1315             chunks = {}
1316 
1317         if isinstance(chunks, (float, str, int)):
1318             # ignoring type; unclear why it won't accept a Literal into the value.
1319             chunks = dict.fromkeys(self.dims, chunks)  # type: ignore
1320         elif isinstance(chunks, (tuple, list)):
1321             chunks = dict(zip(self.dims, chunks))
1322         else:
1323             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
1324 
1325         ds = self._to_temp_dataset().chunk(
1326             chunks,
1327             name_prefix=name_prefix,
1328             token=token,
1329             lock=lock,
1330             inline_array=inline_array,
1331         )
1332         return self._from_temp_dataset(ds)
1333 
1334     def isel(
1335         self: T_DataArray,
1336         indexers: Mapping[Any, Any] | None = None,
1337         drop: bool = False,
1338         missing_dims: ErrorOptionsWithWarn = "raise",
1339         **indexers_kwargs: Any,
1340     ) -> T_DataArray:
1341         """Return a new DataArray whose data is given by selecting indexes
1342         along the specified dimension(s).
1343 
1344         Parameters
1345         ----------
1346         indexers : dict, optional
1347             A dict with keys matching dimensions and values given
1348             by integers, slice objects or arrays.
1349             indexer can be a integer, slice, array-like or DataArray.
1350             If DataArrays are passed as indexers, xarray-style indexing will be
1351             carried out. See :ref:`indexing` for the details.
1352             One of indexers or indexers_kwargs must be provided.
1353         drop : bool, default: False
1354             If ``drop=True``, drop coordinates variables indexed by integers
1355             instead of making them scalar.
1356         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1357             What to do if dimensions that should be selected from are not present in the
1358             DataArray:
1359             - "raise": raise an exception
1360             - "warn": raise a warning, and ignore the missing dimensions
1361             - "ignore": ignore the missing dimensions
1362         **indexers_kwargs : {dim: indexer, ...}, optional
1363             The keyword arguments form of ``indexers``.
1364 
1365         Returns
1366         -------
1367         indexed : xarray.DataArray
1368 
1369         See Also
1370         --------
1371         Dataset.isel
1372         DataArray.sel
1373 
1374         Examples
1375         --------
1376         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("x", "y"))
1377         >>> da
1378         <xarray.DataArray (x: 5, y: 5)>
1379         array([[ 0,  1,  2,  3,  4],
1380                [ 5,  6,  7,  8,  9],
1381                [10, 11, 12, 13, 14],
1382                [15, 16, 17, 18, 19],
1383                [20, 21, 22, 23, 24]])
1384         Dimensions without coordinates: x, y
1385 
1386         >>> tgt_x = xr.DataArray(np.arange(0, 5), dims="points")
1387         >>> tgt_y = xr.DataArray(np.arange(0, 5), dims="points")
1388         >>> da = da.isel(x=tgt_x, y=tgt_y)
1389         >>> da
1390         <xarray.DataArray (points: 5)>
1391         array([ 0,  6, 12, 18, 24])
1392         Dimensions without coordinates: points
1393         """
1394 
1395         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1396 
1397         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1398             ds = self._to_temp_dataset()._isel_fancy(
1399                 indexers, drop=drop, missing_dims=missing_dims
1400             )
1401             return self._from_temp_dataset(ds)
1402 
1403         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1404         # lists, or zero or one-dimensional np.ndarray's
1405 
1406         variable = self._variable.isel(indexers, missing_dims=missing_dims)
1407         indexes, index_variables = isel_indexes(self.xindexes, indexers)
1408 
1409         coords = {}
1410         for coord_name, coord_value in self._coords.items():
1411             if coord_name in index_variables:
1412                 coord_value = index_variables[coord_name]
1413             else:
1414                 coord_indexers = {
1415                     k: v for k, v in indexers.items() if k in coord_value.dims
1416                 }
1417                 if coord_indexers:
1418                     coord_value = coord_value.isel(coord_indexers)
1419                     if drop and coord_value.ndim == 0:
1420                         continue
1421             coords[coord_name] = coord_value
1422 
1423         return self._replace(variable=variable, coords=coords, indexes=indexes)
1424 
1425     def sel(
1426         self: T_DataArray,
1427         indexers: Mapping[Any, Any] | None = None,
1428         method: str | None = None,
1429         tolerance=None,
1430         drop: bool = False,
1431         **indexers_kwargs: Any,
1432     ) -> T_DataArray:
1433         """Return a new DataArray whose data is given by selecting index
1434         labels along the specified dimension(s).
1435 
1436         In contrast to `DataArray.isel`, indexers for this method should use
1437         labels instead of integers.
1438 
1439         Under the hood, this method is powered by using pandas's powerful Index
1440         objects. This makes label based indexing essentially just as fast as
1441         using integer indexing.
1442 
1443         It also means this method uses pandas's (well documented) logic for
1444         indexing. This means you can use string shortcuts for datetime indexes
1445         (e.g., '2000-01' to select all values in January 2000). It also means
1446         that slices are treated as inclusive of both the start and stop values,
1447         unlike normal Python indexing.
1448 
1449         .. warning::
1450 
1451           Do not try to assign values when using any of the indexing methods
1452           ``isel`` or ``sel``::
1453 
1454             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1455             # DO NOT do this
1456             da.isel(x=[0, 1, 2])[1] = -1
1457 
1458           Assigning values with the chained indexing using ``.sel`` or
1459           ``.isel`` fails silently.
1460 
1461         Parameters
1462         ----------
1463         indexers : dict, optional
1464             A dict with keys matching dimensions and values given
1465             by scalars, slices or arrays of tick labels. For dimensions with
1466             multi-index, the indexer may also be a dict-like object with keys
1467             matching index level names.
1468             If DataArrays are passed as indexers, xarray-style indexing will be
1469             carried out. See :ref:`indexing` for the details.
1470             One of indexers or indexers_kwargs must be provided.
1471         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1472             Method to use for inexact matches:
1473 
1474             - None (default): only exact matches
1475             - pad / ffill: propagate last valid index value forward
1476             - backfill / bfill: propagate next valid index value backward
1477             - nearest: use nearest valid index value
1478 
1479         tolerance : optional
1480             Maximum distance between original and new labels for inexact
1481             matches. The values of the index at the matching locations must
1482             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1483         drop : bool, optional
1484             If ``drop=True``, drop coordinates variables in `indexers` instead
1485             of making them scalar.
1486         **indexers_kwargs : {dim: indexer, ...}, optional
1487             The keyword arguments form of ``indexers``.
1488             One of indexers or indexers_kwargs must be provided.
1489 
1490         Returns
1491         -------
1492         obj : DataArray
1493             A new DataArray with the same contents as this DataArray, except the
1494             data and each dimension is indexed by the appropriate indexers.
1495             If indexer DataArrays have coordinates that do not conflict with
1496             this object, then these coordinates will be attached.
1497             In general, each array's data will be a view of the array's data
1498             in this DataArray, unless vectorized indexing was triggered by using
1499             an array indexer, in which case the data will be a copy.
1500 
1501         See Also
1502         --------
1503         Dataset.sel
1504         DataArray.isel
1505 
1506         Examples
1507         --------
1508         >>> da = xr.DataArray(
1509         ...     np.arange(25).reshape(5, 5),
1510         ...     coords={"x": np.arange(5), "y": np.arange(5)},
1511         ...     dims=("x", "y"),
1512         ... )
1513         >>> da
1514         <xarray.DataArray (x: 5, y: 5)>
1515         array([[ 0,  1,  2,  3,  4],
1516                [ 5,  6,  7,  8,  9],
1517                [10, 11, 12, 13, 14],
1518                [15, 16, 17, 18, 19],
1519                [20, 21, 22, 23, 24]])
1520         Coordinates:
1521           * x        (x) int64 0 1 2 3 4
1522           * y        (y) int64 0 1 2 3 4
1523 
1524         >>> tgt_x = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1525         >>> tgt_y = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1526         >>> da = da.sel(x=tgt_x, y=tgt_y, method="nearest")
1527         >>> da
1528         <xarray.DataArray (points: 5)>
1529         array([ 0,  6, 12, 18, 24])
1530         Coordinates:
1531             x        (points) int64 0 1 2 3 4
1532             y        (points) int64 0 1 2 3 4
1533         Dimensions without coordinates: points
1534         """
1535         ds = self._to_temp_dataset().sel(
1536             indexers=indexers,
1537             drop=drop,
1538             method=method,
1539             tolerance=tolerance,
1540             **indexers_kwargs,
1541         )
1542         return self._from_temp_dataset(ds)
1543 
1544     def head(
1545         self: T_DataArray,
1546         indexers: Mapping[Any, int] | int | None = None,
1547         **indexers_kwargs: Any,
1548     ) -> T_DataArray:
1549         """Return a new DataArray whose data is given by the the first `n`
1550         values along the specified dimension(s). Default `n` = 5
1551 
1552         See Also
1553         --------
1554         Dataset.head
1555         DataArray.tail
1556         DataArray.thin
1557 
1558         Examples
1559         --------
1560         >>> da = xr.DataArray(
1561         ...     np.arange(25).reshape(5, 5),
1562         ...     dims=("x", "y"),
1563         ... )
1564         >>> da
1565         <xarray.DataArray (x: 5, y: 5)>
1566         array([[ 0,  1,  2,  3,  4],
1567                [ 5,  6,  7,  8,  9],
1568                [10, 11, 12, 13, 14],
1569                [15, 16, 17, 18, 19],
1570                [20, 21, 22, 23, 24]])
1571         Dimensions without coordinates: x, y
1572 
1573         >>> da.head(x=1)
1574         <xarray.DataArray (x: 1, y: 5)>
1575         array([[0, 1, 2, 3, 4]])
1576         Dimensions without coordinates: x, y
1577 
1578         >>> da.head({"x": 2, "y": 2})
1579         <xarray.DataArray (x: 2, y: 2)>
1580         array([[0, 1],
1581                [5, 6]])
1582         Dimensions without coordinates: x, y
1583         """
1584         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1585         return self._from_temp_dataset(ds)
1586 
1587     def tail(
1588         self: T_DataArray,
1589         indexers: Mapping[Any, int] | int | None = None,
1590         **indexers_kwargs: Any,
1591     ) -> T_DataArray:
1592         """Return a new DataArray whose data is given by the the last `n`
1593         values along the specified dimension(s). Default `n` = 5
1594 
1595         See Also
1596         --------
1597         Dataset.tail
1598         DataArray.head
1599         DataArray.thin
1600 
1601         Examples
1602         --------
1603         >>> da = xr.DataArray(
1604         ...     np.arange(25).reshape(5, 5),
1605         ...     dims=("x", "y"),
1606         ... )
1607         >>> da
1608         <xarray.DataArray (x: 5, y: 5)>
1609         array([[ 0,  1,  2,  3,  4],
1610                [ 5,  6,  7,  8,  9],
1611                [10, 11, 12, 13, 14],
1612                [15, 16, 17, 18, 19],
1613                [20, 21, 22, 23, 24]])
1614         Dimensions without coordinates: x, y
1615 
1616         >>> da.tail(y=1)
1617         <xarray.DataArray (x: 5, y: 1)>
1618         array([[ 4],
1619                [ 9],
1620                [14],
1621                [19],
1622                [24]])
1623         Dimensions without coordinates: x, y
1624 
1625         >>> da.tail({"x": 2, "y": 2})
1626         <xarray.DataArray (x: 2, y: 2)>
1627         array([[18, 19],
1628                [23, 24]])
1629         Dimensions without coordinates: x, y
1630         """
1631         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1632         return self._from_temp_dataset(ds)
1633 
1634     def thin(
1635         self: T_DataArray,
1636         indexers: Mapping[Any, int] | int | None = None,
1637         **indexers_kwargs: Any,
1638     ) -> T_DataArray:
1639         """Return a new DataArray whose data is given by each `n` value
1640         along the specified dimension(s).
1641 
1642         Examples
1643         --------
1644         >>> x_arr = np.arange(0, 26)
1645         >>> x_arr
1646         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
1647                17, 18, 19, 20, 21, 22, 23, 24, 25])
1648         >>> x = xr.DataArray(
1649         ...     np.reshape(x_arr, (2, 13)),
1650         ...     dims=("x", "y"),
1651         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
1652         ... )
1653         >>> x
1654         <xarray.DataArray (x: 2, y: 13)>
1655         array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],
1656                [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]])
1657         Coordinates:
1658           * x        (x) int64 0 1
1659           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
1660 
1661         >>>
1662         >>> x.thin(3)
1663         <xarray.DataArray (x: 1, y: 5)>
1664         array([[ 0,  3,  6,  9, 12]])
1665         Coordinates:
1666           * x        (x) int64 0
1667           * y        (y) int64 0 3 6 9 12
1668         >>> x.thin({"x": 2, "y": 5})
1669         <xarray.DataArray (x: 1, y: 3)>
1670         array([[ 0,  5, 10]])
1671         Coordinates:
1672           * x        (x) int64 0
1673           * y        (y) int64 0 5 10
1674 
1675         See Also
1676         --------
1677         Dataset.thin
1678         DataArray.head
1679         DataArray.tail
1680         """
1681         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1682         return self._from_temp_dataset(ds)
1683 
1684     def broadcast_like(
1685         self: T_DataArray,
1686         other: DataArray | Dataset,
1687         exclude: Iterable[Hashable] | None = None,
1688     ) -> T_DataArray:
1689         """Broadcast this DataArray against another Dataset or DataArray.
1690 
1691         This is equivalent to xr.broadcast(other, self)[1]
1692 
1693         xarray objects are broadcast against each other in arithmetic
1694         operations, so this method is not be necessary for most uses.
1695 
1696         If no change is needed, the input data is returned to the output
1697         without being copied.
1698 
1699         If new coords are added by the broadcast, their values are
1700         NaN filled.
1701 
1702         Parameters
1703         ----------
1704         other : Dataset or DataArray
1705             Object against which to broadcast this array.
1706         exclude : iterable of Hashable, optional
1707             Dimensions that must not be broadcasted
1708 
1709         Returns
1710         -------
1711         new_da : DataArray
1712             The caller broadcasted against ``other``.
1713 
1714         Examples
1715         --------
1716         >>> arr1 = xr.DataArray(
1717         ...     np.random.randn(2, 3),
1718         ...     dims=("x", "y"),
1719         ...     coords={"x": ["a", "b"], "y": ["a", "b", "c"]},
1720         ... )
1721         >>> arr2 = xr.DataArray(
1722         ...     np.random.randn(3, 2),
1723         ...     dims=("x", "y"),
1724         ...     coords={"x": ["a", "b", "c"], "y": ["a", "b"]},
1725         ... )
1726         >>> arr1
1727         <xarray.DataArray (x: 2, y: 3)>
1728         array([[ 1.76405235,  0.40015721,  0.97873798],
1729                [ 2.2408932 ,  1.86755799, -0.97727788]])
1730         Coordinates:
1731           * x        (x) <U1 'a' 'b'
1732           * y        (y) <U1 'a' 'b' 'c'
1733         >>> arr2
1734         <xarray.DataArray (x: 3, y: 2)>
1735         array([[ 0.95008842, -0.15135721],
1736                [-0.10321885,  0.4105985 ],
1737                [ 0.14404357,  1.45427351]])
1738         Coordinates:
1739           * x        (x) <U1 'a' 'b' 'c'
1740           * y        (y) <U1 'a' 'b'
1741         >>> arr1.broadcast_like(arr2)
1742         <xarray.DataArray (x: 3, y: 3)>
1743         array([[ 1.76405235,  0.40015721,  0.97873798],
1744                [ 2.2408932 ,  1.86755799, -0.97727788],
1745                [        nan,         nan,         nan]])
1746         Coordinates:
1747           * x        (x) <U1 'a' 'b' 'c'
1748           * y        (y) <U1 'a' 'b' 'c'
1749         """
1750         if exclude is None:
1751             exclude = set()
1752         else:
1753             exclude = set(exclude)
1754         args = align(other, self, join="outer", copy=False, exclude=exclude)
1755 
1756         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1757 
1758         return _broadcast_helper(
1759             cast("T_DataArray", args[1]), exclude, dims_map, common_coords
1760         )
1761 
1762     def _reindex_callback(
1763         self: T_DataArray,
1764         aligner: alignment.Aligner,
1765         dim_pos_indexers: dict[Hashable, Any],
1766         variables: dict[Hashable, Variable],
1767         indexes: dict[Hashable, Index],
1768         fill_value: Any,
1769         exclude_dims: frozenset[Hashable],
1770         exclude_vars: frozenset[Hashable],
1771     ) -> T_DataArray:
1772         """Callback called from ``Aligner`` to create a new reindexed DataArray."""
1773 
1774         if isinstance(fill_value, dict):
1775             fill_value = fill_value.copy()
1776             sentinel = object()
1777             value = fill_value.pop(self.name, sentinel)
1778             if value is not sentinel:
1779                 fill_value[_THIS_ARRAY] = value
1780 
1781         ds = self._to_temp_dataset()
1782         reindexed = ds._reindex_callback(
1783             aligner,
1784             dim_pos_indexers,
1785             variables,
1786             indexes,
1787             fill_value,
1788             exclude_dims,
1789             exclude_vars,
1790         )
1791         return self._from_temp_dataset(reindexed)
1792 
1793     def reindex_like(
1794         self: T_DataArray,
1795         other: DataArray | Dataset,
1796         method: ReindexMethodOptions = None,
1797         tolerance: int | float | Iterable[int | float] | None = None,
1798         copy: bool = True,
1799         fill_value=dtypes.NA,
1800     ) -> T_DataArray:
1801         """Conform this object onto the indexes of another object, filling in
1802         missing values with ``fill_value``. The default fill value is NaN.
1803 
1804         Parameters
1805         ----------
1806         other : Dataset or DataArray
1807             Object with an 'indexes' attribute giving a mapping from dimension
1808             names to pandas.Index objects, which provides coordinates upon
1809             which to index the variables in this dataset. The indexes on this
1810             other object need not be the same as the indexes on this
1811             dataset. Any mis-matched index values will be filled in with
1812             NaN, and any mis-matched dimension names will simply be ignored.
1813         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1814             Method to use for filling index values from other not found on this
1815             data array:
1816 
1817             - None (default): don't fill gaps
1818             - pad / ffill: propagate last valid index value forward
1819             - backfill / bfill: propagate next valid index value backward
1820             - nearest: use nearest valid index value
1821 
1822         tolerance : optional
1823             Maximum distance between original and new labels for inexact
1824             matches. The values of the index at the matching locations must
1825             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1826             Tolerance may be a scalar value, which applies the same tolerance
1827             to all values, or list-like, which applies variable tolerance per
1828             element. List-like must be the same size as the index and its dtype
1829             must exactly match the index’s type.
1830         copy : bool, default: True
1831             If ``copy=True``, data in the return value is always copied. If
1832             ``copy=False`` and reindexing is unnecessary, or can be performed
1833             with only slice operations, then the output may share memory with
1834             the input. In either case, a new xarray object is always returned.
1835         fill_value : scalar or dict-like, optional
1836             Value to use for newly missing values. If a dict-like, maps
1837             variable names (including coordinates) to fill values. Use this
1838             data array's name to refer to the data array's values.
1839 
1840         Returns
1841         -------
1842         reindexed : DataArray
1843             Another dataset array, with this array's data but coordinates from
1844             the other object.
1845 
1846         Examples
1847         --------
1848         >>> data = np.arange(12).reshape(4, 3)
1849         >>> da1 = xr.DataArray(
1850         ...     data=data,
1851         ...     dims=["x", "y"],
1852         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
1853         ... )
1854         >>> da1
1855         <xarray.DataArray (x: 4, y: 3)>
1856         array([[ 0,  1,  2],
1857                [ 3,  4,  5],
1858                [ 6,  7,  8],
1859                [ 9, 10, 11]])
1860         Coordinates:
1861           * x        (x) int64 10 20 30 40
1862           * y        (y) int64 70 80 90
1863         >>> da2 = xr.DataArray(
1864         ...     data=data,
1865         ...     dims=["x", "y"],
1866         ...     coords={"x": [40, 30, 20, 10], "y": [90, 80, 70]},
1867         ... )
1868         >>> da2
1869         <xarray.DataArray (x: 4, y: 3)>
1870         array([[ 0,  1,  2],
1871                [ 3,  4,  5],
1872                [ 6,  7,  8],
1873                [ 9, 10, 11]])
1874         Coordinates:
1875           * x        (x) int64 40 30 20 10
1876           * y        (y) int64 90 80 70
1877 
1878         Reindexing with both DataArrays having the same coordinates set, but in different order:
1879 
1880         >>> da1.reindex_like(da2)
1881         <xarray.DataArray (x: 4, y: 3)>
1882         array([[11, 10,  9],
1883                [ 8,  7,  6],
1884                [ 5,  4,  3],
1885                [ 2,  1,  0]])
1886         Coordinates:
1887           * x        (x) int64 40 30 20 10
1888           * y        (y) int64 90 80 70
1889 
1890         Reindexing with the other array having coordinates which the source array doesn't have:
1891 
1892         >>> data = np.arange(12).reshape(4, 3)
1893         >>> da1 = xr.DataArray(
1894         ...     data=data,
1895         ...     dims=["x", "y"],
1896         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
1897         ... )
1898         >>> da2 = xr.DataArray(
1899         ...     data=data,
1900         ...     dims=["x", "y"],
1901         ...     coords={"x": [20, 10, 29, 39], "y": [70, 80, 90]},
1902         ... )
1903         >>> da1.reindex_like(da2)
1904         <xarray.DataArray (x: 4, y: 3)>
1905         array([[ 3.,  4.,  5.],
1906                [ 0.,  1.,  2.],
1907                [nan, nan, nan],
1908                [nan, nan, nan]])
1909         Coordinates:
1910           * x        (x) int64 20 10 29 39
1911           * y        (y) int64 70 80 90
1912 
1913         Filling missing values with the previous valid index with respect to the coordinates' value:
1914 
1915         >>> da1.reindex_like(da2, method="ffill")
1916         <xarray.DataArray (x: 4, y: 3)>
1917         array([[3, 4, 5],
1918                [0, 1, 2],
1919                [3, 4, 5],
1920                [6, 7, 8]])
1921         Coordinates:
1922           * x        (x) int64 20 10 29 39
1923           * y        (y) int64 70 80 90
1924 
1925         Filling missing values while tolerating specified error for inexact matches:
1926 
1927         >>> da1.reindex_like(da2, method="ffill", tolerance=5)
1928         <xarray.DataArray (x: 4, y: 3)>
1929         array([[ 3.,  4.,  5.],
1930                [ 0.,  1.,  2.],
1931                [nan, nan, nan],
1932                [nan, nan, nan]])
1933         Coordinates:
1934           * x        (x) int64 20 10 29 39
1935           * y        (y) int64 70 80 90
1936 
1937         Filling missing values with manually specified values:
1938 
1939         >>> da1.reindex_like(da2, fill_value=19)
1940         <xarray.DataArray (x: 4, y: 3)>
1941         array([[ 3,  4,  5],
1942                [ 0,  1,  2],
1943                [19, 19, 19],
1944                [19, 19, 19]])
1945         Coordinates:
1946           * x        (x) int64 20 10 29 39
1947           * y        (y) int64 70 80 90
1948 
1949         See Also
1950         --------
1951         DataArray.reindex
1952         align
1953         """
1954         return alignment.reindex_like(
1955             self,
1956             other=other,
1957             method=method,
1958             tolerance=tolerance,
1959             copy=copy,
1960             fill_value=fill_value,
1961         )
1962 
1963     def reindex(
1964         self: T_DataArray,
1965         indexers: Mapping[Any, Any] | None = None,
1966         method: ReindexMethodOptions = None,
1967         tolerance: float | Iterable[float] | None = None,
1968         copy: bool = True,
1969         fill_value=dtypes.NA,
1970         **indexers_kwargs: Any,
1971     ) -> T_DataArray:
1972         """Conform this object onto the indexes of another object, filling in
1973         missing values with ``fill_value``. The default fill value is NaN.
1974 
1975         Parameters
1976         ----------
1977         indexers : dict, optional
1978             Dictionary with keys given by dimension names and values given by
1979             arrays of coordinates tick labels. Any mis-matched coordinate
1980             values will be filled in with NaN, and any mis-matched dimension
1981             names will simply be ignored.
1982             One of indexers or indexers_kwargs must be provided.
1983         copy : bool, optional
1984             If ``copy=True``, data in the return value is always copied. If
1985             ``copy=False`` and reindexing is unnecessary, or can be performed
1986             with only slice operations, then the output may share memory with
1987             the input. In either case, a new xarray object is always returned.
1988         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1989             Method to use for filling index values in ``indexers`` not found on
1990             this data array:
1991 
1992             - None (default): don't fill gaps
1993             - pad / ffill: propagate last valid index value forward
1994             - backfill / bfill: propagate next valid index value backward
1995             - nearest: use nearest valid index value
1996 
1997         tolerance : float | Iterable[float] | None, default: None
1998             Maximum distance between original and new labels for inexact
1999             matches. The values of the index at the matching locations must
2000             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2001             Tolerance may be a scalar value, which applies the same tolerance
2002             to all values, or list-like, which applies variable tolerance per
2003             element. List-like must be the same size as the index and its dtype
2004             must exactly match the index’s type.
2005         fill_value : scalar or dict-like, optional
2006             Value to use for newly missing values. If a dict-like, maps
2007             variable names (including coordinates) to fill values. Use this
2008             data array's name to refer to the data array's values.
2009         **indexers_kwargs : {dim: indexer, ...}, optional
2010             The keyword arguments form of ``indexers``.
2011             One of indexers or indexers_kwargs must be provided.
2012 
2013         Returns
2014         -------
2015         reindexed : DataArray
2016             Another dataset array, with this array's data but replaced
2017             coordinates.
2018 
2019         Examples
2020         --------
2021         Reverse latitude:
2022 
2023         >>> da = xr.DataArray(
2024         ...     np.arange(4),
2025         ...     coords=[np.array([90, 89, 88, 87])],
2026         ...     dims="lat",
2027         ... )
2028         >>> da
2029         <xarray.DataArray (lat: 4)>
2030         array([0, 1, 2, 3])
2031         Coordinates:
2032           * lat      (lat) int64 90 89 88 87
2033         >>> da.reindex(lat=da.lat[::-1])
2034         <xarray.DataArray (lat: 4)>
2035         array([3, 2, 1, 0])
2036         Coordinates:
2037           * lat      (lat) int64 87 88 89 90
2038 
2039         See Also
2040         --------
2041         DataArray.reindex_like
2042         align
2043         """
2044         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
2045         return alignment.reindex(
2046             self,
2047             indexers=indexers,
2048             method=method,
2049             tolerance=tolerance,
2050             copy=copy,
2051             fill_value=fill_value,
2052         )
2053 
2054     def interp(
2055         self: T_DataArray,
2056         coords: Mapping[Any, Any] | None = None,
2057         method: InterpOptions = "linear",
2058         assume_sorted: bool = False,
2059         kwargs: Mapping[str, Any] | None = None,
2060         **coords_kwargs: Any,
2061     ) -> T_DataArray:
2062         """Interpolate a DataArray onto new coordinates
2063 
2064         Performs univariate or multivariate interpolation of a DataArray onto
2065         new coordinates using scipy's interpolation routines. If interpolating
2066         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
2067         called. When interpolating along multiple existing dimensions, an
2068         attempt is made to decompose the interpolation into multiple
2069         1-dimensional interpolations. If this is possible,
2070         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
2071         :py:func:`scipy.interpolate.interpn` is called.
2072 
2073         Parameters
2074         ----------
2075         coords : dict, optional
2076             Mapping from dimension names to the new coordinates.
2077             New coordinate can be a scalar, array-like or DataArray.
2078             If DataArrays are passed as new coordinates, their dimensions are
2079             used for the broadcasting. Missing values are skipped.
2080         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
2081             The method used to interpolate. The method should be supported by
2082             the scipy interpolator:
2083 
2084             - ``interp1d``: {"linear", "nearest", "zero", "slinear",
2085               "quadratic", "cubic", "polynomial"}
2086             - ``interpn``: {"linear", "nearest"}
2087 
2088             If ``"polynomial"`` is passed, the ``order`` keyword argument must
2089             also be provided.
2090         assume_sorted : bool, default: False
2091             If False, values of x can be in any order and they are sorted
2092             first. If True, x has to be an array of monotonically increasing
2093             values.
2094         kwargs : dict-like or None, default: None
2095             Additional keyword arguments passed to scipy's interpolator. Valid
2096             options and their behavior depend whether ``interp1d`` or
2097             ``interpn`` is used.
2098         **coords_kwargs : {dim: coordinate, ...}, optional
2099             The keyword arguments form of ``coords``.
2100             One of coords or coords_kwargs must be provided.
2101 
2102         Returns
2103         -------
2104         interpolated : DataArray
2105             New dataarray on the new coordinates.
2106 
2107         Notes
2108         -----
2109         scipy is required.
2110 
2111         See Also
2112         --------
2113         scipy.interpolate.interp1d
2114         scipy.interpolate.interpn
2115 
2116         Examples
2117         --------
2118         >>> da = xr.DataArray(
2119         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
2120         ...     dims=("x", "y"),
2121         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
2122         ... )
2123         >>> da
2124         <xarray.DataArray (x: 3, y: 4)>
2125         array([[ 1.,  4.,  2.,  9.],
2126                [ 2.,  7.,  6., nan],
2127                [ 6., nan,  5.,  8.]])
2128         Coordinates:
2129           * x        (x) int64 0 1 2
2130           * y        (y) int64 10 12 14 16
2131 
2132         1D linear interpolation (the default):
2133 
2134         >>> da.interp(x=[0, 0.75, 1.25, 1.75])
2135         <xarray.DataArray (x: 4, y: 4)>
2136         array([[1.  , 4.  , 2.  ,  nan],
2137                [1.75, 6.25, 5.  ,  nan],
2138                [3.  ,  nan, 5.75,  nan],
2139                [5.  ,  nan, 5.25,  nan]])
2140         Coordinates:
2141           * y        (y) int64 10 12 14 16
2142           * x        (x) float64 0.0 0.75 1.25 1.75
2143 
2144         1D nearest interpolation:
2145 
2146         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
2147         <xarray.DataArray (x: 4, y: 4)>
2148         array([[ 1.,  4.,  2.,  9.],
2149                [ 2.,  7.,  6., nan],
2150                [ 2.,  7.,  6., nan],
2151                [ 6., nan,  5.,  8.]])
2152         Coordinates:
2153           * y        (y) int64 10 12 14 16
2154           * x        (x) float64 0.0 0.75 1.25 1.75
2155 
2156         1D linear extrapolation:
2157 
2158         >>> da.interp(
2159         ...     x=[1, 1.5, 2.5, 3.5],
2160         ...     method="linear",
2161         ...     kwargs={"fill_value": "extrapolate"},
2162         ... )
2163         <xarray.DataArray (x: 4, y: 4)>
2164         array([[ 2. ,  7. ,  6. ,  nan],
2165                [ 4. ,  nan,  5.5,  nan],
2166                [ 8. ,  nan,  4.5,  nan],
2167                [12. ,  nan,  3.5,  nan]])
2168         Coordinates:
2169           * y        (y) int64 10 12 14 16
2170           * x        (x) float64 1.0 1.5 2.5 3.5
2171 
2172         2D linear interpolation:
2173 
2174         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
2175         <xarray.DataArray (x: 4, y: 3)>
2176         array([[2.5  , 3.   ,   nan],
2177                [4.   , 5.625,   nan],
2178                [  nan,   nan,   nan],
2179                [  nan,   nan,   nan]])
2180         Coordinates:
2181           * x        (x) float64 0.0 0.75 1.25 1.75
2182           * y        (y) int64 11 13 15
2183         """
2184         if self.dtype.kind not in "uifc":
2185             raise TypeError(
2186                 "interp only works for a numeric type array. "
2187                 "Given {}.".format(self.dtype)
2188             )
2189         ds = self._to_temp_dataset().interp(
2190             coords,
2191             method=method,
2192             kwargs=kwargs,
2193             assume_sorted=assume_sorted,
2194             **coords_kwargs,
2195         )
2196         return self._from_temp_dataset(ds)
2197 
2198     def interp_like(
2199         self: T_DataArray,
2200         other: DataArray | Dataset,
2201         method: InterpOptions = "linear",
2202         assume_sorted: bool = False,
2203         kwargs: Mapping[str, Any] | None = None,
2204     ) -> T_DataArray:
2205         """Interpolate this object onto the coordinates of another object,
2206         filling out of range values with NaN.
2207 
2208         If interpolating along a single existing dimension,
2209         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
2210         along multiple existing dimensions, an attempt is made to decompose the
2211         interpolation into multiple 1-dimensional interpolations. If this is
2212         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
2213         :py:func:`scipy.interpolate.interpn` is called.
2214 
2215         Parameters
2216         ----------
2217         other : Dataset or DataArray
2218             Object with an 'indexes' attribute giving a mapping from dimension
2219             names to an 1d array-like, which provides coordinates upon
2220             which to index the variables in this dataset. Missing values are skipped.
2221         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
2222             The method used to interpolate. The method should be supported by
2223             the scipy interpolator:
2224 
2225             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic",
2226               "polynomial"} when ``interp1d`` is called.
2227             - {"linear", "nearest"} when ``interpn`` is called.
2228 
2229             If ``"polynomial"`` is passed, the ``order`` keyword argument must
2230             also be provided.
2231         assume_sorted : bool, default: False
2232             If False, values of coordinates that are interpolated over can be
2233             in any order and they are sorted first. If True, interpolated
2234             coordinates are assumed to be an array of monotonically increasing
2235             values.
2236         kwargs : dict, optional
2237             Additional keyword passed to scipy's interpolator.
2238 
2239         Returns
2240         -------
2241         interpolated : DataArray
2242             Another dataarray by interpolating this dataarray's data along the
2243             coordinates of the other object.
2244 
2245         Examples
2246         --------
2247         >>> data = np.arange(12).reshape(4, 3)
2248         >>> da1 = xr.DataArray(
2249         ...     data=data,
2250         ...     dims=["x", "y"],
2251         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
2252         ... )
2253         >>> da1
2254         <xarray.DataArray (x: 4, y: 3)>
2255         array([[ 0,  1,  2],
2256                [ 3,  4,  5],
2257                [ 6,  7,  8],
2258                [ 9, 10, 11]])
2259         Coordinates:
2260           * x        (x) int64 10 20 30 40
2261           * y        (y) int64 70 80 90
2262         >>> da2 = xr.DataArray(
2263         ...     data=data,
2264         ...     dims=["x", "y"],
2265         ...     coords={"x": [10, 20, 29, 39], "y": [70, 80, 90]},
2266         ... )
2267         >>> da2
2268         <xarray.DataArray (x: 4, y: 3)>
2269         array([[ 0,  1,  2],
2270                [ 3,  4,  5],
2271                [ 6,  7,  8],
2272                [ 9, 10, 11]])
2273         Coordinates:
2274           * x        (x) int64 10 20 29 39
2275           * y        (y) int64 70 80 90
2276 
2277         Interpolate the values in the coordinates of the other DataArray with respect to the source's values:
2278 
2279         >>> da2.interp_like(da1)
2280         <xarray.DataArray (x: 4, y: 3)>
2281         array([[0. , 1. , 2. ],
2282                [3. , 4. , 5. ],
2283                [6.3, 7.3, 8.3],
2284                [nan, nan, nan]])
2285         Coordinates:
2286           * x        (x) int64 10 20 30 40
2287           * y        (y) int64 70 80 90
2288 
2289         Could also extrapolate missing values:
2290 
2291         >>> da2.interp_like(da1, kwargs={"fill_value": "extrapolate"})
2292         <xarray.DataArray (x: 4, y: 3)>
2293         array([[ 0. ,  1. ,  2. ],
2294                [ 3. ,  4. ,  5. ],
2295                [ 6.3,  7.3,  8.3],
2296                [ 9.3, 10.3, 11.3]])
2297         Coordinates:
2298           * x        (x) int64 10 20 30 40
2299           * y        (y) int64 70 80 90
2300 
2301         Notes
2302         -----
2303         scipy is required.
2304         If the dataarray has object-type coordinates, reindex is used for these
2305         coordinates instead of the interpolation.
2306 
2307         See Also
2308         --------
2309         DataArray.interp
2310         DataArray.reindex_like
2311         """
2312         if self.dtype.kind not in "uifc":
2313             raise TypeError(
2314                 "interp only works for a numeric type array. "
2315                 "Given {}.".format(self.dtype)
2316             )
2317         ds = self._to_temp_dataset().interp_like(
2318             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
2319         )
2320         return self._from_temp_dataset(ds)
2321 
2322     # change type of self and return to T_DataArray once
2323     # https://github.com/python/mypy/issues/12846 is resolved
2324     def rename(
2325         self,
2326         new_name_or_name_dict: Hashable | Mapping[Any, Hashable] | None = None,
2327         **names: Hashable,
2328     ) -> DataArray:
2329         """Returns a new DataArray with renamed coordinates, dimensions or a new name.
2330 
2331         Parameters
2332         ----------
2333         new_name_or_name_dict : str or dict-like, optional
2334             If the argument is dict-like, it used as a mapping from old
2335             names to new names for coordinates or dimensions. Otherwise,
2336             use the argument as the new name for this array.
2337         **names : Hashable, optional
2338             The keyword arguments form of a mapping from old names to
2339             new names for coordinates or dimensions.
2340             One of new_name_or_name_dict or names must be provided.
2341 
2342         Returns
2343         -------
2344         renamed : DataArray
2345             Renamed array or array with renamed coordinates.
2346 
2347         See Also
2348         --------
2349         Dataset.rename
2350         DataArray.swap_dims
2351         """
2352         if new_name_or_name_dict is None and not names:
2353             # change name to None?
2354             return self._replace(name=None)
2355         if utils.is_dict_like(new_name_or_name_dict) or new_name_or_name_dict is None:
2356             # change dims/coords
2357             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
2358             dataset = self._to_temp_dataset()._rename(name_dict)
2359             return self._from_temp_dataset(dataset)
2360         if utils.hashable(new_name_or_name_dict) and names:
2361             # change name + dims/coords
2362             dataset = self._to_temp_dataset()._rename(names)
2363             dataarray = self._from_temp_dataset(dataset)
2364             return dataarray._replace(name=new_name_or_name_dict)
2365         # only change name
2366         return self._replace(name=new_name_or_name_dict)
2367 
2368     def swap_dims(
2369         self: T_DataArray,
2370         dims_dict: Mapping[Any, Hashable] | None = None,
2371         **dims_kwargs,
2372     ) -> T_DataArray:
2373         """Returns a new DataArray with swapped dimensions.
2374 
2375         Parameters
2376         ----------
2377         dims_dict : dict-like
2378             Dictionary whose keys are current dimension names and whose values
2379             are new names.
2380         **dims_kwargs : {existing_dim: new_dim, ...}, optional
2381             The keyword arguments form of ``dims_dict``.
2382             One of dims_dict or dims_kwargs must be provided.
2383 
2384         Returns
2385         -------
2386         swapped : DataArray
2387             DataArray with swapped dimensions.
2388 
2389         Examples
2390         --------
2391         >>> arr = xr.DataArray(
2392         ...     data=[0, 1],
2393         ...     dims="x",
2394         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
2395         ... )
2396         >>> arr
2397         <xarray.DataArray (x: 2)>
2398         array([0, 1])
2399         Coordinates:
2400           * x        (x) <U1 'a' 'b'
2401             y        (x) int64 0 1
2402 
2403         >>> arr.swap_dims({"x": "y"})
2404         <xarray.DataArray (y: 2)>
2405         array([0, 1])
2406         Coordinates:
2407             x        (y) <U1 'a' 'b'
2408           * y        (y) int64 0 1
2409 
2410         >>> arr.swap_dims({"x": "z"})
2411         <xarray.DataArray (z: 2)>
2412         array([0, 1])
2413         Coordinates:
2414             x        (z) <U1 'a' 'b'
2415             y        (z) int64 0 1
2416         Dimensions without coordinates: z
2417 
2418         See Also
2419         --------
2420         DataArray.rename
2421         Dataset.swap_dims
2422         """
2423         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
2424         ds = self._to_temp_dataset().swap_dims(dims_dict)
2425         return self._from_temp_dataset(ds)
2426 
2427     # change type of self and return to T_DataArray once
2428     # https://github.com/python/mypy/issues/12846 is resolved
2429     def expand_dims(
2430         self,
2431         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
2432         axis: None | int | Sequence[int] = None,
2433         **dim_kwargs: Any,
2434     ) -> DataArray:
2435         """Return a new object with an additional axis (or axes) inserted at
2436         the corresponding position in the array shape. The new object is a
2437         view into the underlying array, not a copy.
2438 
2439         If dim is already a scalar coordinate, it will be promoted to a 1D
2440         coordinate consisting of a single value.
2441 
2442         Parameters
2443         ----------
2444         dim : Hashable, sequence of Hashable, dict, or None, optional
2445             Dimensions to include on the new variable.
2446             If provided as str or sequence of str, then dimensions are inserted
2447             with length 1. If provided as a dict, then the keys are the new
2448             dimensions and the values are either integers (giving the length of
2449             the new dimensions) or sequence/ndarray (giving the coordinates of
2450             the new dimensions).
2451         axis : int, sequence of int, or None, default: None
2452             Axis position(s) where new axis is to be inserted (position(s) on
2453             the result array). If a sequence of integers is passed,
2454             multiple axes are inserted. In this case, dim arguments should be
2455             same length list. If axis=None is passed, all the axes will be
2456             inserted to the start of the result array.
2457         **dim_kwargs : int or sequence or ndarray
2458             The keywords are arbitrary dimensions being inserted and the values
2459             are either the lengths of the new dims (if int is given), or their
2460             coordinates. Note, this is an alternative to passing a dict to the
2461             dim kwarg and will only be used if dim is None.
2462 
2463         Returns
2464         -------
2465         expanded : DataArray
2466             This object, but with additional dimension(s).
2467 
2468         See Also
2469         --------
2470         Dataset.expand_dims
2471 
2472         Examples
2473         --------
2474         >>> da = xr.DataArray(np.arange(5), dims=("x"))
2475         >>> da
2476         <xarray.DataArray (x: 5)>
2477         array([0, 1, 2, 3, 4])
2478         Dimensions without coordinates: x
2479 
2480         Add new dimension of length 2:
2481 
2482         >>> da.expand_dims(dim={"y": 2})
2483         <xarray.DataArray (y: 2, x: 5)>
2484         array([[0, 1, 2, 3, 4],
2485                [0, 1, 2, 3, 4]])
2486         Dimensions without coordinates: y, x
2487 
2488         >>> da.expand_dims(dim={"y": 2}, axis=1)
2489         <xarray.DataArray (x: 5, y: 2)>
2490         array([[0, 0],
2491                [1, 1],
2492                [2, 2],
2493                [3, 3],
2494                [4, 4]])
2495         Dimensions without coordinates: x, y
2496 
2497         Add a new dimension with coordinates from array:
2498 
2499         >>> da.expand_dims(dim={"y": np.arange(5)}, axis=0)
2500         <xarray.DataArray (y: 5, x: 5)>
2501         array([[0, 1, 2, 3, 4],
2502                [0, 1, 2, 3, 4],
2503                [0, 1, 2, 3, 4],
2504                [0, 1, 2, 3, 4],
2505                [0, 1, 2, 3, 4]])
2506         Coordinates:
2507           * y        (y) int64 0 1 2 3 4
2508         Dimensions without coordinates: x
2509         """
2510         if isinstance(dim, int):
2511             raise TypeError("dim should be Hashable or sequence/mapping of Hashables")
2512         elif isinstance(dim, Sequence) and not isinstance(dim, str):
2513             if len(dim) != len(set(dim)):
2514                 raise ValueError("dims should not contain duplicate values.")
2515             dim = dict.fromkeys(dim, 1)
2516         elif dim is not None and not isinstance(dim, Mapping):
2517             dim = {cast(Hashable, dim): 1}
2518 
2519         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
2520         ds = self._to_temp_dataset().expand_dims(dim, axis)
2521         return self._from_temp_dataset(ds)
2522 
2523     # change type of self and return to T_DataArray once
2524     # https://github.com/python/mypy/issues/12846 is resolved
2525     def set_index(
2526         self,
2527         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,
2528         append: bool = False,
2529         **indexes_kwargs: Hashable | Sequence[Hashable],
2530     ) -> DataArray:
2531         """Set DataArray (multi-)indexes using one or more existing
2532         coordinates.
2533 
2534         This legacy method is limited to pandas (multi-)indexes and
2535         1-dimensional "dimension" coordinates. See
2536         :py:meth:`~DataArray.set_xindex` for setting a pandas or a custom
2537         Xarray-compatible index from one or more arbitrary coordinates.
2538 
2539         Parameters
2540         ----------
2541         indexes : {dim: index, ...}
2542             Mapping from names matching dimensions and values given
2543             by (lists of) the names of existing coordinates or variables to set
2544             as new (multi-)index.
2545         append : bool, default: False
2546             If True, append the supplied index(es) to the existing index(es).
2547             Otherwise replace the existing index(es).
2548         **indexes_kwargs : optional
2549             The keyword arguments form of ``indexes``.
2550             One of indexes or indexes_kwargs must be provided.
2551 
2552         Returns
2553         -------
2554         obj : DataArray
2555             Another DataArray, with this data but replaced coordinates.
2556 
2557         Examples
2558         --------
2559         >>> arr = xr.DataArray(
2560         ...     data=np.ones((2, 3)),
2561         ...     dims=["x", "y"],
2562         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
2563         ... )
2564         >>> arr
2565         <xarray.DataArray (x: 2, y: 3)>
2566         array([[1., 1., 1.],
2567                [1., 1., 1.]])
2568         Coordinates:
2569           * x        (x) int64 0 1
2570           * y        (y) int64 0 1 2
2571             a        (x) int64 3 4
2572         >>> arr.set_index(x="a")
2573         <xarray.DataArray (x: 2, y: 3)>
2574         array([[1., 1., 1.],
2575                [1., 1., 1.]])
2576         Coordinates:
2577           * x        (x) int64 3 4
2578           * y        (y) int64 0 1 2
2579 
2580         See Also
2581         --------
2582         DataArray.reset_index
2583         DataArray.set_xindex
2584         """
2585         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)
2586         return self._from_temp_dataset(ds)
2587 
2588     # change type of self and return to T_DataArray once
2589     # https://github.com/python/mypy/issues/12846 is resolved
2590     def reset_index(
2591         self,
2592         dims_or_levels: Hashable | Sequence[Hashable],
2593         drop: bool = False,
2594     ) -> DataArray:
2595         """Reset the specified index(es) or multi-index level(s).
2596 
2597         This legacy method is specific to pandas (multi-)indexes and
2598         1-dimensional "dimension" coordinates. See the more generic
2599         :py:meth:`~DataArray.drop_indexes` and :py:meth:`~DataArray.set_xindex`
2600         method to respectively drop and set pandas or custom indexes for
2601         arbitrary coordinates.
2602 
2603         Parameters
2604         ----------
2605         dims_or_levels : Hashable or sequence of Hashable
2606             Name(s) of the dimension(s) and/or multi-index level(s) that will
2607             be reset.
2608         drop : bool, default: False
2609             If True, remove the specified indexes and/or multi-index levels
2610             instead of extracting them as new coordinates (default: False).
2611 
2612         Returns
2613         -------
2614         obj : DataArray
2615             Another dataarray, with this dataarray's data but replaced
2616             coordinates.
2617 
2618         See Also
2619         --------
2620         DataArray.set_index
2621         DataArray.set_xindex
2622         DataArray.drop_indexes
2623         """
2624         ds = self._to_temp_dataset().reset_index(dims_or_levels, drop=drop)
2625         return self._from_temp_dataset(ds)
2626 
2627     def set_xindex(
2628         self: T_DataArray,
2629         coord_names: str | Sequence[Hashable],
2630         index_cls: type[Index] | None = None,
2631         **options,
2632     ) -> T_DataArray:
2633         """Set a new, Xarray-compatible index from one or more existing
2634         coordinate(s).
2635 
2636         Parameters
2637         ----------
2638         coord_names : str or list
2639             Name(s) of the coordinate(s) used to build the index.
2640             If several names are given, their order matters.
2641         index_cls : subclass of :class:`~xarray.indexes.Index`
2642             The type of index to create. By default, try setting
2643             a pandas (multi-)index from the supplied coordinates.
2644         **options
2645             Options passed to the index constructor.
2646 
2647         Returns
2648         -------
2649         obj : DataArray
2650             Another dataarray, with this dataarray's data and with a new index.
2651 
2652         """
2653         ds = self._to_temp_dataset().set_xindex(coord_names, index_cls, **options)
2654         return self._from_temp_dataset(ds)
2655 
2656     def reorder_levels(
2657         self: T_DataArray,
2658         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
2659         **dim_order_kwargs: Sequence[int | Hashable],
2660     ) -> T_DataArray:
2661         """Rearrange index levels using input order.
2662 
2663         Parameters
2664         ----------
2665         dim_order dict-like of Hashable to int or Hashable: optional
2666             Mapping from names matching dimensions and values given
2667             by lists representing new level orders. Every given dimension
2668             must have a multi-index.
2669         **dim_order_kwargs : optional
2670             The keyword arguments form of ``dim_order``.
2671             One of dim_order or dim_order_kwargs must be provided.
2672 
2673         Returns
2674         -------
2675         obj : DataArray
2676             Another dataarray, with this dataarray's data but replaced
2677             coordinates.
2678         """
2679         ds = self._to_temp_dataset().reorder_levels(dim_order, **dim_order_kwargs)
2680         return self._from_temp_dataset(ds)
2681 
2682     def stack(
2683         self: T_DataArray,
2684         dimensions: Mapping[Any, Sequence[Hashable]] | None = None,
2685         create_index: bool | None = True,
2686         index_cls: type[Index] = PandasMultiIndex,
2687         **dimensions_kwargs: Sequence[Hashable],
2688     ) -> T_DataArray:
2689         """
2690         Stack any number of existing dimensions into a single new dimension.
2691 
2692         New dimensions will be added at the end, and the corresponding
2693         coordinate variables will be combined into a MultiIndex.
2694 
2695         Parameters
2696         ----------
2697         dimensions : mapping of Hashable to sequence of Hashable
2698             Mapping of the form `new_name=(dim1, dim2, ...)`.
2699             Names of new dimensions, and the existing dimensions that they
2700             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
2701             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
2702             all dimensions.
2703         create_index : bool or None, default: True
2704             If True, create a multi-index for each of the stacked dimensions.
2705             If False, don't create any index.
2706             If None, create a multi-index only if exactly one single (1-d) coordinate
2707             index is found for every dimension to stack.
2708         index_cls: class, optional
2709             Can be used to pass a custom multi-index type. Must be an Xarray index that
2710             implements `.stack()`. By default, a pandas multi-index wrapper is used.
2711         **dimensions_kwargs
2712             The keyword arguments form of ``dimensions``.
2713             One of dimensions or dimensions_kwargs must be provided.
2714 
2715         Returns
2716         -------
2717         stacked : DataArray
2718             DataArray with stacked data.
2719 
2720         Examples
2721         --------
2722         >>> arr = xr.DataArray(
2723         ...     np.arange(6).reshape(2, 3),
2724         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2725         ... )
2726         >>> arr
2727         <xarray.DataArray (x: 2, y: 3)>
2728         array([[0, 1, 2],
2729                [3, 4, 5]])
2730         Coordinates:
2731           * x        (x) <U1 'a' 'b'
2732           * y        (y) int64 0 1 2
2733         >>> stacked = arr.stack(z=("x", "y"))
2734         >>> stacked.indexes["z"]
2735         MultiIndex([('a', 0),
2736                     ('a', 1),
2737                     ('a', 2),
2738                     ('b', 0),
2739                     ('b', 1),
2740                     ('b', 2)],
2741                    name='z')
2742 
2743         See Also
2744         --------
2745         DataArray.unstack
2746         """
2747         ds = self._to_temp_dataset().stack(
2748             dimensions,
2749             create_index=create_index,
2750             index_cls=index_cls,
2751             **dimensions_kwargs,
2752         )
2753         return self._from_temp_dataset(ds)
2754 
2755     # change type of self and return to T_DataArray once
2756     # https://github.com/python/mypy/issues/12846 is resolved
2757     def unstack(
2758         self,
2759         dim: Dims = None,
2760         fill_value: Any = dtypes.NA,
2761         sparse: bool = False,
2762     ) -> DataArray:
2763         """
2764         Unstack existing dimensions corresponding to MultiIndexes into
2765         multiple new dimensions.
2766 
2767         New dimensions will be added at the end.
2768 
2769         Parameters
2770         ----------
2771         dim : str, Iterable of Hashable or None, optional
2772             Dimension(s) over which to unstack. By default unstacks all
2773             MultiIndexes.
2774         fill_value : scalar or dict-like, default: nan
2775             Value to be filled. If a dict-like, maps variable names to
2776             fill values. Use the data array's name to refer to its
2777             name. If not provided or if the dict-like does not contain
2778             all variables, the dtype's NA value will be used.
2779         sparse : bool, default: False
2780             Use sparse-array if True
2781 
2782         Returns
2783         -------
2784         unstacked : DataArray
2785             Array with unstacked data.
2786 
2787         Examples
2788         --------
2789         >>> arr = xr.DataArray(
2790         ...     np.arange(6).reshape(2, 3),
2791         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2792         ... )
2793         >>> arr
2794         <xarray.DataArray (x: 2, y: 3)>
2795         array([[0, 1, 2],
2796                [3, 4, 5]])
2797         Coordinates:
2798           * x        (x) <U1 'a' 'b'
2799           * y        (y) int64 0 1 2
2800         >>> stacked = arr.stack(z=("x", "y"))
2801         >>> stacked.indexes["z"]
2802         MultiIndex([('a', 0),
2803                     ('a', 1),
2804                     ('a', 2),
2805                     ('b', 0),
2806                     ('b', 1),
2807                     ('b', 2)],
2808                    name='z')
2809         >>> roundtripped = stacked.unstack()
2810         >>> arr.identical(roundtripped)
2811         True
2812 
2813         See Also
2814         --------
2815         DataArray.stack
2816         """
2817         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
2818         return self._from_temp_dataset(ds)
2819 
2820     def to_unstacked_dataset(self, dim: Hashable, level: int | Hashable = 0) -> Dataset:
2821         """Unstack DataArray expanding to Dataset along a given level of a
2822         stacked coordinate.
2823 
2824         This is the inverse operation of Dataset.to_stacked_array.
2825 
2826         Parameters
2827         ----------
2828         dim : Hashable
2829             Name of existing dimension to unstack
2830         level : int or Hashable, default: 0
2831             The MultiIndex level to expand to a dataset along. Can either be
2832             the integer index of the level or its name.
2833 
2834         Returns
2835         -------
2836         unstacked: Dataset
2837 
2838         Examples
2839         --------
2840         >>> arr = xr.DataArray(
2841         ...     np.arange(6).reshape(2, 3),
2842         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2843         ... )
2844         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
2845         >>> data
2846         <xarray.Dataset>
2847         Dimensions:  (x: 2, y: 3)
2848         Coordinates:
2849           * x        (x) <U1 'a' 'b'
2850           * y        (y) int64 0 1 2
2851         Data variables:
2852             a        (x, y) int64 0 1 2 3 4 5
2853             b        (x) int64 0 3
2854         >>> stacked = data.to_stacked_array("z", ["x"])
2855         >>> stacked.indexes["z"]
2856         MultiIndex([('a', 0.0),
2857                     ('a', 1.0),
2858                     ('a', 2.0),
2859                     ('b', nan)],
2860                    name='z')
2861         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
2862         >>> data.identical(roundtripped)
2863         True
2864 
2865         See Also
2866         --------
2867         Dataset.to_stacked_array
2868         """
2869         idx = self._indexes[dim].to_pandas_index()
2870         if not isinstance(idx, pd.MultiIndex):
2871             raise ValueError(f"'{dim}' is not a stacked coordinate")
2872 
2873         level_number = idx._get_level_number(level)
2874         variables = idx.levels[level_number]
2875         variable_dim = idx.names[level_number]
2876 
2877         # pull variables out of datarray
2878         data_dict = {}
2879         for k in variables:
2880             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)
2881 
2882         # unstacked dataset
2883         return Dataset(data_dict)
2884 
2885     def transpose(
2886         self: T_DataArray,
2887         *dims: Hashable,
2888         transpose_coords: bool = True,
2889         missing_dims: ErrorOptionsWithWarn = "raise",
2890     ) -> T_DataArray:
2891         """Return a new DataArray object with transposed dimensions.
2892 
2893         Parameters
2894         ----------
2895         *dims : Hashable, optional
2896             By default, reverse the dimensions. Otherwise, reorder the
2897             dimensions to this order.
2898         transpose_coords : bool, default: True
2899             If True, also transpose the coordinates of this DataArray.
2900         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2901             What to do if dimensions that should be selected from are not present in the
2902             DataArray:
2903             - "raise": raise an exception
2904             - "warn": raise a warning, and ignore the missing dimensions
2905             - "ignore": ignore the missing dimensions
2906 
2907         Returns
2908         -------
2909         transposed : DataArray
2910             The returned DataArray's array is transposed.
2911 
2912         Notes
2913         -----
2914         This operation returns a view of this array's data. It is
2915         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
2916         -- the data will be fully loaded.
2917 
2918         See Also
2919         --------
2920         numpy.transpose
2921         Dataset.transpose
2922         """
2923         if dims:
2924             dims = tuple(utils.infix_dims(dims, self.dims, missing_dims))
2925         variable = self.variable.transpose(*dims)
2926         if transpose_coords:
2927             coords: dict[Hashable, Variable] = {}
2928             for name, coord in self.coords.items():
2929                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
2930                 coords[name] = coord.variable.transpose(*coord_dims)
2931             return self._replace(variable, coords)
2932         else:
2933             return self._replace(variable)
2934 
2935     @property
2936     def T(self: T_DataArray) -> T_DataArray:
2937         return self.transpose()
2938 
2939     # change type of self and return to T_DataArray once
2940     # https://github.com/python/mypy/issues/12846 is resolved
2941     def drop_vars(
2942         self,
2943         names: Hashable | Iterable[Hashable],
2944         *,
2945         errors: ErrorOptions = "raise",
2946     ) -> DataArray:
2947         """Returns an array with dropped variables.
2948 
2949         Parameters
2950         ----------
2951         names : Hashable or iterable of Hashable
2952             Name(s) of variables to drop.
2953         errors : {"raise", "ignore"}, default: "raise"
2954             If 'raise', raises a ValueError error if any of the variable
2955             passed are not in the dataset. If 'ignore', any given names that are in the
2956             DataArray are dropped and no error is raised.
2957 
2958         Returns
2959         -------
2960         dropped : Dataset
2961             New Dataset copied from `self` with variables removed.
2962 
2963         Examples
2964         -------
2965         >>> data = np.arange(12).reshape(4, 3)
2966         >>> da = xr.DataArray(
2967         ...     data=data,
2968         ...     dims=["x", "y"],
2969         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
2970         ... )
2971         >>> da
2972         <xarray.DataArray (x: 4, y: 3)>
2973         array([[ 0,  1,  2],
2974                [ 3,  4,  5],
2975                [ 6,  7,  8],
2976                [ 9, 10, 11]])
2977         Coordinates:
2978           * x        (x) int64 10 20 30 40
2979           * y        (y) int64 70 80 90
2980 
2981         Removing a single variable:
2982 
2983         >>> da.drop_vars("x")
2984         <xarray.DataArray (x: 4, y: 3)>
2985         array([[ 0,  1,  2],
2986                [ 3,  4,  5],
2987                [ 6,  7,  8],
2988                [ 9, 10, 11]])
2989         Coordinates:
2990           * y        (y) int64 70 80 90
2991         Dimensions without coordinates: x
2992 
2993         Removing a list of variables:
2994 
2995         >>> da.drop_vars(["x", "y"])
2996         <xarray.DataArray (x: 4, y: 3)>
2997         array([[ 0,  1,  2],
2998                [ 3,  4,  5],
2999                [ 6,  7,  8],
3000                [ 9, 10, 11]])
3001         Dimensions without coordinates: x, y
3002         """
3003         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
3004         return self._from_temp_dataset(ds)
3005 
3006     def drop_indexes(
3007         self: T_DataArray,
3008         coord_names: Hashable | Iterable[Hashable],
3009         *,
3010         errors: ErrorOptions = "raise",
3011     ) -> T_DataArray:
3012         """Drop the indexes assigned to the given coordinates.
3013 
3014         Parameters
3015         ----------
3016         coord_names : hashable or iterable of hashable
3017             Name(s) of the coordinate(s) for which to drop the index.
3018         errors : {"raise", "ignore"}, default: "raise"
3019             If 'raise', raises a ValueError error if any of the coordinates
3020             passed have no index or are not in the dataset.
3021             If 'ignore', no error is raised.
3022 
3023         Returns
3024         -------
3025         dropped : DataArray
3026             A new dataarray with dropped indexes.
3027         """
3028         ds = self._to_temp_dataset().drop_indexes(coord_names, errors=errors)
3029         return self._from_temp_dataset(ds)
3030 
3031     def drop(
3032         self: T_DataArray,
3033         labels: Mapping[Any, Any] | None = None,
3034         dim: Hashable | None = None,
3035         *,
3036         errors: ErrorOptions = "raise",
3037         **labels_kwargs,
3038     ) -> T_DataArray:
3039         """Backward compatible method based on `drop_vars` and `drop_sel`
3040 
3041         Using either `drop_vars` or `drop_sel` is encouraged
3042 
3043         See Also
3044         --------
3045         DataArray.drop_vars
3046         DataArray.drop_sel
3047         """
3048         ds = self._to_temp_dataset().drop(labels, dim, errors=errors, **labels_kwargs)
3049         return self._from_temp_dataset(ds)
3050 
3051     def drop_sel(
3052         self: T_DataArray,
3053         labels: Mapping[Any, Any] | None = None,
3054         *,
3055         errors: ErrorOptions = "raise",
3056         **labels_kwargs,
3057     ) -> T_DataArray:
3058         """Drop index labels from this DataArray.
3059 
3060         Parameters
3061         ----------
3062         labels : mapping of Hashable to Any
3063             Index labels to drop
3064         errors : {"raise", "ignore"}, default: "raise"
3065             If 'raise', raises a ValueError error if
3066             any of the index labels passed are not
3067             in the dataset. If 'ignore', any given labels that are in the
3068             dataset are dropped and no error is raised.
3069         **labels_kwargs : {dim: label, ...}, optional
3070             The keyword arguments form of ``dim`` and ``labels``
3071 
3072         Returns
3073         -------
3074         dropped : DataArray
3075 
3076         Examples
3077         --------
3078         >>> da = xr.DataArray(
3079         ...     np.arange(25).reshape(5, 5),
3080         ...     coords={"x": np.arange(0, 9, 2), "y": np.arange(0, 13, 3)},
3081         ...     dims=("x", "y"),
3082         ... )
3083         >>> da
3084         <xarray.DataArray (x: 5, y: 5)>
3085         array([[ 0,  1,  2,  3,  4],
3086                [ 5,  6,  7,  8,  9],
3087                [10, 11, 12, 13, 14],
3088                [15, 16, 17, 18, 19],
3089                [20, 21, 22, 23, 24]])
3090         Coordinates:
3091           * x        (x) int64 0 2 4 6 8
3092           * y        (y) int64 0 3 6 9 12
3093 
3094         >>> da.drop_sel(x=[0, 2], y=9)
3095         <xarray.DataArray (x: 3, y: 4)>
3096         array([[10, 11, 12, 14],
3097                [15, 16, 17, 19],
3098                [20, 21, 22, 24]])
3099         Coordinates:
3100           * x        (x) int64 4 6 8
3101           * y        (y) int64 0 3 6 12
3102 
3103         >>> da.drop_sel({"x": 6, "y": [0, 3]})
3104         <xarray.DataArray (x: 4, y: 3)>
3105         array([[ 2,  3,  4],
3106                [ 7,  8,  9],
3107                [12, 13, 14],
3108                [22, 23, 24]])
3109         Coordinates:
3110           * x        (x) int64 0 2 4 8
3111           * y        (y) int64 6 9 12
3112         """
3113         if labels_kwargs or isinstance(labels, dict):
3114             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
3115 
3116         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
3117         return self._from_temp_dataset(ds)
3118 
3119     def drop_isel(
3120         self: T_DataArray, indexers: Mapping[Any, Any] | None = None, **indexers_kwargs
3121     ) -> T_DataArray:
3122         """Drop index positions from this DataArray.
3123 
3124         Parameters
3125         ----------
3126         indexers : mapping of Hashable to Any or None, default: None
3127             Index locations to drop
3128         **indexers_kwargs : {dim: position, ...}, optional
3129             The keyword arguments form of ``dim`` and ``positions``
3130 
3131         Returns
3132         -------
3133         dropped : DataArray
3134 
3135         Raises
3136         ------
3137         IndexError
3138 
3139         Examples
3140         --------
3141         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("X", "Y"))
3142         >>> da
3143         <xarray.DataArray (X: 5, Y: 5)>
3144         array([[ 0,  1,  2,  3,  4],
3145                [ 5,  6,  7,  8,  9],
3146                [10, 11, 12, 13, 14],
3147                [15, 16, 17, 18, 19],
3148                [20, 21, 22, 23, 24]])
3149         Dimensions without coordinates: X, Y
3150 
3151         >>> da.drop_isel(X=[0, 4], Y=2)
3152         <xarray.DataArray (X: 3, Y: 4)>
3153         array([[ 5,  6,  8,  9],
3154                [10, 11, 13, 14],
3155                [15, 16, 18, 19]])
3156         Dimensions without coordinates: X, Y
3157 
3158         >>> da.drop_isel({"X": 3, "Y": 3})
3159         <xarray.DataArray (X: 4, Y: 4)>
3160         array([[ 0,  1,  2,  4],
3161                [ 5,  6,  7,  9],
3162                [10, 11, 12, 14],
3163                [20, 21, 22, 24]])
3164         Dimensions without coordinates: X, Y
3165         """
3166         dataset = self._to_temp_dataset()
3167         dataset = dataset.drop_isel(indexers=indexers, **indexers_kwargs)
3168         return self._from_temp_dataset(dataset)
3169 
3170     def dropna(
3171         self: T_DataArray,
3172         dim: Hashable,
3173         how: Literal["any", "all"] = "any",
3174         thresh: int | None = None,
3175     ) -> T_DataArray:
3176         """Returns a new array with dropped labels for missing values along
3177         the provided dimension.
3178 
3179         Parameters
3180         ----------
3181         dim : Hashable
3182             Dimension along which to drop missing values. Dropping along
3183             multiple dimensions simultaneously is not yet supported.
3184         how : {"any", "all"}, default: "any"
3185             - any : if any NA values are present, drop that label
3186             - all : if all values are NA, drop that label
3187 
3188         thresh : int or None, default: None
3189             If supplied, require this many non-NA values.
3190 
3191         Returns
3192         -------
3193         dropped : DataArray
3194 
3195         Examples
3196         --------
3197         >>> temperature = [
3198         ...     [0, 4, 2, 9],
3199         ...     [np.nan, np.nan, np.nan, np.nan],
3200         ...     [np.nan, 4, 2, 0],
3201         ...     [3, 1, 0, 0],
3202         ... ]
3203         >>> da = xr.DataArray(
3204         ...     data=temperature,
3205         ...     dims=["Y", "X"],
3206         ...     coords=dict(
3207         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75])),
3208         ...         lon=("X", np.array([10.0, 10.25, 10.5, 10.75])),
3209         ...     ),
3210         ... )
3211         >>> da
3212         <xarray.DataArray (Y: 4, X: 4)>
3213         array([[ 0.,  4.,  2.,  9.],
3214                [nan, nan, nan, nan],
3215                [nan,  4.,  2.,  0.],
3216                [ 3.,  1.,  0.,  0.]])
3217         Coordinates:
3218             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75
3219             lon      (X) float64 10.0 10.25 10.5 10.75
3220         Dimensions without coordinates: Y, X
3221 
3222         >>> da.dropna(dim="Y", how="any")
3223         <xarray.DataArray (Y: 2, X: 4)>
3224         array([[0., 4., 2., 9.],
3225                [3., 1., 0., 0.]])
3226         Coordinates:
3227             lat      (Y) float64 -20.0 -20.75
3228             lon      (X) float64 10.0 10.25 10.5 10.75
3229         Dimensions without coordinates: Y, X
3230 
3231         Drop values only if all values along the dimension are NaN:
3232 
3233         >>> da.dropna(dim="Y", how="all")
3234         <xarray.DataArray (Y: 3, X: 4)>
3235         array([[ 0.,  4.,  2.,  9.],
3236                [nan,  4.,  2.,  0.],
3237                [ 3.,  1.,  0.,  0.]])
3238         Coordinates:
3239             lat      (Y) float64 -20.0 -20.5 -20.75
3240             lon      (X) float64 10.0 10.25 10.5 10.75
3241         Dimensions without coordinates: Y, X
3242         """
3243         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
3244         return self._from_temp_dataset(ds)
3245 
3246     def fillna(self: T_DataArray, value: Any) -> T_DataArray:
3247         """Fill missing values in this object.
3248 
3249         This operation follows the normal broadcasting and alignment rules that
3250         xarray uses for binary arithmetic, except the result is aligned to this
3251         object (``join='left'``) instead of aligned to the intersection of
3252         index coordinates (``join='inner'``).
3253 
3254         Parameters
3255         ----------
3256         value : scalar, ndarray or DataArray
3257             Used to fill all matching missing values in this array. If the
3258             argument is a DataArray, it is first aligned with (reindexed to)
3259             this array.
3260 
3261         Returns
3262         -------
3263         filled : DataArray
3264 
3265         Examples
3266         --------
3267         >>> da = xr.DataArray(
3268         ...     np.array([1, 4, np.nan, 0, 3, np.nan]),
3269         ...     dims="Z",
3270         ...     coords=dict(
3271         ...         Z=("Z", np.arange(6)),
3272         ...         height=("Z", np.array([0, 10, 20, 30, 40, 50])),
3273         ...     ),
3274         ... )
3275         >>> da
3276         <xarray.DataArray (Z: 6)>
3277         array([ 1.,  4., nan,  0.,  3., nan])
3278         Coordinates:
3279           * Z        (Z) int64 0 1 2 3 4 5
3280             height   (Z) int64 0 10 20 30 40 50
3281 
3282         Fill all NaN values with 0:
3283 
3284         >>> da.fillna(0)
3285         <xarray.DataArray (Z: 6)>
3286         array([1., 4., 0., 0., 3., 0.])
3287         Coordinates:
3288           * Z        (Z) int64 0 1 2 3 4 5
3289             height   (Z) int64 0 10 20 30 40 50
3290 
3291         Fill NaN values with corresponding values in array:
3292 
3293         >>> da.fillna(np.array([2, 9, 4, 2, 8, 9]))
3294         <xarray.DataArray (Z: 6)>
3295         array([1., 4., 4., 0., 3., 9.])
3296         Coordinates:
3297           * Z        (Z) int64 0 1 2 3 4 5
3298             height   (Z) int64 0 10 20 30 40 50
3299         """
3300         if utils.is_dict_like(value):
3301             raise TypeError(
3302                 "cannot provide fill value as a dictionary with "
3303                 "fillna on a DataArray"
3304             )
3305         out = ops.fillna(self, value)
3306         return out
3307 
3308     def interpolate_na(
3309         self: T_DataArray,
3310         dim: Hashable | None = None,
3311         method: InterpOptions = "linear",
3312         limit: int | None = None,
3313         use_coordinate: bool | str = True,
3314         max_gap: (
3315             None
3316             | int
3317             | float
3318             | str
3319             | pd.Timedelta
3320             | np.timedelta64
3321             | datetime.timedelta
3322         ) = None,
3323         keep_attrs: bool | None = None,
3324         **kwargs: Any,
3325     ) -> T_DataArray:
3326         """Fill in NaNs by interpolating according to different methods.
3327 
3328         Parameters
3329         ----------
3330         dim : Hashable or None, optional
3331             Specifies the dimension along which to interpolate.
3332         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3333             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3334             String indicating which method to use for interpolation:
3335 
3336             - 'linear': linear interpolation. Additional keyword
3337               arguments are passed to :py:func:`numpy.interp`
3338             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3339               are passed to :py:func:`scipy.interpolate.interp1d`. If
3340               ``method='polynomial'``, the ``order`` keyword argument must also be
3341               provided.
3342             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3343               respective :py:class:`scipy.interpolate` classes.
3344 
3345         use_coordinate : bool or str, default: True
3346             Specifies which index to use as the x values in the interpolation
3347             formulated as `y = f(x)`. If False, values are treated as if
3348             equally-spaced along ``dim``. If True, the IndexVariable `dim` is
3349             used. If ``use_coordinate`` is a string, it specifies the name of a
3350             coordinate variable to use as the index.
3351         limit : int or None, default: None
3352             Maximum number of consecutive NaNs to fill. Must be greater than 0
3353             or None for no limit. This filling is done regardless of the size of
3354             the gap in the data. To only interpolate over gaps less than a given length,
3355             see ``max_gap``.
3356         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
3357             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
3358             Use None for no limit. When interpolating along a datetime64 dimension
3359             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
3360 
3361             - a string that is valid input for pandas.to_timedelta
3362             - a :py:class:`numpy.timedelta64` object
3363             - a :py:class:`pandas.Timedelta` object
3364             - a :py:class:`datetime.timedelta` object
3365 
3366             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
3367             dimensions has not been implemented yet. Gap length is defined as the difference
3368             between coordinate values at the first data point after a gap and the last value
3369             before a gap. For gaps at the beginning (end), gap length is defined as the difference
3370             between coordinate values at the first (last) valid data point and the first (last) NaN.
3371             For example, consider::
3372 
3373                 <xarray.DataArray (x: 9)>
3374                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
3375                 Coordinates:
3376                   * x        (x) int64 0 1 2 3 4 5 6 7 8
3377 
3378             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
3379         keep_attrs : bool or None, default: None
3380             If True, the dataarray's attributes (`attrs`) will be copied from
3381             the original object to the new one.  If False, the new
3382             object will be returned without attributes.
3383         **kwargs : dict, optional
3384             parameters passed verbatim to the underlying interpolation function
3385 
3386         Returns
3387         -------
3388         interpolated: DataArray
3389             Filled in DataArray.
3390 
3391         See Also
3392         --------
3393         numpy.interp
3394         scipy.interpolate
3395 
3396         Examples
3397         --------
3398         >>> da = xr.DataArray(
3399         ...     [np.nan, 2, 3, np.nan, 0], dims="x", coords={"x": [0, 1, 2, 3, 4]}
3400         ... )
3401         >>> da
3402         <xarray.DataArray (x: 5)>
3403         array([nan,  2.,  3., nan,  0.])
3404         Coordinates:
3405           * x        (x) int64 0 1 2 3 4
3406 
3407         >>> da.interpolate_na(dim="x", method="linear")
3408         <xarray.DataArray (x: 5)>
3409         array([nan, 2. , 3. , 1.5, 0. ])
3410         Coordinates:
3411           * x        (x) int64 0 1 2 3 4
3412 
3413         >>> da.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
3414         <xarray.DataArray (x: 5)>
3415         array([1. , 2. , 3. , 1.5, 0. ])
3416         Coordinates:
3417           * x        (x) int64 0 1 2 3 4
3418         """
3419         from xarray.core.missing import interp_na
3420 
3421         return interp_na(
3422             self,
3423             dim=dim,
3424             method=method,
3425             limit=limit,
3426             use_coordinate=use_coordinate,
3427             max_gap=max_gap,
3428             keep_attrs=keep_attrs,
3429             **kwargs,
3430         )
3431 
3432     def ffill(
3433         self: T_DataArray, dim: Hashable, limit: int | None = None
3434     ) -> T_DataArray:
3435         """Fill NaN values by propagating values forward
3436 
3437         *Requires bottleneck.*
3438 
3439         Parameters
3440         ----------
3441         dim : Hashable
3442             Specifies the dimension along which to propagate values when
3443             filling.
3444         limit : int or None, default: None
3445             The maximum number of consecutive NaN values to forward fill. In
3446             other words, if there is a gap with more than this number of
3447             consecutive NaNs, it will only be partially filled. Must be greater
3448             than 0 or None for no limit. Must be None or greater than or equal
3449             to axis length if filling along chunked axes (dimensions).
3450 
3451         Returns
3452         -------
3453         filled : DataArray
3454 
3455         Examples
3456         --------
3457         >>> temperature = np.array(
3458         ...     [
3459         ...         [np.nan, 1, 3],
3460         ...         [0, np.nan, 5],
3461         ...         [5, np.nan, np.nan],
3462         ...         [3, np.nan, np.nan],
3463         ...         [0, 2, 0],
3464         ...     ]
3465         ... )
3466         >>> da = xr.DataArray(
3467         ...     data=temperature,
3468         ...     dims=["Y", "X"],
3469         ...     coords=dict(
3470         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75, -21.0])),
3471         ...         lon=("X", np.array([10.0, 10.25, 10.5])),
3472         ...     ),
3473         ... )
3474         >>> da
3475         <xarray.DataArray (Y: 5, X: 3)>
3476         array([[nan,  1.,  3.],
3477                [ 0., nan,  5.],
3478                [ 5., nan, nan],
3479                [ 3., nan, nan],
3480                [ 0.,  2.,  0.]])
3481         Coordinates:
3482             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3483             lon      (X) float64 10.0 10.25 10.5
3484         Dimensions without coordinates: Y, X
3485 
3486         Fill all NaN values:
3487 
3488         >>> da.ffill(dim="Y", limit=None)
3489         <xarray.DataArray (Y: 5, X: 3)>
3490         array([[nan,  1.,  3.],
3491                [ 0.,  1.,  5.],
3492                [ 5.,  1.,  5.],
3493                [ 3.,  1.,  5.],
3494                [ 0.,  2.,  0.]])
3495         Coordinates:
3496             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3497             lon      (X) float64 10.0 10.25 10.5
3498         Dimensions without coordinates: Y, X
3499 
3500         Fill only the first of consecutive NaN values:
3501 
3502         >>> da.ffill(dim="Y", limit=1)
3503         <xarray.DataArray (Y: 5, X: 3)>
3504         array([[nan,  1.,  3.],
3505                [ 0.,  1.,  5.],
3506                [ 5., nan,  5.],
3507                [ 3., nan, nan],
3508                [ 0.,  2.,  0.]])
3509         Coordinates:
3510             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3511             lon      (X) float64 10.0 10.25 10.5
3512         Dimensions without coordinates: Y, X
3513         """
3514         from xarray.core.missing import ffill
3515 
3516         return ffill(self, dim, limit=limit)
3517 
3518     def bfill(
3519         self: T_DataArray, dim: Hashable, limit: int | None = None
3520     ) -> T_DataArray:
3521         """Fill NaN values by propagating values backward
3522 
3523         *Requires bottleneck.*
3524 
3525         Parameters
3526         ----------
3527         dim : str
3528             Specifies the dimension along which to propagate values when
3529             filling.
3530         limit : int or None, default: None
3531             The maximum number of consecutive NaN values to backward fill. In
3532             other words, if there is a gap with more than this number of
3533             consecutive NaNs, it will only be partially filled. Must be greater
3534             than 0 or None for no limit. Must be None or greater than or equal
3535             to axis length if filling along chunked axes (dimensions).
3536 
3537         Returns
3538         -------
3539         filled : DataArray
3540 
3541         Examples
3542         --------
3543         >>> temperature = np.array(
3544         ...     [
3545         ...         [0, 1, 3],
3546         ...         [0, np.nan, 5],
3547         ...         [5, np.nan, np.nan],
3548         ...         [3, np.nan, np.nan],
3549         ...         [np.nan, 2, 0],
3550         ...     ]
3551         ... )
3552         >>> da = xr.DataArray(
3553         ...     data=temperature,
3554         ...     dims=["Y", "X"],
3555         ...     coords=dict(
3556         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75, -21.0])),
3557         ...         lon=("X", np.array([10.0, 10.25, 10.5])),
3558         ...     ),
3559         ... )
3560         >>> da
3561         <xarray.DataArray (Y: 5, X: 3)>
3562         array([[ 0.,  1.,  3.],
3563                [ 0., nan,  5.],
3564                [ 5., nan, nan],
3565                [ 3., nan, nan],
3566                [nan,  2.,  0.]])
3567         Coordinates:
3568             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3569             lon      (X) float64 10.0 10.25 10.5
3570         Dimensions without coordinates: Y, X
3571 
3572         Fill all NaN values:
3573 
3574         >>> da.bfill(dim="Y", limit=None)
3575         <xarray.DataArray (Y: 5, X: 3)>
3576         array([[ 0.,  1.,  3.],
3577                [ 0.,  2.,  5.],
3578                [ 5.,  2.,  0.],
3579                [ 3.,  2.,  0.],
3580                [nan,  2.,  0.]])
3581         Coordinates:
3582             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3583             lon      (X) float64 10.0 10.25 10.5
3584         Dimensions without coordinates: Y, X
3585 
3586         Fill only the first of consecutive NaN values:
3587 
3588         >>> da.bfill(dim="Y", limit=1)
3589         <xarray.DataArray (Y: 5, X: 3)>
3590         array([[ 0.,  1.,  3.],
3591                [ 0., nan,  5.],
3592                [ 5., nan, nan],
3593                [ 3.,  2.,  0.],
3594                [nan,  2.,  0.]])
3595         Coordinates:
3596             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3597             lon      (X) float64 10.0 10.25 10.5
3598         Dimensions without coordinates: Y, X
3599         """
3600         from xarray.core.missing import bfill
3601 
3602         return bfill(self, dim, limit=limit)
3603 
3604     def combine_first(self: T_DataArray, other: T_DataArray) -> T_DataArray:
3605         """Combine two DataArray objects, with union of coordinates.
3606 
3607         This operation follows the normal broadcasting and alignment rules of
3608         ``join='outer'``.  Default to non-null values of array calling the
3609         method.  Use np.nan to fill in vacant cells after alignment.
3610 
3611         Parameters
3612         ----------
3613         other : DataArray
3614             Used to fill all matching missing values in this array.
3615 
3616         Returns
3617         -------
3618         DataArray
3619         """
3620         return ops.fillna(self, other, join="outer")
3621 
3622     def reduce(
3623         self: T_DataArray,
3624         func: Callable[..., Any],
3625         dim: Dims = None,
3626         *,
3627         axis: int | Sequence[int] | None = None,
3628         keep_attrs: bool | None = None,
3629         keepdims: bool = False,
3630         **kwargs: Any,
3631     ) -> T_DataArray:
3632         """Reduce this array by applying `func` along some dimension(s).
3633 
3634         Parameters
3635         ----------
3636         func : callable
3637             Function which can be called in the form
3638             `f(x, axis=axis, **kwargs)` to return the result of reducing an
3639             np.ndarray over an integer valued axis.
3640         dim : "...", str, Iterable of Hashable or None, optional
3641             Dimension(s) over which to apply `func`. By default `func` is
3642             applied over all dimensions.
3643         axis : int or sequence of int, optional
3644             Axis(es) over which to repeatedly apply `func`. Only one of the
3645             'dim' and 'axis' arguments can be supplied. If neither are
3646             supplied, then the reduction is calculated over the flattened array
3647             (by calling `f(x)` without an axis argument).
3648         keep_attrs : bool or None, optional
3649             If True, the variable's attributes (`attrs`) will be copied from
3650             the original object to the new one.  If False (default), the new
3651             object will be returned without attributes.
3652         keepdims : bool, default: False
3653             If True, the dimensions which are reduced are left in the result
3654             as dimensions of size one. Coordinates that use these dimensions
3655             are removed.
3656         **kwargs : dict
3657             Additional keyword arguments passed on to `func`.
3658 
3659         Returns
3660         -------
3661         reduced : DataArray
3662             DataArray with this object's array replaced with an array with
3663             summarized data and the indicated dimension(s) removed.
3664         """
3665 
3666         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
3667         return self._replace_maybe_drop_dims(var)
3668 
3669     def to_pandas(self) -> DataArray | pd.Series | pd.DataFrame:
3670         """Convert this array into a pandas object with the same shape.
3671 
3672         The type of the returned object depends on the number of DataArray
3673         dimensions:
3674 
3675         * 0D -> `xarray.DataArray`
3676         * 1D -> `pandas.Series`
3677         * 2D -> `pandas.DataFrame`
3678 
3679         Only works for arrays with 2 or fewer dimensions.
3680 
3681         The DataArray constructor performs the inverse transformation.
3682 
3683         Returns
3684         -------
3685         result : DataArray | Series | DataFrame
3686             DataArray, pandas Series or pandas DataFrame.
3687         """
3688         # TODO: consolidate the info about pandas constructors and the
3689         # attributes that correspond to their indexes into a separate module?
3690         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
3691         try:
3692             constructor = constructors[self.ndim]
3693         except KeyError:
3694             raise ValueError(
3695                 f"Cannot convert arrays with {self.ndim} dimensions into "
3696                 "pandas objects. Requires 2 or fewer dimensions."
3697             )
3698         indexes = [self.get_index(dim) for dim in self.dims]
3699         return constructor(self.values, *indexes)
3700 
3701     def to_dataframe(
3702         self, name: Hashable | None = None, dim_order: Sequence[Hashable] | None = None
3703     ) -> pd.DataFrame:
3704         """Convert this array and its coordinates into a tidy pandas.DataFrame.
3705 
3706         The DataFrame is indexed by the Cartesian product of index coordinates
3707         (in the form of a :py:class:`pandas.MultiIndex`). Other coordinates are
3708         included as columns in the DataFrame.
3709 
3710         For 1D and 2D DataArrays, see also :py:func:`DataArray.to_pandas` which
3711         doesn't rely on a MultiIndex to build the DataFrame.
3712 
3713         Parameters
3714         ----------
3715         name: Hashable or None, optional
3716             Name to give to this array (required if unnamed).
3717         dim_order: Sequence of Hashable or None, optional
3718             Hierarchical dimension order for the resulting dataframe.
3719             Array content is transposed to this order and then written out as flat
3720             vectors in contiguous order, so the last dimension in this list
3721             will be contiguous in the resulting DataFrame. This has a major
3722             influence on which operations are efficient on the resulting
3723             dataframe.
3724 
3725             If provided, must include all dimensions of this DataArray. By default,
3726             dimensions are sorted according to the DataArray dimensions order.
3727 
3728         Returns
3729         -------
3730         result: DataFrame
3731             DataArray as a pandas DataFrame.
3732 
3733         See also
3734         --------
3735         DataArray.to_pandas
3736         DataArray.to_series
3737         """
3738         if name is None:
3739             name = self.name
3740         if name is None:
3741             raise ValueError(
3742                 "cannot convert an unnamed DataArray to a "
3743                 "DataFrame: use the ``name`` parameter"
3744             )
3745         if self.ndim == 0:
3746             raise ValueError("cannot convert a scalar to a DataFrame")
3747 
3748         # By using a unique name, we can convert a DataArray into a DataFrame
3749         # even if it shares a name with one of its coordinates.
3750         # I would normally use unique_name = object() but that results in a
3751         # dataframe with columns in the wrong order, for reasons I have not
3752         # been able to debug (possibly a pandas bug?).
3753         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
3754         ds = self._to_dataset_whole(name=unique_name)
3755 
3756         if dim_order is None:
3757             ordered_dims = dict(zip(self.dims, self.shape))
3758         else:
3759             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)
3760 
3761         df = ds._to_dataframe(ordered_dims)
3762         df.columns = [name if c == unique_name else c for c in df.columns]
3763         return df
3764 
3765     def to_series(self) -> pd.Series:
3766         """Convert this array into a pandas.Series.
3767 
3768         The Series is indexed by the Cartesian product of index coordinates
3769         (in the form of a :py:class:`pandas.MultiIndex`).
3770 
3771         Returns
3772         -------
3773         result : Series
3774             DataArray as a pandas Series.
3775 
3776         See also
3777         --------
3778         DataArray.to_pandas
3779         DataArray.to_dataframe
3780         """
3781         index = self.coords.to_index()
3782         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
3783 
3784     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
3785         """Convert this array into a numpy.ma.MaskedArray
3786 
3787         Parameters
3788         ----------
3789         copy : bool, default: True
3790             If True make a copy of the array in the result. If False,
3791             a MaskedArray view of DataArray.values is returned.
3792 
3793         Returns
3794         -------
3795         result : MaskedArray
3796             Masked where invalid values (nan or inf) occur.
3797         """
3798         values = self.to_numpy()  # only compute lazy arrays once
3799         isnull = pd.isnull(values)
3800         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
3801 
3802     # path=None writes to bytes
3803     @overload
3804     def to_netcdf(
3805         self,
3806         path: None = None,
3807         mode: Literal["w", "a"] = "w",
3808         format: T_NetcdfTypes | None = None,
3809         group: str | None = None,
3810         engine: T_NetcdfEngine | None = None,
3811         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3812         unlimited_dims: Iterable[Hashable] | None = None,
3813         compute: bool = True,
3814         invalid_netcdf: bool = False,
3815     ) -> bytes:
3816         ...
3817 
3818     # default return None
3819     @overload
3820     def to_netcdf(
3821         self,
3822         path: str | PathLike,
3823         mode: Literal["w", "a"] = "w",
3824         format: T_NetcdfTypes | None = None,
3825         group: str | None = None,
3826         engine: T_NetcdfEngine | None = None,
3827         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3828         unlimited_dims: Iterable[Hashable] | None = None,
3829         compute: Literal[True] = True,
3830         invalid_netcdf: bool = False,
3831     ) -> None:
3832         ...
3833 
3834     # compute=False returns dask.Delayed
3835     @overload
3836     def to_netcdf(
3837         self,
3838         path: str | PathLike,
3839         mode: Literal["w", "a"] = "w",
3840         format: T_NetcdfTypes | None = None,
3841         group: str | None = None,
3842         engine: T_NetcdfEngine | None = None,
3843         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3844         unlimited_dims: Iterable[Hashable] | None = None,
3845         *,
3846         compute: Literal[False],
3847         invalid_netcdf: bool = False,
3848     ) -> Delayed:
3849         ...
3850 
3851     def to_netcdf(
3852         self,
3853         path: str | PathLike | None = None,
3854         mode: Literal["w", "a"] = "w",
3855         format: T_NetcdfTypes | None = None,
3856         group: str | None = None,
3857         engine: T_NetcdfEngine | None = None,
3858         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3859         unlimited_dims: Iterable[Hashable] | None = None,
3860         compute: bool = True,
3861         invalid_netcdf: bool = False,
3862     ) -> bytes | Delayed | None:
3863         """Write DataArray contents to a netCDF file.
3864 
3865         Parameters
3866         ----------
3867         path : str, path-like or None, optional
3868             Path to which to save this dataset. File-like objects are only
3869             supported by the scipy engine. If no path is provided, this
3870             function returns the resulting netCDF file as bytes; in this case,
3871             we need to use scipy, which does not support netCDF version 4 (the
3872             default format becomes NETCDF3_64BIT).
3873         mode : {"w", "a"}, default: "w"
3874             Write ('w') or append ('a') mode. If mode='w', any existing file at
3875             this location will be overwritten. If mode='a', existing variables
3876             will be overwritten.
3877         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
3878                   "NETCDF3_CLASSIC"}, optional
3879             File format for the resulting netCDF file:
3880 
3881             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
3882               features.
3883             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
3884               netCDF 3 compatible API features.
3885             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
3886               which fully supports 2+ GB files, but is only compatible with
3887               clients linked against netCDF version 3.6.0 or later.
3888             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
3889               handle 2+ GB files very well.
3890 
3891             All formats are supported by the netCDF4-python library.
3892             scipy.io.netcdf only supports the last two formats.
3893 
3894             The default format is NETCDF4 if you are saving a file to disk and
3895             have the netCDF4-python library available. Otherwise, xarray falls
3896             back to using scipy to write netCDF files and defaults to the
3897             NETCDF3_64BIT format (scipy does not support netCDF4).
3898         group : str, optional
3899             Path to the netCDF4 group in the given file to open (only works for
3900             format='NETCDF4'). The group(s) will be created if necessary.
3901         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
3902             Engine to use when writing netCDF files. If not provided, the
3903             default engine is chosen based on available dependencies, with a
3904             preference for 'netcdf4' if writing to a file on disk.
3905         encoding : dict, optional
3906             Nested dictionary with variable names as keys and dictionaries of
3907             variable specific encodings as values, e.g.,
3908             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
3909             "zlib": True}, ...}``
3910 
3911             The `h5netcdf` engine supports both the NetCDF4-style compression
3912             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
3913             ones ``{"compression": "gzip", "compression_opts": 9}``.
3914             This allows using any compression plugin installed in the HDF5
3915             library, e.g. LZF.
3916 
3917         unlimited_dims : iterable of Hashable, optional
3918             Dimension(s) that should be serialized as unlimited dimensions.
3919             By default, no dimensions are treated as unlimited dimensions.
3920             Note that unlimited_dims may also be set via
3921             ``dataset.encoding["unlimited_dims"]``.
3922         compute: bool, default: True
3923             If true compute immediately, otherwise return a
3924             ``dask.delayed.Delayed`` object that can be computed later.
3925         invalid_netcdf: bool, default: False
3926             Only valid along with ``engine="h5netcdf"``. If True, allow writing
3927             hdf5 files which are invalid netcdf as described in
3928             https://github.com/h5netcdf/h5netcdf.
3929 
3930         Returns
3931         -------
3932         store: bytes or Delayed or None
3933             * ``bytes`` if path is None
3934             * ``dask.delayed.Delayed`` if compute is False
3935             * None otherwise
3936 
3937         Notes
3938         -----
3939         Only xarray.Dataset objects can be written to netCDF files, so
3940         the xarray.DataArray is converted to a xarray.Dataset object
3941         containing a single variable. If the DataArray has no name, or if the
3942         name is the same as a coordinate name, then it is given the name
3943         ``"__xarray_dataarray_variable__"``.
3944 
3945         See Also
3946         --------
3947         Dataset.to_netcdf
3948         """
3949         from xarray.backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_netcdf
3950 
3951         if self.name is None:
3952             # If no name is set then use a generic xarray name
3953             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3954         elif self.name in self.coords or self.name in self.dims:
3955             # The name is the same as one of the coords names, which netCDF
3956             # doesn't support, so rename it but keep track of the old name
3957             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3958             dataset.attrs[DATAARRAY_NAME] = self.name
3959         else:
3960             # No problems with the name - so we're fine!
3961             dataset = self.to_dataset()
3962 
3963         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
3964             dataset,
3965             path,
3966             mode=mode,
3967             format=format,
3968             group=group,
3969             engine=engine,
3970             encoding=encoding,
3971             unlimited_dims=unlimited_dims,
3972             compute=compute,
3973             multifile=False,
3974             invalid_netcdf=invalid_netcdf,
3975         )
3976 
3977     # compute=True (default) returns ZarrStore
3978     @overload
3979     def to_zarr(
3980         self,
3981         store: MutableMapping | str | PathLike[str] | None = None,
3982         chunk_store: MutableMapping | str | PathLike | None = None,
3983         mode: Literal["w", "w-", "a", "r+", None] = None,
3984         synchronizer=None,
3985         group: str | None = None,
3986         encoding: Mapping | None = None,
3987         compute: Literal[True] = True,
3988         consolidated: bool | None = None,
3989         append_dim: Hashable | None = None,
3990         region: Mapping[str, slice] | None = None,
3991         safe_chunks: bool = True,
3992         storage_options: dict[str, str] | None = None,
3993         zarr_version: int | None = None,
3994     ) -> ZarrStore:
3995         ...
3996 
3997     # compute=False returns dask.Delayed
3998     @overload
3999     def to_zarr(
4000         self,
4001         store: MutableMapping | str | PathLike[str] | None = None,
4002         chunk_store: MutableMapping | str | PathLike | None = None,
4003         mode: Literal["w", "w-", "a", "r+", None] = None,
4004         synchronizer=None,
4005         group: str | None = None,
4006         encoding: Mapping | None = None,
4007         *,
4008         compute: Literal[False],
4009         consolidated: bool | None = None,
4010         append_dim: Hashable | None = None,
4011         region: Mapping[str, slice] | None = None,
4012         safe_chunks: bool = True,
4013         storage_options: dict[str, str] | None = None,
4014         zarr_version: int | None = None,
4015     ) -> Delayed:
4016         ...
4017 
4018     def to_zarr(
4019         self,
4020         store: MutableMapping | str | PathLike[str] | None = None,
4021         chunk_store: MutableMapping | str | PathLike | None = None,
4022         mode: Literal["w", "w-", "a", "r+", None] = None,
4023         synchronizer=None,
4024         group: str | None = None,
4025         encoding: Mapping | None = None,
4026         compute: bool = True,
4027         consolidated: bool | None = None,
4028         append_dim: Hashable | None = None,
4029         region: Mapping[str, slice] | None = None,
4030         safe_chunks: bool = True,
4031         storage_options: dict[str, str] | None = None,
4032         zarr_version: int | None = None,
4033     ) -> ZarrStore | Delayed:
4034         """Write DataArray contents to a Zarr store
4035 
4036         Zarr chunks are determined in the following way:
4037 
4038         - From the ``chunks`` attribute in each variable's ``encoding``
4039           (can be set via `DataArray.chunk`).
4040         - If the variable is a Dask array, from the dask chunks
4041         - If neither Dask chunks nor encoding chunks are present, chunks will
4042           be determined automatically by Zarr
4043         - If both Dask chunks and encoding chunks are present, encoding chunks
4044           will be used, provided that there is a many-to-one relationship between
4045           encoding chunks and dask chunks (i.e. Dask chunks are bigger than and
4046           evenly divide encoding chunks); otherwise raise a ``ValueError``.
4047           This restriction ensures that no synchronization / locks are required
4048           when writing. To disable this restriction, use ``safe_chunks=False``.
4049 
4050         Parameters
4051         ----------
4052         store : MutableMapping, str or path-like, optional
4053             Store or path to directory in local or remote file system.
4054         chunk_store : MutableMapping, str or path-like, optional
4055             Store or path to directory in local or remote file system only for Zarr
4056             array chunks. Requires zarr-python v2.4.0 or later.
4057         mode : {"w", "w-", "a", "r+", None}, optional
4058             Persistence mode: "w" means create (overwrite if exists);
4059             "w-" means create (fail if exists);
4060             "a" means override existing variables (create if does not exist);
4061             "r+" means modify existing array *values* only (raise an error if
4062             any metadata or shapes would change).
4063             The default mode is "a" if ``append_dim`` is set. Otherwise, it is
4064             "r+" if ``region`` is set and ``w-`` otherwise.
4065         synchronizer : object, optional
4066             Zarr array synchronizer.
4067         group : str, optional
4068             Group path. (a.k.a. `path` in zarr terminology.)
4069         encoding : dict, optional
4070             Nested dictionary with variable names as keys and dictionaries of
4071             variable specific encodings as values, e.g.,
4072             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,}, ...}``
4073         compute : bool, default: True
4074             If True write array data immediately, otherwise return a
4075             ``dask.delayed.Delayed`` object that can be computed to write
4076             array data later. Metadata is always updated eagerly.
4077         consolidated : bool, optional
4078             If True, apply zarr's `consolidate_metadata` function to the store
4079             after writing metadata and read existing stores with consolidated
4080             metadata; if False, do not. The default (`consolidated=None`) means
4081             write consolidated metadata and attempt to read consolidated
4082             metadata for existing stores (falling back to non-consolidated).
4083 
4084             When the experimental ``zarr_version=3``, ``consolidated`` must be
4085             either be ``None`` or ``False``.
4086         append_dim : hashable, optional
4087             If set, the dimension along which the data will be appended. All
4088             other dimensions on overridden variables must remain the same size.
4089         region : dict, optional
4090             Optional mapping from dimension names to integer slices along
4091             dataarray dimensions to indicate the region of existing zarr array(s)
4092             in which to write this datarray's data. For example,
4093             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate
4094             that values should be written to the region ``0:1000`` along ``x``
4095             and ``10000:11000`` along ``y``.
4096 
4097             Two restrictions apply to the use of ``region``:
4098 
4099             - If ``region`` is set, _all_ variables in a dataarray must have at
4100               least one dimension in common with the region. Other variables
4101               should be written in a separate call to ``to_zarr()``.
4102             - Dimensions cannot be included in both ``region`` and
4103               ``append_dim`` at the same time. To create empty arrays to fill
4104               in with ``region``, use a separate call to ``to_zarr()`` with
4105               ``compute=False``. See "Appending to existing Zarr stores" in
4106               the reference documentation for full details.
4107         safe_chunks : bool, default: True
4108             If True, only allow writes to when there is a many-to-one relationship
4109             between Zarr chunks (specified in encoding) and Dask chunks.
4110             Set False to override this restriction; however, data may become corrupted
4111             if Zarr arrays are written in parallel. This option may be useful in combination
4112             with ``compute=False`` to initialize a Zarr store from an existing
4113             DataArray with arbitrary chunk structure.
4114         storage_options : dict, optional
4115             Any additional parameters for the storage backend (ignored for local
4116             paths).
4117         zarr_version : int or None, optional
4118             The desired zarr spec version to target (currently 2 or 3). The
4119             default of None will attempt to determine the zarr version from
4120             ``store`` when possible, otherwise defaulting to 2.
4121 
4122         Returns
4123         -------
4124             * ``dask.delayed.Delayed`` if compute is False
4125             * ZarrStore otherwise
4126 
4127         References
4128         ----------
4129         https://zarr.readthedocs.io/
4130 
4131         Notes
4132         -----
4133         Zarr chunking behavior:
4134             If chunks are found in the encoding argument or attribute
4135             corresponding to any DataArray, those chunks are used.
4136             If a DataArray is a dask array, it is written with those chunks.
4137             If not other chunks are found, Zarr uses its own heuristics to
4138             choose automatic chunk sizes.
4139 
4140         encoding:
4141             The encoding attribute (if exists) of the DataArray(s) will be
4142             used. Override any existing encodings by providing the ``encoding`` kwarg.
4143 
4144         See Also
4145         --------
4146         Dataset.to_zarr
4147         :ref:`io.zarr`
4148             The I/O user guide, with more details and examples.
4149         """
4150         from xarray.backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_zarr
4151 
4152         if self.name is None:
4153             # If no name is set then use a generic xarray name
4154             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
4155         elif self.name in self.coords or self.name in self.dims:
4156             # The name is the same as one of the coords names, which the netCDF data model
4157             # does not support, so rename it but keep track of the old name
4158             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
4159             dataset.attrs[DATAARRAY_NAME] = self.name
4160         else:
4161             # No problems with the name - so we're fine!
4162             dataset = self.to_dataset()
4163 
4164         return to_zarr(  # type: ignore[call-overload,misc]
4165             dataset,
4166             store=store,
4167             chunk_store=chunk_store,
4168             mode=mode,
4169             synchronizer=synchronizer,
4170             group=group,
4171             encoding=encoding,
4172             compute=compute,
4173             consolidated=consolidated,
4174             append_dim=append_dim,
4175             region=region,
4176             safe_chunks=safe_chunks,
4177             storage_options=storage_options,
4178             zarr_version=zarr_version,
4179         )
4180 
4181     def to_dict(
4182         self, data: bool | Literal["list", "array"] = "list", encoding: bool = False
4183     ) -> dict[str, Any]:
4184         """
4185         Convert this xarray.DataArray into a dictionary following xarray
4186         naming conventions.
4187 
4188         Converts all variables and attributes to native Python objects.
4189         Useful for converting to json. To avoid datetime incompatibility
4190         use decode_times=False kwarg in xarray.open_dataset.
4191 
4192         Parameters
4193         ----------
4194         data : bool or {"list", "array"}, default: "list"
4195             Whether to include the actual data in the dictionary. When set to
4196             False, returns just the schema. If set to "array", returns data as
4197             underlying array type. If set to "list" (or True for backwards
4198             compatibility), returns data in lists of Python data types. Note
4199             that for obtaining the "list" output efficiently, use
4200             `da.compute().to_dict(data="list")`.
4201 
4202         encoding : bool, default: False
4203             Whether to include the Dataset's encoding in the dictionary.
4204 
4205         Returns
4206         -------
4207         dict: dict
4208 
4209         See Also
4210         --------
4211         DataArray.from_dict
4212         Dataset.to_dict
4213         """
4214         d = self.variable.to_dict(data=data)
4215         d.update({"coords": {}, "name": self.name})
4216         for k, coord in self.coords.items():
4217             d["coords"][k] = coord.variable.to_dict(data=data)
4218         if encoding:
4219             d["encoding"] = dict(self.encoding)
4220         return d
4221 
4222     @classmethod
4223     def from_dict(cls: type[T_DataArray], d: Mapping[str, Any]) -> T_DataArray:
4224         """Convert a dictionary into an xarray.DataArray
4225 
4226         Parameters
4227         ----------
4228         d : dict
4229             Mapping with a minimum structure of {"dims": [...], "data": [...]}
4230 
4231         Returns
4232         -------
4233         obj : xarray.DataArray
4234 
4235         See Also
4236         --------
4237         DataArray.to_dict
4238         Dataset.from_dict
4239 
4240         Examples
4241         --------
4242         >>> d = {"dims": "t", "data": [1, 2, 3]}
4243         >>> da = xr.DataArray.from_dict(d)
4244         >>> da
4245         <xarray.DataArray (t: 3)>
4246         array([1, 2, 3])
4247         Dimensions without coordinates: t
4248 
4249         >>> d = {
4250         ...     "coords": {
4251         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
4252         ...     },
4253         ...     "attrs": {"title": "air temperature"},
4254         ...     "dims": "t",
4255         ...     "data": [10, 20, 30],
4256         ...     "name": "a",
4257         ... }
4258         >>> da = xr.DataArray.from_dict(d)
4259         >>> da
4260         <xarray.DataArray 'a' (t: 3)>
4261         array([10, 20, 30])
4262         Coordinates:
4263           * t        (t) int64 0 1 2
4264         Attributes:
4265             title:    air temperature
4266         """
4267         coords = None
4268         if "coords" in d:
4269             try:
4270                 coords = {
4271                     k: (v["dims"], v["data"], v.get("attrs"))
4272                     for k, v in d["coords"].items()
4273                 }
4274             except KeyError as e:
4275                 raise ValueError(
4276                     "cannot convert dict when coords are missing the key "
4277                     "'{dims_data}'".format(dims_data=str(e.args[0]))
4278                 )
4279         try:
4280             data = d["data"]
4281         except KeyError:
4282             raise ValueError("cannot convert dict without the key 'data''")
4283         else:
4284             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
4285 
4286         obj.encoding.update(d.get("encoding", {}))
4287 
4288         return obj
4289 
4290     @classmethod
4291     def from_series(cls, series: pd.Series, sparse: bool = False) -> DataArray:
4292         """Convert a pandas.Series into an xarray.DataArray.
4293 
4294         If the series's index is a MultiIndex, it will be expanded into a
4295         tensor product of one-dimensional coordinates (filling in missing
4296         values with NaN). Thus this operation should be the inverse of the
4297         `to_series` method.
4298 
4299         Parameters
4300         ----------
4301         series : Series
4302             Pandas Series object to convert.
4303         sparse : bool, default: False
4304             If sparse=True, creates a sparse array instead of a dense NumPy array.
4305             Requires the pydata/sparse package.
4306 
4307         See Also
4308         --------
4309         DataArray.to_series
4310         Dataset.from_dataframe
4311         """
4312         temp_name = "__temporary_name"
4313         df = pd.DataFrame({temp_name: series})
4314         ds = Dataset.from_dataframe(df, sparse=sparse)
4315         result = cast(DataArray, ds[temp_name])
4316         result.name = series.name
4317         return result
4318 
4319     def to_cdms2(self) -> cdms2_Variable:
4320         """Convert this array into a cdms2.Variable"""
4321         from xarray.convert import to_cdms2
4322 
4323         return to_cdms2(self)
4324 
4325     @classmethod
4326     def from_cdms2(cls, variable: cdms2_Variable) -> DataArray:
4327         """Convert a cdms2.Variable into an xarray.DataArray"""
4328         from xarray.convert import from_cdms2
4329 
4330         return from_cdms2(variable)
4331 
4332     def to_iris(self) -> iris_Cube:
4333         """Convert this array into a iris.cube.Cube"""
4334         from xarray.convert import to_iris
4335 
4336         return to_iris(self)
4337 
4338     @classmethod
4339     def from_iris(cls, cube: iris_Cube) -> DataArray:
4340         """Convert a iris.cube.Cube into an xarray.DataArray"""
4341         from xarray.convert import from_iris
4342 
4343         return from_iris(cube)
4344 
4345     def _all_compat(self: T_DataArray, other: T_DataArray, compat_str: str) -> bool:
4346         """Helper function for equals, broadcast_equals, and identical"""
4347 
4348         def compat(x, y):
4349             return getattr(x.variable, compat_str)(y.variable)
4350 
4351         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
4352             self, other
4353         )
4354 
4355     def broadcast_equals(self: T_DataArray, other: T_DataArray) -> bool:
4356         """Two DataArrays are broadcast equal if they are equal after
4357         broadcasting them against each other such that they have the same
4358         dimensions.
4359 
4360         Parameters
4361         ----------
4362         other : DataArray
4363             DataArray to compare to.
4364 
4365         Returns
4366         ----------
4367         equal : bool
4368             True if the two DataArrays are broadcast equal.
4369 
4370         See Also
4371         --------
4372         DataArray.equals
4373         DataArray.identical
4374 
4375         Examples
4376         --------
4377         >>> a = xr.DataArray([1, 2], dims="X")
4378         >>> b = xr.DataArray([[1, 1], [2, 2]], dims=["X", "Y"])
4379         >>> a
4380         <xarray.DataArray (X: 2)>
4381         array([1, 2])
4382         Dimensions without coordinates: X
4383         >>> b
4384         <xarray.DataArray (X: 2, Y: 2)>
4385         array([[1, 1],
4386                [2, 2]])
4387         Dimensions without coordinates: X, Y
4388 
4389         .equals returns True if two DataArrays have the same values, dimensions, and coordinates. .broadcast_equals returns True if the results of broadcasting two DataArrays against eachother have the same values, dimensions, and coordinates.
4390 
4391         >>> a.equals(b)
4392         False
4393         >>> a2, b2 = xr.broadcast(a, b)
4394         >>> a2.equals(b2)
4395         True
4396         >>> a.broadcast_equals(b)
4397         True
4398         """
4399         try:
4400             return self._all_compat(other, "broadcast_equals")
4401         except (TypeError, AttributeError):
4402             return False
4403 
4404     def equals(self: T_DataArray, other: T_DataArray) -> bool:
4405         """True if two DataArrays have the same dimensions, coordinates and
4406         values; otherwise False.
4407 
4408         DataArrays can still be equal (like pandas objects) if they have NaN
4409         values in the same locations.
4410 
4411         This method is necessary because `v1 == v2` for ``DataArray``
4412         does element-wise comparisons (like numpy.ndarrays).
4413 
4414         Parameters
4415         ----------
4416         other : DataArray
4417             DataArray to compare to.
4418 
4419         Returns
4420         ----------
4421         equal : bool
4422             True if the two DataArrays are equal.
4423 
4424         See Also
4425         --------
4426         DataArray.broadcast_equals
4427         DataArray.identical
4428 
4429         Examples
4430         --------
4431         >>> a = xr.DataArray([1, 2, 3], dims="X")
4432         >>> b = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"))
4433         >>> c = xr.DataArray([1, 2, 3], dims="Y")
4434         >>> d = xr.DataArray([3, 2, 1], dims="X")
4435         >>> a
4436         <xarray.DataArray (X: 3)>
4437         array([1, 2, 3])
4438         Dimensions without coordinates: X
4439         >>> b
4440         <xarray.DataArray (X: 3)>
4441         array([1, 2, 3])
4442         Dimensions without coordinates: X
4443         Attributes:
4444             units:    m
4445         >>> c
4446         <xarray.DataArray (Y: 3)>
4447         array([1, 2, 3])
4448         Dimensions without coordinates: Y
4449         >>> d
4450         <xarray.DataArray (X: 3)>
4451         array([3, 2, 1])
4452         Dimensions without coordinates: X
4453 
4454         >>> a.equals(b)
4455         True
4456         >>> a.equals(c)
4457         False
4458         >>> a.equals(d)
4459         False
4460         """
4461         try:
4462             return self._all_compat(other, "equals")
4463         except (TypeError, AttributeError):
4464             return False
4465 
4466     def identical(self: T_DataArray, other: T_DataArray) -> bool:
4467         """Like equals, but also checks the array name and attributes, and
4468         attributes on all coordinates.
4469 
4470         Parameters
4471         ----------
4472         other : DataArray
4473             DataArray to compare to.
4474 
4475         Returns
4476         ----------
4477         equal : bool
4478             True if the two DataArrays are identical.
4479 
4480         See Also
4481         --------
4482         DataArray.broadcast_equals
4483         DataArray.equals
4484 
4485         Examples
4486         --------
4487         >>> a = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"), name="Width")
4488         >>> b = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"), name="Width")
4489         >>> c = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="ft"), name="Width")
4490         >>> a
4491         <xarray.DataArray 'Width' (X: 3)>
4492         array([1, 2, 3])
4493         Dimensions without coordinates: X
4494         Attributes:
4495             units:    m
4496         >>> b
4497         <xarray.DataArray 'Width' (X: 3)>
4498         array([1, 2, 3])
4499         Dimensions without coordinates: X
4500         Attributes:
4501             units:    m
4502         >>> c
4503         <xarray.DataArray 'Width' (X: 3)>
4504         array([1, 2, 3])
4505         Dimensions without coordinates: X
4506         Attributes:
4507             units:    ft
4508 
4509         >>> a.equals(b)
4510         True
4511         >>> a.identical(b)
4512         True
4513 
4514         >>> a.equals(c)
4515         True
4516         >>> a.identical(c)
4517         False
4518         """
4519         try:
4520             return self.name == other.name and self._all_compat(other, "identical")
4521         except (TypeError, AttributeError):
4522             return False
4523 
4524     def _result_name(self, other: Any = None) -> Hashable | None:
4525         # use the same naming heuristics as pandas:
4526         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
4527         other_name = getattr(other, "name", _default)
4528         if other_name is _default or other_name == self.name:
4529             return self.name
4530         else:
4531             return None
4532 
4533     def __array_wrap__(self: T_DataArray, obj, context=None) -> T_DataArray:
4534         new_var = self.variable.__array_wrap__(obj, context)
4535         return self._replace(new_var)
4536 
4537     def __matmul__(self: T_DataArray, obj: T_DataArray) -> T_DataArray:
4538         return self.dot(obj)
4539 
4540     def __rmatmul__(self: T_DataArray, other: T_DataArray) -> T_DataArray:
4541         # currently somewhat duplicative, as only other DataArrays are
4542         # compatible with matmul
4543         return computation.dot(other, self)
4544 
4545     def _unary_op(self: T_DataArray, f: Callable, *args, **kwargs) -> T_DataArray:
4546         keep_attrs = kwargs.pop("keep_attrs", None)
4547         if keep_attrs is None:
4548             keep_attrs = _get_keep_attrs(default=True)
4549         with warnings.catch_warnings():
4550             warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
4551             warnings.filterwarnings(
4552                 "ignore", r"Mean of empty slice", category=RuntimeWarning
4553             )
4554             with np.errstate(all="ignore"):
4555                 da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))
4556             if keep_attrs:
4557                 da.attrs = self.attrs
4558             return da
4559 
4560     def _binary_op(
4561         self: T_DataArray,
4562         other: Any,
4563         f: Callable,
4564         reflexive: bool = False,
4565     ) -> T_DataArray:
4566         from xarray.core.groupby import GroupBy
4567 
4568         if isinstance(other, (Dataset, GroupBy)):
4569             return NotImplemented
4570         if isinstance(other, DataArray):
4571             align_type = OPTIONS["arithmetic_join"]
4572             self, other = align(self, other, join=align_type, copy=False)  # type: ignore
4573         other_variable = getattr(other, "variable", other)
4574         other_coords = getattr(other, "coords", None)
4575 
4576         variable = (
4577             f(self.variable, other_variable)
4578             if not reflexive
4579             else f(other_variable, self.variable)
4580         )
4581         coords, indexes = self.coords._merge_raw(other_coords, reflexive)
4582         name = self._result_name(other)
4583 
4584         return self._replace(variable, coords, name, indexes=indexes)
4585 
4586     def _inplace_binary_op(self: T_DataArray, other: Any, f: Callable) -> T_DataArray:
4587         from xarray.core.groupby import GroupBy
4588 
4589         if isinstance(other, GroupBy):
4590             raise TypeError(
4591                 "in-place operations between a DataArray and "
4592                 "a grouped object are not permitted"
4593             )
4594         # n.b. we can't align other to self (with other.reindex_like(self))
4595         # because `other` may be converted into floats, which would cause
4596         # in-place arithmetic to fail unpredictably. Instead, we simply
4597         # don't support automatic alignment with in-place arithmetic.
4598         other_coords = getattr(other, "coords", None)
4599         other_variable = getattr(other, "variable", other)
4600         try:
4601             with self.coords._merge_inplace(other_coords):
4602                 f(self.variable, other_variable)
4603         except MergeError as exc:
4604             raise MergeError(
4605                 "Automatic alignment is not supported for in-place operations.\n"
4606                 "Consider aligning the indices manually or using a not-in-place operation.\n"
4607                 "See https://github.com/pydata/xarray/issues/3910 for more explanations."
4608             ) from exc
4609         return self
4610 
4611     def _copy_attrs_from(self, other: DataArray | Dataset | Variable) -> None:
4612         self.attrs = other.attrs
4613 
4614     plot = utils.UncachedAccessor(DataArrayPlotAccessor)
4615 
4616     def _title_for_slice(self, truncate: int = 50) -> str:
4617         """
4618         If the dataarray has 1 dimensional coordinates or comes from a slice
4619         we can show that info in the title
4620 
4621         Parameters
4622         ----------
4623         truncate : int, default: 50
4624             maximum number of characters for title
4625 
4626         Returns
4627         -------
4628         title : string
4629             Can be used for plot titles
4630 
4631         """
4632         one_dims = []
4633         for dim, coord in self.coords.items():
4634             if coord.size == 1:
4635                 one_dims.append(
4636                     "{dim} = {v}{unit}".format(
4637                         dim=dim,
4638                         v=format_item(coord.values),
4639                         unit=_get_units_from_attrs(coord),
4640                     )
4641                 )
4642 
4643         title = ", ".join(one_dims)
4644         if len(title) > truncate:
4645             title = title[: (truncate - 3)] + "..."
4646 
4647         return title
4648 
4649     def diff(
4650         self: T_DataArray,
4651         dim: Hashable,
4652         n: int = 1,
4653         label: Literal["upper", "lower"] = "upper",
4654     ) -> T_DataArray:
4655         """Calculate the n-th order discrete difference along given axis.
4656 
4657         Parameters
4658         ----------
4659         dim : Hashable
4660             Dimension over which to calculate the finite difference.
4661         n : int, default: 1
4662             The number of times values are differenced.
4663         label : {"upper", "lower"}, default: "upper"
4664             The new coordinate in dimension ``dim`` will have the
4665             values of either the minuend's or subtrahend's coordinate
4666             for values 'upper' and 'lower', respectively.
4667 
4668         Returns
4669         -------
4670         difference : DataArray
4671             The n-th order finite difference of this object.
4672 
4673         Notes
4674         -----
4675         `n` matches numpy's behavior and is different from pandas' first argument named
4676         `periods`.
4677 
4678         Examples
4679         --------
4680         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
4681         >>> arr.diff("x")
4682         <xarray.DataArray (x: 3)>
4683         array([0, 1, 0])
4684         Coordinates:
4685           * x        (x) int64 2 3 4
4686         >>> arr.diff("x", 2)
4687         <xarray.DataArray (x: 2)>
4688         array([ 1, -1])
4689         Coordinates:
4690           * x        (x) int64 3 4
4691 
4692         See Also
4693         --------
4694         DataArray.differentiate
4695         """
4696         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
4697         return self._from_temp_dataset(ds)
4698 
4699     def shift(
4700         self: T_DataArray,
4701         shifts: Mapping[Any, int] | None = None,
4702         fill_value: Any = dtypes.NA,
4703         **shifts_kwargs: int,
4704     ) -> T_DataArray:
4705         """Shift this DataArray by an offset along one or more dimensions.
4706 
4707         Only the data is moved; coordinates stay in place. This is consistent
4708         with the behavior of ``shift`` in pandas.
4709 
4710         Values shifted from beyond array bounds will appear at one end of
4711         each dimension, which are filled according to `fill_value`. For periodic
4712         offsets instead see `roll`.
4713 
4714         Parameters
4715         ----------
4716         shifts : mapping of Hashable to int or None, optional
4717             Integer offset to shift along each of the given dimensions.
4718             Positive offsets shift to the right; negative offsets shift to the
4719             left.
4720         fill_value : scalar, optional
4721             Value to use for newly missing values
4722         **shifts_kwargs
4723             The keyword arguments form of ``shifts``.
4724             One of shifts or shifts_kwargs must be provided.
4725 
4726         Returns
4727         -------
4728         shifted : DataArray
4729             DataArray with the same coordinates and attributes but shifted
4730             data.
4731 
4732         See Also
4733         --------
4734         roll
4735 
4736         Examples
4737         --------
4738         >>> arr = xr.DataArray([5, 6, 7], dims="x")
4739         >>> arr.shift(x=1)
4740         <xarray.DataArray (x: 3)>
4741         array([nan,  5.,  6.])
4742         Dimensions without coordinates: x
4743         """
4744         variable = self.variable.shift(
4745             shifts=shifts, fill_value=fill_value, **shifts_kwargs
4746         )
4747         return self._replace(variable=variable)
4748 
4749     def roll(
4750         self: T_DataArray,
4751         shifts: Mapping[Hashable, int] | None = None,
4752         roll_coords: bool = False,
4753         **shifts_kwargs: int,
4754     ) -> T_DataArray:
4755         """Roll this array by an offset along one or more dimensions.
4756 
4757         Unlike shift, roll treats the given dimensions as periodic, so will not
4758         create any missing values to be filled.
4759 
4760         Unlike shift, roll may rotate all variables, including coordinates
4761         if specified. The direction of rotation is consistent with
4762         :py:func:`numpy.roll`.
4763 
4764         Parameters
4765         ----------
4766         shifts : mapping of Hashable to int, optional
4767             Integer offset to rotate each of the given dimensions.
4768             Positive offsets roll to the right; negative offsets roll to the
4769             left.
4770         roll_coords : bool, default: False
4771             Indicates whether to roll the coordinates by the offset too.
4772         **shifts_kwargs : {dim: offset, ...}, optional
4773             The keyword arguments form of ``shifts``.
4774             One of shifts or shifts_kwargs must be provided.
4775 
4776         Returns
4777         -------
4778         rolled : DataArray
4779             DataArray with the same attributes but rolled data and coordinates.
4780 
4781         See Also
4782         --------
4783         shift
4784 
4785         Examples
4786         --------
4787         >>> arr = xr.DataArray([5, 6, 7], dims="x")
4788         >>> arr.roll(x=1)
4789         <xarray.DataArray (x: 3)>
4790         array([7, 5, 6])
4791         Dimensions without coordinates: x
4792         """
4793         ds = self._to_temp_dataset().roll(
4794             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
4795         )
4796         return self._from_temp_dataset(ds)
4797 
4798     @property
4799     def real(self: T_DataArray) -> T_DataArray:
4800         """
4801         The real part of the array.
4802 
4803         See Also
4804         --------
4805         numpy.ndarray.real
4806         """
4807         return self._replace(self.variable.real)
4808 
4809     @property
4810     def imag(self: T_DataArray) -> T_DataArray:
4811         """
4812         The imaginary part of the array.
4813 
4814         See Also
4815         --------
4816         numpy.ndarray.imag
4817         """
4818         return self._replace(self.variable.imag)
4819 
4820     def dot(
4821         self: T_DataArray,
4822         other: T_DataArray,
4823         dims: Dims = None,
4824     ) -> T_DataArray:
4825         """Perform dot product of two DataArrays along their shared dims.
4826 
4827         Equivalent to taking taking tensordot over all shared dims.
4828 
4829         Parameters
4830         ----------
4831         other : DataArray
4832             The other array with which the dot product is performed.
4833         dims : ..., str, Iterable of Hashable or None, optional
4834             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.
4835             If not specified, then all the common dimensions are summed over.
4836 
4837         Returns
4838         -------
4839         result : DataArray
4840             Array resulting from the dot product over all shared dimensions.
4841 
4842         See Also
4843         --------
4844         dot
4845         numpy.tensordot
4846 
4847         Examples
4848         --------
4849         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
4850         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
4851         >>> dm_vals = np.arange(4)
4852         >>> dm = xr.DataArray(dm_vals, dims=["z"])
4853 
4854         >>> dm.dims
4855         ('z',)
4856 
4857         >>> da.dims
4858         ('x', 'y', 'z')
4859 
4860         >>> dot_result = da.dot(dm)
4861         >>> dot_result.dims
4862         ('x', 'y')
4863 
4864         """
4865         if isinstance(other, Dataset):
4866             raise NotImplementedError(
4867                 "dot products are not yet supported with Dataset objects."
4868             )
4869         if not isinstance(other, DataArray):
4870             raise TypeError("dot only operates on DataArrays.")
4871 
4872         return computation.dot(self, other, dims=dims)
4873 
4874     # change type of self and return to T_DataArray once
4875     # https://github.com/python/mypy/issues/12846 is resolved
4876     def sortby(
4877         self,
4878         variables: Hashable | DataArray | Sequence[Hashable | DataArray],
4879         ascending: bool = True,
4880     ) -> DataArray:
4881         """Sort object by labels or values (along an axis).
4882 
4883         Sorts the dataarray, either along specified dimensions,
4884         or according to values of 1-D dataarrays that share dimension
4885         with calling object.
4886 
4887         If the input variables are dataarrays, then the dataarrays are aligned
4888         (via left-join) to the calling object prior to sorting by cell values.
4889         NaNs are sorted to the end, following Numpy convention.
4890 
4891         If multiple sorts along the same dimension is
4892         given, numpy's lexsort is performed along that dimension:
4893         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
4894         and the FIRST key in the sequence is used as the primary sort key,
4895         followed by the 2nd key, etc.
4896 
4897         Parameters
4898         ----------
4899         variables : Hashable, DataArray, or sequence of Hashable or DataArray
4900             1D DataArray objects or name(s) of 1D variable(s) in
4901             coords whose values are used to sort this array.
4902         ascending : bool, default: True
4903             Whether to sort by ascending or descending order.
4904 
4905         Returns
4906         -------
4907         sorted : DataArray
4908             A new dataarray where all the specified dims are sorted by dim
4909             labels.
4910 
4911         See Also
4912         --------
4913         Dataset.sortby
4914         numpy.sort
4915         pandas.sort_values
4916         pandas.sort_index
4917 
4918         Examples
4919         --------
4920         >>> da = xr.DataArray(
4921         ...     np.random.rand(5),
4922         ...     coords=[pd.date_range("1/1/2000", periods=5)],
4923         ...     dims="time",
4924         ... )
4925         >>> da
4926         <xarray.DataArray (time: 5)>
4927         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])
4928         Coordinates:
4929           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05
4930 
4931         >>> da.sortby(da)
4932         <xarray.DataArray (time: 5)>
4933         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])
4934         Coordinates:
4935           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02
4936         """
4937         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
4938         return self._from_temp_dataset(ds)
4939 
4940     def quantile(
4941         self: T_DataArray,
4942         q: ArrayLike,
4943         dim: Dims = None,
4944         method: QuantileMethods = "linear",
4945         keep_attrs: bool | None = None,
4946         skipna: bool | None = None,
4947         interpolation: QuantileMethods | None = None,
4948     ) -> T_DataArray:
4949         """Compute the qth quantile of the data along the specified dimension.
4950 
4951         Returns the qth quantiles(s) of the array elements.
4952 
4953         Parameters
4954         ----------
4955         q : float or array-like of float
4956             Quantile to compute, which must be between 0 and 1 inclusive.
4957         dim : str or Iterable of Hashable, optional
4958             Dimension(s) over which to apply quantile.
4959         method : str, default: "linear"
4960             This optional parameter specifies the interpolation method to use when the
4961             desired quantile lies between two data points. The options sorted by their R
4962             type as summarized in the H&F paper [1]_ are:
4963 
4964                 1. "inverted_cdf" (*)
4965                 2. "averaged_inverted_cdf" (*)
4966                 3. "closest_observation" (*)
4967                 4. "interpolated_inverted_cdf" (*)
4968                 5. "hazen" (*)
4969                 6. "weibull" (*)
4970                 7. "linear"  (default)
4971                 8. "median_unbiased" (*)
4972                 9. "normal_unbiased" (*)
4973 
4974             The first three methods are discontiuous. The following discontinuous
4975             variations of the default "linear" (7.) option are also available:
4976 
4977                 * "lower"
4978                 * "higher"
4979                 * "midpoint"
4980                 * "nearest"
4981 
4982             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
4983             was previously called "interpolation", renamed in accordance with numpy
4984             version 1.22.0.
4985 
4986             (*) These methods require numpy version 1.22 or newer.
4987 
4988         keep_attrs : bool or None, optional
4989             If True, the dataset's attributes (`attrs`) will be copied from
4990             the original object to the new one.  If False (default), the new
4991             object will be returned without attributes.
4992         skipna : bool or None, optional
4993             If True, skip missing values (as marked by NaN). By default, only
4994             skips missing values for float dtypes; other dtypes either do not
4995             have a sentinel missing value (int) or skipna=True has not been
4996             implemented (object, datetime64 or timedelta64).
4997 
4998         Returns
4999         -------
5000         quantiles : DataArray
5001             If `q` is a single quantile, then the result
5002             is a scalar. If multiple percentiles are given, first axis of
5003             the result corresponds to the quantile and a quantile dimension
5004             is added to the return array. The other dimensions are the
5005             dimensions that remain after the reduction of the array.
5006 
5007         See Also
5008         --------
5009         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
5010 
5011         Examples
5012         --------
5013         >>> da = xr.DataArray(
5014         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
5015         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
5016         ...     dims=("x", "y"),
5017         ... )
5018         >>> da.quantile(0)  # or da.quantile(0, dim=...)
5019         <xarray.DataArray ()>
5020         array(0.7)
5021         Coordinates:
5022             quantile  float64 0.0
5023         >>> da.quantile(0, dim="x")
5024         <xarray.DataArray (y: 4)>
5025         array([0.7, 4.2, 2.6, 1.5])
5026         Coordinates:
5027           * y         (y) float64 1.0 1.5 2.0 2.5
5028             quantile  float64 0.0
5029         >>> da.quantile([0, 0.5, 1])
5030         <xarray.DataArray (quantile: 3)>
5031         array([0.7, 3.4, 9.4])
5032         Coordinates:
5033           * quantile  (quantile) float64 0.0 0.5 1.0
5034         >>> da.quantile([0, 0.5, 1], dim="x")
5035         <xarray.DataArray (quantile: 3, y: 4)>
5036         array([[0.7 , 4.2 , 2.6 , 1.5 ],
5037                [3.6 , 5.75, 6.  , 1.7 ],
5038                [6.5 , 7.3 , 9.4 , 1.9 ]])
5039         Coordinates:
5040           * y         (y) float64 1.0 1.5 2.0 2.5
5041           * quantile  (quantile) float64 0.0 0.5 1.0
5042 
5043         References
5044         ----------
5045         .. [1] R. J. Hyndman and Y. Fan,
5046            "Sample quantiles in statistical packages,"
5047            The American Statistician, 50(4), pp. 361-365, 1996
5048         """
5049 
5050         ds = self._to_temp_dataset().quantile(
5051             q,
5052             dim=dim,
5053             keep_attrs=keep_attrs,
5054             method=method,
5055             skipna=skipna,
5056             interpolation=interpolation,
5057         )
5058         return self._from_temp_dataset(ds)
5059 
5060     def rank(
5061         self: T_DataArray,
5062         dim: Hashable,
5063         pct: bool = False,
5064         keep_attrs: bool | None = None,
5065     ) -> T_DataArray:
5066         """Ranks the data.
5067 
5068         Equal values are assigned a rank that is the average of the ranks that
5069         would have been otherwise assigned to all of the values within that
5070         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
5071 
5072         NaNs in the input array are returned as NaNs.
5073 
5074         The `bottleneck` library is required.
5075 
5076         Parameters
5077         ----------
5078         dim : Hashable
5079             Dimension over which to compute rank.
5080         pct : bool, default: False
5081             If True, compute percentage ranks, otherwise compute integer ranks.
5082         keep_attrs : bool or None, optional
5083             If True, the dataset's attributes (`attrs`) will be copied from
5084             the original object to the new one.  If False (default), the new
5085             object will be returned without attributes.
5086 
5087         Returns
5088         -------
5089         ranked : DataArray
5090             DataArray with the same coordinates and dtype 'float64'.
5091 
5092         Examples
5093         --------
5094         >>> arr = xr.DataArray([5, 6, 7], dims="x")
5095         >>> arr.rank("x")
5096         <xarray.DataArray (x: 3)>
5097         array([1., 2., 3.])
5098         Dimensions without coordinates: x
5099         """
5100 
5101         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
5102         return self._from_temp_dataset(ds)
5103 
5104     def differentiate(
5105         self: T_DataArray,
5106         coord: Hashable,
5107         edge_order: Literal[1, 2] = 1,
5108         datetime_unit: DatetimeUnitOptions = None,
5109     ) -> T_DataArray:
5110         """ Differentiate the array with the second order accurate central
5111         differences.
5112 
5113         .. note::
5114             This feature is limited to simple cartesian geometry, i.e. coord
5115             must be one dimensional.
5116 
5117         Parameters
5118         ----------
5119         coord : Hashable
5120             The coordinate to be used to compute the gradient.
5121         edge_order : {1, 2}, default: 1
5122             N-th order accurate differences at the boundaries.
5123         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", \
5124                          "us", "ns", "ps", "fs", "as", None}, optional
5125             Unit to compute gradient. Only valid for datetime coordinate.
5126 
5127         Returns
5128         -------
5129         differentiated: DataArray
5130 
5131         See also
5132         --------
5133         numpy.gradient: corresponding numpy function
5134 
5135         Examples
5136         --------
5137 
5138         >>> da = xr.DataArray(
5139         ...     np.arange(12).reshape(4, 3),
5140         ...     dims=["x", "y"],
5141         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
5142         ... )
5143         >>> da
5144         <xarray.DataArray (x: 4, y: 3)>
5145         array([[ 0,  1,  2],
5146                [ 3,  4,  5],
5147                [ 6,  7,  8],
5148                [ 9, 10, 11]])
5149         Coordinates:
5150           * x        (x) float64 0.0 0.1 1.1 1.2
5151         Dimensions without coordinates: y
5152         >>>
5153         >>> da.differentiate("x")
5154         <xarray.DataArray (x: 4, y: 3)>
5155         array([[30.        , 30.        , 30.        ],
5156                [27.54545455, 27.54545455, 27.54545455],
5157                [27.54545455, 27.54545455, 27.54545455],
5158                [30.        , 30.        , 30.        ]])
5159         Coordinates:
5160           * x        (x) float64 0.0 0.1 1.1 1.2
5161         Dimensions without coordinates: y
5162         """
5163         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
5164         return self._from_temp_dataset(ds)
5165 
5166     # change type of self and return to T_DataArray once
5167     # https://github.com/python/mypy/issues/12846 is resolved
5168     def integrate(
5169         self,
5170         coord: Hashable | Sequence[Hashable] = None,
5171         datetime_unit: DatetimeUnitOptions = None,
5172     ) -> DataArray:
5173         """Integrate along the given coordinate using the trapezoidal rule.
5174 
5175         .. note::
5176             This feature is limited to simple cartesian geometry, i.e. coord
5177             must be one dimensional.
5178 
5179         Parameters
5180         ----------
5181         coord : Hashable, or sequence of Hashable
5182             Coordinate(s) used for the integration.
5183         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
5184                         'ps', 'fs', 'as', None}, optional
5185             Specify the unit if a datetime coordinate is used.
5186 
5187         Returns
5188         -------
5189         integrated : DataArray
5190 
5191         See also
5192         --------
5193         Dataset.integrate
5194         numpy.trapz : corresponding numpy function
5195 
5196         Examples
5197         --------
5198 
5199         >>> da = xr.DataArray(
5200         ...     np.arange(12).reshape(4, 3),
5201         ...     dims=["x", "y"],
5202         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
5203         ... )
5204         >>> da
5205         <xarray.DataArray (x: 4, y: 3)>
5206         array([[ 0,  1,  2],
5207                [ 3,  4,  5],
5208                [ 6,  7,  8],
5209                [ 9, 10, 11]])
5210         Coordinates:
5211           * x        (x) float64 0.0 0.1 1.1 1.2
5212         Dimensions without coordinates: y
5213         >>>
5214         >>> da.integrate("x")
5215         <xarray.DataArray (y: 3)>
5216         array([5.4, 6.6, 7.8])
5217         Dimensions without coordinates: y
5218         """
5219         ds = self._to_temp_dataset().integrate(coord, datetime_unit)
5220         return self._from_temp_dataset(ds)
5221 
5222     # change type of self and return to T_DataArray once
5223     # https://github.com/python/mypy/issues/12846 is resolved
5224     def cumulative_integrate(
5225         self,
5226         coord: Hashable | Sequence[Hashable] = None,
5227         datetime_unit: DatetimeUnitOptions = None,
5228     ) -> DataArray:
5229         """Integrate cumulatively along the given coordinate using the trapezoidal rule.
5230 
5231         .. note::
5232             This feature is limited to simple cartesian geometry, i.e. coord
5233             must be one dimensional.
5234 
5235             The first entry of the cumulative integral is always 0, in order to keep the
5236             length of the dimension unchanged between input and output.
5237 
5238         Parameters
5239         ----------
5240         coord : Hashable, or sequence of Hashable
5241             Coordinate(s) used for the integration.
5242         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
5243                         'ps', 'fs', 'as', None}, optional
5244             Specify the unit if a datetime coordinate is used.
5245 
5246         Returns
5247         -------
5248         integrated : DataArray
5249 
5250         See also
5251         --------
5252         Dataset.cumulative_integrate
5253         scipy.integrate.cumulative_trapezoid : corresponding scipy function
5254 
5255         Examples
5256         --------
5257 
5258         >>> da = xr.DataArray(
5259         ...     np.arange(12).reshape(4, 3),
5260         ...     dims=["x", "y"],
5261         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
5262         ... )
5263         >>> da
5264         <xarray.DataArray (x: 4, y: 3)>
5265         array([[ 0,  1,  2],
5266                [ 3,  4,  5],
5267                [ 6,  7,  8],
5268                [ 9, 10, 11]])
5269         Coordinates:
5270           * x        (x) float64 0.0 0.1 1.1 1.2
5271         Dimensions without coordinates: y
5272         >>>
5273         >>> da.cumulative_integrate("x")
5274         <xarray.DataArray (x: 4, y: 3)>
5275         array([[0.  , 0.  , 0.  ],
5276                [0.15, 0.25, 0.35],
5277                [4.65, 5.75, 6.85],
5278                [5.4 , 6.6 , 7.8 ]])
5279         Coordinates:
5280           * x        (x) float64 0.0 0.1 1.1 1.2
5281         Dimensions without coordinates: y
5282         """
5283         ds = self._to_temp_dataset().cumulative_integrate(coord, datetime_unit)
5284         return self._from_temp_dataset(ds)
5285 
5286     def unify_chunks(self) -> DataArray:
5287         """Unify chunk size along all chunked dimensions of this DataArray.
5288 
5289         Returns
5290         -------
5291         DataArray with consistent chunk sizes for all dask-array variables
5292 
5293         See Also
5294         --------
5295         dask.array.core.unify_chunks
5296         """
5297 
5298         return unify_chunks(self)[0]
5299 
5300     def map_blocks(
5301         self,
5302         func: Callable[..., T_Xarray],
5303         args: Sequence[Any] = (),
5304         kwargs: Mapping[str, Any] | None = None,
5305         template: DataArray | Dataset | None = None,
5306     ) -> T_Xarray:
5307         """
5308         Apply a function to each block of this DataArray.
5309 
5310         .. warning::
5311             This method is experimental and its signature may change.
5312 
5313         Parameters
5314         ----------
5315         func : callable
5316             User-provided function that accepts a DataArray as its first
5317             parameter. The function will receive a subset or 'block' of this DataArray (see below),
5318             corresponding to one chunk along each chunked dimension. ``func`` will be
5319             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.
5320 
5321             This function must return either a single DataArray or a single Dataset.
5322 
5323             This function cannot add a new chunked dimension.
5324         args : sequence
5325             Passed to func after unpacking and subsetting any xarray objects by blocks.
5326             xarray objects in args must be aligned with this object, otherwise an error is raised.
5327         kwargs : mapping
5328             Passed verbatim to func after unpacking. xarray objects, if any, will not be
5329             subset to blocks. Passing dask collections in kwargs is not allowed.
5330         template : DataArray or Dataset, optional
5331             xarray object representing the final result after compute is called. If not provided,
5332             the function will be first run on mocked-up data, that looks like this object but
5333             has sizes 0, to determine properties of the returned object such as dtype,
5334             variable names, attributes, new dimensions and new indexes (if any).
5335             ``template`` must be provided if the function changes the size of existing dimensions.
5336             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
5337             ``attrs`` set by ``func`` will be ignored.
5338 
5339         Returns
5340         -------
5341         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
5342         function.
5343 
5344         Notes
5345         -----
5346         This function is designed for when ``func`` needs to manipulate a whole xarray object
5347         subset to each block. Each block is loaded into memory. In the more common case where
5348         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
5349 
5350         If none of the variables in this object is backed by dask arrays, calling this function is
5351         equivalent to calling ``func(obj, *args, **kwargs)``.
5352 
5353         See Also
5354         --------
5355         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
5356         xarray.DataArray.map_blocks
5357 
5358         Examples
5359         --------
5360         Calculate an anomaly from climatology using ``.groupby()``. Using
5361         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
5362         its indices, and its methods like ``.groupby()``.
5363 
5364         >>> def calculate_anomaly(da, groupby_type="time.month"):
5365         ...     gb = da.groupby(groupby_type)
5366         ...     clim = gb.mean(dim="time")
5367         ...     return gb - clim
5368         ...
5369         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
5370         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
5371         >>> np.random.seed(123)
5372         >>> array = xr.DataArray(
5373         ...     np.random.rand(len(time)),
5374         ...     dims=["time"],
5375         ...     coords={"time": time, "month": month},
5376         ... ).chunk()
5377         >>> array.map_blocks(calculate_anomaly, template=array).compute()
5378         <xarray.DataArray (time: 24)>
5379         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,
5380                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,
5381                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,
5382                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,
5383                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])
5384         Coordinates:
5385           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
5386             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
5387 
5388         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
5389         to the function being applied in ``xr.map_blocks()``:
5390 
5391         >>> array.map_blocks(
5392         ...     calculate_anomaly, kwargs={"groupby_type": "time.year"}, template=array
5393         ... )  # doctest: +ELLIPSIS
5394         <xarray.DataArray (time: 24)>
5395         dask.array<<this-array>-calculate_anomaly, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>
5396         Coordinates:
5397           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
5398             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
5399         """
5400         from xarray.core.parallel import map_blocks
5401 
5402         return map_blocks(func, self, args, kwargs, template)
5403 
5404     def polyfit(
5405         self,
5406         dim: Hashable,
5407         deg: int,
5408         skipna: bool | None = None,
5409         rcond: float | None = None,
5410         w: Hashable | Any | None = None,
5411         full: bool = False,
5412         cov: bool | Literal["unscaled"] = False,
5413     ) -> Dataset:
5414         """
5415         Least squares polynomial fit.
5416 
5417         This replicates the behaviour of `numpy.polyfit` but differs by skipping
5418         invalid values when `skipna = True`.
5419 
5420         Parameters
5421         ----------
5422         dim : Hashable
5423             Coordinate along which to fit the polynomials.
5424         deg : int
5425             Degree of the fitting polynomial.
5426         skipna : bool or None, optional
5427             If True, removes all invalid values before fitting each 1D slices of the array.
5428             Default is True if data is stored in a dask.array or if there is any
5429             invalid values, False otherwise.
5430         rcond : float or None, optional
5431             Relative condition number to the fit.
5432         w : Hashable, array-like or None, optional
5433             Weights to apply to the y-coordinate of the sample points.
5434             Can be an array-like object or the name of a coordinate in the dataset.
5435         full : bool, default: False
5436             Whether to return the residuals, matrix rank and singular values in addition
5437             to the coefficients.
5438         cov : bool or "unscaled", default: False
5439             Whether to return to the covariance matrix in addition to the coefficients.
5440             The matrix is not scaled if `cov='unscaled'`.
5441 
5442         Returns
5443         -------
5444         polyfit_results : Dataset
5445             A single dataset which contains:
5446 
5447             polyfit_coefficients
5448                 The coefficients of the best fit.
5449             polyfit_residuals
5450                 The residuals of the least-square computation (only included if `full=True`).
5451                 When the matrix rank is deficient, np.nan is returned.
5452             [dim]_matrix_rank
5453                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
5454             [dim]_singular_value
5455                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
5456             polyfit_covariance
5457                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
5458 
5459         See Also
5460         --------
5461         numpy.polyfit
5462         numpy.polyval
5463         xarray.polyval
5464         """
5465         return self._to_temp_dataset().polyfit(
5466             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov
5467         )
5468 
5469     def pad(
5470         self: T_DataArray,
5471         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,
5472         mode: PadModeOptions = "constant",
5473         stat_length: int
5474         | tuple[int, int]
5475         | Mapping[Any, tuple[int, int]]
5476         | None = None,
5477         constant_values: float
5478         | tuple[float, float]
5479         | Mapping[Any, tuple[float, float]]
5480         | None = None,
5481         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
5482         reflect_type: PadReflectOptions = None,
5483         keep_attrs: bool | None = None,
5484         **pad_width_kwargs: Any,
5485     ) -> T_DataArray:
5486         """Pad this array along one or more dimensions.
5487 
5488         .. warning::
5489             This function is experimental and its behaviour is likely to change
5490             especially regarding padding of dimension coordinates (or IndexVariables).
5491 
5492         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
5493         coordinates will be padded with the same mode, otherwise coordinates
5494         are padded using the "constant" mode with fill_value dtypes.NA.
5495 
5496         Parameters
5497         ----------
5498         pad_width : mapping of Hashable to tuple of int
5499             Mapping with the form of {dim: (pad_before, pad_after)}
5500             describing the number of values padded along each dimension.
5501             {dim: pad} is a shortcut for pad_before = pad_after = pad
5502         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
5503             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
5504             How to pad the DataArray (taken from numpy docs):
5505 
5506             - "constant": Pads with a constant value.
5507             - "edge": Pads with the edge values of array.
5508             - "linear_ramp": Pads with the linear ramp between end_value and the
5509               array edge value.
5510             - "maximum": Pads with the maximum value of all or part of the
5511               vector along each axis.
5512             - "mean": Pads with the mean value of all or part of the
5513               vector along each axis.
5514             - "median": Pads with the median value of all or part of the
5515               vector along each axis.
5516             - "minimum": Pads with the minimum value of all or part of the
5517               vector along each axis.
5518             - "reflect": Pads with the reflection of the vector mirrored on
5519               the first and last values of the vector along each axis.
5520             - "symmetric": Pads with the reflection of the vector mirrored
5521               along the edge of the array.
5522             - "wrap": Pads with the wrap of the vector along the axis.
5523               The first values are used to pad the end and the
5524               end values are used to pad the beginning.
5525 
5526         stat_length : int, tuple or mapping of Hashable to tuple, default: None
5527             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
5528             values at edge of each axis used to calculate the statistic value.
5529             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
5530             statistic lengths along each dimension.
5531             ((before, after),) yields same before and after statistic lengths
5532             for each dimension.
5533             (stat_length,) or int is a shortcut for before = after = statistic
5534             length for all axes.
5535             Default is ``None``, to use the entire axis.
5536         constant_values : scalar, tuple or mapping of Hashable to tuple, default: 0
5537             Used in 'constant'.  The values to set the padded values for each
5538             axis.
5539             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
5540             pad constants along each dimension.
5541             ``((before, after),)`` yields same before and after constants for each
5542             dimension.
5543             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
5544             all dimensions.
5545             Default is 0.
5546         end_values : scalar, tuple or mapping of Hashable to tuple, default: 0
5547             Used in 'linear_ramp'.  The values used for the ending value of the
5548             linear_ramp and that will form the edge of the padded array.
5549             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
5550             end values along each dimension.
5551             ``((before, after),)`` yields same before and after end values for each
5552             axis.
5553             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
5554             all axes.
5555             Default is 0.
5556         reflect_type : {"even", "odd", None}, optional
5557             Used in "reflect", and "symmetric". The "even" style is the
5558             default with an unaltered reflection around the edge value. For
5559             the "odd" style, the extended part of the array is created by
5560             subtracting the reflected values from two times the edge value.
5561         keep_attrs : bool or None, optional
5562             If True, the attributes (``attrs``) will be copied from the
5563             original object to the new one. If False, the new object
5564             will be returned without attributes.
5565         **pad_width_kwargs
5566             The keyword arguments form of ``pad_width``.
5567             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
5568 
5569         Returns
5570         -------
5571         padded : DataArray
5572             DataArray with the padded coordinates and data.
5573 
5574         See Also
5575         --------
5576         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad
5577 
5578         Notes
5579         -----
5580         For ``mode="constant"`` and ``constant_values=None``, integer types will be
5581         promoted to ``float`` and padded with ``np.nan``.
5582 
5583         Padding coordinates will drop their corresponding index (if any) and will reset default
5584         indexes for dimension coordinates.
5585 
5586         Examples
5587         --------
5588         >>> arr = xr.DataArray([5, 6, 7], coords=[("x", [0, 1, 2])])
5589         >>> arr.pad(x=(1, 2), constant_values=0)
5590         <xarray.DataArray (x: 6)>
5591         array([0, 5, 6, 7, 0, 0])
5592         Coordinates:
5593           * x        (x) float64 nan 0.0 1.0 2.0 nan nan
5594 
5595         >>> da = xr.DataArray(
5596         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],
5597         ...     dims=["x", "y"],
5598         ...     coords={"x": [0, 1], "y": [10, 20, 30, 40], "z": ("x", [100, 200])},
5599         ... )
5600         >>> da.pad(x=1)
5601         <xarray.DataArray (x: 4, y: 4)>
5602         array([[nan, nan, nan, nan],
5603                [ 0.,  1.,  2.,  3.],
5604                [10., 11., 12., 13.],
5605                [nan, nan, nan, nan]])
5606         Coordinates:
5607           * x        (x) float64 nan 0.0 1.0 nan
5608           * y        (y) int64 10 20 30 40
5609             z        (x) float64 nan 100.0 200.0 nan
5610 
5611         Careful, ``constant_values`` are coerced to the data type of the array which may
5612         lead to a loss of precision:
5613 
5614         >>> da.pad(x=1, constant_values=1.23456789)
5615         <xarray.DataArray (x: 4, y: 4)>
5616         array([[ 1,  1,  1,  1],
5617                [ 0,  1,  2,  3],
5618                [10, 11, 12, 13],
5619                [ 1,  1,  1,  1]])
5620         Coordinates:
5621           * x        (x) float64 nan 0.0 1.0 nan
5622           * y        (y) int64 10 20 30 40
5623             z        (x) float64 nan 100.0 200.0 nan
5624         """
5625         ds = self._to_temp_dataset().pad(
5626             pad_width=pad_width,
5627             mode=mode,
5628             stat_length=stat_length,
5629             constant_values=constant_values,
5630             end_values=end_values,
5631             reflect_type=reflect_type,
5632             keep_attrs=keep_attrs,
5633             **pad_width_kwargs,
5634         )
5635         return self._from_temp_dataset(ds)
5636 
5637     def idxmin(
5638         self,
5639         dim: Hashable | None = None,
5640         skipna: bool | None = None,
5641         fill_value: Any = dtypes.NA,
5642         keep_attrs: bool | None = None,
5643     ) -> DataArray:
5644         """Return the coordinate label of the minimum value along a dimension.
5645 
5646         Returns a new `DataArray` named after the dimension with the values of
5647         the coordinate labels along that dimension corresponding to minimum
5648         values along that dimension.
5649 
5650         In comparison to :py:meth:`~DataArray.argmin`, this returns the
5651         coordinate label while :py:meth:`~DataArray.argmin` returns the index.
5652 
5653         Parameters
5654         ----------
5655         dim : str, optional
5656             Dimension over which to apply `idxmin`.  This is optional for 1D
5657             arrays, but required for arrays with 2 or more dimensions.
5658         skipna : bool or None, default: None
5659             If True, skip missing values (as marked by NaN). By default, only
5660             skips missing values for ``float``, ``complex``, and ``object``
5661             dtypes; other dtypes either do not have a sentinel missing value
5662             (``int``) or ``skipna=True`` has not been implemented
5663             (``datetime64`` or ``timedelta64``).
5664         fill_value : Any, default: NaN
5665             Value to be filled in case all of the values along a dimension are
5666             null.  By default this is NaN.  The fill value and result are
5667             automatically converted to a compatible dtype if possible.
5668             Ignored if ``skipna`` is False.
5669         keep_attrs : bool or None, optional
5670             If True, the attributes (``attrs``) will be copied from the
5671             original object to the new one. If False, the new object
5672             will be returned without attributes.
5673 
5674         Returns
5675         -------
5676         reduced : DataArray
5677             New `DataArray` object with `idxmin` applied to its data and the
5678             indicated dimension removed.
5679 
5680         See Also
5681         --------
5682         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin
5683 
5684         Examples
5685         --------
5686         >>> array = xr.DataArray(
5687         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
5688         ... )
5689         >>> array.min()
5690         <xarray.DataArray ()>
5691         array(-2)
5692         >>> array.argmin(...)
5693         {'x': <xarray.DataArray ()>
5694         array(4)}
5695         >>> array.idxmin()
5696         <xarray.DataArray 'x' ()>
5697         array('e', dtype='<U1')
5698 
5699         >>> array = xr.DataArray(
5700         ...     [
5701         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
5702         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
5703         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
5704         ...     ],
5705         ...     dims=["y", "x"],
5706         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
5707         ... )
5708         >>> array.min(dim="x")
5709         <xarray.DataArray (y: 3)>
5710         array([-2., -4.,  1.])
5711         Coordinates:
5712           * y        (y) int64 -1 0 1
5713         >>> array.argmin(dim="x")
5714         <xarray.DataArray (y: 3)>
5715         array([4, 0, 2])
5716         Coordinates:
5717           * y        (y) int64 -1 0 1
5718         >>> array.idxmin(dim="x")
5719         <xarray.DataArray 'x' (y: 3)>
5720         array([16.,  0.,  4.])
5721         Coordinates:
5722           * y        (y) int64 -1 0 1
5723         """
5724         return computation._calc_idxminmax(
5725             array=self,
5726             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),
5727             dim=dim,
5728             skipna=skipna,
5729             fill_value=fill_value,
5730             keep_attrs=keep_attrs,
5731         )
5732 
5733     def idxmax(
5734         self,
5735         dim: Hashable = None,
5736         skipna: bool | None = None,
5737         fill_value: Any = dtypes.NA,
5738         keep_attrs: bool | None = None,
5739     ) -> DataArray:
5740         """Return the coordinate label of the maximum value along a dimension.
5741 
5742         Returns a new `DataArray` named after the dimension with the values of
5743         the coordinate labels along that dimension corresponding to maximum
5744         values along that dimension.
5745 
5746         In comparison to :py:meth:`~DataArray.argmax`, this returns the
5747         coordinate label while :py:meth:`~DataArray.argmax` returns the index.
5748 
5749         Parameters
5750         ----------
5751         dim : Hashable, optional
5752             Dimension over which to apply `idxmax`.  This is optional for 1D
5753             arrays, but required for arrays with 2 or more dimensions.
5754         skipna : bool or None, default: None
5755             If True, skip missing values (as marked by NaN). By default, only
5756             skips missing values for ``float``, ``complex``, and ``object``
5757             dtypes; other dtypes either do not have a sentinel missing value
5758             (``int``) or ``skipna=True`` has not been implemented
5759             (``datetime64`` or ``timedelta64``).
5760         fill_value : Any, default: NaN
5761             Value to be filled in case all of the values along a dimension are
5762             null.  By default this is NaN.  The fill value and result are
5763             automatically converted to a compatible dtype if possible.
5764             Ignored if ``skipna`` is False.
5765         keep_attrs : bool or None, optional
5766             If True, the attributes (``attrs``) will be copied from the
5767             original object to the new one. If False, the new object
5768             will be returned without attributes.
5769 
5770         Returns
5771         -------
5772         reduced : DataArray
5773             New `DataArray` object with `idxmax` applied to its data and the
5774             indicated dimension removed.
5775 
5776         See Also
5777         --------
5778         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax
5779 
5780         Examples
5781         --------
5782         >>> array = xr.DataArray(
5783         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
5784         ... )
5785         >>> array.max()
5786         <xarray.DataArray ()>
5787         array(2)
5788         >>> array.argmax(...)
5789         {'x': <xarray.DataArray ()>
5790         array(1)}
5791         >>> array.idxmax()
5792         <xarray.DataArray 'x' ()>
5793         array('b', dtype='<U1')
5794 
5795         >>> array = xr.DataArray(
5796         ...     [
5797         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
5798         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
5799         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
5800         ...     ],
5801         ...     dims=["y", "x"],
5802         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
5803         ... )
5804         >>> array.max(dim="x")
5805         <xarray.DataArray (y: 3)>
5806         array([2., 2., 1.])
5807         Coordinates:
5808           * y        (y) int64 -1 0 1
5809         >>> array.argmax(dim="x")
5810         <xarray.DataArray (y: 3)>
5811         array([0, 2, 2])
5812         Coordinates:
5813           * y        (y) int64 -1 0 1
5814         >>> array.idxmax(dim="x")
5815         <xarray.DataArray 'x' (y: 3)>
5816         array([0., 4., 4.])
5817         Coordinates:
5818           * y        (y) int64 -1 0 1
5819         """
5820         return computation._calc_idxminmax(
5821             array=self,
5822             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),
5823             dim=dim,
5824             skipna=skipna,
5825             fill_value=fill_value,
5826             keep_attrs=keep_attrs,
5827         )
5828 
5829     # change type of self and return to T_DataArray once
5830     # https://github.com/python/mypy/issues/12846 is resolved
5831     def argmin(
5832         self,
5833         dim: Dims = None,
5834         axis: int | None = None,
5835         keep_attrs: bool | None = None,
5836         skipna: bool | None = None,
5837     ) -> DataArray | dict[Hashable, DataArray]:
5838         """Index or indices of the minimum of the DataArray over one or more dimensions.
5839 
5840         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
5841         which can be passed directly to isel(). If a single str is passed to 'dim' then
5842         returns a DataArray with dtype int.
5843 
5844         If there are multiple minima, the indices of the first one found will be
5845         returned.
5846 
5847         Parameters
5848         ----------
5849         dim : "...", str, Iterable of Hashable or None, optional
5850             The dimensions over which to find the minimum. By default, finds minimum over
5851             all dimensions - for now returning an int for backward compatibility, but
5852             this is deprecated, in future will return a dict with indices for all
5853             dimensions; to return a dict with all dimensions now, pass '...'.
5854         axis : int or None, optional
5855             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
5856             can be supplied.
5857         keep_attrs : bool or None, optional
5858             If True, the attributes (`attrs`) will be copied from the original
5859             object to the new one. If False, the new object will be
5860             returned without attributes.
5861         skipna : bool or None, optional
5862             If True, skip missing values (as marked by NaN). By default, only
5863             skips missing values for float dtypes; other dtypes either do not
5864             have a sentinel missing value (int) or skipna=True has not been
5865             implemented (object, datetime64 or timedelta64).
5866 
5867         Returns
5868         -------
5869         result : DataArray or dict of DataArray
5870 
5871         See Also
5872         --------
5873         Variable.argmin, DataArray.idxmin
5874 
5875         Examples
5876         --------
5877         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
5878         >>> array.min()
5879         <xarray.DataArray ()>
5880         array(-1)
5881         >>> array.argmin(...)
5882         {'x': <xarray.DataArray ()>
5883         array(2)}
5884         >>> array.isel(array.argmin(...))
5885         <xarray.DataArray ()>
5886         array(-1)
5887 
5888         >>> array = xr.DataArray(
5889         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],
5890         ...     dims=("x", "y", "z"),
5891         ... )
5892         >>> array.min(dim="x")
5893         <xarray.DataArray (y: 3, z: 3)>
5894         array([[ 1,  2,  1],
5895                [ 2, -5,  1],
5896                [ 2,  1,  1]])
5897         Dimensions without coordinates: y, z
5898         >>> array.argmin(dim="x")
5899         <xarray.DataArray (y: 3, z: 3)>
5900         array([[1, 0, 0],
5901                [1, 1, 1],
5902                [0, 0, 1]])
5903         Dimensions without coordinates: y, z
5904         >>> array.argmin(dim=["x"])
5905         {'x': <xarray.DataArray (y: 3, z: 3)>
5906         array([[1, 0, 0],
5907                [1, 1, 1],
5908                [0, 0, 1]])
5909         Dimensions without coordinates: y, z}
5910         >>> array.min(dim=("x", "z"))
5911         <xarray.DataArray (y: 3)>
5912         array([ 1, -5,  1])
5913         Dimensions without coordinates: y
5914         >>> array.argmin(dim=["x", "z"])
5915         {'x': <xarray.DataArray (y: 3)>
5916         array([0, 1, 0])
5917         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
5918         array([2, 1, 1])
5919         Dimensions without coordinates: y}
5920         >>> array.isel(array.argmin(dim=["x", "z"]))
5921         <xarray.DataArray (y: 3)>
5922         array([ 1, -5,  1])
5923         Dimensions without coordinates: y
5924         """
5925         result = self.variable.argmin(dim, axis, keep_attrs, skipna)
5926         if isinstance(result, dict):
5927             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
5928         else:
5929             return self._replace_maybe_drop_dims(result)
5930 
5931     # change type of self and return to T_DataArray once
5932     # https://github.com/python/mypy/issues/12846 is resolved
5933     def argmax(
5934         self,
5935         dim: Dims = None,
5936         axis: int | None = None,
5937         keep_attrs: bool | None = None,
5938         skipna: bool | None = None,
5939     ) -> DataArray | dict[Hashable, DataArray]:
5940         """Index or indices of the maximum of the DataArray over one or more dimensions.
5941 
5942         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
5943         which can be passed directly to isel(). If a single str is passed to 'dim' then
5944         returns a DataArray with dtype int.
5945 
5946         If there are multiple maxima, the indices of the first one found will be
5947         returned.
5948 
5949         Parameters
5950         ----------
5951         dim : "...", str, Iterable of Hashable or None, optional
5952             The dimensions over which to find the maximum. By default, finds maximum over
5953             all dimensions - for now returning an int for backward compatibility, but
5954             this is deprecated, in future will return a dict with indices for all
5955             dimensions; to return a dict with all dimensions now, pass '...'.
5956         axis : int or None, optional
5957             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
5958             can be supplied.
5959         keep_attrs : bool or None, optional
5960             If True, the attributes (`attrs`) will be copied from the original
5961             object to the new one. If False, the new object will be
5962             returned without attributes.
5963         skipna : bool or None, optional
5964             If True, skip missing values (as marked by NaN). By default, only
5965             skips missing values for float dtypes; other dtypes either do not
5966             have a sentinel missing value (int) or skipna=True has not been
5967             implemented (object, datetime64 or timedelta64).
5968 
5969         Returns
5970         -------
5971         result : DataArray or dict of DataArray
5972 
5973         See Also
5974         --------
5975         Variable.argmax, DataArray.idxmax
5976 
5977         Examples
5978         --------
5979         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
5980         >>> array.max()
5981         <xarray.DataArray ()>
5982         array(3)
5983         >>> array.argmax(...)
5984         {'x': <xarray.DataArray ()>
5985         array(3)}
5986         >>> array.isel(array.argmax(...))
5987         <xarray.DataArray ()>
5988         array(3)
5989 
5990         >>> array = xr.DataArray(
5991         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],
5992         ...     dims=("x", "y", "z"),
5993         ... )
5994         >>> array.max(dim="x")
5995         <xarray.DataArray (y: 3, z: 3)>
5996         array([[3, 3, 2],
5997                [3, 5, 2],
5998                [2, 3, 3]])
5999         Dimensions without coordinates: y, z
6000         >>> array.argmax(dim="x")
6001         <xarray.DataArray (y: 3, z: 3)>
6002         array([[0, 1, 1],
6003                [0, 1, 0],
6004                [0, 1, 0]])
6005         Dimensions without coordinates: y, z
6006         >>> array.argmax(dim=["x"])
6007         {'x': <xarray.DataArray (y: 3, z: 3)>
6008         array([[0, 1, 1],
6009                [0, 1, 0],
6010                [0, 1, 0]])
6011         Dimensions without coordinates: y, z}
6012         >>> array.max(dim=("x", "z"))
6013         <xarray.DataArray (y: 3)>
6014         array([3, 5, 3])
6015         Dimensions without coordinates: y
6016         >>> array.argmax(dim=["x", "z"])
6017         {'x': <xarray.DataArray (y: 3)>
6018         array([0, 1, 0])
6019         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
6020         array([0, 1, 2])
6021         Dimensions without coordinates: y}
6022         >>> array.isel(array.argmax(dim=["x", "z"]))
6023         <xarray.DataArray (y: 3)>
6024         array([3, 5, 3])
6025         Dimensions without coordinates: y
6026         """
6027         result = self.variable.argmax(dim, axis, keep_attrs, skipna)
6028         if isinstance(result, dict):
6029             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
6030         else:
6031             return self._replace_maybe_drop_dims(result)
6032 
6033     def query(
6034         self,
6035         queries: Mapping[Any, Any] | None = None,
6036         parser: QueryParserOptions = "pandas",
6037         engine: QueryEngineOptions = None,
6038         missing_dims: ErrorOptionsWithWarn = "raise",
6039         **queries_kwargs: Any,
6040     ) -> DataArray:
6041         """Return a new data array indexed along the specified
6042         dimension(s), where the indexers are given as strings containing
6043         Python expressions to be evaluated against the values in the array.
6044 
6045         Parameters
6046         ----------
6047         queries : dict-like or None, optional
6048             A dict-like with keys matching dimensions and values given by strings
6049             containing Python expressions to be evaluated against the data variables
6050             in the dataset. The expressions will be evaluated using the pandas
6051             eval() function, and can contain any valid Python expressions but cannot
6052             contain any Python statements.
6053         parser : {"pandas", "python"}, default: "pandas"
6054             The parser to use to construct the syntax tree from the expression.
6055             The default of 'pandas' parses code slightly different than standard
6056             Python. Alternatively, you can parse an expression using the 'python'
6057             parser to retain strict Python semantics.
6058         engine : {"python", "numexpr", None}, default: None
6059             The engine used to evaluate the expression. Supported engines are:
6060 
6061             - None: tries to use numexpr, falls back to python
6062             - "numexpr": evaluates expressions using numexpr
6063             - "python": performs operations as if you had eval’d in top level python
6064 
6065         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
6066             What to do if dimensions that should be selected from are not present in the
6067             DataArray:
6068 
6069             - "raise": raise an exception
6070             - "warn": raise a warning, and ignore the missing dimensions
6071             - "ignore": ignore the missing dimensions
6072 
6073         **queries_kwargs : {dim: query, ...}, optional
6074             The keyword arguments form of ``queries``.
6075             One of queries or queries_kwargs must be provided.
6076 
6077         Returns
6078         -------
6079         obj : DataArray
6080             A new DataArray with the same contents as this dataset, indexed by
6081             the results of the appropriate queries.
6082 
6083         See Also
6084         --------
6085         DataArray.isel
6086         Dataset.query
6087         pandas.eval
6088 
6089         Examples
6090         --------
6091         >>> da = xr.DataArray(np.arange(0, 5, 1), dims="x", name="a")
6092         >>> da
6093         <xarray.DataArray 'a' (x: 5)>
6094         array([0, 1, 2, 3, 4])
6095         Dimensions without coordinates: x
6096         >>> da.query(x="a > 2")
6097         <xarray.DataArray 'a' (x: 2)>
6098         array([3, 4])
6099         Dimensions without coordinates: x
6100         """
6101 
6102         ds = self._to_dataset_whole(shallow_copy=True)
6103         ds = ds.query(
6104             queries=queries,
6105             parser=parser,
6106             engine=engine,
6107             missing_dims=missing_dims,
6108             **queries_kwargs,
6109         )
6110         return ds[self.name]
6111 
6112     def curvefit(
6113         self,
6114         coords: str | DataArray | Iterable[str | DataArray],
6115         func: Callable[..., Any],
6116         reduce_dims: Dims = None,
6117         skipna: bool = True,
6118         p0: dict[str, Any] | None = None,
6119         bounds: dict[str, Any] | None = None,
6120         param_names: Sequence[str] | None = None,
6121         kwargs: dict[str, Any] | None = None,
6122     ) -> Dataset:
6123         """
6124         Curve fitting optimization for arbitrary functions.
6125 
6126         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
6127 
6128         Parameters
6129         ----------
6130         coords : Hashable, DataArray, or sequence of DataArray or Hashable
6131             Independent coordinate(s) over which to perform the curve fitting. Must share
6132             at least one dimension with the calling object. When fitting multi-dimensional
6133             functions, supply `coords` as a sequence in the same order as arguments in
6134             `func`. To fit along existing dimensions of the calling object, `coords` can
6135             also be specified as a str or sequence of strs.
6136         func : callable
6137             User specified function in the form `f(x, *params)` which returns a numpy
6138             array of length `len(x)`. `params` are the fittable parameters which are optimized
6139             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
6140             coordinates, e.g. `f((x0, x1), *params)`.
6141         reduce_dims : str, Iterable of Hashable or None, optional
6142             Additional dimension(s) over which to aggregate while fitting. For example,
6143             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
6144             aggregate all lat and lon points and fit the specified function along the
6145             time dimension.
6146         skipna : bool, default: True
6147             Whether to skip missing values when fitting. Default is True.
6148         p0 : dict-like or None, optional
6149             Optional dictionary of parameter names to initial guesses passed to the
6150             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
6151             be assigned initial values following the default scipy behavior.
6152         bounds : dict-like or None, optional
6153             Optional dictionary of parameter names to bounding values passed to the
6154             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
6155             will be unbounded following the default scipy behavior.
6156         param_names : sequence of Hashable or None, optional
6157             Sequence of names for the fittable parameters of `func`. If not supplied,
6158             this will be automatically determined by arguments of `func`. `param_names`
6159             should be manually supplied when fitting a function that takes a variable
6160             number of parameters.
6161         **kwargs : optional
6162             Additional keyword arguments to passed to scipy curve_fit.
6163 
6164         Returns
6165         -------
6166         curvefit_results : Dataset
6167             A single dataset which contains:
6168 
6169             [var]_curvefit_coefficients
6170                 The coefficients of the best fit.
6171             [var]_curvefit_covariance
6172                 The covariance matrix of the coefficient estimates.
6173 
6174         See Also
6175         --------
6176         DataArray.polyfit
6177         scipy.optimize.curve_fit
6178         """
6179         return self._to_temp_dataset().curvefit(
6180             coords,
6181             func,
6182             reduce_dims=reduce_dims,
6183             skipna=skipna,
6184             p0=p0,
6185             bounds=bounds,
6186             param_names=param_names,
6187             kwargs=kwargs,
6188         )
6189 
6190     def drop_duplicates(
6191         self: T_DataArray,
6192         dim: Hashable | Iterable[Hashable],
6193         keep: Literal["first", "last", False] = "first",
6194     ) -> T_DataArray:
6195         """Returns a new DataArray with duplicate dimension values removed.
6196 
6197         Parameters
6198         ----------
6199         dim : dimension label or labels
6200             Pass `...` to drop duplicates along all dimensions.
6201         keep : {"first", "last", False}, default: "first"
6202             Determines which duplicates (if any) to keep.
6203 
6204             - ``"first"`` : Drop duplicates except for the first occurrence.
6205             - ``"last"`` : Drop duplicates except for the last occurrence.
6206             - False : Drop all duplicates.
6207 
6208         Returns
6209         -------
6210         DataArray
6211 
6212         See Also
6213         --------
6214         Dataset.drop_duplicates
6215 
6216         Examples
6217         --------
6218         >>> da = xr.DataArray(
6219         ...     np.arange(25).reshape(5, 5),
6220         ...     dims=("x", "y"),
6221         ...     coords={"x": np.array([0, 0, 1, 2, 3]), "y": np.array([0, 1, 2, 3, 3])},
6222         ... )
6223         >>> da
6224         <xarray.DataArray (x: 5, y: 5)>
6225         array([[ 0,  1,  2,  3,  4],
6226                [ 5,  6,  7,  8,  9],
6227                [10, 11, 12, 13, 14],
6228                [15, 16, 17, 18, 19],
6229                [20, 21, 22, 23, 24]])
6230         Coordinates:
6231           * x        (x) int64 0 0 1 2 3
6232           * y        (y) int64 0 1 2 3 3
6233 
6234         >>> da.drop_duplicates(dim="x")
6235         <xarray.DataArray (x: 4, y: 5)>
6236         array([[ 0,  1,  2,  3,  4],
6237                [10, 11, 12, 13, 14],
6238                [15, 16, 17, 18, 19],
6239                [20, 21, 22, 23, 24]])
6240         Coordinates:
6241           * x        (x) int64 0 1 2 3
6242           * y        (y) int64 0 1 2 3 3
6243 
6244         >>> da.drop_duplicates(dim="x", keep="last")
6245         <xarray.DataArray (x: 4, y: 5)>
6246         array([[ 5,  6,  7,  8,  9],
6247                [10, 11, 12, 13, 14],
6248                [15, 16, 17, 18, 19],
6249                [20, 21, 22, 23, 24]])
6250         Coordinates:
6251           * x        (x) int64 0 1 2 3
6252           * y        (y) int64 0 1 2 3 3
6253 
6254         Drop all duplicate dimension values:
6255 
6256         >>> da.drop_duplicates(dim=...)
6257         <xarray.DataArray (x: 4, y: 4)>
6258         array([[ 0,  1,  2,  3],
6259                [10, 11, 12, 13],
6260                [15, 16, 17, 18],
6261                [20, 21, 22, 23]])
6262         Coordinates:
6263           * x        (x) int64 0 1 2 3
6264           * y        (y) int64 0 1 2 3
6265         """
6266         deduplicated = self._to_temp_dataset().drop_duplicates(dim, keep=keep)
6267         return self._from_temp_dataset(deduplicated)
6268 
6269     def convert_calendar(
6270         self,
6271         calendar: str,
6272         dim: str = "time",
6273         align_on: str | None = None,
6274         missing: Any | None = None,
6275         use_cftime: bool | None = None,
6276     ) -> DataArray:
6277         """Convert the DataArray to another calendar.
6278 
6279         Only converts the individual timestamps, does not modify any data except
6280         in dropping invalid/surplus dates or inserting missing dates.
6281 
6282         If the source and target calendars are either no_leap, all_leap or a
6283         standard type, only the type of the time array is modified.
6284         When converting to a leap year from a non-leap year, the 29th of February
6285         is removed from the array. In the other direction the 29th of February
6286         will be missing in the output, unless `missing` is specified,
6287         in which case that value is inserted.
6288 
6289         For conversions involving `360_day` calendars, see Notes.
6290 
6291         This method is safe to use with sub-daily data as it doesn't touch the
6292         time part of the timestamps.
6293 
6294         Parameters
6295         ---------
6296         calendar : str
6297             The target calendar name.
6298         dim : str
6299             Name of the time coordinate.
6300         align_on : {None, 'date', 'year'}
6301             Must be specified when either source or target is a `360_day` calendar,
6302            ignored otherwise. See Notes.
6303         missing : Optional[any]
6304             By default, i.e. if the value is None, this method will simply attempt
6305             to convert the dates in the source calendar to the same dates in the
6306             target calendar, and drop any of those that are not possible to
6307             represent.  If a value is provided, a new time coordinate will be
6308             created in the target calendar with the same frequency as the original
6309             time coordinate; for any dates that are not present in the source, the
6310             data will be filled with this value.  Note that using this mode requires
6311             that the source data have an inferable frequency; for more information
6312             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
6313             target calendar combinations, this could result in many missing values, see notes.
6314         use_cftime : boolean, optional
6315             Whether to use cftime objects in the output, only used if `calendar`
6316             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
6317             If True, the new time axis uses cftime objects.
6318             If None (default), it uses :py:class:`numpy.datetime64` values if the
6319             date range permits it, and :py:class:`cftime.datetime` objects if not.
6320             If False, it uses :py:class:`numpy.datetime64`  or fails.
6321 
6322         Returns
6323         -------
6324         DataArray
6325             Copy of the dataarray with the time coordinate converted to the
6326             target calendar. If 'missing' was None (default), invalid dates in
6327             the new calendar are dropped, but missing dates are not inserted.
6328             If `missing` was given, the new data is reindexed to have a time axis
6329             with the same frequency as the source, but in the new calendar; any
6330             missing datapoints are filled with `missing`.
6331 
6332         Notes
6333         -----
6334         Passing a value to `missing` is only usable if the source's time coordinate as an
6335         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
6336         if the target coordinate, generated from this frequency, has dates equivalent to the
6337         source. It is usually **not** appropriate to use this mode with:
6338 
6339         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
6340         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
6341             or 'mH' where 24 % m != 0).
6342 
6343         If one of the source or target calendars is `"360_day"`, `align_on` must
6344         be specified and two options are offered.
6345 
6346         - "year"
6347             The dates are translated according to their relative position in the year,
6348             ignoring their original month and day information, meaning that the
6349             missing/surplus days are added/removed at regular intervals.
6350 
6351             From a `360_day` to a standard calendar, the output will be missing the
6352             following dates (day of year in parentheses):
6353 
6354             To a leap year:
6355                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
6356                 September 31st (275) and November 30th (335).
6357             To a non-leap year:
6358                 February 6th (36), April 19th (109), July 2nd (183),
6359                 September 12th (255), November 25th (329).
6360 
6361             From a standard calendar to a `"360_day"`, the following dates in the
6362             source array will be dropped:
6363 
6364             From a leap year:
6365                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
6366                 September 31st (275), December 1st (336)
6367             From a non-leap year:
6368                 February 6th (37), April 20th (110), July 2nd (183),
6369                 September 13th (256), November 25th (329)
6370 
6371             This option is best used on daily and subdaily data.
6372 
6373         - "date"
6374             The month/day information is conserved and invalid dates are dropped
6375             from the output. This means that when converting from a `"360_day"` to a
6376             standard calendar, all 31st (Jan, March, May, July, August, October and
6377             December) will be missing as there is no equivalent dates in the
6378             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
6379             will be dropped as there are no equivalent dates in a standard calendar.
6380 
6381             This option is best used with data on a frequency coarser than daily.
6382         """
6383         return convert_calendar(
6384             self,
6385             calendar,
6386             dim=dim,
6387             align_on=align_on,
6388             missing=missing,
6389             use_cftime=use_cftime,
6390         )
6391 
6392     def interp_calendar(
6393         self,
6394         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
6395         dim: str = "time",
6396     ) -> DataArray:
6397         """Interpolates the DataArray to another calendar based on decimal year measure.
6398 
6399         Each timestamp in `source` and `target` are first converted to their decimal
6400         year equivalent then `source` is interpolated on the target coordinate.
6401         The decimal year of a timestamp is its year plus its sub-year component
6402         converted to the fraction of its year. For example "2000-03-01 12:00" is
6403         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
6404 
6405         This method should only be used when the time (HH:MM:SS) information of
6406         time coordinate is not important.
6407 
6408         Parameters
6409         ----------
6410         target: DataArray or DatetimeIndex or CFTimeIndex
6411             The target time coordinate of a valid dtype
6412             (np.datetime64 or cftime objects)
6413         dim : str
6414             The time coordinate name.
6415 
6416         Return
6417         ------
6418         DataArray
6419             The source interpolated on the decimal years of target,
6420         """
6421         return interp_calendar(self, target, dim=dim)
6422 
6423     def groupby(
6424         self,
6425         group: Hashable | DataArray | IndexVariable,
6426         squeeze: bool = True,
6427         restore_coord_dims: bool = False,
6428     ) -> DataArrayGroupBy:
6429         """Returns a DataArrayGroupBy object for performing grouped operations.
6430 
6431         Parameters
6432         ----------
6433         group : Hashable, DataArray or IndexVariable
6434             Array whose unique values should be used to group this array. If a
6435             Hashable, must be the name of a coordinate contained in this dataarray.
6436         squeeze : bool, default: True
6437             If "group" is a dimension of any arrays in this dataset, `squeeze`
6438             controls whether the subarrays have a dimension of length 1 along
6439             that dimension or if the dimension is squeezed out.
6440         restore_coord_dims : bool, default: False
6441             If True, also restore the dimension order of multi-dimensional
6442             coordinates.
6443 
6444         Returns
6445         -------
6446         grouped : DataArrayGroupBy
6447             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
6448             iterated over in the form of `(unique_value, grouped_array)` pairs.
6449 
6450         Examples
6451         --------
6452         Calculate daily anomalies for daily data:
6453 
6454         >>> da = xr.DataArray(
6455         ...     np.linspace(0, 1826, num=1827),
6456         ...     coords=[pd.date_range("2000-01-01", "2004-12-31", freq="D")],
6457         ...     dims="time",
6458         ... )
6459         >>> da
6460         <xarray.DataArray (time: 1827)>
6461         array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03,
6462                1.826e+03])
6463         Coordinates:
6464           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
6465         >>> da.groupby("time.dayofyear") - da.groupby("time.dayofyear").mean("time")
6466         <xarray.DataArray (time: 1827)>
6467         array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])
6468         Coordinates:
6469           * time       (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
6470             dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 359 360 361 362 363 364 365 366
6471 
6472         See Also
6473         --------
6474         :ref:`groupby`
6475             Users guide explanation of how to group and bin data.
6476         DataArray.groupby_bins
6477         Dataset.groupby
6478         core.groupby.DataArrayGroupBy
6479         pandas.DataFrame.groupby
6480         """
6481         from xarray.core.groupby import (
6482             DataArrayGroupBy,
6483             ResolvedUniqueGrouper,
6484             UniqueGrouper,
6485             _validate_groupby_squeeze,
6486         )
6487 
6488         _validate_groupby_squeeze(squeeze)
6489         rgrouper = ResolvedUniqueGrouper(UniqueGrouper(), group, self)
6490         return DataArrayGroupBy(
6491             self,
6492             (rgrouper,),
6493             squeeze=squeeze,
6494             restore_coord_dims=restore_coord_dims,
6495         )
6496 
6497     def groupby_bins(
6498         self,
6499         group: Hashable | DataArray | IndexVariable,
6500         bins: ArrayLike,
6501         right: bool = True,
6502         labels: ArrayLike | Literal[False] | None = None,
6503         precision: int = 3,
6504         include_lowest: bool = False,
6505         squeeze: bool = True,
6506         restore_coord_dims: bool = False,
6507     ) -> DataArrayGroupBy:
6508         """Returns a DataArrayGroupBy object for performing grouped operations.
6509 
6510         Rather than using all unique values of `group`, the values are discretized
6511         first by applying `pandas.cut` [1]_ to `group`.
6512 
6513         Parameters
6514         ----------
6515         group : Hashable, DataArray or IndexVariable
6516             Array whose binned values should be used to group this array. If a
6517             Hashable, must be the name of a coordinate contained in this dataarray.
6518         bins : int or array-like
6519             If bins is an int, it defines the number of equal-width bins in the
6520             range of x. However, in this case, the range of x is extended by .1%
6521             on each side to include the min or max values of x. If bins is a
6522             sequence it defines the bin edges allowing for non-uniform bin
6523             width. No extension of the range of x is done in this case.
6524         right : bool, default: True
6525             Indicates whether the bins include the rightmost edge or not. If
6526             right == True (the default), then the bins [1,2,3,4] indicate
6527             (1,2], (2,3], (3,4].
6528         labels : array-like, False or None, default: None
6529             Used as labels for the resulting bins. Must be of the same length as
6530             the resulting bins. If False, string bin labels are assigned by
6531             `pandas.cut`.
6532         precision : int, default: 3
6533             The precision at which to store and display the bins labels.
6534         include_lowest : bool, default: False
6535             Whether the first interval should be left-inclusive or not.
6536         squeeze : bool, default: True
6537             If "group" is a dimension of any arrays in this dataset, `squeeze`
6538             controls whether the subarrays have a dimension of length 1 along
6539             that dimension or if the dimension is squeezed out.
6540         restore_coord_dims : bool, default: False
6541             If True, also restore the dimension order of multi-dimensional
6542             coordinates.
6543 
6544         Returns
6545         -------
6546         grouped : DataArrayGroupBy
6547             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
6548             iterated over in the form of `(unique_value, grouped_array)` pairs.
6549             The name of the group has the added suffix `_bins` in order to
6550             distinguish it from the original variable.
6551 
6552         See Also
6553         --------
6554         :ref:`groupby`
6555             Users guide explanation of how to group and bin data.
6556         DataArray.groupby
6557         Dataset.groupby_bins
6558         core.groupby.DataArrayGroupBy
6559         pandas.DataFrame.groupby
6560 
6561         References
6562         ----------
6563         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
6564         """
6565         from xarray.core.groupby import (
6566             BinGrouper,
6567             DataArrayGroupBy,
6568             ResolvedBinGrouper,
6569             _validate_groupby_squeeze,
6570         )
6571 
6572         _validate_groupby_squeeze(squeeze)
6573         grouper = BinGrouper(
6574             bins=bins,
6575             cut_kwargs={
6576                 "right": right,
6577                 "labels": labels,
6578                 "precision": precision,
6579                 "include_lowest": include_lowest,
6580             },
6581         )
6582         rgrouper = ResolvedBinGrouper(grouper, group, self)
6583 
6584         return DataArrayGroupBy(
6585             self,
6586             (rgrouper,),
6587             squeeze=squeeze,
6588             restore_coord_dims=restore_coord_dims,
6589         )
6590 
6591     def weighted(self, weights: DataArray) -> DataArrayWeighted:
6592         """
6593         Weighted DataArray operations.
6594 
6595         Parameters
6596         ----------
6597         weights : DataArray
6598             An array of weights associated with the values in this Dataset.
6599             Each value in the data contributes to the reduction operation
6600             according to its associated weight.
6601 
6602         Notes
6603         -----
6604         ``weights`` must be a DataArray and cannot contain missing values.
6605         Missing values can be replaced by ``weights.fillna(0)``.
6606 
6607         Returns
6608         -------
6609         core.weighted.DataArrayWeighted
6610 
6611         See Also
6612         --------
6613         Dataset.weighted
6614         """
6615         from xarray.core.weighted import DataArrayWeighted
6616 
6617         return DataArrayWeighted(self, weights)
6618 
6619     def rolling(
6620         self,
6621         dim: Mapping[Any, int] | None = None,
6622         min_periods: int | None = None,
6623         center: bool | Mapping[Any, bool] = False,
6624         **window_kwargs: int,
6625     ) -> DataArrayRolling:
6626         """
6627         Rolling window object for DataArrays.
6628 
6629         Parameters
6630         ----------
6631         dim : dict, optional
6632             Mapping from the dimension name to create the rolling iterator
6633             along (e.g. `time`) to its moving window size.
6634         min_periods : int or None, default: None
6635             Minimum number of observations in window required to have a value
6636             (otherwise result is NA). The default, None, is equivalent to
6637             setting min_periods equal to the size of the window.
6638         center : bool or Mapping to int, default: False
6639             Set the labels at the center of the window.
6640         **window_kwargs : optional
6641             The keyword arguments form of ``dim``.
6642             One of dim or window_kwargs must be provided.
6643 
6644         Returns
6645         -------
6646         core.rolling.DataArrayRolling
6647 
6648         Examples
6649         --------
6650         Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:
6651 
6652         >>> da = xr.DataArray(
6653         ...     np.linspace(0, 11, num=12),
6654         ...     coords=[
6655         ...         pd.date_range(
6656         ...             "1999-12-15",
6657         ...             periods=12,
6658         ...             freq=pd.DateOffset(months=1),
6659         ...         )
6660         ...     ],
6661         ...     dims="time",
6662         ... )
6663         >>> da
6664         <xarray.DataArray (time: 12)>
6665         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
6666         Coordinates:
6667           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6668         >>> da.rolling(time=3, center=True).mean()
6669         <xarray.DataArray (time: 12)>
6670         array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])
6671         Coordinates:
6672           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6673 
6674         Remove the NaNs using ``dropna()``:
6675 
6676         >>> da.rolling(time=3, center=True).mean().dropna("time")
6677         <xarray.DataArray (time: 10)>
6678         array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])
6679         Coordinates:
6680           * time     (time) datetime64[ns] 2000-01-15 2000-02-15 ... 2000-10-15
6681 
6682         See Also
6683         --------
6684         core.rolling.DataArrayRolling
6685         Dataset.rolling
6686         """
6687         from xarray.core.rolling import DataArrayRolling
6688 
6689         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
6690         return DataArrayRolling(self, dim, min_periods=min_periods, center=center)
6691 
6692     def coarsen(
6693         self,
6694         dim: Mapping[Any, int] | None = None,
6695         boundary: CoarsenBoundaryOptions = "exact",
6696         side: SideOptions | Mapping[Any, SideOptions] = "left",
6697         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
6698         **window_kwargs: int,
6699     ) -> DataArrayCoarsen:
6700         """
6701         Coarsen object for DataArrays.
6702 
6703         Parameters
6704         ----------
6705         dim : mapping of hashable to int, optional
6706             Mapping from the dimension name to the window size.
6707         boundary : {"exact", "trim", "pad"}, default: "exact"
6708             If 'exact', a ValueError will be raised if dimension size is not a
6709             multiple of the window size. If 'trim', the excess entries are
6710             dropped. If 'pad', NA will be padded.
6711         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
6712         coord_func : str or mapping of hashable to str, default: "mean"
6713             function (name) that is applied to the coordinates,
6714             or a mapping from coordinate name to function (name).
6715 
6716         Returns
6717         -------
6718         core.rolling.DataArrayCoarsen
6719 
6720         Examples
6721         --------
6722         Coarsen the long time series by averaging over every three days.
6723 
6724         >>> da = xr.DataArray(
6725         ...     np.linspace(0, 364, num=364),
6726         ...     dims="time",
6727         ...     coords={"time": pd.date_range("1999-12-15", periods=364)},
6728         ... )
6729         >>> da  # +doctest: ELLIPSIS
6730         <xarray.DataArray (time: 364)>
6731         array([  0.        ,   1.00275482,   2.00550964,   3.00826446,
6732                  4.01101928,   5.0137741 ,   6.01652893,   7.01928375,
6733                  8.02203857,   9.02479339,  10.02754821,  11.03030303,
6734         ...
6735                356.98071625, 357.98347107, 358.9862259 , 359.98898072,
6736                360.99173554, 361.99449036, 362.99724518, 364.        ])
6737         Coordinates:
6738           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-12-12
6739         >>> da.coarsen(time=3, boundary="trim").mean()  # +doctest: ELLIPSIS
6740         <xarray.DataArray (time: 121)>
6741         array([  1.00275482,   4.01101928,   7.01928375,  10.02754821,
6742                 13.03581267,  16.04407713,  19.0523416 ,  22.06060606,
6743                 25.06887052,  28.07713499,  31.08539945,  34.09366391,
6744         ...
6745                349.96143251, 352.96969697, 355.97796143, 358.9862259 ,
6746                361.99449036])
6747         Coordinates:
6748           * time     (time) datetime64[ns] 1999-12-16 1999-12-19 ... 2000-12-10
6749         >>>
6750 
6751         See Also
6752         --------
6753         core.rolling.DataArrayCoarsen
6754         Dataset.coarsen
6755         """
6756         from xarray.core.rolling import DataArrayCoarsen
6757 
6758         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
6759         return DataArrayCoarsen(
6760             self,
6761             dim,
6762             boundary=boundary,
6763             side=side,
6764             coord_func=coord_func,
6765         )
6766 
6767     def resample(
6768         self,
6769         indexer: Mapping[Any, str] | None = None,
6770         skipna: bool | None = None,
6771         closed: SideOptions | None = None,
6772         label: SideOptions | None = None,
6773         base: int | None = None,
6774         offset: pd.Timedelta | datetime.timedelta | str | None = None,
6775         origin: str | DatetimeLike = "start_day",
6776         keep_attrs: bool | None = None,
6777         loffset: datetime.timedelta | str | None = None,
6778         restore_coord_dims: bool | None = None,
6779         **indexer_kwargs: str,
6780     ) -> DataArrayResample:
6781         """Returns a Resample object for performing resampling operations.
6782 
6783         Handles both downsampling and upsampling. The resampled
6784         dimension must be a datetime-like coordinate. If any intervals
6785         contain no values from the original object, they will be given
6786         the value ``NaN``.
6787 
6788         Parameters
6789         ----------
6790         indexer : Mapping of Hashable to str, optional
6791             Mapping from the dimension name to resample frequency [1]_. The
6792             dimension must be datetime-like.
6793         skipna : bool, optional
6794             Whether to skip missing values when aggregating in downsampling.
6795         closed : {"left", "right"}, optional
6796             Side of each interval to treat as closed.
6797         label : {"left", "right"}, optional
6798             Side of each interval to use for labeling.
6799         base : int, optional
6800             For frequencies that evenly subdivide 1 day, the "origin" of the
6801             aggregated intervals. For example, for "24H" frequency, base could
6802             range from 0 through 23.
6803         origin : {'epoch', 'start', 'start_day', 'end', 'end_day'}, pd.Timestamp, datetime.datetime, np.datetime64, or cftime.datetime, default 'start_day'
6804             The datetime on which to adjust the grouping. The timezone of origin
6805             must match the timezone of the index.
6806 
6807             If a datetime is not used, these values are also supported:
6808             - 'epoch': `origin` is 1970-01-01
6809             - 'start': `origin` is the first value of the timeseries
6810             - 'start_day': `origin` is the first day at midnight of the timeseries
6811             - 'end': `origin` is the last value of the timeseries
6812             - 'end_day': `origin` is the ceiling midnight of the last day
6813         offset : pd.Timedelta, datetime.timedelta, or str, default is None
6814             An offset timedelta added to the origin.
6815         loffset : timedelta or str, optional
6816             Offset used to adjust the resampled time labels. Some pandas date
6817             offset strings are supported.
6818         restore_coord_dims : bool, optional
6819             If True, also restore the dimension order of multi-dimensional
6820             coordinates.
6821         **indexer_kwargs : str
6822             The keyword arguments form of ``indexer``.
6823             One of indexer or indexer_kwargs must be provided.
6824 
6825         Returns
6826         -------
6827         resampled : core.resample.DataArrayResample
6828             This object resampled.
6829 
6830         Examples
6831         --------
6832         Downsample monthly time-series data to seasonal data:
6833 
6834         >>> da = xr.DataArray(
6835         ...     np.linspace(0, 11, num=12),
6836         ...     coords=[
6837         ...         pd.date_range(
6838         ...             "1999-12-15",
6839         ...             periods=12,
6840         ...             freq=pd.DateOffset(months=1),
6841         ...         )
6842         ...     ],
6843         ...     dims="time",
6844         ... )
6845         >>> da
6846         <xarray.DataArray (time: 12)>
6847         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
6848         Coordinates:
6849           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6850         >>> da.resample(time="QS-DEC").mean()
6851         <xarray.DataArray (time: 4)>
6852         array([ 1.,  4.,  7., 10.])
6853         Coordinates:
6854           * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01
6855 
6856         Upsample monthly time-series data to daily data:
6857 
6858         >>> da.resample(time="1D").interpolate("linear")  # +doctest: ELLIPSIS
6859         <xarray.DataArray (time: 337)>
6860         array([ 0.        ,  0.03225806,  0.06451613,  0.09677419,  0.12903226,
6861                 0.16129032,  0.19354839,  0.22580645,  0.25806452,  0.29032258,
6862                 0.32258065,  0.35483871,  0.38709677,  0.41935484,  0.4516129 ,
6863         ...
6864                10.80645161, 10.83870968, 10.87096774, 10.90322581, 10.93548387,
6865                10.96774194, 11.        ])
6866         Coordinates:
6867           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
6868 
6869         Limit scope of upsampling method
6870 
6871         >>> da.resample(time="1D").nearest(tolerance="1D")
6872         <xarray.DataArray (time: 337)>
6873         array([ 0.,  0., nan, ..., nan, 11., 11.])
6874         Coordinates:
6875           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
6876 
6877         See Also
6878         --------
6879         Dataset.resample
6880         pandas.Series.resample
6881         pandas.DataFrame.resample
6882 
6883         References
6884         ----------
6885         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
6886         """
6887         from xarray.core.resample import DataArrayResample
6888 
6889         return self._resample(
6890             resample_cls=DataArrayResample,
6891             indexer=indexer,
6892             skipna=skipna,
6893             closed=closed,
6894             label=label,
6895             base=base,
6896             offset=offset,
6897             origin=origin,
6898             keep_attrs=keep_attrs,
6899             loffset=loffset,
6900             restore_coord_dims=restore_coord_dims,
6901             **indexer_kwargs,
6902         )
6903 
6904     def to_dask_dataframe(
6905         self,
6906         dim_order: Sequence[Hashable] | None = None,
6907         set_index: bool = False,
6908     ) -> DaskDataFrame:
6909         """Convert this array into a dask.dataframe.DataFrame.
6910 
6911         Parameters
6912         ----------
6913         dim_order : Sequence of Hashable or None , optional
6914             Hierarchical dimension order for the resulting dataframe.
6915             Array content is transposed to this order and then written out as flat
6916             vectors in contiguous order, so the last dimension in this list
6917             will be contiguous in the resulting DataFrame. This has a major influence
6918             on which operations are efficient on the resulting dask dataframe.
6919         set_index : bool, default: False
6920             If set_index=True, the dask DataFrame is indexed by this dataset's
6921             coordinate. Since dask DataFrames do not support multi-indexes,
6922             set_index only works if the dataset only contains one dimension.
6923 
6924         Returns
6925         -------
6926         dask.dataframe.DataFrame
6927 
6928         Examples
6929         --------
6930         >>> da = xr.DataArray(
6931         ...     np.arange(4 * 2 * 2).reshape(4, 2, 2),
6932         ...     dims=("time", "lat", "lon"),
6933         ...     coords={
6934         ...         "time": np.arange(4),
6935         ...         "lat": [-30, -20],
6936         ...         "lon": [120, 130],
6937         ...     },
6938         ...     name="eg_dataarray",
6939         ...     attrs={"units": "Celsius", "description": "Random temperature data"},
6940         ... )
6941         >>> da.to_dask_dataframe(["lat", "lon", "time"]).compute()
6942             lat  lon  time  eg_dataarray
6943         0   -30  120     0             0
6944         1   -30  120     1             4
6945         2   -30  120     2             8
6946         3   -30  120     3            12
6947         4   -30  130     0             1
6948         5   -30  130     1             5
6949         6   -30  130     2             9
6950         7   -30  130     3            13
6951         8   -20  120     0             2
6952         9   -20  120     1             6
6953         10  -20  120     2            10
6954         11  -20  120     3            14
6955         12  -20  130     0             3
6956         13  -20  130     1             7
6957         14  -20  130     2            11
6958         15  -20  130     3            15
6959         """
6960         if self.name is None:
6961             raise ValueError(
6962                 "Cannot convert an unnamed DataArray to a "
6963                 "dask dataframe : use the ``.rename`` method to assign a name."
6964             )
6965         name = self.name
6966         ds = self._to_dataset_whole(name, shallow_copy=False)
6967         return ds.to_dask_dataframe(dim_order, set_index)
6968 
6969     # this needs to be at the end, or mypy will confuse with `str`
6970     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
6971     str = utils.UncachedAccessor(StringAccessor["DataArray"])

</code><intended_behavior>These methods should be refactored to an interface that can dynamically dispatch the calls to the appropriate backend based on user configuration or an attribute within the DataArray/Dataset. The existing functionality for dask should be preserved, allowing for a seamless transition between backends without altering the user's experience.</intended_behavior>, <file>xarray/core/dataset.py</file>
<class>None</class>
<method>None</method>
<code>
1 from __future__ import annotations
2 
3 import copy
4 import datetime
5 import inspect
6 import itertools
7 import math
8 import sys
9 import warnings
10 from collections import defaultdict
11 from collections.abc import (
12     Collection,
13     Hashable,
14     Iterable,
15     Iterator,
16     Mapping,
17     MutableMapping,
18     Sequence,
19 )
20 from html import escape
21 from numbers import Number
22 from operator import methodcaller
23 from os import PathLike
24 from typing import IO, TYPE_CHECKING, Any, Callable, Generic, Literal, cast, overload
25 
26 import numpy as np
27 import pandas as pd
28 
29 from xarray.coding.calendar_ops import convert_calendar, interp_calendar
30 from xarray.coding.cftimeindex import CFTimeIndex, _parse_array_of_cftime_strings
31 from xarray.core import (
32     alignment,
33     duck_array_ops,
34     formatting,
35     formatting_html,
36     ops,
37     utils,
38 )
39 from xarray.core import dtypes as xrdtypes
40 from xarray.core._aggregations import DatasetAggregations
41 from xarray.core.alignment import (
42     _broadcast_helper,
43     _get_broadcast_dims_map_common_coords,
44     align,
45 )
46 from xarray.core.arithmetic import DatasetArithmetic
47 from xarray.core.common import (
48     DataWithCoords,
49     _contains_datetime_like_objects,
50     get_chunksizes,
51 )
52 from xarray.core.computation import unify_chunks
53 from xarray.core.coordinates import DatasetCoordinates, assert_coordinate_consistent
54 from xarray.core.duck_array_ops import datetime_to_numeric
55 from xarray.core.indexes import (
56     Index,
57     Indexes,
58     PandasIndex,
59     PandasMultiIndex,
60     assert_no_index_corrupted,
61     create_default_index_implicit,
62     filter_indexes_from_coords,
63     isel_indexes,
64     remove_unused_levels_categories,
65     roll_indexes,
66 )
67 from xarray.core.indexing import is_fancy_indexer, map_index_queries
68 from xarray.core.merge import (
69     dataset_merge_method,
70     dataset_update_method,
71     merge_coordinates_without_align,
72     merge_data_and_coords,
73 )
74 from xarray.core.missing import get_clean_interp_index
75 from xarray.core.options import OPTIONS, _get_keep_attrs
76 from xarray.core.pycompat import array_type, is_duck_array, is_duck_dask_array
77 from xarray.core.types import QuantileMethods, T_Dataset
78 from xarray.core.utils import (
79     Default,
80     Frozen,
81     HybridMappingProxy,
82     OrderedSet,
83     _default,
84     decode_numpy_dict_values,
85     drop_dims_from_indexers,
86     either_dict_or_kwargs,
87     infix_dims,
88     is_dict_like,
89     is_scalar,
90     maybe_wrap_array,
91 )
92 from xarray.core.variable import (
93     IndexVariable,
94     Variable,
95     as_variable,
96     broadcast_variables,
97     calculate_dimensions,
98 )
99 from xarray.plot.accessor import DatasetPlotAccessor
100 
101 if TYPE_CHECKING:
102     from numpy.typing import ArrayLike
103 
104     from xarray.backends import AbstractDataStore, ZarrStore
105     from xarray.backends.api import T_NetcdfEngine, T_NetcdfTypes
106     from xarray.core.coordinates import Coordinates
107     from xarray.core.dataarray import DataArray
108     from xarray.core.groupby import DatasetGroupBy
109     from xarray.core.merge import CoercibleMapping
110     from xarray.core.resample import DatasetResample
111     from xarray.core.rolling import DatasetCoarsen, DatasetRolling
112     from xarray.core.types import (
113         CFCalendar,
114         CoarsenBoundaryOptions,
115         CombineAttrsOptions,
116         CompatOptions,
117         DatetimeLike,
118         DatetimeUnitOptions,
119         Dims,
120         ErrorOptions,
121         ErrorOptionsWithWarn,
122         InterpOptions,
123         JoinOptions,
124         PadModeOptions,
125         PadReflectOptions,
126         QueryEngineOptions,
127         QueryParserOptions,
128         ReindexMethodOptions,
129         SideOptions,
130         T_Xarray,
131     )
132     from xarray.core.weighted import DatasetWeighted
133 
134     try:
135         from dask.delayed import Delayed
136     except ImportError:
137         Delayed = None  # type: ignore
138     try:
139         from dask.dataframe import DataFrame as DaskDataFrame
140     except ImportError:
141         DaskDataFrame = None  # type: ignore
142 
143 
144 # list of attributes of pd.DatetimeIndex that are ndarrays of time info
145 _DATETIMEINDEX_COMPONENTS = [
146     "year",
147     "month",
148     "day",
149     "hour",
150     "minute",
151     "second",
152     "microsecond",
153     "nanosecond",
154     "date",
155     "time",
156     "dayofyear",
157     "weekofyear",
158     "dayofweek",
159     "quarter",
160 ]
161 
162 
163 def _get_virtual_variable(
164     variables, key: Hashable, dim_sizes: Mapping | None = None
165 ) -> tuple[Hashable, Hashable, Variable]:
166     """Get a virtual variable (e.g., 'time.year') from a dict of xarray.Variable
167     objects (if possible)
168 
169     """
170     from xarray.core.dataarray import DataArray
171 
172     if dim_sizes is None:
173         dim_sizes = {}
174 
175     if key in dim_sizes:
176         data = pd.Index(range(dim_sizes[key]), name=key)
177         variable = IndexVariable((key,), data)
178         return key, key, variable
179 
180     if not isinstance(key, str):
181         raise KeyError(key)
182 
183     split_key = key.split(".", 1)
184     if len(split_key) != 2:
185         raise KeyError(key)
186 
187     ref_name, var_name = split_key
188     ref_var = variables[ref_name]
189 
190     if _contains_datetime_like_objects(ref_var):
191         ref_var = DataArray(ref_var)
192         data = getattr(ref_var.dt, var_name).data
193     else:
194         data = getattr(ref_var, var_name).data
195     virtual_var = Variable(ref_var.dims, data)
196 
197     return ref_name, var_name, virtual_var
198 
199 
200 def _assert_empty(args: tuple, msg: str = "%s") -> None:
201     if args:
202         raise ValueError(msg % args)
203 
204 
205 def _get_chunk(var, chunks):
206     """
207     Return map from each dim to chunk sizes, accounting for backend's preferred chunks.
208     """
209 
210     import dask.array as da
211 
212     if isinstance(var, IndexVariable):
213         return {}
214     dims = var.dims
215     shape = var.shape
216 
217     # Determine the explicit requested chunks.
218     preferred_chunks = var.encoding.get("preferred_chunks", {})
219     preferred_chunk_shape = tuple(
220         preferred_chunks.get(dim, size) for dim, size in zip(dims, shape)
221     )
222     if isinstance(chunks, Number) or (chunks == "auto"):
223         chunks = dict.fromkeys(dims, chunks)
224     chunk_shape = tuple(
225         chunks.get(dim, None) or preferred_chunk_sizes
226         for dim, preferred_chunk_sizes in zip(dims, preferred_chunk_shape)
227     )
228     chunk_shape = da.core.normalize_chunks(
229         chunk_shape, shape=shape, dtype=var.dtype, previous_chunks=preferred_chunk_shape
230     )
231 
232     # Warn where requested chunks break preferred chunks, provided that the variable
233     # contains data.
234     if var.size:
235         for dim, size, chunk_sizes in zip(dims, shape, chunk_shape):
236             try:
237                 preferred_chunk_sizes = preferred_chunks[dim]
238             except KeyError:
239                 continue
240             # Determine the stop indices of the preferred chunks, but omit the last stop
241             # (equal to the dim size).  In particular, assume that when a sequence
242             # expresses the preferred chunks, the sequence sums to the size.
243             preferred_stops = (
244                 range(preferred_chunk_sizes, size, preferred_chunk_sizes)
245                 if isinstance(preferred_chunk_sizes, Number)
246                 else itertools.accumulate(preferred_chunk_sizes[:-1])
247             )
248             # Gather any stop indices of the specified chunks that are not a stop index
249             # of a preferred chunk.  Again, omit the last stop, assuming that it equals
250             # the dim size.
251             breaks = set(itertools.accumulate(chunk_sizes[:-1])).difference(
252                 preferred_stops
253             )
254             if breaks:
255                 warnings.warn(
256                     "The specified Dask chunks separate the stored chunks along "
257                     f'dimension "{dim}" starting at index {min(breaks)}. This could '
258                     "degrade performance. Instead, consider rechunking after loading."
259                 )
260 
261     return dict(zip(dims, chunk_shape))
262 
263 
264 def _maybe_chunk(
265     name,
266     var,
267     chunks,
268     token=None,
269     lock=None,
270     name_prefix="xarray-",
271     overwrite_encoded_chunks=False,
272     inline_array=False,
273 ):
274     from dask.base import tokenize
275 
276     if chunks is not None:
277         chunks = {dim: chunks[dim] for dim in var.dims if dim in chunks}
278     if var.ndim:
279         # when rechunking by different amounts, make sure dask names change
280         # by provinding chunks as an input to tokenize.
281         # subtle bugs result otherwise. see GH3350
282         token2 = tokenize(name, token if token else var._data, chunks)
283         name2 = f"{name_prefix}{name}-{token2}"
284         var = var.chunk(chunks, name=name2, lock=lock, inline_array=inline_array)
285 
286         if overwrite_encoded_chunks and var.chunks is not None:
287             var.encoding["chunks"] = tuple(x[0] for x in var.chunks)
288         return var
289     else:
290         return var
291 
292 
293 def as_dataset(obj: Any) -> Dataset:
294     """Cast the given object to a Dataset.
295 
296     Handles Datasets, DataArrays and dictionaries of variables. A new Dataset
297     object is only created if the provided object is not already one.
298     """
299     if hasattr(obj, "to_dataset"):
300         obj = obj.to_dataset()
301     if not isinstance(obj, Dataset):
302         obj = Dataset(obj)
303     return obj
304 
305 
306 def _get_func_args(func, param_names):
307     """Use `inspect.signature` to try accessing `func` args. Otherwise, ensure
308     they are provided by user.
309     """
310     try:
311         func_args = inspect.signature(func).parameters
312     except ValueError:
313         func_args = {}
314         if not param_names:
315             raise ValueError(
316                 "Unable to inspect `func` signature, and `param_names` was not provided."
317             )
318     if param_names:
319         params = param_names
320     else:
321         params = list(func_args)[1:]
322         if any(
323             [(p.kind in [p.VAR_POSITIONAL, p.VAR_KEYWORD]) for p in func_args.values()]
324         ):
325             raise ValueError(
326                 "`param_names` must be provided because `func` takes variable length arguments."
327             )
328     return params, func_args
329 
330 
331 def _initialize_curvefit_params(params, p0, bounds, func_args):
332     """Set initial guess and bounds for curvefit.
333     Priority: 1) passed args 2) func signature 3) scipy defaults
334     """
335 
336     def _initialize_feasible(lb, ub):
337         # Mimics functionality of scipy.optimize.minpack._initialize_feasible
338         lb_finite = np.isfinite(lb)
339         ub_finite = np.isfinite(ub)
340         p0 = np.nansum(
341             [
342                 0.5 * (lb + ub) * int(lb_finite & ub_finite),
343                 (lb + 1) * int(lb_finite & ~ub_finite),
344                 (ub - 1) * int(~lb_finite & ub_finite),
345             ]
346         )
347         return p0
348 
349     param_defaults = {p: 1 for p in params}
350     bounds_defaults = {p: (-np.inf, np.inf) for p in params}
351     for p in params:
352         if p in func_args and func_args[p].default is not func_args[p].empty:
353             param_defaults[p] = func_args[p].default
354         if p in bounds:
355             bounds_defaults[p] = tuple(bounds[p])
356             if param_defaults[p] < bounds[p][0] or param_defaults[p] > bounds[p][1]:
357                 param_defaults[p] = _initialize_feasible(bounds[p][0], bounds[p][1])
358         if p in p0:
359             param_defaults[p] = p0[p]
360     return param_defaults, bounds_defaults
361 
362 
363 class DataVariables(Mapping[Any, "DataArray"]):
364     __slots__ = ("_dataset",)
365 
366     def __init__(self, dataset: Dataset):
367         self._dataset = dataset
368 
369     def __iter__(self) -> Iterator[Hashable]:
370         return (
371             key
372             for key in self._dataset._variables
373             if key not in self._dataset._coord_names
374         )
375 
376     def __len__(self) -> int:
377         return len(self._dataset._variables) - len(self._dataset._coord_names)
378 
379     def __contains__(self, key: Hashable) -> bool:
380         return key in self._dataset._variables and key not in self._dataset._coord_names
381 
382     def __getitem__(self, key: Hashable) -> DataArray:
383         if key not in self._dataset._coord_names:
384             return cast("DataArray", self._dataset[key])
385         raise KeyError(key)
386 
387     def __repr__(self) -> str:
388         return formatting.data_vars_repr(self)
389 
390     @property
391     def variables(self) -> Mapping[Hashable, Variable]:
392         all_variables = self._dataset.variables
393         return Frozen({k: all_variables[k] for k in self})
394 
395     @property
396     def dtypes(self) -> Frozen[Hashable, np.dtype]:
397         """Mapping from data variable names to dtypes.
398 
399         Cannot be modified directly, but is updated when adding new variables.
400 
401         See Also
402         --------
403         Dataset.dtype
404         """
405         return self._dataset.dtypes
406 
407     def _ipython_key_completions_(self):
408         """Provide method for the key-autocompletions in IPython."""
409         return [
410             key
411             for key in self._dataset._ipython_key_completions_()
412             if key not in self._dataset._coord_names
413         ]
414 
415 
416 class _LocIndexer(Generic[T_Dataset]):
417     __slots__ = ("dataset",)
418 
419     def __init__(self, dataset: T_Dataset):
420         self.dataset = dataset
421 
422     def __getitem__(self, key: Mapping[Any, Any]) -> T_Dataset:
423         if not utils.is_dict_like(key):
424             raise TypeError("can only lookup dictionaries from Dataset.loc")
425         return self.dataset.sel(key)
426 
427     def __setitem__(self, key, value) -> None:
428         if not utils.is_dict_like(key):
429             raise TypeError(
430                 "can only set locations defined by dictionaries from Dataset.loc."
431                 f" Got: {key}"
432             )
433 
434         # set new values
435         dim_indexers = map_index_queries(self.dataset, key).dim_indexers
436         self.dataset[dim_indexers] = value
437 
438 
439 class Dataset(
440     DataWithCoords,
441     DatasetAggregations,
442     DatasetArithmetic,
443     Mapping[Hashable, "DataArray"],
444 ):
445     """A multi-dimensional, in memory, array database.
446 
447     A dataset resembles an in-memory representation of a NetCDF file,
448     and consists of variables, coordinates and attributes which
449     together form a self describing dataset.
450 
451     Dataset implements the mapping interface with keys given by variable
452     names and values given by DataArray objects for each variable name.
453 
454     One dimensional variables with name equal to their dimension are
455     index coordinates used for label based indexing.
456 
457     To load data from a file or file-like object, use the `open_dataset`
458     function.
459 
460     Parameters
461     ----------
462     data_vars : dict-like, optional
463         A mapping from variable names to :py:class:`~xarray.DataArray`
464         objects, :py:class:`~xarray.Variable` objects or to tuples of
465         the form ``(dims, data[, attrs])`` which can be used as
466         arguments to create a new ``Variable``. Each dimension must
467         have the same length in all variables in which it appears.
468 
469         The following notations are accepted:
470 
471         - mapping {var name: DataArray}
472         - mapping {var name: Variable}
473         - mapping {var name: (dimension name, array-like)}
474         - mapping {var name: (tuple of dimension names, array-like)}
475         - mapping {dimension name: array-like}
476           (it will be automatically moved to coords, see below)
477 
478         Each dimension must have the same length in all variables in
479         which it appears.
480     coords : dict-like, optional
481         Another mapping in similar form as the `data_vars` argument,
482         except the each item is saved on the dataset as a "coordinate".
483         These variables have an associated meaning: they describe
484         constant/fixed/independent quantities, unlike the
485         varying/measured/dependent quantities that belong in
486         `variables`. Coordinates values may be given by 1-dimensional
487         arrays or scalars, in which case `dims` do not need to be
488         supplied: 1D arrays will be assumed to give index values along
489         the dimension with the same name.
490 
491         The following notations are accepted:
492 
493         - mapping {coord name: DataArray}
494         - mapping {coord name: Variable}
495         - mapping {coord name: (dimension name, array-like)}
496         - mapping {coord name: (tuple of dimension names, array-like)}
497         - mapping {dimension name: array-like}
498           (the dimension name is implicitly set to be the same as the
499           coord name)
500 
501         The last notation implies that the coord name is the same as
502         the dimension name.
503 
504     attrs : dict-like, optional
505         Global attributes to save on this dataset.
506 
507     Examples
508     --------
509     Create data:
510 
511     >>> np.random.seed(0)
512     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
513     >>> precipitation = 10 * np.random.rand(2, 2, 3)
514     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
515     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
516     >>> time = pd.date_range("2014-09-06", periods=3)
517     >>> reference_time = pd.Timestamp("2014-09-05")
518 
519     Initialize a dataset with multiple dimensions:
520 
521     >>> ds = xr.Dataset(
522     ...     data_vars=dict(
523     ...         temperature=(["x", "y", "time"], temperature),
524     ...         precipitation=(["x", "y", "time"], precipitation),
525     ...     ),
526     ...     coords=dict(
527     ...         lon=(["x", "y"], lon),
528     ...         lat=(["x", "y"], lat),
529     ...         time=time,
530     ...         reference_time=reference_time,
531     ...     ),
532     ...     attrs=dict(description="Weather related data."),
533     ... )
534     >>> ds
535     <xarray.Dataset>
536     Dimensions:         (x: 2, y: 2, time: 3)
537     Coordinates:
538         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
539         lat             (x, y) float64 42.25 42.21 42.63 42.59
540       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
541         reference_time  datetime64[ns] 2014-09-05
542     Dimensions without coordinates: x, y
543     Data variables:
544         temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
545         precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
546     Attributes:
547         description:  Weather related data.
548 
549     Find out where the coldest temperature was and what values the
550     other variables had:
551 
552     >>> ds.isel(ds.temperature.argmin(...))
553     <xarray.Dataset>
554     Dimensions:         ()
555     Coordinates:
556         lon             float64 -99.32
557         lat             float64 42.21
558         time            datetime64[ns] 2014-09-08
559         reference_time  datetime64[ns] 2014-09-05
560     Data variables:
561         temperature     float64 7.182
562         precipitation   float64 8.326
563     Attributes:
564         description:  Weather related data.
565     """
566 
567     _attrs: dict[Hashable, Any] | None
568     _cache: dict[str, Any]
569     _coord_names: set[Hashable]
570     _dims: dict[Hashable, int]
571     _encoding: dict[Hashable, Any] | None
572     _close: Callable[[], None] | None
573     _indexes: dict[Hashable, Index]
574     _variables: dict[Hashable, Variable]
575 
576     __slots__ = (
577         "_attrs",
578         "_cache",
579         "_coord_names",
580         "_dims",
581         "_encoding",
582         "_close",
583         "_indexes",
584         "_variables",
585         "__weakref__",
586     )
587 
588     def __init__(
589         self,
590         # could make a VariableArgs to use more generally, and refine these
591         # categories
592         data_vars: Mapping[Any, Any] | None = None,
593         coords: Mapping[Any, Any] | None = None,
594         attrs: Mapping[Any, Any] | None = None,
595     ) -> None:
596         # TODO(shoyer): expose indexes as a public argument in __init__
597 
598         if data_vars is None:
599             data_vars = {}
600         if coords is None:
601             coords = {}
602 
603         both_data_and_coords = set(data_vars) & set(coords)
604         if both_data_and_coords:
605             raise ValueError(
606                 f"variables {both_data_and_coords!r} are found in both data_vars and coords"
607             )
608 
609         if isinstance(coords, Dataset):
610             coords = coords.variables
611 
612         variables, coord_names, dims, indexes, _ = merge_data_and_coords(
613             data_vars, coords, compat="broadcast_equals"
614         )
615 
616         self._attrs = dict(attrs) if attrs is not None else None
617         self._close = None
618         self._encoding = None
619         self._variables = variables
620         self._coord_names = coord_names
621         self._dims = dims
622         self._indexes = indexes
623 
624     @classmethod
625     def load_store(cls: type[T_Dataset], store, decoder=None) -> T_Dataset:
626         """Create a new dataset from the contents of a backends.*DataStore
627         object
628         """
629         variables, attributes = store.load()
630         if decoder:
631             variables, attributes = decoder(variables, attributes)
632         obj = cls(variables, attrs=attributes)
633         obj.set_close(store.close)
634         return obj
635 
636     @property
637     def variables(self) -> Frozen[Hashable, Variable]:
638         """Low level interface to Dataset contents as dict of Variable objects.
639 
640         This ordered dictionary is frozen to prevent mutation that could
641         violate Dataset invariants. It contains all variable objects
642         constituting the Dataset, including both data variables and
643         coordinates.
644         """
645         return Frozen(self._variables)
646 
647     @property
648     def attrs(self) -> dict[Any, Any]:
649         """Dictionary of global attributes on this dataset"""
650         if self._attrs is None:
651             self._attrs = {}
652         return self._attrs
653 
654     @attrs.setter
655     def attrs(self, value: Mapping[Any, Any]) -> None:
656         self._attrs = dict(value)
657 
658     @property
659     def encoding(self) -> dict[Any, Any]:
660         """Dictionary of global encoding attributes on this dataset"""
661         if self._encoding is None:
662             self._encoding = {}
663         return self._encoding
664 
665     @encoding.setter
666     def encoding(self, value: Mapping[Any, Any]) -> None:
667         self._encoding = dict(value)
668 
669     def reset_encoding(self: T_Dataset) -> T_Dataset:
670         """Return a new Dataset without encoding on the dataset or any of its
671         variables/coords."""
672         variables = {k: v.reset_encoding() for k, v in self.variables.items()}
673         return self._replace(variables=variables, encoding={})
674 
675     @property
676     def dims(self) -> Frozen[Hashable, int]:
677         """Mapping from dimension names to lengths.
678 
679         Cannot be modified directly, but is updated when adding new variables.
680 
681         Note that type of this object differs from `DataArray.dims`.
682         See `Dataset.sizes` and `DataArray.sizes` for consistently named
683         properties.
684 
685         See Also
686         --------
687         Dataset.sizes
688         DataArray.dims
689         """
690         return Frozen(self._dims)
691 
692     @property
693     def sizes(self) -> Frozen[Hashable, int]:
694         """Mapping from dimension names to lengths.
695 
696         Cannot be modified directly, but is updated when adding new variables.
697 
698         This is an alias for `Dataset.dims` provided for the benefit of
699         consistency with `DataArray.sizes`.
700 
701         See Also
702         --------
703         DataArray.sizes
704         """
705         return self.dims
706 
707     @property
708     def dtypes(self) -> Frozen[Hashable, np.dtype]:
709         """Mapping from data variable names to dtypes.
710 
711         Cannot be modified directly, but is updated when adding new variables.
712 
713         See Also
714         --------
715         DataArray.dtype
716         """
717         return Frozen(
718             {
719                 n: v.dtype
720                 for n, v in self._variables.items()
721                 if n not in self._coord_names
722             }
723         )
724 
725     def load(self: T_Dataset, **kwargs) -> T_Dataset:
726         """Manually trigger loading and/or computation of this dataset's data
727         from disk or a remote source into memory and return this dataset.
728         Unlike compute, the original dataset is modified and returned.
729 
730         Normally, it should not be necessary to call this method in user code,
731         because all xarray functions should either work on deferred data or
732         load data automatically. However, this method can be necessary when
733         working with many file objects on disk.
734 
735         Parameters
736         ----------
737         **kwargs : dict
738             Additional keyword arguments passed on to ``dask.compute``.
739 
740         See Also
741         --------
742         dask.compute
743         """
744         # access .data to coerce everything to numpy or dask arrays
745         lazy_data = {
746             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
747         }
748         if lazy_data:
749             import dask.array as da
750 
751             # evaluate all the dask arrays simultaneously
752             evaluated_data = da.compute(*lazy_data.values(), **kwargs)
753 
754             for k, data in zip(lazy_data, evaluated_data):
755                 self.variables[k].data = data
756 
757         # load everything else sequentially
758         for k, v in self.variables.items():
759             if k not in lazy_data:
760                 v.load()
761 
762         return self
763 
764     def __dask_tokenize__(self):
765         from dask.base import normalize_token
766 
767         return normalize_token(
768             (type(self), self._variables, self._coord_names, self._attrs)
769         )
770 
771     def __dask_graph__(self):
772         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}
773         graphs = {k: v for k, v in graphs.items() if v is not None}
774         if not graphs:
775             return None
776         else:
777             try:
778                 from dask.highlevelgraph import HighLevelGraph
779 
780                 return HighLevelGraph.merge(*graphs.values())
781             except ImportError:
782                 from dask import sharedict
783 
784                 return sharedict.merge(*graphs.values())
785 
786     def __dask_keys__(self):
787         import dask
788 
789         return [
790             v.__dask_keys__()
791             for v in self.variables.values()
792             if dask.is_dask_collection(v)
793         ]
794 
795     def __dask_layers__(self):
796         import dask
797 
798         return sum(
799             (
800                 v.__dask_layers__()
801                 for v in self.variables.values()
802                 if dask.is_dask_collection(v)
803             ),
804             (),
805         )
806 
807     @property
808     def __dask_optimize__(self):
809         import dask.array as da
810 
811         return da.Array.__dask_optimize__
812 
813     @property
814     def __dask_scheduler__(self):
815         import dask.array as da
816 
817         return da.Array.__dask_scheduler__
818 
819     def __dask_postcompute__(self):
820         return self._dask_postcompute, ()
821 
822     def __dask_postpersist__(self):
823         return self._dask_postpersist, ()
824 
825     def _dask_postcompute(self: T_Dataset, results: Iterable[Variable]) -> T_Dataset:
826         import dask
827 
828         variables = {}
829         results_iter = iter(results)
830 
831         for k, v in self._variables.items():
832             if dask.is_dask_collection(v):
833                 rebuild, args = v.__dask_postcompute__()
834                 v = rebuild(next(results_iter), *args)
835             variables[k] = v
836 
837         return type(self)._construct_direct(
838             variables,
839             self._coord_names,
840             self._dims,
841             self._attrs,
842             self._indexes,
843             self._encoding,
844             self._close,
845         )
846 
847     def _dask_postpersist(
848         self: T_Dataset, dsk: Mapping, *, rename: Mapping[str, str] | None = None
849     ) -> T_Dataset:
850         from dask import is_dask_collection
851         from dask.highlevelgraph import HighLevelGraph
852         from dask.optimization import cull
853 
854         variables = {}
855 
856         for k, v in self._variables.items():
857             if not is_dask_collection(v):
858                 variables[k] = v
859                 continue
860 
861             if isinstance(dsk, HighLevelGraph):
862                 # dask >= 2021.3
863                 # __dask_postpersist__() was called by dask.highlevelgraph.
864                 # Don't use dsk.cull(), as we need to prevent partial layers:
865                 # https://github.com/dask/dask/issues/7137
866                 layers = v.__dask_layers__()
867                 if rename:
868                     layers = [rename.get(k, k) for k in layers]
869                 dsk2 = dsk.cull_layers(layers)
870             elif rename:  # pragma: nocover
871                 # At the moment of writing, this is only for forward compatibility.
872                 # replace_name_in_key requires dask >= 2021.3.
873                 from dask.base import flatten, replace_name_in_key
874 
875                 keys = [
876                     replace_name_in_key(k, rename) for k in flatten(v.__dask_keys__())
877                 ]
878                 dsk2, _ = cull(dsk, keys)
879             else:
880                 # __dask_postpersist__() was called by dask.optimize or dask.persist
881                 dsk2, _ = cull(dsk, v.__dask_keys__())
882 
883             rebuild, args = v.__dask_postpersist__()
884             # rename was added in dask 2021.3
885             kwargs = {"rename": rename} if rename else {}
886             variables[k] = rebuild(dsk2, *args, **kwargs)
887 
888         return type(self)._construct_direct(
889             variables,
890             self._coord_names,
891             self._dims,
892             self._attrs,
893             self._indexes,
894             self._encoding,
895             self._close,
896         )
897 
898     def compute(self: T_Dataset, **kwargs) -> T_Dataset:
899         """Manually trigger loading and/or computation of this dataset's data
900         from disk or a remote source into memory and return a new dataset.
901         Unlike load, the original dataset is left unaltered.
902 
903         Normally, it should not be necessary to call this method in user code,
904         because all xarray functions should either work on deferred data or
905         load data automatically. However, this method can be necessary when
906         working with many file objects on disk.
907 
908         Parameters
909         ----------
910         **kwargs : dict
911             Additional keyword arguments passed on to ``dask.compute``.
912 
913         See Also
914         --------
915         dask.compute
916         """
917         new = self.copy(deep=False)
918         return new.load(**kwargs)
919 
920     def _persist_inplace(self: T_Dataset, **kwargs) -> T_Dataset:
921         """Persist all Dask arrays in memory"""
922         # access .data to coerce everything to numpy or dask arrays
923         lazy_data = {
924             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
925         }
926         if lazy_data:
927             import dask
928 
929             # evaluate all the dask arrays simultaneously
930             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)
931 
932             for k, data in zip(lazy_data, evaluated_data):
933                 self.variables[k].data = data
934 
935         return self
936 
937     def persist(self: T_Dataset, **kwargs) -> T_Dataset:
938         """Trigger computation, keeping data as dask arrays
939 
940         This operation can be used to trigger computation on underlying dask
941         arrays, similar to ``.compute()`` or ``.load()``.  However this
942         operation keeps the data as dask arrays. This is particularly useful
943         when using the dask.distributed scheduler and you want to load a large
944         amount of data into distributed memory.
945 
946         Parameters
947         ----------
948         **kwargs : dict
949             Additional keyword arguments passed on to ``dask.persist``.
950 
951         See Also
952         --------
953         dask.persist
954         """
955         new = self.copy(deep=False)
956         return new._persist_inplace(**kwargs)
957 
958     @classmethod
959     def _construct_direct(
960         cls: type[T_Dataset],
961         variables: dict[Any, Variable],
962         coord_names: set[Hashable],
963         dims: dict[Any, int] | None = None,
964         attrs: dict | None = None,
965         indexes: dict[Any, Index] | None = None,
966         encoding: dict | None = None,
967         close: Callable[[], None] | None = None,
968     ) -> T_Dataset:
969         """Shortcut around __init__ for internal use when we want to skip
970         costly validation
971         """
972         if dims is None:
973             dims = calculate_dimensions(variables)
974         if indexes is None:
975             indexes = {}
976         obj = object.__new__(cls)
977         obj._variables = variables
978         obj._coord_names = coord_names
979         obj._dims = dims
980         obj._indexes = indexes
981         obj._attrs = attrs
982         obj._close = close
983         obj._encoding = encoding
984         return obj
985 
986     def _replace(
987         self: T_Dataset,
988         variables: dict[Hashable, Variable] | None = None,
989         coord_names: set[Hashable] | None = None,
990         dims: dict[Any, int] | None = None,
991         attrs: dict[Hashable, Any] | None | Default = _default,
992         indexes: dict[Hashable, Index] | None = None,
993         encoding: dict | None | Default = _default,
994         inplace: bool = False,
995     ) -> T_Dataset:
996         """Fastpath constructor for internal use.
997 
998         Returns an object with optionally with replaced attributes.
999 
1000         Explicitly passed arguments are *not* copied when placed on the new
1001         dataset. It is up to the caller to ensure that they have the right type
1002         and are not used elsewhere.
1003         """
1004         if inplace:
1005             if variables is not None:
1006                 self._variables = variables
1007             if coord_names is not None:
1008                 self._coord_names = coord_names
1009             if dims is not None:
1010                 self._dims = dims
1011             if attrs is not _default:
1012                 self._attrs = attrs
1013             if indexes is not None:
1014                 self._indexes = indexes
1015             if encoding is not _default:
1016                 self._encoding = encoding
1017             obj = self
1018         else:
1019             if variables is None:
1020                 variables = self._variables.copy()
1021             if coord_names is None:
1022                 coord_names = self._coord_names.copy()
1023             if dims is None:
1024                 dims = self._dims.copy()
1025             if attrs is _default:
1026                 attrs = copy.copy(self._attrs)
1027             if indexes is None:
1028                 indexes = self._indexes.copy()
1029             if encoding is _default:
1030                 encoding = copy.copy(self._encoding)
1031             obj = self._construct_direct(
1032                 variables, coord_names, dims, attrs, indexes, encoding
1033             )
1034         return obj
1035 
1036     def _replace_with_new_dims(
1037         self: T_Dataset,
1038         variables: dict[Hashable, Variable],
1039         coord_names: set | None = None,
1040         attrs: dict[Hashable, Any] | None | Default = _default,
1041         indexes: dict[Hashable, Index] | None = None,
1042         inplace: bool = False,
1043     ) -> T_Dataset:
1044         """Replace variables with recalculated dimensions."""
1045         dims = calculate_dimensions(variables)
1046         return self._replace(
1047             variables, coord_names, dims, attrs, indexes, inplace=inplace
1048         )
1049 
1050     def _replace_vars_and_dims(
1051         self: T_Dataset,
1052         variables: dict[Hashable, Variable],
1053         coord_names: set | None = None,
1054         dims: dict[Hashable, int] | None = None,
1055         attrs: dict[Hashable, Any] | None | Default = _default,
1056         inplace: bool = False,
1057     ) -> T_Dataset:
1058         """Deprecated version of _replace_with_new_dims().
1059 
1060         Unlike _replace_with_new_dims(), this method always recalculates
1061         indexes from variables.
1062         """
1063         if dims is None:
1064             dims = calculate_dimensions(variables)
1065         return self._replace(
1066             variables, coord_names, dims, attrs, indexes=None, inplace=inplace
1067         )
1068 
1069     def _overwrite_indexes(
1070         self: T_Dataset,
1071         indexes: Mapping[Hashable, Index],
1072         variables: Mapping[Hashable, Variable] | None = None,
1073         drop_variables: list[Hashable] | None = None,
1074         drop_indexes: list[Hashable] | None = None,
1075         rename_dims: Mapping[Hashable, Hashable] | None = None,
1076     ) -> T_Dataset:
1077         """Maybe replace indexes.
1078 
1079         This function may do a lot more depending on index query
1080         results.
1081 
1082         """
1083         if not indexes:
1084             return self
1085 
1086         if variables is None:
1087             variables = {}
1088         if drop_variables is None:
1089             drop_variables = []
1090         if drop_indexes is None:
1091             drop_indexes = []
1092 
1093         new_variables = self._variables.copy()
1094         new_coord_names = self._coord_names.copy()
1095         new_indexes = dict(self._indexes)
1096 
1097         index_variables = {}
1098         no_index_variables = {}
1099         for name, var in variables.items():
1100             old_var = self._variables.get(name)
1101             if old_var is not None:
1102                 var.attrs.update(old_var.attrs)
1103                 var.encoding.update(old_var.encoding)
1104             if name in indexes:
1105                 index_variables[name] = var
1106             else:
1107                 no_index_variables[name] = var
1108 
1109         for name in indexes:
1110             new_indexes[name] = indexes[name]
1111 
1112         for name, var in index_variables.items():
1113             new_coord_names.add(name)
1114             new_variables[name] = var
1115 
1116         # append no-index variables at the end
1117         for k in no_index_variables:
1118             new_variables.pop(k)
1119         new_variables.update(no_index_variables)
1120 
1121         for name in drop_indexes:
1122             new_indexes.pop(name)
1123 
1124         for name in drop_variables:
1125             new_variables.pop(name)
1126             new_indexes.pop(name, None)
1127             new_coord_names.remove(name)
1128 
1129         replaced = self._replace(
1130             variables=new_variables, coord_names=new_coord_names, indexes=new_indexes
1131         )
1132 
1133         if rename_dims:
1134             # skip rename indexes: they should already have the right name(s)
1135             dims = replaced._rename_dims(rename_dims)
1136             new_variables, new_coord_names = replaced._rename_vars({}, rename_dims)
1137             return replaced._replace(
1138                 variables=new_variables, coord_names=new_coord_names, dims=dims
1139             )
1140         else:
1141             return replaced
1142 
1143     def copy(
1144         self: T_Dataset, deep: bool = False, data: Mapping[Any, ArrayLike] | None = None
1145     ) -> T_Dataset:
1146         """Returns a copy of this dataset.
1147 
1148         If `deep=True`, a deep copy is made of each of the component variables.
1149         Otherwise, a shallow copy of each of the component variable is made, so
1150         that the underlying memory region of the new dataset is the same as in
1151         the original dataset.
1152 
1153         Use `data` to create a new object with the same structure as
1154         original but entirely new data.
1155 
1156         Parameters
1157         ----------
1158         deep : bool, default: False
1159             Whether each component variable is loaded into memory and copied onto
1160             the new object. Default is False.
1161         data : dict-like or None, optional
1162             Data to use in the new object. Each item in `data` must have same
1163             shape as corresponding data variable in original. When `data` is
1164             used, `deep` is ignored for the data variables and only used for
1165             coords.
1166 
1167         Returns
1168         -------
1169         object : Dataset
1170             New object with dimensions, attributes, coordinates, name, encoding,
1171             and optionally data copied from original.
1172 
1173         Examples
1174         --------
1175         Shallow copy versus deep copy
1176 
1177         >>> da = xr.DataArray(np.random.randn(2, 3))
1178         >>> ds = xr.Dataset(
1179         ...     {"foo": da, "bar": ("x", [-1, 2])},
1180         ...     coords={"x": ["one", "two"]},
1181         ... )
1182         >>> ds.copy()
1183         <xarray.Dataset>
1184         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1185         Coordinates:
1186           * x        (x) <U3 'one' 'two'
1187         Dimensions without coordinates: dim_0, dim_1
1188         Data variables:
1189             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
1190             bar      (x) int64 -1 2
1191 
1192         >>> ds_0 = ds.copy(deep=False)
1193         >>> ds_0["foo"][0, 0] = 7
1194         >>> ds_0
1195         <xarray.Dataset>
1196         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1197         Coordinates:
1198           * x        (x) <U3 'one' 'two'
1199         Dimensions without coordinates: dim_0, dim_1
1200         Data variables:
1201             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1202             bar      (x) int64 -1 2
1203 
1204         >>> ds
1205         <xarray.Dataset>
1206         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1207         Coordinates:
1208           * x        (x) <U3 'one' 'two'
1209         Dimensions without coordinates: dim_0, dim_1
1210         Data variables:
1211             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1212             bar      (x) int64 -1 2
1213 
1214         Changing the data using the ``data`` argument maintains the
1215         structure of the original object, but with the new data. Original
1216         object is unaffected.
1217 
1218         >>> ds.copy(data={"foo": np.arange(6).reshape(2, 3), "bar": ["a", "b"]})
1219         <xarray.Dataset>
1220         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1221         Coordinates:
1222           * x        (x) <U3 'one' 'two'
1223         Dimensions without coordinates: dim_0, dim_1
1224         Data variables:
1225             foo      (dim_0, dim_1) int64 0 1 2 3 4 5
1226             bar      (x) <U1 'a' 'b'
1227 
1228         >>> ds
1229         <xarray.Dataset>
1230         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1231         Coordinates:
1232           * x        (x) <U3 'one' 'two'
1233         Dimensions without coordinates: dim_0, dim_1
1234         Data variables:
1235             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1236             bar      (x) int64 -1 2
1237 
1238         See Also
1239         --------
1240         pandas.DataFrame.copy
1241         """
1242         return self._copy(deep=deep, data=data)
1243 
1244     def _copy(
1245         self: T_Dataset,
1246         deep: bool = False,
1247         data: Mapping[Any, ArrayLike] | None = None,
1248         memo: dict[int, Any] | None = None,
1249     ) -> T_Dataset:
1250         if data is None:
1251             data = {}
1252         elif not utils.is_dict_like(data):
1253             raise ValueError("Data must be dict-like")
1254 
1255         if data:
1256             var_keys = set(self.data_vars.keys())
1257             data_keys = set(data.keys())
1258             keys_not_in_vars = data_keys - var_keys
1259             if keys_not_in_vars:
1260                 raise ValueError(
1261                     "Data must only contain variables in original "
1262                     "dataset. Extra variables: {}".format(keys_not_in_vars)
1263                 )
1264             keys_missing_from_data = var_keys - data_keys
1265             if keys_missing_from_data:
1266                 raise ValueError(
1267                     "Data must contain all variables in original "
1268                     "dataset. Data is missing {}".format(keys_missing_from_data)
1269                 )
1270 
1271         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1272 
1273         variables = {}
1274         for k, v in self._variables.items():
1275             if k in index_vars:
1276                 variables[k] = index_vars[k]
1277             else:
1278                 variables[k] = v._copy(deep=deep, data=data.get(k), memo=memo)
1279 
1280         attrs = copy.deepcopy(self._attrs, memo) if deep else copy.copy(self._attrs)
1281         encoding = (
1282             copy.deepcopy(self._encoding, memo) if deep else copy.copy(self._encoding)
1283         )
1284 
1285         return self._replace(variables, indexes=indexes, attrs=attrs, encoding=encoding)
1286 
1287     def __copy__(self: T_Dataset) -> T_Dataset:
1288         return self._copy(deep=False)
1289 
1290     def __deepcopy__(self: T_Dataset, memo: dict[int, Any] | None = None) -> T_Dataset:
1291         return self._copy(deep=True, memo=memo)
1292 
1293     def as_numpy(self: T_Dataset) -> T_Dataset:
1294         """
1295         Coerces wrapped data and coordinates into numpy arrays, returning a Dataset.
1296 
1297         See also
1298         --------
1299         DataArray.as_numpy
1300         DataArray.to_numpy : Returns only the data as a numpy.ndarray object.
1301         """
1302         numpy_variables = {k: v.as_numpy() for k, v in self.variables.items()}
1303         return self._replace(variables=numpy_variables)
1304 
1305     def _copy_listed(self: T_Dataset, names: Iterable[Hashable]) -> T_Dataset:
1306         """Create a new Dataset with the listed variables from this dataset and
1307         the all relevant coordinates. Skips all validation.
1308         """
1309         variables: dict[Hashable, Variable] = {}
1310         coord_names = set()
1311         indexes: dict[Hashable, Index] = {}
1312 
1313         for name in names:
1314             try:
1315                 variables[name] = self._variables[name]
1316             except KeyError:
1317                 ref_name, var_name, var = _get_virtual_variable(
1318                     self._variables, name, self.dims
1319                 )
1320                 variables[var_name] = var
1321                 if ref_name in self._coord_names or ref_name in self.dims:
1322                     coord_names.add(var_name)
1323                 if (var_name,) == var.dims:
1324                     index, index_vars = create_default_index_implicit(var, names)
1325                     indexes.update({k: index for k in index_vars})
1326                     variables.update(index_vars)
1327                     coord_names.update(index_vars)
1328 
1329         needed_dims: OrderedSet[Hashable] = OrderedSet()
1330         for v in variables.values():
1331             needed_dims.update(v.dims)
1332 
1333         dims = {k: self.dims[k] for k in needed_dims}
1334 
1335         # preserves ordering of coordinates
1336         for k in self._variables:
1337             if k not in self._coord_names:
1338                 continue
1339 
1340             if set(self.variables[k].dims) <= needed_dims:
1341                 variables[k] = self._variables[k]
1342                 coord_names.add(k)
1343 
1344         indexes.update(filter_indexes_from_coords(self._indexes, coord_names))
1345 
1346         return self._replace(variables, coord_names, dims, indexes=indexes)
1347 
1348     def _construct_dataarray(self, name: Hashable) -> DataArray:
1349         """Construct a DataArray by indexing this dataset"""
1350         from xarray.core.dataarray import DataArray
1351 
1352         try:
1353             variable = self._variables[name]
1354         except KeyError:
1355             _, name, variable = _get_virtual_variable(self._variables, name, self.dims)
1356 
1357         needed_dims = set(variable.dims)
1358 
1359         coords: dict[Hashable, Variable] = {}
1360         # preserve ordering
1361         for k in self._variables:
1362             if k in self._coord_names and set(self.variables[k].dims) <= needed_dims:
1363                 coords[k] = self.variables[k]
1364 
1365         indexes = filter_indexes_from_coords(self._indexes, set(coords))
1366 
1367         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1368 
1369     @property
1370     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1371         """Places to look-up items for attribute-style access"""
1372         yield from self._item_sources
1373         yield self.attrs
1374 
1375     @property
1376     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1377         """Places to look-up items for key-completion"""
1378         yield self.data_vars
1379         yield HybridMappingProxy(keys=self._coord_names, mapping=self.coords)
1380 
1381         # virtual coordinates
1382         yield HybridMappingProxy(keys=self.dims, mapping=self)
1383 
1384     def __contains__(self, key: object) -> bool:
1385         """The 'in' operator will return true or false depending on whether
1386         'key' is an array in the dataset or not.
1387         """
1388         return key in self._variables
1389 
1390     def __len__(self) -> int:
1391         return len(self.data_vars)
1392 
1393     def __bool__(self) -> bool:
1394         return bool(self.data_vars)
1395 
1396     def __iter__(self) -> Iterator[Hashable]:
1397         return iter(self.data_vars)
1398 
1399     def __array__(self, dtype=None):
1400         raise TypeError(
1401             "cannot directly convert an xarray.Dataset into a "
1402             "numpy array. Instead, create an xarray.DataArray "
1403             "first, either with indexing on the Dataset or by "
1404             "invoking the `to_array()` method."
1405         )
1406 
1407     @property
1408     def nbytes(self) -> int:
1409         """
1410         Total bytes consumed by the data arrays of all variables in this dataset.
1411 
1412         If the backend array for any variable does not include ``nbytes``, estimates
1413         the total bytes for that array based on the ``size`` and ``dtype``.
1414         """
1415         return sum(v.nbytes for v in self.variables.values())
1416 
1417     @property
1418     def loc(self: T_Dataset) -> _LocIndexer[T_Dataset]:
1419         """Attribute for location based indexing. Only supports __getitem__,
1420         and only when the key is a dict of the form {dim: labels}.
1421         """
1422         return _LocIndexer(self)
1423 
1424     @overload
1425     def __getitem__(self, key: Hashable) -> DataArray:
1426         ...
1427 
1428     # Mapping is Iterable
1429     @overload
1430     def __getitem__(self: T_Dataset, key: Iterable[Hashable]) -> T_Dataset:
1431         ...
1432 
1433     def __getitem__(
1434         self: T_Dataset, key: Mapping[Any, Any] | Hashable | Iterable[Hashable]
1435     ) -> T_Dataset | DataArray:
1436         """Access variables or coordinates of this dataset as a
1437         :py:class:`~xarray.DataArray` or a subset of variables or a indexed dataset.
1438 
1439         Indexing with a list of names will return a new ``Dataset`` object.
1440         """
1441         if utils.is_dict_like(key):
1442             return self.isel(**key)
1443         if utils.hashable(key):
1444             return self._construct_dataarray(key)
1445         if utils.iterable_of_hashable(key):
1446             return self._copy_listed(key)
1447         raise ValueError(f"Unsupported key-type {type(key)}")
1448 
1449     def __setitem__(
1450         self, key: Hashable | Iterable[Hashable] | Mapping, value: Any
1451     ) -> None:
1452         """Add an array to this dataset.
1453         Multiple arrays can be added at the same time, in which case each of
1454         the following operations is applied to the respective value.
1455 
1456         If key is dict-like, update all variables in the dataset
1457         one by one with the given value at the given location.
1458         If the given value is also a dataset, select corresponding variables
1459         in the given value and in the dataset to be changed.
1460 
1461         If value is a `
1462         from .dataarray import DataArray`, call its `select_vars()` method, rename it
1463         to `key` and merge the contents of the resulting dataset into this
1464         dataset.
1465 
1466         If value is a `Variable` object (or tuple of form
1467         ``(dims, data[, attrs])``), add it to this dataset as a new
1468         variable.
1469         """
1470         from xarray.core.dataarray import DataArray
1471 
1472         if utils.is_dict_like(key):
1473             # check for consistency and convert value to dataset
1474             value = self._setitem_check(key, value)
1475             # loop over dataset variables and set new values
1476             processed = []
1477             for name, var in self.items():
1478                 try:
1479                     var[key] = value[name]
1480                     processed.append(name)
1481                 except Exception as e:
1482                     if processed:
1483                         raise RuntimeError(
1484                             "An error occurred while setting values of the"
1485                             f" variable '{name}'. The following variables have"
1486                             f" been successfully updated:\n{processed}"
1487                         ) from e
1488                     else:
1489                         raise e
1490 
1491         elif utils.hashable(key):
1492             if isinstance(value, Dataset):
1493                 raise TypeError(
1494                     "Cannot assign a Dataset to a single key - only a DataArray or Variable "
1495                     "object can be stored under a single key."
1496                 )
1497             self.update({key: value})
1498 
1499         elif utils.iterable_of_hashable(key):
1500             keylist = list(key)
1501             if len(keylist) == 0:
1502                 raise ValueError("Empty list of variables to be set")
1503             if len(keylist) == 1:
1504                 self.update({keylist[0]: value})
1505             else:
1506                 if len(keylist) != len(value):
1507                     raise ValueError(
1508                         f"Different lengths of variables to be set "
1509                         f"({len(keylist)}) and data used as input for "
1510                         f"setting ({len(value)})"
1511                     )
1512                 if isinstance(value, Dataset):
1513                     self.update(dict(zip(keylist, value.data_vars.values())))
1514                 elif isinstance(value, DataArray):
1515                     raise ValueError("Cannot assign single DataArray to multiple keys")
1516                 else:
1517                     self.update(dict(zip(keylist, value)))
1518 
1519         else:
1520             raise ValueError(f"Unsupported key-type {type(key)}")
1521 
1522     def _setitem_check(self, key, value):
1523         """Consistency check for __setitem__
1524 
1525         When assigning values to a subset of a Dataset, do consistency check beforehand
1526         to avoid leaving the dataset in a partially updated state when an error occurs.
1527         """
1528         from xarray.core.alignment import align
1529         from xarray.core.dataarray import DataArray
1530 
1531         if isinstance(value, Dataset):
1532             missing_vars = [
1533                 name for name in value.data_vars if name not in self.data_vars
1534             ]
1535             if missing_vars:
1536                 raise ValueError(
1537                     f"Variables {missing_vars} in new values"
1538                     f" not available in original dataset:\n{self}"
1539                 )
1540         elif not any([isinstance(value, t) for t in [DataArray, Number, str]]):
1541             raise TypeError(
1542                 "Dataset assignment only accepts DataArrays, Datasets, and scalars."
1543             )
1544 
1545         new_value = Dataset()
1546         for name, var in self.items():
1547             # test indexing
1548             try:
1549                 var_k = var[key]
1550             except Exception as e:
1551                 raise ValueError(
1552                     f"Variable '{name}': indexer {key} not available"
1553                 ) from e
1554 
1555             if isinstance(value, Dataset):
1556                 val = value[name]
1557             else:
1558                 val = value
1559 
1560             if isinstance(val, DataArray):
1561                 # check consistency of dimensions
1562                 for dim in val.dims:
1563                     if dim not in var_k.dims:
1564                         raise KeyError(
1565                             f"Variable '{name}': dimension '{dim}' appears in new values "
1566                             f"but not in the indexed original data"
1567                         )
1568                 dims = tuple(dim for dim in var_k.dims if dim in val.dims)
1569                 if dims != val.dims:
1570                     raise ValueError(
1571                         f"Variable '{name}': dimension order differs between"
1572                         f" original and new data:\n{dims}\nvs.\n{val.dims}"
1573                     )
1574             else:
1575                 val = np.array(val)
1576 
1577             # type conversion
1578             new_value[name] = val.astype(var_k.dtype, copy=False)
1579 
1580         # check consistency of dimension sizes and dimension coordinates
1581         if isinstance(value, DataArray) or isinstance(value, Dataset):
1582             align(self[key], value, join="exact", copy=False)
1583 
1584         return new_value
1585 
1586     def __delitem__(self, key: Hashable) -> None:
1587         """Remove a variable from this dataset."""
1588         assert_no_index_corrupted(self.xindexes, {key})
1589 
1590         if key in self._indexes:
1591             del self._indexes[key]
1592         del self._variables[key]
1593         self._coord_names.discard(key)
1594         self._dims = calculate_dimensions(self._variables)
1595 
1596     # mutable objects should not be hashable
1597     # https://github.com/python/mypy/issues/4266
1598     __hash__ = None  # type: ignore[assignment]
1599 
1600     def _all_compat(self, other: Dataset, compat_str: str) -> bool:
1601         """Helper function for equals and identical"""
1602 
1603         # some stores (e.g., scipy) do not seem to preserve order, so don't
1604         # require matching order for equality
1605         def compat(x: Variable, y: Variable) -> bool:
1606             return getattr(x, compat_str)(y)
1607 
1608         return self._coord_names == other._coord_names and utils.dict_equiv(
1609             self._variables, other._variables, compat=compat
1610         )
1611 
1612     def broadcast_equals(self, other: Dataset) -> bool:
1613         """Two Datasets are broadcast equal if they are equal after
1614         broadcasting all variables against each other.
1615 
1616         For example, variables that are scalar in one dataset but non-scalar in
1617         the other dataset can still be broadcast equal if the the non-scalar
1618         variable is a constant.
1619 
1620         See Also
1621         --------
1622         Dataset.equals
1623         Dataset.identical
1624         """
1625         try:
1626             return self._all_compat(other, "broadcast_equals")
1627         except (TypeError, AttributeError):
1628             return False
1629 
1630     def equals(self, other: Dataset) -> bool:
1631         """Two Datasets are equal if they have matching variables and
1632         coordinates, all of which are equal.
1633 
1634         Datasets can still be equal (like pandas objects) if they have NaN
1635         values in the same locations.
1636 
1637         This method is necessary because `v1 == v2` for ``Dataset``
1638         does element-wise comparisons (like numpy.ndarrays).
1639 
1640         See Also
1641         --------
1642         Dataset.broadcast_equals
1643         Dataset.identical
1644         """
1645         try:
1646             return self._all_compat(other, "equals")
1647         except (TypeError, AttributeError):
1648             return False
1649 
1650     def identical(self, other: Dataset) -> bool:
1651         """Like equals, but also checks all dataset attributes and the
1652         attributes on all variables and coordinates.
1653 
1654         See Also
1655         --------
1656         Dataset.broadcast_equals
1657         Dataset.equals
1658         """
1659         try:
1660             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(
1661                 other, "identical"
1662             )
1663         except (TypeError, AttributeError):
1664             return False
1665 
1666     @property
1667     def indexes(self) -> Indexes[pd.Index]:
1668         """Mapping of pandas.Index objects used for label based indexing.
1669 
1670         Raises an error if this Dataset has indexes that cannot be coerced
1671         to pandas.Index objects.
1672 
1673         See Also
1674         --------
1675         Dataset.xindexes
1676 
1677         """
1678         return self.xindexes.to_pandas_indexes()
1679 
1680     @property
1681     def xindexes(self) -> Indexes[Index]:
1682         """Mapping of xarray Index objects used for label based indexing."""
1683         return Indexes(self._indexes, {k: self._variables[k] for k in self._indexes})
1684 
1685     @property
1686     def coords(self) -> DatasetCoordinates:
1687         """Dictionary of xarray.DataArray objects corresponding to coordinate
1688         variables
1689         """
1690         return DatasetCoordinates(self)
1691 
1692     @property
1693     def data_vars(self) -> DataVariables:
1694         """Dictionary of DataArray objects corresponding to data variables"""
1695         return DataVariables(self)
1696 
1697     def set_coords(self: T_Dataset, names: Hashable | Iterable[Hashable]) -> T_Dataset:
1698         """Given names of one or more variables, set them as coordinates
1699 
1700         Parameters
1701         ----------
1702         names : hashable or iterable of hashable
1703             Name(s) of variables in this dataset to convert into coordinates.
1704 
1705         Returns
1706         -------
1707         Dataset
1708 
1709         See Also
1710         --------
1711         Dataset.swap_dims
1712         Dataset.assign_coords
1713         """
1714         # TODO: allow inserting new coordinates with this method, like
1715         # DataFrame.set_index?
1716         # nb. check in self._variables, not self.data_vars to insure that the
1717         # operation is idempotent
1718         if isinstance(names, str) or not isinstance(names, Iterable):
1719             names = [names]
1720         else:
1721             names = list(names)
1722         self._assert_all_in_dataset(names)
1723         obj = self.copy()
1724         obj._coord_names.update(names)
1725         return obj
1726 
1727     def reset_coords(
1728         self: T_Dataset,
1729         names: Dims = None,
1730         drop: bool = False,
1731     ) -> T_Dataset:
1732         """Given names of coordinates, reset them to become variables
1733 
1734         Parameters
1735         ----------
1736         names : str, Iterable of Hashable or None, optional
1737             Name(s) of non-index coordinates in this dataset to reset into
1738             variables. By default, all non-index coordinates are reset.
1739         drop : bool, default: False
1740             If True, remove coordinates instead of converting them into
1741             variables.
1742 
1743         Returns
1744         -------
1745         Dataset
1746         """
1747         if names is None:
1748             names = self._coord_names - set(self._indexes)
1749         else:
1750             if isinstance(names, str) or not isinstance(names, Iterable):
1751                 names = [names]
1752             else:
1753                 names = list(names)
1754             self._assert_all_in_dataset(names)
1755             bad_coords = set(names) & set(self._indexes)
1756             if bad_coords:
1757                 raise ValueError(
1758                     f"cannot remove index coordinates with reset_coords: {bad_coords}"
1759                 )
1760         obj = self.copy()
1761         obj._coord_names.difference_update(names)
1762         if drop:
1763             for name in names:
1764                 del obj._variables[name]
1765         return obj
1766 
1767     def dump_to_store(self, store: AbstractDataStore, **kwargs) -> None:
1768         """Store dataset contents to a backends.*DataStore object."""
1769         from xarray.backends.api import dump_to_store
1770 
1771         # TODO: rename and/or cleanup this method to make it more consistent
1772         # with to_netcdf()
1773         dump_to_store(self, store, **kwargs)
1774 
1775     # path=None writes to bytes
1776     @overload
1777     def to_netcdf(
1778         self,
1779         path: None = None,
1780         mode: Literal["w", "a"] = "w",
1781         format: T_NetcdfTypes | None = None,
1782         group: str | None = None,
1783         engine: T_NetcdfEngine | None = None,
1784         encoding: Mapping[Any, Mapping[str, Any]] | None = None,
1785         unlimited_dims: Iterable[Hashable] | None = None,
1786         compute: bool = True,
1787         invalid_netcdf: bool = False,
1788     ) -> bytes:
1789         ...
1790 
1791     # default return None
1792     @overload
1793     def to_netcdf(
1794         self,
1795         path: str | PathLike,
1796         mode: Literal["w", "a"] = "w",
1797         format: T_NetcdfTypes | None = None,
1798         group: str | None = None,
1799         engine: T_NetcdfEngine | None = None,
1800         encoding: Mapping[Any, Mapping[str, Any]] | None = None,
1801         unlimited_dims: Iterable[Hashable] | None = None,
1802         compute: Literal[True] = True,
1803         invalid_netcdf: bool = False,
1804     ) -> None:
1805         ...
1806 
1807     # compute=False returns dask.Delayed
1808     @overload
1809     def to_netcdf(
1810         self,
1811         path: str | PathLike,
1812         mode: Literal["w", "a"] = "w",
1813         format: T_NetcdfTypes | None = None,
1814         group: str | None = None,
1815         engine: T_NetcdfEngine | None = None,
1816         encoding: Mapping[Any, Mapping[str, Any]] | None = None,
1817         unlimited_dims: Iterable[Hashable] | None = None,
1818         *,
1819         compute: Literal[False],
1820         invalid_netcdf: bool = False,
1821     ) -> Delayed:
1822         ...
1823 
1824     def to_netcdf(
1825         self,
1826         path: str | PathLike | None = None,
1827         mode: Literal["w", "a"] = "w",
1828         format: T_NetcdfTypes | None = None,
1829         group: str | None = None,
1830         engine: T_NetcdfEngine | None = None,
1831         encoding: Mapping[Any, Mapping[str, Any]] | None = None,
1832         unlimited_dims: Iterable[Hashable] | None = None,
1833         compute: bool = True,
1834         invalid_netcdf: bool = False,
1835     ) -> bytes | Delayed | None:
1836         """Write dataset contents to a netCDF file.
1837 
1838         Parameters
1839         ----------
1840         path : str, path-like or file-like, optional
1841             Path to which to save this dataset. File-like objects are only
1842             supported by the scipy engine. If no path is provided, this
1843             function returns the resulting netCDF file as bytes; in this case,
1844             we need to use scipy, which does not support netCDF version 4 (the
1845             default format becomes NETCDF3_64BIT).
1846         mode : {"w", "a"}, default: "w"
1847             Write ('w') or append ('a') mode. If mode='w', any existing file at
1848             this location will be overwritten. If mode='a', existing variables
1849             will be overwritten.
1850         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
1851                   "NETCDF3_CLASSIC"}, optional
1852             File format for the resulting netCDF file:
1853 
1854             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
1855               features.
1856             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
1857               netCDF 3 compatible API features.
1858             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
1859               which fully supports 2+ GB files, but is only compatible with
1860               clients linked against netCDF version 3.6.0 or later.
1861             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
1862               handle 2+ GB files very well.
1863 
1864             All formats are supported by the netCDF4-python library.
1865             scipy.io.netcdf only supports the last two formats.
1866 
1867             The default format is NETCDF4 if you are saving a file to disk and
1868             have the netCDF4-python library available. Otherwise, xarray falls
1869             back to using scipy to write netCDF files and defaults to the
1870             NETCDF3_64BIT format (scipy does not support netCDF4).
1871         group : str, optional
1872             Path to the netCDF4 group in the given file to open (only works for
1873             format='NETCDF4'). The group(s) will be created if necessary.
1874         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
1875             Engine to use when writing netCDF files. If not provided, the
1876             default engine is chosen based on available dependencies, with a
1877             preference for 'netcdf4' if writing to a file on disk.
1878         encoding : dict, optional
1879             Nested dictionary with variable names as keys and dictionaries of
1880             variable specific encodings as values, e.g.,
1881             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
1882             "zlib": True}, ...}``
1883 
1884             The `h5netcdf` engine supports both the NetCDF4-style compression
1885             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
1886             ones ``{"compression": "gzip", "compression_opts": 9}``.
1887             This allows using any compression plugin installed in the HDF5
1888             library, e.g. LZF.
1889 
1890         unlimited_dims : iterable of hashable, optional
1891             Dimension(s) that should be serialized as unlimited dimensions.
1892             By default, no dimensions are treated as unlimited dimensions.
1893             Note that unlimited_dims may also be set via
1894             ``dataset.encoding["unlimited_dims"]``.
1895         compute: bool, default: True
1896             If true compute immediately, otherwise return a
1897             ``dask.delayed.Delayed`` object that can be computed later.
1898         invalid_netcdf: bool, default: False
1899             Only valid along with ``engine="h5netcdf"``. If True, allow writing
1900             hdf5 files which are invalid netcdf as described in
1901             https://github.com/h5netcdf/h5netcdf.
1902 
1903         Returns
1904         -------
1905             * ``bytes`` if path is None
1906             * ``dask.delayed.Delayed`` if compute is False
1907             * None otherwise
1908 
1909         See Also
1910         --------
1911         DataArray.to_netcdf
1912         """
1913         if encoding is None:
1914             encoding = {}
1915         from xarray.backends.api import to_netcdf
1916 
1917         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
1918             self,
1919             path,
1920             mode=mode,
1921             format=format,
1922             group=group,
1923             engine=engine,
1924             encoding=encoding,
1925             unlimited_dims=unlimited_dims,
1926             compute=compute,
1927             multifile=False,
1928             invalid_netcdf=invalid_netcdf,
1929         )
1930 
1931     # compute=True (default) returns ZarrStore
1932     @overload
1933     def to_zarr(
1934         self,
1935         store: MutableMapping | str | PathLike[str] | None = None,
1936         chunk_store: MutableMapping | str | PathLike | None = None,
1937         mode: Literal["w", "w-", "a", "r+", None] = None,
1938         synchronizer=None,
1939         group: str | None = None,
1940         encoding: Mapping | None = None,
1941         compute: Literal[True] = True,
1942         consolidated: bool | None = None,
1943         append_dim: Hashable | None = None,
1944         region: Mapping[str, slice] | None = None,
1945         safe_chunks: bool = True,
1946         storage_options: dict[str, str] | None = None,
1947         zarr_version: int | None = None,
1948     ) -> ZarrStore:
1949         ...
1950 
1951     # compute=False returns dask.Delayed
1952     @overload
1953     def to_zarr(
1954         self,
1955         store: MutableMapping | str | PathLike[str] | None = None,
1956         chunk_store: MutableMapping | str | PathLike | None = None,
1957         mode: Literal["w", "w-", "a", "r+", None] = None,
1958         synchronizer=None,
1959         group: str | None = None,
1960         encoding: Mapping | None = None,
1961         *,
1962         compute: Literal[False],
1963         consolidated: bool | None = None,
1964         append_dim: Hashable | None = None,
1965         region: Mapping[str, slice] | None = None,
1966         safe_chunks: bool = True,
1967         storage_options: dict[str, str] | None = None,
1968         zarr_version: int | None = None,
1969     ) -> Delayed:
1970         ...
1971 
1972     def to_zarr(
1973         self,
1974         store: MutableMapping | str | PathLike[str] | None = None,
1975         chunk_store: MutableMapping | str | PathLike | None = None,
1976         mode: Literal["w", "w-", "a", "r+", None] = None,
1977         synchronizer=None,
1978         group: str | None = None,
1979         encoding: Mapping | None = None,
1980         compute: bool = True,
1981         consolidated: bool | None = None,
1982         append_dim: Hashable | None = None,
1983         region: Mapping[str, slice] | None = None,
1984         safe_chunks: bool = True,
1985         storage_options: dict[str, str] | None = None,
1986         zarr_version: int | None = None,
1987     ) -> ZarrStore | Delayed:
1988         """Write dataset contents to a zarr group.
1989 
1990         Zarr chunks are determined in the following way:
1991 
1992         - From the ``chunks`` attribute in each variable's ``encoding``
1993           (can be set via `Dataset.chunk`).
1994         - If the variable is a Dask array, from the dask chunks
1995         - If neither Dask chunks nor encoding chunks are present, chunks will
1996           be determined automatically by Zarr
1997         - If both Dask chunks and encoding chunks are present, encoding chunks
1998           will be used, provided that there is a many-to-one relationship between
1999           encoding chunks and dask chunks (i.e. Dask chunks are bigger than and
2000           evenly divide encoding chunks); otherwise raise a ``ValueError``.
2001           This restriction ensures that no synchronization / locks are required
2002           when writing. To disable this restriction, use ``safe_chunks=False``.
2003 
2004         Parameters
2005         ----------
2006         store : MutableMapping, str or path-like, optional
2007             Store or path to directory in local or remote file system.
2008         chunk_store : MutableMapping, str or path-like, optional
2009             Store or path to directory in local or remote file system only for Zarr
2010             array chunks. Requires zarr-python v2.4.0 or later.
2011         mode : {"w", "w-", "a", "r+", None}, optional
2012             Persistence mode: "w" means create (overwrite if exists);
2013             "w-" means create (fail if exists);
2014             "a" means override existing variables (create if does not exist);
2015             "r+" means modify existing array *values* only (raise an error if
2016             any metadata or shapes would change).
2017             The default mode is "a" if ``append_dim`` is set. Otherwise, it is
2018             "r+" if ``region`` is set and ``w-`` otherwise.
2019         synchronizer : object, optional
2020             Zarr array synchronizer.
2021         group : str, optional
2022             Group path. (a.k.a. `path` in zarr terminology.)
2023         encoding : dict, optional
2024             Nested dictionary with variable names as keys and dictionaries of
2025             variable specific encodings as values, e.g.,
2026             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,}, ...}``
2027         compute : bool, default: True
2028             If True write array data immediately, otherwise return a
2029             ``dask.delayed.Delayed`` object that can be computed to write
2030             array data later. Metadata is always updated eagerly.
2031         consolidated : bool, optional
2032             If True, apply zarr's `consolidate_metadata` function to the store
2033             after writing metadata and read existing stores with consolidated
2034             metadata; if False, do not. The default (`consolidated=None`) means
2035             write consolidated metadata and attempt to read consolidated
2036             metadata for existing stores (falling back to non-consolidated).
2037 
2038             When the experimental ``zarr_version=3``, ``consolidated`` must be
2039             either be ``None`` or ``False``.
2040         append_dim : hashable, optional
2041             If set, the dimension along which the data will be appended. All
2042             other dimensions on overridden variables must remain the same size.
2043         region : dict, optional
2044             Optional mapping from dimension names to integer slices along
2045             dataset dimensions to indicate the region of existing zarr array(s)
2046             in which to write this dataset's data. For example,
2047             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate
2048             that values should be written to the region ``0:1000`` along ``x``
2049             and ``10000:11000`` along ``y``.
2050 
2051             Two restrictions apply to the use of ``region``:
2052 
2053             - If ``region`` is set, _all_ variables in a dataset must have at
2054               least one dimension in common with the region. Other variables
2055               should be written in a separate call to ``to_zarr()``.
2056             - Dimensions cannot be included in both ``region`` and
2057               ``append_dim`` at the same time. To create empty arrays to fill
2058               in with ``region``, use a separate call to ``to_zarr()`` with
2059               ``compute=False``. See "Appending to existing Zarr stores" in
2060               the reference documentation for full details.
2061         safe_chunks : bool, default: True
2062             If True, only allow writes to when there is a many-to-one relationship
2063             between Zarr chunks (specified in encoding) and Dask chunks.
2064             Set False to override this restriction; however, data may become corrupted
2065             if Zarr arrays are written in parallel. This option may be useful in combination
2066             with ``compute=False`` to initialize a Zarr from an existing
2067             Dataset with arbitrary chunk structure.
2068         storage_options : dict, optional
2069             Any additional parameters for the storage backend (ignored for local
2070             paths).
2071         zarr_version : int or None, optional
2072             The desired zarr spec version to target (currently 2 or 3). The
2073             default of None will attempt to determine the zarr version from
2074             ``store`` when possible, otherwise defaulting to 2.
2075 
2076         Returns
2077         -------
2078             * ``dask.delayed.Delayed`` if compute is False
2079             * ZarrStore otherwise
2080 
2081         References
2082         ----------
2083         https://zarr.readthedocs.io/
2084 
2085         Notes
2086         -----
2087         Zarr chunking behavior:
2088             If chunks are found in the encoding argument or attribute
2089             corresponding to any DataArray, those chunks are used.
2090             If a DataArray is a dask array, it is written with those chunks.
2091             If not other chunks are found, Zarr uses its own heuristics to
2092             choose automatic chunk sizes.
2093 
2094         encoding:
2095             The encoding attribute (if exists) of the DataArray(s) will be
2096             used. Override any existing encodings by providing the ``encoding`` kwarg.
2097 
2098         See Also
2099         --------
2100         :ref:`io.zarr`
2101             The I/O user guide, with more details and examples.
2102         """
2103         from xarray.backends.api import to_zarr
2104 
2105         return to_zarr(  # type: ignore[call-overload,misc]
2106             self,
2107             store=store,
2108             chunk_store=chunk_store,
2109             storage_options=storage_options,
2110             mode=mode,
2111             synchronizer=synchronizer,
2112             group=group,
2113             encoding=encoding,
2114             compute=compute,
2115             consolidated=consolidated,
2116             append_dim=append_dim,
2117             region=region,
2118             safe_chunks=safe_chunks,
2119             zarr_version=zarr_version,
2120         )
2121 
2122     def __repr__(self) -> str:
2123         return formatting.dataset_repr(self)
2124 
2125     def _repr_html_(self) -> str:
2126         if OPTIONS["display_style"] == "text":
2127             return f"<pre>{escape(repr(self))}</pre>"
2128         return formatting_html.dataset_repr(self)
2129 
2130     def info(self, buf: IO | None = None) -> None:
2131         """
2132         Concise summary of a Dataset variables and attributes.
2133 
2134         Parameters
2135         ----------
2136         buf : file-like, default: sys.stdout
2137             writable buffer
2138 
2139         See Also
2140         --------
2141         pandas.DataFrame.assign
2142         ncdump : netCDF's ncdump
2143         """
2144         if buf is None:  # pragma: no cover
2145             buf = sys.stdout
2146 
2147         lines = []
2148         lines.append("xarray.Dataset {")
2149         lines.append("dimensions:")
2150         for name, size in self.dims.items():
2151             lines.append(f"\t{name} = {size} ;")
2152         lines.append("\nvariables:")
2153         for name, da in self.variables.items():
2154             dims = ", ".join(map(str, da.dims))
2155             lines.append(f"\t{da.dtype} {name}({dims}) ;")
2156             for k, v in da.attrs.items():
2157                 lines.append(f"\t\t{name}:{k} = {v} ;")
2158         lines.append("\n// global attributes:")
2159         for k, v in self.attrs.items():
2160             lines.append(f"\t:{k} = {v} ;")
2161         lines.append("}")
2162 
2163         buf.write("\n".join(lines))
2164 
2165     @property
2166     def chunks(self) -> Mapping[Hashable, tuple[int, ...]]:
2167         """
2168         Mapping from dimension names to block lengths for this dataset's data, or None if
2169         the underlying data is not a dask array.
2170         Cannot be modified directly, but can be modified by calling .chunk().
2171 
2172         Same as Dataset.chunksizes, but maintained for backwards compatibility.
2173 
2174         See Also
2175         --------
2176         Dataset.chunk
2177         Dataset.chunksizes
2178         xarray.unify_chunks
2179         """
2180         return get_chunksizes(self.variables.values())
2181 
2182     @property
2183     def chunksizes(self) -> Mapping[Hashable, tuple[int, ...]]:
2184         """
2185         Mapping from dimension names to block lengths for this dataset's data, or None if
2186         the underlying data is not a dask array.
2187         Cannot be modified directly, but can be modified by calling .chunk().
2188 
2189         Same as Dataset.chunks.
2190 
2191         See Also
2192         --------
2193         Dataset.chunk
2194         Dataset.chunks
2195         xarray.unify_chunks
2196         """
2197         return get_chunksizes(self.variables.values())
2198 
2199     def chunk(
2200         self: T_Dataset,
2201         chunks: (
2202             int | Literal["auto"] | Mapping[Any, None | int | str | tuple[int, ...]]
2203         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
2204         name_prefix: str = "xarray-",
2205         token: str | None = None,
2206         lock: bool = False,
2207         inline_array: bool = False,
2208         **chunks_kwargs: None | int | str | tuple[int, ...],
2209     ) -> T_Dataset:
2210         """Coerce all arrays in this dataset into dask arrays with the given
2211         chunks.
2212 
2213         Non-dask arrays in this dataset will be converted to dask arrays. Dask
2214         arrays will be rechunked to the given chunk sizes.
2215 
2216         If neither chunks is not provided for one or more dimensions, chunk
2217         sizes along that dimension will not be updated; non-dask arrays will be
2218         converted into dask arrays with a single block.
2219 
2220         Parameters
2221         ----------
2222         chunks : int, tuple of int, "auto" or mapping of hashable to int, optional
2223             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, or
2224             ``{"x": 5, "y": 5}``.
2225         name_prefix : str, default: "xarray-"
2226             Prefix for the name of any new dask arrays.
2227         token : str, optional
2228             Token uniquely identifying this dataset.
2229         lock : bool, default: False
2230             Passed on to :py:func:`dask.array.from_array`, if the array is not
2231             already as dask array.
2232         inline_array: bool, default: False
2233             Passed on to :py:func:`dask.array.from_array`, if the array is not
2234             already as dask array.
2235         **chunks_kwargs : {dim: chunks, ...}, optional
2236             The keyword arguments form of ``chunks``.
2237             One of chunks or chunks_kwargs must be provided
2238 
2239         Returns
2240         -------
2241         chunked : xarray.Dataset
2242 
2243         See Also
2244         --------
2245         Dataset.chunks
2246         Dataset.chunksizes
2247         xarray.unify_chunks
2248         dask.array.from_array
2249         """
2250         if chunks is None and chunks_kwargs is None:
2251             warnings.warn(
2252                 "None value for 'chunks' is deprecated. "
2253                 "It will raise an error in the future. Use instead '{}'",
2254                 category=FutureWarning,
2255             )
2256             chunks = {}
2257 
2258         if isinstance(chunks, (Number, str, int)):
2259             chunks = dict.fromkeys(self.dims, chunks)
2260         else:
2261             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
2262 
2263         bad_dims = chunks.keys() - self.dims.keys()
2264         if bad_dims:
2265             raise ValueError(
2266                 f"some chunks keys are not dimensions on this object: {bad_dims}"
2267             )
2268 
2269         variables = {
2270             k: _maybe_chunk(k, v, chunks, token, lock, name_prefix)
2271             for k, v in self.variables.items()
2272         }
2273         return self._replace(variables)
2274 
2275     def _validate_indexers(
2276         self, indexers: Mapping[Any, Any], missing_dims: ErrorOptionsWithWarn = "raise"
2277     ) -> Iterator[tuple[Hashable, int | slice | np.ndarray | Variable]]:
2278         """Here we make sure
2279         + indexer has a valid keys
2280         + indexer is in a valid data type
2281         + string indexers are cast to the appropriate date type if the
2282           associated index is a DatetimeIndex or CFTimeIndex
2283         """
2284         from xarray.coding.cftimeindex import CFTimeIndex
2285         from xarray.core.dataarray import DataArray
2286 
2287         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2288 
2289         # all indexers should be int, slice, np.ndarrays, or Variable
2290         for k, v in indexers.items():
2291             if isinstance(v, (int, slice, Variable)):
2292                 yield k, v
2293             elif isinstance(v, DataArray):
2294                 yield k, v.variable
2295             elif isinstance(v, tuple):
2296                 yield k, as_variable(v)
2297             elif isinstance(v, Dataset):
2298                 raise TypeError("cannot use a Dataset as an indexer")
2299             elif isinstance(v, Sequence) and len(v) == 0:
2300                 yield k, np.empty((0,), dtype="int64")
2301             else:
2302                 if not is_duck_array(v):
2303                     v = np.asarray(v)
2304 
2305                 if v.dtype.kind in "US":
2306                     index = self._indexes[k].to_pandas_index()
2307                     if isinstance(index, pd.DatetimeIndex):
2308                         v = v.astype("datetime64[ns]")
2309                     elif isinstance(index, CFTimeIndex):
2310                         v = _parse_array_of_cftime_strings(v, index.date_type)
2311 
2312                 if v.ndim > 1:
2313                     raise IndexError(
2314                         "Unlabeled multi-dimensional array cannot be "
2315                         "used for indexing: {}".format(k)
2316                     )
2317                 yield k, v
2318 
2319     def _validate_interp_indexers(
2320         self, indexers: Mapping[Any, Any]
2321     ) -> Iterator[tuple[Hashable, Variable]]:
2322         """Variant of _validate_indexers to be used for interpolation"""
2323         for k, v in self._validate_indexers(indexers):
2324             if isinstance(v, Variable):
2325                 if v.ndim == 1:
2326                     yield k, v.to_index_variable()
2327                 else:
2328                     yield k, v
2329             elif isinstance(v, int):
2330                 yield k, Variable((), v, attrs=self.coords[k].attrs)
2331             elif isinstance(v, np.ndarray):
2332                 if v.ndim == 0:
2333                     yield k, Variable((), v, attrs=self.coords[k].attrs)
2334                 elif v.ndim == 1:
2335                     yield k, IndexVariable((k,), v, attrs=self.coords[k].attrs)
2336                 else:
2337                     raise AssertionError()  # Already tested by _validate_indexers
2338             else:
2339                 raise TypeError(type(v))
2340 
2341     def _get_indexers_coords_and_indexes(self, indexers):
2342         """Extract coordinates and indexes from indexers.
2343 
2344         Only coordinate with a name different from any of self.variables will
2345         be attached.
2346         """
2347         from xarray.core.dataarray import DataArray
2348 
2349         coords_list = []
2350         for k, v in indexers.items():
2351             if isinstance(v, DataArray):
2352                 if v.dtype.kind == "b":
2353                     if v.ndim != 1:  # we only support 1-d boolean array
2354                         raise ValueError(
2355                             "{:d}d-boolean array is used for indexing along "
2356                             "dimension {!r}, but only 1d boolean arrays are "
2357                             "supported.".format(v.ndim, k)
2358                         )
2359                     # Make sure in case of boolean DataArray, its
2360                     # coordinate also should be indexed.
2361                     v_coords = v[v.values.nonzero()[0]].coords
2362                 else:
2363                     v_coords = v.coords
2364                 coords_list.append(v_coords)
2365 
2366         # we don't need to call align() explicitly or check indexes for
2367         # alignment, because merge_variables already checks for exact alignment
2368         # between dimension coordinates
2369         coords, indexes = merge_coordinates_without_align(coords_list)
2370         assert_coordinate_consistent(self, coords)
2371 
2372         # silently drop the conflicted variables.
2373         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}
2374         attached_indexes = {
2375             k: v for k, v in indexes.items() if k not in self._variables
2376         }
2377         return attached_coords, attached_indexes
2378 
2379     def isel(
2380         self: T_Dataset,
2381         indexers: Mapping[Any, Any] | None = None,
2382         drop: bool = False,
2383         missing_dims: ErrorOptionsWithWarn = "raise",
2384         **indexers_kwargs: Any,
2385     ) -> T_Dataset:
2386         """Returns a new dataset with each array indexed along the specified
2387         dimension(s).
2388 
2389         This method selects values from each array using its `__getitem__`
2390         method, except this method does not require knowing the order of
2391         each array's dimensions.
2392 
2393         Parameters
2394         ----------
2395         indexers : dict, optional
2396             A dict with keys matching dimensions and values given
2397             by integers, slice objects or arrays.
2398             indexer can be a integer, slice, array-like or DataArray.
2399             If DataArrays are passed as indexers, xarray-style indexing will be
2400             carried out. See :ref:`indexing` for the details.
2401             One of indexers or indexers_kwargs must be provided.
2402         drop : bool, default: False
2403             If ``drop=True``, drop coordinates variables indexed by integers
2404             instead of making them scalar.
2405         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2406             What to do if dimensions that should be selected from are not present in the
2407             Dataset:
2408             - "raise": raise an exception
2409             - "warn": raise a warning, and ignore the missing dimensions
2410             - "ignore": ignore the missing dimensions
2411 
2412         **indexers_kwargs : {dim: indexer, ...}, optional
2413             The keyword arguments form of ``indexers``.
2414             One of indexers or indexers_kwargs must be provided.
2415 
2416         Returns
2417         -------
2418         obj : Dataset
2419             A new Dataset with the same contents as this dataset, except each
2420             array and dimension is indexed by the appropriate indexers.
2421             If indexer DataArrays have coordinates that do not conflict with
2422             this object, then these coordinates will be attached.
2423             In general, each array's data will be a view of the array's data
2424             in this dataset, unless vectorized indexing was triggered by using
2425             an array indexer, in which case the data will be a copy.
2426 
2427         See Also
2428         --------
2429         Dataset.sel
2430         DataArray.isel
2431         """
2432         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
2433         if any(is_fancy_indexer(idx) for idx in indexers.values()):
2434             return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)
2435 
2436         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
2437         # lists, or zero or one-dimensional np.ndarray's
2438         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2439 
2440         variables = {}
2441         dims: dict[Hashable, int] = {}
2442         coord_names = self._coord_names.copy()
2443 
2444         indexes, index_variables = isel_indexes(self.xindexes, indexers)
2445 
2446         for name, var in self._variables.items():
2447             # preserve variable order
2448             if name in index_variables:
2449                 var = index_variables[name]
2450             else:
2451                 var_indexers = {k: v for k, v in indexers.items() if k in var.dims}
2452                 if var_indexers:
2453                     var = var.isel(var_indexers)
2454                     if drop and var.ndim == 0 and name in coord_names:
2455                         coord_names.remove(name)
2456                         continue
2457             variables[name] = var
2458             dims.update(zip(var.dims, var.shape))
2459 
2460         return self._construct_direct(
2461             variables=variables,
2462             coord_names=coord_names,
2463             dims=dims,
2464             attrs=self._attrs,
2465             indexes=indexes,
2466             encoding=self._encoding,
2467             close=self._close,
2468         )
2469 
2470     def _isel_fancy(
2471         self: T_Dataset,
2472         indexers: Mapping[Any, Any],
2473         *,
2474         drop: bool,
2475         missing_dims: ErrorOptionsWithWarn = "raise",
2476     ) -> T_Dataset:
2477         valid_indexers = dict(self._validate_indexers(indexers, missing_dims))
2478 
2479         variables: dict[Hashable, Variable] = {}
2480         indexes, index_variables = isel_indexes(self.xindexes, valid_indexers)
2481 
2482         for name, var in self.variables.items():
2483             if name in index_variables:
2484                 new_var = index_variables[name]
2485             else:
2486                 var_indexers = {
2487                     k: v for k, v in valid_indexers.items() if k in var.dims
2488                 }
2489                 if var_indexers:
2490                     new_var = var.isel(indexers=var_indexers)
2491                     # drop scalar coordinates
2492                     # https://github.com/pydata/xarray/issues/6554
2493                     if name in self.coords and drop and new_var.ndim == 0:
2494                         continue
2495                 else:
2496                     new_var = var.copy(deep=False)
2497                 if name not in indexes:
2498                     new_var = new_var.to_base_variable()
2499             variables[name] = new_var
2500 
2501         coord_names = self._coord_names & variables.keys()
2502         selected = self._replace_with_new_dims(variables, coord_names, indexes)
2503 
2504         # Extract coordinates from indexers
2505         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)
2506         variables.update(coord_vars)
2507         indexes.update(new_indexes)
2508         coord_names = self._coord_names & variables.keys() | coord_vars.keys()
2509         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2510 
2511     def sel(
2512         self: T_Dataset,
2513         indexers: Mapping[Any, Any] | None = None,
2514         method: str | None = None,
2515         tolerance: int | float | Iterable[int | float] | None = None,
2516         drop: bool = False,
2517         **indexers_kwargs: Any,
2518     ) -> T_Dataset:
2519         """Returns a new dataset with each array indexed by tick labels
2520         along the specified dimension(s).
2521 
2522         In contrast to `Dataset.isel`, indexers for this method should use
2523         labels instead of integers.
2524 
2525         Under the hood, this method is powered by using pandas's powerful Index
2526         objects. This makes label based indexing essentially just as fast as
2527         using integer indexing.
2528 
2529         It also means this method uses pandas's (well documented) logic for
2530         indexing. This means you can use string shortcuts for datetime indexes
2531         (e.g., '2000-01' to select all values in January 2000). It also means
2532         that slices are treated as inclusive of both the start and stop values,
2533         unlike normal Python indexing.
2534 
2535         Parameters
2536         ----------
2537         indexers : dict, optional
2538             A dict with keys matching dimensions and values given
2539             by scalars, slices or arrays of tick labels. For dimensions with
2540             multi-index, the indexer may also be a dict-like object with keys
2541             matching index level names.
2542             If DataArrays are passed as indexers, xarray-style indexing will be
2543             carried out. See :ref:`indexing` for the details.
2544             One of indexers or indexers_kwargs must be provided.
2545         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2546             Method to use for inexact matches:
2547 
2548             * None (default): only exact matches
2549             * pad / ffill: propagate last valid index value forward
2550             * backfill / bfill: propagate next valid index value backward
2551             * nearest: use nearest valid index value
2552         tolerance : optional
2553             Maximum distance between original and new labels for inexact
2554             matches. The values of the index at the matching locations must
2555             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2556         drop : bool, optional
2557             If ``drop=True``, drop coordinates variables in `indexers` instead
2558             of making them scalar.
2559         **indexers_kwargs : {dim: indexer, ...}, optional
2560             The keyword arguments form of ``indexers``.
2561             One of indexers or indexers_kwargs must be provided.
2562 
2563         Returns
2564         -------
2565         obj : Dataset
2566             A new Dataset with the same contents as this dataset, except each
2567             variable and dimension is indexed by the appropriate indexers.
2568             If indexer DataArrays have coordinates that do not conflict with
2569             this object, then these coordinates will be attached.
2570             In general, each array's data will be a view of the array's data
2571             in this dataset, unless vectorized indexing was triggered by using
2572             an array indexer, in which case the data will be a copy.
2573 
2574         See Also
2575         --------
2576         Dataset.isel
2577         DataArray.sel
2578         """
2579         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "sel")
2580         query_results = map_index_queries(
2581             self, indexers=indexers, method=method, tolerance=tolerance
2582         )
2583 
2584         if drop:
2585             no_scalar_variables = {}
2586             for k, v in query_results.variables.items():
2587                 if v.dims:
2588                     no_scalar_variables[k] = v
2589                 else:
2590                     if k in self._coord_names:
2591                         query_results.drop_coords.append(k)
2592             query_results.variables = no_scalar_variables
2593 
2594         result = self.isel(indexers=query_results.dim_indexers, drop=drop)
2595         return result._overwrite_indexes(*query_results.as_tuple()[1:])
2596 
2597     def head(
2598         self: T_Dataset,
2599         indexers: Mapping[Any, int] | int | None = None,
2600         **indexers_kwargs: Any,
2601     ) -> T_Dataset:
2602         """Returns a new dataset with the first `n` values of each array
2603         for the specified dimension(s).
2604 
2605         Parameters
2606         ----------
2607         indexers : dict or int, default: 5
2608             A dict with keys matching dimensions and integer values `n`
2609             or a single integer `n` applied over all dimensions.
2610             One of indexers or indexers_kwargs must be provided.
2611         **indexers_kwargs : {dim: n, ...}, optional
2612             The keyword arguments form of ``indexers``.
2613             One of indexers or indexers_kwargs must be provided.
2614 
2615         See Also
2616         --------
2617         Dataset.tail
2618         Dataset.thin
2619         DataArray.head
2620         """
2621         if not indexers_kwargs:
2622             if indexers is None:
2623                 indexers = 5
2624             if not isinstance(indexers, int) and not is_dict_like(indexers):
2625                 raise TypeError("indexers must be either dict-like or a single integer")
2626         if isinstance(indexers, int):
2627             indexers = {dim: indexers for dim in self.dims}
2628         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "head")
2629         for k, v in indexers.items():
2630             if not isinstance(v, int):
2631                 raise TypeError(
2632                     "expected integer type indexer for "
2633                     f"dimension {k!r}, found {type(v)!r}"
2634                 )
2635             elif v < 0:
2636                 raise ValueError(
2637                     "expected positive integer as indexer "
2638                     f"for dimension {k!r}, found {v}"
2639                 )
2640         indexers_slices = {k: slice(val) for k, val in indexers.items()}
2641         return self.isel(indexers_slices)
2642 
2643     def tail(
2644         self: T_Dataset,
2645         indexers: Mapping[Any, int] | int | None = None,
2646         **indexers_kwargs: Any,
2647     ) -> T_Dataset:
2648         """Returns a new dataset with the last `n` values of each array
2649         for the specified dimension(s).
2650 
2651         Parameters
2652         ----------
2653         indexers : dict or int, default: 5
2654             A dict with keys matching dimensions and integer values `n`
2655             or a single integer `n` applied over all dimensions.
2656             One of indexers or indexers_kwargs must be provided.
2657         **indexers_kwargs : {dim: n, ...}, optional
2658             The keyword arguments form of ``indexers``.
2659             One of indexers or indexers_kwargs must be provided.
2660 
2661         See Also
2662         --------
2663         Dataset.head
2664         Dataset.thin
2665         DataArray.tail
2666         """
2667         if not indexers_kwargs:
2668             if indexers is None:
2669                 indexers = 5
2670             if not isinstance(indexers, int) and not is_dict_like(indexers):
2671                 raise TypeError("indexers must be either dict-like or a single integer")
2672         if isinstance(indexers, int):
2673             indexers = {dim: indexers for dim in self.dims}
2674         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "tail")
2675         for k, v in indexers.items():
2676             if not isinstance(v, int):
2677                 raise TypeError(
2678                     "expected integer type indexer for "
2679                     f"dimension {k!r}, found {type(v)!r}"
2680                 )
2681             elif v < 0:
2682                 raise ValueError(
2683                     "expected positive integer as indexer "
2684                     f"for dimension {k!r}, found {v}"
2685                 )
2686         indexers_slices = {
2687             k: slice(-val, None) if val != 0 else slice(val)
2688             for k, val in indexers.items()
2689         }
2690         return self.isel(indexers_slices)
2691 
2692     def thin(
2693         self: T_Dataset,
2694         indexers: Mapping[Any, int] | int | None = None,
2695         **indexers_kwargs: Any,
2696     ) -> T_Dataset:
2697         """Returns a new dataset with each array indexed along every `n`-th
2698         value for the specified dimension(s)
2699 
2700         Parameters
2701         ----------
2702         indexers : dict or int
2703             A dict with keys matching dimensions and integer values `n`
2704             or a single integer `n` applied over all dimensions.
2705             One of indexers or indexers_kwargs must be provided.
2706         **indexers_kwargs : {dim: n, ...}, optional
2707             The keyword arguments form of ``indexers``.
2708             One of indexers or indexers_kwargs must be provided.
2709 
2710         Examples
2711         --------
2712         >>> x_arr = np.arange(0, 26)
2713         >>> x_arr
2714         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
2715                17, 18, 19, 20, 21, 22, 23, 24, 25])
2716         >>> x = xr.DataArray(
2717         ...     np.reshape(x_arr, (2, 13)),
2718         ...     dims=("x", "y"),
2719         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
2720         ... )
2721         >>> x_ds = xr.Dataset({"foo": x})
2722         >>> x_ds
2723         <xarray.Dataset>
2724         Dimensions:  (x: 2, y: 13)
2725         Coordinates:
2726           * x        (x) int64 0 1
2727           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
2728         Data variables:
2729             foo      (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24 25
2730 
2731         >>> x_ds.thin(3)
2732         <xarray.Dataset>
2733         Dimensions:  (x: 1, y: 5)
2734         Coordinates:
2735           * x        (x) int64 0
2736           * y        (y) int64 0 3 6 9 12
2737         Data variables:
2738             foo      (x, y) int64 0 3 6 9 12
2739         >>> x.thin({"x": 2, "y": 5})
2740         <xarray.DataArray (x: 1, y: 3)>
2741         array([[ 0,  5, 10]])
2742         Coordinates:
2743           * x        (x) int64 0
2744           * y        (y) int64 0 5 10
2745 
2746         See Also
2747         --------
2748         Dataset.head
2749         Dataset.tail
2750         DataArray.thin
2751         """
2752         if (
2753             not indexers_kwargs
2754             and not isinstance(indexers, int)
2755             and not is_dict_like(indexers)
2756         ):
2757             raise TypeError("indexers must be either dict-like or a single integer")
2758         if isinstance(indexers, int):
2759             indexers = {dim: indexers for dim in self.dims}
2760         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "thin")
2761         for k, v in indexers.items():
2762             if not isinstance(v, int):
2763                 raise TypeError(
2764                     "expected integer type indexer for "
2765                     f"dimension {k!r}, found {type(v)!r}"
2766                 )
2767             elif v < 0:
2768                 raise ValueError(
2769                     "expected positive integer as indexer "
2770                     f"for dimension {k!r}, found {v}"
2771                 )
2772             elif v == 0:
2773                 raise ValueError("step cannot be zero")
2774         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}
2775         return self.isel(indexers_slices)
2776 
2777     def broadcast_like(
2778         self: T_Dataset,
2779         other: Dataset | DataArray,
2780         exclude: Iterable[Hashable] | None = None,
2781     ) -> T_Dataset:
2782         """Broadcast this DataArray against another Dataset or DataArray.
2783         This is equivalent to xr.broadcast(other, self)[1]
2784 
2785         Parameters
2786         ----------
2787         other : Dataset or DataArray
2788             Object against which to broadcast this array.
2789         exclude : iterable of hashable, optional
2790             Dimensions that must not be broadcasted
2791 
2792         """
2793         if exclude is None:
2794             exclude = set()
2795         else:
2796             exclude = set(exclude)
2797         args = align(other, self, join="outer", copy=False, exclude=exclude)
2798 
2799         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
2800 
2801         return _broadcast_helper(
2802             cast("T_Dataset", args[1]), exclude, dims_map, common_coords
2803         )
2804 
2805     def _reindex_callback(
2806         self,
2807         aligner: alignment.Aligner,
2808         dim_pos_indexers: dict[Hashable, Any],
2809         variables: dict[Hashable, Variable],
2810         indexes: dict[Hashable, Index],
2811         fill_value: Any,
2812         exclude_dims: frozenset[Hashable],
2813         exclude_vars: frozenset[Hashable],
2814     ) -> Dataset:
2815         """Callback called from ``Aligner`` to create a new reindexed Dataset."""
2816 
2817         new_variables = variables.copy()
2818         new_indexes = indexes.copy()
2819 
2820         # re-assign variable metadata
2821         for name, new_var in new_variables.items():
2822             var = self._variables.get(name)
2823             if var is not None:
2824                 new_var.attrs = var.attrs
2825                 new_var.encoding = var.encoding
2826 
2827         # pass through indexes from excluded dimensions
2828         # no extra check needed for multi-coordinate indexes, potential conflicts
2829         # should already have been detected when aligning the indexes
2830         for name, idx in self._indexes.items():
2831             var = self._variables[name]
2832             if set(var.dims) <= exclude_dims:
2833                 new_indexes[name] = idx
2834                 new_variables[name] = var
2835 
2836         if not dim_pos_indexers:
2837             # fast path for no reindexing necessary
2838             if set(new_indexes) - set(self._indexes):
2839                 # this only adds new indexes and their coordinate variables
2840                 reindexed = self._overwrite_indexes(new_indexes, new_variables)
2841             else:
2842                 reindexed = self.copy(deep=aligner.copy)
2843         else:
2844             to_reindex = {
2845                 k: v
2846                 for k, v in self.variables.items()
2847                 if k not in variables and k not in exclude_vars
2848             }
2849             reindexed_vars = alignment.reindex_variables(
2850                 to_reindex,
2851                 dim_pos_indexers,
2852                 copy=aligner.copy,
2853                 fill_value=fill_value,
2854                 sparse=aligner.sparse,
2855             )
2856             new_variables.update(reindexed_vars)
2857             new_coord_names = self._coord_names | set(new_indexes)
2858             reindexed = self._replace_with_new_dims(
2859                 new_variables, new_coord_names, indexes=new_indexes
2860             )
2861 
2862         return reindexed
2863 
2864     def reindex_like(
2865         self: T_Dataset,
2866         other: Dataset | DataArray,
2867         method: ReindexMethodOptions = None,
2868         tolerance: int | float | Iterable[int | float] | None = None,
2869         copy: bool = True,
2870         fill_value: Any = xrdtypes.NA,
2871     ) -> T_Dataset:
2872         """Conform this object onto the indexes of another object, filling in
2873         missing values with ``fill_value``. The default fill value is NaN.
2874 
2875         Parameters
2876         ----------
2877         other : Dataset or DataArray
2878             Object with an 'indexes' attribute giving a mapping from dimension
2879             names to pandas.Index objects, which provides coordinates upon
2880             which to index the variables in this dataset. The indexes on this
2881             other object need not be the same as the indexes on this
2882             dataset. Any mis-matched index values will be filled in with
2883             NaN, and any mis-matched dimension names will simply be ignored.
2884         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2885             Method to use for filling index values from other not found in this
2886             dataset:
2887 
2888             - None (default): don't fill gaps
2889             - "pad" / "ffill": propagate last valid index value forward
2890             - "backfill" / "bfill": propagate next valid index value backward
2891             - "nearest": use nearest valid index value
2892 
2893         tolerance : optional
2894             Maximum distance between original and new labels for inexact
2895             matches. The values of the index at the matching locations must
2896             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2897             Tolerance may be a scalar value, which applies the same tolerance
2898             to all values, or list-like, which applies variable tolerance per
2899             element. List-like must be the same size as the index and its dtype
2900             must exactly match the index’s type.
2901         copy : bool, default: True
2902             If ``copy=True``, data in the return value is always copied. If
2903             ``copy=False`` and reindexing is unnecessary, or can be performed
2904             with only slice operations, then the output may share memory with
2905             the input. In either case, a new xarray object is always returned.
2906         fill_value : scalar or dict-like, optional
2907             Value to use for newly missing values. If a dict-like maps
2908             variable names to fill values.
2909 
2910         Returns
2911         -------
2912         reindexed : Dataset
2913             Another dataset, with this dataset's data but coordinates from the
2914             other object.
2915 
2916         See Also
2917         --------
2918         Dataset.reindex
2919         align
2920         """
2921         return alignment.reindex_like(
2922             self,
2923             other=other,
2924             method=method,
2925             tolerance=tolerance,
2926             copy=copy,
2927             fill_value=fill_value,
2928         )
2929 
2930     def reindex(
2931         self: T_Dataset,
2932         indexers: Mapping[Any, Any] | None = None,
2933         method: ReindexMethodOptions = None,
2934         tolerance: int | float | Iterable[int | float] | None = None,
2935         copy: bool = True,
2936         fill_value: Any = xrdtypes.NA,
2937         **indexers_kwargs: Any,
2938     ) -> T_Dataset:
2939         """Conform this object onto a new set of indexes, filling in
2940         missing values with ``fill_value``. The default fill value is NaN.
2941 
2942         Parameters
2943         ----------
2944         indexers : dict, optional
2945             Dictionary with keys given by dimension names and values given by
2946             arrays of coordinates tick labels. Any mis-matched coordinate
2947             values will be filled in with NaN, and any mis-matched dimension
2948             names will simply be ignored.
2949             One of indexers or indexers_kwargs must be provided.
2950         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2951             Method to use for filling index values in ``indexers`` not found in
2952             this dataset:
2953 
2954             - None (default): don't fill gaps
2955             - "pad" / "ffill": propagate last valid index value forward
2956             - "backfill" / "bfill": propagate next valid index value backward
2957             - "nearest": use nearest valid index value
2958 
2959         tolerance : optional
2960             Maximum distance between original and new labels for inexact
2961             matches. The values of the index at the matching locations must
2962             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2963             Tolerance may be a scalar value, which applies the same tolerance
2964             to all values, or list-like, which applies variable tolerance per
2965             element. List-like must be the same size as the index and its dtype
2966             must exactly match the index’s type.
2967         copy : bool, default: True
2968             If ``copy=True``, data in the return value is always copied. If
2969             ``copy=False`` and reindexing is unnecessary, or can be performed
2970             with only slice operations, then the output may share memory with
2971             the input. In either case, a new xarray object is always returned.
2972         fill_value : scalar or dict-like, optional
2973             Value to use for newly missing values. If a dict-like,
2974             maps variable names (including coordinates) to fill values.
2975         sparse : bool, default: False
2976             use sparse-array.
2977         **indexers_kwargs : {dim: indexer, ...}, optional
2978             Keyword arguments in the same form as ``indexers``.
2979             One of indexers or indexers_kwargs must be provided.
2980 
2981         Returns
2982         -------
2983         reindexed : Dataset
2984             Another dataset, with this dataset's data but replaced coordinates.
2985 
2986         See Also
2987         --------
2988         Dataset.reindex_like
2989         align
2990         pandas.Index.get_indexer
2991 
2992         Examples
2993         --------
2994         Create a dataset with some fictional data.
2995 
2996         >>> x = xr.Dataset(
2997         ...     {
2998         ...         "temperature": ("station", 20 * np.random.rand(4)),
2999         ...         "pressure": ("station", 500 * np.random.rand(4)),
3000         ...     },
3001         ...     coords={"station": ["boston", "nyc", "seattle", "denver"]},
3002         ... )
3003         >>> x
3004         <xarray.Dataset>
3005         Dimensions:      (station: 4)
3006         Coordinates:
3007           * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'
3008         Data variables:
3009             temperature  (station) float64 10.98 14.3 12.06 10.9
3010             pressure     (station) float64 211.8 322.9 218.8 445.9
3011         >>> x.indexes
3012         Indexes:
3013             station  Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')
3014 
3015         Create a new index and reindex the dataset. By default values in the new index that
3016         do not have corresponding records in the dataset are assigned `NaN`.
3017 
3018         >>> new_index = ["boston", "austin", "seattle", "lincoln"]
3019         >>> x.reindex({"station": new_index})
3020         <xarray.Dataset>
3021         Dimensions:      (station: 4)
3022         Coordinates:
3023           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
3024         Data variables:
3025             temperature  (station) float64 10.98 nan 12.06 nan
3026             pressure     (station) float64 211.8 nan 218.8 nan
3027 
3028         We can fill in the missing values by passing a value to the keyword `fill_value`.
3029 
3030         >>> x.reindex({"station": new_index}, fill_value=0)
3031         <xarray.Dataset>
3032         Dimensions:      (station: 4)
3033         Coordinates:
3034           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
3035         Data variables:
3036             temperature  (station) float64 10.98 0.0 12.06 0.0
3037             pressure     (station) float64 211.8 0.0 218.8 0.0
3038 
3039         We can also use different fill values for each variable.
3040 
3041         >>> x.reindex(
3042         ...     {"station": new_index}, fill_value={"temperature": 0, "pressure": 100}
3043         ... )
3044         <xarray.Dataset>
3045         Dimensions:      (station: 4)
3046         Coordinates:
3047           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
3048         Data variables:
3049             temperature  (station) float64 10.98 0.0 12.06 0.0
3050             pressure     (station) float64 211.8 100.0 218.8 100.0
3051 
3052         Because the index is not monotonically increasing or decreasing, we cannot use arguments
3053         to the keyword method to fill the `NaN` values.
3054 
3055         >>> x.reindex({"station": new_index}, method="nearest")
3056         Traceback (most recent call last):
3057         ...
3058             raise ValueError('index must be monotonic increasing or decreasing')
3059         ValueError: index must be monotonic increasing or decreasing
3060 
3061         To further illustrate the filling functionality in reindex, we will create a
3062         dataset with a monotonically increasing index (for example, a sequence of dates).
3063 
3064         >>> x2 = xr.Dataset(
3065         ...     {
3066         ...         "temperature": (
3067         ...             "time",
3068         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],
3069         ...         ),
3070         ...         "pressure": ("time", 500 * np.random.rand(6)),
3071         ...     },
3072         ...     coords={"time": pd.date_range("01/01/2019", periods=6, freq="D")},
3073         ... )
3074         >>> x2
3075         <xarray.Dataset>
3076         Dimensions:      (time: 6)
3077         Coordinates:
3078           * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06
3079         Data variables:
3080             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12
3081             pressure     (time) float64 481.8 191.7 395.9 264.4 284.0 462.8
3082 
3083         Suppose we decide to expand the dataset to cover a wider date range.
3084 
3085         >>> time_index2 = pd.date_range("12/29/2018", periods=10, freq="D")
3086         >>> x2.reindex({"time": time_index2})
3087         <xarray.Dataset>
3088         Dimensions:      (time: 10)
3089         Coordinates:
3090           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3091         Data variables:
3092             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan
3093             pressure     (time) float64 nan nan nan 481.8 ... 264.4 284.0 462.8 nan
3094 
3095         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)
3096         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.
3097 
3098         For example, to back-propagate the last valid value to fill the `NaN` values,
3099         pass `bfill` as an argument to the `method` keyword.
3100 
3101         >>> x3 = x2.reindex({"time": time_index2}, method="bfill")
3102         >>> x3
3103         <xarray.Dataset>
3104         Dimensions:      (time: 10)
3105         Coordinates:
3106           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3107         Data variables:
3108             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan
3109             pressure     (time) float64 481.8 481.8 481.8 481.8 ... 284.0 462.8 nan
3110 
3111         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)
3112         will not be filled by any of the value propagation schemes.
3113 
3114         >>> x2.where(x2.temperature.isnull(), drop=True)
3115         <xarray.Dataset>
3116         Dimensions:      (time: 1)
3117         Coordinates:
3118           * time         (time) datetime64[ns] 2019-01-03
3119         Data variables:
3120             temperature  (time) float64 nan
3121             pressure     (time) float64 395.9
3122         >>> x3.where(x3.temperature.isnull(), drop=True)
3123         <xarray.Dataset>
3124         Dimensions:      (time: 2)
3125         Coordinates:
3126           * time         (time) datetime64[ns] 2019-01-03 2019-01-07
3127         Data variables:
3128             temperature  (time) float64 nan nan
3129             pressure     (time) float64 395.9 nan
3130 
3131         This is because filling while reindexing does not look at dataset values, but only compares
3132         the original and desired indexes. If you do want to fill in the `NaN` values present in the
3133         original dataset, use the :py:meth:`~Dataset.fillna()` method.
3134 
3135         """
3136         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3137         return alignment.reindex(
3138             self,
3139             indexers=indexers,
3140             method=method,
3141             tolerance=tolerance,
3142             copy=copy,
3143             fill_value=fill_value,
3144         )
3145 
3146     def _reindex(
3147         self: T_Dataset,
3148         indexers: Mapping[Any, Any] | None = None,
3149         method: str | None = None,
3150         tolerance: int | float | Iterable[int | float] | None = None,
3151         copy: bool = True,
3152         fill_value: Any = xrdtypes.NA,
3153         sparse: bool = False,
3154         **indexers_kwargs: Any,
3155     ) -> T_Dataset:
3156         """
3157         Same as reindex but supports sparse option.
3158         """
3159         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3160         return alignment.reindex(
3161             self,
3162             indexers=indexers,
3163             method=method,
3164             tolerance=tolerance,
3165             copy=copy,
3166             fill_value=fill_value,
3167             sparse=sparse,
3168         )
3169 
3170     def interp(
3171         self: T_Dataset,
3172         coords: Mapping[Any, Any] | None = None,
3173         method: InterpOptions = "linear",
3174         assume_sorted: bool = False,
3175         kwargs: Mapping[str, Any] | None = None,
3176         method_non_numeric: str = "nearest",
3177         **coords_kwargs: Any,
3178     ) -> T_Dataset:
3179         """Interpolate a Dataset onto new coordinates
3180 
3181         Performs univariate or multivariate interpolation of a Dataset onto
3182         new coordinates using scipy's interpolation routines. If interpolating
3183         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
3184         called.  When interpolating along multiple existing dimensions, an
3185         attempt is made to decompose the interpolation into multiple
3186         1-dimensional interpolations. If this is possible,
3187         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3188         :py:func:`scipy.interpolate.interpn` is called.
3189 
3190         Parameters
3191         ----------
3192         coords : dict, optional
3193             Mapping from dimension names to the new coordinates.
3194             New coordinate can be a scalar, array-like or DataArray.
3195             If DataArrays are passed as new coordinates, their dimensions are
3196             used for the broadcasting. Missing values are skipped.
3197         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3198             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3199             String indicating which method to use for interpolation:
3200 
3201             - 'linear': linear interpolation. Additional keyword
3202               arguments are passed to :py:func:`numpy.interp`
3203             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3204               are passed to :py:func:`scipy.interpolate.interp1d`. If
3205               ``method='polynomial'``, the ``order`` keyword argument must also be
3206               provided.
3207             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3208               respective :py:class:`scipy.interpolate` classes.
3209 
3210         assume_sorted : bool, default: False
3211             If False, values of coordinates that are interpolated over can be
3212             in any order and they are sorted first. If True, interpolated
3213             coordinates are assumed to be an array of monotonically increasing
3214             values.
3215         kwargs : dict, optional
3216             Additional keyword arguments passed to scipy's interpolator. Valid
3217             options and their behavior depend whether ``interp1d`` or
3218             ``interpn`` is used.
3219         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3220             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3221             ``"nearest"`` is used by default.
3222         **coords_kwargs : {dim: coordinate, ...}, optional
3223             The keyword arguments form of ``coords``.
3224             One of coords or coords_kwargs must be provided.
3225 
3226         Returns
3227         -------
3228         interpolated : Dataset
3229             New dataset on the new coordinates.
3230 
3231         Notes
3232         -----
3233         scipy is required.
3234 
3235         See Also
3236         --------
3237         scipy.interpolate.interp1d
3238         scipy.interpolate.interpn
3239 
3240         Examples
3241         --------
3242         >>> ds = xr.Dataset(
3243         ...     data_vars={
3244         ...         "a": ("x", [5, 7, 4]),
3245         ...         "b": (
3246         ...             ("x", "y"),
3247         ...             [[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
3248         ...         ),
3249         ...     },
3250         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
3251         ... )
3252         >>> ds
3253         <xarray.Dataset>
3254         Dimensions:  (x: 3, y: 4)
3255         Coordinates:
3256           * x        (x) int64 0 1 2
3257           * y        (y) int64 10 12 14 16
3258         Data variables:
3259             a        (x) int64 5 7 4
3260             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 6.0 nan 6.0 nan 5.0 8.0
3261 
3262         1D interpolation with the default method (linear):
3263 
3264         >>> ds.interp(x=[0, 0.75, 1.25, 1.75])
3265         <xarray.Dataset>
3266         Dimensions:  (x: 4, y: 4)
3267         Coordinates:
3268           * y        (y) int64 10 12 14 16
3269           * x        (x) float64 0.0 0.75 1.25 1.75
3270         Data variables:
3271             a        (x) float64 5.0 6.5 6.25 4.75
3272             b        (x, y) float64 1.0 4.0 2.0 nan 1.75 6.25 ... nan 5.0 nan 5.25 nan
3273 
3274         1D interpolation with a different method:
3275 
3276         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
3277         <xarray.Dataset>
3278         Dimensions:  (x: 4, y: 4)
3279         Coordinates:
3280           * y        (y) int64 10 12 14 16
3281           * x        (x) float64 0.0 0.75 1.25 1.75
3282         Data variables:
3283             a        (x) float64 5.0 7.0 7.0 4.0
3284             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 ... 6.0 nan 6.0 nan 5.0 8.0
3285 
3286         1D extrapolation:
3287 
3288         >>> ds.interp(
3289         ...     x=[1, 1.5, 2.5, 3.5],
3290         ...     method="linear",
3291         ...     kwargs={"fill_value": "extrapolate"},
3292         ... )
3293         <xarray.Dataset>
3294         Dimensions:  (x: 4, y: 4)
3295         Coordinates:
3296           * y        (y) int64 10 12 14 16
3297           * x        (x) float64 1.0 1.5 2.5 3.5
3298         Data variables:
3299             a        (x) float64 7.0 5.5 2.5 -0.5
3300             b        (x, y) float64 2.0 7.0 6.0 nan 4.0 nan ... 4.5 nan 12.0 nan 3.5 nan
3301 
3302         2D interpolation:
3303 
3304         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
3305         <xarray.Dataset>
3306         Dimensions:  (x: 4, y: 3)
3307         Coordinates:
3308           * x        (x) float64 0.0 0.75 1.25 1.75
3309           * y        (y) int64 11 13 15
3310         Data variables:
3311             a        (x) float64 5.0 6.5 6.25 4.75
3312             b        (x, y) float64 2.5 3.0 nan 4.0 5.625 nan nan nan nan nan nan nan
3313         """
3314         from xarray.core import missing
3315 
3316         if kwargs is None:
3317             kwargs = {}
3318 
3319         coords = either_dict_or_kwargs(coords, coords_kwargs, "interp")
3320         indexers = dict(self._validate_interp_indexers(coords))
3321 
3322         if coords:
3323             # This avoids broadcasting over coordinates that are both in
3324             # the original array AND in the indexing array. It essentially
3325             # forces interpolation along the shared coordinates.
3326             sdims = (
3327                 set(self.dims)
3328                 .intersection(*[set(nx.dims) for nx in indexers.values()])
3329                 .difference(coords.keys())
3330             )
3331             indexers.update({d: self.variables[d] for d in sdims})
3332 
3333         obj = self if assume_sorted else self.sortby([k for k in coords])
3334 
3335         def maybe_variable(obj, k):
3336             # workaround to get variable for dimension without coordinate.
3337             try:
3338                 return obj._variables[k]
3339             except KeyError:
3340                 return as_variable((k, range(obj.dims[k])))
3341 
3342         def _validate_interp_indexer(x, new_x):
3343             # In the case of datetimes, the restrictions placed on indexers
3344             # used with interp are stronger than those which are placed on
3345             # isel, so we need an additional check after _validate_indexers.
3346             if _contains_datetime_like_objects(
3347                 x
3348             ) and not _contains_datetime_like_objects(new_x):
3349                 raise TypeError(
3350                     "When interpolating over a datetime-like "
3351                     "coordinate, the coordinates to "
3352                     "interpolate to must be either datetime "
3353                     "strings or datetimes. "
3354                     "Instead got\n{}".format(new_x)
3355                 )
3356             return x, new_x
3357 
3358         validated_indexers = {
3359             k: _validate_interp_indexer(maybe_variable(obj, k), v)
3360             for k, v in indexers.items()
3361         }
3362 
3363         # optimization: subset to coordinate range of the target index
3364         if method in ["linear", "nearest"]:
3365             for k, v in validated_indexers.items():
3366                 obj, newidx = missing._localize(obj, {k: v})
3367                 validated_indexers[k] = newidx[k]
3368 
3369         # optimization: create dask coordinate arrays once per Dataset
3370         # rather than once per Variable when dask.array.unify_chunks is called later
3371         # GH4739
3372         if obj.__dask_graph__():
3373             dask_indexers = {
3374                 k: (index.to_base_variable().chunk(), dest.to_base_variable().chunk())
3375                 for k, (index, dest) in validated_indexers.items()
3376             }
3377 
3378         variables: dict[Hashable, Variable] = {}
3379         reindex: bool = False
3380         for name, var in obj._variables.items():
3381             if name in indexers:
3382                 continue
3383 
3384             if is_duck_dask_array(var.data):
3385                 use_indexers = dask_indexers
3386             else:
3387                 use_indexers = validated_indexers
3388 
3389             dtype_kind = var.dtype.kind
3390             if dtype_kind in "uifc":
3391                 # For normal number types do the interpolation:
3392                 var_indexers = {k: v for k, v in use_indexers.items() if k in var.dims}
3393                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)
3394             elif dtype_kind in "ObU" and (use_indexers.keys() & var.dims):
3395                 # For types that we do not understand do stepwise
3396                 # interpolation to avoid modifying the elements.
3397                 # reindex the variable instead because it supports
3398                 # booleans and objects and retains the dtype but inside
3399                 # this loop there might be some duplicate code that slows it
3400                 # down, therefore collect these signals and run it later:
3401                 reindex = True
3402             elif all(d not in indexers for d in var.dims):
3403                 # For anything else we can only keep variables if they
3404                 # are not dependent on any coords that are being
3405                 # interpolated along:
3406                 variables[name] = var
3407 
3408         if reindex:
3409             reindex_indexers = {
3410                 k: v for k, (_, v) in validated_indexers.items() if v.dims == (k,)
3411             }
3412             reindexed = alignment.reindex(
3413                 obj,
3414                 indexers=reindex_indexers,
3415                 method=method_non_numeric,
3416                 exclude_vars=variables.keys(),
3417             )
3418             indexes = dict(reindexed._indexes)
3419             variables.update(reindexed.variables)
3420         else:
3421             # Get the indexes that are not being interpolated along
3422             indexes = {k: v for k, v in obj._indexes.items() if k not in indexers}
3423 
3424         # Get the coords that also exist in the variables:
3425         coord_names = obj._coord_names & variables.keys()
3426         selected = self._replace_with_new_dims(
3427             variables.copy(), coord_names, indexes=indexes
3428         )
3429 
3430         # Attach indexer as coordinate
3431         for k, v in indexers.items():
3432             assert isinstance(v, Variable)
3433             if v.dims == (k,):
3434                 index = PandasIndex(v, k, coord_dtype=v.dtype)
3435                 index_vars = index.create_variables({k: v})
3436                 indexes[k] = index
3437                 variables.update(index_vars)
3438             else:
3439                 variables[k] = v
3440 
3441         # Extract coordinates from indexers
3442         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)
3443         variables.update(coord_vars)
3444         indexes.update(new_indexes)
3445 
3446         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()
3447         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3448 
3449     def interp_like(
3450         self,
3451         other: Dataset | DataArray,
3452         method: InterpOptions = "linear",
3453         assume_sorted: bool = False,
3454         kwargs: Mapping[str, Any] | None = None,
3455         method_non_numeric: str = "nearest",
3456     ) -> Dataset:
3457         """Interpolate this object onto the coordinates of another object,
3458         filling the out of range values with NaN.
3459 
3460         If interpolating along a single existing dimension,
3461         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
3462         along multiple existing dimensions, an attempt is made to decompose the
3463         interpolation into multiple 1-dimensional interpolations. If this is
3464         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3465         :py:func:`scipy.interpolate.interpn` is called.
3466 
3467         Parameters
3468         ----------
3469         other : Dataset or DataArray
3470             Object with an 'indexes' attribute giving a mapping from dimension
3471             names to an 1d array-like, which provides coordinates upon
3472             which to index the variables in this dataset. Missing values are skipped.
3473         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3474             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3475             String indicating which method to use for interpolation:
3476 
3477             - 'linear': linear interpolation. Additional keyword
3478               arguments are passed to :py:func:`numpy.interp`
3479             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3480               are passed to :py:func:`scipy.interpolate.interp1d`. If
3481               ``method='polynomial'``, the ``order`` keyword argument must also be
3482               provided.
3483             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3484               respective :py:class:`scipy.interpolate` classes.
3485 
3486         assume_sorted : bool, default: False
3487             If False, values of coordinates that are interpolated over can be
3488             in any order and they are sorted first. If True, interpolated
3489             coordinates are assumed to be an array of monotonically increasing
3490             values.
3491         kwargs : dict, optional
3492             Additional keyword passed to scipy's interpolator.
3493         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3494             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3495             ``"nearest"`` is used by default.
3496 
3497         Returns
3498         -------
3499         interpolated : Dataset
3500             Another dataset by interpolating this dataset's data along the
3501             coordinates of the other object.
3502 
3503         Notes
3504         -----
3505         scipy is required.
3506         If the dataset has object-type coordinates, reindex is used for these
3507         coordinates instead of the interpolation.
3508 
3509         See Also
3510         --------
3511         Dataset.interp
3512         Dataset.reindex_like
3513         """
3514         if kwargs is None:
3515             kwargs = {}
3516 
3517         # pick only dimension coordinates with a single index
3518         coords = {}
3519         other_indexes = other.xindexes
3520         for dim in self.dims:
3521             other_dim_coords = other_indexes.get_all_coords(dim, errors="ignore")
3522             if len(other_dim_coords) == 1:
3523                 coords[dim] = other_dim_coords[dim]
3524 
3525         numeric_coords: dict[Hashable, pd.Index] = {}
3526         object_coords: dict[Hashable, pd.Index] = {}
3527         for k, v in coords.items():
3528             if v.dtype.kind in "uifcMm":
3529                 numeric_coords[k] = v
3530             else:
3531                 object_coords[k] = v
3532 
3533         ds = self
3534         if object_coords:
3535             # We do not support interpolation along object coordinate.
3536             # reindex instead.
3537             ds = self.reindex(object_coords)
3538         return ds.interp(
3539             coords=numeric_coords,
3540             method=method,
3541             assume_sorted=assume_sorted,
3542             kwargs=kwargs,
3543             method_non_numeric=method_non_numeric,
3544         )
3545 
3546     # Helper methods for rename()
3547     def _rename_vars(
3548         self, name_dict, dims_dict
3549     ) -> tuple[dict[Hashable, Variable], set[Hashable]]:
3550         variables = {}
3551         coord_names = set()
3552         for k, v in self.variables.items():
3553             var = v.copy(deep=False)
3554             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3555             name = name_dict.get(k, k)
3556             if name in variables:
3557                 raise ValueError(f"the new name {name!r} conflicts")
3558             variables[name] = var
3559             if k in self._coord_names:
3560                 coord_names.add(name)
3561         return variables, coord_names
3562 
3563     def _rename_dims(self, name_dict: Mapping[Any, Hashable]) -> dict[Hashable, int]:
3564         return {name_dict.get(k, k): v for k, v in self.dims.items()}
3565 
3566     def _rename_indexes(
3567         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3568     ) -> tuple[dict[Hashable, Index], dict[Hashable, Variable]]:
3569         if not self._indexes:
3570             return {}, {}
3571 
3572         indexes = {}
3573         variables = {}
3574 
3575         for index, coord_names in self.xindexes.group_by_index():
3576             new_index = index.rename(name_dict, dims_dict)
3577             new_coord_names = [name_dict.get(k, k) for k in coord_names]
3578             indexes.update({k: new_index for k in new_coord_names})
3579             new_index_vars = new_index.create_variables(
3580                 {
3581                     new: self._variables[old]
3582                     for old, new in zip(coord_names, new_coord_names)
3583                 }
3584             )
3585             variables.update(new_index_vars)
3586 
3587         return indexes, variables
3588 
3589     def _rename_all(
3590         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3591     ) -> tuple[
3592         dict[Hashable, Variable],
3593         set[Hashable],
3594         dict[Hashable, int],
3595         dict[Hashable, Index],
3596     ]:
3597         variables, coord_names = self._rename_vars(name_dict, dims_dict)
3598         dims = self._rename_dims(dims_dict)
3599 
3600         indexes, index_vars = self._rename_indexes(name_dict, dims_dict)
3601         variables = {k: index_vars.get(k, v) for k, v in variables.items()}
3602 
3603         return variables, coord_names, dims, indexes
3604 
3605     def _rename(
3606         self: T_Dataset,
3607         name_dict: Mapping[Any, Hashable] | None = None,
3608         **names: Hashable,
3609     ) -> T_Dataset:
3610         """Also used internally by DataArray so that the warning (if any)
3611         is raised at the right stack level.
3612         """
3613         name_dict = either_dict_or_kwargs(name_dict, names, "rename")
3614         for k in name_dict.keys():
3615             if k not in self and k not in self.dims:
3616                 raise ValueError(
3617                     f"cannot rename {k!r} because it is not a "
3618                     "variable or dimension in this dataset"
3619                 )
3620 
3621             create_dim_coord = False
3622             new_k = name_dict[k]
3623 
3624             if k in self.dims and new_k in self._coord_names:
3625                 coord_dims = self._variables[name_dict[k]].dims
3626                 if coord_dims == (k,):
3627                     create_dim_coord = True
3628             elif k in self._coord_names and new_k in self.dims:
3629                 coord_dims = self._variables[k].dims
3630                 if coord_dims == (new_k,):
3631                     create_dim_coord = True
3632 
3633             if create_dim_coord:
3634                 warnings.warn(
3635                     f"rename {k!r} to {name_dict[k]!r} does not create an index "
3636                     "anymore. Try using swap_dims instead or use set_index "
3637                     "after rename to create an indexed coordinate.",
3638                     UserWarning,
3639                     stacklevel=3,
3640                 )
3641 
3642         variables, coord_names, dims, indexes = self._rename_all(
3643             name_dict=name_dict, dims_dict=name_dict
3644         )
3645         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3646 
3647     def rename(
3648         self: T_Dataset,
3649         name_dict: Mapping[Any, Hashable] | None = None,
3650         **names: Hashable,
3651     ) -> T_Dataset:
3652         """Returns a new object with renamed variables, coordinates and dimensions.
3653 
3654         Parameters
3655         ----------
3656         name_dict : dict-like, optional
3657             Dictionary whose keys are current variable, coordinate or dimension names and
3658             whose values are the desired names.
3659         **names : optional
3660             Keyword form of ``name_dict``.
3661             One of name_dict or names must be provided.
3662 
3663         Returns
3664         -------
3665         renamed : Dataset
3666             Dataset with renamed variables, coordinates and dimensions.
3667 
3668         See Also
3669         --------
3670         Dataset.swap_dims
3671         Dataset.rename_vars
3672         Dataset.rename_dims
3673         DataArray.rename
3674         """
3675         return self._rename(name_dict=name_dict, **names)
3676 
3677     def rename_dims(
3678         self: T_Dataset,
3679         dims_dict: Mapping[Any, Hashable] | None = None,
3680         **dims: Hashable,
3681     ) -> T_Dataset:
3682         """Returns a new object with renamed dimensions only.
3683 
3684         Parameters
3685         ----------
3686         dims_dict : dict-like, optional
3687             Dictionary whose keys are current dimension names and
3688             whose values are the desired names. The desired names must
3689             not be the name of an existing dimension or Variable in the Dataset.
3690         **dims : optional
3691             Keyword form of ``dims_dict``.
3692             One of dims_dict or dims must be provided.
3693 
3694         Returns
3695         -------
3696         renamed : Dataset
3697             Dataset with renamed dimensions.
3698 
3699         See Also
3700         --------
3701         Dataset.swap_dims
3702         Dataset.rename
3703         Dataset.rename_vars
3704         DataArray.rename
3705         """
3706         dims_dict = either_dict_or_kwargs(dims_dict, dims, "rename_dims")
3707         for k, v in dims_dict.items():
3708             if k not in self.dims:
3709                 raise ValueError(
3710                     f"cannot rename {k!r} because it is not a "
3711                     "dimension in this dataset"
3712                 )
3713             if v in self.dims or v in self:
3714                 raise ValueError(
3715                     f"Cannot rename {k} to {v} because {v} already exists. "
3716                     "Try using swap_dims instead."
3717                 )
3718 
3719         variables, coord_names, sizes, indexes = self._rename_all(
3720             name_dict={}, dims_dict=dims_dict
3721         )
3722         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)
3723 
3724     def rename_vars(
3725         self: T_Dataset,
3726         name_dict: Mapping[Any, Hashable] | None = None,
3727         **names: Hashable,
3728     ) -> T_Dataset:
3729         """Returns a new object with renamed variables including coordinates
3730 
3731         Parameters
3732         ----------
3733         name_dict : dict-like, optional
3734             Dictionary whose keys are current variable or coordinate names and
3735             whose values are the desired names.
3736         **names : optional
3737             Keyword form of ``name_dict``.
3738             One of name_dict or names must be provided.
3739 
3740         Returns
3741         -------
3742         renamed : Dataset
3743             Dataset with renamed variables including coordinates
3744 
3745         See Also
3746         --------
3747         Dataset.swap_dims
3748         Dataset.rename
3749         Dataset.rename_dims
3750         DataArray.rename
3751         """
3752         name_dict = either_dict_or_kwargs(name_dict, names, "rename_vars")
3753         for k in name_dict:
3754             if k not in self:
3755                 raise ValueError(
3756                     f"cannot rename {k!r} because it is not a "
3757                     "variable or coordinate in this dataset"
3758                 )
3759         variables, coord_names, dims, indexes = self._rename_all(
3760             name_dict=name_dict, dims_dict={}
3761         )
3762         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3763 
3764     def swap_dims(
3765         self: T_Dataset, dims_dict: Mapping[Any, Hashable] | None = None, **dims_kwargs
3766     ) -> T_Dataset:
3767         """Returns a new object with swapped dimensions.
3768 
3769         Parameters
3770         ----------
3771         dims_dict : dict-like
3772             Dictionary whose keys are current dimension names and whose values
3773             are new names.
3774         **dims_kwargs : {existing_dim: new_dim, ...}, optional
3775             The keyword arguments form of ``dims_dict``.
3776             One of dims_dict or dims_kwargs must be provided.
3777 
3778         Returns
3779         -------
3780         swapped : Dataset
3781             Dataset with swapped dimensions.
3782 
3783         Examples
3784         --------
3785         >>> ds = xr.Dataset(
3786         ...     data_vars={"a": ("x", [5, 7]), "b": ("x", [0.1, 2.4])},
3787         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
3788         ... )
3789         >>> ds
3790         <xarray.Dataset>
3791         Dimensions:  (x: 2)
3792         Coordinates:
3793           * x        (x) <U1 'a' 'b'
3794             y        (x) int64 0 1
3795         Data variables:
3796             a        (x) int64 5 7
3797             b        (x) float64 0.1 2.4
3798 
3799         >>> ds.swap_dims({"x": "y"})
3800         <xarray.Dataset>
3801         Dimensions:  (y: 2)
3802         Coordinates:
3803             x        (y) <U1 'a' 'b'
3804           * y        (y) int64 0 1
3805         Data variables:
3806             a        (y) int64 5 7
3807             b        (y) float64 0.1 2.4
3808 
3809         >>> ds.swap_dims({"x": "z"})
3810         <xarray.Dataset>
3811         Dimensions:  (z: 2)
3812         Coordinates:
3813             x        (z) <U1 'a' 'b'
3814             y        (z) int64 0 1
3815         Dimensions without coordinates: z
3816         Data variables:
3817             a        (z) int64 5 7
3818             b        (z) float64 0.1 2.4
3819 
3820         See Also
3821         --------
3822         Dataset.rename
3823         DataArray.swap_dims
3824         """
3825         # TODO: deprecate this method in favor of a (less confusing)
3826         # rename_dims() method that only renames dimensions.
3827 
3828         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
3829         for k, v in dims_dict.items():
3830             if k not in self.dims:
3831                 raise ValueError(
3832                     f"cannot swap from dimension {k!r} because it is "
3833                     "not an existing dimension"
3834                 )
3835             if v in self.variables and self.variables[v].dims != (k,):
3836                 raise ValueError(
3837                     f"replacement dimension {v!r} is not a 1D "
3838                     f"variable along the old dimension {k!r}"
3839                 )
3840 
3841         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}
3842 
3843         coord_names = self._coord_names.copy()
3844         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})
3845 
3846         variables: dict[Hashable, Variable] = {}
3847         indexes: dict[Hashable, Index] = {}
3848         for k, v in self.variables.items():
3849             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3850             var: Variable
3851             if k in result_dims:
3852                 var = v.to_index_variable()
3853                 var.dims = dims
3854                 if k in self._indexes:
3855                     indexes[k] = self._indexes[k]
3856                     variables[k] = var
3857                 else:
3858                     index, index_vars = create_default_index_implicit(var)
3859                     indexes.update({name: index for name in index_vars})
3860                     variables.update(index_vars)
3861                     coord_names.update(index_vars)
3862             else:
3863                 var = v.to_base_variable()
3864                 var.dims = dims
3865                 variables[k] = var
3866 
3867         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3868 
3869     # change type of self and return to T_Dataset once
3870     # https://github.com/python/mypy/issues/12846 is resolved
3871     def expand_dims(
3872         self,
3873         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
3874         axis: None | int | Sequence[int] = None,
3875         **dim_kwargs: Any,
3876     ) -> Dataset:
3877         """Return a new object with an additional axis (or axes) inserted at
3878         the corresponding position in the array shape.  The new object is a
3879         view into the underlying array, not a copy.
3880 
3881         If dim is already a scalar coordinate, it will be promoted to a 1D
3882         coordinate consisting of a single value.
3883 
3884         Parameters
3885         ----------
3886         dim : hashable, sequence of hashable, mapping, or None
3887             Dimensions to include on the new variable. If provided as hashable
3888             or sequence of hashable, then dimensions are inserted with length
3889             1. If provided as a mapping, then the keys are the new dimensions
3890             and the values are either integers (giving the length of the new
3891             dimensions) or array-like (giving the coordinates of the new
3892             dimensions).
3893         axis : int, sequence of int, or None, default: None
3894             Axis position(s) where new axis is to be inserted (position(s) on
3895             the result array). If a sequence of integers is passed,
3896             multiple axes are inserted. In this case, dim arguments should be
3897             same length list. If axis=None is passed, all the axes will be
3898             inserted to the start of the result array.
3899         **dim_kwargs : int or sequence or ndarray
3900             The keywords are arbitrary dimensions being inserted and the values
3901             are either the lengths of the new dims (if int is given), or their
3902             coordinates. Note, this is an alternative to passing a dict to the
3903             dim kwarg and will only be used if dim is None.
3904 
3905         Returns
3906         -------
3907         expanded : Dataset
3908             This object, but with additional dimension(s).
3909 
3910         See Also
3911         --------
3912         DataArray.expand_dims
3913         """
3914         if dim is None:
3915             pass
3916         elif isinstance(dim, Mapping):
3917             # We're later going to modify dim in place; don't tamper with
3918             # the input
3919             dim = dict(dim)
3920         elif isinstance(dim, int):
3921             raise TypeError(
3922                 "dim should be hashable or sequence of hashables or mapping"
3923             )
3924         elif isinstance(dim, str) or not isinstance(dim, Sequence):
3925             dim = {dim: 1}
3926         elif isinstance(dim, Sequence):
3927             if len(dim) != len(set(dim)):
3928                 raise ValueError("dims should not contain duplicate values.")
3929             dim = {d: 1 for d in dim}
3930 
3931         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
3932         assert isinstance(dim, MutableMapping)
3933 
3934         if axis is None:
3935             axis = list(range(len(dim)))
3936         elif not isinstance(axis, Sequence):
3937             axis = [axis]
3938 
3939         if len(dim) != len(axis):
3940             raise ValueError("lengths of dim and axis should be identical.")
3941         for d in dim:
3942             if d in self.dims:
3943                 raise ValueError(f"Dimension {d} already exists.")
3944             if d in self._variables and not utils.is_scalar(self._variables[d]):
3945                 raise ValueError(
3946                     "{dim} already exists as coordinate or"
3947                     " variable name.".format(dim=d)
3948                 )
3949 
3950         variables: dict[Hashable, Variable] = {}
3951         indexes: dict[Hashable, Index] = dict(self._indexes)
3952         coord_names = self._coord_names.copy()
3953         # If dim is a dict, then ensure that the values are either integers
3954         # or iterables.
3955         for k, v in dim.items():
3956             if hasattr(v, "__iter__"):
3957                 # If the value for the new dimension is an iterable, then
3958                 # save the coordinates to the variables dict, and set the
3959                 # value within the dim dict to the length of the iterable
3960                 # for later use.
3961                 index = PandasIndex(v, k)
3962                 indexes[k] = index
3963                 variables.update(index.create_variables())
3964                 coord_names.add(k)
3965                 dim[k] = variables[k].size
3966             elif isinstance(v, int):
3967                 pass  # Do nothing if the dimensions value is just an int
3968             else:
3969                 raise TypeError(
3970                     "The value of new dimension {k} must be "
3971                     "an iterable or an int".format(k=k)
3972                 )
3973 
3974         for k, v in self._variables.items():
3975             if k not in dim:
3976                 if k in coord_names:  # Do not change coordinates
3977                     variables[k] = v
3978                 else:
3979                     result_ndim = len(v.dims) + len(axis)
3980                     for a in axis:
3981                         if a < -result_ndim or result_ndim - 1 < a:
3982                             raise IndexError(
3983                                 f"Axis {a} of variable {k} is out of bounds of the "
3984                                 f"expanded dimension size {result_ndim}"
3985                             )
3986 
3987                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]
3988                     if len(axis_pos) != len(set(axis_pos)):
3989                         raise ValueError("axis should not contain duplicate values")
3990                     # We need to sort them to make sure `axis` equals to the
3991                     # axis positions of the result array.
3992                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))
3993 
3994                     all_dims = list(zip(v.dims, v.shape))
3995                     for d, c in zip_axis_dim:
3996                         all_dims.insert(d, c)
3997                     variables[k] = v.set_dims(dict(all_dims))
3998             else:
3999                 if k not in variables:
4000                     # If dims includes a label of a non-dimension coordinate,
4001                     # it will be promoted to a 1D coordinate with a single value.
4002                     index, index_vars = create_default_index_implicit(v.set_dims(k))
4003                     indexes[k] = index
4004                     variables.update(index_vars)
4005 
4006         return self._replace_with_new_dims(
4007             variables, coord_names=coord_names, indexes=indexes
4008         )
4009 
4010     # change type of self and return to T_Dataset once
4011     # https://github.com/python/mypy/issues/12846 is resolved
4012     def set_index(
4013         self,
4014         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,
4015         append: bool = False,
4016         **indexes_kwargs: Hashable | Sequence[Hashable],
4017     ) -> Dataset:
4018         """Set Dataset (multi-)indexes using one or more existing coordinates
4019         or variables.
4020 
4021         This legacy method is limited to pandas (multi-)indexes and
4022         1-dimensional "dimension" coordinates. See
4023         :py:meth:`~Dataset.set_xindex` for setting a pandas or a custom
4024         Xarray-compatible index from one or more arbitrary coordinates.
4025 
4026         Parameters
4027         ----------
4028         indexes : {dim: index, ...}
4029             Mapping from names matching dimensions and values given
4030             by (lists of) the names of existing coordinates or variables to set
4031             as new (multi-)index.
4032         append : bool, default: False
4033             If True, append the supplied index(es) to the existing index(es).
4034             Otherwise replace the existing index(es) (default).
4035         **indexes_kwargs : optional
4036             The keyword arguments form of ``indexes``.
4037             One of indexes or indexes_kwargs must be provided.
4038 
4039         Returns
4040         -------
4041         obj : Dataset
4042             Another dataset, with this dataset's data but replaced coordinates.
4043 
4044         Examples
4045         --------
4046         >>> arr = xr.DataArray(
4047         ...     data=np.ones((2, 3)),
4048         ...     dims=["x", "y"],
4049         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
4050         ... )
4051         >>> ds = xr.Dataset({"v": arr})
4052         >>> ds
4053         <xarray.Dataset>
4054         Dimensions:  (x: 2, y: 3)
4055         Coordinates:
4056           * x        (x) int64 0 1
4057           * y        (y) int64 0 1 2
4058             a        (x) int64 3 4
4059         Data variables:
4060             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
4061         >>> ds.set_index(x="a")
4062         <xarray.Dataset>
4063         Dimensions:  (x: 2, y: 3)
4064         Coordinates:
4065           * x        (x) int64 3 4
4066           * y        (y) int64 0 1 2
4067         Data variables:
4068             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
4069 
4070         See Also
4071         --------
4072         Dataset.reset_index
4073         Dataset.set_xindex
4074         Dataset.swap_dims
4075         """
4076         dim_coords = either_dict_or_kwargs(indexes, indexes_kwargs, "set_index")
4077 
4078         new_indexes: dict[Hashable, Index] = {}
4079         new_variables: dict[Hashable, Variable] = {}
4080         drop_indexes: set[Hashable] = set()
4081         drop_variables: set[Hashable] = set()
4082         replace_dims: dict[Hashable, Hashable] = {}
4083         all_var_names: set[Hashable] = set()
4084 
4085         for dim, _var_names in dim_coords.items():
4086             if isinstance(_var_names, str) or not isinstance(_var_names, Sequence):
4087                 var_names = [_var_names]
4088             else:
4089                 var_names = list(_var_names)
4090 
4091             invalid_vars = set(var_names) - set(self._variables)
4092             if invalid_vars:
4093                 raise ValueError(
4094                     ", ".join([str(v) for v in invalid_vars])
4095                     + " variable(s) do not exist"
4096                 )
4097 
4098             all_var_names.update(var_names)
4099             drop_variables.update(var_names)
4100 
4101             # drop any pre-existing index involved and its corresponding coordinates
4102             index_coord_names = self.xindexes.get_all_coords(dim, errors="ignore")
4103             all_index_coord_names = set(index_coord_names)
4104             for k in var_names:
4105                 all_index_coord_names.update(
4106                     self.xindexes.get_all_coords(k, errors="ignore")
4107                 )
4108 
4109             drop_indexes.update(all_index_coord_names)
4110             drop_variables.update(all_index_coord_names)
4111 
4112             if len(var_names) == 1 and (not append or dim not in self._indexes):
4113                 var_name = var_names[0]
4114                 var = self._variables[var_name]
4115                 if var.dims != (dim,):
4116                     raise ValueError(
4117                         f"dimension mismatch: try setting an index for dimension {dim!r} with "
4118                         f"variable {var_name!r} that has dimensions {var.dims}"
4119                     )
4120                 idx = PandasIndex.from_variables({dim: var}, options={})
4121                 idx_vars = idx.create_variables({var_name: var})
4122 
4123                 # trick to preserve coordinate order in this case
4124                 if dim in self._coord_names:
4125                     drop_variables.remove(dim)
4126             else:
4127                 if append:
4128                     current_variables = {
4129                         k: self._variables[k] for k in index_coord_names
4130                     }
4131                 else:
4132                     current_variables = {}
4133                 idx, idx_vars = PandasMultiIndex.from_variables_maybe_expand(
4134                     dim,
4135                     current_variables,
4136                     {k: self._variables[k] for k in var_names},
4137                 )
4138                 for n in idx.index.names:
4139                     replace_dims[n] = dim
4140 
4141             new_indexes.update({k: idx for k in idx_vars})
4142             new_variables.update(idx_vars)
4143 
4144         # re-add deindexed coordinates (convert to base variables)
4145         for k in drop_variables:
4146             if (
4147                 k not in new_variables
4148                 and k not in all_var_names
4149                 and k in self._coord_names
4150             ):
4151                 new_variables[k] = self._variables[k].to_base_variable()
4152 
4153         indexes_: dict[Any, Index] = {
4154             k: v for k, v in self._indexes.items() if k not in drop_indexes
4155         }
4156         indexes_.update(new_indexes)
4157 
4158         variables = {
4159             k: v for k, v in self._variables.items() if k not in drop_variables
4160         }
4161         variables.update(new_variables)
4162 
4163         # update dimensions if necessary, GH: 3512
4164         for k, v in variables.items():
4165             if any(d in replace_dims for d in v.dims):
4166                 new_dims = [replace_dims.get(d, d) for d in v.dims]
4167                 variables[k] = v._replace(dims=new_dims)
4168 
4169         coord_names = self._coord_names - drop_variables | set(new_variables)
4170 
4171         return self._replace_with_new_dims(
4172             variables, coord_names=coord_names, indexes=indexes_
4173         )
4174 
4175     def reset_index(
4176         self: T_Dataset,
4177         dims_or_levels: Hashable | Sequence[Hashable],
4178         drop: bool = False,
4179     ) -> T_Dataset:
4180         """Reset the specified index(es) or multi-index level(s).
4181 
4182         This legacy method is specific to pandas (multi-)indexes and
4183         1-dimensional "dimension" coordinates. See the more generic
4184         :py:meth:`~Dataset.drop_indexes` and :py:meth:`~Dataset.set_xindex`
4185         method to respectively drop and set pandas or custom indexes for
4186         arbitrary coordinates.
4187 
4188         Parameters
4189         ----------
4190         dims_or_levels : Hashable or Sequence of Hashable
4191             Name(s) of the dimension(s) and/or multi-index level(s) that will
4192             be reset.
4193         drop : bool, default: False
4194             If True, remove the specified indexes and/or multi-index levels
4195             instead of extracting them as new coordinates (default: False).
4196 
4197         Returns
4198         -------
4199         obj : Dataset
4200             Another dataset, with this dataset's data but replaced coordinates.
4201 
4202         See Also
4203         --------
4204         Dataset.set_index
4205         Dataset.set_xindex
4206         Dataset.drop_indexes
4207         """
4208         if isinstance(dims_or_levels, str) or not isinstance(dims_or_levels, Sequence):
4209             dims_or_levels = [dims_or_levels]
4210 
4211         invalid_coords = set(dims_or_levels) - set(self._indexes)
4212         if invalid_coords:
4213             raise ValueError(
4214                 f"{tuple(invalid_coords)} are not coordinates with an index"
4215             )
4216 
4217         drop_indexes: set[Hashable] = set()
4218         drop_variables: set[Hashable] = set()
4219         seen: set[Index] = set()
4220         new_indexes: dict[Hashable, Index] = {}
4221         new_variables: dict[Hashable, Variable] = {}
4222 
4223         def drop_or_convert(var_names):
4224             if drop:
4225                 drop_variables.update(var_names)
4226             else:
4227                 base_vars = {
4228                     k: self._variables[k].to_base_variable() for k in var_names
4229                 }
4230                 new_variables.update(base_vars)
4231 
4232         for name in dims_or_levels:
4233             index = self._indexes[name]
4234 
4235             if index in seen:
4236                 continue
4237             seen.add(index)
4238 
4239             idx_var_names = set(self.xindexes.get_all_coords(name))
4240             drop_indexes.update(idx_var_names)
4241 
4242             if isinstance(index, PandasMultiIndex):
4243                 # special case for pd.MultiIndex
4244                 level_names = index.index.names
4245                 keep_level_vars = {
4246                     k: self._variables[k]
4247                     for k in level_names
4248                     if k not in dims_or_levels
4249                 }
4250 
4251                 if index.dim not in dims_or_levels and keep_level_vars:
4252                     # do not drop the multi-index completely
4253                     # instead replace it by a new (multi-)index with dropped level(s)
4254                     idx = index.keep_levels(keep_level_vars)
4255                     idx_vars = idx.create_variables(keep_level_vars)
4256                     new_indexes.update({k: idx for k in idx_vars})
4257                     new_variables.update(idx_vars)
4258                     if not isinstance(idx, PandasMultiIndex):
4259                         # multi-index reduced to single index
4260                         # backward compatibility: unique level coordinate renamed to dimension
4261                         drop_variables.update(keep_level_vars)
4262                     drop_or_convert(
4263                         [k for k in level_names if k not in keep_level_vars]
4264                     )
4265                 else:
4266                     # always drop the multi-index dimension variable
4267                     drop_variables.add(index.dim)
4268                     drop_or_convert(level_names)
4269             else:
4270                 drop_or_convert(idx_var_names)
4271 
4272         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4273         indexes.update(new_indexes)
4274 
4275         variables = {
4276             k: v for k, v in self._variables.items() if k not in drop_variables
4277         }
4278         variables.update(new_variables)
4279 
4280         coord_names = self._coord_names - drop_variables
4281 
4282         return self._replace_with_new_dims(
4283             variables, coord_names=coord_names, indexes=indexes
4284         )
4285 
4286     def set_xindex(
4287         self: T_Dataset,
4288         coord_names: str | Sequence[Hashable],
4289         index_cls: type[Index] | None = None,
4290         **options,
4291     ) -> T_Dataset:
4292         """Set a new, Xarray-compatible index from one or more existing
4293         coordinate(s).
4294 
4295         Parameters
4296         ----------
4297         coord_names : str or list
4298             Name(s) of the coordinate(s) used to build the index.
4299             If several names are given, their order matters.
4300         index_cls : subclass of :class:`~xarray.indexes.Index`, optional
4301             The type of index to create. By default, try setting
4302             a ``PandasIndex`` if ``len(coord_names) == 1``,
4303             otherwise a ``PandasMultiIndex``.
4304         **options
4305             Options passed to the index constructor.
4306 
4307         Returns
4308         -------
4309         obj : Dataset
4310             Another dataset, with this dataset's data and with a new index.
4311 
4312         """
4313         # the Sequence check is required for mypy
4314         if is_scalar(coord_names) or not isinstance(coord_names, Sequence):
4315             coord_names = [coord_names]
4316 
4317         if index_cls is None:
4318             if len(coord_names) == 1:
4319                 index_cls = PandasIndex
4320             else:
4321                 index_cls = PandasMultiIndex
4322         else:
4323             if not issubclass(index_cls, Index):
4324                 raise TypeError(f"{index_cls} is not a subclass of xarray.Index")
4325 
4326         invalid_coords = set(coord_names) - self._coord_names
4327 
4328         if invalid_coords:
4329             msg = ["invalid coordinate(s)"]
4330             no_vars = invalid_coords - set(self._variables)
4331             data_vars = invalid_coords - no_vars
4332             if no_vars:
4333                 msg.append(f"those variables don't exist: {no_vars}")
4334             if data_vars:
4335                 msg.append(
4336                     f"those variables are data variables: {data_vars}, use `set_coords` first"
4337                 )
4338             raise ValueError("\n".join(msg))
4339 
4340         # we could be more clever here (e.g., drop-in index replacement if index
4341         # coordinates do not conflict), but let's not allow this for now
4342         indexed_coords = set(coord_names) & set(self._indexes)
4343 
4344         if indexed_coords:
4345             raise ValueError(
4346                 f"those coordinates already have an index: {indexed_coords}"
4347             )
4348 
4349         coord_vars = {name: self._variables[name] for name in coord_names}
4350 
4351         index = index_cls.from_variables(coord_vars, options=options)
4352 
4353         new_coord_vars = index.create_variables(coord_vars)
4354 
4355         # special case for setting a pandas multi-index from level coordinates
4356         # TODO: remove it once we depreciate pandas multi-index dimension (tuple
4357         # elements) coordinate
4358         if isinstance(index, PandasMultiIndex):
4359             coord_names = [index.dim] + list(coord_names)
4360 
4361         variables: dict[Hashable, Variable]
4362         indexes: dict[Hashable, Index]
4363 
4364         if len(coord_names) == 1:
4365             variables = self._variables.copy()
4366             indexes = self._indexes.copy()
4367 
4368             name = list(coord_names).pop()
4369             if name in new_coord_vars:
4370                 variables[name] = new_coord_vars[name]
4371             indexes[name] = index
4372         else:
4373             # reorder variables and indexes so that coordinates having the same
4374             # index are next to each other
4375             variables = {}
4376             for name, var in self._variables.items():
4377                 if name not in coord_names:
4378                     variables[name] = var
4379 
4380             indexes = {}
4381             for name, idx in self._indexes.items():
4382                 if name not in coord_names:
4383                     indexes[name] = idx
4384 
4385             for name in coord_names:
4386                 try:
4387                     variables[name] = new_coord_vars[name]
4388                 except KeyError:
4389                     variables[name] = self._variables[name]
4390                 indexes[name] = index
4391 
4392         return self._replace(
4393             variables=variables,
4394             coord_names=self._coord_names | set(coord_names),
4395             indexes=indexes,
4396         )
4397 
4398     def reorder_levels(
4399         self: T_Dataset,
4400         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
4401         **dim_order_kwargs: Sequence[int | Hashable],
4402     ) -> T_Dataset:
4403         """Rearrange index levels using input order.
4404 
4405         Parameters
4406         ----------
4407         dim_order : dict-like of Hashable to Sequence of int or Hashable, optional
4408             Mapping from names matching dimensions and values given
4409             by lists representing new level orders. Every given dimension
4410             must have a multi-index.
4411         **dim_order_kwargs : Sequence of int or Hashable, optional
4412             The keyword arguments form of ``dim_order``.
4413             One of dim_order or dim_order_kwargs must be provided.
4414 
4415         Returns
4416         -------
4417         obj : Dataset
4418             Another dataset, with this dataset's data but replaced
4419             coordinates.
4420         """
4421         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
4422         variables = self._variables.copy()
4423         indexes = dict(self._indexes)
4424         new_indexes: dict[Hashable, Index] = {}
4425         new_variables: dict[Hashable, IndexVariable] = {}
4426 
4427         for dim, order in dim_order.items():
4428             index = self._indexes[dim]
4429 
4430             if not isinstance(index, PandasMultiIndex):
4431                 raise ValueError(f"coordinate {dim} has no MultiIndex")
4432 
4433             level_vars = {k: self._variables[k] for k in order}
4434             idx = index.reorder_levels(level_vars)
4435             idx_vars = idx.create_variables(level_vars)
4436             new_indexes.update({k: idx for k in idx_vars})
4437             new_variables.update(idx_vars)
4438 
4439         indexes = {k: v for k, v in self._indexes.items() if k not in new_indexes}
4440         indexes.update(new_indexes)
4441 
4442         variables = {k: v for k, v in self._variables.items() if k not in new_variables}
4443         variables.update(new_variables)
4444 
4445         return self._replace(variables, indexes=indexes)
4446 
4447     def _get_stack_index(
4448         self,
4449         dim,
4450         multi=False,
4451         create_index=False,
4452     ) -> tuple[Index | None, dict[Hashable, Variable]]:
4453         """Used by stack and unstack to get one pandas (multi-)index among
4454         the indexed coordinates along dimension `dim`.
4455 
4456         If exactly one index is found, return it with its corresponding
4457         coordinate variables(s), otherwise return None and an empty dict.
4458 
4459         If `create_index=True`, create a new index if none is found or raise
4460         an error if multiple indexes are found.
4461 
4462         """
4463         stack_index: Index | None = None
4464         stack_coords: dict[Hashable, Variable] = {}
4465 
4466         for name, index in self._indexes.items():
4467             var = self._variables[name]
4468             if (
4469                 var.ndim == 1
4470                 and var.dims[0] == dim
4471                 and (
4472                     # stack: must be a single coordinate index
4473                     not multi
4474                     and not self.xindexes.is_multi(name)
4475                     # unstack: must be an index that implements .unstack
4476                     or multi
4477                     and type(index).unstack is not Index.unstack
4478                 )
4479             ):
4480                 if stack_index is not None and index is not stack_index:
4481                     # more than one index found, stop
4482                     if create_index:
4483                         raise ValueError(
4484                             f"cannot stack dimension {dim!r} with `create_index=True` "
4485                             "and with more than one index found along that dimension"
4486                         )
4487                     return None, {}
4488                 stack_index = index
4489                 stack_coords[name] = var
4490 
4491         if create_index and stack_index is None:
4492             if dim in self._variables:
4493                 var = self._variables[dim]
4494             else:
4495                 _, _, var = _get_virtual_variable(self._variables, dim, self.dims)
4496             # dummy index (only `stack_coords` will be used to construct the multi-index)
4497             stack_index = PandasIndex([0], dim)
4498             stack_coords = {dim: var}
4499 
4500         return stack_index, stack_coords
4501 
4502     def _stack_once(
4503         self: T_Dataset,
4504         dims: Sequence[Hashable | ellipsis],
4505         new_dim: Hashable,
4506         index_cls: type[Index],
4507         create_index: bool | None = True,
4508     ) -> T_Dataset:
4509         if dims == ...:
4510             raise ValueError("Please use [...] for dims, rather than just ...")
4511         if ... in dims:
4512             dims = list(infix_dims(dims, self.dims))
4513 
4514         new_variables: dict[Hashable, Variable] = {}
4515         stacked_var_names: list[Hashable] = []
4516         drop_indexes: list[Hashable] = []
4517 
4518         for name, var in self.variables.items():
4519             if any(d in var.dims for d in dims):
4520                 add_dims = [d for d in dims if d not in var.dims]
4521                 vdims = list(var.dims) + add_dims
4522                 shape = [self.dims[d] for d in vdims]
4523                 exp_var = var.set_dims(vdims, shape)
4524                 stacked_var = exp_var.stack(**{new_dim: dims})
4525                 new_variables[name] = stacked_var
4526                 stacked_var_names.append(name)
4527             else:
4528                 new_variables[name] = var.copy(deep=False)
4529 
4530         # drop indexes of stacked coordinates (if any)
4531         for name in stacked_var_names:
4532             drop_indexes += list(self.xindexes.get_all_coords(name, errors="ignore"))
4533 
4534         new_indexes = {}
4535         new_coord_names = set(self._coord_names)
4536         if create_index or create_index is None:
4537             product_vars: dict[Any, Variable] = {}
4538             for dim in dims:
4539                 idx, idx_vars = self._get_stack_index(dim, create_index=create_index)
4540                 if idx is not None:
4541                     product_vars.update(idx_vars)
4542 
4543             if len(product_vars) == len(dims):
4544                 idx = index_cls.stack(product_vars, new_dim)
4545                 new_indexes[new_dim] = idx
4546                 new_indexes.update({k: idx for k in product_vars})
4547                 idx_vars = idx.create_variables(product_vars)
4548                 # keep consistent multi-index coordinate order
4549                 for k in idx_vars:
4550                     new_variables.pop(k, None)
4551                 new_variables.update(idx_vars)
4552                 new_coord_names.update(idx_vars)
4553 
4554         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4555         indexes.update(new_indexes)
4556 
4557         return self._replace_with_new_dims(
4558             new_variables, coord_names=new_coord_names, indexes=indexes
4559         )
4560 
4561     def stack(
4562         self: T_Dataset,
4563         dimensions: Mapping[Any, Sequence[Hashable | ellipsis]] | None = None,
4564         create_index: bool | None = True,
4565         index_cls: type[Index] = PandasMultiIndex,
4566         **dimensions_kwargs: Sequence[Hashable | ellipsis],
4567     ) -> T_Dataset:
4568         """
4569         Stack any number of existing dimensions into a single new dimension.
4570 
4571         New dimensions will be added at the end, and by default the corresponding
4572         coordinate variables will be combined into a MultiIndex.
4573 
4574         Parameters
4575         ----------
4576         dimensions : mapping of hashable to sequence of hashable
4577             Mapping of the form `new_name=(dim1, dim2, ...)`. Names of new
4578             dimensions, and the existing dimensions that they replace. An
4579             ellipsis (`...`) will be replaced by all unlisted dimensions.
4580             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
4581             all dimensions.
4582         create_index : bool or None, default: True
4583 
4584             - True: create a multi-index for each of the stacked dimensions.
4585             - False: don't create any index.
4586             - None. create a multi-index only if exactly one single (1-d) coordinate
4587               index is found for every dimension to stack.
4588 
4589         index_cls: Index-class, default: PandasMultiIndex
4590             Can be used to pass a custom multi-index type (must be an Xarray index that
4591             implements `.stack()`). By default, a pandas multi-index wrapper is used.
4592         **dimensions_kwargs
4593             The keyword arguments form of ``dimensions``.
4594             One of dimensions or dimensions_kwargs must be provided.
4595 
4596         Returns
4597         -------
4598         stacked : Dataset
4599             Dataset with stacked data.
4600 
4601         See Also
4602         --------
4603         Dataset.unstack
4604         """
4605         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
4606         result = self
4607         for new_dim, dims in dimensions.items():
4608             result = result._stack_once(dims, new_dim, index_cls, create_index)
4609         return result
4610 
4611     def to_stacked_array(
4612         self,
4613         new_dim: Hashable,
4614         sample_dims: Collection[Hashable],
4615         variable_dim: Hashable = "variable",
4616         name: Hashable | None = None,
4617     ) -> DataArray:
4618         """Combine variables of differing dimensionality into a DataArray
4619         without broadcasting.
4620 
4621         This method is similar to Dataset.to_array but does not broadcast the
4622         variables.
4623 
4624         Parameters
4625         ----------
4626         new_dim : hashable
4627             Name of the new stacked coordinate
4628         sample_dims : Collection of hashables
4629             List of dimensions that **will not** be stacked. Each array in the
4630             dataset must share these dimensions. For machine learning
4631             applications, these define the dimensions over which samples are
4632             drawn.
4633         variable_dim : hashable, default: "variable"
4634             Name of the level in the stacked coordinate which corresponds to
4635             the variables.
4636         name : hashable, optional
4637             Name of the new data array.
4638 
4639         Returns
4640         -------
4641         stacked : DataArray
4642             DataArray with the specified dimensions and data variables
4643             stacked together. The stacked coordinate is named ``new_dim``
4644             and represented by a MultiIndex object with a level containing the
4645             data variable names. The name of this level is controlled using
4646             the ``variable_dim`` argument.
4647 
4648         See Also
4649         --------
4650         Dataset.to_array
4651         Dataset.stack
4652         DataArray.to_unstacked_dataset
4653 
4654         Examples
4655         --------
4656         >>> data = xr.Dataset(
4657         ...     data_vars={
4658         ...         "a": (("x", "y"), [[0, 1, 2], [3, 4, 5]]),
4659         ...         "b": ("x", [6, 7]),
4660         ...     },
4661         ...     coords={"y": ["u", "v", "w"]},
4662         ... )
4663 
4664         >>> data
4665         <xarray.Dataset>
4666         Dimensions:  (x: 2, y: 3)
4667         Coordinates:
4668           * y        (y) <U1 'u' 'v' 'w'
4669         Dimensions without coordinates: x
4670         Data variables:
4671             a        (x, y) int64 0 1 2 3 4 5
4672             b        (x) int64 6 7
4673 
4674         >>> data.to_stacked_array("z", sample_dims=["x"])
4675         <xarray.DataArray 'a' (x: 2, z: 4)>
4676         array([[0, 1, 2, 6],
4677                [3, 4, 5, 7]])
4678         Coordinates:
4679           * z         (z) object MultiIndex
4680           * variable  (z) object 'a' 'a' 'a' 'b'
4681           * y         (z) object 'u' 'v' 'w' nan
4682         Dimensions without coordinates: x
4683 
4684         """
4685         from xarray.core.concat import concat
4686 
4687         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)
4688 
4689         for variable in self:
4690             dims = self[variable].dims
4691             dims_include_sample_dims = set(sample_dims) <= set(dims)
4692             if not dims_include_sample_dims:
4693                 raise ValueError(
4694                     "All variables in the dataset must contain the "
4695                     "dimensions {}.".format(dims)
4696                 )
4697 
4698         def ensure_stackable(val):
4699             assign_coords = {variable_dim: val.name}
4700             for dim in stacking_dims:
4701                 if dim not in val.dims:
4702                     assign_coords[dim] = None
4703 
4704             expand_dims = set(stacking_dims).difference(set(val.dims))
4705             expand_dims.add(variable_dim)
4706             # must be list for .expand_dims
4707             expand_dims = list(expand_dims)
4708 
4709             return (
4710                 val.assign_coords(**assign_coords)
4711                 .expand_dims(expand_dims)
4712                 .stack({new_dim: (variable_dim,) + stacking_dims})
4713             )
4714 
4715         # concatenate the arrays
4716         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]
4717         data_array = concat(stackable_vars, dim=new_dim)
4718 
4719         if name is not None:
4720             data_array.name = name
4721 
4722         return data_array
4723 
4724     def _unstack_once(
4725         self: T_Dataset,
4726         dim: Hashable,
4727         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4728         fill_value,
4729         sparse: bool = False,
4730     ) -> T_Dataset:
4731         index, index_vars = index_and_vars
4732         variables: dict[Hashable, Variable] = {}
4733         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4734 
4735         new_indexes, clean_index = index.unstack()
4736         indexes.update(new_indexes)
4737 
4738         for name, idx in new_indexes.items():
4739             variables.update(idx.create_variables(index_vars))
4740 
4741         for name, var in self.variables.items():
4742             if name not in index_vars:
4743                 if dim in var.dims:
4744                     if isinstance(fill_value, Mapping):
4745                         fill_value_ = fill_value[name]
4746                     else:
4747                         fill_value_ = fill_value
4748 
4749                     variables[name] = var._unstack_once(
4750                         index=clean_index,
4751                         dim=dim,
4752                         fill_value=fill_value_,
4753                         sparse=sparse,
4754                     )
4755                 else:
4756                     variables[name] = var
4757 
4758         coord_names = set(self._coord_names) - {dim} | set(new_indexes)
4759 
4760         return self._replace_with_new_dims(
4761             variables, coord_names=coord_names, indexes=indexes
4762         )
4763 
4764     def _unstack_full_reindex(
4765         self: T_Dataset,
4766         dim: Hashable,
4767         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4768         fill_value,
4769         sparse: bool,
4770     ) -> T_Dataset:
4771         index, index_vars = index_and_vars
4772         variables: dict[Hashable, Variable] = {}
4773         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4774 
4775         new_indexes, clean_index = index.unstack()
4776         indexes.update(new_indexes)
4777 
4778         new_index_variables = {}
4779         for name, idx in new_indexes.items():
4780             new_index_variables.update(idx.create_variables(index_vars))
4781 
4782         new_dim_sizes = {k: v.size for k, v in new_index_variables.items()}
4783         variables.update(new_index_variables)
4784 
4785         # take a shortcut in case the MultiIndex was not modified.
4786         full_idx = pd.MultiIndex.from_product(
4787             clean_index.levels, names=clean_index.names
4788         )
4789         if clean_index.equals(full_idx):
4790             obj = self
4791         else:
4792             # TODO: we may depreciate implicit re-indexing with a pandas.MultiIndex
4793             xr_full_idx = PandasMultiIndex(full_idx, dim)
4794             indexers = Indexes(
4795                 {k: xr_full_idx for k in index_vars},
4796                 xr_full_idx.create_variables(index_vars),
4797             )
4798             obj = self._reindex(
4799                 indexers, copy=False, fill_value=fill_value, sparse=sparse
4800             )
4801 
4802         for name, var in obj.variables.items():
4803             if name not in index_vars:
4804                 if dim in var.dims:
4805                     variables[name] = var.unstack({dim: new_dim_sizes})
4806                 else:
4807                     variables[name] = var
4808 
4809         coord_names = set(self._coord_names) - {dim} | set(new_dim_sizes)
4810 
4811         return self._replace_with_new_dims(
4812             variables, coord_names=coord_names, indexes=indexes
4813         )
4814 
4815     def unstack(
4816         self: T_Dataset,
4817         dim: Dims = None,
4818         fill_value: Any = xrdtypes.NA,
4819         sparse: bool = False,
4820     ) -> T_Dataset:
4821         """
4822         Unstack existing dimensions corresponding to MultiIndexes into
4823         multiple new dimensions.
4824 
4825         New dimensions will be added at the end.
4826 
4827         Parameters
4828         ----------
4829         dim : str, Iterable of Hashable or None, optional
4830             Dimension(s) over which to unstack. By default unstacks all
4831             MultiIndexes.
4832         fill_value : scalar or dict-like, default: nan
4833             value to be filled. If a dict-like, maps variable names to
4834             fill values. If not provided or if the dict-like does not
4835             contain all variables, the dtype's NA value will be used.
4836         sparse : bool, default: False
4837             use sparse-array if True
4838 
4839         Returns
4840         -------
4841         unstacked : Dataset
4842             Dataset with unstacked data.
4843 
4844         See Also
4845         --------
4846         Dataset.stack
4847         """
4848 
4849         if dim is None:
4850             dims = list(self.dims)
4851         else:
4852             if isinstance(dim, str) or not isinstance(dim, Iterable):
4853                 dims = [dim]
4854             else:
4855                 dims = list(dim)
4856 
4857             missing_dims = [d for d in dims if d not in self.dims]
4858             if missing_dims:
4859                 raise ValueError(
4860                     f"Dataset does not contain the dimensions: {missing_dims}"
4861                 )
4862 
4863         # each specified dimension must have exactly one multi-index
4864         stacked_indexes: dict[Any, tuple[Index, dict[Hashable, Variable]]] = {}
4865         for d in dims:
4866             idx, idx_vars = self._get_stack_index(d, multi=True)
4867             if idx is not None:
4868                 stacked_indexes[d] = idx, idx_vars
4869 
4870         if dim is None:
4871             dims = list(stacked_indexes)
4872         else:
4873             non_multi_dims = set(dims) - set(stacked_indexes)
4874             if non_multi_dims:
4875                 raise ValueError(
4876                     "cannot unstack dimensions that do not "
4877                     f"have exactly one multi-index: {tuple(non_multi_dims)}"
4878                 )
4879 
4880         result = self.copy(deep=False)
4881 
4882         # we want to avoid allocating an object-dtype ndarray for a MultiIndex,
4883         # so we can't just access self.variables[v].data for every variable.
4884         # We only check the non-index variables.
4885         # https://github.com/pydata/xarray/issues/5902
4886         nonindexes = [
4887             self.variables[k] for k in set(self.variables) - set(self._indexes)
4888         ]
4889         # Notes for each of these cases:
4890         # 1. Dask arrays don't support assignment by index, which the fast unstack
4891         #    function requires.
4892         #    https://github.com/pydata/xarray/pull/4746#issuecomment-753282125
4893         # 2. Sparse doesn't currently support (though we could special-case it)
4894         #    https://github.com/pydata/sparse/issues/422
4895         # 3. pint requires checking if it's a NumPy array until
4896         #    https://github.com/pydata/xarray/pull/4751 is resolved,
4897         #    Once that is resolved, explicitly exclude pint arrays.
4898         #    pint doesn't implement `np.full_like` in a way that's
4899         #    currently compatible.
4900         sparse_array_type = array_type("sparse")
4901         needs_full_reindex = any(
4902             is_duck_dask_array(v.data)
4903             or isinstance(v.data, sparse_array_type)
4904             or not isinstance(v.data, np.ndarray)
4905             for v in nonindexes
4906         )
4907 
4908         for d in dims:
4909             if needs_full_reindex:
4910                 result = result._unstack_full_reindex(
4911                     d, stacked_indexes[d], fill_value, sparse
4912                 )
4913             else:
4914                 result = result._unstack_once(d, stacked_indexes[d], fill_value, sparse)
4915         return result
4916 
4917     def update(self: T_Dataset, other: CoercibleMapping) -> T_Dataset:
4918         """Update this dataset's variables with those from another dataset.
4919 
4920         Just like :py:meth:`dict.update` this is a in-place operation.
4921         For a non-inplace version, see :py:meth:`Dataset.merge`.
4922 
4923         Parameters
4924         ----------
4925         other : Dataset or mapping
4926             Variables with which to update this dataset. One of:
4927 
4928             - Dataset
4929             - mapping {var name: DataArray}
4930             - mapping {var name: Variable}
4931             - mapping {var name: (dimension name, array-like)}
4932             - mapping {var name: (tuple of dimension names, array-like)}
4933 
4934         Returns
4935         -------
4936         updated : Dataset
4937             Updated dataset. Note that since the update is in-place this is the input
4938             dataset.
4939 
4940             It is deprecated since version 0.17 and scheduled to be removed in 0.21.
4941 
4942         Raises
4943         ------
4944         ValueError
4945             If any dimensions would have inconsistent sizes in the updated
4946             dataset.
4947 
4948         See Also
4949         --------
4950         Dataset.assign
4951         Dataset.merge
4952         """
4953         merge_result = dataset_update_method(self, other)
4954         return self._replace(inplace=True, **merge_result._asdict())
4955 
4956     def merge(
4957         self: T_Dataset,
4958         other: CoercibleMapping | DataArray,
4959         overwrite_vars: Hashable | Iterable[Hashable] = frozenset(),
4960         compat: CompatOptions = "no_conflicts",
4961         join: JoinOptions = "outer",
4962         fill_value: Any = xrdtypes.NA,
4963         combine_attrs: CombineAttrsOptions = "override",
4964     ) -> T_Dataset:
4965         """Merge the arrays of two datasets into a single dataset.
4966 
4967         This method generally does not allow for overriding data, with the
4968         exception of attributes, which are ignored on the second dataset.
4969         Variables with the same name are checked for conflicts via the equals
4970         or identical methods.
4971 
4972         Parameters
4973         ----------
4974         other : Dataset or mapping
4975             Dataset or variables to merge with this dataset.
4976         overwrite_vars : hashable or iterable of hashable, optional
4977             If provided, update variables of these name(s) without checking for
4978             conflicts in this dataset.
4979         compat : {"identical", "equals", "broadcast_equals", \
4980                   "no_conflicts", "override", "minimal"}, default: "no_conflicts"
4981             String indicating how to compare variables of the same name for
4982             potential conflicts:
4983 
4984             - 'identical': all values, dimensions and attributes must be the
4985               same.
4986             - 'equals': all values and dimensions must be the same.
4987             - 'broadcast_equals': all values must be equal when variables are
4988               broadcast against each other to ensure common dimensions.
4989             - 'no_conflicts': only values which are not null in both datasets
4990               must be equal. The returned dataset then contains the combination
4991               of all non-null values.
4992             - 'override': skip comparing and pick variable from first dataset
4993             - 'minimal': drop conflicting coordinates
4994 
4995         join : {"outer", "inner", "left", "right", "exact", "override"}, \
4996                default: "outer"
4997             Method for joining ``self`` and ``other`` along shared dimensions:
4998 
4999             - 'outer': use the union of the indexes
5000             - 'inner': use the intersection of the indexes
5001             - 'left': use indexes from ``self``
5002             - 'right': use indexes from ``other``
5003             - 'exact': error instead of aligning non-equal indexes
5004             - 'override': use indexes from ``self`` that are the same size
5005               as those of ``other`` in that dimension
5006 
5007         fill_value : scalar or dict-like, optional
5008             Value to use for newly missing values. If a dict-like, maps
5009             variable names (including coordinates) to fill values.
5010         combine_attrs : {"drop", "identical", "no_conflicts", "drop_conflicts", \
5011                          "override"} or callable, default: "override"
5012             A callable or a string indicating how to combine attrs of the objects being
5013             merged:
5014 
5015             - "drop": empty attrs on returned Dataset.
5016             - "identical": all attrs must be the same on every object.
5017             - "no_conflicts": attrs from all objects are combined, any that have
5018               the same name must also have the same value.
5019             - "drop_conflicts": attrs from all objects are combined, any that have
5020               the same name but different values are dropped.
5021             - "override": skip comparing and copy attrs from the first dataset to
5022               the result.
5023 
5024             If a callable, it must expect a sequence of ``attrs`` dicts and a context object
5025             as its only parameters.
5026 
5027         Returns
5028         -------
5029         merged : Dataset
5030             Merged dataset.
5031 
5032         Raises
5033         ------
5034         MergeError
5035             If any variables conflict (see ``compat``).
5036 
5037         See Also
5038         --------
5039         Dataset.update
5040         """
5041         from xarray.core.dataarray import DataArray
5042 
5043         other = other.to_dataset() if isinstance(other, DataArray) else other
5044         merge_result = dataset_merge_method(
5045             self,
5046             other,
5047             overwrite_vars=overwrite_vars,
5048             compat=compat,
5049             join=join,
5050             fill_value=fill_value,
5051             combine_attrs=combine_attrs,
5052         )
5053         return self._replace(**merge_result._asdict())
5054 
5055     def _assert_all_in_dataset(
5056         self, names: Iterable[Hashable], virtual_okay: bool = False
5057     ) -> None:
5058         bad_names = set(names) - set(self._variables)
5059         if virtual_okay:
5060             bad_names -= self.virtual_variables
5061         if bad_names:
5062             ordered_bad_names = [name for name in names if name in bad_names]
5063             raise ValueError(
5064                 f"These variables cannot be found in this dataset: {ordered_bad_names}"
5065             )
5066 
5067     def drop_vars(
5068         self: T_Dataset,
5069         names: Hashable | Iterable[Hashable],
5070         *,
5071         errors: ErrorOptions = "raise",
5072     ) -> T_Dataset:
5073         """Drop variables from this dataset.
5074 
5075         Parameters
5076         ----------
5077         names : hashable or iterable of hashable
5078             Name(s) of variables to drop.
5079         errors : {"raise", "ignore"}, default: "raise"
5080             If 'raise', raises a ValueError error if any of the variable
5081             passed are not in the dataset. If 'ignore', any given names that are in the
5082             dataset are dropped and no error is raised.
5083 
5084         Returns
5085         -------
5086         dropped : Dataset
5087 
5088         """
5089         # the Iterable check is required for mypy
5090         if is_scalar(names) or not isinstance(names, Iterable):
5091             names = {names}
5092         else:
5093             names = set(names)
5094         if errors == "raise":
5095             self._assert_all_in_dataset(names)
5096 
5097         # GH6505
5098         other_names = set()
5099         for var in names:
5100             maybe_midx = self._indexes.get(var, None)
5101             if isinstance(maybe_midx, PandasMultiIndex):
5102                 idx_coord_names = set(maybe_midx.index.names + [maybe_midx.dim])
5103                 idx_other_names = idx_coord_names - set(names)
5104                 other_names.update(idx_other_names)
5105         if other_names:
5106             names |= set(other_names)
5107             warnings.warn(
5108                 f"Deleting a single level of a MultiIndex is deprecated. Previously, this deleted all levels of a MultiIndex. "
5109                 f"Please also drop the following variables: {other_names!r} to avoid an error in the future.",
5110                 DeprecationWarning,
5111                 stacklevel=2,
5112             )
5113 
5114         assert_no_index_corrupted(self.xindexes, names)
5115 
5116         variables = {k: v for k, v in self._variables.items() if k not in names}
5117         coord_names = {k for k in self._coord_names if k in variables}
5118         indexes = {k: v for k, v in self._indexes.items() if k not in names}
5119         return self._replace_with_new_dims(
5120             variables, coord_names=coord_names, indexes=indexes
5121         )
5122 
5123     def drop_indexes(
5124         self: T_Dataset,
5125         coord_names: Hashable | Iterable[Hashable],
5126         *,
5127         errors: ErrorOptions = "raise",
5128     ) -> T_Dataset:
5129         """Drop the indexes assigned to the given coordinates.
5130 
5131         Parameters
5132         ----------
5133         coord_names : hashable or iterable of hashable
5134             Name(s) of the coordinate(s) for which to drop the index.
5135         errors : {"raise", "ignore"}, default: "raise"
5136             If 'raise', raises a ValueError error if any of the coordinates
5137             passed have no index or are not in the dataset.
5138             If 'ignore', no error is raised.
5139 
5140         Returns
5141         -------
5142         dropped : Dataset
5143             A new dataset with dropped indexes.
5144 
5145         """
5146         # the Iterable check is required for mypy
5147         if is_scalar(coord_names) or not isinstance(coord_names, Iterable):
5148             coord_names = {coord_names}
5149         else:
5150             coord_names = set(coord_names)
5151 
5152         if errors == "raise":
5153             invalid_coords = coord_names - self._coord_names
5154             if invalid_coords:
5155                 raise ValueError(f"those coordinates don't exist: {invalid_coords}")
5156 
5157             unindexed_coords = set(coord_names) - set(self._indexes)
5158             if unindexed_coords:
5159                 raise ValueError(
5160                     f"those coordinates do not have an index: {unindexed_coords}"
5161                 )
5162 
5163         assert_no_index_corrupted(self.xindexes, coord_names, action="remove index(es)")
5164 
5165         variables = {}
5166         for name, var in self._variables.items():
5167             if name in coord_names:
5168                 variables[name] = var.to_base_variable()
5169             else:
5170                 variables[name] = var
5171 
5172         indexes = {k: v for k, v in self._indexes.items() if k not in coord_names}
5173 
5174         return self._replace(variables=variables, indexes=indexes)
5175 
5176     def drop(
5177         self: T_Dataset,
5178         labels=None,
5179         dim=None,
5180         *,
5181         errors: ErrorOptions = "raise",
5182         **labels_kwargs,
5183     ) -> T_Dataset:
5184         """Backward compatible method based on `drop_vars` and `drop_sel`
5185 
5186         Using either `drop_vars` or `drop_sel` is encouraged
5187 
5188         See Also
5189         --------
5190         Dataset.drop_vars
5191         Dataset.drop_sel
5192         """
5193         if errors not in ["raise", "ignore"]:
5194             raise ValueError('errors must be either "raise" or "ignore"')
5195 
5196         if is_dict_like(labels) and not isinstance(labels, dict):
5197             warnings.warn(
5198                 "dropping coordinates using `drop` is be deprecated; use drop_vars.",
5199                 FutureWarning,
5200                 stacklevel=2,
5201             )
5202             return self.drop_vars(labels, errors=errors)
5203 
5204         if labels_kwargs or isinstance(labels, dict):
5205             if dim is not None:
5206                 raise ValueError("cannot specify dim and dict-like arguments.")
5207             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
5208 
5209         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):
5210             warnings.warn(
5211                 "dropping variables using `drop` will be deprecated; using drop_vars is encouraged.",
5212                 PendingDeprecationWarning,
5213                 stacklevel=2,
5214             )
5215             return self.drop_vars(labels, errors=errors)
5216         if dim is not None:
5217             warnings.warn(
5218                 "dropping labels using list-like labels is deprecated; using "
5219                 "dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).",
5220                 DeprecationWarning,
5221                 stacklevel=2,
5222             )
5223             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)
5224 
5225         warnings.warn(
5226             "dropping labels using `drop` will be deprecated; using drop_sel is encouraged.",
5227             PendingDeprecationWarning,
5228             stacklevel=2,
5229         )
5230         return self.drop_sel(labels, errors=errors)
5231 
5232     def drop_sel(
5233         self: T_Dataset, labels=None, *, errors: ErrorOptions = "raise", **labels_kwargs
5234     ) -> T_Dataset:
5235         """Drop index labels from this dataset.
5236 
5237         Parameters
5238         ----------
5239         labels : mapping of hashable to Any
5240             Index labels to drop
5241         errors : {"raise", "ignore"}, default: "raise"
5242             If 'raise', raises a ValueError error if
5243             any of the index labels passed are not
5244             in the dataset. If 'ignore', any given labels that are in the
5245             dataset are dropped and no error is raised.
5246         **labels_kwargs : {dim: label, ...}, optional
5247             The keyword arguments form of ``dim`` and ``labels``
5248 
5249         Returns
5250         -------
5251         dropped : Dataset
5252 
5253         Examples
5254         --------
5255         >>> data = np.arange(6).reshape(2, 3)
5256         >>> labels = ["a", "b", "c"]
5257         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
5258         >>> ds
5259         <xarray.Dataset>
5260         Dimensions:  (x: 2, y: 3)
5261         Coordinates:
5262           * y        (y) <U1 'a' 'b' 'c'
5263         Dimensions without coordinates: x
5264         Data variables:
5265             A        (x, y) int64 0 1 2 3 4 5
5266         >>> ds.drop_sel(y=["a", "c"])
5267         <xarray.Dataset>
5268         Dimensions:  (x: 2, y: 1)
5269         Coordinates:
5270           * y        (y) <U1 'b'
5271         Dimensions without coordinates: x
5272         Data variables:
5273             A        (x, y) int64 1 4
5274         >>> ds.drop_sel(y="b")
5275         <xarray.Dataset>
5276         Dimensions:  (x: 2, y: 2)
5277         Coordinates:
5278           * y        (y) <U1 'a' 'c'
5279         Dimensions without coordinates: x
5280         Data variables:
5281             A        (x, y) int64 0 2 3 5
5282         """
5283         if errors not in ["raise", "ignore"]:
5284             raise ValueError('errors must be either "raise" or "ignore"')
5285 
5286         labels = either_dict_or_kwargs(labels, labels_kwargs, "drop_sel")
5287 
5288         ds = self
5289         for dim, labels_for_dim in labels.items():
5290             # Don't cast to set, as it would harm performance when labels
5291             # is a large numpy array
5292             if utils.is_scalar(labels_for_dim):
5293                 labels_for_dim = [labels_for_dim]
5294             labels_for_dim = np.asarray(labels_for_dim)
5295             try:
5296                 index = self.get_index(dim)
5297             except KeyError:
5298                 raise ValueError(f"dimension {dim!r} does not have coordinate labels")
5299             new_index = index.drop(labels_for_dim, errors=errors)
5300             ds = ds.loc[{dim: new_index}]
5301         return ds
5302 
5303     def drop_isel(self: T_Dataset, indexers=None, **indexers_kwargs) -> T_Dataset:
5304         """Drop index positions from this Dataset.
5305 
5306         Parameters
5307         ----------
5308         indexers : mapping of hashable to Any
5309             Index locations to drop
5310         **indexers_kwargs : {dim: position, ...}, optional
5311             The keyword arguments form of ``dim`` and ``positions``
5312 
5313         Returns
5314         -------
5315         dropped : Dataset
5316 
5317         Raises
5318         ------
5319         IndexError
5320 
5321         Examples
5322         --------
5323         >>> data = np.arange(6).reshape(2, 3)
5324         >>> labels = ["a", "b", "c"]
5325         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
5326         >>> ds
5327         <xarray.Dataset>
5328         Dimensions:  (x: 2, y: 3)
5329         Coordinates:
5330           * y        (y) <U1 'a' 'b' 'c'
5331         Dimensions without coordinates: x
5332         Data variables:
5333             A        (x, y) int64 0 1 2 3 4 5
5334         >>> ds.drop_isel(y=[0, 2])
5335         <xarray.Dataset>
5336         Dimensions:  (x: 2, y: 1)
5337         Coordinates:
5338           * y        (y) <U1 'b'
5339         Dimensions without coordinates: x
5340         Data variables:
5341             A        (x, y) int64 1 4
5342         >>> ds.drop_isel(y=1)
5343         <xarray.Dataset>
5344         Dimensions:  (x: 2, y: 2)
5345         Coordinates:
5346           * y        (y) <U1 'a' 'c'
5347         Dimensions without coordinates: x
5348         Data variables:
5349             A        (x, y) int64 0 2 3 5
5350         """
5351 
5352         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "drop_isel")
5353 
5354         ds = self
5355         dimension_index = {}
5356         for dim, pos_for_dim in indexers.items():
5357             # Don't cast to set, as it would harm performance when labels
5358             # is a large numpy array
5359             if utils.is_scalar(pos_for_dim):
5360                 pos_for_dim = [pos_for_dim]
5361             pos_for_dim = np.asarray(pos_for_dim)
5362             index = self.get_index(dim)
5363             new_index = index.delete(pos_for_dim)
5364             dimension_index[dim] = new_index
5365         ds = ds.loc[dimension_index]
5366         return ds
5367 
5368     def drop_dims(
5369         self: T_Dataset,
5370         drop_dims: str | Iterable[Hashable],
5371         *,
5372         errors: ErrorOptions = "raise",
5373     ) -> T_Dataset:
5374         """Drop dimensions and associated variables from this dataset.
5375 
5376         Parameters
5377         ----------
5378         drop_dims : str or Iterable of Hashable
5379             Dimension or dimensions to drop.
5380         errors : {"raise", "ignore"}, default: "raise"
5381             If 'raise', raises a ValueError error if any of the
5382             dimensions passed are not in the dataset. If 'ignore', any given
5383             dimensions that are in the dataset are dropped and no error is raised.
5384 
5385         Returns
5386         -------
5387         obj : Dataset
5388             The dataset without the given dimensions (or any variables
5389             containing those dimensions).
5390         """
5391         if errors not in ["raise", "ignore"]:
5392             raise ValueError('errors must be either "raise" or "ignore"')
5393 
5394         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):
5395             drop_dims = {drop_dims}
5396         else:
5397             drop_dims = set(drop_dims)
5398 
5399         if errors == "raise":
5400             missing_dims = drop_dims - set(self.dims)
5401             if missing_dims:
5402                 raise ValueError(
5403                     f"Dataset does not contain the dimensions: {missing_dims}"
5404                 )
5405 
5406         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}
5407         return self.drop_vars(drop_vars)
5408 
5409     def transpose(
5410         self: T_Dataset,
5411         *dims: Hashable,
5412         missing_dims: ErrorOptionsWithWarn = "raise",
5413     ) -> T_Dataset:
5414         """Return a new Dataset object with all array dimensions transposed.
5415 
5416         Although the order of dimensions on each array will change, the dataset
5417         dimensions themselves will remain in fixed (sorted) order.
5418 
5419         Parameters
5420         ----------
5421         *dims : hashable, optional
5422             By default, reverse the dimensions on each array. Otherwise,
5423             reorder the dimensions to this order.
5424         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
5425             What to do if dimensions that should be selected from are not present in the
5426             Dataset:
5427             - "raise": raise an exception
5428             - "warn": raise a warning, and ignore the missing dimensions
5429             - "ignore": ignore the missing dimensions
5430 
5431         Returns
5432         -------
5433         transposed : Dataset
5434             Each array in the dataset (including) coordinates will be
5435             transposed to the given order.
5436 
5437         Notes
5438         -----
5439         This operation returns a view of each array's data. It is
5440         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
5441         -- the data will be fully loaded into memory.
5442 
5443         See Also
5444         --------
5445         numpy.transpose
5446         DataArray.transpose
5447         """
5448         # Raise error if list is passed as dims
5449         if (len(dims) > 0) and (isinstance(dims[0], list)):
5450             list_fix = [f"{repr(x)}" if isinstance(x, str) else f"{x}" for x in dims[0]]
5451             raise TypeError(
5452                 f'transpose requires dims to be passed as multiple arguments. Expected `{", ".join(list_fix)}`. Received `{dims[0]}` instead'
5453             )
5454 
5455         # Use infix_dims to check once for missing dimensions
5456         if len(dims) != 0:
5457             _ = list(infix_dims(dims, self.dims, missing_dims))
5458 
5459         ds = self.copy()
5460         for name, var in self._variables.items():
5461             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))
5462             ds._variables[name] = var.transpose(*var_dims)
5463         return ds
5464 
5465     def dropna(
5466         self: T_Dataset,
5467         dim: Hashable,
5468         how: Literal["any", "all"] = "any",
5469         thresh: int | None = None,
5470         subset: Iterable[Hashable] | None = None,
5471     ) -> T_Dataset:
5472         """Returns a new dataset with dropped labels for missing values along
5473         the provided dimension.
5474 
5475         Parameters
5476         ----------
5477         dim : hashable
5478             Dimension along which to drop missing values. Dropping along
5479             multiple dimensions simultaneously is not yet supported.
5480         how : {"any", "all"}, default: "any"
5481             - any : if any NA values are present, drop that label
5482             - all : if all values are NA, drop that label
5483 
5484         thresh : int or None, optional
5485             If supplied, require this many non-NA values (summed over all the subset variables).
5486         subset : iterable of hashable or None, optional
5487             Which variables to check for missing values. By default, all
5488             variables in the dataset are checked.
5489 
5490         Returns
5491         -------
5492         Dataset
5493         """
5494         # TODO: consider supporting multiple dimensions? Or not, given that
5495         # there are some ugly edge cases, e.g., pandas's dropna differs
5496         # depending on the order of the supplied axes.
5497 
5498         if dim not in self.dims:
5499             raise ValueError(f"{dim} must be a single dataset dimension")
5500 
5501         if subset is None:
5502             subset = iter(self.data_vars)
5503 
5504         count = np.zeros(self.dims[dim], dtype=np.int64)
5505         size = np.int_(0)  # for type checking
5506 
5507         for k in subset:
5508             array = self._variables[k]
5509             if dim in array.dims:
5510                 dims = [d for d in array.dims if d != dim]
5511                 count += np.asarray(array.count(dims))  # type: ignore[attr-defined]
5512                 size += math.prod([self.dims[d] for d in dims])
5513 
5514         if thresh is not None:
5515             mask = count >= thresh
5516         elif how == "any":
5517             mask = count == size
5518         elif how == "all":
5519             mask = count > 0
5520         elif how is not None:
5521             raise ValueError(f"invalid how option: {how}")
5522         else:
5523             raise TypeError("must specify how or thresh")
5524 
5525         return self.isel({dim: mask})
5526 
5527     def fillna(self: T_Dataset, value: Any) -> T_Dataset:
5528         """Fill missing values in this object.
5529 
5530         This operation follows the normal broadcasting and alignment rules that
5531         xarray uses for binary arithmetic, except the result is aligned to this
5532         object (``join='left'``) instead of aligned to the intersection of
5533         index coordinates (``join='inner'``).
5534 
5535         Parameters
5536         ----------
5537         value : scalar, ndarray, DataArray, dict or Dataset
5538             Used to fill all matching missing values in this dataset's data
5539             variables. Scalars, ndarrays or DataArrays arguments are used to
5540             fill all data with aligned coordinates (for DataArrays).
5541             Dictionaries or datasets match data variables and then align
5542             coordinates if necessary.
5543 
5544         Returns
5545         -------
5546         Dataset
5547 
5548         Examples
5549         --------
5550         >>> ds = xr.Dataset(
5551         ...     {
5552         ...         "A": ("x", [np.nan, 2, np.nan, 0]),
5553         ...         "B": ("x", [3, 4, np.nan, 1]),
5554         ...         "C": ("x", [np.nan, np.nan, np.nan, 5]),
5555         ...         "D": ("x", [np.nan, 3, np.nan, 4]),
5556         ...     },
5557         ...     coords={"x": [0, 1, 2, 3]},
5558         ... )
5559         >>> ds
5560         <xarray.Dataset>
5561         Dimensions:  (x: 4)
5562         Coordinates:
5563           * x        (x) int64 0 1 2 3
5564         Data variables:
5565             A        (x) float64 nan 2.0 nan 0.0
5566             B        (x) float64 3.0 4.0 nan 1.0
5567             C        (x) float64 nan nan nan 5.0
5568             D        (x) float64 nan 3.0 nan 4.0
5569 
5570         Replace all `NaN` values with 0s.
5571 
5572         >>> ds.fillna(0)
5573         <xarray.Dataset>
5574         Dimensions:  (x: 4)
5575         Coordinates:
5576           * x        (x) int64 0 1 2 3
5577         Data variables:
5578             A        (x) float64 0.0 2.0 0.0 0.0
5579             B        (x) float64 3.0 4.0 0.0 1.0
5580             C        (x) float64 0.0 0.0 0.0 5.0
5581             D        (x) float64 0.0 3.0 0.0 4.0
5582 
5583         Replace all `NaN` elements in column ‘A’, ‘B’, ‘C’, and ‘D’, with 0, 1, 2, and 3 respectively.
5584 
5585         >>> values = {"A": 0, "B": 1, "C": 2, "D": 3}
5586         >>> ds.fillna(value=values)
5587         <xarray.Dataset>
5588         Dimensions:  (x: 4)
5589         Coordinates:
5590           * x        (x) int64 0 1 2 3
5591         Data variables:
5592             A        (x) float64 0.0 2.0 0.0 0.0
5593             B        (x) float64 3.0 4.0 1.0 1.0
5594             C        (x) float64 2.0 2.0 2.0 5.0
5595             D        (x) float64 3.0 3.0 3.0 4.0
5596         """
5597         if utils.is_dict_like(value):
5598             value_keys = getattr(value, "data_vars", value).keys()
5599             if not set(value_keys) <= set(self.data_vars.keys()):
5600                 raise ValueError(
5601                     "all variables in the argument to `fillna` "
5602                     "must be contained in the original dataset"
5603                 )
5604         out = ops.fillna(self, value)
5605         return out
5606 
5607     def interpolate_na(
5608         self: T_Dataset,
5609         dim: Hashable | None = None,
5610         method: InterpOptions = "linear",
5611         limit: int | None = None,
5612         use_coordinate: bool | Hashable = True,
5613         max_gap: (
5614             int | float | str | pd.Timedelta | np.timedelta64 | datetime.timedelta
5615         ) = None,
5616         **kwargs: Any,
5617     ) -> T_Dataset:
5618         """Fill in NaNs by interpolating according to different methods.
5619 
5620         Parameters
5621         ----------
5622         dim : Hashable or None, optional
5623             Specifies the dimension along which to interpolate.
5624         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
5625             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
5626             String indicating which method to use for interpolation:
5627 
5628             - 'linear': linear interpolation. Additional keyword
5629               arguments are passed to :py:func:`numpy.interp`
5630             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
5631               are passed to :py:func:`scipy.interpolate.interp1d`. If
5632               ``method='polynomial'``, the ``order`` keyword argument must also be
5633               provided.
5634             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
5635               respective :py:class:`scipy.interpolate` classes.
5636 
5637         use_coordinate : bool or Hashable, default: True
5638             Specifies which index to use as the x values in the interpolation
5639             formulated as `y = f(x)`. If False, values are treated as if
5640             equally-spaced along ``dim``. If True, the IndexVariable `dim` is
5641             used. If ``use_coordinate`` is a string, it specifies the name of a
5642             coordinate variable to use as the index.
5643         limit : int, default: None
5644             Maximum number of consecutive NaNs to fill. Must be greater than 0
5645             or None for no limit. This filling is done regardless of the size of
5646             the gap in the data. To only interpolate over gaps less than a given length,
5647             see ``max_gap``.
5648         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
5649             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
5650             Use None for no limit. When interpolating along a datetime64 dimension
5651             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
5652 
5653             - a string that is valid input for pandas.to_timedelta
5654             - a :py:class:`numpy.timedelta64` object
5655             - a :py:class:`pandas.Timedelta` object
5656             - a :py:class:`datetime.timedelta` object
5657 
5658             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
5659             dimensions has not been implemented yet. Gap length is defined as the difference
5660             between coordinate values at the first data point after a gap and the last value
5661             before a gap. For gaps at the beginning (end), gap length is defined as the difference
5662             between coordinate values at the first (last) valid data point and the first (last) NaN.
5663             For example, consider::
5664 
5665                 <xarray.DataArray (x: 9)>
5666                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
5667                 Coordinates:
5668                   * x        (x) int64 0 1 2 3 4 5 6 7 8
5669 
5670             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
5671         **kwargs : dict, optional
5672             parameters passed verbatim to the underlying interpolation function
5673 
5674         Returns
5675         -------
5676         interpolated: Dataset
5677             Filled in Dataset.
5678 
5679         See Also
5680         --------
5681         numpy.interp
5682         scipy.interpolate
5683 
5684         Examples
5685         --------
5686         >>> ds = xr.Dataset(
5687         ...     {
5688         ...         "A": ("x", [np.nan, 2, 3, np.nan, 0]),
5689         ...         "B": ("x", [3, 4, np.nan, 1, 7]),
5690         ...         "C": ("x", [np.nan, np.nan, np.nan, 5, 0]),
5691         ...         "D": ("x", [np.nan, 3, np.nan, -1, 4]),
5692         ...     },
5693         ...     coords={"x": [0, 1, 2, 3, 4]},
5694         ... )
5695         >>> ds
5696         <xarray.Dataset>
5697         Dimensions:  (x: 5)
5698         Coordinates:
5699           * x        (x) int64 0 1 2 3 4
5700         Data variables:
5701             A        (x) float64 nan 2.0 3.0 nan 0.0
5702             B        (x) float64 3.0 4.0 nan 1.0 7.0
5703             C        (x) float64 nan nan nan 5.0 0.0
5704             D        (x) float64 nan 3.0 nan -1.0 4.0
5705 
5706         >>> ds.interpolate_na(dim="x", method="linear")
5707         <xarray.Dataset>
5708         Dimensions:  (x: 5)
5709         Coordinates:
5710           * x        (x) int64 0 1 2 3 4
5711         Data variables:
5712             A        (x) float64 nan 2.0 3.0 1.5 0.0
5713             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5714             C        (x) float64 nan nan nan 5.0 0.0
5715             D        (x) float64 nan 3.0 1.0 -1.0 4.0
5716 
5717         >>> ds.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
5718         <xarray.Dataset>
5719         Dimensions:  (x: 5)
5720         Coordinates:
5721           * x        (x) int64 0 1 2 3 4
5722         Data variables:
5723             A        (x) float64 1.0 2.0 3.0 1.5 0.0
5724             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5725             C        (x) float64 20.0 15.0 10.0 5.0 0.0
5726             D        (x) float64 5.0 3.0 1.0 -1.0 4.0
5727         """
5728         from xarray.core.missing import _apply_over_vars_with_dim, interp_na
5729 
5730         new = _apply_over_vars_with_dim(
5731             interp_na,
5732             self,
5733             dim=dim,
5734             method=method,
5735             limit=limit,
5736             use_coordinate=use_coordinate,
5737             max_gap=max_gap,
5738             **kwargs,
5739         )
5740         return new
5741 
5742     def ffill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5743         """Fill NaN values by propagating values forward
5744 
5745         *Requires bottleneck.*
5746 
5747         Parameters
5748         ----------
5749         dim : Hashable
5750             Specifies the dimension along which to propagate values when
5751             filling.
5752         limit : int or None, optional
5753             The maximum number of consecutive NaN values to forward fill. In
5754             other words, if there is a gap with more than this number of
5755             consecutive NaNs, it will only be partially filled. Must be greater
5756             than 0 or None for no limit. Must be None or greater than or equal
5757             to axis length if filling along chunked axes (dimensions).
5758 
5759         Returns
5760         -------
5761         Dataset
5762         """
5763         from xarray.core.missing import _apply_over_vars_with_dim, ffill
5764 
5765         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)
5766         return new
5767 
5768     def bfill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5769         """Fill NaN values by propagating values backward
5770 
5771         *Requires bottleneck.*
5772 
5773         Parameters
5774         ----------
5775         dim : Hashable
5776             Specifies the dimension along which to propagate values when
5777             filling.
5778         limit : int or None, optional
5779             The maximum number of consecutive NaN values to backward fill. In
5780             other words, if there is a gap with more than this number of
5781             consecutive NaNs, it will only be partially filled. Must be greater
5782             than 0 or None for no limit. Must be None or greater than or equal
5783             to axis length if filling along chunked axes (dimensions).
5784 
5785         Returns
5786         -------
5787         Dataset
5788         """
5789         from xarray.core.missing import _apply_over_vars_with_dim, bfill
5790 
5791         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)
5792         return new
5793 
5794     def combine_first(self: T_Dataset, other: T_Dataset) -> T_Dataset:
5795         """Combine two Datasets, default to data_vars of self.
5796 
5797         The new coordinates follow the normal broadcasting and alignment rules
5798         of ``join='outer'``.  Vacant cells in the expanded coordinates are
5799         filled with np.nan.
5800 
5801         Parameters
5802         ----------
5803         other : Dataset
5804             Used to fill all matching missing values in this array.
5805 
5806         Returns
5807         -------
5808         Dataset
5809         """
5810         out = ops.fillna(self, other, join="outer", dataset_join="outer")
5811         return out
5812 
5813     def reduce(
5814         self: T_Dataset,
5815         func: Callable,
5816         dim: Dims = None,
5817         *,
5818         keep_attrs: bool | None = None,
5819         keepdims: bool = False,
5820         numeric_only: bool = False,
5821         **kwargs: Any,
5822     ) -> T_Dataset:
5823         """Reduce this dataset by applying `func` along some dimension(s).
5824 
5825         Parameters
5826         ----------
5827         func : callable
5828             Function which can be called in the form
5829             `f(x, axis=axis, **kwargs)` to return the result of reducing an
5830             np.ndarray over an integer valued axis.
5831         dim : str, Iterable of Hashable or None, optional
5832             Dimension(s) over which to apply `func`. By default `func` is
5833             applied over all dimensions.
5834         keep_attrs : bool or None, optional
5835             If True, the dataset's attributes (`attrs`) will be copied from
5836             the original object to the new one.  If False (default), the new
5837             object will be returned without attributes.
5838         keepdims : bool, default: False
5839             If True, the dimensions which are reduced are left in the result
5840             as dimensions of size one. Coordinates that use these dimensions
5841             are removed.
5842         numeric_only : bool, default: False
5843             If True, only apply ``func`` to variables with a numeric dtype.
5844         **kwargs : Any
5845             Additional keyword arguments passed on to ``func``.
5846 
5847         Returns
5848         -------
5849         reduced : Dataset
5850             Dataset with this object's DataArrays replaced with new DataArrays
5851             of summarized data and the indicated dimension(s) removed.
5852         """
5853         if kwargs.get("axis", None) is not None:
5854             raise ValueError(
5855                 "passing 'axis' to Dataset reduce methods is ambiguous."
5856                 " Please use 'dim' instead."
5857             )
5858 
5859         if dim is None or dim is ...:
5860             dims = set(self.dims)
5861         elif isinstance(dim, str) or not isinstance(dim, Iterable):
5862             dims = {dim}
5863         else:
5864             dims = set(dim)
5865 
5866         missing_dimensions = [d for d in dims if d not in self.dims]
5867         if missing_dimensions:
5868             raise ValueError(
5869                 f"Dataset does not contain the dimensions: {missing_dimensions}"
5870             )
5871 
5872         if keep_attrs is None:
5873             keep_attrs = _get_keep_attrs(default=False)
5874 
5875         variables: dict[Hashable, Variable] = {}
5876         for name, var in self._variables.items():
5877             reduce_dims = [d for d in var.dims if d in dims]
5878             if name in self.coords:
5879                 if not reduce_dims:
5880                     variables[name] = var
5881             else:
5882                 if (
5883                     # Some reduction functions (e.g. std, var) need to run on variables
5884                     # that don't have the reduce dims: PR5393
5885                     not reduce_dims
5886                     or not numeric_only
5887                     or np.issubdtype(var.dtype, np.number)
5888                     or (var.dtype == np.bool_)
5889                 ):
5890                     # prefer to aggregate over axis=None rather than
5891                     # axis=(0, 1) if they will be equivalent, because
5892                     # the former is often more efficient
5893                     # keep single-element dims as list, to support Hashables
5894                     reduce_maybe_single = (
5895                         None
5896                         if len(reduce_dims) == var.ndim and var.ndim != 1
5897                         else reduce_dims
5898                     )
5899                     variables[name] = var.reduce(
5900                         func,
5901                         dim=reduce_maybe_single,
5902                         keep_attrs=keep_attrs,
5903                         keepdims=keepdims,
5904                         **kwargs,
5905                     )
5906 
5907         coord_names = {k for k in self.coords if k in variables}
5908         indexes = {k: v for k, v in self._indexes.items() if k in variables}
5909         attrs = self.attrs if keep_attrs else None
5910         return self._replace_with_new_dims(
5911             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
5912         )
5913 
5914     def map(
5915         self: T_Dataset,
5916         func: Callable,
5917         keep_attrs: bool | None = None,
5918         args: Iterable[Any] = (),
5919         **kwargs: Any,
5920     ) -> T_Dataset:
5921         """Apply a function to each data variable in this dataset
5922 
5923         Parameters
5924         ----------
5925         func : callable
5926             Function which can be called in the form `func(x, *args, **kwargs)`
5927             to transform each DataArray `x` in this dataset into another
5928             DataArray.
5929         keep_attrs : bool or None, optional
5930             If True, both the dataset's and variables' attributes (`attrs`) will be
5931             copied from the original objects to the new ones. If False, the new dataset
5932             and variables will be returned without copying the attributes.
5933         args : iterable, optional
5934             Positional arguments passed on to `func`.
5935         **kwargs : Any
5936             Keyword arguments passed on to `func`.
5937 
5938         Returns
5939         -------
5940         applied : Dataset
5941             Resulting dataset from applying ``func`` to each data variable.
5942 
5943         Examples
5944         --------
5945         >>> da = xr.DataArray(np.random.randn(2, 3))
5946         >>> ds = xr.Dataset({"foo": da, "bar": ("x", [-1, 2])})
5947         >>> ds
5948         <xarray.Dataset>
5949         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5950         Dimensions without coordinates: dim_0, dim_1, x
5951         Data variables:
5952             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
5953             bar      (x) int64 -1 2
5954         >>> ds.map(np.fabs)
5955         <xarray.Dataset>
5956         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5957         Dimensions without coordinates: dim_0, dim_1, x
5958         Data variables:
5959             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 0.9773
5960             bar      (x) float64 1.0 2.0
5961         """
5962         if keep_attrs is None:
5963             keep_attrs = _get_keep_attrs(default=False)
5964         variables = {
5965             k: maybe_wrap_array(v, func(v, *args, **kwargs))
5966             for k, v in self.data_vars.items()
5967         }
5968         if keep_attrs:
5969             for k, v in variables.items():
5970                 v._copy_attrs_from(self.data_vars[k])
5971         attrs = self.attrs if keep_attrs else None
5972         return type(self)(variables, attrs=attrs)
5973 
5974     def apply(
5975         self: T_Dataset,
5976         func: Callable,
5977         keep_attrs: bool | None = None,
5978         args: Iterable[Any] = (),
5979         **kwargs: Any,
5980     ) -> T_Dataset:
5981         """
5982         Backward compatible implementation of ``map``
5983 
5984         See Also
5985         --------
5986         Dataset.map
5987         """
5988         warnings.warn(
5989             "Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged",
5990             PendingDeprecationWarning,
5991             stacklevel=2,
5992         )
5993         return self.map(func, keep_attrs, args, **kwargs)
5994 
5995     def assign(
5996         self: T_Dataset,
5997         variables: Mapping[Any, Any] | None = None,
5998         **variables_kwargs: Any,
5999     ) -> T_Dataset:
6000         """Assign new data variables to a Dataset, returning a new object
6001         with all the original variables in addition to the new ones.
6002 
6003         Parameters
6004         ----------
6005         variables : mapping of hashable to Any
6006             Mapping from variables names to the new values. If the new values
6007             are callable, they are computed on the Dataset and assigned to new
6008             data variables. If the values are not callable, (e.g. a DataArray,
6009             scalar, or array), they are simply assigned.
6010         **variables_kwargs
6011             The keyword arguments form of ``variables``.
6012             One of variables or variables_kwargs must be provided.
6013 
6014         Returns
6015         -------
6016         ds : Dataset
6017             A new Dataset with the new variables in addition to all the
6018             existing variables.
6019 
6020         Notes
6021         -----
6022         Since ``kwargs`` is a dictionary, the order of your arguments may not
6023         be preserved, and so the order of the new variables is not well
6024         defined. Assigning multiple variables within the same ``assign`` is
6025         possible, but you cannot reference other variables created within the
6026         same ``assign`` call.
6027 
6028         See Also
6029         --------
6030         pandas.DataFrame.assign
6031 
6032         Examples
6033         --------
6034         >>> x = xr.Dataset(
6035         ...     {
6036         ...         "temperature_c": (
6037         ...             ("lat", "lon"),
6038         ...             20 * np.random.rand(4).reshape(2, 2),
6039         ...         ),
6040         ...         "precipitation": (("lat", "lon"), np.random.rand(4).reshape(2, 2)),
6041         ...     },
6042         ...     coords={"lat": [10, 20], "lon": [150, 160]},
6043         ... )
6044         >>> x
6045         <xarray.Dataset>
6046         Dimensions:        (lat: 2, lon: 2)
6047         Coordinates:
6048           * lat            (lat) int64 10 20
6049           * lon            (lon) int64 150 160
6050         Data variables:
6051             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6052             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6053 
6054         Where the value is a callable, evaluated on dataset:
6055 
6056         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)
6057         <xarray.Dataset>
6058         Dimensions:        (lat: 2, lon: 2)
6059         Coordinates:
6060           * lat            (lat) int64 10 20
6061           * lon            (lon) int64 150 160
6062         Data variables:
6063             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6064             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6065             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
6066 
6067         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:
6068 
6069         >>> x.assign(temperature_f=x["temperature_c"] * 9 / 5 + 32)
6070         <xarray.Dataset>
6071         Dimensions:        (lat: 2, lon: 2)
6072         Coordinates:
6073           * lat            (lat) int64 10 20
6074           * lon            (lon) int64 150 160
6075         Data variables:
6076             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6077             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6078             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
6079 
6080         """
6081         variables = either_dict_or_kwargs(variables, variables_kwargs, "assign")
6082         data = self.copy()
6083         # do all calculations first...
6084         results: CoercibleMapping = data._calc_assign_results(variables)
6085         data.coords._maybe_drop_multiindex_coords(set(results.keys()))
6086         # ... and then assign
6087         data.update(results)
6088         return data
6089 
6090     def to_array(
6091         self, dim: Hashable = "variable", name: Hashable | None = None
6092     ) -> DataArray:
6093         """Convert this dataset into an xarray.DataArray
6094 
6095         The data variables of this dataset will be broadcast against each other
6096         and stacked along the first axis of the new array. All coordinates of
6097         this dataset will remain coordinates.
6098 
6099         Parameters
6100         ----------
6101         dim : Hashable, default: "variable"
6102             Name of the new dimension.
6103         name : Hashable or None, optional
6104             Name of the new data array.
6105 
6106         Returns
6107         -------
6108         array : xarray.DataArray
6109         """
6110         from xarray.core.dataarray import DataArray
6111 
6112         data_vars = [self.variables[k] for k in self.data_vars]
6113         broadcast_vars = broadcast_variables(*data_vars)
6114         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)
6115 
6116         dims = (dim,) + broadcast_vars[0].dims
6117         variable = Variable(dims, data, self.attrs, fastpath=True)
6118 
6119         coords = {k: v.variable for k, v in self.coords.items()}
6120         indexes = filter_indexes_from_coords(self._indexes, set(coords))
6121         new_dim_index = PandasIndex(list(self.data_vars), dim)
6122         indexes[dim] = new_dim_index
6123         coords.update(new_dim_index.create_variables())
6124 
6125         return DataArray._construct_direct(variable, coords, name, indexes)
6126 
6127     def _normalize_dim_order(
6128         self, dim_order: Sequence[Hashable] | None = None
6129     ) -> dict[Hashable, int]:
6130         """
6131         Check the validity of the provided dimensions if any and return the mapping
6132         between dimension name and their size.
6133 
6134         Parameters
6135         ----------
6136         dim_order: Sequence of Hashable or None, optional
6137             Dimension order to validate (default to the alphabetical order if None).
6138 
6139         Returns
6140         -------
6141         result : dict[Hashable, int]
6142             Validated dimensions mapping.
6143 
6144         """
6145         if dim_order is None:
6146             dim_order = list(self.dims)
6147         elif set(dim_order) != set(self.dims):
6148             raise ValueError(
6149                 "dim_order {} does not match the set of dimensions of this "
6150                 "Dataset: {}".format(dim_order, list(self.dims))
6151             )
6152 
6153         ordered_dims = {k: self.dims[k] for k in dim_order}
6154 
6155         return ordered_dims
6156 
6157     def to_pandas(self) -> pd.Series | pd.DataFrame:
6158         """Convert this dataset into a pandas object without changing the number of dimensions.
6159 
6160         The type of the returned object depends on the number of Dataset
6161         dimensions:
6162 
6163         * 0D -> `pandas.Series`
6164         * 1D -> `pandas.DataFrame`
6165 
6166         Only works for Datasets with 1 or fewer dimensions.
6167         """
6168         if len(self.dims) == 0:
6169             return pd.Series({k: v.item() for k, v in self.items()})
6170         if len(self.dims) == 1:
6171             return self.to_dataframe()
6172         raise ValueError(
6173             "cannot convert Datasets with %s dimensions into "
6174             "pandas objects without changing the number of dimensions. "
6175             "Please use Dataset.to_dataframe() instead." % len(self.dims)
6176         )
6177 
6178     def _to_dataframe(self, ordered_dims: Mapping[Any, int]):
6179         columns = [k for k in self.variables if k not in self.dims]
6180         data = [
6181             self._variables[k].set_dims(ordered_dims).values.reshape(-1)
6182             for k in columns
6183         ]
6184         index = self.coords.to_index([*ordered_dims])
6185         return pd.DataFrame(dict(zip(columns, data)), index=index)
6186 
6187     def to_dataframe(self, dim_order: Sequence[Hashable] | None = None) -> pd.DataFrame:
6188         """Convert this dataset into a pandas.DataFrame.
6189 
6190         Non-index variables in this dataset form the columns of the
6191         DataFrame. The DataFrame is indexed by the Cartesian product of
6192         this dataset's indices.
6193 
6194         Parameters
6195         ----------
6196         dim_order: Sequence of Hashable or None, optional
6197             Hierarchical dimension order for the resulting dataframe. All
6198             arrays are transposed to this order and then written out as flat
6199             vectors in contiguous order, so the last dimension in this list
6200             will be contiguous in the resulting DataFrame. This has a major
6201             influence on which operations are efficient on the resulting
6202             dataframe.
6203 
6204             If provided, must include all dimensions of this dataset. By
6205             default, dimensions are sorted alphabetically.
6206 
6207         Returns
6208         -------
6209         result : DataFrame
6210             Dataset as a pandas DataFrame.
6211 
6212         """
6213 
6214         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
6215 
6216         return self._to_dataframe(ordered_dims=ordered_dims)
6217 
6218     def _set_sparse_data_from_dataframe(
6219         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
6220     ) -> None:
6221         from sparse import COO
6222 
6223         if isinstance(idx, pd.MultiIndex):
6224             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)
6225             is_sorted = idx.is_monotonic_increasing
6226             shape = tuple(lev.size for lev in idx.levels)
6227         else:
6228             coords = np.arange(idx.size).reshape(1, -1)
6229             is_sorted = True
6230             shape = (idx.size,)
6231 
6232         for name, values in arrays:
6233             # In virtually all real use cases, the sparse array will now have
6234             # missing values and needs a fill_value. For consistency, don't
6235             # special case the rare exceptions (e.g., dtype=int without a
6236             # MultiIndex).
6237             dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
6238             values = np.asarray(values, dtype=dtype)
6239 
6240             data = COO(
6241                 coords,
6242                 values,
6243                 shape,
6244                 has_duplicates=False,
6245                 sorted=is_sorted,
6246                 fill_value=fill_value,
6247             )
6248             self[name] = (dims, data)
6249 
6250     def _set_numpy_data_from_dataframe(
6251         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
6252     ) -> None:
6253         if not isinstance(idx, pd.MultiIndex):
6254             for name, values in arrays:
6255                 self[name] = (dims, values)
6256             return
6257 
6258         # NB: similar, more general logic, now exists in
6259         # variable.unstack_once; we could consider combining them at some
6260         # point.
6261 
6262         shape = tuple(lev.size for lev in idx.levels)
6263         indexer = tuple(idx.codes)
6264 
6265         # We already verified that the MultiIndex has all unique values, so
6266         # there are missing values if and only if the size of output arrays is
6267         # larger that the index.
6268         missing_values = math.prod(shape) > idx.shape[0]
6269 
6270         for name, values in arrays:
6271             # NumPy indexing is much faster than using DataFrame.reindex() to
6272             # fill in missing values:
6273             # https://stackoverflow.com/a/35049899/809705
6274             if missing_values:
6275                 dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
6276                 data = np.full(shape, fill_value, dtype)
6277             else:
6278                 # If there are no missing values, keep the existing dtype
6279                 # instead of promoting to support NA, e.g., keep integer
6280                 # columns as integers.
6281                 # TODO: consider removing this special case, which doesn't
6282                 # exist for sparse=True.
6283                 data = np.zeros(shape, values.dtype)
6284             data[indexer] = values
6285             self[name] = (dims, data)
6286 
6287     @classmethod
6288     def from_dataframe(
6289         cls: type[T_Dataset], dataframe: pd.DataFrame, sparse: bool = False
6290     ) -> T_Dataset:
6291         """Convert a pandas.DataFrame into an xarray.Dataset
6292 
6293         Each column will be converted into an independent variable in the
6294         Dataset. If the dataframe's index is a MultiIndex, it will be expanded
6295         into a tensor product of one-dimensional indices (filling in missing
6296         values with NaN). This method will produce a Dataset very similar to
6297         that on which the 'to_dataframe' method was called, except with
6298         possibly redundant dimensions (since all dataset variables will have
6299         the same dimensionality)
6300 
6301         Parameters
6302         ----------
6303         dataframe : DataFrame
6304             DataFrame from which to copy data and indices.
6305         sparse : bool, default: False
6306             If true, create a sparse arrays instead of dense numpy arrays. This
6307             can potentially save a large amount of memory if the DataFrame has
6308             a MultiIndex. Requires the sparse package (sparse.pydata.org).
6309 
6310         Returns
6311         -------
6312         New Dataset.
6313 
6314         See Also
6315         --------
6316         xarray.DataArray.from_series
6317         pandas.DataFrame.to_xarray
6318         """
6319         # TODO: Add an option to remove dimensions along which the variables
6320         # are constant, to enable consistent serialization to/from a dataframe,
6321         # even if some variables have different dimensionality.
6322 
6323         if not dataframe.columns.is_unique:
6324             raise ValueError("cannot convert DataFrame with non-unique columns")
6325 
6326         idx = remove_unused_levels_categories(dataframe.index)
6327 
6328         if isinstance(idx, pd.MultiIndex) and not idx.is_unique:
6329             raise ValueError(
6330                 "cannot convert a DataFrame with a non-unique MultiIndex into xarray"
6331             )
6332 
6333         # Cast to a NumPy array first, in case the Series is a pandas Extension
6334         # array (which doesn't have a valid NumPy dtype)
6335         # TODO: allow users to control how this casting happens, e.g., by
6336         # forwarding arguments to pandas.Series.to_numpy?
6337         arrays = [(k, np.asarray(v)) for k, v in dataframe.items()]
6338 
6339         indexes: dict[Hashable, Index] = {}
6340         index_vars: dict[Hashable, Variable] = {}
6341 
6342         if isinstance(idx, pd.MultiIndex):
6343             dims = tuple(
6344                 name if name is not None else "level_%i" % n
6345                 for n, name in enumerate(idx.names)
6346             )
6347             for dim, lev in zip(dims, idx.levels):
6348                 xr_idx = PandasIndex(lev, dim)
6349                 indexes[dim] = xr_idx
6350                 index_vars.update(xr_idx.create_variables())
6351         else:
6352             index_name = idx.name if idx.name is not None else "index"
6353             dims = (index_name,)
6354             xr_idx = PandasIndex(idx, index_name)
6355             indexes[index_name] = xr_idx
6356             index_vars.update(xr_idx.create_variables())
6357 
6358         obj = cls._construct_direct(index_vars, set(index_vars), indexes=indexes)
6359 
6360         if sparse:
6361             obj._set_sparse_data_from_dataframe(idx, arrays, dims)
6362         else:
6363             obj._set_numpy_data_from_dataframe(idx, arrays, dims)
6364         return obj
6365 
6366     def to_dask_dataframe(
6367         self, dim_order: Sequence[Hashable] | None = None, set_index: bool = False
6368     ) -> DaskDataFrame:
6369         """
6370         Convert this dataset into a dask.dataframe.DataFrame.
6371 
6372         The dimensions, coordinates and data variables in this dataset form
6373         the columns of the DataFrame.
6374 
6375         Parameters
6376         ----------
6377         dim_order : list, optional
6378             Hierarchical dimension order for the resulting dataframe. All
6379             arrays are transposed to this order and then written out as flat
6380             vectors in contiguous order, so the last dimension in this list
6381             will be contiguous in the resulting DataFrame. This has a major
6382             influence on which operations are efficient on the resulting dask
6383             dataframe.
6384 
6385             If provided, must include all dimensions of this dataset. By
6386             default, dimensions are sorted alphabetically.
6387         set_index : bool, default: False
6388             If set_index=True, the dask DataFrame is indexed by this dataset's
6389             coordinate. Since dask DataFrames do not support multi-indexes,
6390             set_index only works if the dataset only contains one dimension.
6391 
6392         Returns
6393         -------
6394         dask.dataframe.DataFrame
6395         """
6396 
6397         import dask.array as da
6398         import dask.dataframe as dd
6399 
6400         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
6401 
6402         columns = list(ordered_dims)
6403         columns.extend(k for k in self.coords if k not in self.dims)
6404         columns.extend(self.data_vars)
6405 
6406         series_list = []
6407         for name in columns:
6408             try:
6409                 var = self.variables[name]
6410             except KeyError:
6411                 # dimension without a matching coordinate
6412                 size = self.dims[name]
6413                 data = da.arange(size, chunks=size, dtype=np.int64)
6414                 var = Variable((name,), data)
6415 
6416             # IndexVariable objects have a dummy .chunk() method
6417             if isinstance(var, IndexVariable):
6418                 var = var.to_base_variable()
6419 
6420             # Make sure var is a dask array, otherwise the array can become too large
6421             # when it is broadcasted to several dimensions:
6422             if not is_duck_dask_array(var._data):
6423                 var = var.chunk()
6424 
6425             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data
6426             series = dd.from_array(dask_array.reshape(-1), columns=[name])
6427             series_list.append(series)
6428 
6429         df = dd.concat(series_list, axis=1)
6430 
6431         if set_index:
6432             dim_order = [*ordered_dims]
6433 
6434             if len(dim_order) == 1:
6435                 (dim,) = dim_order
6436                 df = df.set_index(dim)
6437             else:
6438                 # triggers an error about multi-indexes, even if only one
6439                 # dimension is passed
6440                 df = df.set_index(dim_order)
6441 
6442         return df
6443 
6444     def to_dict(
6445         self, data: bool | Literal["list", "array"] = "list", encoding: bool = False
6446     ) -> dict[str, Any]:
6447         """
6448         Convert this dataset to a dictionary following xarray naming
6449         conventions.
6450 
6451         Converts all variables and attributes to native Python objects
6452         Useful for converting to json. To avoid datetime incompatibility
6453         use decode_times=False kwarg in xarrray.open_dataset.
6454 
6455         Parameters
6456         ----------
6457         data : bool or {"list", "array"}, default: "list"
6458             Whether to include the actual data in the dictionary. When set to
6459             False, returns just the schema. If set to "array", returns data as
6460             underlying array type. If set to "list" (or True for backwards
6461             compatibility), returns data in lists of Python data types. Note
6462             that for obtaining the "list" output efficiently, use
6463             `ds.compute().to_dict(data="list")`.
6464 
6465         encoding : bool, default: False
6466             Whether to include the Dataset's encoding in the dictionary.
6467 
6468         Returns
6469         -------
6470         d : dict
6471             Dict with keys: "coords", "attrs", "dims", "data_vars" and optionally
6472             "encoding".
6473 
6474         See Also
6475         --------
6476         Dataset.from_dict
6477         DataArray.to_dict
6478         """
6479         d: dict = {
6480             "coords": {},
6481             "attrs": decode_numpy_dict_values(self.attrs),
6482             "dims": dict(self.dims),
6483             "data_vars": {},
6484         }
6485         for k in self.coords:
6486             d["coords"].update(
6487                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6488             )
6489         for k in self.data_vars:
6490             d["data_vars"].update(
6491                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6492             )
6493         if encoding:
6494             d["encoding"] = dict(self.encoding)
6495         return d
6496 
6497     @classmethod
6498     def from_dict(cls: type[T_Dataset], d: Mapping[Any, Any]) -> T_Dataset:
6499         """Convert a dictionary into an xarray.Dataset.
6500 
6501         Parameters
6502         ----------
6503         d : dict-like
6504             Mapping with a minimum structure of
6505                 ``{"var_0": {"dims": [..], "data": [..]}, \
6506                             ...}``
6507 
6508         Returns
6509         -------
6510         obj : Dataset
6511 
6512         See also
6513         --------
6514         Dataset.to_dict
6515         DataArray.from_dict
6516 
6517         Examples
6518         --------
6519         >>> d = {
6520         ...     "t": {"dims": ("t"), "data": [0, 1, 2]},
6521         ...     "a": {"dims": ("t"), "data": ["a", "b", "c"]},
6522         ...     "b": {"dims": ("t"), "data": [10, 20, 30]},
6523         ... }
6524         >>> ds = xr.Dataset.from_dict(d)
6525         >>> ds
6526         <xarray.Dataset>
6527         Dimensions:  (t: 3)
6528         Coordinates:
6529           * t        (t) int64 0 1 2
6530         Data variables:
6531             a        (t) <U1 'a' 'b' 'c'
6532             b        (t) int64 10 20 30
6533 
6534         >>> d = {
6535         ...     "coords": {
6536         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
6537         ...     },
6538         ...     "attrs": {"title": "air temperature"},
6539         ...     "dims": "t",
6540         ...     "data_vars": {
6541         ...         "a": {"dims": "t", "data": [10, 20, 30]},
6542         ...         "b": {"dims": "t", "data": ["a", "b", "c"]},
6543         ...     },
6544         ... }
6545         >>> ds = xr.Dataset.from_dict(d)
6546         >>> ds
6547         <xarray.Dataset>
6548         Dimensions:  (t: 3)
6549         Coordinates:
6550           * t        (t) int64 0 1 2
6551         Data variables:
6552             a        (t) int64 10 20 30
6553             b        (t) <U1 'a' 'b' 'c'
6554         Attributes:
6555             title:    air temperature
6556 
6557         """
6558 
6559         variables: Iterable[tuple[Hashable, Any]]
6560         if not {"coords", "data_vars"}.issubset(set(d)):
6561             variables = d.items()
6562         else:
6563             import itertools
6564 
6565             variables = itertools.chain(
6566                 d.get("coords", {}).items(), d.get("data_vars", {}).items()
6567             )
6568         try:
6569             variable_dict = {
6570                 k: (v["dims"], v["data"], v.get("attrs"), v.get("encoding"))
6571                 for k, v in variables
6572             }
6573         except KeyError as e:
6574             raise ValueError(
6575                 "cannot convert dict without the key "
6576                 "'{dims_data}'".format(dims_data=str(e.args[0]))
6577             )
6578         obj = cls(variable_dict)
6579 
6580         # what if coords aren't dims?
6581         coords = set(d.get("coords", {})) - set(d.get("dims", {}))
6582         obj = obj.set_coords(coords)
6583 
6584         obj.attrs.update(d.get("attrs", {}))
6585         obj.encoding.update(d.get("encoding", {}))
6586 
6587         return obj
6588 
6589     def _unary_op(self: T_Dataset, f, *args, **kwargs) -> T_Dataset:
6590         variables = {}
6591         keep_attrs = kwargs.pop("keep_attrs", None)
6592         if keep_attrs is None:
6593             keep_attrs = _get_keep_attrs(default=True)
6594         for k, v in self._variables.items():
6595             if k in self._coord_names:
6596                 variables[k] = v
6597             else:
6598                 variables[k] = f(v, *args, **kwargs)
6599                 if keep_attrs:
6600                     variables[k].attrs = v._attrs
6601         attrs = self._attrs if keep_attrs else None
6602         return self._replace_with_new_dims(variables, attrs=attrs)
6603 
6604     def _binary_op(self, other, f, reflexive=False, join=None) -> Dataset:
6605         from xarray.core.dataarray import DataArray
6606         from xarray.core.groupby import GroupBy
6607 
6608         if isinstance(other, GroupBy):
6609             return NotImplemented
6610         align_type = OPTIONS["arithmetic_join"] if join is None else join
6611         if isinstance(other, (DataArray, Dataset)):
6612             self, other = align(self, other, join=align_type, copy=False)  # type: ignore[assignment]
6613         g = f if not reflexive else lambda x, y: f(y, x)
6614         ds = self._calculate_binary_op(g, other, join=align_type)
6615         keep_attrs = _get_keep_attrs(default=False)
6616         if keep_attrs:
6617             ds.attrs = self.attrs
6618         return ds
6619 
6620     def _inplace_binary_op(self: T_Dataset, other, f) -> T_Dataset:
6621         from xarray.core.dataarray import DataArray
6622         from xarray.core.groupby import GroupBy
6623 
6624         if isinstance(other, GroupBy):
6625             raise TypeError(
6626                 "in-place operations between a Dataset and "
6627                 "a grouped object are not permitted"
6628             )
6629         # we don't actually modify arrays in-place with in-place Dataset
6630         # arithmetic -- this lets us automatically align things
6631         if isinstance(other, (DataArray, Dataset)):
6632             other = other.reindex_like(self, copy=False)
6633         g = ops.inplace_to_noninplace_op(f)
6634         ds = self._calculate_binary_op(g, other, inplace=True)
6635         self._replace_with_new_dims(
6636             ds._variables,
6637             ds._coord_names,
6638             attrs=ds._attrs,
6639             indexes=ds._indexes,
6640             inplace=True,
6641         )
6642         return self
6643 
6644     def _calculate_binary_op(
6645         self, f, other, join="inner", inplace: bool = False
6646     ) -> Dataset:
6647         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):
6648             if inplace and set(lhs_data_vars) != set(rhs_data_vars):
6649                 raise ValueError(
6650                     "datasets must have the same data variables "
6651                     f"for in-place arithmetic operations: {list(lhs_data_vars)}, {list(rhs_data_vars)}"
6652                 )
6653 
6654             dest_vars = {}
6655 
6656             for k in lhs_data_vars:
6657                 if k in rhs_data_vars:
6658                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])
6659                 elif join in ["left", "outer"]:
6660                     dest_vars[k] = f(lhs_vars[k], np.nan)
6661             for k in rhs_data_vars:
6662                 if k not in dest_vars and join in ["right", "outer"]:
6663                     dest_vars[k] = f(rhs_vars[k], np.nan)
6664             return dest_vars
6665 
6666         if utils.is_dict_like(other) and not isinstance(other, Dataset):
6667             # can't use our shortcut of doing the binary operation with
6668             # Variable objects, so apply over our data vars instead.
6669             new_data_vars = apply_over_both(
6670                 self.data_vars, other, self.data_vars, other
6671             )
6672             return type(self)(new_data_vars)
6673 
6674         other_coords: Coordinates | None = getattr(other, "coords", None)
6675         ds = self.coords.merge(other_coords)
6676 
6677         if isinstance(other, Dataset):
6678             new_vars = apply_over_both(
6679                 self.data_vars, other.data_vars, self.variables, other.variables
6680             )
6681         else:
6682             other_variable = getattr(other, "variable", other)
6683             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}
6684         ds._variables.update(new_vars)
6685         ds._dims = calculate_dimensions(ds._variables)
6686         return ds
6687 
6688     def _copy_attrs_from(self, other):
6689         self.attrs = other.attrs
6690         for v in other.variables:
6691             if v in self.variables:
6692                 self.variables[v].attrs = other.variables[v].attrs
6693 
6694     def diff(
6695         self: T_Dataset,
6696         dim: Hashable,
6697         n: int = 1,
6698         label: Literal["upper", "lower"] = "upper",
6699     ) -> T_Dataset:
6700         """Calculate the n-th order discrete difference along given axis.
6701 
6702         Parameters
6703         ----------
6704         dim : Hashable
6705             Dimension over which to calculate the finite difference.
6706         n : int, default: 1
6707             The number of times values are differenced.
6708         label : {"upper", "lower"}, default: "upper"
6709             The new coordinate in dimension ``dim`` will have the
6710             values of either the minuend's or subtrahend's coordinate
6711             for values 'upper' and 'lower', respectively.
6712 
6713         Returns
6714         -------
6715         difference : Dataset
6716             The n-th order finite difference of this object.
6717 
6718         Notes
6719         -----
6720         `n` matches numpy's behavior and is different from pandas' first argument named
6721         `periods`.
6722 
6723         Examples
6724         --------
6725         >>> ds = xr.Dataset({"foo": ("x", [5, 5, 6, 6])})
6726         >>> ds.diff("x")
6727         <xarray.Dataset>
6728         Dimensions:  (x: 3)
6729         Dimensions without coordinates: x
6730         Data variables:
6731             foo      (x) int64 0 1 0
6732         >>> ds.diff("x", 2)
6733         <xarray.Dataset>
6734         Dimensions:  (x: 2)
6735         Dimensions without coordinates: x
6736         Data variables:
6737             foo      (x) int64 1 -1
6738 
6739         See Also
6740         --------
6741         Dataset.differentiate
6742         """
6743         if n == 0:
6744             return self
6745         if n < 0:
6746             raise ValueError(f"order `n` must be non-negative but got {n}")
6747 
6748         # prepare slices
6749         slice_start = {dim: slice(None, -1)}
6750         slice_end = {dim: slice(1, None)}
6751 
6752         # prepare new coordinate
6753         if label == "upper":
6754             slice_new = slice_end
6755         elif label == "lower":
6756             slice_new = slice_start
6757         else:
6758             raise ValueError("The 'label' argument has to be either 'upper' or 'lower'")
6759 
6760         indexes, index_vars = isel_indexes(self.xindexes, slice_new)
6761         variables = {}
6762 
6763         for name, var in self.variables.items():
6764             if name in index_vars:
6765                 variables[name] = index_vars[name]
6766             elif dim in var.dims:
6767                 if name in self.data_vars:
6768                     variables[name] = var.isel(slice_end) - var.isel(slice_start)
6769                 else:
6770                     variables[name] = var.isel(slice_new)
6771             else:
6772                 variables[name] = var
6773 
6774         difference = self._replace_with_new_dims(variables, indexes=indexes)
6775 
6776         if n > 1:
6777             return difference.diff(dim, n - 1)
6778         else:
6779             return difference
6780 
6781     def shift(
6782         self: T_Dataset,
6783         shifts: Mapping[Any, int] | None = None,
6784         fill_value: Any = xrdtypes.NA,
6785         **shifts_kwargs: int,
6786     ) -> T_Dataset:
6787         """Shift this dataset by an offset along one or more dimensions.
6788 
6789         Only data variables are moved; coordinates stay in place. This is
6790         consistent with the behavior of ``shift`` in pandas.
6791 
6792         Values shifted from beyond array bounds will appear at one end of
6793         each dimension, which are filled according to `fill_value`. For periodic
6794         offsets instead see `roll`.
6795 
6796         Parameters
6797         ----------
6798         shifts : mapping of hashable to int
6799             Integer offset to shift along each of the given dimensions.
6800             Positive offsets shift to the right; negative offsets shift to the
6801             left.
6802         fill_value : scalar or dict-like, optional
6803             Value to use for newly missing values. If a dict-like, maps
6804             variable names (including coordinates) to fill values.
6805         **shifts_kwargs
6806             The keyword arguments form of ``shifts``.
6807             One of shifts or shifts_kwargs must be provided.
6808 
6809         Returns
6810         -------
6811         shifted : Dataset
6812             Dataset with the same coordinates and attributes but shifted data
6813             variables.
6814 
6815         See Also
6816         --------
6817         roll
6818 
6819         Examples
6820         --------
6821         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
6822         >>> ds.shift(x=2)
6823         <xarray.Dataset>
6824         Dimensions:  (x: 5)
6825         Dimensions without coordinates: x
6826         Data variables:
6827             foo      (x) object nan nan 'a' 'b' 'c'
6828         """
6829         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
6830         invalid = [k for k in shifts if k not in self.dims]
6831         if invalid:
6832             raise ValueError(f"dimensions {invalid!r} do not exist")
6833 
6834         variables = {}
6835         for name, var in self.variables.items():
6836             if name in self.data_vars:
6837                 fill_value_ = (
6838                     fill_value.get(name, xrdtypes.NA)
6839                     if isinstance(fill_value, dict)
6840                     else fill_value
6841                 )
6842 
6843                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}
6844                 variables[name] = var.shift(fill_value=fill_value_, shifts=var_shifts)
6845             else:
6846                 variables[name] = var
6847 
6848         return self._replace(variables)
6849 
6850     def roll(
6851         self: T_Dataset,
6852         shifts: Mapping[Any, int] | None = None,
6853         roll_coords: bool = False,
6854         **shifts_kwargs: int,
6855     ) -> T_Dataset:
6856         """Roll this dataset by an offset along one or more dimensions.
6857 
6858         Unlike shift, roll treats the given dimensions as periodic, so will not
6859         create any missing values to be filled.
6860 
6861         Also unlike shift, roll may rotate all variables, including coordinates
6862         if specified. The direction of rotation is consistent with
6863         :py:func:`numpy.roll`.
6864 
6865         Parameters
6866         ----------
6867         shifts : mapping of hashable to int, optional
6868             A dict with keys matching dimensions and values given
6869             by integers to rotate each of the given dimensions. Positive
6870             offsets roll to the right; negative offsets roll to the left.
6871         roll_coords : bool, default: False
6872             Indicates whether to roll the coordinates by the offset too.
6873         **shifts_kwargs : {dim: offset, ...}, optional
6874             The keyword arguments form of ``shifts``.
6875             One of shifts or shifts_kwargs must be provided.
6876 
6877         Returns
6878         -------
6879         rolled : Dataset
6880             Dataset with the same attributes but rolled data and coordinates.
6881 
6882         See Also
6883         --------
6884         shift
6885 
6886         Examples
6887         --------
6888         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))}, coords={"x": np.arange(5)})
6889         >>> ds.roll(x=2)
6890         <xarray.Dataset>
6891         Dimensions:  (x: 5)
6892         Coordinates:
6893           * x        (x) int64 0 1 2 3 4
6894         Data variables:
6895             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6896 
6897         >>> ds.roll(x=2, roll_coords=True)
6898         <xarray.Dataset>
6899         Dimensions:  (x: 5)
6900         Coordinates:
6901           * x        (x) int64 3 4 0 1 2
6902         Data variables:
6903             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6904 
6905         """
6906         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
6907         invalid = [k for k in shifts if k not in self.dims]
6908         if invalid:
6909             raise ValueError(f"dimensions {invalid!r} do not exist")
6910 
6911         unrolled_vars: tuple[Hashable, ...]
6912 
6913         if roll_coords:
6914             indexes, index_vars = roll_indexes(self.xindexes, shifts)
6915             unrolled_vars = ()
6916         else:
6917             indexes = dict(self._indexes)
6918             index_vars = dict(self.xindexes.variables)
6919             unrolled_vars = tuple(self.coords)
6920 
6921         variables = {}
6922         for k, var in self.variables.items():
6923             if k in index_vars:
6924                 variables[k] = index_vars[k]
6925             elif k not in unrolled_vars:
6926                 variables[k] = var.roll(
6927                     shifts={k: s for k, s in shifts.items() if k in var.dims}
6928                 )
6929             else:
6930                 variables[k] = var
6931 
6932         return self._replace(variables, indexes=indexes)
6933 
6934     def sortby(
6935         self: T_Dataset,
6936         variables: Hashable | DataArray | list[Hashable | DataArray],
6937         ascending: bool = True,
6938     ) -> T_Dataset:
6939         """
6940         Sort object by labels or values (along an axis).
6941 
6942         Sorts the dataset, either along specified dimensions,
6943         or according to values of 1-D dataarrays that share dimension
6944         with calling object.
6945 
6946         If the input variables are dataarrays, then the dataarrays are aligned
6947         (via left-join) to the calling object prior to sorting by cell values.
6948         NaNs are sorted to the end, following Numpy convention.
6949 
6950         If multiple sorts along the same dimension is
6951         given, numpy's lexsort is performed along that dimension:
6952         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
6953         and the FIRST key in the sequence is used as the primary sort key,
6954         followed by the 2nd key, etc.
6955 
6956         Parameters
6957         ----------
6958         variables : Hashable, DataArray, or list of hashable or DataArray
6959             1D DataArray objects or name(s) of 1D variable(s) in
6960             coords/data_vars whose values are used to sort the dataset.
6961         ascending : bool, default: True
6962             Whether to sort by ascending or descending order.
6963 
6964         Returns
6965         -------
6966         sorted : Dataset
6967             A new dataset where all the specified dims are sorted by dim
6968             labels.
6969 
6970         See Also
6971         --------
6972         DataArray.sortby
6973         numpy.sort
6974         pandas.sort_values
6975         pandas.sort_index
6976 
6977         Examples
6978         --------
6979         >>> ds = xr.Dataset(
6980         ...     {
6981         ...         "A": (("x", "y"), [[1, 2], [3, 4]]),
6982         ...         "B": (("x", "y"), [[5, 6], [7, 8]]),
6983         ...     },
6984         ...     coords={"x": ["b", "a"], "y": [1, 0]},
6985         ... )
6986         >>> ds = ds.sortby("x")
6987         >>> ds
6988         <xarray.Dataset>
6989         Dimensions:  (x: 2, y: 2)
6990         Coordinates:
6991           * x        (x) <U1 'a' 'b'
6992           * y        (y) int64 1 0
6993         Data variables:
6994             A        (x, y) int64 3 4 1 2
6995             B        (x, y) int64 7 8 5 6
6996         """
6997         from xarray.core.dataarray import DataArray
6998 
6999         if not isinstance(variables, list):
7000             variables = [variables]
7001         else:
7002             variables = variables
7003         arrays = [v if isinstance(v, DataArray) else self[v] for v in variables]
7004         aligned_vars = align(self, *arrays, join="left")  # type: ignore[type-var]
7005         aligned_self: T_Dataset = aligned_vars[0]  # type: ignore[assignment]
7006         aligned_other_vars: tuple[DataArray, ...] = aligned_vars[1:]  # type: ignore[assignment]
7007         vars_by_dim = defaultdict(list)
7008         for data_array in aligned_other_vars:
7009             if data_array.ndim != 1:
7010                 raise ValueError("Input DataArray is not 1-D.")
7011             (key,) = data_array.dims
7012             vars_by_dim[key].append(data_array)
7013 
7014         indices = {}
7015         for key, arrays in vars_by_dim.items():
7016             order = np.lexsort(tuple(reversed(arrays)))
7017             indices[key] = order if ascending else order[::-1]
7018         return aligned_self.isel(indices)
7019 
7020     def quantile(
7021         self: T_Dataset,
7022         q: ArrayLike,
7023         dim: Dims = None,
7024         method: QuantileMethods = "linear",
7025         numeric_only: bool = False,
7026         keep_attrs: bool | None = None,
7027         skipna: bool | None = None,
7028         interpolation: QuantileMethods | None = None,
7029     ) -> T_Dataset:
7030         """Compute the qth quantile of the data along the specified dimension.
7031 
7032         Returns the qth quantiles(s) of the array elements for each variable
7033         in the Dataset.
7034 
7035         Parameters
7036         ----------
7037         q : float or array-like of float
7038             Quantile to compute, which must be between 0 and 1 inclusive.
7039         dim : str or Iterable of Hashable, optional
7040             Dimension(s) over which to apply quantile.
7041         method : str, default: "linear"
7042             This optional parameter specifies the interpolation method to use when the
7043             desired quantile lies between two data points. The options sorted by their R
7044             type as summarized in the H&F paper [1]_ are:
7045 
7046                 1. "inverted_cdf" (*)
7047                 2. "averaged_inverted_cdf" (*)
7048                 3. "closest_observation" (*)
7049                 4. "interpolated_inverted_cdf" (*)
7050                 5. "hazen" (*)
7051                 6. "weibull" (*)
7052                 7. "linear"  (default)
7053                 8. "median_unbiased" (*)
7054                 9. "normal_unbiased" (*)
7055 
7056             The first three methods are discontiuous.  The following discontinuous
7057             variations of the default "linear" (7.) option are also available:
7058 
7059                 * "lower"
7060                 * "higher"
7061                 * "midpoint"
7062                 * "nearest"
7063 
7064             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
7065             was previously called "interpolation", renamed in accordance with numpy
7066             version 1.22.0.
7067 
7068             (*) These methods require numpy version 1.22 or newer.
7069 
7070         keep_attrs : bool, optional
7071             If True, the dataset's attributes (`attrs`) will be copied from
7072             the original object to the new one.  If False (default), the new
7073             object will be returned without attributes.
7074         numeric_only : bool, optional
7075             If True, only apply ``func`` to variables with a numeric dtype.
7076         skipna : bool, optional
7077             If True, skip missing values (as marked by NaN). By default, only
7078             skips missing values for float dtypes; other dtypes either do not
7079             have a sentinel missing value (int) or skipna=True has not been
7080             implemented (object, datetime64 or timedelta64).
7081 
7082         Returns
7083         -------
7084         quantiles : Dataset
7085             If `q` is a single quantile, then the result is a scalar for each
7086             variable in data_vars. If multiple percentiles are given, first
7087             axis of the result corresponds to the quantile and a quantile
7088             dimension is added to the return Dataset. The other dimensions are
7089             the dimensions that remain after the reduction of the array.
7090 
7091         See Also
7092         --------
7093         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile
7094 
7095         Examples
7096         --------
7097         >>> ds = xr.Dataset(
7098         ...     {"a": (("x", "y"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},
7099         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
7100         ... )
7101         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)
7102         <xarray.Dataset>
7103         Dimensions:   ()
7104         Coordinates:
7105             quantile  float64 0.0
7106         Data variables:
7107             a         float64 0.7
7108         >>> ds.quantile(0, dim="x")
7109         <xarray.Dataset>
7110         Dimensions:   (y: 4)
7111         Coordinates:
7112           * y         (y) float64 1.0 1.5 2.0 2.5
7113             quantile  float64 0.0
7114         Data variables:
7115             a         (y) float64 0.7 4.2 2.6 1.5
7116         >>> ds.quantile([0, 0.5, 1])
7117         <xarray.Dataset>
7118         Dimensions:   (quantile: 3)
7119         Coordinates:
7120           * quantile  (quantile) float64 0.0 0.5 1.0
7121         Data variables:
7122             a         (quantile) float64 0.7 3.4 9.4
7123         >>> ds.quantile([0, 0.5, 1], dim="x")
7124         <xarray.Dataset>
7125         Dimensions:   (quantile: 3, y: 4)
7126         Coordinates:
7127           * y         (y) float64 1.0 1.5 2.0 2.5
7128           * quantile  (quantile) float64 0.0 0.5 1.0
7129         Data variables:
7130             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9
7131 
7132         References
7133         ----------
7134         .. [1] R. J. Hyndman and Y. Fan,
7135            "Sample quantiles in statistical packages,"
7136            The American Statistician, 50(4), pp. 361-365, 1996
7137         """
7138 
7139         # interpolation renamed to method in version 0.21.0
7140         # check here and in variable to avoid repeated warnings
7141         if interpolation is not None:
7142             warnings.warn(
7143                 "The `interpolation` argument to quantile was renamed to `method`.",
7144                 FutureWarning,
7145             )
7146 
7147             if method != "linear":
7148                 raise TypeError("Cannot pass interpolation and method keywords!")
7149 
7150             method = interpolation
7151 
7152         dims: set[Hashable]
7153         if isinstance(dim, str):
7154             dims = {dim}
7155         elif dim is None or dim is ...:
7156             dims = set(self.dims)
7157         else:
7158             dims = set(dim)
7159 
7160         _assert_empty(
7161             tuple(d for d in dims if d not in self.dims),
7162             "Dataset does not contain the dimensions: %s",
7163         )
7164 
7165         q = np.asarray(q, dtype=np.float64)
7166 
7167         variables = {}
7168         for name, var in self.variables.items():
7169             reduce_dims = [d for d in var.dims if d in dims]
7170             if reduce_dims or not var.dims:
7171                 if name not in self.coords:
7172                     if (
7173                         not numeric_only
7174                         or np.issubdtype(var.dtype, np.number)
7175                         or var.dtype == np.bool_
7176                     ):
7177                         variables[name] = var.quantile(
7178                             q,
7179                             dim=reduce_dims,
7180                             method=method,
7181                             keep_attrs=keep_attrs,
7182                             skipna=skipna,
7183                         )
7184 
7185             else:
7186                 variables[name] = var
7187 
7188         # construct the new dataset
7189         coord_names = {k for k in self.coords if k in variables}
7190         indexes = {k: v for k, v in self._indexes.items() if k in variables}
7191         if keep_attrs is None:
7192             keep_attrs = _get_keep_attrs(default=False)
7193         attrs = self.attrs if keep_attrs else None
7194         new = self._replace_with_new_dims(
7195             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
7196         )
7197         return new.assign_coords(quantile=q)
7198 
7199     def rank(
7200         self: T_Dataset,
7201         dim: Hashable,
7202         pct: bool = False,
7203         keep_attrs: bool | None = None,
7204     ) -> T_Dataset:
7205         """Ranks the data.
7206 
7207         Equal values are assigned a rank that is the average of the ranks that
7208         would have been otherwise assigned to all of the values within
7209         that set.
7210         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.
7211 
7212         NaNs in the input array are returned as NaNs.
7213 
7214         The `bottleneck` library is required.
7215 
7216         Parameters
7217         ----------
7218         dim : Hashable
7219             Dimension over which to compute rank.
7220         pct : bool, default: False
7221             If True, compute percentage ranks, otherwise compute integer ranks.
7222         keep_attrs : bool or None, optional
7223             If True, the dataset's attributes (`attrs`) will be copied from
7224             the original object to the new one.  If False, the new
7225             object will be returned without attributes.
7226 
7227         Returns
7228         -------
7229         ranked : Dataset
7230             Variables that do not depend on `dim` are dropped.
7231         """
7232         if not OPTIONS["use_bottleneck"]:
7233             raise RuntimeError(
7234                 "rank requires bottleneck to be enabled."
7235                 " Call `xr.set_options(use_bottleneck=True)` to enable it."
7236             )
7237 
7238         if dim not in self.dims:
7239             raise ValueError(f"Dataset does not contain the dimension: {dim}")
7240 
7241         variables = {}
7242         for name, var in self.variables.items():
7243             if name in self.data_vars:
7244                 if dim in var.dims:
7245                     variables[name] = var.rank(dim, pct=pct)
7246             else:
7247                 variables[name] = var
7248 
7249         coord_names = set(self.coords)
7250         if keep_attrs is None:
7251             keep_attrs = _get_keep_attrs(default=False)
7252         attrs = self.attrs if keep_attrs else None
7253         return self._replace(variables, coord_names, attrs=attrs)
7254 
7255     def differentiate(
7256         self: T_Dataset,
7257         coord: Hashable,
7258         edge_order: Literal[1, 2] = 1,
7259         datetime_unit: DatetimeUnitOptions | None = None,
7260     ) -> T_Dataset:
7261         """ Differentiate with the second order accurate central
7262         differences.
7263 
7264         .. note::
7265             This feature is limited to simple cartesian geometry, i.e. coord
7266             must be one dimensional.
7267 
7268         Parameters
7269         ----------
7270         coord : Hashable
7271             The coordinate to be used to compute the gradient.
7272         edge_order : {1, 2}, default: 1
7273             N-th order accurate differences at the boundaries.
7274         datetime_unit : None or {"Y", "M", "W", "D", "h", "m", "s", "ms", \
7275             "us", "ns", "ps", "fs", "as", None}, default: None
7276             Unit to compute gradient. Only valid for datetime coordinate.
7277 
7278         Returns
7279         -------
7280         differentiated: Dataset
7281 
7282         See also
7283         --------
7284         numpy.gradient: corresponding numpy function
7285         """
7286         from xarray.core.variable import Variable
7287 
7288         if coord not in self.variables and coord not in self.dims:
7289             raise ValueError(f"Coordinate {coord} does not exist.")
7290 
7291         coord_var = self[coord].variable
7292         if coord_var.ndim != 1:
7293             raise ValueError(
7294                 "Coordinate {} must be 1 dimensional but is {}"
7295                 " dimensional".format(coord, coord_var.ndim)
7296             )
7297 
7298         dim = coord_var.dims[0]
7299         if _contains_datetime_like_objects(coord_var):
7300             if coord_var.dtype.kind in "mM" and datetime_unit is None:
7301                 datetime_unit = cast(
7302                     "DatetimeUnitOptions", np.datetime_data(coord_var.dtype)[0]
7303                 )
7304             elif datetime_unit is None:
7305                 datetime_unit = "s"  # Default to seconds for cftime objects
7306             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)
7307 
7308         variables = {}
7309         for k, v in self.variables.items():
7310             if k in self.data_vars and dim in v.dims and k not in self.coords:
7311                 if _contains_datetime_like_objects(v):
7312                     v = v._to_numeric(datetime_unit=datetime_unit)
7313                 grad = duck_array_ops.gradient(
7314                     v.data,
7315                     coord_var.data,
7316                     edge_order=edge_order,
7317                     axis=v.get_axis_num(dim),
7318                 )
7319                 variables[k] = Variable(v.dims, grad)
7320             else:
7321                 variables[k] = v
7322         return self._replace(variables)
7323 
7324     def integrate(
7325         self: T_Dataset,
7326         coord: Hashable | Sequence[Hashable],
7327         datetime_unit: DatetimeUnitOptions = None,
7328     ) -> T_Dataset:
7329         """Integrate along the given coordinate using the trapezoidal rule.
7330 
7331         .. note::
7332             This feature is limited to simple cartesian geometry, i.e. coord
7333             must be one dimensional.
7334 
7335         Parameters
7336         ----------
7337         coord : hashable, or sequence of hashable
7338             Coordinate(s) used for the integration.
7339         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7340                         'ps', 'fs', 'as', None}, optional
7341             Specify the unit if datetime coordinate is used.
7342 
7343         Returns
7344         -------
7345         integrated : Dataset
7346 
7347         See also
7348         --------
7349         DataArray.integrate
7350         numpy.trapz : corresponding numpy function
7351 
7352         Examples
7353         --------
7354         >>> ds = xr.Dataset(
7355         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7356         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7357         ... )
7358         >>> ds
7359         <xarray.Dataset>
7360         Dimensions:  (x: 4)
7361         Coordinates:
7362           * x        (x) int64 0 1 2 3
7363             y        (x) int64 1 7 3 5
7364         Data variables:
7365             a        (x) int64 5 5 6 6
7366             b        (x) int64 1 2 1 0
7367         >>> ds.integrate("x")
7368         <xarray.Dataset>
7369         Dimensions:  ()
7370         Data variables:
7371             a        float64 16.5
7372             b        float64 3.5
7373         >>> ds.integrate("y")
7374         <xarray.Dataset>
7375         Dimensions:  ()
7376         Data variables:
7377             a        float64 20.0
7378             b        float64 4.0
7379         """
7380         if not isinstance(coord, (list, tuple)):
7381             coord = (coord,)
7382         result = self
7383         for c in coord:
7384             result = result._integrate_one(c, datetime_unit=datetime_unit)
7385         return result
7386 
7387     def _integrate_one(self, coord, datetime_unit=None, cumulative=False):
7388         from xarray.core.variable import Variable
7389 
7390         if coord not in self.variables and coord not in self.dims:
7391             raise ValueError(f"Coordinate {coord} does not exist.")
7392 
7393         coord_var = self[coord].variable
7394         if coord_var.ndim != 1:
7395             raise ValueError(
7396                 "Coordinate {} must be 1 dimensional but is {}"
7397                 " dimensional".format(coord, coord_var.ndim)
7398             )
7399 
7400         dim = coord_var.dims[0]
7401         if _contains_datetime_like_objects(coord_var):
7402             if coord_var.dtype.kind in "mM" and datetime_unit is None:
7403                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
7404             elif datetime_unit is None:
7405                 datetime_unit = "s"  # Default to seconds for cftime objects
7406             coord_var = coord_var._replace(
7407                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)
7408             )
7409 
7410         variables = {}
7411         coord_names = set()
7412         for k, v in self.variables.items():
7413             if k in self.coords:
7414                 if dim not in v.dims or cumulative:
7415                     variables[k] = v
7416                     coord_names.add(k)
7417             else:
7418                 if k in self.data_vars and dim in v.dims:
7419                     if _contains_datetime_like_objects(v):
7420                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)
7421                     if cumulative:
7422                         integ = duck_array_ops.cumulative_trapezoid(
7423                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7424                         )
7425                         v_dims = v.dims
7426                     else:
7427                         integ = duck_array_ops.trapz(
7428                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7429                         )
7430                         v_dims = list(v.dims)
7431                         v_dims.remove(dim)
7432                     variables[k] = Variable(v_dims, integ)
7433                 else:
7434                     variables[k] = v
7435         indexes = {k: v for k, v in self._indexes.items() if k in variables}
7436         return self._replace_with_new_dims(
7437             variables, coord_names=coord_names, indexes=indexes
7438         )
7439 
7440     def cumulative_integrate(
7441         self: T_Dataset,
7442         coord: Hashable | Sequence[Hashable],
7443         datetime_unit: DatetimeUnitOptions = None,
7444     ) -> T_Dataset:
7445         """Integrate along the given coordinate using the trapezoidal rule.
7446 
7447         .. note::
7448             This feature is limited to simple cartesian geometry, i.e. coord
7449             must be one dimensional.
7450 
7451             The first entry of the cumulative integral of each variable is always 0, in
7452             order to keep the length of the dimension unchanged between input and
7453             output.
7454 
7455         Parameters
7456         ----------
7457         coord : hashable, or sequence of hashable
7458             Coordinate(s) used for the integration.
7459         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7460                         'ps', 'fs', 'as', None}, optional
7461             Specify the unit if datetime coordinate is used.
7462 
7463         Returns
7464         -------
7465         integrated : Dataset
7466 
7467         See also
7468         --------
7469         DataArray.cumulative_integrate
7470         scipy.integrate.cumulative_trapezoid : corresponding scipy function
7471 
7472         Examples
7473         --------
7474         >>> ds = xr.Dataset(
7475         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7476         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7477         ... )
7478         >>> ds
7479         <xarray.Dataset>
7480         Dimensions:  (x: 4)
7481         Coordinates:
7482           * x        (x) int64 0 1 2 3
7483             y        (x) int64 1 7 3 5
7484         Data variables:
7485             a        (x) int64 5 5 6 6
7486             b        (x) int64 1 2 1 0
7487         >>> ds.cumulative_integrate("x")
7488         <xarray.Dataset>
7489         Dimensions:  (x: 4)
7490         Coordinates:
7491           * x        (x) int64 0 1 2 3
7492             y        (x) int64 1 7 3 5
7493         Data variables:
7494             a        (x) float64 0.0 5.0 10.5 16.5
7495             b        (x) float64 0.0 1.5 3.0 3.5
7496         >>> ds.cumulative_integrate("y")
7497         <xarray.Dataset>
7498         Dimensions:  (x: 4)
7499         Coordinates:
7500           * x        (x) int64 0 1 2 3
7501             y        (x) int64 1 7 3 5
7502         Data variables:
7503             a        (x) float64 0.0 30.0 8.0 20.0
7504             b        (x) float64 0.0 9.0 3.0 4.0
7505         """
7506         if not isinstance(coord, (list, tuple)):
7507             coord = (coord,)
7508         result = self
7509         for c in coord:
7510             result = result._integrate_one(
7511                 c, datetime_unit=datetime_unit, cumulative=True
7512             )
7513         return result
7514 
7515     @property
7516     def real(self: T_Dataset) -> T_Dataset:
7517         """
7518         The real part of each data variable.
7519 
7520         See Also
7521         --------
7522         numpy.ndarray.real
7523         """
7524         return self.map(lambda x: x.real, keep_attrs=True)
7525 
7526     @property
7527     def imag(self: T_Dataset) -> T_Dataset:
7528         """
7529         The imaginary part of each data variable.
7530 
7531         See Also
7532         --------
7533         numpy.ndarray.imag
7534         """
7535         return self.map(lambda x: x.imag, keep_attrs=True)
7536 
7537     plot = utils.UncachedAccessor(DatasetPlotAccessor)
7538 
7539     def filter_by_attrs(self: T_Dataset, **kwargs) -> T_Dataset:
7540         """Returns a ``Dataset`` with variables that match specific conditions.
7541 
7542         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned
7543         containing only the variables for which all the filter tests pass.
7544         These tests are either ``key=value`` for which the attribute ``key``
7545         has the exact value ``value`` or the callable passed into
7546         ``key=callable`` returns True. The callable will be passed a single
7547         value, either the value of the attribute ``key`` or ``None`` if the
7548         DataArray does not have an attribute with the name ``key``.
7549 
7550         Parameters
7551         ----------
7552         **kwargs
7553             key : str
7554                 Attribute name.
7555             value : callable or obj
7556                 If value is a callable, it should return a boolean in the form
7557                 of bool = func(attr) where attr is da.attrs[key].
7558                 Otherwise, value will be compared to the each
7559                 DataArray's attrs[key].
7560 
7561         Returns
7562         -------
7563         new : Dataset
7564             New dataset with variables filtered by attribute.
7565 
7566         Examples
7567         --------
7568         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)
7569         >>> precip = 10 * np.random.rand(2, 2, 3)
7570         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
7571         >>> lat = [[42.25, 42.21], [42.63, 42.59]]
7572         >>> dims = ["x", "y", "time"]
7573         >>> temp_attr = dict(standard_name="air_potential_temperature")
7574         >>> precip_attr = dict(standard_name="convective_precipitation_flux")
7575 
7576         >>> ds = xr.Dataset(
7577         ...     dict(
7578         ...         temperature=(dims, temp, temp_attr),
7579         ...         precipitation=(dims, precip, precip_attr),
7580         ...     ),
7581         ...     coords=dict(
7582         ...         lon=(["x", "y"], lon),
7583         ...         lat=(["x", "y"], lat),
7584         ...         time=pd.date_range("2014-09-06", periods=3),
7585         ...         reference_time=pd.Timestamp("2014-09-05"),
7586         ...     ),
7587         ... )
7588 
7589         Get variables matching a specific standard_name:
7590 
7591         >>> ds.filter_by_attrs(standard_name="convective_precipitation_flux")
7592         <xarray.Dataset>
7593         Dimensions:         (x: 2, y: 2, time: 3)
7594         Coordinates:
7595             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7596             lat             (x, y) float64 42.25 42.21 42.63 42.59
7597           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7598             reference_time  datetime64[ns] 2014-09-05
7599         Dimensions without coordinates: x, y
7600         Data variables:
7601             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7602 
7603         Get all variables that have a standard_name attribute:
7604 
7605         >>> standard_name = lambda v: v is not None
7606         >>> ds.filter_by_attrs(standard_name=standard_name)
7607         <xarray.Dataset>
7608         Dimensions:         (x: 2, y: 2, time: 3)
7609         Coordinates:
7610             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7611             lat             (x, y) float64 42.25 42.21 42.63 42.59
7612           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7613             reference_time  datetime64[ns] 2014-09-05
7614         Dimensions without coordinates: x, y
7615         Data variables:
7616             temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
7617             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7618 
7619         """
7620         selection = []
7621         for var_name, variable in self.variables.items():
7622             has_value_flag = False
7623             for attr_name, pattern in kwargs.items():
7624                 attr_value = variable.attrs.get(attr_name)
7625                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:
7626                     has_value_flag = True
7627                 else:
7628                     has_value_flag = False
7629                     break
7630             if has_value_flag is True:
7631                 selection.append(var_name)
7632         return self[selection]
7633 
7634     def unify_chunks(self: T_Dataset) -> T_Dataset:
7635         """Unify chunk size along all chunked dimensions of this Dataset.
7636 
7637         Returns
7638         -------
7639         Dataset with consistent chunk sizes for all dask-array variables
7640 
7641         See Also
7642         --------
7643         dask.array.core.unify_chunks
7644         """
7645 
7646         return unify_chunks(self)[0]
7647 
7648     def map_blocks(
7649         self,
7650         func: Callable[..., T_Xarray],
7651         args: Sequence[Any] = (),
7652         kwargs: Mapping[str, Any] | None = None,
7653         template: DataArray | Dataset | None = None,
7654     ) -> T_Xarray:
7655         """
7656         Apply a function to each block of this Dataset.
7657 
7658         .. warning::
7659             This method is experimental and its signature may change.
7660 
7661         Parameters
7662         ----------
7663         func : callable
7664             User-provided function that accepts a Dataset as its first
7665             parameter. The function will receive a subset or 'block' of this Dataset (see below),
7666             corresponding to one chunk along each chunked dimension. ``func`` will be
7667             executed as ``func(subset_dataset, *subset_args, **kwargs)``.
7668 
7669             This function must return either a single DataArray or a single Dataset.
7670 
7671             This function cannot add a new chunked dimension.
7672         args : sequence
7673             Passed to func after unpacking and subsetting any xarray objects by blocks.
7674             xarray objects in args must be aligned with obj, otherwise an error is raised.
7675         kwargs : Mapping or None
7676             Passed verbatim to func after unpacking. xarray objects, if any, will not be
7677             subset to blocks. Passing dask collections in kwargs is not allowed.
7678         template : DataArray, Dataset or None, optional
7679             xarray object representing the final result after compute is called. If not provided,
7680             the function will be first run on mocked-up data, that looks like this object but
7681             has sizes 0, to determine properties of the returned object such as dtype,
7682             variable names, attributes, new dimensions and new indexes (if any).
7683             ``template`` must be provided if the function changes the size of existing dimensions.
7684             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
7685             ``attrs`` set by ``func`` will be ignored.
7686 
7687         Returns
7688         -------
7689         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
7690         function.
7691 
7692         Notes
7693         -----
7694         This function is designed for when ``func`` needs to manipulate a whole xarray object
7695         subset to each block. Each block is loaded into memory. In the more common case where
7696         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
7697 
7698         If none of the variables in this object is backed by dask arrays, calling this function is
7699         equivalent to calling ``func(obj, *args, **kwargs)``.
7700 
7701         See Also
7702         --------
7703         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
7704         xarray.DataArray.map_blocks
7705 
7706         Examples
7707         --------
7708         Calculate an anomaly from climatology using ``.groupby()``. Using
7709         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
7710         its indices, and its methods like ``.groupby()``.
7711 
7712         >>> def calculate_anomaly(da, groupby_type="time.month"):
7713         ...     gb = da.groupby(groupby_type)
7714         ...     clim = gb.mean(dim="time")
7715         ...     return gb - clim
7716         ...
7717         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
7718         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
7719         >>> np.random.seed(123)
7720         >>> array = xr.DataArray(
7721         ...     np.random.rand(len(time)),
7722         ...     dims=["time"],
7723         ...     coords={"time": time, "month": month},
7724         ... ).chunk()
7725         >>> ds = xr.Dataset({"a": array})
7726         >>> ds.map_blocks(calculate_anomaly, template=ds).compute()
7727         <xarray.Dataset>
7728         Dimensions:  (time: 24)
7729         Coordinates:
7730           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7731             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
7732         Data variables:
7733             a        (time) float64 0.1289 0.1132 -0.0856 ... 0.2287 0.1906 -0.05901
7734 
7735         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
7736         to the function being applied in ``xr.map_blocks()``:
7737 
7738         >>> ds.map_blocks(
7739         ...     calculate_anomaly,
7740         ...     kwargs={"groupby_type": "time.year"},
7741         ...     template=ds,
7742         ... )
7743         <xarray.Dataset>
7744         Dimensions:  (time: 24)
7745         Coordinates:
7746           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7747             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
7748         Data variables:
7749             a        (time) float64 dask.array<chunksize=(24,), meta=np.ndarray>
7750         """
7751         from xarray.core.parallel import map_blocks
7752 
7753         return map_blocks(func, self, args, kwargs, template)
7754 
7755     def polyfit(
7756         self: T_Dataset,
7757         dim: Hashable,
7758         deg: int,
7759         skipna: bool | None = None,
7760         rcond: float | None = None,
7761         w: Hashable | Any = None,
7762         full: bool = False,
7763         cov: bool | Literal["unscaled"] = False,
7764     ) -> T_Dataset:
7765         """
7766         Least squares polynomial fit.
7767 
7768         This replicates the behaviour of `numpy.polyfit` but differs by skipping
7769         invalid values when `skipna = True`.
7770 
7771         Parameters
7772         ----------
7773         dim : hashable
7774             Coordinate along which to fit the polynomials.
7775         deg : int
7776             Degree of the fitting polynomial.
7777         skipna : bool or None, optional
7778             If True, removes all invalid values before fitting each 1D slices of the array.
7779             Default is True if data is stored in a dask.array or if there is any
7780             invalid values, False otherwise.
7781         rcond : float or None, optional
7782             Relative condition number to the fit.
7783         w : hashable or Any, optional
7784             Weights to apply to the y-coordinate of the sample points.
7785             Can be an array-like object or the name of a coordinate in the dataset.
7786         full : bool, default: False
7787             Whether to return the residuals, matrix rank and singular values in addition
7788             to the coefficients.
7789         cov : bool or "unscaled", default: False
7790             Whether to return to the covariance matrix in addition to the coefficients.
7791             The matrix is not scaled if `cov='unscaled'`.
7792 
7793         Returns
7794         -------
7795         polyfit_results : Dataset
7796             A single dataset which contains (for each "var" in the input dataset):
7797 
7798             [var]_polyfit_coefficients
7799                 The coefficients of the best fit for each variable in this dataset.
7800             [var]_polyfit_residuals
7801                 The residuals of the least-square computation for each variable (only included if `full=True`)
7802                 When the matrix rank is deficient, np.nan is returned.
7803             [dim]_matrix_rank
7804                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7805                 The rank is computed ignoring the NaN values that might be skipped.
7806             [dim]_singular_values
7807                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7808             [var]_polyfit_covariance
7809                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
7810 
7811         Warns
7812         -----
7813         RankWarning
7814             The rank of the coefficient matrix in the least-squares fit is deficient.
7815             The warning is not raised with in-memory (not dask) data and `full=True`.
7816 
7817         See Also
7818         --------
7819         numpy.polyfit
7820         numpy.polyval
7821         xarray.polyval
7822         """
7823         from xarray.core.dataarray import DataArray
7824 
7825         variables = {}
7826         skipna_da = skipna
7827 
7828         x = get_clean_interp_index(self, dim, strict=False)
7829         xname = f"{self[dim].name}_"
7830         order = int(deg) + 1
7831         lhs = np.vander(x, order)
7832 
7833         if rcond is None:
7834             rcond = (
7835                 x.shape[0] * np.core.finfo(x.dtype).eps  # type: ignore[attr-defined]
7836             )
7837 
7838         # Weights:
7839         if w is not None:
7840             if isinstance(w, Hashable):
7841                 w = self.coords[w]
7842             w = np.asarray(w)
7843             if w.ndim != 1:
7844                 raise TypeError("Expected a 1-d array for weights.")
7845             if w.shape[0] != lhs.shape[0]:
7846                 raise TypeError(f"Expected w and {dim} to have the same length")
7847             lhs *= w[:, np.newaxis]
7848 
7849         # Scaling
7850         scale = np.sqrt((lhs * lhs).sum(axis=0))
7851         lhs /= scale
7852 
7853         degree_dim = utils.get_temp_dimname(self.dims, "degree")
7854 
7855         rank = np.linalg.matrix_rank(lhs)
7856 
7857         if full:
7858             rank = DataArray(rank, name=xname + "matrix_rank")
7859             variables[rank.name] = rank
7860             _sing = np.linalg.svd(lhs, compute_uv=False)
7861             sing = DataArray(
7862                 _sing,
7863                 dims=(degree_dim,),
7864                 coords={degree_dim: np.arange(rank - 1, -1, -1)},
7865                 name=xname + "singular_values",
7866             )
7867             variables[sing.name] = sing
7868 
7869         for name, da in self.data_vars.items():
7870             if dim not in da.dims:
7871                 continue
7872 
7873             if is_duck_dask_array(da.data) and (
7874                 rank != order or full or skipna is None
7875             ):
7876                 # Current algorithm with dask and skipna=False neither supports
7877                 # deficient ranks nor does it output the "full" info (issue dask/dask#6516)
7878                 skipna_da = True
7879             elif skipna is None:
7880                 skipna_da = bool(np.any(da.isnull()))
7881 
7882             dims_to_stack = [dimname for dimname in da.dims if dimname != dim]
7883             stacked_coords: dict[Hashable, DataArray] = {}
7884             if dims_to_stack:
7885                 stacked_dim = utils.get_temp_dimname(dims_to_stack, "stacked")
7886                 rhs = da.transpose(dim, *dims_to_stack).stack(
7887                     {stacked_dim: dims_to_stack}
7888                 )
7889                 stacked_coords = {stacked_dim: rhs[stacked_dim]}
7890                 scale_da = scale[:, np.newaxis]
7891             else:
7892                 rhs = da
7893                 scale_da = scale
7894 
7895             if w is not None:
7896                 rhs *= w[:, np.newaxis]
7897 
7898             with warnings.catch_warnings():
7899                 if full:  # Copy np.polyfit behavior
7900                     warnings.simplefilter("ignore", np.RankWarning)
7901                 else:  # Raise only once per variable
7902                     warnings.simplefilter("once", np.RankWarning)
7903 
7904                 coeffs, residuals = duck_array_ops.least_squares(
7905                     lhs, rhs.data, rcond=rcond, skipna=skipna_da
7906                 )
7907 
7908             if isinstance(name, str):
7909                 name = f"{name}_"
7910             else:
7911                 # Thus a ReprObject => polyfit was called on a DataArray
7912                 name = ""
7913 
7914             coeffs = DataArray(
7915                 coeffs / scale_da,
7916                 dims=[degree_dim] + list(stacked_coords.keys()),
7917                 coords={degree_dim: np.arange(order)[::-1], **stacked_coords},
7918                 name=name + "polyfit_coefficients",
7919             )
7920             if dims_to_stack:
7921                 coeffs = coeffs.unstack(stacked_dim)
7922             variables[coeffs.name] = coeffs
7923 
7924             if full or (cov is True):
7925                 residuals = DataArray(
7926                     residuals if dims_to_stack else residuals.squeeze(),
7927                     dims=list(stacked_coords.keys()),
7928                     coords=stacked_coords,
7929                     name=name + "polyfit_residuals",
7930                 )
7931                 if dims_to_stack:
7932                     residuals = residuals.unstack(stacked_dim)
7933                 variables[residuals.name] = residuals
7934 
7935             if cov:
7936                 Vbase = np.linalg.inv(np.dot(lhs.T, lhs))
7937                 Vbase /= np.outer(scale, scale)
7938                 if cov == "unscaled":
7939                     fac = 1
7940                 else:
7941                     if x.shape[0] <= order:
7942                         raise ValueError(
7943                             "The number of data points must exceed order to scale the covariance matrix."
7944                         )
7945                     fac = residuals / (x.shape[0] - order)
7946                 covariance = DataArray(Vbase, dims=("cov_i", "cov_j")) * fac
7947                 variables[name + "polyfit_covariance"] = covariance
7948 
7949         return type(self)(data_vars=variables, attrs=self.attrs.copy())
7950 
7951     def pad(
7952         self: T_Dataset,
7953         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,
7954         mode: PadModeOptions = "constant",
7955         stat_length: int
7956         | tuple[int, int]
7957         | Mapping[Any, tuple[int, int]]
7958         | None = None,
7959         constant_values: (
7960             float | tuple[float, float] | Mapping[Any, tuple[float, float]] | None
7961         ) = None,
7962         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
7963         reflect_type: PadReflectOptions = None,
7964         keep_attrs: bool | None = None,
7965         **pad_width_kwargs: Any,
7966     ) -> T_Dataset:
7967         """Pad this dataset along one or more dimensions.
7968 
7969         .. warning::
7970             This function is experimental and its behaviour is likely to change
7971             especially regarding padding of dimension coordinates (or IndexVariables).
7972 
7973         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
7974         coordinates will be padded with the same mode, otherwise coordinates
7975         are padded using the "constant" mode with fill_value dtypes.NA.
7976 
7977         Parameters
7978         ----------
7979         pad_width : mapping of hashable to tuple of int
7980             Mapping with the form of {dim: (pad_before, pad_after)}
7981             describing the number of values padded along each dimension.
7982             {dim: pad} is a shortcut for pad_before = pad_after = pad
7983         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
7984             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
7985             How to pad the DataArray (taken from numpy docs):
7986 
7987             - "constant": Pads with a constant value.
7988             - "edge": Pads with the edge values of array.
7989             - "linear_ramp": Pads with the linear ramp between end_value and the
7990               array edge value.
7991             - "maximum": Pads with the maximum value of all or part of the
7992               vector along each axis.
7993             - "mean": Pads with the mean value of all or part of the
7994               vector along each axis.
7995             - "median": Pads with the median value of all or part of the
7996               vector along each axis.
7997             - "minimum": Pads with the minimum value of all or part of the
7998               vector along each axis.
7999             - "reflect": Pads with the reflection of the vector mirrored on
8000               the first and last values of the vector along each axis.
8001             - "symmetric": Pads with the reflection of the vector mirrored
8002               along the edge of the array.
8003             - "wrap": Pads with the wrap of the vector along the axis.
8004               The first values are used to pad the end and the
8005               end values are used to pad the beginning.
8006 
8007         stat_length : int, tuple or mapping of hashable to tuple, default: None
8008             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
8009             values at edge of each axis used to calculate the statistic value.
8010             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
8011             statistic lengths along each dimension.
8012             ((before, after),) yields same before and after statistic lengths
8013             for each dimension.
8014             (stat_length,) or int is a shortcut for before = after = statistic
8015             length for all axes.
8016             Default is ``None``, to use the entire axis.
8017         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
8018             Used in 'constant'.  The values to set the padded values for each
8019             axis.
8020             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
8021             pad constants along each dimension.
8022             ``((before, after),)`` yields same before and after constants for each
8023             dimension.
8024             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
8025             all dimensions.
8026             Default is 0.
8027         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
8028             Used in 'linear_ramp'.  The values used for the ending value of the
8029             linear_ramp and that will form the edge of the padded array.
8030             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
8031             end values along each dimension.
8032             ``((before, after),)`` yields same before and after end values for each
8033             axis.
8034             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
8035             all axes.
8036             Default is 0.
8037         reflect_type : {"even", "odd", None}, optional
8038             Used in "reflect", and "symmetric".  The "even" style is the
8039             default with an unaltered reflection around the edge value.  For
8040             the "odd" style, the extended part of the array is created by
8041             subtracting the reflected values from two times the edge value.
8042         keep_attrs : bool or None, optional
8043             If True, the attributes (``attrs``) will be copied from the
8044             original object to the new one. If False, the new object
8045             will be returned without attributes.
8046         **pad_width_kwargs
8047             The keyword arguments form of ``pad_width``.
8048             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
8049 
8050         Returns
8051         -------
8052         padded : Dataset
8053             Dataset with the padded coordinates and data.
8054 
8055         See Also
8056         --------
8057         Dataset.shift, Dataset.roll, Dataset.bfill, Dataset.ffill, numpy.pad, dask.array.pad
8058 
8059         Notes
8060         -----
8061         By default when ``mode="constant"`` and ``constant_values=None``, integer types will be
8062         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion
8063         specify ``constant_values=np.nan``
8064 
8065         Padding coordinates will drop their corresponding index (if any) and will reset default
8066         indexes for dimension coordinates.
8067 
8068         Examples
8069         --------
8070         >>> ds = xr.Dataset({"foo": ("x", range(5))})
8071         >>> ds.pad(x=(1, 2))
8072         <xarray.Dataset>
8073         Dimensions:  (x: 8)
8074         Dimensions without coordinates: x
8075         Data variables:
8076             foo      (x) float64 nan 0.0 1.0 2.0 3.0 4.0 nan nan
8077         """
8078         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, "pad")
8079 
8080         if mode in ("edge", "reflect", "symmetric", "wrap"):
8081             coord_pad_mode = mode
8082             coord_pad_options = {
8083                 "stat_length": stat_length,
8084                 "constant_values": constant_values,
8085                 "end_values": end_values,
8086                 "reflect_type": reflect_type,
8087             }
8088         else:
8089             coord_pad_mode = "constant"
8090             coord_pad_options = {}
8091 
8092         if keep_attrs is None:
8093             keep_attrs = _get_keep_attrs(default=True)
8094 
8095         variables = {}
8096 
8097         # keep indexes that won't be affected by pad and drop all other indexes
8098         xindexes = self.xindexes
8099         pad_dims = set(pad_width)
8100         indexes = {}
8101         for k, idx in xindexes.items():
8102             if not pad_dims.intersection(xindexes.get_all_dims(k)):
8103                 indexes[k] = idx
8104 
8105         for name, var in self.variables.items():
8106             var_pad_width = {k: v for k, v in pad_width.items() if k in var.dims}
8107             if not var_pad_width:
8108                 variables[name] = var
8109             elif name in self.data_vars:
8110                 variables[name] = var.pad(
8111                     pad_width=var_pad_width,
8112                     mode=mode,
8113                     stat_length=stat_length,
8114                     constant_values=constant_values,
8115                     end_values=end_values,
8116                     reflect_type=reflect_type,
8117                     keep_attrs=keep_attrs,
8118                 )
8119             else:
8120                 variables[name] = var.pad(
8121                     pad_width=var_pad_width,
8122                     mode=coord_pad_mode,
8123                     keep_attrs=keep_attrs,
8124                     **coord_pad_options,  # type: ignore[arg-type]
8125                 )
8126                 # reset default index of dimension coordinates
8127                 if (name,) == var.dims:
8128                     dim_var = {name: variables[name]}
8129                     index = PandasIndex.from_variables(dim_var, options={})
8130                     index_vars = index.create_variables(dim_var)
8131                     indexes[name] = index
8132                     variables[name] = index_vars[name]
8133 
8134         attrs = self._attrs if keep_attrs else None
8135         return self._replace_with_new_dims(variables, indexes=indexes, attrs=attrs)
8136 
8137     def idxmin(
8138         self: T_Dataset,
8139         dim: Hashable | None = None,
8140         skipna: bool | None = None,
8141         fill_value: Any = xrdtypes.NA,
8142         keep_attrs: bool | None = None,
8143     ) -> T_Dataset:
8144         """Return the coordinate label of the minimum value along a dimension.
8145 
8146         Returns a new `Dataset` named after the dimension with the values of
8147         the coordinate labels along that dimension corresponding to minimum
8148         values along that dimension.
8149 
8150         In comparison to :py:meth:`~Dataset.argmin`, this returns the
8151         coordinate label while :py:meth:`~Dataset.argmin` returns the index.
8152 
8153         Parameters
8154         ----------
8155         dim : Hashable, optional
8156             Dimension over which to apply `idxmin`.  This is optional for 1D
8157             variables, but required for variables with 2 or more dimensions.
8158         skipna : bool or None, optional
8159             If True, skip missing values (as marked by NaN). By default, only
8160             skips missing values for ``float``, ``complex``, and ``object``
8161             dtypes; other dtypes either do not have a sentinel missing value
8162             (``int``) or ``skipna=True`` has not been implemented
8163             (``datetime64`` or ``timedelta64``).
8164         fill_value : Any, default: NaN
8165             Value to be filled in case all of the values along a dimension are
8166             null.  By default this is NaN.  The fill value and result are
8167             automatically converted to a compatible dtype if possible.
8168             Ignored if ``skipna`` is False.
8169         keep_attrs : bool or None, optional
8170             If True, the attributes (``attrs``) will be copied from the
8171             original object to the new one. If False, the new object
8172             will be returned without attributes.
8173 
8174         Returns
8175         -------
8176         reduced : Dataset
8177             New `Dataset` object with `idxmin` applied to its data and the
8178             indicated dimension removed.
8179 
8180         See Also
8181         --------
8182         DataArray.idxmin, Dataset.idxmax, Dataset.min, Dataset.argmin
8183 
8184         Examples
8185         --------
8186         >>> array1 = xr.DataArray(
8187         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
8188         ... )
8189         >>> array2 = xr.DataArray(
8190         ...     [
8191         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
8192         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
8193         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
8194         ...     ],
8195         ...     dims=["y", "x"],
8196         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
8197         ... )
8198         >>> ds = xr.Dataset({"int": array1, "float": array2})
8199         >>> ds.min(dim="x")
8200         <xarray.Dataset>
8201         Dimensions:  (y: 3)
8202         Coordinates:
8203           * y        (y) int64 -1 0 1
8204         Data variables:
8205             int      int64 -2
8206             float    (y) float64 -2.0 -4.0 1.0
8207         >>> ds.argmin(dim="x")
8208         <xarray.Dataset>
8209         Dimensions:  (y: 3)
8210         Coordinates:
8211           * y        (y) int64 -1 0 1
8212         Data variables:
8213             int      int64 4
8214             float    (y) int64 4 0 2
8215         >>> ds.idxmin(dim="x")
8216         <xarray.Dataset>
8217         Dimensions:  (y: 3)
8218         Coordinates:
8219           * y        (y) int64 -1 0 1
8220         Data variables:
8221             int      <U1 'e'
8222             float    (y) object 'e' 'a' 'c'
8223         """
8224         return self.map(
8225             methodcaller(
8226                 "idxmin",
8227                 dim=dim,
8228                 skipna=skipna,
8229                 fill_value=fill_value,
8230                 keep_attrs=keep_attrs,
8231             )
8232         )
8233 
8234     def idxmax(
8235         self: T_Dataset,
8236         dim: Hashable | None = None,
8237         skipna: bool | None = None,
8238         fill_value: Any = xrdtypes.NA,
8239         keep_attrs: bool | None = None,
8240     ) -> T_Dataset:
8241         """Return the coordinate label of the maximum value along a dimension.
8242 
8243         Returns a new `Dataset` named after the dimension with the values of
8244         the coordinate labels along that dimension corresponding to maximum
8245         values along that dimension.
8246 
8247         In comparison to :py:meth:`~Dataset.argmax`, this returns the
8248         coordinate label while :py:meth:`~Dataset.argmax` returns the index.
8249 
8250         Parameters
8251         ----------
8252         dim : str, optional
8253             Dimension over which to apply `idxmax`.  This is optional for 1D
8254             variables, but required for variables with 2 or more dimensions.
8255         skipna : bool or None, optional
8256             If True, skip missing values (as marked by NaN). By default, only
8257             skips missing values for ``float``, ``complex``, and ``object``
8258             dtypes; other dtypes either do not have a sentinel missing value
8259             (``int``) or ``skipna=True`` has not been implemented
8260             (``datetime64`` or ``timedelta64``).
8261         fill_value : Any, default: NaN
8262             Value to be filled in case all of the values along a dimension are
8263             null.  By default this is NaN.  The fill value and result are
8264             automatically converted to a compatible dtype if possible.
8265             Ignored if ``skipna`` is False.
8266         keep_attrs : bool or None, optional
8267             If True, the attributes (``attrs``) will be copied from the
8268             original object to the new one. If False, the new object
8269             will be returned without attributes.
8270 
8271         Returns
8272         -------
8273         reduced : Dataset
8274             New `Dataset` object with `idxmax` applied to its data and the
8275             indicated dimension removed.
8276 
8277         See Also
8278         --------
8279         DataArray.idxmax, Dataset.idxmin, Dataset.max, Dataset.argmax
8280 
8281         Examples
8282         --------
8283         >>> array1 = xr.DataArray(
8284         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
8285         ... )
8286         >>> array2 = xr.DataArray(
8287         ...     [
8288         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
8289         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
8290         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
8291         ...     ],
8292         ...     dims=["y", "x"],
8293         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
8294         ... )
8295         >>> ds = xr.Dataset({"int": array1, "float": array2})
8296         >>> ds.max(dim="x")
8297         <xarray.Dataset>
8298         Dimensions:  (y: 3)
8299         Coordinates:
8300           * y        (y) int64 -1 0 1
8301         Data variables:
8302             int      int64 2
8303             float    (y) float64 2.0 2.0 1.0
8304         >>> ds.argmax(dim="x")
8305         <xarray.Dataset>
8306         Dimensions:  (y: 3)
8307         Coordinates:
8308           * y        (y) int64 -1 0 1
8309         Data variables:
8310             int      int64 1
8311             float    (y) int64 0 2 2
8312         >>> ds.idxmax(dim="x")
8313         <xarray.Dataset>
8314         Dimensions:  (y: 3)
8315         Coordinates:
8316           * y        (y) int64 -1 0 1
8317         Data variables:
8318             int      <U1 'b'
8319             float    (y) object 'a' 'c' 'c'
8320         """
8321         return self.map(
8322             methodcaller(
8323                 "idxmax",
8324                 dim=dim,
8325                 skipna=skipna,
8326                 fill_value=fill_value,
8327                 keep_attrs=keep_attrs,
8328             )
8329         )
8330 
8331     def argmin(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
8332         """Indices of the minima of the member variables.
8333 
8334         If there are multiple minima, the indices of the first one found will be
8335         returned.
8336 
8337         Parameters
8338         ----------
8339         dim : Hashable, optional
8340             The dimension over which to find the minimum. By default, finds minimum over
8341             all dimensions - for now returning an int for backward compatibility, but
8342             this is deprecated, in future will be an error, since DataArray.argmin will
8343             return a dict with indices for all dimensions, which does not make sense for
8344             a Dataset.
8345         keep_attrs : bool, optional
8346             If True, the attributes (`attrs`) will be copied from the original
8347             object to the new one.  If False (default), the new object will be
8348             returned without attributes.
8349         skipna : bool, optional
8350             If True, skip missing values (as marked by NaN). By default, only
8351             skips missing values for float dtypes; other dtypes either do not
8352             have a sentinel missing value (int) or skipna=True has not been
8353             implemented (object, datetime64 or timedelta64).
8354 
8355         Returns
8356         -------
8357         result : Dataset
8358 
8359         See Also
8360         --------
8361         DataArray.argmin
8362         """
8363         if dim is None:
8364             warnings.warn(
8365                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8366                 "dim changes to return a dict of indices of each dimension, for "
8367                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8368                 "since we don't return a dict of Datasets.",
8369                 DeprecationWarning,
8370                 stacklevel=2,
8371             )
8372         if (
8373             dim is None
8374             or (not isinstance(dim, Sequence) and dim is not ...)
8375             or isinstance(dim, str)
8376         ):
8377             # Return int index if single dimension is passed, and is not part of a
8378             # sequence
8379             argmin_func = getattr(duck_array_ops, "argmin")
8380             return self.reduce(
8381                 argmin_func, dim=None if dim is None else [dim], **kwargs
8382             )
8383         else:
8384             raise ValueError(
8385                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8386                 "dicts cannot be contained in a Dataset, so cannot call "
8387                 "Dataset.argmin() with a sequence or ... for dim"
8388             )
8389 
8390     def argmax(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
8391         """Indices of the maxima of the member variables.
8392 
8393         If there are multiple maxima, the indices of the first one found will be
8394         returned.
8395 
8396         Parameters
8397         ----------
8398         dim : str, optional
8399             The dimension over which to find the maximum. By default, finds maximum over
8400             all dimensions - for now returning an int for backward compatibility, but
8401             this is deprecated, in future will be an error, since DataArray.argmax will
8402             return a dict with indices for all dimensions, which does not make sense for
8403             a Dataset.
8404         keep_attrs : bool, optional
8405             If True, the attributes (`attrs`) will be copied from the original
8406             object to the new one.  If False (default), the new object will be
8407             returned without attributes.
8408         skipna : bool, optional
8409             If True, skip missing values (as marked by NaN). By default, only
8410             skips missing values for float dtypes; other dtypes either do not
8411             have a sentinel missing value (int) or skipna=True has not been
8412             implemented (object, datetime64 or timedelta64).
8413 
8414         Returns
8415         -------
8416         result : Dataset
8417 
8418         See Also
8419         --------
8420         DataArray.argmax
8421 
8422         """
8423         if dim is None:
8424             warnings.warn(
8425                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8426                 "dim changes to return a dict of indices of each dimension, for "
8427                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8428                 "since we don't return a dict of Datasets.",
8429                 DeprecationWarning,
8430                 stacklevel=2,
8431             )
8432         if (
8433             dim is None
8434             or (not isinstance(dim, Sequence) and dim is not ...)
8435             or isinstance(dim, str)
8436         ):
8437             # Return int index if single dimension is passed, and is not part of a
8438             # sequence
8439             argmax_func = getattr(duck_array_ops, "argmax")
8440             return self.reduce(
8441                 argmax_func, dim=None if dim is None else [dim], **kwargs
8442             )
8443         else:
8444             raise ValueError(
8445                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8446                 "dicts cannot be contained in a Dataset, so cannot call "
8447                 "Dataset.argmin() with a sequence or ... for dim"
8448             )
8449 
8450     def query(
8451         self: T_Dataset,
8452         queries: Mapping[Any, Any] | None = None,
8453         parser: QueryParserOptions = "pandas",
8454         engine: QueryEngineOptions = None,
8455         missing_dims: ErrorOptionsWithWarn = "raise",
8456         **queries_kwargs: Any,
8457     ) -> T_Dataset:
8458         """Return a new dataset with each array indexed along the specified
8459         dimension(s), where the indexers are given as strings containing
8460         Python expressions to be evaluated against the data variables in the
8461         dataset.
8462 
8463         Parameters
8464         ----------
8465         queries : dict-like, optional
8466             A dict-like with keys matching dimensions and values given by strings
8467             containing Python expressions to be evaluated against the data variables
8468             in the dataset. The expressions will be evaluated using the pandas
8469             eval() function, and can contain any valid Python expressions but cannot
8470             contain any Python statements.
8471         parser : {"pandas", "python"}, default: "pandas"
8472             The parser to use to construct the syntax tree from the expression.
8473             The default of 'pandas' parses code slightly different than standard
8474             Python. Alternatively, you can parse an expression using the 'python'
8475             parser to retain strict Python semantics.
8476         engine : {"python", "numexpr", None}, default: None
8477             The engine used to evaluate the expression. Supported engines are:
8478 
8479             - None: tries to use numexpr, falls back to python
8480             - "numexpr": evaluates expressions using numexpr
8481             - "python": performs operations as if you had eval’d in top level python
8482 
8483         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
8484             What to do if dimensions that should be selected from are not present in the
8485             Dataset:
8486 
8487             - "raise": raise an exception
8488             - "warn": raise a warning, and ignore the missing dimensions
8489             - "ignore": ignore the missing dimensions
8490 
8491         **queries_kwargs : {dim: query, ...}, optional
8492             The keyword arguments form of ``queries``.
8493             One of queries or queries_kwargs must be provided.
8494 
8495         Returns
8496         -------
8497         obj : Dataset
8498             A new Dataset with the same contents as this dataset, except each
8499             array and dimension is indexed by the results of the appropriate
8500             queries.
8501 
8502         See Also
8503         --------
8504         Dataset.isel
8505         pandas.eval
8506 
8507         Examples
8508         --------
8509         >>> a = np.arange(0, 5, 1)
8510         >>> b = np.linspace(0, 1, 5)
8511         >>> ds = xr.Dataset({"a": ("x", a), "b": ("x", b)})
8512         >>> ds
8513         <xarray.Dataset>
8514         Dimensions:  (x: 5)
8515         Dimensions without coordinates: x
8516         Data variables:
8517             a        (x) int64 0 1 2 3 4
8518             b        (x) float64 0.0 0.25 0.5 0.75 1.0
8519         >>> ds.query(x="a > 2")
8520         <xarray.Dataset>
8521         Dimensions:  (x: 2)
8522         Dimensions without coordinates: x
8523         Data variables:
8524             a        (x) int64 3 4
8525             b        (x) float64 0.75 1.0
8526         """
8527 
8528         # allow queries to be given either as a dict or as kwargs
8529         queries = either_dict_or_kwargs(queries, queries_kwargs, "query")
8530 
8531         # check queries
8532         for dim, expr in queries.items():
8533             if not isinstance(expr, str):
8534                 msg = f"expr for dim {dim} must be a string to be evaluated, {type(expr)} given"
8535                 raise ValueError(msg)
8536 
8537         # evaluate the queries to create the indexers
8538         indexers = {
8539             dim: pd.eval(expr, resolvers=[self], parser=parser, engine=engine)
8540             for dim, expr in queries.items()
8541         }
8542 
8543         # apply the selection
8544         return self.isel(indexers, missing_dims=missing_dims)
8545 
8546     def curvefit(
8547         self: T_Dataset,
8548         coords: str | DataArray | Iterable[str | DataArray],
8549         func: Callable[..., Any],
8550         reduce_dims: Dims = None,
8551         skipna: bool = True,
8552         p0: dict[str, Any] | None = None,
8553         bounds: dict[str, Any] | None = None,
8554         param_names: Sequence[str] | None = None,
8555         kwargs: dict[str, Any] | None = None,
8556     ) -> T_Dataset:
8557         """
8558         Curve fitting optimization for arbitrary functions.
8559 
8560         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
8561 
8562         Parameters
8563         ----------
8564         coords : hashable, DataArray, or sequence of hashable or DataArray
8565             Independent coordinate(s) over which to perform the curve fitting. Must share
8566             at least one dimension with the calling object. When fitting multi-dimensional
8567             functions, supply `coords` as a sequence in the same order as arguments in
8568             `func`. To fit along existing dimensions of the calling object, `coords` can
8569             also be specified as a str or sequence of strs.
8570         func : callable
8571             User specified function in the form `f(x, *params)` which returns a numpy
8572             array of length `len(x)`. `params` are the fittable parameters which are optimized
8573             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
8574             coordinates, e.g. `f((x0, x1), *params)`.
8575         reduce_dims : str, Iterable of Hashable or None, optional
8576             Additional dimension(s) over which to aggregate while fitting. For example,
8577             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
8578             aggregate all lat and lon points and fit the specified function along the
8579             time dimension.
8580         skipna : bool, default: True
8581             Whether to skip missing values when fitting. Default is True.
8582         p0 : dict-like, optional
8583             Optional dictionary of parameter names to initial guesses passed to the
8584             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
8585             be assigned initial values following the default scipy behavior.
8586         bounds : dict-like, optional
8587             Optional dictionary of parameter names to bounding values passed to the
8588             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
8589             will be unbounded following the default scipy behavior.
8590         param_names : sequence of hashable, optional
8591             Sequence of names for the fittable parameters of `func`. If not supplied,
8592             this will be automatically determined by arguments of `func`. `param_names`
8593             should be manually supplied when fitting a function that takes a variable
8594             number of parameters.
8595         **kwargs : optional
8596             Additional keyword arguments to passed to scipy curve_fit.
8597 
8598         Returns
8599         -------
8600         curvefit_results : Dataset
8601             A single dataset which contains:
8602 
8603             [var]_curvefit_coefficients
8604                 The coefficients of the best fit.
8605             [var]_curvefit_covariance
8606                 The covariance matrix of the coefficient estimates.
8607 
8608         See Also
8609         --------
8610         Dataset.polyfit
8611         scipy.optimize.curve_fit
8612         """
8613         from scipy.optimize import curve_fit
8614 
8615         from xarray.core.alignment import broadcast
8616         from xarray.core.computation import apply_ufunc
8617         from xarray.core.dataarray import _THIS_ARRAY, DataArray
8618 
8619         if p0 is None:
8620             p0 = {}
8621         if bounds is None:
8622             bounds = {}
8623         if kwargs is None:
8624             kwargs = {}
8625 
8626         reduce_dims_: list[Hashable]
8627         if not reduce_dims:
8628             reduce_dims_ = []
8629         elif isinstance(reduce_dims, str) or not isinstance(reduce_dims, Iterable):
8630             reduce_dims_ = [reduce_dims]
8631         else:
8632             reduce_dims_ = list(reduce_dims)
8633 
8634         if (
8635             isinstance(coords, str)
8636             or isinstance(coords, DataArray)
8637             or not isinstance(coords, Iterable)
8638         ):
8639             coords = [coords]
8640         coords_: Sequence[DataArray] = [
8641             self[coord] if isinstance(coord, str) else coord for coord in coords
8642         ]
8643 
8644         # Determine whether any coords are dims on self
8645         for coord in coords_:
8646             reduce_dims_ += [c for c in self.dims if coord.equals(self[c])]
8647         reduce_dims_ = list(set(reduce_dims_))
8648         preserved_dims = list(set(self.dims) - set(reduce_dims_))
8649         if not reduce_dims_:
8650             raise ValueError(
8651                 "No arguments to `coords` were identified as a dimension on the calling "
8652                 "object, and no dims were supplied to `reduce_dims`. This would result "
8653                 "in fitting on scalar data."
8654             )
8655 
8656         # Broadcast all coords with each other
8657         coords_ = broadcast(*coords_)
8658         coords_ = [
8659             coord.broadcast_like(self, exclude=preserved_dims) for coord in coords_
8660         ]
8661 
8662         params, func_args = _get_func_args(func, param_names)
8663         param_defaults, bounds_defaults = _initialize_curvefit_params(
8664             params, p0, bounds, func_args
8665         )
8666         n_params = len(params)
8667         kwargs.setdefault("p0", [param_defaults[p] for p in params])
8668         kwargs.setdefault(
8669             "bounds",
8670             [
8671                 [bounds_defaults[p][0] for p in params],
8672                 [bounds_defaults[p][1] for p in params],
8673             ],
8674         )
8675 
8676         def _wrapper(Y, *coords_, **kwargs):
8677             # Wrap curve_fit with raveled coordinates and pointwise NaN handling
8678             x = np.vstack([c.ravel() for c in coords_])
8679             y = Y.ravel()
8680             if skipna:
8681                 mask = np.all([np.any(~np.isnan(x), axis=0), ~np.isnan(y)], axis=0)
8682                 x = x[:, mask]
8683                 y = y[mask]
8684                 if not len(y):
8685                     popt = np.full([n_params], np.nan)
8686                     pcov = np.full([n_params, n_params], np.nan)
8687                     return popt, pcov
8688             x = np.squeeze(x)
8689             popt, pcov = curve_fit(func, x, y, **kwargs)
8690             return popt, pcov
8691 
8692         result = type(self)()
8693         for name, da in self.data_vars.items():
8694             if name is _THIS_ARRAY:
8695                 name = ""
8696             else:
8697                 name = f"{str(name)}_"
8698 
8699             popt, pcov = apply_ufunc(
8700                 _wrapper,
8701                 da,
8702                 *coords_,
8703                 vectorize=True,
8704                 dask="parallelized",
8705                 input_core_dims=[reduce_dims_ for d in range(len(coords_) + 1)],
8706                 output_core_dims=[["param"], ["cov_i", "cov_j"]],
8707                 dask_gufunc_kwargs={
8708                     "output_sizes": {
8709                         "param": n_params,
8710                         "cov_i": n_params,
8711                         "cov_j": n_params,
8712                     },
8713                 },
8714                 output_dtypes=(np.float64, np.float64),
8715                 exclude_dims=set(reduce_dims_),
8716                 kwargs=kwargs,
8717             )
8718             result[name + "curvefit_coefficients"] = popt
8719             result[name + "curvefit_covariance"] = pcov
8720 
8721         result = result.assign_coords(
8722             {"param": params, "cov_i": params, "cov_j": params}
8723         )
8724         result.attrs = self.attrs.copy()
8725 
8726         return result
8727 
8728     def drop_duplicates(
8729         self: T_Dataset,
8730         dim: Hashable | Iterable[Hashable],
8731         keep: Literal["first", "last", False] = "first",
8732     ) -> T_Dataset:
8733         """Returns a new Dataset with duplicate dimension values removed.
8734 
8735         Parameters
8736         ----------
8737         dim : dimension label or labels
8738             Pass `...` to drop duplicates along all dimensions.
8739         keep : {"first", "last", False}, default: "first"
8740             Determines which duplicates (if any) to keep.
8741             - ``"first"`` : Drop duplicates except for the first occurrence.
8742             - ``"last"`` : Drop duplicates except for the last occurrence.
8743             - False : Drop all duplicates.
8744 
8745         Returns
8746         -------
8747         Dataset
8748 
8749         See Also
8750         --------
8751         DataArray.drop_duplicates
8752         """
8753         if isinstance(dim, str):
8754             dims: Iterable = (dim,)
8755         elif dim is ...:
8756             dims = self.dims
8757         elif not isinstance(dim, Iterable):
8758             dims = [dim]
8759         else:
8760             dims = dim
8761 
8762         missing_dims = set(dims) - set(self.dims)
8763         if missing_dims:
8764             raise ValueError(f"'{missing_dims}' not found in dimensions")
8765 
8766         indexes = {dim: ~self.get_index(dim).duplicated(keep=keep) for dim in dims}
8767         return self.isel(indexes)
8768 
8769     def convert_calendar(
8770         self: T_Dataset,
8771         calendar: CFCalendar,
8772         dim: Hashable = "time",
8773         align_on: Literal["date", "year", None] = None,
8774         missing: Any | None = None,
8775         use_cftime: bool | None = None,
8776     ) -> T_Dataset:
8777         """Convert the Dataset to another calendar.
8778 
8779         Only converts the individual timestamps, does not modify any data except
8780         in dropping invalid/surplus dates or inserting missing dates.
8781 
8782         If the source and target calendars are either no_leap, all_leap or a
8783         standard type, only the type of the time array is modified.
8784         When converting to a leap year from a non-leap year, the 29th of February
8785         is removed from the array. In the other direction the 29th of February
8786         will be missing in the output, unless `missing` is specified,
8787         in which case that value is inserted.
8788 
8789         For conversions involving `360_day` calendars, see Notes.
8790 
8791         This method is safe to use with sub-daily data as it doesn't touch the
8792         time part of the timestamps.
8793 
8794         Parameters
8795         ---------
8796         calendar : str
8797             The target calendar name.
8798         dim : Hashable, default: "time"
8799             Name of the time coordinate.
8800         align_on : {None, 'date', 'year'}, optional
8801             Must be specified when either source or target is a `360_day` calendar,
8802             ignored otherwise. See Notes.
8803         missing : Any or None, optional
8804             By default, i.e. if the value is None, this method will simply attempt
8805             to convert the dates in the source calendar to the same dates in the
8806             target calendar, and drop any of those that are not possible to
8807             represent.  If a value is provided, a new time coordinate will be
8808             created in the target calendar with the same frequency as the original
8809             time coordinate; for any dates that are not present in the source, the
8810             data will be filled with this value.  Note that using this mode requires
8811             that the source data have an inferable frequency; for more information
8812             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
8813             target calendar combinations, this could result in many missing values, see notes.
8814         use_cftime : bool or None, optional
8815             Whether to use cftime objects in the output, only used if `calendar`
8816             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
8817             If True, the new time axis uses cftime objects.
8818             If None (default), it uses :py:class:`numpy.datetime64` values if the
8819             date range permits it, and :py:class:`cftime.datetime` objects if not.
8820             If False, it uses :py:class:`numpy.datetime64`  or fails.
8821 
8822         Returns
8823         -------
8824         Dataset
8825             Copy of the dataarray with the time coordinate converted to the
8826             target calendar. If 'missing' was None (default), invalid dates in
8827             the new calendar are dropped, but missing dates are not inserted.
8828             If `missing` was given, the new data is reindexed to have a time axis
8829             with the same frequency as the source, but in the new calendar; any
8830             missing datapoints are filled with `missing`.
8831 
8832         Notes
8833         -----
8834         Passing a value to `missing` is only usable if the source's time coordinate as an
8835         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
8836         if the target coordinate, generated from this frequency, has dates equivalent to the
8837         source. It is usually **not** appropriate to use this mode with:
8838 
8839         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
8840         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
8841             or 'mH' where 24 % m != 0).
8842 
8843         If one of the source or target calendars is `"360_day"`, `align_on` must
8844         be specified and two options are offered.
8845 
8846         - "year"
8847             The dates are translated according to their relative position in the year,
8848             ignoring their original month and day information, meaning that the
8849             missing/surplus days are added/removed at regular intervals.
8850 
8851             From a `360_day` to a standard calendar, the output will be missing the
8852             following dates (day of year in parentheses):
8853 
8854             To a leap year:
8855                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
8856                 September 31st (275) and November 30th (335).
8857             To a non-leap year:
8858                 February 6th (36), April 19th (109), July 2nd (183),
8859                 September 12th (255), November 25th (329).
8860 
8861             From a standard calendar to a `"360_day"`, the following dates in the
8862             source array will be dropped:
8863 
8864             From a leap year:
8865                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
8866                 September 31st (275), December 1st (336)
8867             From a non-leap year:
8868                 February 6th (37), April 20th (110), July 2nd (183),
8869                 September 13th (256), November 25th (329)
8870 
8871             This option is best used on daily and subdaily data.
8872 
8873         - "date"
8874             The month/day information is conserved and invalid dates are dropped
8875             from the output. This means that when converting from a `"360_day"` to a
8876             standard calendar, all 31st (Jan, March, May, July, August, October and
8877             December) will be missing as there is no equivalent dates in the
8878             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
8879             will be dropped as there are no equivalent dates in a standard calendar.
8880 
8881             This option is best used with data on a frequency coarser than daily.
8882         """
8883         return convert_calendar(
8884             self,
8885             calendar,
8886             dim=dim,
8887             align_on=align_on,
8888             missing=missing,
8889             use_cftime=use_cftime,
8890         )
8891 
8892     def interp_calendar(
8893         self: T_Dataset,
8894         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
8895         dim: Hashable = "time",
8896     ) -> T_Dataset:
8897         """Interpolates the Dataset to another calendar based on decimal year measure.
8898 
8899         Each timestamp in `source` and `target` are first converted to their decimal
8900         year equivalent then `source` is interpolated on the target coordinate.
8901         The decimal year of a timestamp is its year plus its sub-year component
8902         converted to the fraction of its year. For example "2000-03-01 12:00" is
8903         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
8904 
8905         This method should only be used when the time (HH:MM:SS) information of
8906         time coordinate is not important.
8907 
8908         Parameters
8909         ----------
8910         target: DataArray or DatetimeIndex or CFTimeIndex
8911             The target time coordinate of a valid dtype
8912             (np.datetime64 or cftime objects)
8913         dim : Hashable, default: "time"
8914             The time coordinate name.
8915 
8916         Return
8917         ------
8918         DataArray
8919             The source interpolated on the decimal years of target,
8920         """
8921         return interp_calendar(self, target, dim=dim)
8922 
8923     def groupby(
8924         self,
8925         group: Hashable | DataArray | IndexVariable,
8926         squeeze: bool = True,
8927         restore_coord_dims: bool = False,
8928     ) -> DatasetGroupBy:
8929         """Returns a DatasetGroupBy object for performing grouped operations.
8930 
8931         Parameters
8932         ----------
8933         group : Hashable, DataArray or IndexVariable
8934             Array whose unique values should be used to group this array. If a
8935             string, must be the name of a variable contained in this dataset.
8936         squeeze : bool, default: True
8937             If "group" is a dimension of any arrays in this dataset, `squeeze`
8938             controls whether the subarrays have a dimension of length 1 along
8939             that dimension or if the dimension is squeezed out.
8940         restore_coord_dims : bool, default: False
8941             If True, also restore the dimension order of multi-dimensional
8942             coordinates.
8943 
8944         Returns
8945         -------
8946         grouped : DatasetGroupBy
8947             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
8948             iterated over in the form of `(unique_value, grouped_array)` pairs.
8949 
8950         See Also
8951         --------
8952         :ref:`groupby`
8953             Users guide explanation of how to group and bin data.
8954         Dataset.groupby_bins
8955         DataArray.groupby
8956         core.groupby.DatasetGroupBy
8957         pandas.DataFrame.groupby
8958         Dataset.resample
8959         DataArray.resample
8960         """
8961         from xarray.core.groupby import (
8962             DatasetGroupBy,
8963             ResolvedUniqueGrouper,
8964             UniqueGrouper,
8965             _validate_groupby_squeeze,
8966         )
8967 
8968         _validate_groupby_squeeze(squeeze)
8969         rgrouper = ResolvedUniqueGrouper(UniqueGrouper(), group, self)
8970 
8971         return DatasetGroupBy(
8972             self,
8973             (rgrouper,),
8974             squeeze=squeeze,
8975             restore_coord_dims=restore_coord_dims,
8976         )
8977 
8978     def groupby_bins(
8979         self,
8980         group: Hashable | DataArray | IndexVariable,
8981         bins: ArrayLike,
8982         right: bool = True,
8983         labels: ArrayLike | None = None,
8984         precision: int = 3,
8985         include_lowest: bool = False,
8986         squeeze: bool = True,
8987         restore_coord_dims: bool = False,
8988     ) -> DatasetGroupBy:
8989         """Returns a DatasetGroupBy object for performing grouped operations.
8990 
8991         Rather than using all unique values of `group`, the values are discretized
8992         first by applying `pandas.cut` [1]_ to `group`.
8993 
8994         Parameters
8995         ----------
8996         group : Hashable, DataArray or IndexVariable
8997             Array whose binned values should be used to group this array. If a
8998             string, must be the name of a variable contained in this dataset.
8999         bins : int or array-like
9000             If bins is an int, it defines the number of equal-width bins in the
9001             range of x. However, in this case, the range of x is extended by .1%
9002             on each side to include the min or max values of x. If bins is a
9003             sequence it defines the bin edges allowing for non-uniform bin
9004             width. No extension of the range of x is done in this case.
9005         right : bool, default: True
9006             Indicates whether the bins include the rightmost edge or not. If
9007             right == True (the default), then the bins [1,2,3,4] indicate
9008             (1,2], (2,3], (3,4].
9009         labels : array-like or bool, default: None
9010             Used as labels for the resulting bins. Must be of the same length as
9011             the resulting bins. If False, string bin labels are assigned by
9012             `pandas.cut`.
9013         precision : int, default: 3
9014             The precision at which to store and display the bins labels.
9015         include_lowest : bool, default: False
9016             Whether the first interval should be left-inclusive or not.
9017         squeeze : bool, default: True
9018             If "group" is a dimension of any arrays in this dataset, `squeeze`
9019             controls whether the subarrays have a dimension of length 1 along
9020             that dimension or if the dimension is squeezed out.
9021         restore_coord_dims : bool, default: False
9022             If True, also restore the dimension order of multi-dimensional
9023             coordinates.
9024 
9025         Returns
9026         -------
9027         grouped : DatasetGroupBy
9028             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
9029             iterated over in the form of `(unique_value, grouped_array)` pairs.
9030             The name of the group has the added suffix `_bins` in order to
9031             distinguish it from the original variable.
9032 
9033         See Also
9034         --------
9035         :ref:`groupby`
9036             Users guide explanation of how to group and bin data.
9037         Dataset.groupby
9038         DataArray.groupby_bins
9039         core.groupby.DatasetGroupBy
9040         pandas.DataFrame.groupby
9041 
9042         References
9043         ----------
9044         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
9045         """
9046         from xarray.core.groupby import (
9047             BinGrouper,
9048             DatasetGroupBy,
9049             ResolvedBinGrouper,
9050             _validate_groupby_squeeze,
9051         )
9052 
9053         _validate_groupby_squeeze(squeeze)
9054         grouper = BinGrouper(
9055             bins=bins,
9056             cut_kwargs={
9057                 "right": right,
9058                 "labels": labels,
9059                 "precision": precision,
9060                 "include_lowest": include_lowest,
9061             },
9062         )
9063         rgrouper = ResolvedBinGrouper(grouper, group, self)
9064 
9065         return DatasetGroupBy(
9066             self,
9067             (rgrouper,),
9068             squeeze=squeeze,
9069             restore_coord_dims=restore_coord_dims,
9070         )
9071 
9072     def weighted(self, weights: DataArray) -> DatasetWeighted:
9073         """
9074         Weighted Dataset operations.
9075 
9076         Parameters
9077         ----------
9078         weights : DataArray
9079             An array of weights associated with the values in this Dataset.
9080             Each value in the data contributes to the reduction operation
9081             according to its associated weight.
9082 
9083         Notes
9084         -----
9085         ``weights`` must be a DataArray and cannot contain missing values.
9086         Missing values can be replaced by ``weights.fillna(0)``.
9087 
9088         Returns
9089         -------
9090         core.weighted.DatasetWeighted
9091 
9092         See Also
9093         --------
9094         DataArray.weighted
9095         """
9096         from xarray.core.weighted import DatasetWeighted
9097 
9098         return DatasetWeighted(self, weights)
9099 
9100     def rolling(
9101         self,
9102         dim: Mapping[Any, int] | None = None,
9103         min_periods: int | None = None,
9104         center: bool | Mapping[Any, bool] = False,
9105         **window_kwargs: int,
9106     ) -> DatasetRolling:
9107         """
9108         Rolling window object for Datasets.
9109 
9110         Parameters
9111         ----------
9112         dim : dict, optional
9113             Mapping from the dimension name to create the rolling iterator
9114             along (e.g. `time`) to its moving window size.
9115         min_periods : int or None, default: None
9116             Minimum number of observations in window required to have a value
9117             (otherwise result is NA). The default, None, is equivalent to
9118             setting min_periods equal to the size of the window.
9119         center : bool or Mapping to int, default: False
9120             Set the labels at the center of the window.
9121         **window_kwargs : optional
9122             The keyword arguments form of ``dim``.
9123             One of dim or window_kwargs must be provided.
9124 
9125         Returns
9126         -------
9127         core.rolling.DatasetRolling
9128 
9129         See Also
9130         --------
9131         core.rolling.DatasetRolling
9132         DataArray.rolling
9133         """
9134         from xarray.core.rolling import DatasetRolling
9135 
9136         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
9137         return DatasetRolling(self, dim, min_periods=min_periods, center=center)
9138 
9139     def coarsen(
9140         self,
9141         dim: Mapping[Any, int] | None = None,
9142         boundary: CoarsenBoundaryOptions = "exact",
9143         side: SideOptions | Mapping[Any, SideOptions] = "left",
9144         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
9145         **window_kwargs: int,
9146     ) -> DatasetCoarsen:
9147         """
9148         Coarsen object for Datasets.
9149 
9150         Parameters
9151         ----------
9152         dim : mapping of hashable to int, optional
9153             Mapping from the dimension name to the window size.
9154         boundary : {"exact", "trim", "pad"}, default: "exact"
9155             If 'exact', a ValueError will be raised if dimension size is not a
9156             multiple of the window size. If 'trim', the excess entries are
9157             dropped. If 'pad', NA will be padded.
9158         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
9159         coord_func : str or mapping of hashable to str, default: "mean"
9160             function (name) that is applied to the coordinates,
9161             or a mapping from coordinate name to function (name).
9162 
9163         Returns
9164         -------
9165         core.rolling.DatasetCoarsen
9166 
9167         See Also
9168         --------
9169         core.rolling.DatasetCoarsen
9170         DataArray.coarsen
9171         """
9172         from xarray.core.rolling import DatasetCoarsen
9173 
9174         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
9175         return DatasetCoarsen(
9176             self,
9177             dim,
9178             boundary=boundary,
9179             side=side,
9180             coord_func=coord_func,
9181         )
9182 
9183     def resample(
9184         self,
9185         indexer: Mapping[Any, str] | None = None,
9186         skipna: bool | None = None,
9187         closed: SideOptions | None = None,
9188         label: SideOptions | None = None,
9189         base: int | None = None,
9190         offset: pd.Timedelta | datetime.timedelta | str | None = None,
9191         origin: str | DatetimeLike = "start_day",
9192         keep_attrs: bool | None = None,
9193         loffset: datetime.timedelta | str | None = None,
9194         restore_coord_dims: bool | None = None,
9195         **indexer_kwargs: str,
9196     ) -> DatasetResample:
9197         """Returns a Resample object for performing resampling operations.
9198 
9199         Handles both downsampling and upsampling. The resampled
9200         dimension must be a datetime-like coordinate. If any intervals
9201         contain no values from the original object, they will be given
9202         the value ``NaN``.
9203 
9204         Parameters
9205         ----------
9206         indexer : Mapping of Hashable to str, optional
9207             Mapping from the dimension name to resample frequency [1]_. The
9208             dimension must be datetime-like.
9209         skipna : bool, optional
9210             Whether to skip missing values when aggregating in downsampling.
9211         closed : {"left", "right"}, optional
9212             Side of each interval to treat as closed.
9213         label : {"left", "right"}, optional
9214             Side of each interval to use for labeling.
9215         base : int, optional
9216             For frequencies that evenly subdivide 1 day, the "origin" of the
9217             aggregated intervals. For example, for "24H" frequency, base could
9218             range from 0 through 23.
9219         origin : {'epoch', 'start', 'start_day', 'end', 'end_day'}, pd.Timestamp, datetime.datetime, np.datetime64, or cftime.datetime, default 'start_day'
9220             The datetime on which to adjust the grouping. The timezone of origin
9221             must match the timezone of the index.
9222 
9223             If a datetime is not used, these values are also supported:
9224             - 'epoch': `origin` is 1970-01-01
9225             - 'start': `origin` is the first value of the timeseries
9226             - 'start_day': `origin` is the first day at midnight of the timeseries
9227             - 'end': `origin` is the last value of the timeseries
9228             - 'end_day': `origin` is the ceiling midnight of the last day
9229         offset : pd.Timedelta, datetime.timedelta, or str, default is None
9230             An offset timedelta added to the origin.
9231         loffset : timedelta or str, optional
9232             Offset used to adjust the resampled time labels. Some pandas date
9233             offset strings are supported.
9234         restore_coord_dims : bool, optional
9235             If True, also restore the dimension order of multi-dimensional
9236             coordinates.
9237         **indexer_kwargs : str
9238             The keyword arguments form of ``indexer``.
9239             One of indexer or indexer_kwargs must be provided.
9240 
9241         Returns
9242         -------
9243         resampled : core.resample.DataArrayResample
9244             This object resampled.
9245 
9246         See Also
9247         --------
9248         DataArray.resample
9249         pandas.Series.resample
9250         pandas.DataFrame.resample
9251         Dataset.groupby
9252         DataArray.groupby
9253 
9254         References
9255         ----------
9256         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
9257         """
9258         from xarray.core.resample import DatasetResample
9259 
9260         return self._resample(
9261             resample_cls=DatasetResample,
9262             indexer=indexer,
9263             skipna=skipna,
9264             closed=closed,
9265             label=label,
9266             base=base,
9267             offset=offset,
9268             origin=origin,
9269             keep_attrs=keep_attrs,
9270             loffset=loffset,
9271             restore_coord_dims=restore_coord_dims,
9272             **indexer_kwargs,
9273         )

</code><intended_behavior>Implement a backend-agnostic interface such that the Dataset class can operate with different parallel computing frameworks beyond dask. Maintain current dask functionalities and ensure new parallel backends can be introduced with minimal friction.</intended_behavior>, <file>xarray/core/variable.py</file>
<class>None</class>
<method>None</method>
<code>
1 from __future__ import annotations
2 
3 import copy
4 import itertools
5 import math
6 import numbers
7 import warnings
8 from collections.abc import Hashable, Iterable, Mapping, Sequence
9 from datetime import timedelta
10 from typing import TYPE_CHECKING, Any, Callable, Literal, NoReturn
11 
12 import numpy as np
13 import pandas as pd
14 from numpy.typing import ArrayLike
15 from packaging.version import Version
16 
17 import xarray as xr  # only for Dataset and DataArray
18 from xarray.core import common, dtypes, duck_array_ops, indexing, nputils, ops, utils
19 from xarray.core.arithmetic import VariableArithmetic
20 from xarray.core.common import AbstractArray
21 from xarray.core.indexing import (
22     BasicIndexer,
23     OuterIndexer,
24     PandasIndexingAdapter,
25     VectorizedIndexer,
26     as_indexable,
27 )
28 from xarray.core.options import OPTIONS, _get_keep_attrs
29 from xarray.core.pycompat import (
30     array_type,
31     integer_types,
32     is_0d_dask_array,
33     is_duck_dask_array,
34 )
35 from xarray.core.utils import (
36     Frozen,
37     NdimSizeLenMixin,
38     OrderedSet,
39     _default,
40     decode_numpy_dict_values,
41     drop_dims_from_indexers,
42     either_dict_or_kwargs,
43     ensure_us_time_resolution,
44     infix_dims,
45     is_duck_array,
46     maybe_coerce_to_str,
47 )
48 
49 NON_NUMPY_SUPPORTED_ARRAY_TYPES = (
50     indexing.ExplicitlyIndexed,
51     pd.Index,
52 )
53 # https://github.com/python/mypy/issues/224
54 BASIC_INDEXING_TYPES = integer_types + (slice,)
55 
56 if TYPE_CHECKING:
57     from xarray.core.types import (
58         Dims,
59         ErrorOptionsWithWarn,
60         PadModeOptions,
61         PadReflectOptions,
62         QuantileMethods,
63         T_Variable,
64     )
65 
66 NON_NANOSECOND_WARNING = (
67     "Converting non-nanosecond precision {case} values to nanosecond precision. "
68     "This behavior can eventually be relaxed in xarray, as it is an artifact from "
69     "pandas which is now beginning to support non-nanosecond precision values. "
70     "This warning is caused by passing non-nanosecond np.datetime64 or "
71     "np.timedelta64 values to the DataArray or Variable constructor; it can be "
72     "silenced by converting the values to nanosecond precision ahead of time."
73 )
74 
75 
76 class MissingDimensionsError(ValueError):
77     """Error class used when we can't safely guess a dimension name."""
78 
79     # inherits from ValueError for backward compatibility
80     # TODO: move this to an xarray.exceptions module?
81 
82 
83 def as_variable(obj, name=None) -> Variable | IndexVariable:
84     """Convert an object into a Variable.
85 
86     Parameters
87     ----------
88     obj : object
89         Object to convert into a Variable.
90 
91         - If the object is already a Variable, return a shallow copy.
92         - Otherwise, if the object has 'dims' and 'data' attributes, convert
93           it into a new Variable.
94         - If all else fails, attempt to convert the object into a Variable by
95           unpacking it into the arguments for creating a new Variable.
96     name : str, optional
97         If provided:
98 
99         - `obj` can be a 1D array, which is assumed to label coordinate values
100           along a dimension of this given name.
101         - Variables with name matching one of their dimensions are converted
102           into `IndexVariable` objects.
103 
104     Returns
105     -------
106     var : Variable
107         The newly created variable.
108 
109     """
110     from xarray.core.dataarray import DataArray
111 
112     # TODO: consider extending this method to automatically handle Iris and
113     if isinstance(obj, DataArray):
114         # extract the primary Variable from DataArrays
115         obj = obj.variable
116 
117     if isinstance(obj, Variable):
118         obj = obj.copy(deep=False)
119     elif isinstance(obj, tuple):
120         if isinstance(obj[1], DataArray):
121             raise TypeError(
122                 "Using a DataArray object to construct a variable is"
123                 " ambiguous, please extract the data using the .data property."
124             )
125         try:
126             obj = Variable(*obj)
127         except (TypeError, ValueError) as error:
128             # use .format() instead of % because it handles tuples consistently
129             raise error.__class__(
130                 "Could not convert tuple of form "
131                 "(dims, data[, attrs, encoding]): "
132                 "{} to Variable.".format(obj)
133             )
134     elif utils.is_scalar(obj):
135         obj = Variable([], obj)
136     elif isinstance(obj, (pd.Index, IndexVariable)) and obj.name is not None:
137         obj = Variable(obj.name, obj)
138     elif isinstance(obj, (set, dict)):
139         raise TypeError(f"variable {name!r} has invalid type {type(obj)!r}")
140     elif name is not None:
141         data = as_compatible_data(obj)
142         if data.ndim != 1:
143             raise MissingDimensionsError(
144                 f"cannot set variable {name!r} with {data.ndim!r}-dimensional data "
145                 "without explicit dimension names. Pass a tuple of "
146                 "(dims, data) instead."
147             )
148         obj = Variable(name, data, fastpath=True)
149     else:
150         raise TypeError(
151             "unable to convert object into a variable without an "
152             f"explicit list of dimensions: {obj!r}"
153         )
154 
155     if name is not None and name in obj.dims:
156         # convert the Variable into an Index
157         if obj.ndim != 1:
158             raise MissingDimensionsError(
159                 f"{name!r} has more than 1-dimension and the same name as one of its "
160                 f"dimensions {obj.dims!r}. xarray disallows such variables because they "
161                 "conflict with the coordinates used to label dimensions."
162             )
163         obj = obj.to_index_variable()
164 
165     return obj
166 
167 
168 def _maybe_wrap_data(data):
169     """
170     Put pandas.Index and numpy.ndarray arguments in adapter objects to ensure
171     they can be indexed properly.
172 
173     NumpyArrayAdapter, PandasIndexingAdapter and LazilyIndexedArray should
174     all pass through unmodified.
175     """
176     if isinstance(data, pd.Index):
177         return PandasIndexingAdapter(data)
178     return data
179 
180 
181 def _as_nanosecond_precision(data):
182     dtype = data.dtype
183     non_ns_datetime64 = (
184         dtype.kind == "M"
185         and isinstance(dtype, np.dtype)
186         and dtype != np.dtype("datetime64[ns]")
187     )
188     non_ns_datetime_tz_dtype = (
189         isinstance(dtype, pd.DatetimeTZDtype) and dtype.unit != "ns"
190     )
191     if non_ns_datetime64 or non_ns_datetime_tz_dtype:
192         utils.emit_user_level_warning(NON_NANOSECOND_WARNING.format(case="datetime"))
193         if isinstance(dtype, pd.DatetimeTZDtype):
194             nanosecond_precision_dtype = pd.DatetimeTZDtype("ns", dtype.tz)
195         else:
196             nanosecond_precision_dtype = "datetime64[ns]"
197         return data.astype(nanosecond_precision_dtype)
198     elif dtype.kind == "m" and dtype != np.dtype("timedelta64[ns]"):
199         utils.emit_user_level_warning(NON_NANOSECOND_WARNING.format(case="timedelta"))
200         return data.astype("timedelta64[ns]")
201     else:
202         return data
203 
204 
205 def _possibly_convert_objects(values):
206     """Convert arrays of datetime.datetime and datetime.timedelta objects into
207     datetime64 and timedelta64, according to the pandas convention. For the time
208     being, convert any non-nanosecond precision DatetimeIndex or TimedeltaIndex
209     objects to nanosecond precision.  While pandas is relaxing this in version
210     2.0.0, in xarray we will need to make sure we are ready to handle
211     non-nanosecond precision datetimes or timedeltas in our code before allowing
212     such values to pass through unchanged.  Converting to nanosecond precision
213     through pandas.Series objects ensures that datetimes and timedeltas are
214     within the valid date range for ns precision, as pandas will raise an error
215     if they are not.
216     """
217     as_series = pd.Series(values.ravel(), copy=False)
218     if as_series.dtype.kind in "mM":
219         as_series = _as_nanosecond_precision(as_series)
220     return np.asarray(as_series).reshape(values.shape)
221 
222 
223 def _possibly_convert_datetime_or_timedelta_index(data):
224     """For the time being, convert any non-nanosecond precision DatetimeIndex or
225     TimedeltaIndex objects to nanosecond precision.  While pandas is relaxing
226     this in version 2.0.0, in xarray we will need to make sure we are ready to
227     handle non-nanosecond precision datetimes or timedeltas in our code
228     before allowing such values to pass through unchanged."""
229     if isinstance(data, (pd.DatetimeIndex, pd.TimedeltaIndex)):
230         return _as_nanosecond_precision(data)
231     else:
232         return data
233 
234 
235 def as_compatible_data(data, fastpath=False):
236     """Prepare and wrap data to put in a Variable.
237 
238     - If data does not have the necessary attributes, convert it to ndarray.
239     - If data has dtype=datetime64, ensure that it has ns precision. If it's a
240       pandas.Timestamp, convert it to datetime64.
241     - If data is already a pandas or xarray object (other than an Index), just
242       use the values.
243 
244     Finally, wrap it up with an adapter if necessary.
245     """
246     if fastpath and getattr(data, "ndim", 0) > 0:
247         # can't use fastpath (yet) for scalars
248         return _maybe_wrap_data(data)
249 
250     from xarray.core.dataarray import DataArray
251 
252     if isinstance(data, (Variable, DataArray)):
253         return data.data
254 
255     if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):
256         data = _possibly_convert_datetime_or_timedelta_index(data)
257         return _maybe_wrap_data(data)
258 
259     if isinstance(data, tuple):
260         data = utils.to_0d_object_array(data)
261 
262     if isinstance(data, pd.Timestamp):
263         # TODO: convert, handle datetime objects, too
264         data = np.datetime64(data.value, "ns")
265 
266     if isinstance(data, timedelta):
267         data = np.timedelta64(getattr(data, "value", data), "ns")
268 
269     # we don't want nested self-described arrays
270     if isinstance(data, (pd.Series, pd.Index, pd.DataFrame)):
271         data = data.values
272 
273     if isinstance(data, np.ma.MaskedArray):
274         mask = np.ma.getmaskarray(data)
275         if mask.any():
276             dtype, fill_value = dtypes.maybe_promote(data.dtype)
277             data = np.asarray(data, dtype=dtype)
278             data[mask] = fill_value
279         else:
280             data = np.asarray(data)
281 
282     if not isinstance(data, np.ndarray) and (
283         hasattr(data, "__array_function__") or hasattr(data, "__array_namespace__")
284     ):
285         return data
286 
287     # validate whether the data is valid data types.
288     data = np.asarray(data)
289 
290     if isinstance(data, np.ndarray) and data.dtype.kind in "OMm":
291         data = _possibly_convert_objects(data)
292     return _maybe_wrap_data(data)
293 
294 
295 def _as_array_or_item(data):
296     """Return the given values as a numpy array, or as an individual item if
297     it's a 0d datetime64 or timedelta64 array.
298 
299     Importantly, this function does not copy data if it is already an ndarray -
300     otherwise, it will not be possible to update Variable values in place.
301 
302     This function mostly exists because 0-dimensional ndarrays with
303     dtype=datetime64 are broken :(
304     https://github.com/numpy/numpy/issues/4337
305     https://github.com/numpy/numpy/issues/7619
306 
307     TODO: remove this (replace with np.asarray) once these issues are fixed
308     """
309     data = np.asarray(data)
310     if data.ndim == 0:
311         if data.dtype.kind == "M":
312             data = np.datetime64(data, "ns")
313         elif data.dtype.kind == "m":
314             data = np.timedelta64(data, "ns")
315     return data
316 
317 
318 class Variable(AbstractArray, NdimSizeLenMixin, VariableArithmetic):
319     """A netcdf-like variable consisting of dimensions, data and attributes
320     which describe a single Array. A single Variable object is not fully
321     described outside the context of its parent Dataset (if you want such a
322     fully described object, use a DataArray instead).
323 
324     The main functional difference between Variables and numpy arrays is that
325     numerical operations on Variables implement array broadcasting by dimension
326     name. For example, adding an Variable with dimensions `('time',)` to
327     another Variable with dimensions `('space',)` results in a new Variable
328     with dimensions `('time', 'space')`. Furthermore, numpy reduce operations
329     like ``mean`` or ``sum`` are overwritten to take a "dimension" argument
330     instead of an "axis".
331 
332     Variables are light-weight objects used as the building block for datasets.
333     They are more primitive objects, so operations with them provide marginally
334     higher performance than using DataArrays. However, manipulating data in the
335     form of a Dataset or DataArray should almost always be preferred, because
336     they can use more complete metadata in context of coordinate labels.
337     """
338 
339     __slots__ = ("_dims", "_data", "_attrs", "_encoding")
340 
341     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):
342         """
343         Parameters
344         ----------
345         dims : str or sequence of str
346             Name(s) of the the data dimension(s). Must be either a string (only
347             for 1D data) or a sequence of strings with length equal to the
348             number of dimensions.
349         data : array_like
350             Data array which supports numpy-like data access.
351         attrs : dict_like or None, optional
352             Attributes to assign to the new variable. If None (default), an
353             empty attribute dictionary is initialized.
354         encoding : dict_like or None, optional
355             Dictionary specifying how to encode this array's data into a
356             serialized format like netCDF4. Currently used keys (for netCDF)
357             include '_FillValue', 'scale_factor', 'add_offset' and 'dtype'.
358             Well-behaved code to serialize a Variable should ignore
359             unrecognized encoding items.
360         """
361         self._data = as_compatible_data(data, fastpath=fastpath)
362         self._dims = self._parse_dimensions(dims)
363         self._attrs = None
364         self._encoding = None
365         if attrs is not None:
366             self.attrs = attrs
367         if encoding is not None:
368             self.encoding = encoding
369 
370     @property
371     def dtype(self):
372         """
373         Data-type of the array’s elements.
374 
375         See Also
376         --------
377         ndarray.dtype
378         numpy.dtype
379         """
380         return self._data.dtype
381 
382     @property
383     def shape(self):
384         """
385         Tuple of array dimensions.
386 
387         See Also
388         --------
389         numpy.ndarray.shape
390         """
391         return self._data.shape
392 
393     @property
394     def nbytes(self) -> int:
395         """
396         Total bytes consumed by the elements of the data array.
397 
398         If the underlying data array does not include ``nbytes``, estimates
399         the bytes consumed based on the ``size`` and ``dtype``.
400         """
401         if hasattr(self._data, "nbytes"):
402             return self._data.nbytes
403         else:
404             return self.size * self.dtype.itemsize
405 
406     @property
407     def _in_memory(self):
408         return isinstance(
409             self._data, (np.ndarray, np.number, PandasIndexingAdapter)
410         ) or (
411             isinstance(self._data, indexing.MemoryCachedArray)
412             and isinstance(self._data.array, indexing.NumpyIndexingAdapter)
413         )
414 
415     @property
416     def data(self) -> Any:
417         """
418         The Variable's data as an array. The underlying array type
419         (e.g. dask, sparse, pint) is preserved.
420 
421         See Also
422         --------
423         Variable.to_numpy
424         Variable.as_numpy
425         Variable.values
426         """
427         if is_duck_array(self._data):
428             return self._data
429         elif isinstance(self._data, indexing.ExplicitlyIndexed):
430             return self._data.get_duck_array()
431         else:
432             return self.values
433 
434     @data.setter
435     def data(self, data):
436         data = as_compatible_data(data)
437         if data.shape != self.shape:
438             raise ValueError(
439                 f"replacement data must match the Variable's shape. "
440                 f"replacement data has shape {data.shape}; Variable has shape {self.shape}"
441             )
442         self._data = data
443 
444     def astype(
445         self: T_Variable,
446         dtype,
447         *,
448         order=None,
449         casting=None,
450         subok=None,
451         copy=None,
452         keep_attrs=True,
453     ) -> T_Variable:
454         """
455         Copy of the Variable object, with data cast to a specified type.
456 
457         Parameters
458         ----------
459         dtype : str or dtype
460             Typecode or data-type to which the array is cast.
461         order : {'C', 'F', 'A', 'K'}, optional
462             Controls the memory layout order of the result. ‘C’ means C order,
463             ‘F’ means Fortran order, ‘A’ means ‘F’ order if all the arrays are
464             Fortran contiguous, ‘C’ order otherwise, and ‘K’ means as close to
465             the order the array elements appear in memory as possible.
466         casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional
467             Controls what kind of data casting may occur.
468 
469             * 'no' means the data types should not be cast at all.
470             * 'equiv' means only byte-order changes are allowed.
471             * 'safe' means only casts which can preserve values are allowed.
472             * 'same_kind' means only safe casts or casts within a kind,
473               like float64 to float32, are allowed.
474             * 'unsafe' means any data conversions may be done.
475         subok : bool, optional
476             If True, then sub-classes will be passed-through, otherwise the
477             returned array will be forced to be a base-class array.
478         copy : bool, optional
479             By default, astype always returns a newly allocated array. If this
480             is set to False and the `dtype` requirement is satisfied, the input
481             array is returned instead of a copy.
482         keep_attrs : bool, optional
483             By default, astype keeps attributes. Set to False to remove
484             attributes in the returned object.
485 
486         Returns
487         -------
488         out : same as object
489             New object with data cast to the specified type.
490 
491         Notes
492         -----
493         The ``order``, ``casting``, ``subok`` and ``copy`` arguments are only passed
494         through to the ``astype`` method of the underlying array when a value
495         different than ``None`` is supplied.
496         Make sure to only supply these arguments if the underlying array class
497         supports them.
498 
499         See Also
500         --------
501         numpy.ndarray.astype
502         dask.array.Array.astype
503         sparse.COO.astype
504         """
505         from xarray.core.computation import apply_ufunc
506 
507         kwargs = dict(order=order, casting=casting, subok=subok, copy=copy)
508         kwargs = {k: v for k, v in kwargs.items() if v is not None}
509 
510         return apply_ufunc(
511             duck_array_ops.astype,
512             self,
513             dtype,
514             kwargs=kwargs,
515             keep_attrs=keep_attrs,
516             dask="allowed",
517         )
518 
519     def load(self, **kwargs):
520         """Manually trigger loading of this variable's data from disk or a
521         remote source into memory and return this variable.
522 
523         Normally, it should not be necessary to call this method in user code,
524         because all xarray functions should either work on deferred data or
525         load data automatically.
526 
527         Parameters
528         ----------
529         **kwargs : dict
530             Additional keyword arguments passed on to ``dask.array.compute``.
531 
532         See Also
533         --------
534         dask.array.compute
535         """
536         if is_duck_dask_array(self._data):
537             self._data = as_compatible_data(self._data.compute(**kwargs))
538         elif isinstance(self._data, indexing.ExplicitlyIndexed):
539             self._data = self._data.get_duck_array()
540         elif not is_duck_array(self._data):
541             self._data = np.asarray(self._data)
542         return self
543 
544     def compute(self, **kwargs):
545         """Manually trigger loading of this variable's data from disk or a
546         remote source into memory and return a new variable. The original is
547         left unaltered.
548 
549         Normally, it should not be necessary to call this method in user code,
550         because all xarray functions should either work on deferred data or
551         load data automatically.
552 
553         Parameters
554         ----------
555         **kwargs : dict
556             Additional keyword arguments passed on to ``dask.array.compute``.
557 
558         See Also
559         --------
560         dask.array.compute
561         """
562         new = self.copy(deep=False)
563         return new.load(**kwargs)
564 
565     def __dask_tokenize__(self):
566         # Use v.data, instead of v._data, in order to cope with the wrappers
567         # around NetCDF and the like
568         from dask.base import normalize_token
569 
570         return normalize_token((type(self), self._dims, self.data, self._attrs))
571 
572     def __dask_graph__(self):
573         if is_duck_dask_array(self._data):
574             return self._data.__dask_graph__()
575         else:
576             return None
577 
578     def __dask_keys__(self):
579         return self._data.__dask_keys__()
580 
581     def __dask_layers__(self):
582         return self._data.__dask_layers__()
583 
584     @property
585     def __dask_optimize__(self):
586         return self._data.__dask_optimize__
587 
588     @property
589     def __dask_scheduler__(self):
590         return self._data.__dask_scheduler__
591 
592     def __dask_postcompute__(self):
593         array_func, array_args = self._data.__dask_postcompute__()
594         return self._dask_finalize, (array_func,) + array_args
595 
596     def __dask_postpersist__(self):
597         array_func, array_args = self._data.__dask_postpersist__()
598         return self._dask_finalize, (array_func,) + array_args
599 
600     def _dask_finalize(self, results, array_func, *args, **kwargs):
601         data = array_func(results, *args, **kwargs)
602         return Variable(self._dims, data, attrs=self._attrs, encoding=self._encoding)
603 
604     @property
605     def values(self):
606         """The variable's data as a numpy.ndarray"""
607         return _as_array_or_item(self._data)
608 
609     @values.setter
610     def values(self, values):
611         self.data = values
612 
613     def to_base_variable(self) -> Variable:
614         """Return this variable as a base xarray.Variable"""
615         return Variable(
616             self._dims, self._data, self._attrs, encoding=self._encoding, fastpath=True
617         )
618 
619     to_variable = utils.alias(to_base_variable, "to_variable")
620 
621     def to_index_variable(self) -> IndexVariable:
622         """Return this variable as an xarray.IndexVariable"""
623         return IndexVariable(
624             self._dims, self._data, self._attrs, encoding=self._encoding, fastpath=True
625         )
626 
627     to_coord = utils.alias(to_index_variable, "to_coord")
628 
629     def _to_index(self) -> pd.Index:
630         return self.to_index_variable()._to_index()
631 
632     def to_index(self) -> pd.Index:
633         """Convert this variable to a pandas.Index"""
634         return self.to_index_variable().to_index()
635 
636     def to_dict(
637         self, data: bool | str = "list", encoding: bool = False
638     ) -> dict[str, Any]:
639         """Dictionary representation of variable."""
640         item: dict[str, Any] = {
641             "dims": self.dims,
642             "attrs": decode_numpy_dict_values(self.attrs),
643         }
644         if data is not False:
645             if data in [True, "list"]:
646                 item["data"] = ensure_us_time_resolution(self.to_numpy()).tolist()
647             elif data == "array":
648                 item["data"] = ensure_us_time_resolution(self.data)
649             else:
650                 msg = 'data argument must be bool, "list", or "array"'
651                 raise ValueError(msg)
652 
653         else:
654             item.update({"dtype": str(self.dtype), "shape": self.shape})
655 
656         if encoding:
657             item["encoding"] = dict(self.encoding)
658 
659         return item
660 
661     @property
662     def dims(self) -> tuple[Hashable, ...]:
663         """Tuple of dimension names with which this variable is associated."""
664         return self._dims
665 
666     @dims.setter
667     def dims(self, value: str | Iterable[Hashable]) -> None:
668         self._dims = self._parse_dimensions(value)
669 
670     def _parse_dimensions(self, dims: str | Iterable[Hashable]) -> tuple[Hashable, ...]:
671         if isinstance(dims, str):
672             dims = (dims,)
673         dims = tuple(dims)
674         if len(dims) != self.ndim:
675             raise ValueError(
676                 f"dimensions {dims} must have the same length as the "
677                 f"number of data dimensions, ndim={self.ndim}"
678             )
679         return dims
680 
681     def _item_key_to_tuple(self, key):
682         if utils.is_dict_like(key):
683             return tuple(key.get(dim, slice(None)) for dim in self.dims)
684         else:
685             return key
686 
687     def _broadcast_indexes(self, key):
688         """Prepare an indexing key for an indexing operation.
689 
690         Parameters
691         ----------
692         key : int, slice, array-like, dict or tuple of integer, slice and array-like
693             Any valid input for indexing.
694 
695         Returns
696         -------
697         dims : tuple
698             Dimension of the resultant variable.
699         indexers : IndexingTuple subclass
700             Tuple of integer, array-like, or slices to use when indexing
701             self._data. The type of this argument indicates the type of
702             indexing to perform, either basic, outer or vectorized.
703         new_order : Optional[Sequence[int]]
704             Optional reordering to do on the result of indexing. If not None,
705             the first len(new_order) indexing should be moved to these
706             positions.
707         """
708         key = self._item_key_to_tuple(key)  # key is a tuple
709         # key is a tuple of full size
710         key = indexing.expanded_indexer(key, self.ndim)
711         # Convert a scalar Variable to a 0d-array
712         key = tuple(
713             k.data if isinstance(k, Variable) and k.ndim == 0 else k for k in key
714         )
715         # Convert a 0d numpy arrays to an integer
716         # dask 0d arrays are passed through
717         key = tuple(
718             k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key
719         )
720 
721         if all(isinstance(k, BASIC_INDEXING_TYPES) for k in key):
722             return self._broadcast_indexes_basic(key)
723 
724         self._validate_indexers(key)
725         # Detect it can be mapped as an outer indexer
726         # If all key is unlabeled, or
727         # key can be mapped as an OuterIndexer.
728         if all(not isinstance(k, Variable) for k in key):
729             return self._broadcast_indexes_outer(key)
730 
731         # If all key is 1-dimensional and there are no duplicate labels,
732         # key can be mapped as an OuterIndexer.
733         dims = []
734         for k, d in zip(key, self.dims):
735             if isinstance(k, Variable):
736                 if len(k.dims) > 1:
737                     return self._broadcast_indexes_vectorized(key)
738                 dims.append(k.dims[0])
739             elif not isinstance(k, integer_types):
740                 dims.append(d)
741         if len(set(dims)) == len(dims):
742             return self._broadcast_indexes_outer(key)
743 
744         return self._broadcast_indexes_vectorized(key)
745 
746     def _broadcast_indexes_basic(self, key):
747         dims = tuple(
748             dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)
749         )
750         return dims, BasicIndexer(key), None
751 
752     def _validate_indexers(self, key):
753         """Make sanity checks"""
754         for dim, k in zip(self.dims, key):
755             if not isinstance(k, BASIC_INDEXING_TYPES):
756                 if not isinstance(k, Variable):
757                     if not is_duck_array(k):
758                         k = np.asarray(k)
759                     if k.ndim > 1:
760                         raise IndexError(
761                             "Unlabeled multi-dimensional array cannot be "
762                             "used for indexing: {}".format(k)
763                         )
764                 if k.dtype.kind == "b":
765                     if self.shape[self.get_axis_num(dim)] != len(k):
766                         raise IndexError(
767                             "Boolean array size {:d} is used to index array "
768                             "with shape {:s}.".format(len(k), str(self.shape))
769                         )
770                     if k.ndim > 1:
771                         raise IndexError(
772                             "{}-dimensional boolean indexing is "
773                             "not supported. ".format(k.ndim)
774                         )
775                     if is_duck_dask_array(k.data):
776                         raise KeyError(
777                             "Indexing with a boolean dask array is not allowed. "
778                             "This will result in a dask array of unknown shape. "
779                             "Such arrays are unsupported by Xarray."
780                             "Please compute the indexer first using .compute()"
781                         )
782                     if getattr(k, "dims", (dim,)) != (dim,):
783                         raise IndexError(
784                             "Boolean indexer should be unlabeled or on the "
785                             "same dimension to the indexed array. Indexer is "
786                             "on {:s} but the target dimension is {:s}.".format(
787                                 str(k.dims), dim
788                             )
789                         )
790 
791     def _broadcast_indexes_outer(self, key):
792         # drop dim if k is integer or if k is a 0d dask array
793         dims = tuple(
794             k.dims[0] if isinstance(k, Variable) else dim
795             for k, dim in zip(key, self.dims)
796             if (not isinstance(k, integer_types) and not is_0d_dask_array(k))
797         )
798 
799         new_key = []
800         for k in key:
801             if isinstance(k, Variable):
802                 k = k.data
803             if not isinstance(k, BASIC_INDEXING_TYPES):
804                 if not is_duck_array(k):
805                     k = np.asarray(k)
806                 if k.size == 0:
807                     # Slice by empty list; numpy could not infer the dtype
808                     k = k.astype(int)
809                 elif k.dtype.kind == "b":
810                     (k,) = np.nonzero(k)
811             new_key.append(k)
812 
813         return dims, OuterIndexer(tuple(new_key)), None
814 
815     def _nonzero(self):
816         """Equivalent numpy's nonzero but returns a tuple of Variables."""
817         # TODO we should replace dask's native nonzero
818         # after https://github.com/dask/dask/issues/1076 is implemented.
819         nonzeros = np.nonzero(self.data)
820         return tuple(Variable((dim), nz) for nz, dim in zip(nonzeros, self.dims))
821 
822     def _broadcast_indexes_vectorized(self, key):
823         variables = []
824         out_dims_set = OrderedSet()
825         for dim, value in zip(self.dims, key):
826             if isinstance(value, slice):
827                 out_dims_set.add(dim)
828             else:
829                 variable = (
830                     value
831                     if isinstance(value, Variable)
832                     else as_variable(value, name=dim)
833                 )
834                 if variable.dtype.kind == "b":  # boolean indexing case
835                     (variable,) = variable._nonzero()
836 
837                 variables.append(variable)
838                 out_dims_set.update(variable.dims)
839 
840         variable_dims = set()
841         for variable in variables:
842             variable_dims.update(variable.dims)
843 
844         slices = []
845         for i, (dim, value) in enumerate(zip(self.dims, key)):
846             if isinstance(value, slice):
847                 if dim in variable_dims:
848                     # We only convert slice objects to variables if they share
849                     # a dimension with at least one other variable. Otherwise,
850                     # we can equivalently leave them as slices aknd transpose
851                     # the result. This is significantly faster/more efficient
852                     # for most array backends.
853                     values = np.arange(*value.indices(self.sizes[dim]))
854                     variables.insert(i - len(slices), Variable((dim,), values))
855                 else:
856                     slices.append((i, value))
857 
858         try:
859             variables = _broadcast_compat_variables(*variables)
860         except ValueError:
861             raise IndexError(f"Dimensions of indexers mismatch: {key}")
862 
863         out_key = [variable.data for variable in variables]
864         out_dims = tuple(out_dims_set)
865         slice_positions = set()
866         for i, value in slices:
867             out_key.insert(i, value)
868             new_position = out_dims.index(self.dims[i])
869             slice_positions.add(new_position)
870 
871         if slice_positions:
872             new_order = [i for i in range(len(out_dims)) if i not in slice_positions]
873         else:
874             new_order = None
875 
876         return out_dims, VectorizedIndexer(tuple(out_key)), new_order
877 
878     def __getitem__(self: T_Variable, key) -> T_Variable:
879         """Return a new Variable object whose contents are consistent with
880         getting the provided key from the underlying data.
881 
882         NB. __getitem__ and __setitem__ implement xarray-style indexing,
883         where if keys are unlabeled arrays, we index the array orthogonally
884         with them. If keys are labeled array (such as Variables), they are
885         broadcasted with our usual scheme and then the array is indexed with
886         the broadcasted key, like numpy's fancy indexing.
887 
888         If you really want to do indexing like `x[x > 0]`, manipulate the numpy
889         array `x.values` directly.
890         """
891         dims, indexer, new_order = self._broadcast_indexes(key)
892         data = as_indexable(self._data)[indexer]
893         if new_order:
894             data = np.moveaxis(data, range(len(new_order)), new_order)
895         return self._finalize_indexing_result(dims, data)
896 
897     def _finalize_indexing_result(self: T_Variable, dims, data) -> T_Variable:
898         """Used by IndexVariable to return IndexVariable objects when possible."""
899         return self._replace(dims=dims, data=data)
900 
901     def _getitem_with_mask(self, key, fill_value=dtypes.NA):
902         """Index this Variable with -1 remapped to fill_value."""
903         # TODO(shoyer): expose this method in public API somewhere (isel?) and
904         # use it for reindex.
905         # TODO(shoyer): add a sanity check that all other integers are
906         # non-negative
907         # TODO(shoyer): add an optimization, remapping -1 to an adjacent value
908         # that is actually indexed rather than mapping it to the last value
909         # along each axis.
910 
911         if fill_value is dtypes.NA:
912             fill_value = dtypes.get_fill_value(self.dtype)
913 
914         dims, indexer, new_order = self._broadcast_indexes(key)
915 
916         if self.size:
917             if is_duck_dask_array(self._data):
918                 # dask's indexing is faster this way; also vindex does not
919                 # support negative indices yet:
920                 # https://github.com/dask/dask/pull/2967
921                 actual_indexer = indexing.posify_mask_indexer(indexer)
922             else:
923                 actual_indexer = indexer
924 
925             data = as_indexable(self._data)[actual_indexer]
926             mask = indexing.create_mask(indexer, self.shape, data)
927             # we need to invert the mask in order to pass data first. This helps
928             # pint to choose the correct unit
929             # TODO: revert after https://github.com/hgrecco/pint/issues/1019 is fixed
930             data = duck_array_ops.where(np.logical_not(mask), data, fill_value)
931         else:
932             # array cannot be indexed along dimensions of size 0, so just
933             # build the mask directly instead.
934             mask = indexing.create_mask(indexer, self.shape)
935             data = np.broadcast_to(fill_value, getattr(mask, "shape", ()))
936 
937         if new_order:
938             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)
939         return self._finalize_indexing_result(dims, data)
940 
941     def __setitem__(self, key, value):
942         """__setitem__ is overloaded to access the underlying numpy values with
943         orthogonal indexing.
944 
945         See __getitem__ for more details.
946         """
947         dims, index_tuple, new_order = self._broadcast_indexes(key)
948 
949         if not isinstance(value, Variable):
950             value = as_compatible_data(value)
951             if value.ndim > len(dims):
952                 raise ValueError(
953                     f"shape mismatch: value array of shape {value.shape} could not be "
954                     f"broadcast to indexing result with {len(dims)} dimensions"
955                 )
956             if value.ndim == 0:
957                 value = Variable((), value)
958             else:
959                 value = Variable(dims[-value.ndim :], value)
960         # broadcast to become assignable
961         value = value.set_dims(dims).data
962 
963         if new_order:
964             value = duck_array_ops.asarray(value)
965             value = value[(len(dims) - value.ndim) * (np.newaxis,) + (Ellipsis,)]
966             value = np.moveaxis(value, new_order, range(len(new_order)))
967 
968         indexable = as_indexable(self._data)
969         indexable[index_tuple] = value
970 
971     @property
972     def attrs(self) -> dict[Any, Any]:
973         """Dictionary of local attributes on this variable."""
974         if self._attrs is None:
975             self._attrs = {}
976         return self._attrs
977 
978     @attrs.setter
979     def attrs(self, value: Mapping[Any, Any]) -> None:
980         self._attrs = dict(value)
981 
982     @property
983     def encoding(self) -> dict[Any, Any]:
984         """Dictionary of encodings on this variable."""
985         if self._encoding is None:
986             self._encoding = {}
987         return self._encoding
988 
989     @encoding.setter
990     def encoding(self, value):
991         try:
992             self._encoding = dict(value)
993         except ValueError:
994             raise ValueError("encoding must be castable to a dictionary")
995 
996     def reset_encoding(self: T_Variable) -> T_Variable:
997         """Return a new Variable without encoding."""
998         return self._replace(encoding={})
999 
1000     def copy(
1001         self: T_Variable, deep: bool = True, data: ArrayLike | None = None
1002     ) -> T_Variable:
1003         """Returns a copy of this object.
1004 
1005         If `deep=True`, the data array is loaded into memory and copied onto
1006         the new object. Dimensions, attributes and encodings are always copied.
1007 
1008         Use `data` to create a new object with the same structure as
1009         original but entirely new data.
1010 
1011         Parameters
1012         ----------
1013         deep : bool, default: True
1014             Whether the data array is loaded into memory and copied onto
1015             the new object. Default is True.
1016         data : array_like, optional
1017             Data to use in the new object. Must have same shape as original.
1018             When `data` is used, `deep` is ignored.
1019 
1020         Returns
1021         -------
1022         object : Variable
1023             New object with dimensions, attributes, encodings, and optionally
1024             data copied from original.
1025 
1026         Examples
1027         --------
1028         Shallow copy versus deep copy
1029 
1030         >>> var = xr.Variable(data=[1, 2, 3], dims="x")
1031         >>> var.copy()
1032         <xarray.Variable (x: 3)>
1033         array([1, 2, 3])
1034         >>> var_0 = var.copy(deep=False)
1035         >>> var_0[0] = 7
1036         >>> var_0
1037         <xarray.Variable (x: 3)>
1038         array([7, 2, 3])
1039         >>> var
1040         <xarray.Variable (x: 3)>
1041         array([7, 2, 3])
1042 
1043         Changing the data using the ``data`` argument maintains the
1044         structure of the original object, but with the new data. Original
1045         object is unaffected.
1046 
1047         >>> var.copy(data=[0.1, 0.2, 0.3])
1048         <xarray.Variable (x: 3)>
1049         array([0.1, 0.2, 0.3])
1050         >>> var
1051         <xarray.Variable (x: 3)>
1052         array([7, 2, 3])
1053 
1054         See Also
1055         --------
1056         pandas.DataFrame.copy
1057         """
1058         return self._copy(deep=deep, data=data)
1059 
1060     def _copy(
1061         self: T_Variable,
1062         deep: bool = True,
1063         data: ArrayLike | None = None,
1064         memo: dict[int, Any] | None = None,
1065     ) -> T_Variable:
1066         if data is None:
1067             ndata = self._data
1068 
1069             if isinstance(ndata, indexing.MemoryCachedArray):
1070                 # don't share caching between copies
1071                 ndata = indexing.MemoryCachedArray(ndata.array)
1072 
1073             if deep:
1074                 ndata = copy.deepcopy(ndata, memo)
1075 
1076         else:
1077             ndata = as_compatible_data(data)
1078             if self.shape != ndata.shape:
1079                 raise ValueError(
1080                     "Data shape {} must match shape of object {}".format(
1081                         ndata.shape, self.shape
1082                     )
1083                 )
1084 
1085         attrs = copy.deepcopy(self._attrs, memo) if deep else copy.copy(self._attrs)
1086         encoding = (
1087             copy.deepcopy(self._encoding, memo) if deep else copy.copy(self._encoding)
1088         )
1089 
1090         # note: dims is already an immutable tuple
1091         return self._replace(data=ndata, attrs=attrs, encoding=encoding)
1092 
1093     def _replace(
1094         self: T_Variable,
1095         dims=_default,
1096         data=_default,
1097         attrs=_default,
1098         encoding=_default,
1099     ) -> T_Variable:
1100         if dims is _default:
1101             dims = copy.copy(self._dims)
1102         if data is _default:
1103             data = copy.copy(self.data)
1104         if attrs is _default:
1105             attrs = copy.copy(self._attrs)
1106         if encoding is _default:
1107             encoding = copy.copy(self._encoding)
1108         return type(self)(dims, data, attrs, encoding, fastpath=True)
1109 
1110     def __copy__(self: T_Variable) -> T_Variable:
1111         return self._copy(deep=False)
1112 
1113     def __deepcopy__(
1114         self: T_Variable, memo: dict[int, Any] | None = None
1115     ) -> T_Variable:
1116         return self._copy(deep=True, memo=memo)
1117 
1118     # mutable objects should not be hashable
1119     # https://github.com/python/mypy/issues/4266
1120     __hash__ = None  # type: ignore[assignment]
1121 
1122     @property
1123     def chunks(self) -> tuple[tuple[int, ...], ...] | None:
1124         """
1125         Tuple of block lengths for this dataarray's data, in order of dimensions, or None if
1126         the underlying data is not a dask array.
1127 
1128         See Also
1129         --------
1130         Variable.chunk
1131         Variable.chunksizes
1132         xarray.unify_chunks
1133         """
1134         return getattr(self._data, "chunks", None)
1135 
1136     @property
1137     def chunksizes(self) -> Mapping[Any, tuple[int, ...]]:
1138         """
1139         Mapping from dimension names to block lengths for this variable's data, or None if
1140         the underlying data is not a dask array.
1141         Cannot be modified directly, but can be modified by calling .chunk().
1142 
1143         Differs from variable.chunks because it returns a mapping of dimensions to chunk shapes
1144         instead of a tuple of chunk shapes.
1145 
1146         See Also
1147         --------
1148         Variable.chunk
1149         Variable.chunks
1150         xarray.unify_chunks
1151         """
1152         if hasattr(self._data, "chunks"):
1153             return Frozen({dim: c for dim, c in zip(self.dims, self.data.chunks)})
1154         else:
1155             return {}
1156 
1157     _array_counter = itertools.count()
1158 
1159     def chunk(
1160         self,
1161         chunks: (
1162             int
1163             | Literal["auto"]
1164             | tuple[int, ...]
1165             | tuple[tuple[int, ...], ...]
1166             | Mapping[Any, None | int | tuple[int, ...]]
1167         ) = {},
1168         name: str | None = None,
1169         lock: bool = False,
1170         inline_array: bool = False,
1171         **chunks_kwargs: Any,
1172     ) -> Variable:
1173         """Coerce this array's data into a dask array with the given chunks.
1174 
1175         If this variable is a non-dask array, it will be converted to dask
1176         array. If it's a dask array, it will be rechunked to the given chunk
1177         sizes.
1178 
1179         If neither chunks is not provided for one or more dimensions, chunk
1180         sizes along that dimension will not be updated; non-dask arrays will be
1181         converted into dask arrays with a single block.
1182 
1183         Parameters
1184         ----------
1185         chunks : int, tuple or dict, optional
1186             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
1187             ``{'x': 5, 'y': 5}``.
1188         name : str, optional
1189             Used to generate the name for this array in the internal dask
1190             graph. Does not need not be unique.
1191         lock : optional
1192             Passed on to :py:func:`dask.array.from_array`, if the array is not
1193             already as dask array.
1194         inline_array: optional
1195             Passed on to :py:func:`dask.array.from_array`, if the array is not
1196             already as dask array.
1197         **chunks_kwargs : {dim: chunks, ...}, optional
1198             The keyword arguments form of ``chunks``.
1199             One of chunks or chunks_kwargs must be provided.
1200 
1201         Returns
1202         -------
1203         chunked : xarray.Variable
1204 
1205         See Also
1206         --------
1207         Variable.chunks
1208         Variable.chunksizes
1209         xarray.unify_chunks
1210         dask.array.from_array
1211         """
1212         import dask.array as da
1213 
1214         if chunks is None:
1215             warnings.warn(
1216                 "None value for 'chunks' is deprecated. "
1217                 "It will raise an error in the future. Use instead '{}'",
1218                 category=FutureWarning,
1219             )
1220             chunks = {}
1221 
1222         if isinstance(chunks, (float, str, int, tuple, list)):
1223             pass  # dask.array.from_array can handle these directly
1224         else:
1225             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
1226 
1227         if utils.is_dict_like(chunks):
1228             chunks = {self.get_axis_num(dim): chunk for dim, chunk in chunks.items()}
1229 
1230         data = self._data
1231         if is_duck_dask_array(data):
1232             data = data.rechunk(chunks)
1233         else:
1234             if isinstance(data, indexing.ExplicitlyIndexed):
1235                 # Unambiguously handle array storage backends (like NetCDF4 and h5py)
1236                 # that can't handle general array indexing. For example, in netCDF4 you
1237                 # can do "outer" indexing along two dimensions independent, which works
1238                 # differently from how NumPy handles it.
1239                 # da.from_array works by using lazy indexing with a tuple of slices.
1240                 # Using OuterIndexer is a pragmatic choice: dask does not yet handle
1241                 # different indexing types in an explicit way:
1242                 # https://github.com/dask/dask/issues/2883
1243                 data = indexing.ImplicitToExplicitIndexingAdapter(
1244                     data, indexing.OuterIndexer
1245                 )
1246 
1247                 # All of our lazily loaded backend array classes should use NumPy
1248                 # array operations.
1249                 kwargs = {"meta": np.ndarray}
1250             else:
1251                 kwargs = {}
1252 
1253             if utils.is_dict_like(chunks):
1254                 chunks = tuple(chunks.get(n, s) for n, s in enumerate(self.shape))
1255 
1256             data = da.from_array(
1257                 data, chunks, name=name, lock=lock, inline_array=inline_array, **kwargs
1258             )
1259 
1260         return self._replace(data=data)
1261 
1262     def to_numpy(self) -> np.ndarray:
1263         """Coerces wrapped data to numpy and returns a numpy.ndarray"""
1264         # TODO an entrypoint so array libraries can choose coercion method?
1265         data = self.data
1266 
1267         # TODO first attempt to call .to_numpy() once some libraries implement it
1268         if hasattr(data, "chunks"):
1269             data = data.compute()
1270         if isinstance(data, array_type("cupy")):
1271             data = data.get()
1272         # pint has to be imported dynamically as pint imports xarray
1273         if isinstance(data, array_type("pint")):
1274             data = data.magnitude
1275         if isinstance(data, array_type("sparse")):
1276             data = data.todense()
1277         data = np.asarray(data)
1278 
1279         return data
1280 
1281     def as_numpy(self: T_Variable) -> T_Variable:
1282         """Coerces wrapped data into a numpy array, returning a Variable."""
1283         return self._replace(data=self.to_numpy())
1284 
1285     def _as_sparse(self, sparse_format=_default, fill_value=dtypes.NA):
1286         """
1287         use sparse-array as backend.
1288         """
1289         import sparse
1290 
1291         # TODO: what to do if dask-backended?
1292         if fill_value is dtypes.NA:
1293             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1294         else:
1295             dtype = dtypes.result_type(self.dtype, fill_value)
1296 
1297         if sparse_format is _default:
1298             sparse_format = "coo"
1299         try:
1300             as_sparse = getattr(sparse, f"as_{sparse_format.lower()}")
1301         except AttributeError:
1302             raise ValueError(f"{sparse_format} is not a valid sparse format")
1303 
1304         data = as_sparse(self.data.astype(dtype), fill_value=fill_value)
1305         return self._replace(data=data)
1306 
1307     def _to_dense(self):
1308         """
1309         Change backend from sparse to np.array
1310         """
1311         if hasattr(self._data, "todense"):
1312             return self._replace(data=self._data.todense())
1313         return self.copy(deep=False)
1314 
1315     def isel(
1316         self: T_Variable,
1317         indexers: Mapping[Any, Any] | None = None,
1318         missing_dims: ErrorOptionsWithWarn = "raise",
1319         **indexers_kwargs: Any,
1320     ) -> T_Variable:
1321         """Return a new array indexed along the specified dimension(s).
1322 
1323         Parameters
1324         ----------
1325         **indexers : {dim: indexer, ...}
1326             Keyword arguments with names matching dimensions and values given
1327             by integers, slice objects or arrays.
1328         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1329             What to do if dimensions that should be selected from are not present in the
1330             DataArray:
1331             - "raise": raise an exception
1332             - "warn": raise a warning, and ignore the missing dimensions
1333             - "ignore": ignore the missing dimensions
1334 
1335         Returns
1336         -------
1337         obj : Array object
1338             A new Array with the selected data and dimensions. In general,
1339             the new variable's data will be a view of this variable's data,
1340             unless numpy fancy indexing was triggered by using an array
1341             indexer, in which case the data will be a copy.
1342         """
1343         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1344 
1345         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
1346 
1347         key = tuple(indexers.get(dim, slice(None)) for dim in self.dims)
1348         return self[key]
1349 
1350     def squeeze(self, dim=None):
1351         """Return a new object with squeezed data.
1352 
1353         Parameters
1354         ----------
1355         dim : None or str or tuple of str, optional
1356             Selects a subset of the length one dimensions. If a dimension is
1357             selected with length greater than one, an error is raised. If
1358             None, all length one dimensions are squeezed.
1359 
1360         Returns
1361         -------
1362         squeezed : same type as caller
1363             This object, but with with all or a subset of the dimensions of
1364             length 1 removed.
1365 
1366         See Also
1367         --------
1368         numpy.squeeze
1369         """
1370         dims = common.get_squeeze_dims(self, dim)
1371         return self.isel({d: 0 for d in dims})
1372 
1373     def _shift_one_dim(self, dim, count, fill_value=dtypes.NA):
1374         axis = self.get_axis_num(dim)
1375 
1376         if count > 0:
1377             keep = slice(None, -count)
1378         elif count < 0:
1379             keep = slice(-count, None)
1380         else:
1381             keep = slice(None)
1382 
1383         trimmed_data = self[(slice(None),) * axis + (keep,)].data
1384 
1385         if fill_value is dtypes.NA:
1386             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1387         else:
1388             dtype = self.dtype
1389 
1390         width = min(abs(count), self.shape[axis])
1391         dim_pad = (width, 0) if count >= 0 else (0, width)
1392         pads = [(0, 0) if d != dim else dim_pad for d in self.dims]
1393 
1394         data = np.pad(
1395             trimmed_data.astype(dtype),
1396             pads,
1397             mode="constant",
1398             constant_values=fill_value,
1399         )
1400 
1401         if is_duck_dask_array(data):
1402             # chunked data should come out with the same chunks; this makes
1403             # it feasible to combine shifted and unshifted data
1404             # TODO: remove this once dask.array automatically aligns chunks
1405             data = data.rechunk(self.data.chunks)
1406 
1407         return self._replace(data=data)
1408 
1409     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):
1410         """
1411         Return a new Variable with shifted data.
1412 
1413         Parameters
1414         ----------
1415         shifts : mapping of the form {dim: offset}
1416             Integer offset to shift along each of the given dimensions.
1417             Positive offsets shift to the right; negative offsets shift to the
1418             left.
1419         fill_value : scalar, optional
1420             Value to use for newly missing values
1421         **shifts_kwargs
1422             The keyword arguments form of ``shifts``.
1423             One of shifts or shifts_kwargs must be provided.
1424 
1425         Returns
1426         -------
1427         shifted : Variable
1428             Variable with the same dimensions and attributes but shifted data.
1429         """
1430         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
1431         result = self
1432         for dim, count in shifts.items():
1433             result = result._shift_one_dim(dim, count, fill_value=fill_value)
1434         return result
1435 
1436     def _pad_options_dim_to_index(
1437         self,
1438         pad_option: Mapping[Any, int | tuple[int, int]],
1439         fill_with_shape=False,
1440     ):
1441         if fill_with_shape:
1442             return [
1443                 (n, n) if d not in pad_option else pad_option[d]
1444                 for d, n in zip(self.dims, self.data.shape)
1445             ]
1446         return [(0, 0) if d not in pad_option else pad_option[d] for d in self.dims]
1447 
1448     def pad(
1449         self,
1450         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,
1451         mode: PadModeOptions = "constant",
1452         stat_length: int
1453         | tuple[int, int]
1454         | Mapping[Any, tuple[int, int]]
1455         | None = None,
1456         constant_values: float
1457         | tuple[float, float]
1458         | Mapping[Any, tuple[float, float]]
1459         | None = None,
1460         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
1461         reflect_type: PadReflectOptions = None,
1462         keep_attrs: bool | None = None,
1463         **pad_width_kwargs: Any,
1464     ):
1465         """
1466         Return a new Variable with padded data.
1467 
1468         Parameters
1469         ----------
1470         pad_width : mapping of hashable to tuple of int
1471             Mapping with the form of {dim: (pad_before, pad_after)}
1472             describing the number of values padded along each dimension.
1473             {dim: pad} is a shortcut for pad_before = pad_after = pad
1474         mode : str, default: "constant"
1475             See numpy / Dask docs
1476         stat_length : int, tuple or mapping of hashable to tuple
1477             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
1478             values at edge of each axis used to calculate the statistic value.
1479         constant_values : scalar, tuple or mapping of hashable to tuple
1480             Used in 'constant'.  The values to set the padded values for each
1481             axis.
1482         end_values : scalar, tuple or mapping of hashable to tuple
1483             Used in 'linear_ramp'.  The values used for the ending value of the
1484             linear_ramp and that will form the edge of the padded array.
1485         reflect_type : {"even", "odd"}, optional
1486             Used in "reflect", and "symmetric".  The "even" style is the
1487             default with an unaltered reflection around the edge value.  For
1488             the "odd" style, the extended part of the array is created by
1489             subtracting the reflected values from two times the edge value.
1490         keep_attrs : bool, optional
1491             If True, the variable's attributes (`attrs`) will be copied from
1492             the original object to the new one.  If False (default), the new
1493             object will be returned without attributes.
1494         **pad_width_kwargs
1495             One of pad_width or pad_width_kwargs must be provided.
1496 
1497         Returns
1498         -------
1499         padded : Variable
1500             Variable with the same dimensions and attributes but padded data.
1501         """
1502         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, "pad")
1503 
1504         # change default behaviour of pad with mode constant
1505         if mode == "constant" and (
1506             constant_values is None or constant_values is dtypes.NA
1507         ):
1508             dtype, constant_values = dtypes.maybe_promote(self.dtype)
1509         else:
1510             dtype = self.dtype
1511 
1512         # create pad_options_kwargs, numpy requires only relevant kwargs to be nonempty
1513         if isinstance(stat_length, dict):
1514             stat_length = self._pad_options_dim_to_index(
1515                 stat_length, fill_with_shape=True
1516             )
1517         if isinstance(constant_values, dict):
1518             constant_values = self._pad_options_dim_to_index(constant_values)
1519         if isinstance(end_values, dict):
1520             end_values = self._pad_options_dim_to_index(end_values)
1521 
1522         # workaround for bug in Dask's default value of stat_length https://github.com/dask/dask/issues/5303
1523         if stat_length is None and mode in ["maximum", "mean", "median", "minimum"]:
1524             stat_length = [(n, n) for n in self.data.shape]  # type: ignore[assignment]
1525 
1526         # change integer values to a tuple of two of those values and change pad_width to index
1527         for k, v in pad_width.items():
1528             if isinstance(v, numbers.Number):
1529                 pad_width[k] = (v, v)
1530         pad_width_by_index = self._pad_options_dim_to_index(pad_width)
1531 
1532         # create pad_options_kwargs, numpy/dask requires only relevant kwargs to be nonempty
1533         pad_option_kwargs: dict[str, Any] = {}
1534         if stat_length is not None:
1535             pad_option_kwargs["stat_length"] = stat_length
1536         if constant_values is not None:
1537             pad_option_kwargs["constant_values"] = constant_values
1538         if end_values is not None:
1539             pad_option_kwargs["end_values"] = end_values
1540         if reflect_type is not None:
1541             pad_option_kwargs["reflect_type"] = reflect_type
1542 
1543         array = np.pad(
1544             self.data.astype(dtype, copy=False),
1545             pad_width_by_index,
1546             mode=mode,
1547             **pad_option_kwargs,
1548         )
1549 
1550         if keep_attrs is None:
1551             keep_attrs = _get_keep_attrs(default=True)
1552         attrs = self._attrs if keep_attrs else None
1553 
1554         return type(self)(self.dims, array, attrs=attrs)
1555 
1556     def _roll_one_dim(self, dim, count):
1557         axis = self.get_axis_num(dim)
1558 
1559         count %= self.shape[axis]
1560         if count != 0:
1561             indices = [slice(-count, None), slice(None, -count)]
1562         else:
1563             indices = [slice(None)]
1564 
1565         arrays = [self[(slice(None),) * axis + (idx,)].data for idx in indices]
1566 
1567         data = duck_array_ops.concatenate(arrays, axis)
1568 
1569         if is_duck_dask_array(data):
1570             # chunked data should come out with the same chunks; this makes
1571             # it feasible to combine shifted and unshifted data
1572             # TODO: remove this once dask.array automatically aligns chunks
1573             data = data.rechunk(self.data.chunks)
1574 
1575         return self._replace(data=data)
1576 
1577     def roll(self, shifts=None, **shifts_kwargs):
1578         """
1579         Return a new Variable with rolld data.
1580 
1581         Parameters
1582         ----------
1583         shifts : mapping of hashable to int
1584             Integer offset to roll along each of the given dimensions.
1585             Positive offsets roll to the right; negative offsets roll to the
1586             left.
1587         **shifts_kwargs
1588             The keyword arguments form of ``shifts``.
1589             One of shifts or shifts_kwargs must be provided.
1590 
1591         Returns
1592         -------
1593         shifted : Variable
1594             Variable with the same dimensions and attributes but rolled data.
1595         """
1596         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
1597 
1598         result = self
1599         for dim, count in shifts.items():
1600             result = result._roll_one_dim(dim, count)
1601         return result
1602 
1603     def transpose(
1604         self,
1605         *dims: Hashable | ellipsis,
1606         missing_dims: ErrorOptionsWithWarn = "raise",
1607     ) -> Variable:
1608         """Return a new Variable object with transposed dimensions.
1609 
1610         Parameters
1611         ----------
1612         *dims : Hashable, optional
1613             By default, reverse the dimensions. Otherwise, reorder the
1614             dimensions to this order.
1615         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1616             What to do if dimensions that should be selected from are not present in the
1617             Variable:
1618             - "raise": raise an exception
1619             - "warn": raise a warning, and ignore the missing dimensions
1620             - "ignore": ignore the missing dimensions
1621 
1622         Returns
1623         -------
1624         transposed : Variable
1625             The returned object has transposed data and dimensions with the
1626             same attributes as the original.
1627 
1628         Notes
1629         -----
1630         This operation returns a view of this variable's data. It is
1631         lazy for dask-backed Variables but not for numpy-backed Variables.
1632 
1633         See Also
1634         --------
1635         numpy.transpose
1636         """
1637         if len(dims) == 0:
1638             dims = self.dims[::-1]
1639         else:
1640             dims = tuple(infix_dims(dims, self.dims, missing_dims))
1641 
1642         if len(dims) < 2 or dims == self.dims:
1643             # no need to transpose if only one dimension
1644             # or dims are in same order
1645             return self.copy(deep=False)
1646 
1647         axes = self.get_axis_num(dims)
1648         data = as_indexable(self._data).transpose(axes)
1649         return self._replace(dims=dims, data=data)
1650 
1651     @property
1652     def T(self) -> Variable:
1653         return self.transpose()
1654 
1655     def set_dims(self, dims, shape=None):
1656         """Return a new variable with given set of dimensions.
1657         This method might be used to attach new dimension(s) to variable.
1658 
1659         When possible, this operation does not copy this variable's data.
1660 
1661         Parameters
1662         ----------
1663         dims : str or sequence of str or dict
1664             Dimensions to include on the new variable. If a dict, values are
1665             used to provide the sizes of new dimensions; otherwise, new
1666             dimensions are inserted with length 1.
1667 
1668         Returns
1669         -------
1670         Variable
1671         """
1672         if isinstance(dims, str):
1673             dims = [dims]
1674 
1675         if shape is None and utils.is_dict_like(dims):
1676             shape = dims.values()
1677 
1678         missing_dims = set(self.dims) - set(dims)
1679         if missing_dims:
1680             raise ValueError(
1681                 f"new dimensions {dims!r} must be a superset of "
1682                 f"existing dimensions {self.dims!r}"
1683             )
1684 
1685         self_dims = set(self.dims)
1686         expanded_dims = tuple(d for d in dims if d not in self_dims) + self.dims
1687 
1688         if self.dims == expanded_dims:
1689             # don't use broadcast_to unless necessary so the result remains
1690             # writeable if possible
1691             expanded_data = self.data
1692         elif shape is not None:
1693             dims_map = dict(zip(dims, shape))
1694             tmp_shape = tuple(dims_map[d] for d in expanded_dims)
1695             expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)
1696         else:
1697             expanded_data = self.data[(None,) * (len(expanded_dims) - self.ndim)]
1698 
1699         expanded_var = Variable(
1700             expanded_dims, expanded_data, self._attrs, self._encoding, fastpath=True
1701         )
1702         return expanded_var.transpose(*dims)
1703 
1704     def _stack_once(self, dims: list[Hashable], new_dim: Hashable):
1705         if not set(dims) <= set(self.dims):
1706             raise ValueError(f"invalid existing dimensions: {dims}")
1707 
1708         if new_dim in self.dims:
1709             raise ValueError(
1710                 "cannot create a new dimension with the same "
1711                 "name as an existing dimension"
1712             )
1713 
1714         if len(dims) == 0:
1715             # don't stack
1716             return self.copy(deep=False)
1717 
1718         other_dims = [d for d in self.dims if d not in dims]
1719         dim_order = other_dims + list(dims)
1720         reordered = self.transpose(*dim_order)
1721 
1722         new_shape = reordered.shape[: len(other_dims)] + (-1,)
1723         new_data = duck_array_ops.reshape(reordered.data, new_shape)
1724         new_dims = reordered.dims[: len(other_dims)] + (new_dim,)
1725 
1726         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)
1727 
1728     def stack(self, dimensions=None, **dimensions_kwargs):
1729         """
1730         Stack any number of existing dimensions into a single new dimension.
1731 
1732         New dimensions will be added at the end, and the order of the data
1733         along each new dimension will be in contiguous (C) order.
1734 
1735         Parameters
1736         ----------
1737         dimensions : mapping of hashable to tuple of hashable
1738             Mapping of form new_name=(dim1, dim2, ...) describing the
1739             names of new dimensions, and the existing dimensions that
1740             they replace.
1741         **dimensions_kwargs
1742             The keyword arguments form of ``dimensions``.
1743             One of dimensions or dimensions_kwargs must be provided.
1744 
1745         Returns
1746         -------
1747         stacked : Variable
1748             Variable with the same attributes but stacked data.
1749 
1750         See Also
1751         --------
1752         Variable.unstack
1753         """
1754         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
1755         result = self
1756         for new_dim, dims in dimensions.items():
1757             result = result._stack_once(dims, new_dim)
1758         return result
1759 
1760     def _unstack_once_full(
1761         self, dims: Mapping[Any, int], old_dim: Hashable
1762     ) -> Variable:
1763         """
1764         Unstacks the variable without needing an index.
1765 
1766         Unlike `_unstack_once`, this function requires the existing dimension to
1767         contain the full product of the new dimensions.
1768         """
1769         new_dim_names = tuple(dims.keys())
1770         new_dim_sizes = tuple(dims.values())
1771 
1772         if old_dim not in self.dims:
1773             raise ValueError(f"invalid existing dimension: {old_dim}")
1774 
1775         if set(new_dim_names).intersection(self.dims):
1776             raise ValueError(
1777                 "cannot create a new dimension with the same "
1778                 "name as an existing dimension"
1779             )
1780 
1781         if math.prod(new_dim_sizes) != self.sizes[old_dim]:
1782             raise ValueError(
1783                 "the product of the new dimension sizes must "
1784                 "equal the size of the old dimension"
1785             )
1786 
1787         other_dims = [d for d in self.dims if d != old_dim]
1788         dim_order = other_dims + [old_dim]
1789         reordered = self.transpose(*dim_order)
1790 
1791         new_shape = reordered.shape[: len(other_dims)] + new_dim_sizes
1792         new_data = reordered.data.reshape(new_shape)
1793         new_dims = reordered.dims[: len(other_dims)] + new_dim_names
1794 
1795         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)
1796 
1797     def _unstack_once(
1798         self,
1799         index: pd.MultiIndex,
1800         dim: Hashable,
1801         fill_value=dtypes.NA,
1802         sparse: bool = False,
1803     ) -> Variable:
1804         """
1805         Unstacks this variable given an index to unstack and the name of the
1806         dimension to which the index refers.
1807         """
1808 
1809         reordered = self.transpose(..., dim)
1810 
1811         new_dim_sizes = [lev.size for lev in index.levels]
1812         new_dim_names = index.names
1813         indexer = index.codes
1814 
1815         # Potentially we could replace `len(other_dims)` with just `-1`
1816         other_dims = [d for d in self.dims if d != dim]
1817         new_shape = tuple(list(reordered.shape[: len(other_dims)]) + new_dim_sizes)
1818         new_dims = reordered.dims[: len(other_dims)] + new_dim_names
1819 
1820         if fill_value is dtypes.NA:
1821             is_missing_values = math.prod(new_shape) > math.prod(self.shape)
1822             if is_missing_values:
1823                 dtype, fill_value = dtypes.maybe_promote(self.dtype)
1824             else:
1825                 dtype = self.dtype
1826                 fill_value = dtypes.get_fill_value(dtype)
1827         else:
1828             dtype = self.dtype
1829 
1830         if sparse:
1831             # unstacking a dense multitindexed array to a sparse array
1832             from sparse import COO
1833 
1834             codes = zip(*index.codes)
1835             if reordered.ndim == 1:
1836                 indexes = codes
1837             else:
1838                 sizes = itertools.product(*[range(s) for s in reordered.shape[:-1]])
1839                 tuple_indexes = itertools.product(sizes, codes)
1840                 indexes = map(lambda x: list(itertools.chain(*x)), tuple_indexes)  # type: ignore
1841 
1842             data = COO(
1843                 coords=np.array(list(indexes)).T,
1844                 data=self.data.astype(dtype).ravel(),
1845                 fill_value=fill_value,
1846                 shape=new_shape,
1847                 sorted=index.is_monotonic_increasing,
1848             )
1849 
1850         else:
1851             data = np.full_like(
1852                 self.data,
1853                 fill_value=fill_value,
1854                 shape=new_shape,
1855                 dtype=dtype,
1856             )
1857 
1858             # Indexer is a list of lists of locations. Each list is the locations
1859             # on the new dimension. This is robust to the data being sparse; in that
1860             # case the destinations will be NaN / zero.
1861             data[(..., *indexer)] = reordered
1862 
1863         return self._replace(dims=new_dims, data=data)
1864 
1865     def unstack(self, dimensions=None, **dimensions_kwargs):
1866         """
1867         Unstack an existing dimension into multiple new dimensions.
1868 
1869         New dimensions will be added at the end, and the order of the data
1870         along each new dimension will be in contiguous (C) order.
1871 
1872         Note that unlike ``DataArray.unstack`` and ``Dataset.unstack``, this
1873         method requires the existing dimension to contain the full product of
1874         the new dimensions.
1875 
1876         Parameters
1877         ----------
1878         dimensions : mapping of hashable to mapping of hashable to int
1879             Mapping of the form old_dim={dim1: size1, ...} describing the
1880             names of existing dimensions, and the new dimensions and sizes
1881             that they map to.
1882         **dimensions_kwargs
1883             The keyword arguments form of ``dimensions``.
1884             One of dimensions or dimensions_kwargs must be provided.
1885 
1886         Returns
1887         -------
1888         unstacked : Variable
1889             Variable with the same attributes but unstacked data.
1890 
1891         See Also
1892         --------
1893         Variable.stack
1894         DataArray.unstack
1895         Dataset.unstack
1896         """
1897         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "unstack")
1898         result = self
1899         for old_dim, dims in dimensions.items():
1900             result = result._unstack_once_full(dims, old_dim)
1901         return result
1902 
1903     def fillna(self, value):
1904         return ops.fillna(self, value)
1905 
1906     def where(self, cond, other=dtypes.NA):
1907         return ops.where_method(self, cond, other)
1908 
1909     def clip(self, min=None, max=None):
1910         """
1911         Return an array whose values are limited to ``[min, max]``.
1912         At least one of max or min must be given.
1913 
1914         Refer to `numpy.clip` for full documentation.
1915 
1916         See Also
1917         --------
1918         numpy.clip : equivalent function
1919         """
1920         from xarray.core.computation import apply_ufunc
1921 
1922         return apply_ufunc(np.clip, self, min, max, dask="allowed")
1923 
1924     def reduce(
1925         self,
1926         func: Callable[..., Any],
1927         dim: Dims = None,
1928         axis: int | Sequence[int] | None = None,
1929         keep_attrs: bool | None = None,
1930         keepdims: bool = False,
1931         **kwargs,
1932     ) -> Variable:
1933         """Reduce this array by applying `func` along some dimension(s).
1934 
1935         Parameters
1936         ----------
1937         func : callable
1938             Function which can be called in the form
1939             `func(x, axis=axis, **kwargs)` to return the result of reducing an
1940             np.ndarray over an integer valued axis.
1941         dim : "...", str, Iterable of Hashable or None, optional
1942             Dimension(s) over which to apply `func`. By default `func` is
1943             applied over all dimensions.
1944         axis : int or Sequence of int, optional
1945             Axis(es) over which to apply `func`. Only one of the 'dim'
1946             and 'axis' arguments can be supplied. If neither are supplied, then
1947             the reduction is calculated over the flattened array (by calling
1948             `func(x)` without an axis argument).
1949         keep_attrs : bool, optional
1950             If True, the variable's attributes (`attrs`) will be copied from
1951             the original object to the new one.  If False (default), the new
1952             object will be returned without attributes.
1953         keepdims : bool, default: False
1954             If True, the dimensions which are reduced are left in the result
1955             as dimensions of size one
1956         **kwargs : dict
1957             Additional keyword arguments passed on to `func`.
1958 
1959         Returns
1960         -------
1961         reduced : Array
1962             Array with summarized data and the indicated dimension(s)
1963             removed.
1964         """
1965         if dim == ...:
1966             dim = None
1967         if dim is not None and axis is not None:
1968             raise ValueError("cannot supply both 'axis' and 'dim' arguments")
1969 
1970         if dim is not None:
1971             axis = self.get_axis_num(dim)
1972 
1973         with warnings.catch_warnings():
1974             warnings.filterwarnings(
1975                 "ignore", r"Mean of empty slice", category=RuntimeWarning
1976             )
1977             if axis is not None:
1978                 if isinstance(axis, tuple) and len(axis) == 1:
1979                     # unpack axis for the benefit of functions
1980                     # like np.argmin which can't handle tuple arguments
1981                     axis = axis[0]
1982                 data = func(self.data, axis=axis, **kwargs)
1983             else:
1984                 data = func(self.data, **kwargs)
1985 
1986         if getattr(data, "shape", ()) == self.shape:
1987             dims = self.dims
1988         else:
1989             removed_axes: Iterable[int]
1990             if axis is None:
1991                 removed_axes = range(self.ndim)
1992             else:
1993                 removed_axes = np.atleast_1d(axis) % self.ndim
1994             if keepdims:
1995                 # Insert np.newaxis for removed dims
1996                 slices = tuple(
1997                     np.newaxis if i in removed_axes else slice(None, None)
1998                     for i in range(self.ndim)
1999                 )
2000                 if getattr(data, "shape", None) is None:
2001                     # Reduce has produced a scalar value, not an array-like
2002                     data = np.asanyarray(data)[slices]
2003                 else:
2004                     data = data[slices]
2005                 dims = self.dims
2006             else:
2007                 dims = tuple(
2008                     adim for n, adim in enumerate(self.dims) if n not in removed_axes
2009                 )
2010 
2011         if keep_attrs is None:
2012             keep_attrs = _get_keep_attrs(default=False)
2013         attrs = self._attrs if keep_attrs else None
2014 
2015         return Variable(dims, data, attrs=attrs)
2016 
2017     @classmethod
2018     def concat(
2019         cls,
2020         variables,
2021         dim="concat_dim",
2022         positions=None,
2023         shortcut=False,
2024         combine_attrs="override",
2025     ):
2026         """Concatenate variables along a new or existing dimension.
2027 
2028         Parameters
2029         ----------
2030         variables : iterable of Variable
2031             Arrays to stack together. Each variable is expected to have
2032             matching dimensions and shape except for along the stacked
2033             dimension.
2034         dim : str or DataArray, optional
2035             Name of the dimension to stack along. This can either be a new
2036             dimension name, in which case it is added along axis=0, or an
2037             existing dimension name, in which case the location of the
2038             dimension is unchanged. Where to insert the new dimension is
2039             determined by the first variable.
2040         positions : None or list of array-like, optional
2041             List of integer arrays which specifies the integer positions to
2042             which to assign each dataset along the concatenated dimension.
2043             If not supplied, objects are concatenated in the provided order.
2044         shortcut : bool, optional
2045             This option is used internally to speed-up groupby operations.
2046             If `shortcut` is True, some checks of internal consistency between
2047             arrays to concatenate are skipped.
2048         combine_attrs : {"drop", "identical", "no_conflicts", "drop_conflicts", \
2049                          "override"}, default: "override"
2050             String indicating how to combine attrs of the objects being merged:
2051 
2052             - "drop": empty attrs on returned Dataset.
2053             - "identical": all attrs must be the same on every object.
2054             - "no_conflicts": attrs from all objects are combined, any that have
2055               the same name must also have the same value.
2056             - "drop_conflicts": attrs from all objects are combined, any that have
2057               the same name but different values are dropped.
2058             - "override": skip comparing and copy attrs from the first dataset to
2059               the result.
2060 
2061         Returns
2062         -------
2063         stacked : Variable
2064             Concatenated Variable formed by stacking all the supplied variables
2065             along the given dimension.
2066         """
2067         from xarray.core.merge import merge_attrs
2068 
2069         if not isinstance(dim, str):
2070             (dim,) = dim.dims
2071 
2072         # can't do this lazily: we need to loop through variables at least
2073         # twice
2074         variables = list(variables)
2075         first_var = variables[0]
2076 
2077         arrays = [v.data for v in variables]
2078 
2079         if dim in first_var.dims:
2080             axis = first_var.get_axis_num(dim)
2081             dims = first_var.dims
2082             data = duck_array_ops.concatenate(arrays, axis=axis)
2083             if positions is not None:
2084                 # TODO: deprecate this option -- we don't need it for groupby
2085                 # any more.
2086                 indices = nputils.inverse_permutation(np.concatenate(positions))
2087                 data = duck_array_ops.take(data, indices, axis=axis)
2088         else:
2089             axis = 0
2090             dims = (dim,) + first_var.dims
2091             data = duck_array_ops.stack(arrays, axis=axis)
2092 
2093         attrs = merge_attrs(
2094             [var.attrs for var in variables], combine_attrs=combine_attrs
2095         )
2096         encoding = dict(first_var.encoding)
2097         if not shortcut:
2098             for var in variables:
2099                 if var.dims != first_var.dims:
2100                     raise ValueError(
2101                         f"Variable has dimensions {list(var.dims)} but first Variable has dimensions {list(first_var.dims)}"
2102                     )
2103 
2104         return cls(dims, data, attrs, encoding)
2105 
2106     def equals(self, other, equiv=duck_array_ops.array_equiv):
2107         """True if two Variables have the same dimensions and values;
2108         otherwise False.
2109 
2110         Variables can still be equal (like pandas objects) if they have NaN
2111         values in the same locations.
2112 
2113         This method is necessary because `v1 == v2` for Variables
2114         does element-wise comparisons (like numpy.ndarrays).
2115         """
2116         other = getattr(other, "variable", other)
2117         try:
2118             return self.dims == other.dims and (
2119                 self._data is other._data or equiv(self.data, other.data)
2120             )
2121         except (TypeError, AttributeError):
2122             return False
2123 
2124     def broadcast_equals(self, other, equiv=duck_array_ops.array_equiv):
2125         """True if two Variables have the values after being broadcast against
2126         each other; otherwise False.
2127 
2128         Variables can still be equal (like pandas objects) if they have NaN
2129         values in the same locations.
2130         """
2131         try:
2132             self, other = broadcast_variables(self, other)
2133         except (ValueError, AttributeError):
2134             return False
2135         return self.equals(other, equiv=equiv)
2136 
2137     def identical(self, other, equiv=duck_array_ops.array_equiv):
2138         """Like equals, but also checks attributes."""
2139         try:
2140             return utils.dict_equiv(self.attrs, other.attrs) and self.equals(
2141                 other, equiv=equiv
2142             )
2143         except (TypeError, AttributeError):
2144             return False
2145 
2146     def no_conflicts(self, other, equiv=duck_array_ops.array_notnull_equiv):
2147         """True if the intersection of two Variable's non-null data is
2148         equal; otherwise false.
2149 
2150         Variables can thus still be equal if there are locations where either,
2151         or both, contain NaN values.
2152         """
2153         return self.broadcast_equals(other, equiv=equiv)
2154 
2155     def quantile(
2156         self,
2157         q: ArrayLike,
2158         dim: str | Sequence[Hashable] | None = None,
2159         method: QuantileMethods = "linear",
2160         keep_attrs: bool | None = None,
2161         skipna: bool | None = None,
2162         interpolation: QuantileMethods | None = None,
2163     ) -> Variable:
2164         """Compute the qth quantile of the data along the specified dimension.
2165 
2166         Returns the qth quantiles(s) of the array elements.
2167 
2168         Parameters
2169         ----------
2170         q : float or sequence of float
2171             Quantile to compute, which must be between 0 and 1
2172             inclusive.
2173         dim : str or sequence of str, optional
2174             Dimension(s) over which to apply quantile.
2175         method : str, default: "linear"
2176             This optional parameter specifies the interpolation method to use when the
2177             desired quantile lies between two data points. The options sorted by their R
2178             type as summarized in the H&F paper [1]_ are:
2179 
2180                 1. "inverted_cdf" (*)
2181                 2. "averaged_inverted_cdf" (*)
2182                 3. "closest_observation" (*)
2183                 4. "interpolated_inverted_cdf" (*)
2184                 5. "hazen" (*)
2185                 6. "weibull" (*)
2186                 7. "linear"  (default)
2187                 8. "median_unbiased" (*)
2188                 9. "normal_unbiased" (*)
2189 
2190             The first three methods are discontiuous.  The following discontinuous
2191             variations of the default "linear" (7.) option are also available:
2192 
2193                 * "lower"
2194                 * "higher"
2195                 * "midpoint"
2196                 * "nearest"
2197 
2198             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
2199             was previously called "interpolation", renamed in accordance with numpy
2200             version 1.22.0.
2201 
2202             (*) These methods require numpy version 1.22 or newer.
2203 
2204         keep_attrs : bool, optional
2205             If True, the variable's attributes (`attrs`) will be copied from
2206             the original object to the new one.  If False (default), the new
2207             object will be returned without attributes.
2208         skipna : bool, optional
2209             If True, skip missing values (as marked by NaN). By default, only
2210             skips missing values for float dtypes; other dtypes either do not
2211             have a sentinel missing value (int) or skipna=True has not been
2212             implemented (object, datetime64 or timedelta64).
2213 
2214         Returns
2215         -------
2216         quantiles : Variable
2217             If `q` is a single quantile, then the result
2218             is a scalar. If multiple percentiles are given, first axis of
2219             the result corresponds to the quantile and a quantile dimension
2220             is added to the return array. The other dimensions are the
2221             dimensions that remain after the reduction of the array.
2222 
2223         See Also
2224         --------
2225         numpy.nanquantile, pandas.Series.quantile, Dataset.quantile
2226         DataArray.quantile
2227 
2228         References
2229         ----------
2230         .. [1] R. J. Hyndman and Y. Fan,
2231            "Sample quantiles in statistical packages,"
2232            The American Statistician, 50(4), pp. 361-365, 1996
2233         """
2234 
2235         from xarray.core.computation import apply_ufunc
2236 
2237         if interpolation is not None:
2238             warnings.warn(
2239                 "The `interpolation` argument to quantile was renamed to `method`.",
2240                 FutureWarning,
2241             )
2242 
2243             if method != "linear":
2244                 raise TypeError("Cannot pass interpolation and method keywords!")
2245 
2246             method = interpolation
2247 
2248         if skipna or (skipna is None and self.dtype.kind in "cfO"):
2249             _quantile_func = np.nanquantile
2250         else:
2251             _quantile_func = np.quantile
2252 
2253         if keep_attrs is None:
2254             keep_attrs = _get_keep_attrs(default=False)
2255 
2256         scalar = utils.is_scalar(q)
2257         q = np.atleast_1d(np.asarray(q, dtype=np.float64))
2258 
2259         if dim is None:
2260             dim = self.dims
2261 
2262         if utils.is_scalar(dim):
2263             dim = [dim]
2264 
2265         def _wrapper(npa, **kwargs):
2266             # move quantile axis to end. required for apply_ufunc
2267             return np.moveaxis(_quantile_func(npa, **kwargs), 0, -1)
2268 
2269         axis = np.arange(-1, -1 * len(dim) - 1, -1)
2270 
2271         if Version(np.__version__) >= Version("1.22.0"):
2272             kwargs = {"q": q, "axis": axis, "method": method}
2273         else:
2274             if method not in ("linear", "lower", "higher", "midpoint", "nearest"):
2275                 raise ValueError(
2276                     f"Interpolation method '{method}' requires numpy >= 1.22 or is not supported."
2277                 )
2278             kwargs = {"q": q, "axis": axis, "interpolation": method}
2279 
2280         result = apply_ufunc(
2281             _wrapper,
2282             self,
2283             input_core_dims=[dim],
2284             exclude_dims=set(dim),
2285             output_core_dims=[["quantile"]],
2286             output_dtypes=[np.float64],
2287             dask_gufunc_kwargs=dict(output_sizes={"quantile": len(q)}),
2288             dask="parallelized",
2289             kwargs=kwargs,
2290         )
2291 
2292         # for backward compatibility
2293         result = result.transpose("quantile", ...)
2294         if scalar:
2295             result = result.squeeze("quantile")
2296         if keep_attrs:
2297             result.attrs = self._attrs
2298         return result
2299 
2300     def rank(self, dim, pct=False):
2301         """Ranks the data.
2302 
2303         Equal values are assigned a rank that is the average of the ranks that
2304         would have been otherwise assigned to all of the values within that
2305         set.  Ranks begin at 1, not 0. If `pct`, computes percentage ranks.
2306 
2307         NaNs in the input array are returned as NaNs.
2308 
2309         The `bottleneck` library is required.
2310 
2311         Parameters
2312         ----------
2313         dim : str
2314             Dimension over which to compute rank.
2315         pct : bool, optional
2316             If True, compute percentage ranks, otherwise compute integer ranks.
2317 
2318         Returns
2319         -------
2320         ranked : Variable
2321 
2322         See Also
2323         --------
2324         Dataset.rank, DataArray.rank
2325         """
2326         if not OPTIONS["use_bottleneck"]:
2327             raise RuntimeError(
2328                 "rank requires bottleneck to be enabled."
2329                 " Call `xr.set_options(use_bottleneck=True)` to enable it."
2330             )
2331 
2332         import bottleneck as bn
2333 
2334         data = self.data
2335 
2336         if is_duck_dask_array(data):
2337             raise TypeError(
2338                 "rank does not work for arrays stored as dask "
2339                 "arrays. Load the data via .compute() or .load() "
2340                 "prior to calling this method."
2341             )
2342         elif not isinstance(data, np.ndarray):
2343             raise TypeError(f"rank is not implemented for {type(data)} objects.")
2344 
2345         axis = self.get_axis_num(dim)
2346         func = bn.nanrankdata if self.dtype.kind == "f" else bn.rankdata
2347         ranked = func(data, axis=axis)
2348         if pct:
2349             count = np.sum(~np.isnan(data), axis=axis, keepdims=True)
2350             ranked /= count
2351         return Variable(self.dims, ranked)
2352 
2353     def rolling_window(
2354         self, dim, window, window_dim, center=False, fill_value=dtypes.NA
2355     ):
2356         """
2357         Make a rolling_window along dim and add a new_dim to the last place.
2358 
2359         Parameters
2360         ----------
2361         dim : str
2362             Dimension over which to compute rolling_window.
2363             For nd-rolling, should be list of dimensions.
2364         window : int
2365             Window size of the rolling
2366             For nd-rolling, should be list of integers.
2367         window_dim : str
2368             New name of the window dimension.
2369             For nd-rolling, should be list of strings.
2370         center : bool, default: False
2371             If True, pad fill_value for both ends. Otherwise, pad in the head
2372             of the axis.
2373         fill_value
2374             value to be filled.
2375 
2376         Returns
2377         -------
2378         Variable that is a view of the original array with a added dimension of
2379         size w.
2380         The return dim: self.dims + (window_dim, )
2381         The return shape: self.shape + (window, )
2382 
2383         Examples
2384         --------
2385         >>> v = Variable(("a", "b"), np.arange(8).reshape((2, 4)))
2386         >>> v.rolling_window("b", 3, "window_dim")
2387         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
2388         array([[[nan, nan,  0.],
2389                 [nan,  0.,  1.],
2390                 [ 0.,  1.,  2.],
2391                 [ 1.,  2.,  3.]],
2392         <BLANKLINE>
2393                [[nan, nan,  4.],
2394                 [nan,  4.,  5.],
2395                 [ 4.,  5.,  6.],
2396                 [ 5.,  6.,  7.]]])
2397 
2398         >>> v.rolling_window("b", 3, "window_dim", center=True)
2399         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
2400         array([[[nan,  0.,  1.],
2401                 [ 0.,  1.,  2.],
2402                 [ 1.,  2.,  3.],
2403                 [ 2.,  3., nan]],
2404         <BLANKLINE>
2405                [[nan,  4.,  5.],
2406                 [ 4.,  5.,  6.],
2407                 [ 5.,  6.,  7.],
2408                 [ 6.,  7., nan]]])
2409         """
2410         if fill_value is dtypes.NA:  # np.nan is passed
2411             dtype, fill_value = dtypes.maybe_promote(self.dtype)
2412             var = self.astype(dtype, copy=False)
2413         else:
2414             dtype = self.dtype
2415             var = self
2416 
2417         if utils.is_scalar(dim):
2418             for name, arg in zip(
2419                 ["window", "window_dim", "center"], [window, window_dim, center]
2420             ):
2421                 if not utils.is_scalar(arg):
2422                     raise ValueError(
2423                         f"Expected {name}={arg!r} to be a scalar like 'dim'."
2424                     )
2425             dim = [dim]
2426 
2427         # dim is now a list
2428         nroll = len(dim)
2429         if utils.is_scalar(window):
2430             window = [window] * nroll
2431         if utils.is_scalar(window_dim):
2432             window_dim = [window_dim] * nroll
2433         if utils.is_scalar(center):
2434             center = [center] * nroll
2435         if (
2436             len(dim) != len(window)
2437             or len(dim) != len(window_dim)
2438             or len(dim) != len(center)
2439         ):
2440             raise ValueError(
2441                 "'dim', 'window', 'window_dim', and 'center' must be the same length. "
2442                 f"Received dim={dim!r}, window={window!r}, window_dim={window_dim!r},"
2443                 f" and center={center!r}."
2444             )
2445 
2446         pads = {}
2447         for d, win, cent in zip(dim, window, center):
2448             if cent:
2449                 start = win // 2  # 10 -> 5,  9 -> 4
2450                 end = win - 1 - start
2451                 pads[d] = (start, end)
2452             else:
2453                 pads[d] = (win - 1, 0)
2454 
2455         padded = var.pad(pads, mode="constant", constant_values=fill_value)
2456         axis = [self.get_axis_num(d) for d in dim]
2457         new_dims = self.dims + tuple(window_dim)
2458         return Variable(
2459             new_dims,
2460             duck_array_ops.sliding_window_view(
2461                 padded.data, window_shape=window, axis=axis
2462             ),
2463         )
2464 
2465     def coarsen(
2466         self, windows, func, boundary="exact", side="left", keep_attrs=None, **kwargs
2467     ):
2468         """
2469         Apply reduction function.
2470         """
2471         windows = {k: v for k, v in windows.items() if k in self.dims}
2472 
2473         if keep_attrs is None:
2474             keep_attrs = _get_keep_attrs(default=True)
2475 
2476         if keep_attrs:
2477             _attrs = self.attrs
2478         else:
2479             _attrs = None
2480 
2481         if not windows:
2482             return self._replace(attrs=_attrs)
2483 
2484         reshaped, axes = self.coarsen_reshape(windows, boundary, side)
2485         if isinstance(func, str):
2486             name = func
2487             func = getattr(duck_array_ops, name, None)
2488             if func is None:
2489                 raise NameError(f"{name} is not a valid method.")
2490 
2491         return self._replace(data=func(reshaped, axis=axes, **kwargs), attrs=_attrs)
2492 
2493     def coarsen_reshape(self, windows, boundary, side):
2494         """
2495         Construct a reshaped-array for coarsen
2496         """
2497         if not utils.is_dict_like(boundary):
2498             boundary = {d: boundary for d in windows.keys()}
2499 
2500         if not utils.is_dict_like(side):
2501             side = {d: side for d in windows.keys()}
2502 
2503         # remove unrelated dimensions
2504         boundary = {k: v for k, v in boundary.items() if k in windows}
2505         side = {k: v for k, v in side.items() if k in windows}
2506 
2507         for d, window in windows.items():
2508             if window <= 0:
2509                 raise ValueError(
2510                     f"window must be > 0. Given {window} for dimension {d}"
2511                 )
2512 
2513         variable = self
2514         for d, window in windows.items():
2515             # trim or pad the object
2516             size = variable.shape[self._get_axis_num(d)]
2517             n = int(size / window)
2518             if boundary[d] == "exact":
2519                 if n * window != size:
2520                     raise ValueError(
2521                         f"Could not coarsen a dimension of size {size} with "
2522                         f"window {window} and boundary='exact'. Try a different 'boundary' option."
2523                     )
2524             elif boundary[d] == "trim":
2525                 if side[d] == "left":
2526                     variable = variable.isel({d: slice(0, window * n)})
2527                 else:
2528                     excess = size - window * n
2529                     variable = variable.isel({d: slice(excess, None)})
2530             elif boundary[d] == "pad":  # pad
2531                 pad = window * n - size
2532                 if pad < 0:
2533                     pad += window
2534                 if side[d] == "left":
2535                     pad_width = {d: (0, pad)}
2536                 else:
2537                     pad_width = {d: (pad, 0)}
2538                 variable = variable.pad(pad_width, mode="constant")
2539             else:
2540                 raise TypeError(
2541                     "{} is invalid for boundary. Valid option is 'exact', "
2542                     "'trim' and 'pad'".format(boundary[d])
2543                 )
2544 
2545         shape = []
2546         axes = []
2547         axis_count = 0
2548         for i, d in enumerate(variable.dims):
2549             if d in windows:
2550                 size = variable.shape[i]
2551                 shape.append(int(size / windows[d]))
2552                 shape.append(windows[d])
2553                 axis_count += 1
2554                 axes.append(i + axis_count)
2555             else:
2556                 shape.append(variable.shape[i])
2557 
2558         return variable.data.reshape(shape), tuple(axes)
2559 
2560     def isnull(self, keep_attrs: bool | None = None):
2561         """Test each value in the array for whether it is a missing value.
2562 
2563         Returns
2564         -------
2565         isnull : Variable
2566             Same type and shape as object, but the dtype of the data is bool.
2567 
2568         See Also
2569         --------
2570         pandas.isnull
2571 
2572         Examples
2573         --------
2574         >>> var = xr.Variable("x", [1, np.nan, 3])
2575         >>> var
2576         <xarray.Variable (x: 3)>
2577         array([ 1., nan,  3.])
2578         >>> var.isnull()
2579         <xarray.Variable (x: 3)>
2580         array([False,  True, False])
2581         """
2582         from xarray.core.computation import apply_ufunc
2583 
2584         if keep_attrs is None:
2585             keep_attrs = _get_keep_attrs(default=False)
2586 
2587         return apply_ufunc(
2588             duck_array_ops.isnull,
2589             self,
2590             dask="allowed",
2591             keep_attrs=keep_attrs,
2592         )
2593 
2594     def notnull(self, keep_attrs: bool | None = None):
2595         """Test each value in the array for whether it is not a missing value.
2596 
2597         Returns
2598         -------
2599         notnull : Variable
2600             Same type and shape as object, but the dtype of the data is bool.
2601 
2602         See Also
2603         --------
2604         pandas.notnull
2605 
2606         Examples
2607         --------
2608         >>> var = xr.Variable("x", [1, np.nan, 3])
2609         >>> var
2610         <xarray.Variable (x: 3)>
2611         array([ 1., nan,  3.])
2612         >>> var.notnull()
2613         <xarray.Variable (x: 3)>
2614         array([ True, False,  True])
2615         """
2616         from xarray.core.computation import apply_ufunc
2617 
2618         if keep_attrs is None:
2619             keep_attrs = _get_keep_attrs(default=False)
2620 
2621         return apply_ufunc(
2622             duck_array_ops.notnull,
2623             self,
2624             dask="allowed",
2625             keep_attrs=keep_attrs,
2626         )
2627 
2628     @property
2629     def real(self):
2630         """
2631         The real part of the variable.
2632 
2633         See Also
2634         --------
2635         numpy.ndarray.real
2636         """
2637         return self._replace(data=self.data.real)
2638 
2639     @property
2640     def imag(self):
2641         """
2642         The imaginary part of the variable.
2643 
2644         See Also
2645         --------
2646         numpy.ndarray.imag
2647         """
2648         return self._replace(data=self.data.imag)
2649 
2650     def __array_wrap__(self, obj, context=None):
2651         return Variable(self.dims, obj)
2652 
2653     def _unary_op(self, f, *args, **kwargs):
2654         keep_attrs = kwargs.pop("keep_attrs", None)
2655         if keep_attrs is None:
2656             keep_attrs = _get_keep_attrs(default=True)
2657         with np.errstate(all="ignore"):
2658             result = self.__array_wrap__(f(self.data, *args, **kwargs))
2659             if keep_attrs:
2660                 result.attrs = self.attrs
2661             return result
2662 
2663     def _binary_op(self, other, f, reflexive=False):
2664         if isinstance(other, (xr.DataArray, xr.Dataset)):
2665             return NotImplemented
2666         if reflexive and issubclass(type(self), type(other)):
2667             other_data, self_data, dims = _broadcast_compat_data(other, self)
2668         else:
2669             self_data, other_data, dims = _broadcast_compat_data(self, other)
2670         keep_attrs = _get_keep_attrs(default=False)
2671         attrs = self._attrs if keep_attrs else None
2672         with np.errstate(all="ignore"):
2673             new_data = (
2674                 f(self_data, other_data) if not reflexive else f(other_data, self_data)
2675             )
2676         result = Variable(dims, new_data, attrs=attrs)
2677         return result
2678 
2679     def _inplace_binary_op(self, other, f):
2680         if isinstance(other, xr.Dataset):
2681             raise TypeError("cannot add a Dataset to a Variable in-place")
2682         self_data, other_data, dims = _broadcast_compat_data(self, other)
2683         if dims != self.dims:
2684             raise ValueError("dimensions cannot change for in-place operations")
2685         with np.errstate(all="ignore"):
2686             self.values = f(self_data, other_data)
2687         return self
2688 
2689     def _to_numeric(self, offset=None, datetime_unit=None, dtype=float):
2690         """A (private) method to convert datetime array to numeric dtype
2691         See duck_array_ops.datetime_to_numeric
2692         """
2693         numeric_array = duck_array_ops.datetime_to_numeric(
2694             self.data, offset, datetime_unit, dtype
2695         )
2696         return type(self)(self.dims, numeric_array, self._attrs)
2697 
2698     def _unravel_argminmax(
2699         self,
2700         argminmax: str,
2701         dim: Dims,
2702         axis: int | None,
2703         keep_attrs: bool | None,
2704         skipna: bool | None,
2705     ) -> Variable | dict[Hashable, Variable]:
2706         """Apply argmin or argmax over one or more dimensions, returning the result as a
2707         dict of DataArray that can be passed directly to isel.
2708         """
2709         if dim is None and axis is None:
2710             warnings.warn(
2711                 "Behaviour of argmin/argmax with neither dim nor axis argument will "
2712                 "change to return a dict of indices of each dimension. To get a "
2713                 "single, flat index, please use np.argmin(da.data) or "
2714                 "np.argmax(da.data) instead of da.argmin() or da.argmax().",
2715                 DeprecationWarning,
2716                 stacklevel=3,
2717             )
2718 
2719         argminmax_func = getattr(duck_array_ops, argminmax)
2720 
2721         if dim is ...:
2722             # In future, should do this also when (dim is None and axis is None)
2723             dim = self.dims
2724         if (
2725             dim is None
2726             or axis is not None
2727             or not isinstance(dim, Sequence)
2728             or isinstance(dim, str)
2729         ):
2730             # Return int index if single dimension is passed, and is not part of a
2731             # sequence
2732             return self.reduce(
2733                 argminmax_func, dim=dim, axis=axis, keep_attrs=keep_attrs, skipna=skipna
2734             )
2735 
2736         # Get a name for the new dimension that does not conflict with any existing
2737         # dimension
2738         newdimname = "_unravel_argminmax_dim_0"
2739         count = 1
2740         while newdimname in self.dims:
2741             newdimname = f"_unravel_argminmax_dim_{count}"
2742             count += 1
2743 
2744         stacked = self.stack({newdimname: dim})
2745 
2746         result_dims = stacked.dims[:-1]
2747         reduce_shape = tuple(self.sizes[d] for d in dim)
2748 
2749         result_flat_indices = stacked.reduce(argminmax_func, axis=-1, skipna=skipna)
2750 
2751         result_unravelled_indices = duck_array_ops.unravel_index(
2752             result_flat_indices.data, reduce_shape
2753         )
2754 
2755         result = {
2756             d: Variable(dims=result_dims, data=i)
2757             for d, i in zip(dim, result_unravelled_indices)
2758         }
2759 
2760         if keep_attrs is None:
2761             keep_attrs = _get_keep_attrs(default=False)
2762         if keep_attrs:
2763             for v in result.values():
2764                 v.attrs = self.attrs
2765 
2766         return result
2767 
2768     def argmin(
2769         self,
2770         dim: Dims = None,
2771         axis: int | None = None,
2772         keep_attrs: bool | None = None,
2773         skipna: bool | None = None,
2774     ) -> Variable | dict[Hashable, Variable]:
2775         """Index or indices of the minimum of the Variable over one or more dimensions.
2776         If a sequence is passed to 'dim', then result returned as dict of Variables,
2777         which can be passed directly to isel(). If a single str is passed to 'dim' then
2778         returns a Variable with dtype int.
2779 
2780         If there are multiple minima, the indices of the first one found will be
2781         returned.
2782 
2783         Parameters
2784         ----------
2785         dim : "...", str, Iterable of Hashable or None, optional
2786             The dimensions over which to find the minimum. By default, finds minimum over
2787             all dimensions - for now returning an int for backward compatibility, but
2788             this is deprecated, in future will return a dict with indices for all
2789             dimensions; to return a dict with all dimensions now, pass '...'.
2790         axis : int, optional
2791             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
2792             can be supplied.
2793         keep_attrs : bool, optional
2794             If True, the attributes (`attrs`) will be copied from the original
2795             object to the new one.  If False (default), the new object will be
2796             returned without attributes.
2797         skipna : bool, optional
2798             If True, skip missing values (as marked by NaN). By default, only
2799             skips missing values for float dtypes; other dtypes either do not
2800             have a sentinel missing value (int) or skipna=True has not been
2801             implemented (object, datetime64 or timedelta64).
2802 
2803         Returns
2804         -------
2805         result : Variable or dict of Variable
2806 
2807         See Also
2808         --------
2809         DataArray.argmin, DataArray.idxmin
2810         """
2811         return self._unravel_argminmax("argmin", dim, axis, keep_attrs, skipna)
2812 
2813     def argmax(
2814         self,
2815         dim: Dims = None,
2816         axis: int | None = None,
2817         keep_attrs: bool | None = None,
2818         skipna: bool | None = None,
2819     ) -> Variable | dict[Hashable, Variable]:
2820         """Index or indices of the maximum of the Variable over one or more dimensions.
2821         If a sequence is passed to 'dim', then result returned as dict of Variables,
2822         which can be passed directly to isel(). If a single str is passed to 'dim' then
2823         returns a Variable with dtype int.
2824 
2825         If there are multiple maxima, the indices of the first one found will be
2826         returned.
2827 
2828         Parameters
2829         ----------
2830         dim : "...", str, Iterable of Hashable or None, optional
2831             The dimensions over which to find the maximum. By default, finds maximum over
2832             all dimensions - for now returning an int for backward compatibility, but
2833             this is deprecated, in future will return a dict with indices for all
2834             dimensions; to return a dict with all dimensions now, pass '...'.
2835         axis : int, optional
2836             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
2837             can be supplied.
2838         keep_attrs : bool, optional
2839             If True, the attributes (`attrs`) will be copied from the original
2840             object to the new one.  If False (default), the new object will be
2841             returned without attributes.
2842         skipna : bool, optional
2843             If True, skip missing values (as marked by NaN). By default, only
2844             skips missing values for float dtypes; other dtypes either do not
2845             have a sentinel missing value (int) or skipna=True has not been
2846             implemented (object, datetime64 or timedelta64).
2847 
2848         Returns
2849         -------
2850         result : Variable or dict of Variable
2851 
2852         See Also
2853         --------
2854         DataArray.argmax, DataArray.idxmax
2855         """
2856         return self._unravel_argminmax("argmax", dim, axis, keep_attrs, skipna)
2857 
2858 
2859 class IndexVariable(Variable):
2860     """Wrapper for accommodating a pandas.Index in an xarray.Variable.
2861 
2862     IndexVariable preserve loaded values in the form of a pandas.Index instead
2863     of a NumPy array. Hence, their values are immutable and must always be one-
2864     dimensional.
2865 
2866     They also have a name property, which is the name of their sole dimension
2867     unless another name is given.
2868     """
2869 
2870     __slots__ = ()
2871 
2872     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):
2873         super().__init__(dims, data, attrs, encoding, fastpath)
2874         if self.ndim != 1:
2875             raise ValueError(f"{type(self).__name__} objects must be 1-dimensional")
2876 
2877         # Unlike in Variable, always eagerly load values into memory
2878         if not isinstance(self._data, PandasIndexingAdapter):
2879             self._data = PandasIndexingAdapter(self._data)
2880 
2881     def __dask_tokenize__(self):
2882         from dask.base import normalize_token
2883 
2884         # Don't waste time converting pd.Index to np.ndarray
2885         return normalize_token((type(self), self._dims, self._data.array, self._attrs))
2886 
2887     def load(self):
2888         # data is already loaded into memory for IndexVariable
2889         return self
2890 
2891     # https://github.com/python/mypy/issues/1465
2892     @Variable.data.setter  # type: ignore[attr-defined]
2893     def data(self, data):
2894         raise ValueError(
2895             f"Cannot assign to the .data attribute of dimension coordinate a.k.a IndexVariable {self.name!r}. "
2896             f"Please use DataArray.assign_coords, Dataset.assign_coords or Dataset.assign as appropriate."
2897         )
2898 
2899     @Variable.values.setter  # type: ignore[attr-defined]
2900     def values(self, values):
2901         raise ValueError(
2902             f"Cannot assign to the .values attribute of dimension coordinate a.k.a IndexVariable {self.name!r}. "
2903             f"Please use DataArray.assign_coords, Dataset.assign_coords or Dataset.assign as appropriate."
2904         )
2905 
2906     def chunk(self, chunks={}, name=None, lock=False, inline_array=False):
2907         # Dummy - do not chunk. This method is invoked e.g. by Dataset.chunk()
2908         return self.copy(deep=False)
2909 
2910     def _as_sparse(self, sparse_format=_default, fill_value=_default):
2911         # Dummy
2912         return self.copy(deep=False)
2913 
2914     def _to_dense(self):
2915         # Dummy
2916         return self.copy(deep=False)
2917 
2918     def _finalize_indexing_result(self, dims, data):
2919         if getattr(data, "ndim", 0) != 1:
2920             # returns Variable rather than IndexVariable if multi-dimensional
2921             return Variable(dims, data, self._attrs, self._encoding)
2922         else:
2923             return self._replace(dims=dims, data=data)
2924 
2925     def __setitem__(self, key, value):
2926         raise TypeError(f"{type(self).__name__} values cannot be modified")
2927 
2928     @classmethod
2929     def concat(
2930         cls,
2931         variables,
2932         dim="concat_dim",
2933         positions=None,
2934         shortcut=False,
2935         combine_attrs="override",
2936     ):
2937         """Specialized version of Variable.concat for IndexVariable objects.
2938 
2939         This exists because we want to avoid converting Index objects to NumPy
2940         arrays, if possible.
2941         """
2942         from xarray.core.merge import merge_attrs
2943 
2944         if not isinstance(dim, str):
2945             (dim,) = dim.dims
2946 
2947         variables = list(variables)
2948         first_var = variables[0]
2949 
2950         if any(not isinstance(v, cls) for v in variables):
2951             raise TypeError(
2952                 "IndexVariable.concat requires that all input "
2953                 "variables be IndexVariable objects"
2954             )
2955 
2956         indexes = [v._data.array for v in variables]
2957 
2958         if not indexes:
2959             data = []
2960         else:
2961             data = indexes[0].append(indexes[1:])
2962 
2963             if positions is not None:
2964                 indices = nputils.inverse_permutation(np.concatenate(positions))
2965                 data = data.take(indices)
2966 
2967         # keep as str if possible as pandas.Index uses object (converts to numpy array)
2968         data = maybe_coerce_to_str(data, variables)
2969 
2970         attrs = merge_attrs(
2971             [var.attrs for var in variables], combine_attrs=combine_attrs
2972         )
2973         if not shortcut:
2974             for var in variables:
2975                 if var.dims != first_var.dims:
2976                     raise ValueError("inconsistent dimensions")
2977 
2978         return cls(first_var.dims, data, attrs)
2979 
2980     def copy(self, deep: bool = True, data: ArrayLike | None = None):
2981         """Returns a copy of this object.
2982 
2983         `deep` is ignored since data is stored in the form of
2984         pandas.Index, which is already immutable. Dimensions, attributes
2985         and encodings are always copied.
2986 
2987         Use `data` to create a new object with the same structure as
2988         original but entirely new data.
2989 
2990         Parameters
2991         ----------
2992         deep : bool, default: True
2993             Deep is ignored when data is given. Whether the data array is
2994             loaded into memory and copied onto the new object. Default is True.
2995         data : array_like, optional
2996             Data to use in the new object. Must have same shape as original.
2997 
2998         Returns
2999         -------
3000         object : Variable
3001             New object with dimensions, attributes, encodings, and optionally
3002             data copied from original.
3003         """
3004         if data is None:
3005             ndata = self._data.copy(deep=deep)
3006         else:
3007             ndata = as_compatible_data(data)
3008             if self.shape != ndata.shape:
3009                 raise ValueError(
3010                     "Data shape {} must match shape of object {}".format(
3011                         ndata.shape, self.shape
3012                     )
3013                 )
3014 
3015         attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)
3016         encoding = copy.deepcopy(self._encoding) if deep else copy.copy(self._encoding)
3017 
3018         return self._replace(data=ndata, attrs=attrs, encoding=encoding)
3019 
3020     def equals(self, other, equiv=None):
3021         # if equiv is specified, super up
3022         if equiv is not None:
3023             return super().equals(other, equiv)
3024 
3025         # otherwise use the native index equals, rather than looking at _data
3026         other = getattr(other, "variable", other)
3027         try:
3028             return self.dims == other.dims and self._data_equals(other)
3029         except (TypeError, AttributeError):
3030             return False
3031 
3032     def _data_equals(self, other):
3033         return self._to_index().equals(other._to_index())
3034 
3035     def to_index_variable(self) -> IndexVariable:
3036         """Return this variable as an xarray.IndexVariable"""
3037         return self.copy(deep=False)
3038 
3039     to_coord = utils.alias(to_index_variable, "to_coord")
3040 
3041     def _to_index(self) -> pd.Index:
3042         # n.b. creating a new pandas.Index from an old pandas.Index is
3043         # basically free as pandas.Index objects are immutable.
3044         # n.b.2. this method returns the multi-index instance for
3045         # a pandas multi-index level variable.
3046         assert self.ndim == 1
3047         index = self._data.array
3048         if isinstance(index, pd.MultiIndex):
3049             # set default names for multi-index unnamed levels so that
3050             # we can safely rename dimension / coordinate later
3051             valid_level_names = [
3052                 name or f"{self.dims[0]}_level_{i}"
3053                 for i, name in enumerate(index.names)
3054             ]
3055             index = index.set_names(valid_level_names)
3056         else:
3057             index = index.set_names(self.name)
3058         return index
3059 
3060     def to_index(self) -> pd.Index:
3061         """Convert this variable to a pandas.Index"""
3062         index = self._to_index()
3063         level = getattr(self._data, "level", None)
3064         if level is not None:
3065             # return multi-index level converted to a single index
3066             return index.get_level_values(level)
3067         else:
3068             return index
3069 
3070     @property
3071     def level_names(self) -> list[str] | None:
3072         """Return MultiIndex level names or None if this IndexVariable has no
3073         MultiIndex.
3074         """
3075         index = self.to_index()
3076         if isinstance(index, pd.MultiIndex):
3077             return index.names
3078         else:
3079             return None
3080 
3081     def get_level_variable(self, level):
3082         """Return a new IndexVariable from a given MultiIndex level."""
3083         if self.level_names is None:
3084             raise ValueError(f"IndexVariable {self.name!r} has no MultiIndex")
3085         index = self.to_index()
3086         return type(self)(self.dims, index.get_level_values(level))
3087 
3088     @property
3089     def name(self) -> Hashable:
3090         return self.dims[0]
3091 
3092     @name.setter
3093     def name(self, value) -> NoReturn:
3094         raise AttributeError("cannot modify name of IndexVariable in-place")
3095 
3096     def _inplace_binary_op(self, other, f):
3097         raise TypeError(
3098             "Values of an IndexVariable are immutable and can not be modified inplace"
3099         )
3100 
3101 
3102 # for backwards compatibility
3103 Coordinate = utils.alias(IndexVariable, "Coordinate")
3104 
3105 
3106 def _unified_dims(variables):
3107     # validate dimensions
3108     all_dims = {}
3109     for var in variables:
3110         var_dims = var.dims
3111         if len(set(var_dims)) < len(var_dims):
3112             raise ValueError(
3113                 "broadcasting cannot handle duplicate "
3114                 f"dimensions: {list(var_dims)!r}"
3115             )
3116         for d, s in zip(var_dims, var.shape):
3117             if d not in all_dims:
3118                 all_dims[d] = s
3119             elif all_dims[d] != s:
3120                 raise ValueError(
3121                     "operands cannot be broadcast together "
3122                     f"with mismatched lengths for dimension {d!r}: {(all_dims[d], s)}"
3123                 )
3124     return all_dims
3125 
3126 
3127 def _broadcast_compat_variables(*variables):
3128     """Create broadcast compatible variables, with the same dimensions.
3129 
3130     Unlike the result of broadcast_variables(), some variables may have
3131     dimensions of size 1 instead of the size of the broadcast dimension.
3132     """
3133     dims = tuple(_unified_dims(variables))
3134     return tuple(var.set_dims(dims) if var.dims != dims else var for var in variables)
3135 
3136 
3137 def broadcast_variables(*variables: Variable) -> tuple[Variable, ...]:
3138     """Given any number of variables, return variables with matching dimensions
3139     and broadcast data.
3140 
3141     The data on the returned variables will be a view of the data on the
3142     corresponding original arrays, but dimensions will be reordered and
3143     inserted so that both broadcast arrays have the same dimensions. The new
3144     dimensions are sorted in order of appearance in the first variable's
3145     dimensions followed by the second variable's dimensions.
3146     """
3147     dims_map = _unified_dims(variables)
3148     dims_tuple = tuple(dims_map)
3149     return tuple(
3150         var.set_dims(dims_map) if var.dims != dims_tuple else var for var in variables
3151     )
3152 
3153 
3154 def _broadcast_compat_data(self, other):
3155     if all(hasattr(other, attr) for attr in ["dims", "data", "shape", "encoding"]):
3156         # `other` satisfies the necessary Variable API for broadcast_variables
3157         new_self, new_other = _broadcast_compat_variables(self, other)
3158         self_data = new_self.data
3159         other_data = new_other.data
3160         dims = new_self.dims
3161     else:
3162         # rely on numpy broadcasting rules
3163         self_data = self.data
3164         other_data = other
3165         dims = self.dims
3166     return self_data, other_data, dims
3167 
3168 
3169 def concat(
3170     variables,
3171     dim="concat_dim",
3172     positions=None,
3173     shortcut=False,
3174     combine_attrs="override",
3175 ):
3176     """Concatenate variables along a new or existing dimension.
3177 
3178     Parameters
3179     ----------
3180     variables : iterable of Variable
3181         Arrays to stack together. Each variable is expected to have
3182         matching dimensions and shape except for along the stacked
3183         dimension.
3184     dim : str or DataArray, optional
3185         Name of the dimension to stack along. This can either be a new
3186         dimension name, in which case it is added along axis=0, or an
3187         existing dimension name, in which case the location of the
3188         dimension is unchanged. Where to insert the new dimension is
3189         determined by the first variable.
3190     positions : None or list of array-like, optional
3191         List of integer arrays which specifies the integer positions to which
3192         to assign each dataset along the concatenated dimension. If not
3193         supplied, objects are concatenated in the provided order.
3194     shortcut : bool, optional
3195         This option is used internally to speed-up groupby operations.
3196         If `shortcut` is True, some checks of internal consistency between
3197         arrays to concatenate are skipped.
3198     combine_attrs : {"drop", "identical", "no_conflicts", "drop_conflicts", \
3199                      "override"}, default: "override"
3200         String indicating how to combine attrs of the objects being merged:
3201 
3202         - "drop": empty attrs on returned Dataset.
3203         - "identical": all attrs must be the same on every object.
3204         - "no_conflicts": attrs from all objects are combined, any that have
3205           the same name must also have the same value.
3206         - "drop_conflicts": attrs from all objects are combined, any that have
3207           the same name but different values are dropped.
3208         - "override": skip comparing and copy attrs from the first dataset to
3209           the result.
3210 
3211     Returns
3212     -------
3213     stacked : Variable
3214         Concatenated Variable formed by stacking all the supplied variables
3215         along the given dimension.
3216     """
3217     variables = list(variables)
3218     if all(isinstance(v, IndexVariable) for v in variables):
3219         return IndexVariable.concat(variables, dim, positions, shortcut, combine_attrs)
3220     else:
3221         return Variable.concat(variables, dim, positions, shortcut, combine_attrs)
3222 
3223 
3224 def calculate_dimensions(variables: Mapping[Any, Variable]) -> dict[Hashable, int]:
3225     """Calculate the dimensions corresponding to a set of variables.
3226 
3227     Returns dictionary mapping from dimension names to sizes. Raises ValueError
3228     if any of the dimension sizes conflict.
3229     """
3230     dims: dict[Hashable, int] = {}
3231     last_used = {}
3232     scalar_vars = {k for k, v in variables.items() if not v.dims}
3233     for k, var in variables.items():
3234         for dim, size in zip(var.dims, var.shape):
3235             if dim in scalar_vars:
3236                 raise ValueError(
3237                     f"dimension {dim!r} already exists as a scalar variable"
3238                 )
3239             if dim not in dims:
3240                 dims[dim] = size
3241                 last_used[dim] = k
3242             elif dims[dim] != size:
3243                 raise ValueError(
3244                     f"conflicting sizes for dimension {dim!r}: "
3245                     f"length {size} on {k!r} and length {dims[dim]} on {last_used!r}"
3246                 )
3247     return dims

</code><intended_behavior>Adapt these methods to utilize a generalized computational backend system that decides at runtime which backend to use for operations, based on user configuration or data attributes. Ensure that the behavior for dask remains unaffected for current users.</intended_behavior>, <file>xarray/core/computation.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 Functions for applying functions that act on arrays to xarray's labeled data.
3 """
4 from __future__ import annotations
5 
6 import functools
7 import itertools
8 import operator
9 import warnings
10 from collections import Counter
11 from collections.abc import Hashable, Iterable, Mapping, Sequence, Set
12 from typing import TYPE_CHECKING, Any, Callable, Literal, TypeVar, Union, overload
13 
14 import numpy as np
15 
16 from xarray.core import dtypes, duck_array_ops, utils
17 from xarray.core.alignment import align, deep_align
18 from xarray.core.common import zeros_like
19 from xarray.core.duck_array_ops import datetime_to_numeric
20 from xarray.core.indexes import Index, filter_indexes_from_coords
21 from xarray.core.merge import merge_attrs, merge_coordinates_without_align
22 from xarray.core.options import OPTIONS, _get_keep_attrs
23 from xarray.core.pycompat import is_duck_dask_array
24 from xarray.core.types import Dims, T_DataArray
25 from xarray.core.utils import is_dict_like, is_scalar
26 from xarray.core.variable import Variable
27 
28 if TYPE_CHECKING:
29     from xarray.core.coordinates import Coordinates
30     from xarray.core.dataarray import DataArray
31     from xarray.core.dataset import Dataset
32     from xarray.core.types import CombineAttrsOptions, JoinOptions
33 
34 _NO_FILL_VALUE = utils.ReprObject("<no-fill-value>")
35 _DEFAULT_NAME = utils.ReprObject("<default-name>")
36 _JOINS_WITHOUT_FILL_VALUES = frozenset({"inner", "exact"})
37 
38 
39 def _first_of_type(args, kind):
40     """Return either first object of type 'kind' or raise if not found."""
41     for arg in args:
42         if isinstance(arg, kind):
43             return arg
44     raise ValueError("This should be unreachable.")
45 
46 
47 def _all_of_type(args, kind):
48     """Return all objects of type 'kind'"""
49     return [arg for arg in args if isinstance(arg, kind)]
50 
51 
52 class _UFuncSignature:
53     """Core dimensions signature for a given function.
54 
55     Based on the signature provided by generalized ufuncs in NumPy.
56 
57     Attributes
58     ----------
59     input_core_dims : tuple[tuple]
60         Core dimension names on each input variable.
61     output_core_dims : tuple[tuple]
62         Core dimension names on each output variable.
63     """
64 
65     __slots__ = (
66         "input_core_dims",
67         "output_core_dims",
68         "_all_input_core_dims",
69         "_all_output_core_dims",
70         "_all_core_dims",
71     )
72 
73     def __init__(self, input_core_dims, output_core_dims=((),)):
74         self.input_core_dims = tuple(tuple(a) for a in input_core_dims)
75         self.output_core_dims = tuple(tuple(a) for a in output_core_dims)
76         self._all_input_core_dims = None
77         self._all_output_core_dims = None
78         self._all_core_dims = None
79 
80     @property
81     def all_input_core_dims(self):
82         if self._all_input_core_dims is None:
83             self._all_input_core_dims = frozenset(
84                 dim for dims in self.input_core_dims for dim in dims
85             )
86         return self._all_input_core_dims
87 
88     @property
89     def all_output_core_dims(self):
90         if self._all_output_core_dims is None:
91             self._all_output_core_dims = frozenset(
92                 dim for dims in self.output_core_dims for dim in dims
93             )
94         return self._all_output_core_dims
95 
96     @property
97     def all_core_dims(self):
98         if self._all_core_dims is None:
99             self._all_core_dims = self.all_input_core_dims | self.all_output_core_dims
100         return self._all_core_dims
101 
102     @property
103     def dims_map(self):
104         return {
105             core_dim: f"dim{n}" for n, core_dim in enumerate(sorted(self.all_core_dims))
106         }
107 
108     @property
109     def num_inputs(self):
110         return len(self.input_core_dims)
111 
112     @property
113     def num_outputs(self):
114         return len(self.output_core_dims)
115 
116     def __eq__(self, other):
117         try:
118             return (
119                 self.input_core_dims == other.input_core_dims
120                 and self.output_core_dims == other.output_core_dims
121             )
122         except AttributeError:
123             return False
124 
125     def __ne__(self, other):
126         return not self == other
127 
128     def __repr__(self):
129         return "{}({!r}, {!r})".format(
130             type(self).__name__,
131             list(self.input_core_dims),
132             list(self.output_core_dims),
133         )
134 
135     def __str__(self):
136         lhs = ",".join("({})".format(",".join(dims)) for dims in self.input_core_dims)
137         rhs = ",".join("({})".format(",".join(dims)) for dims in self.output_core_dims)
138         return f"{lhs}->{rhs}"
139 
140     def to_gufunc_string(self, exclude_dims=frozenset()):
141         """Create an equivalent signature string for a NumPy gufunc.
142 
143         Unlike __str__, handles dimensions that don't map to Python
144         identifiers.
145 
146         Also creates unique names for input_core_dims contained in exclude_dims.
147         """
148         input_core_dims = [
149             [self.dims_map[dim] for dim in core_dims]
150             for core_dims in self.input_core_dims
151         ]
152         output_core_dims = [
153             [self.dims_map[dim] for dim in core_dims]
154             for core_dims in self.output_core_dims
155         ]
156 
157         # enumerate input_core_dims contained in exclude_dims to make them unique
158         if exclude_dims:
159             exclude_dims = [self.dims_map[dim] for dim in exclude_dims]
160 
161             counter = Counter()
162 
163             def _enumerate(dim):
164                 if dim in exclude_dims:
165                     n = counter[dim]
166                     counter.update([dim])
167                     dim = f"{dim}_{n}"
168                 return dim
169 
170             input_core_dims = [
171                 [_enumerate(dim) for dim in arg] for arg in input_core_dims
172             ]
173 
174         alt_signature = type(self)(input_core_dims, output_core_dims)
175         return str(alt_signature)
176 
177 
178 def result_name(objects: Iterable[Any]) -> Any:
179     # use the same naming heuristics as pandas:
180     # https://github.com/blaze/blaze/issues/458#issuecomment-51936356
181     names = {getattr(obj, "name", _DEFAULT_NAME) for obj in objects}
182     names.discard(_DEFAULT_NAME)
183     if len(names) == 1:
184         (name,) = names
185     else:
186         name = None
187     return name
188 
189 
190 def _get_coords_list(args: Iterable[Any]) -> list[Coordinates]:
191     coords_list = []
192     for arg in args:
193         try:
194             coords = arg.coords
195         except AttributeError:
196             pass  # skip this argument
197         else:
198             coords_list.append(coords)
199     return coords_list
200 
201 
202 def build_output_coords_and_indexes(
203     args: Iterable[Any],
204     signature: _UFuncSignature,
205     exclude_dims: Set = frozenset(),
206     combine_attrs: CombineAttrsOptions = "override",
207 ) -> tuple[list[dict[Any, Variable]], list[dict[Any, Index]]]:
208     """Build output coordinates and indexes for an operation.
209 
210     Parameters
211     ----------
212     args : Iterable
213         List of raw operation arguments. Any valid types for xarray operations
214         are OK, e.g., scalars, Variable, DataArray, Dataset.
215     signature : _UfuncSignature
216         Core dimensions signature for the operation.
217     exclude_dims : set, optional
218         Dimensions excluded from the operation. Coordinates along these
219         dimensions are dropped.
220     combine_attrs : {"drop", "identical", "no_conflicts", "drop_conflicts", \
221                      "override"} or callable, default: "drop"
222         A callable or a string indicating how to combine attrs of the objects being
223         merged:
224 
225         - "drop": empty attrs on returned Dataset.
226         - "identical": all attrs must be the same on every object.
227         - "no_conflicts": attrs from all objects are combined, any that have
228           the same name must also have the same value.
229         - "drop_conflicts": attrs from all objects are combined, any that have
230           the same name but different values are dropped.
231         - "override": skip comparing and copy attrs from the first dataset to
232           the result.
233 
234         If a callable, it must expect a sequence of ``attrs`` dicts and a context object
235         as its only parameters.
236 
237     Returns
238     -------
239     Dictionaries of Variable and Index objects with merged coordinates.
240     """
241     coords_list = _get_coords_list(args)
242 
243     if len(coords_list) == 1 and not exclude_dims:
244         # we can skip the expensive merge
245         (unpacked_coords,) = coords_list
246         merged_vars = dict(unpacked_coords.variables)
247         merged_indexes = dict(unpacked_coords.xindexes)
248     else:
249         merged_vars, merged_indexes = merge_coordinates_without_align(
250             coords_list, exclude_dims=exclude_dims, combine_attrs=combine_attrs
251         )
252 
253     output_coords = []
254     output_indexes = []
255     for output_dims in signature.output_core_dims:
256         dropped_dims = signature.all_input_core_dims - set(output_dims)
257         if dropped_dims:
258             filtered_coords = {
259                 k: v for k, v in merged_vars.items() if dropped_dims.isdisjoint(v.dims)
260             }
261             filtered_indexes = filter_indexes_from_coords(
262                 merged_indexes, set(filtered_coords)
263             )
264         else:
265             filtered_coords = merged_vars
266             filtered_indexes = merged_indexes
267         output_coords.append(filtered_coords)
268         output_indexes.append(filtered_indexes)
269 
270     return output_coords, output_indexes
271 
272 
273 def apply_dataarray_vfunc(
274     func,
275     *args,
276     signature: _UFuncSignature,
277     join: JoinOptions = "inner",
278     exclude_dims=frozenset(),
279     keep_attrs="override",
280 ) -> tuple[DataArray, ...] | DataArray:
281     """Apply a variable level function over DataArray, Variable and/or ndarray
282     objects.
283     """
284     from xarray.core.dataarray import DataArray
285 
286     if len(args) > 1:
287         args = deep_align(
288             args, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False
289         )
290 
291     objs = _all_of_type(args, DataArray)
292 
293     if keep_attrs == "drop":
294         name = result_name(args)
295     else:
296         first_obj = _first_of_type(args, DataArray)
297         name = first_obj.name
298     result_coords, result_indexes = build_output_coords_and_indexes(
299         args, signature, exclude_dims, combine_attrs=keep_attrs
300     )
301 
302     data_vars = [getattr(a, "variable", a) for a in args]
303     result_var = func(*data_vars)
304 
305     out: tuple[DataArray, ...] | DataArray
306     if signature.num_outputs > 1:
307         out = tuple(
308             DataArray(
309                 variable, coords=coords, indexes=indexes, name=name, fastpath=True
310             )
311             for variable, coords, indexes in zip(
312                 result_var, result_coords, result_indexes
313             )
314         )
315     else:
316         (coords,) = result_coords
317         (indexes,) = result_indexes
318         out = DataArray(
319             result_var, coords=coords, indexes=indexes, name=name, fastpath=True
320         )
321 
322     attrs = merge_attrs([x.attrs for x in objs], combine_attrs=keep_attrs)
323     if isinstance(out, tuple):
324         for da in out:
325             da.attrs = attrs
326     else:
327         out.attrs = attrs
328 
329     return out
330 
331 
332 def ordered_set_union(all_keys: list[Iterable]) -> Iterable:
333     return {key: None for keys in all_keys for key in keys}.keys()
334 
335 
336 def ordered_set_intersection(all_keys: list[Iterable]) -> Iterable:
337     intersection = set(all_keys[0])
338     for keys in all_keys[1:]:
339         intersection.intersection_update(keys)
340     return [key for key in all_keys[0] if key in intersection]
341 
342 
343 def assert_and_return_exact_match(all_keys):
344     first_keys = all_keys[0]
345     for keys in all_keys[1:]:
346         if keys != first_keys:
347             raise ValueError(
348                 "exact match required for all data variable names, "
349                 f"but {keys!r} != {first_keys!r}"
350             )
351     return first_keys
352 
353 
354 _JOINERS: dict[str, Callable] = {
355     "inner": ordered_set_intersection,
356     "outer": ordered_set_union,
357     "left": operator.itemgetter(0),
358     "right": operator.itemgetter(-1),
359     "exact": assert_and_return_exact_match,
360 }
361 
362 
363 def join_dict_keys(objects: Iterable[Mapping | Any], how: str = "inner") -> Iterable:
364     joiner = _JOINERS[how]
365     all_keys = [obj.keys() for obj in objects if hasattr(obj, "keys")]
366     return joiner(all_keys)
367 
368 
369 def collect_dict_values(
370     objects: Iterable[Mapping | Any], keys: Iterable, fill_value: object = None
371 ) -> list[list]:
372     return [
373         [obj.get(key, fill_value) if is_dict_like(obj) else obj for obj in objects]
374         for key in keys
375     ]
376 
377 
378 def _as_variables_or_variable(arg):
379     try:
380         return arg.variables
381     except AttributeError:
382         try:
383             return arg.variable
384         except AttributeError:
385             return arg
386 
387 
388 def _unpack_dict_tuples(
389     result_vars: Mapping[Any, tuple[Variable, ...]], num_outputs: int
390 ) -> tuple[dict[Hashable, Variable], ...]:
391     out: tuple[dict[Hashable, Variable], ...] = tuple({} for _ in range(num_outputs))
392     for name, values in result_vars.items():
393         for value, results_dict in zip(values, out):
394             results_dict[name] = value
395     return out
396 
397 
398 def apply_dict_of_variables_vfunc(
399     func, *args, signature: _UFuncSignature, join="inner", fill_value=None
400 ):
401     """Apply a variable level function over dicts of DataArray, DataArray,
402     Variable and ndarray objects.
403     """
404     args = tuple(_as_variables_or_variable(arg) for arg in args)
405     names = join_dict_keys(args, how=join)
406     grouped_by_name = collect_dict_values(args, names, fill_value)
407 
408     result_vars = {}
409     for name, variable_args in zip(names, grouped_by_name):
410         result_vars[name] = func(*variable_args)
411 
412     if signature.num_outputs > 1:
413         return _unpack_dict_tuples(result_vars, signature.num_outputs)
414     else:
415         return result_vars
416 
417 
418 def _fast_dataset(
419     variables: dict[Hashable, Variable],
420     coord_variables: Mapping[Hashable, Variable],
421     indexes: dict[Hashable, Index],
422 ) -> Dataset:
423     """Create a dataset as quickly as possible.
424 
425     Beware: the `variables` dict is modified INPLACE.
426     """
427     from xarray.core.dataset import Dataset
428 
429     variables.update(coord_variables)
430     coord_names = set(coord_variables)
431     return Dataset._construct_direct(variables, coord_names, indexes=indexes)
432 
433 
434 def apply_dataset_vfunc(
435     func,
436     *args,
437     signature: _UFuncSignature,
438     join="inner",
439     dataset_join="exact",
440     fill_value=_NO_FILL_VALUE,
441     exclude_dims=frozenset(),
442     keep_attrs="override",
443 ) -> Dataset | tuple[Dataset, ...]:
444     """Apply a variable level function over Dataset, dict of DataArray,
445     DataArray, Variable and/or ndarray objects.
446     """
447     from xarray.core.dataset import Dataset
448 
449     if dataset_join not in _JOINS_WITHOUT_FILL_VALUES and fill_value is _NO_FILL_VALUE:
450         raise TypeError(
451             "to apply an operation to datasets with different "
452             "data variables with apply_ufunc, you must supply the "
453             "dataset_fill_value argument."
454         )
455 
456     objs = _all_of_type(args, Dataset)
457 
458     if len(args) > 1:
459         args = deep_align(
460             args, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False
461         )
462 
463     list_of_coords, list_of_indexes = build_output_coords_and_indexes(
464         args, signature, exclude_dims, combine_attrs=keep_attrs
465     )
466     args = tuple(getattr(arg, "data_vars", arg) for arg in args)
467 
468     result_vars = apply_dict_of_variables_vfunc(
469         func, *args, signature=signature, join=dataset_join, fill_value=fill_value
470     )
471 
472     out: Dataset | tuple[Dataset, ...]
473     if signature.num_outputs > 1:
474         out = tuple(
475             _fast_dataset(*args)
476             for args in zip(result_vars, list_of_coords, list_of_indexes)
477         )
478     else:
479         (coord_vars,) = list_of_coords
480         (indexes,) = list_of_indexes
481         out = _fast_dataset(result_vars, coord_vars, indexes=indexes)
482 
483     attrs = merge_attrs([x.attrs for x in objs], combine_attrs=keep_attrs)
484     if isinstance(out, tuple):
485         for ds in out:
486             ds.attrs = attrs
487     else:
488         out.attrs = attrs
489 
490     return out
491 
492 
493 def _iter_over_selections(obj, dim, values):
494     """Iterate over selections of an xarray object in the provided order."""
495     from xarray.core.groupby import _dummy_copy
496 
497     dummy = None
498     for value in values:
499         try:
500             obj_sel = obj.sel(**{dim: value})
501         except (KeyError, IndexError):
502             if dummy is None:
503                 dummy = _dummy_copy(obj)
504             obj_sel = dummy
505         yield obj_sel
506 
507 
508 def apply_groupby_func(func, *args):
509     """Apply a dataset or datarray level function over GroupBy, Dataset,
510     DataArray, Variable and/or ndarray objects.
511     """
512     from xarray.core.groupby import GroupBy, peek_at
513     from xarray.core.variable import Variable
514 
515     groupbys = [arg for arg in args if isinstance(arg, GroupBy)]
516     assert groupbys, "must have at least one groupby to iterate over"
517     first_groupby = groupbys[0]
518     (grouper,) = first_groupby.groupers
519     if any(not grouper.group.equals(gb.groupers[0].group) for gb in groupbys[1:]):
520         raise ValueError(
521             "apply_ufunc can only perform operations over "
522             "multiple GroupBy objects at once if they are all "
523             "grouped the same way"
524         )
525 
526     grouped_dim = grouper.name
527     unique_values = grouper.unique_coord.values
528 
529     iterators = []
530     for arg in args:
531         if isinstance(arg, GroupBy):
532             iterator = (value for _, value in arg)
533         elif hasattr(arg, "dims") and grouped_dim in arg.dims:
534             if isinstance(arg, Variable):
535                 raise ValueError(
536                     "groupby operations cannot be performed with "
537                     "xarray.Variable objects that share a dimension with "
538                     "the grouped dimension"
539                 )
540             iterator = _iter_over_selections(arg, grouped_dim, unique_values)
541         else:
542             iterator = itertools.repeat(arg)
543         iterators.append(iterator)
544 
545     applied = (func(*zipped_args) for zipped_args in zip(*iterators))
546     applied_example, applied = peek_at(applied)
547     combine = first_groupby._combine
548     if isinstance(applied_example, tuple):
549         combined = tuple(combine(output) for output in zip(*applied))
550     else:
551         combined = combine(applied)
552     return combined
553 
554 
555 def unified_dim_sizes(
556     variables: Iterable[Variable], exclude_dims: Set = frozenset()
557 ) -> dict[Hashable, int]:
558     dim_sizes: dict[Hashable, int] = {}
559 
560     for var in variables:
561         if len(set(var.dims)) < len(var.dims):
562             raise ValueError(
563                 "broadcasting cannot handle duplicate "
564                 f"dimensions on a variable: {list(var.dims)}"
565             )
566         for dim, size in zip(var.dims, var.shape):
567             if dim not in exclude_dims:
568                 if dim not in dim_sizes:
569                     dim_sizes[dim] = size
570                 elif dim_sizes[dim] != size:
571                     raise ValueError(
572                         "operands cannot be broadcast together "
573                         "with mismatched lengths for dimension "
574                         f"{dim}: {dim_sizes[dim]} vs {size}"
575                     )
576     return dim_sizes
577 
578 
579 SLICE_NONE = slice(None)
580 
581 
582 def broadcast_compat_data(
583     variable: Variable,
584     broadcast_dims: tuple[Hashable, ...],
585     core_dims: tuple[Hashable, ...],
586 ) -> Any:
587     data = variable.data
588 
589     old_dims = variable.dims
590     new_dims = broadcast_dims + core_dims
591 
592     if new_dims == old_dims:
593         # optimize for the typical case
594         return data
595 
596     set_old_dims = set(old_dims)
597     missing_core_dims = [d for d in core_dims if d not in set_old_dims]
598     if missing_core_dims:
599         raise ValueError(
600             "operand to apply_ufunc has required core dimensions {}, but "
601             "some of these dimensions are absent on an input variable: {}".format(
602                 list(core_dims), missing_core_dims
603             )
604         )
605 
606     set_new_dims = set(new_dims)
607     unexpected_dims = [d for d in old_dims if d not in set_new_dims]
608     if unexpected_dims:
609         raise ValueError(
610             "operand to apply_ufunc encountered unexpected "
611             f"dimensions {unexpected_dims!r} on an input variable: these are core "
612             "dimensions on other input or output variables"
613         )
614 
615     # for consistency with numpy, keep broadcast dimensions to the left
616     old_broadcast_dims = tuple(d for d in broadcast_dims if d in set_old_dims)
617     reordered_dims = old_broadcast_dims + core_dims
618     if reordered_dims != old_dims:
619         order = tuple(old_dims.index(d) for d in reordered_dims)
620         data = duck_array_ops.transpose(data, order)
621 
622     if new_dims != reordered_dims:
623         key_parts: list[slice | None] = []
624         for dim in new_dims:
625             if dim in set_old_dims:
626                 key_parts.append(SLICE_NONE)
627             elif key_parts:
628                 # no need to insert new axes at the beginning that are already
629                 # handled by broadcasting
630                 key_parts.append(np.newaxis)
631         data = data[tuple(key_parts)]
632 
633     return data
634 
635 
636 def _vectorize(func, signature, output_dtypes, exclude_dims):
637     if signature.all_core_dims:
638         func = np.vectorize(
639             func,
640             otypes=output_dtypes,
641             signature=signature.to_gufunc_string(exclude_dims),
642         )
643     else:
644         func = np.vectorize(func, otypes=output_dtypes)
645 
646     return func
647 
648 
649 def apply_variable_ufunc(
650     func,
651     *args,
652     signature: _UFuncSignature,
653     exclude_dims=frozenset(),
654     dask="forbidden",
655     output_dtypes=None,
656     vectorize=False,
657     keep_attrs="override",
658     dask_gufunc_kwargs=None,
659 ) -> Variable | tuple[Variable, ...]:
660     """Apply a ndarray level function over Variable and/or ndarray objects."""
661     from xarray.core.variable import Variable, as_compatible_data
662 
663     dim_sizes = unified_dim_sizes(
664         (a for a in args if hasattr(a, "dims")), exclude_dims=exclude_dims
665     )
666     broadcast_dims = tuple(
667         dim for dim in dim_sizes if dim not in signature.all_core_dims
668     )
669     output_dims = [broadcast_dims + out for out in signature.output_core_dims]
670 
671     input_data = [
672         broadcast_compat_data(arg, broadcast_dims, core_dims)
673         if isinstance(arg, Variable)
674         else arg
675         for arg, core_dims in zip(args, signature.input_core_dims)
676     ]
677 
678     if any(is_duck_dask_array(array) for array in input_data):
679         if dask == "forbidden":
680             raise ValueError(
681                 "apply_ufunc encountered a dask array on an "
682                 "argument, but handling for dask arrays has not "
683                 "been enabled. Either set the ``dask`` argument "
684                 "or load your data into memory first with "
685                 "``.load()`` or ``.compute()``"
686             )
687         elif dask == "parallelized":
688             numpy_func = func
689 
690             if dask_gufunc_kwargs is None:
691                 dask_gufunc_kwargs = {}
692             else:
693                 dask_gufunc_kwargs = dask_gufunc_kwargs.copy()
694 
695             allow_rechunk = dask_gufunc_kwargs.get("allow_rechunk", None)
696             if allow_rechunk is None:
697                 for n, (data, core_dims) in enumerate(
698                     zip(input_data, signature.input_core_dims)
699                 ):
700                     if is_duck_dask_array(data):
701                         # core dimensions cannot span multiple chunks
702                         for axis, dim in enumerate(core_dims, start=-len(core_dims)):
703                             if len(data.chunks[axis]) != 1:
704                                 raise ValueError(
705                                     f"dimension {dim} on {n}th function argument to "
706                                     "apply_ufunc with dask='parallelized' consists of "
707                                     "multiple chunks, but is also a core dimension. To "
708                                     "fix, either rechunk into a single dask array chunk along "
709                                     f"this dimension, i.e., ``.chunk(dict({dim}=-1))``, or "
710                                     "pass ``allow_rechunk=True`` in ``dask_gufunc_kwargs`` "
711                                     "but beware that this may significantly increase memory usage."
712                                 )
713                 dask_gufunc_kwargs["allow_rechunk"] = True
714 
715             output_sizes = dask_gufunc_kwargs.pop("output_sizes", {})
716             if output_sizes:
717                 output_sizes_renamed = {}
718                 for key, value in output_sizes.items():
719                     if key not in signature.all_output_core_dims:
720                         raise ValueError(
721                             f"dimension '{key}' in 'output_sizes' must correspond to output_core_dims"
722                         )
723                     output_sizes_renamed[signature.dims_map[key]] = value
724                 dask_gufunc_kwargs["output_sizes"] = output_sizes_renamed
725 
726             for key in signature.all_output_core_dims:
727                 if (
728                     key not in signature.all_input_core_dims or key in exclude_dims
729                 ) and key not in output_sizes:
730                     raise ValueError(
731                         f"dimension '{key}' in 'output_core_dims' needs corresponding (dim, size) in 'output_sizes'"
732                     )
733 
734             def func(*arrays):
735                 import dask.array as da
736 
737                 res = da.apply_gufunc(
738                     numpy_func,
739                     signature.to_gufunc_string(exclude_dims),
740                     *arrays,
741                     vectorize=vectorize,
742                     output_dtypes=output_dtypes,
743                     **dask_gufunc_kwargs,
744                 )
745 
746                 return res
747 
748         elif dask == "allowed":
749             pass
750         else:
751             raise ValueError(
752                 "unknown setting for dask array handling in "
753                 "apply_ufunc: {}".format(dask)
754             )
755     else:
756         if vectorize:
757             func = _vectorize(
758                 func, signature, output_dtypes=output_dtypes, exclude_dims=exclude_dims
759             )
760 
761     result_data = func(*input_data)
762 
763     if signature.num_outputs == 1:
764         result_data = (result_data,)
765     elif (
766         not isinstance(result_data, tuple) or len(result_data) != signature.num_outputs
767     ):
768         raise ValueError(
769             "applied function does not have the number of "
770             "outputs specified in the ufunc signature. "
771             "Result is not a tuple of {} elements: {!r}".format(
772                 signature.num_outputs, result_data
773             )
774         )
775 
776     objs = _all_of_type(args, Variable)
777     attrs = merge_attrs(
778         [obj.attrs for obj in objs],
779         combine_attrs=keep_attrs,
780     )
781 
782     output: list[Variable] = []
783     for dims, data in zip(output_dims, result_data):
784         data = as_compatible_data(data)
785         if data.ndim != len(dims):
786             raise ValueError(
787                 "applied function returned data with unexpected "
788                 f"number of dimensions. Received {data.ndim} dimension(s) but "
789                 f"expected {len(dims)} dimensions with names: {dims!r}"
790             )
791 
792         var = Variable(dims, data, fastpath=True)
793         for dim, new_size in var.sizes.items():
794             if dim in dim_sizes and new_size != dim_sizes[dim]:
795                 raise ValueError(
796                     "size of dimension {!r} on inputs was unexpectedly "
797                     "changed by applied function from {} to {}. Only "
798                     "dimensions specified in ``exclude_dims`` with "
799                     "xarray.apply_ufunc are allowed to change size.".format(
800                         dim, dim_sizes[dim], new_size
801                     )
802                 )
803 
804         var.attrs = attrs
805         output.append(var)
806 
807     if signature.num_outputs == 1:
808         return output[0]
809     else:
810         return tuple(output)
811 
812 
813 def apply_array_ufunc(func, *args, dask="forbidden"):
814     """Apply a ndarray level function over ndarray objects."""
815     if any(is_duck_dask_array(arg) for arg in args):
816         if dask == "forbidden":
817             raise ValueError(
818                 "apply_ufunc encountered a dask array on an "
819                 "argument, but handling for dask arrays has not "
820                 "been enabled. Either set the ``dask`` argument "
821                 "or load your data into memory first with "
822                 "``.load()`` or ``.compute()``"
823             )
824         elif dask == "parallelized":
825             raise ValueError(
826                 "cannot use dask='parallelized' for apply_ufunc "
827                 "unless at least one input is an xarray object"
828             )
829         elif dask == "allowed":
830             pass
831         else:
832             raise ValueError(f"unknown setting for dask array handling: {dask}")
833     return func(*args)
834 
835 
836 def apply_ufunc(
837     func: Callable,
838     *args: Any,
839     input_core_dims: Sequence[Sequence] | None = None,
840     output_core_dims: Sequence[Sequence] | None = ((),),
841     exclude_dims: Set = frozenset(),
842     vectorize: bool = False,
843     join: JoinOptions = "exact",
844     dataset_join: str = "exact",
845     dataset_fill_value: object = _NO_FILL_VALUE,
846     keep_attrs: bool | str | None = None,
847     kwargs: Mapping | None = None,
848     dask: str = "forbidden",
849     output_dtypes: Sequence | None = None,
850     output_sizes: Mapping[Any, int] | None = None,
851     meta: Any = None,
852     dask_gufunc_kwargs: dict[str, Any] | None = None,
853 ) -> Any:
854     """Apply a vectorized function for unlabeled arrays on xarray objects.
855 
856     The function will be mapped over the data variable(s) of the input
857     arguments using xarray's standard rules for labeled computation, including
858     alignment, broadcasting, looping over GroupBy/Dataset variables, and
859     merging of coordinates.
860 
861     Parameters
862     ----------
863     func : callable
864         Function to call like ``func(*args, **kwargs)`` on unlabeled arrays
865         (``.data``) that returns an array or tuple of arrays. If multiple
866         arguments with non-matching dimensions are supplied, this function is
867         expected to vectorize (broadcast) over axes of positional arguments in
868         the style of NumPy universal functions [1]_ (if this is not the case,
869         set ``vectorize=True``). If this function returns multiple outputs, you
870         must set ``output_core_dims`` as well.
871     *args : Dataset, DataArray, DataArrayGroupBy, DatasetGroupBy, Variable, \
872         numpy.ndarray, dask.array.Array or scalar
873         Mix of labeled and/or unlabeled arrays to which to apply the function.
874     input_core_dims : sequence of sequence, optional
875         List of the same length as ``args`` giving the list of core dimensions
876         on each input argument that should not be broadcast. By default, we
877         assume there are no core dimensions on any input arguments.
878 
879         For example, ``input_core_dims=[[], ['time']]`` indicates that all
880         dimensions on the first argument and all dimensions other than 'time'
881         on the second argument should be broadcast.
882 
883         Core dimensions are automatically moved to the last axes of input
884         variables before applying ``func``, which facilitates using NumPy style
885         generalized ufuncs [2]_.
886     output_core_dims : list of tuple, optional
887         List of the same length as the number of output arguments from
888         ``func``, giving the list of core dimensions on each output that were
889         not broadcast on the inputs. By default, we assume that ``func``
890         outputs exactly one array, with axes corresponding to each broadcast
891         dimension.
892 
893         Core dimensions are assumed to appear as the last dimensions of each
894         output in the provided order.
895     exclude_dims : set, optional
896         Core dimensions on the inputs to exclude from alignment and
897         broadcasting entirely. Any input coordinates along these dimensions
898         will be dropped. Each excluded dimension must also appear in
899         ``input_core_dims`` for at least one argument. Only dimensions listed
900         here are allowed to change size between input and output objects.
901     vectorize : bool, optional
902         If True, then assume ``func`` only takes arrays defined over core
903         dimensions as input and vectorize it automatically with
904         :py:func:`numpy.vectorize`. This option exists for convenience, but is
905         almost always slower than supplying a pre-vectorized function.
906     join : {"outer", "inner", "left", "right", "exact"}, default: "exact"
907         Method for joining the indexes of the passed objects along each
908         dimension, and the variables of Dataset objects with mismatched
909         data variables:
910 
911         - 'outer': use the union of object indexes
912         - 'inner': use the intersection of object indexes
913         - 'left': use indexes from the first object with each dimension
914         - 'right': use indexes from the last object with each dimension
915         - 'exact': raise `ValueError` instead of aligning when indexes to be
916           aligned are not equal
917     dataset_join : {"outer", "inner", "left", "right", "exact"}, default: "exact"
918         Method for joining variables of Dataset objects with mismatched
919         data variables.
920 
921         - 'outer': take variables from both Dataset objects
922         - 'inner': take only overlapped variables
923         - 'left': take only variables from the first object
924         - 'right': take only variables from the last object
925         - 'exact': data variables on all Dataset objects must match exactly
926     dataset_fill_value : optional
927         Value used in place of missing variables on Dataset inputs when the
928         datasets do not share the exact same ``data_vars``. Required if
929         ``dataset_join not in {'inner', 'exact'}``, otherwise ignored.
930     keep_attrs : {"drop", "identical", "no_conflicts", "drop_conflicts", "override"} or bool, optional
931         - 'drop' or False: empty attrs on returned xarray object.
932         - 'identical': all attrs must be the same on every object.
933         - 'no_conflicts': attrs from all objects are combined, any that have the same name must also have the same value.
934         - 'drop_conflicts': attrs from all objects are combined, any that have the same name but different values are dropped.
935         - 'override' or True: skip comparing and copy attrs from the first object to the result.
936     kwargs : dict, optional
937         Optional keyword arguments passed directly on to call ``func``.
938     dask : {"forbidden", "allowed", "parallelized"}, default: "forbidden"
939         How to handle applying to objects containing lazy data in the form of
940         dask arrays:
941 
942         - 'forbidden' (default): raise an error if a dask array is encountered.
943         - 'allowed': pass dask arrays directly on to ``func``. Prefer this option if
944           ``func`` natively supports dask arrays.
945         - 'parallelized': automatically parallelize ``func`` if any of the
946           inputs are a dask array by using :py:func:`dask.array.apply_gufunc`. Multiple output
947           arguments are supported. Only use this option if ``func`` does not natively
948           support dask arrays (e.g. converts them to numpy arrays).
949     dask_gufunc_kwargs : dict, optional
950         Optional keyword arguments passed to :py:func:`dask.array.apply_gufunc` if
951         dask='parallelized'. Possible keywords are ``output_sizes``, ``allow_rechunk``
952         and ``meta``.
953     output_dtypes : list of dtype, optional
954         Optional list of output dtypes. Only used if ``dask='parallelized'`` or
955         ``vectorize=True``.
956     output_sizes : dict, optional
957         Optional mapping from dimension names to sizes for outputs. Only used
958         if dask='parallelized' and new dimensions (not found on inputs) appear
959         on outputs. ``output_sizes`` should be given in the ``dask_gufunc_kwargs``
960         parameter. It will be removed as direct parameter in a future version.
961     meta : optional
962         Size-0 object representing the type of array wrapped by dask array. Passed on to
963         :py:func:`dask.array.apply_gufunc`. ``meta`` should be given in the
964         ``dask_gufunc_kwargs`` parameter . It will be removed as direct parameter
965         a future version.
966 
967     Returns
968     -------
969     Single value or tuple of Dataset, DataArray, Variable, dask.array.Array or
970     numpy.ndarray, the first type on that list to appear on an input.
971 
972     Notes
973     -----
974     This function is designed for the more common case where ``func`` can work on numpy
975     arrays. If ``func`` needs to manipulate a whole xarray object subset to each block
976     it is possible to use :py:func:`xarray.map_blocks`.
977 
978     Note that due to the overhead :py:func:`xarray.map_blocks` is considerably slower than ``apply_ufunc``.
979 
980     Examples
981     --------
982     Calculate the vector magnitude of two arguments:
983 
984     >>> def magnitude(a, b):
985     ...     func = lambda x, y: np.sqrt(x**2 + y**2)
986     ...     return xr.apply_ufunc(func, a, b)
987     ...
988 
989     You can now apply ``magnitude()`` to :py:class:`DataArray` and :py:class:`Dataset`
990     objects, with automatically preserved dimensions and coordinates, e.g.,
991 
992     >>> array = xr.DataArray([1, 2, 3], coords=[("x", [0.1, 0.2, 0.3])])
993     >>> magnitude(array, -array)
994     <xarray.DataArray (x: 3)>
995     array([1.41421356, 2.82842712, 4.24264069])
996     Coordinates:
997       * x        (x) float64 0.1 0.2 0.3
998 
999     Plain scalars, numpy arrays and a mix of these with xarray objects is also
1000     supported:
1001 
1002     >>> magnitude(3, 4)
1003     5.0
1004     >>> magnitude(3, np.array([0, 4]))
1005     array([3., 5.])
1006     >>> magnitude(array, 0)
1007     <xarray.DataArray (x: 3)>
1008     array([1., 2., 3.])
1009     Coordinates:
1010       * x        (x) float64 0.1 0.2 0.3
1011 
1012     Other examples of how you could use ``apply_ufunc`` to write functions to
1013     (very nearly) replicate existing xarray functionality:
1014 
1015     Compute the mean (``.mean``) over one dimension:
1016 
1017     >>> def mean(obj, dim):
1018     ...     # note: apply always moves core dimensions to the end
1019     ...     return apply_ufunc(
1020     ...         np.mean, obj, input_core_dims=[[dim]], kwargs={"axis": -1}
1021     ...     )
1022     ...
1023 
1024     Inner product over a specific dimension (like :py:func:`dot`):
1025 
1026     >>> def _inner(x, y):
1027     ...     result = np.matmul(x[..., np.newaxis, :], y[..., :, np.newaxis])
1028     ...     return result[..., 0, 0]
1029     ...
1030     >>> def inner_product(a, b, dim):
1031     ...     return apply_ufunc(_inner, a, b, input_core_dims=[[dim], [dim]])
1032     ...
1033 
1034     Stack objects along a new dimension (like :py:func:`concat`):
1035 
1036     >>> def stack(objects, dim, new_coord):
1037     ...     # note: this version does not stack coordinates
1038     ...     func = lambda *x: np.stack(x, axis=-1)
1039     ...     result = apply_ufunc(
1040     ...         func,
1041     ...         *objects,
1042     ...         output_core_dims=[[dim]],
1043     ...         join="outer",
1044     ...         dataset_fill_value=np.nan
1045     ...     )
1046     ...     result[dim] = new_coord
1047     ...     return result
1048     ...
1049 
1050     If your function is not vectorized but can be applied only to core
1051     dimensions, you can use ``vectorize=True`` to turn into a vectorized
1052     function. This wraps :py:func:`numpy.vectorize`, so the operation isn't
1053     terribly fast. Here we'll use it to calculate the distance between
1054     empirical samples from two probability distributions, using a scipy
1055     function that needs to be applied to vectors:
1056 
1057     >>> import scipy.stats
1058     >>> def earth_mover_distance(first_samples, second_samples, dim="ensemble"):
1059     ...     return apply_ufunc(
1060     ...         scipy.stats.wasserstein_distance,
1061     ...         first_samples,
1062     ...         second_samples,
1063     ...         input_core_dims=[[dim], [dim]],
1064     ...         vectorize=True,
1065     ...     )
1066     ...
1067 
1068     Most of NumPy's builtin functions already broadcast their inputs
1069     appropriately for use in ``apply_ufunc``. You may find helper functions such as
1070     :py:func:`numpy.broadcast_arrays` helpful in writing your function. ``apply_ufunc`` also
1071     works well with :py:func:`numba.vectorize` and :py:func:`numba.guvectorize`.
1072 
1073     See Also
1074     --------
1075     numpy.broadcast_arrays
1076     numba.vectorize
1077     numba.guvectorize
1078     dask.array.apply_gufunc
1079     xarray.map_blocks
1080     :ref:`dask.automatic-parallelization`
1081         User guide describing :py:func:`apply_ufunc` and :py:func:`map_blocks`.
1082 
1083     References
1084     ----------
1085     .. [1] https://numpy.org/doc/stable/reference/ufuncs.html
1086     .. [2] https://numpy.org/doc/stable/reference/c-api/generalized-ufuncs.html
1087     """
1088     from xarray.core.dataarray import DataArray
1089     from xarray.core.groupby import GroupBy
1090     from xarray.core.variable import Variable
1091 
1092     if input_core_dims is None:
1093         input_core_dims = ((),) * (len(args))
1094     elif len(input_core_dims) != len(args):
1095         raise ValueError(
1096             f"input_core_dims must be None or a tuple with the length same to "
1097             f"the number of arguments. "
1098             f"Given {len(input_core_dims)} input_core_dims: {input_core_dims}, "
1099             f" but number of args is {len(args)}."
1100         )
1101 
1102     if kwargs is None:
1103         kwargs = {}
1104 
1105     signature = _UFuncSignature(input_core_dims, output_core_dims)
1106 
1107     if exclude_dims:
1108         if not isinstance(exclude_dims, set):
1109             raise TypeError(
1110                 f"Expected exclude_dims to be a 'set'. Received '{type(exclude_dims).__name__}' instead."
1111             )
1112         if not exclude_dims <= signature.all_core_dims:
1113             raise ValueError(
1114                 f"each dimension in `exclude_dims` must also be a "
1115                 f"core dimension in the function signature. "
1116                 f"Please make {(exclude_dims - signature.all_core_dims)} a core dimension"
1117             )
1118 
1119     # handle dask_gufunc_kwargs
1120     if dask == "parallelized":
1121         if dask_gufunc_kwargs is None:
1122             dask_gufunc_kwargs = {}
1123         else:
1124             dask_gufunc_kwargs = dask_gufunc_kwargs.copy()
1125         # todo: remove warnings after deprecation cycle
1126         if meta is not None:
1127             warnings.warn(
1128                 "``meta`` should be given in the ``dask_gufunc_kwargs`` parameter."
1129                 " It will be removed as direct parameter in a future version.",
1130                 FutureWarning,
1131                 stacklevel=2,
1132             )
1133             dask_gufunc_kwargs.setdefault("meta", meta)
1134         if output_sizes is not None:
1135             warnings.warn(
1136                 "``output_sizes`` should be given in the ``dask_gufunc_kwargs`` "
1137                 "parameter. It will be removed as direct parameter in a future "
1138                 "version.",
1139                 FutureWarning,
1140                 stacklevel=2,
1141             )
1142             dask_gufunc_kwargs.setdefault("output_sizes", output_sizes)
1143 
1144     if kwargs:
1145         func = functools.partial(func, **kwargs)
1146 
1147     if keep_attrs is None:
1148         keep_attrs = _get_keep_attrs(default=False)
1149 
1150     if isinstance(keep_attrs, bool):
1151         keep_attrs = "override" if keep_attrs else "drop"
1152 
1153     variables_vfunc = functools.partial(
1154         apply_variable_ufunc,
1155         func,
1156         signature=signature,
1157         exclude_dims=exclude_dims,
1158         keep_attrs=keep_attrs,
1159         dask=dask,
1160         vectorize=vectorize,
1161         output_dtypes=output_dtypes,
1162         dask_gufunc_kwargs=dask_gufunc_kwargs,
1163     )
1164 
1165     # feed groupby-apply_ufunc through apply_groupby_func
1166     if any(isinstance(a, GroupBy) for a in args):
1167         this_apply = functools.partial(
1168             apply_ufunc,
1169             func,
1170             input_core_dims=input_core_dims,
1171             output_core_dims=output_core_dims,
1172             exclude_dims=exclude_dims,
1173             join=join,
1174             dataset_join=dataset_join,
1175             dataset_fill_value=dataset_fill_value,
1176             keep_attrs=keep_attrs,
1177             dask=dask,
1178             vectorize=vectorize,
1179             output_dtypes=output_dtypes,
1180             dask_gufunc_kwargs=dask_gufunc_kwargs,
1181         )
1182         return apply_groupby_func(this_apply, *args)
1183     # feed datasets apply_variable_ufunc through apply_dataset_vfunc
1184     elif any(is_dict_like(a) for a in args):
1185         return apply_dataset_vfunc(
1186             variables_vfunc,
1187             *args,
1188             signature=signature,
1189             join=join,
1190             exclude_dims=exclude_dims,
1191             dataset_join=dataset_join,
1192             fill_value=dataset_fill_value,
1193             keep_attrs=keep_attrs,
1194         )
1195     # feed DataArray apply_variable_ufunc through apply_dataarray_vfunc
1196     elif any(isinstance(a, DataArray) for a in args):
1197         return apply_dataarray_vfunc(
1198             variables_vfunc,
1199             *args,
1200             signature=signature,
1201             join=join,
1202             exclude_dims=exclude_dims,
1203             keep_attrs=keep_attrs,
1204         )
1205     # feed Variables directly through apply_variable_ufunc
1206     elif any(isinstance(a, Variable) for a in args):
1207         return variables_vfunc(*args)
1208     else:
1209         # feed anything else through apply_array_ufunc
1210         return apply_array_ufunc(func, *args, dask=dask)
1211 
1212 
1213 def cov(
1214     da_a: T_DataArray, da_b: T_DataArray, dim: Dims = None, ddof: int = 1
1215 ) -> T_DataArray:
1216     """
1217     Compute covariance between two DataArray objects along a shared dimension.
1218 
1219     Parameters
1220     ----------
1221     da_a : DataArray
1222         Array to compute.
1223     da_b : DataArray
1224         Array to compute.
1225     dim : str, iterable of hashable, "..." or None, optional
1226         The dimension along which the covariance will be computed
1227     ddof : int, default: 1
1228         If ddof=1, covariance is normalized by N-1, giving an unbiased estimate,
1229         else normalization is by N.
1230 
1231     Returns
1232     -------
1233     covariance : DataArray
1234 
1235     See Also
1236     --------
1237     pandas.Series.cov : corresponding pandas function
1238     xarray.corr : respective function to calculate correlation
1239 
1240     Examples
1241     --------
1242     >>> from xarray import DataArray
1243     >>> da_a = DataArray(
1244     ...     np.array([[1, 2, 3], [0.1, 0.2, 0.3], [3.2, 0.6, 1.8]]),
1245     ...     dims=("space", "time"),
1246     ...     coords=[
1247     ...         ("space", ["IA", "IL", "IN"]),
1248     ...         ("time", pd.date_range("2000-01-01", freq="1D", periods=3)),
1249     ...     ],
1250     ... )
1251     >>> da_a
1252     <xarray.DataArray (space: 3, time: 3)>
1253     array([[1. , 2. , 3. ],
1254            [0.1, 0.2, 0.3],
1255            [3.2, 0.6, 1.8]])
1256     Coordinates:
1257       * space    (space) <U2 'IA' 'IL' 'IN'
1258       * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03
1259     >>> da_b = DataArray(
1260     ...     np.array([[0.2, 0.4, 0.6], [15, 10, 5], [3.2, 0.6, 1.8]]),
1261     ...     dims=("space", "time"),
1262     ...     coords=[
1263     ...         ("space", ["IA", "IL", "IN"]),
1264     ...         ("time", pd.date_range("2000-01-01", freq="1D", periods=3)),
1265     ...     ],
1266     ... )
1267     >>> da_b
1268     <xarray.DataArray (space: 3, time: 3)>
1269     array([[ 0.2,  0.4,  0.6],
1270            [15. , 10. ,  5. ],
1271            [ 3.2,  0.6,  1.8]])
1272     Coordinates:
1273       * space    (space) <U2 'IA' 'IL' 'IN'
1274       * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03
1275     >>> xr.cov(da_a, da_b)
1276     <xarray.DataArray ()>
1277     array(-3.53055556)
1278     >>> xr.cov(da_a, da_b, dim="time")
1279     <xarray.DataArray (space: 3)>
1280     array([ 0.2       , -0.5       ,  1.69333333])
1281     Coordinates:
1282       * space    (space) <U2 'IA' 'IL' 'IN'
1283     """
1284     from xarray.core.dataarray import DataArray
1285 
1286     if any(not isinstance(arr, DataArray) for arr in [da_a, da_b]):
1287         raise TypeError(
1288             "Only xr.DataArray is supported."
1289             "Given {}.".format([type(arr) for arr in [da_a, da_b]])
1290         )
1291 
1292     return _cov_corr(da_a, da_b, dim=dim, ddof=ddof, method="cov")
1293 
1294 
1295 def corr(da_a: T_DataArray, da_b: T_DataArray, dim: Dims = None) -> T_DataArray:
1296     """
1297     Compute the Pearson correlation coefficient between
1298     two DataArray objects along a shared dimension.
1299 
1300     Parameters
1301     ----------
1302     da_a : DataArray
1303         Array to compute.
1304     da_b : DataArray
1305         Array to compute.
1306     dim : str, iterable of hashable, "..." or None, optional
1307         The dimension along which the correlation will be computed
1308 
1309     Returns
1310     -------
1311     correlation: DataArray
1312 
1313     See Also
1314     --------
1315     pandas.Series.corr : corresponding pandas function
1316     xarray.cov : underlying covariance function
1317 
1318     Examples
1319     --------
1320     >>> from xarray import DataArray
1321     >>> da_a = DataArray(
1322     ...     np.array([[1, 2, 3], [0.1, 0.2, 0.3], [3.2, 0.6, 1.8]]),
1323     ...     dims=("space", "time"),
1324     ...     coords=[
1325     ...         ("space", ["IA", "IL", "IN"]),
1326     ...         ("time", pd.date_range("2000-01-01", freq="1D", periods=3)),
1327     ...     ],
1328     ... )
1329     >>> da_a
1330     <xarray.DataArray (space: 3, time: 3)>
1331     array([[1. , 2. , 3. ],
1332            [0.1, 0.2, 0.3],
1333            [3.2, 0.6, 1.8]])
1334     Coordinates:
1335       * space    (space) <U2 'IA' 'IL' 'IN'
1336       * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03
1337     >>> da_b = DataArray(
1338     ...     np.array([[0.2, 0.4, 0.6], [15, 10, 5], [3.2, 0.6, 1.8]]),
1339     ...     dims=("space", "time"),
1340     ...     coords=[
1341     ...         ("space", ["IA", "IL", "IN"]),
1342     ...         ("time", pd.date_range("2000-01-01", freq="1D", periods=3)),
1343     ...     ],
1344     ... )
1345     >>> da_b
1346     <xarray.DataArray (space: 3, time: 3)>
1347     array([[ 0.2,  0.4,  0.6],
1348            [15. , 10. ,  5. ],
1349            [ 3.2,  0.6,  1.8]])
1350     Coordinates:
1351       * space    (space) <U2 'IA' 'IL' 'IN'
1352       * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03
1353     >>> xr.corr(da_a, da_b)
1354     <xarray.DataArray ()>
1355     array(-0.57087777)
1356     >>> xr.corr(da_a, da_b, dim="time")
1357     <xarray.DataArray (space: 3)>
1358     array([ 1., -1.,  1.])
1359     Coordinates:
1360       * space    (space) <U2 'IA' 'IL' 'IN'
1361     """
1362     from xarray.core.dataarray import DataArray
1363 
1364     if any(not isinstance(arr, DataArray) for arr in [da_a, da_b]):
1365         raise TypeError(
1366             "Only xr.DataArray is supported."
1367             "Given {}.".format([type(arr) for arr in [da_a, da_b]])
1368         )
1369 
1370     return _cov_corr(da_a, da_b, dim=dim, method="corr")
1371 
1372 
1373 def _cov_corr(
1374     da_a: T_DataArray,
1375     da_b: T_DataArray,
1376     dim: Dims = None,
1377     ddof: int = 0,
1378     method: Literal["cov", "corr", None] = None,
1379 ) -> T_DataArray:
1380     """
1381     Internal method for xr.cov() and xr.corr() so only have to
1382     sanitize the input arrays once and we don't repeat code.
1383     """
1384     # 1. Broadcast the two arrays
1385     da_a, da_b = align(da_a, da_b, join="inner", copy=False)
1386 
1387     # 2. Ignore the nans
1388     valid_values = da_a.notnull() & da_b.notnull()
1389     da_a = da_a.where(valid_values)
1390     da_b = da_b.where(valid_values)
1391     valid_count = valid_values.sum(dim) - ddof
1392 
1393     # 3. Detrend along the given dim
1394     demeaned_da_a = da_a - da_a.mean(dim=dim)
1395     demeaned_da_b = da_b - da_b.mean(dim=dim)
1396 
1397     # 4. Compute covariance along the given dim
1398     # N.B. `skipna=True` is required or auto-covariance is computed incorrectly. E.g.
1399     # Try xr.cov(da,da) for da = xr.DataArray([[1, 2], [1, np.nan]], dims=["x", "time"])
1400     cov = (demeaned_da_a.conj() * demeaned_da_b).sum(
1401         dim=dim, skipna=True, min_count=1
1402     ) / (valid_count)
1403 
1404     if method == "cov":
1405         return cov  # type: ignore[return-value]
1406 
1407     else:
1408         # compute std + corr
1409         da_a_std = da_a.std(dim=dim)
1410         da_b_std = da_b.std(dim=dim)
1411         corr = cov / (da_a_std * da_b_std)
1412         return corr  # type: ignore[return-value]
1413 
1414 
1415 def cross(
1416     a: DataArray | Variable, b: DataArray | Variable, *, dim: Hashable
1417 ) -> DataArray | Variable:
1418     """
1419     Compute the cross product of two (arrays of) vectors.
1420 
1421     The cross product of `a` and `b` in :math:`R^3` is a vector
1422     perpendicular to both `a` and `b`. The vectors in `a` and `b` are
1423     defined by the values along the dimension `dim` and can have sizes
1424     1, 2 or 3. Where the size of either `a` or `b` is
1425     1 or 2, the remaining components of the input vector is assumed to
1426     be zero and the cross product calculated accordingly. In cases where
1427     both input vectors have dimension 2, the z-component of the cross
1428     product is returned.
1429 
1430     Parameters
1431     ----------
1432     a, b : DataArray or Variable
1433         Components of the first and second vector(s).
1434     dim : hashable
1435         The dimension along which the cross product will be computed.
1436         Must be available in both vectors.
1437 
1438     Examples
1439     --------
1440     Vector cross-product with 3 dimensions:
1441 
1442     >>> a = xr.DataArray([1, 2, 3])
1443     >>> b = xr.DataArray([4, 5, 6])
1444     >>> xr.cross(a, b, dim="dim_0")
1445     <xarray.DataArray (dim_0: 3)>
1446     array([-3,  6, -3])
1447     Dimensions without coordinates: dim_0
1448 
1449     Vector cross-product with 2 dimensions, returns in the perpendicular
1450     direction:
1451 
1452     >>> a = xr.DataArray([1, 2])
1453     >>> b = xr.DataArray([4, 5])
1454     >>> xr.cross(a, b, dim="dim_0")
1455     <xarray.DataArray ()>
1456     array(-3)
1457 
1458     Vector cross-product with 3 dimensions but zeros at the last axis
1459     yields the same results as with 2 dimensions:
1460 
1461     >>> a = xr.DataArray([1, 2, 0])
1462     >>> b = xr.DataArray([4, 5, 0])
1463     >>> xr.cross(a, b, dim="dim_0")
1464     <xarray.DataArray (dim_0: 3)>
1465     array([ 0,  0, -3])
1466     Dimensions without coordinates: dim_0
1467 
1468     One vector with dimension 2:
1469 
1470     >>> a = xr.DataArray(
1471     ...     [1, 2],
1472     ...     dims=["cartesian"],
1473     ...     coords=dict(cartesian=(["cartesian"], ["x", "y"])),
1474     ... )
1475     >>> b = xr.DataArray(
1476     ...     [4, 5, 6],
1477     ...     dims=["cartesian"],
1478     ...     coords=dict(cartesian=(["cartesian"], ["x", "y", "z"])),
1479     ... )
1480     >>> xr.cross(a, b, dim="cartesian")
1481     <xarray.DataArray (cartesian: 3)>
1482     array([12, -6, -3])
1483     Coordinates:
1484       * cartesian  (cartesian) <U1 'x' 'y' 'z'
1485 
1486     One vector with dimension 2 but coords in other positions:
1487 
1488     >>> a = xr.DataArray(
1489     ...     [1, 2],
1490     ...     dims=["cartesian"],
1491     ...     coords=dict(cartesian=(["cartesian"], ["x", "z"])),
1492     ... )
1493     >>> b = xr.DataArray(
1494     ...     [4, 5, 6],
1495     ...     dims=["cartesian"],
1496     ...     coords=dict(cartesian=(["cartesian"], ["x", "y", "z"])),
1497     ... )
1498     >>> xr.cross(a, b, dim="cartesian")
1499     <xarray.DataArray (cartesian: 3)>
1500     array([-10,   2,   5])
1501     Coordinates:
1502       * cartesian  (cartesian) <U1 'x' 'y' 'z'
1503 
1504     Multiple vector cross-products. Note that the direction of the
1505     cross product vector is defined by the right-hand rule:
1506 
1507     >>> a = xr.DataArray(
1508     ...     [[1, 2, 3], [4, 5, 6]],
1509     ...     dims=("time", "cartesian"),
1510     ...     coords=dict(
1511     ...         time=(["time"], [0, 1]),
1512     ...         cartesian=(["cartesian"], ["x", "y", "z"]),
1513     ...     ),
1514     ... )
1515     >>> b = xr.DataArray(
1516     ...     [[4, 5, 6], [1, 2, 3]],
1517     ...     dims=("time", "cartesian"),
1518     ...     coords=dict(
1519     ...         time=(["time"], [0, 1]),
1520     ...         cartesian=(["cartesian"], ["x", "y", "z"]),
1521     ...     ),
1522     ... )
1523     >>> xr.cross(a, b, dim="cartesian")
1524     <xarray.DataArray (time: 2, cartesian: 3)>
1525     array([[-3,  6, -3],
1526            [ 3, -6,  3]])
1527     Coordinates:
1528       * time       (time) int64 0 1
1529       * cartesian  (cartesian) <U1 'x' 'y' 'z'
1530 
1531     Cross can be called on Datasets by converting to DataArrays and later
1532     back to a Dataset:
1533 
1534     >>> ds_a = xr.Dataset(dict(x=("dim_0", [1]), y=("dim_0", [2]), z=("dim_0", [3])))
1535     >>> ds_b = xr.Dataset(dict(x=("dim_0", [4]), y=("dim_0", [5]), z=("dim_0", [6])))
1536     >>> c = xr.cross(
1537     ...     ds_a.to_array("cartesian"), ds_b.to_array("cartesian"), dim="cartesian"
1538     ... )
1539     >>> c.to_dataset(dim="cartesian")
1540     <xarray.Dataset>
1541     Dimensions:  (dim_0: 1)
1542     Dimensions without coordinates: dim_0
1543     Data variables:
1544         x        (dim_0) int64 -3
1545         y        (dim_0) int64 6
1546         z        (dim_0) int64 -3
1547 
1548     See Also
1549     --------
1550     numpy.cross : Corresponding numpy function
1551     """
1552 
1553     if dim not in a.dims:
1554         raise ValueError(f"Dimension {dim!r} not on a")
1555     elif dim not in b.dims:
1556         raise ValueError(f"Dimension {dim!r} not on b")
1557 
1558     if not 1 <= a.sizes[dim] <= 3:
1559         raise ValueError(
1560             f"The size of {dim!r} on a must be 1, 2, or 3 to be "
1561             f"compatible with a cross product but is {a.sizes[dim]}"
1562         )
1563     elif not 1 <= b.sizes[dim] <= 3:
1564         raise ValueError(
1565             f"The size of {dim!r} on b must be 1, 2, or 3 to be "
1566             f"compatible with a cross product but is {b.sizes[dim]}"
1567         )
1568 
1569     all_dims = list(dict.fromkeys(a.dims + b.dims))
1570 
1571     if a.sizes[dim] != b.sizes[dim]:
1572         # Arrays have different sizes. Append zeros where the smaller
1573         # array is missing a value, zeros will not affect np.cross:
1574 
1575         if (
1576             not isinstance(a, Variable)  # Only used to make mypy happy.
1577             and dim in getattr(a, "coords", {})
1578             and not isinstance(b, Variable)  # Only used to make mypy happy.
1579             and dim in getattr(b, "coords", {})
1580         ):
1581             # If the arrays have coords we know which indexes to fill
1582             # with zeros:
1583             a, b = align(
1584                 a,
1585                 b,
1586                 fill_value=0,
1587                 join="outer",
1588                 exclude=set(all_dims) - {dim},
1589             )
1590         elif min(a.sizes[dim], b.sizes[dim]) == 2:
1591             # If the array doesn't have coords we can only infer
1592             # that it has composite values if the size is at least 2.
1593             # Once padded, rechunk the padded array because apply_ufunc
1594             # requires core dimensions not to be chunked:
1595             if a.sizes[dim] < b.sizes[dim]:
1596                 a = a.pad({dim: (0, 1)}, constant_values=0)
1597                 # TODO: Should pad or apply_ufunc handle correct chunking?
1598                 a = a.chunk({dim: -1}) if is_duck_dask_array(a.data) else a
1599             else:
1600                 b = b.pad({dim: (0, 1)}, constant_values=0)
1601                 # TODO: Should pad or apply_ufunc handle correct chunking?
1602                 b = b.chunk({dim: -1}) if is_duck_dask_array(b.data) else b
1603         else:
1604             raise ValueError(
1605                 f"{dim!r} on {'a' if a.sizes[dim] == 1 else 'b'} is incompatible:"
1606                 " dimensions without coordinates must have have a length of 2 or 3"
1607             )
1608 
1609     c = apply_ufunc(
1610         np.cross,
1611         a,
1612         b,
1613         input_core_dims=[[dim], [dim]],
1614         output_core_dims=[[dim] if a.sizes[dim] == 3 else []],
1615         dask="parallelized",
1616         output_dtypes=[np.result_type(a, b)],
1617     )
1618     c = c.transpose(*all_dims, missing_dims="ignore")
1619 
1620     return c
1621 
1622 
1623 def dot(
1624     *arrays,
1625     dims: Dims = None,
1626     **kwargs: Any,
1627 ):
1628     """Generalized dot product for xarray objects. Like np.einsum, but
1629     provides a simpler interface based on array dimensions.
1630 
1631     Parameters
1632     ----------
1633     *arrays : DataArray or Variable
1634         Arrays to compute.
1635     dims : str, iterable of hashable, "..." or None, optional
1636         Which dimensions to sum over. Ellipsis ('...') sums over all dimensions.
1637         If not specified, then all the common dimensions are summed over.
1638     **kwargs : dict
1639         Additional keyword arguments passed to numpy.einsum or
1640         dask.array.einsum
1641 
1642     Returns
1643     -------
1644     DataArray
1645 
1646     Examples
1647     --------
1648     >>> da_a = xr.DataArray(np.arange(3 * 2).reshape(3, 2), dims=["a", "b"])
1649     >>> da_b = xr.DataArray(np.arange(3 * 2 * 2).reshape(3, 2, 2), dims=["a", "b", "c"])
1650     >>> da_c = xr.DataArray(np.arange(2 * 3).reshape(2, 3), dims=["c", "d"])
1651 
1652     >>> da_a
1653     <xarray.DataArray (a: 3, b: 2)>
1654     array([[0, 1],
1655            [2, 3],
1656            [4, 5]])
1657     Dimensions without coordinates: a, b
1658 
1659     >>> da_b
1660     <xarray.DataArray (a: 3, b: 2, c: 2)>
1661     array([[[ 0,  1],
1662             [ 2,  3]],
1663     <BLANKLINE>
1664            [[ 4,  5],
1665             [ 6,  7]],
1666     <BLANKLINE>
1667            [[ 8,  9],
1668             [10, 11]]])
1669     Dimensions without coordinates: a, b, c
1670 
1671     >>> da_c
1672     <xarray.DataArray (c: 2, d: 3)>
1673     array([[0, 1, 2],
1674            [3, 4, 5]])
1675     Dimensions without coordinates: c, d
1676 
1677     >>> xr.dot(da_a, da_b, dims=["a", "b"])
1678     <xarray.DataArray (c: 2)>
1679     array([110, 125])
1680     Dimensions without coordinates: c
1681 
1682     >>> xr.dot(da_a, da_b, dims=["a"])
1683     <xarray.DataArray (b: 2, c: 2)>
1684     array([[40, 46],
1685            [70, 79]])
1686     Dimensions without coordinates: b, c
1687 
1688     >>> xr.dot(da_a, da_b, da_c, dims=["b", "c"])
1689     <xarray.DataArray (a: 3, d: 3)>
1690     array([[  9,  14,  19],
1691            [ 93, 150, 207],
1692            [273, 446, 619]])
1693     Dimensions without coordinates: a, d
1694 
1695     >>> xr.dot(da_a, da_b)
1696     <xarray.DataArray (c: 2)>
1697     array([110, 125])
1698     Dimensions without coordinates: c
1699 
1700     >>> xr.dot(da_a, da_b, dims=...)
1701     <xarray.DataArray ()>
1702     array(235)
1703     """
1704     from xarray.core.dataarray import DataArray
1705     from xarray.core.variable import Variable
1706 
1707     if any(not isinstance(arr, (Variable, DataArray)) for arr in arrays):
1708         raise TypeError(
1709             "Only xr.DataArray and xr.Variable are supported."
1710             "Given {}.".format([type(arr) for arr in arrays])
1711         )
1712 
1713     if len(arrays) == 0:
1714         raise TypeError("At least one array should be given.")
1715 
1716     common_dims: set[Hashable] = set.intersection(*(set(arr.dims) for arr in arrays))
1717     all_dims = []
1718     for arr in arrays:
1719         all_dims += [d for d in arr.dims if d not in all_dims]
1720 
1721     einsum_axes = "abcdefghijklmnopqrstuvwxyz"
1722     dim_map = {d: einsum_axes[i] for i, d in enumerate(all_dims)}
1723 
1724     if dims is ...:
1725         dims = all_dims
1726     elif isinstance(dims, str):
1727         dims = (dims,)
1728     elif dims is None:
1729         # find dimensions that occur more than one times
1730         dim_counts: Counter = Counter()
1731         for arr in arrays:
1732             dim_counts.update(arr.dims)
1733         dims = tuple(d for d, c in dim_counts.items() if c > 1)
1734 
1735     dot_dims: set[Hashable] = set(dims)
1736 
1737     # dimensions to be parallelized
1738     broadcast_dims = common_dims - dot_dims
1739     input_core_dims = [
1740         [d for d in arr.dims if d not in broadcast_dims] for arr in arrays
1741     ]
1742     output_core_dims = [
1743         [d for d in all_dims if d not in dot_dims and d not in broadcast_dims]
1744     ]
1745 
1746     # construct einsum subscripts, such as '...abc,...ab->...c'
1747     # Note: input_core_dims are always moved to the last position
1748     subscripts_list = [
1749         "..." + "".join(dim_map[d] for d in ds) for ds in input_core_dims
1750     ]
1751     subscripts = ",".join(subscripts_list)
1752     subscripts += "->..." + "".join(dim_map[d] for d in output_core_dims[0])
1753 
1754     join = OPTIONS["arithmetic_join"]
1755     # using "inner" emulates `(a * b).sum()` for all joins (except "exact")
1756     if join != "exact":
1757         join = "inner"
1758 
1759     # subscripts should be passed to np.einsum as arg, not as kwargs. We need
1760     # to construct a partial function for apply_ufunc to work.
1761     func = functools.partial(duck_array_ops.einsum, subscripts, **kwargs)
1762     result = apply_ufunc(
1763         func,
1764         *arrays,
1765         input_core_dims=input_core_dims,
1766         output_core_dims=output_core_dims,
1767         join=join,
1768         dask="allowed",
1769     )
1770     return result.transpose(*all_dims, missing_dims="ignore")
1771 
1772 
1773 def where(cond, x, y, keep_attrs=None):
1774     """Return elements from `x` or `y` depending on `cond`.
1775 
1776     Performs xarray-like broadcasting across input arguments.
1777 
1778     All dimension coordinates on `x` and `y`  must be aligned with each
1779     other and with `cond`.
1780 
1781     Parameters
1782     ----------
1783     cond : scalar, array, Variable, DataArray or Dataset
1784         When True, return values from `x`, otherwise returns values from `y`.
1785     x : scalar, array, Variable, DataArray or Dataset
1786         values to choose from where `cond` is True
1787     y : scalar, array, Variable, DataArray or Dataset
1788         values to choose from where `cond` is False
1789     keep_attrs : bool or str or callable, optional
1790         How to treat attrs. If True, keep the attrs of `x`.
1791 
1792     Returns
1793     -------
1794     Dataset, DataArray, Variable or array
1795         In priority order: Dataset, DataArray, Variable or array, whichever
1796         type appears as an input argument.
1797 
1798     Examples
1799     --------
1800     >>> x = xr.DataArray(
1801     ...     0.1 * np.arange(10),
1802     ...     dims=["lat"],
1803     ...     coords={"lat": np.arange(10)},
1804     ...     name="sst",
1805     ... )
1806     >>> x
1807     <xarray.DataArray 'sst' (lat: 10)>
1808     array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])
1809     Coordinates:
1810       * lat      (lat) int64 0 1 2 3 4 5 6 7 8 9
1811 
1812     >>> xr.where(x < 0.5, x, x * 100)
1813     <xarray.DataArray 'sst' (lat: 10)>
1814     array([ 0. ,  0.1,  0.2,  0.3,  0.4, 50. , 60. , 70. , 80. , 90. ])
1815     Coordinates:
1816       * lat      (lat) int64 0 1 2 3 4 5 6 7 8 9
1817 
1818     >>> y = xr.DataArray(
1819     ...     0.1 * np.arange(9).reshape(3, 3),
1820     ...     dims=["lat", "lon"],
1821     ...     coords={"lat": np.arange(3), "lon": 10 + np.arange(3)},
1822     ...     name="sst",
1823     ... )
1824     >>> y
1825     <xarray.DataArray 'sst' (lat: 3, lon: 3)>
1826     array([[0. , 0.1, 0.2],
1827            [0.3, 0.4, 0.5],
1828            [0.6, 0.7, 0.8]])
1829     Coordinates:
1830       * lat      (lat) int64 0 1 2
1831       * lon      (lon) int64 10 11 12
1832 
1833     >>> xr.where(y.lat < 1, y, -1)
1834     <xarray.DataArray (lat: 3, lon: 3)>
1835     array([[ 0. ,  0.1,  0.2],
1836            [-1. , -1. , -1. ],
1837            [-1. , -1. , -1. ]])
1838     Coordinates:
1839       * lat      (lat) int64 0 1 2
1840       * lon      (lon) int64 10 11 12
1841 
1842     >>> cond = xr.DataArray([True, False], dims=["x"])
1843     >>> x = xr.DataArray([1, 2], dims=["y"])
1844     >>> xr.where(cond, x, 0)
1845     <xarray.DataArray (x: 2, y: 2)>
1846     array([[1, 2],
1847            [0, 0]])
1848     Dimensions without coordinates: x, y
1849 
1850     See Also
1851     --------
1852     numpy.where : corresponding numpy function
1853     Dataset.where, DataArray.where :
1854         equivalent methods
1855     """
1856     from xarray.core.dataset import Dataset
1857 
1858     if keep_attrs is None:
1859         keep_attrs = _get_keep_attrs(default=False)
1860 
1861     # alignment for three arguments is complicated, so don't support it yet
1862     result = apply_ufunc(
1863         duck_array_ops.where,
1864         cond,
1865         x,
1866         y,
1867         join="exact",
1868         dataset_join="exact",
1869         dask="allowed",
1870         keep_attrs=keep_attrs,
1871     )
1872 
1873     # keep the attributes of x, the second parameter, by default to
1874     # be consistent with the `where` method of `DataArray` and `Dataset`
1875     # rebuild the attrs from x at each level of the output, which could be
1876     # Dataset, DataArray, or Variable, and also handle coords
1877     if keep_attrs is True and hasattr(result, "attrs"):
1878         if isinstance(y, Dataset) and not isinstance(x, Dataset):
1879             # handle special case where x gets promoted to Dataset
1880             result.attrs = {}
1881             if getattr(x, "name", None) in result.data_vars:
1882                 result[x.name].attrs = getattr(x, "attrs", {})
1883         else:
1884             # otherwise, fill in global attrs and variable attrs (if they exist)
1885             result.attrs = getattr(x, "attrs", {})
1886             for v in getattr(result, "data_vars", []):
1887                 result[v].attrs = getattr(getattr(x, v, None), "attrs", {})
1888         for c in getattr(result, "coords", []):
1889             # always fill coord attrs of x
1890             result[c].attrs = getattr(getattr(x, c, None), "attrs", {})
1891 
1892     return result
1893 
1894 
1895 @overload
1896 def polyval(
1897     coord: DataArray, coeffs: DataArray, degree_dim: Hashable = "degree"
1898 ) -> DataArray:
1899     ...
1900 
1901 
1902 @overload
1903 def polyval(
1904     coord: DataArray, coeffs: Dataset, degree_dim: Hashable = "degree"
1905 ) -> Dataset:
1906     ...
1907 
1908 
1909 @overload
1910 def polyval(
1911     coord: Dataset, coeffs: DataArray, degree_dim: Hashable = "degree"
1912 ) -> Dataset:
1913     ...
1914 
1915 
1916 @overload
1917 def polyval(
1918     coord: Dataset, coeffs: Dataset, degree_dim: Hashable = "degree"
1919 ) -> Dataset:
1920     ...
1921 
1922 
1923 @overload
1924 def polyval(
1925     coord: Dataset | DataArray,
1926     coeffs: Dataset | DataArray,
1927     degree_dim: Hashable = "degree",
1928 ) -> Dataset | DataArray:
1929     ...
1930 
1931 
1932 def polyval(
1933     coord: Dataset | DataArray,
1934     coeffs: Dataset | DataArray,
1935     degree_dim: Hashable = "degree",
1936 ) -> Dataset | DataArray:
1937     """Evaluate a polynomial at specific values
1938 
1939     Parameters
1940     ----------
1941     coord : DataArray or Dataset
1942         Values at which to evaluate the polynomial.
1943     coeffs : DataArray or Dataset
1944         Coefficients of the polynomial.
1945     degree_dim : Hashable, default: "degree"
1946         Name of the polynomial degree dimension in `coeffs`.
1947 
1948     Returns
1949     -------
1950     DataArray or Dataset
1951         Evaluated polynomial.
1952 
1953     See Also
1954     --------
1955     xarray.DataArray.polyfit
1956     numpy.polynomial.polynomial.polyval
1957     """
1958 
1959     if degree_dim not in coeffs._indexes:
1960         raise ValueError(
1961             f"Dimension `{degree_dim}` should be a coordinate variable with labels."
1962         )
1963     if not np.issubdtype(coeffs[degree_dim].dtype, np.integer):
1964         raise ValueError(
1965             f"Dimension `{degree_dim}` should be of integer dtype. Received {coeffs[degree_dim].dtype} instead."
1966         )
1967     max_deg = coeffs[degree_dim].max().item()
1968     coeffs = coeffs.reindex(
1969         {degree_dim: np.arange(max_deg + 1)}, fill_value=0, copy=False
1970     )
1971     coord = _ensure_numeric(coord)
1972 
1973     # using Horner's method
1974     # https://en.wikipedia.org/wiki/Horner%27s_method
1975     res = zeros_like(coord) + coeffs.isel({degree_dim: max_deg}, drop=True)
1976     for deg in range(max_deg - 1, -1, -1):
1977         res *= coord
1978         res += coeffs.isel({degree_dim: deg}, drop=True)
1979 
1980     return res
1981 
1982 
1983 def _ensure_numeric(data: Dataset | DataArray) -> Dataset | DataArray:
1984     """Converts all datetime64 variables to float64
1985 
1986     Parameters
1987     ----------
1988     data : DataArray or Dataset
1989         Variables with possible datetime dtypes.
1990 
1991     Returns
1992     -------
1993     DataArray or Dataset
1994         Variables with datetime64 dtypes converted to float64.
1995     """
1996     from xarray.core.dataset import Dataset
1997 
1998     def _cfoffset(x: DataArray) -> Any:
1999         scalar = x.compute().data[0]
2000         if not is_scalar(scalar):
2001             # we do not get a scalar back on dask == 2021.04.1
2002             scalar = scalar.item()
2003         return type(scalar)(1970, 1, 1)
2004 
2005     def to_floatable(x: DataArray) -> DataArray:
2006         if x.dtype.kind in "MO":
2007             # datetimes (CFIndexes are object type)
2008             offset = (
2009                 np.datetime64("1970-01-01") if x.dtype.kind == "M" else _cfoffset(x)
2010             )
2011             return x.copy(
2012                 data=datetime_to_numeric(x.data, offset=offset, datetime_unit="ns"),
2013             )
2014         elif x.dtype.kind == "m":
2015             # timedeltas
2016             return x.astype(float)
2017         return x
2018 
2019     if isinstance(data, Dataset):
2020         return data.map(to_floatable)
2021     else:
2022         return to_floatable(data)
2023 
2024 
2025 def _calc_idxminmax(
2026     *,
2027     array,
2028     func: Callable,
2029     dim: Hashable | None = None,
2030     skipna: bool | None = None,
2031     fill_value: Any = dtypes.NA,
2032     keep_attrs: bool | None = None,
2033 ):
2034     """Apply common operations for idxmin and idxmax."""
2035     # This function doesn't make sense for scalars so don't try
2036     if not array.ndim:
2037         raise ValueError("This function does not apply for scalars")
2038 
2039     if dim is not None:
2040         pass  # Use the dim if available
2041     elif array.ndim == 1:
2042         # it is okay to guess the dim if there is only 1
2043         dim = array.dims[0]
2044     else:
2045         # The dim is not specified and ambiguous.  Don't guess.
2046         raise ValueError("Must supply 'dim' argument for multidimensional arrays")
2047 
2048     if dim not in array.dims:
2049         raise KeyError(f'Dimension "{dim}" not in dimension')
2050     if dim not in array.coords:
2051         raise KeyError(f'Dimension "{dim}" does not have coordinates')
2052 
2053     # These are dtypes with NaN values argmin and argmax can handle
2054     na_dtypes = "cfO"
2055 
2056     if skipna or (skipna is None and array.dtype.kind in na_dtypes):
2057         # Need to skip NaN values since argmin and argmax can't handle them
2058         allna = array.isnull().all(dim)
2059         array = array.where(~allna, 0)
2060 
2061     # This will run argmin or argmax.
2062     indx = func(array, dim=dim, axis=None, keep_attrs=keep_attrs, skipna=skipna)
2063 
2064     # Handle dask arrays.
2065     if is_duck_dask_array(array.data):
2066         import dask.array
2067 
2068         chunks = dict(zip(array.dims, array.chunks))
2069         dask_coord = dask.array.from_array(array[dim].data, chunks=chunks[dim])
2070         res = indx.copy(data=dask_coord[indx.data.ravel()].reshape(indx.shape))
2071         # we need to attach back the dim name
2072         res.name = dim
2073     else:
2074         res = array[dim][(indx,)]
2075         # The dim is gone but we need to remove the corresponding coordinate.
2076         del res.coords[dim]
2077 
2078     if skipna or (skipna is None and array.dtype.kind in na_dtypes):
2079         # Put the NaN values back in after removing them
2080         res = res.where(~allna, fill_value)
2081 
2082     # Copy attributes from argmin/argmax, if any
2083     res.attrs = indx.attrs
2084 
2085     return res
2086 
2087 
2088 _T = TypeVar("_T", bound=Union["Dataset", "DataArray"])
2089 _U = TypeVar("_U", bound=Union["Dataset", "DataArray"])
2090 _V = TypeVar("_V", bound=Union["Dataset", "DataArray"])
2091 
2092 
2093 @overload
2094 def unify_chunks(__obj: _T) -> tuple[_T]:
2095     ...
2096 
2097 
2098 @overload
2099 def unify_chunks(__obj1: _T, __obj2: _U) -> tuple[_T, _U]:
2100     ...
2101 
2102 
2103 @overload
2104 def unify_chunks(__obj1: _T, __obj2: _U, __obj3: _V) -> tuple[_T, _U, _V]:
2105     ...
2106 
2107 
2108 @overload
2109 def unify_chunks(*objects: Dataset | DataArray) -> tuple[Dataset | DataArray, ...]:
2110     ...
2111 
2112 
2113 def unify_chunks(*objects: Dataset | DataArray) -> tuple[Dataset | DataArray, ...]:
2114     """
2115     Given any number of Dataset and/or DataArray objects, returns
2116     new objects with unified chunk size along all chunked dimensions.
2117 
2118     Returns
2119     -------
2120     unified (DataArray or Dataset) – Tuple of objects with the same type as
2121     *objects with consistent chunk sizes for all dask-array variables
2122 
2123     See Also
2124     --------
2125     dask.array.core.unify_chunks
2126     """
2127     from xarray.core.dataarray import DataArray
2128 
2129     # Convert all objects to datasets
2130     datasets = [
2131         obj._to_temp_dataset() if isinstance(obj, DataArray) else obj.copy()
2132         for obj in objects
2133     ]
2134 
2135     # Get arguments to pass into dask.array.core.unify_chunks
2136     unify_chunks_args = []
2137     sizes: dict[Hashable, int] = {}
2138     for ds in datasets:
2139         for v in ds._variables.values():
2140             if v.chunks is not None:
2141                 # Check that sizes match across different datasets
2142                 for dim, size in v.sizes.items():
2143                     try:
2144                         if sizes[dim] != size:
2145                             raise ValueError(
2146                                 f"Dimension {dim!r} size mismatch: {sizes[dim]} != {size}"
2147                             )
2148                     except KeyError:
2149                         sizes[dim] = size
2150                 unify_chunks_args += [v._data, v._dims]
2151 
2152     # No dask arrays: Return inputs
2153     if not unify_chunks_args:
2154         return objects
2155 
2156     # Run dask.array.core.unify_chunks
2157     from dask.array.core import unify_chunks
2158 
2159     _, dask_data = unify_chunks(*unify_chunks_args)
2160     dask_data_iter = iter(dask_data)
2161     out: list[Dataset | DataArray] = []
2162     for obj, ds in zip(objects, datasets):
2163         for k, v in ds._variables.items():
2164             if v.chunks is not None:
2165                 ds._variables[k] = v.copy(data=next(dask_data_iter))
2166         out.append(obj._from_temp_dataset(ds) if isinstance(obj, DataArray) else ds)
2167 
2168     return tuple(out)

</code><intended_behavior>Abstract the operation handling to be compatible with multiple array backends, enabling the support for cubed and other potential future backends. This might involve creating helper functions or classes responsible for executing backend-specific code.</intended_behavior>]
2025-10-17 16:02:30.616 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-17 16:02:30.617 | INFO     | Search-only: skipping patch selection
2025-10-17 16:02:30.716 | INFO     | Task pydata__xarray-7019 completed successfully.
2025-10-17 16:02:30.716 | INFO     | No patch generated. You can try running ACR again.
