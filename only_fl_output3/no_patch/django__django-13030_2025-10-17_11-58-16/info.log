2025-10-17 11:58:16.633 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-17 11:58:19.738 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=534, output_tokens=12, cost=0.005700
2025-10-17 11:58:33.181 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=714, output_tokens=615, cost=0.025590
2025-10-17 11:58:34.449 | INFO     | registered non reproducing test 0
2025-10-17 11:58:49.015 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2776, output_tokens=701, cost=0.048790
2025-10-17 11:58:50.135 | INFO     | registered non reproducing test 1
2025-10-17 11:59:06.510 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2071, output_tokens=720, cost=0.042310
2025-10-17 11:59:07.611 | INFO     | registered non reproducing test 2
2025-10-17 11:59:07.612 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-17 11:59:07.618 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 11:59:16.207 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=842, output_tokens=382, cost=0.019880
2025-10-17 11:59:16.216 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 11:59:18.718 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=736, output_tokens=44, cost=0.008680
2025-10-17 11:59:18.719 | DEBUG    | Extracted a valid json.
2025-10-17 11:59:18.720 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_method(\"prefetch_related\")",
        "search_class(\"ForeignKey\")",
        "search_code(\"IN (NULL\")"
    ],
    "bug_locations": []
}
2025-10-17 11:59:18.777 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-17 11:59:18.777 | DEBUG    | <Agent search> Analyzing search results.
2025-10-17 11:59:43.490 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1822, output_tokens=694, cost=0.039040
2025-10-17 11:59:43.498 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 11:59:54.436 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2732, output_tokens=449, cost=0.040790
2025-10-17 11:59:54.445 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 11:59:59.321 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=803, output_tokens=146, cost=0.012410
2025-10-17 11:59:59.322 | DEBUG    | Extracted a valid json.
2025-10-17 11:59:59.322 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "django/db/models/query.py",
            "class": "",
            "method": "",
            "intended_behavior": "The method constructing the IN query for prefetch_related should exclude NULL values from the list of IDs it uses to prefetch related objects. This change aims to optimize query performance and integrity by ensuring that NULL values, which won't match any related objects (since FK constraints prevent such matches), are not used in an IN clause. This behavior should be implemented in a way that does not affect other functionalities of prefetch_related, such as handling other valid lookup values or the ability to prefetch multiple related objects efficiently."
        }
    ]
}
2025-10-17 11:59:59.335 | DEBUG    | Bug location extracted successfully: [<file>django/db/models/query.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 The main QuerySet implementation. This provides the public API for the ORM.
3 """
4 
5 import copy
6 import operator
7 import warnings
8 from collections import namedtuple
9 from functools import lru_cache
10 from itertools import chain
11 
12 import django
13 from django.conf import settings
14 from django.core import exceptions
15 from django.db import (
16     DJANGO_VERSION_PICKLE_KEY, IntegrityError, NotSupportedError, connections,
17     router, transaction,
18 )
19 from django.db.models import AutoField, DateField, DateTimeField, sql
20 from django.db.models.constants import LOOKUP_SEP
21 from django.db.models.deletion import Collector
22 from django.db.models.expressions import Case, Expression, F, Value, When
23 from django.db.models.functions import Cast, Trunc
24 from django.db.models.query_utils import FilteredRelation, Q
25 from django.db.models.sql.constants import CURSOR, GET_ITERATOR_CHUNK_SIZE
26 from django.db.models.utils import resolve_callables
27 from django.utils import timezone
28 from django.utils.functional import cached_property, partition
29 
30 # The maximum number of results to fetch in a get() query.
31 MAX_GET_RESULTS = 21
32 
33 # The maximum number of items to display in a QuerySet.__repr__
34 REPR_OUTPUT_SIZE = 20
35 
36 
37 class BaseIterable:
38     def __init__(self, queryset, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE):
39         self.queryset = queryset
40         self.chunked_fetch = chunked_fetch
41         self.chunk_size = chunk_size
42 
43 
44 class ModelIterable(BaseIterable):
45     """Iterable that yields a model instance for each row."""
46 
47     def __iter__(self):
48         queryset = self.queryset
49         db = queryset.db
50         compiler = queryset.query.get_compiler(using=db)
51         # Execute the query. This will also fill compiler.select, klass_info,
52         # and annotations.
53         results = compiler.execute_sql(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)
54         select, klass_info, annotation_col_map = (compiler.select, compiler.klass_info,
55                                                   compiler.annotation_col_map)
56         model_cls = klass_info['model']
57         select_fields = klass_info['select_fields']
58         model_fields_start, model_fields_end = select_fields[0], select_fields[-1] + 1
59         init_list = [f[0].target.attname
60                      for f in select[model_fields_start:model_fields_end]]
61         related_populators = get_related_populators(klass_info, select, db)
62         known_related_objects = [
63             (field, related_objs, operator.attrgetter(*[
64                 field.attname
65                 if from_field == 'self' else
66                 queryset.model._meta.get_field(from_field).attname
67                 for from_field in field.from_fields
68             ])) for field, related_objs in queryset._known_related_objects.items()
69         ]
70         for row in compiler.results_iter(results):
71             obj = model_cls.from_db(db, init_list, row[model_fields_start:model_fields_end])
72             for rel_populator in related_populators:
73                 rel_populator.populate(row, obj)
74             if annotation_col_map:
75                 for attr_name, col_pos in annotation_col_map.items():
76                     setattr(obj, attr_name, row[col_pos])
77 
78             # Add the known related objects to the model.
79             for field, rel_objs, rel_getter in known_related_objects:
80                 # Avoid overwriting objects loaded by, e.g., select_related().
81                 if field.is_cached(obj):
82                     continue
83                 rel_obj_id = rel_getter(obj)
84                 try:
85                     rel_obj = rel_objs[rel_obj_id]
86                 except KeyError:
87                     pass  # May happen in qs1 | qs2 scenarios.
88                 else:
89                     setattr(obj, field.name, rel_obj)
90 
91             yield obj
92 
93 
94 class ValuesIterable(BaseIterable):
95     """
96     Iterable returned by QuerySet.values() that yields a dict for each row.
97     """
98 
99     def __iter__(self):
100         queryset = self.queryset
101         query = queryset.query
102         compiler = query.get_compiler(queryset.db)
103 
104         # extra(select=...) cols are always at the start of the row.
105         names = [
106             *query.extra_select,
107             *query.values_select,
108             *query.annotation_select,
109         ]
110         indexes = range(len(names))
111         for row in compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size):
112             yield {names[i]: row[i] for i in indexes}
113 
114 
115 class ValuesListIterable(BaseIterable):
116     """
117     Iterable returned by QuerySet.values_list(flat=False) that yields a tuple
118     for each row.
119     """
120 
121     def __iter__(self):
122         queryset = self.queryset
123         query = queryset.query
124         compiler = query.get_compiler(queryset.db)
125 
126         if queryset._fields:
127             # extra(select=...) cols are always at the start of the row.
128             names = [
129                 *query.extra_select,
130                 *query.values_select,
131                 *query.annotation_select,
132             ]
133             fields = [*queryset._fields, *(f for f in query.annotation_select if f not in queryset._fields)]
134             if fields != names:
135                 # Reorder according to fields.
136                 index_map = {name: idx for idx, name in enumerate(names)}
137                 rowfactory = operator.itemgetter(*[index_map[f] for f in fields])
138                 return map(
139                     rowfactory,
140                     compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)
141                 )
142         return compiler.results_iter(tuple_expected=True, chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)
143 
144 
145 class NamedValuesListIterable(ValuesListIterable):
146     """
147     Iterable returned by QuerySet.values_list(named=True) that yields a
148     namedtuple for each row.
149     """
150 
151     @staticmethod
152     @lru_cache()
153     def create_namedtuple_class(*names):
154         # Cache namedtuple() with @lru_cache() since it's too slow to be
155         # called for every QuerySet evaluation.
156         return namedtuple('Row', names)
157 
158     def __iter__(self):
159         queryset = self.queryset
160         if queryset._fields:
161             names = queryset._fields
162         else:
163             query = queryset.query
164             names = [*query.extra_select, *query.values_select, *query.annotation_select]
165         tuple_class = self.create_namedtuple_class(*names)
166         new = tuple.__new__
167         for row in super().__iter__():
168             yield new(tuple_class, row)
169 
170 
171 class FlatValuesListIterable(BaseIterable):
172     """
173     Iterable returned by QuerySet.values_list(flat=True) that yields single
174     values.
175     """
176 
177     def __iter__(self):
178         queryset = self.queryset
179         compiler = queryset.query.get_compiler(queryset.db)
180         for row in compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size):
181             yield row[0]
182 
183 
184 class QuerySet:
185     """Represent a lazy database lookup for a set of objects."""
186 
187     def __init__(self, model=None, query=None, using=None, hints=None):
188         self.model = model
189         self._db = using
190         self._hints = hints or {}
191         self._query = query or sql.Query(self.model)
192         self._result_cache = None
193         self._sticky_filter = False
194         self._for_write = False
195         self._prefetch_related_lookups = ()
196         self._prefetch_done = False
197         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
198         self._iterable_class = ModelIterable
199         self._fields = None
200         self._defer_next_filter = False
201         self._deferred_filter = None
202 
203     @property
204     def query(self):
205         if self._deferred_filter:
206             negate, args, kwargs = self._deferred_filter
207             self._filter_or_exclude_inplace(negate, *args, **kwargs)
208             self._deferred_filter = None
209         return self._query
210 
211     @query.setter
212     def query(self, value):
213         self._query = value
214 
215     def as_manager(cls):
216         # Address the circular dependency between `Queryset` and `Manager`.
217         from django.db.models.manager import Manager
218         manager = Manager.from_queryset(cls)()
219         manager._built_with_as_manager = True
220         return manager
221     as_manager.queryset_only = True
222     as_manager = classmethod(as_manager)
223 
224     ########################
225     # PYTHON MAGIC METHODS #
226     ########################
227 
228     def __deepcopy__(self, memo):
229         """Don't populate the QuerySet's cache."""
230         obj = self.__class__()
231         for k, v in self.__dict__.items():
232             if k == '_result_cache':
233                 obj.__dict__[k] = None
234             else:
235                 obj.__dict__[k] = copy.deepcopy(v, memo)
236         return obj
237 
238     def __getstate__(self):
239         # Force the cache to be fully populated.
240         self._fetch_all()
241         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
242 
243     def __setstate__(self, state):
244         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
245         if pickled_version:
246             if pickled_version != django.__version__:
247                 warnings.warn(
248                     "Pickled queryset instance's Django version %s does not "
249                     "match the current version %s."
250                     % (pickled_version, django.__version__),
251                     RuntimeWarning,
252                     stacklevel=2,
253                 )
254         else:
255             warnings.warn(
256                 "Pickled queryset instance's Django version is not specified.",
257                 RuntimeWarning,
258                 stacklevel=2,
259             )
260         self.__dict__.update(state)
261 
262     def __repr__(self):
263         data = list(self[:REPR_OUTPUT_SIZE + 1])
264         if len(data) > REPR_OUTPUT_SIZE:
265             data[-1] = "...(remaining elements truncated)..."
266         return '<%s %r>' % (self.__class__.__name__, data)
267 
268     def __len__(self):
269         self._fetch_all()
270         return len(self._result_cache)
271 
272     def __iter__(self):
273         """
274         The queryset iterator protocol uses three nested iterators in the
275         default case:
276             1. sql.compiler.execute_sql()
277                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
278                  using cursor.fetchmany(). This part is responsible for
279                  doing some column masking, and returning the rows in chunks.
280             2. sql.compiler.results_iter()
281                - Returns one row at time. At this point the rows are still just
282                  tuples. In some cases the return values are converted to
283                  Python values at this location.
284             3. self.iterator()
285                - Responsible for turning the rows into model objects.
286         """
287         self._fetch_all()
288         return iter(self._result_cache)
289 
290     def __bool__(self):
291         self._fetch_all()
292         return bool(self._result_cache)
293 
294     def __getitem__(self, k):
295         """Retrieve an item or slice from the set of results."""
296         if not isinstance(k, (int, slice)):
297             raise TypeError(
298                 'QuerySet indices must be integers or slices, not %s.'
299                 % type(k).__name__
300             )
301         assert ((not isinstance(k, slice) and (k >= 0)) or
302                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
303                  (k.stop is None or k.stop >= 0))), \
304             "Negative indexing is not supported."
305 
306         if self._result_cache is not None:
307             return self._result_cache[k]
308 
309         if isinstance(k, slice):
310             qs = self._chain()
311             if k.start is not None:
312                 start = int(k.start)
313             else:
314                 start = None
315             if k.stop is not None:
316                 stop = int(k.stop)
317             else:
318                 stop = None
319             qs.query.set_limits(start, stop)
320             return list(qs)[::k.step] if k.step else qs
321 
322         qs = self._chain()
323         qs.query.set_limits(k, k + 1)
324         qs._fetch_all()
325         return qs._result_cache[0]
326 
327     def __class_getitem__(cls, *args, **kwargs):
328         return cls
329 
330     def __and__(self, other):
331         self._merge_sanity_check(other)
332         if isinstance(other, EmptyQuerySet):
333             return other
334         if isinstance(self, EmptyQuerySet):
335             return self
336         combined = self._chain()
337         combined._merge_known_related_objects(other)
338         combined.query.combine(other.query, sql.AND)
339         return combined
340 
341     def __or__(self, other):
342         self._merge_sanity_check(other)
343         if isinstance(self, EmptyQuerySet):
344             return other
345         if isinstance(other, EmptyQuerySet):
346             return self
347         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
348         combined = query._chain()
349         combined._merge_known_related_objects(other)
350         if not other.query.can_filter():
351             other = other.model._base_manager.filter(pk__in=other.values('pk'))
352         combined.query.combine(other.query, sql.OR)
353         return combined
354 
355     ####################################
356     # METHODS THAT DO DATABASE QUERIES #
357     ####################################
358 
359     def _iterator(self, use_chunked_fetch, chunk_size):
360         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
361 
362     def iterator(self, chunk_size=2000):
363         """
364         An iterator over the results from applying this QuerySet to the
365         database.
366         """
367         if chunk_size <= 0:
368             raise ValueError('Chunk size must be strictly positive.')
369         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
370         return self._iterator(use_chunked_fetch, chunk_size)
371 
372     def aggregate(self, *args, **kwargs):
373         """
374         Return a dictionary containing the calculations (aggregation)
375         over the current queryset.
376 
377         If args is present the expression is passed as a kwarg using
378         the Aggregate object's default alias.
379         """
380         if self.query.distinct_fields:
381             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
382         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
383         for arg in args:
384             # The default_alias property raises TypeError if default_alias
385             # can't be set automatically or AttributeError if it isn't an
386             # attribute.
387             try:
388                 arg.default_alias
389             except (AttributeError, TypeError):
390                 raise TypeError("Complex aggregates require an alias")
391             kwargs[arg.default_alias] = arg
392 
393         query = self.query.chain()
394         for (alias, aggregate_expr) in kwargs.items():
395             query.add_annotation(aggregate_expr, alias, is_summary=True)
396             if not query.annotations[alias].contains_aggregate:
397                 raise TypeError("%s is not an aggregate expression" % alias)
398         return query.get_aggregation(self.db, kwargs)
399 
400     def count(self):
401         """
402         Perform a SELECT COUNT() and return the number of records as an
403         integer.
404 
405         If the QuerySet is already fully cached, return the length of the
406         cached results set to avoid multiple SELECT COUNT(*) calls.
407         """
408         if self._result_cache is not None:
409             return len(self._result_cache)
410 
411         return self.query.get_count(using=self.db)
412 
413     def get(self, *args, **kwargs):
414         """
415         Perform the query and return a single object matching the given
416         keyword arguments.
417         """
418         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
419         if self.query.can_filter() and not self.query.distinct_fields:
420             clone = clone.order_by()
421         limit = None
422         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
423             limit = MAX_GET_RESULTS
424             clone.query.set_limits(high=limit)
425         num = len(clone)
426         if num == 1:
427             return clone._result_cache[0]
428         if not num:
429             raise self.model.DoesNotExist(
430                 "%s matching query does not exist." %
431                 self.model._meta.object_name
432             )
433         raise self.model.MultipleObjectsReturned(
434             'get() returned more than one %s -- it returned %s!' % (
435                 self.model._meta.object_name,
436                 num if not limit or num < limit else 'more than %s' % (limit - 1),
437             )
438         )
439 
440     def create(self, **kwargs):
441         """
442         Create a new object with the given kwargs, saving it to the database
443         and returning the created object.
444         """
445         obj = self.model(**kwargs)
446         self._for_write = True
447         obj.save(force_insert=True, using=self.db)
448         return obj
449 
450     def _populate_pk_values(self, objs):
451         for obj in objs:
452             if obj.pk is None:
453                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
454 
455     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
456         """
457         Insert each of the instances into the database. Do *not* call
458         save() on each of the instances, do not send any pre/post_save
459         signals, and do not set the primary key attribute if it is an
460         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
461         Multi-table models are not supported.
462         """
463         # When you bulk insert you don't get the primary keys back (if it's an
464         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
465         # you can't insert into the child tables which references this. There
466         # are two workarounds:
467         # 1) This could be implemented if you didn't have an autoincrement pk
468         # 2) You could do it by doing O(n) normal inserts into the parent
469         #    tables to get the primary keys back and then doing a single bulk
470         #    insert into the childmost table.
471         # We currently set the primary keys on the objects when using
472         # PostgreSQL via the RETURNING ID clause. It should be possible for
473         # Oracle as well, but the semantics for extracting the primary keys is
474         # trickier so it's not done yet.
475         assert batch_size is None or batch_size > 0
476         # Check that the parents share the same concrete model with the our
477         # model to detect the inheritance pattern ConcreteGrandParent ->
478         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
479         # would not identify that case as involving multiple tables.
480         for parent in self.model._meta.get_parent_list():
481             if parent._meta.concrete_model is not self.model._meta.concrete_model:
482                 raise ValueError("Can't bulk create a multi-table inherited model")
483         if not objs:
484             return objs
485         self._for_write = True
486         connection = connections[self.db]
487         opts = self.model._meta
488         fields = opts.concrete_fields
489         objs = list(objs)
490         self._populate_pk_values(objs)
491         with transaction.atomic(using=self.db, savepoint=False):
492             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
493             if objs_with_pk:
494                 returned_columns = self._batched_insert(
495                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
496                 )
497                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
498                     for result, field in zip(results, opts.db_returning_fields):
499                         if field != opts.pk:
500                             setattr(obj_with_pk, field.attname, result)
501                 for obj_with_pk in objs_with_pk:
502                     obj_with_pk._state.adding = False
503                     obj_with_pk._state.db = self.db
504             if objs_without_pk:
505                 fields = [f for f in fields if not isinstance(f, AutoField)]
506                 returned_columns = self._batched_insert(
507                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
508                 )
509                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
510                     assert len(returned_columns) == len(objs_without_pk)
511                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
512                     for result, field in zip(results, opts.db_returning_fields):
513                         setattr(obj_without_pk, field.attname, result)
514                     obj_without_pk._state.adding = False
515                     obj_without_pk._state.db = self.db
516 
517         return objs
518 
519     def bulk_update(self, objs, fields, batch_size=None):
520         """
521         Update the given fields in each of the given objects in the database.
522         """
523         if batch_size is not None and batch_size < 0:
524             raise ValueError('Batch size must be a positive integer.')
525         if not fields:
526             raise ValueError('Field names must be given to bulk_update().')
527         objs = tuple(objs)
528         if any(obj.pk is None for obj in objs):
529             raise ValueError('All bulk_update() objects must have a primary key set.')
530         fields = [self.model._meta.get_field(name) for name in fields]
531         if any(not f.concrete or f.many_to_many for f in fields):
532             raise ValueError('bulk_update() can only be used with concrete fields.')
533         if any(f.primary_key for f in fields):
534             raise ValueError('bulk_update() cannot be used with primary key fields.')
535         if not objs:
536             return
537         # PK is used twice in the resulting update query, once in the filter
538         # and once in the WHEN. Each field will also have one CAST.
539         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
540         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
541         requires_casting = connections[self.db].features.requires_casted_case_in_updates
542         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
543         updates = []
544         for batch_objs in batches:
545             update_kwargs = {}
546             for field in fields:
547                 when_statements = []
548                 for obj in batch_objs:
549                     attr = getattr(obj, field.attname)
550                     if not isinstance(attr, Expression):
551                         attr = Value(attr, output_field=field)
552                     when_statements.append(When(pk=obj.pk, then=attr))
553                 case_statement = Case(*when_statements, output_field=field)
554                 if requires_casting:
555                     case_statement = Cast(case_statement, output_field=field)
556                 update_kwargs[field.attname] = case_statement
557             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
558         with transaction.atomic(using=self.db, savepoint=False):
559             for pks, update_kwargs in updates:
560                 self.filter(pk__in=pks).update(**update_kwargs)
561     bulk_update.alters_data = True
562 
563     def get_or_create(self, defaults=None, **kwargs):
564         """
565         Look up an object with the given kwargs, creating one if necessary.
566         Return a tuple of (object, created), where created is a boolean
567         specifying whether an object was created.
568         """
569         # The get() needs to be targeted at the write database in order
570         # to avoid potential transaction consistency problems.
571         self._for_write = True
572         try:
573             return self.get(**kwargs), False
574         except self.model.DoesNotExist:
575             params = self._extract_model_params(defaults, **kwargs)
576             return self._create_object_from_params(kwargs, params)
577 
578     def update_or_create(self, defaults=None, **kwargs):
579         """
580         Look up an object with the given kwargs, updating one with defaults
581         if it exists, otherwise create a new one.
582         Return a tuple (object, created), where created is a boolean
583         specifying whether an object was created.
584         """
585         defaults = defaults or {}
586         self._for_write = True
587         with transaction.atomic(using=self.db):
588             try:
589                 obj = self.select_for_update().get(**kwargs)
590             except self.model.DoesNotExist:
591                 params = self._extract_model_params(defaults, **kwargs)
592                 # Lock the row so that a concurrent update is blocked until
593                 # after update_or_create() has performed its save.
594                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
595                 if created:
596                     return obj, created
597             for k, v in resolve_callables(defaults):
598                 setattr(obj, k, v)
599             obj.save(using=self.db)
600         return obj, False
601 
602     def _create_object_from_params(self, lookup, params, lock=False):
603         """
604         Try to create an object using passed params. Used by get_or_create()
605         and update_or_create().
606         """
607         try:
608             with transaction.atomic(using=self.db):
609                 params = dict(resolve_callables(params))
610                 obj = self.create(**params)
611             return obj, True
612         except IntegrityError:
613             try:
614                 qs = self.select_for_update() if lock else self
615                 return qs.get(**lookup), False
616             except self.model.DoesNotExist:
617                 pass
618             raise
619 
620     def _extract_model_params(self, defaults, **kwargs):
621         """
622         Prepare `params` for creating a model instance based on the given
623         kwargs; for use by get_or_create() and update_or_create().
624         """
625         defaults = defaults or {}
626         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
627         params.update(defaults)
628         property_names = self.model._meta._property_names
629         invalid_params = []
630         for param in params:
631             try:
632                 self.model._meta.get_field(param)
633             except exceptions.FieldDoesNotExist:
634                 # It's okay to use a model's property if it has a setter.
635                 if not (param in property_names and getattr(self.model, param).fset):
636                     invalid_params.append(param)
637         if invalid_params:
638             raise exceptions.FieldError(
639                 "Invalid field name(s) for model %s: '%s'." % (
640                     self.model._meta.object_name,
641                     "', '".join(sorted(invalid_params)),
642                 ))
643         return params
644 
645     def _earliest(self, *fields):
646         """
647         Return the earliest object according to fields (if given) or by the
648         model's Meta.get_latest_by.
649         """
650         if fields:
651             order_by = fields
652         else:
653             order_by = getattr(self.model._meta, 'get_latest_by')
654             if order_by and not isinstance(order_by, (tuple, list)):
655                 order_by = (order_by,)
656         if order_by is None:
657             raise ValueError(
658                 "earliest() and latest() require either fields as positional "
659                 "arguments or 'get_latest_by' in the model's Meta."
660             )
661 
662         assert not self.query.is_sliced, \
663             "Cannot change a query once a slice has been taken."
664         obj = self._chain()
665         obj.query.set_limits(high=1)
666         obj.query.clear_ordering(force_empty=True)
667         obj.query.add_ordering(*order_by)
668         return obj.get()
669 
670     def earliest(self, *fields):
671         return self._earliest(*fields)
672 
673     def latest(self, *fields):
674         return self.reverse()._earliest(*fields)
675 
676     def first(self):
677         """Return the first object of a query or None if no match is found."""
678         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
679             return obj
680 
681     def last(self):
682         """Return the last object of a query or None if no match is found."""
683         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
684             return obj
685 
686     def in_bulk(self, id_list=None, *, field_name='pk'):
687         """
688         Return a dictionary mapping each of the given IDs to the object with
689         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
690         """
691         assert not self.query.is_sliced, \
692             "Cannot use 'limit' or 'offset' with in_bulk"
693         opts = self.model._meta
694         unique_fields = [
695             constraint.fields[0]
696             for constraint in opts.total_unique_constraints
697             if len(constraint.fields) == 1
698         ]
699         if (
700             field_name != 'pk' and
701             not opts.get_field(field_name).unique and
702             field_name not in unique_fields
703         ):
704             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
705         if id_list is not None:
706             if not id_list:
707                 return {}
708             filter_key = '{}__in'.format(field_name)
709             batch_size = connections[self.db].features.max_query_params
710             id_list = tuple(id_list)
711             # If the database has a limit on the number of query parameters
712             # (e.g. SQLite), retrieve objects in batches if necessary.
713             if batch_size and batch_size < len(id_list):
714                 qs = ()
715                 for offset in range(0, len(id_list), batch_size):
716                     batch = id_list[offset:offset + batch_size]
717                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
718             else:
719                 qs = self.filter(**{filter_key: id_list}).order_by()
720         else:
721             qs = self._chain()
722         return {getattr(obj, field_name): obj for obj in qs}
723 
724     def delete(self):
725         """Delete the records in the current QuerySet."""
726         self._not_support_combined_queries('delete')
727         assert not self.query.is_sliced, \
728             "Cannot use 'limit' or 'offset' with delete."
729 
730         if self._fields is not None:
731             raise TypeError("Cannot call delete() after .values() or .values_list()")
732 
733         del_query = self._chain()
734 
735         # The delete is actually 2 queries - one to find related objects,
736         # and one to delete. Make sure that the discovery of related
737         # objects is performed on the same database as the deletion.
738         del_query._for_write = True
739 
740         # Disable non-supported fields.
741         del_query.query.select_for_update = False
742         del_query.query.select_related = False
743         del_query.query.clear_ordering(force_empty=True)
744 
745         collector = Collector(using=del_query.db)
746         collector.collect(del_query)
747         deleted, _rows_count = collector.delete()
748 
749         # Clear the result cache, in case this QuerySet gets reused.
750         self._result_cache = None
751         return deleted, _rows_count
752 
753     delete.alters_data = True
754     delete.queryset_only = True
755 
756     def _raw_delete(self, using):
757         """
758         Delete objects found from the given queryset in single direct SQL
759         query. No signals are sent and there is no protection for cascades.
760         """
761         query = self.query.clone()
762         query.__class__ = sql.DeleteQuery
763         cursor = query.get_compiler(using).execute_sql(CURSOR)
764         if cursor:
765             with cursor:
766                 return cursor.rowcount
767         return 0
768     _raw_delete.alters_data = True
769 
770     def update(self, **kwargs):
771         """
772         Update all elements in the current QuerySet, setting all the given
773         fields to the appropriate values.
774         """
775         self._not_support_combined_queries('update')
776         assert not self.query.is_sliced, \
777             "Cannot update a query once a slice has been taken."
778         self._for_write = True
779         query = self.query.chain(sql.UpdateQuery)
780         query.add_update_values(kwargs)
781         # Clear any annotations so that they won't be present in subqueries.
782         query.annotations = {}
783         with transaction.mark_for_rollback_on_error(using=self.db):
784             rows = query.get_compiler(self.db).execute_sql(CURSOR)
785         self._result_cache = None
786         return rows
787     update.alters_data = True
788 
789     def _update(self, values):
790         """
791         A version of update() that accepts field objects instead of field names.
792         Used primarily for model saving and not intended for use by general
793         code (it requires too much poking around at model internals to be
794         useful at that level).
795         """
796         assert not self.query.is_sliced, \
797             "Cannot update a query once a slice has been taken."
798         query = self.query.chain(sql.UpdateQuery)
799         query.add_update_fields(values)
800         # Clear any annotations so that they won't be present in subqueries.
801         query.annotations = {}
802         self._result_cache = None
803         return query.get_compiler(self.db).execute_sql(CURSOR)
804     _update.alters_data = True
805     _update.queryset_only = False
806 
807     def exists(self):
808         if self._result_cache is None:
809             return self.query.has_results(using=self.db)
810         return bool(self._result_cache)
811 
812     def _prefetch_related_objects(self):
813         # This method can only be called once the result cache has been filled.
814         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
815         self._prefetch_done = True
816 
817     def explain(self, *, format=None, **options):
818         return self.query.explain(using=self.db, format=format, **options)
819 
820     ##################################################
821     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
822     ##################################################
823 
824     def raw(self, raw_query, params=None, translations=None, using=None):
825         if using is None:
826             using = self.db
827         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
828         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
829         return qs
830 
831     def _values(self, *fields, **expressions):
832         clone = self._chain()
833         if expressions:
834             clone = clone.annotate(**expressions)
835         clone._fields = fields
836         clone.query.set_values(fields)
837         return clone
838 
839     def values(self, *fields, **expressions):
840         fields += tuple(expressions)
841         clone = self._values(*fields, **expressions)
842         clone._iterable_class = ValuesIterable
843         return clone
844 
845     def values_list(self, *fields, flat=False, named=False):
846         if flat and named:
847             raise TypeError("'flat' and 'named' can't be used together.")
848         if flat and len(fields) > 1:
849             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
850 
851         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
852         _fields = []
853         expressions = {}
854         counter = 1
855         for field in fields:
856             if hasattr(field, 'resolve_expression'):
857                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
858                 while True:
859                     field_id = field_id_prefix + str(counter)
860                     counter += 1
861                     if field_id not in field_names:
862                         break
863                 expressions[field_id] = field
864                 _fields.append(field_id)
865             else:
866                 _fields.append(field)
867 
868         clone = self._values(*_fields, **expressions)
869         clone._iterable_class = (
870             NamedValuesListIterable if named
871             else FlatValuesListIterable if flat
872             else ValuesListIterable
873         )
874         return clone
875 
876     def dates(self, field_name, kind, order='ASC'):
877         """
878         Return a list of date objects representing all available dates for
879         the given field_name, scoped to 'kind'.
880         """
881         assert kind in ('year', 'month', 'week', 'day'), \
882             "'kind' must be one of 'year', 'month', 'week', or 'day'."
883         assert order in ('ASC', 'DESC'), \
884             "'order' must be either 'ASC' or 'DESC'."
885         return self.annotate(
886             datefield=Trunc(field_name, kind, output_field=DateField()),
887             plain_field=F(field_name)
888         ).values_list(
889             'datefield', flat=True
890         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
891 
892     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):
893         """
894         Return a list of datetime objects representing all available
895         datetimes for the given field_name, scoped to 'kind'.
896         """
897         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
898             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
899         assert order in ('ASC', 'DESC'), \
900             "'order' must be either 'ASC' or 'DESC'."
901         if settings.USE_TZ:
902             if tzinfo is None:
903                 tzinfo = timezone.get_current_timezone()
904         else:
905             tzinfo = None
906         return self.annotate(
907             datetimefield=Trunc(
908                 field_name,
909                 kind,
910                 output_field=DateTimeField(),
911                 tzinfo=tzinfo,
912                 is_dst=is_dst,
913             ),
914             plain_field=F(field_name)
915         ).values_list(
916             'datetimefield', flat=True
917         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
918 
919     def none(self):
920         """Return an empty QuerySet."""
921         clone = self._chain()
922         clone.query.set_empty()
923         return clone
924 
925     ##################################################################
926     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
927     ##################################################################
928 
929     def all(self):
930         """
931         Return a new QuerySet that is a copy of the current one. This allows a
932         QuerySet to proxy for a model manager in some cases.
933         """
934         return self._chain()
935 
936     def filter(self, *args, **kwargs):
937         """
938         Return a new QuerySet instance with the args ANDed to the existing
939         set.
940         """
941         self._not_support_combined_queries('filter')
942         return self._filter_or_exclude(False, *args, **kwargs)
943 
944     def exclude(self, *args, **kwargs):
945         """
946         Return a new QuerySet instance with NOT (args) ANDed to the existing
947         set.
948         """
949         self._not_support_combined_queries('exclude')
950         return self._filter_or_exclude(True, *args, **kwargs)
951 
952     def _filter_or_exclude(self, negate, *args, **kwargs):
953         if args or kwargs:
954             assert not self.query.is_sliced, \
955                 "Cannot filter a query once a slice has been taken."
956 
957         clone = self._chain()
958         if self._defer_next_filter:
959             self._defer_next_filter = False
960             clone._deferred_filter = negate, args, kwargs
961         else:
962             clone._filter_or_exclude_inplace(negate, *args, **kwargs)
963         return clone
964 
965     def _filter_or_exclude_inplace(self, negate, *args, **kwargs):
966         if negate:
967             self._query.add_q(~Q(*args, **kwargs))
968         else:
969             self._query.add_q(Q(*args, **kwargs))
970 
971     def complex_filter(self, filter_obj):
972         """
973         Return a new QuerySet instance with filter_obj added to the filters.
974 
975         filter_obj can be a Q object or a dictionary of keyword lookup
976         arguments.
977 
978         This exists to support framework features such as 'limit_choices_to',
979         and usually it will be more natural to use other methods.
980         """
981         if isinstance(filter_obj, Q):
982             clone = self._chain()
983             clone.query.add_q(filter_obj)
984             return clone
985         else:
986             return self._filter_or_exclude(False, **filter_obj)
987 
988     def _combinator_query(self, combinator, *other_qs, all=False):
989         # Clone the query to inherit the select list and everything
990         clone = self._chain()
991         # Clear limits and ordering so they can be reapplied
992         clone.query.clear_ordering(True)
993         clone.query.clear_limits()
994         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
995         clone.query.combinator = combinator
996         clone.query.combinator_all = all
997         return clone
998 
999     def union(self, *other_qs, all=False):
1000         # If the query is an EmptyQuerySet, combine all nonempty querysets.
1001         if isinstance(self, EmptyQuerySet):
1002             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1003             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
1004         return self._combinator_query('union', *other_qs, all=all)
1005 
1006     def intersection(self, *other_qs):
1007         # If any query is an EmptyQuerySet, return it.
1008         if isinstance(self, EmptyQuerySet):
1009             return self
1010         for other in other_qs:
1011             if isinstance(other, EmptyQuerySet):
1012                 return other
1013         return self._combinator_query('intersection', *other_qs)
1014 
1015     def difference(self, *other_qs):
1016         # If the query is an EmptyQuerySet, return it.
1017         if isinstance(self, EmptyQuerySet):
1018             return self
1019         return self._combinator_query('difference', *other_qs)
1020 
1021     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1022         """
1023         Return a new QuerySet instance that will select objects with a
1024         FOR UPDATE lock.
1025         """
1026         if nowait and skip_locked:
1027             raise ValueError('The nowait option cannot be used with skip_locked.')
1028         obj = self._chain()
1029         obj._for_write = True
1030         obj.query.select_for_update = True
1031         obj.query.select_for_update_nowait = nowait
1032         obj.query.select_for_update_skip_locked = skip_locked
1033         obj.query.select_for_update_of = of
1034         obj.query.select_for_no_key_update = no_key
1035         return obj
1036 
1037     def select_related(self, *fields):
1038         """
1039         Return a new QuerySet instance that will select related objects.
1040 
1041         If fields are specified, they must be ForeignKey fields and only those
1042         related objects are included in the selection.
1043 
1044         If select_related(None) is called, clear the list.
1045         """
1046         self._not_support_combined_queries('select_related')
1047         if self._fields is not None:
1048             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1049 
1050         obj = self._chain()
1051         if fields == (None,):
1052             obj.query.select_related = False
1053         elif fields:
1054             obj.query.add_select_related(fields)
1055         else:
1056             obj.query.select_related = True
1057         return obj
1058 
1059     def prefetch_related(self, *lookups):
1060         """
1061         Return a new QuerySet instance that will prefetch the specified
1062         Many-To-One and Many-To-Many related objects when the QuerySet is
1063         evaluated.
1064 
1065         When prefetch_related() is called more than once, append to the list of
1066         prefetch lookups. If prefetch_related(None) is called, clear the list.
1067         """
1068         self._not_support_combined_queries('prefetch_related')
1069         clone = self._chain()
1070         if lookups == (None,):
1071             clone._prefetch_related_lookups = ()
1072         else:
1073             for lookup in lookups:
1074                 if isinstance(lookup, Prefetch):
1075                     lookup = lookup.prefetch_to
1076                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1077                 if lookup in self.query._filtered_relations:
1078                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1079             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1080         return clone
1081 
1082     def annotate(self, *args, **kwargs):
1083         """
1084         Return a query set in which the returned objects have been annotated
1085         with extra data or aggregations.
1086         """
1087         self._not_support_combined_queries('annotate')
1088         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1089         annotations = {}
1090         for arg in args:
1091             # The default_alias property may raise a TypeError.
1092             try:
1093                 if arg.default_alias in kwargs:
1094                     raise ValueError("The named annotation '%s' conflicts with the "
1095                                      "default name for another annotation."
1096                                      % arg.default_alias)
1097             except TypeError:
1098                 raise TypeError("Complex annotations require an alias")
1099             annotations[arg.default_alias] = arg
1100         annotations.update(kwargs)
1101 
1102         clone = self._chain()
1103         names = self._fields
1104         if names is None:
1105             names = set(chain.from_iterable(
1106                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1107                 for field in self.model._meta.get_fields()
1108             ))
1109 
1110         for alias, annotation in annotations.items():
1111             if alias in names:
1112                 raise ValueError("The annotation '%s' conflicts with a field on "
1113                                  "the model." % alias)
1114             if isinstance(annotation, FilteredRelation):
1115                 clone.query.add_filtered_relation(annotation, alias)
1116             else:
1117                 clone.query.add_annotation(annotation, alias, is_summary=False)
1118 
1119         for alias, annotation in clone.query.annotations.items():
1120             if alias in annotations and annotation.contains_aggregate:
1121                 if clone._fields is None:
1122                     clone.query.group_by = True
1123                 else:
1124                     clone.query.set_group_by()
1125                 break
1126 
1127         return clone
1128 
1129     def order_by(self, *field_names):
1130         """Return a new QuerySet instance with the ordering changed."""
1131         assert not self.query.is_sliced, \
1132             "Cannot reorder a query once a slice has been taken."
1133         obj = self._chain()
1134         obj.query.clear_ordering(force_empty=False)
1135         obj.query.add_ordering(*field_names)
1136         return obj
1137 
1138     def distinct(self, *field_names):
1139         """
1140         Return a new QuerySet instance that will select only distinct results.
1141         """
1142         self._not_support_combined_queries('distinct')
1143         assert not self.query.is_sliced, \
1144             "Cannot create distinct fields once a slice has been taken."
1145         obj = self._chain()
1146         obj.query.add_distinct_fields(*field_names)
1147         return obj
1148 
1149     def extra(self, select=None, where=None, params=None, tables=None,
1150               order_by=None, select_params=None):
1151         """Add extra SQL fragments to the query."""
1152         self._not_support_combined_queries('extra')
1153         assert not self.query.is_sliced, \
1154             "Cannot change a query once a slice has been taken"
1155         clone = self._chain()
1156         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1157         return clone
1158 
1159     def reverse(self):
1160         """Reverse the ordering of the QuerySet."""
1161         if self.query.is_sliced:
1162             raise TypeError('Cannot reverse a query once a slice has been taken.')
1163         clone = self._chain()
1164         clone.query.standard_ordering = not clone.query.standard_ordering
1165         return clone
1166 
1167     def defer(self, *fields):
1168         """
1169         Defer the loading of data for certain fields until they are accessed.
1170         Add the set of deferred fields to any existing set of deferred fields.
1171         The only exception to this is if None is passed in as the only
1172         parameter, in which case removal all deferrals.
1173         """
1174         self._not_support_combined_queries('defer')
1175         if self._fields is not None:
1176             raise TypeError("Cannot call defer() after .values() or .values_list()")
1177         clone = self._chain()
1178         if fields == (None,):
1179             clone.query.clear_deferred_loading()
1180         else:
1181             clone.query.add_deferred_loading(fields)
1182         return clone
1183 
1184     def only(self, *fields):
1185         """
1186         Essentially, the opposite of defer(). Only the fields passed into this
1187         method and that are not already specified as deferred are loaded
1188         immediately when the queryset is evaluated.
1189         """
1190         self._not_support_combined_queries('only')
1191         if self._fields is not None:
1192             raise TypeError("Cannot call only() after .values() or .values_list()")
1193         if fields == (None,):
1194             # Can only pass None to defer(), not only(), as the rest option.
1195             # That won't stop people trying to do this, so let's be explicit.
1196             raise TypeError("Cannot pass None as an argument to only().")
1197         for field in fields:
1198             field = field.split(LOOKUP_SEP, 1)[0]
1199             if field in self.query._filtered_relations:
1200                 raise ValueError('only() is not supported with FilteredRelation.')
1201         clone = self._chain()
1202         clone.query.add_immediate_loading(fields)
1203         return clone
1204 
1205     def using(self, alias):
1206         """Select which database this QuerySet should execute against."""
1207         clone = self._chain()
1208         clone._db = alias
1209         return clone
1210 
1211     ###################################
1212     # PUBLIC INTROSPECTION ATTRIBUTES #
1213     ###################################
1214 
1215     @property
1216     def ordered(self):
1217         """
1218         Return True if the QuerySet is ordered -- i.e. has an order_by()
1219         clause or a default ordering on the model (or is empty).
1220         """
1221         if isinstance(self, EmptyQuerySet):
1222             return True
1223         if self.query.extra_order_by or self.query.order_by:
1224             return True
1225         elif self.query.default_ordering and self.query.get_meta().ordering:
1226             return True
1227         else:
1228             return False
1229 
1230     @property
1231     def db(self):
1232         """Return the database used if this query is executed now."""
1233         if self._for_write:
1234             return self._db or router.db_for_write(self.model, **self._hints)
1235         return self._db or router.db_for_read(self.model, **self._hints)
1236 
1237     ###################
1238     # PRIVATE METHODS #
1239     ###################
1240 
1241     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1242         """
1243         Insert a new record for the given model. This provides an interface to
1244         the InsertQuery class and is how Model.save() is implemented.
1245         """
1246         self._for_write = True
1247         if using is None:
1248             using = self.db
1249         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1250         query.insert_values(fields, objs, raw=raw)
1251         return query.get_compiler(using=using).execute_sql(returning_fields)
1252     _insert.alters_data = True
1253     _insert.queryset_only = False
1254 
1255     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1256         """
1257         Helper method for bulk_create() to insert objs one batch at a time.
1258         """
1259         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1260             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1261         ops = connections[self.db].ops
1262         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1263         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1264         inserted_rows = []
1265         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1266         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1267             if bulk_return and not ignore_conflicts:
1268                 inserted_rows.extend(self._insert(
1269                     item, fields=fields, using=self.db,
1270                     returning_fields=self.model._meta.db_returning_fields,
1271                     ignore_conflicts=ignore_conflicts,
1272                 ))
1273             else:
1274                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1275         return inserted_rows
1276 
1277     def _chain(self, **kwargs):
1278         """
1279         Return a copy of the current QuerySet that's ready for another
1280         operation.
1281         """
1282         obj = self._clone()
1283         if obj._sticky_filter:
1284             obj.query.filter_is_sticky = True
1285             obj._sticky_filter = False
1286         obj.__dict__.update(kwargs)
1287         return obj
1288 
1289     def _clone(self):
1290         """
1291         Return a copy of the current QuerySet. A lightweight alternative
1292         to deepcopy().
1293         """
1294         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1295         c._sticky_filter = self._sticky_filter
1296         c._for_write = self._for_write
1297         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1298         c._known_related_objects = self._known_related_objects
1299         c._iterable_class = self._iterable_class
1300         c._fields = self._fields
1301         return c
1302 
1303     def _fetch_all(self):
1304         if self._result_cache is None:
1305             self._result_cache = list(self._iterable_class(self))
1306         if self._prefetch_related_lookups and not self._prefetch_done:
1307             self._prefetch_related_objects()
1308 
1309     def _next_is_sticky(self):
1310         """
1311         Indicate that the next filter call and the one following that should
1312         be treated as a single filter. This is only important when it comes to
1313         determining when to reuse tables for many-to-many filters. Required so
1314         that we can filter naturally on the results of related managers.
1315 
1316         This doesn't return a clone of the current QuerySet (it returns
1317         "self"). The method is only used internally and should be immediately
1318         followed by a filter() that does create a clone.
1319         """
1320         self._sticky_filter = True
1321         return self
1322 
1323     def _merge_sanity_check(self, other):
1324         """Check that two QuerySet classes may be merged."""
1325         if self._fields is not None and (
1326                 set(self.query.values_select) != set(other.query.values_select) or
1327                 set(self.query.extra_select) != set(other.query.extra_select) or
1328                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1329             raise TypeError(
1330                 "Merging '%s' classes must involve the same values in each case."
1331                 % self.__class__.__name__
1332             )
1333 
1334     def _merge_known_related_objects(self, other):
1335         """
1336         Keep track of all known related objects from either QuerySet instance.
1337         """
1338         for field, objects in other._known_related_objects.items():
1339             self._known_related_objects.setdefault(field, {}).update(objects)
1340 
1341     def resolve_expression(self, *args, **kwargs):
1342         if self._fields and len(self._fields) > 1:
1343             # values() queryset can only be used as nested queries
1344             # if they are set up to select only a single field.
1345             raise TypeError('Cannot use multi-field values as a filter value.')
1346         query = self.query.resolve_expression(*args, **kwargs)
1347         query._db = self._db
1348         return query
1349     resolve_expression.queryset_only = True
1350 
1351     def _add_hints(self, **hints):
1352         """
1353         Update hinting information for use by routers. Add new key/values or
1354         overwrite existing key/values.
1355         """
1356         self._hints.update(hints)
1357 
1358     def _has_filters(self):
1359         """
1360         Check if this QuerySet has any filtering going on. This isn't
1361         equivalent with checking if all objects are present in results, for
1362         example, qs[1:]._has_filters() -> False.
1363         """
1364         return self.query.has_filters()
1365 
1366     @staticmethod
1367     def _validate_values_are_expressions(values, method_name):
1368         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1369         if invalid_args:
1370             raise TypeError(
1371                 'QuerySet.%s() received non-expression(s): %s.' % (
1372                     method_name,
1373                     ', '.join(invalid_args),
1374                 )
1375             )
1376 
1377     def _not_support_combined_queries(self, operation_name):
1378         if self.query.combinator:
1379             raise NotSupportedError(
1380                 'Calling QuerySet.%s() after %s() is not supported.'
1381                 % (operation_name, self.query.combinator)
1382             )
1383 
1384 
1385 class InstanceCheckMeta(type):
1386     def __instancecheck__(self, instance):
1387         return isinstance(instance, QuerySet) and instance.query.is_empty()
1388 
1389 
1390 class EmptyQuerySet(metaclass=InstanceCheckMeta):
1391     """
1392     Marker class to checking if a queryset is empty by .none():
1393         isinstance(qs.none(), EmptyQuerySet) -> True
1394     """
1395 
1396     def __init__(self, *args, **kwargs):
1397         raise TypeError("EmptyQuerySet can't be instantiated")
1398 
1399 
1400 class RawQuerySet:
1401     """
1402     Provide an iterator which converts the results of raw SQL queries into
1403     annotated model instances.
1404     """
1405     def __init__(self, raw_query, model=None, query=None, params=None,
1406                  translations=None, using=None, hints=None):
1407         self.raw_query = raw_query
1408         self.model = model
1409         self._db = using
1410         self._hints = hints or {}
1411         self.query = query or sql.RawQuery(sql=raw_query, using=self.db, params=params)
1412         self.params = params or ()
1413         self.translations = translations or {}
1414         self._result_cache = None
1415         self._prefetch_related_lookups = ()
1416         self._prefetch_done = False
1417 
1418     def resolve_model_init_order(self):
1419         """Resolve the init field names and value positions."""
1420         converter = connections[self.db].introspection.identifier_converter
1421         model_init_fields = [f for f in self.model._meta.fields if converter(f.column) in self.columns]
1422         annotation_fields = [(column, pos) for pos, column in enumerate(self.columns)
1423                              if column not in self.model_fields]
1424         model_init_order = [self.columns.index(converter(f.column)) for f in model_init_fields]
1425         model_init_names = [f.attname for f in model_init_fields]
1426         return model_init_names, model_init_order, annotation_fields
1427 
1428     def prefetch_related(self, *lookups):
1429         """Same as QuerySet.prefetch_related()"""
1430         clone = self._clone()
1431         if lookups == (None,):
1432             clone._prefetch_related_lookups = ()
1433         else:
1434             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1435         return clone
1436 
1437     def _prefetch_related_objects(self):
1438         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
1439         self._prefetch_done = True
1440 
1441     def _clone(self):
1442         """Same as QuerySet._clone()"""
1443         c = self.__class__(
1444             self.raw_query, model=self.model, query=self.query, params=self.params,
1445             translations=self.translations, using=self._db, hints=self._hints
1446         )
1447         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1448         return c
1449 
1450     def _fetch_all(self):
1451         if self._result_cache is None:
1452             self._result_cache = list(self.iterator())
1453         if self._prefetch_related_lookups and not self._prefetch_done:
1454             self._prefetch_related_objects()
1455 
1456     def __len__(self):
1457         self._fetch_all()
1458         return len(self._result_cache)
1459 
1460     def __bool__(self):
1461         self._fetch_all()
1462         return bool(self._result_cache)
1463 
1464     def __iter__(self):
1465         self._fetch_all()
1466         return iter(self._result_cache)
1467 
1468     def iterator(self):
1469         # Cache some things for performance reasons outside the loop.
1470         db = self.db
1471         compiler = connections[db].ops.compiler('SQLCompiler')(
1472             self.query, connections[db], db
1473         )
1474 
1475         query = iter(self.query)
1476 
1477         try:
1478             model_init_names, model_init_pos, annotation_fields = self.resolve_model_init_order()
1479             if self.model._meta.pk.attname not in model_init_names:
1480                 raise exceptions.FieldDoesNotExist(
1481                     'Raw query must include the primary key'
1482                 )
1483             model_cls = self.model
1484             fields = [self.model_fields.get(c) for c in self.columns]
1485             converters = compiler.get_converters([
1486                 f.get_col(f.model._meta.db_table) if f else None for f in fields
1487             ])
1488             if converters:
1489                 query = compiler.apply_converters(query, converters)
1490             for values in query:
1491                 # Associate fields to values
1492                 model_init_values = [values[pos] for pos in model_init_pos]
1493                 instance = model_cls.from_db(db, model_init_names, model_init_values)
1494                 if annotation_fields:
1495                     for column, pos in annotation_fields:
1496                         setattr(instance, column, values[pos])
1497                 yield instance
1498         finally:
1499             # Done iterating the Query. If it has its own cursor, close it.
1500             if hasattr(self.query, 'cursor') and self.query.cursor:
1501                 self.query.cursor.close()
1502 
1503     def __repr__(self):
1504         return "<%s: %s>" % (self.__class__.__name__, self.query)
1505 
1506     def __getitem__(self, k):
1507         return list(self)[k]
1508 
1509     @property
1510     def db(self):
1511         """Return the database used if this query is executed now."""
1512         return self._db or router.db_for_read(self.model, **self._hints)
1513 
1514     def using(self, alias):
1515         """Select the database this RawQuerySet should execute against."""
1516         return RawQuerySet(
1517             self.raw_query, model=self.model,
1518             query=self.query.chain(using=alias),
1519             params=self.params, translations=self.translations,
1520             using=alias,
1521         )
1522 
1523     @cached_property
1524     def columns(self):
1525         """
1526         A list of model field names in the order they'll appear in the
1527         query results.
1528         """
1529         columns = self.query.get_columns()
1530         # Adjust any column names which don't match field names
1531         for (query_name, model_name) in self.translations.items():
1532             # Ignore translations for nonexistent column names
1533             try:
1534                 index = columns.index(query_name)
1535             except ValueError:
1536                 pass
1537             else:
1538                 columns[index] = model_name
1539         return columns
1540 
1541     @cached_property
1542     def model_fields(self):
1543         """A dict mapping column names to model field names."""
1544         converter = connections[self.db].introspection.identifier_converter
1545         model_fields = {}
1546         for field in self.model._meta.fields:
1547             name, column = field.get_attname_column()
1548             model_fields[converter(column)] = field
1549         return model_fields
1550 
1551 
1552 class Prefetch:
1553     def __init__(self, lookup, queryset=None, to_attr=None):
1554         # `prefetch_through` is the path we traverse to perform the prefetch.
1555         self.prefetch_through = lookup
1556         # `prefetch_to` is the path to the attribute that stores the result.
1557         self.prefetch_to = lookup
1558         if queryset is not None and (
1559             isinstance(queryset, RawQuerySet) or (
1560                 hasattr(queryset, '_iterable_class') and
1561                 not issubclass(queryset._iterable_class, ModelIterable)
1562             )
1563         ):
1564             raise ValueError(
1565                 'Prefetch querysets cannot use raw(), values(), and '
1566                 'values_list().'
1567             )
1568         if to_attr:
1569             self.prefetch_to = LOOKUP_SEP.join(lookup.split(LOOKUP_SEP)[:-1] + [to_attr])
1570 
1571         self.queryset = queryset
1572         self.to_attr = to_attr
1573 
1574     def __getstate__(self):
1575         obj_dict = self.__dict__.copy()
1576         if self.queryset is not None:
1577             # Prevent the QuerySet from being evaluated
1578             obj_dict['queryset'] = self.queryset._chain(
1579                 _result_cache=[],
1580                 _prefetch_done=True,
1581             )
1582         return obj_dict
1583 
1584     def add_prefix(self, prefix):
1585         self.prefetch_through = prefix + LOOKUP_SEP + self.prefetch_through
1586         self.prefetch_to = prefix + LOOKUP_SEP + self.prefetch_to
1587 
1588     def get_current_prefetch_to(self, level):
1589         return LOOKUP_SEP.join(self.prefetch_to.split(LOOKUP_SEP)[:level + 1])
1590 
1591     def get_current_to_attr(self, level):
1592         parts = self.prefetch_to.split(LOOKUP_SEP)
1593         to_attr = parts[level]
1594         as_attr = self.to_attr and level == len(parts) - 1
1595         return to_attr, as_attr
1596 
1597     def get_current_queryset(self, level):
1598         if self.get_current_prefetch_to(level) == self.prefetch_to:
1599             return self.queryset
1600         return None
1601 
1602     def __eq__(self, other):
1603         if not isinstance(other, Prefetch):
1604             return NotImplemented
1605         return self.prefetch_to == other.prefetch_to
1606 
1607     def __hash__(self):
1608         return hash((self.__class__, self.prefetch_to))
1609 
1610 
1611 def normalize_prefetch_lookups(lookups, prefix=None):
1612     """Normalize lookups into Prefetch objects."""
1613     ret = []
1614     for lookup in lookups:
1615         if not isinstance(lookup, Prefetch):
1616             lookup = Prefetch(lookup)
1617         if prefix:
1618             lookup.add_prefix(prefix)
1619         ret.append(lookup)
1620     return ret
1621 
1622 
1623 def prefetch_related_objects(model_instances, *related_lookups):
1624     """
1625     Populate prefetched object caches for a list of model instances based on
1626     the lookups/Prefetch instances given.
1627     """
1628     if not model_instances:
1629         return  # nothing to do
1630 
1631     # We need to be able to dynamically add to the list of prefetch_related
1632     # lookups that we look up (see below).  So we need some book keeping to
1633     # ensure we don't do duplicate work.
1634     done_queries = {}    # dictionary of things like 'foo__bar': [results]
1635 
1636     auto_lookups = set()  # we add to this as we go through.
1637     followed_descriptors = set()  # recursion protection
1638 
1639     all_lookups = normalize_prefetch_lookups(reversed(related_lookups))
1640     while all_lookups:
1641         lookup = all_lookups.pop()
1642         if lookup.prefetch_to in done_queries:
1643             if lookup.queryset is not None:
1644                 raise ValueError("'%s' lookup was already seen with a different queryset. "
1645                                  "You may need to adjust the ordering of your lookups." % lookup.prefetch_to)
1646 
1647             continue
1648 
1649         # Top level, the list of objects to decorate is the result cache
1650         # from the primary QuerySet. It won't be for deeper levels.
1651         obj_list = model_instances
1652 
1653         through_attrs = lookup.prefetch_through.split(LOOKUP_SEP)
1654         for level, through_attr in enumerate(through_attrs):
1655             # Prepare main instances
1656             if not obj_list:
1657                 break
1658 
1659             prefetch_to = lookup.get_current_prefetch_to(level)
1660             if prefetch_to in done_queries:
1661                 # Skip any prefetching, and any object preparation
1662                 obj_list = done_queries[prefetch_to]
1663                 continue
1664 
1665             # Prepare objects:
1666             good_objects = True
1667             for obj in obj_list:
1668                 # Since prefetching can re-use instances, it is possible to have
1669                 # the same instance multiple times in obj_list, so obj might
1670                 # already be prepared.
1671                 if not hasattr(obj, '_prefetched_objects_cache'):
1672                     try:
1673                         obj._prefetched_objects_cache = {}
1674                     except (AttributeError, TypeError):
1675                         # Must be an immutable object from
1676                         # values_list(flat=True), for example (TypeError) or
1677                         # a QuerySet subclass that isn't returning Model
1678                         # instances (AttributeError), either in Django or a 3rd
1679                         # party. prefetch_related() doesn't make sense, so quit.
1680                         good_objects = False
1681                         break
1682             if not good_objects:
1683                 break
1684 
1685             # Descend down tree
1686 
1687             # We assume that objects retrieved are homogeneous (which is the premise
1688             # of prefetch_related), so what applies to first object applies to all.
1689             first_obj = obj_list[0]
1690             to_attr = lookup.get_current_to_attr(level)[0]
1691             prefetcher, descriptor, attr_found, is_fetched = get_prefetcher(first_obj, through_attr, to_attr)
1692 
1693             if not attr_found:
1694                 raise AttributeError("Cannot find '%s' on %s object, '%s' is an invalid "
1695                                      "parameter to prefetch_related()" %
1696                                      (through_attr, first_obj.__class__.__name__, lookup.prefetch_through))
1697 
1698             if level == len(through_attrs) - 1 and prefetcher is None:
1699                 # Last one, this *must* resolve to something that supports
1700                 # prefetching, otherwise there is no point adding it and the
1701                 # developer asking for it has made a mistake.
1702                 raise ValueError("'%s' does not resolve to an item that supports "
1703                                  "prefetching - this is an invalid parameter to "
1704                                  "prefetch_related()." % lookup.prefetch_through)
1705 
1706             if prefetcher is not None and not is_fetched:
1707                 obj_list, additional_lookups = prefetch_one_level(obj_list, prefetcher, lookup, level)
1708                 # We need to ensure we don't keep adding lookups from the
1709                 # same relationships to stop infinite recursion. So, if we
1710                 # are already on an automatically added lookup, don't add
1711                 # the new lookups from relationships we've seen already.
1712                 if not (prefetch_to in done_queries and lookup in auto_lookups and descriptor in followed_descriptors):
1713                     done_queries[prefetch_to] = obj_list
1714                     new_lookups = normalize_prefetch_lookups(reversed(additional_lookups), prefetch_to)
1715                     auto_lookups.update(new_lookups)
1716                     all_lookups.extend(new_lookups)
1717                 followed_descriptors.add(descriptor)
1718             else:
1719                 # Either a singly related object that has already been fetched
1720                 # (e.g. via select_related), or hopefully some other property
1721                 # that doesn't support prefetching but needs to be traversed.
1722 
1723                 # We replace the current list of parent objects with the list
1724                 # of related objects, filtering out empty or missing values so
1725                 # that we can continue with nullable or reverse relations.
1726                 new_obj_list = []
1727                 for obj in obj_list:
1728                     if through_attr in getattr(obj, '_prefetched_objects_cache', ()):
1729                         # If related objects have been prefetched, use the
1730                         # cache rather than the object's through_attr.
1731                         new_obj = list(obj._prefetched_objects_cache.get(through_attr))
1732                     else:
1733                         try:
1734                             new_obj = getattr(obj, through_attr)
1735                         except exceptions.ObjectDoesNotExist:
1736                             continue
1737                     if new_obj is None:
1738                         continue
1739                     # We special-case `list` rather than something more generic
1740                     # like `Iterable` because we don't want to accidentally match
1741                     # user models that define __iter__.
1742                     if isinstance(new_obj, list):
1743                         new_obj_list.extend(new_obj)
1744                     else:
1745                         new_obj_list.append(new_obj)
1746                 obj_list = new_obj_list
1747 
1748 
1749 def get_prefetcher(instance, through_attr, to_attr):
1750     """
1751     For the attribute 'through_attr' on the given instance, find
1752     an object that has a get_prefetch_queryset().
1753     Return a 4 tuple containing:
1754     (the object with get_prefetch_queryset (or None),
1755      the descriptor object representing this relationship (or None),
1756      a boolean that is False if the attribute was not found at all,
1757      a boolean that is True if the attribute has already been fetched)
1758     """
1759     prefetcher = None
1760     is_fetched = False
1761 
1762     # For singly related objects, we have to avoid getting the attribute
1763     # from the object, as this will trigger the query. So we first try
1764     # on the class, in order to get the descriptor object.
1765     rel_obj_descriptor = getattr(instance.__class__, through_attr, None)
1766     if rel_obj_descriptor is None:
1767         attr_found = hasattr(instance, through_attr)
1768     else:
1769         attr_found = True
1770         if rel_obj_descriptor:
1771             # singly related object, descriptor object has the
1772             # get_prefetch_queryset() method.
1773             if hasattr(rel_obj_descriptor, 'get_prefetch_queryset'):
1774                 prefetcher = rel_obj_descriptor
1775                 if rel_obj_descriptor.is_cached(instance):
1776                     is_fetched = True
1777             else:
1778                 # descriptor doesn't support prefetching, so we go ahead and get
1779                 # the attribute on the instance rather than the class to
1780                 # support many related managers
1781                 rel_obj = getattr(instance, through_attr)
1782                 if hasattr(rel_obj, 'get_prefetch_queryset'):
1783                     prefetcher = rel_obj
1784                 if through_attr != to_attr:
1785                     # Special case cached_property instances because hasattr
1786                     # triggers attribute computation and assignment.
1787                     if isinstance(getattr(instance.__class__, to_attr, None), cached_property):
1788                         is_fetched = to_attr in instance.__dict__
1789                     else:
1790                         is_fetched = hasattr(instance, to_attr)
1791                 else:
1792                     is_fetched = through_attr in instance._prefetched_objects_cache
1793     return prefetcher, rel_obj_descriptor, attr_found, is_fetched
1794 
1795 
1796 def prefetch_one_level(instances, prefetcher, lookup, level):
1797     """
1798     Helper function for prefetch_related_objects().
1799 
1800     Run prefetches on all instances using the prefetcher object,
1801     assigning results to relevant caches in instance.
1802 
1803     Return the prefetched objects along with any additional prefetches that
1804     must be done due to prefetch_related lookups found from default managers.
1805     """
1806     # prefetcher must have a method get_prefetch_queryset() which takes a list
1807     # of instances, and returns a tuple:
1808 
1809     # (queryset of instances of self.model that are related to passed in instances,
1810     #  callable that gets value to be matched for returned instances,
1811     #  callable that gets value to be matched for passed in instances,
1812     #  boolean that is True for singly related objects,
1813     #  cache or field name to assign to,
1814     #  boolean that is True when the previous argument is a cache name vs a field name).
1815 
1816     # The 'values to be matched' must be hashable as they will be used
1817     # in a dictionary.
1818 
1819     rel_qs, rel_obj_attr, instance_attr, single, cache_name, is_descriptor = (
1820         prefetcher.get_prefetch_queryset(instances, lookup.get_current_queryset(level)))
1821     # We have to handle the possibility that the QuerySet we just got back
1822     # contains some prefetch_related lookups. We don't want to trigger the
1823     # prefetch_related functionality by evaluating the query. Rather, we need
1824     # to merge in the prefetch_related lookups.
1825     # Copy the lookups in case it is a Prefetch object which could be reused
1826     # later (happens in nested prefetch_related).
1827     additional_lookups = [
1828         copy.copy(additional_lookup) for additional_lookup
1829         in getattr(rel_qs, '_prefetch_related_lookups', ())
1830     ]
1831     if additional_lookups:
1832         # Don't need to clone because the manager should have given us a fresh
1833         # instance, so we access an internal instead of using public interface
1834         # for performance reasons.
1835         rel_qs._prefetch_related_lookups = ()
1836 
1837     all_related_objects = list(rel_qs)
1838 
1839     rel_obj_cache = {}
1840     for rel_obj in all_related_objects:
1841         rel_attr_val = rel_obj_attr(rel_obj)
1842         rel_obj_cache.setdefault(rel_attr_val, []).append(rel_obj)
1843 
1844     to_attr, as_attr = lookup.get_current_to_attr(level)
1845     # Make sure `to_attr` does not conflict with a field.
1846     if as_attr and instances:
1847         # We assume that objects retrieved are homogeneous (which is the premise
1848         # of prefetch_related), so what applies to first object applies to all.
1849         model = instances[0].__class__
1850         try:
1851             model._meta.get_field(to_attr)
1852         except exceptions.FieldDoesNotExist:
1853             pass
1854         else:
1855             msg = 'to_attr={} conflicts with a field on the {} model.'
1856             raise ValueError(msg.format(to_attr, model.__name__))
1857 
1858     # Whether or not we're prefetching the last part of the lookup.
1859     leaf = len(lookup.prefetch_through.split(LOOKUP_SEP)) - 1 == level
1860 
1861     for obj in instances:
1862         instance_attr_val = instance_attr(obj)
1863         vals = rel_obj_cache.get(instance_attr_val, [])
1864 
1865         if single:
1866             val = vals[0] if vals else None
1867             if as_attr:
1868                 # A to_attr has been given for the prefetch.
1869                 setattr(obj, to_attr, val)
1870             elif is_descriptor:
1871                 # cache_name points to a field name in obj.
1872                 # This field is a descriptor for a related object.
1873                 setattr(obj, cache_name, val)
1874             else:
1875                 # No to_attr has been given for this prefetch operation and the
1876                 # cache_name does not point to a descriptor. Store the value of
1877                 # the field in the object's field cache.
1878                 obj._state.fields_cache[cache_name] = val
1879         else:
1880             if as_attr:
1881                 setattr(obj, to_attr, vals)
1882             else:
1883                 manager = getattr(obj, to_attr)
1884                 if leaf and lookup.queryset is not None:
1885                     qs = manager._apply_rel_filters(lookup.queryset)
1886                 else:
1887                     qs = manager.get_queryset()
1888                 qs._result_cache = vals
1889                 # We don't want the individual qs doing prefetch_related now,
1890                 # since we have merged this into the current work.
1891                 qs._prefetch_done = True
1892                 obj._prefetched_objects_cache[cache_name] = qs
1893     return all_related_objects, additional_lookups
1894 
1895 
1896 class RelatedPopulator:
1897     """
1898     RelatedPopulator is used for select_related() object instantiation.
1899 
1900     The idea is that each select_related() model will be populated by a
1901     different RelatedPopulator instance. The RelatedPopulator instances get
1902     klass_info and select (computed in SQLCompiler) plus the used db as
1903     input for initialization. That data is used to compute which columns
1904     to use, how to instantiate the model, and how to populate the links
1905     between the objects.
1906 
1907     The actual creation of the objects is done in populate() method. This
1908     method gets row and from_obj as input and populates the select_related()
1909     model instance.
1910     """
1911     def __init__(self, klass_info, select, db):
1912         self.db = db
1913         # Pre-compute needed attributes. The attributes are:
1914         #  - model_cls: the possibly deferred model class to instantiate
1915         #  - either:
1916         #    - cols_start, cols_end: usually the columns in the row are
1917         #      in the same order model_cls.__init__ expects them, so we
1918         #      can instantiate by model_cls(*row[cols_start:cols_end])
1919         #    - reorder_for_init: When select_related descends to a child
1920         #      class, then we want to reuse the already selected parent
1921         #      data. However, in this case the parent data isn't necessarily
1922         #      in the same order that Model.__init__ expects it to be, so
1923         #      we have to reorder the parent data. The reorder_for_init
1924         #      attribute contains a function used to reorder the field data
1925         #      in the order __init__ expects it.
1926         #  - pk_idx: the index of the primary key field in the reordered
1927         #    model data. Used to check if a related object exists at all.
1928         #  - init_list: the field attnames fetched from the database. For
1929         #    deferred models this isn't the same as all attnames of the
1930         #    model's fields.
1931         #  - related_populators: a list of RelatedPopulator instances if
1932         #    select_related() descends to related models from this model.
1933         #  - local_setter, remote_setter: Methods to set cached values on
1934         #    the object being populated and on the remote object. Usually
1935         #    these are Field.set_cached_value() methods.
1936         select_fields = klass_info['select_fields']
1937         from_parent = klass_info['from_parent']
1938         if not from_parent:
1939             self.cols_start = select_fields[0]
1940             self.cols_end = select_fields[-1] + 1
1941             self.init_list = [
1942                 f[0].target.attname for f in select[self.cols_start:self.cols_end]
1943             ]
1944             self.reorder_for_init = None
1945         else:
1946             attname_indexes = {select[idx][0].target.attname: idx for idx in select_fields}
1947             model_init_attnames = (f.attname for f in klass_info['model']._meta.concrete_fields)
1948             self.init_list = [attname for attname in model_init_attnames if attname in attname_indexes]
1949             self.reorder_for_init = operator.itemgetter(*[attname_indexes[attname] for attname in self.init_list])
1950 
1951         self.model_cls = klass_info['model']
1952         self.pk_idx = self.init_list.index(self.model_cls._meta.pk.attname)
1953         self.related_populators = get_related_populators(klass_info, select, self.db)
1954         self.local_setter = klass_info['local_setter']
1955         self.remote_setter = klass_info['remote_setter']
1956 
1957     def populate(self, row, from_obj):
1958         if self.reorder_for_init:
1959             obj_data = self.reorder_for_init(row)
1960         else:
1961             obj_data = row[self.cols_start:self.cols_end]
1962         if obj_data[self.pk_idx] is None:
1963             obj = None
1964         else:
1965             obj = self.model_cls.from_db(self.db, self.init_list, obj_data)
1966             for rel_iter in self.related_populators:
1967                 rel_iter.populate(row, obj)
1968         self.local_setter(from_obj, obj)
1969         if obj is not None:
1970             self.remote_setter(obj, from_obj)
1971 
1972 
1973 def get_related_populators(klass_info, select, db):
1974     iterators = []
1975     related_klass_infos = klass_info.get('related_klass_infos', [])
1976     for rel_klass_info in related_klass_infos:
1977         rel_cls = RelatedPopulator(rel_klass_info, select, db)
1978         iterators.append(rel_cls)
1979     return iterators

</code><intended_behavior>The method constructing the IN query for prefetch_related should exclude NULL values from the list of IDs it uses to prefetch related objects. This change aims to optimize query performance and integrity by ensuring that NULL values, which won't match any related objects (since FK constraints prevent such matches), are not used in an IN clause. This behavior should be implemented in a way that does not affect other functionalities of prefetch_related, such as handling other valid lookup values or the ability to prefetch multiple related objects efficiently.</intended_behavior>]
2025-10-17 11:59:59.335 | INFO     | Search completed. Bug locations: [<file>django/db/models/query.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 The main QuerySet implementation. This provides the public API for the ORM.
3 """
4 
5 import copy
6 import operator
7 import warnings
8 from collections import namedtuple
9 from functools import lru_cache
10 from itertools import chain
11 
12 import django
13 from django.conf import settings
14 from django.core import exceptions
15 from django.db import (
16     DJANGO_VERSION_PICKLE_KEY, IntegrityError, NotSupportedError, connections,
17     router, transaction,
18 )
19 from django.db.models import AutoField, DateField, DateTimeField, sql
20 from django.db.models.constants import LOOKUP_SEP
21 from django.db.models.deletion import Collector
22 from django.db.models.expressions import Case, Expression, F, Value, When
23 from django.db.models.functions import Cast, Trunc
24 from django.db.models.query_utils import FilteredRelation, Q
25 from django.db.models.sql.constants import CURSOR, GET_ITERATOR_CHUNK_SIZE
26 from django.db.models.utils import resolve_callables
27 from django.utils import timezone
28 from django.utils.functional import cached_property, partition
29 
30 # The maximum number of results to fetch in a get() query.
31 MAX_GET_RESULTS = 21
32 
33 # The maximum number of items to display in a QuerySet.__repr__
34 REPR_OUTPUT_SIZE = 20
35 
36 
37 class BaseIterable:
38     def __init__(self, queryset, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE):
39         self.queryset = queryset
40         self.chunked_fetch = chunked_fetch
41         self.chunk_size = chunk_size
42 
43 
44 class ModelIterable(BaseIterable):
45     """Iterable that yields a model instance for each row."""
46 
47     def __iter__(self):
48         queryset = self.queryset
49         db = queryset.db
50         compiler = queryset.query.get_compiler(using=db)
51         # Execute the query. This will also fill compiler.select, klass_info,
52         # and annotations.
53         results = compiler.execute_sql(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)
54         select, klass_info, annotation_col_map = (compiler.select, compiler.klass_info,
55                                                   compiler.annotation_col_map)
56         model_cls = klass_info['model']
57         select_fields = klass_info['select_fields']
58         model_fields_start, model_fields_end = select_fields[0], select_fields[-1] + 1
59         init_list = [f[0].target.attname
60                      for f in select[model_fields_start:model_fields_end]]
61         related_populators = get_related_populators(klass_info, select, db)
62         known_related_objects = [
63             (field, related_objs, operator.attrgetter(*[
64                 field.attname
65                 if from_field == 'self' else
66                 queryset.model._meta.get_field(from_field).attname
67                 for from_field in field.from_fields
68             ])) for field, related_objs in queryset._known_related_objects.items()
69         ]
70         for row in compiler.results_iter(results):
71             obj = model_cls.from_db(db, init_list, row[model_fields_start:model_fields_end])
72             for rel_populator in related_populators:
73                 rel_populator.populate(row, obj)
74             if annotation_col_map:
75                 for attr_name, col_pos in annotation_col_map.items():
76                     setattr(obj, attr_name, row[col_pos])
77 
78             # Add the known related objects to the model.
79             for field, rel_objs, rel_getter in known_related_objects:
80                 # Avoid overwriting objects loaded by, e.g., select_related().
81                 if field.is_cached(obj):
82                     continue
83                 rel_obj_id = rel_getter(obj)
84                 try:
85                     rel_obj = rel_objs[rel_obj_id]
86                 except KeyError:
87                     pass  # May happen in qs1 | qs2 scenarios.
88                 else:
89                     setattr(obj, field.name, rel_obj)
90 
91             yield obj
92 
93 
94 class ValuesIterable(BaseIterable):
95     """
96     Iterable returned by QuerySet.values() that yields a dict for each row.
97     """
98 
99     def __iter__(self):
100         queryset = self.queryset
101         query = queryset.query
102         compiler = query.get_compiler(queryset.db)
103 
104         # extra(select=...) cols are always at the start of the row.
105         names = [
106             *query.extra_select,
107             *query.values_select,
108             *query.annotation_select,
109         ]
110         indexes = range(len(names))
111         for row in compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size):
112             yield {names[i]: row[i] for i in indexes}
113 
114 
115 class ValuesListIterable(BaseIterable):
116     """
117     Iterable returned by QuerySet.values_list(flat=False) that yields a tuple
118     for each row.
119     """
120 
121     def __iter__(self):
122         queryset = self.queryset
123         query = queryset.query
124         compiler = query.get_compiler(queryset.db)
125 
126         if queryset._fields:
127             # extra(select=...) cols are always at the start of the row.
128             names = [
129                 *query.extra_select,
130                 *query.values_select,
131                 *query.annotation_select,
132             ]
133             fields = [*queryset._fields, *(f for f in query.annotation_select if f not in queryset._fields)]
134             if fields != names:
135                 # Reorder according to fields.
136                 index_map = {name: idx for idx, name in enumerate(names)}
137                 rowfactory = operator.itemgetter(*[index_map[f] for f in fields])
138                 return map(
139                     rowfactory,
140                     compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)
141                 )
142         return compiler.results_iter(tuple_expected=True, chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)
143 
144 
145 class NamedValuesListIterable(ValuesListIterable):
146     """
147     Iterable returned by QuerySet.values_list(named=True) that yields a
148     namedtuple for each row.
149     """
150 
151     @staticmethod
152     @lru_cache()
153     def create_namedtuple_class(*names):
154         # Cache namedtuple() with @lru_cache() since it's too slow to be
155         # called for every QuerySet evaluation.
156         return namedtuple('Row', names)
157 
158     def __iter__(self):
159         queryset = self.queryset
160         if queryset._fields:
161             names = queryset._fields
162         else:
163             query = queryset.query
164             names = [*query.extra_select, *query.values_select, *query.annotation_select]
165         tuple_class = self.create_namedtuple_class(*names)
166         new = tuple.__new__
167         for row in super().__iter__():
168             yield new(tuple_class, row)
169 
170 
171 class FlatValuesListIterable(BaseIterable):
172     """
173     Iterable returned by QuerySet.values_list(flat=True) that yields single
174     values.
175     """
176 
177     def __iter__(self):
178         queryset = self.queryset
179         compiler = queryset.query.get_compiler(queryset.db)
180         for row in compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size):
181             yield row[0]
182 
183 
184 class QuerySet:
185     """Represent a lazy database lookup for a set of objects."""
186 
187     def __init__(self, model=None, query=None, using=None, hints=None):
188         self.model = model
189         self._db = using
190         self._hints = hints or {}
191         self._query = query or sql.Query(self.model)
192         self._result_cache = None
193         self._sticky_filter = False
194         self._for_write = False
195         self._prefetch_related_lookups = ()
196         self._prefetch_done = False
197         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
198         self._iterable_class = ModelIterable
199         self._fields = None
200         self._defer_next_filter = False
201         self._deferred_filter = None
202 
203     @property
204     def query(self):
205         if self._deferred_filter:
206             negate, args, kwargs = self._deferred_filter
207             self._filter_or_exclude_inplace(negate, *args, **kwargs)
208             self._deferred_filter = None
209         return self._query
210 
211     @query.setter
212     def query(self, value):
213         self._query = value
214 
215     def as_manager(cls):
216         # Address the circular dependency between `Queryset` and `Manager`.
217         from django.db.models.manager import Manager
218         manager = Manager.from_queryset(cls)()
219         manager._built_with_as_manager = True
220         return manager
221     as_manager.queryset_only = True
222     as_manager = classmethod(as_manager)
223 
224     ########################
225     # PYTHON MAGIC METHODS #
226     ########################
227 
228     def __deepcopy__(self, memo):
229         """Don't populate the QuerySet's cache."""
230         obj = self.__class__()
231         for k, v in self.__dict__.items():
232             if k == '_result_cache':
233                 obj.__dict__[k] = None
234             else:
235                 obj.__dict__[k] = copy.deepcopy(v, memo)
236         return obj
237 
238     def __getstate__(self):
239         # Force the cache to be fully populated.
240         self._fetch_all()
241         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
242 
243     def __setstate__(self, state):
244         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
245         if pickled_version:
246             if pickled_version != django.__version__:
247                 warnings.warn(
248                     "Pickled queryset instance's Django version %s does not "
249                     "match the current version %s."
250                     % (pickled_version, django.__version__),
251                     RuntimeWarning,
252                     stacklevel=2,
253                 )
254         else:
255             warnings.warn(
256                 "Pickled queryset instance's Django version is not specified.",
257                 RuntimeWarning,
258                 stacklevel=2,
259             )
260         self.__dict__.update(state)
261 
262     def __repr__(self):
263         data = list(self[:REPR_OUTPUT_SIZE + 1])
264         if len(data) > REPR_OUTPUT_SIZE:
265             data[-1] = "...(remaining elements truncated)..."
266         return '<%s %r>' % (self.__class__.__name__, data)
267 
268     def __len__(self):
269         self._fetch_all()
270         return len(self._result_cache)
271 
272     def __iter__(self):
273         """
274         The queryset iterator protocol uses three nested iterators in the
275         default case:
276             1. sql.compiler.execute_sql()
277                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
278                  using cursor.fetchmany(). This part is responsible for
279                  doing some column masking, and returning the rows in chunks.
280             2. sql.compiler.results_iter()
281                - Returns one row at time. At this point the rows are still just
282                  tuples. In some cases the return values are converted to
283                  Python values at this location.
284             3. self.iterator()
285                - Responsible for turning the rows into model objects.
286         """
287         self._fetch_all()
288         return iter(self._result_cache)
289 
290     def __bool__(self):
291         self._fetch_all()
292         return bool(self._result_cache)
293 
294     def __getitem__(self, k):
295         """Retrieve an item or slice from the set of results."""
296         if not isinstance(k, (int, slice)):
297             raise TypeError(
298                 'QuerySet indices must be integers or slices, not %s.'
299                 % type(k).__name__
300             )
301         assert ((not isinstance(k, slice) and (k >= 0)) or
302                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
303                  (k.stop is None or k.stop >= 0))), \
304             "Negative indexing is not supported."
305 
306         if self._result_cache is not None:
307             return self._result_cache[k]
308 
309         if isinstance(k, slice):
310             qs = self._chain()
311             if k.start is not None:
312                 start = int(k.start)
313             else:
314                 start = None
315             if k.stop is not None:
316                 stop = int(k.stop)
317             else:
318                 stop = None
319             qs.query.set_limits(start, stop)
320             return list(qs)[::k.step] if k.step else qs
321 
322         qs = self._chain()
323         qs.query.set_limits(k, k + 1)
324         qs._fetch_all()
325         return qs._result_cache[0]
326 
327     def __class_getitem__(cls, *args, **kwargs):
328         return cls
329 
330     def __and__(self, other):
331         self._merge_sanity_check(other)
332         if isinstance(other, EmptyQuerySet):
333             return other
334         if isinstance(self, EmptyQuerySet):
335             return self
336         combined = self._chain()
337         combined._merge_known_related_objects(other)
338         combined.query.combine(other.query, sql.AND)
339         return combined
340 
341     def __or__(self, other):
342         self._merge_sanity_check(other)
343         if isinstance(self, EmptyQuerySet):
344             return other
345         if isinstance(other, EmptyQuerySet):
346             return self
347         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
348         combined = query._chain()
349         combined._merge_known_related_objects(other)
350         if not other.query.can_filter():
351             other = other.model._base_manager.filter(pk__in=other.values('pk'))
352         combined.query.combine(other.query, sql.OR)
353         return combined
354 
355     ####################################
356     # METHODS THAT DO DATABASE QUERIES #
357     ####################################
358 
359     def _iterator(self, use_chunked_fetch, chunk_size):
360         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
361 
362     def iterator(self, chunk_size=2000):
363         """
364         An iterator over the results from applying this QuerySet to the
365         database.
366         """
367         if chunk_size <= 0:
368             raise ValueError('Chunk size must be strictly positive.')
369         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
370         return self._iterator(use_chunked_fetch, chunk_size)
371 
372     def aggregate(self, *args, **kwargs):
373         """
374         Return a dictionary containing the calculations (aggregation)
375         over the current queryset.
376 
377         If args is present the expression is passed as a kwarg using
378         the Aggregate object's default alias.
379         """
380         if self.query.distinct_fields:
381             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
382         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
383         for arg in args:
384             # The default_alias property raises TypeError if default_alias
385             # can't be set automatically or AttributeError if it isn't an
386             # attribute.
387             try:
388                 arg.default_alias
389             except (AttributeError, TypeError):
390                 raise TypeError("Complex aggregates require an alias")
391             kwargs[arg.default_alias] = arg
392 
393         query = self.query.chain()
394         for (alias, aggregate_expr) in kwargs.items():
395             query.add_annotation(aggregate_expr, alias, is_summary=True)
396             if not query.annotations[alias].contains_aggregate:
397                 raise TypeError("%s is not an aggregate expression" % alias)
398         return query.get_aggregation(self.db, kwargs)
399 
400     def count(self):
401         """
402         Perform a SELECT COUNT() and return the number of records as an
403         integer.
404 
405         If the QuerySet is already fully cached, return the length of the
406         cached results set to avoid multiple SELECT COUNT(*) calls.
407         """
408         if self._result_cache is not None:
409             return len(self._result_cache)
410 
411         return self.query.get_count(using=self.db)
412 
413     def get(self, *args, **kwargs):
414         """
415         Perform the query and return a single object matching the given
416         keyword arguments.
417         """
418         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
419         if self.query.can_filter() and not self.query.distinct_fields:
420             clone = clone.order_by()
421         limit = None
422         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
423             limit = MAX_GET_RESULTS
424             clone.query.set_limits(high=limit)
425         num = len(clone)
426         if num == 1:
427             return clone._result_cache[0]
428         if not num:
429             raise self.model.DoesNotExist(
430                 "%s matching query does not exist." %
431                 self.model._meta.object_name
432             )
433         raise self.model.MultipleObjectsReturned(
434             'get() returned more than one %s -- it returned %s!' % (
435                 self.model._meta.object_name,
436                 num if not limit or num < limit else 'more than %s' % (limit - 1),
437             )
438         )
439 
440     def create(self, **kwargs):
441         """
442         Create a new object with the given kwargs, saving it to the database
443         and returning the created object.
444         """
445         obj = self.model(**kwargs)
446         self._for_write = True
447         obj.save(force_insert=True, using=self.db)
448         return obj
449 
450     def _populate_pk_values(self, objs):
451         for obj in objs:
452             if obj.pk is None:
453                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
454 
455     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
456         """
457         Insert each of the instances into the database. Do *not* call
458         save() on each of the instances, do not send any pre/post_save
459         signals, and do not set the primary key attribute if it is an
460         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
461         Multi-table models are not supported.
462         """
463         # When you bulk insert you don't get the primary keys back (if it's an
464         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
465         # you can't insert into the child tables which references this. There
466         # are two workarounds:
467         # 1) This could be implemented if you didn't have an autoincrement pk
468         # 2) You could do it by doing O(n) normal inserts into the parent
469         #    tables to get the primary keys back and then doing a single bulk
470         #    insert into the childmost table.
471         # We currently set the primary keys on the objects when using
472         # PostgreSQL via the RETURNING ID clause. It should be possible for
473         # Oracle as well, but the semantics for extracting the primary keys is
474         # trickier so it's not done yet.
475         assert batch_size is None or batch_size > 0
476         # Check that the parents share the same concrete model with the our
477         # model to detect the inheritance pattern ConcreteGrandParent ->
478         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
479         # would not identify that case as involving multiple tables.
480         for parent in self.model._meta.get_parent_list():
481             if parent._meta.concrete_model is not self.model._meta.concrete_model:
482                 raise ValueError("Can't bulk create a multi-table inherited model")
483         if not objs:
484             return objs
485         self._for_write = True
486         connection = connections[self.db]
487         opts = self.model._meta
488         fields = opts.concrete_fields
489         objs = list(objs)
490         self._populate_pk_values(objs)
491         with transaction.atomic(using=self.db, savepoint=False):
492             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
493             if objs_with_pk:
494                 returned_columns = self._batched_insert(
495                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
496                 )
497                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
498                     for result, field in zip(results, opts.db_returning_fields):
499                         if field != opts.pk:
500                             setattr(obj_with_pk, field.attname, result)
501                 for obj_with_pk in objs_with_pk:
502                     obj_with_pk._state.adding = False
503                     obj_with_pk._state.db = self.db
504             if objs_without_pk:
505                 fields = [f for f in fields if not isinstance(f, AutoField)]
506                 returned_columns = self._batched_insert(
507                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
508                 )
509                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
510                     assert len(returned_columns) == len(objs_without_pk)
511                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
512                     for result, field in zip(results, opts.db_returning_fields):
513                         setattr(obj_without_pk, field.attname, result)
514                     obj_without_pk._state.adding = False
515                     obj_without_pk._state.db = self.db
516 
517         return objs
518 
519     def bulk_update(self, objs, fields, batch_size=None):
520         """
521         Update the given fields in each of the given objects in the database.
522         """
523         if batch_size is not None and batch_size < 0:
524             raise ValueError('Batch size must be a positive integer.')
525         if not fields:
526             raise ValueError('Field names must be given to bulk_update().')
527         objs = tuple(objs)
528         if any(obj.pk is None for obj in objs):
529             raise ValueError('All bulk_update() objects must have a primary key set.')
530         fields = [self.model._meta.get_field(name) for name in fields]
531         if any(not f.concrete or f.many_to_many for f in fields):
532             raise ValueError('bulk_update() can only be used with concrete fields.')
533         if any(f.primary_key for f in fields):
534             raise ValueError('bulk_update() cannot be used with primary key fields.')
535         if not objs:
536             return
537         # PK is used twice in the resulting update query, once in the filter
538         # and once in the WHEN. Each field will also have one CAST.
539         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
540         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
541         requires_casting = connections[self.db].features.requires_casted_case_in_updates
542         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
543         updates = []
544         for batch_objs in batches:
545             update_kwargs = {}
546             for field in fields:
547                 when_statements = []
548                 for obj in batch_objs:
549                     attr = getattr(obj, field.attname)
550                     if not isinstance(attr, Expression):
551                         attr = Value(attr, output_field=field)
552                     when_statements.append(When(pk=obj.pk, then=attr))
553                 case_statement = Case(*when_statements, output_field=field)
554                 if requires_casting:
555                     case_statement = Cast(case_statement, output_field=field)
556                 update_kwargs[field.attname] = case_statement
557             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
558         with transaction.atomic(using=self.db, savepoint=False):
559             for pks, update_kwargs in updates:
560                 self.filter(pk__in=pks).update(**update_kwargs)
561     bulk_update.alters_data = True
562 
563     def get_or_create(self, defaults=None, **kwargs):
564         """
565         Look up an object with the given kwargs, creating one if necessary.
566         Return a tuple of (object, created), where created is a boolean
567         specifying whether an object was created.
568         """
569         # The get() needs to be targeted at the write database in order
570         # to avoid potential transaction consistency problems.
571         self._for_write = True
572         try:
573             return self.get(**kwargs), False
574         except self.model.DoesNotExist:
575             params = self._extract_model_params(defaults, **kwargs)
576             return self._create_object_from_params(kwargs, params)
577 
578     def update_or_create(self, defaults=None, **kwargs):
579         """
580         Look up an object with the given kwargs, updating one with defaults
581         if it exists, otherwise create a new one.
582         Return a tuple (object, created), where created is a boolean
583         specifying whether an object was created.
584         """
585         defaults = defaults or {}
586         self._for_write = True
587         with transaction.atomic(using=self.db):
588             try:
589                 obj = self.select_for_update().get(**kwargs)
590             except self.model.DoesNotExist:
591                 params = self._extract_model_params(defaults, **kwargs)
592                 # Lock the row so that a concurrent update is blocked until
593                 # after update_or_create() has performed its save.
594                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
595                 if created:
596                     return obj, created
597             for k, v in resolve_callables(defaults):
598                 setattr(obj, k, v)
599             obj.save(using=self.db)
600         return obj, False
601 
602     def _create_object_from_params(self, lookup, params, lock=False):
603         """
604         Try to create an object using passed params. Used by get_or_create()
605         and update_or_create().
606         """
607         try:
608             with transaction.atomic(using=self.db):
609                 params = dict(resolve_callables(params))
610                 obj = self.create(**params)
611             return obj, True
612         except IntegrityError:
613             try:
614                 qs = self.select_for_update() if lock else self
615                 return qs.get(**lookup), False
616             except self.model.DoesNotExist:
617                 pass
618             raise
619 
620     def _extract_model_params(self, defaults, **kwargs):
621         """
622         Prepare `params` for creating a model instance based on the given
623         kwargs; for use by get_or_create() and update_or_create().
624         """
625         defaults = defaults or {}
626         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
627         params.update(defaults)
628         property_names = self.model._meta._property_names
629         invalid_params = []
630         for param in params:
631             try:
632                 self.model._meta.get_field(param)
633             except exceptions.FieldDoesNotExist:
634                 # It's okay to use a model's property if it has a setter.
635                 if not (param in property_names and getattr(self.model, param).fset):
636                     invalid_params.append(param)
637         if invalid_params:
638             raise exceptions.FieldError(
639                 "Invalid field name(s) for model %s: '%s'." % (
640                     self.model._meta.object_name,
641                     "', '".join(sorted(invalid_params)),
642                 ))
643         return params
644 
645     def _earliest(self, *fields):
646         """
647         Return the earliest object according to fields (if given) or by the
648         model's Meta.get_latest_by.
649         """
650         if fields:
651             order_by = fields
652         else:
653             order_by = getattr(self.model._meta, 'get_latest_by')
654             if order_by and not isinstance(order_by, (tuple, list)):
655                 order_by = (order_by,)
656         if order_by is None:
657             raise ValueError(
658                 "earliest() and latest() require either fields as positional "
659                 "arguments or 'get_latest_by' in the model's Meta."
660             )
661 
662         assert not self.query.is_sliced, \
663             "Cannot change a query once a slice has been taken."
664         obj = self._chain()
665         obj.query.set_limits(high=1)
666         obj.query.clear_ordering(force_empty=True)
667         obj.query.add_ordering(*order_by)
668         return obj.get()
669 
670     def earliest(self, *fields):
671         return self._earliest(*fields)
672 
673     def latest(self, *fields):
674         return self.reverse()._earliest(*fields)
675 
676     def first(self):
677         """Return the first object of a query or None if no match is found."""
678         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
679             return obj
680 
681     def last(self):
682         """Return the last object of a query or None if no match is found."""
683         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
684             return obj
685 
686     def in_bulk(self, id_list=None, *, field_name='pk'):
687         """
688         Return a dictionary mapping each of the given IDs to the object with
689         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
690         """
691         assert not self.query.is_sliced, \
692             "Cannot use 'limit' or 'offset' with in_bulk"
693         opts = self.model._meta
694         unique_fields = [
695             constraint.fields[0]
696             for constraint in opts.total_unique_constraints
697             if len(constraint.fields) == 1
698         ]
699         if (
700             field_name != 'pk' and
701             not opts.get_field(field_name).unique and
702             field_name not in unique_fields
703         ):
704             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
705         if id_list is not None:
706             if not id_list:
707                 return {}
708             filter_key = '{}__in'.format(field_name)
709             batch_size = connections[self.db].features.max_query_params
710             id_list = tuple(id_list)
711             # If the database has a limit on the number of query parameters
712             # (e.g. SQLite), retrieve objects in batches if necessary.
713             if batch_size and batch_size < len(id_list):
714                 qs = ()
715                 for offset in range(0, len(id_list), batch_size):
716                     batch = id_list[offset:offset + batch_size]
717                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
718             else:
719                 qs = self.filter(**{filter_key: id_list}).order_by()
720         else:
721             qs = self._chain()
722         return {getattr(obj, field_name): obj for obj in qs}
723 
724     def delete(self):
725         """Delete the records in the current QuerySet."""
726         self._not_support_combined_queries('delete')
727         assert not self.query.is_sliced, \
728             "Cannot use 'limit' or 'offset' with delete."
729 
730         if self._fields is not None:
731             raise TypeError("Cannot call delete() after .values() or .values_list()")
732 
733         del_query = self._chain()
734 
735         # The delete is actually 2 queries - one to find related objects,
736         # and one to delete. Make sure that the discovery of related
737         # objects is performed on the same database as the deletion.
738         del_query._for_write = True
739 
740         # Disable non-supported fields.
741         del_query.query.select_for_update = False
742         del_query.query.select_related = False
743         del_query.query.clear_ordering(force_empty=True)
744 
745         collector = Collector(using=del_query.db)
746         collector.collect(del_query)
747         deleted, _rows_count = collector.delete()
748 
749         # Clear the result cache, in case this QuerySet gets reused.
750         self._result_cache = None
751         return deleted, _rows_count
752 
753     delete.alters_data = True
754     delete.queryset_only = True
755 
756     def _raw_delete(self, using):
757         """
758         Delete objects found from the given queryset in single direct SQL
759         query. No signals are sent and there is no protection for cascades.
760         """
761         query = self.query.clone()
762         query.__class__ = sql.DeleteQuery
763         cursor = query.get_compiler(using).execute_sql(CURSOR)
764         if cursor:
765             with cursor:
766                 return cursor.rowcount
767         return 0
768     _raw_delete.alters_data = True
769 
770     def update(self, **kwargs):
771         """
772         Update all elements in the current QuerySet, setting all the given
773         fields to the appropriate values.
774         """
775         self._not_support_combined_queries('update')
776         assert not self.query.is_sliced, \
777             "Cannot update a query once a slice has been taken."
778         self._for_write = True
779         query = self.query.chain(sql.UpdateQuery)
780         query.add_update_values(kwargs)
781         # Clear any annotations so that they won't be present in subqueries.
782         query.annotations = {}
783         with transaction.mark_for_rollback_on_error(using=self.db):
784             rows = query.get_compiler(self.db).execute_sql(CURSOR)
785         self._result_cache = None
786         return rows
787     update.alters_data = True
788 
789     def _update(self, values):
790         """
791         A version of update() that accepts field objects instead of field names.
792         Used primarily for model saving and not intended for use by general
793         code (it requires too much poking around at model internals to be
794         useful at that level).
795         """
796         assert not self.query.is_sliced, \
797             "Cannot update a query once a slice has been taken."
798         query = self.query.chain(sql.UpdateQuery)
799         query.add_update_fields(values)
800         # Clear any annotations so that they won't be present in subqueries.
801         query.annotations = {}
802         self._result_cache = None
803         return query.get_compiler(self.db).execute_sql(CURSOR)
804     _update.alters_data = True
805     _update.queryset_only = False
806 
807     def exists(self):
808         if self._result_cache is None:
809             return self.query.has_results(using=self.db)
810         return bool(self._result_cache)
811 
812     def _prefetch_related_objects(self):
813         # This method can only be called once the result cache has been filled.
814         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
815         self._prefetch_done = True
816 
817     def explain(self, *, format=None, **options):
818         return self.query.explain(using=self.db, format=format, **options)
819 
820     ##################################################
821     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
822     ##################################################
823 
824     def raw(self, raw_query, params=None, translations=None, using=None):
825         if using is None:
826             using = self.db
827         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
828         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
829         return qs
830 
831     def _values(self, *fields, **expressions):
832         clone = self._chain()
833         if expressions:
834             clone = clone.annotate(**expressions)
835         clone._fields = fields
836         clone.query.set_values(fields)
837         return clone
838 
839     def values(self, *fields, **expressions):
840         fields += tuple(expressions)
841         clone = self._values(*fields, **expressions)
842         clone._iterable_class = ValuesIterable
843         return clone
844 
845     def values_list(self, *fields, flat=False, named=False):
846         if flat and named:
847             raise TypeError("'flat' and 'named' can't be used together.")
848         if flat and len(fields) > 1:
849             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
850 
851         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
852         _fields = []
853         expressions = {}
854         counter = 1
855         for field in fields:
856             if hasattr(field, 'resolve_expression'):
857                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
858                 while True:
859                     field_id = field_id_prefix + str(counter)
860                     counter += 1
861                     if field_id not in field_names:
862                         break
863                 expressions[field_id] = field
864                 _fields.append(field_id)
865             else:
866                 _fields.append(field)
867 
868         clone = self._values(*_fields, **expressions)
869         clone._iterable_class = (
870             NamedValuesListIterable if named
871             else FlatValuesListIterable if flat
872             else ValuesListIterable
873         )
874         return clone
875 
876     def dates(self, field_name, kind, order='ASC'):
877         """
878         Return a list of date objects representing all available dates for
879         the given field_name, scoped to 'kind'.
880         """
881         assert kind in ('year', 'month', 'week', 'day'), \
882             "'kind' must be one of 'year', 'month', 'week', or 'day'."
883         assert order in ('ASC', 'DESC'), \
884             "'order' must be either 'ASC' or 'DESC'."
885         return self.annotate(
886             datefield=Trunc(field_name, kind, output_field=DateField()),
887             plain_field=F(field_name)
888         ).values_list(
889             'datefield', flat=True
890         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
891 
892     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):
893         """
894         Return a list of datetime objects representing all available
895         datetimes for the given field_name, scoped to 'kind'.
896         """
897         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
898             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
899         assert order in ('ASC', 'DESC'), \
900             "'order' must be either 'ASC' or 'DESC'."
901         if settings.USE_TZ:
902             if tzinfo is None:
903                 tzinfo = timezone.get_current_timezone()
904         else:
905             tzinfo = None
906         return self.annotate(
907             datetimefield=Trunc(
908                 field_name,
909                 kind,
910                 output_field=DateTimeField(),
911                 tzinfo=tzinfo,
912                 is_dst=is_dst,
913             ),
914             plain_field=F(field_name)
915         ).values_list(
916             'datetimefield', flat=True
917         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
918 
919     def none(self):
920         """Return an empty QuerySet."""
921         clone = self._chain()
922         clone.query.set_empty()
923         return clone
924 
925     ##################################################################
926     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
927     ##################################################################
928 
929     def all(self):
930         """
931         Return a new QuerySet that is a copy of the current one. This allows a
932         QuerySet to proxy for a model manager in some cases.
933         """
934         return self._chain()
935 
936     def filter(self, *args, **kwargs):
937         """
938         Return a new QuerySet instance with the args ANDed to the existing
939         set.
940         """
941         self._not_support_combined_queries('filter')
942         return self._filter_or_exclude(False, *args, **kwargs)
943 
944     def exclude(self, *args, **kwargs):
945         """
946         Return a new QuerySet instance with NOT (args) ANDed to the existing
947         set.
948         """
949         self._not_support_combined_queries('exclude')
950         return self._filter_or_exclude(True, *args, **kwargs)
951 
952     def _filter_or_exclude(self, negate, *args, **kwargs):
953         if args or kwargs:
954             assert not self.query.is_sliced, \
955                 "Cannot filter a query once a slice has been taken."
956 
957         clone = self._chain()
958         if self._defer_next_filter:
959             self._defer_next_filter = False
960             clone._deferred_filter = negate, args, kwargs
961         else:
962             clone._filter_or_exclude_inplace(negate, *args, **kwargs)
963         return clone
964 
965     def _filter_or_exclude_inplace(self, negate, *args, **kwargs):
966         if negate:
967             self._query.add_q(~Q(*args, **kwargs))
968         else:
969             self._query.add_q(Q(*args, **kwargs))
970 
971     def complex_filter(self, filter_obj):
972         """
973         Return a new QuerySet instance with filter_obj added to the filters.
974 
975         filter_obj can be a Q object or a dictionary of keyword lookup
976         arguments.
977 
978         This exists to support framework features such as 'limit_choices_to',
979         and usually it will be more natural to use other methods.
980         """
981         if isinstance(filter_obj, Q):
982             clone = self._chain()
983             clone.query.add_q(filter_obj)
984             return clone
985         else:
986             return self._filter_or_exclude(False, **filter_obj)
987 
988     def _combinator_query(self, combinator, *other_qs, all=False):
989         # Clone the query to inherit the select list and everything
990         clone = self._chain()
991         # Clear limits and ordering so they can be reapplied
992         clone.query.clear_ordering(True)
993         clone.query.clear_limits()
994         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
995         clone.query.combinator = combinator
996         clone.query.combinator_all = all
997         return clone
998 
999     def union(self, *other_qs, all=False):
1000         # If the query is an EmptyQuerySet, combine all nonempty querysets.
1001         if isinstance(self, EmptyQuerySet):
1002             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1003             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
1004         return self._combinator_query('union', *other_qs, all=all)
1005 
1006     def intersection(self, *other_qs):
1007         # If any query is an EmptyQuerySet, return it.
1008         if isinstance(self, EmptyQuerySet):
1009             return self
1010         for other in other_qs:
1011             if isinstance(other, EmptyQuerySet):
1012                 return other
1013         return self._combinator_query('intersection', *other_qs)
1014 
1015     def difference(self, *other_qs):
1016         # If the query is an EmptyQuerySet, return it.
1017         if isinstance(self, EmptyQuerySet):
1018             return self
1019         return self._combinator_query('difference', *other_qs)
1020 
1021     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1022         """
1023         Return a new QuerySet instance that will select objects with a
1024         FOR UPDATE lock.
1025         """
1026         if nowait and skip_locked:
1027             raise ValueError('The nowait option cannot be used with skip_locked.')
1028         obj = self._chain()
1029         obj._for_write = True
1030         obj.query.select_for_update = True
1031         obj.query.select_for_update_nowait = nowait
1032         obj.query.select_for_update_skip_locked = skip_locked
1033         obj.query.select_for_update_of = of
1034         obj.query.select_for_no_key_update = no_key
1035         return obj
1036 
1037     def select_related(self, *fields):
1038         """
1039         Return a new QuerySet instance that will select related objects.
1040 
1041         If fields are specified, they must be ForeignKey fields and only those
1042         related objects are included in the selection.
1043 
1044         If select_related(None) is called, clear the list.
1045         """
1046         self._not_support_combined_queries('select_related')
1047         if self._fields is not None:
1048             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1049 
1050         obj = self._chain()
1051         if fields == (None,):
1052             obj.query.select_related = False
1053         elif fields:
1054             obj.query.add_select_related(fields)
1055         else:
1056             obj.query.select_related = True
1057         return obj
1058 
1059     def prefetch_related(self, *lookups):
1060         """
1061         Return a new QuerySet instance that will prefetch the specified
1062         Many-To-One and Many-To-Many related objects when the QuerySet is
1063         evaluated.
1064 
1065         When prefetch_related() is called more than once, append to the list of
1066         prefetch lookups. If prefetch_related(None) is called, clear the list.
1067         """
1068         self._not_support_combined_queries('prefetch_related')
1069         clone = self._chain()
1070         if lookups == (None,):
1071             clone._prefetch_related_lookups = ()
1072         else:
1073             for lookup in lookups:
1074                 if isinstance(lookup, Prefetch):
1075                     lookup = lookup.prefetch_to
1076                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1077                 if lookup in self.query._filtered_relations:
1078                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1079             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1080         return clone
1081 
1082     def annotate(self, *args, **kwargs):
1083         """
1084         Return a query set in which the returned objects have been annotated
1085         with extra data or aggregations.
1086         """
1087         self._not_support_combined_queries('annotate')
1088         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1089         annotations = {}
1090         for arg in args:
1091             # The default_alias property may raise a TypeError.
1092             try:
1093                 if arg.default_alias in kwargs:
1094                     raise ValueError("The named annotation '%s' conflicts with the "
1095                                      "default name for another annotation."
1096                                      % arg.default_alias)
1097             except TypeError:
1098                 raise TypeError("Complex annotations require an alias")
1099             annotations[arg.default_alias] = arg
1100         annotations.update(kwargs)
1101 
1102         clone = self._chain()
1103         names = self._fields
1104         if names is None:
1105             names = set(chain.from_iterable(
1106                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1107                 for field in self.model._meta.get_fields()
1108             ))
1109 
1110         for alias, annotation in annotations.items():
1111             if alias in names:
1112                 raise ValueError("The annotation '%s' conflicts with a field on "
1113                                  "the model." % alias)
1114             if isinstance(annotation, FilteredRelation):
1115                 clone.query.add_filtered_relation(annotation, alias)
1116             else:
1117                 clone.query.add_annotation(annotation, alias, is_summary=False)
1118 
1119         for alias, annotation in clone.query.annotations.items():
1120             if alias in annotations and annotation.contains_aggregate:
1121                 if clone._fields is None:
1122                     clone.query.group_by = True
1123                 else:
1124                     clone.query.set_group_by()
1125                 break
1126 
1127         return clone
1128 
1129     def order_by(self, *field_names):
1130         """Return a new QuerySet instance with the ordering changed."""
1131         assert not self.query.is_sliced, \
1132             "Cannot reorder a query once a slice has been taken."
1133         obj = self._chain()
1134         obj.query.clear_ordering(force_empty=False)
1135         obj.query.add_ordering(*field_names)
1136         return obj
1137 
1138     def distinct(self, *field_names):
1139         """
1140         Return a new QuerySet instance that will select only distinct results.
1141         """
1142         self._not_support_combined_queries('distinct')
1143         assert not self.query.is_sliced, \
1144             "Cannot create distinct fields once a slice has been taken."
1145         obj = self._chain()
1146         obj.query.add_distinct_fields(*field_names)
1147         return obj
1148 
1149     def extra(self, select=None, where=None, params=None, tables=None,
1150               order_by=None, select_params=None):
1151         """Add extra SQL fragments to the query."""
1152         self._not_support_combined_queries('extra')
1153         assert not self.query.is_sliced, \
1154             "Cannot change a query once a slice has been taken"
1155         clone = self._chain()
1156         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1157         return clone
1158 
1159     def reverse(self):
1160         """Reverse the ordering of the QuerySet."""
1161         if self.query.is_sliced:
1162             raise TypeError('Cannot reverse a query once a slice has been taken.')
1163         clone = self._chain()
1164         clone.query.standard_ordering = not clone.query.standard_ordering
1165         return clone
1166 
1167     def defer(self, *fields):
1168         """
1169         Defer the loading of data for certain fields until they are accessed.
1170         Add the set of deferred fields to any existing set of deferred fields.
1171         The only exception to this is if None is passed in as the only
1172         parameter, in which case removal all deferrals.
1173         """
1174         self._not_support_combined_queries('defer')
1175         if self._fields is not None:
1176             raise TypeError("Cannot call defer() after .values() or .values_list()")
1177         clone = self._chain()
1178         if fields == (None,):
1179             clone.query.clear_deferred_loading()
1180         else:
1181             clone.query.add_deferred_loading(fields)
1182         return clone
1183 
1184     def only(self, *fields):
1185         """
1186         Essentially, the opposite of defer(). Only the fields passed into this
1187         method and that are not already specified as deferred are loaded
1188         immediately when the queryset is evaluated.
1189         """
1190         self._not_support_combined_queries('only')
1191         if self._fields is not None:
1192             raise TypeError("Cannot call only() after .values() or .values_list()")
1193         if fields == (None,):
1194             # Can only pass None to defer(), not only(), as the rest option.
1195             # That won't stop people trying to do this, so let's be explicit.
1196             raise TypeError("Cannot pass None as an argument to only().")
1197         for field in fields:
1198             field = field.split(LOOKUP_SEP, 1)[0]
1199             if field in self.query._filtered_relations:
1200                 raise ValueError('only() is not supported with FilteredRelation.')
1201         clone = self._chain()
1202         clone.query.add_immediate_loading(fields)
1203         return clone
1204 
1205     def using(self, alias):
1206         """Select which database this QuerySet should execute against."""
1207         clone = self._chain()
1208         clone._db = alias
1209         return clone
1210 
1211     ###################################
1212     # PUBLIC INTROSPECTION ATTRIBUTES #
1213     ###################################
1214 
1215     @property
1216     def ordered(self):
1217         """
1218         Return True if the QuerySet is ordered -- i.e. has an order_by()
1219         clause or a default ordering on the model (or is empty).
1220         """
1221         if isinstance(self, EmptyQuerySet):
1222             return True
1223         if self.query.extra_order_by or self.query.order_by:
1224             return True
1225         elif self.query.default_ordering and self.query.get_meta().ordering:
1226             return True
1227         else:
1228             return False
1229 
1230     @property
1231     def db(self):
1232         """Return the database used if this query is executed now."""
1233         if self._for_write:
1234             return self._db or router.db_for_write(self.model, **self._hints)
1235         return self._db or router.db_for_read(self.model, **self._hints)
1236 
1237     ###################
1238     # PRIVATE METHODS #
1239     ###################
1240 
1241     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1242         """
1243         Insert a new record for the given model. This provides an interface to
1244         the InsertQuery class and is how Model.save() is implemented.
1245         """
1246         self._for_write = True
1247         if using is None:
1248             using = self.db
1249         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1250         query.insert_values(fields, objs, raw=raw)
1251         return query.get_compiler(using=using).execute_sql(returning_fields)
1252     _insert.alters_data = True
1253     _insert.queryset_only = False
1254 
1255     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1256         """
1257         Helper method for bulk_create() to insert objs one batch at a time.
1258         """
1259         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1260             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1261         ops = connections[self.db].ops
1262         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1263         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1264         inserted_rows = []
1265         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1266         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1267             if bulk_return and not ignore_conflicts:
1268                 inserted_rows.extend(self._insert(
1269                     item, fields=fields, using=self.db,
1270                     returning_fields=self.model._meta.db_returning_fields,
1271                     ignore_conflicts=ignore_conflicts,
1272                 ))
1273             else:
1274                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1275         return inserted_rows
1276 
1277     def _chain(self, **kwargs):
1278         """
1279         Return a copy of the current QuerySet that's ready for another
1280         operation.
1281         """
1282         obj = self._clone()
1283         if obj._sticky_filter:
1284             obj.query.filter_is_sticky = True
1285             obj._sticky_filter = False
1286         obj.__dict__.update(kwargs)
1287         return obj
1288 
1289     def _clone(self):
1290         """
1291         Return a copy of the current QuerySet. A lightweight alternative
1292         to deepcopy().
1293         """
1294         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1295         c._sticky_filter = self._sticky_filter
1296         c._for_write = self._for_write
1297         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1298         c._known_related_objects = self._known_related_objects
1299         c._iterable_class = self._iterable_class
1300         c._fields = self._fields
1301         return c
1302 
1303     def _fetch_all(self):
1304         if self._result_cache is None:
1305             self._result_cache = list(self._iterable_class(self))
1306         if self._prefetch_related_lookups and not self._prefetch_done:
1307             self._prefetch_related_objects()
1308 
1309     def _next_is_sticky(self):
1310         """
1311         Indicate that the next filter call and the one following that should
1312         be treated as a single filter. This is only important when it comes to
1313         determining when to reuse tables for many-to-many filters. Required so
1314         that we can filter naturally on the results of related managers.
1315 
1316         This doesn't return a clone of the current QuerySet (it returns
1317         "self"). The method is only used internally and should be immediately
1318         followed by a filter() that does create a clone.
1319         """
1320         self._sticky_filter = True
1321         return self
1322 
1323     def _merge_sanity_check(self, other):
1324         """Check that two QuerySet classes may be merged."""
1325         if self._fields is not None and (
1326                 set(self.query.values_select) != set(other.query.values_select) or
1327                 set(self.query.extra_select) != set(other.query.extra_select) or
1328                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1329             raise TypeError(
1330                 "Merging '%s' classes must involve the same values in each case."
1331                 % self.__class__.__name__
1332             )
1333 
1334     def _merge_known_related_objects(self, other):
1335         """
1336         Keep track of all known related objects from either QuerySet instance.
1337         """
1338         for field, objects in other._known_related_objects.items():
1339             self._known_related_objects.setdefault(field, {}).update(objects)
1340 
1341     def resolve_expression(self, *args, **kwargs):
1342         if self._fields and len(self._fields) > 1:
1343             # values() queryset can only be used as nested queries
1344             # if they are set up to select only a single field.
1345             raise TypeError('Cannot use multi-field values as a filter value.')
1346         query = self.query.resolve_expression(*args, **kwargs)
1347         query._db = self._db
1348         return query
1349     resolve_expression.queryset_only = True
1350 
1351     def _add_hints(self, **hints):
1352         """
1353         Update hinting information for use by routers. Add new key/values or
1354         overwrite existing key/values.
1355         """
1356         self._hints.update(hints)
1357 
1358     def _has_filters(self):
1359         """
1360         Check if this QuerySet has any filtering going on. This isn't
1361         equivalent with checking if all objects are present in results, for
1362         example, qs[1:]._has_filters() -> False.
1363         """
1364         return self.query.has_filters()
1365 
1366     @staticmethod
1367     def _validate_values_are_expressions(values, method_name):
1368         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1369         if invalid_args:
1370             raise TypeError(
1371                 'QuerySet.%s() received non-expression(s): %s.' % (
1372                     method_name,
1373                     ', '.join(invalid_args),
1374                 )
1375             )
1376 
1377     def _not_support_combined_queries(self, operation_name):
1378         if self.query.combinator:
1379             raise NotSupportedError(
1380                 'Calling QuerySet.%s() after %s() is not supported.'
1381                 % (operation_name, self.query.combinator)
1382             )
1383 
1384 
1385 class InstanceCheckMeta(type):
1386     def __instancecheck__(self, instance):
1387         return isinstance(instance, QuerySet) and instance.query.is_empty()
1388 
1389 
1390 class EmptyQuerySet(metaclass=InstanceCheckMeta):
1391     """
1392     Marker class to checking if a queryset is empty by .none():
1393         isinstance(qs.none(), EmptyQuerySet) -> True
1394     """
1395 
1396     def __init__(self, *args, **kwargs):
1397         raise TypeError("EmptyQuerySet can't be instantiated")
1398 
1399 
1400 class RawQuerySet:
1401     """
1402     Provide an iterator which converts the results of raw SQL queries into
1403     annotated model instances.
1404     """
1405     def __init__(self, raw_query, model=None, query=None, params=None,
1406                  translations=None, using=None, hints=None):
1407         self.raw_query = raw_query
1408         self.model = model
1409         self._db = using
1410         self._hints = hints or {}
1411         self.query = query or sql.RawQuery(sql=raw_query, using=self.db, params=params)
1412         self.params = params or ()
1413         self.translations = translations or {}
1414         self._result_cache = None
1415         self._prefetch_related_lookups = ()
1416         self._prefetch_done = False
1417 
1418     def resolve_model_init_order(self):
1419         """Resolve the init field names and value positions."""
1420         converter = connections[self.db].introspection.identifier_converter
1421         model_init_fields = [f for f in self.model._meta.fields if converter(f.column) in self.columns]
1422         annotation_fields = [(column, pos) for pos, column in enumerate(self.columns)
1423                              if column not in self.model_fields]
1424         model_init_order = [self.columns.index(converter(f.column)) for f in model_init_fields]
1425         model_init_names = [f.attname for f in model_init_fields]
1426         return model_init_names, model_init_order, annotation_fields
1427 
1428     def prefetch_related(self, *lookups):
1429         """Same as QuerySet.prefetch_related()"""
1430         clone = self._clone()
1431         if lookups == (None,):
1432             clone._prefetch_related_lookups = ()
1433         else:
1434             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1435         return clone
1436 
1437     def _prefetch_related_objects(self):
1438         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
1439         self._prefetch_done = True
1440 
1441     def _clone(self):
1442         """Same as QuerySet._clone()"""
1443         c = self.__class__(
1444             self.raw_query, model=self.model, query=self.query, params=self.params,
1445             translations=self.translations, using=self._db, hints=self._hints
1446         )
1447         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1448         return c
1449 
1450     def _fetch_all(self):
1451         if self._result_cache is None:
1452             self._result_cache = list(self.iterator())
1453         if self._prefetch_related_lookups and not self._prefetch_done:
1454             self._prefetch_related_objects()
1455 
1456     def __len__(self):
1457         self._fetch_all()
1458         return len(self._result_cache)
1459 
1460     def __bool__(self):
1461         self._fetch_all()
1462         return bool(self._result_cache)
1463 
1464     def __iter__(self):
1465         self._fetch_all()
1466         return iter(self._result_cache)
1467 
1468     def iterator(self):
1469         # Cache some things for performance reasons outside the loop.
1470         db = self.db
1471         compiler = connections[db].ops.compiler('SQLCompiler')(
1472             self.query, connections[db], db
1473         )
1474 
1475         query = iter(self.query)
1476 
1477         try:
1478             model_init_names, model_init_pos, annotation_fields = self.resolve_model_init_order()
1479             if self.model._meta.pk.attname not in model_init_names:
1480                 raise exceptions.FieldDoesNotExist(
1481                     'Raw query must include the primary key'
1482                 )
1483             model_cls = self.model
1484             fields = [self.model_fields.get(c) for c in self.columns]
1485             converters = compiler.get_converters([
1486                 f.get_col(f.model._meta.db_table) if f else None for f in fields
1487             ])
1488             if converters:
1489                 query = compiler.apply_converters(query, converters)
1490             for values in query:
1491                 # Associate fields to values
1492                 model_init_values = [values[pos] for pos in model_init_pos]
1493                 instance = model_cls.from_db(db, model_init_names, model_init_values)
1494                 if annotation_fields:
1495                     for column, pos in annotation_fields:
1496                         setattr(instance, column, values[pos])
1497                 yield instance
1498         finally:
1499             # Done iterating the Query. If it has its own cursor, close it.
1500             if hasattr(self.query, 'cursor') and self.query.cursor:
1501                 self.query.cursor.close()
1502 
1503     def __repr__(self):
1504         return "<%s: %s>" % (self.__class__.__name__, self.query)
1505 
1506     def __getitem__(self, k):
1507         return list(self)[k]
1508 
1509     @property
1510     def db(self):
1511         """Return the database used if this query is executed now."""
1512         return self._db or router.db_for_read(self.model, **self._hints)
1513 
1514     def using(self, alias):
1515         """Select the database this RawQuerySet should execute against."""
1516         return RawQuerySet(
1517             self.raw_query, model=self.model,
1518             query=self.query.chain(using=alias),
1519             params=self.params, translations=self.translations,
1520             using=alias,
1521         )
1522 
1523     @cached_property
1524     def columns(self):
1525         """
1526         A list of model field names in the order they'll appear in the
1527         query results.
1528         """
1529         columns = self.query.get_columns()
1530         # Adjust any column names which don't match field names
1531         for (query_name, model_name) in self.translations.items():
1532             # Ignore translations for nonexistent column names
1533             try:
1534                 index = columns.index(query_name)
1535             except ValueError:
1536                 pass
1537             else:
1538                 columns[index] = model_name
1539         return columns
1540 
1541     @cached_property
1542     def model_fields(self):
1543         """A dict mapping column names to model field names."""
1544         converter = connections[self.db].introspection.identifier_converter
1545         model_fields = {}
1546         for field in self.model._meta.fields:
1547             name, column = field.get_attname_column()
1548             model_fields[converter(column)] = field
1549         return model_fields
1550 
1551 
1552 class Prefetch:
1553     def __init__(self, lookup, queryset=None, to_attr=None):
1554         # `prefetch_through` is the path we traverse to perform the prefetch.
1555         self.prefetch_through = lookup
1556         # `prefetch_to` is the path to the attribute that stores the result.
1557         self.prefetch_to = lookup
1558         if queryset is not None and (
1559             isinstance(queryset, RawQuerySet) or (
1560                 hasattr(queryset, '_iterable_class') and
1561                 not issubclass(queryset._iterable_class, ModelIterable)
1562             )
1563         ):
1564             raise ValueError(
1565                 'Prefetch querysets cannot use raw(), values(), and '
1566                 'values_list().'
1567             )
1568         if to_attr:
1569             self.prefetch_to = LOOKUP_SEP.join(lookup.split(LOOKUP_SEP)[:-1] + [to_attr])
1570 
1571         self.queryset = queryset
1572         self.to_attr = to_attr
1573 
1574     def __getstate__(self):
1575         obj_dict = self.__dict__.copy()
1576         if self.queryset is not None:
1577             # Prevent the QuerySet from being evaluated
1578             obj_dict['queryset'] = self.queryset._chain(
1579                 _result_cache=[],
1580                 _prefetch_done=True,
1581             )
1582         return obj_dict
1583 
1584     def add_prefix(self, prefix):
1585         self.prefetch_through = prefix + LOOKUP_SEP + self.prefetch_through
1586         self.prefetch_to = prefix + LOOKUP_SEP + self.prefetch_to
1587 
1588     def get_current_prefetch_to(self, level):
1589         return LOOKUP_SEP.join(self.prefetch_to.split(LOOKUP_SEP)[:level + 1])
1590 
1591     def get_current_to_attr(self, level):
1592         parts = self.prefetch_to.split(LOOKUP_SEP)
1593         to_attr = parts[level]
1594         as_attr = self.to_attr and level == len(parts) - 1
1595         return to_attr, as_attr
1596 
1597     def get_current_queryset(self, level):
1598         if self.get_current_prefetch_to(level) == self.prefetch_to:
1599             return self.queryset
1600         return None
1601 
1602     def __eq__(self, other):
1603         if not isinstance(other, Prefetch):
1604             return NotImplemented
1605         return self.prefetch_to == other.prefetch_to
1606 
1607     def __hash__(self):
1608         return hash((self.__class__, self.prefetch_to))
1609 
1610 
1611 def normalize_prefetch_lookups(lookups, prefix=None):
1612     """Normalize lookups into Prefetch objects."""
1613     ret = []
1614     for lookup in lookups:
1615         if not isinstance(lookup, Prefetch):
1616             lookup = Prefetch(lookup)
1617         if prefix:
1618             lookup.add_prefix(prefix)
1619         ret.append(lookup)
1620     return ret
1621 
1622 
1623 def prefetch_related_objects(model_instances, *related_lookups):
1624     """
1625     Populate prefetched object caches for a list of model instances based on
1626     the lookups/Prefetch instances given.
1627     """
1628     if not model_instances:
1629         return  # nothing to do
1630 
1631     # We need to be able to dynamically add to the list of prefetch_related
1632     # lookups that we look up (see below).  So we need some book keeping to
1633     # ensure we don't do duplicate work.
1634     done_queries = {}    # dictionary of things like 'foo__bar': [results]
1635 
1636     auto_lookups = set()  # we add to this as we go through.
1637     followed_descriptors = set()  # recursion protection
1638 
1639     all_lookups = normalize_prefetch_lookups(reversed(related_lookups))
1640     while all_lookups:
1641         lookup = all_lookups.pop()
1642         if lookup.prefetch_to in done_queries:
1643             if lookup.queryset is not None:
1644                 raise ValueError("'%s' lookup was already seen with a different queryset. "
1645                                  "You may need to adjust the ordering of your lookups." % lookup.prefetch_to)
1646 
1647             continue
1648 
1649         # Top level, the list of objects to decorate is the result cache
1650         # from the primary QuerySet. It won't be for deeper levels.
1651         obj_list = model_instances
1652 
1653         through_attrs = lookup.prefetch_through.split(LOOKUP_SEP)
1654         for level, through_attr in enumerate(through_attrs):
1655             # Prepare main instances
1656             if not obj_list:
1657                 break
1658 
1659             prefetch_to = lookup.get_current_prefetch_to(level)
1660             if prefetch_to in done_queries:
1661                 # Skip any prefetching, and any object preparation
1662                 obj_list = done_queries[prefetch_to]
1663                 continue
1664 
1665             # Prepare objects:
1666             good_objects = True
1667             for obj in obj_list:
1668                 # Since prefetching can re-use instances, it is possible to have
1669                 # the same instance multiple times in obj_list, so obj might
1670                 # already be prepared.
1671                 if not hasattr(obj, '_prefetched_objects_cache'):
1672                     try:
1673                         obj._prefetched_objects_cache = {}
1674                     except (AttributeError, TypeError):
1675                         # Must be an immutable object from
1676                         # values_list(flat=True), for example (TypeError) or
1677                         # a QuerySet subclass that isn't returning Model
1678                         # instances (AttributeError), either in Django or a 3rd
1679                         # party. prefetch_related() doesn't make sense, so quit.
1680                         good_objects = False
1681                         break
1682             if not good_objects:
1683                 break
1684 
1685             # Descend down tree
1686 
1687             # We assume that objects retrieved are homogeneous (which is the premise
1688             # of prefetch_related), so what applies to first object applies to all.
1689             first_obj = obj_list[0]
1690             to_attr = lookup.get_current_to_attr(level)[0]
1691             prefetcher, descriptor, attr_found, is_fetched = get_prefetcher(first_obj, through_attr, to_attr)
1692 
1693             if not attr_found:
1694                 raise AttributeError("Cannot find '%s' on %s object, '%s' is an invalid "
1695                                      "parameter to prefetch_related()" %
1696                                      (through_attr, first_obj.__class__.__name__, lookup.prefetch_through))
1697 
1698             if level == len(through_attrs) - 1 and prefetcher is None:
1699                 # Last one, this *must* resolve to something that supports
1700                 # prefetching, otherwise there is no point adding it and the
1701                 # developer asking for it has made a mistake.
1702                 raise ValueError("'%s' does not resolve to an item that supports "
1703                                  "prefetching - this is an invalid parameter to "
1704                                  "prefetch_related()." % lookup.prefetch_through)
1705 
1706             if prefetcher is not None and not is_fetched:
1707                 obj_list, additional_lookups = prefetch_one_level(obj_list, prefetcher, lookup, level)
1708                 # We need to ensure we don't keep adding lookups from the
1709                 # same relationships to stop infinite recursion. So, if we
1710                 # are already on an automatically added lookup, don't add
1711                 # the new lookups from relationships we've seen already.
1712                 if not (prefetch_to in done_queries and lookup in auto_lookups and descriptor in followed_descriptors):
1713                     done_queries[prefetch_to] = obj_list
1714                     new_lookups = normalize_prefetch_lookups(reversed(additional_lookups), prefetch_to)
1715                     auto_lookups.update(new_lookups)
1716                     all_lookups.extend(new_lookups)
1717                 followed_descriptors.add(descriptor)
1718             else:
1719                 # Either a singly related object that has already been fetched
1720                 # (e.g. via select_related), or hopefully some other property
1721                 # that doesn't support prefetching but needs to be traversed.
1722 
1723                 # We replace the current list of parent objects with the list
1724                 # of related objects, filtering out empty or missing values so
1725                 # that we can continue with nullable or reverse relations.
1726                 new_obj_list = []
1727                 for obj in obj_list:
1728                     if through_attr in getattr(obj, '_prefetched_objects_cache', ()):
1729                         # If related objects have been prefetched, use the
1730                         # cache rather than the object's through_attr.
1731                         new_obj = list(obj._prefetched_objects_cache.get(through_attr))
1732                     else:
1733                         try:
1734                             new_obj = getattr(obj, through_attr)
1735                         except exceptions.ObjectDoesNotExist:
1736                             continue
1737                     if new_obj is None:
1738                         continue
1739                     # We special-case `list` rather than something more generic
1740                     # like `Iterable` because we don't want to accidentally match
1741                     # user models that define __iter__.
1742                     if isinstance(new_obj, list):
1743                         new_obj_list.extend(new_obj)
1744                     else:
1745                         new_obj_list.append(new_obj)
1746                 obj_list = new_obj_list
1747 
1748 
1749 def get_prefetcher(instance, through_attr, to_attr):
1750     """
1751     For the attribute 'through_attr' on the given instance, find
1752     an object that has a get_prefetch_queryset().
1753     Return a 4 tuple containing:
1754     (the object with get_prefetch_queryset (or None),
1755      the descriptor object representing this relationship (or None),
1756      a boolean that is False if the attribute was not found at all,
1757      a boolean that is True if the attribute has already been fetched)
1758     """
1759     prefetcher = None
1760     is_fetched = False
1761 
1762     # For singly related objects, we have to avoid getting the attribute
1763     # from the object, as this will trigger the query. So we first try
1764     # on the class, in order to get the descriptor object.
1765     rel_obj_descriptor = getattr(instance.__class__, through_attr, None)
1766     if rel_obj_descriptor is None:
1767         attr_found = hasattr(instance, through_attr)
1768     else:
1769         attr_found = True
1770         if rel_obj_descriptor:
1771             # singly related object, descriptor object has the
1772             # get_prefetch_queryset() method.
1773             if hasattr(rel_obj_descriptor, 'get_prefetch_queryset'):
1774                 prefetcher = rel_obj_descriptor
1775                 if rel_obj_descriptor.is_cached(instance):
1776                     is_fetched = True
1777             else:
1778                 # descriptor doesn't support prefetching, so we go ahead and get
1779                 # the attribute on the instance rather than the class to
1780                 # support many related managers
1781                 rel_obj = getattr(instance, through_attr)
1782                 if hasattr(rel_obj, 'get_prefetch_queryset'):
1783                     prefetcher = rel_obj
1784                 if through_attr != to_attr:
1785                     # Special case cached_property instances because hasattr
1786                     # triggers attribute computation and assignment.
1787                     if isinstance(getattr(instance.__class__, to_attr, None), cached_property):
1788                         is_fetched = to_attr in instance.__dict__
1789                     else:
1790                         is_fetched = hasattr(instance, to_attr)
1791                 else:
1792                     is_fetched = through_attr in instance._prefetched_objects_cache
1793     return prefetcher, rel_obj_descriptor, attr_found, is_fetched
1794 
1795 
1796 def prefetch_one_level(instances, prefetcher, lookup, level):
1797     """
1798     Helper function for prefetch_related_objects().
1799 
1800     Run prefetches on all instances using the prefetcher object,
1801     assigning results to relevant caches in instance.
1802 
1803     Return the prefetched objects along with any additional prefetches that
1804     must be done due to prefetch_related lookups found from default managers.
1805     """
1806     # prefetcher must have a method get_prefetch_queryset() which takes a list
1807     # of instances, and returns a tuple:
1808 
1809     # (queryset of instances of self.model that are related to passed in instances,
1810     #  callable that gets value to be matched for returned instances,
1811     #  callable that gets value to be matched for passed in instances,
1812     #  boolean that is True for singly related objects,
1813     #  cache or field name to assign to,
1814     #  boolean that is True when the previous argument is a cache name vs a field name).
1815 
1816     # The 'values to be matched' must be hashable as they will be used
1817     # in a dictionary.
1818 
1819     rel_qs, rel_obj_attr, instance_attr, single, cache_name, is_descriptor = (
1820         prefetcher.get_prefetch_queryset(instances, lookup.get_current_queryset(level)))
1821     # We have to handle the possibility that the QuerySet we just got back
1822     # contains some prefetch_related lookups. We don't want to trigger the
1823     # prefetch_related functionality by evaluating the query. Rather, we need
1824     # to merge in the prefetch_related lookups.
1825     # Copy the lookups in case it is a Prefetch object which could be reused
1826     # later (happens in nested prefetch_related).
1827     additional_lookups = [
1828         copy.copy(additional_lookup) for additional_lookup
1829         in getattr(rel_qs, '_prefetch_related_lookups', ())
1830     ]
1831     if additional_lookups:
1832         # Don't need to clone because the manager should have given us a fresh
1833         # instance, so we access an internal instead of using public interface
1834         # for performance reasons.
1835         rel_qs._prefetch_related_lookups = ()
1836 
1837     all_related_objects = list(rel_qs)
1838 
1839     rel_obj_cache = {}
1840     for rel_obj in all_related_objects:
1841         rel_attr_val = rel_obj_attr(rel_obj)
1842         rel_obj_cache.setdefault(rel_attr_val, []).append(rel_obj)
1843 
1844     to_attr, as_attr = lookup.get_current_to_attr(level)
1845     # Make sure `to_attr` does not conflict with a field.
1846     if as_attr and instances:
1847         # We assume that objects retrieved are homogeneous (which is the premise
1848         # of prefetch_related), so what applies to first object applies to all.
1849         model = instances[0].__class__
1850         try:
1851             model._meta.get_field(to_attr)
1852         except exceptions.FieldDoesNotExist:
1853             pass
1854         else:
1855             msg = 'to_attr={} conflicts with a field on the {} model.'
1856             raise ValueError(msg.format(to_attr, model.__name__))
1857 
1858     # Whether or not we're prefetching the last part of the lookup.
1859     leaf = len(lookup.prefetch_through.split(LOOKUP_SEP)) - 1 == level
1860 
1861     for obj in instances:
1862         instance_attr_val = instance_attr(obj)
1863         vals = rel_obj_cache.get(instance_attr_val, [])
1864 
1865         if single:
1866             val = vals[0] if vals else None
1867             if as_attr:
1868                 # A to_attr has been given for the prefetch.
1869                 setattr(obj, to_attr, val)
1870             elif is_descriptor:
1871                 # cache_name points to a field name in obj.
1872                 # This field is a descriptor for a related object.
1873                 setattr(obj, cache_name, val)
1874             else:
1875                 # No to_attr has been given for this prefetch operation and the
1876                 # cache_name does not point to a descriptor. Store the value of
1877                 # the field in the object's field cache.
1878                 obj._state.fields_cache[cache_name] = val
1879         else:
1880             if as_attr:
1881                 setattr(obj, to_attr, vals)
1882             else:
1883                 manager = getattr(obj, to_attr)
1884                 if leaf and lookup.queryset is not None:
1885                     qs = manager._apply_rel_filters(lookup.queryset)
1886                 else:
1887                     qs = manager.get_queryset()
1888                 qs._result_cache = vals
1889                 # We don't want the individual qs doing prefetch_related now,
1890                 # since we have merged this into the current work.
1891                 qs._prefetch_done = True
1892                 obj._prefetched_objects_cache[cache_name] = qs
1893     return all_related_objects, additional_lookups
1894 
1895 
1896 class RelatedPopulator:
1897     """
1898     RelatedPopulator is used for select_related() object instantiation.
1899 
1900     The idea is that each select_related() model will be populated by a
1901     different RelatedPopulator instance. The RelatedPopulator instances get
1902     klass_info and select (computed in SQLCompiler) plus the used db as
1903     input for initialization. That data is used to compute which columns
1904     to use, how to instantiate the model, and how to populate the links
1905     between the objects.
1906 
1907     The actual creation of the objects is done in populate() method. This
1908     method gets row and from_obj as input and populates the select_related()
1909     model instance.
1910     """
1911     def __init__(self, klass_info, select, db):
1912         self.db = db
1913         # Pre-compute needed attributes. The attributes are:
1914         #  - model_cls: the possibly deferred model class to instantiate
1915         #  - either:
1916         #    - cols_start, cols_end: usually the columns in the row are
1917         #      in the same order model_cls.__init__ expects them, so we
1918         #      can instantiate by model_cls(*row[cols_start:cols_end])
1919         #    - reorder_for_init: When select_related descends to a child
1920         #      class, then we want to reuse the already selected parent
1921         #      data. However, in this case the parent data isn't necessarily
1922         #      in the same order that Model.__init__ expects it to be, so
1923         #      we have to reorder the parent data. The reorder_for_init
1924         #      attribute contains a function used to reorder the field data
1925         #      in the order __init__ expects it.
1926         #  - pk_idx: the index of the primary key field in the reordered
1927         #    model data. Used to check if a related object exists at all.
1928         #  - init_list: the field attnames fetched from the database. For
1929         #    deferred models this isn't the same as all attnames of the
1930         #    model's fields.
1931         #  - related_populators: a list of RelatedPopulator instances if
1932         #    select_related() descends to related models from this model.
1933         #  - local_setter, remote_setter: Methods to set cached values on
1934         #    the object being populated and on the remote object. Usually
1935         #    these are Field.set_cached_value() methods.
1936         select_fields = klass_info['select_fields']
1937         from_parent = klass_info['from_parent']
1938         if not from_parent:
1939             self.cols_start = select_fields[0]
1940             self.cols_end = select_fields[-1] + 1
1941             self.init_list = [
1942                 f[0].target.attname for f in select[self.cols_start:self.cols_end]
1943             ]
1944             self.reorder_for_init = None
1945         else:
1946             attname_indexes = {select[idx][0].target.attname: idx for idx in select_fields}
1947             model_init_attnames = (f.attname for f in klass_info['model']._meta.concrete_fields)
1948             self.init_list = [attname for attname in model_init_attnames if attname in attname_indexes]
1949             self.reorder_for_init = operator.itemgetter(*[attname_indexes[attname] for attname in self.init_list])
1950 
1951         self.model_cls = klass_info['model']
1952         self.pk_idx = self.init_list.index(self.model_cls._meta.pk.attname)
1953         self.related_populators = get_related_populators(klass_info, select, self.db)
1954         self.local_setter = klass_info['local_setter']
1955         self.remote_setter = klass_info['remote_setter']
1956 
1957     def populate(self, row, from_obj):
1958         if self.reorder_for_init:
1959             obj_data = self.reorder_for_init(row)
1960         else:
1961             obj_data = row[self.cols_start:self.cols_end]
1962         if obj_data[self.pk_idx] is None:
1963             obj = None
1964         else:
1965             obj = self.model_cls.from_db(self.db, self.init_list, obj_data)
1966             for rel_iter in self.related_populators:
1967                 rel_iter.populate(row, obj)
1968         self.local_setter(from_obj, obj)
1969         if obj is not None:
1970             self.remote_setter(obj, from_obj)
1971 
1972 
1973 def get_related_populators(klass_info, select, db):
1974     iterators = []
1975     related_klass_infos = klass_info.get('related_klass_infos', [])
1976     for rel_klass_info in related_klass_infos:
1977         rel_cls = RelatedPopulator(rel_klass_info, select, db)
1978         iterators.append(rel_cls)
1979     return iterators

</code><intended_behavior>The method constructing the IN query for prefetch_related should exclude NULL values from the list of IDs it uses to prefetch related objects. This change aims to optimize query performance and integrity by ensuring that NULL values, which won't match any related objects (since FK constraints prevent such matches), are not used in an IN clause. This behavior should be implemented in a way that does not affect other functionalities of prefetch_related, such as handling other valid lookup values or the ability to prefetch multiple related objects efficiently.</intended_behavior>]
2025-10-17 11:59:59.336 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-17 11:59:59.336 | INFO     | Search-only: skipping patch selection
2025-10-17 11:59:59.474 | INFO     | Task django__django-13030 completed successfully.
2025-10-17 11:59:59.475 | INFO     | No patch generated. You can try running ACR again.
