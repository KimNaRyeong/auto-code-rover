2025-10-17 17:38:41.441 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-17 17:38:44.214 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=905, output_tokens=12, cost=0.009410
2025-10-17 17:38:58.610 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1085, output_tokens=538, cost=0.026990
2025-10-17 17:38:59.694 | INFO     | registered non reproducing test 0
2025-10-17 17:39:17.363 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2476, output_tokens=660, cost=0.044560
2025-10-17 17:39:18.421 | INFO     | registered non reproducing test 1
2025-10-17 17:39:33.515 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2598, output_tokens=598, cost=0.043920
2025-10-17 17:39:34.564 | INFO     | registered non reproducing test 2
2025-10-17 17:39:34.564 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-17 17:39:34.570 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 17:39:44.588 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1204, output_tokens=388, cost=0.023680
2025-10-17 17:39:44.596 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 17:39:47.425 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=742, output_tokens=62, cost=0.009280
2025-10-17 17:39:47.426 | DEBUG    | Extracted a valid json.
2025-10-17 17:39:47.426 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"GradientBoostingRegressor\")",
        "search_method_in_class(\"feature_importances_\", \"GradientBoostingRegressor\")",
        "search_method_in_class(\"fit\", \"GradientBoostingRegressor\")"
    ],
    "bug_locations": []
}
2025-10-17 17:39:47.449 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-17 17:39:47.450 | DEBUG    | <Agent search> Analyzing search results.
2025-10-17 17:40:02.772 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2025, output_tokens=653, cost=0.039840
2025-10-17 17:40:02.785 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 17:40:05.942 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2894, output_tokens=27, cost=0.029750
2025-10-17 17:40:05.945 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 17:40:07.864 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=381, output_tokens=46, cost=0.005190
2025-10-17 17:40:07.866 | DEBUG    | Extracted a valid json.
2025-10-17 17:40:07.872 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"BaseGradientBoosting\")",
        "search_code(\"feature_importances_\")",
        "search_method(\"fit\")"
    ],
    "bug_locations": []
}
2025-10-17 17:40:07.992 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-17 17:40:07.992 | DEBUG    | <Agent search> Analyzing search results.
2025-10-17 17:40:27.100 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=5542, output_tokens=690, cost=0.076120
2025-10-17 17:40:27.110 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 17:40:38.554 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=6448, output_tokens=358, cost=0.075220
2025-10-17 17:40:38.563 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 17:40:47.189 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=712, output_tokens=327, cost=0.016930
2025-10-17 17:40:47.190 | DEBUG    | Extracted a valid json.
2025-10-17 17:40:47.190 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "sklearn/ensemble/gradient_boosting.py",
            "class": "BaseGradientBoosting",
            "method": "feature_importances_",
            "intended_behavior": "The method (or property) for `feature_importances_` must ensure that the calculation of feature importances accurately reflects the relative contribution of each feature to the model, normalized such that their sum equals 1 across the entire ensemble of trees. This involves either revising the aggregation logic to mitigate floating-point precision errors or introducing a normalization step that accurately adjusts for the total sum, especially when dealing with a large number of estimators or deep trees. The update should ensure that the method remains computationally efficient even with large ensembles and maintains backward compatibility with existing functionality."
        },
        {
            "file": "sklearn/ensemble/gradient_boosting.py",
            "class": "BaseGradientBoosting",
            "method": "fit",
            "intended_behavior": "While the `fit` method itself may not directly calculate `feature_importances_`, it's crucial to ensure that during the fitting process, all components contributing to the calculation of feature importances are handling large estimator numbers and deep tree structures correctly. This includes ensuring that any data structures or algorithms used during the fit process do not introduce cumulative precision errors that could impact the feature importance calculation. Additionally, consider implementing checks or warnings when the potential for such precision errors becomes significant due to parameter choices, guiding users towards configurations less susceptible to this issue."
        }
    ]
}
2025-10-17 17:40:47.200 | DEBUG    | Bug location extracted successfully: [<file>sklearn/ensemble/gradient_boosting.py</file>
<class>BaseGradientBoosting</class>
<method>feature_importances_</method>
<code>
1705     def feature_importances_(self):
1706         """Return the feature importances (the higher, the more important the
1707            feature).
1708 
1709         Returns
1710         -------
1711         feature_importances_ : array, shape (n_features,)
1712         """
1713         self._check_initialized()
1714 
1715         total_sum = np.zeros((self.n_features_, ), dtype=np.float64)
1716         for stage in self.estimators_:
1717             stage_sum = sum(tree.tree_.compute_feature_importances(
1718                 normalize=False) for tree in stage) / len(stage)
1719             total_sum += stage_sum
1720 
1721         importances = total_sum / total_sum.sum()
1722         return importances

</code><intended_behavior>The method (or property) for `feature_importances_` must ensure that the calculation of feature importances accurately reflects the relative contribution of each feature to the model, normalized such that their sum equals 1 across the entire ensemble of trees. This involves either revising the aggregation logic to mitigate floating-point precision errors or introducing a normalization step that accurately adjusts for the total sum, especially when dealing with a large number of estimators or deep trees. The update should ensure that the method remains computationally efficient even with large ensembles and maintains backward compatibility with existing functionality.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>BaseGradientBoosting</class>
<method>None</method>
<code>
1166 class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
1167     """Abstract base class for Gradient Boosting. """
1168 
1169     @abstractmethod
1170     def __init__(self, loss, learning_rate, n_estimators, criterion,
1171                  min_samples_split, min_samples_leaf, min_weight_fraction_leaf,
1172                  max_depth, min_impurity_decrease, min_impurity_split,
1173                  init, subsample, max_features,
1174                  random_state, alpha=0.9, verbose=0, max_leaf_nodes=None,
1175                  warm_start=False, presort='auto',
1176                  validation_fraction=0.1, n_iter_no_change=None,
1177                  tol=1e-4):
1178 
1179         self.n_estimators = n_estimators
1180         self.learning_rate = learning_rate
1181         self.loss = loss
1182         self.criterion = criterion
1183         self.min_samples_split = min_samples_split
1184         self.min_samples_leaf = min_samples_leaf
1185         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1186         self.subsample = subsample
1187         self.max_features = max_features
1188         self.max_depth = max_depth
1189         self.min_impurity_decrease = min_impurity_decrease
1190         self.min_impurity_split = min_impurity_split
1191         self.init = init
1192         self.random_state = random_state
1193         self.alpha = alpha
1194         self.verbose = verbose
1195         self.max_leaf_nodes = max_leaf_nodes
1196         self.warm_start = warm_start
1197         self.presort = presort
1198         self.validation_fraction = validation_fraction
1199         self.n_iter_no_change = n_iter_no_change
1200         self.tol = tol
1201 
1202     def _fit_stage(self, i, X, y, raw_predictions, sample_weight, sample_mask,
1203                    random_state, X_idx_sorted, X_csc=None, X_csr=None):
1204         """Fit another stage of ``n_classes_`` trees to the boosting model. """
1205 
1206         assert sample_mask.dtype == np.bool
1207         loss = self.loss_
1208         original_y = y
1209 
1210         # Need to pass a copy of raw_predictions to negative_gradient()
1211         # because raw_predictions is partially updated at the end of the loop
1212         # in update_terminal_regions(), and gradients need to be evaluated at
1213         # iteration i - 1.
1214         raw_predictions_copy = raw_predictions.copy()
1215 
1216         for k in range(loss.K):
1217             if loss.is_multi_class:
1218                 y = np.array(original_y == k, dtype=np.float64)
1219 
1220             residual = loss.negative_gradient(y, raw_predictions_copy, k=k,
1221                                               sample_weight=sample_weight)
1222 
1223             # induce regression tree on residuals
1224             tree = DecisionTreeRegressor(
1225                 criterion=self.criterion,
1226                 splitter='best',
1227                 max_depth=self.max_depth,
1228                 min_samples_split=self.min_samples_split,
1229                 min_samples_leaf=self.min_samples_leaf,
1230                 min_weight_fraction_leaf=self.min_weight_fraction_leaf,
1231                 min_impurity_decrease=self.min_impurity_decrease,
1232                 min_impurity_split=self.min_impurity_split,
1233                 max_features=self.max_features,
1234                 max_leaf_nodes=self.max_leaf_nodes,
1235                 random_state=random_state,
1236                 presort=self.presort)
1237 
1238             if self.subsample < 1.0:
1239                 # no inplace multiplication!
1240                 sample_weight = sample_weight * sample_mask.astype(np.float64)
1241 
1242             X = X_csr if X_csr is not None else X
1243             tree.fit(X, residual, sample_weight=sample_weight,
1244                      check_input=False, X_idx_sorted=X_idx_sorted)
1245 
1246             # update tree leaves
1247             loss.update_terminal_regions(
1248                 tree.tree_, X, y, residual, raw_predictions, sample_weight,
1249                 sample_mask, learning_rate=self.learning_rate, k=k)
1250 
1251             # add tree to ensemble
1252             self.estimators_[i, k] = tree
1253 
1254         return raw_predictions
1255 
1256     def _check_params(self):
1257         """Check validity of parameters and raise ValueError if not valid. """
1258         if self.n_estimators <= 0:
1259             raise ValueError("n_estimators must be greater than 0 but "
1260                              "was %r" % self.n_estimators)
1261 
1262         if self.learning_rate <= 0.0:
1263             raise ValueError("learning_rate must be greater than 0 but "
1264                              "was %r" % self.learning_rate)
1265 
1266         if (self.loss not in self._SUPPORTED_LOSS
1267                 or self.loss not in _gb_losses.LOSS_FUNCTIONS):
1268             raise ValueError("Loss '{0:s}' not supported. ".format(self.loss))
1269 
1270         if self.loss == 'deviance':
1271             loss_class = (_gb_losses.MultinomialDeviance
1272                           if len(self.classes_) > 2
1273                           else _gb_losses.BinomialDeviance)
1274         else:
1275             loss_class = _gb_losses.LOSS_FUNCTIONS[self.loss]
1276 
1277         if self.loss in ('huber', 'quantile'):
1278             self.loss_ = loss_class(self.n_classes_, self.alpha)
1279         else:
1280             self.loss_ = loss_class(self.n_classes_)
1281 
1282         if not (0.0 < self.subsample <= 1.0):
1283             raise ValueError("subsample must be in (0,1] but "
1284                              "was %r" % self.subsample)
1285 
1286         if self.init is not None:
1287             # init must be an estimator or 'zero'
1288             if isinstance(self.init, BaseEstimator):
1289                 self.loss_.check_init_estimator(self.init)
1290             elif not (isinstance(self.init, str) and self.init == 'zero'):
1291                 raise ValueError(
1292                     "The init parameter must be an estimator or 'zero'. "
1293                     "Got init={}".format(self.init)
1294                 )
1295 
1296         if not (0.0 < self.alpha < 1.0):
1297             raise ValueError("alpha must be in (0.0, 1.0) but "
1298                              "was %r" % self.alpha)
1299 
1300         if isinstance(self.max_features, str):
1301             if self.max_features == "auto":
1302                 # if is_classification
1303                 if self.n_classes_ > 1:
1304                     max_features = max(1, int(np.sqrt(self.n_features_)))
1305                 else:
1306                     # is regression
1307                     max_features = self.n_features_
1308             elif self.max_features == "sqrt":
1309                 max_features = max(1, int(np.sqrt(self.n_features_)))
1310             elif self.max_features == "log2":
1311                 max_features = max(1, int(np.log2(self.n_features_)))
1312             else:
1313                 raise ValueError("Invalid value for max_features: %r. "
1314                                  "Allowed string values are 'auto', 'sqrt' "
1315                                  "or 'log2'." % self.max_features)
1316         elif self.max_features is None:
1317             max_features = self.n_features_
1318         elif isinstance(self.max_features, (numbers.Integral, np.integer)):
1319             max_features = self.max_features
1320         else:  # float
1321             if 0. < self.max_features <= 1.:
1322                 max_features = max(int(self.max_features *
1323                                        self.n_features_), 1)
1324             else:
1325                 raise ValueError("max_features must be in (0, n_features]")
1326 
1327         self.max_features_ = max_features
1328 
1329         if not isinstance(self.n_iter_no_change,
1330                           (numbers.Integral, np.integer, type(None))):
1331             raise ValueError("n_iter_no_change should either be None or an "
1332                              "integer. %r was passed"
1333                              % self.n_iter_no_change)
1334 
1335         allowed_presort = ('auto', True, False)
1336         if self.presort not in allowed_presort:
1337             raise ValueError("'presort' should be in {}. Got {!r} instead."
1338                              .format(allowed_presort, self.presort))
1339 
1340     def _init_state(self):
1341         """Initialize model state and allocate model state data structures. """
1342 
1343         self.init_ = self.init
1344         if self.init_ is None:
1345             self.init_ = self.loss_.init_estimator()
1346 
1347         self.estimators_ = np.empty((self.n_estimators, self.loss_.K),
1348                                     dtype=np.object)
1349         self.train_score_ = np.zeros((self.n_estimators,), dtype=np.float64)
1350         # do oob?
1351         if self.subsample < 1.0:
1352             self.oob_improvement_ = np.zeros((self.n_estimators),
1353                                              dtype=np.float64)
1354 
1355     def _clear_state(self):
1356         """Clear the state of the gradient boosting model. """
1357         if hasattr(self, 'estimators_'):
1358             self.estimators_ = np.empty((0, 0), dtype=np.object)
1359         if hasattr(self, 'train_score_'):
1360             del self.train_score_
1361         if hasattr(self, 'oob_improvement_'):
1362             del self.oob_improvement_
1363         if hasattr(self, 'init_'):
1364             del self.init_
1365         if hasattr(self, '_rng'):
1366             del self._rng
1367 
1368     def _resize_state(self):
1369         """Add additional ``n_estimators`` entries to all attributes. """
1370         # self.n_estimators is the number of additional est to fit
1371         total_n_estimators = self.n_estimators
1372         if total_n_estimators < self.estimators_.shape[0]:
1373             raise ValueError('resize with smaller n_estimators %d < %d' %
1374                              (total_n_estimators, self.estimators_[0]))
1375 
1376         self.estimators_ = np.resize(self.estimators_,
1377                                      (total_n_estimators, self.loss_.K))
1378         self.train_score_ = np.resize(self.train_score_, total_n_estimators)
1379         if (self.subsample < 1 or hasattr(self, 'oob_improvement_')):
1380             # if do oob resize arrays or create new if not available
1381             if hasattr(self, 'oob_improvement_'):
1382                 self.oob_improvement_ = np.resize(self.oob_improvement_,
1383                                                   total_n_estimators)
1384             else:
1385                 self.oob_improvement_ = np.zeros((total_n_estimators,),
1386                                                  dtype=np.float64)
1387 
1388     def _is_initialized(self):
1389         return len(getattr(self, 'estimators_', [])) > 0
1390 
1391     def _check_initialized(self):
1392         """Check that the estimator is initialized, raising an error if not."""
1393         check_is_fitted(self, 'estimators_')
1394 
1395     def fit(self, X, y, sample_weight=None, monitor=None):
1396         """Fit the gradient boosting model.
1397 
1398         Parameters
1399         ----------
1400         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1401             The input samples. Internally, it will be converted to
1402             ``dtype=np.float32`` and if a sparse matrix is provided
1403             to a sparse ``csr_matrix``.
1404 
1405         y : array-like, shape (n_samples,)
1406             Target values (strings or integers in classification, real numbers
1407             in regression)
1408             For classification, labels must correspond to classes.
1409 
1410         sample_weight : array-like, shape (n_samples,) or None
1411             Sample weights. If None, then samples are equally weighted. Splits
1412             that would create child nodes with net zero or negative weight are
1413             ignored while searching for a split in each node. In the case of
1414             classification, splits are also ignored if they would result in any
1415             single class carrying a negative weight in either child node.
1416 
1417         monitor : callable, optional
1418             The monitor is called after each iteration with the current
1419             iteration, a reference to the estimator and the local variables of
1420             ``_fit_stages`` as keyword arguments ``callable(i, self,
1421             locals())``. If the callable returns ``True`` the fitting procedure
1422             is stopped. The monitor can be used for various things such as
1423             computing held-out estimates, early stopping, model introspect, and
1424             snapshoting.
1425 
1426         Returns
1427         -------
1428         self : object
1429         """
1430         # if not warmstart - clear the estimator state
1431         if not self.warm_start:
1432             self._clear_state()
1433 
1434         # Check input
1435         # Since check_array converts both X and y to the same dtype, but the
1436         # trees use different types for X and y, checking them separately.
1437         X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
1438         n_samples, self.n_features_ = X.shape
1439 
1440         sample_weight_is_none = sample_weight is None
1441         if sample_weight_is_none:
1442             sample_weight = np.ones(n_samples, dtype=np.float32)
1443         else:
1444             sample_weight = column_or_1d(sample_weight, warn=True)
1445             sample_weight_is_none = False
1446 
1447         check_consistent_length(X, y, sample_weight)
1448 
1449         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
1450         y = column_or_1d(y, warn=True)
1451         y = self._validate_y(y, sample_weight)
1452 
1453         if self.n_iter_no_change is not None:
1454             stratify = y if is_classifier(self) else None
1455             X, X_val, y, y_val, sample_weight, sample_weight_val = (
1456                 train_test_split(X, y, sample_weight,
1457                                  random_state=self.random_state,
1458                                  test_size=self.validation_fraction,
1459                                  stratify=stratify))
1460             if is_classifier(self):
1461                 if self.n_classes_ != np.unique(y).shape[0]:
1462                     # We choose to error here. The problem is that the init
1463                     # estimator would be trained on y, which has some missing
1464                     # classes now, so its predictions would not have the
1465                     # correct shape.
1466                     raise ValueError(
1467                         'The training data after the early stopping split '
1468                         'is missing some classes. Try using another random '
1469                         'seed.'
1470                     )
1471         else:
1472             X_val = y_val = sample_weight_val = None
1473 
1474         self._check_params()
1475 
1476         if not self._is_initialized():
1477             # init state
1478             self._init_state()
1479 
1480             # fit initial model and initialize raw predictions
1481             if self.init_ == 'zero':
1482                 raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1483                                            dtype=np.float64)
1484             else:
1485                 # XXX clean this once we have a support_sample_weight tag
1486                 if sample_weight_is_none:
1487                     self.init_.fit(X, y)
1488                 else:
1489                     msg = ("The initial estimator {} does not support sample "
1490                            "weights.".format(self.init_.__class__.__name__))
1491                     try:
1492                         self.init_.fit(X, y, sample_weight=sample_weight)
1493                     except TypeError:  # regular estimator without SW support
1494                         raise ValueError(msg)
1495                     except ValueError as e:
1496                         if "pass parameters to specific steps of "\
1497                            "your pipeline using the "\
1498                            "stepname__parameter" in str(e):  # pipeline
1499                             raise ValueError(msg) from e
1500                         else:  # regular estimator whose input checking failed
1501                             raise
1502 
1503                 raw_predictions = \
1504                     self.loss_.get_init_raw_predictions(X, self.init_)
1505 
1506             begin_at_stage = 0
1507 
1508             # The rng state must be preserved if warm_start is True
1509             self._rng = check_random_state(self.random_state)
1510 
1511         else:
1512             # add more estimators to fitted model
1513             # invariant: warm_start = True
1514             if self.n_estimators < self.estimators_.shape[0]:
1515                 raise ValueError('n_estimators=%d must be larger or equal to '
1516                                  'estimators_.shape[0]=%d when '
1517                                  'warm_start==True'
1518                                  % (self.n_estimators,
1519                                     self.estimators_.shape[0]))
1520             begin_at_stage = self.estimators_.shape[0]
1521             # The requirements of _decision_function (called in two lines
1522             # below) are more constrained than fit. It accepts only CSR
1523             # matrices.
1524             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1525             raw_predictions = self._raw_predict(X)
1526             self._resize_state()
1527 
1528         if self.presort is True and issparse(X):
1529             raise ValueError(
1530                 "Presorting is not supported for sparse matrices.")
1531 
1532         presort = self.presort
1533         # Allow presort to be 'auto', which means True if the dataset is dense,
1534         # otherwise it will be False.
1535         if presort == 'auto':
1536             presort = not issparse(X)
1537 
1538         X_idx_sorted = None
1539         if presort:
1540             X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),
1541                                              dtype=np.int32)
1542 
1543         # fit the boosting stages
1544         n_stages = self._fit_stages(
1545             X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,
1546             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)
1547 
1548         # change shape of arrays after fit (early-stopping or additional ests)
1549         if n_stages != self.estimators_.shape[0]:
1550             self.estimators_ = self.estimators_[:n_stages]
1551             self.train_score_ = self.train_score_[:n_stages]
1552             if hasattr(self, 'oob_improvement_'):
1553                 self.oob_improvement_ = self.oob_improvement_[:n_stages]
1554 
1555         self.n_estimators_ = n_stages
1556         return self
1557 
1558     def _fit_stages(self, X, y, raw_predictions, sample_weight, random_state,
1559                     X_val, y_val, sample_weight_val,
1560                     begin_at_stage=0, monitor=None, X_idx_sorted=None):
1561         """Iteratively fits the stages.
1562 
1563         For each stage it computes the progress (OOB, train score)
1564         and delegates to ``_fit_stage``.
1565         Returns the number of stages fit; might differ from ``n_estimators``
1566         due to early stopping.
1567         """
1568         n_samples = X.shape[0]
1569         do_oob = self.subsample < 1.0
1570         sample_mask = np.ones((n_samples, ), dtype=np.bool)
1571         n_inbag = max(1, int(self.subsample * n_samples))
1572         loss_ = self.loss_
1573 
1574         # Set min_weight_leaf from min_weight_fraction_leaf
1575         if self.min_weight_fraction_leaf != 0. and sample_weight is not None:
1576             min_weight_leaf = (self.min_weight_fraction_leaf *
1577                                np.sum(sample_weight))
1578         else:
1579             min_weight_leaf = 0.
1580 
1581         if self.verbose:
1582             verbose_reporter = VerboseReporter(self.verbose)
1583             verbose_reporter.init(self, begin_at_stage)
1584 
1585         X_csc = csc_matrix(X) if issparse(X) else None
1586         X_csr = csr_matrix(X) if issparse(X) else None
1587 
1588         if self.n_iter_no_change is not None:
1589             loss_history = np.full(self.n_iter_no_change, np.inf)
1590             # We create a generator to get the predictions for X_val after
1591             # the addition of each successive stage
1592             y_val_pred_iter = self._staged_raw_predict(X_val)
1593 
1594         # perform boosting iterations
1595         i = begin_at_stage
1596         for i in range(begin_at_stage, self.n_estimators):
1597 
1598             # subsampling
1599             if do_oob:
1600                 sample_mask = _random_sample_mask(n_samples, n_inbag,
1601                                                   random_state)
1602                 # OOB score before adding this stage
1603                 old_oob_score = loss_(y[~sample_mask],
1604                                       raw_predictions[~sample_mask],
1605                                       sample_weight[~sample_mask])
1606 
1607             # fit next stage of trees
1608             raw_predictions = self._fit_stage(
1609                 i, X, y, raw_predictions, sample_weight, sample_mask,
1610                 random_state, X_idx_sorted, X_csc, X_csr)
1611 
1612             # track deviance (= loss)
1613             if do_oob:
1614                 self.train_score_[i] = loss_(y[sample_mask],
1615                                              raw_predictions[sample_mask],
1616                                              sample_weight[sample_mask])
1617                 self.oob_improvement_[i] = (
1618                     old_oob_score - loss_(y[~sample_mask],
1619                                           raw_predictions[~sample_mask],
1620                                           sample_weight[~sample_mask]))
1621             else:
1622                 # no need to fancy index w/ no subsampling
1623                 self.train_score_[i] = loss_(y, raw_predictions, sample_weight)
1624 
1625             if self.verbose > 0:
1626                 verbose_reporter.update(i, self)
1627 
1628             if monitor is not None:
1629                 early_stopping = monitor(i, self, locals())
1630                 if early_stopping:
1631                     break
1632 
1633             # We also provide an early stopping based on the score from
1634             # validation set (X_val, y_val), if n_iter_no_change is set
1635             if self.n_iter_no_change is not None:
1636                 # By calling next(y_val_pred_iter), we get the predictions
1637                 # for X_val after the addition of the current stage
1638                 validation_loss = loss_(y_val, next(y_val_pred_iter),
1639                                         sample_weight_val)
1640 
1641                 # Require validation_score to be better (less) than at least
1642                 # one of the last n_iter_no_change evaluations
1643                 if np.any(validation_loss + self.tol < loss_history):
1644                     loss_history[i % len(loss_history)] = validation_loss
1645                 else:
1646                     break
1647 
1648         return i + 1
1649 
1650     def _make_estimator(self, append=True):
1651         # we don't need _make_estimator
1652         raise NotImplementedError()
1653 
1654     def _raw_predict_init(self, X):
1655         """Check input and compute raw predictions of the init estimtor."""
1656         self._check_initialized()
1657         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1658         if X.shape[1] != self.n_features_:
1659             raise ValueError("X.shape[1] should be {0:d}, not {1:d}.".format(
1660                 self.n_features_, X.shape[1]))
1661         if self.init_ == 'zero':
1662             raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1663                                        dtype=np.float64)
1664         else:
1665             raw_predictions = self.loss_.get_init_raw_predictions(
1666                 X, self.init_).astype(np.float64)
1667         return raw_predictions
1668 
1669     def _raw_predict(self, X):
1670         """Return the sum of the trees raw predictions (+ init estimator)."""
1671         raw_predictions = self._raw_predict_init(X)
1672         predict_stages(self.estimators_, X, self.learning_rate,
1673                        raw_predictions)
1674         return raw_predictions
1675 
1676     def _staged_raw_predict(self, X):
1677         """Compute raw predictions of ``X`` for each iteration.
1678 
1679         This method allows monitoring (i.e. determine error on testing set)
1680         after each stage.
1681 
1682         Parameters
1683         ----------
1684         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1685             The input samples. Internally, it will be converted to
1686             ``dtype=np.float32`` and if a sparse matrix is provided
1687             to a sparse ``csr_matrix``.
1688 
1689         Returns
1690         -------
1691         raw_predictions : generator of array, shape (n_samples, k)
1692             The raw predictions of the input samples. The order of the
1693             classes corresponds to that in the attribute `classes_`.
1694             Regression and binary classification are special cases with
1695             ``k == 1``, otherwise ``k==n_classes``.
1696         """
1697         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
1698         raw_predictions = self._raw_predict_init(X)
1699         for i in range(self.estimators_.shape[0]):
1700             predict_stage(self.estimators_, i, X, self.learning_rate,
1701                           raw_predictions)
1702             yield raw_predictions.copy()
1703 
1704     @property
1705     def feature_importances_(self):
1706         """Return the feature importances (the higher, the more important the
1707            feature).
1708 
1709         Returns
1710         -------
1711         feature_importances_ : array, shape (n_features,)
1712         """
1713         self._check_initialized()
1714 
1715         total_sum = np.zeros((self.n_features_, ), dtype=np.float64)
1716         for stage in self.estimators_:
1717             stage_sum = sum(tree.tree_.compute_feature_importances(
1718                 normalize=False) for tree in stage) / len(stage)
1719             total_sum += stage_sum
1720 
1721         importances = total_sum / total_sum.sum()
1722         return importances
1723 
1724     def _validate_y(self, y, sample_weight):
1725         # 'sample_weight' is not utilised but is used for
1726         # consistency with similar method _validate_y of GBC
1727         self.n_classes_ = 1
1728         if y.dtype.kind == 'O':
1729             y = y.astype(DOUBLE)
1730         # Default implementation
1731         return y
1732 
1733     def apply(self, X):
1734         """Apply trees in the ensemble to X, return leaf indices.
1735 
1736         .. versionadded:: 0.17
1737 
1738         Parameters
1739         ----------
1740         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1741             The input samples. Internally, its dtype will be converted to
1742             ``dtype=np.float32``. If a sparse matrix is provided, it will
1743             be converted to a sparse ``csr_matrix``.
1744 
1745         Returns
1746         -------
1747         X_leaves : array-like, shape (n_samples, n_estimators, n_classes)
1748             For each datapoint x in X and for each tree in the ensemble,
1749             return the index of the leaf x ends up in each estimator.
1750             In the case of binary classification n_classes is 1.
1751         """
1752 
1753         self._check_initialized()
1754         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1755 
1756         # n_classes will be equal to 1 in the binary classification or the
1757         # regression case.
1758         n_estimators, n_classes = self.estimators_.shape
1759         leaves = np.zeros((X.shape[0], n_estimators, n_classes))
1760 
1761         for i in range(n_estimators):
1762             for j in range(n_classes):
1763                 estimator = self.estimators_[i, j]
1764                 leaves[:, i, j] = estimator.apply(X, check_input=False)
1765 
1766         return leaves

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>BaseGradientBoosting</class>
<method>fit</method>
<code>
1395     def fit(self, X, y, sample_weight=None, monitor=None):
1396         """Fit the gradient boosting model.
1397 
1398         Parameters
1399         ----------
1400         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1401             The input samples. Internally, it will be converted to
1402             ``dtype=np.float32`` and if a sparse matrix is provided
1403             to a sparse ``csr_matrix``.
1404 
1405         y : array-like, shape (n_samples,)
1406             Target values (strings or integers in classification, real numbers
1407             in regression)
1408             For classification, labels must correspond to classes.
1409 
1410         sample_weight : array-like, shape (n_samples,) or None
1411             Sample weights. If None, then samples are equally weighted. Splits
1412             that would create child nodes with net zero or negative weight are
1413             ignored while searching for a split in each node. In the case of
1414             classification, splits are also ignored if they would result in any
1415             single class carrying a negative weight in either child node.
1416 
1417         monitor : callable, optional
1418             The monitor is called after each iteration with the current
1419             iteration, a reference to the estimator and the local variables of
1420             ``_fit_stages`` as keyword arguments ``callable(i, self,
1421             locals())``. If the callable returns ``True`` the fitting procedure
1422             is stopped. The monitor can be used for various things such as
1423             computing held-out estimates, early stopping, model introspect, and
1424             snapshoting.
1425 
1426         Returns
1427         -------
1428         self : object
1429         """
1430         # if not warmstart - clear the estimator state
1431         if not self.warm_start:
1432             self._clear_state()
1433 
1434         # Check input
1435         # Since check_array converts both X and y to the same dtype, but the
1436         # trees use different types for X and y, checking them separately.
1437         X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
1438         n_samples, self.n_features_ = X.shape
1439 
1440         sample_weight_is_none = sample_weight is None
1441         if sample_weight_is_none:
1442             sample_weight = np.ones(n_samples, dtype=np.float32)
1443         else:
1444             sample_weight = column_or_1d(sample_weight, warn=True)
1445             sample_weight_is_none = False
1446 
1447         check_consistent_length(X, y, sample_weight)
1448 
1449         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
1450         y = column_or_1d(y, warn=True)
1451         y = self._validate_y(y, sample_weight)
1452 
1453         if self.n_iter_no_change is not None:
1454             stratify = y if is_classifier(self) else None
1455             X, X_val, y, y_val, sample_weight, sample_weight_val = (
1456                 train_test_split(X, y, sample_weight,
1457                                  random_state=self.random_state,
1458                                  test_size=self.validation_fraction,
1459                                  stratify=stratify))
1460             if is_classifier(self):
1461                 if self.n_classes_ != np.unique(y).shape[0]:
1462                     # We choose to error here. The problem is that the init
1463                     # estimator would be trained on y, which has some missing
1464                     # classes now, so its predictions would not have the
1465                     # correct shape.
1466                     raise ValueError(
1467                         'The training data after the early stopping split '
1468                         'is missing some classes. Try using another random '
1469                         'seed.'
1470                     )
1471         else:
1472             X_val = y_val = sample_weight_val = None
1473 
1474         self._check_params()
1475 
1476         if not self._is_initialized():
1477             # init state
1478             self._init_state()
1479 
1480             # fit initial model and initialize raw predictions
1481             if self.init_ == 'zero':
1482                 raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1483                                            dtype=np.float64)
1484             else:
1485                 # XXX clean this once we have a support_sample_weight tag
1486                 if sample_weight_is_none:
1487                     self.init_.fit(X, y)
1488                 else:
1489                     msg = ("The initial estimator {} does not support sample "
1490                            "weights.".format(self.init_.__class__.__name__))
1491                     try:
1492                         self.init_.fit(X, y, sample_weight=sample_weight)
1493                     except TypeError:  # regular estimator without SW support
1494                         raise ValueError(msg)
1495                     except ValueError as e:
1496                         if "pass parameters to specific steps of "\
1497                            "your pipeline using the "\
1498                            "stepname__parameter" in str(e):  # pipeline
1499                             raise ValueError(msg) from e
1500                         else:  # regular estimator whose input checking failed
1501                             raise
1502 
1503                 raw_predictions = \
1504                     self.loss_.get_init_raw_predictions(X, self.init_)
1505 
1506             begin_at_stage = 0
1507 
1508             # The rng state must be preserved if warm_start is True
1509             self._rng = check_random_state(self.random_state)
1510 
1511         else:
1512             # add more estimators to fitted model
1513             # invariant: warm_start = True
1514             if self.n_estimators < self.estimators_.shape[0]:
1515                 raise ValueError('n_estimators=%d must be larger or equal to '
1516                                  'estimators_.shape[0]=%d when '
1517                                  'warm_start==True'
1518                                  % (self.n_estimators,
1519                                     self.estimators_.shape[0]))
1520             begin_at_stage = self.estimators_.shape[0]
1521             # The requirements of _decision_function (called in two lines
1522             # below) are more constrained than fit. It accepts only CSR
1523             # matrices.
1524             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1525             raw_predictions = self._raw_predict(X)
1526             self._resize_state()
1527 
1528         if self.presort is True and issparse(X):
1529             raise ValueError(
1530                 "Presorting is not supported for sparse matrices.")
1531 
1532         presort = self.presort
1533         # Allow presort to be 'auto', which means True if the dataset is dense,
1534         # otherwise it will be False.
1535         if presort == 'auto':
1536             presort = not issparse(X)
1537 
1538         X_idx_sorted = None
1539         if presort:
1540             X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),
1541                                              dtype=np.int32)
1542 
1543         # fit the boosting stages
1544         n_stages = self._fit_stages(
1545             X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,
1546             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)
1547 
1548         # change shape of arrays after fit (early-stopping or additional ests)
1549         if n_stages != self.estimators_.shape[0]:
1550             self.estimators_ = self.estimators_[:n_stages]
1551             self.train_score_ = self.train_score_[:n_stages]
1552             if hasattr(self, 'oob_improvement_'):
1553                 self.oob_improvement_ = self.oob_improvement_[:n_stages]
1554 
1555         self.n_estimators_ = n_stages
1556         return self

</code><intended_behavior>While the `fit` method itself may not directly calculate `feature_importances_`, it's crucial to ensure that during the fitting process, all components contributing to the calculation of feature importances are handling large estimator numbers and deep tree structures correctly. This includes ensuring that any data structures or algorithms used during the fit process do not introduce cumulative precision errors that could impact the feature importance calculation. Additionally, consider implementing checks or warnings when the potential for such precision errors becomes significant due to parameter choices, guiding users towards configurations less susceptible to this issue.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>BaseGradientBoosting</class>
<method>None</method>
<code>
1166 class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
1167     """Abstract base class for Gradient Boosting. """
1168 
1169     @abstractmethod
1170     def __init__(self, loss, learning_rate, n_estimators, criterion,
1171                  min_samples_split, min_samples_leaf, min_weight_fraction_leaf,
1172                  max_depth, min_impurity_decrease, min_impurity_split,
1173                  init, subsample, max_features,
1174                  random_state, alpha=0.9, verbose=0, max_leaf_nodes=None,
1175                  warm_start=False, presort='auto',
1176                  validation_fraction=0.1, n_iter_no_change=None,
1177                  tol=1e-4):
1178 
1179         self.n_estimators = n_estimators
1180         self.learning_rate = learning_rate
1181         self.loss = loss
1182         self.criterion = criterion
1183         self.min_samples_split = min_samples_split
1184         self.min_samples_leaf = min_samples_leaf
1185         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1186         self.subsample = subsample
1187         self.max_features = max_features
1188         self.max_depth = max_depth
1189         self.min_impurity_decrease = min_impurity_decrease
1190         self.min_impurity_split = min_impurity_split
1191         self.init = init
1192         self.random_state = random_state
1193         self.alpha = alpha
1194         self.verbose = verbose
1195         self.max_leaf_nodes = max_leaf_nodes
1196         self.warm_start = warm_start
1197         self.presort = presort
1198         self.validation_fraction = validation_fraction
1199         self.n_iter_no_change = n_iter_no_change
1200         self.tol = tol
1201 
1202     def _fit_stage(self, i, X, y, raw_predictions, sample_weight, sample_mask,
1203                    random_state, X_idx_sorted, X_csc=None, X_csr=None):
1204         """Fit another stage of ``n_classes_`` trees to the boosting model. """
1205 
1206         assert sample_mask.dtype == np.bool
1207         loss = self.loss_
1208         original_y = y
1209 
1210         # Need to pass a copy of raw_predictions to negative_gradient()
1211         # because raw_predictions is partially updated at the end of the loop
1212         # in update_terminal_regions(), and gradients need to be evaluated at
1213         # iteration i - 1.
1214         raw_predictions_copy = raw_predictions.copy()
1215 
1216         for k in range(loss.K):
1217             if loss.is_multi_class:
1218                 y = np.array(original_y == k, dtype=np.float64)
1219 
1220             residual = loss.negative_gradient(y, raw_predictions_copy, k=k,
1221                                               sample_weight=sample_weight)
1222 
1223             # induce regression tree on residuals
1224             tree = DecisionTreeRegressor(
1225                 criterion=self.criterion,
1226                 splitter='best',
1227                 max_depth=self.max_depth,
1228                 min_samples_split=self.min_samples_split,
1229                 min_samples_leaf=self.min_samples_leaf,
1230                 min_weight_fraction_leaf=self.min_weight_fraction_leaf,
1231                 min_impurity_decrease=self.min_impurity_decrease,
1232                 min_impurity_split=self.min_impurity_split,
1233                 max_features=self.max_features,
1234                 max_leaf_nodes=self.max_leaf_nodes,
1235                 random_state=random_state,
1236                 presort=self.presort)
1237 
1238             if self.subsample < 1.0:
1239                 # no inplace multiplication!
1240                 sample_weight = sample_weight * sample_mask.astype(np.float64)
1241 
1242             X = X_csr if X_csr is not None else X
1243             tree.fit(X, residual, sample_weight=sample_weight,
1244                      check_input=False, X_idx_sorted=X_idx_sorted)
1245 
1246             # update tree leaves
1247             loss.update_terminal_regions(
1248                 tree.tree_, X, y, residual, raw_predictions, sample_weight,
1249                 sample_mask, learning_rate=self.learning_rate, k=k)
1250 
1251             # add tree to ensemble
1252             self.estimators_[i, k] = tree
1253 
1254         return raw_predictions
1255 
1256     def _check_params(self):
1257         """Check validity of parameters and raise ValueError if not valid. """
1258         if self.n_estimators <= 0:
1259             raise ValueError("n_estimators must be greater than 0 but "
1260                              "was %r" % self.n_estimators)
1261 
1262         if self.learning_rate <= 0.0:
1263             raise ValueError("learning_rate must be greater than 0 but "
1264                              "was %r" % self.learning_rate)
1265 
1266         if (self.loss not in self._SUPPORTED_LOSS
1267                 or self.loss not in _gb_losses.LOSS_FUNCTIONS):
1268             raise ValueError("Loss '{0:s}' not supported. ".format(self.loss))
1269 
1270         if self.loss == 'deviance':
1271             loss_class = (_gb_losses.MultinomialDeviance
1272                           if len(self.classes_) > 2
1273                           else _gb_losses.BinomialDeviance)
1274         else:
1275             loss_class = _gb_losses.LOSS_FUNCTIONS[self.loss]
1276 
1277         if self.loss in ('huber', 'quantile'):
1278             self.loss_ = loss_class(self.n_classes_, self.alpha)
1279         else:
1280             self.loss_ = loss_class(self.n_classes_)
1281 
1282         if not (0.0 < self.subsample <= 1.0):
1283             raise ValueError("subsample must be in (0,1] but "
1284                              "was %r" % self.subsample)
1285 
1286         if self.init is not None:
1287             # init must be an estimator or 'zero'
1288             if isinstance(self.init, BaseEstimator):
1289                 self.loss_.check_init_estimator(self.init)
1290             elif not (isinstance(self.init, str) and self.init == 'zero'):
1291                 raise ValueError(
1292                     "The init parameter must be an estimator or 'zero'. "
1293                     "Got init={}".format(self.init)
1294                 )
1295 
1296         if not (0.0 < self.alpha < 1.0):
1297             raise ValueError("alpha must be in (0.0, 1.0) but "
1298                              "was %r" % self.alpha)
1299 
1300         if isinstance(self.max_features, str):
1301             if self.max_features == "auto":
1302                 # if is_classification
1303                 if self.n_classes_ > 1:
1304                     max_features = max(1, int(np.sqrt(self.n_features_)))
1305                 else:
1306                     # is regression
1307                     max_features = self.n_features_
1308             elif self.max_features == "sqrt":
1309                 max_features = max(1, int(np.sqrt(self.n_features_)))
1310             elif self.max_features == "log2":
1311                 max_features = max(1, int(np.log2(self.n_features_)))
1312             else:
1313                 raise ValueError("Invalid value for max_features: %r. "
1314                                  "Allowed string values are 'auto', 'sqrt' "
1315                                  "or 'log2'." % self.max_features)
1316         elif self.max_features is None:
1317             max_features = self.n_features_
1318         elif isinstance(self.max_features, (numbers.Integral, np.integer)):
1319             max_features = self.max_features
1320         else:  # float
1321             if 0. < self.max_features <= 1.:
1322                 max_features = max(int(self.max_features *
1323                                        self.n_features_), 1)
1324             else:
1325                 raise ValueError("max_features must be in (0, n_features]")
1326 
1327         self.max_features_ = max_features
1328 
1329         if not isinstance(self.n_iter_no_change,
1330                           (numbers.Integral, np.integer, type(None))):
1331             raise ValueError("n_iter_no_change should either be None or an "
1332                              "integer. %r was passed"
1333                              % self.n_iter_no_change)
1334 
1335         allowed_presort = ('auto', True, False)
1336         if self.presort not in allowed_presort:
1337             raise ValueError("'presort' should be in {}. Got {!r} instead."
1338                              .format(allowed_presort, self.presort))
1339 
1340     def _init_state(self):
1341         """Initialize model state and allocate model state data structures. """
1342 
1343         self.init_ = self.init
1344         if self.init_ is None:
1345             self.init_ = self.loss_.init_estimator()
1346 
1347         self.estimators_ = np.empty((self.n_estimators, self.loss_.K),
1348                                     dtype=np.object)
1349         self.train_score_ = np.zeros((self.n_estimators,), dtype=np.float64)
1350         # do oob?
1351         if self.subsample < 1.0:
1352             self.oob_improvement_ = np.zeros((self.n_estimators),
1353                                              dtype=np.float64)
1354 
1355     def _clear_state(self):
1356         """Clear the state of the gradient boosting model. """
1357         if hasattr(self, 'estimators_'):
1358             self.estimators_ = np.empty((0, 0), dtype=np.object)
1359         if hasattr(self, 'train_score_'):
1360             del self.train_score_
1361         if hasattr(self, 'oob_improvement_'):
1362             del self.oob_improvement_
1363         if hasattr(self, 'init_'):
1364             del self.init_
1365         if hasattr(self, '_rng'):
1366             del self._rng
1367 
1368     def _resize_state(self):
1369         """Add additional ``n_estimators`` entries to all attributes. """
1370         # self.n_estimators is the number of additional est to fit
1371         total_n_estimators = self.n_estimators
1372         if total_n_estimators < self.estimators_.shape[0]:
1373             raise ValueError('resize with smaller n_estimators %d < %d' %
1374                              (total_n_estimators, self.estimators_[0]))
1375 
1376         self.estimators_ = np.resize(self.estimators_,
1377                                      (total_n_estimators, self.loss_.K))
1378         self.train_score_ = np.resize(self.train_score_, total_n_estimators)
1379         if (self.subsample < 1 or hasattr(self, 'oob_improvement_')):
1380             # if do oob resize arrays or create new if not available
1381             if hasattr(self, 'oob_improvement_'):
1382                 self.oob_improvement_ = np.resize(self.oob_improvement_,
1383                                                   total_n_estimators)
1384             else:
1385                 self.oob_improvement_ = np.zeros((total_n_estimators,),
1386                                                  dtype=np.float64)
1387 
1388     def _is_initialized(self):
1389         return len(getattr(self, 'estimators_', [])) > 0
1390 
1391     def _check_initialized(self):
1392         """Check that the estimator is initialized, raising an error if not."""
1393         check_is_fitted(self, 'estimators_')
1394 
1395     def fit(self, X, y, sample_weight=None, monitor=None):
1396         """Fit the gradient boosting model.
1397 
1398         Parameters
1399         ----------
1400         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1401             The input samples. Internally, it will be converted to
1402             ``dtype=np.float32`` and if a sparse matrix is provided
1403             to a sparse ``csr_matrix``.
1404 
1405         y : array-like, shape (n_samples,)
1406             Target values (strings or integers in classification, real numbers
1407             in regression)
1408             For classification, labels must correspond to classes.
1409 
1410         sample_weight : array-like, shape (n_samples,) or None
1411             Sample weights. If None, then samples are equally weighted. Splits
1412             that would create child nodes with net zero or negative weight are
1413             ignored while searching for a split in each node. In the case of
1414             classification, splits are also ignored if they would result in any
1415             single class carrying a negative weight in either child node.
1416 
1417         monitor : callable, optional
1418             The monitor is called after each iteration with the current
1419             iteration, a reference to the estimator and the local variables of
1420             ``_fit_stages`` as keyword arguments ``callable(i, self,
1421             locals())``. If the callable returns ``True`` the fitting procedure
1422             is stopped. The monitor can be used for various things such as
1423             computing held-out estimates, early stopping, model introspect, and
1424             snapshoting.
1425 
1426         Returns
1427         -------
1428         self : object
1429         """
1430         # if not warmstart - clear the estimator state
1431         if not self.warm_start:
1432             self._clear_state()
1433 
1434         # Check input
1435         # Since check_array converts both X and y to the same dtype, but the
1436         # trees use different types for X and y, checking them separately.
1437         X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
1438         n_samples, self.n_features_ = X.shape
1439 
1440         sample_weight_is_none = sample_weight is None
1441         if sample_weight_is_none:
1442             sample_weight = np.ones(n_samples, dtype=np.float32)
1443         else:
1444             sample_weight = column_or_1d(sample_weight, warn=True)
1445             sample_weight_is_none = False
1446 
1447         check_consistent_length(X, y, sample_weight)
1448 
1449         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
1450         y = column_or_1d(y, warn=True)
1451         y = self._validate_y(y, sample_weight)
1452 
1453         if self.n_iter_no_change is not None:
1454             stratify = y if is_classifier(self) else None
1455             X, X_val, y, y_val, sample_weight, sample_weight_val = (
1456                 train_test_split(X, y, sample_weight,
1457                                  random_state=self.random_state,
1458                                  test_size=self.validation_fraction,
1459                                  stratify=stratify))
1460             if is_classifier(self):
1461                 if self.n_classes_ != np.unique(y).shape[0]:
1462                     # We choose to error here. The problem is that the init
1463                     # estimator would be trained on y, which has some missing
1464                     # classes now, so its predictions would not have the
1465                     # correct shape.
1466                     raise ValueError(
1467                         'The training data after the early stopping split '
1468                         'is missing some classes. Try using another random '
1469                         'seed.'
1470                     )
1471         else:
1472             X_val = y_val = sample_weight_val = None
1473 
1474         self._check_params()
1475 
1476         if not self._is_initialized():
1477             # init state
1478             self._init_state()
1479 
1480             # fit initial model and initialize raw predictions
1481             if self.init_ == 'zero':
1482                 raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1483                                            dtype=np.float64)
1484             else:
1485                 # XXX clean this once we have a support_sample_weight tag
1486                 if sample_weight_is_none:
1487                     self.init_.fit(X, y)
1488                 else:
1489                     msg = ("The initial estimator {} does not support sample "
1490                            "weights.".format(self.init_.__class__.__name__))
1491                     try:
1492                         self.init_.fit(X, y, sample_weight=sample_weight)
1493                     except TypeError:  # regular estimator without SW support
1494                         raise ValueError(msg)
1495                     except ValueError as e:
1496                         if "pass parameters to specific steps of "\
1497                            "your pipeline using the "\
1498                            "stepname__parameter" in str(e):  # pipeline
1499                             raise ValueError(msg) from e
1500                         else:  # regular estimator whose input checking failed
1501                             raise
1502 
1503                 raw_predictions = \
1504                     self.loss_.get_init_raw_predictions(X, self.init_)
1505 
1506             begin_at_stage = 0
1507 
1508             # The rng state must be preserved if warm_start is True
1509             self._rng = check_random_state(self.random_state)
1510 
1511         else:
1512             # add more estimators to fitted model
1513             # invariant: warm_start = True
1514             if self.n_estimators < self.estimators_.shape[0]:
1515                 raise ValueError('n_estimators=%d must be larger or equal to '
1516                                  'estimators_.shape[0]=%d when '
1517                                  'warm_start==True'
1518                                  % (self.n_estimators,
1519                                     self.estimators_.shape[0]))
1520             begin_at_stage = self.estimators_.shape[0]
1521             # The requirements of _decision_function (called in two lines
1522             # below) are more constrained than fit. It accepts only CSR
1523             # matrices.
1524             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1525             raw_predictions = self._raw_predict(X)
1526             self._resize_state()
1527 
1528         if self.presort is True and issparse(X):
1529             raise ValueError(
1530                 "Presorting is not supported for sparse matrices.")
1531 
1532         presort = self.presort
1533         # Allow presort to be 'auto', which means True if the dataset is dense,
1534         # otherwise it will be False.
1535         if presort == 'auto':
1536             presort = not issparse(X)
1537 
1538         X_idx_sorted = None
1539         if presort:
1540             X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),
1541                                              dtype=np.int32)
1542 
1543         # fit the boosting stages
1544         n_stages = self._fit_stages(
1545             X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,
1546             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)
1547 
1548         # change shape of arrays after fit (early-stopping or additional ests)
1549         if n_stages != self.estimators_.shape[0]:
1550             self.estimators_ = self.estimators_[:n_stages]
1551             self.train_score_ = self.train_score_[:n_stages]
1552             if hasattr(self, 'oob_improvement_'):
1553                 self.oob_improvement_ = self.oob_improvement_[:n_stages]
1554 
1555         self.n_estimators_ = n_stages
1556         return self
1557 
1558     def _fit_stages(self, X, y, raw_predictions, sample_weight, random_state,
1559                     X_val, y_val, sample_weight_val,
1560                     begin_at_stage=0, monitor=None, X_idx_sorted=None):
1561         """Iteratively fits the stages.
1562 
1563         For each stage it computes the progress (OOB, train score)
1564         and delegates to ``_fit_stage``.
1565         Returns the number of stages fit; might differ from ``n_estimators``
1566         due to early stopping.
1567         """
1568         n_samples = X.shape[0]
1569         do_oob = self.subsample < 1.0
1570         sample_mask = np.ones((n_samples, ), dtype=np.bool)
1571         n_inbag = max(1, int(self.subsample * n_samples))
1572         loss_ = self.loss_
1573 
1574         # Set min_weight_leaf from min_weight_fraction_leaf
1575         if self.min_weight_fraction_leaf != 0. and sample_weight is not None:
1576             min_weight_leaf = (self.min_weight_fraction_leaf *
1577                                np.sum(sample_weight))
1578         else:
1579             min_weight_leaf = 0.
1580 
1581         if self.verbose:
1582             verbose_reporter = VerboseReporter(self.verbose)
1583             verbose_reporter.init(self, begin_at_stage)
1584 
1585         X_csc = csc_matrix(X) if issparse(X) else None
1586         X_csr = csr_matrix(X) if issparse(X) else None
1587 
1588         if self.n_iter_no_change is not None:
1589             loss_history = np.full(self.n_iter_no_change, np.inf)
1590             # We create a generator to get the predictions for X_val after
1591             # the addition of each successive stage
1592             y_val_pred_iter = self._staged_raw_predict(X_val)
1593 
1594         # perform boosting iterations
1595         i = begin_at_stage
1596         for i in range(begin_at_stage, self.n_estimators):
1597 
1598             # subsampling
1599             if do_oob:
1600                 sample_mask = _random_sample_mask(n_samples, n_inbag,
1601                                                   random_state)
1602                 # OOB score before adding this stage
1603                 old_oob_score = loss_(y[~sample_mask],
1604                                       raw_predictions[~sample_mask],
1605                                       sample_weight[~sample_mask])
1606 
1607             # fit next stage of trees
1608             raw_predictions = self._fit_stage(
1609                 i, X, y, raw_predictions, sample_weight, sample_mask,
1610                 random_state, X_idx_sorted, X_csc, X_csr)
1611 
1612             # track deviance (= loss)
1613             if do_oob:
1614                 self.train_score_[i] = loss_(y[sample_mask],
1615                                              raw_predictions[sample_mask],
1616                                              sample_weight[sample_mask])
1617                 self.oob_improvement_[i] = (
1618                     old_oob_score - loss_(y[~sample_mask],
1619                                           raw_predictions[~sample_mask],
1620                                           sample_weight[~sample_mask]))
1621             else:
1622                 # no need to fancy index w/ no subsampling
1623                 self.train_score_[i] = loss_(y, raw_predictions, sample_weight)
1624 
1625             if self.verbose > 0:
1626                 verbose_reporter.update(i, self)
1627 
1628             if monitor is not None:
1629                 early_stopping = monitor(i, self, locals())
1630                 if early_stopping:
1631                     break
1632 
1633             # We also provide an early stopping based on the score from
1634             # validation set (X_val, y_val), if n_iter_no_change is set
1635             if self.n_iter_no_change is not None:
1636                 # By calling next(y_val_pred_iter), we get the predictions
1637                 # for X_val after the addition of the current stage
1638                 validation_loss = loss_(y_val, next(y_val_pred_iter),
1639                                         sample_weight_val)
1640 
1641                 # Require validation_score to be better (less) than at least
1642                 # one of the last n_iter_no_change evaluations
1643                 if np.any(validation_loss + self.tol < loss_history):
1644                     loss_history[i % len(loss_history)] = validation_loss
1645                 else:
1646                     break
1647 
1648         return i + 1
1649 
1650     def _make_estimator(self, append=True):
1651         # we don't need _make_estimator
1652         raise NotImplementedError()
1653 
1654     def _raw_predict_init(self, X):
1655         """Check input and compute raw predictions of the init estimtor."""
1656         self._check_initialized()
1657         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1658         if X.shape[1] != self.n_features_:
1659             raise ValueError("X.shape[1] should be {0:d}, not {1:d}.".format(
1660                 self.n_features_, X.shape[1]))
1661         if self.init_ == 'zero':
1662             raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1663                                        dtype=np.float64)
1664         else:
1665             raw_predictions = self.loss_.get_init_raw_predictions(
1666                 X, self.init_).astype(np.float64)
1667         return raw_predictions
1668 
1669     def _raw_predict(self, X):
1670         """Return the sum of the trees raw predictions (+ init estimator)."""
1671         raw_predictions = self._raw_predict_init(X)
1672         predict_stages(self.estimators_, X, self.learning_rate,
1673                        raw_predictions)
1674         return raw_predictions
1675 
1676     def _staged_raw_predict(self, X):
1677         """Compute raw predictions of ``X`` for each iteration.
1678 
1679         This method allows monitoring (i.e. determine error on testing set)
1680         after each stage.
1681 
1682         Parameters
1683         ----------
1684         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1685             The input samples. Internally, it will be converted to
1686             ``dtype=np.float32`` and if a sparse matrix is provided
1687             to a sparse ``csr_matrix``.
1688 
1689         Returns
1690         -------
1691         raw_predictions : generator of array, shape (n_samples, k)
1692             The raw predictions of the input samples. The order of the
1693             classes corresponds to that in the attribute `classes_`.
1694             Regression and binary classification are special cases with
1695             ``k == 1``, otherwise ``k==n_classes``.
1696         """
1697         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
1698         raw_predictions = self._raw_predict_init(X)
1699         for i in range(self.estimators_.shape[0]):
1700             predict_stage(self.estimators_, i, X, self.learning_rate,
1701                           raw_predictions)
1702             yield raw_predictions.copy()
1703 
1704     @property
1705     def feature_importances_(self):
1706         """Return the feature importances (the higher, the more important the
1707            feature).
1708 
1709         Returns
1710         -------
1711         feature_importances_ : array, shape (n_features,)
1712         """
1713         self._check_initialized()
1714 
1715         total_sum = np.zeros((self.n_features_, ), dtype=np.float64)
1716         for stage in self.estimators_:
1717             stage_sum = sum(tree.tree_.compute_feature_importances(
1718                 normalize=False) for tree in stage) / len(stage)
1719             total_sum += stage_sum
1720 
1721         importances = total_sum / total_sum.sum()
1722         return importances
1723 
1724     def _validate_y(self, y, sample_weight):
1725         # 'sample_weight' is not utilised but is used for
1726         # consistency with similar method _validate_y of GBC
1727         self.n_classes_ = 1
1728         if y.dtype.kind == 'O':
1729             y = y.astype(DOUBLE)
1730         # Default implementation
1731         return y
1732 
1733     def apply(self, X):
1734         """Apply trees in the ensemble to X, return leaf indices.
1735 
1736         .. versionadded:: 0.17
1737 
1738         Parameters
1739         ----------
1740         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1741             The input samples. Internally, its dtype will be converted to
1742             ``dtype=np.float32``. If a sparse matrix is provided, it will
1743             be converted to a sparse ``csr_matrix``.
1744 
1745         Returns
1746         -------
1747         X_leaves : array-like, shape (n_samples, n_estimators, n_classes)
1748             For each datapoint x in X and for each tree in the ensemble,
1749             return the index of the leaf x ends up in each estimator.
1750             In the case of binary classification n_classes is 1.
1751         """
1752 
1753         self._check_initialized()
1754         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1755 
1756         # n_classes will be equal to 1 in the binary classification or the
1757         # regression case.
1758         n_estimators, n_classes = self.estimators_.shape
1759         leaves = np.zeros((X.shape[0], n_estimators, n_classes))
1760 
1761         for i in range(n_estimators):
1762             for j in range(n_classes):
1763                 estimator = self.estimators_[i, j]
1764                 leaves[:, i, j] = estimator.apply(X, check_input=False)
1765 
1766         return leaves

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-17 17:40:47.200 | INFO     | Search completed. Bug locations: [<file>sklearn/ensemble/gradient_boosting.py</file>
<class>BaseGradientBoosting</class>
<method>feature_importances_</method>
<code>
1705     def feature_importances_(self):
1706         """Return the feature importances (the higher, the more important the
1707            feature).
1708 
1709         Returns
1710         -------
1711         feature_importances_ : array, shape (n_features,)
1712         """
1713         self._check_initialized()
1714 
1715         total_sum = np.zeros((self.n_features_, ), dtype=np.float64)
1716         for stage in self.estimators_:
1717             stage_sum = sum(tree.tree_.compute_feature_importances(
1718                 normalize=False) for tree in stage) / len(stage)
1719             total_sum += stage_sum
1720 
1721         importances = total_sum / total_sum.sum()
1722         return importances

</code><intended_behavior>The method (or property) for `feature_importances_` must ensure that the calculation of feature importances accurately reflects the relative contribution of each feature to the model, normalized such that their sum equals 1 across the entire ensemble of trees. This involves either revising the aggregation logic to mitigate floating-point precision errors or introducing a normalization step that accurately adjusts for the total sum, especially when dealing with a large number of estimators or deep trees. The update should ensure that the method remains computationally efficient even with large ensembles and maintains backward compatibility with existing functionality.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>BaseGradientBoosting</class>
<method>None</method>
<code>
1166 class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
1167     """Abstract base class for Gradient Boosting. """
1168 
1169     @abstractmethod
1170     def __init__(self, loss, learning_rate, n_estimators, criterion,
1171                  min_samples_split, min_samples_leaf, min_weight_fraction_leaf,
1172                  max_depth, min_impurity_decrease, min_impurity_split,
1173                  init, subsample, max_features,
1174                  random_state, alpha=0.9, verbose=0, max_leaf_nodes=None,
1175                  warm_start=False, presort='auto',
1176                  validation_fraction=0.1, n_iter_no_change=None,
1177                  tol=1e-4):
1178 
1179         self.n_estimators = n_estimators
1180         self.learning_rate = learning_rate
1181         self.loss = loss
1182         self.criterion = criterion
1183         self.min_samples_split = min_samples_split
1184         self.min_samples_leaf = min_samples_leaf
1185         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1186         self.subsample = subsample
1187         self.max_features = max_features
1188         self.max_depth = max_depth
1189         self.min_impurity_decrease = min_impurity_decrease
1190         self.min_impurity_split = min_impurity_split
1191         self.init = init
1192         self.random_state = random_state
1193         self.alpha = alpha
1194         self.verbose = verbose
1195         self.max_leaf_nodes = max_leaf_nodes
1196         self.warm_start = warm_start
1197         self.presort = presort
1198         self.validation_fraction = validation_fraction
1199         self.n_iter_no_change = n_iter_no_change
1200         self.tol = tol
1201 
1202     def _fit_stage(self, i, X, y, raw_predictions, sample_weight, sample_mask,
1203                    random_state, X_idx_sorted, X_csc=None, X_csr=None):
1204         """Fit another stage of ``n_classes_`` trees to the boosting model. """
1205 
1206         assert sample_mask.dtype == np.bool
1207         loss = self.loss_
1208         original_y = y
1209 
1210         # Need to pass a copy of raw_predictions to negative_gradient()
1211         # because raw_predictions is partially updated at the end of the loop
1212         # in update_terminal_regions(), and gradients need to be evaluated at
1213         # iteration i - 1.
1214         raw_predictions_copy = raw_predictions.copy()
1215 
1216         for k in range(loss.K):
1217             if loss.is_multi_class:
1218                 y = np.array(original_y == k, dtype=np.float64)
1219 
1220             residual = loss.negative_gradient(y, raw_predictions_copy, k=k,
1221                                               sample_weight=sample_weight)
1222 
1223             # induce regression tree on residuals
1224             tree = DecisionTreeRegressor(
1225                 criterion=self.criterion,
1226                 splitter='best',
1227                 max_depth=self.max_depth,
1228                 min_samples_split=self.min_samples_split,
1229                 min_samples_leaf=self.min_samples_leaf,
1230                 min_weight_fraction_leaf=self.min_weight_fraction_leaf,
1231                 min_impurity_decrease=self.min_impurity_decrease,
1232                 min_impurity_split=self.min_impurity_split,
1233                 max_features=self.max_features,
1234                 max_leaf_nodes=self.max_leaf_nodes,
1235                 random_state=random_state,
1236                 presort=self.presort)
1237 
1238             if self.subsample < 1.0:
1239                 # no inplace multiplication!
1240                 sample_weight = sample_weight * sample_mask.astype(np.float64)
1241 
1242             X = X_csr if X_csr is not None else X
1243             tree.fit(X, residual, sample_weight=sample_weight,
1244                      check_input=False, X_idx_sorted=X_idx_sorted)
1245 
1246             # update tree leaves
1247             loss.update_terminal_regions(
1248                 tree.tree_, X, y, residual, raw_predictions, sample_weight,
1249                 sample_mask, learning_rate=self.learning_rate, k=k)
1250 
1251             # add tree to ensemble
1252             self.estimators_[i, k] = tree
1253 
1254         return raw_predictions
1255 
1256     def _check_params(self):
1257         """Check validity of parameters and raise ValueError if not valid. """
1258         if self.n_estimators <= 0:
1259             raise ValueError("n_estimators must be greater than 0 but "
1260                              "was %r" % self.n_estimators)
1261 
1262         if self.learning_rate <= 0.0:
1263             raise ValueError("learning_rate must be greater than 0 but "
1264                              "was %r" % self.learning_rate)
1265 
1266         if (self.loss not in self._SUPPORTED_LOSS
1267                 or self.loss not in _gb_losses.LOSS_FUNCTIONS):
1268             raise ValueError("Loss '{0:s}' not supported. ".format(self.loss))
1269 
1270         if self.loss == 'deviance':
1271             loss_class = (_gb_losses.MultinomialDeviance
1272                           if len(self.classes_) > 2
1273                           else _gb_losses.BinomialDeviance)
1274         else:
1275             loss_class = _gb_losses.LOSS_FUNCTIONS[self.loss]
1276 
1277         if self.loss in ('huber', 'quantile'):
1278             self.loss_ = loss_class(self.n_classes_, self.alpha)
1279         else:
1280             self.loss_ = loss_class(self.n_classes_)
1281 
1282         if not (0.0 < self.subsample <= 1.0):
1283             raise ValueError("subsample must be in (0,1] but "
1284                              "was %r" % self.subsample)
1285 
1286         if self.init is not None:
1287             # init must be an estimator or 'zero'
1288             if isinstance(self.init, BaseEstimator):
1289                 self.loss_.check_init_estimator(self.init)
1290             elif not (isinstance(self.init, str) and self.init == 'zero'):
1291                 raise ValueError(
1292                     "The init parameter must be an estimator or 'zero'. "
1293                     "Got init={}".format(self.init)
1294                 )
1295 
1296         if not (0.0 < self.alpha < 1.0):
1297             raise ValueError("alpha must be in (0.0, 1.0) but "
1298                              "was %r" % self.alpha)
1299 
1300         if isinstance(self.max_features, str):
1301             if self.max_features == "auto":
1302                 # if is_classification
1303                 if self.n_classes_ > 1:
1304                     max_features = max(1, int(np.sqrt(self.n_features_)))
1305                 else:
1306                     # is regression
1307                     max_features = self.n_features_
1308             elif self.max_features == "sqrt":
1309                 max_features = max(1, int(np.sqrt(self.n_features_)))
1310             elif self.max_features == "log2":
1311                 max_features = max(1, int(np.log2(self.n_features_)))
1312             else:
1313                 raise ValueError("Invalid value for max_features: %r. "
1314                                  "Allowed string values are 'auto', 'sqrt' "
1315                                  "or 'log2'." % self.max_features)
1316         elif self.max_features is None:
1317             max_features = self.n_features_
1318         elif isinstance(self.max_features, (numbers.Integral, np.integer)):
1319             max_features = self.max_features
1320         else:  # float
1321             if 0. < self.max_features <= 1.:
1322                 max_features = max(int(self.max_features *
1323                                        self.n_features_), 1)
1324             else:
1325                 raise ValueError("max_features must be in (0, n_features]")
1326 
1327         self.max_features_ = max_features
1328 
1329         if not isinstance(self.n_iter_no_change,
1330                           (numbers.Integral, np.integer, type(None))):
1331             raise ValueError("n_iter_no_change should either be None or an "
1332                              "integer. %r was passed"
1333                              % self.n_iter_no_change)
1334 
1335         allowed_presort = ('auto', True, False)
1336         if self.presort not in allowed_presort:
1337             raise ValueError("'presort' should be in {}. Got {!r} instead."
1338                              .format(allowed_presort, self.presort))
1339 
1340     def _init_state(self):
1341         """Initialize model state and allocate model state data structures. """
1342 
1343         self.init_ = self.init
1344         if self.init_ is None:
1345             self.init_ = self.loss_.init_estimator()
1346 
1347         self.estimators_ = np.empty((self.n_estimators, self.loss_.K),
1348                                     dtype=np.object)
1349         self.train_score_ = np.zeros((self.n_estimators,), dtype=np.float64)
1350         # do oob?
1351         if self.subsample < 1.0:
1352             self.oob_improvement_ = np.zeros((self.n_estimators),
1353                                              dtype=np.float64)
1354 
1355     def _clear_state(self):
1356         """Clear the state of the gradient boosting model. """
1357         if hasattr(self, 'estimators_'):
1358             self.estimators_ = np.empty((0, 0), dtype=np.object)
1359         if hasattr(self, 'train_score_'):
1360             del self.train_score_
1361         if hasattr(self, 'oob_improvement_'):
1362             del self.oob_improvement_
1363         if hasattr(self, 'init_'):
1364             del self.init_
1365         if hasattr(self, '_rng'):
1366             del self._rng
1367 
1368     def _resize_state(self):
1369         """Add additional ``n_estimators`` entries to all attributes. """
1370         # self.n_estimators is the number of additional est to fit
1371         total_n_estimators = self.n_estimators
1372         if total_n_estimators < self.estimators_.shape[0]:
1373             raise ValueError('resize with smaller n_estimators %d < %d' %
1374                              (total_n_estimators, self.estimators_[0]))
1375 
1376         self.estimators_ = np.resize(self.estimators_,
1377                                      (total_n_estimators, self.loss_.K))
1378         self.train_score_ = np.resize(self.train_score_, total_n_estimators)
1379         if (self.subsample < 1 or hasattr(self, 'oob_improvement_')):
1380             # if do oob resize arrays or create new if not available
1381             if hasattr(self, 'oob_improvement_'):
1382                 self.oob_improvement_ = np.resize(self.oob_improvement_,
1383                                                   total_n_estimators)
1384             else:
1385                 self.oob_improvement_ = np.zeros((total_n_estimators,),
1386                                                  dtype=np.float64)
1387 
1388     def _is_initialized(self):
1389         return len(getattr(self, 'estimators_', [])) > 0
1390 
1391     def _check_initialized(self):
1392         """Check that the estimator is initialized, raising an error if not."""
1393         check_is_fitted(self, 'estimators_')
1394 
1395     def fit(self, X, y, sample_weight=None, monitor=None):
1396         """Fit the gradient boosting model.
1397 
1398         Parameters
1399         ----------
1400         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1401             The input samples. Internally, it will be converted to
1402             ``dtype=np.float32`` and if a sparse matrix is provided
1403             to a sparse ``csr_matrix``.
1404 
1405         y : array-like, shape (n_samples,)
1406             Target values (strings or integers in classification, real numbers
1407             in regression)
1408             For classification, labels must correspond to classes.
1409 
1410         sample_weight : array-like, shape (n_samples,) or None
1411             Sample weights. If None, then samples are equally weighted. Splits
1412             that would create child nodes with net zero or negative weight are
1413             ignored while searching for a split in each node. In the case of
1414             classification, splits are also ignored if they would result in any
1415             single class carrying a negative weight in either child node.
1416 
1417         monitor : callable, optional
1418             The monitor is called after each iteration with the current
1419             iteration, a reference to the estimator and the local variables of
1420             ``_fit_stages`` as keyword arguments ``callable(i, self,
1421             locals())``. If the callable returns ``True`` the fitting procedure
1422             is stopped. The monitor can be used for various things such as
1423             computing held-out estimates, early stopping, model introspect, and
1424             snapshoting.
1425 
1426         Returns
1427         -------
1428         self : object
1429         """
1430         # if not warmstart - clear the estimator state
1431         if not self.warm_start:
1432             self._clear_state()
1433 
1434         # Check input
1435         # Since check_array converts both X and y to the same dtype, but the
1436         # trees use different types for X and y, checking them separately.
1437         X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
1438         n_samples, self.n_features_ = X.shape
1439 
1440         sample_weight_is_none = sample_weight is None
1441         if sample_weight_is_none:
1442             sample_weight = np.ones(n_samples, dtype=np.float32)
1443         else:
1444             sample_weight = column_or_1d(sample_weight, warn=True)
1445             sample_weight_is_none = False
1446 
1447         check_consistent_length(X, y, sample_weight)
1448 
1449         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
1450         y = column_or_1d(y, warn=True)
1451         y = self._validate_y(y, sample_weight)
1452 
1453         if self.n_iter_no_change is not None:
1454             stratify = y if is_classifier(self) else None
1455             X, X_val, y, y_val, sample_weight, sample_weight_val = (
1456                 train_test_split(X, y, sample_weight,
1457                                  random_state=self.random_state,
1458                                  test_size=self.validation_fraction,
1459                                  stratify=stratify))
1460             if is_classifier(self):
1461                 if self.n_classes_ != np.unique(y).shape[0]:
1462                     # We choose to error here. The problem is that the init
1463                     # estimator would be trained on y, which has some missing
1464                     # classes now, so its predictions would not have the
1465                     # correct shape.
1466                     raise ValueError(
1467                         'The training data after the early stopping split '
1468                         'is missing some classes. Try using another random '
1469                         'seed.'
1470                     )
1471         else:
1472             X_val = y_val = sample_weight_val = None
1473 
1474         self._check_params()
1475 
1476         if not self._is_initialized():
1477             # init state
1478             self._init_state()
1479 
1480             # fit initial model and initialize raw predictions
1481             if self.init_ == 'zero':
1482                 raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1483                                            dtype=np.float64)
1484             else:
1485                 # XXX clean this once we have a support_sample_weight tag
1486                 if sample_weight_is_none:
1487                     self.init_.fit(X, y)
1488                 else:
1489                     msg = ("The initial estimator {} does not support sample "
1490                            "weights.".format(self.init_.__class__.__name__))
1491                     try:
1492                         self.init_.fit(X, y, sample_weight=sample_weight)
1493                     except TypeError:  # regular estimator without SW support
1494                         raise ValueError(msg)
1495                     except ValueError as e:
1496                         if "pass parameters to specific steps of "\
1497                            "your pipeline using the "\
1498                            "stepname__parameter" in str(e):  # pipeline
1499                             raise ValueError(msg) from e
1500                         else:  # regular estimator whose input checking failed
1501                             raise
1502 
1503                 raw_predictions = \
1504                     self.loss_.get_init_raw_predictions(X, self.init_)
1505 
1506             begin_at_stage = 0
1507 
1508             # The rng state must be preserved if warm_start is True
1509             self._rng = check_random_state(self.random_state)
1510 
1511         else:
1512             # add more estimators to fitted model
1513             # invariant: warm_start = True
1514             if self.n_estimators < self.estimators_.shape[0]:
1515                 raise ValueError('n_estimators=%d must be larger or equal to '
1516                                  'estimators_.shape[0]=%d when '
1517                                  'warm_start==True'
1518                                  % (self.n_estimators,
1519                                     self.estimators_.shape[0]))
1520             begin_at_stage = self.estimators_.shape[0]
1521             # The requirements of _decision_function (called in two lines
1522             # below) are more constrained than fit. It accepts only CSR
1523             # matrices.
1524             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1525             raw_predictions = self._raw_predict(X)
1526             self._resize_state()
1527 
1528         if self.presort is True and issparse(X):
1529             raise ValueError(
1530                 "Presorting is not supported for sparse matrices.")
1531 
1532         presort = self.presort
1533         # Allow presort to be 'auto', which means True if the dataset is dense,
1534         # otherwise it will be False.
1535         if presort == 'auto':
1536             presort = not issparse(X)
1537 
1538         X_idx_sorted = None
1539         if presort:
1540             X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),
1541                                              dtype=np.int32)
1542 
1543         # fit the boosting stages
1544         n_stages = self._fit_stages(
1545             X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,
1546             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)
1547 
1548         # change shape of arrays after fit (early-stopping or additional ests)
1549         if n_stages != self.estimators_.shape[0]:
1550             self.estimators_ = self.estimators_[:n_stages]
1551             self.train_score_ = self.train_score_[:n_stages]
1552             if hasattr(self, 'oob_improvement_'):
1553                 self.oob_improvement_ = self.oob_improvement_[:n_stages]
1554 
1555         self.n_estimators_ = n_stages
1556         return self
1557 
1558     def _fit_stages(self, X, y, raw_predictions, sample_weight, random_state,
1559                     X_val, y_val, sample_weight_val,
1560                     begin_at_stage=0, monitor=None, X_idx_sorted=None):
1561         """Iteratively fits the stages.
1562 
1563         For each stage it computes the progress (OOB, train score)
1564         and delegates to ``_fit_stage``.
1565         Returns the number of stages fit; might differ from ``n_estimators``
1566         due to early stopping.
1567         """
1568         n_samples = X.shape[0]
1569         do_oob = self.subsample < 1.0
1570         sample_mask = np.ones((n_samples, ), dtype=np.bool)
1571         n_inbag = max(1, int(self.subsample * n_samples))
1572         loss_ = self.loss_
1573 
1574         # Set min_weight_leaf from min_weight_fraction_leaf
1575         if self.min_weight_fraction_leaf != 0. and sample_weight is not None:
1576             min_weight_leaf = (self.min_weight_fraction_leaf *
1577                                np.sum(sample_weight))
1578         else:
1579             min_weight_leaf = 0.
1580 
1581         if self.verbose:
1582             verbose_reporter = VerboseReporter(self.verbose)
1583             verbose_reporter.init(self, begin_at_stage)
1584 
1585         X_csc = csc_matrix(X) if issparse(X) else None
1586         X_csr = csr_matrix(X) if issparse(X) else None
1587 
1588         if self.n_iter_no_change is not None:
1589             loss_history = np.full(self.n_iter_no_change, np.inf)
1590             # We create a generator to get the predictions for X_val after
1591             # the addition of each successive stage
1592             y_val_pred_iter = self._staged_raw_predict(X_val)
1593 
1594         # perform boosting iterations
1595         i = begin_at_stage
1596         for i in range(begin_at_stage, self.n_estimators):
1597 
1598             # subsampling
1599             if do_oob:
1600                 sample_mask = _random_sample_mask(n_samples, n_inbag,
1601                                                   random_state)
1602                 # OOB score before adding this stage
1603                 old_oob_score = loss_(y[~sample_mask],
1604                                       raw_predictions[~sample_mask],
1605                                       sample_weight[~sample_mask])
1606 
1607             # fit next stage of trees
1608             raw_predictions = self._fit_stage(
1609                 i, X, y, raw_predictions, sample_weight, sample_mask,
1610                 random_state, X_idx_sorted, X_csc, X_csr)
1611 
1612             # track deviance (= loss)
1613             if do_oob:
1614                 self.train_score_[i] = loss_(y[sample_mask],
1615                                              raw_predictions[sample_mask],
1616                                              sample_weight[sample_mask])
1617                 self.oob_improvement_[i] = (
1618                     old_oob_score - loss_(y[~sample_mask],
1619                                           raw_predictions[~sample_mask],
1620                                           sample_weight[~sample_mask]))
1621             else:
1622                 # no need to fancy index w/ no subsampling
1623                 self.train_score_[i] = loss_(y, raw_predictions, sample_weight)
1624 
1625             if self.verbose > 0:
1626                 verbose_reporter.update(i, self)
1627 
1628             if monitor is not None:
1629                 early_stopping = monitor(i, self, locals())
1630                 if early_stopping:
1631                     break
1632 
1633             # We also provide an early stopping based on the score from
1634             # validation set (X_val, y_val), if n_iter_no_change is set
1635             if self.n_iter_no_change is not None:
1636                 # By calling next(y_val_pred_iter), we get the predictions
1637                 # for X_val after the addition of the current stage
1638                 validation_loss = loss_(y_val, next(y_val_pred_iter),
1639                                         sample_weight_val)
1640 
1641                 # Require validation_score to be better (less) than at least
1642                 # one of the last n_iter_no_change evaluations
1643                 if np.any(validation_loss + self.tol < loss_history):
1644                     loss_history[i % len(loss_history)] = validation_loss
1645                 else:
1646                     break
1647 
1648         return i + 1
1649 
1650     def _make_estimator(self, append=True):
1651         # we don't need _make_estimator
1652         raise NotImplementedError()
1653 
1654     def _raw_predict_init(self, X):
1655         """Check input and compute raw predictions of the init estimtor."""
1656         self._check_initialized()
1657         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1658         if X.shape[1] != self.n_features_:
1659             raise ValueError("X.shape[1] should be {0:d}, not {1:d}.".format(
1660                 self.n_features_, X.shape[1]))
1661         if self.init_ == 'zero':
1662             raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1663                                        dtype=np.float64)
1664         else:
1665             raw_predictions = self.loss_.get_init_raw_predictions(
1666                 X, self.init_).astype(np.float64)
1667         return raw_predictions
1668 
1669     def _raw_predict(self, X):
1670         """Return the sum of the trees raw predictions (+ init estimator)."""
1671         raw_predictions = self._raw_predict_init(X)
1672         predict_stages(self.estimators_, X, self.learning_rate,
1673                        raw_predictions)
1674         return raw_predictions
1675 
1676     def _staged_raw_predict(self, X):
1677         """Compute raw predictions of ``X`` for each iteration.
1678 
1679         This method allows monitoring (i.e. determine error on testing set)
1680         after each stage.
1681 
1682         Parameters
1683         ----------
1684         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1685             The input samples. Internally, it will be converted to
1686             ``dtype=np.float32`` and if a sparse matrix is provided
1687             to a sparse ``csr_matrix``.
1688 
1689         Returns
1690         -------
1691         raw_predictions : generator of array, shape (n_samples, k)
1692             The raw predictions of the input samples. The order of the
1693             classes corresponds to that in the attribute `classes_`.
1694             Regression and binary classification are special cases with
1695             ``k == 1``, otherwise ``k==n_classes``.
1696         """
1697         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
1698         raw_predictions = self._raw_predict_init(X)
1699         for i in range(self.estimators_.shape[0]):
1700             predict_stage(self.estimators_, i, X, self.learning_rate,
1701                           raw_predictions)
1702             yield raw_predictions.copy()
1703 
1704     @property
1705     def feature_importances_(self):
1706         """Return the feature importances (the higher, the more important the
1707            feature).
1708 
1709         Returns
1710         -------
1711         feature_importances_ : array, shape (n_features,)
1712         """
1713         self._check_initialized()
1714 
1715         total_sum = np.zeros((self.n_features_, ), dtype=np.float64)
1716         for stage in self.estimators_:
1717             stage_sum = sum(tree.tree_.compute_feature_importances(
1718                 normalize=False) for tree in stage) / len(stage)
1719             total_sum += stage_sum
1720 
1721         importances = total_sum / total_sum.sum()
1722         return importances
1723 
1724     def _validate_y(self, y, sample_weight):
1725         # 'sample_weight' is not utilised but is used for
1726         # consistency with similar method _validate_y of GBC
1727         self.n_classes_ = 1
1728         if y.dtype.kind == 'O':
1729             y = y.astype(DOUBLE)
1730         # Default implementation
1731         return y
1732 
1733     def apply(self, X):
1734         """Apply trees in the ensemble to X, return leaf indices.
1735 
1736         .. versionadded:: 0.17
1737 
1738         Parameters
1739         ----------
1740         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1741             The input samples. Internally, its dtype will be converted to
1742             ``dtype=np.float32``. If a sparse matrix is provided, it will
1743             be converted to a sparse ``csr_matrix``.
1744 
1745         Returns
1746         -------
1747         X_leaves : array-like, shape (n_samples, n_estimators, n_classes)
1748             For each datapoint x in X and for each tree in the ensemble,
1749             return the index of the leaf x ends up in each estimator.
1750             In the case of binary classification n_classes is 1.
1751         """
1752 
1753         self._check_initialized()
1754         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1755 
1756         # n_classes will be equal to 1 in the binary classification or the
1757         # regression case.
1758         n_estimators, n_classes = self.estimators_.shape
1759         leaves = np.zeros((X.shape[0], n_estimators, n_classes))
1760 
1761         for i in range(n_estimators):
1762             for j in range(n_classes):
1763                 estimator = self.estimators_[i, j]
1764                 leaves[:, i, j] = estimator.apply(X, check_input=False)
1765 
1766         return leaves

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>BaseGradientBoosting</class>
<method>fit</method>
<code>
1395     def fit(self, X, y, sample_weight=None, monitor=None):
1396         """Fit the gradient boosting model.
1397 
1398         Parameters
1399         ----------
1400         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1401             The input samples. Internally, it will be converted to
1402             ``dtype=np.float32`` and if a sparse matrix is provided
1403             to a sparse ``csr_matrix``.
1404 
1405         y : array-like, shape (n_samples,)
1406             Target values (strings or integers in classification, real numbers
1407             in regression)
1408             For classification, labels must correspond to classes.
1409 
1410         sample_weight : array-like, shape (n_samples,) or None
1411             Sample weights. If None, then samples are equally weighted. Splits
1412             that would create child nodes with net zero or negative weight are
1413             ignored while searching for a split in each node. In the case of
1414             classification, splits are also ignored if they would result in any
1415             single class carrying a negative weight in either child node.
1416 
1417         monitor : callable, optional
1418             The monitor is called after each iteration with the current
1419             iteration, a reference to the estimator and the local variables of
1420             ``_fit_stages`` as keyword arguments ``callable(i, self,
1421             locals())``. If the callable returns ``True`` the fitting procedure
1422             is stopped. The monitor can be used for various things such as
1423             computing held-out estimates, early stopping, model introspect, and
1424             snapshoting.
1425 
1426         Returns
1427         -------
1428         self : object
1429         """
1430         # if not warmstart - clear the estimator state
1431         if not self.warm_start:
1432             self._clear_state()
1433 
1434         # Check input
1435         # Since check_array converts both X and y to the same dtype, but the
1436         # trees use different types for X and y, checking them separately.
1437         X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
1438         n_samples, self.n_features_ = X.shape
1439 
1440         sample_weight_is_none = sample_weight is None
1441         if sample_weight_is_none:
1442             sample_weight = np.ones(n_samples, dtype=np.float32)
1443         else:
1444             sample_weight = column_or_1d(sample_weight, warn=True)
1445             sample_weight_is_none = False
1446 
1447         check_consistent_length(X, y, sample_weight)
1448 
1449         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
1450         y = column_or_1d(y, warn=True)
1451         y = self._validate_y(y, sample_weight)
1452 
1453         if self.n_iter_no_change is not None:
1454             stratify = y if is_classifier(self) else None
1455             X, X_val, y, y_val, sample_weight, sample_weight_val = (
1456                 train_test_split(X, y, sample_weight,
1457                                  random_state=self.random_state,
1458                                  test_size=self.validation_fraction,
1459                                  stratify=stratify))
1460             if is_classifier(self):
1461                 if self.n_classes_ != np.unique(y).shape[0]:
1462                     # We choose to error here. The problem is that the init
1463                     # estimator would be trained on y, which has some missing
1464                     # classes now, so its predictions would not have the
1465                     # correct shape.
1466                     raise ValueError(
1467                         'The training data after the early stopping split '
1468                         'is missing some classes. Try using another random '
1469                         'seed.'
1470                     )
1471         else:
1472             X_val = y_val = sample_weight_val = None
1473 
1474         self._check_params()
1475 
1476         if not self._is_initialized():
1477             # init state
1478             self._init_state()
1479 
1480             # fit initial model and initialize raw predictions
1481             if self.init_ == 'zero':
1482                 raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1483                                            dtype=np.float64)
1484             else:
1485                 # XXX clean this once we have a support_sample_weight tag
1486                 if sample_weight_is_none:
1487                     self.init_.fit(X, y)
1488                 else:
1489                     msg = ("The initial estimator {} does not support sample "
1490                            "weights.".format(self.init_.__class__.__name__))
1491                     try:
1492                         self.init_.fit(X, y, sample_weight=sample_weight)
1493                     except TypeError:  # regular estimator without SW support
1494                         raise ValueError(msg)
1495                     except ValueError as e:
1496                         if "pass parameters to specific steps of "\
1497                            "your pipeline using the "\
1498                            "stepname__parameter" in str(e):  # pipeline
1499                             raise ValueError(msg) from e
1500                         else:  # regular estimator whose input checking failed
1501                             raise
1502 
1503                 raw_predictions = \
1504                     self.loss_.get_init_raw_predictions(X, self.init_)
1505 
1506             begin_at_stage = 0
1507 
1508             # The rng state must be preserved if warm_start is True
1509             self._rng = check_random_state(self.random_state)
1510 
1511         else:
1512             # add more estimators to fitted model
1513             # invariant: warm_start = True
1514             if self.n_estimators < self.estimators_.shape[0]:
1515                 raise ValueError('n_estimators=%d must be larger or equal to '
1516                                  'estimators_.shape[0]=%d when '
1517                                  'warm_start==True'
1518                                  % (self.n_estimators,
1519                                     self.estimators_.shape[0]))
1520             begin_at_stage = self.estimators_.shape[0]
1521             # The requirements of _decision_function (called in two lines
1522             # below) are more constrained than fit. It accepts only CSR
1523             # matrices.
1524             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1525             raw_predictions = self._raw_predict(X)
1526             self._resize_state()
1527 
1528         if self.presort is True and issparse(X):
1529             raise ValueError(
1530                 "Presorting is not supported for sparse matrices.")
1531 
1532         presort = self.presort
1533         # Allow presort to be 'auto', which means True if the dataset is dense,
1534         # otherwise it will be False.
1535         if presort == 'auto':
1536             presort = not issparse(X)
1537 
1538         X_idx_sorted = None
1539         if presort:
1540             X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),
1541                                              dtype=np.int32)
1542 
1543         # fit the boosting stages
1544         n_stages = self._fit_stages(
1545             X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,
1546             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)
1547 
1548         # change shape of arrays after fit (early-stopping or additional ests)
1549         if n_stages != self.estimators_.shape[0]:
1550             self.estimators_ = self.estimators_[:n_stages]
1551             self.train_score_ = self.train_score_[:n_stages]
1552             if hasattr(self, 'oob_improvement_'):
1553                 self.oob_improvement_ = self.oob_improvement_[:n_stages]
1554 
1555         self.n_estimators_ = n_stages
1556         return self

</code><intended_behavior>While the `fit` method itself may not directly calculate `feature_importances_`, it's crucial to ensure that during the fitting process, all components contributing to the calculation of feature importances are handling large estimator numbers and deep tree structures correctly. This includes ensuring that any data structures or algorithms used during the fit process do not introduce cumulative precision errors that could impact the feature importance calculation. Additionally, consider implementing checks or warnings when the potential for such precision errors becomes significant due to parameter choices, guiding users towards configurations less susceptible to this issue.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>BaseGradientBoosting</class>
<method>None</method>
<code>
1166 class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
1167     """Abstract base class for Gradient Boosting. """
1168 
1169     @abstractmethod
1170     def __init__(self, loss, learning_rate, n_estimators, criterion,
1171                  min_samples_split, min_samples_leaf, min_weight_fraction_leaf,
1172                  max_depth, min_impurity_decrease, min_impurity_split,
1173                  init, subsample, max_features,
1174                  random_state, alpha=0.9, verbose=0, max_leaf_nodes=None,
1175                  warm_start=False, presort='auto',
1176                  validation_fraction=0.1, n_iter_no_change=None,
1177                  tol=1e-4):
1178 
1179         self.n_estimators = n_estimators
1180         self.learning_rate = learning_rate
1181         self.loss = loss
1182         self.criterion = criterion
1183         self.min_samples_split = min_samples_split
1184         self.min_samples_leaf = min_samples_leaf
1185         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1186         self.subsample = subsample
1187         self.max_features = max_features
1188         self.max_depth = max_depth
1189         self.min_impurity_decrease = min_impurity_decrease
1190         self.min_impurity_split = min_impurity_split
1191         self.init = init
1192         self.random_state = random_state
1193         self.alpha = alpha
1194         self.verbose = verbose
1195         self.max_leaf_nodes = max_leaf_nodes
1196         self.warm_start = warm_start
1197         self.presort = presort
1198         self.validation_fraction = validation_fraction
1199         self.n_iter_no_change = n_iter_no_change
1200         self.tol = tol
1201 
1202     def _fit_stage(self, i, X, y, raw_predictions, sample_weight, sample_mask,
1203                    random_state, X_idx_sorted, X_csc=None, X_csr=None):
1204         """Fit another stage of ``n_classes_`` trees to the boosting model. """
1205 
1206         assert sample_mask.dtype == np.bool
1207         loss = self.loss_
1208         original_y = y
1209 
1210         # Need to pass a copy of raw_predictions to negative_gradient()
1211         # because raw_predictions is partially updated at the end of the loop
1212         # in update_terminal_regions(), and gradients need to be evaluated at
1213         # iteration i - 1.
1214         raw_predictions_copy = raw_predictions.copy()
1215 
1216         for k in range(loss.K):
1217             if loss.is_multi_class:
1218                 y = np.array(original_y == k, dtype=np.float64)
1219 
1220             residual = loss.negative_gradient(y, raw_predictions_copy, k=k,
1221                                               sample_weight=sample_weight)
1222 
1223             # induce regression tree on residuals
1224             tree = DecisionTreeRegressor(
1225                 criterion=self.criterion,
1226                 splitter='best',
1227                 max_depth=self.max_depth,
1228                 min_samples_split=self.min_samples_split,
1229                 min_samples_leaf=self.min_samples_leaf,
1230                 min_weight_fraction_leaf=self.min_weight_fraction_leaf,
1231                 min_impurity_decrease=self.min_impurity_decrease,
1232                 min_impurity_split=self.min_impurity_split,
1233                 max_features=self.max_features,
1234                 max_leaf_nodes=self.max_leaf_nodes,
1235                 random_state=random_state,
1236                 presort=self.presort)
1237 
1238             if self.subsample < 1.0:
1239                 # no inplace multiplication!
1240                 sample_weight = sample_weight * sample_mask.astype(np.float64)
1241 
1242             X = X_csr if X_csr is not None else X
1243             tree.fit(X, residual, sample_weight=sample_weight,
1244                      check_input=False, X_idx_sorted=X_idx_sorted)
1245 
1246             # update tree leaves
1247             loss.update_terminal_regions(
1248                 tree.tree_, X, y, residual, raw_predictions, sample_weight,
1249                 sample_mask, learning_rate=self.learning_rate, k=k)
1250 
1251             # add tree to ensemble
1252             self.estimators_[i, k] = tree
1253 
1254         return raw_predictions
1255 
1256     def _check_params(self):
1257         """Check validity of parameters and raise ValueError if not valid. """
1258         if self.n_estimators <= 0:
1259             raise ValueError("n_estimators must be greater than 0 but "
1260                              "was %r" % self.n_estimators)
1261 
1262         if self.learning_rate <= 0.0:
1263             raise ValueError("learning_rate must be greater than 0 but "
1264                              "was %r" % self.learning_rate)
1265 
1266         if (self.loss not in self._SUPPORTED_LOSS
1267                 or self.loss not in _gb_losses.LOSS_FUNCTIONS):
1268             raise ValueError("Loss '{0:s}' not supported. ".format(self.loss))
1269 
1270         if self.loss == 'deviance':
1271             loss_class = (_gb_losses.MultinomialDeviance
1272                           if len(self.classes_) > 2
1273                           else _gb_losses.BinomialDeviance)
1274         else:
1275             loss_class = _gb_losses.LOSS_FUNCTIONS[self.loss]
1276 
1277         if self.loss in ('huber', 'quantile'):
1278             self.loss_ = loss_class(self.n_classes_, self.alpha)
1279         else:
1280             self.loss_ = loss_class(self.n_classes_)
1281 
1282         if not (0.0 < self.subsample <= 1.0):
1283             raise ValueError("subsample must be in (0,1] but "
1284                              "was %r" % self.subsample)
1285 
1286         if self.init is not None:
1287             # init must be an estimator or 'zero'
1288             if isinstance(self.init, BaseEstimator):
1289                 self.loss_.check_init_estimator(self.init)
1290             elif not (isinstance(self.init, str) and self.init == 'zero'):
1291                 raise ValueError(
1292                     "The init parameter must be an estimator or 'zero'. "
1293                     "Got init={}".format(self.init)
1294                 )
1295 
1296         if not (0.0 < self.alpha < 1.0):
1297             raise ValueError("alpha must be in (0.0, 1.0) but "
1298                              "was %r" % self.alpha)
1299 
1300         if isinstance(self.max_features, str):
1301             if self.max_features == "auto":
1302                 # if is_classification
1303                 if self.n_classes_ > 1:
1304                     max_features = max(1, int(np.sqrt(self.n_features_)))
1305                 else:
1306                     # is regression
1307                     max_features = self.n_features_
1308             elif self.max_features == "sqrt":
1309                 max_features = max(1, int(np.sqrt(self.n_features_)))
1310             elif self.max_features == "log2":
1311                 max_features = max(1, int(np.log2(self.n_features_)))
1312             else:
1313                 raise ValueError("Invalid value for max_features: %r. "
1314                                  "Allowed string values are 'auto', 'sqrt' "
1315                                  "or 'log2'." % self.max_features)
1316         elif self.max_features is None:
1317             max_features = self.n_features_
1318         elif isinstance(self.max_features, (numbers.Integral, np.integer)):
1319             max_features = self.max_features
1320         else:  # float
1321             if 0. < self.max_features <= 1.:
1322                 max_features = max(int(self.max_features *
1323                                        self.n_features_), 1)
1324             else:
1325                 raise ValueError("max_features must be in (0, n_features]")
1326 
1327         self.max_features_ = max_features
1328 
1329         if not isinstance(self.n_iter_no_change,
1330                           (numbers.Integral, np.integer, type(None))):
1331             raise ValueError("n_iter_no_change should either be None or an "
1332                              "integer. %r was passed"
1333                              % self.n_iter_no_change)
1334 
1335         allowed_presort = ('auto', True, False)
1336         if self.presort not in allowed_presort:
1337             raise ValueError("'presort' should be in {}. Got {!r} instead."
1338                              .format(allowed_presort, self.presort))
1339 
1340     def _init_state(self):
1341         """Initialize model state and allocate model state data structures. """
1342 
1343         self.init_ = self.init
1344         if self.init_ is None:
1345             self.init_ = self.loss_.init_estimator()
1346 
1347         self.estimators_ = np.empty((self.n_estimators, self.loss_.K),
1348                                     dtype=np.object)
1349         self.train_score_ = np.zeros((self.n_estimators,), dtype=np.float64)
1350         # do oob?
1351         if self.subsample < 1.0:
1352             self.oob_improvement_ = np.zeros((self.n_estimators),
1353                                              dtype=np.float64)
1354 
1355     def _clear_state(self):
1356         """Clear the state of the gradient boosting model. """
1357         if hasattr(self, 'estimators_'):
1358             self.estimators_ = np.empty((0, 0), dtype=np.object)
1359         if hasattr(self, 'train_score_'):
1360             del self.train_score_
1361         if hasattr(self, 'oob_improvement_'):
1362             del self.oob_improvement_
1363         if hasattr(self, 'init_'):
1364             del self.init_
1365         if hasattr(self, '_rng'):
1366             del self._rng
1367 
1368     def _resize_state(self):
1369         """Add additional ``n_estimators`` entries to all attributes. """
1370         # self.n_estimators is the number of additional est to fit
1371         total_n_estimators = self.n_estimators
1372         if total_n_estimators < self.estimators_.shape[0]:
1373             raise ValueError('resize with smaller n_estimators %d < %d' %
1374                              (total_n_estimators, self.estimators_[0]))
1375 
1376         self.estimators_ = np.resize(self.estimators_,
1377                                      (total_n_estimators, self.loss_.K))
1378         self.train_score_ = np.resize(self.train_score_, total_n_estimators)
1379         if (self.subsample < 1 or hasattr(self, 'oob_improvement_')):
1380             # if do oob resize arrays or create new if not available
1381             if hasattr(self, 'oob_improvement_'):
1382                 self.oob_improvement_ = np.resize(self.oob_improvement_,
1383                                                   total_n_estimators)
1384             else:
1385                 self.oob_improvement_ = np.zeros((total_n_estimators,),
1386                                                  dtype=np.float64)
1387 
1388     def _is_initialized(self):
1389         return len(getattr(self, 'estimators_', [])) > 0
1390 
1391     def _check_initialized(self):
1392         """Check that the estimator is initialized, raising an error if not."""
1393         check_is_fitted(self, 'estimators_')
1394 
1395     def fit(self, X, y, sample_weight=None, monitor=None):
1396         """Fit the gradient boosting model.
1397 
1398         Parameters
1399         ----------
1400         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1401             The input samples. Internally, it will be converted to
1402             ``dtype=np.float32`` and if a sparse matrix is provided
1403             to a sparse ``csr_matrix``.
1404 
1405         y : array-like, shape (n_samples,)
1406             Target values (strings or integers in classification, real numbers
1407             in regression)
1408             For classification, labels must correspond to classes.
1409 
1410         sample_weight : array-like, shape (n_samples,) or None
1411             Sample weights. If None, then samples are equally weighted. Splits
1412             that would create child nodes with net zero or negative weight are
1413             ignored while searching for a split in each node. In the case of
1414             classification, splits are also ignored if they would result in any
1415             single class carrying a negative weight in either child node.
1416 
1417         monitor : callable, optional
1418             The monitor is called after each iteration with the current
1419             iteration, a reference to the estimator and the local variables of
1420             ``_fit_stages`` as keyword arguments ``callable(i, self,
1421             locals())``. If the callable returns ``True`` the fitting procedure
1422             is stopped. The monitor can be used for various things such as
1423             computing held-out estimates, early stopping, model introspect, and
1424             snapshoting.
1425 
1426         Returns
1427         -------
1428         self : object
1429         """
1430         # if not warmstart - clear the estimator state
1431         if not self.warm_start:
1432             self._clear_state()
1433 
1434         # Check input
1435         # Since check_array converts both X and y to the same dtype, but the
1436         # trees use different types for X and y, checking them separately.
1437         X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
1438         n_samples, self.n_features_ = X.shape
1439 
1440         sample_weight_is_none = sample_weight is None
1441         if sample_weight_is_none:
1442             sample_weight = np.ones(n_samples, dtype=np.float32)
1443         else:
1444             sample_weight = column_or_1d(sample_weight, warn=True)
1445             sample_weight_is_none = False
1446 
1447         check_consistent_length(X, y, sample_weight)
1448 
1449         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
1450         y = column_or_1d(y, warn=True)
1451         y = self._validate_y(y, sample_weight)
1452 
1453         if self.n_iter_no_change is not None:
1454             stratify = y if is_classifier(self) else None
1455             X, X_val, y, y_val, sample_weight, sample_weight_val = (
1456                 train_test_split(X, y, sample_weight,
1457                                  random_state=self.random_state,
1458                                  test_size=self.validation_fraction,
1459                                  stratify=stratify))
1460             if is_classifier(self):
1461                 if self.n_classes_ != np.unique(y).shape[0]:
1462                     # We choose to error here. The problem is that the init
1463                     # estimator would be trained on y, which has some missing
1464                     # classes now, so its predictions would not have the
1465                     # correct shape.
1466                     raise ValueError(
1467                         'The training data after the early stopping split '
1468                         'is missing some classes. Try using another random '
1469                         'seed.'
1470                     )
1471         else:
1472             X_val = y_val = sample_weight_val = None
1473 
1474         self._check_params()
1475 
1476         if not self._is_initialized():
1477             # init state
1478             self._init_state()
1479 
1480             # fit initial model and initialize raw predictions
1481             if self.init_ == 'zero':
1482                 raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1483                                            dtype=np.float64)
1484             else:
1485                 # XXX clean this once we have a support_sample_weight tag
1486                 if sample_weight_is_none:
1487                     self.init_.fit(X, y)
1488                 else:
1489                     msg = ("The initial estimator {} does not support sample "
1490                            "weights.".format(self.init_.__class__.__name__))
1491                     try:
1492                         self.init_.fit(X, y, sample_weight=sample_weight)
1493                     except TypeError:  # regular estimator without SW support
1494                         raise ValueError(msg)
1495                     except ValueError as e:
1496                         if "pass parameters to specific steps of "\
1497                            "your pipeline using the "\
1498                            "stepname__parameter" in str(e):  # pipeline
1499                             raise ValueError(msg) from e
1500                         else:  # regular estimator whose input checking failed
1501                             raise
1502 
1503                 raw_predictions = \
1504                     self.loss_.get_init_raw_predictions(X, self.init_)
1505 
1506             begin_at_stage = 0
1507 
1508             # The rng state must be preserved if warm_start is True
1509             self._rng = check_random_state(self.random_state)
1510 
1511         else:
1512             # add more estimators to fitted model
1513             # invariant: warm_start = True
1514             if self.n_estimators < self.estimators_.shape[0]:
1515                 raise ValueError('n_estimators=%d must be larger or equal to '
1516                                  'estimators_.shape[0]=%d when '
1517                                  'warm_start==True'
1518                                  % (self.n_estimators,
1519                                     self.estimators_.shape[0]))
1520             begin_at_stage = self.estimators_.shape[0]
1521             # The requirements of _decision_function (called in two lines
1522             # below) are more constrained than fit. It accepts only CSR
1523             # matrices.
1524             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1525             raw_predictions = self._raw_predict(X)
1526             self._resize_state()
1527 
1528         if self.presort is True and issparse(X):
1529             raise ValueError(
1530                 "Presorting is not supported for sparse matrices.")
1531 
1532         presort = self.presort
1533         # Allow presort to be 'auto', which means True if the dataset is dense,
1534         # otherwise it will be False.
1535         if presort == 'auto':
1536             presort = not issparse(X)
1537 
1538         X_idx_sorted = None
1539         if presort:
1540             X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),
1541                                              dtype=np.int32)
1542 
1543         # fit the boosting stages
1544         n_stages = self._fit_stages(
1545             X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,
1546             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)
1547 
1548         # change shape of arrays after fit (early-stopping or additional ests)
1549         if n_stages != self.estimators_.shape[0]:
1550             self.estimators_ = self.estimators_[:n_stages]
1551             self.train_score_ = self.train_score_[:n_stages]
1552             if hasattr(self, 'oob_improvement_'):
1553                 self.oob_improvement_ = self.oob_improvement_[:n_stages]
1554 
1555         self.n_estimators_ = n_stages
1556         return self
1557 
1558     def _fit_stages(self, X, y, raw_predictions, sample_weight, random_state,
1559                     X_val, y_val, sample_weight_val,
1560                     begin_at_stage=0, monitor=None, X_idx_sorted=None):
1561         """Iteratively fits the stages.
1562 
1563         For each stage it computes the progress (OOB, train score)
1564         and delegates to ``_fit_stage``.
1565         Returns the number of stages fit; might differ from ``n_estimators``
1566         due to early stopping.
1567         """
1568         n_samples = X.shape[0]
1569         do_oob = self.subsample < 1.0
1570         sample_mask = np.ones((n_samples, ), dtype=np.bool)
1571         n_inbag = max(1, int(self.subsample * n_samples))
1572         loss_ = self.loss_
1573 
1574         # Set min_weight_leaf from min_weight_fraction_leaf
1575         if self.min_weight_fraction_leaf != 0. and sample_weight is not None:
1576             min_weight_leaf = (self.min_weight_fraction_leaf *
1577                                np.sum(sample_weight))
1578         else:
1579             min_weight_leaf = 0.
1580 
1581         if self.verbose:
1582             verbose_reporter = VerboseReporter(self.verbose)
1583             verbose_reporter.init(self, begin_at_stage)
1584 
1585         X_csc = csc_matrix(X) if issparse(X) else None
1586         X_csr = csr_matrix(X) if issparse(X) else None
1587 
1588         if self.n_iter_no_change is not None:
1589             loss_history = np.full(self.n_iter_no_change, np.inf)
1590             # We create a generator to get the predictions for X_val after
1591             # the addition of each successive stage
1592             y_val_pred_iter = self._staged_raw_predict(X_val)
1593 
1594         # perform boosting iterations
1595         i = begin_at_stage
1596         for i in range(begin_at_stage, self.n_estimators):
1597 
1598             # subsampling
1599             if do_oob:
1600                 sample_mask = _random_sample_mask(n_samples, n_inbag,
1601                                                   random_state)
1602                 # OOB score before adding this stage
1603                 old_oob_score = loss_(y[~sample_mask],
1604                                       raw_predictions[~sample_mask],
1605                                       sample_weight[~sample_mask])
1606 
1607             # fit next stage of trees
1608             raw_predictions = self._fit_stage(
1609                 i, X, y, raw_predictions, sample_weight, sample_mask,
1610                 random_state, X_idx_sorted, X_csc, X_csr)
1611 
1612             # track deviance (= loss)
1613             if do_oob:
1614                 self.train_score_[i] = loss_(y[sample_mask],
1615                                              raw_predictions[sample_mask],
1616                                              sample_weight[sample_mask])
1617                 self.oob_improvement_[i] = (
1618                     old_oob_score - loss_(y[~sample_mask],
1619                                           raw_predictions[~sample_mask],
1620                                           sample_weight[~sample_mask]))
1621             else:
1622                 # no need to fancy index w/ no subsampling
1623                 self.train_score_[i] = loss_(y, raw_predictions, sample_weight)
1624 
1625             if self.verbose > 0:
1626                 verbose_reporter.update(i, self)
1627 
1628             if monitor is not None:
1629                 early_stopping = monitor(i, self, locals())
1630                 if early_stopping:
1631                     break
1632 
1633             # We also provide an early stopping based on the score from
1634             # validation set (X_val, y_val), if n_iter_no_change is set
1635             if self.n_iter_no_change is not None:
1636                 # By calling next(y_val_pred_iter), we get the predictions
1637                 # for X_val after the addition of the current stage
1638                 validation_loss = loss_(y_val, next(y_val_pred_iter),
1639                                         sample_weight_val)
1640 
1641                 # Require validation_score to be better (less) than at least
1642                 # one of the last n_iter_no_change evaluations
1643                 if np.any(validation_loss + self.tol < loss_history):
1644                     loss_history[i % len(loss_history)] = validation_loss
1645                 else:
1646                     break
1647 
1648         return i + 1
1649 
1650     def _make_estimator(self, append=True):
1651         # we don't need _make_estimator
1652         raise NotImplementedError()
1653 
1654     def _raw_predict_init(self, X):
1655         """Check input and compute raw predictions of the init estimtor."""
1656         self._check_initialized()
1657         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1658         if X.shape[1] != self.n_features_:
1659             raise ValueError("X.shape[1] should be {0:d}, not {1:d}.".format(
1660                 self.n_features_, X.shape[1]))
1661         if self.init_ == 'zero':
1662             raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1663                                        dtype=np.float64)
1664         else:
1665             raw_predictions = self.loss_.get_init_raw_predictions(
1666                 X, self.init_).astype(np.float64)
1667         return raw_predictions
1668 
1669     def _raw_predict(self, X):
1670         """Return the sum of the trees raw predictions (+ init estimator)."""
1671         raw_predictions = self._raw_predict_init(X)
1672         predict_stages(self.estimators_, X, self.learning_rate,
1673                        raw_predictions)
1674         return raw_predictions
1675 
1676     def _staged_raw_predict(self, X):
1677         """Compute raw predictions of ``X`` for each iteration.
1678 
1679         This method allows monitoring (i.e. determine error on testing set)
1680         after each stage.
1681 
1682         Parameters
1683         ----------
1684         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1685             The input samples. Internally, it will be converted to
1686             ``dtype=np.float32`` and if a sparse matrix is provided
1687             to a sparse ``csr_matrix``.
1688 
1689         Returns
1690         -------
1691         raw_predictions : generator of array, shape (n_samples, k)
1692             The raw predictions of the input samples. The order of the
1693             classes corresponds to that in the attribute `classes_`.
1694             Regression and binary classification are special cases with
1695             ``k == 1``, otherwise ``k==n_classes``.
1696         """
1697         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
1698         raw_predictions = self._raw_predict_init(X)
1699         for i in range(self.estimators_.shape[0]):
1700             predict_stage(self.estimators_, i, X, self.learning_rate,
1701                           raw_predictions)
1702             yield raw_predictions.copy()
1703 
1704     @property
1705     def feature_importances_(self):
1706         """Return the feature importances (the higher, the more important the
1707            feature).
1708 
1709         Returns
1710         -------
1711         feature_importances_ : array, shape (n_features,)
1712         """
1713         self._check_initialized()
1714 
1715         total_sum = np.zeros((self.n_features_, ), dtype=np.float64)
1716         for stage in self.estimators_:
1717             stage_sum = sum(tree.tree_.compute_feature_importances(
1718                 normalize=False) for tree in stage) / len(stage)
1719             total_sum += stage_sum
1720 
1721         importances = total_sum / total_sum.sum()
1722         return importances
1723 
1724     def _validate_y(self, y, sample_weight):
1725         # 'sample_weight' is not utilised but is used for
1726         # consistency with similar method _validate_y of GBC
1727         self.n_classes_ = 1
1728         if y.dtype.kind == 'O':
1729             y = y.astype(DOUBLE)
1730         # Default implementation
1731         return y
1732 
1733     def apply(self, X):
1734         """Apply trees in the ensemble to X, return leaf indices.
1735 
1736         .. versionadded:: 0.17
1737 
1738         Parameters
1739         ----------
1740         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1741             The input samples. Internally, its dtype will be converted to
1742             ``dtype=np.float32``. If a sparse matrix is provided, it will
1743             be converted to a sparse ``csr_matrix``.
1744 
1745         Returns
1746         -------
1747         X_leaves : array-like, shape (n_samples, n_estimators, n_classes)
1748             For each datapoint x in X and for each tree in the ensemble,
1749             return the index of the leaf x ends up in each estimator.
1750             In the case of binary classification n_classes is 1.
1751         """
1752 
1753         self._check_initialized()
1754         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1755 
1756         # n_classes will be equal to 1 in the binary classification or the
1757         # regression case.
1758         n_estimators, n_classes = self.estimators_.shape
1759         leaves = np.zeros((X.shape[0], n_estimators, n_classes))
1760 
1761         for i in range(n_estimators):
1762             for j in range(n_classes):
1763                 estimator = self.estimators_[i, j]
1764                 leaves[:, i, j] = estimator.apply(X, check_input=False)
1765 
1766         return leaves

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-17 17:40:47.201 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-17 17:40:47.201 | INFO     | Search-only: skipping patch selection
2025-10-17 17:40:47.290 | INFO     | Task scikit-learn__scikit-learn-13620 completed successfully.
2025-10-17 17:40:47.291 | INFO     | No patch generated. You can try running ACR again.
