2025-10-17 15:46:37.121 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-17 15:46:40.396 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=343, output_tokens=12, cost=0.003790
2025-10-17 15:46:40.397 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-17 15:46:40.404 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 15:46:45.719 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=642, output_tokens=115, cost=0.009870
2025-10-17 15:46:45.726 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 15:46:48.698 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=469, output_tokens=54, cost=0.006310
2025-10-17 15:46:48.699 | DEBUG    | Extracted a valid json.
2025-10-17 15:46:48.699 | DEBUG    | Agent proxy return the following json: {
    "API_calls": ["search_method(\"merge\")", "search_code(\"combine_attrs : {\\\"drop\\\", \\\"identical\\\", \\\"no_conflicts\\\", \\\"override\\\"}\")"],
    "bug_locations": []
}
2025-10-17 15:46:48.721 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-17 15:46:48.722 | DEBUG    | <Agent search> Analyzing search results.
2025-10-17 15:47:05.231 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1054, output_tokens=624, cost=0.029260
2025-10-17 15:47:05.244 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 15:47:15.527 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1894, output_tokens=427, cost=0.031750
2025-10-17 15:47:15.536 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 15:47:25.909 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=781, output_tokens=389, cost=0.019480
2025-10-17 15:47:25.910 | DEBUG    | Extracted a valid json.
2025-10-17 15:47:25.916 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_code_in_file(\"combine_attrs\", \"xarray/core/merge.py\")",
        "search_code_in_file(\"combine_attrs\", \"xarray/core/combine.py\")",
        "search_code_in_file(\"combine_attrs\", \"xarray/core/concat.py\")"
    ],
    "bug_locations": [
        {
            "file": "xarray/core/merge.py",
            "class": "",
            "method": "",
            "intended_behavior": "This file is expected to contain the core logic for the `merge()` function and handling of the `combine_attrs` parameter. The intended behavior here would involve incorporating logic to handle the \"drop_conflicts\" option which would iteratively check for attribute conflicts among datasets being merged and discard any attributes where conflicts arise, while preserving existing merge functionalities for other `combine_attrs` options."
        },
        {
            "file": "xarray/core/combine.py",
            "class": "",
            "method": "",
            "intended_behavior": "This file likely contains implementations for combining operations that also leverage the `combine_attrs` logic, especially in functions like combining datasets into one cohesive dataset. The intended behavior would include the ability to handle the \"drop_conflicts\" option in a similar manner to `merge.py`, ensuring attributes with conflicting values are silently dropped during the combine operation."
        },
        {
            "file": "xarray/core/concat.py",
            "class": "",
            "method": "",
            "intended_behavior": "Considering this file deals with concatenation operations where `combine_attrs` could also play a role, incorporating the \"drop_conflicts\" logic to manage attribute conflicts during dataset concatenation would be necessary. Intended modifications would ensure concatenation operations can also utilise the \"drop_conflicts\" strategy to manage attributes, specifically by dropping attributes that conflict across the datasets being concatenated."
        }
    ]
}
2025-10-17 15:47:25.964 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-17 15:47:25.965 | DEBUG    | <Agent search> Analyzing search results.
2025-10-17 15:47:43.756 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2900, output_tokens=702, cost=0.050060
2025-10-17 15:47:43.772 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 15:47:54.589 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3818, output_tokens=423, cost=0.050870
2025-10-17 15:47:54.599 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 15:48:03.703 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=777, output_tokens=328, cost=0.017610
2025-10-17 15:48:03.704 | DEBUG    | Extracted a valid json.
2025-10-17 15:48:03.704 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "xarray/core/merge.py",
            "class": "",
            "method": "merge_attrs",
            "intended_behavior": "Update this method to handle the 'drop_conflicts' option for 'combine_attrs'. This involves writing logic to compare attributes from different datasets and discard any attribute that has conflicting values across the datasets being merged, thus ensuring that only consistent attributes or those without any conflicts are retained."
        },
        {
            "file": "xarray/core/combine.py",
            "class": "",
            "method": "Various (_combine_nd, _combine_all_along_first_dim, _combine_1d, _nested_combine, combine_nested, and combine_by_coords need consideration)",
            "intended_behavior": "Modify these methods to interpret and apply the 'drop_conflicts' strategy within their respective 'combine_attrs' handling. The implementation should merge dataset attributes according to the specified 'combine_attrs' strategy, specifically adding logic to handle attribute conflicts by dropping conflicting attributes during dataset combination operations."
        },
        {
            "file": "xarray/core/concat.py",
            "class": "",
            "method": "Various (concat, _dataset_concat, _dataarray_concat are mentioned)",
            "intended_behavior": "Ensure these methods support the 'drop_conflicts' option within their 'combine_attrs' parameter processing. The new logic should assess attributes from the datasets being concatenated, excluding attributes with conflicting values from the concatenated result to adhere to the 'drop_conflicts' rule."
        }
    ]
}
2025-10-17 15:48:03.712 | DEBUG    | Bug location extracted successfully: [<file>xarray/core/merge.py</file>
<class>None</class>
<method>merge_attrs</method>
<code>
495 def merge_attrs(variable_attrs, combine_attrs):
496     """Combine attributes from different variables according to combine_attrs"""
497     if not variable_attrs:
498         # no attributes to merge
499         return None
500 
501     if combine_attrs == "drop":
502         return {}
503     elif combine_attrs == "override":
504         return dict(variable_attrs[0])
505     elif combine_attrs == "no_conflicts":
506         result = dict(variable_attrs[0])
507         for attrs in variable_attrs[1:]:
508             try:
509                 result = compat_dict_union(result, attrs)
510             except ValueError:
511                 raise MergeError(
512                     "combine_attrs='no_conflicts', but some values are not "
513                     "the same. Merging %s with %s" % (str(result), str(attrs))
514                 )
515         return result
516     elif combine_attrs == "identical":
517         result = dict(variable_attrs[0])
518         for attrs in variable_attrs[1:]:
519             if not dict_equiv(result, attrs):
520                 raise MergeError(
521                     "combine_attrs='identical', but attrs differ. First is %s "
522                     ", other is %s." % (str(result), str(attrs))
523                 )
524         return result
525     else:
526         raise ValueError("Unrecognised value for combine_attrs=%s" % combine_attrs)

</code><intended_behavior>Update this method to handle the 'drop_conflicts' option for 'combine_attrs'. This involves writing logic to compare attributes from different datasets and discard any attribute that has conflicting values across the datasets being merged, thus ensuring that only consistent attributes or those without any conflicts are retained.</intended_behavior>, <file>xarray/core/combine.py</file>
<class>None</class>
<method>None</method>
<code>
1 import itertools
2 from collections import Counter
3 
4 import pandas as pd
5 
6 from . import dtypes
7 from .concat import concat
8 from .dataarray import DataArray
9 from .dataset import Dataset
10 from .merge import merge
11 
12 
13 def _infer_concat_order_from_positions(datasets):
14     combined_ids = dict(_infer_tile_ids_from_nested_list(datasets, ()))
15     return combined_ids
16 
17 
18 def _infer_tile_ids_from_nested_list(entry, current_pos):
19     """
20     Given a list of lists (of lists...) of objects, returns a iterator
21     which returns a tuple containing the index of each object in the nested
22     list structure as the key, and the object. This can then be called by the
23     dict constructor to create a dictionary of the objects organised by their
24     position in the original nested list.
25 
26     Recursively traverses the given structure, while keeping track of the
27     current position. Should work for any type of object which isn't a list.
28 
29     Parameters
30     ----------
31     entry : list[list[obj, obj, ...], ...]
32         List of lists of arbitrary depth, containing objects in the order
33         they are to be concatenated.
34 
35     Returns
36     -------
37     combined_tile_ids : dict[tuple(int, ...), obj]
38     """
39 
40     if isinstance(entry, list):
41         for i, item in enumerate(entry):
42             yield from _infer_tile_ids_from_nested_list(item, current_pos + (i,))
43     else:
44         yield current_pos, entry
45 
46 
47 def _infer_concat_order_from_coords(datasets):
48 
49     concat_dims = []
50     tile_ids = [() for ds in datasets]
51 
52     # All datasets have same variables because they've been grouped as such
53     ds0 = datasets[0]
54     for dim in ds0.dims:
55 
56         # Check if dim is a coordinate dimension
57         if dim in ds0:
58 
59             # Need to read coordinate values to do ordering
60             indexes = [ds.indexes.get(dim) for ds in datasets]
61             if any(index is None for index in indexes):
62                 raise ValueError(
63                     "Every dimension needs a coordinate for "
64                     "inferring concatenation order"
65                 )
66 
67             # If dimension coordinate values are same on every dataset then
68             # should be leaving this dimension alone (it's just a "bystander")
69             if not all(index.equals(indexes[0]) for index in indexes[1:]):
70 
71                 # Infer order datasets should be arranged in along this dim
72                 concat_dims.append(dim)
73 
74                 if all(index.is_monotonic_increasing for index in indexes):
75                     ascending = True
76                 elif all(index.is_monotonic_decreasing for index in indexes):
77                     ascending = False
78                 else:
79                     raise ValueError(
80                         "Coordinate variable {} is neither "
81                         "monotonically increasing nor "
82                         "monotonically decreasing on all datasets".format(dim)
83                     )
84 
85                 # Assume that any two datasets whose coord along dim starts
86                 # with the same value have the same coord values throughout.
87                 if any(index.size == 0 for index in indexes):
88                     raise ValueError("Cannot handle size zero dimensions")
89                 first_items = pd.Index([index[0] for index in indexes])
90 
91                 # Sort datasets along dim
92                 # We want rank but with identical elements given identical
93                 # position indices - they should be concatenated along another
94                 # dimension, not along this one
95                 series = first_items.to_series()
96                 rank = series.rank(
97                     method="dense", ascending=ascending, numeric_only=False
98                 )
99                 order = rank.astype(int).values - 1
100 
101                 # Append positions along extra dimension to structure which
102                 # encodes the multi-dimensional concatentation order
103                 tile_ids = [
104                     tile_id + (position,) for tile_id, position in zip(tile_ids, order)
105                 ]
106 
107     if len(datasets) > 1 and not concat_dims:
108         raise ValueError(
109             "Could not find any dimension coordinates to use to "
110             "order the datasets for concatenation"
111         )
112 
113     combined_ids = dict(zip(tile_ids, datasets))
114 
115     return combined_ids, concat_dims
116 
117 
118 def _check_dimension_depth_tile_ids(combined_tile_ids):
119     """
120     Check all tuples are the same length, i.e. check that all lists are
121     nested to the same depth.
122     """
123     tile_ids = combined_tile_ids.keys()
124     nesting_depths = [len(tile_id) for tile_id in tile_ids]
125     if not nesting_depths:
126         nesting_depths = [0]
127     if not set(nesting_depths) == {nesting_depths[0]}:
128         raise ValueError(
129             "The supplied objects do not form a hypercube because"
130             " sub-lists do not have consistent depths"
131         )
132     # return these just to be reused in _check_shape_tile_ids
133     return tile_ids, nesting_depths
134 
135 
136 def _check_shape_tile_ids(combined_tile_ids):
137     """Check all lists along one dimension are same length."""
138     tile_ids, nesting_depths = _check_dimension_depth_tile_ids(combined_tile_ids)
139     for dim in range(nesting_depths[0]):
140         indices_along_dim = [tile_id[dim] for tile_id in tile_ids]
141         occurrences = Counter(indices_along_dim)
142         if len(set(occurrences.values())) != 1:
143             raise ValueError(
144                 "The supplied objects do not form a hypercube "
145                 "because sub-lists do not have consistent "
146                 "lengths along dimension" + str(dim)
147             )
148 
149 
150 def _combine_nd(
151     combined_ids,
152     concat_dims,
153     data_vars="all",
154     coords="different",
155     compat="no_conflicts",
156     fill_value=dtypes.NA,
157     join="outer",
158     combine_attrs="drop",
159 ):
160     """
161     Combines an N-dimensional structure of datasets into one by applying a
162     series of either concat and merge operations along each dimension.
163 
164     No checks are performed on the consistency of the datasets, concat_dims or
165     tile_IDs, because it is assumed that this has already been done.
166 
167     Parameters
168     ----------
169     combined_ids : Dict[Tuple[int, ...]], xarray.Dataset]
170         Structure containing all datasets to be concatenated with "tile_IDs" as
171         keys, which specify position within the desired final combined result.
172     concat_dims : sequence of str
173         The dimensions along which the datasets should be concatenated. Must be
174         in order, and the length must match the length of the tuples used as
175         keys in combined_ids. If the string is a dimension name then concat
176         along that dimension, if it is None then merge.
177 
178     Returns
179     -------
180     combined_ds : xarray.Dataset
181     """
182 
183     example_tile_id = next(iter(combined_ids.keys()))
184 
185     n_dims = len(example_tile_id)
186     if len(concat_dims) != n_dims:
187         raise ValueError(
188             "concat_dims has length {} but the datasets "
189             "passed are nested in a {}-dimensional structure".format(
190                 len(concat_dims), n_dims
191             )
192         )
193 
194     # Each iteration of this loop reduces the length of the tile_ids tuples
195     # by one. It always combines along the first dimension, removing the first
196     # element of the tuple
197     for concat_dim in concat_dims:
198         combined_ids = _combine_all_along_first_dim(
199             combined_ids,
200             dim=concat_dim,
201             data_vars=data_vars,
202             coords=coords,
203             compat=compat,
204             fill_value=fill_value,
205             join=join,
206             combine_attrs=combine_attrs,
207         )
208     (combined_ds,) = combined_ids.values()
209     return combined_ds
210 
211 
212 def _combine_all_along_first_dim(
213     combined_ids,
214     dim,
215     data_vars,
216     coords,
217     compat,
218     fill_value=dtypes.NA,
219     join="outer",
220     combine_attrs="drop",
221 ):
222 
223     # Group into lines of datasets which must be combined along dim
224     # need to sort by _new_tile_id first for groupby to work
225     # TODO: is the sorted need?
226     combined_ids = dict(sorted(combined_ids.items(), key=_new_tile_id))
227     grouped = itertools.groupby(combined_ids.items(), key=_new_tile_id)
228 
229     # Combine all of these datasets along dim
230     new_combined_ids = {}
231     for new_id, group in grouped:
232         combined_ids = dict(sorted(group))
233         datasets = combined_ids.values()
234         new_combined_ids[new_id] = _combine_1d(
235             datasets, dim, compat, data_vars, coords, fill_value, join, combine_attrs
236         )
237     return new_combined_ids
238 
239 
240 def _combine_1d(
241     datasets,
242     concat_dim,
243     compat="no_conflicts",
244     data_vars="all",
245     coords="different",
246     fill_value=dtypes.NA,
247     join="outer",
248     combine_attrs="drop",
249 ):
250     """
251     Applies either concat or merge to 1D list of datasets depending on value
252     of concat_dim
253     """
254 
255     if concat_dim is not None:
256         try:
257             combined = concat(
258                 datasets,
259                 dim=concat_dim,
260                 data_vars=data_vars,
261                 coords=coords,
262                 compat=compat,
263                 fill_value=fill_value,
264                 join=join,
265                 combine_attrs=combine_attrs,
266             )
267         except ValueError as err:
268             if "encountered unexpected variable" in str(err):
269                 raise ValueError(
270                     "These objects cannot be combined using only "
271                     "xarray.combine_nested, instead either use "
272                     "xarray.combine_by_coords, or do it manually "
273                     "with xarray.concat, xarray.merge and "
274                     "xarray.align"
275                 )
276             else:
277                 raise
278     else:
279         combined = merge(
280             datasets,
281             compat=compat,
282             fill_value=fill_value,
283             join=join,
284             combine_attrs=combine_attrs,
285         )
286 
287     return combined
288 
289 
290 def _new_tile_id(single_id_ds_pair):
291     tile_id, ds = single_id_ds_pair
292     return tile_id[1:]
293 
294 
295 def _nested_combine(
296     datasets,
297     concat_dims,
298     compat,
299     data_vars,
300     coords,
301     ids,
302     fill_value=dtypes.NA,
303     join="outer",
304     combine_attrs="drop",
305 ):
306 
307     if len(datasets) == 0:
308         return Dataset()
309 
310     # Arrange datasets for concatenation
311     # Use information from the shape of the user input
312     if not ids:
313         # Determine tile_IDs by structure of input in N-D
314         # (i.e. ordering in list-of-lists)
315         combined_ids = _infer_concat_order_from_positions(datasets)
316     else:
317         # Already sorted so just use the ids already passed
318         combined_ids = dict(zip(ids, datasets))
319 
320     # Check that the inferred shape is combinable
321     _check_shape_tile_ids(combined_ids)
322 
323     # Apply series of concatenate or merge operations along each dimension
324     combined = _combine_nd(
325         combined_ids,
326         concat_dims,
327         compat=compat,
328         data_vars=data_vars,
329         coords=coords,
330         fill_value=fill_value,
331         join=join,
332         combine_attrs=combine_attrs,
333     )
334     return combined
335 
336 
337 def combine_nested(
338     datasets,
339     concat_dim,
340     compat="no_conflicts",
341     data_vars="all",
342     coords="different",
343     fill_value=dtypes.NA,
344     join="outer",
345     combine_attrs="drop",
346 ):
347     """
348     Explicitly combine an N-dimensional grid of datasets into one by using a
349     succession of concat and merge operations along each dimension of the grid.
350 
351     Does not sort the supplied datasets under any circumstances, so the
352     datasets must be passed in the order you wish them to be concatenated. It
353     does align coordinates, but different variables on datasets can cause it to
354     fail under some scenarios. In complex cases, you may need to clean up your
355     data and use concat/merge explicitly.
356 
357     To concatenate along multiple dimensions the datasets must be passed as a
358     nested list-of-lists, with a depth equal to the length of ``concat_dims``.
359     ``manual_combine`` will concatenate along the top-level list first.
360 
361     Useful for combining datasets from a set of nested directories, or for
362     collecting the output of a simulation parallelized along multiple
363     dimensions.
364 
365     Parameters
366     ----------
367     datasets : list or nested list of Dataset
368         Dataset objects to combine.
369         If concatenation or merging along more than one dimension is desired,
370         then datasets must be supplied in a nested list-of-lists.
371     concat_dim : str, or list of str, DataArray, Index or None
372         Dimensions along which to concatenate variables, as used by
373         :py:func:`xarray.concat`.
374         Set ``concat_dim=[..., None, ...]`` explicitly to disable concatenation
375         and merge instead along a particular dimension.
376         The position of ``None`` in the list specifies the dimension of the
377         nested-list input along which to merge.
378         Must be the same length as the depth of the list passed to
379         ``datasets``.
380     compat : {"identical", "equals", "broadcast_equals", \
381               "no_conflicts", "override"}, optional
382         String indicating how to compare variables of the same name for
383         potential merge conflicts:
384 
385         - "broadcast_equals": all values must be equal when variables are
386           broadcast against each other to ensure common dimensions.
387         - "equals": all values and dimensions must be the same.
388         - "identical": all values, dimensions and attributes must be the
389           same.
390         - "no_conflicts": only values which are not null in both datasets
391           must be equal. The returned dataset then contains the combination
392           of all non-null values.
393         - "override": skip comparing and pick variable from first dataset
394     data_vars : {"minimal", "different", "all" or list of str}, optional
395         Details are in the documentation of concat
396     coords : {"minimal", "different", "all" or list of str}, optional
397         Details are in the documentation of concat
398     fill_value : scalar or dict-like, optional
399         Value to use for newly missing values. If a dict-like, maps
400         variable names to fill values. Use a data array's name to
401         refer to its values.
402     join : {"outer", "inner", "left", "right", "exact"}, optional
403         String indicating how to combine differing indexes
404         (excluding concat_dim) in objects
405 
406         - "outer": use the union of object indexes
407         - "inner": use the intersection of object indexes
408         - "left": use indexes from the first object with each dimension
409         - "right": use indexes from the last object with each dimension
410         - "exact": instead of aligning, raise `ValueError` when indexes to be
411           aligned are not equal
412         - "override": if indexes are of same size, rewrite indexes to be
413           those of the first object with that dimension. Indexes for the same
414           dimension must have the same size in all objects.
415     combine_attrs : {"drop", "identical", "no_conflicts", "override"}, \
416                     default: "drop"
417         String indicating how to combine attrs of the objects being merged:
418 
419         - "drop": empty attrs on returned Dataset.
420         - "identical": all attrs must be the same on every object.
421         - "no_conflicts": attrs from all objects are combined, any that have
422           the same name must also have the same value.
423         - "override": skip comparing and copy attrs from the first dataset to
424           the result.
425 
426     Returns
427     -------
428     combined : xarray.Dataset
429 
430     Examples
431     --------
432 
433     A common task is collecting data from a parallelized simulation in which
434     each process wrote out to a separate file. A domain which was decomposed
435     into 4 parts, 2 each along both the x and y axes, requires organising the
436     datasets into a doubly-nested list, e.g:
437 
438     >>> x1y1 = xr.Dataset(
439     ...     {
440     ...         "temperature": (("x", "y"), np.random.randn(2, 2)),
441     ...         "precipitation": (("x", "y"), np.random.randn(2, 2)),
442     ...     }
443     ... )
444     >>> x1y1
445     <xarray.Dataset>
446     Dimensions:        (x: 2, y: 2)
447     Dimensions without coordinates: x, y
448     Data variables:
449         temperature    (x, y) float64 1.764 0.4002 0.9787 2.241
450         precipitation  (x, y) float64 1.868 -0.9773 0.9501 -0.1514
451     >>> x1y2 = xr.Dataset(
452     ...     {
453     ...         "temperature": (("x", "y"), np.random.randn(2, 2)),
454     ...         "precipitation": (("x", "y"), np.random.randn(2, 2)),
455     ...     }
456     ... )
457     >>> x2y1 = xr.Dataset(
458     ...     {
459     ...         "temperature": (("x", "y"), np.random.randn(2, 2)),
460     ...         "precipitation": (("x", "y"), np.random.randn(2, 2)),
461     ...     }
462     ... )
463     >>> x2y2 = xr.Dataset(
464     ...     {
465     ...         "temperature": (("x", "y"), np.random.randn(2, 2)),
466     ...         "precipitation": (("x", "y"), np.random.randn(2, 2)),
467     ...     }
468     ... )
469 
470 
471     >>> ds_grid = [[x1y1, x1y2], [x2y1, x2y2]]
472     >>> combined = xr.combine_nested(ds_grid, concat_dim=["x", "y"])
473     >>> combined
474     <xarray.Dataset>
475     Dimensions:        (x: 4, y: 4)
476     Dimensions without coordinates: x, y
477     Data variables:
478         temperature    (x, y) float64 1.764 0.4002 -0.1032 ... 0.04576 -0.1872
479         precipitation  (x, y) float64 1.868 -0.9773 0.761 ... -0.7422 0.1549 0.3782
480 
481     ``manual_combine`` can also be used to explicitly merge datasets with
482     different variables. For example if we have 4 datasets, which are divided
483     along two times, and contain two different variables, we can pass ``None``
484     to ``concat_dim`` to specify the dimension of the nested list over which
485     we wish to use ``merge`` instead of ``concat``:
486 
487     >>> t1temp = xr.Dataset({"temperature": ("t", np.random.randn(5))})
488     >>> t1temp
489     <xarray.Dataset>
490     Dimensions:      (t: 5)
491     Dimensions without coordinates: t
492     Data variables:
493         temperature  (t) float64 -0.8878 -1.981 -0.3479 0.1563 1.23
494 
495     >>> t1precip = xr.Dataset({"precipitation": ("t", np.random.randn(5))})
496     >>> t1precip
497     <xarray.Dataset>
498     Dimensions:        (t: 5)
499     Dimensions without coordinates: t
500     Data variables:
501         precipitation  (t) float64 1.202 -0.3873 -0.3023 -1.049 -1.42
502 
503     >>> t2temp = xr.Dataset({"temperature": ("t", np.random.randn(5))})
504     >>> t2precip = xr.Dataset({"precipitation": ("t", np.random.randn(5))})
505 
506 
507     >>> ds_grid = [[t1temp, t1precip], [t2temp, t2precip]]
508     >>> combined = xr.combine_nested(ds_grid, concat_dim=["t", None])
509     >>> combined
510     <xarray.Dataset>
511     Dimensions:        (t: 10)
512     Dimensions without coordinates: t
513     Data variables:
514         temperature    (t) float64 -0.8878 -1.981 -0.3479 ... -0.5097 -0.4381 -1.253
515         precipitation  (t) float64 1.202 -0.3873 -0.3023 ... -0.2127 -0.8955 0.3869
516 
517     See also
518     --------
519     concat
520     merge
521     """
522     if isinstance(concat_dim, (str, DataArray)) or concat_dim is None:
523         concat_dim = [concat_dim]
524 
525     # The IDs argument tells _manual_combine that datasets aren't yet sorted
526     return _nested_combine(
527         datasets,
528         concat_dims=concat_dim,
529         compat=compat,
530         data_vars=data_vars,
531         coords=coords,
532         ids=False,
533         fill_value=fill_value,
534         join=join,
535         combine_attrs=combine_attrs,
536     )
537 
538 
539 def vars_as_keys(ds):
540     return tuple(sorted(ds))
541 
542 
543 def combine_by_coords(
544     datasets,
545     compat="no_conflicts",
546     data_vars="all",
547     coords="different",
548     fill_value=dtypes.NA,
549     join="outer",
550     combine_attrs="no_conflicts",
551 ):
552     """
553     Attempt to auto-magically combine the given datasets into one by using
554     dimension coordinates.
555 
556     This method attempts to combine a group of datasets along any number of
557     dimensions into a single entity by inspecting coords and metadata and using
558     a combination of concat and merge.
559 
560     Will attempt to order the datasets such that the values in their dimension
561     coordinates are monotonic along all dimensions. If it cannot determine the
562     order in which to concatenate the datasets, it will raise a ValueError.
563     Non-coordinate dimensions will be ignored, as will any coordinate
564     dimensions which do not vary between each dataset.
565 
566     Aligns coordinates, but different variables on datasets can cause it
567     to fail under some scenarios. In complex cases, you may need to clean up
568     your data and use concat/merge explicitly (also see `manual_combine`).
569 
570     Works well if, for example, you have N years of data and M data variables,
571     and each combination of a distinct time period and set of data variables is
572     saved as its own dataset. Also useful for if you have a simulation which is
573     parallelized in multiple dimensions, but has global coordinates saved in
574     each file specifying the positions of points within the global domain.
575 
576     Parameters
577     ----------
578     datasets : sequence of xarray.Dataset
579         Dataset objects to combine.
580     compat : {"identical", "equals", "broadcast_equals", "no_conflicts", "override"}, optional
581         String indicating how to compare variables of the same name for
582         potential conflicts:
583 
584         - "broadcast_equals": all values must be equal when variables are
585           broadcast against each other to ensure common dimensions.
586         - "equals": all values and dimensions must be the same.
587         - "identical": all values, dimensions and attributes must be the
588           same.
589         - "no_conflicts": only values which are not null in both datasets
590           must be equal. The returned dataset then contains the combination
591           of all non-null values.
592         - "override": skip comparing and pick variable from first dataset
593     data_vars : {"minimal", "different", "all" or list of str}, optional
594         These data variables will be concatenated together:
595 
596         * "minimal": Only data variables in which the dimension already
597           appears are included.
598         * "different": Data variables which are not equal (ignoring
599           attributes) across all datasets are also concatenated (as well as
600           all for which dimension already appears). Beware: this option may
601           load the data payload of data variables into memory if they are not
602           already loaded.
603         * "all": All data variables will be concatenated.
604         * list of str: The listed data variables will be concatenated, in
605           addition to the "minimal" data variables.
606 
607         If objects are DataArrays, `data_vars` must be "all".
608     coords : {"minimal", "different", "all"} or list of str, optional
609         As per the "data_vars" kwarg, but for coordinate variables.
610     fill_value : scalar or dict-like, optional
611         Value to use for newly missing values. If a dict-like, maps
612         variable names to fill values. Use a data array's name to
613         refer to its values. If None, raises a ValueError if
614         the passed Datasets do not create a complete hypercube.
615     join : {"outer", "inner", "left", "right", "exact"}, optional
616         String indicating how to combine differing indexes
617         (excluding concat_dim) in objects
618 
619         - "outer": use the union of object indexes
620         - "inner": use the intersection of object indexes
621         - "left": use indexes from the first object with each dimension
622         - "right": use indexes from the last object with each dimension
623         - "exact": instead of aligning, raise `ValueError` when indexes to be
624           aligned are not equal
625         - "override": if indexes are of same size, rewrite indexes to be
626           those of the first object with that dimension. Indexes for the same
627           dimension must have the same size in all objects.
628     combine_attrs : {"drop", "identical", "no_conflicts", "override"}, \
629                     default: "drop"
630         String indicating how to combine attrs of the objects being merged:
631 
632         - "drop": empty attrs on returned Dataset.
633         - "identical": all attrs must be the same on every object.
634         - "no_conflicts": attrs from all objects are combined, any that have
635           the same name must also have the same value.
636         - "override": skip comparing and copy attrs from the first dataset to
637           the result.
638 
639     Returns
640     -------
641     combined : xarray.Dataset
642 
643     See also
644     --------
645     concat
646     merge
647     combine_nested
648 
649     Examples
650     --------
651 
652     Combining two datasets using their common dimension coordinates. Notice
653     they are concatenated based on the values in their dimension coordinates,
654     not on their position in the list passed to `combine_by_coords`.
655 
656     >>> import numpy as np
657     >>> import xarray as xr
658 
659     >>> x1 = xr.Dataset(
660     ...     {
661     ...         "temperature": (("y", "x"), 20 * np.random.rand(6).reshape(2, 3)),
662     ...         "precipitation": (("y", "x"), np.random.rand(6).reshape(2, 3)),
663     ...     },
664     ...     coords={"y": [0, 1], "x": [10, 20, 30]},
665     ... )
666     >>> x2 = xr.Dataset(
667     ...     {
668     ...         "temperature": (("y", "x"), 20 * np.random.rand(6).reshape(2, 3)),
669     ...         "precipitation": (("y", "x"), np.random.rand(6).reshape(2, 3)),
670     ...     },
671     ...     coords={"y": [2, 3], "x": [10, 20, 30]},
672     ... )
673     >>> x3 = xr.Dataset(
674     ...     {
675     ...         "temperature": (("y", "x"), 20 * np.random.rand(6).reshape(2, 3)),
676     ...         "precipitation": (("y", "x"), np.random.rand(6).reshape(2, 3)),
677     ...     },
678     ...     coords={"y": [2, 3], "x": [40, 50, 60]},
679     ... )
680 
681     >>> x1
682     <xarray.Dataset>
683     Dimensions:        (x: 3, y: 2)
684     Coordinates:
685       * y              (y) int64 0 1
686       * x              (x) int64 10 20 30
687     Data variables:
688         temperature    (y, x) float64 10.98 14.3 12.06 10.9 8.473 12.92
689         precipitation  (y, x) float64 0.4376 0.8918 0.9637 0.3834 0.7917 0.5289
690 
691     >>> x2
692     <xarray.Dataset>
693     Dimensions:        (x: 3, y: 2)
694     Coordinates:
695       * y              (y) int64 2 3
696       * x              (x) int64 10 20 30
697     Data variables:
698         temperature    (y, x) float64 11.36 18.51 1.421 1.743 0.4044 16.65
699         precipitation  (y, x) float64 0.7782 0.87 0.9786 0.7992 0.4615 0.7805
700 
701     >>> x3
702     <xarray.Dataset>
703     Dimensions:        (x: 3, y: 2)
704     Coordinates:
705       * y              (y) int64 2 3
706       * x              (x) int64 40 50 60
707     Data variables:
708         temperature    (y, x) float64 2.365 12.8 2.867 18.89 10.44 8.293
709         precipitation  (y, x) float64 0.2646 0.7742 0.4562 0.5684 0.01879 0.6176
710 
711     >>> xr.combine_by_coords([x2, x1])
712     <xarray.Dataset>
713     Dimensions:        (x: 3, y: 4)
714     Coordinates:
715       * y              (y) int64 0 1 2 3
716       * x              (x) int64 10 20 30
717     Data variables:
718         temperature    (y, x) float64 10.98 14.3 12.06 10.9 ... 1.743 0.4044 16.65
719         precipitation  (y, x) float64 0.4376 0.8918 0.9637 ... 0.7992 0.4615 0.7805
720 
721     >>> xr.combine_by_coords([x3, x1])
722     <xarray.Dataset>
723     Dimensions:        (x: 6, y: 4)
724     Coordinates:
725       * x              (x) int64 10 20 30 40 50 60
726       * y              (y) int64 0 1 2 3
727     Data variables:
728         temperature    (y, x) float64 10.98 14.3 12.06 nan ... nan 18.89 10.44 8.293
729         precipitation  (y, x) float64 0.4376 0.8918 0.9637 ... 0.5684 0.01879 0.6176
730 
731     >>> xr.combine_by_coords([x3, x1], join="override")
732     <xarray.Dataset>
733     Dimensions:        (x: 3, y: 4)
734     Coordinates:
735       * x              (x) int64 10 20 30
736       * y              (y) int64 0 1 2 3
737     Data variables:
738         temperature    (y, x) float64 10.98 14.3 12.06 10.9 ... 18.89 10.44 8.293
739         precipitation  (y, x) float64 0.4376 0.8918 0.9637 ... 0.5684 0.01879 0.6176
740 
741     >>> xr.combine_by_coords([x1, x2, x3])
742     <xarray.Dataset>
743     Dimensions:        (x: 6, y: 4)
744     Coordinates:
745       * x              (x) int64 10 20 30 40 50 60
746       * y              (y) int64 0 1 2 3
747     Data variables:
748         temperature    (y, x) float64 10.98 14.3 12.06 nan ... 18.89 10.44 8.293
749         precipitation  (y, x) float64 0.4376 0.8918 0.9637 ... 0.5684 0.01879 0.6176
750     """
751 
752     # Group by data vars
753     sorted_datasets = sorted(datasets, key=vars_as_keys)
754     grouped_by_vars = itertools.groupby(sorted_datasets, key=vars_as_keys)
755 
756     # Perform the multidimensional combine on each group of data variables
757     # before merging back together
758     concatenated_grouped_by_data_vars = []
759     for vars, datasets_with_same_vars in grouped_by_vars:
760         combined_ids, concat_dims = _infer_concat_order_from_coords(
761             list(datasets_with_same_vars)
762         )
763 
764         if fill_value is None:
765             # check that datasets form complete hypercube
766             _check_shape_tile_ids(combined_ids)
767         else:
768             # check only that all datasets have same dimension depth for these
769             # vars
770             _check_dimension_depth_tile_ids(combined_ids)
771 
772         # Concatenate along all of concat_dims one by one to create single ds
773         concatenated = _combine_nd(
774             combined_ids,
775             concat_dims=concat_dims,
776             data_vars=data_vars,
777             coords=coords,
778             compat=compat,
779             fill_value=fill_value,
780             join=join,
781             combine_attrs=combine_attrs,
782         )
783 
784         # Check the overall coordinates are monotonically increasing
785         for dim in concat_dims:
786             indexes = concatenated.indexes.get(dim)
787             if not (indexes.is_monotonic_increasing or indexes.is_monotonic_decreasing):
788                 raise ValueError(
789                     "Resulting object does not have monotonic"
790                     " global indexes along dimension {}".format(dim)
791                 )
792         concatenated_grouped_by_data_vars.append(concatenated)
793 
794     return merge(
795         concatenated_grouped_by_data_vars,
796         compat=compat,
797         fill_value=fill_value,
798         join=join,
799         combine_attrs=combine_attrs,
800     )

</code><intended_behavior>Modify these methods to interpret and apply the 'drop_conflicts' strategy within their respective 'combine_attrs' handling. The implementation should merge dataset attributes according to the specified 'combine_attrs' strategy, specifically adding logic to handle attribute conflicts by dropping conflicting attributes during dataset combination operations.</intended_behavior>, <file>xarray/core/concat.py</file>
<class>None</class>
<method>None</method>
<code>
1 from typing import (
2     TYPE_CHECKING,
3     Dict,
4     Hashable,
5     Iterable,
6     List,
7     Optional,
8     Set,
9     Tuple,
10     Union,
11     overload,
12 )
13 
14 import pandas as pd
15 
16 from . import dtypes, utils
17 from .alignment import align
18 from .duck_array_ops import lazy_array_equiv
19 from .merge import _VALID_COMPAT, merge_attrs, unique_variable
20 from .variable import IndexVariable, Variable, as_variable
21 from .variable import concat as concat_vars
22 
23 if TYPE_CHECKING:
24     from .dataarray import DataArray
25     from .dataset import Dataset
26 
27 
28 @overload
29 def concat(
30     objs: Iterable["Dataset"],
31     dim: Union[str, "DataArray", pd.Index],
32     data_vars: Union[str, List[str]] = "all",
33     coords: Union[str, List[str]] = "different",
34     compat: str = "equals",
35     positions: Optional[Iterable[int]] = None,
36     fill_value: object = dtypes.NA,
37     join: str = "outer",
38     combine_attrs: str = "override",
39 ) -> "Dataset":
40     ...
41 
42 
43 @overload
44 def concat(
45     objs: Iterable["DataArray"],
46     dim: Union[str, "DataArray", pd.Index],
47     data_vars: Union[str, List[str]] = "all",
48     coords: Union[str, List[str]] = "different",
49     compat: str = "equals",
50     positions: Optional[Iterable[int]] = None,
51     fill_value: object = dtypes.NA,
52     join: str = "outer",
53     combine_attrs: str = "override",
54 ) -> "DataArray":
55     ...
56 
57 
58 def concat(
59     objs,
60     dim,
61     data_vars="all",
62     coords="different",
63     compat="equals",
64     positions=None,
65     fill_value=dtypes.NA,
66     join="outer",
67     combine_attrs="override",
68 ):
69     """Concatenate xarray objects along a new or existing dimension.
70 
71     Parameters
72     ----------
73     objs : sequence of Dataset and DataArray
74         xarray objects to concatenate together. Each object is expected to
75         consist of variables and coordinates with matching shapes except for
76         along the concatenated dimension.
77     dim : str or DataArray or pandas.Index
78         Name of the dimension to concatenate along. This can either be a new
79         dimension name, in which case it is added along axis=0, or an existing
80         dimension name, in which case the location of the dimension is
81         unchanged. If dimension is provided as a DataArray or Index, its name
82         is used as the dimension to concatenate along and the values are added
83         as a coordinate.
84     data_vars : {"minimal", "different", "all"} or list of str, optional
85         These data variables will be concatenated together:
86           * "minimal": Only data variables in which the dimension already
87             appears are included.
88           * "different": Data variables which are not equal (ignoring
89             attributes) across all datasets are also concatenated (as well as
90             all for which dimension already appears). Beware: this option may
91             load the data payload of data variables into memory if they are not
92             already loaded.
93           * "all": All data variables will be concatenated.
94           * list of str: The listed data variables will be concatenated, in
95             addition to the "minimal" data variables.
96 
97         If objects are DataArrays, data_vars must be "all".
98     coords : {"minimal", "different", "all"} or list of str, optional
99         These coordinate variables will be concatenated together:
100           * "minimal": Only coordinates in which the dimension already appears
101             are included.
102           * "different": Coordinates which are not equal (ignoring attributes)
103             across all datasets are also concatenated (as well as all for which
104             dimension already appears). Beware: this option may load the data
105             payload of coordinate variables into memory if they are not already
106             loaded.
107           * "all": All coordinate variables will be concatenated, except
108             those corresponding to other dimensions.
109           * list of str: The listed coordinate variables will be concatenated,
110             in addition to the "minimal" coordinates.
111     compat : {"identical", "equals", "broadcast_equals", "no_conflicts", "override"}, optional
112         String indicating how to compare non-concatenated variables of the same name for
113         potential conflicts. This is passed down to merge.
114 
115         - "broadcast_equals": all values must be equal when variables are
116           broadcast against each other to ensure common dimensions.
117         - "equals": all values and dimensions must be the same.
118         - "identical": all values, dimensions and attributes must be the
119           same.
120         - "no_conflicts": only values which are not null in both datasets
121           must be equal. The returned dataset then contains the combination
122           of all non-null values.
123         - "override": skip comparing and pick variable from first dataset
124     positions : None or list of integer arrays, optional
125         List of integer arrays which specifies the integer positions to which
126         to assign each dataset along the concatenated dimension. If not
127         supplied, objects are concatenated in the provided order.
128     fill_value : scalar or dict-like, optional
129         Value to use for newly missing values. If a dict-like, maps
130         variable names to fill values. Use a data array's name to
131         refer to its values.
132     join : {"outer", "inner", "left", "right", "exact"}, optional
133         String indicating how to combine differing indexes
134         (excluding dim) in objects
135 
136         - "outer": use the union of object indexes
137         - "inner": use the intersection of object indexes
138         - "left": use indexes from the first object with each dimension
139         - "right": use indexes from the last object with each dimension
140         - "exact": instead of aligning, raise `ValueError` when indexes to be
141           aligned are not equal
142         - "override": if indexes are of same size, rewrite indexes to be
143           those of the first object with that dimension. Indexes for the same
144           dimension must have the same size in all objects.
145     combine_attrs : {"drop", "identical", "no_conflicts", "override"}, \
146                     default: "override"
147         String indicating how to combine attrs of the objects being merged:
148 
149         - "drop": empty attrs on returned Dataset.
150         - "identical": all attrs must be the same on every object.
151         - "no_conflicts": attrs from all objects are combined, any that have
152           the same name must also have the same value.
153         - "override": skip comparing and copy attrs from the first dataset to
154           the result.
155 
156     Returns
157     -------
158     concatenated : type of objs
159 
160     See also
161     --------
162     merge
163 
164     Examples
165     --------
166     >>> da = xr.DataArray(
167     ...     np.arange(6).reshape(2, 3), [("x", ["a", "b"]), ("y", [10, 20, 30])]
168     ... )
169     >>> da
170     <xarray.DataArray (x: 2, y: 3)>
171     array([[0, 1, 2],
172            [3, 4, 5]])
173     Coordinates:
174       * x        (x) <U1 'a' 'b'
175       * y        (y) int64 10 20 30
176 
177     >>> xr.concat([da.isel(y=slice(0, 1)), da.isel(y=slice(1, None))], dim="y")
178     <xarray.DataArray (x: 2, y: 3)>
179     array([[0, 1, 2],
180            [3, 4, 5]])
181     Coordinates:
182       * x        (x) <U1 'a' 'b'
183       * y        (y) int64 10 20 30
184 
185     >>> xr.concat([da.isel(x=0), da.isel(x=1)], "x")
186     <xarray.DataArray (x: 2, y: 3)>
187     array([[0, 1, 2],
188            [3, 4, 5]])
189     Coordinates:
190       * x        (x) <U1 'a' 'b'
191       * y        (y) int64 10 20 30
192 
193     >>> xr.concat([da.isel(x=0), da.isel(x=1)], "new_dim")
194     <xarray.DataArray (new_dim: 2, y: 3)>
195     array([[0, 1, 2],
196            [3, 4, 5]])
197     Coordinates:
198         x        (new_dim) <U1 'a' 'b'
199       * y        (y) int64 10 20 30
200     Dimensions without coordinates: new_dim
201 
202     >>> xr.concat([da.isel(x=0), da.isel(x=1)], pd.Index([-90, -100], name="new_dim"))
203     <xarray.DataArray (new_dim: 2, y: 3)>
204     array([[0, 1, 2],
205            [3, 4, 5]])
206     Coordinates:
207         x        (new_dim) <U1 'a' 'b'
208       * y        (y) int64 10 20 30
209       * new_dim  (new_dim) int64 -90 -100
210     """
211     # TODO: add ignore_index arguments copied from pandas.concat
212     # TODO: support concatenating scalar coordinates even if the concatenated
213     # dimension already exists
214     from .dataarray import DataArray
215     from .dataset import Dataset
216 
217     try:
218         first_obj, objs = utils.peek_at(objs)
219     except StopIteration:
220         raise ValueError("must supply at least one object to concatenate")
221 
222     if compat not in _VALID_COMPAT:
223         raise ValueError(
224             "compat=%r invalid: must be 'broadcast_equals', 'equals', 'identical', 'no_conflicts' or 'override'"
225             % compat
226         )
227 
228     if isinstance(first_obj, DataArray):
229         f = _dataarray_concat
230     elif isinstance(first_obj, Dataset):
231         f = _dataset_concat
232     else:
233         raise TypeError(
234             "can only concatenate xarray Dataset and DataArray "
235             "objects, got %s" % type(first_obj)
236         )
237     return f(
238         objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs
239     )
240 
241 
242 def _calc_concat_dim_coord(dim):
243     """
244     Infer the dimension name and 1d coordinate variable (if appropriate)
245     for concatenating along the new dimension.
246     """
247     from .dataarray import DataArray
248 
249     if isinstance(dim, str):
250         coord = None
251     elif not isinstance(dim, (DataArray, Variable)):
252         dim_name = getattr(dim, "name", None)
253         if dim_name is None:
254             dim_name = "concat_dim"
255         coord = IndexVariable(dim_name, dim)
256         dim = dim_name
257     elif not isinstance(dim, DataArray):
258         coord = as_variable(dim).to_index_variable()
259         (dim,) = coord.dims
260     else:
261         coord = dim
262         (dim,) = coord.dims
263     return dim, coord
264 
265 
266 def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):
267     """
268     Determine which dataset variables need to be concatenated in the result,
269     """
270     # Return values
271     concat_over = set()
272     equals = {}
273 
274     if dim in dim_names:
275         concat_over_existing_dim = True
276         concat_over.add(dim)
277     else:
278         concat_over_existing_dim = False
279 
280     concat_dim_lengths = []
281     for ds in datasets:
282         if concat_over_existing_dim:
283             if dim not in ds.dims:
284                 if dim in ds:
285                     ds = ds.set_coords(dim)
286         concat_over.update(k for k, v in ds.variables.items() if dim in v.dims)
287         concat_dim_lengths.append(ds.dims.get(dim, 1))
288 
289     def process_subset_opt(opt, subset):
290         if isinstance(opt, str):
291             if opt == "different":
292                 if compat == "override":
293                     raise ValueError(
294                         "Cannot specify both %s='different' and compat='override'."
295                         % subset
296                     )
297                 # all nonindexes that are not the same in each dataset
298                 for k in getattr(datasets[0], subset):
299                     if k not in concat_over:
300                         equals[k] = None
301 
302                         variables = []
303                         for ds in datasets:
304                             if k in ds.variables:
305                                 variables.append(ds.variables[k])
306 
307                         if len(variables) == 1:
308                             # coords="different" doesn't make sense when only one object
309                             # contains a particular variable.
310                             break
311                         elif len(variables) != len(datasets) and opt == "different":
312                             raise ValueError(
313                                 f"{k!r} not present in all datasets and coords='different'. "
314                                 f"Either add {k!r} to datasets where it is missing or "
315                                 "specify coords='minimal'."
316                             )
317 
318                         # first check without comparing values i.e. no computes
319                         for var in variables[1:]:
320                             equals[k] = getattr(variables[0], compat)(
321                                 var, equiv=lazy_array_equiv
322                             )
323                             if equals[k] is not True:
324                                 # exit early if we know these are not equal or that
325                                 # equality cannot be determined i.e. one or all of
326                                 # the variables wraps a numpy array
327                                 break
328 
329                         if equals[k] is False:
330                             concat_over.add(k)
331 
332                         elif equals[k] is None:
333                             # Compare the variable of all datasets vs. the one
334                             # of the first dataset. Perform the minimum amount of
335                             # loads in order to avoid multiple loads from disk
336                             # while keeping the RAM footprint low.
337                             v_lhs = datasets[0].variables[k].load()
338                             # We'll need to know later on if variables are equal.
339                             computed = []
340                             for ds_rhs in datasets[1:]:
341                                 v_rhs = ds_rhs.variables[k].compute()
342                                 computed.append(v_rhs)
343                                 if not getattr(v_lhs, compat)(v_rhs):
344                                     concat_over.add(k)
345                                     equals[k] = False
346                                     # computed variables are not to be re-computed
347                                     # again in the future
348                                     for ds, v in zip(datasets[1:], computed):
349                                         ds.variables[k].data = v.data
350                                     break
351                             else:
352                                 equals[k] = True
353 
354             elif opt == "all":
355                 concat_over.update(
356                     set(getattr(datasets[0], subset)) - set(datasets[0].dims)
357                 )
358             elif opt == "minimal":
359                 pass
360             else:
361                 raise ValueError(f"unexpected value for {subset}: {opt}")
362         else:
363             invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]
364             if invalid_vars:
365                 if subset == "coords":
366                     raise ValueError(
367                         "some variables in coords are not coordinates on "
368                         "the first dataset: %s" % (invalid_vars,)
369                     )
370                 else:
371                     raise ValueError(
372                         "some variables in data_vars are not data variables "
373                         "on the first dataset: %s" % (invalid_vars,)
374                     )
375             concat_over.update(opt)
376 
377     process_subset_opt(data_vars, "data_vars")
378     process_subset_opt(coords, "coords")
379     return concat_over, equals, concat_dim_lengths
380 
381 
382 # determine dimensional coordinate names and a dict mapping name to DataArray
383 def _parse_datasets(
384     datasets: Iterable["Dataset"],
385 ) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, int], Set[Hashable], Set[Hashable]]:
386 
387     dims: Set[Hashable] = set()
388     all_coord_names: Set[Hashable] = set()
389     data_vars: Set[Hashable] = set()  # list of data_vars
390     dim_coords: Dict[Hashable, Variable] = {}  # maps dim name to variable
391     dims_sizes: Dict[Hashable, int] = {}  # shared dimension sizes to expand variables
392 
393     for ds in datasets:
394         dims_sizes.update(ds.dims)
395         all_coord_names.update(ds.coords)
396         data_vars.update(ds.data_vars)
397 
398         # preserves ordering of dimensions
399         for dim in ds.dims:
400             if dim in dims:
401                 continue
402 
403             if dim not in dim_coords:
404                 dim_coords[dim] = ds.coords[dim].variable
405         dims = dims | set(ds.dims)
406 
407     return dim_coords, dims_sizes, all_coord_names, data_vars
408 
409 
410 def _dataset_concat(
411     datasets: List["Dataset"],
412     dim: Union[str, "DataArray", pd.Index],
413     data_vars: Union[str, List[str]],
414     coords: Union[str, List[str]],
415     compat: str,
416     positions: Optional[Iterable[int]],
417     fill_value: object = dtypes.NA,
418     join: str = "outer",
419     combine_attrs: str = "override",
420 ) -> "Dataset":
421     """
422     Concatenate a sequence of datasets along a new or existing dimension
423     """
424     from .dataset import Dataset
425 
426     dim, coord = _calc_concat_dim_coord(dim)
427     # Make sure we're working on a copy (we'll be loading variables)
428     datasets = [ds.copy() for ds in datasets]
429     datasets = list(
430         align(*datasets, join=join, copy=False, exclude=[dim], fill_value=fill_value)
431     )
432 
433     dim_coords, dims_sizes, coord_names, data_names = _parse_datasets(datasets)
434     dim_names = set(dim_coords)
435     unlabeled_dims = dim_names - coord_names
436 
437     both_data_and_coords = coord_names & data_names
438     if both_data_and_coords:
439         raise ValueError(
440             "%r is a coordinate in some datasets but not others." % both_data_and_coords
441         )
442     # we don't want the concat dimension in the result dataset yet
443     dim_coords.pop(dim, None)
444     dims_sizes.pop(dim, None)
445 
446     # case where concat dimension is a coordinate or data_var but not a dimension
447     if (dim in coord_names or dim in data_names) and dim not in dim_names:
448         datasets = [ds.expand_dims(dim) for ds in datasets]
449 
450     # determine which variables to concatentate
451     concat_over, equals, concat_dim_lengths = _calc_concat_over(
452         datasets, dim, dim_names, data_vars, coords, compat
453     )
454 
455     # determine which variables to merge, and then merge them according to compat
456     variables_to_merge = (coord_names | data_names) - concat_over - dim_names
457 
458     result_vars = {}
459     if variables_to_merge:
460         to_merge: Dict[Hashable, List[Variable]] = {
461             var: [] for var in variables_to_merge
462         }
463 
464         for ds in datasets:
465             for var in variables_to_merge:
466                 if var in ds:
467                     to_merge[var].append(ds.variables[var])
468 
469         for var in variables_to_merge:
470             result_vars[var] = unique_variable(
471                 var, to_merge[var], compat=compat, equals=equals.get(var, None)
472             )
473     else:
474         result_vars = {}
475     result_vars.update(dim_coords)
476 
477     # assign attrs and encoding from first dataset
478     result_attrs = merge_attrs([ds.attrs for ds in datasets], combine_attrs)
479     result_encoding = datasets[0].encoding
480 
481     # check that global attributes are fixed across all datasets if necessary
482     for ds in datasets[1:]:
483         if compat == "identical" and not utils.dict_equiv(ds.attrs, result_attrs):
484             raise ValueError("Dataset global attributes not equal.")
485 
486     # we've already verified everything is consistent; now, calculate
487     # shared dimension sizes so we can expand the necessary variables
488     def ensure_common_dims(vars):
489         # ensure each variable with the given name shares the same
490         # dimensions and the same shape for all of them except along the
491         # concat dimension
492         common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))
493         if dim not in common_dims:
494             common_dims = (dim,) + common_dims
495         for var, dim_len in zip(vars, concat_dim_lengths):
496             if var.dims != common_dims:
497                 common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)
498                 var = var.set_dims(common_dims, common_shape)
499             yield var
500 
501     # stack up each variable to fill-out the dataset (in order)
502     # n.b. this loop preserves variable order, needed for groupby.
503     for k in datasets[0].variables:
504         if k in concat_over:
505             try:
506                 vars = ensure_common_dims([ds[k].variable for ds in datasets])
507             except KeyError:
508                 raise ValueError("%r is not present in all datasets." % k)
509             combined = concat_vars(vars, dim, positions)
510             assert isinstance(combined, Variable)
511             result_vars[k] = combined
512         elif k in result_vars:
513             # preserves original variable order
514             result_vars[k] = result_vars.pop(k)
515 
516     result = Dataset(result_vars, attrs=result_attrs)
517     absent_coord_names = coord_names - set(result.variables)
518     if absent_coord_names:
519         raise ValueError(
520             "Variables %r are coordinates in some datasets but not others."
521             % absent_coord_names
522         )
523     result = result.set_coords(coord_names)
524     result.encoding = result_encoding
525 
526     result = result.drop_vars(unlabeled_dims, errors="ignore")
527 
528     if coord is not None:
529         # add concat dimension last to ensure that its in the final Dataset
530         result[coord.name] = coord
531 
532     return result
533 
534 
535 def _dataarray_concat(
536     arrays: Iterable["DataArray"],
537     dim: Union[str, "DataArray", pd.Index],
538     data_vars: Union[str, List[str]],
539     coords: Union[str, List[str]],
540     compat: str,
541     positions: Optional[Iterable[int]],
542     fill_value: object = dtypes.NA,
543     join: str = "outer",
544     combine_attrs: str = "override",
545 ) -> "DataArray":
546     arrays = list(arrays)
547 
548     if data_vars != "all":
549         raise ValueError(
550             "data_vars is not a valid argument when concatenating DataArray objects"
551         )
552 
553     datasets = []
554     for n, arr in enumerate(arrays):
555         if n == 0:
556             name = arr.name
557         elif name != arr.name:
558             if compat == "identical":
559                 raise ValueError("array names not identical")
560             else:
561                 arr = arr.rename(name)
562         datasets.append(arr._to_temp_dataset())
563 
564     ds = _dataset_concat(
565         datasets,
566         dim,
567         data_vars,
568         coords,
569         compat,
570         positions,
571         fill_value=fill_value,
572         join=join,
573         combine_attrs="drop",
574     )
575 
576     merged_attrs = merge_attrs([da.attrs for da in arrays], combine_attrs)
577 
578     result = arrays[0]._from_temp_dataset(ds, name)
579     result.attrs = merged_attrs
580 
581     return result

</code><intended_behavior>Ensure these methods support the 'drop_conflicts' option within their 'combine_attrs' parameter processing. The new logic should assess attributes from the datasets being concatenated, excluding attributes with conflicting values from the concatenated result to adhere to the 'drop_conflicts' rule.</intended_behavior>]
2025-10-17 15:48:03.713 | INFO     | Search completed. Bug locations: [<file>xarray/core/merge.py</file>
<class>None</class>
<method>merge_attrs</method>
<code>
495 def merge_attrs(variable_attrs, combine_attrs):
496     """Combine attributes from different variables according to combine_attrs"""
497     if not variable_attrs:
498         # no attributes to merge
499         return None
500 
501     if combine_attrs == "drop":
502         return {}
503     elif combine_attrs == "override":
504         return dict(variable_attrs[0])
505     elif combine_attrs == "no_conflicts":
506         result = dict(variable_attrs[0])
507         for attrs in variable_attrs[1:]:
508             try:
509                 result = compat_dict_union(result, attrs)
510             except ValueError:
511                 raise MergeError(
512                     "combine_attrs='no_conflicts', but some values are not "
513                     "the same. Merging %s with %s" % (str(result), str(attrs))
514                 )
515         return result
516     elif combine_attrs == "identical":
517         result = dict(variable_attrs[0])
518         for attrs in variable_attrs[1:]:
519             if not dict_equiv(result, attrs):
520                 raise MergeError(
521                     "combine_attrs='identical', but attrs differ. First is %s "
522                     ", other is %s." % (str(result), str(attrs))
523                 )
524         return result
525     else:
526         raise ValueError("Unrecognised value for combine_attrs=%s" % combine_attrs)

</code><intended_behavior>Update this method to handle the 'drop_conflicts' option for 'combine_attrs'. This involves writing logic to compare attributes from different datasets and discard any attribute that has conflicting values across the datasets being merged, thus ensuring that only consistent attributes or those without any conflicts are retained.</intended_behavior>, <file>xarray/core/combine.py</file>
<class>None</class>
<method>None</method>
<code>
1 import itertools
2 from collections import Counter
3 
4 import pandas as pd
5 
6 from . import dtypes
7 from .concat import concat
8 from .dataarray import DataArray
9 from .dataset import Dataset
10 from .merge import merge
11 
12 
13 def _infer_concat_order_from_positions(datasets):
14     combined_ids = dict(_infer_tile_ids_from_nested_list(datasets, ()))
15     return combined_ids
16 
17 
18 def _infer_tile_ids_from_nested_list(entry, current_pos):
19     """
20     Given a list of lists (of lists...) of objects, returns a iterator
21     which returns a tuple containing the index of each object in the nested
22     list structure as the key, and the object. This can then be called by the
23     dict constructor to create a dictionary of the objects organised by their
24     position in the original nested list.
25 
26     Recursively traverses the given structure, while keeping track of the
27     current position. Should work for any type of object which isn't a list.
28 
29     Parameters
30     ----------
31     entry : list[list[obj, obj, ...], ...]
32         List of lists of arbitrary depth, containing objects in the order
33         they are to be concatenated.
34 
35     Returns
36     -------
37     combined_tile_ids : dict[tuple(int, ...), obj]
38     """
39 
40     if isinstance(entry, list):
41         for i, item in enumerate(entry):
42             yield from _infer_tile_ids_from_nested_list(item, current_pos + (i,))
43     else:
44         yield current_pos, entry
45 
46 
47 def _infer_concat_order_from_coords(datasets):
48 
49     concat_dims = []
50     tile_ids = [() for ds in datasets]
51 
52     # All datasets have same variables because they've been grouped as such
53     ds0 = datasets[0]
54     for dim in ds0.dims:
55 
56         # Check if dim is a coordinate dimension
57         if dim in ds0:
58 
59             # Need to read coordinate values to do ordering
60             indexes = [ds.indexes.get(dim) for ds in datasets]
61             if any(index is None for index in indexes):
62                 raise ValueError(
63                     "Every dimension needs a coordinate for "
64                     "inferring concatenation order"
65                 )
66 
67             # If dimension coordinate values are same on every dataset then
68             # should be leaving this dimension alone (it's just a "bystander")
69             if not all(index.equals(indexes[0]) for index in indexes[1:]):
70 
71                 # Infer order datasets should be arranged in along this dim
72                 concat_dims.append(dim)
73 
74                 if all(index.is_monotonic_increasing for index in indexes):
75                     ascending = True
76                 elif all(index.is_monotonic_decreasing for index in indexes):
77                     ascending = False
78                 else:
79                     raise ValueError(
80                         "Coordinate variable {} is neither "
81                         "monotonically increasing nor "
82                         "monotonically decreasing on all datasets".format(dim)
83                     )
84 
85                 # Assume that any two datasets whose coord along dim starts
86                 # with the same value have the same coord values throughout.
87                 if any(index.size == 0 for index in indexes):
88                     raise ValueError("Cannot handle size zero dimensions")
89                 first_items = pd.Index([index[0] for index in indexes])
90 
91                 # Sort datasets along dim
92                 # We want rank but with identical elements given identical
93                 # position indices - they should be concatenated along another
94                 # dimension, not along this one
95                 series = first_items.to_series()
96                 rank = series.rank(
97                     method="dense", ascending=ascending, numeric_only=False
98                 )
99                 order = rank.astype(int).values - 1
100 
101                 # Append positions along extra dimension to structure which
102                 # encodes the multi-dimensional concatentation order
103                 tile_ids = [
104                     tile_id + (position,) for tile_id, position in zip(tile_ids, order)
105                 ]
106 
107     if len(datasets) > 1 and not concat_dims:
108         raise ValueError(
109             "Could not find any dimension coordinates to use to "
110             "order the datasets for concatenation"
111         )
112 
113     combined_ids = dict(zip(tile_ids, datasets))
114 
115     return combined_ids, concat_dims
116 
117 
118 def _check_dimension_depth_tile_ids(combined_tile_ids):
119     """
120     Check all tuples are the same length, i.e. check that all lists are
121     nested to the same depth.
122     """
123     tile_ids = combined_tile_ids.keys()
124     nesting_depths = [len(tile_id) for tile_id in tile_ids]
125     if not nesting_depths:
126         nesting_depths = [0]
127     if not set(nesting_depths) == {nesting_depths[0]}:
128         raise ValueError(
129             "The supplied objects do not form a hypercube because"
130             " sub-lists do not have consistent depths"
131         )
132     # return these just to be reused in _check_shape_tile_ids
133     return tile_ids, nesting_depths
134 
135 
136 def _check_shape_tile_ids(combined_tile_ids):
137     """Check all lists along one dimension are same length."""
138     tile_ids, nesting_depths = _check_dimension_depth_tile_ids(combined_tile_ids)
139     for dim in range(nesting_depths[0]):
140         indices_along_dim = [tile_id[dim] for tile_id in tile_ids]
141         occurrences = Counter(indices_along_dim)
142         if len(set(occurrences.values())) != 1:
143             raise ValueError(
144                 "The supplied objects do not form a hypercube "
145                 "because sub-lists do not have consistent "
146                 "lengths along dimension" + str(dim)
147             )
148 
149 
150 def _combine_nd(
151     combined_ids,
152     concat_dims,
153     data_vars="all",
154     coords="different",
155     compat="no_conflicts",
156     fill_value=dtypes.NA,
157     join="outer",
158     combine_attrs="drop",
159 ):
160     """
161     Combines an N-dimensional structure of datasets into one by applying a
162     series of either concat and merge operations along each dimension.
163 
164     No checks are performed on the consistency of the datasets, concat_dims or
165     tile_IDs, because it is assumed that this has already been done.
166 
167     Parameters
168     ----------
169     combined_ids : Dict[Tuple[int, ...]], xarray.Dataset]
170         Structure containing all datasets to be concatenated with "tile_IDs" as
171         keys, which specify position within the desired final combined result.
172     concat_dims : sequence of str
173         The dimensions along which the datasets should be concatenated. Must be
174         in order, and the length must match the length of the tuples used as
175         keys in combined_ids. If the string is a dimension name then concat
176         along that dimension, if it is None then merge.
177 
178     Returns
179     -------
180     combined_ds : xarray.Dataset
181     """
182 
183     example_tile_id = next(iter(combined_ids.keys()))
184 
185     n_dims = len(example_tile_id)
186     if len(concat_dims) != n_dims:
187         raise ValueError(
188             "concat_dims has length {} but the datasets "
189             "passed are nested in a {}-dimensional structure".format(
190                 len(concat_dims), n_dims
191             )
192         )
193 
194     # Each iteration of this loop reduces the length of the tile_ids tuples
195     # by one. It always combines along the first dimension, removing the first
196     # element of the tuple
197     for concat_dim in concat_dims:
198         combined_ids = _combine_all_along_first_dim(
199             combined_ids,
200             dim=concat_dim,
201             data_vars=data_vars,
202             coords=coords,
203             compat=compat,
204             fill_value=fill_value,
205             join=join,
206             combine_attrs=combine_attrs,
207         )
208     (combined_ds,) = combined_ids.values()
209     return combined_ds
210 
211 
212 def _combine_all_along_first_dim(
213     combined_ids,
214     dim,
215     data_vars,
216     coords,
217     compat,
218     fill_value=dtypes.NA,
219     join="outer",
220     combine_attrs="drop",
221 ):
222 
223     # Group into lines of datasets which must be combined along dim
224     # need to sort by _new_tile_id first for groupby to work
225     # TODO: is the sorted need?
226     combined_ids = dict(sorted(combined_ids.items(), key=_new_tile_id))
227     grouped = itertools.groupby(combined_ids.items(), key=_new_tile_id)
228 
229     # Combine all of these datasets along dim
230     new_combined_ids = {}
231     for new_id, group in grouped:
232         combined_ids = dict(sorted(group))
233         datasets = combined_ids.values()
234         new_combined_ids[new_id] = _combine_1d(
235             datasets, dim, compat, data_vars, coords, fill_value, join, combine_attrs
236         )
237     return new_combined_ids
238 
239 
240 def _combine_1d(
241     datasets,
242     concat_dim,
243     compat="no_conflicts",
244     data_vars="all",
245     coords="different",
246     fill_value=dtypes.NA,
247     join="outer",
248     combine_attrs="drop",
249 ):
250     """
251     Applies either concat or merge to 1D list of datasets depending on value
252     of concat_dim
253     """
254 
255     if concat_dim is not None:
256         try:
257             combined = concat(
258                 datasets,
259                 dim=concat_dim,
260                 data_vars=data_vars,
261                 coords=coords,
262                 compat=compat,
263                 fill_value=fill_value,
264                 join=join,
265                 combine_attrs=combine_attrs,
266             )
267         except ValueError as err:
268             if "encountered unexpected variable" in str(err):
269                 raise ValueError(
270                     "These objects cannot be combined using only "
271                     "xarray.combine_nested, instead either use "
272                     "xarray.combine_by_coords, or do it manually "
273                     "with xarray.concat, xarray.merge and "
274                     "xarray.align"
275                 )
276             else:
277                 raise
278     else:
279         combined = merge(
280             datasets,
281             compat=compat,
282             fill_value=fill_value,
283             join=join,
284             combine_attrs=combine_attrs,
285         )
286 
287     return combined
288 
289 
290 def _new_tile_id(single_id_ds_pair):
291     tile_id, ds = single_id_ds_pair
292     return tile_id[1:]
293 
294 
295 def _nested_combine(
296     datasets,
297     concat_dims,
298     compat,
299     data_vars,
300     coords,
301     ids,
302     fill_value=dtypes.NA,
303     join="outer",
304     combine_attrs="drop",
305 ):
306 
307     if len(datasets) == 0:
308         return Dataset()
309 
310     # Arrange datasets for concatenation
311     # Use information from the shape of the user input
312     if not ids:
313         # Determine tile_IDs by structure of input in N-D
314         # (i.e. ordering in list-of-lists)
315         combined_ids = _infer_concat_order_from_positions(datasets)
316     else:
317         # Already sorted so just use the ids already passed
318         combined_ids = dict(zip(ids, datasets))
319 
320     # Check that the inferred shape is combinable
321     _check_shape_tile_ids(combined_ids)
322 
323     # Apply series of concatenate or merge operations along each dimension
324     combined = _combine_nd(
325         combined_ids,
326         concat_dims,
327         compat=compat,
328         data_vars=data_vars,
329         coords=coords,
330         fill_value=fill_value,
331         join=join,
332         combine_attrs=combine_attrs,
333     )
334     return combined
335 
336 
337 def combine_nested(
338     datasets,
339     concat_dim,
340     compat="no_conflicts",
341     data_vars="all",
342     coords="different",
343     fill_value=dtypes.NA,
344     join="outer",
345     combine_attrs="drop",
346 ):
347     """
348     Explicitly combine an N-dimensional grid of datasets into one by using a
349     succession of concat and merge operations along each dimension of the grid.
350 
351     Does not sort the supplied datasets under any circumstances, so the
352     datasets must be passed in the order you wish them to be concatenated. It
353     does align coordinates, but different variables on datasets can cause it to
354     fail under some scenarios. In complex cases, you may need to clean up your
355     data and use concat/merge explicitly.
356 
357     To concatenate along multiple dimensions the datasets must be passed as a
358     nested list-of-lists, with a depth equal to the length of ``concat_dims``.
359     ``manual_combine`` will concatenate along the top-level list first.
360 
361     Useful for combining datasets from a set of nested directories, or for
362     collecting the output of a simulation parallelized along multiple
363     dimensions.
364 
365     Parameters
366     ----------
367     datasets : list or nested list of Dataset
368         Dataset objects to combine.
369         If concatenation or merging along more than one dimension is desired,
370         then datasets must be supplied in a nested list-of-lists.
371     concat_dim : str, or list of str, DataArray, Index or None
372         Dimensions along which to concatenate variables, as used by
373         :py:func:`xarray.concat`.
374         Set ``concat_dim=[..., None, ...]`` explicitly to disable concatenation
375         and merge instead along a particular dimension.
376         The position of ``None`` in the list specifies the dimension of the
377         nested-list input along which to merge.
378         Must be the same length as the depth of the list passed to
379         ``datasets``.
380     compat : {"identical", "equals", "broadcast_equals", \
381               "no_conflicts", "override"}, optional
382         String indicating how to compare variables of the same name for
383         potential merge conflicts:
384 
385         - "broadcast_equals": all values must be equal when variables are
386           broadcast against each other to ensure common dimensions.
387         - "equals": all values and dimensions must be the same.
388         - "identical": all values, dimensions and attributes must be the
389           same.
390         - "no_conflicts": only values which are not null in both datasets
391           must be equal. The returned dataset then contains the combination
392           of all non-null values.
393         - "override": skip comparing and pick variable from first dataset
394     data_vars : {"minimal", "different", "all" or list of str}, optional
395         Details are in the documentation of concat
396     coords : {"minimal", "different", "all" or list of str}, optional
397         Details are in the documentation of concat
398     fill_value : scalar or dict-like, optional
399         Value to use for newly missing values. If a dict-like, maps
400         variable names to fill values. Use a data array's name to
401         refer to its values.
402     join : {"outer", "inner", "left", "right", "exact"}, optional
403         String indicating how to combine differing indexes
404         (excluding concat_dim) in objects
405 
406         - "outer": use the union of object indexes
407         - "inner": use the intersection of object indexes
408         - "left": use indexes from the first object with each dimension
409         - "right": use indexes from the last object with each dimension
410         - "exact": instead of aligning, raise `ValueError` when indexes to be
411           aligned are not equal
412         - "override": if indexes are of same size, rewrite indexes to be
413           those of the first object with that dimension. Indexes for the same
414           dimension must have the same size in all objects.
415     combine_attrs : {"drop", "identical", "no_conflicts", "override"}, \
416                     default: "drop"
417         String indicating how to combine attrs of the objects being merged:
418 
419         - "drop": empty attrs on returned Dataset.
420         - "identical": all attrs must be the same on every object.
421         - "no_conflicts": attrs from all objects are combined, any that have
422           the same name must also have the same value.
423         - "override": skip comparing and copy attrs from the first dataset to
424           the result.
425 
426     Returns
427     -------
428     combined : xarray.Dataset
429 
430     Examples
431     --------
432 
433     A common task is collecting data from a parallelized simulation in which
434     each process wrote out to a separate file. A domain which was decomposed
435     into 4 parts, 2 each along both the x and y axes, requires organising the
436     datasets into a doubly-nested list, e.g:
437 
438     >>> x1y1 = xr.Dataset(
439     ...     {
440     ...         "temperature": (("x", "y"), np.random.randn(2, 2)),
441     ...         "precipitation": (("x", "y"), np.random.randn(2, 2)),
442     ...     }
443     ... )
444     >>> x1y1
445     <xarray.Dataset>
446     Dimensions:        (x: 2, y: 2)
447     Dimensions without coordinates: x, y
448     Data variables:
449         temperature    (x, y) float64 1.764 0.4002 0.9787 2.241
450         precipitation  (x, y) float64 1.868 -0.9773 0.9501 -0.1514
451     >>> x1y2 = xr.Dataset(
452     ...     {
453     ...         "temperature": (("x", "y"), np.random.randn(2, 2)),
454     ...         "precipitation": (("x", "y"), np.random.randn(2, 2)),
455     ...     }
456     ... )
457     >>> x2y1 = xr.Dataset(
458     ...     {
459     ...         "temperature": (("x", "y"), np.random.randn(2, 2)),
460     ...         "precipitation": (("x", "y"), np.random.randn(2, 2)),
461     ...     }
462     ... )
463     >>> x2y2 = xr.Dataset(
464     ...     {
465     ...         "temperature": (("x", "y"), np.random.randn(2, 2)),
466     ...         "precipitation": (("x", "y"), np.random.randn(2, 2)),
467     ...     }
468     ... )
469 
470 
471     >>> ds_grid = [[x1y1, x1y2], [x2y1, x2y2]]
472     >>> combined = xr.combine_nested(ds_grid, concat_dim=["x", "y"])
473     >>> combined
474     <xarray.Dataset>
475     Dimensions:        (x: 4, y: 4)
476     Dimensions without coordinates: x, y
477     Data variables:
478         temperature    (x, y) float64 1.764 0.4002 -0.1032 ... 0.04576 -0.1872
479         precipitation  (x, y) float64 1.868 -0.9773 0.761 ... -0.7422 0.1549 0.3782
480 
481     ``manual_combine`` can also be used to explicitly merge datasets with
482     different variables. For example if we have 4 datasets, which are divided
483     along two times, and contain two different variables, we can pass ``None``
484     to ``concat_dim`` to specify the dimension of the nested list over which
485     we wish to use ``merge`` instead of ``concat``:
486 
487     >>> t1temp = xr.Dataset({"temperature": ("t", np.random.randn(5))})
488     >>> t1temp
489     <xarray.Dataset>
490     Dimensions:      (t: 5)
491     Dimensions without coordinates: t
492     Data variables:
493         temperature  (t) float64 -0.8878 -1.981 -0.3479 0.1563 1.23
494 
495     >>> t1precip = xr.Dataset({"precipitation": ("t", np.random.randn(5))})
496     >>> t1precip
497     <xarray.Dataset>
498     Dimensions:        (t: 5)
499     Dimensions without coordinates: t
500     Data variables:
501         precipitation  (t) float64 1.202 -0.3873 -0.3023 -1.049 -1.42
502 
503     >>> t2temp = xr.Dataset({"temperature": ("t", np.random.randn(5))})
504     >>> t2precip = xr.Dataset({"precipitation": ("t", np.random.randn(5))})
505 
506 
507     >>> ds_grid = [[t1temp, t1precip], [t2temp, t2precip]]
508     >>> combined = xr.combine_nested(ds_grid, concat_dim=["t", None])
509     >>> combined
510     <xarray.Dataset>
511     Dimensions:        (t: 10)
512     Dimensions without coordinates: t
513     Data variables:
514         temperature    (t) float64 -0.8878 -1.981 -0.3479 ... -0.5097 -0.4381 -1.253
515         precipitation  (t) float64 1.202 -0.3873 -0.3023 ... -0.2127 -0.8955 0.3869
516 
517     See also
518     --------
519     concat
520     merge
521     """
522     if isinstance(concat_dim, (str, DataArray)) or concat_dim is None:
523         concat_dim = [concat_dim]
524 
525     # The IDs argument tells _manual_combine that datasets aren't yet sorted
526     return _nested_combine(
527         datasets,
528         concat_dims=concat_dim,
529         compat=compat,
530         data_vars=data_vars,
531         coords=coords,
532         ids=False,
533         fill_value=fill_value,
534         join=join,
535         combine_attrs=combine_attrs,
536     )
537 
538 
539 def vars_as_keys(ds):
540     return tuple(sorted(ds))
541 
542 
543 def combine_by_coords(
544     datasets,
545     compat="no_conflicts",
546     data_vars="all",
547     coords="different",
548     fill_value=dtypes.NA,
549     join="outer",
550     combine_attrs="no_conflicts",
551 ):
552     """
553     Attempt to auto-magically combine the given datasets into one by using
554     dimension coordinates.
555 
556     This method attempts to combine a group of datasets along any number of
557     dimensions into a single entity by inspecting coords and metadata and using
558     a combination of concat and merge.
559 
560     Will attempt to order the datasets such that the values in their dimension
561     coordinates are monotonic along all dimensions. If it cannot determine the
562     order in which to concatenate the datasets, it will raise a ValueError.
563     Non-coordinate dimensions will be ignored, as will any coordinate
564     dimensions which do not vary between each dataset.
565 
566     Aligns coordinates, but different variables on datasets can cause it
567     to fail under some scenarios. In complex cases, you may need to clean up
568     your data and use concat/merge explicitly (also see `manual_combine`).
569 
570     Works well if, for example, you have N years of data and M data variables,
571     and each combination of a distinct time period and set of data variables is
572     saved as its own dataset. Also useful for if you have a simulation which is
573     parallelized in multiple dimensions, but has global coordinates saved in
574     each file specifying the positions of points within the global domain.
575 
576     Parameters
577     ----------
578     datasets : sequence of xarray.Dataset
579         Dataset objects to combine.
580     compat : {"identical", "equals", "broadcast_equals", "no_conflicts", "override"}, optional
581         String indicating how to compare variables of the same name for
582         potential conflicts:
583 
584         - "broadcast_equals": all values must be equal when variables are
585           broadcast against each other to ensure common dimensions.
586         - "equals": all values and dimensions must be the same.
587         - "identical": all values, dimensions and attributes must be the
588           same.
589         - "no_conflicts": only values which are not null in both datasets
590           must be equal. The returned dataset then contains the combination
591           of all non-null values.
592         - "override": skip comparing and pick variable from first dataset
593     data_vars : {"minimal", "different", "all" or list of str}, optional
594         These data variables will be concatenated together:
595 
596         * "minimal": Only data variables in which the dimension already
597           appears are included.
598         * "different": Data variables which are not equal (ignoring
599           attributes) across all datasets are also concatenated (as well as
600           all for which dimension already appears). Beware: this option may
601           load the data payload of data variables into memory if they are not
602           already loaded.
603         * "all": All data variables will be concatenated.
604         * list of str: The listed data variables will be concatenated, in
605           addition to the "minimal" data variables.
606 
607         If objects are DataArrays, `data_vars` must be "all".
608     coords : {"minimal", "different", "all"} or list of str, optional
609         As per the "data_vars" kwarg, but for coordinate variables.
610     fill_value : scalar or dict-like, optional
611         Value to use for newly missing values. If a dict-like, maps
612         variable names to fill values. Use a data array's name to
613         refer to its values. If None, raises a ValueError if
614         the passed Datasets do not create a complete hypercube.
615     join : {"outer", "inner", "left", "right", "exact"}, optional
616         String indicating how to combine differing indexes
617         (excluding concat_dim) in objects
618 
619         - "outer": use the union of object indexes
620         - "inner": use the intersection of object indexes
621         - "left": use indexes from the first object with each dimension
622         - "right": use indexes from the last object with each dimension
623         - "exact": instead of aligning, raise `ValueError` when indexes to be
624           aligned are not equal
625         - "override": if indexes are of same size, rewrite indexes to be
626           those of the first object with that dimension. Indexes for the same
627           dimension must have the same size in all objects.
628     combine_attrs : {"drop", "identical", "no_conflicts", "override"}, \
629                     default: "drop"
630         String indicating how to combine attrs of the objects being merged:
631 
632         - "drop": empty attrs on returned Dataset.
633         - "identical": all attrs must be the same on every object.
634         - "no_conflicts": attrs from all objects are combined, any that have
635           the same name must also have the same value.
636         - "override": skip comparing and copy attrs from the first dataset to
637           the result.
638 
639     Returns
640     -------
641     combined : xarray.Dataset
642 
643     See also
644     --------
645     concat
646     merge
647     combine_nested
648 
649     Examples
650     --------
651 
652     Combining two datasets using their common dimension coordinates. Notice
653     they are concatenated based on the values in their dimension coordinates,
654     not on their position in the list passed to `combine_by_coords`.
655 
656     >>> import numpy as np
657     >>> import xarray as xr
658 
659     >>> x1 = xr.Dataset(
660     ...     {
661     ...         "temperature": (("y", "x"), 20 * np.random.rand(6).reshape(2, 3)),
662     ...         "precipitation": (("y", "x"), np.random.rand(6).reshape(2, 3)),
663     ...     },
664     ...     coords={"y": [0, 1], "x": [10, 20, 30]},
665     ... )
666     >>> x2 = xr.Dataset(
667     ...     {
668     ...         "temperature": (("y", "x"), 20 * np.random.rand(6).reshape(2, 3)),
669     ...         "precipitation": (("y", "x"), np.random.rand(6).reshape(2, 3)),
670     ...     },
671     ...     coords={"y": [2, 3], "x": [10, 20, 30]},
672     ... )
673     >>> x3 = xr.Dataset(
674     ...     {
675     ...         "temperature": (("y", "x"), 20 * np.random.rand(6).reshape(2, 3)),
676     ...         "precipitation": (("y", "x"), np.random.rand(6).reshape(2, 3)),
677     ...     },
678     ...     coords={"y": [2, 3], "x": [40, 50, 60]},
679     ... )
680 
681     >>> x1
682     <xarray.Dataset>
683     Dimensions:        (x: 3, y: 2)
684     Coordinates:
685       * y              (y) int64 0 1
686       * x              (x) int64 10 20 30
687     Data variables:
688         temperature    (y, x) float64 10.98 14.3 12.06 10.9 8.473 12.92
689         precipitation  (y, x) float64 0.4376 0.8918 0.9637 0.3834 0.7917 0.5289
690 
691     >>> x2
692     <xarray.Dataset>
693     Dimensions:        (x: 3, y: 2)
694     Coordinates:
695       * y              (y) int64 2 3
696       * x              (x) int64 10 20 30
697     Data variables:
698         temperature    (y, x) float64 11.36 18.51 1.421 1.743 0.4044 16.65
699         precipitation  (y, x) float64 0.7782 0.87 0.9786 0.7992 0.4615 0.7805
700 
701     >>> x3
702     <xarray.Dataset>
703     Dimensions:        (x: 3, y: 2)
704     Coordinates:
705       * y              (y) int64 2 3
706       * x              (x) int64 40 50 60
707     Data variables:
708         temperature    (y, x) float64 2.365 12.8 2.867 18.89 10.44 8.293
709         precipitation  (y, x) float64 0.2646 0.7742 0.4562 0.5684 0.01879 0.6176
710 
711     >>> xr.combine_by_coords([x2, x1])
712     <xarray.Dataset>
713     Dimensions:        (x: 3, y: 4)
714     Coordinates:
715       * y              (y) int64 0 1 2 3
716       * x              (x) int64 10 20 30
717     Data variables:
718         temperature    (y, x) float64 10.98 14.3 12.06 10.9 ... 1.743 0.4044 16.65
719         precipitation  (y, x) float64 0.4376 0.8918 0.9637 ... 0.7992 0.4615 0.7805
720 
721     >>> xr.combine_by_coords([x3, x1])
722     <xarray.Dataset>
723     Dimensions:        (x: 6, y: 4)
724     Coordinates:
725       * x              (x) int64 10 20 30 40 50 60
726       * y              (y) int64 0 1 2 3
727     Data variables:
728         temperature    (y, x) float64 10.98 14.3 12.06 nan ... nan 18.89 10.44 8.293
729         precipitation  (y, x) float64 0.4376 0.8918 0.9637 ... 0.5684 0.01879 0.6176
730 
731     >>> xr.combine_by_coords([x3, x1], join="override")
732     <xarray.Dataset>
733     Dimensions:        (x: 3, y: 4)
734     Coordinates:
735       * x              (x) int64 10 20 30
736       * y              (y) int64 0 1 2 3
737     Data variables:
738         temperature    (y, x) float64 10.98 14.3 12.06 10.9 ... 18.89 10.44 8.293
739         precipitation  (y, x) float64 0.4376 0.8918 0.9637 ... 0.5684 0.01879 0.6176
740 
741     >>> xr.combine_by_coords([x1, x2, x3])
742     <xarray.Dataset>
743     Dimensions:        (x: 6, y: 4)
744     Coordinates:
745       * x              (x) int64 10 20 30 40 50 60
746       * y              (y) int64 0 1 2 3
747     Data variables:
748         temperature    (y, x) float64 10.98 14.3 12.06 nan ... 18.89 10.44 8.293
749         precipitation  (y, x) float64 0.4376 0.8918 0.9637 ... 0.5684 0.01879 0.6176
750     """
751 
752     # Group by data vars
753     sorted_datasets = sorted(datasets, key=vars_as_keys)
754     grouped_by_vars = itertools.groupby(sorted_datasets, key=vars_as_keys)
755 
756     # Perform the multidimensional combine on each group of data variables
757     # before merging back together
758     concatenated_grouped_by_data_vars = []
759     for vars, datasets_with_same_vars in grouped_by_vars:
760         combined_ids, concat_dims = _infer_concat_order_from_coords(
761             list(datasets_with_same_vars)
762         )
763 
764         if fill_value is None:
765             # check that datasets form complete hypercube
766             _check_shape_tile_ids(combined_ids)
767         else:
768             # check only that all datasets have same dimension depth for these
769             # vars
770             _check_dimension_depth_tile_ids(combined_ids)
771 
772         # Concatenate along all of concat_dims one by one to create single ds
773         concatenated = _combine_nd(
774             combined_ids,
775             concat_dims=concat_dims,
776             data_vars=data_vars,
777             coords=coords,
778             compat=compat,
779             fill_value=fill_value,
780             join=join,
781             combine_attrs=combine_attrs,
782         )
783 
784         # Check the overall coordinates are monotonically increasing
785         for dim in concat_dims:
786             indexes = concatenated.indexes.get(dim)
787             if not (indexes.is_monotonic_increasing or indexes.is_monotonic_decreasing):
788                 raise ValueError(
789                     "Resulting object does not have monotonic"
790                     " global indexes along dimension {}".format(dim)
791                 )
792         concatenated_grouped_by_data_vars.append(concatenated)
793 
794     return merge(
795         concatenated_grouped_by_data_vars,
796         compat=compat,
797         fill_value=fill_value,
798         join=join,
799         combine_attrs=combine_attrs,
800     )

</code><intended_behavior>Modify these methods to interpret and apply the 'drop_conflicts' strategy within their respective 'combine_attrs' handling. The implementation should merge dataset attributes according to the specified 'combine_attrs' strategy, specifically adding logic to handle attribute conflicts by dropping conflicting attributes during dataset combination operations.</intended_behavior>, <file>xarray/core/concat.py</file>
<class>None</class>
<method>None</method>
<code>
1 from typing import (
2     TYPE_CHECKING,
3     Dict,
4     Hashable,
5     Iterable,
6     List,
7     Optional,
8     Set,
9     Tuple,
10     Union,
11     overload,
12 )
13 
14 import pandas as pd
15 
16 from . import dtypes, utils
17 from .alignment import align
18 from .duck_array_ops import lazy_array_equiv
19 from .merge import _VALID_COMPAT, merge_attrs, unique_variable
20 from .variable import IndexVariable, Variable, as_variable
21 from .variable import concat as concat_vars
22 
23 if TYPE_CHECKING:
24     from .dataarray import DataArray
25     from .dataset import Dataset
26 
27 
28 @overload
29 def concat(
30     objs: Iterable["Dataset"],
31     dim: Union[str, "DataArray", pd.Index],
32     data_vars: Union[str, List[str]] = "all",
33     coords: Union[str, List[str]] = "different",
34     compat: str = "equals",
35     positions: Optional[Iterable[int]] = None,
36     fill_value: object = dtypes.NA,
37     join: str = "outer",
38     combine_attrs: str = "override",
39 ) -> "Dataset":
40     ...
41 
42 
43 @overload
44 def concat(
45     objs: Iterable["DataArray"],
46     dim: Union[str, "DataArray", pd.Index],
47     data_vars: Union[str, List[str]] = "all",
48     coords: Union[str, List[str]] = "different",
49     compat: str = "equals",
50     positions: Optional[Iterable[int]] = None,
51     fill_value: object = dtypes.NA,
52     join: str = "outer",
53     combine_attrs: str = "override",
54 ) -> "DataArray":
55     ...
56 
57 
58 def concat(
59     objs,
60     dim,
61     data_vars="all",
62     coords="different",
63     compat="equals",
64     positions=None,
65     fill_value=dtypes.NA,
66     join="outer",
67     combine_attrs="override",
68 ):
69     """Concatenate xarray objects along a new or existing dimension.
70 
71     Parameters
72     ----------
73     objs : sequence of Dataset and DataArray
74         xarray objects to concatenate together. Each object is expected to
75         consist of variables and coordinates with matching shapes except for
76         along the concatenated dimension.
77     dim : str or DataArray or pandas.Index
78         Name of the dimension to concatenate along. This can either be a new
79         dimension name, in which case it is added along axis=0, or an existing
80         dimension name, in which case the location of the dimension is
81         unchanged. If dimension is provided as a DataArray or Index, its name
82         is used as the dimension to concatenate along and the values are added
83         as a coordinate.
84     data_vars : {"minimal", "different", "all"} or list of str, optional
85         These data variables will be concatenated together:
86           * "minimal": Only data variables in which the dimension already
87             appears are included.
88           * "different": Data variables which are not equal (ignoring
89             attributes) across all datasets are also concatenated (as well as
90             all for which dimension already appears). Beware: this option may
91             load the data payload of data variables into memory if they are not
92             already loaded.
93           * "all": All data variables will be concatenated.
94           * list of str: The listed data variables will be concatenated, in
95             addition to the "minimal" data variables.
96 
97         If objects are DataArrays, data_vars must be "all".
98     coords : {"minimal", "different", "all"} or list of str, optional
99         These coordinate variables will be concatenated together:
100           * "minimal": Only coordinates in which the dimension already appears
101             are included.
102           * "different": Coordinates which are not equal (ignoring attributes)
103             across all datasets are also concatenated (as well as all for which
104             dimension already appears). Beware: this option may load the data
105             payload of coordinate variables into memory if they are not already
106             loaded.
107           * "all": All coordinate variables will be concatenated, except
108             those corresponding to other dimensions.
109           * list of str: The listed coordinate variables will be concatenated,
110             in addition to the "minimal" coordinates.
111     compat : {"identical", "equals", "broadcast_equals", "no_conflicts", "override"}, optional
112         String indicating how to compare non-concatenated variables of the same name for
113         potential conflicts. This is passed down to merge.
114 
115         - "broadcast_equals": all values must be equal when variables are
116           broadcast against each other to ensure common dimensions.
117         - "equals": all values and dimensions must be the same.
118         - "identical": all values, dimensions and attributes must be the
119           same.
120         - "no_conflicts": only values which are not null in both datasets
121           must be equal. The returned dataset then contains the combination
122           of all non-null values.
123         - "override": skip comparing and pick variable from first dataset
124     positions : None or list of integer arrays, optional
125         List of integer arrays which specifies the integer positions to which
126         to assign each dataset along the concatenated dimension. If not
127         supplied, objects are concatenated in the provided order.
128     fill_value : scalar or dict-like, optional
129         Value to use for newly missing values. If a dict-like, maps
130         variable names to fill values. Use a data array's name to
131         refer to its values.
132     join : {"outer", "inner", "left", "right", "exact"}, optional
133         String indicating how to combine differing indexes
134         (excluding dim) in objects
135 
136         - "outer": use the union of object indexes
137         - "inner": use the intersection of object indexes
138         - "left": use indexes from the first object with each dimension
139         - "right": use indexes from the last object with each dimension
140         - "exact": instead of aligning, raise `ValueError` when indexes to be
141           aligned are not equal
142         - "override": if indexes are of same size, rewrite indexes to be
143           those of the first object with that dimension. Indexes for the same
144           dimension must have the same size in all objects.
145     combine_attrs : {"drop", "identical", "no_conflicts", "override"}, \
146                     default: "override"
147         String indicating how to combine attrs of the objects being merged:
148 
149         - "drop": empty attrs on returned Dataset.
150         - "identical": all attrs must be the same on every object.
151         - "no_conflicts": attrs from all objects are combined, any that have
152           the same name must also have the same value.
153         - "override": skip comparing and copy attrs from the first dataset to
154           the result.
155 
156     Returns
157     -------
158     concatenated : type of objs
159 
160     See also
161     --------
162     merge
163 
164     Examples
165     --------
166     >>> da = xr.DataArray(
167     ...     np.arange(6).reshape(2, 3), [("x", ["a", "b"]), ("y", [10, 20, 30])]
168     ... )
169     >>> da
170     <xarray.DataArray (x: 2, y: 3)>
171     array([[0, 1, 2],
172            [3, 4, 5]])
173     Coordinates:
174       * x        (x) <U1 'a' 'b'
175       * y        (y) int64 10 20 30
176 
177     >>> xr.concat([da.isel(y=slice(0, 1)), da.isel(y=slice(1, None))], dim="y")
178     <xarray.DataArray (x: 2, y: 3)>
179     array([[0, 1, 2],
180            [3, 4, 5]])
181     Coordinates:
182       * x        (x) <U1 'a' 'b'
183       * y        (y) int64 10 20 30
184 
185     >>> xr.concat([da.isel(x=0), da.isel(x=1)], "x")
186     <xarray.DataArray (x: 2, y: 3)>
187     array([[0, 1, 2],
188            [3, 4, 5]])
189     Coordinates:
190       * x        (x) <U1 'a' 'b'
191       * y        (y) int64 10 20 30
192 
193     >>> xr.concat([da.isel(x=0), da.isel(x=1)], "new_dim")
194     <xarray.DataArray (new_dim: 2, y: 3)>
195     array([[0, 1, 2],
196            [3, 4, 5]])
197     Coordinates:
198         x        (new_dim) <U1 'a' 'b'
199       * y        (y) int64 10 20 30
200     Dimensions without coordinates: new_dim
201 
202     >>> xr.concat([da.isel(x=0), da.isel(x=1)], pd.Index([-90, -100], name="new_dim"))
203     <xarray.DataArray (new_dim: 2, y: 3)>
204     array([[0, 1, 2],
205            [3, 4, 5]])
206     Coordinates:
207         x        (new_dim) <U1 'a' 'b'
208       * y        (y) int64 10 20 30
209       * new_dim  (new_dim) int64 -90 -100
210     """
211     # TODO: add ignore_index arguments copied from pandas.concat
212     # TODO: support concatenating scalar coordinates even if the concatenated
213     # dimension already exists
214     from .dataarray import DataArray
215     from .dataset import Dataset
216 
217     try:
218         first_obj, objs = utils.peek_at(objs)
219     except StopIteration:
220         raise ValueError("must supply at least one object to concatenate")
221 
222     if compat not in _VALID_COMPAT:
223         raise ValueError(
224             "compat=%r invalid: must be 'broadcast_equals', 'equals', 'identical', 'no_conflicts' or 'override'"
225             % compat
226         )
227 
228     if isinstance(first_obj, DataArray):
229         f = _dataarray_concat
230     elif isinstance(first_obj, Dataset):
231         f = _dataset_concat
232     else:
233         raise TypeError(
234             "can only concatenate xarray Dataset and DataArray "
235             "objects, got %s" % type(first_obj)
236         )
237     return f(
238         objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs
239     )
240 
241 
242 def _calc_concat_dim_coord(dim):
243     """
244     Infer the dimension name and 1d coordinate variable (if appropriate)
245     for concatenating along the new dimension.
246     """
247     from .dataarray import DataArray
248 
249     if isinstance(dim, str):
250         coord = None
251     elif not isinstance(dim, (DataArray, Variable)):
252         dim_name = getattr(dim, "name", None)
253         if dim_name is None:
254             dim_name = "concat_dim"
255         coord = IndexVariable(dim_name, dim)
256         dim = dim_name
257     elif not isinstance(dim, DataArray):
258         coord = as_variable(dim).to_index_variable()
259         (dim,) = coord.dims
260     else:
261         coord = dim
262         (dim,) = coord.dims
263     return dim, coord
264 
265 
266 def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):
267     """
268     Determine which dataset variables need to be concatenated in the result,
269     """
270     # Return values
271     concat_over = set()
272     equals = {}
273 
274     if dim in dim_names:
275         concat_over_existing_dim = True
276         concat_over.add(dim)
277     else:
278         concat_over_existing_dim = False
279 
280     concat_dim_lengths = []
281     for ds in datasets:
282         if concat_over_existing_dim:
283             if dim not in ds.dims:
284                 if dim in ds:
285                     ds = ds.set_coords(dim)
286         concat_over.update(k for k, v in ds.variables.items() if dim in v.dims)
287         concat_dim_lengths.append(ds.dims.get(dim, 1))
288 
289     def process_subset_opt(opt, subset):
290         if isinstance(opt, str):
291             if opt == "different":
292                 if compat == "override":
293                     raise ValueError(
294                         "Cannot specify both %s='different' and compat='override'."
295                         % subset
296                     )
297                 # all nonindexes that are not the same in each dataset
298                 for k in getattr(datasets[0], subset):
299                     if k not in concat_over:
300                         equals[k] = None
301 
302                         variables = []
303                         for ds in datasets:
304                             if k in ds.variables:
305                                 variables.append(ds.variables[k])
306 
307                         if len(variables) == 1:
308                             # coords="different" doesn't make sense when only one object
309                             # contains a particular variable.
310                             break
311                         elif len(variables) != len(datasets) and opt == "different":
312                             raise ValueError(
313                                 f"{k!r} not present in all datasets and coords='different'. "
314                                 f"Either add {k!r} to datasets where it is missing or "
315                                 "specify coords='minimal'."
316                             )
317 
318                         # first check without comparing values i.e. no computes
319                         for var in variables[1:]:
320                             equals[k] = getattr(variables[0], compat)(
321                                 var, equiv=lazy_array_equiv
322                             )
323                             if equals[k] is not True:
324                                 # exit early if we know these are not equal or that
325                                 # equality cannot be determined i.e. one or all of
326                                 # the variables wraps a numpy array
327                                 break
328 
329                         if equals[k] is False:
330                             concat_over.add(k)
331 
332                         elif equals[k] is None:
333                             # Compare the variable of all datasets vs. the one
334                             # of the first dataset. Perform the minimum amount of
335                             # loads in order to avoid multiple loads from disk
336                             # while keeping the RAM footprint low.
337                             v_lhs = datasets[0].variables[k].load()
338                             # We'll need to know later on if variables are equal.
339                             computed = []
340                             for ds_rhs in datasets[1:]:
341                                 v_rhs = ds_rhs.variables[k].compute()
342                                 computed.append(v_rhs)
343                                 if not getattr(v_lhs, compat)(v_rhs):
344                                     concat_over.add(k)
345                                     equals[k] = False
346                                     # computed variables are not to be re-computed
347                                     # again in the future
348                                     for ds, v in zip(datasets[1:], computed):
349                                         ds.variables[k].data = v.data
350                                     break
351                             else:
352                                 equals[k] = True
353 
354             elif opt == "all":
355                 concat_over.update(
356                     set(getattr(datasets[0], subset)) - set(datasets[0].dims)
357                 )
358             elif opt == "minimal":
359                 pass
360             else:
361                 raise ValueError(f"unexpected value for {subset}: {opt}")
362         else:
363             invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]
364             if invalid_vars:
365                 if subset == "coords":
366                     raise ValueError(
367                         "some variables in coords are not coordinates on "
368                         "the first dataset: %s" % (invalid_vars,)
369                     )
370                 else:
371                     raise ValueError(
372                         "some variables in data_vars are not data variables "
373                         "on the first dataset: %s" % (invalid_vars,)
374                     )
375             concat_over.update(opt)
376 
377     process_subset_opt(data_vars, "data_vars")
378     process_subset_opt(coords, "coords")
379     return concat_over, equals, concat_dim_lengths
380 
381 
382 # determine dimensional coordinate names and a dict mapping name to DataArray
383 def _parse_datasets(
384     datasets: Iterable["Dataset"],
385 ) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, int], Set[Hashable], Set[Hashable]]:
386 
387     dims: Set[Hashable] = set()
388     all_coord_names: Set[Hashable] = set()
389     data_vars: Set[Hashable] = set()  # list of data_vars
390     dim_coords: Dict[Hashable, Variable] = {}  # maps dim name to variable
391     dims_sizes: Dict[Hashable, int] = {}  # shared dimension sizes to expand variables
392 
393     for ds in datasets:
394         dims_sizes.update(ds.dims)
395         all_coord_names.update(ds.coords)
396         data_vars.update(ds.data_vars)
397 
398         # preserves ordering of dimensions
399         for dim in ds.dims:
400             if dim in dims:
401                 continue
402 
403             if dim not in dim_coords:
404                 dim_coords[dim] = ds.coords[dim].variable
405         dims = dims | set(ds.dims)
406 
407     return dim_coords, dims_sizes, all_coord_names, data_vars
408 
409 
410 def _dataset_concat(
411     datasets: List["Dataset"],
412     dim: Union[str, "DataArray", pd.Index],
413     data_vars: Union[str, List[str]],
414     coords: Union[str, List[str]],
415     compat: str,
416     positions: Optional[Iterable[int]],
417     fill_value: object = dtypes.NA,
418     join: str = "outer",
419     combine_attrs: str = "override",
420 ) -> "Dataset":
421     """
422     Concatenate a sequence of datasets along a new or existing dimension
423     """
424     from .dataset import Dataset
425 
426     dim, coord = _calc_concat_dim_coord(dim)
427     # Make sure we're working on a copy (we'll be loading variables)
428     datasets = [ds.copy() for ds in datasets]
429     datasets = list(
430         align(*datasets, join=join, copy=False, exclude=[dim], fill_value=fill_value)
431     )
432 
433     dim_coords, dims_sizes, coord_names, data_names = _parse_datasets(datasets)
434     dim_names = set(dim_coords)
435     unlabeled_dims = dim_names - coord_names
436 
437     both_data_and_coords = coord_names & data_names
438     if both_data_and_coords:
439         raise ValueError(
440             "%r is a coordinate in some datasets but not others." % both_data_and_coords
441         )
442     # we don't want the concat dimension in the result dataset yet
443     dim_coords.pop(dim, None)
444     dims_sizes.pop(dim, None)
445 
446     # case where concat dimension is a coordinate or data_var but not a dimension
447     if (dim in coord_names or dim in data_names) and dim not in dim_names:
448         datasets = [ds.expand_dims(dim) for ds in datasets]
449 
450     # determine which variables to concatentate
451     concat_over, equals, concat_dim_lengths = _calc_concat_over(
452         datasets, dim, dim_names, data_vars, coords, compat
453     )
454 
455     # determine which variables to merge, and then merge them according to compat
456     variables_to_merge = (coord_names | data_names) - concat_over - dim_names
457 
458     result_vars = {}
459     if variables_to_merge:
460         to_merge: Dict[Hashable, List[Variable]] = {
461             var: [] for var in variables_to_merge
462         }
463 
464         for ds in datasets:
465             for var in variables_to_merge:
466                 if var in ds:
467                     to_merge[var].append(ds.variables[var])
468 
469         for var in variables_to_merge:
470             result_vars[var] = unique_variable(
471                 var, to_merge[var], compat=compat, equals=equals.get(var, None)
472             )
473     else:
474         result_vars = {}
475     result_vars.update(dim_coords)
476 
477     # assign attrs and encoding from first dataset
478     result_attrs = merge_attrs([ds.attrs for ds in datasets], combine_attrs)
479     result_encoding = datasets[0].encoding
480 
481     # check that global attributes are fixed across all datasets if necessary
482     for ds in datasets[1:]:
483         if compat == "identical" and not utils.dict_equiv(ds.attrs, result_attrs):
484             raise ValueError("Dataset global attributes not equal.")
485 
486     # we've already verified everything is consistent; now, calculate
487     # shared dimension sizes so we can expand the necessary variables
488     def ensure_common_dims(vars):
489         # ensure each variable with the given name shares the same
490         # dimensions and the same shape for all of them except along the
491         # concat dimension
492         common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))
493         if dim not in common_dims:
494             common_dims = (dim,) + common_dims
495         for var, dim_len in zip(vars, concat_dim_lengths):
496             if var.dims != common_dims:
497                 common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)
498                 var = var.set_dims(common_dims, common_shape)
499             yield var
500 
501     # stack up each variable to fill-out the dataset (in order)
502     # n.b. this loop preserves variable order, needed for groupby.
503     for k in datasets[0].variables:
504         if k in concat_over:
505             try:
506                 vars = ensure_common_dims([ds[k].variable for ds in datasets])
507             except KeyError:
508                 raise ValueError("%r is not present in all datasets." % k)
509             combined = concat_vars(vars, dim, positions)
510             assert isinstance(combined, Variable)
511             result_vars[k] = combined
512         elif k in result_vars:
513             # preserves original variable order
514             result_vars[k] = result_vars.pop(k)
515 
516     result = Dataset(result_vars, attrs=result_attrs)
517     absent_coord_names = coord_names - set(result.variables)
518     if absent_coord_names:
519         raise ValueError(
520             "Variables %r are coordinates in some datasets but not others."
521             % absent_coord_names
522         )
523     result = result.set_coords(coord_names)
524     result.encoding = result_encoding
525 
526     result = result.drop_vars(unlabeled_dims, errors="ignore")
527 
528     if coord is not None:
529         # add concat dimension last to ensure that its in the final Dataset
530         result[coord.name] = coord
531 
532     return result
533 
534 
535 def _dataarray_concat(
536     arrays: Iterable["DataArray"],
537     dim: Union[str, "DataArray", pd.Index],
538     data_vars: Union[str, List[str]],
539     coords: Union[str, List[str]],
540     compat: str,
541     positions: Optional[Iterable[int]],
542     fill_value: object = dtypes.NA,
543     join: str = "outer",
544     combine_attrs: str = "override",
545 ) -> "DataArray":
546     arrays = list(arrays)
547 
548     if data_vars != "all":
549         raise ValueError(
550             "data_vars is not a valid argument when concatenating DataArray objects"
551         )
552 
553     datasets = []
554     for n, arr in enumerate(arrays):
555         if n == 0:
556             name = arr.name
557         elif name != arr.name:
558             if compat == "identical":
559                 raise ValueError("array names not identical")
560             else:
561                 arr = arr.rename(name)
562         datasets.append(arr._to_temp_dataset())
563 
564     ds = _dataset_concat(
565         datasets,
566         dim,
567         data_vars,
568         coords,
569         compat,
570         positions,
571         fill_value=fill_value,
572         join=join,
573         combine_attrs="drop",
574     )
575 
576     merged_attrs = merge_attrs([da.attrs for da in arrays], combine_attrs)
577 
578     result = arrays[0]._from_temp_dataset(ds, name)
579     result.attrs = merged_attrs
580 
581     return result

</code><intended_behavior>Ensure these methods support the 'drop_conflicts' option within their 'combine_attrs' parameter processing. The new logic should assess attributes from the datasets being concatenated, excluding attributes with conflicting values from the concatenated result to adhere to the 'drop_conflicts' rule.</intended_behavior>]
2025-10-17 15:48:03.713 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-17 15:48:03.713 | INFO     | Search-only: skipping patch selection
2025-10-17 15:48:03.791 | INFO     | Task pydata__xarray-4827 completed successfully.
2025-10-17 15:48:03.792 | INFO     | No patch generated. You can try running ACR again.
