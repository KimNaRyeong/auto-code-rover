[
    {
        "rel_file_path": "xarray/core/merge.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/merge.py",
        "start": 495,
        "end": 526,
        "class_name": null,
        "method_name": "merge_attrs",
        "code": "495 def merge_attrs(variable_attrs, combine_attrs):\n496     \"\"\"Combine attributes from different variables according to combine_attrs\"\"\"\n497     if not variable_attrs:\n498         # no attributes to merge\n499         return None\n500 \n501     if combine_attrs == \"drop\":\n502         return {}\n503     elif combine_attrs == \"override\":\n504         return dict(variable_attrs[0])\n505     elif combine_attrs == \"no_conflicts\":\n506         result = dict(variable_attrs[0])\n507         for attrs in variable_attrs[1:]:\n508             try:\n509                 result = compat_dict_union(result, attrs)\n510             except ValueError:\n511                 raise MergeError(\n512                     \"combine_attrs='no_conflicts', but some values are not \"\n513                     \"the same. Merging %s with %s\" % (str(result), str(attrs))\n514                 )\n515         return result\n516     elif combine_attrs == \"identical\":\n517         result = dict(variable_attrs[0])\n518         for attrs in variable_attrs[1:]:\n519             if not dict_equiv(result, attrs):\n520                 raise MergeError(\n521                     \"combine_attrs='identical', but attrs differ. First is %s \"\n522                     \", other is %s.\" % (str(result), str(attrs))\n523                 )\n524         return result\n525     else:\n526         raise ValueError(\"Unrecognised value for combine_attrs=%s\" % combine_attrs)\n",
        "intended_behavior": "Update this method to handle the 'drop_conflicts' option for 'combine_attrs'. This involves writing logic to compare attributes from different datasets and discard any attribute that has conflicting values across the datasets being merged, thus ensuring that only consistent attributes or those without any conflicts are retained."
    },
    {
        "rel_file_path": "xarray/core/combine.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/combine.py",
        "start": 1,
        "end": 800,
        "class_name": null,
        "method_name": null,
        "code": "1 import itertools\n2 from collections import Counter\n3 \n4 import pandas as pd\n5 \n6 from . import dtypes\n7 from .concat import concat\n8 from .dataarray import DataArray\n9 from .dataset import Dataset\n10 from .merge import merge\n11 \n12 \n13 def _infer_concat_order_from_positions(datasets):\n14     combined_ids = dict(_infer_tile_ids_from_nested_list(datasets, ()))\n15     return combined_ids\n16 \n17 \n18 def _infer_tile_ids_from_nested_list(entry, current_pos):\n19     \"\"\"\n20     Given a list of lists (of lists...) of objects, returns a iterator\n21     which returns a tuple containing the index of each object in the nested\n22     list structure as the key, and the object. This can then be called by the\n23     dict constructor to create a dictionary of the objects organised by their\n24     position in the original nested list.\n25 \n26     Recursively traverses the given structure, while keeping track of the\n27     current position. Should work for any type of object which isn't a list.\n28 \n29     Parameters\n30     ----------\n31     entry : list[list[obj, obj, ...], ...]\n32         List of lists of arbitrary depth, containing objects in the order\n33         they are to be concatenated.\n34 \n35     Returns\n36     -------\n37     combined_tile_ids : dict[tuple(int, ...), obj]\n38     \"\"\"\n39 \n40     if isinstance(entry, list):\n41         for i, item in enumerate(entry):\n42             yield from _infer_tile_ids_from_nested_list(item, current_pos + (i,))\n43     else:\n44         yield current_pos, entry\n45 \n46 \n47 def _infer_concat_order_from_coords(datasets):\n48 \n49     concat_dims = []\n50     tile_ids = [() for ds in datasets]\n51 \n52     # All datasets have same variables because they've been grouped as such\n53     ds0 = datasets[0]\n54     for dim in ds0.dims:\n55 \n56         # Check if dim is a coordinate dimension\n57         if dim in ds0:\n58 \n59             # Need to read coordinate values to do ordering\n60             indexes = [ds.indexes.get(dim) for ds in datasets]\n61             if any(index is None for index in indexes):\n62                 raise ValueError(\n63                     \"Every dimension needs a coordinate for \"\n64                     \"inferring concatenation order\"\n65                 )\n66 \n67             # If dimension coordinate values are same on every dataset then\n68             # should be leaving this dimension alone (it's just a \"bystander\")\n69             if not all(index.equals(indexes[0]) for index in indexes[1:]):\n70 \n71                 # Infer order datasets should be arranged in along this dim\n72                 concat_dims.append(dim)\n73 \n74                 if all(index.is_monotonic_increasing for index in indexes):\n75                     ascending = True\n76                 elif all(index.is_monotonic_decreasing for index in indexes):\n77                     ascending = False\n78                 else:\n79                     raise ValueError(\n80                         \"Coordinate variable {} is neither \"\n81                         \"monotonically increasing nor \"\n82                         \"monotonically decreasing on all datasets\".format(dim)\n83                     )\n84 \n85                 # Assume that any two datasets whose coord along dim starts\n86                 # with the same value have the same coord values throughout.\n87                 if any(index.size == 0 for index in indexes):\n88                     raise ValueError(\"Cannot handle size zero dimensions\")\n89                 first_items = pd.Index([index[0] for index in indexes])\n90 \n91                 # Sort datasets along dim\n92                 # We want rank but with identical elements given identical\n93                 # position indices - they should be concatenated along another\n94                 # dimension, not along this one\n95                 series = first_items.to_series()\n96                 rank = series.rank(\n97                     method=\"dense\", ascending=ascending, numeric_only=False\n98                 )\n99                 order = rank.astype(int).values - 1\n100 \n101                 # Append positions along extra dimension to structure which\n102                 # encodes the multi-dimensional concatentation order\n103                 tile_ids = [\n104                     tile_id + (position,) for tile_id, position in zip(tile_ids, order)\n105                 ]\n106 \n107     if len(datasets) > 1 and not concat_dims:\n108         raise ValueError(\n109             \"Could not find any dimension coordinates to use to \"\n110             \"order the datasets for concatenation\"\n111         )\n112 \n113     combined_ids = dict(zip(tile_ids, datasets))\n114 \n115     return combined_ids, concat_dims\n116 \n117 \n118 def _check_dimension_depth_tile_ids(combined_tile_ids):\n119     \"\"\"\n120     Check all tuples are the same length, i.e. check that all lists are\n121     nested to the same depth.\n122     \"\"\"\n123     tile_ids = combined_tile_ids.keys()\n124     nesting_depths = [len(tile_id) for tile_id in tile_ids]\n125     if not nesting_depths:\n126         nesting_depths = [0]\n127     if not set(nesting_depths) == {nesting_depths[0]}:\n128         raise ValueError(\n129             \"The supplied objects do not form a hypercube because\"\n130             \" sub-lists do not have consistent depths\"\n131         )\n132     # return these just to be reused in _check_shape_tile_ids\n133     return tile_ids, nesting_depths\n134 \n135 \n136 def _check_shape_tile_ids(combined_tile_ids):\n137     \"\"\"Check all lists along one dimension are same length.\"\"\"\n138     tile_ids, nesting_depths = _check_dimension_depth_tile_ids(combined_tile_ids)\n139     for dim in range(nesting_depths[0]):\n140         indices_along_dim = [tile_id[dim] for tile_id in tile_ids]\n141         occurrences = Counter(indices_along_dim)\n142         if len(set(occurrences.values())) != 1:\n143             raise ValueError(\n144                 \"The supplied objects do not form a hypercube \"\n145                 \"because sub-lists do not have consistent \"\n146                 \"lengths along dimension\" + str(dim)\n147             )\n148 \n149 \n150 def _combine_nd(\n151     combined_ids,\n152     concat_dims,\n153     data_vars=\"all\",\n154     coords=\"different\",\n155     compat=\"no_conflicts\",\n156     fill_value=dtypes.NA,\n157     join=\"outer\",\n158     combine_attrs=\"drop\",\n159 ):\n160     \"\"\"\n161     Combines an N-dimensional structure of datasets into one by applying a\n162     series of either concat and merge operations along each dimension.\n163 \n164     No checks are performed on the consistency of the datasets, concat_dims or\n165     tile_IDs, because it is assumed that this has already been done.\n166 \n167     Parameters\n168     ----------\n169     combined_ids : Dict[Tuple[int, ...]], xarray.Dataset]\n170         Structure containing all datasets to be concatenated with \"tile_IDs\" as\n171         keys, which specify position within the desired final combined result.\n172     concat_dims : sequence of str\n173         The dimensions along which the datasets should be concatenated. Must be\n174         in order, and the length must match the length of the tuples used as\n175         keys in combined_ids. If the string is a dimension name then concat\n176         along that dimension, if it is None then merge.\n177 \n178     Returns\n179     -------\n180     combined_ds : xarray.Dataset\n181     \"\"\"\n182 \n183     example_tile_id = next(iter(combined_ids.keys()))\n184 \n185     n_dims = len(example_tile_id)\n186     if len(concat_dims) != n_dims:\n187         raise ValueError(\n188             \"concat_dims has length {} but the datasets \"\n189             \"passed are nested in a {}-dimensional structure\".format(\n190                 len(concat_dims), n_dims\n191             )\n192         )\n193 \n194     # Each iteration of this loop reduces the length of the tile_ids tuples\n195     # by one. It always combines along the first dimension, removing the first\n196     # element of the tuple\n197     for concat_dim in concat_dims:\n198         combined_ids = _combine_all_along_first_dim(\n199             combined_ids,\n200             dim=concat_dim,\n201             data_vars=data_vars,\n202             coords=coords,\n203             compat=compat,\n204             fill_value=fill_value,\n205             join=join,\n206             combine_attrs=combine_attrs,\n207         )\n208     (combined_ds,) = combined_ids.values()\n209     return combined_ds\n210 \n211 \n212 def _combine_all_along_first_dim(\n213     combined_ids,\n214     dim,\n215     data_vars,\n216     coords,\n217     compat,\n218     fill_value=dtypes.NA,\n219     join=\"outer\",\n220     combine_attrs=\"drop\",\n221 ):\n222 \n223     # Group into lines of datasets which must be combined along dim\n224     # need to sort by _new_tile_id first for groupby to work\n225     # TODO: is the sorted need?\n226     combined_ids = dict(sorted(combined_ids.items(), key=_new_tile_id))\n227     grouped = itertools.groupby(combined_ids.items(), key=_new_tile_id)\n228 \n229     # Combine all of these datasets along dim\n230     new_combined_ids = {}\n231     for new_id, group in grouped:\n232         combined_ids = dict(sorted(group))\n233         datasets = combined_ids.values()\n234         new_combined_ids[new_id] = _combine_1d(\n235             datasets, dim, compat, data_vars, coords, fill_value, join, combine_attrs\n236         )\n237     return new_combined_ids\n238 \n239 \n240 def _combine_1d(\n241     datasets,\n242     concat_dim,\n243     compat=\"no_conflicts\",\n244     data_vars=\"all\",\n245     coords=\"different\",\n246     fill_value=dtypes.NA,\n247     join=\"outer\",\n248     combine_attrs=\"drop\",\n249 ):\n250     \"\"\"\n251     Applies either concat or merge to 1D list of datasets depending on value\n252     of concat_dim\n253     \"\"\"\n254 \n255     if concat_dim is not None:\n256         try:\n257             combined = concat(\n258                 datasets,\n259                 dim=concat_dim,\n260                 data_vars=data_vars,\n261                 coords=coords,\n262                 compat=compat,\n263                 fill_value=fill_value,\n264                 join=join,\n265                 combine_attrs=combine_attrs,\n266             )\n267         except ValueError as err:\n268             if \"encountered unexpected variable\" in str(err):\n269                 raise ValueError(\n270                     \"These objects cannot be combined using only \"\n271                     \"xarray.combine_nested, instead either use \"\n272                     \"xarray.combine_by_coords, or do it manually \"\n273                     \"with xarray.concat, xarray.merge and \"\n274                     \"xarray.align\"\n275                 )\n276             else:\n277                 raise\n278     else:\n279         combined = merge(\n280             datasets,\n281             compat=compat,\n282             fill_value=fill_value,\n283             join=join,\n284             combine_attrs=combine_attrs,\n285         )\n286 \n287     return combined\n288 \n289 \n290 def _new_tile_id(single_id_ds_pair):\n291     tile_id, ds = single_id_ds_pair\n292     return tile_id[1:]\n293 \n294 \n295 def _nested_combine(\n296     datasets,\n297     concat_dims,\n298     compat,\n299     data_vars,\n300     coords,\n301     ids,\n302     fill_value=dtypes.NA,\n303     join=\"outer\",\n304     combine_attrs=\"drop\",\n305 ):\n306 \n307     if len(datasets) == 0:\n308         return Dataset()\n309 \n310     # Arrange datasets for concatenation\n311     # Use information from the shape of the user input\n312     if not ids:\n313         # Determine tile_IDs by structure of input in N-D\n314         # (i.e. ordering in list-of-lists)\n315         combined_ids = _infer_concat_order_from_positions(datasets)\n316     else:\n317         # Already sorted so just use the ids already passed\n318         combined_ids = dict(zip(ids, datasets))\n319 \n320     # Check that the inferred shape is combinable\n321     _check_shape_tile_ids(combined_ids)\n322 \n323     # Apply series of concatenate or merge operations along each dimension\n324     combined = _combine_nd(\n325         combined_ids,\n326         concat_dims,\n327         compat=compat,\n328         data_vars=data_vars,\n329         coords=coords,\n330         fill_value=fill_value,\n331         join=join,\n332         combine_attrs=combine_attrs,\n333     )\n334     return combined\n335 \n336 \n337 def combine_nested(\n338     datasets,\n339     concat_dim,\n340     compat=\"no_conflicts\",\n341     data_vars=\"all\",\n342     coords=\"different\",\n343     fill_value=dtypes.NA,\n344     join=\"outer\",\n345     combine_attrs=\"drop\",\n346 ):\n347     \"\"\"\n348     Explicitly combine an N-dimensional grid of datasets into one by using a\n349     succession of concat and merge operations along each dimension of the grid.\n350 \n351     Does not sort the supplied datasets under any circumstances, so the\n352     datasets must be passed in the order you wish them to be concatenated. It\n353     does align coordinates, but different variables on datasets can cause it to\n354     fail under some scenarios. In complex cases, you may need to clean up your\n355     data and use concat/merge explicitly.\n356 \n357     To concatenate along multiple dimensions the datasets must be passed as a\n358     nested list-of-lists, with a depth equal to the length of ``concat_dims``.\n359     ``manual_combine`` will concatenate along the top-level list first.\n360 \n361     Useful for combining datasets from a set of nested directories, or for\n362     collecting the output of a simulation parallelized along multiple\n363     dimensions.\n364 \n365     Parameters\n366     ----------\n367     datasets : list or nested list of Dataset\n368         Dataset objects to combine.\n369         If concatenation or merging along more than one dimension is desired,\n370         then datasets must be supplied in a nested list-of-lists.\n371     concat_dim : str, or list of str, DataArray, Index or None\n372         Dimensions along which to concatenate variables, as used by\n373         :py:func:`xarray.concat`.\n374         Set ``concat_dim=[..., None, ...]`` explicitly to disable concatenation\n375         and merge instead along a particular dimension.\n376         The position of ``None`` in the list specifies the dimension of the\n377         nested-list input along which to merge.\n378         Must be the same length as the depth of the list passed to\n379         ``datasets``.\n380     compat : {\"identical\", \"equals\", \"broadcast_equals\", \\\n381               \"no_conflicts\", \"override\"}, optional\n382         String indicating how to compare variables of the same name for\n383         potential merge conflicts:\n384 \n385         - \"broadcast_equals\": all values must be equal when variables are\n386           broadcast against each other to ensure common dimensions.\n387         - \"equals\": all values and dimensions must be the same.\n388         - \"identical\": all values, dimensions and attributes must be the\n389           same.\n390         - \"no_conflicts\": only values which are not null in both datasets\n391           must be equal. The returned dataset then contains the combination\n392           of all non-null values.\n393         - \"override\": skip comparing and pick variable from first dataset\n394     data_vars : {\"minimal\", \"different\", \"all\" or list of str}, optional\n395         Details are in the documentation of concat\n396     coords : {\"minimal\", \"different\", \"all\" or list of str}, optional\n397         Details are in the documentation of concat\n398     fill_value : scalar or dict-like, optional\n399         Value to use for newly missing values. If a dict-like, maps\n400         variable names to fill values. Use a data array's name to\n401         refer to its values.\n402     join : {\"outer\", \"inner\", \"left\", \"right\", \"exact\"}, optional\n403         String indicating how to combine differing indexes\n404         (excluding concat_dim) in objects\n405 \n406         - \"outer\": use the union of object indexes\n407         - \"inner\": use the intersection of object indexes\n408         - \"left\": use indexes from the first object with each dimension\n409         - \"right\": use indexes from the last object with each dimension\n410         - \"exact\": instead of aligning, raise `ValueError` when indexes to be\n411           aligned are not equal\n412         - \"override\": if indexes are of same size, rewrite indexes to be\n413           those of the first object with that dimension. Indexes for the same\n414           dimension must have the same size in all objects.\n415     combine_attrs : {\"drop\", \"identical\", \"no_conflicts\", \"override\"}, \\\n416                     default: \"drop\"\n417         String indicating how to combine attrs of the objects being merged:\n418 \n419         - \"drop\": empty attrs on returned Dataset.\n420         - \"identical\": all attrs must be the same on every object.\n421         - \"no_conflicts\": attrs from all objects are combined, any that have\n422           the same name must also have the same value.\n423         - \"override\": skip comparing and copy attrs from the first dataset to\n424           the result.\n425 \n426     Returns\n427     -------\n428     combined : xarray.Dataset\n429 \n430     Examples\n431     --------\n432 \n433     A common task is collecting data from a parallelized simulation in which\n434     each process wrote out to a separate file. A domain which was decomposed\n435     into 4 parts, 2 each along both the x and y axes, requires organising the\n436     datasets into a doubly-nested list, e.g:\n437 \n438     >>> x1y1 = xr.Dataset(\n439     ...     {\n440     ...         \"temperature\": ((\"x\", \"y\"), np.random.randn(2, 2)),\n441     ...         \"precipitation\": ((\"x\", \"y\"), np.random.randn(2, 2)),\n442     ...     }\n443     ... )\n444     >>> x1y1\n445     <xarray.Dataset>\n446     Dimensions:        (x: 2, y: 2)\n447     Dimensions without coordinates: x, y\n448     Data variables:\n449         temperature    (x, y) float64 1.764 0.4002 0.9787 2.241\n450         precipitation  (x, y) float64 1.868 -0.9773 0.9501 -0.1514\n451     >>> x1y2 = xr.Dataset(\n452     ...     {\n453     ...         \"temperature\": ((\"x\", \"y\"), np.random.randn(2, 2)),\n454     ...         \"precipitation\": ((\"x\", \"y\"), np.random.randn(2, 2)),\n455     ...     }\n456     ... )\n457     >>> x2y1 = xr.Dataset(\n458     ...     {\n459     ...         \"temperature\": ((\"x\", \"y\"), np.random.randn(2, 2)),\n460     ...         \"precipitation\": ((\"x\", \"y\"), np.random.randn(2, 2)),\n461     ...     }\n462     ... )\n463     >>> x2y2 = xr.Dataset(\n464     ...     {\n465     ...         \"temperature\": ((\"x\", \"y\"), np.random.randn(2, 2)),\n466     ...         \"precipitation\": ((\"x\", \"y\"), np.random.randn(2, 2)),\n467     ...     }\n468     ... )\n469 \n470 \n471     >>> ds_grid = [[x1y1, x1y2], [x2y1, x2y2]]\n472     >>> combined = xr.combine_nested(ds_grid, concat_dim=[\"x\", \"y\"])\n473     >>> combined\n474     <xarray.Dataset>\n475     Dimensions:        (x: 4, y: 4)\n476     Dimensions without coordinates: x, y\n477     Data variables:\n478         temperature    (x, y) float64 1.764 0.4002 -0.1032 ... 0.04576 -0.1872\n479         precipitation  (x, y) float64 1.868 -0.9773 0.761 ... -0.7422 0.1549 0.3782\n480 \n481     ``manual_combine`` can also be used to explicitly merge datasets with\n482     different variables. For example if we have 4 datasets, which are divided\n483     along two times, and contain two different variables, we can pass ``None``\n484     to ``concat_dim`` to specify the dimension of the nested list over which\n485     we wish to use ``merge`` instead of ``concat``:\n486 \n487     >>> t1temp = xr.Dataset({\"temperature\": (\"t\", np.random.randn(5))})\n488     >>> t1temp\n489     <xarray.Dataset>\n490     Dimensions:      (t: 5)\n491     Dimensions without coordinates: t\n492     Data variables:\n493         temperature  (t) float64 -0.8878 -1.981 -0.3479 0.1563 1.23\n494 \n495     >>> t1precip = xr.Dataset({\"precipitation\": (\"t\", np.random.randn(5))})\n496     >>> t1precip\n497     <xarray.Dataset>\n498     Dimensions:        (t: 5)\n499     Dimensions without coordinates: t\n500     Data variables:\n501         precipitation  (t) float64 1.202 -0.3873 -0.3023 -1.049 -1.42\n502 \n503     >>> t2temp = xr.Dataset({\"temperature\": (\"t\", np.random.randn(5))})\n504     >>> t2precip = xr.Dataset({\"precipitation\": (\"t\", np.random.randn(5))})\n505 \n506 \n507     >>> ds_grid = [[t1temp, t1precip], [t2temp, t2precip]]\n508     >>> combined = xr.combine_nested(ds_grid, concat_dim=[\"t\", None])\n509     >>> combined\n510     <xarray.Dataset>\n511     Dimensions:        (t: 10)\n512     Dimensions without coordinates: t\n513     Data variables:\n514         temperature    (t) float64 -0.8878 -1.981 -0.3479 ... -0.5097 -0.4381 -1.253\n515         precipitation  (t) float64 1.202 -0.3873 -0.3023 ... -0.2127 -0.8955 0.3869\n516 \n517     See also\n518     --------\n519     concat\n520     merge\n521     \"\"\"\n522     if isinstance(concat_dim, (str, DataArray)) or concat_dim is None:\n523         concat_dim = [concat_dim]\n524 \n525     # The IDs argument tells _manual_combine that datasets aren't yet sorted\n526     return _nested_combine(\n527         datasets,\n528         concat_dims=concat_dim,\n529         compat=compat,\n530         data_vars=data_vars,\n531         coords=coords,\n532         ids=False,\n533         fill_value=fill_value,\n534         join=join,\n535         combine_attrs=combine_attrs,\n536     )\n537 \n538 \n539 def vars_as_keys(ds):\n540     return tuple(sorted(ds))\n541 \n542 \n543 def combine_by_coords(\n544     datasets,\n545     compat=\"no_conflicts\",\n546     data_vars=\"all\",\n547     coords=\"different\",\n548     fill_value=dtypes.NA,\n549     join=\"outer\",\n550     combine_attrs=\"no_conflicts\",\n551 ):\n552     \"\"\"\n553     Attempt to auto-magically combine the given datasets into one by using\n554     dimension coordinates.\n555 \n556     This method attempts to combine a group of datasets along any number of\n557     dimensions into a single entity by inspecting coords and metadata and using\n558     a combination of concat and merge.\n559 \n560     Will attempt to order the datasets such that the values in their dimension\n561     coordinates are monotonic along all dimensions. If it cannot determine the\n562     order in which to concatenate the datasets, it will raise a ValueError.\n563     Non-coordinate dimensions will be ignored, as will any coordinate\n564     dimensions which do not vary between each dataset.\n565 \n566     Aligns coordinates, but different variables on datasets can cause it\n567     to fail under some scenarios. In complex cases, you may need to clean up\n568     your data and use concat/merge explicitly (also see `manual_combine`).\n569 \n570     Works well if, for example, you have N years of data and M data variables,\n571     and each combination of a distinct time period and set of data variables is\n572     saved as its own dataset. Also useful for if you have a simulation which is\n573     parallelized in multiple dimensions, but has global coordinates saved in\n574     each file specifying the positions of points within the global domain.\n575 \n576     Parameters\n577     ----------\n578     datasets : sequence of xarray.Dataset\n579         Dataset objects to combine.\n580     compat : {\"identical\", \"equals\", \"broadcast_equals\", \"no_conflicts\", \"override\"}, optional\n581         String indicating how to compare variables of the same name for\n582         potential conflicts:\n583 \n584         - \"broadcast_equals\": all values must be equal when variables are\n585           broadcast against each other to ensure common dimensions.\n586         - \"equals\": all values and dimensions must be the same.\n587         - \"identical\": all values, dimensions and attributes must be the\n588           same.\n589         - \"no_conflicts\": only values which are not null in both datasets\n590           must be equal. The returned dataset then contains the combination\n591           of all non-null values.\n592         - \"override\": skip comparing and pick variable from first dataset\n593     data_vars : {\"minimal\", \"different\", \"all\" or list of str}, optional\n594         These data variables will be concatenated together:\n595 \n596         * \"minimal\": Only data variables in which the dimension already\n597           appears are included.\n598         * \"different\": Data variables which are not equal (ignoring\n599           attributes) across all datasets are also concatenated (as well as\n600           all for which dimension already appears). Beware: this option may\n601           load the data payload of data variables into memory if they are not\n602           already loaded.\n603         * \"all\": All data variables will be concatenated.\n604         * list of str: The listed data variables will be concatenated, in\n605           addition to the \"minimal\" data variables.\n606 \n607         If objects are DataArrays, `data_vars` must be \"all\".\n608     coords : {\"minimal\", \"different\", \"all\"} or list of str, optional\n609         As per the \"data_vars\" kwarg, but for coordinate variables.\n610     fill_value : scalar or dict-like, optional\n611         Value to use for newly missing values. If a dict-like, maps\n612         variable names to fill values. Use a data array's name to\n613         refer to its values. If None, raises a ValueError if\n614         the passed Datasets do not create a complete hypercube.\n615     join : {\"outer\", \"inner\", \"left\", \"right\", \"exact\"}, optional\n616         String indicating how to combine differing indexes\n617         (excluding concat_dim) in objects\n618 \n619         - \"outer\": use the union of object indexes\n620         - \"inner\": use the intersection of object indexes\n621         - \"left\": use indexes from the first object with each dimension\n622         - \"right\": use indexes from the last object with each dimension\n623         - \"exact\": instead of aligning, raise `ValueError` when indexes to be\n624           aligned are not equal\n625         - \"override\": if indexes are of same size, rewrite indexes to be\n626           those of the first object with that dimension. Indexes for the same\n627           dimension must have the same size in all objects.\n628     combine_attrs : {\"drop\", \"identical\", \"no_conflicts\", \"override\"}, \\\n629                     default: \"drop\"\n630         String indicating how to combine attrs of the objects being merged:\n631 \n632         - \"drop\": empty attrs on returned Dataset.\n633         - \"identical\": all attrs must be the same on every object.\n634         - \"no_conflicts\": attrs from all objects are combined, any that have\n635           the same name must also have the same value.\n636         - \"override\": skip comparing and copy attrs from the first dataset to\n637           the result.\n638 \n639     Returns\n640     -------\n641     combined : xarray.Dataset\n642 \n643     See also\n644     --------\n645     concat\n646     merge\n647     combine_nested\n648 \n649     Examples\n650     --------\n651 \n652     Combining two datasets using their common dimension coordinates. Notice\n653     they are concatenated based on the values in their dimension coordinates,\n654     not on their position in the list passed to `combine_by_coords`.\n655 \n656     >>> import numpy as np\n657     >>> import xarray as xr\n658 \n659     >>> x1 = xr.Dataset(\n660     ...     {\n661     ...         \"temperature\": ((\"y\", \"x\"), 20 * np.random.rand(6).reshape(2, 3)),\n662     ...         \"precipitation\": ((\"y\", \"x\"), np.random.rand(6).reshape(2, 3)),\n663     ...     },\n664     ...     coords={\"y\": [0, 1], \"x\": [10, 20, 30]},\n665     ... )\n666     >>> x2 = xr.Dataset(\n667     ...     {\n668     ...         \"temperature\": ((\"y\", \"x\"), 20 * np.random.rand(6).reshape(2, 3)),\n669     ...         \"precipitation\": ((\"y\", \"x\"), np.random.rand(6).reshape(2, 3)),\n670     ...     },\n671     ...     coords={\"y\": [2, 3], \"x\": [10, 20, 30]},\n672     ... )\n673     >>> x3 = xr.Dataset(\n674     ...     {\n675     ...         \"temperature\": ((\"y\", \"x\"), 20 * np.random.rand(6).reshape(2, 3)),\n676     ...         \"precipitation\": ((\"y\", \"x\"), np.random.rand(6).reshape(2, 3)),\n677     ...     },\n678     ...     coords={\"y\": [2, 3], \"x\": [40, 50, 60]},\n679     ... )\n680 \n681     >>> x1\n682     <xarray.Dataset>\n683     Dimensions:        (x: 3, y: 2)\n684     Coordinates:\n685       * y              (y) int64 0 1\n686       * x              (x) int64 10 20 30\n687     Data variables:\n688         temperature    (y, x) float64 10.98 14.3 12.06 10.9 8.473 12.92\n689         precipitation  (y, x) float64 0.4376 0.8918 0.9637 0.3834 0.7917 0.5289\n690 \n691     >>> x2\n692     <xarray.Dataset>\n693     Dimensions:        (x: 3, y: 2)\n694     Coordinates:\n695       * y              (y) int64 2 3\n696       * x              (x) int64 10 20 30\n697     Data variables:\n698         temperature    (y, x) float64 11.36 18.51 1.421 1.743 0.4044 16.65\n699         precipitation  (y, x) float64 0.7782 0.87 0.9786 0.7992 0.4615 0.7805\n700 \n701     >>> x3\n702     <xarray.Dataset>\n703     Dimensions:        (x: 3, y: 2)\n704     Coordinates:\n705       * y              (y) int64 2 3\n706       * x              (x) int64 40 50 60\n707     Data variables:\n708         temperature    (y, x) float64 2.365 12.8 2.867 18.89 10.44 8.293\n709         precipitation  (y, x) float64 0.2646 0.7742 0.4562 0.5684 0.01879 0.6176\n710 \n711     >>> xr.combine_by_coords([x2, x1])\n712     <xarray.Dataset>\n713     Dimensions:        (x: 3, y: 4)\n714     Coordinates:\n715       * y              (y) int64 0 1 2 3\n716       * x              (x) int64 10 20 30\n717     Data variables:\n718         temperature    (y, x) float64 10.98 14.3 12.06 10.9 ... 1.743 0.4044 16.65\n719         precipitation  (y, x) float64 0.4376 0.8918 0.9637 ... 0.7992 0.4615 0.7805\n720 \n721     >>> xr.combine_by_coords([x3, x1])\n722     <xarray.Dataset>\n723     Dimensions:        (x: 6, y: 4)\n724     Coordinates:\n725       * x              (x) int64 10 20 30 40 50 60\n726       * y              (y) int64 0 1 2 3\n727     Data variables:\n728         temperature    (y, x) float64 10.98 14.3 12.06 nan ... nan 18.89 10.44 8.293\n729         precipitation  (y, x) float64 0.4376 0.8918 0.9637 ... 0.5684 0.01879 0.6176\n730 \n731     >>> xr.combine_by_coords([x3, x1], join=\"override\")\n732     <xarray.Dataset>\n733     Dimensions:        (x: 3, y: 4)\n734     Coordinates:\n735       * x              (x) int64 10 20 30\n736       * y              (y) int64 0 1 2 3\n737     Data variables:\n738         temperature    (y, x) float64 10.98 14.3 12.06 10.9 ... 18.89 10.44 8.293\n739         precipitation  (y, x) float64 0.4376 0.8918 0.9637 ... 0.5684 0.01879 0.6176\n740 \n741     >>> xr.combine_by_coords([x1, x2, x3])\n742     <xarray.Dataset>\n743     Dimensions:        (x: 6, y: 4)\n744     Coordinates:\n745       * x              (x) int64 10 20 30 40 50 60\n746       * y              (y) int64 0 1 2 3\n747     Data variables:\n748         temperature    (y, x) float64 10.98 14.3 12.06 nan ... 18.89 10.44 8.293\n749         precipitation  (y, x) float64 0.4376 0.8918 0.9637 ... 0.5684 0.01879 0.6176\n750     \"\"\"\n751 \n752     # Group by data vars\n753     sorted_datasets = sorted(datasets, key=vars_as_keys)\n754     grouped_by_vars = itertools.groupby(sorted_datasets, key=vars_as_keys)\n755 \n756     # Perform the multidimensional combine on each group of data variables\n757     # before merging back together\n758     concatenated_grouped_by_data_vars = []\n759     for vars, datasets_with_same_vars in grouped_by_vars:\n760         combined_ids, concat_dims = _infer_concat_order_from_coords(\n761             list(datasets_with_same_vars)\n762         )\n763 \n764         if fill_value is None:\n765             # check that datasets form complete hypercube\n766             _check_shape_tile_ids(combined_ids)\n767         else:\n768             # check only that all datasets have same dimension depth for these\n769             # vars\n770             _check_dimension_depth_tile_ids(combined_ids)\n771 \n772         # Concatenate along all of concat_dims one by one to create single ds\n773         concatenated = _combine_nd(\n774             combined_ids,\n775             concat_dims=concat_dims,\n776             data_vars=data_vars,\n777             coords=coords,\n778             compat=compat,\n779             fill_value=fill_value,\n780             join=join,\n781             combine_attrs=combine_attrs,\n782         )\n783 \n784         # Check the overall coordinates are monotonically increasing\n785         for dim in concat_dims:\n786             indexes = concatenated.indexes.get(dim)\n787             if not (indexes.is_monotonic_increasing or indexes.is_monotonic_decreasing):\n788                 raise ValueError(\n789                     \"Resulting object does not have monotonic\"\n790                     \" global indexes along dimension {}\".format(dim)\n791                 )\n792         concatenated_grouped_by_data_vars.append(concatenated)\n793 \n794     return merge(\n795         concatenated_grouped_by_data_vars,\n796         compat=compat,\n797         fill_value=fill_value,\n798         join=join,\n799         combine_attrs=combine_attrs,\n800     )\n",
        "intended_behavior": "Modify these methods to interpret and apply the 'drop_conflicts' strategy within their respective 'combine_attrs' handling. The implementation should merge dataset attributes according to the specified 'combine_attrs' strategy, specifically adding logic to handle attribute conflicts by dropping conflicting attributes during dataset combination operations."
    },
    {
        "rel_file_path": "xarray/core/concat.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/concat.py",
        "start": 1,
        "end": 581,
        "class_name": null,
        "method_name": null,
        "code": "1 from typing import (\n2     TYPE_CHECKING,\n3     Dict,\n4     Hashable,\n5     Iterable,\n6     List,\n7     Optional,\n8     Set,\n9     Tuple,\n10     Union,\n11     overload,\n12 )\n13 \n14 import pandas as pd\n15 \n16 from . import dtypes, utils\n17 from .alignment import align\n18 from .duck_array_ops import lazy_array_equiv\n19 from .merge import _VALID_COMPAT, merge_attrs, unique_variable\n20 from .variable import IndexVariable, Variable, as_variable\n21 from .variable import concat as concat_vars\n22 \n23 if TYPE_CHECKING:\n24     from .dataarray import DataArray\n25     from .dataset import Dataset\n26 \n27 \n28 @overload\n29 def concat(\n30     objs: Iterable[\"Dataset\"],\n31     dim: Union[str, \"DataArray\", pd.Index],\n32     data_vars: Union[str, List[str]] = \"all\",\n33     coords: Union[str, List[str]] = \"different\",\n34     compat: str = \"equals\",\n35     positions: Optional[Iterable[int]] = None,\n36     fill_value: object = dtypes.NA,\n37     join: str = \"outer\",\n38     combine_attrs: str = \"override\",\n39 ) -> \"Dataset\":\n40     ...\n41 \n42 \n43 @overload\n44 def concat(\n45     objs: Iterable[\"DataArray\"],\n46     dim: Union[str, \"DataArray\", pd.Index],\n47     data_vars: Union[str, List[str]] = \"all\",\n48     coords: Union[str, List[str]] = \"different\",\n49     compat: str = \"equals\",\n50     positions: Optional[Iterable[int]] = None,\n51     fill_value: object = dtypes.NA,\n52     join: str = \"outer\",\n53     combine_attrs: str = \"override\",\n54 ) -> \"DataArray\":\n55     ...\n56 \n57 \n58 def concat(\n59     objs,\n60     dim,\n61     data_vars=\"all\",\n62     coords=\"different\",\n63     compat=\"equals\",\n64     positions=None,\n65     fill_value=dtypes.NA,\n66     join=\"outer\",\n67     combine_attrs=\"override\",\n68 ):\n69     \"\"\"Concatenate xarray objects along a new or existing dimension.\n70 \n71     Parameters\n72     ----------\n73     objs : sequence of Dataset and DataArray\n74         xarray objects to concatenate together. Each object is expected to\n75         consist of variables and coordinates with matching shapes except for\n76         along the concatenated dimension.\n77     dim : str or DataArray or pandas.Index\n78         Name of the dimension to concatenate along. This can either be a new\n79         dimension name, in which case it is added along axis=0, or an existing\n80         dimension name, in which case the location of the dimension is\n81         unchanged. If dimension is provided as a DataArray or Index, its name\n82         is used as the dimension to concatenate along and the values are added\n83         as a coordinate.\n84     data_vars : {\"minimal\", \"different\", \"all\"} or list of str, optional\n85         These data variables will be concatenated together:\n86           * \"minimal\": Only data variables in which the dimension already\n87             appears are included.\n88           * \"different\": Data variables which are not equal (ignoring\n89             attributes) across all datasets are also concatenated (as well as\n90             all for which dimension already appears). Beware: this option may\n91             load the data payload of data variables into memory if they are not\n92             already loaded.\n93           * \"all\": All data variables will be concatenated.\n94           * list of str: The listed data variables will be concatenated, in\n95             addition to the \"minimal\" data variables.\n96 \n97         If objects are DataArrays, data_vars must be \"all\".\n98     coords : {\"minimal\", \"different\", \"all\"} or list of str, optional\n99         These coordinate variables will be concatenated together:\n100           * \"minimal\": Only coordinates in which the dimension already appears\n101             are included.\n102           * \"different\": Coordinates which are not equal (ignoring attributes)\n103             across all datasets are also concatenated (as well as all for which\n104             dimension already appears). Beware: this option may load the data\n105             payload of coordinate variables into memory if they are not already\n106             loaded.\n107           * \"all\": All coordinate variables will be concatenated, except\n108             those corresponding to other dimensions.\n109           * list of str: The listed coordinate variables will be concatenated,\n110             in addition to the \"minimal\" coordinates.\n111     compat : {\"identical\", \"equals\", \"broadcast_equals\", \"no_conflicts\", \"override\"}, optional\n112         String indicating how to compare non-concatenated variables of the same name for\n113         potential conflicts. This is passed down to merge.\n114 \n115         - \"broadcast_equals\": all values must be equal when variables are\n116           broadcast against each other to ensure common dimensions.\n117         - \"equals\": all values and dimensions must be the same.\n118         - \"identical\": all values, dimensions and attributes must be the\n119           same.\n120         - \"no_conflicts\": only values which are not null in both datasets\n121           must be equal. The returned dataset then contains the combination\n122           of all non-null values.\n123         - \"override\": skip comparing and pick variable from first dataset\n124     positions : None or list of integer arrays, optional\n125         List of integer arrays which specifies the integer positions to which\n126         to assign each dataset along the concatenated dimension. If not\n127         supplied, objects are concatenated in the provided order.\n128     fill_value : scalar or dict-like, optional\n129         Value to use for newly missing values. If a dict-like, maps\n130         variable names to fill values. Use a data array's name to\n131         refer to its values.\n132     join : {\"outer\", \"inner\", \"left\", \"right\", \"exact\"}, optional\n133         String indicating how to combine differing indexes\n134         (excluding dim) in objects\n135 \n136         - \"outer\": use the union of object indexes\n137         - \"inner\": use the intersection of object indexes\n138         - \"left\": use indexes from the first object with each dimension\n139         - \"right\": use indexes from the last object with each dimension\n140         - \"exact\": instead of aligning, raise `ValueError` when indexes to be\n141           aligned are not equal\n142         - \"override\": if indexes are of same size, rewrite indexes to be\n143           those of the first object with that dimension. Indexes for the same\n144           dimension must have the same size in all objects.\n145     combine_attrs : {\"drop\", \"identical\", \"no_conflicts\", \"override\"}, \\\n146                     default: \"override\"\n147         String indicating how to combine attrs of the objects being merged:\n148 \n149         - \"drop\": empty attrs on returned Dataset.\n150         - \"identical\": all attrs must be the same on every object.\n151         - \"no_conflicts\": attrs from all objects are combined, any that have\n152           the same name must also have the same value.\n153         - \"override\": skip comparing and copy attrs from the first dataset to\n154           the result.\n155 \n156     Returns\n157     -------\n158     concatenated : type of objs\n159 \n160     See also\n161     --------\n162     merge\n163 \n164     Examples\n165     --------\n166     >>> da = xr.DataArray(\n167     ...     np.arange(6).reshape(2, 3), [(\"x\", [\"a\", \"b\"]), (\"y\", [10, 20, 30])]\n168     ... )\n169     >>> da\n170     <xarray.DataArray (x: 2, y: 3)>\n171     array([[0, 1, 2],\n172            [3, 4, 5]])\n173     Coordinates:\n174       * x        (x) <U1 'a' 'b'\n175       * y        (y) int64 10 20 30\n176 \n177     >>> xr.concat([da.isel(y=slice(0, 1)), da.isel(y=slice(1, None))], dim=\"y\")\n178     <xarray.DataArray (x: 2, y: 3)>\n179     array([[0, 1, 2],\n180            [3, 4, 5]])\n181     Coordinates:\n182       * x        (x) <U1 'a' 'b'\n183       * y        (y) int64 10 20 30\n184 \n185     >>> xr.concat([da.isel(x=0), da.isel(x=1)], \"x\")\n186     <xarray.DataArray (x: 2, y: 3)>\n187     array([[0, 1, 2],\n188            [3, 4, 5]])\n189     Coordinates:\n190       * x        (x) <U1 'a' 'b'\n191       * y        (y) int64 10 20 30\n192 \n193     >>> xr.concat([da.isel(x=0), da.isel(x=1)], \"new_dim\")\n194     <xarray.DataArray (new_dim: 2, y: 3)>\n195     array([[0, 1, 2],\n196            [3, 4, 5]])\n197     Coordinates:\n198         x        (new_dim) <U1 'a' 'b'\n199       * y        (y) int64 10 20 30\n200     Dimensions without coordinates: new_dim\n201 \n202     >>> xr.concat([da.isel(x=0), da.isel(x=1)], pd.Index([-90, -100], name=\"new_dim\"))\n203     <xarray.DataArray (new_dim: 2, y: 3)>\n204     array([[0, 1, 2],\n205            [3, 4, 5]])\n206     Coordinates:\n207         x        (new_dim) <U1 'a' 'b'\n208       * y        (y) int64 10 20 30\n209       * new_dim  (new_dim) int64 -90 -100\n210     \"\"\"\n211     # TODO: add ignore_index arguments copied from pandas.concat\n212     # TODO: support concatenating scalar coordinates even if the concatenated\n213     # dimension already exists\n214     from .dataarray import DataArray\n215     from .dataset import Dataset\n216 \n217     try:\n218         first_obj, objs = utils.peek_at(objs)\n219     except StopIteration:\n220         raise ValueError(\"must supply at least one object to concatenate\")\n221 \n222     if compat not in _VALID_COMPAT:\n223         raise ValueError(\n224             \"compat=%r invalid: must be 'broadcast_equals', 'equals', 'identical', 'no_conflicts' or 'override'\"\n225             % compat\n226         )\n227 \n228     if isinstance(first_obj, DataArray):\n229         f = _dataarray_concat\n230     elif isinstance(first_obj, Dataset):\n231         f = _dataset_concat\n232     else:\n233         raise TypeError(\n234             \"can only concatenate xarray Dataset and DataArray \"\n235             \"objects, got %s\" % type(first_obj)\n236         )\n237     return f(\n238         objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs\n239     )\n240 \n241 \n242 def _calc_concat_dim_coord(dim):\n243     \"\"\"\n244     Infer the dimension name and 1d coordinate variable (if appropriate)\n245     for concatenating along the new dimension.\n246     \"\"\"\n247     from .dataarray import DataArray\n248 \n249     if isinstance(dim, str):\n250         coord = None\n251     elif not isinstance(dim, (DataArray, Variable)):\n252         dim_name = getattr(dim, \"name\", None)\n253         if dim_name is None:\n254             dim_name = \"concat_dim\"\n255         coord = IndexVariable(dim_name, dim)\n256         dim = dim_name\n257     elif not isinstance(dim, DataArray):\n258         coord = as_variable(dim).to_index_variable()\n259         (dim,) = coord.dims\n260     else:\n261         coord = dim\n262         (dim,) = coord.dims\n263     return dim, coord\n264 \n265 \n266 def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n267     \"\"\"\n268     Determine which dataset variables need to be concatenated in the result,\n269     \"\"\"\n270     # Return values\n271     concat_over = set()\n272     equals = {}\n273 \n274     if dim in dim_names:\n275         concat_over_existing_dim = True\n276         concat_over.add(dim)\n277     else:\n278         concat_over_existing_dim = False\n279 \n280     concat_dim_lengths = []\n281     for ds in datasets:\n282         if concat_over_existing_dim:\n283             if dim not in ds.dims:\n284                 if dim in ds:\n285                     ds = ds.set_coords(dim)\n286         concat_over.update(k for k, v in ds.variables.items() if dim in v.dims)\n287         concat_dim_lengths.append(ds.dims.get(dim, 1))\n288 \n289     def process_subset_opt(opt, subset):\n290         if isinstance(opt, str):\n291             if opt == \"different\":\n292                 if compat == \"override\":\n293                     raise ValueError(\n294                         \"Cannot specify both %s='different' and compat='override'.\"\n295                         % subset\n296                     )\n297                 # all nonindexes that are not the same in each dataset\n298                 for k in getattr(datasets[0], subset):\n299                     if k not in concat_over:\n300                         equals[k] = None\n301 \n302                         variables = []\n303                         for ds in datasets:\n304                             if k in ds.variables:\n305                                 variables.append(ds.variables[k])\n306 \n307                         if len(variables) == 1:\n308                             # coords=\"different\" doesn't make sense when only one object\n309                             # contains a particular variable.\n310                             break\n311                         elif len(variables) != len(datasets) and opt == \"different\":\n312                             raise ValueError(\n313                                 f\"{k!r} not present in all datasets and coords='different'. \"\n314                                 f\"Either add {k!r} to datasets where it is missing or \"\n315                                 \"specify coords='minimal'.\"\n316                             )\n317 \n318                         # first check without comparing values i.e. no computes\n319                         for var in variables[1:]:\n320                             equals[k] = getattr(variables[0], compat)(\n321                                 var, equiv=lazy_array_equiv\n322                             )\n323                             if equals[k] is not True:\n324                                 # exit early if we know these are not equal or that\n325                                 # equality cannot be determined i.e. one or all of\n326                                 # the variables wraps a numpy array\n327                                 break\n328 \n329                         if equals[k] is False:\n330                             concat_over.add(k)\n331 \n332                         elif equals[k] is None:\n333                             # Compare the variable of all datasets vs. the one\n334                             # of the first dataset. Perform the minimum amount of\n335                             # loads in order to avoid multiple loads from disk\n336                             # while keeping the RAM footprint low.\n337                             v_lhs = datasets[0].variables[k].load()\n338                             # We'll need to know later on if variables are equal.\n339                             computed = []\n340                             for ds_rhs in datasets[1:]:\n341                                 v_rhs = ds_rhs.variables[k].compute()\n342                                 computed.append(v_rhs)\n343                                 if not getattr(v_lhs, compat)(v_rhs):\n344                                     concat_over.add(k)\n345                                     equals[k] = False\n346                                     # computed variables are not to be re-computed\n347                                     # again in the future\n348                                     for ds, v in zip(datasets[1:], computed):\n349                                         ds.variables[k].data = v.data\n350                                     break\n351                             else:\n352                                 equals[k] = True\n353 \n354             elif opt == \"all\":\n355                 concat_over.update(\n356                     set(getattr(datasets[0], subset)) - set(datasets[0].dims)\n357                 )\n358             elif opt == \"minimal\":\n359                 pass\n360             else:\n361                 raise ValueError(f\"unexpected value for {subset}: {opt}\")\n362         else:\n363             invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n364             if invalid_vars:\n365                 if subset == \"coords\":\n366                     raise ValueError(\n367                         \"some variables in coords are not coordinates on \"\n368                         \"the first dataset: %s\" % (invalid_vars,)\n369                     )\n370                 else:\n371                     raise ValueError(\n372                         \"some variables in data_vars are not data variables \"\n373                         \"on the first dataset: %s\" % (invalid_vars,)\n374                     )\n375             concat_over.update(opt)\n376 \n377     process_subset_opt(data_vars, \"data_vars\")\n378     process_subset_opt(coords, \"coords\")\n379     return concat_over, equals, concat_dim_lengths\n380 \n381 \n382 # determine dimensional coordinate names and a dict mapping name to DataArray\n383 def _parse_datasets(\n384     datasets: Iterable[\"Dataset\"],\n385 ) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, int], Set[Hashable], Set[Hashable]]:\n386 \n387     dims: Set[Hashable] = set()\n388     all_coord_names: Set[Hashable] = set()\n389     data_vars: Set[Hashable] = set()  # list of data_vars\n390     dim_coords: Dict[Hashable, Variable] = {}  # maps dim name to variable\n391     dims_sizes: Dict[Hashable, int] = {}  # shared dimension sizes to expand variables\n392 \n393     for ds in datasets:\n394         dims_sizes.update(ds.dims)\n395         all_coord_names.update(ds.coords)\n396         data_vars.update(ds.data_vars)\n397 \n398         # preserves ordering of dimensions\n399         for dim in ds.dims:\n400             if dim in dims:\n401                 continue\n402 \n403             if dim not in dim_coords:\n404                 dim_coords[dim] = ds.coords[dim].variable\n405         dims = dims | set(ds.dims)\n406 \n407     return dim_coords, dims_sizes, all_coord_names, data_vars\n408 \n409 \n410 def _dataset_concat(\n411     datasets: List[\"Dataset\"],\n412     dim: Union[str, \"DataArray\", pd.Index],\n413     data_vars: Union[str, List[str]],\n414     coords: Union[str, List[str]],\n415     compat: str,\n416     positions: Optional[Iterable[int]],\n417     fill_value: object = dtypes.NA,\n418     join: str = \"outer\",\n419     combine_attrs: str = \"override\",\n420 ) -> \"Dataset\":\n421     \"\"\"\n422     Concatenate a sequence of datasets along a new or existing dimension\n423     \"\"\"\n424     from .dataset import Dataset\n425 \n426     dim, coord = _calc_concat_dim_coord(dim)\n427     # Make sure we're working on a copy (we'll be loading variables)\n428     datasets = [ds.copy() for ds in datasets]\n429     datasets = list(\n430         align(*datasets, join=join, copy=False, exclude=[dim], fill_value=fill_value)\n431     )\n432 \n433     dim_coords, dims_sizes, coord_names, data_names = _parse_datasets(datasets)\n434     dim_names = set(dim_coords)\n435     unlabeled_dims = dim_names - coord_names\n436 \n437     both_data_and_coords = coord_names & data_names\n438     if both_data_and_coords:\n439         raise ValueError(\n440             \"%r is a coordinate in some datasets but not others.\" % both_data_and_coords\n441         )\n442     # we don't want the concat dimension in the result dataset yet\n443     dim_coords.pop(dim, None)\n444     dims_sizes.pop(dim, None)\n445 \n446     # case where concat dimension is a coordinate or data_var but not a dimension\n447     if (dim in coord_names or dim in data_names) and dim not in dim_names:\n448         datasets = [ds.expand_dims(dim) for ds in datasets]\n449 \n450     # determine which variables to concatentate\n451     concat_over, equals, concat_dim_lengths = _calc_concat_over(\n452         datasets, dim, dim_names, data_vars, coords, compat\n453     )\n454 \n455     # determine which variables to merge, and then merge them according to compat\n456     variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n457 \n458     result_vars = {}\n459     if variables_to_merge:\n460         to_merge: Dict[Hashable, List[Variable]] = {\n461             var: [] for var in variables_to_merge\n462         }\n463 \n464         for ds in datasets:\n465             for var in variables_to_merge:\n466                 if var in ds:\n467                     to_merge[var].append(ds.variables[var])\n468 \n469         for var in variables_to_merge:\n470             result_vars[var] = unique_variable(\n471                 var, to_merge[var], compat=compat, equals=equals.get(var, None)\n472             )\n473     else:\n474         result_vars = {}\n475     result_vars.update(dim_coords)\n476 \n477     # assign attrs and encoding from first dataset\n478     result_attrs = merge_attrs([ds.attrs for ds in datasets], combine_attrs)\n479     result_encoding = datasets[0].encoding\n480 \n481     # check that global attributes are fixed across all datasets if necessary\n482     for ds in datasets[1:]:\n483         if compat == \"identical\" and not utils.dict_equiv(ds.attrs, result_attrs):\n484             raise ValueError(\"Dataset global attributes not equal.\")\n485 \n486     # we've already verified everything is consistent; now, calculate\n487     # shared dimension sizes so we can expand the necessary variables\n488     def ensure_common_dims(vars):\n489         # ensure each variable with the given name shares the same\n490         # dimensions and the same shape for all of them except along the\n491         # concat dimension\n492         common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n493         if dim not in common_dims:\n494             common_dims = (dim,) + common_dims\n495         for var, dim_len in zip(vars, concat_dim_lengths):\n496             if var.dims != common_dims:\n497                 common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n498                 var = var.set_dims(common_dims, common_shape)\n499             yield var\n500 \n501     # stack up each variable to fill-out the dataset (in order)\n502     # n.b. this loop preserves variable order, needed for groupby.\n503     for k in datasets[0].variables:\n504         if k in concat_over:\n505             try:\n506                 vars = ensure_common_dims([ds[k].variable for ds in datasets])\n507             except KeyError:\n508                 raise ValueError(\"%r is not present in all datasets.\" % k)\n509             combined = concat_vars(vars, dim, positions)\n510             assert isinstance(combined, Variable)\n511             result_vars[k] = combined\n512         elif k in result_vars:\n513             # preserves original variable order\n514             result_vars[k] = result_vars.pop(k)\n515 \n516     result = Dataset(result_vars, attrs=result_attrs)\n517     absent_coord_names = coord_names - set(result.variables)\n518     if absent_coord_names:\n519         raise ValueError(\n520             \"Variables %r are coordinates in some datasets but not others.\"\n521             % absent_coord_names\n522         )\n523     result = result.set_coords(coord_names)\n524     result.encoding = result_encoding\n525 \n526     result = result.drop_vars(unlabeled_dims, errors=\"ignore\")\n527 \n528     if coord is not None:\n529         # add concat dimension last to ensure that its in the final Dataset\n530         result[coord.name] = coord\n531 \n532     return result\n533 \n534 \n535 def _dataarray_concat(\n536     arrays: Iterable[\"DataArray\"],\n537     dim: Union[str, \"DataArray\", pd.Index],\n538     data_vars: Union[str, List[str]],\n539     coords: Union[str, List[str]],\n540     compat: str,\n541     positions: Optional[Iterable[int]],\n542     fill_value: object = dtypes.NA,\n543     join: str = \"outer\",\n544     combine_attrs: str = \"override\",\n545 ) -> \"DataArray\":\n546     arrays = list(arrays)\n547 \n548     if data_vars != \"all\":\n549         raise ValueError(\n550             \"data_vars is not a valid argument when concatenating DataArray objects\"\n551         )\n552 \n553     datasets = []\n554     for n, arr in enumerate(arrays):\n555         if n == 0:\n556             name = arr.name\n557         elif name != arr.name:\n558             if compat == \"identical\":\n559                 raise ValueError(\"array names not identical\")\n560             else:\n561                 arr = arr.rename(name)\n562         datasets.append(arr._to_temp_dataset())\n563 \n564     ds = _dataset_concat(\n565         datasets,\n566         dim,\n567         data_vars,\n568         coords,\n569         compat,\n570         positions,\n571         fill_value=fill_value,\n572         join=join,\n573         combine_attrs=\"drop\",\n574     )\n575 \n576     merged_attrs = merge_attrs([da.attrs for da in arrays], combine_attrs)\n577 \n578     result = arrays[0]._from_temp_dataset(ds, name)\n579     result.attrs = merged_attrs\n580 \n581     return result\n",
        "intended_behavior": "Ensure these methods support the 'drop_conflicts' option within their 'combine_attrs' parameter processing. The new logic should assess attributes from the datasets being concatenated, excluding attributes with conflicting values from the concatenated result to adhere to the 'drop_conflicts' rule."
    }
]