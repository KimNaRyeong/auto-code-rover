2025-10-17 16:52:22.606 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-17 16:52:24.657 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=355, output_tokens=12, cost=0.003910
2025-10-17 16:52:38.285 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=535, output_tokens=535, cost=0.021400
2025-10-17 16:52:39.453 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 16:52:49.900 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=657, output_tokens=388, cost=0.018210
2025-10-17 16:52:49.908 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 16:52:53.071 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=742, output_tokens=101, cost=0.010450
2025-10-17 16:52:53.073 | DEBUG    | Extracted a valid json.
2025-10-17 16:52:53.073 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"Report\")",
        "search_method(\"generate_report\")",
        "search_method(\"format_skipped\")",
        "search_method(\"relpath\")",
        "search_code(\"path of tested file\")",
        "search_code(\"SKIPPED\")",
        "search_method(\"collect_tests\")",
        "search_method(\"execute_test\")",
        "search_code(\"SKIPPED\")"
    ],
    "bug_locations": []
}
2025-10-17 16:52:53.148 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-17 16:52:53.148 | DEBUG    | <Agent search> Analyzing search results.
2025-10-17 16:53:08.594 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1784, output_tokens=613, cost=0.036230
2025-10-17 16:53:08.606 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 16:53:19.735 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2613, output_tokens=418, cost=0.038670
2025-10-17 16:53:19.744 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 16:53:28.338 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=772, output_tokens=312, cost=0.017080
2025-10-17 16:53:28.339 | INFO     | Refactored invocation argument parsing failed on 'get_code_around_line("testing/test_terminal.py", line_number, 10)': malformed node or string on line 1: <ast.Name object at 0x7ea0bcaed890>
2025-10-17 16:53:28.340 | INFO     | Refactored invocation argument parsing failed on 'get_code_around_line("testing/test_skipping.py", line_number, 10)': malformed node or string on line 1: <ast.Name object at 0x7ea0bcaefed0>
2025-10-17 16:53:28.341 | INFO     | Refactored invocation argument parsing failed on 'get_code_around_line("src/_pytest/runner.py", line_number, 10)': malformed node or string on line 1: <ast.Name object at 0x7ea0bcaeed50>
2025-10-17 16:53:28.342 | DEBUG    | Extracted a valid json.
2025-10-17 16:53:28.342 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "get_code_around_line(\"testing/test_terminal.py\", line_number, 10)",
        "get_code_around_line(\"testing/test_skipping.py\", line_number, 10)",
        "get_code_around_line(\"src/_pytest/runner.py\", line_number, 10)"
    ],
    "bug_locations": [
        {
            "file": "testing/test_terminal.py",
            "expected_behavior": "This file is expected to handle terminal reporting, including the formatting of paths for skipped tests. The intended behavior here is to ensure that when reporting a skipped test, the path displayed should be relative to the current invocation directory, accounting for paths outside the invocation directory as well."
        },
        {
            "file": "testing/test_skipping.py",
            "expected_behavior": "While this mainly contains tests for skipping functionality, there could be utility functions or fixtures that impact how skipped paths are computed or represented in reports. The intended behavior would focus on accurately reflecting relative paths from the invocation directory, especially when involving tests declared outside of it."
        },
        {
            "file": "src/_pytest/runner.py",
            "expected_behavior": "As a core part of pytest that deals with the execution of tests, this file might contain logic that influences how the paths of skipped tests are determined and reported. The correction here should ensure paths are accurately calculated relative to the invocation point of pytest, keeping the representation consistent, whether tests are located inside or outside the invoked directory."
        }
    ]
}
2025-10-17 16:53:28.347 | INFO     | Refactored invocation argument parsing failed on 'get_code_around_line("testing/test_terminal.py", line_number, 10)': malformed node or string on line 1: <ast.Name object at 0x7ea0bbc70dd0>
2025-10-17 16:53:28.347 | ERROR    | invalid literal for int() with base 10: 'line_number'
Traceback (most recent call last):

  File "/home/kimnal0/auto-code-rover/app/main.py", line 656, in <module>
    main()
    └ <function main at 0x7ea0c4f22c00>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 120, in main
    run_task_groups(groups, num_processes, organize_output=True)
    │               │       └ 1
    │               └ {'setup_astropy__astropy__4.3': [<app.raw_tasks.RawSweTask object at 0x7e9fd295fb00>, <app.raw_tasks.RawSweTask object at 0x7...
    └ <function run_task_groups at 0x7e9fd4b7b240>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 396, in run_task_groups
    run_tasks_serial(all_tasks)
    │                └ [<app.raw_tasks.RawSweTask object at 0x7e9fd295fb00>, <app.raw_tasks.RawSweTask object at 0x7e9fd295fb30>, <app.raw_tasks.Raw...
    └ <function run_tasks_serial at 0x7e9fd4b7b2e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 414, in run_tasks_serial
    run_task_in_subprocess(task)
    │                      └ <app.raw_tasks.RawSweTask object at 0x7e9fd26e6ae0>
    └ <function run_task_in_subprocess at 0x7e9fd4b7b4c0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 462, in run_task_in_subprocess
    executor.submit(run_raw_task, task)
    │        │      │             └ <app.raw_tasks.RawSweTask object at 0x7e9fd26e6ae0>
    │        │      └ <function run_raw_task at 0x7e9fd4b7b560>
    │        └ <function ProcessPoolExecutor.submit at 0x7ea0c4842020>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7e9fd5a2e120>

  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 831, in submit
    self._start_executor_manager_thread()
    │    └ <function ProcessPoolExecutor._start_executor_manager_thread at 0x7ea0c4841da0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7e9fd5a2e120>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 770, in _start_executor_manager_thread
    self._launch_processes()
    │    └ <function ProcessPoolExecutor._launch_processes at 0x7ea0c4841ee0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7e9fd5a2e120>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 797, in _launch_processes
    self._spawn_process()
    │    └ <function ProcessPoolExecutor._spawn_process at 0x7ea0c4841f80>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7e9fd5a2e120>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 807, in _spawn_process
    p.start()
    │ └ <function BaseProcess.start at 0x7ea0c4a04ea0>
    └ <ForkProcess name='ForkProcess-310' parent=307479 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
    │    │        │    │      └ <ForkProcess name='ForkProcess-310' parent=307479 started>
    │    │        │    └ <staticmethod(<function ForkProcess._Popen at 0x7ea0c4a760c0>)>
    │    │        └ <ForkProcess name='ForkProcess-310' parent=307479 started>
    │    └ None
    └ <ForkProcess name='ForkProcess-310' parent=307479 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/context.py", line 282, in _Popen
    return Popen(process_obj)
           │     └ <ForkProcess name='ForkProcess-310' parent=307479 started>
           └ <class 'multiprocessing.popen_fork.Popen'>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
    │    │       └ <ForkProcess name='ForkProcess-310' parent=307479 started>
    │    └ <function Popen._launch at 0x7e9fd2ad4ea0>
    └ <multiprocessing.popen_fork.Popen object at 0x7e9fd314e450>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
           │           │                          └ 13
           │           └ <function BaseProcess._bootstrap at 0x7ea0c4a058a0>
           └ <ForkProcess name='ForkProcess-310' parent=307479 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x7ea0c4a04e00>
    └ <ForkProcess name='ForkProcess-310' parent=307479 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {}
    │    │        │    │        └ <ForkProcess name='ForkProcess-310' parent=307479 started>
    │    │        │    └ (<concurrent.futures.process._SafeQueue object at 0x7e9fd3864920>, <multiprocessing.queues.SimpleQueue object at 0x7e9fd2b908...
    │    │        └ <ForkProcess name='ForkProcess-310' parent=307479 started>
    │    └ <function _process_worker at 0x7ea0c48411c0>
    └ <ForkProcess name='ForkProcess-310' parent=307479 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 263, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
        │         │   │         │       │         └ {}
        │         │   │         │       └ <concurrent.futures.process._CallItem object at 0x7e9fd4fdd670>
        │         │   │         └ (<app.raw_tasks.RawSweTask object at 0x7e9fd314c650>,)
        │         │   └ <concurrent.futures.process._CallItem object at 0x7e9fd4fdd670>
        │         └ <function run_raw_task at 0x7e9fd4b7b560>
        └ <concurrent.futures.process._CallItem object at 0x7e9fd4fdd670>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 494, in run_raw_task
    run_ok = do_inference(task.to_task(), task_output_dir)
             │            │    │          └ '/home/kimnal0/auto-code-rover/only_fl_output3/pytest-dev__pytest-7158_2025-10-17_16-52-22'
             │            │    └ <function RawSweTask.to_task at 0x7e9fd4b7a520>
             │            └ <app.raw_tasks.RawSweTask object at 0x7e9fd314c650>
             └ <function do_inference at 0x7e9fd4b7b740>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 590, in do_inference
    run_ok = inference.run_one_task(
             │         └ <function run_one_task at 0x7e9fd4bd5080>
             └ <module 'app.inference' from '/home/kimnal0/auto-code-rover/app/inference.py'>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 128, in run_one_task
    if _run_one_task(str(out_dir), api_manager, task.get_issue_statement()):
       │                 │         │            │    └ <function SweTask.get_issue_statement at 0x7e9fd5360ae0>
       │                 │         │            └ SweTask(task_id='pytest-dev__pytest-7158', problem_statement='Wrong relative path in skip report when tested file is upper th...
       │                 │         └ <app.manage.ProjectApiManager object at 0x7e9fd314fd10>
       │                 └ Path('/home/kimnal0/auto-code-rover/only_fl_output3/pytest-dev__pytest-7158_2025-10-17_16-52-22/output_0')
       └ <function _run_one_task at 0x7e9fd4bd6660>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 303, in _run_one_task
    bug_locs, search_msg_thread = api_manager.search_manager.search_iterative(
                                  │           │              └ <function SearchManager.search_iterative at 0x7e9fd4fea840>
                                  │           └ <app.search.search_manage.SearchManager object at 0x7e9fd314ff80>
                                  └ <app.manage.ProjectApiManager object at 0x7e9fd314fd10>

  File "/home/kimnal0/auto-code-rover/app/search/search_manage.py", line 178, in search_iterative
    result_str, _, call_ok = function(**kwargs)
    │           │            │          └ {'file_name': 'testing/test_terminal.py', 'line_no_str': 'line_number', 'window_size_str': '10'}
    │           │            └ <bound method SearchBackend.get_code_around_line of <app.search.search_backend.SearchBackend object at 0x7e9fd532bf50>>
    │           └ [SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/pytest-dev__pytest/setup_pytest-dev__pytest__5.4/tes...
    └ 'Found 7 snippets containing `SKIPPED` in the codebase:\n\nThey appeared in the following files:\n- <file>testing/test_termin...

> File "/home/kimnal0/auto-code-rover/app/utils.py", line 343, in wrapper
    return func(*args, **kwargs)
           │     │       └ {'file_name': 'testing/test_terminal.py', 'line_no_str': 'line_number', 'window_size_str': '10'}
           │     └ (<app.search.search_backend.SearchBackend object at 0x7e9fd532bf50>,)
           └ <function SearchBackend.get_code_around_line at 0x7e9fd4fea3e0>

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 600, in get_code_around_line
    line_no = int(line_no_str)
                  └ 'line_number'

ValueError: invalid literal for int() with base 10: 'line_number'
2025-10-17 16:53:28.353 | INFO     | Refactored invocation argument parsing failed on 'get_code_around_line("testing/test_skipping.py", line_number, 10)': malformed node or string on line 1: <ast.Name object at 0x7e9fd4714a10>
2025-10-17 16:53:28.353 | ERROR    | invalid literal for int() with base 10: 'line_number'
Traceback (most recent call last):

  File "/home/kimnal0/auto-code-rover/app/main.py", line 656, in <module>
    main()
    └ <function main at 0x7ea0c4f22c00>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 120, in main
    run_task_groups(groups, num_processes, organize_output=True)
    │               │       └ 1
    │               └ {'setup_astropy__astropy__4.3': [<app.raw_tasks.RawSweTask object at 0x7e9fd295fb00>, <app.raw_tasks.RawSweTask object at 0x7...
    └ <function run_task_groups at 0x7e9fd4b7b240>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 396, in run_task_groups
    run_tasks_serial(all_tasks)
    │                └ [<app.raw_tasks.RawSweTask object at 0x7e9fd295fb00>, <app.raw_tasks.RawSweTask object at 0x7e9fd295fb30>, <app.raw_tasks.Raw...
    └ <function run_tasks_serial at 0x7e9fd4b7b2e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 414, in run_tasks_serial
    run_task_in_subprocess(task)
    │                      └ <app.raw_tasks.RawSweTask object at 0x7e9fd26e6ae0>
    └ <function run_task_in_subprocess at 0x7e9fd4b7b4c0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 462, in run_task_in_subprocess
    executor.submit(run_raw_task, task)
    │        │      │             └ <app.raw_tasks.RawSweTask object at 0x7e9fd26e6ae0>
    │        │      └ <function run_raw_task at 0x7e9fd4b7b560>
    │        └ <function ProcessPoolExecutor.submit at 0x7ea0c4842020>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7e9fd5a2e120>

  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 831, in submit
    self._start_executor_manager_thread()
    │    └ <function ProcessPoolExecutor._start_executor_manager_thread at 0x7ea0c4841da0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7e9fd5a2e120>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 770, in _start_executor_manager_thread
    self._launch_processes()
    │    └ <function ProcessPoolExecutor._launch_processes at 0x7ea0c4841ee0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7e9fd5a2e120>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 797, in _launch_processes
    self._spawn_process()
    │    └ <function ProcessPoolExecutor._spawn_process at 0x7ea0c4841f80>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7e9fd5a2e120>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 807, in _spawn_process
    p.start()
    │ └ <function BaseProcess.start at 0x7ea0c4a04ea0>
    └ <ForkProcess name='ForkProcess-310' parent=307479 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
    │    │        │    │      └ <ForkProcess name='ForkProcess-310' parent=307479 started>
    │    │        │    └ <staticmethod(<function ForkProcess._Popen at 0x7ea0c4a760c0>)>
    │    │        └ <ForkProcess name='ForkProcess-310' parent=307479 started>
    │    └ None
    └ <ForkProcess name='ForkProcess-310' parent=307479 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/context.py", line 282, in _Popen
    return Popen(process_obj)
           │     └ <ForkProcess name='ForkProcess-310' parent=307479 started>
           └ <class 'multiprocessing.popen_fork.Popen'>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
    │    │       └ <ForkProcess name='ForkProcess-310' parent=307479 started>
    │    └ <function Popen._launch at 0x7e9fd2ad4ea0>
    └ <multiprocessing.popen_fork.Popen object at 0x7e9fd314e450>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
           │           │                          └ 13
           │           └ <function BaseProcess._bootstrap at 0x7ea0c4a058a0>
           └ <ForkProcess name='ForkProcess-310' parent=307479 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x7ea0c4a04e00>
    └ <ForkProcess name='ForkProcess-310' parent=307479 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {}
    │    │        │    │        └ <ForkProcess name='ForkProcess-310' parent=307479 started>
    │    │        │    └ (<concurrent.futures.process._SafeQueue object at 0x7e9fd3864920>, <multiprocessing.queues.SimpleQueue object at 0x7e9fd2b908...
    │    │        └ <ForkProcess name='ForkProcess-310' parent=307479 started>
    │    └ <function _process_worker at 0x7ea0c48411c0>
    └ <ForkProcess name='ForkProcess-310' parent=307479 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 263, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
        │         │   │         │       │         └ {}
        │         │   │         │       └ <concurrent.futures.process._CallItem object at 0x7e9fd4fdd670>
        │         │   │         └ (<app.raw_tasks.RawSweTask object at 0x7e9fd314c650>,)
        │         │   └ <concurrent.futures.process._CallItem object at 0x7e9fd4fdd670>
        │         └ <function run_raw_task at 0x7e9fd4b7b560>
        └ <concurrent.futures.process._CallItem object at 0x7e9fd4fdd670>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 494, in run_raw_task
    run_ok = do_inference(task.to_task(), task_output_dir)
             │            │    │          └ '/home/kimnal0/auto-code-rover/only_fl_output3/pytest-dev__pytest-7158_2025-10-17_16-52-22'
             │            │    └ <function RawSweTask.to_task at 0x7e9fd4b7a520>
             │            └ <app.raw_tasks.RawSweTask object at 0x7e9fd314c650>
             └ <function do_inference at 0x7e9fd4b7b740>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 590, in do_inference
    run_ok = inference.run_one_task(
             │         └ <function run_one_task at 0x7e9fd4bd5080>
             └ <module 'app.inference' from '/home/kimnal0/auto-code-rover/app/inference.py'>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 128, in run_one_task
    if _run_one_task(str(out_dir), api_manager, task.get_issue_statement()):
       │                 │         │            │    └ <function SweTask.get_issue_statement at 0x7e9fd5360ae0>
       │                 │         │            └ SweTask(task_id='pytest-dev__pytest-7158', problem_statement='Wrong relative path in skip report when tested file is upper th...
       │                 │         └ <app.manage.ProjectApiManager object at 0x7e9fd314fd10>
       │                 └ Path('/home/kimnal0/auto-code-rover/only_fl_output3/pytest-dev__pytest-7158_2025-10-17_16-52-22/output_0')
       └ <function _run_one_task at 0x7e9fd4bd6660>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 303, in _run_one_task
    bug_locs, search_msg_thread = api_manager.search_manager.search_iterative(
                                  │           │              └ <function SearchManager.search_iterative at 0x7e9fd4fea840>
                                  │           └ <app.search.search_manage.SearchManager object at 0x7e9fd314ff80>
                                  └ <app.manage.ProjectApiManager object at 0x7e9fd314fd10>

  File "/home/kimnal0/auto-code-rover/app/search/search_manage.py", line 178, in search_iterative
    result_str, _, call_ok = function(**kwargs)
    │           │            │          └ {'file_name': 'testing/test_skipping.py', 'line_no_str': 'line_number', 'window_size_str': '10'}
    │           │            └ <bound method SearchBackend.get_code_around_line of <app.search.search_backend.SearchBackend object at 0x7e9fd532bf50>>
    │           └ 'The tool returned error message.'
    └ "invalid literal for int() with base 10: 'line_number'"

> File "/home/kimnal0/auto-code-rover/app/utils.py", line 343, in wrapper
    return func(*args, **kwargs)
           │     │       └ {'file_name': 'testing/test_skipping.py', 'line_no_str': 'line_number', 'window_size_str': '10'}
           │     └ (<app.search.search_backend.SearchBackend object at 0x7e9fd532bf50>,)
           └ <function SearchBackend.get_code_around_line at 0x7e9fd4fea3e0>

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 600, in get_code_around_line
    line_no = int(line_no_str)
                  └ 'line_number'

ValueError: invalid literal for int() with base 10: 'line_number'
2025-10-17 16:53:28.356 | INFO     | Refactored invocation argument parsing failed on 'get_code_around_line("src/_pytest/runner.py", line_number, 10)': malformed node or string on line 1: <ast.Name object at 0x7e9fd2954690>
2025-10-17 16:53:28.357 | ERROR    | invalid literal for int() with base 10: 'line_number'
Traceback (most recent call last):

  File "/home/kimnal0/auto-code-rover/app/main.py", line 656, in <module>
    main()
    └ <function main at 0x7ea0c4f22c00>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 120, in main
    run_task_groups(groups, num_processes, organize_output=True)
    │               │       └ 1
    │               └ {'setup_astropy__astropy__4.3': [<app.raw_tasks.RawSweTask object at 0x7e9fd295fb00>, <app.raw_tasks.RawSweTask object at 0x7...
    └ <function run_task_groups at 0x7e9fd4b7b240>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 396, in run_task_groups
    run_tasks_serial(all_tasks)
    │                └ [<app.raw_tasks.RawSweTask object at 0x7e9fd295fb00>, <app.raw_tasks.RawSweTask object at 0x7e9fd295fb30>, <app.raw_tasks.Raw...
    └ <function run_tasks_serial at 0x7e9fd4b7b2e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 414, in run_tasks_serial
    run_task_in_subprocess(task)
    │                      └ <app.raw_tasks.RawSweTask object at 0x7e9fd26e6ae0>
    └ <function run_task_in_subprocess at 0x7e9fd4b7b4c0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 462, in run_task_in_subprocess
    executor.submit(run_raw_task, task)
    │        │      │             └ <app.raw_tasks.RawSweTask object at 0x7e9fd26e6ae0>
    │        │      └ <function run_raw_task at 0x7e9fd4b7b560>
    │        └ <function ProcessPoolExecutor.submit at 0x7ea0c4842020>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7e9fd5a2e120>

  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 831, in submit
    self._start_executor_manager_thread()
    │    └ <function ProcessPoolExecutor._start_executor_manager_thread at 0x7ea0c4841da0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7e9fd5a2e120>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 770, in _start_executor_manager_thread
    self._launch_processes()
    │    └ <function ProcessPoolExecutor._launch_processes at 0x7ea0c4841ee0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7e9fd5a2e120>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 797, in _launch_processes
    self._spawn_process()
    │    └ <function ProcessPoolExecutor._spawn_process at 0x7ea0c4841f80>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7e9fd5a2e120>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 807, in _spawn_process
    p.start()
    │ └ <function BaseProcess.start at 0x7ea0c4a04ea0>
    └ <ForkProcess name='ForkProcess-310' parent=307479 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
    │    │        │    │      └ <ForkProcess name='ForkProcess-310' parent=307479 started>
    │    │        │    └ <staticmethod(<function ForkProcess._Popen at 0x7ea0c4a760c0>)>
    │    │        └ <ForkProcess name='ForkProcess-310' parent=307479 started>
    │    └ None
    └ <ForkProcess name='ForkProcess-310' parent=307479 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/context.py", line 282, in _Popen
    return Popen(process_obj)
           │     └ <ForkProcess name='ForkProcess-310' parent=307479 started>
           └ <class 'multiprocessing.popen_fork.Popen'>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
    │    │       └ <ForkProcess name='ForkProcess-310' parent=307479 started>
    │    └ <function Popen._launch at 0x7e9fd2ad4ea0>
    └ <multiprocessing.popen_fork.Popen object at 0x7e9fd314e450>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
           │           │                          └ 13
           │           └ <function BaseProcess._bootstrap at 0x7ea0c4a058a0>
           └ <ForkProcess name='ForkProcess-310' parent=307479 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x7ea0c4a04e00>
    └ <ForkProcess name='ForkProcess-310' parent=307479 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {}
    │    │        │    │        └ <ForkProcess name='ForkProcess-310' parent=307479 started>
    │    │        │    └ (<concurrent.futures.process._SafeQueue object at 0x7e9fd3864920>, <multiprocessing.queues.SimpleQueue object at 0x7e9fd2b908...
    │    │        └ <ForkProcess name='ForkProcess-310' parent=307479 started>
    │    └ <function _process_worker at 0x7ea0c48411c0>
    └ <ForkProcess name='ForkProcess-310' parent=307479 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 263, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
        │         │   │         │       │         └ {}
        │         │   │         │       └ <concurrent.futures.process._CallItem object at 0x7e9fd4fdd670>
        │         │   │         └ (<app.raw_tasks.RawSweTask object at 0x7e9fd314c650>,)
        │         │   └ <concurrent.futures.process._CallItem object at 0x7e9fd4fdd670>
        │         └ <function run_raw_task at 0x7e9fd4b7b560>
        └ <concurrent.futures.process._CallItem object at 0x7e9fd4fdd670>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 494, in run_raw_task
    run_ok = do_inference(task.to_task(), task_output_dir)
             │            │    │          └ '/home/kimnal0/auto-code-rover/only_fl_output3/pytest-dev__pytest-7158_2025-10-17_16-52-22'
             │            │    └ <function RawSweTask.to_task at 0x7e9fd4b7a520>
             │            └ <app.raw_tasks.RawSweTask object at 0x7e9fd314c650>
             └ <function do_inference at 0x7e9fd4b7b740>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 590, in do_inference
    run_ok = inference.run_one_task(
             │         └ <function run_one_task at 0x7e9fd4bd5080>
             └ <module 'app.inference' from '/home/kimnal0/auto-code-rover/app/inference.py'>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 128, in run_one_task
    if _run_one_task(str(out_dir), api_manager, task.get_issue_statement()):
       │                 │         │            │    └ <function SweTask.get_issue_statement at 0x7e9fd5360ae0>
       │                 │         │            └ SweTask(task_id='pytest-dev__pytest-7158', problem_statement='Wrong relative path in skip report when tested file is upper th...
       │                 │         └ <app.manage.ProjectApiManager object at 0x7e9fd314fd10>
       │                 └ Path('/home/kimnal0/auto-code-rover/only_fl_output3/pytest-dev__pytest-7158_2025-10-17_16-52-22/output_0')
       └ <function _run_one_task at 0x7e9fd4bd6660>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 303, in _run_one_task
    bug_locs, search_msg_thread = api_manager.search_manager.search_iterative(
                                  │           │              └ <function SearchManager.search_iterative at 0x7e9fd4fea840>
                                  │           └ <app.search.search_manage.SearchManager object at 0x7e9fd314ff80>
                                  └ <app.manage.ProjectApiManager object at 0x7e9fd314fd10>

  File "/home/kimnal0/auto-code-rover/app/search/search_manage.py", line 178, in search_iterative
    result_str, _, call_ok = function(**kwargs)
    │           │            │          └ {'file_name': 'src/_pytest/runner.py', 'line_no_str': 'line_number', 'window_size_str': '10'}
    │           │            └ <bound method SearchBackend.get_code_around_line of <app.search.search_backend.SearchBackend object at 0x7e9fd532bf50>>
    │           └ 'The tool returned error message.'
    └ "invalid literal for int() with base 10: 'line_number'"

> File "/home/kimnal0/auto-code-rover/app/utils.py", line 343, in wrapper
    return func(*args, **kwargs)
           │     │       └ {'file_name': 'src/_pytest/runner.py', 'line_no_str': 'line_number', 'window_size_str': '10'}
           │     └ (<app.search.search_backend.SearchBackend object at 0x7e9fd532bf50>,)
           └ <function SearchBackend.get_code_around_line at 0x7e9fd4fea3e0>

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 600, in get_code_around_line
    line_no = int(line_no_str)
                  └ 'line_number'

ValueError: invalid literal for int() with base 10: 'line_number'
2025-10-17 16:53:28.361 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-17 16:53:28.362 | DEBUG    | <Agent search> Analyzing search results.
2025-10-17 16:53:39.035 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3244, output_tokens=434, cost=0.045460
2025-10-17 16:53:39.044 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 16:53:49.802 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3894, output_tokens=407, cost=0.051150
2025-10-17 16:53:49.810 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 16:53:57.323 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=761, output_tokens=240, cost=0.014810
2025-10-17 16:53:57.324 | DEBUG    | Extracted a valid json.
2025-10-17 16:53:57.324 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "src/_pytest/runner.py",
            "class": "",
            "method": "",
            "intended_behavior": "Ensure that when a test is skipped, the method responsible for generating the report entry should dynamically calculate the path relative to the current invocation directory, not just from a static root directory or the file's location. The existing functionality for collecting and reporting test outcomes should be preserved, with the extension to handle relative path calculation more gracefully."
        },
        {
            "file": "testing/test_terminal.py",
            "class": "",
            "method": "",
            "intended_behavior": "The function or method responsible for outputting the skipped test report to the terminal should ensure the path displayed is relative to the directory from which `pytest` was invoked. This may require integrating with the file path calculation logic to adjust how the final path is resolved before being displayed. The goal is to make sure that regardless of where the test file resides in relation to the invocation directory, the displayed path accurately reflects its location in a manner consistent with the user's command-line input."
        }
    ]
}
2025-10-17 16:53:57.338 | DEBUG    | Bug location extracted successfully: [<file>src/_pytest/runner.py</file>
<class>None</class>
<method>None</method>
<code>
1 """ basic collect and runtest protocol implementations """
2 import bdb
3 import os
4 import sys
5 from time import perf_counter  # Intentionally not `import time` to avoid being
6 from time import time  # affected by tests which monkeypatch `time` (issue #185).
7 from typing import Callable
8 from typing import Dict
9 from typing import List
10 from typing import Optional
11 from typing import Tuple
12 
13 import attr
14 
15 from .reports import CollectErrorRepr
16 from .reports import CollectReport
17 from .reports import TestReport
18 from _pytest._code.code import ExceptionChainRepr
19 from _pytest._code.code import ExceptionInfo
20 from _pytest.compat import TYPE_CHECKING
21 from _pytest.nodes import Collector
22 from _pytest.nodes import Node
23 from _pytest.outcomes import Exit
24 from _pytest.outcomes import Skipped
25 from _pytest.outcomes import TEST_OUTCOME
26 
27 if TYPE_CHECKING:
28     from typing import Type
29     from typing_extensions import Literal
30 
31 #
32 # pytest plugin hooks
33 
34 
35 def pytest_addoption(parser):
36     group = parser.getgroup("terminal reporting", "reporting", after="general")
37     group.addoption(
38         "--durations",
39         action="store",
40         type=int,
41         default=None,
42         metavar="N",
43         help="show N slowest setup/test durations (N=0 for all).",
44     )
45 
46 
47 def pytest_terminal_summary(terminalreporter):
48     durations = terminalreporter.config.option.durations
49     verbose = terminalreporter.config.getvalue("verbose")
50     if durations is None:
51         return
52     tr = terminalreporter
53     dlist = []
54     for replist in tr.stats.values():
55         for rep in replist:
56             if hasattr(rep, "duration"):
57                 dlist.append(rep)
58     if not dlist:
59         return
60     dlist.sort(key=lambda x: x.duration)
61     dlist.reverse()
62     if not durations:
63         tr.write_sep("=", "slowest durations")
64     else:
65         tr.write_sep("=", "slowest %s durations" % durations)
66         dlist = dlist[:durations]
67 
68     for i, rep in enumerate(dlist):
69         if verbose < 2 and rep.duration < 0.005:
70             tr.write_line("")
71             tr.write_line(
72                 "(%s durations < 0.005s hidden.  Use -vv to show these durations.)"
73                 % (len(dlist) - i)
74             )
75             break
76         tr.write_line("{:02.2f}s {:<8} {}".format(rep.duration, rep.when, rep.nodeid))
77 
78 
79 def pytest_sessionstart(session):
80     session._setupstate = SetupState()
81 
82 
83 def pytest_sessionfinish(session):
84     session._setupstate.teardown_all()
85 
86 
87 def pytest_runtest_protocol(item, nextitem):
88     item.ihook.pytest_runtest_logstart(nodeid=item.nodeid, location=item.location)
89     runtestprotocol(item, nextitem=nextitem)
90     item.ihook.pytest_runtest_logfinish(nodeid=item.nodeid, location=item.location)
91     return True
92 
93 
94 def runtestprotocol(item, log=True, nextitem=None):
95     hasrequest = hasattr(item, "_request")
96     if hasrequest and not item._request:
97         item._initrequest()
98     rep = call_and_report(item, "setup", log)
99     reports = [rep]
100     if rep.passed:
101         if item.config.getoption("setupshow", False):
102             show_test_item(item)
103         if not item.config.getoption("setuponly", False):
104             reports.append(call_and_report(item, "call", log))
105     reports.append(call_and_report(item, "teardown", log, nextitem=nextitem))
106     # after all teardown hooks have been called
107     # want funcargs and request info to go away
108     if hasrequest:
109         item._request = False
110         item.funcargs = None
111     return reports
112 
113 
114 def show_test_item(item):
115     """Show test function, parameters and the fixtures of the test item."""
116     tw = item.config.get_terminal_writer()
117     tw.line()
118     tw.write(" " * 8)
119     tw.write(item.nodeid)
120     used_fixtures = sorted(getattr(item, "fixturenames", []))
121     if used_fixtures:
122         tw.write(" (fixtures used: {})".format(", ".join(used_fixtures)))
123 
124 
125 def pytest_runtest_setup(item):
126     _update_current_test_var(item, "setup")
127     item.session._setupstate.prepare(item)
128 
129 
130 def pytest_runtest_call(item):
131     _update_current_test_var(item, "call")
132     try:
133         del sys.last_type
134         del sys.last_value
135         del sys.last_traceback
136     except AttributeError:
137         pass
138     try:
139         item.runtest()
140     except Exception as e:
141         # Store trace info to allow postmortem debugging
142         sys.last_type = type(e)
143         sys.last_value = e
144         assert e.__traceback__ is not None
145         # Skip *this* frame
146         sys.last_traceback = e.__traceback__.tb_next
147         raise e
148 
149 
150 def pytest_runtest_teardown(item, nextitem):
151     _update_current_test_var(item, "teardown")
152     item.session._setupstate.teardown_exact(item, nextitem)
153     _update_current_test_var(item, None)
154 
155 
156 def _update_current_test_var(item, when):
157     """
158     Update :envvar:`PYTEST_CURRENT_TEST` to reflect the current item and stage.
159 
160     If ``when`` is None, delete ``PYTEST_CURRENT_TEST`` from the environment.
161     """
162     var_name = "PYTEST_CURRENT_TEST"
163     if when:
164         value = "{} ({})".format(item.nodeid, when)
165         # don't allow null bytes on environment variables (see #2644, #2957)
166         value = value.replace("\x00", "(null)")
167         os.environ[var_name] = value
168     else:
169         os.environ.pop(var_name)
170 
171 
172 def pytest_report_teststatus(report):
173     if report.when in ("setup", "teardown"):
174         if report.failed:
175             #      category, shortletter, verbose-word
176             return "error", "E", "ERROR"
177         elif report.skipped:
178             return "skipped", "s", "SKIPPED"
179         else:
180             return "", "", ""
181 
182 
183 #
184 # Implementation
185 
186 
187 def call_and_report(
188     item, when: "Literal['setup', 'call', 'teardown']", log=True, **kwds
189 ):
190     call = call_runtest_hook(item, when, **kwds)
191     hook = item.ihook
192     report = hook.pytest_runtest_makereport(item=item, call=call)
193     if log:
194         hook.pytest_runtest_logreport(report=report)
195     if check_interactive_exception(call, report):
196         hook.pytest_exception_interact(node=item, call=call, report=report)
197     return report
198 
199 
200 def check_interactive_exception(call, report):
201     return call.excinfo and not (
202         hasattr(report, "wasxfail")
203         or call.excinfo.errisinstance(Skipped)
204         or call.excinfo.errisinstance(bdb.BdbQuit)
205     )
206 
207 
208 def call_runtest_hook(item, when: "Literal['setup', 'call', 'teardown']", **kwds):
209     if when == "setup":
210         ihook = item.ihook.pytest_runtest_setup
211     elif when == "call":
212         ihook = item.ihook.pytest_runtest_call
213     elif when == "teardown":
214         ihook = item.ihook.pytest_runtest_teardown
215     else:
216         assert False, "Unhandled runtest hook case: {}".format(when)
217     reraise = (Exit,)  # type: Tuple[Type[BaseException], ...]
218     if not item.config.getoption("usepdb", False):
219         reraise += (KeyboardInterrupt,)
220     return CallInfo.from_call(
221         lambda: ihook(item=item, **kwds), when=when, reraise=reraise
222     )
223 
224 
225 @attr.s(repr=False)
226 class CallInfo:
227     """ Result/Exception info a function invocation.
228 
229     :param result: The return value of the call, if it didn't raise. Can only be accessed
230         if excinfo is None.
231     :param Optional[ExceptionInfo] excinfo: The captured exception of the call, if it raised.
232     :param float start: The system time when the call started, in seconds since the epoch.
233     :param float stop: The system time when the call ended, in seconds since the epoch.
234     :param float duration: The call duration, in seconds.
235     :param str when: The context of invocation: "setup", "call", "teardown", ...
236     """
237 
238     _result = attr.ib()
239     excinfo = attr.ib(type=Optional[ExceptionInfo])
240     start = attr.ib(type=float)
241     stop = attr.ib(type=float)
242     duration = attr.ib(type=float)
243     when = attr.ib(type=str)
244 
245     @property
246     def result(self):
247         if self.excinfo is not None:
248             raise AttributeError("{!r} has no valid result".format(self))
249         return self._result
250 
251     @classmethod
252     def from_call(cls, func, when, reraise=None) -> "CallInfo":
253         #: context of invocation: one of "setup", "call",
254         #: "teardown", "memocollect"
255         excinfo = None
256         start = time()
257         precise_start = perf_counter()
258         try:
259             result = func()
260         except:  # noqa
261             excinfo = ExceptionInfo.from_current()
262             if reraise is not None and excinfo.errisinstance(reraise):
263                 raise
264             result = None
265         # use the perf counter
266         precise_stop = perf_counter()
267         duration = precise_stop - precise_start
268         stop = time()
269         return cls(
270             start=start,
271             stop=stop,
272             duration=duration,
273             when=when,
274             result=result,
275             excinfo=excinfo,
276         )
277 
278     def __repr__(self):
279         if self.excinfo is None:
280             return "<CallInfo when={!r} result: {!r}>".format(self.when, self._result)
281         return "<CallInfo when={!r} excinfo={!r}>".format(self.when, self.excinfo)
282 
283 
284 def pytest_runtest_makereport(item, call):
285     return TestReport.from_item_and_call(item, call)
286 
287 
288 def pytest_make_collect_report(collector: Collector) -> CollectReport:
289     call = CallInfo.from_call(lambda: list(collector.collect()), "collect")
290     longrepr = None
291     if not call.excinfo:
292         outcome = "passed"
293     else:
294         skip_exceptions = [Skipped]
295         unittest = sys.modules.get("unittest")
296         if unittest is not None:
297             # Type ignored because unittest is loaded dynamically.
298             skip_exceptions.append(unittest.SkipTest)  # type: ignore
299         if call.excinfo.errisinstance(tuple(skip_exceptions)):
300             outcome = "skipped"
301             r_ = collector._repr_failure_py(call.excinfo, "line")
302             assert isinstance(r_, ExceptionChainRepr), repr(r_)
303             r = r_.reprcrash
304             assert r
305             longrepr = (str(r.path), r.lineno, r.message)
306         else:
307             outcome = "failed"
308             errorinfo = collector.repr_failure(call.excinfo)
309             if not hasattr(errorinfo, "toterminal"):
310                 errorinfo = CollectErrorRepr(errorinfo)
311             longrepr = errorinfo
312     rep = CollectReport(
313         collector.nodeid, outcome, longrepr, getattr(call, "result", None)
314     )
315     rep.call = call  # type: ignore # see collect_one_node
316     return rep
317 
318 
319 class SetupState:
320     """ shared state for setting up/tearing down test items or collectors. """
321 
322     def __init__(self):
323         self.stack = []  # type: List[Node]
324         self._finalizers = {}  # type: Dict[Node, List[Callable[[], None]]]
325 
326     def addfinalizer(self, finalizer, colitem):
327         """ attach a finalizer to the given colitem. """
328         assert colitem and not isinstance(colitem, tuple)
329         assert callable(finalizer)
330         # assert colitem in self.stack  # some unit tests don't setup stack :/
331         self._finalizers.setdefault(colitem, []).append(finalizer)
332 
333     def _pop_and_teardown(self):
334         colitem = self.stack.pop()
335         self._teardown_with_finalization(colitem)
336 
337     def _callfinalizers(self, colitem):
338         finalizers = self._finalizers.pop(colitem, None)
339         exc = None
340         while finalizers:
341             fin = finalizers.pop()
342             try:
343                 fin()
344             except TEST_OUTCOME as e:
345                 # XXX Only first exception will be seen by user,
346                 #     ideally all should be reported.
347                 if exc is None:
348                     exc = e
349         if exc:
350             raise exc
351 
352     def _teardown_with_finalization(self, colitem):
353         self._callfinalizers(colitem)
354         colitem.teardown()
355         for colitem in self._finalizers:
356             assert colitem in self.stack
357 
358     def teardown_all(self):
359         while self.stack:
360             self._pop_and_teardown()
361         for key in list(self._finalizers):
362             self._teardown_with_finalization(key)
363         assert not self._finalizers
364 
365     def teardown_exact(self, item, nextitem):
366         needed_collectors = nextitem and nextitem.listchain() or []
367         self._teardown_towards(needed_collectors)
368 
369     def _teardown_towards(self, needed_collectors):
370         exc = None
371         while self.stack:
372             if self.stack == needed_collectors[: len(self.stack)]:
373                 break
374             try:
375                 self._pop_and_teardown()
376             except TEST_OUTCOME as e:
377                 # XXX Only first exception will be seen by user,
378                 #     ideally all should be reported.
379                 if exc is None:
380                     exc = e
381         if exc:
382             raise exc
383 
384     def prepare(self, colitem):
385         """ setup objects along the collector chain to the test-method
386             and teardown previously setup objects."""
387         needed_collectors = colitem.listchain()
388         self._teardown_towards(needed_collectors)
389 
390         # check if the last collection node has raised an error
391         for col in self.stack:
392             if hasattr(col, "_prepare_exc"):
393                 exc = col._prepare_exc
394                 raise exc
395         for col in needed_collectors[len(self.stack) :]:
396             self.stack.append(col)
397             try:
398                 col.setup()
399             except TEST_OUTCOME as e:
400                 col._prepare_exc = e
401                 raise e
402 
403 
404 def collect_one_node(collector):
405     ihook = collector.ihook
406     ihook.pytest_collectstart(collector=collector)
407     rep = ihook.pytest_make_collect_report(collector=collector)
408     call = rep.__dict__.pop("call", None)
409     if call and check_interactive_exception(call, rep):
410         ihook.pytest_exception_interact(node=collector, call=call, report=rep)
411     return rep

</code><intended_behavior>Ensure that when a test is skipped, the method responsible for generating the report entry should dynamically calculate the path relative to the current invocation directory, not just from a static root directory or the file's location. The existing functionality for collecting and reporting test outcomes should be preserved, with the extension to handle relative path calculation more gracefully.</intended_behavior>, <file>testing/test_terminal.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 terminal reporting of the full testing process.
3 """
4 import collections
5 import os
6 import sys
7 import textwrap
8 from io import StringIO
9 from typing import Dict
10 from typing import List
11 from typing import Tuple
12 
13 import pluggy
14 import py
15 
16 import _pytest.config
17 import pytest
18 from _pytest.config import ExitCode
19 from _pytest.pytester import Testdir
20 from _pytest.reports import BaseReport
21 from _pytest.terminal import _folded_skips
22 from _pytest.terminal import _get_line_with_reprcrash_message
23 from _pytest.terminal import _plugin_nameversions
24 from _pytest.terminal import getreportopt
25 from _pytest.terminal import TerminalReporter
26 
27 DistInfo = collections.namedtuple("DistInfo", ["project_name", "version"])
28 
29 
30 TRANS_FNMATCH = str.maketrans({"[": "[[]", "]": "[]]"})
31 
32 
33 class Option:
34     def __init__(self, verbosity=0):
35         self.verbosity = verbosity
36 
37     @property
38     def args(self):
39         values = []
40         values.append("--verbosity=%d" % self.verbosity)
41         return values
42 
43 
44 @pytest.fixture(
45     params=[Option(verbosity=0), Option(verbosity=1), Option(verbosity=-1)],
46     ids=["default", "verbose", "quiet"],
47 )
48 def option(request):
49     return request.param
50 
51 
52 @pytest.mark.parametrize(
53     "input,expected",
54     [
55         ([DistInfo(project_name="test", version=1)], ["test-1"]),
56         ([DistInfo(project_name="pytest-test", version=1)], ["test-1"]),
57         (
58             [
59                 DistInfo(project_name="test", version=1),
60                 DistInfo(project_name="test", version=1),
61             ],
62             ["test-1"],
63         ),
64     ],
65     ids=["normal", "prefix-strip", "deduplicate"],
66 )
67 def test_plugin_nameversion(input, expected):
68     pluginlist = [(None, x) for x in input]
69     result = _plugin_nameversions(pluginlist)
70     assert result == expected
71 
72 
73 class TestTerminal:
74     def test_pass_skip_fail(self, testdir, option):
75         testdir.makepyfile(
76             """
77             import pytest
78             def test_ok():
79                 pass
80             def test_skip():
81                 pytest.skip("xx")
82             def test_func():
83                 assert 0
84         """
85         )
86         result = testdir.runpytest(*option.args)
87         if option.verbosity > 0:
88             result.stdout.fnmatch_lines(
89                 [
90                     "*test_pass_skip_fail.py::test_ok PASS*",
91                     "*test_pass_skip_fail.py::test_skip SKIP*",
92                     "*test_pass_skip_fail.py::test_func FAIL*",
93                 ]
94             )
95         elif option.verbosity == 0:
96             result.stdout.fnmatch_lines(["*test_pass_skip_fail.py .sF*"])
97         else:
98             result.stdout.fnmatch_lines([".sF*"])
99         result.stdout.fnmatch_lines(
100             ["    def test_func():", ">       assert 0", "E       assert 0"]
101         )
102 
103     def test_internalerror(self, testdir, linecomp):
104         modcol = testdir.getmodulecol("def test_one(): pass")
105         rep = TerminalReporter(modcol.config, file=linecomp.stringio)
106         with pytest.raises(ValueError) as excinfo:
107             raise ValueError("hello")
108         rep.pytest_internalerror(excinfo.getrepr())
109         linecomp.assert_contains_lines(["INTERNALERROR> *ValueError*hello*"])
110 
111     def test_writeline(self, testdir, linecomp):
112         modcol = testdir.getmodulecol("def test_one(): pass")
113         rep = TerminalReporter(modcol.config, file=linecomp.stringio)
114         rep.write_fspath_result(modcol.nodeid, ".")
115         rep.write_line("hello world")
116         lines = linecomp.stringio.getvalue().split("\n")
117         assert not lines[0]
118         assert lines[1].endswith(modcol.name + " .")
119         assert lines[2] == "hello world"
120 
121     def test_show_runtest_logstart(self, testdir, linecomp):
122         item = testdir.getitem("def test_func(): pass")
123         tr = TerminalReporter(item.config, file=linecomp.stringio)
124         item.config.pluginmanager.register(tr)
125         location = item.reportinfo()
126         tr.config.hook.pytest_runtest_logstart(
127             nodeid=item.nodeid, location=location, fspath=str(item.fspath)
128         )
129         linecomp.assert_contains_lines(["*test_show_runtest_logstart.py*"])
130 
131     def test_runtest_location_shown_before_test_starts(self, testdir):
132         testdir.makepyfile(
133             """
134             def test_1():
135                 import time
136                 time.sleep(20)
137         """
138         )
139         child = testdir.spawn_pytest("")
140         child.expect(".*test_runtest_location.*py")
141         child.sendeof()
142         child.kill(15)
143 
144     def test_report_collect_after_half_a_second(self, testdir):
145         """Test for "collecting" being updated after 0.5s"""
146 
147         testdir.makepyfile(
148             **{
149                 "test1.py": """
150                 import _pytest.terminal
151 
152                 _pytest.terminal.REPORT_COLLECTING_RESOLUTION = 0
153 
154                 def test_1():
155                     pass
156                     """,
157                 "test2.py": "def test_2(): pass",
158             }
159         )
160         # Explicitly test colored output.
161         testdir.monkeypatch.setenv("PY_COLORS", "1")
162 
163         child = testdir.spawn_pytest("-v test1.py test2.py")
164         child.expect(r"collecting \.\.\.")
165         child.expect(r"collecting 1 item")
166         child.expect(r"collecting 2 items")
167         child.expect(r"collected 2 items")
168         rest = child.read().decode("utf8")
169         assert "= \x1b[32m\x1b[1m2 passed\x1b[0m\x1b[32m in" in rest
170 
171     def test_itemreport_subclasses_show_subclassed_file(self, testdir):
172         testdir.makepyfile(
173             **{
174                 "tests/test_p1": """
175             class BaseTests(object):
176                 fail = False
177 
178                 def test_p1(self):
179                     if self.fail: assert 0
180                 """,
181                 "tests/test_p2": """
182             from test_p1 import BaseTests
183 
184             class TestMore(BaseTests): pass
185                 """,
186                 "tests/test_p3.py": """
187             from test_p1 import BaseTests
188 
189             BaseTests.fail = True
190 
191             class TestMore(BaseTests): pass
192         """,
193             }
194         )
195         result = testdir.runpytest("tests/test_p2.py", "--rootdir=tests")
196         result.stdout.fnmatch_lines(["tests/test_p2.py .*", "=* 1 passed in *"])
197 
198         result = testdir.runpytest("-vv", "-rA", "tests/test_p2.py", "--rootdir=tests")
199         result.stdout.fnmatch_lines(
200             [
201                 "tests/test_p2.py::TestMore::test_p1 <- test_p1.py PASSED *",
202                 "*= short test summary info =*",
203                 "PASSED tests/test_p2.py::TestMore::test_p1",
204             ]
205         )
206         result = testdir.runpytest("-vv", "-rA", "tests/test_p3.py", "--rootdir=tests")
207         result.stdout.fnmatch_lines(
208             [
209                 "tests/test_p3.py::TestMore::test_p1 <- test_p1.py FAILED *",
210                 "*_ TestMore.test_p1 _*",
211                 "    def test_p1(self):",
212                 ">       if self.fail: assert 0",
213                 "E       assert 0",
214                 "",
215                 "tests/test_p1.py:5: AssertionError",
216                 "*= short test summary info =*",
217                 "FAILED tests/test_p3.py::TestMore::test_p1 - assert 0",
218                 "*= 1 failed in *",
219             ]
220         )
221 
222     def test_itemreport_directclasses_not_shown_as_subclasses(self, testdir):
223         a = testdir.mkpydir("a123")
224         a.join("test_hello123.py").write(
225             textwrap.dedent(
226                 """\
227                 class TestClass(object):
228                     def test_method(self):
229                         pass
230                 """
231             )
232         )
233         result = testdir.runpytest("-vv")
234         assert result.ret == 0
235         result.stdout.fnmatch_lines(["*a123/test_hello123.py*PASS*"])
236         result.stdout.no_fnmatch_line("* <- *")
237 
238     @pytest.mark.parametrize("fulltrace", ("", "--fulltrace"))
239     def test_keyboard_interrupt(self, testdir, fulltrace):
240         testdir.makepyfile(
241             """
242             def test_foobar():
243                 assert 0
244             def test_spamegg():
245                 import py; pytest.skip('skip me please!')
246             def test_interrupt_me():
247                 raise KeyboardInterrupt   # simulating the user
248         """
249         )
250 
251         result = testdir.runpytest(fulltrace, no_reraise_ctrlc=True)
252         result.stdout.fnmatch_lines(
253             [
254                 "    def test_foobar():",
255                 ">       assert 0",
256                 "E       assert 0",
257                 "*_keyboard_interrupt.py:6: KeyboardInterrupt*",
258             ]
259         )
260         if fulltrace:
261             result.stdout.fnmatch_lines(
262                 ["*raise KeyboardInterrupt   # simulating the user*"]
263             )
264         else:
265             result.stdout.fnmatch_lines(
266                 ["(to show a full traceback on KeyboardInterrupt use --full-trace)"]
267             )
268         result.stdout.fnmatch_lines(["*KeyboardInterrupt*"])
269 
270     def test_keyboard_in_sessionstart(self, testdir):
271         testdir.makeconftest(
272             """
273             def pytest_sessionstart():
274                 raise KeyboardInterrupt
275         """
276         )
277         testdir.makepyfile(
278             """
279             def test_foobar():
280                 pass
281         """
282         )
283 
284         result = testdir.runpytest(no_reraise_ctrlc=True)
285         assert result.ret == 2
286         result.stdout.fnmatch_lines(["*KeyboardInterrupt*"])
287 
288     def test_collect_single_item(self, testdir):
289         """Use singular 'item' when reporting a single test item"""
290         testdir.makepyfile(
291             """
292             def test_foobar():
293                 pass
294         """
295         )
296         result = testdir.runpytest()
297         result.stdout.fnmatch_lines(["collected 1 item"])
298 
299     def test_rewrite(self, testdir, monkeypatch):
300         config = testdir.parseconfig()
301         f = StringIO()
302         monkeypatch.setattr(f, "isatty", lambda *args: True)
303         tr = TerminalReporter(config, f)
304         tr._tw.fullwidth = 10
305         tr.write("hello")
306         tr.rewrite("hey", erase=True)
307         assert f.getvalue() == "hello" + "\r" + "hey" + (6 * " ")
308 
309     def test_report_teststatus_explicit_markup(
310         self, testdir: Testdir, color_mapping
311     ) -> None:
312         """Test that TerminalReporter handles markup explicitly provided by
313         a pytest_report_teststatus hook."""
314         testdir.monkeypatch.setenv("PY_COLORS", "1")
315         testdir.makeconftest(
316             """
317             def pytest_report_teststatus(report):
318                 return 'foo', 'F', ('FOO', {'red': True})
319         """
320         )
321         testdir.makepyfile(
322             """
323             def test_foobar():
324                 pass
325         """
326         )
327         result = testdir.runpytest("-v")
328         result.stdout.fnmatch_lines(
329             color_mapping.format_for_fnmatch(["*{red}FOO{reset}*"])
330         )
331 
332 
333 class TestCollectonly:
334     def test_collectonly_basic(self, testdir):
335         testdir.makepyfile(
336             """
337             def test_func():
338                 pass
339         """
340         )
341         result = testdir.runpytest("--collect-only")
342         result.stdout.fnmatch_lines(
343             ["<Module test_collectonly_basic.py>", "  <Function test_func>"]
344         )
345 
346     def test_collectonly_skipped_module(self, testdir):
347         testdir.makepyfile(
348             """
349             import pytest
350             pytest.skip("hello")
351         """
352         )
353         result = testdir.runpytest("--collect-only", "-rs")
354         result.stdout.fnmatch_lines(["*ERROR collecting*"])
355 
356     def test_collectonly_display_test_description(self, testdir):
357         testdir.makepyfile(
358             """
359             def test_with_description():
360                 \""" This test has a description.
361                 \"""
362                 assert True
363         """
364         )
365         result = testdir.runpytest("--collect-only", "--verbose")
366         result.stdout.fnmatch_lines(["    This test has a description."])
367 
368     def test_collectonly_failed_module(self, testdir):
369         testdir.makepyfile("""raise ValueError(0)""")
370         result = testdir.runpytest("--collect-only")
371         result.stdout.fnmatch_lines(["*raise ValueError*", "*1 error*"])
372 
373     def test_collectonly_fatal(self, testdir):
374         testdir.makeconftest(
375             """
376             def pytest_collectstart(collector):
377                 assert 0, "urgs"
378         """
379         )
380         result = testdir.runpytest("--collect-only")
381         result.stdout.fnmatch_lines(["*INTERNAL*args*"])
382         assert result.ret == 3
383 
384     def test_collectonly_simple(self, testdir):
385         p = testdir.makepyfile(
386             """
387             def test_func1():
388                 pass
389             class TestClass(object):
390                 def test_method(self):
391                     pass
392         """
393         )
394         result = testdir.runpytest("--collect-only", p)
395         # assert stderr.startswith("inserting into sys.path")
396         assert result.ret == 0
397         result.stdout.fnmatch_lines(
398             [
399                 "*<Module *.py>",
400                 "* <Function test_func1>",
401                 "* <Class TestClass>",
402                 "*   <Function test_method>",
403             ]
404         )
405 
406     def test_collectonly_error(self, testdir):
407         p = testdir.makepyfile("import Errlkjqweqwe")
408         result = testdir.runpytest("--collect-only", p)
409         assert result.ret == 2
410         result.stdout.fnmatch_lines(
411             textwrap.dedent(
412                 """\
413                 *ERROR*
414                 *ImportError*
415                 *No module named *Errlk*
416                 *1 error*
417                 """
418             ).strip()
419         )
420 
421     def test_collectonly_missing_path(self, testdir):
422         """this checks issue 115,
423             failure in parseargs will cause session
424             not to have the items attribute
425         """
426         result = testdir.runpytest("--collect-only", "uhm_missing_path")
427         assert result.ret == 4
428         result.stderr.fnmatch_lines(["*ERROR: file not found*"])
429 
430     def test_collectonly_quiet(self, testdir):
431         testdir.makepyfile("def test_foo(): pass")
432         result = testdir.runpytest("--collect-only", "-q")
433         result.stdout.fnmatch_lines(["*test_foo*"])
434 
435     def test_collectonly_more_quiet(self, testdir):
436         testdir.makepyfile(test_fun="def test_foo(): pass")
437         result = testdir.runpytest("--collect-only", "-qq")
438         result.stdout.fnmatch_lines(["*test_fun.py: 1*"])
439 
440 
441 class TestFixtureReporting:
442     def test_setup_fixture_error(self, testdir):
443         testdir.makepyfile(
444             """
445             def setup_function(function):
446                 print("setup func")
447                 assert 0
448             def test_nada():
449                 pass
450         """
451         )
452         result = testdir.runpytest()
453         result.stdout.fnmatch_lines(
454             [
455                 "*ERROR at setup of test_nada*",
456                 "*setup_function(function):*",
457                 "*setup func*",
458                 "*assert 0*",
459                 "*1 error*",
460             ]
461         )
462         assert result.ret != 0
463 
464     def test_teardown_fixture_error(self, testdir):
465         testdir.makepyfile(
466             """
467             def test_nada():
468                 pass
469             def teardown_function(function):
470                 print("teardown func")
471                 assert 0
472         """
473         )
474         result = testdir.runpytest()
475         result.stdout.fnmatch_lines(
476             [
477                 "*ERROR at teardown*",
478                 "*teardown_function(function):*",
479                 "*assert 0*",
480                 "*Captured stdout*",
481                 "*teardown func*",
482                 "*1 passed*1 error*",
483             ]
484         )
485 
486     def test_teardown_fixture_error_and_test_failure(self, testdir):
487         testdir.makepyfile(
488             """
489             def test_fail():
490                 assert 0, "failingfunc"
491 
492             def teardown_function(function):
493                 print("teardown func")
494                 assert False
495         """
496         )
497         result = testdir.runpytest()
498         result.stdout.fnmatch_lines(
499             [
500                 "*ERROR at teardown of test_fail*",
501                 "*teardown_function(function):*",
502                 "*assert False*",
503                 "*Captured stdout*",
504                 "*teardown func*",
505                 "*test_fail*",
506                 "*def test_fail():",
507                 "*failingfunc*",
508                 "*1 failed*1 error*",
509             ]
510         )
511 
512     def test_setup_teardown_output_and_test_failure(self, testdir):
513         """ Test for issue #442 """
514         testdir.makepyfile(
515             """
516             def setup_function(function):
517                 print("setup func")
518 
519             def test_fail():
520                 assert 0, "failingfunc"
521 
522             def teardown_function(function):
523                 print("teardown func")
524         """
525         )
526         result = testdir.runpytest()
527         result.stdout.fnmatch_lines(
528             [
529                 "*test_fail*",
530                 "*def test_fail():",
531                 "*failingfunc*",
532                 "*Captured stdout setup*",
533                 "*setup func*",
534                 "*Captured stdout teardown*",
535                 "*teardown func*",
536                 "*1 failed*",
537             ]
538         )
539 
540 
541 class TestTerminalFunctional:
542     def test_deselected(self, testdir):
543         testpath = testdir.makepyfile(
544             """
545                 def test_one():
546                     pass
547                 def test_two():
548                     pass
549                 def test_three():
550                     pass
551            """
552         )
553         result = testdir.runpytest("-k", "test_two:", testpath)
554         result.stdout.fnmatch_lines(
555             ["collected 3 items / 1 deselected / 2 selected", "*test_deselected.py ..*"]
556         )
557         assert result.ret == 0
558 
559     def test_deselected_with_hookwrapper(self, testdir):
560         testpath = testdir.makeconftest(
561             """
562             import pytest
563 
564             @pytest.hookimpl(hookwrapper=True)
565             def pytest_collection_modifyitems(config, items):
566                 yield
567                 deselected = items.pop()
568                 config.hook.pytest_deselected(items=[deselected])
569             """
570         )
571         testpath = testdir.makepyfile(
572             """
573                 def test_one():
574                     pass
575                 def test_two():
576                     pass
577                 def test_three():
578                     pass
579            """
580         )
581         result = testdir.runpytest(testpath)
582         result.stdout.fnmatch_lines(
583             [
584                 "collected 3 items / 1 deselected / 2 selected",
585                 "*= 2 passed, 1 deselected in*",
586             ]
587         )
588         assert result.ret == 0
589 
590     def test_show_deselected_items_using_markexpr_before_test_execution(self, testdir):
591         testdir.makepyfile(
592             test_show_deselected="""
593             import pytest
594 
595             @pytest.mark.foo
596             def test_foobar():
597                 pass
598 
599             @pytest.mark.bar
600             def test_bar():
601                 pass
602 
603             def test_pass():
604                 pass
605         """
606         )
607         result = testdir.runpytest("-m", "not foo")
608         result.stdout.fnmatch_lines(
609             [
610                 "collected 3 items / 1 deselected / 2 selected",
611                 "*test_show_deselected.py ..*",
612                 "*= 2 passed, 1 deselected in * =*",
613             ]
614         )
615         result.stdout.no_fnmatch_line("*= 1 deselected =*")
616         assert result.ret == 0
617 
618     def test_no_skip_summary_if_failure(self, testdir):
619         testdir.makepyfile(
620             """
621             import pytest
622             def test_ok():
623                 pass
624             def test_fail():
625                 assert 0
626             def test_skip():
627                 pytest.skip("dontshow")
628         """
629         )
630         result = testdir.runpytest()
631         assert result.stdout.str().find("skip test summary") == -1
632         assert result.ret == 1
633 
634     def test_passes(self, testdir):
635         p1 = testdir.makepyfile(
636             """
637             def test_passes():
638                 pass
639             class TestClass(object):
640                 def test_method(self):
641                     pass
642         """
643         )
644         old = p1.dirpath().chdir()
645         try:
646             result = testdir.runpytest()
647         finally:
648             old.chdir()
649         result.stdout.fnmatch_lines(["test_passes.py ..*", "* 2 pass*"])
650         assert result.ret == 0
651 
652     def test_header_trailer_info(self, testdir, request):
653         testdir.monkeypatch.delenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD")
654         testdir.makepyfile(
655             """
656             def test_passes():
657                 pass
658         """
659         )
660         result = testdir.runpytest()
661         verinfo = ".".join(map(str, sys.version_info[:3]))
662         result.stdout.fnmatch_lines(
663             [
664                 "*===== test session starts ====*",
665                 "platform %s -- Python %s*pytest-%s*py-%s*pluggy-%s"
666                 % (
667                     sys.platform,
668                     verinfo,
669                     pytest.__version__,
670                     py.__version__,
671                     pluggy.__version__,
672                 ),
673                 "*test_header_trailer_info.py .*",
674                 "=* 1 passed*in *.[0-9][0-9]s *=",
675             ]
676         )
677         if request.config.pluginmanager.list_plugin_distinfo():
678             result.stdout.fnmatch_lines(["plugins: *"])
679 
680     def test_header(self, testdir):
681         testdir.tmpdir.join("tests").ensure_dir()
682         testdir.tmpdir.join("gui").ensure_dir()
683 
684         # no ini file
685         result = testdir.runpytest()
686         result.stdout.fnmatch_lines(["rootdir: *test_header0"])
687 
688         # with inifile
689         testdir.makeini("""[pytest]""")
690         result = testdir.runpytest()
691         result.stdout.fnmatch_lines(["rootdir: *test_header0, inifile: tox.ini"])
692 
693         # with testpaths option, and not passing anything in the command-line
694         testdir.makeini(
695             """
696             [pytest]
697             testpaths = tests gui
698         """
699         )
700         result = testdir.runpytest()
701         result.stdout.fnmatch_lines(
702             ["rootdir: *test_header0, inifile: tox.ini, testpaths: tests, gui"]
703         )
704 
705         # with testpaths option, passing directory in command-line: do not show testpaths then
706         result = testdir.runpytest("tests")
707         result.stdout.fnmatch_lines(["rootdir: *test_header0, inifile: tox.ini"])
708 
709     def test_showlocals(self, testdir):
710         p1 = testdir.makepyfile(
711             """
712             def test_showlocals():
713                 x = 3
714                 y = "x" * 5000
715                 assert 0
716         """
717         )
718         result = testdir.runpytest(p1, "-l")
719         result.stdout.fnmatch_lines(
720             [
721                 # "_ _ * Locals *",
722                 "x* = 3",
723                 "y* = 'xxxxxx*",
724             ]
725         )
726 
727     def test_showlocals_short(self, testdir):
728         p1 = testdir.makepyfile(
729             """
730             def test_showlocals_short():
731                 x = 3
732                 y = "xxxx"
733                 assert 0
734         """
735         )
736         result = testdir.runpytest(p1, "-l", "--tb=short")
737         result.stdout.fnmatch_lines(
738             [
739                 "test_showlocals_short.py:*",
740                 "    assert 0",
741                 "E   assert 0",
742                 "        x          = 3",
743                 "        y          = 'xxxx'",
744             ]
745         )
746 
747     @pytest.fixture
748     def verbose_testfile(self, testdir):
749         return testdir.makepyfile(
750             """
751             import pytest
752             def test_fail():
753                 raise ValueError()
754             def test_pass():
755                 pass
756             class TestClass(object):
757                 def test_skip(self):
758                     pytest.skip("hello")
759             def test_gen():
760                 def check(x):
761                     assert x == 1
762                 yield check, 0
763         """
764         )
765 
766     def test_verbose_reporting(self, verbose_testfile, testdir):
767         result = testdir.runpytest(
768             verbose_testfile, "-v", "-Walways::pytest.PytestWarning"
769         )
770         result.stdout.fnmatch_lines(
771             [
772                 "*test_verbose_reporting.py::test_fail *FAIL*",
773                 "*test_verbose_reporting.py::test_pass *PASS*",
774                 "*test_verbose_reporting.py::TestClass::test_skip *SKIP*",
775                 "*test_verbose_reporting.py::test_gen *XFAIL*",
776             ]
777         )
778         assert result.ret == 1
779 
780     def test_verbose_reporting_xdist(self, verbose_testfile, testdir, pytestconfig):
781         if not pytestconfig.pluginmanager.get_plugin("xdist"):
782             pytest.skip("xdist plugin not installed")
783 
784         testdir.monkeypatch.delenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD")
785         result = testdir.runpytest(
786             verbose_testfile, "-v", "-n 1", "-Walways::pytest.PytestWarning"
787         )
788         result.stdout.fnmatch_lines(
789             ["*FAIL*test_verbose_reporting_xdist.py::test_fail*"]
790         )
791         assert result.ret == 1
792 
793     def test_quiet_reporting(self, testdir):
794         p1 = testdir.makepyfile("def test_pass(): pass")
795         result = testdir.runpytest(p1, "-q")
796         s = result.stdout.str()
797         assert "test session starts" not in s
798         assert p1.basename not in s
799         assert "===" not in s
800         assert "passed" in s
801 
802     def test_more_quiet_reporting(self, testdir):
803         p1 = testdir.makepyfile("def test_pass(): pass")
804         result = testdir.runpytest(p1, "-qq")
805         s = result.stdout.str()
806         assert "test session starts" not in s
807         assert p1.basename not in s
808         assert "===" not in s
809         assert "passed" not in s
810 
811     @pytest.mark.parametrize(
812         "params", [(), ("--collect-only",)], ids=["no-params", "collect-only"]
813     )
814     def test_report_collectionfinish_hook(self, testdir, params):
815         testdir.makeconftest(
816             """
817             def pytest_report_collectionfinish(config, startdir, items):
818                 return ['hello from hook: {0} items'.format(len(items))]
819         """
820         )
821         testdir.makepyfile(
822             """
823             import pytest
824             @pytest.mark.parametrize('i', range(3))
825             def test(i):
826                 pass
827         """
828         )
829         result = testdir.runpytest(*params)
830         result.stdout.fnmatch_lines(["collected 3 items", "hello from hook: 3 items"])
831 
832     def test_summary_f_alias(self, testdir):
833         """Test that 'f' and 'F' report chars are aliases and don't show up twice in the summary (#6334)"""
834         testdir.makepyfile(
835             """
836             def test():
837                 assert False
838             """
839         )
840         result = testdir.runpytest("-rfF")
841         expected = "FAILED test_summary_f_alias.py::test - assert False"
842         result.stdout.fnmatch_lines([expected])
843         assert result.stdout.lines.count(expected) == 1
844 
845     def test_summary_s_alias(self, testdir):
846         """Test that 's' and 'S' report chars are aliases and don't show up twice in the summary"""
847         testdir.makepyfile(
848             """
849             import pytest
850 
851             @pytest.mark.skip
852             def test():
853                 pass
854             """
855         )
856         result = testdir.runpytest("-rsS")
857         expected = "SKIPPED [1] test_summary_s_alias.py:3: unconditional skip"
858         result.stdout.fnmatch_lines([expected])
859         assert result.stdout.lines.count(expected) == 1
860 
861 
862 def test_fail_extra_reporting(testdir, monkeypatch):
863     monkeypatch.setenv("COLUMNS", "80")
864     testdir.makepyfile("def test_this(): assert 0, 'this_failed' * 100")
865     result = testdir.runpytest("-rN")
866     result.stdout.no_fnmatch_line("*short test summary*")
867     result = testdir.runpytest()
868     result.stdout.fnmatch_lines(
869         [
870             "*test summary*",
871             "FAILED test_fail_extra_reporting.py::test_this - AssertionError: this_failedt...",
872         ]
873     )
874 
875 
876 def test_fail_reporting_on_pass(testdir):
877     testdir.makepyfile("def test_this(): assert 1")
878     result = testdir.runpytest("-rf")
879     result.stdout.no_fnmatch_line("*short test summary*")
880 
881 
882 def test_pass_extra_reporting(testdir):
883     testdir.makepyfile("def test_this(): assert 1")
884     result = testdir.runpytest()
885     result.stdout.no_fnmatch_line("*short test summary*")
886     result = testdir.runpytest("-rp")
887     result.stdout.fnmatch_lines(["*test summary*", "PASS*test_pass_extra_reporting*"])
888 
889 
890 def test_pass_reporting_on_fail(testdir):
891     testdir.makepyfile("def test_this(): assert 0")
892     result = testdir.runpytest("-rp")
893     result.stdout.no_fnmatch_line("*short test summary*")
894 
895 
896 def test_pass_output_reporting(testdir):
897     testdir.makepyfile(
898         """
899         def setup_module():
900             print("setup_module")
901 
902         def teardown_module():
903             print("teardown_module")
904 
905         def test_pass_has_output():
906             print("Four score and seven years ago...")
907 
908         def test_pass_no_output():
909             pass
910     """
911     )
912     result = testdir.runpytest()
913     s = result.stdout.str()
914     assert "test_pass_has_output" not in s
915     assert "Four score and seven years ago..." not in s
916     assert "test_pass_no_output" not in s
917     result = testdir.runpytest("-rPp")
918     result.stdout.fnmatch_lines(
919         [
920             "*= PASSES =*",
921             "*_ test_pass_has_output _*",
922             "*- Captured stdout setup -*",
923             "setup_module",
924             "*- Captured stdout call -*",
925             "Four score and seven years ago...",
926             "*- Captured stdout teardown -*",
927             "teardown_module",
928             "*= short test summary info =*",
929             "PASSED test_pass_output_reporting.py::test_pass_has_output",
930             "PASSED test_pass_output_reporting.py::test_pass_no_output",
931             "*= 2 passed in *",
932         ]
933     )
934 
935 
936 def test_color_yes(testdir, color_mapping):
937     p1 = testdir.makepyfile(
938         """
939         def fail():
940             assert 0
941 
942         def test_this():
943             fail()
944         """
945     )
946     result = testdir.runpytest("--color=yes", str(p1))
947     color_mapping.requires_ordered_markup(result)
948     result.stdout.fnmatch_lines(
949         color_mapping.format_for_fnmatch(
950             [
951                 "{bold}=*= test session starts =*={reset}",
952                 "collected 1 item",
953                 "",
954                 "test_color_yes.py {red}F{reset}{red} * [100%]{reset}",
955                 "",
956                 "=*= FAILURES =*=",
957                 "{red}{bold}_*_ test_this _*_{reset}",
958                 "",
959                 "    {kw}def{hl-reset} {function}test_this{hl-reset}():",
960                 ">       fail()",
961                 "",
962                 "{bold}{red}test_color_yes.py{reset}:5: ",
963                 "_ _ * _ _*",
964                 "",
965                 "    {kw}def{hl-reset} {function}fail{hl-reset}():",
966                 ">       {kw}assert{hl-reset} {number}0{hl-reset}",
967                 "{bold}{red}E       assert 0{reset}",
968                 "",
969                 "{bold}{red}test_color_yes.py{reset}:2: AssertionError",
970                 "{red}=*= {red}{bold}1 failed{reset}{red} in *s{reset}{red} =*={reset}",
971             ]
972         )
973     )
974     result = testdir.runpytest("--color=yes", "--tb=short", str(p1))
975     result.stdout.fnmatch_lines(
976         color_mapping.format_for_fnmatch(
977             [
978                 "{bold}=*= test session starts =*={reset}",
979                 "collected 1 item",
980                 "",
981                 "test_color_yes.py {red}F{reset}{red} * [100%]{reset}",
982                 "",
983                 "=*= FAILURES =*=",
984                 "{red}{bold}_*_ test_this _*_{reset}",
985                 "{bold}{red}test_color_yes.py{reset}:5: in test_this",
986                 "    fail()",
987                 "{bold}{red}test_color_yes.py{reset}:2: in fail",
988                 "    {kw}assert{hl-reset} {number}0{hl-reset}",
989                 "{bold}{red}E   assert 0{reset}",
990                 "{red}=*= {red}{bold}1 failed{reset}{red} in *s{reset}{red} =*={reset}",
991             ]
992         )
993     )
994 
995 
996 def test_color_no(testdir):
997     testdir.makepyfile("def test_this(): assert 1")
998     result = testdir.runpytest("--color=no")
999     assert "test session starts" in result.stdout.str()
1000     result.stdout.no_fnmatch_line("*\x1b[1m*")
1001 
1002 
1003 @pytest.mark.parametrize("verbose", [True, False])
1004 def test_color_yes_collection_on_non_atty(testdir, verbose):
1005     """skip collect progress report when working on non-terminals.
1006     #1397
1007     """
1008     testdir.makepyfile(
1009         """
1010         import pytest
1011         @pytest.mark.parametrize('i', range(10))
1012         def test_this(i):
1013             assert 1
1014     """
1015     )
1016     args = ["--color=yes"]
1017     if verbose:
1018         args.append("-vv")
1019     result = testdir.runpytest(*args)
1020     assert "test session starts" in result.stdout.str()
1021     assert "\x1b[1m" in result.stdout.str()
1022     result.stdout.no_fnmatch_line("*collecting 10 items*")
1023     if verbose:
1024         assert "collecting ..." in result.stdout.str()
1025     assert "collected 10 items" in result.stdout.str()
1026 
1027 
1028 def test_getreportopt():
1029     from _pytest.terminal import _REPORTCHARS_DEFAULT
1030 
1031     class Config:
1032         class Option:
1033             reportchars = _REPORTCHARS_DEFAULT
1034             disable_warnings = False
1035 
1036         option = Option()
1037 
1038     config = Config()
1039 
1040     assert _REPORTCHARS_DEFAULT == "fE"
1041 
1042     # Default.
1043     assert getreportopt(config) == "wfE"
1044 
1045     config.option.reportchars = "sf"
1046     assert getreportopt(config) == "wsf"
1047 
1048     config.option.reportchars = "sfxw"
1049     assert getreportopt(config) == "sfxw"
1050 
1051     config.option.reportchars = "a"
1052     assert getreportopt(config) == "wsxXEf"
1053 
1054     config.option.reportchars = "N"
1055     assert getreportopt(config) == "w"
1056 
1057     config.option.reportchars = "NwfE"
1058     assert getreportopt(config) == "wfE"
1059 
1060     config.option.reportchars = "NfENx"
1061     assert getreportopt(config) == "wx"
1062 
1063     # Now with --disable-warnings.
1064     config.option.disable_warnings = True
1065     config.option.reportchars = "a"
1066     assert getreportopt(config) == "sxXEf"
1067 
1068     config.option.reportchars = "sfx"
1069     assert getreportopt(config) == "sfx"
1070 
1071     config.option.reportchars = "sfxw"
1072     assert getreportopt(config) == "sfx"
1073 
1074     config.option.reportchars = "a"
1075     assert getreportopt(config) == "sxXEf"
1076 
1077     config.option.reportchars = "A"
1078     assert getreportopt(config) == "PpsxXEf"
1079 
1080     config.option.reportchars = "AN"
1081     assert getreportopt(config) == ""
1082 
1083     config.option.reportchars = "NwfE"
1084     assert getreportopt(config) == "fE"
1085 
1086 
1087 def test_terminalreporter_reportopt_addopts(testdir):
1088     testdir.makeini("[pytest]\naddopts=-rs")
1089     testdir.makepyfile(
1090         """
1091         import pytest
1092 
1093         @pytest.fixture
1094         def tr(request):
1095             tr = request.config.pluginmanager.getplugin("terminalreporter")
1096             return tr
1097         def test_opt(tr):
1098             assert tr.hasopt('skipped')
1099             assert not tr.hasopt('qwe')
1100     """
1101     )
1102     result = testdir.runpytest()
1103     result.stdout.fnmatch_lines(["*1 passed*"])
1104 
1105 
1106 def test_tbstyle_short(testdir):
1107     p = testdir.makepyfile(
1108         """
1109         import pytest
1110 
1111         @pytest.fixture
1112         def arg(request):
1113             return 42
1114         def test_opt(arg):
1115             x = 0
1116             assert x
1117     """
1118     )
1119     result = testdir.runpytest("--tb=short")
1120     s = result.stdout.str()
1121     assert "arg = 42" not in s
1122     assert "x = 0" not in s
1123     result.stdout.fnmatch_lines(["*%s:8*" % p.basename, "    assert x", "E   assert*"])
1124     result = testdir.runpytest()
1125     s = result.stdout.str()
1126     assert "x = 0" in s
1127     assert "assert x" in s
1128 
1129 
1130 def test_traceconfig(testdir):
1131     result = testdir.runpytest("--traceconfig")
1132     result.stdout.fnmatch_lines(["*active plugins*"])
1133     assert result.ret == ExitCode.NO_TESTS_COLLECTED
1134 
1135 
1136 class TestGenericReporting:
1137     """ this test class can be subclassed with a different option
1138         provider to run e.g. distributed tests.
1139     """
1140 
1141     def test_collect_fail(self, testdir, option):
1142         testdir.makepyfile("import xyz\n")
1143         result = testdir.runpytest(*option.args)
1144         result.stdout.fnmatch_lines(
1145             ["ImportError while importing*", "*No module named *xyz*", "*1 error*"]
1146         )
1147 
1148     def test_maxfailures(self, testdir, option):
1149         testdir.makepyfile(
1150             """
1151             def test_1():
1152                 assert 0
1153             def test_2():
1154                 assert 0
1155             def test_3():
1156                 assert 0
1157         """
1158         )
1159         result = testdir.runpytest("--maxfail=2", *option.args)
1160         result.stdout.fnmatch_lines(
1161             [
1162                 "*def test_1():*",
1163                 "*def test_2():*",
1164                 "*! stopping after 2 failures !*",
1165                 "*2 failed*",
1166             ]
1167         )
1168 
1169     def test_maxfailures_with_interrupted(self, testdir):
1170         testdir.makepyfile(
1171             """
1172             def test(request):
1173                 request.session.shouldstop = "session_interrupted"
1174                 assert 0
1175         """
1176         )
1177         result = testdir.runpytest("--maxfail=1", "-ra")
1178         result.stdout.fnmatch_lines(
1179             [
1180                 "*= short test summary info =*",
1181                 "FAILED *",
1182                 "*! stopping after 1 failures !*",
1183                 "*! session_interrupted !*",
1184                 "*= 1 failed in*",
1185             ]
1186         )
1187 
1188     def test_tb_option(self, testdir, option):
1189         testdir.makepyfile(
1190             """
1191             import pytest
1192             def g():
1193                 raise IndexError
1194             def test_func():
1195                 print(6*7)
1196                 g()  # --calling--
1197         """
1198         )
1199         for tbopt in ["long", "short", "no"]:
1200             print("testing --tb=%s..." % tbopt)
1201             result = testdir.runpytest("-rN", "--tb=%s" % tbopt)
1202             s = result.stdout.str()
1203             if tbopt == "long":
1204                 assert "print(6*7)" in s
1205             else:
1206                 assert "print(6*7)" not in s
1207             if tbopt != "no":
1208                 assert "--calling--" in s
1209                 assert "IndexError" in s
1210             else:
1211                 assert "FAILURES" not in s
1212                 assert "--calling--" not in s
1213                 assert "IndexError" not in s
1214 
1215     def test_tb_crashline(self, testdir, option):
1216         p = testdir.makepyfile(
1217             """
1218             import pytest
1219             def g():
1220                 raise IndexError
1221             def test_func1():
1222                 print(6*7)
1223                 g()  # --calling--
1224             def test_func2():
1225                 assert 0, "hello"
1226         """
1227         )
1228         result = testdir.runpytest("--tb=line")
1229         bn = p.basename
1230         result.stdout.fnmatch_lines(
1231             ["*%s:3: IndexError*" % bn, "*%s:8: AssertionError: hello*" % bn]
1232         )
1233         s = result.stdout.str()
1234         assert "def test_func2" not in s
1235 
1236     def test_pytest_report_header(self, testdir, option):
1237         testdir.makeconftest(
1238             """
1239             def pytest_sessionstart(session):
1240                 session.config._somevalue = 42
1241             def pytest_report_header(config):
1242                 return "hello: %s" % config._somevalue
1243         """
1244         )
1245         testdir.mkdir("a").join("conftest.py").write(
1246             """
1247 def pytest_report_header(config, startdir):
1248     return ["line1", str(startdir)]
1249 """
1250         )
1251         result = testdir.runpytest("a")
1252         result.stdout.fnmatch_lines(["*hello: 42*", "line1", str(testdir.tmpdir)])
1253 
1254     def test_show_capture(self, testdir):
1255         testdir.makepyfile(
1256             """
1257             import sys
1258             import logging
1259             def test_one():
1260                 sys.stdout.write('!This is stdout!')
1261                 sys.stderr.write('!This is stderr!')
1262                 logging.warning('!This is a warning log msg!')
1263                 assert False, 'Something failed'
1264         """
1265         )
1266 
1267         result = testdir.runpytest("--tb=short")
1268         result.stdout.fnmatch_lines(
1269             [
1270                 "!This is stdout!",
1271                 "!This is stderr!",
1272                 "*WARNING*!This is a warning log msg!",
1273             ]
1274         )
1275 
1276         result = testdir.runpytest("--show-capture=all", "--tb=short")
1277         result.stdout.fnmatch_lines(
1278             [
1279                 "!This is stdout!",
1280                 "!This is stderr!",
1281                 "*WARNING*!This is a warning log msg!",
1282             ]
1283         )
1284 
1285         stdout = testdir.runpytest("--show-capture=stdout", "--tb=short").stdout.str()
1286         assert "!This is stderr!" not in stdout
1287         assert "!This is stdout!" in stdout
1288         assert "!This is a warning log msg!" not in stdout
1289 
1290         stdout = testdir.runpytest("--show-capture=stderr", "--tb=short").stdout.str()
1291         assert "!This is stdout!" not in stdout
1292         assert "!This is stderr!" in stdout
1293         assert "!This is a warning log msg!" not in stdout
1294 
1295         stdout = testdir.runpytest("--show-capture=log", "--tb=short").stdout.str()
1296         assert "!This is stdout!" not in stdout
1297         assert "!This is stderr!" not in stdout
1298         assert "!This is a warning log msg!" in stdout
1299 
1300         stdout = testdir.runpytest("--show-capture=no", "--tb=short").stdout.str()
1301         assert "!This is stdout!" not in stdout
1302         assert "!This is stderr!" not in stdout
1303         assert "!This is a warning log msg!" not in stdout
1304 
1305     def test_show_capture_with_teardown_logs(self, testdir):
1306         """Ensure that the capturing of teardown logs honor --show-capture setting"""
1307         testdir.makepyfile(
1308             """
1309             import logging
1310             import sys
1311             import pytest
1312 
1313             @pytest.fixture(scope="function", autouse="True")
1314             def hook_each_test(request):
1315                 yield
1316                 sys.stdout.write("!stdout!")
1317                 sys.stderr.write("!stderr!")
1318                 logging.warning("!log!")
1319 
1320             def test_func():
1321                 assert False
1322         """
1323         )
1324 
1325         result = testdir.runpytest("--show-capture=stdout", "--tb=short").stdout.str()
1326         assert "!stdout!" in result
1327         assert "!stderr!" not in result
1328         assert "!log!" not in result
1329 
1330         result = testdir.runpytest("--show-capture=stderr", "--tb=short").stdout.str()
1331         assert "!stdout!" not in result
1332         assert "!stderr!" in result
1333         assert "!log!" not in result
1334 
1335         result = testdir.runpytest("--show-capture=log", "--tb=short").stdout.str()
1336         assert "!stdout!" not in result
1337         assert "!stderr!" not in result
1338         assert "!log!" in result
1339 
1340         result = testdir.runpytest("--show-capture=no", "--tb=short").stdout.str()
1341         assert "!stdout!" not in result
1342         assert "!stderr!" not in result
1343         assert "!log!" not in result
1344 
1345 
1346 @pytest.mark.xfail("not hasattr(os, 'dup')")
1347 def test_fdopen_kept_alive_issue124(testdir):
1348     testdir.makepyfile(
1349         """
1350         import os, sys
1351         k = []
1352         def test_open_file_and_keep_alive(capfd):
1353             stdout = os.fdopen(1, 'w', 1)
1354             k.append(stdout)
1355 
1356         def test_close_kept_alive_file():
1357             stdout = k.pop()
1358             stdout.close()
1359     """
1360     )
1361     result = testdir.runpytest()
1362     result.stdout.fnmatch_lines(["*2 passed*"])
1363 
1364 
1365 def test_tbstyle_native_setup_error(testdir):
1366     testdir.makepyfile(
1367         """
1368         import pytest
1369         @pytest.fixture
1370         def setup_error_fixture():
1371             raise Exception("error in exception")
1372 
1373         def test_error_fixture(setup_error_fixture):
1374             pass
1375     """
1376     )
1377     result = testdir.runpytest("--tb=native")
1378     result.stdout.fnmatch_lines(
1379         ['*File *test_tbstyle_native_setup_error.py", line *, in setup_error_fixture*']
1380     )
1381 
1382 
1383 def test_terminal_summary(testdir):
1384     testdir.makeconftest(
1385         """
1386         def pytest_terminal_summary(terminalreporter, exitstatus):
1387             w = terminalreporter
1388             w.section("hello")
1389             w.line("world")
1390             w.line("exitstatus: {0}".format(exitstatus))
1391     """
1392     )
1393     result = testdir.runpytest()
1394     result.stdout.fnmatch_lines(
1395         """
1396         *==== hello ====*
1397         world
1398         exitstatus: 5
1399     """
1400     )
1401 
1402 
1403 @pytest.mark.filterwarnings("default")
1404 def test_terminal_summary_warnings_are_displayed(testdir):
1405     """Test that warnings emitted during pytest_terminal_summary are displayed.
1406     (#1305).
1407     """
1408     testdir.makeconftest(
1409         """
1410         import warnings
1411         def pytest_terminal_summary(terminalreporter):
1412             warnings.warn(UserWarning('internal warning'))
1413     """
1414     )
1415     testdir.makepyfile(
1416         """
1417         def test_failure():
1418             import warnings
1419             warnings.warn("warning_from_" + "test")
1420             assert 0
1421     """
1422     )
1423     result = testdir.runpytest("-ra")
1424     result.stdout.fnmatch_lines(
1425         [
1426             "*= warnings summary =*",
1427             "*warning_from_test*",
1428             "*= short test summary info =*",
1429             "*= warnings summary (final) =*",
1430             "*conftest.py:3:*internal warning",
1431             "*== 1 failed, 2 warnings in *",
1432         ]
1433     )
1434     result.stdout.no_fnmatch_line("*None*")
1435     stdout = result.stdout.str()
1436     assert stdout.count("warning_from_test") == 1
1437     assert stdout.count("=== warnings summary ") == 2
1438 
1439 
1440 @pytest.mark.filterwarnings("default")
1441 def test_terminal_summary_warnings_header_once(testdir):
1442     testdir.makepyfile(
1443         """
1444         def test_failure():
1445             import warnings
1446             warnings.warn("warning_from_" + "test")
1447             assert 0
1448     """
1449     )
1450     result = testdir.runpytest("-ra")
1451     result.stdout.fnmatch_lines(
1452         [
1453             "*= warnings summary =*",
1454             "*warning_from_test*",
1455             "*= short test summary info =*",
1456             "*== 1 failed, 1 warning in *",
1457         ]
1458     )
1459     result.stdout.no_fnmatch_line("*None*")
1460     stdout = result.stdout.str()
1461     assert stdout.count("warning_from_test") == 1
1462     assert stdout.count("=== warnings summary ") == 1
1463 
1464 
1465 @pytest.fixture(scope="session")
1466 def tr() -> TerminalReporter:
1467     config = _pytest.config._prepareconfig()
1468     return TerminalReporter(config)
1469 
1470 
1471 @pytest.mark.parametrize(
1472     "exp_color, exp_line, stats_arg",
1473     [
1474         # The method under test only cares about the length of each
1475         # dict value, not the actual contents, so tuples of anything
1476         # suffice
1477         # Important statuses -- the highest priority of these always wins
1478         ("red", [("1 failed", {"bold": True, "red": True})], {"failed": (1,)}),
1479         (
1480             "red",
1481             [
1482                 ("1 failed", {"bold": True, "red": True}),
1483                 ("1 passed", {"bold": False, "green": True}),
1484             ],
1485             {"failed": (1,), "passed": (1,)},
1486         ),
1487         ("red", [("1 error", {"bold": True, "red": True})], {"error": (1,)}),
1488         ("red", [("2 errors", {"bold": True, "red": True})], {"error": (1, 2)}),
1489         (
1490             "red",
1491             [
1492                 ("1 passed", {"bold": False, "green": True}),
1493                 ("1 error", {"bold": True, "red": True}),
1494             ],
1495             {"error": (1,), "passed": (1,)},
1496         ),
1497         # (a status that's not known to the code)
1498         ("yellow", [("1 weird", {"bold": True, "yellow": True})], {"weird": (1,)}),
1499         (
1500             "yellow",
1501             [
1502                 ("1 passed", {"bold": False, "green": True}),
1503                 ("1 weird", {"bold": True, "yellow": True}),
1504             ],
1505             {"weird": (1,), "passed": (1,)},
1506         ),
1507         ("yellow", [("1 warning", {"bold": True, "yellow": True})], {"warnings": (1,)}),
1508         (
1509             "yellow",
1510             [
1511                 ("1 passed", {"bold": False, "green": True}),
1512                 ("1 warning", {"bold": True, "yellow": True}),
1513             ],
1514             {"warnings": (1,), "passed": (1,)},
1515         ),
1516         (
1517             "green",
1518             [("5 passed", {"bold": True, "green": True})],
1519             {"passed": (1, 2, 3, 4, 5)},
1520         ),
1521         # "Boring" statuses.  These have no effect on the color of the summary
1522         # line.  Thus, if *every* test has a boring status, the summary line stays
1523         # at its default color, i.e. yellow, to warn the user that the test run
1524         # produced no useful information
1525         ("yellow", [("1 skipped", {"bold": True, "yellow": True})], {"skipped": (1,)}),
1526         (
1527             "green",
1528             [
1529                 ("1 passed", {"bold": True, "green": True}),
1530                 ("1 skipped", {"bold": False, "yellow": True}),
1531             ],
1532             {"skipped": (1,), "passed": (1,)},
1533         ),
1534         (
1535             "yellow",
1536             [("1 deselected", {"bold": True, "yellow": True})],
1537             {"deselected": (1,)},
1538         ),
1539         (
1540             "green",
1541             [
1542                 ("1 passed", {"bold": True, "green": True}),
1543                 ("1 deselected", {"bold": False, "yellow": True}),
1544             ],
1545             {"deselected": (1,), "passed": (1,)},
1546         ),
1547         ("yellow", [("1 xfailed", {"bold": True, "yellow": True})], {"xfailed": (1,)}),
1548         (
1549             "green",
1550             [
1551                 ("1 passed", {"bold": True, "green": True}),
1552                 ("1 xfailed", {"bold": False, "yellow": True}),
1553             ],
1554             {"xfailed": (1,), "passed": (1,)},
1555         ),
1556         ("yellow", [("1 xpassed", {"bold": True, "yellow": True})], {"xpassed": (1,)}),
1557         (
1558             "yellow",
1559             [
1560                 ("1 passed", {"bold": False, "green": True}),
1561                 ("1 xpassed", {"bold": True, "yellow": True}),
1562             ],
1563             {"xpassed": (1,), "passed": (1,)},
1564         ),
1565         # Likewise if no tests were found at all
1566         ("yellow", [("no tests ran", {"yellow": True})], {}),
1567         # Test the empty-key special case
1568         ("yellow", [("no tests ran", {"yellow": True})], {"": (1,)}),
1569         (
1570             "green",
1571             [("1 passed", {"bold": True, "green": True})],
1572             {"": (1,), "passed": (1,)},
1573         ),
1574         # A couple more complex combinations
1575         (
1576             "red",
1577             [
1578                 ("1 failed", {"bold": True, "red": True}),
1579                 ("2 passed", {"bold": False, "green": True}),
1580                 ("3 xfailed", {"bold": False, "yellow": True}),
1581             ],
1582             {"passed": (1, 2), "failed": (1,), "xfailed": (1, 2, 3)},
1583         ),
1584         (
1585             "green",
1586             [
1587                 ("1 passed", {"bold": True, "green": True}),
1588                 ("2 skipped", {"bold": False, "yellow": True}),
1589                 ("3 deselected", {"bold": False, "yellow": True}),
1590                 ("2 xfailed", {"bold": False, "yellow": True}),
1591             ],
1592             {
1593                 "passed": (1,),
1594                 "skipped": (1, 2),
1595                 "deselected": (1, 2, 3),
1596                 "xfailed": (1, 2),
1597             },
1598         ),
1599     ],
1600 )
1601 def test_summary_stats(
1602     tr: TerminalReporter,
1603     exp_line: List[Tuple[str, Dict[str, bool]]],
1604     exp_color: str,
1605     stats_arg: Dict[str, List],
1606 ) -> None:
1607     tr.stats = stats_arg
1608 
1609     # Fake "_is_last_item" to be True.
1610     class fake_session:
1611         testscollected = 0
1612 
1613     tr._session = fake_session  # type: ignore[assignment]  # noqa: F821
1614     assert tr._is_last_item
1615 
1616     # Reset cache.
1617     tr._main_color = None
1618 
1619     print("Based on stats: %s" % stats_arg)
1620     print('Expect summary: "{}"; with color "{}"'.format(exp_line, exp_color))
1621     (line, color) = tr.build_summary_stats_line()
1622     print('Actually got:   "{}"; with color "{}"'.format(line, color))
1623     assert line == exp_line
1624     assert color == exp_color
1625 
1626 
1627 def test_skip_counting_towards_summary(tr):
1628     class DummyReport(BaseReport):
1629         count_towards_summary = True
1630 
1631     r1 = DummyReport()
1632     r2 = DummyReport()
1633     tr.stats = {"failed": (r1, r2)}
1634     tr._main_color = None
1635     res = tr.build_summary_stats_line()
1636     assert res == ([("2 failed", {"bold": True, "red": True})], "red")
1637 
1638     r1.count_towards_summary = False
1639     tr.stats = {"failed": (r1, r2)}
1640     tr._main_color = None
1641     res = tr.build_summary_stats_line()
1642     assert res == ([("1 failed", {"bold": True, "red": True})], "red")
1643 
1644 
1645 class TestClassicOutputStyle:
1646     """Ensure classic output style works as expected (#3883)"""
1647 
1648     @pytest.fixture
1649     def test_files(self, testdir):
1650         testdir.makepyfile(
1651             **{
1652                 "test_one.py": "def test_one(): pass",
1653                 "test_two.py": "def test_two(): assert 0",
1654                 "sub/test_three.py": """
1655                     def test_three_1(): pass
1656                     def test_three_2(): assert 0
1657                     def test_three_3(): pass
1658                 """,
1659             }
1660         )
1661 
1662     def test_normal_verbosity(self, testdir, test_files):
1663         result = testdir.runpytest("-o", "console_output_style=classic")
1664         result.stdout.fnmatch_lines(
1665             [
1666                 "test_one.py .",
1667                 "test_two.py F",
1668                 "sub{}test_three.py .F.".format(os.sep),
1669                 "*2 failed, 3 passed in*",
1670             ]
1671         )
1672 
1673     def test_verbose(self, testdir, test_files):
1674         result = testdir.runpytest("-o", "console_output_style=classic", "-v")
1675         result.stdout.fnmatch_lines(
1676             [
1677                 "test_one.py::test_one PASSED",
1678                 "test_two.py::test_two FAILED",
1679                 "sub{}test_three.py::test_three_1 PASSED".format(os.sep),
1680                 "sub{}test_three.py::test_three_2 FAILED".format(os.sep),
1681                 "sub{}test_three.py::test_three_3 PASSED".format(os.sep),
1682                 "*2 failed, 3 passed in*",
1683             ]
1684         )
1685 
1686     def test_quiet(self, testdir, test_files):
1687         result = testdir.runpytest("-o", "console_output_style=classic", "-q")
1688         result.stdout.fnmatch_lines([".F.F.", "*2 failed, 3 passed in*"])
1689 
1690 
1691 class TestProgressOutputStyle:
1692     @pytest.fixture
1693     def many_tests_files(self, testdir):
1694         testdir.makepyfile(
1695             test_bar="""
1696                 import pytest
1697                 @pytest.mark.parametrize('i', range(10))
1698                 def test_bar(i): pass
1699             """,
1700             test_foo="""
1701                 import pytest
1702                 @pytest.mark.parametrize('i', range(5))
1703                 def test_foo(i): pass
1704             """,
1705             test_foobar="""
1706                 import pytest
1707                 @pytest.mark.parametrize('i', range(5))
1708                 def test_foobar(i): pass
1709             """,
1710         )
1711 
1712     def test_zero_tests_collected(self, testdir):
1713         """Some plugins (testmon for example) might issue pytest_runtest_logreport without any tests being
1714         actually collected (#2971)."""
1715         testdir.makeconftest(
1716             """
1717         def pytest_collection_modifyitems(items, config):
1718             from _pytest.runner import CollectReport
1719             for node_id in ('nodeid1', 'nodeid2'):
1720                 rep = CollectReport(node_id, 'passed', None, None)
1721                 rep.when = 'passed'
1722                 rep.duration = 0.1
1723                 config.hook.pytest_runtest_logreport(report=rep)
1724         """
1725         )
1726         output = testdir.runpytest()
1727         output.stdout.no_fnmatch_line("*ZeroDivisionError*")
1728         output.stdout.fnmatch_lines(["=* 2 passed in *="])
1729 
1730     def test_normal(self, many_tests_files, testdir):
1731         output = testdir.runpytest()
1732         output.stdout.re_match_lines(
1733             [
1734                 r"test_bar.py \.{10} \s+ \[ 50%\]",
1735                 r"test_foo.py \.{5} \s+ \[ 75%\]",
1736                 r"test_foobar.py \.{5} \s+ \[100%\]",
1737             ]
1738         )
1739 
1740     def test_colored_progress(self, testdir, monkeypatch, color_mapping):
1741         monkeypatch.setenv("PY_COLORS", "1")
1742         testdir.makepyfile(
1743             test_axfail="""
1744                 import pytest
1745                 @pytest.mark.xfail
1746                 def test_axfail(): assert 0
1747             """,
1748             test_bar="""
1749                 import pytest
1750                 @pytest.mark.parametrize('i', range(10))
1751                 def test_bar(i): pass
1752             """,
1753             test_foo="""
1754                 import pytest
1755                 import warnings
1756                 @pytest.mark.parametrize('i', range(5))
1757                 def test_foo(i):
1758                     warnings.warn(DeprecationWarning("collection"))
1759                     pass
1760             """,
1761             test_foobar="""
1762                 import pytest
1763                 @pytest.mark.parametrize('i', range(5))
1764                 def test_foobar(i): raise ValueError()
1765             """,
1766         )
1767         result = testdir.runpytest()
1768         result.stdout.re_match_lines(
1769             color_mapping.format_for_rematch(
1770                 [
1771                     r"test_axfail.py {yellow}x{reset}{green} \s+ \[  4%\]{reset}",
1772                     r"test_bar.py ({green}\.{reset}){{10}}{green} \s+ \[ 52%\]{reset}",
1773                     r"test_foo.py ({green}\.{reset}){{5}}{yellow} \s+ \[ 76%\]{reset}",
1774                     r"test_foobar.py ({red}F{reset}){{5}}{red} \s+ \[100%\]{reset}",
1775                 ]
1776             )
1777         )
1778 
1779         # Only xfail should have yellow progress indicator.
1780         result = testdir.runpytest("test_axfail.py")
1781         result.stdout.re_match_lines(
1782             color_mapping.format_for_rematch(
1783                 [
1784                     r"test_axfail.py {yellow}x{reset}{yellow} \s+ \[100%\]{reset}",
1785                     r"^{yellow}=+ ({yellow}{bold}|{bold}{yellow})1 xfailed{reset}{yellow} in ",
1786                 ]
1787             )
1788         )
1789 
1790     def test_count(self, many_tests_files, testdir):
1791         testdir.makeini(
1792             """
1793             [pytest]
1794             console_output_style = count
1795         """
1796         )
1797         output = testdir.runpytest()
1798         output.stdout.re_match_lines(
1799             [
1800                 r"test_bar.py \.{10} \s+ \[10/20\]",
1801                 r"test_foo.py \.{5} \s+ \[15/20\]",
1802                 r"test_foobar.py \.{5} \s+ \[20/20\]",
1803             ]
1804         )
1805 
1806     def test_verbose(self, many_tests_files, testdir):
1807         output = testdir.runpytest("-v")
1808         output.stdout.re_match_lines(
1809             [
1810                 r"test_bar.py::test_bar\[0\] PASSED \s+ \[  5%\]",
1811                 r"test_foo.py::test_foo\[4\] PASSED \s+ \[ 75%\]",
1812                 r"test_foobar.py::test_foobar\[4\] PASSED \s+ \[100%\]",
1813             ]
1814         )
1815 
1816     def test_verbose_count(self, many_tests_files, testdir):
1817         testdir.makeini(
1818             """
1819             [pytest]
1820             console_output_style = count
1821         """
1822         )
1823         output = testdir.runpytest("-v")
1824         output.stdout.re_match_lines(
1825             [
1826                 r"test_bar.py::test_bar\[0\] PASSED \s+ \[ 1/20\]",
1827                 r"test_foo.py::test_foo\[4\] PASSED \s+ \[15/20\]",
1828                 r"test_foobar.py::test_foobar\[4\] PASSED \s+ \[20/20\]",
1829             ]
1830         )
1831 
1832     def test_xdist_normal(self, many_tests_files, testdir, monkeypatch):
1833         pytest.importorskip("xdist")
1834         monkeypatch.delenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", raising=False)
1835         output = testdir.runpytest("-n2")
1836         output.stdout.re_match_lines([r"\.{20} \s+ \[100%\]"])
1837 
1838     def test_xdist_normal_count(self, many_tests_files, testdir, monkeypatch):
1839         pytest.importorskip("xdist")
1840         monkeypatch.delenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", raising=False)
1841         testdir.makeini(
1842             """
1843             [pytest]
1844             console_output_style = count
1845         """
1846         )
1847         output = testdir.runpytest("-n2")
1848         output.stdout.re_match_lines([r"\.{20} \s+ \[20/20\]"])
1849 
1850     def test_xdist_verbose(self, many_tests_files, testdir, monkeypatch):
1851         pytest.importorskip("xdist")
1852         monkeypatch.delenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", raising=False)
1853         output = testdir.runpytest("-n2", "-v")
1854         output.stdout.re_match_lines_random(
1855             [
1856                 r"\[gw\d\] \[\s*\d+%\] PASSED test_bar.py::test_bar\[1\]",
1857                 r"\[gw\d\] \[\s*\d+%\] PASSED test_foo.py::test_foo\[1\]",
1858                 r"\[gw\d\] \[\s*\d+%\] PASSED test_foobar.py::test_foobar\[1\]",
1859             ]
1860         )
1861         output.stdout.fnmatch_lines_random(
1862             [
1863                 line.translate(TRANS_FNMATCH)
1864                 for line in [
1865                     "test_bar.py::test_bar[0] ",
1866                     "test_foo.py::test_foo[0] ",
1867                     "test_foobar.py::test_foobar[0] ",
1868                     "[gw?] [  5%] PASSED test_*[?] ",
1869                     "[gw?] [ 10%] PASSED test_*[?] ",
1870                     "[gw?] [ 55%] PASSED test_*[?] ",
1871                     "[gw?] [ 60%] PASSED test_*[?] ",
1872                     "[gw?] [ 95%] PASSED test_*[?] ",
1873                     "[gw?] [100%] PASSED test_*[?] ",
1874                 ]
1875             ]
1876         )
1877 
1878     def test_capture_no(self, many_tests_files, testdir):
1879         output = testdir.runpytest("-s")
1880         output.stdout.re_match_lines(
1881             [r"test_bar.py \.{10}", r"test_foo.py \.{5}", r"test_foobar.py \.{5}"]
1882         )
1883 
1884         output = testdir.runpytest("--capture=no")
1885         output.stdout.no_fnmatch_line("*%]*")
1886 
1887 
1888 class TestProgressWithTeardown:
1889     """Ensure we show the correct percentages for tests that fail during teardown (#3088)"""
1890 
1891     @pytest.fixture
1892     def contest_with_teardown_fixture(self, testdir):
1893         testdir.makeconftest(
1894             """
1895             import pytest
1896 
1897             @pytest.fixture
1898             def fail_teardown():
1899                 yield
1900                 assert False
1901         """
1902         )
1903 
1904     @pytest.fixture
1905     def many_files(self, testdir, contest_with_teardown_fixture):
1906         testdir.makepyfile(
1907             test_bar="""
1908                 import pytest
1909                 @pytest.mark.parametrize('i', range(5))
1910                 def test_bar(fail_teardown, i):
1911                     pass
1912             """,
1913             test_foo="""
1914                 import pytest
1915                 @pytest.mark.parametrize('i', range(15))
1916                 def test_foo(fail_teardown, i):
1917                     pass
1918             """,
1919         )
1920 
1921     def test_teardown_simple(self, testdir, contest_with_teardown_fixture):
1922         testdir.makepyfile(
1923             """
1924             def test_foo(fail_teardown):
1925                 pass
1926         """
1927         )
1928         output = testdir.runpytest()
1929         output.stdout.re_match_lines([r"test_teardown_simple.py \.E\s+\[100%\]"])
1930 
1931     def test_teardown_with_test_also_failing(
1932         self, testdir, contest_with_teardown_fixture
1933     ):
1934         testdir.makepyfile(
1935             """
1936             def test_foo(fail_teardown):
1937                 assert 0
1938         """
1939         )
1940         output = testdir.runpytest("-rfE")
1941         output.stdout.re_match_lines(
1942             [
1943                 r"test_teardown_with_test_also_failing.py FE\s+\[100%\]",
1944                 "FAILED test_teardown_with_test_also_failing.py::test_foo - assert 0",
1945                 "ERROR test_teardown_with_test_also_failing.py::test_foo - assert False",
1946             ]
1947         )
1948 
1949     def test_teardown_many(self, testdir, many_files):
1950         output = testdir.runpytest()
1951         output.stdout.re_match_lines(
1952             [r"test_bar.py (\.E){5}\s+\[ 25%\]", r"test_foo.py (\.E){15}\s+\[100%\]"]
1953         )
1954 
1955     def test_teardown_many_verbose(
1956         self, testdir: Testdir, many_files, color_mapping
1957     ) -> None:
1958         result = testdir.runpytest("-v")
1959         result.stdout.fnmatch_lines(
1960             color_mapping.format_for_fnmatch(
1961                 [
1962                     "test_bar.py::test_bar[0] PASSED  * [  5%]",
1963                     "test_bar.py::test_bar[0] ERROR   * [  5%]",
1964                     "test_bar.py::test_bar[4] PASSED  * [ 25%]",
1965                     "test_foo.py::test_foo[14] PASSED * [100%]",
1966                     "test_foo.py::test_foo[14] ERROR  * [100%]",
1967                     "=* 20 passed, 20 errors in *",
1968                 ]
1969             )
1970         )
1971 
1972     def test_xdist_normal(self, many_files, testdir, monkeypatch):
1973         pytest.importorskip("xdist")
1974         monkeypatch.delenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", raising=False)
1975         output = testdir.runpytest("-n2")
1976         output.stdout.re_match_lines([r"[\.E]{40} \s+ \[100%\]"])
1977 
1978 
1979 def test_skip_reasons_folding():
1980     path = "xyz"
1981     lineno = 3
1982     message = "justso"
1983     longrepr = (path, lineno, message)
1984 
1985     class X:
1986         pass
1987 
1988     ev1 = X()
1989     ev1.when = "execute"
1990     ev1.skipped = True
1991     ev1.longrepr = longrepr
1992 
1993     ev2 = X()
1994     ev2.when = "execute"
1995     ev2.longrepr = longrepr
1996     ev2.skipped = True
1997 
1998     # ev3 might be a collection report
1999     ev3 = X()
2000     ev3.when = "collect"
2001     ev3.longrepr = longrepr
2002     ev3.skipped = True
2003 
2004     values = _folded_skips(py.path.local(), [ev1, ev2, ev3])
2005     assert len(values) == 1
2006     num, fspath, lineno, reason = values[0]
2007     assert num == 3
2008     assert fspath == path
2009     assert lineno == lineno
2010     assert reason == message
2011 
2012 
2013 def test_line_with_reprcrash(monkeypatch):
2014     import _pytest.terminal
2015     from wcwidth import wcswidth
2016 
2017     mocked_verbose_word = "FAILED"
2018 
2019     mocked_pos = "some::nodeid"
2020 
2021     def mock_get_pos(*args):
2022         return mocked_pos
2023 
2024     monkeypatch.setattr(_pytest.terminal, "_get_pos", mock_get_pos)
2025 
2026     class config:
2027         pass
2028 
2029     class rep:
2030         def _get_verbose_word(self, *args):
2031             return mocked_verbose_word
2032 
2033         class longrepr:
2034             class reprcrash:
2035                 pass
2036 
2037     def check(msg, width, expected):
2038         __tracebackhide__ = True
2039         if msg:
2040             rep.longrepr.reprcrash.message = msg
2041         actual = _get_line_with_reprcrash_message(config, rep(), width)
2042 
2043         assert actual == expected
2044         if actual != "{} {}".format(mocked_verbose_word, mocked_pos):
2045             assert len(actual) <= width
2046             assert wcswidth(actual) <= width
2047 
2048     # AttributeError with message
2049     check(None, 80, "FAILED some::nodeid")
2050 
2051     check("msg", 80, "FAILED some::nodeid - msg")
2052     check("msg", 3, "FAILED some::nodeid")
2053 
2054     check("msg", 24, "FAILED some::nodeid")
2055     check("msg", 25, "FAILED some::nodeid - msg")
2056 
2057     check("some longer msg", 24, "FAILED some::nodeid")
2058     check("some longer msg", 25, "FAILED some::nodeid - ...")
2059     check("some longer msg", 26, "FAILED some::nodeid - s...")
2060 
2061     check("some\nmessage", 25, "FAILED some::nodeid - ...")
2062     check("some\nmessage", 26, "FAILED some::nodeid - some")
2063     check("some\nmessage", 80, "FAILED some::nodeid - some")
2064 
2065     # Test unicode safety.
2066     check("😄😄😄😄😄\n2nd line", 25, "FAILED some::nodeid - ...")
2067     check("😄😄😄😄😄\n2nd line", 26, "FAILED some::nodeid - ...")
2068     check("😄😄😄😄😄\n2nd line", 27, "FAILED some::nodeid - 😄...")
2069     check("😄😄😄😄😄\n2nd line", 28, "FAILED some::nodeid - 😄...")
2070     check("😄😄😄😄😄\n2nd line", 29, "FAILED some::nodeid - 😄😄...")
2071 
2072     # NOTE: constructed, not sure if this is supported.
2073     mocked_pos = "nodeid::😄::withunicode"
2074     check("😄😄😄😄😄\n2nd line", 29, "FAILED nodeid::😄::withunicode")
2075     check("😄😄😄😄😄\n2nd line", 40, "FAILED nodeid::😄::withunicode - 😄😄...")
2076     check("😄😄😄😄😄\n2nd line", 41, "FAILED nodeid::😄::withunicode - 😄😄...")
2077     check("😄😄😄😄😄\n2nd line", 42, "FAILED nodeid::😄::withunicode - 😄😄😄...")
2078     check("😄😄😄😄😄\n2nd line", 80, "FAILED nodeid::😄::withunicode - 😄😄😄😄😄")
2079 
2080 
2081 @pytest.mark.parametrize(
2082     "seconds, expected",
2083     [
2084         (10.0, "10.00s"),
2085         (10.34, "10.34s"),
2086         (59.99, "59.99s"),
2087         (60.55, "60.55s (0:01:00)"),
2088         (123.55, "123.55s (0:02:03)"),
2089         (60 * 60 + 0.5, "3600.50s (1:00:00)"),
2090     ],
2091 )
2092 def test_format_session_duration(seconds, expected):
2093     from _pytest.terminal import format_session_duration
2094 
2095     assert format_session_duration(seconds) == expected
2096 
2097 
2098 def test_collecterror(testdir):
2099     p1 = testdir.makepyfile("raise SyntaxError()")
2100     result = testdir.runpytest("-ra", str(p1))
2101     result.stdout.fnmatch_lines(
2102         [
2103             "collected 0 items / 1 error",
2104             "*= ERRORS =*",
2105             "*_ ERROR collecting test_collecterror.py _*",
2106             "E   SyntaxError: *",
2107             "*= short test summary info =*",
2108             "ERROR test_collecterror.py",
2109             "*! Interrupted: 1 error during collection !*",
2110             "*= 1 error in *",
2111         ]
2112     )
2113 
2114 
2115 def test_via_exec(testdir: Testdir) -> None:
2116     p1 = testdir.makepyfile("exec('def test_via_exec(): pass')")
2117     result = testdir.runpytest(str(p1), "-vv")
2118     result.stdout.fnmatch_lines(
2119         ["test_via_exec.py::test_via_exec <- <string> PASSED*", "*= 1 passed in *"]
2120     )
2121 
2122 
2123 class TestCodeHighlight:
2124     def test_code_highlight_simple(self, testdir: Testdir, color_mapping) -> None:
2125         testdir.makepyfile(
2126             """
2127             def test_foo():
2128                 assert 1 == 10
2129         """
2130         )
2131         result = testdir.runpytest("--color=yes")
2132         color_mapping.requires_ordered_markup(result)
2133         result.stdout.fnmatch_lines(
2134             color_mapping.format_for_fnmatch(
2135                 [
2136                     "    {kw}def{hl-reset} {function}test_foo{hl-reset}():",
2137                     ">       {kw}assert{hl-reset} {number}1{hl-reset} == {number}10{hl-reset}",
2138                     "{bold}{red}E       assert 1 == 10{reset}",
2139                 ]
2140             )
2141         )
2142 
2143     def test_code_highlight_continuation(self, testdir: Testdir, color_mapping) -> None:
2144         testdir.makepyfile(
2145             """
2146             def test_foo():
2147                 print('''
2148                 '''); assert 0
2149         """
2150         )
2151         result = testdir.runpytest("--color=yes")
2152         color_mapping.requires_ordered_markup(result)
2153 
2154         result.stdout.fnmatch_lines(
2155             color_mapping.format_for_fnmatch(
2156                 [
2157                     "    {kw}def{hl-reset} {function}test_foo{hl-reset}():",
2158                     "        {print}print{hl-reset}({str}'''{hl-reset}{str}{hl-reset}",
2159                     ">   {str}    {hl-reset}{str}'''{hl-reset}); {kw}assert{hl-reset} {number}0{hl-reset}",
2160                     "{bold}{red}E       assert 0{reset}",
2161                 ]
2162             )
2163         )

</code><intended_behavior>The function or method responsible for outputting the skipped test report to the terminal should ensure the path displayed is relative to the directory from which `pytest` was invoked. This may require integrating with the file path calculation logic to adjust how the final path is resolved before being displayed. The goal is to make sure that regardless of where the test file resides in relation to the invocation directory, the displayed path accurately reflects its location in a manner consistent with the user's command-line input.</intended_behavior>]
2025-10-17 16:53:57.339 | INFO     | Search completed. Bug locations: [<file>src/_pytest/runner.py</file>
<class>None</class>
<method>None</method>
<code>
1 """ basic collect and runtest protocol implementations """
2 import bdb
3 import os
4 import sys
5 from time import perf_counter  # Intentionally not `import time` to avoid being
6 from time import time  # affected by tests which monkeypatch `time` (issue #185).
7 from typing import Callable
8 from typing import Dict
9 from typing import List
10 from typing import Optional
11 from typing import Tuple
12 
13 import attr
14 
15 from .reports import CollectErrorRepr
16 from .reports import CollectReport
17 from .reports import TestReport
18 from _pytest._code.code import ExceptionChainRepr
19 from _pytest._code.code import ExceptionInfo
20 from _pytest.compat import TYPE_CHECKING
21 from _pytest.nodes import Collector
22 from _pytest.nodes import Node
23 from _pytest.outcomes import Exit
24 from _pytest.outcomes import Skipped
25 from _pytest.outcomes import TEST_OUTCOME
26 
27 if TYPE_CHECKING:
28     from typing import Type
29     from typing_extensions import Literal
30 
31 #
32 # pytest plugin hooks
33 
34 
35 def pytest_addoption(parser):
36     group = parser.getgroup("terminal reporting", "reporting", after="general")
37     group.addoption(
38         "--durations",
39         action="store",
40         type=int,
41         default=None,
42         metavar="N",
43         help="show N slowest setup/test durations (N=0 for all).",
44     )
45 
46 
47 def pytest_terminal_summary(terminalreporter):
48     durations = terminalreporter.config.option.durations
49     verbose = terminalreporter.config.getvalue("verbose")
50     if durations is None:
51         return
52     tr = terminalreporter
53     dlist = []
54     for replist in tr.stats.values():
55         for rep in replist:
56             if hasattr(rep, "duration"):
57                 dlist.append(rep)
58     if not dlist:
59         return
60     dlist.sort(key=lambda x: x.duration)
61     dlist.reverse()
62     if not durations:
63         tr.write_sep("=", "slowest durations")
64     else:
65         tr.write_sep("=", "slowest %s durations" % durations)
66         dlist = dlist[:durations]
67 
68     for i, rep in enumerate(dlist):
69         if verbose < 2 and rep.duration < 0.005:
70             tr.write_line("")
71             tr.write_line(
72                 "(%s durations < 0.005s hidden.  Use -vv to show these durations.)"
73                 % (len(dlist) - i)
74             )
75             break
76         tr.write_line("{:02.2f}s {:<8} {}".format(rep.duration, rep.when, rep.nodeid))
77 
78 
79 def pytest_sessionstart(session):
80     session._setupstate = SetupState()
81 
82 
83 def pytest_sessionfinish(session):
84     session._setupstate.teardown_all()
85 
86 
87 def pytest_runtest_protocol(item, nextitem):
88     item.ihook.pytest_runtest_logstart(nodeid=item.nodeid, location=item.location)
89     runtestprotocol(item, nextitem=nextitem)
90     item.ihook.pytest_runtest_logfinish(nodeid=item.nodeid, location=item.location)
91     return True
92 
93 
94 def runtestprotocol(item, log=True, nextitem=None):
95     hasrequest = hasattr(item, "_request")
96     if hasrequest and not item._request:
97         item._initrequest()
98     rep = call_and_report(item, "setup", log)
99     reports = [rep]
100     if rep.passed:
101         if item.config.getoption("setupshow", False):
102             show_test_item(item)
103         if not item.config.getoption("setuponly", False):
104             reports.append(call_and_report(item, "call", log))
105     reports.append(call_and_report(item, "teardown", log, nextitem=nextitem))
106     # after all teardown hooks have been called
107     # want funcargs and request info to go away
108     if hasrequest:
109         item._request = False
110         item.funcargs = None
111     return reports
112 
113 
114 def show_test_item(item):
115     """Show test function, parameters and the fixtures of the test item."""
116     tw = item.config.get_terminal_writer()
117     tw.line()
118     tw.write(" " * 8)
119     tw.write(item.nodeid)
120     used_fixtures = sorted(getattr(item, "fixturenames", []))
121     if used_fixtures:
122         tw.write(" (fixtures used: {})".format(", ".join(used_fixtures)))
123 
124 
125 def pytest_runtest_setup(item):
126     _update_current_test_var(item, "setup")
127     item.session._setupstate.prepare(item)
128 
129 
130 def pytest_runtest_call(item):
131     _update_current_test_var(item, "call")
132     try:
133         del sys.last_type
134         del sys.last_value
135         del sys.last_traceback
136     except AttributeError:
137         pass
138     try:
139         item.runtest()
140     except Exception as e:
141         # Store trace info to allow postmortem debugging
142         sys.last_type = type(e)
143         sys.last_value = e
144         assert e.__traceback__ is not None
145         # Skip *this* frame
146         sys.last_traceback = e.__traceback__.tb_next
147         raise e
148 
149 
150 def pytest_runtest_teardown(item, nextitem):
151     _update_current_test_var(item, "teardown")
152     item.session._setupstate.teardown_exact(item, nextitem)
153     _update_current_test_var(item, None)
154 
155 
156 def _update_current_test_var(item, when):
157     """
158     Update :envvar:`PYTEST_CURRENT_TEST` to reflect the current item and stage.
159 
160     If ``when`` is None, delete ``PYTEST_CURRENT_TEST`` from the environment.
161     """
162     var_name = "PYTEST_CURRENT_TEST"
163     if when:
164         value = "{} ({})".format(item.nodeid, when)
165         # don't allow null bytes on environment variables (see #2644, #2957)
166         value = value.replace("\x00", "(null)")
167         os.environ[var_name] = value
168     else:
169         os.environ.pop(var_name)
170 
171 
172 def pytest_report_teststatus(report):
173     if report.when in ("setup", "teardown"):
174         if report.failed:
175             #      category, shortletter, verbose-word
176             return "error", "E", "ERROR"
177         elif report.skipped:
178             return "skipped", "s", "SKIPPED"
179         else:
180             return "", "", ""
181 
182 
183 #
184 # Implementation
185 
186 
187 def call_and_report(
188     item, when: "Literal['setup', 'call', 'teardown']", log=True, **kwds
189 ):
190     call = call_runtest_hook(item, when, **kwds)
191     hook = item.ihook
192     report = hook.pytest_runtest_makereport(item=item, call=call)
193     if log:
194         hook.pytest_runtest_logreport(report=report)
195     if check_interactive_exception(call, report):
196         hook.pytest_exception_interact(node=item, call=call, report=report)
197     return report
198 
199 
200 def check_interactive_exception(call, report):
201     return call.excinfo and not (
202         hasattr(report, "wasxfail")
203         or call.excinfo.errisinstance(Skipped)
204         or call.excinfo.errisinstance(bdb.BdbQuit)
205     )
206 
207 
208 def call_runtest_hook(item, when: "Literal['setup', 'call', 'teardown']", **kwds):
209     if when == "setup":
210         ihook = item.ihook.pytest_runtest_setup
211     elif when == "call":
212         ihook = item.ihook.pytest_runtest_call
213     elif when == "teardown":
214         ihook = item.ihook.pytest_runtest_teardown
215     else:
216         assert False, "Unhandled runtest hook case: {}".format(when)
217     reraise = (Exit,)  # type: Tuple[Type[BaseException], ...]
218     if not item.config.getoption("usepdb", False):
219         reraise += (KeyboardInterrupt,)
220     return CallInfo.from_call(
221         lambda: ihook(item=item, **kwds), when=when, reraise=reraise
222     )
223 
224 
225 @attr.s(repr=False)
226 class CallInfo:
227     """ Result/Exception info a function invocation.
228 
229     :param result: The return value of the call, if it didn't raise. Can only be accessed
230         if excinfo is None.
231     :param Optional[ExceptionInfo] excinfo: The captured exception of the call, if it raised.
232     :param float start: The system time when the call started, in seconds since the epoch.
233     :param float stop: The system time when the call ended, in seconds since the epoch.
234     :param float duration: The call duration, in seconds.
235     :param str when: The context of invocation: "setup", "call", "teardown", ...
236     """
237 
238     _result = attr.ib()
239     excinfo = attr.ib(type=Optional[ExceptionInfo])
240     start = attr.ib(type=float)
241     stop = attr.ib(type=float)
242     duration = attr.ib(type=float)
243     when = attr.ib(type=str)
244 
245     @property
246     def result(self):
247         if self.excinfo is not None:
248             raise AttributeError("{!r} has no valid result".format(self))
249         return self._result
250 
251     @classmethod
252     def from_call(cls, func, when, reraise=None) -> "CallInfo":
253         #: context of invocation: one of "setup", "call",
254         #: "teardown", "memocollect"
255         excinfo = None
256         start = time()
257         precise_start = perf_counter()
258         try:
259             result = func()
260         except:  # noqa
261             excinfo = ExceptionInfo.from_current()
262             if reraise is not None and excinfo.errisinstance(reraise):
263                 raise
264             result = None
265         # use the perf counter
266         precise_stop = perf_counter()
267         duration = precise_stop - precise_start
268         stop = time()
269         return cls(
270             start=start,
271             stop=stop,
272             duration=duration,
273             when=when,
274             result=result,
275             excinfo=excinfo,
276         )
277 
278     def __repr__(self):
279         if self.excinfo is None:
280             return "<CallInfo when={!r} result: {!r}>".format(self.when, self._result)
281         return "<CallInfo when={!r} excinfo={!r}>".format(self.when, self.excinfo)
282 
283 
284 def pytest_runtest_makereport(item, call):
285     return TestReport.from_item_and_call(item, call)
286 
287 
288 def pytest_make_collect_report(collector: Collector) -> CollectReport:
289     call = CallInfo.from_call(lambda: list(collector.collect()), "collect")
290     longrepr = None
291     if not call.excinfo:
292         outcome = "passed"
293     else:
294         skip_exceptions = [Skipped]
295         unittest = sys.modules.get("unittest")
296         if unittest is not None:
297             # Type ignored because unittest is loaded dynamically.
298             skip_exceptions.append(unittest.SkipTest)  # type: ignore
299         if call.excinfo.errisinstance(tuple(skip_exceptions)):
300             outcome = "skipped"
301             r_ = collector._repr_failure_py(call.excinfo, "line")
302             assert isinstance(r_, ExceptionChainRepr), repr(r_)
303             r = r_.reprcrash
304             assert r
305             longrepr = (str(r.path), r.lineno, r.message)
306         else:
307             outcome = "failed"
308             errorinfo = collector.repr_failure(call.excinfo)
309             if not hasattr(errorinfo, "toterminal"):
310                 errorinfo = CollectErrorRepr(errorinfo)
311             longrepr = errorinfo
312     rep = CollectReport(
313         collector.nodeid, outcome, longrepr, getattr(call, "result", None)
314     )
315     rep.call = call  # type: ignore # see collect_one_node
316     return rep
317 
318 
319 class SetupState:
320     """ shared state for setting up/tearing down test items or collectors. """
321 
322     def __init__(self):
323         self.stack = []  # type: List[Node]
324         self._finalizers = {}  # type: Dict[Node, List[Callable[[], None]]]
325 
326     def addfinalizer(self, finalizer, colitem):
327         """ attach a finalizer to the given colitem. """
328         assert colitem and not isinstance(colitem, tuple)
329         assert callable(finalizer)
330         # assert colitem in self.stack  # some unit tests don't setup stack :/
331         self._finalizers.setdefault(colitem, []).append(finalizer)
332 
333     def _pop_and_teardown(self):
334         colitem = self.stack.pop()
335         self._teardown_with_finalization(colitem)
336 
337     def _callfinalizers(self, colitem):
338         finalizers = self._finalizers.pop(colitem, None)
339         exc = None
340         while finalizers:
341             fin = finalizers.pop()
342             try:
343                 fin()
344             except TEST_OUTCOME as e:
345                 # XXX Only first exception will be seen by user,
346                 #     ideally all should be reported.
347                 if exc is None:
348                     exc = e
349         if exc:
350             raise exc
351 
352     def _teardown_with_finalization(self, colitem):
353         self._callfinalizers(colitem)
354         colitem.teardown()
355         for colitem in self._finalizers:
356             assert colitem in self.stack
357 
358     def teardown_all(self):
359         while self.stack:
360             self._pop_and_teardown()
361         for key in list(self._finalizers):
362             self._teardown_with_finalization(key)
363         assert not self._finalizers
364 
365     def teardown_exact(self, item, nextitem):
366         needed_collectors = nextitem and nextitem.listchain() or []
367         self._teardown_towards(needed_collectors)
368 
369     def _teardown_towards(self, needed_collectors):
370         exc = None
371         while self.stack:
372             if self.stack == needed_collectors[: len(self.stack)]:
373                 break
374             try:
375                 self._pop_and_teardown()
376             except TEST_OUTCOME as e:
377                 # XXX Only first exception will be seen by user,
378                 #     ideally all should be reported.
379                 if exc is None:
380                     exc = e
381         if exc:
382             raise exc
383 
384     def prepare(self, colitem):
385         """ setup objects along the collector chain to the test-method
386             and teardown previously setup objects."""
387         needed_collectors = colitem.listchain()
388         self._teardown_towards(needed_collectors)
389 
390         # check if the last collection node has raised an error
391         for col in self.stack:
392             if hasattr(col, "_prepare_exc"):
393                 exc = col._prepare_exc
394                 raise exc
395         for col in needed_collectors[len(self.stack) :]:
396             self.stack.append(col)
397             try:
398                 col.setup()
399             except TEST_OUTCOME as e:
400                 col._prepare_exc = e
401                 raise e
402 
403 
404 def collect_one_node(collector):
405     ihook = collector.ihook
406     ihook.pytest_collectstart(collector=collector)
407     rep = ihook.pytest_make_collect_report(collector=collector)
408     call = rep.__dict__.pop("call", None)
409     if call and check_interactive_exception(call, rep):
410         ihook.pytest_exception_interact(node=collector, call=call, report=rep)
411     return rep

</code><intended_behavior>Ensure that when a test is skipped, the method responsible for generating the report entry should dynamically calculate the path relative to the current invocation directory, not just from a static root directory or the file's location. The existing functionality for collecting and reporting test outcomes should be preserved, with the extension to handle relative path calculation more gracefully.</intended_behavior>, <file>testing/test_terminal.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 terminal reporting of the full testing process.
3 """
4 import collections
5 import os
6 import sys
7 import textwrap
8 from io import StringIO
9 from typing import Dict
10 from typing import List
11 from typing import Tuple
12 
13 import pluggy
14 import py
15 
16 import _pytest.config
17 import pytest
18 from _pytest.config import ExitCode
19 from _pytest.pytester import Testdir
20 from _pytest.reports import BaseReport
21 from _pytest.terminal import _folded_skips
22 from _pytest.terminal import _get_line_with_reprcrash_message
23 from _pytest.terminal import _plugin_nameversions
24 from _pytest.terminal import getreportopt
25 from _pytest.terminal import TerminalReporter
26 
27 DistInfo = collections.namedtuple("DistInfo", ["project_name", "version"])
28 
29 
30 TRANS_FNMATCH = str.maketrans({"[": "[[]", "]": "[]]"})
31 
32 
33 class Option:
34     def __init__(self, verbosity=0):
35         self.verbosity = verbosity
36 
37     @property
38     def args(self):
39         values = []
40         values.append("--verbosity=%d" % self.verbosity)
41         return values
42 
43 
44 @pytest.fixture(
45     params=[Option(verbosity=0), Option(verbosity=1), Option(verbosity=-1)],
46     ids=["default", "verbose", "quiet"],
47 )
48 def option(request):
49     return request.param
50 
51 
52 @pytest.mark.parametrize(
53     "input,expected",
54     [
55         ([DistInfo(project_name="test", version=1)], ["test-1"]),
56         ([DistInfo(project_name="pytest-test", version=1)], ["test-1"]),
57         (
58             [
59                 DistInfo(project_name="test", version=1),
60                 DistInfo(project_name="test", version=1),
61             ],
62             ["test-1"],
63         ),
64     ],
65     ids=["normal", "prefix-strip", "deduplicate"],
66 )
67 def test_plugin_nameversion(input, expected):
68     pluginlist = [(None, x) for x in input]
69     result = _plugin_nameversions(pluginlist)
70     assert result == expected
71 
72 
73 class TestTerminal:
74     def test_pass_skip_fail(self, testdir, option):
75         testdir.makepyfile(
76             """
77             import pytest
78             def test_ok():
79                 pass
80             def test_skip():
81                 pytest.skip("xx")
82             def test_func():
83                 assert 0
84         """
85         )
86         result = testdir.runpytest(*option.args)
87         if option.verbosity > 0:
88             result.stdout.fnmatch_lines(
89                 [
90                     "*test_pass_skip_fail.py::test_ok PASS*",
91                     "*test_pass_skip_fail.py::test_skip SKIP*",
92                     "*test_pass_skip_fail.py::test_func FAIL*",
93                 ]
94             )
95         elif option.verbosity == 0:
96             result.stdout.fnmatch_lines(["*test_pass_skip_fail.py .sF*"])
97         else:
98             result.stdout.fnmatch_lines([".sF*"])
99         result.stdout.fnmatch_lines(
100             ["    def test_func():", ">       assert 0", "E       assert 0"]
101         )
102 
103     def test_internalerror(self, testdir, linecomp):
104         modcol = testdir.getmodulecol("def test_one(): pass")
105         rep = TerminalReporter(modcol.config, file=linecomp.stringio)
106         with pytest.raises(ValueError) as excinfo:
107             raise ValueError("hello")
108         rep.pytest_internalerror(excinfo.getrepr())
109         linecomp.assert_contains_lines(["INTERNALERROR> *ValueError*hello*"])
110 
111     def test_writeline(self, testdir, linecomp):
112         modcol = testdir.getmodulecol("def test_one(): pass")
113         rep = TerminalReporter(modcol.config, file=linecomp.stringio)
114         rep.write_fspath_result(modcol.nodeid, ".")
115         rep.write_line("hello world")
116         lines = linecomp.stringio.getvalue().split("\n")
117         assert not lines[0]
118         assert lines[1].endswith(modcol.name + " .")
119         assert lines[2] == "hello world"
120 
121     def test_show_runtest_logstart(self, testdir, linecomp):
122         item = testdir.getitem("def test_func(): pass")
123         tr = TerminalReporter(item.config, file=linecomp.stringio)
124         item.config.pluginmanager.register(tr)
125         location = item.reportinfo()
126         tr.config.hook.pytest_runtest_logstart(
127             nodeid=item.nodeid, location=location, fspath=str(item.fspath)
128         )
129         linecomp.assert_contains_lines(["*test_show_runtest_logstart.py*"])
130 
131     def test_runtest_location_shown_before_test_starts(self, testdir):
132         testdir.makepyfile(
133             """
134             def test_1():
135                 import time
136                 time.sleep(20)
137         """
138         )
139         child = testdir.spawn_pytest("")
140         child.expect(".*test_runtest_location.*py")
141         child.sendeof()
142         child.kill(15)
143 
144     def test_report_collect_after_half_a_second(self, testdir):
145         """Test for "collecting" being updated after 0.5s"""
146 
147         testdir.makepyfile(
148             **{
149                 "test1.py": """
150                 import _pytest.terminal
151 
152                 _pytest.terminal.REPORT_COLLECTING_RESOLUTION = 0
153 
154                 def test_1():
155                     pass
156                     """,
157                 "test2.py": "def test_2(): pass",
158             }
159         )
160         # Explicitly test colored output.
161         testdir.monkeypatch.setenv("PY_COLORS", "1")
162 
163         child = testdir.spawn_pytest("-v test1.py test2.py")
164         child.expect(r"collecting \.\.\.")
165         child.expect(r"collecting 1 item")
166         child.expect(r"collecting 2 items")
167         child.expect(r"collected 2 items")
168         rest = child.read().decode("utf8")
169         assert "= \x1b[32m\x1b[1m2 passed\x1b[0m\x1b[32m in" in rest
170 
171     def test_itemreport_subclasses_show_subclassed_file(self, testdir):
172         testdir.makepyfile(
173             **{
174                 "tests/test_p1": """
175             class BaseTests(object):
176                 fail = False
177 
178                 def test_p1(self):
179                     if self.fail: assert 0
180                 """,
181                 "tests/test_p2": """
182             from test_p1 import BaseTests
183 
184             class TestMore(BaseTests): pass
185                 """,
186                 "tests/test_p3.py": """
187             from test_p1 import BaseTests
188 
189             BaseTests.fail = True
190 
191             class TestMore(BaseTests): pass
192         """,
193             }
194         )
195         result = testdir.runpytest("tests/test_p2.py", "--rootdir=tests")
196         result.stdout.fnmatch_lines(["tests/test_p2.py .*", "=* 1 passed in *"])
197 
198         result = testdir.runpytest("-vv", "-rA", "tests/test_p2.py", "--rootdir=tests")
199         result.stdout.fnmatch_lines(
200             [
201                 "tests/test_p2.py::TestMore::test_p1 <- test_p1.py PASSED *",
202                 "*= short test summary info =*",
203                 "PASSED tests/test_p2.py::TestMore::test_p1",
204             ]
205         )
206         result = testdir.runpytest("-vv", "-rA", "tests/test_p3.py", "--rootdir=tests")
207         result.stdout.fnmatch_lines(
208             [
209                 "tests/test_p3.py::TestMore::test_p1 <- test_p1.py FAILED *",
210                 "*_ TestMore.test_p1 _*",
211                 "    def test_p1(self):",
212                 ">       if self.fail: assert 0",
213                 "E       assert 0",
214                 "",
215                 "tests/test_p1.py:5: AssertionError",
216                 "*= short test summary info =*",
217                 "FAILED tests/test_p3.py::TestMore::test_p1 - assert 0",
218                 "*= 1 failed in *",
219             ]
220         )
221 
222     def test_itemreport_directclasses_not_shown_as_subclasses(self, testdir):
223         a = testdir.mkpydir("a123")
224         a.join("test_hello123.py").write(
225             textwrap.dedent(
226                 """\
227                 class TestClass(object):
228                     def test_method(self):
229                         pass
230                 """
231             )
232         )
233         result = testdir.runpytest("-vv")
234         assert result.ret == 0
235         result.stdout.fnmatch_lines(["*a123/test_hello123.py*PASS*"])
236         result.stdout.no_fnmatch_line("* <- *")
237 
238     @pytest.mark.parametrize("fulltrace", ("", "--fulltrace"))
239     def test_keyboard_interrupt(self, testdir, fulltrace):
240         testdir.makepyfile(
241             """
242             def test_foobar():
243                 assert 0
244             def test_spamegg():
245                 import py; pytest.skip('skip me please!')
246             def test_interrupt_me():
247                 raise KeyboardInterrupt   # simulating the user
248         """
249         )
250 
251         result = testdir.runpytest(fulltrace, no_reraise_ctrlc=True)
252         result.stdout.fnmatch_lines(
253             [
254                 "    def test_foobar():",
255                 ">       assert 0",
256                 "E       assert 0",
257                 "*_keyboard_interrupt.py:6: KeyboardInterrupt*",
258             ]
259         )
260         if fulltrace:
261             result.stdout.fnmatch_lines(
262                 ["*raise KeyboardInterrupt   # simulating the user*"]
263             )
264         else:
265             result.stdout.fnmatch_lines(
266                 ["(to show a full traceback on KeyboardInterrupt use --full-trace)"]
267             )
268         result.stdout.fnmatch_lines(["*KeyboardInterrupt*"])
269 
270     def test_keyboard_in_sessionstart(self, testdir):
271         testdir.makeconftest(
272             """
273             def pytest_sessionstart():
274                 raise KeyboardInterrupt
275         """
276         )
277         testdir.makepyfile(
278             """
279             def test_foobar():
280                 pass
281         """
282         )
283 
284         result = testdir.runpytest(no_reraise_ctrlc=True)
285         assert result.ret == 2
286         result.stdout.fnmatch_lines(["*KeyboardInterrupt*"])
287 
288     def test_collect_single_item(self, testdir):
289         """Use singular 'item' when reporting a single test item"""
290         testdir.makepyfile(
291             """
292             def test_foobar():
293                 pass
294         """
295         )
296         result = testdir.runpytest()
297         result.stdout.fnmatch_lines(["collected 1 item"])
298 
299     def test_rewrite(self, testdir, monkeypatch):
300         config = testdir.parseconfig()
301         f = StringIO()
302         monkeypatch.setattr(f, "isatty", lambda *args: True)
303         tr = TerminalReporter(config, f)
304         tr._tw.fullwidth = 10
305         tr.write("hello")
306         tr.rewrite("hey", erase=True)
307         assert f.getvalue() == "hello" + "\r" + "hey" + (6 * " ")
308 
309     def test_report_teststatus_explicit_markup(
310         self, testdir: Testdir, color_mapping
311     ) -> None:
312         """Test that TerminalReporter handles markup explicitly provided by
313         a pytest_report_teststatus hook."""
314         testdir.monkeypatch.setenv("PY_COLORS", "1")
315         testdir.makeconftest(
316             """
317             def pytest_report_teststatus(report):
318                 return 'foo', 'F', ('FOO', {'red': True})
319         """
320         )
321         testdir.makepyfile(
322             """
323             def test_foobar():
324                 pass
325         """
326         )
327         result = testdir.runpytest("-v")
328         result.stdout.fnmatch_lines(
329             color_mapping.format_for_fnmatch(["*{red}FOO{reset}*"])
330         )
331 
332 
333 class TestCollectonly:
334     def test_collectonly_basic(self, testdir):
335         testdir.makepyfile(
336             """
337             def test_func():
338                 pass
339         """
340         )
341         result = testdir.runpytest("--collect-only")
342         result.stdout.fnmatch_lines(
343             ["<Module test_collectonly_basic.py>", "  <Function test_func>"]
344         )
345 
346     def test_collectonly_skipped_module(self, testdir):
347         testdir.makepyfile(
348             """
349             import pytest
350             pytest.skip("hello")
351         """
352         )
353         result = testdir.runpytest("--collect-only", "-rs")
354         result.stdout.fnmatch_lines(["*ERROR collecting*"])
355 
356     def test_collectonly_display_test_description(self, testdir):
357         testdir.makepyfile(
358             """
359             def test_with_description():
360                 \""" This test has a description.
361                 \"""
362                 assert True
363         """
364         )
365         result = testdir.runpytest("--collect-only", "--verbose")
366         result.stdout.fnmatch_lines(["    This test has a description."])
367 
368     def test_collectonly_failed_module(self, testdir):
369         testdir.makepyfile("""raise ValueError(0)""")
370         result = testdir.runpytest("--collect-only")
371         result.stdout.fnmatch_lines(["*raise ValueError*", "*1 error*"])
372 
373     def test_collectonly_fatal(self, testdir):
374         testdir.makeconftest(
375             """
376             def pytest_collectstart(collector):
377                 assert 0, "urgs"
378         """
379         )
380         result = testdir.runpytest("--collect-only")
381         result.stdout.fnmatch_lines(["*INTERNAL*args*"])
382         assert result.ret == 3
383 
384     def test_collectonly_simple(self, testdir):
385         p = testdir.makepyfile(
386             """
387             def test_func1():
388                 pass
389             class TestClass(object):
390                 def test_method(self):
391                     pass
392         """
393         )
394         result = testdir.runpytest("--collect-only", p)
395         # assert stderr.startswith("inserting into sys.path")
396         assert result.ret == 0
397         result.stdout.fnmatch_lines(
398             [
399                 "*<Module *.py>",
400                 "* <Function test_func1>",
401                 "* <Class TestClass>",
402                 "*   <Function test_method>",
403             ]
404         )
405 
406     def test_collectonly_error(self, testdir):
407         p = testdir.makepyfile("import Errlkjqweqwe")
408         result = testdir.runpytest("--collect-only", p)
409         assert result.ret == 2
410         result.stdout.fnmatch_lines(
411             textwrap.dedent(
412                 """\
413                 *ERROR*
414                 *ImportError*
415                 *No module named *Errlk*
416                 *1 error*
417                 """
418             ).strip()
419         )
420 
421     def test_collectonly_missing_path(self, testdir):
422         """this checks issue 115,
423             failure in parseargs will cause session
424             not to have the items attribute
425         """
426         result = testdir.runpytest("--collect-only", "uhm_missing_path")
427         assert result.ret == 4
428         result.stderr.fnmatch_lines(["*ERROR: file not found*"])
429 
430     def test_collectonly_quiet(self, testdir):
431         testdir.makepyfile("def test_foo(): pass")
432         result = testdir.runpytest("--collect-only", "-q")
433         result.stdout.fnmatch_lines(["*test_foo*"])
434 
435     def test_collectonly_more_quiet(self, testdir):
436         testdir.makepyfile(test_fun="def test_foo(): pass")
437         result = testdir.runpytest("--collect-only", "-qq")
438         result.stdout.fnmatch_lines(["*test_fun.py: 1*"])
439 
440 
441 class TestFixtureReporting:
442     def test_setup_fixture_error(self, testdir):
443         testdir.makepyfile(
444             """
445             def setup_function(function):
446                 print("setup func")
447                 assert 0
448             def test_nada():
449                 pass
450         """
451         )
452         result = testdir.runpytest()
453         result.stdout.fnmatch_lines(
454             [
455                 "*ERROR at setup of test_nada*",
456                 "*setup_function(function):*",
457                 "*setup func*",
458                 "*assert 0*",
459                 "*1 error*",
460             ]
461         )
462         assert result.ret != 0
463 
464     def test_teardown_fixture_error(self, testdir):
465         testdir.makepyfile(
466             """
467             def test_nada():
468                 pass
469             def teardown_function(function):
470                 print("teardown func")
471                 assert 0
472         """
473         )
474         result = testdir.runpytest()
475         result.stdout.fnmatch_lines(
476             [
477                 "*ERROR at teardown*",
478                 "*teardown_function(function):*",
479                 "*assert 0*",
480                 "*Captured stdout*",
481                 "*teardown func*",
482                 "*1 passed*1 error*",
483             ]
484         )
485 
486     def test_teardown_fixture_error_and_test_failure(self, testdir):
487         testdir.makepyfile(
488             """
489             def test_fail():
490                 assert 0, "failingfunc"
491 
492             def teardown_function(function):
493                 print("teardown func")
494                 assert False
495         """
496         )
497         result = testdir.runpytest()
498         result.stdout.fnmatch_lines(
499             [
500                 "*ERROR at teardown of test_fail*",
501                 "*teardown_function(function):*",
502                 "*assert False*",
503                 "*Captured stdout*",
504                 "*teardown func*",
505                 "*test_fail*",
506                 "*def test_fail():",
507                 "*failingfunc*",
508                 "*1 failed*1 error*",
509             ]
510         )
511 
512     def test_setup_teardown_output_and_test_failure(self, testdir):
513         """ Test for issue #442 """
514         testdir.makepyfile(
515             """
516             def setup_function(function):
517                 print("setup func")
518 
519             def test_fail():
520                 assert 0, "failingfunc"
521 
522             def teardown_function(function):
523                 print("teardown func")
524         """
525         )
526         result = testdir.runpytest()
527         result.stdout.fnmatch_lines(
528             [
529                 "*test_fail*",
530                 "*def test_fail():",
531                 "*failingfunc*",
532                 "*Captured stdout setup*",
533                 "*setup func*",
534                 "*Captured stdout teardown*",
535                 "*teardown func*",
536                 "*1 failed*",
537             ]
538         )
539 
540 
541 class TestTerminalFunctional:
542     def test_deselected(self, testdir):
543         testpath = testdir.makepyfile(
544             """
545                 def test_one():
546                     pass
547                 def test_two():
548                     pass
549                 def test_three():
550                     pass
551            """
552         )
553         result = testdir.runpytest("-k", "test_two:", testpath)
554         result.stdout.fnmatch_lines(
555             ["collected 3 items / 1 deselected / 2 selected", "*test_deselected.py ..*"]
556         )
557         assert result.ret == 0
558 
559     def test_deselected_with_hookwrapper(self, testdir):
560         testpath = testdir.makeconftest(
561             """
562             import pytest
563 
564             @pytest.hookimpl(hookwrapper=True)
565             def pytest_collection_modifyitems(config, items):
566                 yield
567                 deselected = items.pop()
568                 config.hook.pytest_deselected(items=[deselected])
569             """
570         )
571         testpath = testdir.makepyfile(
572             """
573                 def test_one():
574                     pass
575                 def test_two():
576                     pass
577                 def test_three():
578                     pass
579            """
580         )
581         result = testdir.runpytest(testpath)
582         result.stdout.fnmatch_lines(
583             [
584                 "collected 3 items / 1 deselected / 2 selected",
585                 "*= 2 passed, 1 deselected in*",
586             ]
587         )
588         assert result.ret == 0
589 
590     def test_show_deselected_items_using_markexpr_before_test_execution(self, testdir):
591         testdir.makepyfile(
592             test_show_deselected="""
593             import pytest
594 
595             @pytest.mark.foo
596             def test_foobar():
597                 pass
598 
599             @pytest.mark.bar
600             def test_bar():
601                 pass
602 
603             def test_pass():
604                 pass
605         """
606         )
607         result = testdir.runpytest("-m", "not foo")
608         result.stdout.fnmatch_lines(
609             [
610                 "collected 3 items / 1 deselected / 2 selected",
611                 "*test_show_deselected.py ..*",
612                 "*= 2 passed, 1 deselected in * =*",
613             ]
614         )
615         result.stdout.no_fnmatch_line("*= 1 deselected =*")
616         assert result.ret == 0
617 
618     def test_no_skip_summary_if_failure(self, testdir):
619         testdir.makepyfile(
620             """
621             import pytest
622             def test_ok():
623                 pass
624             def test_fail():
625                 assert 0
626             def test_skip():
627                 pytest.skip("dontshow")
628         """
629         )
630         result = testdir.runpytest()
631         assert result.stdout.str().find("skip test summary") == -1
632         assert result.ret == 1
633 
634     def test_passes(self, testdir):
635         p1 = testdir.makepyfile(
636             """
637             def test_passes():
638                 pass
639             class TestClass(object):
640                 def test_method(self):
641                     pass
642         """
643         )
644         old = p1.dirpath().chdir()
645         try:
646             result = testdir.runpytest()
647         finally:
648             old.chdir()
649         result.stdout.fnmatch_lines(["test_passes.py ..*", "* 2 pass*"])
650         assert result.ret == 0
651 
652     def test_header_trailer_info(self, testdir, request):
653         testdir.monkeypatch.delenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD")
654         testdir.makepyfile(
655             """
656             def test_passes():
657                 pass
658         """
659         )
660         result = testdir.runpytest()
661         verinfo = ".".join(map(str, sys.version_info[:3]))
662         result.stdout.fnmatch_lines(
663             [
664                 "*===== test session starts ====*",
665                 "platform %s -- Python %s*pytest-%s*py-%s*pluggy-%s"
666                 % (
667                     sys.platform,
668                     verinfo,
669                     pytest.__version__,
670                     py.__version__,
671                     pluggy.__version__,
672                 ),
673                 "*test_header_trailer_info.py .*",
674                 "=* 1 passed*in *.[0-9][0-9]s *=",
675             ]
676         )
677         if request.config.pluginmanager.list_plugin_distinfo():
678             result.stdout.fnmatch_lines(["plugins: *"])
679 
680     def test_header(self, testdir):
681         testdir.tmpdir.join("tests").ensure_dir()
682         testdir.tmpdir.join("gui").ensure_dir()
683 
684         # no ini file
685         result = testdir.runpytest()
686         result.stdout.fnmatch_lines(["rootdir: *test_header0"])
687 
688         # with inifile
689         testdir.makeini("""[pytest]""")
690         result = testdir.runpytest()
691         result.stdout.fnmatch_lines(["rootdir: *test_header0, inifile: tox.ini"])
692 
693         # with testpaths option, and not passing anything in the command-line
694         testdir.makeini(
695             """
696             [pytest]
697             testpaths = tests gui
698         """
699         )
700         result = testdir.runpytest()
701         result.stdout.fnmatch_lines(
702             ["rootdir: *test_header0, inifile: tox.ini, testpaths: tests, gui"]
703         )
704 
705         # with testpaths option, passing directory in command-line: do not show testpaths then
706         result = testdir.runpytest("tests")
707         result.stdout.fnmatch_lines(["rootdir: *test_header0, inifile: tox.ini"])
708 
709     def test_showlocals(self, testdir):
710         p1 = testdir.makepyfile(
711             """
712             def test_showlocals():
713                 x = 3
714                 y = "x" * 5000
715                 assert 0
716         """
717         )
718         result = testdir.runpytest(p1, "-l")
719         result.stdout.fnmatch_lines(
720             [
721                 # "_ _ * Locals *",
722                 "x* = 3",
723                 "y* = 'xxxxxx*",
724             ]
725         )
726 
727     def test_showlocals_short(self, testdir):
728         p1 = testdir.makepyfile(
729             """
730             def test_showlocals_short():
731                 x = 3
732                 y = "xxxx"
733                 assert 0
734         """
735         )
736         result = testdir.runpytest(p1, "-l", "--tb=short")
737         result.stdout.fnmatch_lines(
738             [
739                 "test_showlocals_short.py:*",
740                 "    assert 0",
741                 "E   assert 0",
742                 "        x          = 3",
743                 "        y          = 'xxxx'",
744             ]
745         )
746 
747     @pytest.fixture
748     def verbose_testfile(self, testdir):
749         return testdir.makepyfile(
750             """
751             import pytest
752             def test_fail():
753                 raise ValueError()
754             def test_pass():
755                 pass
756             class TestClass(object):
757                 def test_skip(self):
758                     pytest.skip("hello")
759             def test_gen():
760                 def check(x):
761                     assert x == 1
762                 yield check, 0
763         """
764         )
765 
766     def test_verbose_reporting(self, verbose_testfile, testdir):
767         result = testdir.runpytest(
768             verbose_testfile, "-v", "-Walways::pytest.PytestWarning"
769         )
770         result.stdout.fnmatch_lines(
771             [
772                 "*test_verbose_reporting.py::test_fail *FAIL*",
773                 "*test_verbose_reporting.py::test_pass *PASS*",
774                 "*test_verbose_reporting.py::TestClass::test_skip *SKIP*",
775                 "*test_verbose_reporting.py::test_gen *XFAIL*",
776             ]
777         )
778         assert result.ret == 1
779 
780     def test_verbose_reporting_xdist(self, verbose_testfile, testdir, pytestconfig):
781         if not pytestconfig.pluginmanager.get_plugin("xdist"):
782             pytest.skip("xdist plugin not installed")
783 
784         testdir.monkeypatch.delenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD")
785         result = testdir.runpytest(
786             verbose_testfile, "-v", "-n 1", "-Walways::pytest.PytestWarning"
787         )
788         result.stdout.fnmatch_lines(
789             ["*FAIL*test_verbose_reporting_xdist.py::test_fail*"]
790         )
791         assert result.ret == 1
792 
793     def test_quiet_reporting(self, testdir):
794         p1 = testdir.makepyfile("def test_pass(): pass")
795         result = testdir.runpytest(p1, "-q")
796         s = result.stdout.str()
797         assert "test session starts" not in s
798         assert p1.basename not in s
799         assert "===" not in s
800         assert "passed" in s
801 
802     def test_more_quiet_reporting(self, testdir):
803         p1 = testdir.makepyfile("def test_pass(): pass")
804         result = testdir.runpytest(p1, "-qq")
805         s = result.stdout.str()
806         assert "test session starts" not in s
807         assert p1.basename not in s
808         assert "===" not in s
809         assert "passed" not in s
810 
811     @pytest.mark.parametrize(
812         "params", [(), ("--collect-only",)], ids=["no-params", "collect-only"]
813     )
814     def test_report_collectionfinish_hook(self, testdir, params):
815         testdir.makeconftest(
816             """
817             def pytest_report_collectionfinish(config, startdir, items):
818                 return ['hello from hook: {0} items'.format(len(items))]
819         """
820         )
821         testdir.makepyfile(
822             """
823             import pytest
824             @pytest.mark.parametrize('i', range(3))
825             def test(i):
826                 pass
827         """
828         )
829         result = testdir.runpytest(*params)
830         result.stdout.fnmatch_lines(["collected 3 items", "hello from hook: 3 items"])
831 
832     def test_summary_f_alias(self, testdir):
833         """Test that 'f' and 'F' report chars are aliases and don't show up twice in the summary (#6334)"""
834         testdir.makepyfile(
835             """
836             def test():
837                 assert False
838             """
839         )
840         result = testdir.runpytest("-rfF")
841         expected = "FAILED test_summary_f_alias.py::test - assert False"
842         result.stdout.fnmatch_lines([expected])
843         assert result.stdout.lines.count(expected) == 1
844 
845     def test_summary_s_alias(self, testdir):
846         """Test that 's' and 'S' report chars are aliases and don't show up twice in the summary"""
847         testdir.makepyfile(
848             """
849             import pytest
850 
851             @pytest.mark.skip
852             def test():
853                 pass
854             """
855         )
856         result = testdir.runpytest("-rsS")
857         expected = "SKIPPED [1] test_summary_s_alias.py:3: unconditional skip"
858         result.stdout.fnmatch_lines([expected])
859         assert result.stdout.lines.count(expected) == 1
860 
861 
862 def test_fail_extra_reporting(testdir, monkeypatch):
863     monkeypatch.setenv("COLUMNS", "80")
864     testdir.makepyfile("def test_this(): assert 0, 'this_failed' * 100")
865     result = testdir.runpytest("-rN")
866     result.stdout.no_fnmatch_line("*short test summary*")
867     result = testdir.runpytest()
868     result.stdout.fnmatch_lines(
869         [
870             "*test summary*",
871             "FAILED test_fail_extra_reporting.py::test_this - AssertionError: this_failedt...",
872         ]
873     )
874 
875 
876 def test_fail_reporting_on_pass(testdir):
877     testdir.makepyfile("def test_this(): assert 1")
878     result = testdir.runpytest("-rf")
879     result.stdout.no_fnmatch_line("*short test summary*")
880 
881 
882 def test_pass_extra_reporting(testdir):
883     testdir.makepyfile("def test_this(): assert 1")
884     result = testdir.runpytest()
885     result.stdout.no_fnmatch_line("*short test summary*")
886     result = testdir.runpytest("-rp")
887     result.stdout.fnmatch_lines(["*test summary*", "PASS*test_pass_extra_reporting*"])
888 
889 
890 def test_pass_reporting_on_fail(testdir):
891     testdir.makepyfile("def test_this(): assert 0")
892     result = testdir.runpytest("-rp")
893     result.stdout.no_fnmatch_line("*short test summary*")
894 
895 
896 def test_pass_output_reporting(testdir):
897     testdir.makepyfile(
898         """
899         def setup_module():
900             print("setup_module")
901 
902         def teardown_module():
903             print("teardown_module")
904 
905         def test_pass_has_output():
906             print("Four score and seven years ago...")
907 
908         def test_pass_no_output():
909             pass
910     """
911     )
912     result = testdir.runpytest()
913     s = result.stdout.str()
914     assert "test_pass_has_output" not in s
915     assert "Four score and seven years ago..." not in s
916     assert "test_pass_no_output" not in s
917     result = testdir.runpytest("-rPp")
918     result.stdout.fnmatch_lines(
919         [
920             "*= PASSES =*",
921             "*_ test_pass_has_output _*",
922             "*- Captured stdout setup -*",
923             "setup_module",
924             "*- Captured stdout call -*",
925             "Four score and seven years ago...",
926             "*- Captured stdout teardown -*",
927             "teardown_module",
928             "*= short test summary info =*",
929             "PASSED test_pass_output_reporting.py::test_pass_has_output",
930             "PASSED test_pass_output_reporting.py::test_pass_no_output",
931             "*= 2 passed in *",
932         ]
933     )
934 
935 
936 def test_color_yes(testdir, color_mapping):
937     p1 = testdir.makepyfile(
938         """
939         def fail():
940             assert 0
941 
942         def test_this():
943             fail()
944         """
945     )
946     result = testdir.runpytest("--color=yes", str(p1))
947     color_mapping.requires_ordered_markup(result)
948     result.stdout.fnmatch_lines(
949         color_mapping.format_for_fnmatch(
950             [
951                 "{bold}=*= test session starts =*={reset}",
952                 "collected 1 item",
953                 "",
954                 "test_color_yes.py {red}F{reset}{red} * [100%]{reset}",
955                 "",
956                 "=*= FAILURES =*=",
957                 "{red}{bold}_*_ test_this _*_{reset}",
958                 "",
959                 "    {kw}def{hl-reset} {function}test_this{hl-reset}():",
960                 ">       fail()",
961                 "",
962                 "{bold}{red}test_color_yes.py{reset}:5: ",
963                 "_ _ * _ _*",
964                 "",
965                 "    {kw}def{hl-reset} {function}fail{hl-reset}():",
966                 ">       {kw}assert{hl-reset} {number}0{hl-reset}",
967                 "{bold}{red}E       assert 0{reset}",
968                 "",
969                 "{bold}{red}test_color_yes.py{reset}:2: AssertionError",
970                 "{red}=*= {red}{bold}1 failed{reset}{red} in *s{reset}{red} =*={reset}",
971             ]
972         )
973     )
974     result = testdir.runpytest("--color=yes", "--tb=short", str(p1))
975     result.stdout.fnmatch_lines(
976         color_mapping.format_for_fnmatch(
977             [
978                 "{bold}=*= test session starts =*={reset}",
979                 "collected 1 item",
980                 "",
981                 "test_color_yes.py {red}F{reset}{red} * [100%]{reset}",
982                 "",
983                 "=*= FAILURES =*=",
984                 "{red}{bold}_*_ test_this _*_{reset}",
985                 "{bold}{red}test_color_yes.py{reset}:5: in test_this",
986                 "    fail()",
987                 "{bold}{red}test_color_yes.py{reset}:2: in fail",
988                 "    {kw}assert{hl-reset} {number}0{hl-reset}",
989                 "{bold}{red}E   assert 0{reset}",
990                 "{red}=*= {red}{bold}1 failed{reset}{red} in *s{reset}{red} =*={reset}",
991             ]
992         )
993     )
994 
995 
996 def test_color_no(testdir):
997     testdir.makepyfile("def test_this(): assert 1")
998     result = testdir.runpytest("--color=no")
999     assert "test session starts" in result.stdout.str()
1000     result.stdout.no_fnmatch_line("*\x1b[1m*")
1001 
1002 
1003 @pytest.mark.parametrize("verbose", [True, False])
1004 def test_color_yes_collection_on_non_atty(testdir, verbose):
1005     """skip collect progress report when working on non-terminals.
1006     #1397
1007     """
1008     testdir.makepyfile(
1009         """
1010         import pytest
1011         @pytest.mark.parametrize('i', range(10))
1012         def test_this(i):
1013             assert 1
1014     """
1015     )
1016     args = ["--color=yes"]
1017     if verbose:
1018         args.append("-vv")
1019     result = testdir.runpytest(*args)
1020     assert "test session starts" in result.stdout.str()
1021     assert "\x1b[1m" in result.stdout.str()
1022     result.stdout.no_fnmatch_line("*collecting 10 items*")
1023     if verbose:
1024         assert "collecting ..." in result.stdout.str()
1025     assert "collected 10 items" in result.stdout.str()
1026 
1027 
1028 def test_getreportopt():
1029     from _pytest.terminal import _REPORTCHARS_DEFAULT
1030 
1031     class Config:
1032         class Option:
1033             reportchars = _REPORTCHARS_DEFAULT
1034             disable_warnings = False
1035 
1036         option = Option()
1037 
1038     config = Config()
1039 
1040     assert _REPORTCHARS_DEFAULT == "fE"
1041 
1042     # Default.
1043     assert getreportopt(config) == "wfE"
1044 
1045     config.option.reportchars = "sf"
1046     assert getreportopt(config) == "wsf"
1047 
1048     config.option.reportchars = "sfxw"
1049     assert getreportopt(config) == "sfxw"
1050 
1051     config.option.reportchars = "a"
1052     assert getreportopt(config) == "wsxXEf"
1053 
1054     config.option.reportchars = "N"
1055     assert getreportopt(config) == "w"
1056 
1057     config.option.reportchars = "NwfE"
1058     assert getreportopt(config) == "wfE"
1059 
1060     config.option.reportchars = "NfENx"
1061     assert getreportopt(config) == "wx"
1062 
1063     # Now with --disable-warnings.
1064     config.option.disable_warnings = True
1065     config.option.reportchars = "a"
1066     assert getreportopt(config) == "sxXEf"
1067 
1068     config.option.reportchars = "sfx"
1069     assert getreportopt(config) == "sfx"
1070 
1071     config.option.reportchars = "sfxw"
1072     assert getreportopt(config) == "sfx"
1073 
1074     config.option.reportchars = "a"
1075     assert getreportopt(config) == "sxXEf"
1076 
1077     config.option.reportchars = "A"
1078     assert getreportopt(config) == "PpsxXEf"
1079 
1080     config.option.reportchars = "AN"
1081     assert getreportopt(config) == ""
1082 
1083     config.option.reportchars = "NwfE"
1084     assert getreportopt(config) == "fE"
1085 
1086 
1087 def test_terminalreporter_reportopt_addopts(testdir):
1088     testdir.makeini("[pytest]\naddopts=-rs")
1089     testdir.makepyfile(
1090         """
1091         import pytest
1092 
1093         @pytest.fixture
1094         def tr(request):
1095             tr = request.config.pluginmanager.getplugin("terminalreporter")
1096             return tr
1097         def test_opt(tr):
1098             assert tr.hasopt('skipped')
1099             assert not tr.hasopt('qwe')
1100     """
1101     )
1102     result = testdir.runpytest()
1103     result.stdout.fnmatch_lines(["*1 passed*"])
1104 
1105 
1106 def test_tbstyle_short(testdir):
1107     p = testdir.makepyfile(
1108         """
1109         import pytest
1110 
1111         @pytest.fixture
1112         def arg(request):
1113             return 42
1114         def test_opt(arg):
1115             x = 0
1116             assert x
1117     """
1118     )
1119     result = testdir.runpytest("--tb=short")
1120     s = result.stdout.str()
1121     assert "arg = 42" not in s
1122     assert "x = 0" not in s
1123     result.stdout.fnmatch_lines(["*%s:8*" % p.basename, "    assert x", "E   assert*"])
1124     result = testdir.runpytest()
1125     s = result.stdout.str()
1126     assert "x = 0" in s
1127     assert "assert x" in s
1128 
1129 
1130 def test_traceconfig(testdir):
1131     result = testdir.runpytest("--traceconfig")
1132     result.stdout.fnmatch_lines(["*active plugins*"])
1133     assert result.ret == ExitCode.NO_TESTS_COLLECTED
1134 
1135 
1136 class TestGenericReporting:
1137     """ this test class can be subclassed with a different option
1138         provider to run e.g. distributed tests.
1139     """
1140 
1141     def test_collect_fail(self, testdir, option):
1142         testdir.makepyfile("import xyz\n")
1143         result = testdir.runpytest(*option.args)
1144         result.stdout.fnmatch_lines(
1145             ["ImportError while importing*", "*No module named *xyz*", "*1 error*"]
1146         )
1147 
1148     def test_maxfailures(self, testdir, option):
1149         testdir.makepyfile(
1150             """
1151             def test_1():
1152                 assert 0
1153             def test_2():
1154                 assert 0
1155             def test_3():
1156                 assert 0
1157         """
1158         )
1159         result = testdir.runpytest("--maxfail=2", *option.args)
1160         result.stdout.fnmatch_lines(
1161             [
1162                 "*def test_1():*",
1163                 "*def test_2():*",
1164                 "*! stopping after 2 failures !*",
1165                 "*2 failed*",
1166             ]
1167         )
1168 
1169     def test_maxfailures_with_interrupted(self, testdir):
1170         testdir.makepyfile(
1171             """
1172             def test(request):
1173                 request.session.shouldstop = "session_interrupted"
1174                 assert 0
1175         """
1176         )
1177         result = testdir.runpytest("--maxfail=1", "-ra")
1178         result.stdout.fnmatch_lines(
1179             [
1180                 "*= short test summary info =*",
1181                 "FAILED *",
1182                 "*! stopping after 1 failures !*",
1183                 "*! session_interrupted !*",
1184                 "*= 1 failed in*",
1185             ]
1186         )
1187 
1188     def test_tb_option(self, testdir, option):
1189         testdir.makepyfile(
1190             """
1191             import pytest
1192             def g():
1193                 raise IndexError
1194             def test_func():
1195                 print(6*7)
1196                 g()  # --calling--
1197         """
1198         )
1199         for tbopt in ["long", "short", "no"]:
1200             print("testing --tb=%s..." % tbopt)
1201             result = testdir.runpytest("-rN", "--tb=%s" % tbopt)
1202             s = result.stdout.str()
1203             if tbopt == "long":
1204                 assert "print(6*7)" in s
1205             else:
1206                 assert "print(6*7)" not in s
1207             if tbopt != "no":
1208                 assert "--calling--" in s
1209                 assert "IndexError" in s
1210             else:
1211                 assert "FAILURES" not in s
1212                 assert "--calling--" not in s
1213                 assert "IndexError" not in s
1214 
1215     def test_tb_crashline(self, testdir, option):
1216         p = testdir.makepyfile(
1217             """
1218             import pytest
1219             def g():
1220                 raise IndexError
1221             def test_func1():
1222                 print(6*7)
1223                 g()  # --calling--
1224             def test_func2():
1225                 assert 0, "hello"
1226         """
1227         )
1228         result = testdir.runpytest("--tb=line")
1229         bn = p.basename
1230         result.stdout.fnmatch_lines(
1231             ["*%s:3: IndexError*" % bn, "*%s:8: AssertionError: hello*" % bn]
1232         )
1233         s = result.stdout.str()
1234         assert "def test_func2" not in s
1235 
1236     def test_pytest_report_header(self, testdir, option):
1237         testdir.makeconftest(
1238             """
1239             def pytest_sessionstart(session):
1240                 session.config._somevalue = 42
1241             def pytest_report_header(config):
1242                 return "hello: %s" % config._somevalue
1243         """
1244         )
1245         testdir.mkdir("a").join("conftest.py").write(
1246             """
1247 def pytest_report_header(config, startdir):
1248     return ["line1", str(startdir)]
1249 """
1250         )
1251         result = testdir.runpytest("a")
1252         result.stdout.fnmatch_lines(["*hello: 42*", "line1", str(testdir.tmpdir)])
1253 
1254     def test_show_capture(self, testdir):
1255         testdir.makepyfile(
1256             """
1257             import sys
1258             import logging
1259             def test_one():
1260                 sys.stdout.write('!This is stdout!')
1261                 sys.stderr.write('!This is stderr!')
1262                 logging.warning('!This is a warning log msg!')
1263                 assert False, 'Something failed'
1264         """
1265         )
1266 
1267         result = testdir.runpytest("--tb=short")
1268         result.stdout.fnmatch_lines(
1269             [
1270                 "!This is stdout!",
1271                 "!This is stderr!",
1272                 "*WARNING*!This is a warning log msg!",
1273             ]
1274         )
1275 
1276         result = testdir.runpytest("--show-capture=all", "--tb=short")
1277         result.stdout.fnmatch_lines(
1278             [
1279                 "!This is stdout!",
1280                 "!This is stderr!",
1281                 "*WARNING*!This is a warning log msg!",
1282             ]
1283         )
1284 
1285         stdout = testdir.runpytest("--show-capture=stdout", "--tb=short").stdout.str()
1286         assert "!This is stderr!" not in stdout
1287         assert "!This is stdout!" in stdout
1288         assert "!This is a warning log msg!" not in stdout
1289 
1290         stdout = testdir.runpytest("--show-capture=stderr", "--tb=short").stdout.str()
1291         assert "!This is stdout!" not in stdout
1292         assert "!This is stderr!" in stdout
1293         assert "!This is a warning log msg!" not in stdout
1294 
1295         stdout = testdir.runpytest("--show-capture=log", "--tb=short").stdout.str()
1296         assert "!This is stdout!" not in stdout
1297         assert "!This is stderr!" not in stdout
1298         assert "!This is a warning log msg!" in stdout
1299 
1300         stdout = testdir.runpytest("--show-capture=no", "--tb=short").stdout.str()
1301         assert "!This is stdout!" not in stdout
1302         assert "!This is stderr!" not in stdout
1303         assert "!This is a warning log msg!" not in stdout
1304 
1305     def test_show_capture_with_teardown_logs(self, testdir):
1306         """Ensure that the capturing of teardown logs honor --show-capture setting"""
1307         testdir.makepyfile(
1308             """
1309             import logging
1310             import sys
1311             import pytest
1312 
1313             @pytest.fixture(scope="function", autouse="True")
1314             def hook_each_test(request):
1315                 yield
1316                 sys.stdout.write("!stdout!")
1317                 sys.stderr.write("!stderr!")
1318                 logging.warning("!log!")
1319 
1320             def test_func():
1321                 assert False
1322         """
1323         )
1324 
1325         result = testdir.runpytest("--show-capture=stdout", "--tb=short").stdout.str()
1326         assert "!stdout!" in result
1327         assert "!stderr!" not in result
1328         assert "!log!" not in result
1329 
1330         result = testdir.runpytest("--show-capture=stderr", "--tb=short").stdout.str()
1331         assert "!stdout!" not in result
1332         assert "!stderr!" in result
1333         assert "!log!" not in result
1334 
1335         result = testdir.runpytest("--show-capture=log", "--tb=short").stdout.str()
1336         assert "!stdout!" not in result
1337         assert "!stderr!" not in result
1338         assert "!log!" in result
1339 
1340         result = testdir.runpytest("--show-capture=no", "--tb=short").stdout.str()
1341         assert "!stdout!" not in result
1342         assert "!stderr!" not in result
1343         assert "!log!" not in result
1344 
1345 
1346 @pytest.mark.xfail("not hasattr(os, 'dup')")
1347 def test_fdopen_kept_alive_issue124(testdir):
1348     testdir.makepyfile(
1349         """
1350         import os, sys
1351         k = []
1352         def test_open_file_and_keep_alive(capfd):
1353             stdout = os.fdopen(1, 'w', 1)
1354             k.append(stdout)
1355 
1356         def test_close_kept_alive_file():
1357             stdout = k.pop()
1358             stdout.close()
1359     """
1360     )
1361     result = testdir.runpytest()
1362     result.stdout.fnmatch_lines(["*2 passed*"])
1363 
1364 
1365 def test_tbstyle_native_setup_error(testdir):
1366     testdir.makepyfile(
1367         """
1368         import pytest
1369         @pytest.fixture
1370         def setup_error_fixture():
1371             raise Exception("error in exception")
1372 
1373         def test_error_fixture(setup_error_fixture):
1374             pass
1375     """
1376     )
1377     result = testdir.runpytest("--tb=native")
1378     result.stdout.fnmatch_lines(
1379         ['*File *test_tbstyle_native_setup_error.py", line *, in setup_error_fixture*']
1380     )
1381 
1382 
1383 def test_terminal_summary(testdir):
1384     testdir.makeconftest(
1385         """
1386         def pytest_terminal_summary(terminalreporter, exitstatus):
1387             w = terminalreporter
1388             w.section("hello")
1389             w.line("world")
1390             w.line("exitstatus: {0}".format(exitstatus))
1391     """
1392     )
1393     result = testdir.runpytest()
1394     result.stdout.fnmatch_lines(
1395         """
1396         *==== hello ====*
1397         world
1398         exitstatus: 5
1399     """
1400     )
1401 
1402 
1403 @pytest.mark.filterwarnings("default")
1404 def test_terminal_summary_warnings_are_displayed(testdir):
1405     """Test that warnings emitted during pytest_terminal_summary are displayed.
1406     (#1305).
1407     """
1408     testdir.makeconftest(
1409         """
1410         import warnings
1411         def pytest_terminal_summary(terminalreporter):
1412             warnings.warn(UserWarning('internal warning'))
1413     """
1414     )
1415     testdir.makepyfile(
1416         """
1417         def test_failure():
1418             import warnings
1419             warnings.warn("warning_from_" + "test")
1420             assert 0
1421     """
1422     )
1423     result = testdir.runpytest("-ra")
1424     result.stdout.fnmatch_lines(
1425         [
1426             "*= warnings summary =*",
1427             "*warning_from_test*",
1428             "*= short test summary info =*",
1429             "*= warnings summary (final) =*",
1430             "*conftest.py:3:*internal warning",
1431             "*== 1 failed, 2 warnings in *",
1432         ]
1433     )
1434     result.stdout.no_fnmatch_line("*None*")
1435     stdout = result.stdout.str()
1436     assert stdout.count("warning_from_test") == 1
1437     assert stdout.count("=== warnings summary ") == 2
1438 
1439 
1440 @pytest.mark.filterwarnings("default")
1441 def test_terminal_summary_warnings_header_once(testdir):
1442     testdir.makepyfile(
1443         """
1444         def test_failure():
1445             import warnings
1446             warnings.warn("warning_from_" + "test")
1447             assert 0
1448     """
1449     )
1450     result = testdir.runpytest("-ra")
1451     result.stdout.fnmatch_lines(
1452         [
1453             "*= warnings summary =*",
1454             "*warning_from_test*",
1455             "*= short test summary info =*",
1456             "*== 1 failed, 1 warning in *",
1457         ]
1458     )
1459     result.stdout.no_fnmatch_line("*None*")
1460     stdout = result.stdout.str()
1461     assert stdout.count("warning_from_test") == 1
1462     assert stdout.count("=== warnings summary ") == 1
1463 
1464 
1465 @pytest.fixture(scope="session")
1466 def tr() -> TerminalReporter:
1467     config = _pytest.config._prepareconfig()
1468     return TerminalReporter(config)
1469 
1470 
1471 @pytest.mark.parametrize(
1472     "exp_color, exp_line, stats_arg",
1473     [
1474         # The method under test only cares about the length of each
1475         # dict value, not the actual contents, so tuples of anything
1476         # suffice
1477         # Important statuses -- the highest priority of these always wins
1478         ("red", [("1 failed", {"bold": True, "red": True})], {"failed": (1,)}),
1479         (
1480             "red",
1481             [
1482                 ("1 failed", {"bold": True, "red": True}),
1483                 ("1 passed", {"bold": False, "green": True}),
1484             ],
1485             {"failed": (1,), "passed": (1,)},
1486         ),
1487         ("red", [("1 error", {"bold": True, "red": True})], {"error": (1,)}),
1488         ("red", [("2 errors", {"bold": True, "red": True})], {"error": (1, 2)}),
1489         (
1490             "red",
1491             [
1492                 ("1 passed", {"bold": False, "green": True}),
1493                 ("1 error", {"bold": True, "red": True}),
1494             ],
1495             {"error": (1,), "passed": (1,)},
1496         ),
1497         # (a status that's not known to the code)
1498         ("yellow", [("1 weird", {"bold": True, "yellow": True})], {"weird": (1,)}),
1499         (
1500             "yellow",
1501             [
1502                 ("1 passed", {"bold": False, "green": True}),
1503                 ("1 weird", {"bold": True, "yellow": True}),
1504             ],
1505             {"weird": (1,), "passed": (1,)},
1506         ),
1507         ("yellow", [("1 warning", {"bold": True, "yellow": True})], {"warnings": (1,)}),
1508         (
1509             "yellow",
1510             [
1511                 ("1 passed", {"bold": False, "green": True}),
1512                 ("1 warning", {"bold": True, "yellow": True}),
1513             ],
1514             {"warnings": (1,), "passed": (1,)},
1515         ),
1516         (
1517             "green",
1518             [("5 passed", {"bold": True, "green": True})],
1519             {"passed": (1, 2, 3, 4, 5)},
1520         ),
1521         # "Boring" statuses.  These have no effect on the color of the summary
1522         # line.  Thus, if *every* test has a boring status, the summary line stays
1523         # at its default color, i.e. yellow, to warn the user that the test run
1524         # produced no useful information
1525         ("yellow", [("1 skipped", {"bold": True, "yellow": True})], {"skipped": (1,)}),
1526         (
1527             "green",
1528             [
1529                 ("1 passed", {"bold": True, "green": True}),
1530                 ("1 skipped", {"bold": False, "yellow": True}),
1531             ],
1532             {"skipped": (1,), "passed": (1,)},
1533         ),
1534         (
1535             "yellow",
1536             [("1 deselected", {"bold": True, "yellow": True})],
1537             {"deselected": (1,)},
1538         ),
1539         (
1540             "green",
1541             [
1542                 ("1 passed", {"bold": True, "green": True}),
1543                 ("1 deselected", {"bold": False, "yellow": True}),
1544             ],
1545             {"deselected": (1,), "passed": (1,)},
1546         ),
1547         ("yellow", [("1 xfailed", {"bold": True, "yellow": True})], {"xfailed": (1,)}),
1548         (
1549             "green",
1550             [
1551                 ("1 passed", {"bold": True, "green": True}),
1552                 ("1 xfailed", {"bold": False, "yellow": True}),
1553             ],
1554             {"xfailed": (1,), "passed": (1,)},
1555         ),
1556         ("yellow", [("1 xpassed", {"bold": True, "yellow": True})], {"xpassed": (1,)}),
1557         (
1558             "yellow",
1559             [
1560                 ("1 passed", {"bold": False, "green": True}),
1561                 ("1 xpassed", {"bold": True, "yellow": True}),
1562             ],
1563             {"xpassed": (1,), "passed": (1,)},
1564         ),
1565         # Likewise if no tests were found at all
1566         ("yellow", [("no tests ran", {"yellow": True})], {}),
1567         # Test the empty-key special case
1568         ("yellow", [("no tests ran", {"yellow": True})], {"": (1,)}),
1569         (
1570             "green",
1571             [("1 passed", {"bold": True, "green": True})],
1572             {"": (1,), "passed": (1,)},
1573         ),
1574         # A couple more complex combinations
1575         (
1576             "red",
1577             [
1578                 ("1 failed", {"bold": True, "red": True}),
1579                 ("2 passed", {"bold": False, "green": True}),
1580                 ("3 xfailed", {"bold": False, "yellow": True}),
1581             ],
1582             {"passed": (1, 2), "failed": (1,), "xfailed": (1, 2, 3)},
1583         ),
1584         (
1585             "green",
1586             [
1587                 ("1 passed", {"bold": True, "green": True}),
1588                 ("2 skipped", {"bold": False, "yellow": True}),
1589                 ("3 deselected", {"bold": False, "yellow": True}),
1590                 ("2 xfailed", {"bold": False, "yellow": True}),
1591             ],
1592             {
1593                 "passed": (1,),
1594                 "skipped": (1, 2),
1595                 "deselected": (1, 2, 3),
1596                 "xfailed": (1, 2),
1597             },
1598         ),
1599     ],
1600 )
1601 def test_summary_stats(
1602     tr: TerminalReporter,
1603     exp_line: List[Tuple[str, Dict[str, bool]]],
1604     exp_color: str,
1605     stats_arg: Dict[str, List],
1606 ) -> None:
1607     tr.stats = stats_arg
1608 
1609     # Fake "_is_last_item" to be True.
1610     class fake_session:
1611         testscollected = 0
1612 
1613     tr._session = fake_session  # type: ignore[assignment]  # noqa: F821
1614     assert tr._is_last_item
1615 
1616     # Reset cache.
1617     tr._main_color = None
1618 
1619     print("Based on stats: %s" % stats_arg)
1620     print('Expect summary: "{}"; with color "{}"'.format(exp_line, exp_color))
1621     (line, color) = tr.build_summary_stats_line()
1622     print('Actually got:   "{}"; with color "{}"'.format(line, color))
1623     assert line == exp_line
1624     assert color == exp_color
1625 
1626 
1627 def test_skip_counting_towards_summary(tr):
1628     class DummyReport(BaseReport):
1629         count_towards_summary = True
1630 
1631     r1 = DummyReport()
1632     r2 = DummyReport()
1633     tr.stats = {"failed": (r1, r2)}
1634     tr._main_color = None
1635     res = tr.build_summary_stats_line()
1636     assert res == ([("2 failed", {"bold": True, "red": True})], "red")
1637 
1638     r1.count_towards_summary = False
1639     tr.stats = {"failed": (r1, r2)}
1640     tr._main_color = None
1641     res = tr.build_summary_stats_line()
1642     assert res == ([("1 failed", {"bold": True, "red": True})], "red")
1643 
1644 
1645 class TestClassicOutputStyle:
1646     """Ensure classic output style works as expected (#3883)"""
1647 
1648     @pytest.fixture
1649     def test_files(self, testdir):
1650         testdir.makepyfile(
1651             **{
1652                 "test_one.py": "def test_one(): pass",
1653                 "test_two.py": "def test_two(): assert 0",
1654                 "sub/test_three.py": """
1655                     def test_three_1(): pass
1656                     def test_three_2(): assert 0
1657                     def test_three_3(): pass
1658                 """,
1659             }
1660         )
1661 
1662     def test_normal_verbosity(self, testdir, test_files):
1663         result = testdir.runpytest("-o", "console_output_style=classic")
1664         result.stdout.fnmatch_lines(
1665             [
1666                 "test_one.py .",
1667                 "test_two.py F",
1668                 "sub{}test_three.py .F.".format(os.sep),
1669                 "*2 failed, 3 passed in*",
1670             ]
1671         )
1672 
1673     def test_verbose(self, testdir, test_files):
1674         result = testdir.runpytest("-o", "console_output_style=classic", "-v")
1675         result.stdout.fnmatch_lines(
1676             [
1677                 "test_one.py::test_one PASSED",
1678                 "test_two.py::test_two FAILED",
1679                 "sub{}test_three.py::test_three_1 PASSED".format(os.sep),
1680                 "sub{}test_three.py::test_three_2 FAILED".format(os.sep),
1681                 "sub{}test_three.py::test_three_3 PASSED".format(os.sep),
1682                 "*2 failed, 3 passed in*",
1683             ]
1684         )
1685 
1686     def test_quiet(self, testdir, test_files):
1687         result = testdir.runpytest("-o", "console_output_style=classic", "-q")
1688         result.stdout.fnmatch_lines([".F.F.", "*2 failed, 3 passed in*"])
1689 
1690 
1691 class TestProgressOutputStyle:
1692     @pytest.fixture
1693     def many_tests_files(self, testdir):
1694         testdir.makepyfile(
1695             test_bar="""
1696                 import pytest
1697                 @pytest.mark.parametrize('i', range(10))
1698                 def test_bar(i): pass
1699             """,
1700             test_foo="""
1701                 import pytest
1702                 @pytest.mark.parametrize('i', range(5))
1703                 def test_foo(i): pass
1704             """,
1705             test_foobar="""
1706                 import pytest
1707                 @pytest.mark.parametrize('i', range(5))
1708                 def test_foobar(i): pass
1709             """,
1710         )
1711 
1712     def test_zero_tests_collected(self, testdir):
1713         """Some plugins (testmon for example) might issue pytest_runtest_logreport without any tests being
1714         actually collected (#2971)."""
1715         testdir.makeconftest(
1716             """
1717         def pytest_collection_modifyitems(items, config):
1718             from _pytest.runner import CollectReport
1719             for node_id in ('nodeid1', 'nodeid2'):
1720                 rep = CollectReport(node_id, 'passed', None, None)
1721                 rep.when = 'passed'
1722                 rep.duration = 0.1
1723                 config.hook.pytest_runtest_logreport(report=rep)
1724         """
1725         )
1726         output = testdir.runpytest()
1727         output.stdout.no_fnmatch_line("*ZeroDivisionError*")
1728         output.stdout.fnmatch_lines(["=* 2 passed in *="])
1729 
1730     def test_normal(self, many_tests_files, testdir):
1731         output = testdir.runpytest()
1732         output.stdout.re_match_lines(
1733             [
1734                 r"test_bar.py \.{10} \s+ \[ 50%\]",
1735                 r"test_foo.py \.{5} \s+ \[ 75%\]",
1736                 r"test_foobar.py \.{5} \s+ \[100%\]",
1737             ]
1738         )
1739 
1740     def test_colored_progress(self, testdir, monkeypatch, color_mapping):
1741         monkeypatch.setenv("PY_COLORS", "1")
1742         testdir.makepyfile(
1743             test_axfail="""
1744                 import pytest
1745                 @pytest.mark.xfail
1746                 def test_axfail(): assert 0
1747             """,
1748             test_bar="""
1749                 import pytest
1750                 @pytest.mark.parametrize('i', range(10))
1751                 def test_bar(i): pass
1752             """,
1753             test_foo="""
1754                 import pytest
1755                 import warnings
1756                 @pytest.mark.parametrize('i', range(5))
1757                 def test_foo(i):
1758                     warnings.warn(DeprecationWarning("collection"))
1759                     pass
1760             """,
1761             test_foobar="""
1762                 import pytest
1763                 @pytest.mark.parametrize('i', range(5))
1764                 def test_foobar(i): raise ValueError()
1765             """,
1766         )
1767         result = testdir.runpytest()
1768         result.stdout.re_match_lines(
1769             color_mapping.format_for_rematch(
1770                 [
1771                     r"test_axfail.py {yellow}x{reset}{green} \s+ \[  4%\]{reset}",
1772                     r"test_bar.py ({green}\.{reset}){{10}}{green} \s+ \[ 52%\]{reset}",
1773                     r"test_foo.py ({green}\.{reset}){{5}}{yellow} \s+ \[ 76%\]{reset}",
1774                     r"test_foobar.py ({red}F{reset}){{5}}{red} \s+ \[100%\]{reset}",
1775                 ]
1776             )
1777         )
1778 
1779         # Only xfail should have yellow progress indicator.
1780         result = testdir.runpytest("test_axfail.py")
1781         result.stdout.re_match_lines(
1782             color_mapping.format_for_rematch(
1783                 [
1784                     r"test_axfail.py {yellow}x{reset}{yellow} \s+ \[100%\]{reset}",
1785                     r"^{yellow}=+ ({yellow}{bold}|{bold}{yellow})1 xfailed{reset}{yellow} in ",
1786                 ]
1787             )
1788         )
1789 
1790     def test_count(self, many_tests_files, testdir):
1791         testdir.makeini(
1792             """
1793             [pytest]
1794             console_output_style = count
1795         """
1796         )
1797         output = testdir.runpytest()
1798         output.stdout.re_match_lines(
1799             [
1800                 r"test_bar.py \.{10} \s+ \[10/20\]",
1801                 r"test_foo.py \.{5} \s+ \[15/20\]",
1802                 r"test_foobar.py \.{5} \s+ \[20/20\]",
1803             ]
1804         )
1805 
1806     def test_verbose(self, many_tests_files, testdir):
1807         output = testdir.runpytest("-v")
1808         output.stdout.re_match_lines(
1809             [
1810                 r"test_bar.py::test_bar\[0\] PASSED \s+ \[  5%\]",
1811                 r"test_foo.py::test_foo\[4\] PASSED \s+ \[ 75%\]",
1812                 r"test_foobar.py::test_foobar\[4\] PASSED \s+ \[100%\]",
1813             ]
1814         )
1815 
1816     def test_verbose_count(self, many_tests_files, testdir):
1817         testdir.makeini(
1818             """
1819             [pytest]
1820             console_output_style = count
1821         """
1822         )
1823         output = testdir.runpytest("-v")
1824         output.stdout.re_match_lines(
1825             [
1826                 r"test_bar.py::test_bar\[0\] PASSED \s+ \[ 1/20\]",
1827                 r"test_foo.py::test_foo\[4\] PASSED \s+ \[15/20\]",
1828                 r"test_foobar.py::test_foobar\[4\] PASSED \s+ \[20/20\]",
1829             ]
1830         )
1831 
1832     def test_xdist_normal(self, many_tests_files, testdir, monkeypatch):
1833         pytest.importorskip("xdist")
1834         monkeypatch.delenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", raising=False)
1835         output = testdir.runpytest("-n2")
1836         output.stdout.re_match_lines([r"\.{20} \s+ \[100%\]"])
1837 
1838     def test_xdist_normal_count(self, many_tests_files, testdir, monkeypatch):
1839         pytest.importorskip("xdist")
1840         monkeypatch.delenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", raising=False)
1841         testdir.makeini(
1842             """
1843             [pytest]
1844             console_output_style = count
1845         """
1846         )
1847         output = testdir.runpytest("-n2")
1848         output.stdout.re_match_lines([r"\.{20} \s+ \[20/20\]"])
1849 
1850     def test_xdist_verbose(self, many_tests_files, testdir, monkeypatch):
1851         pytest.importorskip("xdist")
1852         monkeypatch.delenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", raising=False)
1853         output = testdir.runpytest("-n2", "-v")
1854         output.stdout.re_match_lines_random(
1855             [
1856                 r"\[gw\d\] \[\s*\d+%\] PASSED test_bar.py::test_bar\[1\]",
1857                 r"\[gw\d\] \[\s*\d+%\] PASSED test_foo.py::test_foo\[1\]",
1858                 r"\[gw\d\] \[\s*\d+%\] PASSED test_foobar.py::test_foobar\[1\]",
1859             ]
1860         )
1861         output.stdout.fnmatch_lines_random(
1862             [
1863                 line.translate(TRANS_FNMATCH)
1864                 for line in [
1865                     "test_bar.py::test_bar[0] ",
1866                     "test_foo.py::test_foo[0] ",
1867                     "test_foobar.py::test_foobar[0] ",
1868                     "[gw?] [  5%] PASSED test_*[?] ",
1869                     "[gw?] [ 10%] PASSED test_*[?] ",
1870                     "[gw?] [ 55%] PASSED test_*[?] ",
1871                     "[gw?] [ 60%] PASSED test_*[?] ",
1872                     "[gw?] [ 95%] PASSED test_*[?] ",
1873                     "[gw?] [100%] PASSED test_*[?] ",
1874                 ]
1875             ]
1876         )
1877 
1878     def test_capture_no(self, many_tests_files, testdir):
1879         output = testdir.runpytest("-s")
1880         output.stdout.re_match_lines(
1881             [r"test_bar.py \.{10}", r"test_foo.py \.{5}", r"test_foobar.py \.{5}"]
1882         )
1883 
1884         output = testdir.runpytest("--capture=no")
1885         output.stdout.no_fnmatch_line("*%]*")
1886 
1887 
1888 class TestProgressWithTeardown:
1889     """Ensure we show the correct percentages for tests that fail during teardown (#3088)"""
1890 
1891     @pytest.fixture
1892     def contest_with_teardown_fixture(self, testdir):
1893         testdir.makeconftest(
1894             """
1895             import pytest
1896 
1897             @pytest.fixture
1898             def fail_teardown():
1899                 yield
1900                 assert False
1901         """
1902         )
1903 
1904     @pytest.fixture
1905     def many_files(self, testdir, contest_with_teardown_fixture):
1906         testdir.makepyfile(
1907             test_bar="""
1908                 import pytest
1909                 @pytest.mark.parametrize('i', range(5))
1910                 def test_bar(fail_teardown, i):
1911                     pass
1912             """,
1913             test_foo="""
1914                 import pytest
1915                 @pytest.mark.parametrize('i', range(15))
1916                 def test_foo(fail_teardown, i):
1917                     pass
1918             """,
1919         )
1920 
1921     def test_teardown_simple(self, testdir, contest_with_teardown_fixture):
1922         testdir.makepyfile(
1923             """
1924             def test_foo(fail_teardown):
1925                 pass
1926         """
1927         )
1928         output = testdir.runpytest()
1929         output.stdout.re_match_lines([r"test_teardown_simple.py \.E\s+\[100%\]"])
1930 
1931     def test_teardown_with_test_also_failing(
1932         self, testdir, contest_with_teardown_fixture
1933     ):
1934         testdir.makepyfile(
1935             """
1936             def test_foo(fail_teardown):
1937                 assert 0
1938         """
1939         )
1940         output = testdir.runpytest("-rfE")
1941         output.stdout.re_match_lines(
1942             [
1943                 r"test_teardown_with_test_also_failing.py FE\s+\[100%\]",
1944                 "FAILED test_teardown_with_test_also_failing.py::test_foo - assert 0",
1945                 "ERROR test_teardown_with_test_also_failing.py::test_foo - assert False",
1946             ]
1947         )
1948 
1949     def test_teardown_many(self, testdir, many_files):
1950         output = testdir.runpytest()
1951         output.stdout.re_match_lines(
1952             [r"test_bar.py (\.E){5}\s+\[ 25%\]", r"test_foo.py (\.E){15}\s+\[100%\]"]
1953         )
1954 
1955     def test_teardown_many_verbose(
1956         self, testdir: Testdir, many_files, color_mapping
1957     ) -> None:
1958         result = testdir.runpytest("-v")
1959         result.stdout.fnmatch_lines(
1960             color_mapping.format_for_fnmatch(
1961                 [
1962                     "test_bar.py::test_bar[0] PASSED  * [  5%]",
1963                     "test_bar.py::test_bar[0] ERROR   * [  5%]",
1964                     "test_bar.py::test_bar[4] PASSED  * [ 25%]",
1965                     "test_foo.py::test_foo[14] PASSED * [100%]",
1966                     "test_foo.py::test_foo[14] ERROR  * [100%]",
1967                     "=* 20 passed, 20 errors in *",
1968                 ]
1969             )
1970         )
1971 
1972     def test_xdist_normal(self, many_files, testdir, monkeypatch):
1973         pytest.importorskip("xdist")
1974         monkeypatch.delenv("PYTEST_DISABLE_PLUGIN_AUTOLOAD", raising=False)
1975         output = testdir.runpytest("-n2")
1976         output.stdout.re_match_lines([r"[\.E]{40} \s+ \[100%\]"])
1977 
1978 
1979 def test_skip_reasons_folding():
1980     path = "xyz"
1981     lineno = 3
1982     message = "justso"
1983     longrepr = (path, lineno, message)
1984 
1985     class X:
1986         pass
1987 
1988     ev1 = X()
1989     ev1.when = "execute"
1990     ev1.skipped = True
1991     ev1.longrepr = longrepr
1992 
1993     ev2 = X()
1994     ev2.when = "execute"
1995     ev2.longrepr = longrepr
1996     ev2.skipped = True
1997 
1998     # ev3 might be a collection report
1999     ev3 = X()
2000     ev3.when = "collect"
2001     ev3.longrepr = longrepr
2002     ev3.skipped = True
2003 
2004     values = _folded_skips(py.path.local(), [ev1, ev2, ev3])
2005     assert len(values) == 1
2006     num, fspath, lineno, reason = values[0]
2007     assert num == 3
2008     assert fspath == path
2009     assert lineno == lineno
2010     assert reason == message
2011 
2012 
2013 def test_line_with_reprcrash(monkeypatch):
2014     import _pytest.terminal
2015     from wcwidth import wcswidth
2016 
2017     mocked_verbose_word = "FAILED"
2018 
2019     mocked_pos = "some::nodeid"
2020 
2021     def mock_get_pos(*args):
2022         return mocked_pos
2023 
2024     monkeypatch.setattr(_pytest.terminal, "_get_pos", mock_get_pos)
2025 
2026     class config:
2027         pass
2028 
2029     class rep:
2030         def _get_verbose_word(self, *args):
2031             return mocked_verbose_word
2032 
2033         class longrepr:
2034             class reprcrash:
2035                 pass
2036 
2037     def check(msg, width, expected):
2038         __tracebackhide__ = True
2039         if msg:
2040             rep.longrepr.reprcrash.message = msg
2041         actual = _get_line_with_reprcrash_message(config, rep(), width)
2042 
2043         assert actual == expected
2044         if actual != "{} {}".format(mocked_verbose_word, mocked_pos):
2045             assert len(actual) <= width
2046             assert wcswidth(actual) <= width
2047 
2048     # AttributeError with message
2049     check(None, 80, "FAILED some::nodeid")
2050 
2051     check("msg", 80, "FAILED some::nodeid - msg")
2052     check("msg", 3, "FAILED some::nodeid")
2053 
2054     check("msg", 24, "FAILED some::nodeid")
2055     check("msg", 25, "FAILED some::nodeid - msg")
2056 
2057     check("some longer msg", 24, "FAILED some::nodeid")
2058     check("some longer msg", 25, "FAILED some::nodeid - ...")
2059     check("some longer msg", 26, "FAILED some::nodeid - s...")
2060 
2061     check("some\nmessage", 25, "FAILED some::nodeid - ...")
2062     check("some\nmessage", 26, "FAILED some::nodeid - some")
2063     check("some\nmessage", 80, "FAILED some::nodeid - some")
2064 
2065     # Test unicode safety.
2066     check("😄😄😄😄😄\n2nd line", 25, "FAILED some::nodeid - ...")
2067     check("😄😄😄😄😄\n2nd line", 26, "FAILED some::nodeid - ...")
2068     check("😄😄😄😄😄\n2nd line", 27, "FAILED some::nodeid - 😄...")
2069     check("😄😄😄😄😄\n2nd line", 28, "FAILED some::nodeid - 😄...")
2070     check("😄😄😄😄😄\n2nd line", 29, "FAILED some::nodeid - 😄😄...")
2071 
2072     # NOTE: constructed, not sure if this is supported.
2073     mocked_pos = "nodeid::😄::withunicode"
2074     check("😄😄😄😄😄\n2nd line", 29, "FAILED nodeid::😄::withunicode")
2075     check("😄😄😄😄😄\n2nd line", 40, "FAILED nodeid::😄::withunicode - 😄😄...")
2076     check("😄😄😄😄😄\n2nd line", 41, "FAILED nodeid::😄::withunicode - 😄😄...")
2077     check("😄😄😄😄😄\n2nd line", 42, "FAILED nodeid::😄::withunicode - 😄😄😄...")
2078     check("😄😄😄😄😄\n2nd line", 80, "FAILED nodeid::😄::withunicode - 😄😄😄😄😄")
2079 
2080 
2081 @pytest.mark.parametrize(
2082     "seconds, expected",
2083     [
2084         (10.0, "10.00s"),
2085         (10.34, "10.34s"),
2086         (59.99, "59.99s"),
2087         (60.55, "60.55s (0:01:00)"),
2088         (123.55, "123.55s (0:02:03)"),
2089         (60 * 60 + 0.5, "3600.50s (1:00:00)"),
2090     ],
2091 )
2092 def test_format_session_duration(seconds, expected):
2093     from _pytest.terminal import format_session_duration
2094 
2095     assert format_session_duration(seconds) == expected
2096 
2097 
2098 def test_collecterror(testdir):
2099     p1 = testdir.makepyfile("raise SyntaxError()")
2100     result = testdir.runpytest("-ra", str(p1))
2101     result.stdout.fnmatch_lines(
2102         [
2103             "collected 0 items / 1 error",
2104             "*= ERRORS =*",
2105             "*_ ERROR collecting test_collecterror.py _*",
2106             "E   SyntaxError: *",
2107             "*= short test summary info =*",
2108             "ERROR test_collecterror.py",
2109             "*! Interrupted: 1 error during collection !*",
2110             "*= 1 error in *",
2111         ]
2112     )
2113 
2114 
2115 def test_via_exec(testdir: Testdir) -> None:
2116     p1 = testdir.makepyfile("exec('def test_via_exec(): pass')")
2117     result = testdir.runpytest(str(p1), "-vv")
2118     result.stdout.fnmatch_lines(
2119         ["test_via_exec.py::test_via_exec <- <string> PASSED*", "*= 1 passed in *"]
2120     )
2121 
2122 
2123 class TestCodeHighlight:
2124     def test_code_highlight_simple(self, testdir: Testdir, color_mapping) -> None:
2125         testdir.makepyfile(
2126             """
2127             def test_foo():
2128                 assert 1 == 10
2129         """
2130         )
2131         result = testdir.runpytest("--color=yes")
2132         color_mapping.requires_ordered_markup(result)
2133         result.stdout.fnmatch_lines(
2134             color_mapping.format_for_fnmatch(
2135                 [
2136                     "    {kw}def{hl-reset} {function}test_foo{hl-reset}():",
2137                     ">       {kw}assert{hl-reset} {number}1{hl-reset} == {number}10{hl-reset}",
2138                     "{bold}{red}E       assert 1 == 10{reset}",
2139                 ]
2140             )
2141         )
2142 
2143     def test_code_highlight_continuation(self, testdir: Testdir, color_mapping) -> None:
2144         testdir.makepyfile(
2145             """
2146             def test_foo():
2147                 print('''
2148                 '''); assert 0
2149         """
2150         )
2151         result = testdir.runpytest("--color=yes")
2152         color_mapping.requires_ordered_markup(result)
2153 
2154         result.stdout.fnmatch_lines(
2155             color_mapping.format_for_fnmatch(
2156                 [
2157                     "    {kw}def{hl-reset} {function}test_foo{hl-reset}():",
2158                     "        {print}print{hl-reset}({str}'''{hl-reset}{str}{hl-reset}",
2159                     ">   {str}    {hl-reset}{str}'''{hl-reset}); {kw}assert{hl-reset} {number}0{hl-reset}",
2160                     "{bold}{red}E       assert 0{reset}",
2161                 ]
2162             )
2163         )

</code><intended_behavior>The function or method responsible for outputting the skipped test report to the terminal should ensure the path displayed is relative to the directory from which `pytest` was invoked. This may require integrating with the file path calculation logic to adjust how the final path is resolved before being displayed. The goal is to make sure that regardless of where the test file resides in relation to the invocation directory, the displayed path accurately reflects its location in a manner consistent with the user's command-line input.</intended_behavior>]
2025-10-17 16:53:57.340 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-17 16:53:57.340 | INFO     | Search-only: skipping patch selection
2025-10-17 16:53:57.425 | INFO     | Task pytest-dev__pytest-7158 completed successfully.
2025-10-17 16:53:57.426 | INFO     | No patch generated. You can try running ACR again.
