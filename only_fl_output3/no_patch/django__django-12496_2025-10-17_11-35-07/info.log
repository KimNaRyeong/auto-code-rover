2025-10-17 11:35:07.993 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-17 11:35:11.853 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=461, output_tokens=12, cost=0.004970
2025-10-17 11:35:25.043 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=641, output_tokens=618, cost=0.024950
2025-10-17 11:35:26.225 | INFO     | registered non reproducing test 0
2025-10-17 11:35:42.309 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2032, output_tokens=725, cost=0.042070
2025-10-17 11:35:43.502 | INFO     | registered non reproducing test 1
2025-10-17 11:35:59.908 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2911, output_tokens=782, cost=0.052570
2025-10-17 11:36:01.043 | INFO     | registered non reproducing test 2
2025-10-17 11:36:01.043 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-17 11:36:01.046 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 11:36:07.907 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=764, output_tokens=324, cost=0.017360
2025-10-17 11:36:07.914 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 11:36:11.231 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=678, output_tokens=77, cost=0.009090
2025-10-17 11:36:11.232 | DEBUG    | Extracted a valid json.
2025-10-17 11:36:11.232 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"Parent\")",
        "search_class(\"Child\")",
        "search_method_in_class(\"save\", \"Parent\")",
        "search_method_in_class(\"save\", \"Child\")",
        "search_method(\"create\")",
        "search_code(\"Child.objects.create()\")"
    ],
    "bug_locations": []
}
2025-10-17 11:36:11.276 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-17 11:36:11.277 | DEBUG    | <Agent search> Analyzing search results.
2025-10-17 11:36:24.977 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1523, output_tokens=630, cost=0.034130
2025-10-17 11:36:24.992 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 11:36:41.685 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2369, output_tokens=478, cost=0.038030
2025-10-17 11:36:41.690 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 11:36:48.489 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=832, output_tokens=283, cost=0.016810
2025-10-17 11:36:48.490 | DEBUG    | Extracted a valid json.
2025-10-17 11:36:48.495 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_method_in_file(\"create\", \"django/db/models/query.py\")",
        "search_method_in_file(\"create\", \"django/db/models/fields/related_descriptors.py\")"
    ],
    "bug_locations":[
        {
            "file": "django/db/models/query.py", 
            "class": "QuerySet", 
            "method": "create", 
            "intended_behavior": "The `create` method should efficiently handle the inheritance model creation process, especially when the parent model defines a custom primary key like UUID. Ideally, the creation process should involve a single insert operation for both parent and child models without triggering a redundant update operation for filling parent fields. This would require an intelligent inspection of the modelâ€™s primary key during the creation to ensure all necessary fields are populated in the initial INSERT operation."
        },
        {
            "file": "django/db/models/fields/related_descriptors.py", 
            "class": "", 
            "method": "", 
            "intended_behavior": "Ensure that during the creation of a child model object, the handling of the parent's primary key is done in a manner that doesn't require subsequent updates to populate or correct the parent model fields. This might involve pre-calculating or ensuring the UUID or custom primary keys are adequately considered in the initial INSERT logic."
        }
    ]
}
2025-10-17 11:36:48.526 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-17 11:36:48.527 | DEBUG    | <Agent search> Analyzing search results.
2025-10-17 11:37:02.815 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3688, output_tokens=664, cost=0.056800
2025-10-17 11:37:02.828 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 11:37:15.862 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=4568, output_tokens=518, cost=0.061220
2025-10-17 11:37:15.870 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 11:37:21.278 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=872, output_tokens=172, cost=0.013880
2025-10-17 11:37:21.279 | DEBUG    | Extracted a valid json.
2025-10-17 11:37:21.279 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "django/db/models/query.py",
            "class": "QuerySet",
            "method": "create",
            "intended_behavior": "The `create` method should include improved handling for objects that are part of a multi-table inheritance chain, where one or more parents have a custom primary key (e.g., a UUIDField). Specifically, when creating an instance of a child model, Django should optimally manage the insert process to prevent unnecessary updates to the parent model's record after its initial insertion. This could involve a pre-check to assess the inheritance chain of the model being created and to prepare all related data for a streamlined insert operation that doesn't necessitate follow-up updates to correct or complete the record linkage in the database."
        }
    ]
}
2025-10-17 11:37:21.289 | DEBUG    | Bug location extracted successfully: [<file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>create</method>
<code>
436     def create(self, **kwargs):
437         """
438         Create a new object with the given kwargs, saving it to the database
439         and returning the created object.
440         """
441         obj = self.model(**kwargs)
442         self._for_write = True
443         obj.save(force_insert=True, using=self.db)
444         return obj

</code><intended_behavior>The `create` method should include improved handling for objects that are part of a multi-table inheritance chain, where one or more parents have a custom primary key (e.g., a UUIDField). Specifically, when creating an instance of a child model, Django should optimally manage the insert process to prevent unnecessary updates to the parent model's record after its initial insertion. This could involve a pre-check to assess the inheritance chain of the model being created and to prepare all related data for a streamlined insert operation that doesn't necessitate follow-up updates to correct or complete the record linkage in the database.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
184 class QuerySet:
185     """Represent a lazy database lookup for a set of objects."""
186 
187     def __init__(self, model=None, query=None, using=None, hints=None):
188         self.model = model
189         self._db = using
190         self._hints = hints or {}
191         self._query = query or sql.Query(self.model)
192         self._result_cache = None
193         self._sticky_filter = False
194         self._for_write = False
195         self._prefetch_related_lookups = ()
196         self._prefetch_done = False
197         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
198         self._iterable_class = ModelIterable
199         self._fields = None
200         self._defer_next_filter = False
201         self._deferred_filter = None
202 
203     @property
204     def query(self):
205         if self._deferred_filter:
206             negate, args, kwargs = self._deferred_filter
207             self._filter_or_exclude_inplace(negate, *args, **kwargs)
208             self._deferred_filter = None
209         return self._query
210 
211     @query.setter
212     def query(self, value):
213         self._query = value
214 
215     def as_manager(cls):
216         # Address the circular dependency between `Queryset` and `Manager`.
217         from django.db.models.manager import Manager
218         manager = Manager.from_queryset(cls)()
219         manager._built_with_as_manager = True
220         return manager
221     as_manager.queryset_only = True
222     as_manager = classmethod(as_manager)
223 
224     ########################
225     # PYTHON MAGIC METHODS #
226     ########################
227 
228     def __deepcopy__(self, memo):
229         """Don't populate the QuerySet's cache."""
230         obj = self.__class__()
231         for k, v in self.__dict__.items():
232             if k == '_result_cache':
233                 obj.__dict__[k] = None
234             else:
235                 obj.__dict__[k] = copy.deepcopy(v, memo)
236         return obj
237 
238     def __getstate__(self):
239         # Force the cache to be fully populated.
240         self._fetch_all()
241         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}
242 
243     def __setstate__(self, state):
244         msg = None
245         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
246         if pickled_version:
247             current_version = get_version()
248             if current_version != pickled_version:
249                 msg = (
250                     "Pickled queryset instance's Django version %s does not "
251                     "match the current version %s." % (pickled_version, current_version)
252                 )
253         else:
254             msg = "Pickled queryset instance's Django version is not specified."
255 
256         if msg:
257             warnings.warn(msg, RuntimeWarning, stacklevel=2)
258 
259         self.__dict__.update(state)
260 
261     def __repr__(self):
262         data = list(self[:REPR_OUTPUT_SIZE + 1])
263         if len(data) > REPR_OUTPUT_SIZE:
264             data[-1] = "...(remaining elements truncated)..."
265         return '<%s %r>' % (self.__class__.__name__, data)
266 
267     def __len__(self):
268         self._fetch_all()
269         return len(self._result_cache)
270 
271     def __iter__(self):
272         """
273         The queryset iterator protocol uses three nested iterators in the
274         default case:
275             1. sql.compiler.execute_sql()
276                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
277                  using cursor.fetchmany(). This part is responsible for
278                  doing some column masking, and returning the rows in chunks.
279             2. sql.compiler.results_iter()
280                - Returns one row at time. At this point the rows are still just
281                  tuples. In some cases the return values are converted to
282                  Python values at this location.
283             3. self.iterator()
284                - Responsible for turning the rows into model objects.
285         """
286         self._fetch_all()
287         return iter(self._result_cache)
288 
289     def __bool__(self):
290         self._fetch_all()
291         return bool(self._result_cache)
292 
293     def __getitem__(self, k):
294         """Retrieve an item or slice from the set of results."""
295         if not isinstance(k, (int, slice)):
296             raise TypeError(
297                 'QuerySet indices must be integers or slices, not %s.'
298                 % type(k).__name__
299             )
300         assert ((not isinstance(k, slice) and (k >= 0)) or
301                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
302                  (k.stop is None or k.stop >= 0))), \
303             "Negative indexing is not supported."
304 
305         if self._result_cache is not None:
306             return self._result_cache[k]
307 
308         if isinstance(k, slice):
309             qs = self._chain()
310             if k.start is not None:
311                 start = int(k.start)
312             else:
313                 start = None
314             if k.stop is not None:
315                 stop = int(k.stop)
316             else:
317                 stop = None
318             qs.query.set_limits(start, stop)
319             return list(qs)[::k.step] if k.step else qs
320 
321         qs = self._chain()
322         qs.query.set_limits(k, k + 1)
323         qs._fetch_all()
324         return qs._result_cache[0]
325 
326     def __and__(self, other):
327         self._merge_sanity_check(other)
328         if isinstance(other, EmptyQuerySet):
329             return other
330         if isinstance(self, EmptyQuerySet):
331             return self
332         combined = self._chain()
333         combined._merge_known_related_objects(other)
334         combined.query.combine(other.query, sql.AND)
335         return combined
336 
337     def __or__(self, other):
338         self._merge_sanity_check(other)
339         if isinstance(self, EmptyQuerySet):
340             return other
341         if isinstance(other, EmptyQuerySet):
342             return self
343         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
344         combined = query._chain()
345         combined._merge_known_related_objects(other)
346         if not other.query.can_filter():
347             other = other.model._base_manager.filter(pk__in=other.values('pk'))
348         combined.query.combine(other.query, sql.OR)
349         return combined
350 
351     ####################################
352     # METHODS THAT DO DATABASE QUERIES #
353     ####################################
354 
355     def _iterator(self, use_chunked_fetch, chunk_size):
356         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
357 
358     def iterator(self, chunk_size=2000):
359         """
360         An iterator over the results from applying this QuerySet to the
361         database.
362         """
363         if chunk_size <= 0:
364             raise ValueError('Chunk size must be strictly positive.')
365         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
366         return self._iterator(use_chunked_fetch, chunk_size)
367 
368     def aggregate(self, *args, **kwargs):
369         """
370         Return a dictionary containing the calculations (aggregation)
371         over the current queryset.
372 
373         If args is present the expression is passed as a kwarg using
374         the Aggregate object's default alias.
375         """
376         if self.query.distinct_fields:
377             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
378         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
379         for arg in args:
380             # The default_alias property raises TypeError if default_alias
381             # can't be set automatically or AttributeError if it isn't an
382             # attribute.
383             try:
384                 arg.default_alias
385             except (AttributeError, TypeError):
386                 raise TypeError("Complex aggregates require an alias")
387             kwargs[arg.default_alias] = arg
388 
389         query = self.query.chain()
390         for (alias, aggregate_expr) in kwargs.items():
391             query.add_annotation(aggregate_expr, alias, is_summary=True)
392             if not query.annotations[alias].contains_aggregate:
393                 raise TypeError("%s is not an aggregate expression" % alias)
394         return query.get_aggregation(self.db, kwargs)
395 
396     def count(self):
397         """
398         Perform a SELECT COUNT() and return the number of records as an
399         integer.
400 
401         If the QuerySet is already fully cached, return the length of the
402         cached results set to avoid multiple SELECT COUNT(*) calls.
403         """
404         if self._result_cache is not None:
405             return len(self._result_cache)
406 
407         return self.query.get_count(using=self.db)
408 
409     def get(self, *args, **kwargs):
410         """
411         Perform the query and return a single object matching the given
412         keyword arguments.
413         """
414         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
415         if self.query.can_filter() and not self.query.distinct_fields:
416             clone = clone.order_by()
417         limit = None
418         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
419             limit = MAX_GET_RESULTS
420             clone.query.set_limits(high=limit)
421         num = len(clone)
422         if num == 1:
423             return clone._result_cache[0]
424         if not num:
425             raise self.model.DoesNotExist(
426                 "%s matching query does not exist." %
427                 self.model._meta.object_name
428             )
429         raise self.model.MultipleObjectsReturned(
430             'get() returned more than one %s -- it returned %s!' % (
431                 self.model._meta.object_name,
432                 num if not limit or num < limit else 'more than %s' % (limit - 1),
433             )
434         )
435 
436     def create(self, **kwargs):
437         """
438         Create a new object with the given kwargs, saving it to the database
439         and returning the created object.
440         """
441         obj = self.model(**kwargs)
442         self._for_write = True
443         obj.save(force_insert=True, using=self.db)
444         return obj
445 
446     def _populate_pk_values(self, objs):
447         for obj in objs:
448             if obj.pk is None:
449                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
450 
451     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
452         """
453         Insert each of the instances into the database. Do *not* call
454         save() on each of the instances, do not send any pre/post_save
455         signals, and do not set the primary key attribute if it is an
456         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
457         Multi-table models are not supported.
458         """
459         # When you bulk insert you don't get the primary keys back (if it's an
460         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
461         # you can't insert into the child tables which references this. There
462         # are two workarounds:
463         # 1) This could be implemented if you didn't have an autoincrement pk
464         # 2) You could do it by doing O(n) normal inserts into the parent
465         #    tables to get the primary keys back and then doing a single bulk
466         #    insert into the childmost table.
467         # We currently set the primary keys on the objects when using
468         # PostgreSQL via the RETURNING ID clause. It should be possible for
469         # Oracle as well, but the semantics for extracting the primary keys is
470         # trickier so it's not done yet.
471         assert batch_size is None or batch_size > 0
472         # Check that the parents share the same concrete model with the our
473         # model to detect the inheritance pattern ConcreteGrandParent ->
474         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
475         # would not identify that case as involving multiple tables.
476         for parent in self.model._meta.get_parent_list():
477             if parent._meta.concrete_model is not self.model._meta.concrete_model:
478                 raise ValueError("Can't bulk create a multi-table inherited model")
479         if not objs:
480             return objs
481         self._for_write = True
482         connection = connections[self.db]
483         opts = self.model._meta
484         fields = opts.concrete_fields
485         objs = list(objs)
486         self._populate_pk_values(objs)
487         with transaction.atomic(using=self.db, savepoint=False):
488             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
489             if objs_with_pk:
490                 returned_columns = self._batched_insert(
491                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
492                 )
493                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
494                     for result, field in zip(results, opts.db_returning_fields):
495                         if field != opts.pk:
496                             setattr(obj_with_pk, field.attname, result)
497                 for obj_with_pk in objs_with_pk:
498                     obj_with_pk._state.adding = False
499                     obj_with_pk._state.db = self.db
500             if objs_without_pk:
501                 fields = [f for f in fields if not isinstance(f, AutoField)]
502                 returned_columns = self._batched_insert(
503                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
504                 )
505                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
506                     assert len(returned_columns) == len(objs_without_pk)
507                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
508                     for result, field in zip(results, opts.db_returning_fields):
509                         setattr(obj_without_pk, field.attname, result)
510                     obj_without_pk._state.adding = False
511                     obj_without_pk._state.db = self.db
512 
513         return objs
514 
515     def bulk_update(self, objs, fields, batch_size=None):
516         """
517         Update the given fields in each of the given objects in the database.
518         """
519         if batch_size is not None and batch_size < 0:
520             raise ValueError('Batch size must be a positive integer.')
521         if not fields:
522             raise ValueError('Field names must be given to bulk_update().')
523         objs = tuple(objs)
524         if any(obj.pk is None for obj in objs):
525             raise ValueError('All bulk_update() objects must have a primary key set.')
526         fields = [self.model._meta.get_field(name) for name in fields]
527         if any(not f.concrete or f.many_to_many for f in fields):
528             raise ValueError('bulk_update() can only be used with concrete fields.')
529         if any(f.primary_key for f in fields):
530             raise ValueError('bulk_update() cannot be used with primary key fields.')
531         if not objs:
532             return
533         # PK is used twice in the resulting update query, once in the filter
534         # and once in the WHEN. Each field will also have one CAST.
535         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
536         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
537         requires_casting = connections[self.db].features.requires_casted_case_in_updates
538         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
539         updates = []
540         for batch_objs in batches:
541             update_kwargs = {}
542             for field in fields:
543                 when_statements = []
544                 for obj in batch_objs:
545                     attr = getattr(obj, field.attname)
546                     if not isinstance(attr, Expression):
547                         attr = Value(attr, output_field=field)
548                     when_statements.append(When(pk=obj.pk, then=attr))
549                 case_statement = Case(*when_statements, output_field=field)
550                 if requires_casting:
551                     case_statement = Cast(case_statement, output_field=field)
552                 update_kwargs[field.attname] = case_statement
553             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
554         with transaction.atomic(using=self.db, savepoint=False):
555             for pks, update_kwargs in updates:
556                 self.filter(pk__in=pks).update(**update_kwargs)
557     bulk_update.alters_data = True
558 
559     def get_or_create(self, defaults=None, **kwargs):
560         """
561         Look up an object with the given kwargs, creating one if necessary.
562         Return a tuple of (object, created), where created is a boolean
563         specifying whether an object was created.
564         """
565         # The get() needs to be targeted at the write database in order
566         # to avoid potential transaction consistency problems.
567         self._for_write = True
568         try:
569             return self.get(**kwargs), False
570         except self.model.DoesNotExist:
571             params = self._extract_model_params(defaults, **kwargs)
572             return self._create_object_from_params(kwargs, params)
573 
574     def update_or_create(self, defaults=None, **kwargs):
575         """
576         Look up an object with the given kwargs, updating one with defaults
577         if it exists, otherwise create a new one.
578         Return a tuple (object, created), where created is a boolean
579         specifying whether an object was created.
580         """
581         defaults = defaults or {}
582         self._for_write = True
583         with transaction.atomic(using=self.db):
584             try:
585                 obj = self.select_for_update().get(**kwargs)
586             except self.model.DoesNotExist:
587                 params = self._extract_model_params(defaults, **kwargs)
588                 # Lock the row so that a concurrent update is blocked until
589                 # after update_or_create() has performed its save.
590                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
591                 if created:
592                     return obj, created
593             for k, v in resolve_callables(defaults):
594                 setattr(obj, k, v)
595             obj.save(using=self.db)
596         return obj, False
597 
598     def _create_object_from_params(self, lookup, params, lock=False):
599         """
600         Try to create an object using passed params. Used by get_or_create()
601         and update_or_create().
602         """
603         try:
604             with transaction.atomic(using=self.db):
605                 params = dict(resolve_callables(params))
606                 obj = self.create(**params)
607             return obj, True
608         except IntegrityError:
609             try:
610                 qs = self.select_for_update() if lock else self
611                 return qs.get(**lookup), False
612             except self.model.DoesNotExist:
613                 pass
614             raise
615 
616     def _extract_model_params(self, defaults, **kwargs):
617         """
618         Prepare `params` for creating a model instance based on the given
619         kwargs; for use by get_or_create() and update_or_create().
620         """
621         defaults = defaults or {}
622         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
623         params.update(defaults)
624         property_names = self.model._meta._property_names
625         invalid_params = []
626         for param in params:
627             try:
628                 self.model._meta.get_field(param)
629             except exceptions.FieldDoesNotExist:
630                 # It's okay to use a model's property if it has a setter.
631                 if not (param in property_names and getattr(self.model, param).fset):
632                     invalid_params.append(param)
633         if invalid_params:
634             raise exceptions.FieldError(
635                 "Invalid field name(s) for model %s: '%s'." % (
636                     self.model._meta.object_name,
637                     "', '".join(sorted(invalid_params)),
638                 ))
639         return params
640 
641     def _earliest(self, *fields):
642         """
643         Return the earliest object according to fields (if given) or by the
644         model's Meta.get_latest_by.
645         """
646         if fields:
647             order_by = fields
648         else:
649             order_by = getattr(self.model._meta, 'get_latest_by')
650             if order_by and not isinstance(order_by, (tuple, list)):
651                 order_by = (order_by,)
652         if order_by is None:
653             raise ValueError(
654                 "earliest() and latest() require either fields as positional "
655                 "arguments or 'get_latest_by' in the model's Meta."
656             )
657 
658         assert not self.query.is_sliced, \
659             "Cannot change a query once a slice has been taken."
660         obj = self._chain()
661         obj.query.set_limits(high=1)
662         obj.query.clear_ordering(force_empty=True)
663         obj.query.add_ordering(*order_by)
664         return obj.get()
665 
666     def earliest(self, *fields):
667         return self._earliest(*fields)
668 
669     def latest(self, *fields):
670         return self.reverse()._earliest(*fields)
671 
672     def first(self):
673         """Return the first object of a query or None if no match is found."""
674         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
675             return obj
676 
677     def last(self):
678         """Return the last object of a query or None if no match is found."""
679         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
680             return obj
681 
682     def in_bulk(self, id_list=None, *, field_name='pk'):
683         """
684         Return a dictionary mapping each of the given IDs to the object with
685         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
686         """
687         assert not self.query.is_sliced, \
688             "Cannot use 'limit' or 'offset' with in_bulk"
689         if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:
690             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
691         if id_list is not None:
692             if not id_list:
693                 return {}
694             filter_key = '{}__in'.format(field_name)
695             batch_size = connections[self.db].features.max_query_params
696             id_list = tuple(id_list)
697             # If the database has a limit on the number of query parameters
698             # (e.g. SQLite), retrieve objects in batches if necessary.
699             if batch_size and batch_size < len(id_list):
700                 qs = ()
701                 for offset in range(0, len(id_list), batch_size):
702                     batch = id_list[offset:offset + batch_size]
703                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
704             else:
705                 qs = self.filter(**{filter_key: id_list}).order_by()
706         else:
707             qs = self._chain()
708         return {getattr(obj, field_name): obj for obj in qs}
709 
710     def delete(self):
711         """Delete the records in the current QuerySet."""
712         self._not_support_combined_queries('delete')
713         assert not self.query.is_sliced, \
714             "Cannot use 'limit' or 'offset' with delete."
715 
716         if self._fields is not None:
717             raise TypeError("Cannot call delete() after .values() or .values_list()")
718 
719         del_query = self._chain()
720 
721         # The delete is actually 2 queries - one to find related objects,
722         # and one to delete. Make sure that the discovery of related
723         # objects is performed on the same database as the deletion.
724         del_query._for_write = True
725 
726         # Disable non-supported fields.
727         del_query.query.select_for_update = False
728         del_query.query.select_related = False
729         del_query.query.clear_ordering(force_empty=True)
730 
731         collector = Collector(using=del_query.db)
732         collector.collect(del_query)
733         deleted, _rows_count = collector.delete()
734 
735         # Clear the result cache, in case this QuerySet gets reused.
736         self._result_cache = None
737         return deleted, _rows_count
738 
739     delete.alters_data = True
740     delete.queryset_only = True
741 
742     def _raw_delete(self, using):
743         """
744         Delete objects found from the given queryset in single direct SQL
745         query. No signals are sent and there is no protection for cascades.
746         """
747         query = self.query.clone()
748         query.__class__ = sql.DeleteQuery
749         cursor = query.get_compiler(using).execute_sql(CURSOR)
750         if cursor:
751             with cursor:
752                 return cursor.rowcount
753         return 0
754     _raw_delete.alters_data = True
755 
756     def update(self, **kwargs):
757         """
758         Update all elements in the current QuerySet, setting all the given
759         fields to the appropriate values.
760         """
761         self._not_support_combined_queries('update')
762         assert not self.query.is_sliced, \
763             "Cannot update a query once a slice has been taken."
764         self._for_write = True
765         query = self.query.chain(sql.UpdateQuery)
766         query.add_update_values(kwargs)
767         # Clear any annotations so that they won't be present in subqueries.
768         query.annotations = {}
769         with transaction.mark_for_rollback_on_error(using=self.db):
770             rows = query.get_compiler(self.db).execute_sql(CURSOR)
771         self._result_cache = None
772         return rows
773     update.alters_data = True
774 
775     def _update(self, values):
776         """
777         A version of update() that accepts field objects instead of field names.
778         Used primarily for model saving and not intended for use by general
779         code (it requires too much poking around at model internals to be
780         useful at that level).
781         """
782         assert not self.query.is_sliced, \
783             "Cannot update a query once a slice has been taken."
784         query = self.query.chain(sql.UpdateQuery)
785         query.add_update_fields(values)
786         # Clear any annotations so that they won't be present in subqueries.
787         query.annotations = {}
788         self._result_cache = None
789         return query.get_compiler(self.db).execute_sql(CURSOR)
790     _update.alters_data = True
791     _update.queryset_only = False
792 
793     def exists(self):
794         if self._result_cache is None:
795             return self.query.has_results(using=self.db)
796         return bool(self._result_cache)
797 
798     def _prefetch_related_objects(self):
799         # This method can only be called once the result cache has been filled.
800         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
801         self._prefetch_done = True
802 
803     def explain(self, *, format=None, **options):
804         return self.query.explain(using=self.db, format=format, **options)
805 
806     ##################################################
807     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
808     ##################################################
809 
810     def raw(self, raw_query, params=None, translations=None, using=None):
811         if using is None:
812             using = self.db
813         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
814         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
815         return qs
816 
817     def _values(self, *fields, **expressions):
818         clone = self._chain()
819         if expressions:
820             clone = clone.annotate(**expressions)
821         clone._fields = fields
822         clone.query.set_values(fields)
823         return clone
824 
825     def values(self, *fields, **expressions):
826         fields += tuple(expressions)
827         clone = self._values(*fields, **expressions)
828         clone._iterable_class = ValuesIterable
829         return clone
830 
831     def values_list(self, *fields, flat=False, named=False):
832         if flat and named:
833             raise TypeError("'flat' and 'named' can't be used together.")
834         if flat and len(fields) > 1:
835             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
836 
837         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
838         _fields = []
839         expressions = {}
840         counter = 1
841         for field in fields:
842             if hasattr(field, 'resolve_expression'):
843                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
844                 while True:
845                     field_id = field_id_prefix + str(counter)
846                     counter += 1
847                     if field_id not in field_names:
848                         break
849                 expressions[field_id] = field
850                 _fields.append(field_id)
851             else:
852                 _fields.append(field)
853 
854         clone = self._values(*_fields, **expressions)
855         clone._iterable_class = (
856             NamedValuesListIterable if named
857             else FlatValuesListIterable if flat
858             else ValuesListIterable
859         )
860         return clone
861 
862     def dates(self, field_name, kind, order='ASC'):
863         """
864         Return a list of date objects representing all available dates for
865         the given field_name, scoped to 'kind'.
866         """
867         assert kind in ('year', 'month', 'week', 'day'), \
868             "'kind' must be one of 'year', 'month', 'week', or 'day'."
869         assert order in ('ASC', 'DESC'), \
870             "'order' must be either 'ASC' or 'DESC'."
871         return self.annotate(
872             datefield=Trunc(field_name, kind, output_field=DateField()),
873             plain_field=F(field_name)
874         ).values_list(
875             'datefield', flat=True
876         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
877 
878     def datetimes(self, field_name, kind, order='ASC', tzinfo=None):
879         """
880         Return a list of datetime objects representing all available
881         datetimes for the given field_name, scoped to 'kind'.
882         """
883         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
884             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
885         assert order in ('ASC', 'DESC'), \
886             "'order' must be either 'ASC' or 'DESC'."
887         if settings.USE_TZ:
888             if tzinfo is None:
889                 tzinfo = timezone.get_current_timezone()
890         else:
891             tzinfo = None
892         return self.annotate(
893             datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),
894             plain_field=F(field_name)
895         ).values_list(
896             'datetimefield', flat=True
897         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
898 
899     def none(self):
900         """Return an empty QuerySet."""
901         clone = self._chain()
902         clone.query.set_empty()
903         return clone
904 
905     ##################################################################
906     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
907     ##################################################################
908 
909     def all(self):
910         """
911         Return a new QuerySet that is a copy of the current one. This allows a
912         QuerySet to proxy for a model manager in some cases.
913         """
914         return self._chain()
915 
916     def filter(self, *args, **kwargs):
917         """
918         Return a new QuerySet instance with the args ANDed to the existing
919         set.
920         """
921         self._not_support_combined_queries('filter')
922         return self._filter_or_exclude(False, *args, **kwargs)
923 
924     def exclude(self, *args, **kwargs):
925         """
926         Return a new QuerySet instance with NOT (args) ANDed to the existing
927         set.
928         """
929         self._not_support_combined_queries('exclude')
930         return self._filter_or_exclude(True, *args, **kwargs)
931 
932     def _filter_or_exclude(self, negate, *args, **kwargs):
933         if args or kwargs:
934             assert not self.query.is_sliced, \
935                 "Cannot filter a query once a slice has been taken."
936 
937         clone = self._chain()
938         if self._defer_next_filter:
939             self._defer_next_filter = False
940             clone._deferred_filter = negate, args, kwargs
941         else:
942             clone._filter_or_exclude_inplace(negate, *args, **kwargs)
943         return clone
944 
945     def _filter_or_exclude_inplace(self, negate, *args, **kwargs):
946         if negate:
947             self._query.add_q(~Q(*args, **kwargs))
948         else:
949             self._query.add_q(Q(*args, **kwargs))
950 
951     def complex_filter(self, filter_obj):
952         """
953         Return a new QuerySet instance with filter_obj added to the filters.
954 
955         filter_obj can be a Q object or a dictionary of keyword lookup
956         arguments.
957 
958         This exists to support framework features such as 'limit_choices_to',
959         and usually it will be more natural to use other methods.
960         """
961         if isinstance(filter_obj, Q):
962             clone = self._chain()
963             clone.query.add_q(filter_obj)
964             return clone
965         else:
966             return self._filter_or_exclude(False, **filter_obj)
967 
968     def _combinator_query(self, combinator, *other_qs, all=False):
969         # Clone the query to inherit the select list and everything
970         clone = self._chain()
971         # Clear limits and ordering so they can be reapplied
972         clone.query.clear_ordering(True)
973         clone.query.clear_limits()
974         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
975         clone.query.combinator = combinator
976         clone.query.combinator_all = all
977         return clone
978 
979     def union(self, *other_qs, all=False):
980         # If the query is an EmptyQuerySet, combine all nonempty querysets.
981         if isinstance(self, EmptyQuerySet):
982             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
983             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
984         return self._combinator_query('union', *other_qs, all=all)
985 
986     def intersection(self, *other_qs):
987         # If any query is an EmptyQuerySet, return it.
988         if isinstance(self, EmptyQuerySet):
989             return self
990         for other in other_qs:
991             if isinstance(other, EmptyQuerySet):
992                 return other
993         return self._combinator_query('intersection', *other_qs)
994 
995     def difference(self, *other_qs):
996         # If the query is an EmptyQuerySet, return it.
997         if isinstance(self, EmptyQuerySet):
998             return self
999         return self._combinator_query('difference', *other_qs)
1000 
1001     def select_for_update(self, nowait=False, skip_locked=False, of=()):
1002         """
1003         Return a new QuerySet instance that will select objects with a
1004         FOR UPDATE lock.
1005         """
1006         if nowait and skip_locked:
1007             raise ValueError('The nowait option cannot be used with skip_locked.')
1008         obj = self._chain()
1009         obj._for_write = True
1010         obj.query.select_for_update = True
1011         obj.query.select_for_update_nowait = nowait
1012         obj.query.select_for_update_skip_locked = skip_locked
1013         obj.query.select_for_update_of = of
1014         return obj
1015 
1016     def select_related(self, *fields):
1017         """
1018         Return a new QuerySet instance that will select related objects.
1019 
1020         If fields are specified, they must be ForeignKey fields and only those
1021         related objects are included in the selection.
1022 
1023         If select_related(None) is called, clear the list.
1024         """
1025         self._not_support_combined_queries('select_related')
1026         if self._fields is not None:
1027             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1028 
1029         obj = self._chain()
1030         if fields == (None,):
1031             obj.query.select_related = False
1032         elif fields:
1033             obj.query.add_select_related(fields)
1034         else:
1035             obj.query.select_related = True
1036         return obj
1037 
1038     def prefetch_related(self, *lookups):
1039         """
1040         Return a new QuerySet instance that will prefetch the specified
1041         Many-To-One and Many-To-Many related objects when the QuerySet is
1042         evaluated.
1043 
1044         When prefetch_related() is called more than once, append to the list of
1045         prefetch lookups. If prefetch_related(None) is called, clear the list.
1046         """
1047         self._not_support_combined_queries('prefetch_related')
1048         clone = self._chain()
1049         if lookups == (None,):
1050             clone._prefetch_related_lookups = ()
1051         else:
1052             for lookup in lookups:
1053                 if isinstance(lookup, Prefetch):
1054                     lookup = lookup.prefetch_to
1055                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1056                 if lookup in self.query._filtered_relations:
1057                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1058             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1059         return clone
1060 
1061     def annotate(self, *args, **kwargs):
1062         """
1063         Return a query set in which the returned objects have been annotated
1064         with extra data or aggregations.
1065         """
1066         self._not_support_combined_queries('annotate')
1067         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1068         annotations = {}
1069         for arg in args:
1070             # The default_alias property may raise a TypeError.
1071             try:
1072                 if arg.default_alias in kwargs:
1073                     raise ValueError("The named annotation '%s' conflicts with the "
1074                                      "default name for another annotation."
1075                                      % arg.default_alias)
1076             except TypeError:
1077                 raise TypeError("Complex annotations require an alias")
1078             annotations[arg.default_alias] = arg
1079         annotations.update(kwargs)
1080 
1081         clone = self._chain()
1082         names = self._fields
1083         if names is None:
1084             names = set(chain.from_iterable(
1085                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1086                 for field in self.model._meta.get_fields()
1087             ))
1088 
1089         for alias, annotation in annotations.items():
1090             if alias in names:
1091                 raise ValueError("The annotation '%s' conflicts with a field on "
1092                                  "the model." % alias)
1093             if isinstance(annotation, FilteredRelation):
1094                 clone.query.add_filtered_relation(annotation, alias)
1095             else:
1096                 clone.query.add_annotation(annotation, alias, is_summary=False)
1097 
1098         for alias, annotation in clone.query.annotations.items():
1099             if alias in annotations and annotation.contains_aggregate:
1100                 if clone._fields is None:
1101                     clone.query.group_by = True
1102                 else:
1103                     clone.query.set_group_by()
1104                 break
1105 
1106         return clone
1107 
1108     def order_by(self, *field_names):
1109         """Return a new QuerySet instance with the ordering changed."""
1110         assert not self.query.is_sliced, \
1111             "Cannot reorder a query once a slice has been taken."
1112         obj = self._chain()
1113         obj.query.clear_ordering(force_empty=False)
1114         obj.query.add_ordering(*field_names)
1115         return obj
1116 
1117     def distinct(self, *field_names):
1118         """
1119         Return a new QuerySet instance that will select only distinct results.
1120         """
1121         assert not self.query.is_sliced, \
1122             "Cannot create distinct fields once a slice has been taken."
1123         obj = self._chain()
1124         obj.query.add_distinct_fields(*field_names)
1125         return obj
1126 
1127     def extra(self, select=None, where=None, params=None, tables=None,
1128               order_by=None, select_params=None):
1129         """Add extra SQL fragments to the query."""
1130         self._not_support_combined_queries('extra')
1131         assert not self.query.is_sliced, \
1132             "Cannot change a query once a slice has been taken"
1133         clone = self._chain()
1134         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1135         return clone
1136 
1137     def reverse(self):
1138         """Reverse the ordering of the QuerySet."""
1139         if self.query.is_sliced:
1140             raise TypeError('Cannot reverse a query once a slice has been taken.')
1141         clone = self._chain()
1142         clone.query.standard_ordering = not clone.query.standard_ordering
1143         return clone
1144 
1145     def defer(self, *fields):
1146         """
1147         Defer the loading of data for certain fields until they are accessed.
1148         Add the set of deferred fields to any existing set of deferred fields.
1149         The only exception to this is if None is passed in as the only
1150         parameter, in which case removal all deferrals.
1151         """
1152         self._not_support_combined_queries('defer')
1153         if self._fields is not None:
1154             raise TypeError("Cannot call defer() after .values() or .values_list()")
1155         clone = self._chain()
1156         if fields == (None,):
1157             clone.query.clear_deferred_loading()
1158         else:
1159             clone.query.add_deferred_loading(fields)
1160         return clone
1161 
1162     def only(self, *fields):
1163         """
1164         Essentially, the opposite of defer(). Only the fields passed into this
1165         method and that are not already specified as deferred are loaded
1166         immediately when the queryset is evaluated.
1167         """
1168         self._not_support_combined_queries('only')
1169         if self._fields is not None:
1170             raise TypeError("Cannot call only() after .values() or .values_list()")
1171         if fields == (None,):
1172             # Can only pass None to defer(), not only(), as the rest option.
1173             # That won't stop people trying to do this, so let's be explicit.
1174             raise TypeError("Cannot pass None as an argument to only().")
1175         for field in fields:
1176             field = field.split(LOOKUP_SEP, 1)[0]
1177             if field in self.query._filtered_relations:
1178                 raise ValueError('only() is not supported with FilteredRelation.')
1179         clone = self._chain()
1180         clone.query.add_immediate_loading(fields)
1181         return clone
1182 
1183     def using(self, alias):
1184         """Select which database this QuerySet should execute against."""
1185         clone = self._chain()
1186         clone._db = alias
1187         return clone
1188 
1189     ###################################
1190     # PUBLIC INTROSPECTION ATTRIBUTES #
1191     ###################################
1192 
1193     @property
1194     def ordered(self):
1195         """
1196         Return True if the QuerySet is ordered -- i.e. has an order_by()
1197         clause or a default ordering on the model (or is empty).
1198         """
1199         if isinstance(self, EmptyQuerySet):
1200             return True
1201         if self.query.extra_order_by or self.query.order_by:
1202             return True
1203         elif self.query.default_ordering and self.query.get_meta().ordering:
1204             return True
1205         else:
1206             return False
1207 
1208     @property
1209     def db(self):
1210         """Return the database used if this query is executed now."""
1211         if self._for_write:
1212             return self._db or router.db_for_write(self.model, **self._hints)
1213         return self._db or router.db_for_read(self.model, **self._hints)
1214 
1215     ###################
1216     # PRIVATE METHODS #
1217     ###################
1218 
1219     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1220         """
1221         Insert a new record for the given model. This provides an interface to
1222         the InsertQuery class and is how Model.save() is implemented.
1223         """
1224         self._for_write = True
1225         if using is None:
1226             using = self.db
1227         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1228         query.insert_values(fields, objs, raw=raw)
1229         return query.get_compiler(using=using).execute_sql(returning_fields)
1230     _insert.alters_data = True
1231     _insert.queryset_only = False
1232 
1233     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1234         """
1235         Helper method for bulk_create() to insert objs one batch at a time.
1236         """
1237         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1238             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1239         ops = connections[self.db].ops
1240         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1241         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1242         inserted_rows = []
1243         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1244         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1245             if bulk_return and not ignore_conflicts:
1246                 inserted_columns = self._insert(
1247                     item, fields=fields, using=self.db,
1248                     returning_fields=self.model._meta.db_returning_fields,
1249                     ignore_conflicts=ignore_conflicts,
1250                 )
1251                 if isinstance(inserted_columns, list):
1252                     inserted_rows.extend(inserted_columns)
1253                 else:
1254                     inserted_rows.append(inserted_columns)
1255             else:
1256                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1257         return inserted_rows
1258 
1259     def _chain(self, **kwargs):
1260         """
1261         Return a copy of the current QuerySet that's ready for another
1262         operation.
1263         """
1264         obj = self._clone()
1265         if obj._sticky_filter:
1266             obj.query.filter_is_sticky = True
1267             obj._sticky_filter = False
1268         obj.__dict__.update(kwargs)
1269         return obj
1270 
1271     def _clone(self):
1272         """
1273         Return a copy of the current QuerySet. A lightweight alternative
1274         to deepcopy().
1275         """
1276         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1277         c._sticky_filter = self._sticky_filter
1278         c._for_write = self._for_write
1279         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1280         c._known_related_objects = self._known_related_objects
1281         c._iterable_class = self._iterable_class
1282         c._fields = self._fields
1283         return c
1284 
1285     def _fetch_all(self):
1286         if self._result_cache is None:
1287             self._result_cache = list(self._iterable_class(self))
1288         if self._prefetch_related_lookups and not self._prefetch_done:
1289             self._prefetch_related_objects()
1290 
1291     def _next_is_sticky(self):
1292         """
1293         Indicate that the next filter call and the one following that should
1294         be treated as a single filter. This is only important when it comes to
1295         determining when to reuse tables for many-to-many filters. Required so
1296         that we can filter naturally on the results of related managers.
1297 
1298         This doesn't return a clone of the current QuerySet (it returns
1299         "self"). The method is only used internally and should be immediately
1300         followed by a filter() that does create a clone.
1301         """
1302         self._sticky_filter = True
1303         return self
1304 
1305     def _merge_sanity_check(self, other):
1306         """Check that two QuerySet classes may be merged."""
1307         if self._fields is not None and (
1308                 set(self.query.values_select) != set(other.query.values_select) or
1309                 set(self.query.extra_select) != set(other.query.extra_select) or
1310                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1311             raise TypeError(
1312                 "Merging '%s' classes must involve the same values in each case."
1313                 % self.__class__.__name__
1314             )
1315 
1316     def _merge_known_related_objects(self, other):
1317         """
1318         Keep track of all known related objects from either QuerySet instance.
1319         """
1320         for field, objects in other._known_related_objects.items():
1321             self._known_related_objects.setdefault(field, {}).update(objects)
1322 
1323     def resolve_expression(self, *args, **kwargs):
1324         if self._fields and len(self._fields) > 1:
1325             # values() queryset can only be used as nested queries
1326             # if they are set up to select only a single field.
1327             raise TypeError('Cannot use multi-field values as a filter value.')
1328         query = self.query.resolve_expression(*args, **kwargs)
1329         query._db = self._db
1330         return query
1331     resolve_expression.queryset_only = True
1332 
1333     def _add_hints(self, **hints):
1334         """
1335         Update hinting information for use by routers. Add new key/values or
1336         overwrite existing key/values.
1337         """
1338         self._hints.update(hints)
1339 
1340     def _has_filters(self):
1341         """
1342         Check if this QuerySet has any filtering going on. This isn't
1343         equivalent with checking if all objects are present in results, for
1344         example, qs[1:]._has_filters() -> False.
1345         """
1346         return self.query.has_filters()
1347 
1348     @staticmethod
1349     def _validate_values_are_expressions(values, method_name):
1350         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1351         if invalid_args:
1352             raise TypeError(
1353                 'QuerySet.%s() received non-expression(s): %s.' % (
1354                     method_name,
1355                     ', '.join(invalid_args),
1356                 )
1357             )
1358 
1359     def _not_support_combined_queries(self, operation_name):
1360         if self.query.combinator:
1361             raise NotSupportedError(
1362                 'Calling QuerySet.%s() after %s() is not supported.'
1363                 % (operation_name, self.query.combinator)
1364             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-17 11:37:21.289 | INFO     | Search completed. Bug locations: [<file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>create</method>
<code>
436     def create(self, **kwargs):
437         """
438         Create a new object with the given kwargs, saving it to the database
439         and returning the created object.
440         """
441         obj = self.model(**kwargs)
442         self._for_write = True
443         obj.save(force_insert=True, using=self.db)
444         return obj

</code><intended_behavior>The `create` method should include improved handling for objects that are part of a multi-table inheritance chain, where one or more parents have a custom primary key (e.g., a UUIDField). Specifically, when creating an instance of a child model, Django should optimally manage the insert process to prevent unnecessary updates to the parent model's record after its initial insertion. This could involve a pre-check to assess the inheritance chain of the model being created and to prepare all related data for a streamlined insert operation that doesn't necessitate follow-up updates to correct or complete the record linkage in the database.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
184 class QuerySet:
185     """Represent a lazy database lookup for a set of objects."""
186 
187     def __init__(self, model=None, query=None, using=None, hints=None):
188         self.model = model
189         self._db = using
190         self._hints = hints or {}
191         self._query = query or sql.Query(self.model)
192         self._result_cache = None
193         self._sticky_filter = False
194         self._for_write = False
195         self._prefetch_related_lookups = ()
196         self._prefetch_done = False
197         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
198         self._iterable_class = ModelIterable
199         self._fields = None
200         self._defer_next_filter = False
201         self._deferred_filter = None
202 
203     @property
204     def query(self):
205         if self._deferred_filter:
206             negate, args, kwargs = self._deferred_filter
207             self._filter_or_exclude_inplace(negate, *args, **kwargs)
208             self._deferred_filter = None
209         return self._query
210 
211     @query.setter
212     def query(self, value):
213         self._query = value
214 
215     def as_manager(cls):
216         # Address the circular dependency between `Queryset` and `Manager`.
217         from django.db.models.manager import Manager
218         manager = Manager.from_queryset(cls)()
219         manager._built_with_as_manager = True
220         return manager
221     as_manager.queryset_only = True
222     as_manager = classmethod(as_manager)
223 
224     ########################
225     # PYTHON MAGIC METHODS #
226     ########################
227 
228     def __deepcopy__(self, memo):
229         """Don't populate the QuerySet's cache."""
230         obj = self.__class__()
231         for k, v in self.__dict__.items():
232             if k == '_result_cache':
233                 obj.__dict__[k] = None
234             else:
235                 obj.__dict__[k] = copy.deepcopy(v, memo)
236         return obj
237 
238     def __getstate__(self):
239         # Force the cache to be fully populated.
240         self._fetch_all()
241         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}
242 
243     def __setstate__(self, state):
244         msg = None
245         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
246         if pickled_version:
247             current_version = get_version()
248             if current_version != pickled_version:
249                 msg = (
250                     "Pickled queryset instance's Django version %s does not "
251                     "match the current version %s." % (pickled_version, current_version)
252                 )
253         else:
254             msg = "Pickled queryset instance's Django version is not specified."
255 
256         if msg:
257             warnings.warn(msg, RuntimeWarning, stacklevel=2)
258 
259         self.__dict__.update(state)
260 
261     def __repr__(self):
262         data = list(self[:REPR_OUTPUT_SIZE + 1])
263         if len(data) > REPR_OUTPUT_SIZE:
264             data[-1] = "...(remaining elements truncated)..."
265         return '<%s %r>' % (self.__class__.__name__, data)
266 
267     def __len__(self):
268         self._fetch_all()
269         return len(self._result_cache)
270 
271     def __iter__(self):
272         """
273         The queryset iterator protocol uses three nested iterators in the
274         default case:
275             1. sql.compiler.execute_sql()
276                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
277                  using cursor.fetchmany(). This part is responsible for
278                  doing some column masking, and returning the rows in chunks.
279             2. sql.compiler.results_iter()
280                - Returns one row at time. At this point the rows are still just
281                  tuples. In some cases the return values are converted to
282                  Python values at this location.
283             3. self.iterator()
284                - Responsible for turning the rows into model objects.
285         """
286         self._fetch_all()
287         return iter(self._result_cache)
288 
289     def __bool__(self):
290         self._fetch_all()
291         return bool(self._result_cache)
292 
293     def __getitem__(self, k):
294         """Retrieve an item or slice from the set of results."""
295         if not isinstance(k, (int, slice)):
296             raise TypeError(
297                 'QuerySet indices must be integers or slices, not %s.'
298                 % type(k).__name__
299             )
300         assert ((not isinstance(k, slice) and (k >= 0)) or
301                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
302                  (k.stop is None or k.stop >= 0))), \
303             "Negative indexing is not supported."
304 
305         if self._result_cache is not None:
306             return self._result_cache[k]
307 
308         if isinstance(k, slice):
309             qs = self._chain()
310             if k.start is not None:
311                 start = int(k.start)
312             else:
313                 start = None
314             if k.stop is not None:
315                 stop = int(k.stop)
316             else:
317                 stop = None
318             qs.query.set_limits(start, stop)
319             return list(qs)[::k.step] if k.step else qs
320 
321         qs = self._chain()
322         qs.query.set_limits(k, k + 1)
323         qs._fetch_all()
324         return qs._result_cache[0]
325 
326     def __and__(self, other):
327         self._merge_sanity_check(other)
328         if isinstance(other, EmptyQuerySet):
329             return other
330         if isinstance(self, EmptyQuerySet):
331             return self
332         combined = self._chain()
333         combined._merge_known_related_objects(other)
334         combined.query.combine(other.query, sql.AND)
335         return combined
336 
337     def __or__(self, other):
338         self._merge_sanity_check(other)
339         if isinstance(self, EmptyQuerySet):
340             return other
341         if isinstance(other, EmptyQuerySet):
342             return self
343         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
344         combined = query._chain()
345         combined._merge_known_related_objects(other)
346         if not other.query.can_filter():
347             other = other.model._base_manager.filter(pk__in=other.values('pk'))
348         combined.query.combine(other.query, sql.OR)
349         return combined
350 
351     ####################################
352     # METHODS THAT DO DATABASE QUERIES #
353     ####################################
354 
355     def _iterator(self, use_chunked_fetch, chunk_size):
356         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
357 
358     def iterator(self, chunk_size=2000):
359         """
360         An iterator over the results from applying this QuerySet to the
361         database.
362         """
363         if chunk_size <= 0:
364             raise ValueError('Chunk size must be strictly positive.')
365         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
366         return self._iterator(use_chunked_fetch, chunk_size)
367 
368     def aggregate(self, *args, **kwargs):
369         """
370         Return a dictionary containing the calculations (aggregation)
371         over the current queryset.
372 
373         If args is present the expression is passed as a kwarg using
374         the Aggregate object's default alias.
375         """
376         if self.query.distinct_fields:
377             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
378         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
379         for arg in args:
380             # The default_alias property raises TypeError if default_alias
381             # can't be set automatically or AttributeError if it isn't an
382             # attribute.
383             try:
384                 arg.default_alias
385             except (AttributeError, TypeError):
386                 raise TypeError("Complex aggregates require an alias")
387             kwargs[arg.default_alias] = arg
388 
389         query = self.query.chain()
390         for (alias, aggregate_expr) in kwargs.items():
391             query.add_annotation(aggregate_expr, alias, is_summary=True)
392             if not query.annotations[alias].contains_aggregate:
393                 raise TypeError("%s is not an aggregate expression" % alias)
394         return query.get_aggregation(self.db, kwargs)
395 
396     def count(self):
397         """
398         Perform a SELECT COUNT() and return the number of records as an
399         integer.
400 
401         If the QuerySet is already fully cached, return the length of the
402         cached results set to avoid multiple SELECT COUNT(*) calls.
403         """
404         if self._result_cache is not None:
405             return len(self._result_cache)
406 
407         return self.query.get_count(using=self.db)
408 
409     def get(self, *args, **kwargs):
410         """
411         Perform the query and return a single object matching the given
412         keyword arguments.
413         """
414         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
415         if self.query.can_filter() and not self.query.distinct_fields:
416             clone = clone.order_by()
417         limit = None
418         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
419             limit = MAX_GET_RESULTS
420             clone.query.set_limits(high=limit)
421         num = len(clone)
422         if num == 1:
423             return clone._result_cache[0]
424         if not num:
425             raise self.model.DoesNotExist(
426                 "%s matching query does not exist." %
427                 self.model._meta.object_name
428             )
429         raise self.model.MultipleObjectsReturned(
430             'get() returned more than one %s -- it returned %s!' % (
431                 self.model._meta.object_name,
432                 num if not limit or num < limit else 'more than %s' % (limit - 1),
433             )
434         )
435 
436     def create(self, **kwargs):
437         """
438         Create a new object with the given kwargs, saving it to the database
439         and returning the created object.
440         """
441         obj = self.model(**kwargs)
442         self._for_write = True
443         obj.save(force_insert=True, using=self.db)
444         return obj
445 
446     def _populate_pk_values(self, objs):
447         for obj in objs:
448             if obj.pk is None:
449                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
450 
451     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
452         """
453         Insert each of the instances into the database. Do *not* call
454         save() on each of the instances, do not send any pre/post_save
455         signals, and do not set the primary key attribute if it is an
456         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
457         Multi-table models are not supported.
458         """
459         # When you bulk insert you don't get the primary keys back (if it's an
460         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
461         # you can't insert into the child tables which references this. There
462         # are two workarounds:
463         # 1) This could be implemented if you didn't have an autoincrement pk
464         # 2) You could do it by doing O(n) normal inserts into the parent
465         #    tables to get the primary keys back and then doing a single bulk
466         #    insert into the childmost table.
467         # We currently set the primary keys on the objects when using
468         # PostgreSQL via the RETURNING ID clause. It should be possible for
469         # Oracle as well, but the semantics for extracting the primary keys is
470         # trickier so it's not done yet.
471         assert batch_size is None or batch_size > 0
472         # Check that the parents share the same concrete model with the our
473         # model to detect the inheritance pattern ConcreteGrandParent ->
474         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
475         # would not identify that case as involving multiple tables.
476         for parent in self.model._meta.get_parent_list():
477             if parent._meta.concrete_model is not self.model._meta.concrete_model:
478                 raise ValueError("Can't bulk create a multi-table inherited model")
479         if not objs:
480             return objs
481         self._for_write = True
482         connection = connections[self.db]
483         opts = self.model._meta
484         fields = opts.concrete_fields
485         objs = list(objs)
486         self._populate_pk_values(objs)
487         with transaction.atomic(using=self.db, savepoint=False):
488             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
489             if objs_with_pk:
490                 returned_columns = self._batched_insert(
491                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
492                 )
493                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
494                     for result, field in zip(results, opts.db_returning_fields):
495                         if field != opts.pk:
496                             setattr(obj_with_pk, field.attname, result)
497                 for obj_with_pk in objs_with_pk:
498                     obj_with_pk._state.adding = False
499                     obj_with_pk._state.db = self.db
500             if objs_without_pk:
501                 fields = [f for f in fields if not isinstance(f, AutoField)]
502                 returned_columns = self._batched_insert(
503                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
504                 )
505                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
506                     assert len(returned_columns) == len(objs_without_pk)
507                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
508                     for result, field in zip(results, opts.db_returning_fields):
509                         setattr(obj_without_pk, field.attname, result)
510                     obj_without_pk._state.adding = False
511                     obj_without_pk._state.db = self.db
512 
513         return objs
514 
515     def bulk_update(self, objs, fields, batch_size=None):
516         """
517         Update the given fields in each of the given objects in the database.
518         """
519         if batch_size is not None and batch_size < 0:
520             raise ValueError('Batch size must be a positive integer.')
521         if not fields:
522             raise ValueError('Field names must be given to bulk_update().')
523         objs = tuple(objs)
524         if any(obj.pk is None for obj in objs):
525             raise ValueError('All bulk_update() objects must have a primary key set.')
526         fields = [self.model._meta.get_field(name) for name in fields]
527         if any(not f.concrete or f.many_to_many for f in fields):
528             raise ValueError('bulk_update() can only be used with concrete fields.')
529         if any(f.primary_key for f in fields):
530             raise ValueError('bulk_update() cannot be used with primary key fields.')
531         if not objs:
532             return
533         # PK is used twice in the resulting update query, once in the filter
534         # and once in the WHEN. Each field will also have one CAST.
535         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
536         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
537         requires_casting = connections[self.db].features.requires_casted_case_in_updates
538         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
539         updates = []
540         for batch_objs in batches:
541             update_kwargs = {}
542             for field in fields:
543                 when_statements = []
544                 for obj in batch_objs:
545                     attr = getattr(obj, field.attname)
546                     if not isinstance(attr, Expression):
547                         attr = Value(attr, output_field=field)
548                     when_statements.append(When(pk=obj.pk, then=attr))
549                 case_statement = Case(*when_statements, output_field=field)
550                 if requires_casting:
551                     case_statement = Cast(case_statement, output_field=field)
552                 update_kwargs[field.attname] = case_statement
553             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
554         with transaction.atomic(using=self.db, savepoint=False):
555             for pks, update_kwargs in updates:
556                 self.filter(pk__in=pks).update(**update_kwargs)
557     bulk_update.alters_data = True
558 
559     def get_or_create(self, defaults=None, **kwargs):
560         """
561         Look up an object with the given kwargs, creating one if necessary.
562         Return a tuple of (object, created), where created is a boolean
563         specifying whether an object was created.
564         """
565         # The get() needs to be targeted at the write database in order
566         # to avoid potential transaction consistency problems.
567         self._for_write = True
568         try:
569             return self.get(**kwargs), False
570         except self.model.DoesNotExist:
571             params = self._extract_model_params(defaults, **kwargs)
572             return self._create_object_from_params(kwargs, params)
573 
574     def update_or_create(self, defaults=None, **kwargs):
575         """
576         Look up an object with the given kwargs, updating one with defaults
577         if it exists, otherwise create a new one.
578         Return a tuple (object, created), where created is a boolean
579         specifying whether an object was created.
580         """
581         defaults = defaults or {}
582         self._for_write = True
583         with transaction.atomic(using=self.db):
584             try:
585                 obj = self.select_for_update().get(**kwargs)
586             except self.model.DoesNotExist:
587                 params = self._extract_model_params(defaults, **kwargs)
588                 # Lock the row so that a concurrent update is blocked until
589                 # after update_or_create() has performed its save.
590                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
591                 if created:
592                     return obj, created
593             for k, v in resolve_callables(defaults):
594                 setattr(obj, k, v)
595             obj.save(using=self.db)
596         return obj, False
597 
598     def _create_object_from_params(self, lookup, params, lock=False):
599         """
600         Try to create an object using passed params. Used by get_or_create()
601         and update_or_create().
602         """
603         try:
604             with transaction.atomic(using=self.db):
605                 params = dict(resolve_callables(params))
606                 obj = self.create(**params)
607             return obj, True
608         except IntegrityError:
609             try:
610                 qs = self.select_for_update() if lock else self
611                 return qs.get(**lookup), False
612             except self.model.DoesNotExist:
613                 pass
614             raise
615 
616     def _extract_model_params(self, defaults, **kwargs):
617         """
618         Prepare `params` for creating a model instance based on the given
619         kwargs; for use by get_or_create() and update_or_create().
620         """
621         defaults = defaults or {}
622         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
623         params.update(defaults)
624         property_names = self.model._meta._property_names
625         invalid_params = []
626         for param in params:
627             try:
628                 self.model._meta.get_field(param)
629             except exceptions.FieldDoesNotExist:
630                 # It's okay to use a model's property if it has a setter.
631                 if not (param in property_names and getattr(self.model, param).fset):
632                     invalid_params.append(param)
633         if invalid_params:
634             raise exceptions.FieldError(
635                 "Invalid field name(s) for model %s: '%s'." % (
636                     self.model._meta.object_name,
637                     "', '".join(sorted(invalid_params)),
638                 ))
639         return params
640 
641     def _earliest(self, *fields):
642         """
643         Return the earliest object according to fields (if given) or by the
644         model's Meta.get_latest_by.
645         """
646         if fields:
647             order_by = fields
648         else:
649             order_by = getattr(self.model._meta, 'get_latest_by')
650             if order_by and not isinstance(order_by, (tuple, list)):
651                 order_by = (order_by,)
652         if order_by is None:
653             raise ValueError(
654                 "earliest() and latest() require either fields as positional "
655                 "arguments or 'get_latest_by' in the model's Meta."
656             )
657 
658         assert not self.query.is_sliced, \
659             "Cannot change a query once a slice has been taken."
660         obj = self._chain()
661         obj.query.set_limits(high=1)
662         obj.query.clear_ordering(force_empty=True)
663         obj.query.add_ordering(*order_by)
664         return obj.get()
665 
666     def earliest(self, *fields):
667         return self._earliest(*fields)
668 
669     def latest(self, *fields):
670         return self.reverse()._earliest(*fields)
671 
672     def first(self):
673         """Return the first object of a query or None if no match is found."""
674         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
675             return obj
676 
677     def last(self):
678         """Return the last object of a query or None if no match is found."""
679         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
680             return obj
681 
682     def in_bulk(self, id_list=None, *, field_name='pk'):
683         """
684         Return a dictionary mapping each of the given IDs to the object with
685         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
686         """
687         assert not self.query.is_sliced, \
688             "Cannot use 'limit' or 'offset' with in_bulk"
689         if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:
690             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
691         if id_list is not None:
692             if not id_list:
693                 return {}
694             filter_key = '{}__in'.format(field_name)
695             batch_size = connections[self.db].features.max_query_params
696             id_list = tuple(id_list)
697             # If the database has a limit on the number of query parameters
698             # (e.g. SQLite), retrieve objects in batches if necessary.
699             if batch_size and batch_size < len(id_list):
700                 qs = ()
701                 for offset in range(0, len(id_list), batch_size):
702                     batch = id_list[offset:offset + batch_size]
703                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
704             else:
705                 qs = self.filter(**{filter_key: id_list}).order_by()
706         else:
707             qs = self._chain()
708         return {getattr(obj, field_name): obj for obj in qs}
709 
710     def delete(self):
711         """Delete the records in the current QuerySet."""
712         self._not_support_combined_queries('delete')
713         assert not self.query.is_sliced, \
714             "Cannot use 'limit' or 'offset' with delete."
715 
716         if self._fields is not None:
717             raise TypeError("Cannot call delete() after .values() or .values_list()")
718 
719         del_query = self._chain()
720 
721         # The delete is actually 2 queries - one to find related objects,
722         # and one to delete. Make sure that the discovery of related
723         # objects is performed on the same database as the deletion.
724         del_query._for_write = True
725 
726         # Disable non-supported fields.
727         del_query.query.select_for_update = False
728         del_query.query.select_related = False
729         del_query.query.clear_ordering(force_empty=True)
730 
731         collector = Collector(using=del_query.db)
732         collector.collect(del_query)
733         deleted, _rows_count = collector.delete()
734 
735         # Clear the result cache, in case this QuerySet gets reused.
736         self._result_cache = None
737         return deleted, _rows_count
738 
739     delete.alters_data = True
740     delete.queryset_only = True
741 
742     def _raw_delete(self, using):
743         """
744         Delete objects found from the given queryset in single direct SQL
745         query. No signals are sent and there is no protection for cascades.
746         """
747         query = self.query.clone()
748         query.__class__ = sql.DeleteQuery
749         cursor = query.get_compiler(using).execute_sql(CURSOR)
750         if cursor:
751             with cursor:
752                 return cursor.rowcount
753         return 0
754     _raw_delete.alters_data = True
755 
756     def update(self, **kwargs):
757         """
758         Update all elements in the current QuerySet, setting all the given
759         fields to the appropriate values.
760         """
761         self._not_support_combined_queries('update')
762         assert not self.query.is_sliced, \
763             "Cannot update a query once a slice has been taken."
764         self._for_write = True
765         query = self.query.chain(sql.UpdateQuery)
766         query.add_update_values(kwargs)
767         # Clear any annotations so that they won't be present in subqueries.
768         query.annotations = {}
769         with transaction.mark_for_rollback_on_error(using=self.db):
770             rows = query.get_compiler(self.db).execute_sql(CURSOR)
771         self._result_cache = None
772         return rows
773     update.alters_data = True
774 
775     def _update(self, values):
776         """
777         A version of update() that accepts field objects instead of field names.
778         Used primarily for model saving and not intended for use by general
779         code (it requires too much poking around at model internals to be
780         useful at that level).
781         """
782         assert not self.query.is_sliced, \
783             "Cannot update a query once a slice has been taken."
784         query = self.query.chain(sql.UpdateQuery)
785         query.add_update_fields(values)
786         # Clear any annotations so that they won't be present in subqueries.
787         query.annotations = {}
788         self._result_cache = None
789         return query.get_compiler(self.db).execute_sql(CURSOR)
790     _update.alters_data = True
791     _update.queryset_only = False
792 
793     def exists(self):
794         if self._result_cache is None:
795             return self.query.has_results(using=self.db)
796         return bool(self._result_cache)
797 
798     def _prefetch_related_objects(self):
799         # This method can only be called once the result cache has been filled.
800         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
801         self._prefetch_done = True
802 
803     def explain(self, *, format=None, **options):
804         return self.query.explain(using=self.db, format=format, **options)
805 
806     ##################################################
807     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
808     ##################################################
809 
810     def raw(self, raw_query, params=None, translations=None, using=None):
811         if using is None:
812             using = self.db
813         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
814         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
815         return qs
816 
817     def _values(self, *fields, **expressions):
818         clone = self._chain()
819         if expressions:
820             clone = clone.annotate(**expressions)
821         clone._fields = fields
822         clone.query.set_values(fields)
823         return clone
824 
825     def values(self, *fields, **expressions):
826         fields += tuple(expressions)
827         clone = self._values(*fields, **expressions)
828         clone._iterable_class = ValuesIterable
829         return clone
830 
831     def values_list(self, *fields, flat=False, named=False):
832         if flat and named:
833             raise TypeError("'flat' and 'named' can't be used together.")
834         if flat and len(fields) > 1:
835             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
836 
837         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
838         _fields = []
839         expressions = {}
840         counter = 1
841         for field in fields:
842             if hasattr(field, 'resolve_expression'):
843                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
844                 while True:
845                     field_id = field_id_prefix + str(counter)
846                     counter += 1
847                     if field_id not in field_names:
848                         break
849                 expressions[field_id] = field
850                 _fields.append(field_id)
851             else:
852                 _fields.append(field)
853 
854         clone = self._values(*_fields, **expressions)
855         clone._iterable_class = (
856             NamedValuesListIterable if named
857             else FlatValuesListIterable if flat
858             else ValuesListIterable
859         )
860         return clone
861 
862     def dates(self, field_name, kind, order='ASC'):
863         """
864         Return a list of date objects representing all available dates for
865         the given field_name, scoped to 'kind'.
866         """
867         assert kind in ('year', 'month', 'week', 'day'), \
868             "'kind' must be one of 'year', 'month', 'week', or 'day'."
869         assert order in ('ASC', 'DESC'), \
870             "'order' must be either 'ASC' or 'DESC'."
871         return self.annotate(
872             datefield=Trunc(field_name, kind, output_field=DateField()),
873             plain_field=F(field_name)
874         ).values_list(
875             'datefield', flat=True
876         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
877 
878     def datetimes(self, field_name, kind, order='ASC', tzinfo=None):
879         """
880         Return a list of datetime objects representing all available
881         datetimes for the given field_name, scoped to 'kind'.
882         """
883         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
884             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
885         assert order in ('ASC', 'DESC'), \
886             "'order' must be either 'ASC' or 'DESC'."
887         if settings.USE_TZ:
888             if tzinfo is None:
889                 tzinfo = timezone.get_current_timezone()
890         else:
891             tzinfo = None
892         return self.annotate(
893             datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),
894             plain_field=F(field_name)
895         ).values_list(
896             'datetimefield', flat=True
897         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
898 
899     def none(self):
900         """Return an empty QuerySet."""
901         clone = self._chain()
902         clone.query.set_empty()
903         return clone
904 
905     ##################################################################
906     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
907     ##################################################################
908 
909     def all(self):
910         """
911         Return a new QuerySet that is a copy of the current one. This allows a
912         QuerySet to proxy for a model manager in some cases.
913         """
914         return self._chain()
915 
916     def filter(self, *args, **kwargs):
917         """
918         Return a new QuerySet instance with the args ANDed to the existing
919         set.
920         """
921         self._not_support_combined_queries('filter')
922         return self._filter_or_exclude(False, *args, **kwargs)
923 
924     def exclude(self, *args, **kwargs):
925         """
926         Return a new QuerySet instance with NOT (args) ANDed to the existing
927         set.
928         """
929         self._not_support_combined_queries('exclude')
930         return self._filter_or_exclude(True, *args, **kwargs)
931 
932     def _filter_or_exclude(self, negate, *args, **kwargs):
933         if args or kwargs:
934             assert not self.query.is_sliced, \
935                 "Cannot filter a query once a slice has been taken."
936 
937         clone = self._chain()
938         if self._defer_next_filter:
939             self._defer_next_filter = False
940             clone._deferred_filter = negate, args, kwargs
941         else:
942             clone._filter_or_exclude_inplace(negate, *args, **kwargs)
943         return clone
944 
945     def _filter_or_exclude_inplace(self, negate, *args, **kwargs):
946         if negate:
947             self._query.add_q(~Q(*args, **kwargs))
948         else:
949             self._query.add_q(Q(*args, **kwargs))
950 
951     def complex_filter(self, filter_obj):
952         """
953         Return a new QuerySet instance with filter_obj added to the filters.
954 
955         filter_obj can be a Q object or a dictionary of keyword lookup
956         arguments.
957 
958         This exists to support framework features such as 'limit_choices_to',
959         and usually it will be more natural to use other methods.
960         """
961         if isinstance(filter_obj, Q):
962             clone = self._chain()
963             clone.query.add_q(filter_obj)
964             return clone
965         else:
966             return self._filter_or_exclude(False, **filter_obj)
967 
968     def _combinator_query(self, combinator, *other_qs, all=False):
969         # Clone the query to inherit the select list and everything
970         clone = self._chain()
971         # Clear limits and ordering so they can be reapplied
972         clone.query.clear_ordering(True)
973         clone.query.clear_limits()
974         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
975         clone.query.combinator = combinator
976         clone.query.combinator_all = all
977         return clone
978 
979     def union(self, *other_qs, all=False):
980         # If the query is an EmptyQuerySet, combine all nonempty querysets.
981         if isinstance(self, EmptyQuerySet):
982             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
983             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
984         return self._combinator_query('union', *other_qs, all=all)
985 
986     def intersection(self, *other_qs):
987         # If any query is an EmptyQuerySet, return it.
988         if isinstance(self, EmptyQuerySet):
989             return self
990         for other in other_qs:
991             if isinstance(other, EmptyQuerySet):
992                 return other
993         return self._combinator_query('intersection', *other_qs)
994 
995     def difference(self, *other_qs):
996         # If the query is an EmptyQuerySet, return it.
997         if isinstance(self, EmptyQuerySet):
998             return self
999         return self._combinator_query('difference', *other_qs)
1000 
1001     def select_for_update(self, nowait=False, skip_locked=False, of=()):
1002         """
1003         Return a new QuerySet instance that will select objects with a
1004         FOR UPDATE lock.
1005         """
1006         if nowait and skip_locked:
1007             raise ValueError('The nowait option cannot be used with skip_locked.')
1008         obj = self._chain()
1009         obj._for_write = True
1010         obj.query.select_for_update = True
1011         obj.query.select_for_update_nowait = nowait
1012         obj.query.select_for_update_skip_locked = skip_locked
1013         obj.query.select_for_update_of = of
1014         return obj
1015 
1016     def select_related(self, *fields):
1017         """
1018         Return a new QuerySet instance that will select related objects.
1019 
1020         If fields are specified, they must be ForeignKey fields and only those
1021         related objects are included in the selection.
1022 
1023         If select_related(None) is called, clear the list.
1024         """
1025         self._not_support_combined_queries('select_related')
1026         if self._fields is not None:
1027             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1028 
1029         obj = self._chain()
1030         if fields == (None,):
1031             obj.query.select_related = False
1032         elif fields:
1033             obj.query.add_select_related(fields)
1034         else:
1035             obj.query.select_related = True
1036         return obj
1037 
1038     def prefetch_related(self, *lookups):
1039         """
1040         Return a new QuerySet instance that will prefetch the specified
1041         Many-To-One and Many-To-Many related objects when the QuerySet is
1042         evaluated.
1043 
1044         When prefetch_related() is called more than once, append to the list of
1045         prefetch lookups. If prefetch_related(None) is called, clear the list.
1046         """
1047         self._not_support_combined_queries('prefetch_related')
1048         clone = self._chain()
1049         if lookups == (None,):
1050             clone._prefetch_related_lookups = ()
1051         else:
1052             for lookup in lookups:
1053                 if isinstance(lookup, Prefetch):
1054                     lookup = lookup.prefetch_to
1055                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1056                 if lookup in self.query._filtered_relations:
1057                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1058             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1059         return clone
1060 
1061     def annotate(self, *args, **kwargs):
1062         """
1063         Return a query set in which the returned objects have been annotated
1064         with extra data or aggregations.
1065         """
1066         self._not_support_combined_queries('annotate')
1067         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1068         annotations = {}
1069         for arg in args:
1070             # The default_alias property may raise a TypeError.
1071             try:
1072                 if arg.default_alias in kwargs:
1073                     raise ValueError("The named annotation '%s' conflicts with the "
1074                                      "default name for another annotation."
1075                                      % arg.default_alias)
1076             except TypeError:
1077                 raise TypeError("Complex annotations require an alias")
1078             annotations[arg.default_alias] = arg
1079         annotations.update(kwargs)
1080 
1081         clone = self._chain()
1082         names = self._fields
1083         if names is None:
1084             names = set(chain.from_iterable(
1085                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1086                 for field in self.model._meta.get_fields()
1087             ))
1088 
1089         for alias, annotation in annotations.items():
1090             if alias in names:
1091                 raise ValueError("The annotation '%s' conflicts with a field on "
1092                                  "the model." % alias)
1093             if isinstance(annotation, FilteredRelation):
1094                 clone.query.add_filtered_relation(annotation, alias)
1095             else:
1096                 clone.query.add_annotation(annotation, alias, is_summary=False)
1097 
1098         for alias, annotation in clone.query.annotations.items():
1099             if alias in annotations and annotation.contains_aggregate:
1100                 if clone._fields is None:
1101                     clone.query.group_by = True
1102                 else:
1103                     clone.query.set_group_by()
1104                 break
1105 
1106         return clone
1107 
1108     def order_by(self, *field_names):
1109         """Return a new QuerySet instance with the ordering changed."""
1110         assert not self.query.is_sliced, \
1111             "Cannot reorder a query once a slice has been taken."
1112         obj = self._chain()
1113         obj.query.clear_ordering(force_empty=False)
1114         obj.query.add_ordering(*field_names)
1115         return obj
1116 
1117     def distinct(self, *field_names):
1118         """
1119         Return a new QuerySet instance that will select only distinct results.
1120         """
1121         assert not self.query.is_sliced, \
1122             "Cannot create distinct fields once a slice has been taken."
1123         obj = self._chain()
1124         obj.query.add_distinct_fields(*field_names)
1125         return obj
1126 
1127     def extra(self, select=None, where=None, params=None, tables=None,
1128               order_by=None, select_params=None):
1129         """Add extra SQL fragments to the query."""
1130         self._not_support_combined_queries('extra')
1131         assert not self.query.is_sliced, \
1132             "Cannot change a query once a slice has been taken"
1133         clone = self._chain()
1134         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1135         return clone
1136 
1137     def reverse(self):
1138         """Reverse the ordering of the QuerySet."""
1139         if self.query.is_sliced:
1140             raise TypeError('Cannot reverse a query once a slice has been taken.')
1141         clone = self._chain()
1142         clone.query.standard_ordering = not clone.query.standard_ordering
1143         return clone
1144 
1145     def defer(self, *fields):
1146         """
1147         Defer the loading of data for certain fields until they are accessed.
1148         Add the set of deferred fields to any existing set of deferred fields.
1149         The only exception to this is if None is passed in as the only
1150         parameter, in which case removal all deferrals.
1151         """
1152         self._not_support_combined_queries('defer')
1153         if self._fields is not None:
1154             raise TypeError("Cannot call defer() after .values() or .values_list()")
1155         clone = self._chain()
1156         if fields == (None,):
1157             clone.query.clear_deferred_loading()
1158         else:
1159             clone.query.add_deferred_loading(fields)
1160         return clone
1161 
1162     def only(self, *fields):
1163         """
1164         Essentially, the opposite of defer(). Only the fields passed into this
1165         method and that are not already specified as deferred are loaded
1166         immediately when the queryset is evaluated.
1167         """
1168         self._not_support_combined_queries('only')
1169         if self._fields is not None:
1170             raise TypeError("Cannot call only() after .values() or .values_list()")
1171         if fields == (None,):
1172             # Can only pass None to defer(), not only(), as the rest option.
1173             # That won't stop people trying to do this, so let's be explicit.
1174             raise TypeError("Cannot pass None as an argument to only().")
1175         for field in fields:
1176             field = field.split(LOOKUP_SEP, 1)[0]
1177             if field in self.query._filtered_relations:
1178                 raise ValueError('only() is not supported with FilteredRelation.')
1179         clone = self._chain()
1180         clone.query.add_immediate_loading(fields)
1181         return clone
1182 
1183     def using(self, alias):
1184         """Select which database this QuerySet should execute against."""
1185         clone = self._chain()
1186         clone._db = alias
1187         return clone
1188 
1189     ###################################
1190     # PUBLIC INTROSPECTION ATTRIBUTES #
1191     ###################################
1192 
1193     @property
1194     def ordered(self):
1195         """
1196         Return True if the QuerySet is ordered -- i.e. has an order_by()
1197         clause or a default ordering on the model (or is empty).
1198         """
1199         if isinstance(self, EmptyQuerySet):
1200             return True
1201         if self.query.extra_order_by or self.query.order_by:
1202             return True
1203         elif self.query.default_ordering and self.query.get_meta().ordering:
1204             return True
1205         else:
1206             return False
1207 
1208     @property
1209     def db(self):
1210         """Return the database used if this query is executed now."""
1211         if self._for_write:
1212             return self._db or router.db_for_write(self.model, **self._hints)
1213         return self._db or router.db_for_read(self.model, **self._hints)
1214 
1215     ###################
1216     # PRIVATE METHODS #
1217     ###################
1218 
1219     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1220         """
1221         Insert a new record for the given model. This provides an interface to
1222         the InsertQuery class and is how Model.save() is implemented.
1223         """
1224         self._for_write = True
1225         if using is None:
1226             using = self.db
1227         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1228         query.insert_values(fields, objs, raw=raw)
1229         return query.get_compiler(using=using).execute_sql(returning_fields)
1230     _insert.alters_data = True
1231     _insert.queryset_only = False
1232 
1233     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1234         """
1235         Helper method for bulk_create() to insert objs one batch at a time.
1236         """
1237         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1238             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1239         ops = connections[self.db].ops
1240         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1241         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1242         inserted_rows = []
1243         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1244         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1245             if bulk_return and not ignore_conflicts:
1246                 inserted_columns = self._insert(
1247                     item, fields=fields, using=self.db,
1248                     returning_fields=self.model._meta.db_returning_fields,
1249                     ignore_conflicts=ignore_conflicts,
1250                 )
1251                 if isinstance(inserted_columns, list):
1252                     inserted_rows.extend(inserted_columns)
1253                 else:
1254                     inserted_rows.append(inserted_columns)
1255             else:
1256                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1257         return inserted_rows
1258 
1259     def _chain(self, **kwargs):
1260         """
1261         Return a copy of the current QuerySet that's ready for another
1262         operation.
1263         """
1264         obj = self._clone()
1265         if obj._sticky_filter:
1266             obj.query.filter_is_sticky = True
1267             obj._sticky_filter = False
1268         obj.__dict__.update(kwargs)
1269         return obj
1270 
1271     def _clone(self):
1272         """
1273         Return a copy of the current QuerySet. A lightweight alternative
1274         to deepcopy().
1275         """
1276         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1277         c._sticky_filter = self._sticky_filter
1278         c._for_write = self._for_write
1279         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1280         c._known_related_objects = self._known_related_objects
1281         c._iterable_class = self._iterable_class
1282         c._fields = self._fields
1283         return c
1284 
1285     def _fetch_all(self):
1286         if self._result_cache is None:
1287             self._result_cache = list(self._iterable_class(self))
1288         if self._prefetch_related_lookups and not self._prefetch_done:
1289             self._prefetch_related_objects()
1290 
1291     def _next_is_sticky(self):
1292         """
1293         Indicate that the next filter call and the one following that should
1294         be treated as a single filter. This is only important when it comes to
1295         determining when to reuse tables for many-to-many filters. Required so
1296         that we can filter naturally on the results of related managers.
1297 
1298         This doesn't return a clone of the current QuerySet (it returns
1299         "self"). The method is only used internally and should be immediately
1300         followed by a filter() that does create a clone.
1301         """
1302         self._sticky_filter = True
1303         return self
1304 
1305     def _merge_sanity_check(self, other):
1306         """Check that two QuerySet classes may be merged."""
1307         if self._fields is not None and (
1308                 set(self.query.values_select) != set(other.query.values_select) or
1309                 set(self.query.extra_select) != set(other.query.extra_select) or
1310                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1311             raise TypeError(
1312                 "Merging '%s' classes must involve the same values in each case."
1313                 % self.__class__.__name__
1314             )
1315 
1316     def _merge_known_related_objects(self, other):
1317         """
1318         Keep track of all known related objects from either QuerySet instance.
1319         """
1320         for field, objects in other._known_related_objects.items():
1321             self._known_related_objects.setdefault(field, {}).update(objects)
1322 
1323     def resolve_expression(self, *args, **kwargs):
1324         if self._fields and len(self._fields) > 1:
1325             # values() queryset can only be used as nested queries
1326             # if they are set up to select only a single field.
1327             raise TypeError('Cannot use multi-field values as a filter value.')
1328         query = self.query.resolve_expression(*args, **kwargs)
1329         query._db = self._db
1330         return query
1331     resolve_expression.queryset_only = True
1332 
1333     def _add_hints(self, **hints):
1334         """
1335         Update hinting information for use by routers. Add new key/values or
1336         overwrite existing key/values.
1337         """
1338         self._hints.update(hints)
1339 
1340     def _has_filters(self):
1341         """
1342         Check if this QuerySet has any filtering going on. This isn't
1343         equivalent with checking if all objects are present in results, for
1344         example, qs[1:]._has_filters() -> False.
1345         """
1346         return self.query.has_filters()
1347 
1348     @staticmethod
1349     def _validate_values_are_expressions(values, method_name):
1350         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1351         if invalid_args:
1352             raise TypeError(
1353                 'QuerySet.%s() received non-expression(s): %s.' % (
1354                     method_name,
1355                     ', '.join(invalid_args),
1356                 )
1357             )
1358 
1359     def _not_support_combined_queries(self, operation_name):
1360         if self.query.combinator:
1361             raise NotSupportedError(
1362                 'Calling QuerySet.%s() after %s() is not supported.'
1363                 % (operation_name, self.query.combinator)
1364             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-17 11:37:21.290 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-17 11:37:21.290 | INFO     | Search-only: skipping patch selection
2025-10-17 11:37:21.428 | INFO     | Task django__django-12496 completed successfully.
2025-10-17 11:37:21.429 | INFO     | No patch generated. You can try running ACR again.
