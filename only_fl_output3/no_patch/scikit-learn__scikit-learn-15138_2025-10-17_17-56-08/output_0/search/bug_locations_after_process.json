[
    {
        "rel_file_path": "sklearn/ensemble/_stacking.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/ensemble/_stacking.py",
        "start": 346,
        "end": 355,
        "class_name": "StackingClassifier",
        "method_name": "__init__",
        "code": "346     def __init__(self, estimators, final_estimator=None, cv=None,\n347                  stack_method='auto', n_jobs=None, verbose=0):\n348         super().__init__(\n349             estimators=estimators,\n350             final_estimator=final_estimator,\n351             cv=cv,\n352             stack_method=stack_method,\n353             n_jobs=n_jobs,\n354             verbose=verbose\n355         )\n",
        "intended_behavior": "Update the __init__ method to introduce a new boolean parameter named use_original_features with a default value of False. This parameter will control whether the original dataset features should be included alongside the base estimators' predictions when training the final_estimator."
    },
    {
        "rel_file_path": "sklearn/ensemble/_stacking.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/ensemble/_stacking.py",
        "start": 225,
        "end": 470,
        "class_name": "StackingClassifier",
        "method_name": null,
        "code": "225 class StackingClassifier(ClassifierMixin, _BaseStacking):\n226     \"\"\"Stack of estimators with a final classifier.\n227 \n228     Stacked generalization consists in stacking the output of individual\n229     estimator and use a classifier to compute the final prediction. Stacking\n230     allows to use the strength of each individual estimator by using their\n231     output as input of a final estimator.\n232 \n233     Note that `estimators_` are fitted on the full `X` while `final_estimator_`\n234     is trained using cross-validated predictions of the base estimators using\n235     `cross_val_predict`.\n236 \n237     .. versionadded:: 0.22\n238 \n239     Read more in the :ref:`User Guide <stacking>`.\n240 \n241     Parameters\n242     ----------\n243     estimators : list of (str, estimator)\n244         Base estimators which will be stacked together. Each element of the\n245         list is defined as a tuple of string (i.e. name) and an estimator\n246         instance. An estimator can be set to 'drop' using `set_params`.\n247 \n248     final_estimator : estimator, default=None\n249         A classifier which will be used to combine the base estimators.\n250         The default classifier is a `LogisticRegression`.\n251 \n252     cv : int, cross-validation generator or an iterable, default=None\n253         Determines the cross-validation splitting strategy used in\n254         `cross_val_predict` to train `final_estimator`. Possible inputs for\n255         cv are:\n256 \n257         * None, to use the default 5-fold cross validation,\n258         * integer, to specify the number of folds in a (Stratified) KFold,\n259         * An object to be used as a cross-validation generator,\n260         * An iterable yielding train, test splits.\n261 \n262         For integer/None inputs, if the estimator is a classifier and y is\n263         either binary or multiclass, `StratifiedKFold` is used. In all other\n264         cases, `KFold` is used.\n265 \n266         Refer :ref:`User Guide <cross_validation>` for the various\n267         cross-validation strategies that can be used here.\n268 \n269         .. note::\n270            A larger number of split will provide no benefits if the number\n271            of training samples is large enough. Indeed, the training time\n272            will increase. ``cv`` is not used for model evaluation but for\n273            prediction.\n274 \n275     stack_method : {'auto', 'predict_proba', 'decision_function', 'predict'}, \\\n276             default='auto'\n277         Methods called for each base estimator. It can be:\n278 \n279         * if 'auto', it will try to invoke, for each estimator,\n280           `'predict_proba'`, `'decision_function'` or `'predict'` in that\n281           order.\n282         * otherwise, one of `'predict_proba'`, `'decision_function'` or\n283           `'predict'`. If the method is not implemented by the estimator, it\n284           will raise an error.\n285 \n286     n_jobs : int, default=None\n287         The number of jobs to run in parallel all `estimators` `fit`.\n288         `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n289         using all processors. See Glossary for more details.\n290 \n291     Attributes\n292     ----------\n293     estimators_ : list of estimators\n294         The elements of the estimators parameter, having been fitted on the\n295         training data. If an estimator has been set to `'drop'`, it\n296         will not appear in `estimators_`.\n297 \n298     named_estimators_ : Bunch\n299         Attribute to access any fitted sub-estimators by name.\n300 \n301     final_estimator_ : estimator\n302         The classifier which predicts given the output of `estimators_`.\n303 \n304     stack_method_ : list of str\n305         The method used by each base estimator.\n306 \n307     Notes\n308     -----\n309     When `predict_proba` is used by each estimator (i.e. most of the time for\n310     `stack_method='auto'` or specifically for `stack_method='predict_proba'`),\n311     The first column predicted by each estimator will be dropped in the case\n312     of a binary classification problem. Indeed, both feature will be perfectly\n313     collinear.\n314 \n315     References\n316     ----------\n317     .. [1] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2\n318        (1992): 241-259.\n319 \n320     Examples\n321     --------\n322     >>> from sklearn.datasets import load_iris\n323     >>> from sklearn.ensemble import RandomForestClassifier\n324     >>> from sklearn.svm import LinearSVC\n325     >>> from sklearn.linear_model import LogisticRegression\n326     >>> from sklearn.preprocessing import StandardScaler\n327     >>> from sklearn.pipeline import make_pipeline\n328     >>> from sklearn.ensemble import StackingClassifier\n329     >>> X, y = load_iris(return_X_y=True)\n330     >>> estimators = [\n331     ...     ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n332     ...     ('svr', make_pipeline(StandardScaler(),\n333     ...                           LinearSVC(random_state=42)))\n334     ... ]\n335     >>> clf = StackingClassifier(\n336     ...     estimators=estimators, final_estimator=LogisticRegression()\n337     ... )\n338     >>> from sklearn.model_selection import train_test_split\n339     >>> X_train, X_test, y_train, y_test = train_test_split(\n340     ...     X, y, stratify=y, random_state=42\n341     ... )\n342     >>> clf.fit(X_train, y_train).score(X_test, y_test)\n343     0.9...\n344 \n345     \"\"\"\n346     def __init__(self, estimators, final_estimator=None, cv=None,\n347                  stack_method='auto', n_jobs=None, verbose=0):\n348         super().__init__(\n349             estimators=estimators,\n350             final_estimator=final_estimator,\n351             cv=cv,\n352             stack_method=stack_method,\n353             n_jobs=n_jobs,\n354             verbose=verbose\n355         )\n356 \n357     def _validate_final_estimator(self):\n358         self._clone_final_estimator(default=LogisticRegression())\n359         if not is_classifier(self.final_estimator_):\n360             raise ValueError(\n361                 \"'final_estimator' parameter should be a classifier. Got {}\"\n362                 .format(self.final_estimator_)\n363             )\n364 \n365     def fit(self, X, y, sample_weight=None):\n366         \"\"\"Fit the estimators.\n367 \n368         Parameters\n369         ----------\n370         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n371             Training vectors, where `n_samples` is the number of samples and\n372             `n_features` is the number of features.\n373 \n374         y : array-like of shape (n_samples,)\n375             Target values.\n376 \n377         sample_weight : array-like of shape (n_samples,) or None\n378             Sample weights. If None, then samples are equally weighted.\n379             Note that this is supported only if all underlying estimators\n380             support sample weights.\n381 \n382         Returns\n383         -------\n384         self : object\n385         \"\"\"\n386         check_classification_targets(y)\n387         self._le = LabelEncoder().fit(y)\n388         self.classes_ = self._le.classes_\n389         return super().fit(X, self._le.transform(y), sample_weight)\n390 \n391     @if_delegate_has_method(delegate='final_estimator_')\n392     def predict(self, X, **predict_params):\n393         \"\"\"Predict target for X.\n394 \n395         Parameters\n396         ----------\n397         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n398             Training vectors, where n_samples is the number of samples and\n399             n_features is the number of features.\n400 \n401         **predict_params : dict of str -> obj\n402             Parameters to the `predict` called by the `final_estimator`. Note\n403             that this may be used to return uncertainties from some estimators\n404             with `return_std` or `return_cov`. Be aware that it will only\n405             accounts for uncertainty in the final estimator.\n406 \n407         Returns\n408         -------\n409         y_pred : ndarray of shape (n_samples,) or (n_samples, n_output)\n410             Predicted targets.\n411         \"\"\"\n412         y_pred = super().predict(X, **predict_params)\n413         return self._le.inverse_transform(y_pred)\n414 \n415     @if_delegate_has_method(delegate='final_estimator_')\n416     def predict_proba(self, X):\n417         \"\"\"Predict class probabilities for X using\n418         `final_estimator_.predict_proba`.\n419 \n420         Parameters\n421         ----------\n422         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n423             Training vectors, where n_samples is the number of samples and\n424             n_features is the number of features.\n425 \n426         Returns\n427         -------\n428         probabilities : ndarray of shape (n_samples, n_classes) or \\\n429             list of ndarray of shape (n_output,)\n430             The class probabilities of the input samples.\n431         \"\"\"\n432         check_is_fitted(self)\n433         return self.final_estimator_.predict_proba(self.transform(X))\n434 \n435     @if_delegate_has_method(delegate='final_estimator_')\n436     def decision_function(self, X):\n437         \"\"\"Predict decision function for samples in X using\n438         `final_estimator_.decision_function`.\n439 \n440         Parameters\n441         ----------\n442         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n443             Training vectors, where n_samples is the number of samples and\n444             n_features is the number of features.\n445 \n446         Returns\n447         -------\n448         decisions : ndarray of shape (n_samples,), (n_samples, n_classes), \\\n449             or (n_samples, n_classes * (n_classes-1) / 2)\n450             The decision function computed the final estimator.\n451         \"\"\"\n452         check_is_fitted(self)\n453         return self.final_estimator_.decision_function(self.transform(X))\n454 \n455     def transform(self, X):\n456         \"\"\"Return class labels or probabilities for X for each estimator.\n457 \n458         Parameters\n459         ----------\n460         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n461             Training vectors, where `n_samples` is the number of samples and\n462             `n_features` is the number of features.\n463 \n464         Returns\n465         -------\n466         y_preds : ndarray of shape (n_samples, n_estimators) or \\\n467                 (n_samples, n_classes * n_estimators)\n468             Prediction outputs for each estimator.\n469         \"\"\"\n470         return self._transform(X)\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "sklearn/ensemble/_stacking.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/ensemble/_stacking.py",
        "start": 365,
        "end": 389,
        "class_name": "StackingClassifier",
        "method_name": "fit",
        "code": "365     def fit(self, X, y, sample_weight=None):\n366         \"\"\"Fit the estimators.\n367 \n368         Parameters\n369         ----------\n370         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n371             Training vectors, where `n_samples` is the number of samples and\n372             `n_features` is the number of features.\n373 \n374         y : array-like of shape (n_samples,)\n375             Target values.\n376 \n377         sample_weight : array-like of shape (n_samples,) or None\n378             Sample weights. If None, then samples are equally weighted.\n379             Note that this is supported only if all underlying estimators\n380             support sample weights.\n381 \n382         Returns\n383         -------\n384         self : object\n385         \"\"\"\n386         check_classification_targets(y)\n387         self._le = LabelEncoder().fit(y)\n388         self.classes_ = self._le.classes_\n389         return super().fit(X, self._le.transform(y), sample_weight)\n",
        "intended_behavior": "Modify the fit method to conditionally concatenate the original dataset features with the predictions of the base estimators before passing them to the final_estimator_.fit() method. This action is contingent on the use_original_features parameter. If use_original_features is True, the method will concatenate the original dataset features to the predictions array (X_meta). This ensures the final_estimator is trained using both the predictions of the base estimators and the original dataset features, addressing the new feature request. The implementation must preserve the existing behavior when use_original_features is False."
    },
    {
        "rel_file_path": "sklearn/ensemble/_stacking.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/ensemble/_stacking.py",
        "start": 225,
        "end": 470,
        "class_name": "StackingClassifier",
        "method_name": null,
        "code": "225 class StackingClassifier(ClassifierMixin, _BaseStacking):\n226     \"\"\"Stack of estimators with a final classifier.\n227 \n228     Stacked generalization consists in stacking the output of individual\n229     estimator and use a classifier to compute the final prediction. Stacking\n230     allows to use the strength of each individual estimator by using their\n231     output as input of a final estimator.\n232 \n233     Note that `estimators_` are fitted on the full `X` while `final_estimator_`\n234     is trained using cross-validated predictions of the base estimators using\n235     `cross_val_predict`.\n236 \n237     .. versionadded:: 0.22\n238 \n239     Read more in the :ref:`User Guide <stacking>`.\n240 \n241     Parameters\n242     ----------\n243     estimators : list of (str, estimator)\n244         Base estimators which will be stacked together. Each element of the\n245         list is defined as a tuple of string (i.e. name) and an estimator\n246         instance. An estimator can be set to 'drop' using `set_params`.\n247 \n248     final_estimator : estimator, default=None\n249         A classifier which will be used to combine the base estimators.\n250         The default classifier is a `LogisticRegression`.\n251 \n252     cv : int, cross-validation generator or an iterable, default=None\n253         Determines the cross-validation splitting strategy used in\n254         `cross_val_predict` to train `final_estimator`. Possible inputs for\n255         cv are:\n256 \n257         * None, to use the default 5-fold cross validation,\n258         * integer, to specify the number of folds in a (Stratified) KFold,\n259         * An object to be used as a cross-validation generator,\n260         * An iterable yielding train, test splits.\n261 \n262         For integer/None inputs, if the estimator is a classifier and y is\n263         either binary or multiclass, `StratifiedKFold` is used. In all other\n264         cases, `KFold` is used.\n265 \n266         Refer :ref:`User Guide <cross_validation>` for the various\n267         cross-validation strategies that can be used here.\n268 \n269         .. note::\n270            A larger number of split will provide no benefits if the number\n271            of training samples is large enough. Indeed, the training time\n272            will increase. ``cv`` is not used for model evaluation but for\n273            prediction.\n274 \n275     stack_method : {'auto', 'predict_proba', 'decision_function', 'predict'}, \\\n276             default='auto'\n277         Methods called for each base estimator. It can be:\n278 \n279         * if 'auto', it will try to invoke, for each estimator,\n280           `'predict_proba'`, `'decision_function'` or `'predict'` in that\n281           order.\n282         * otherwise, one of `'predict_proba'`, `'decision_function'` or\n283           `'predict'`. If the method is not implemented by the estimator, it\n284           will raise an error.\n285 \n286     n_jobs : int, default=None\n287         The number of jobs to run in parallel all `estimators` `fit`.\n288         `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n289         using all processors. See Glossary for more details.\n290 \n291     Attributes\n292     ----------\n293     estimators_ : list of estimators\n294         The elements of the estimators parameter, having been fitted on the\n295         training data. If an estimator has been set to `'drop'`, it\n296         will not appear in `estimators_`.\n297 \n298     named_estimators_ : Bunch\n299         Attribute to access any fitted sub-estimators by name.\n300 \n301     final_estimator_ : estimator\n302         The classifier which predicts given the output of `estimators_`.\n303 \n304     stack_method_ : list of str\n305         The method used by each base estimator.\n306 \n307     Notes\n308     -----\n309     When `predict_proba` is used by each estimator (i.e. most of the time for\n310     `stack_method='auto'` or specifically for `stack_method='predict_proba'`),\n311     The first column predicted by each estimator will be dropped in the case\n312     of a binary classification problem. Indeed, both feature will be perfectly\n313     collinear.\n314 \n315     References\n316     ----------\n317     .. [1] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2\n318        (1992): 241-259.\n319 \n320     Examples\n321     --------\n322     >>> from sklearn.datasets import load_iris\n323     >>> from sklearn.ensemble import RandomForestClassifier\n324     >>> from sklearn.svm import LinearSVC\n325     >>> from sklearn.linear_model import LogisticRegression\n326     >>> from sklearn.preprocessing import StandardScaler\n327     >>> from sklearn.pipeline import make_pipeline\n328     >>> from sklearn.ensemble import StackingClassifier\n329     >>> X, y = load_iris(return_X_y=True)\n330     >>> estimators = [\n331     ...     ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n332     ...     ('svr', make_pipeline(StandardScaler(),\n333     ...                           LinearSVC(random_state=42)))\n334     ... ]\n335     >>> clf = StackingClassifier(\n336     ...     estimators=estimators, final_estimator=LogisticRegression()\n337     ... )\n338     >>> from sklearn.model_selection import train_test_split\n339     >>> X_train, X_test, y_train, y_test = train_test_split(\n340     ...     X, y, stratify=y, random_state=42\n341     ... )\n342     >>> clf.fit(X_train, y_train).score(X_test, y_test)\n343     0.9...\n344 \n345     \"\"\"\n346     def __init__(self, estimators, final_estimator=None, cv=None,\n347                  stack_method='auto', n_jobs=None, verbose=0):\n348         super().__init__(\n349             estimators=estimators,\n350             final_estimator=final_estimator,\n351             cv=cv,\n352             stack_method=stack_method,\n353             n_jobs=n_jobs,\n354             verbose=verbose\n355         )\n356 \n357     def _validate_final_estimator(self):\n358         self._clone_final_estimator(default=LogisticRegression())\n359         if not is_classifier(self.final_estimator_):\n360             raise ValueError(\n361                 \"'final_estimator' parameter should be a classifier. Got {}\"\n362                 .format(self.final_estimator_)\n363             )\n364 \n365     def fit(self, X, y, sample_weight=None):\n366         \"\"\"Fit the estimators.\n367 \n368         Parameters\n369         ----------\n370         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n371             Training vectors, where `n_samples` is the number of samples and\n372             `n_features` is the number of features.\n373 \n374         y : array-like of shape (n_samples,)\n375             Target values.\n376 \n377         sample_weight : array-like of shape (n_samples,) or None\n378             Sample weights. If None, then samples are equally weighted.\n379             Note that this is supported only if all underlying estimators\n380             support sample weights.\n381 \n382         Returns\n383         -------\n384         self : object\n385         \"\"\"\n386         check_classification_targets(y)\n387         self._le = LabelEncoder().fit(y)\n388         self.classes_ = self._le.classes_\n389         return super().fit(X, self._le.transform(y), sample_weight)\n390 \n391     @if_delegate_has_method(delegate='final_estimator_')\n392     def predict(self, X, **predict_params):\n393         \"\"\"Predict target for X.\n394 \n395         Parameters\n396         ----------\n397         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n398             Training vectors, where n_samples is the number of samples and\n399             n_features is the number of features.\n400 \n401         **predict_params : dict of str -> obj\n402             Parameters to the `predict` called by the `final_estimator`. Note\n403             that this may be used to return uncertainties from some estimators\n404             with `return_std` or `return_cov`. Be aware that it will only\n405             accounts for uncertainty in the final estimator.\n406 \n407         Returns\n408         -------\n409         y_pred : ndarray of shape (n_samples,) or (n_samples, n_output)\n410             Predicted targets.\n411         \"\"\"\n412         y_pred = super().predict(X, **predict_params)\n413         return self._le.inverse_transform(y_pred)\n414 \n415     @if_delegate_has_method(delegate='final_estimator_')\n416     def predict_proba(self, X):\n417         \"\"\"Predict class probabilities for X using\n418         `final_estimator_.predict_proba`.\n419 \n420         Parameters\n421         ----------\n422         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n423             Training vectors, where n_samples is the number of samples and\n424             n_features is the number of features.\n425 \n426         Returns\n427         -------\n428         probabilities : ndarray of shape (n_samples, n_classes) or \\\n429             list of ndarray of shape (n_output,)\n430             The class probabilities of the input samples.\n431         \"\"\"\n432         check_is_fitted(self)\n433         return self.final_estimator_.predict_proba(self.transform(X))\n434 \n435     @if_delegate_has_method(delegate='final_estimator_')\n436     def decision_function(self, X):\n437         \"\"\"Predict decision function for samples in X using\n438         `final_estimator_.decision_function`.\n439 \n440         Parameters\n441         ----------\n442         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n443             Training vectors, where n_samples is the number of samples and\n444             n_features is the number of features.\n445 \n446         Returns\n447         -------\n448         decisions : ndarray of shape (n_samples,), (n_samples, n_classes), \\\n449             or (n_samples, n_classes * (n_classes-1) / 2)\n450             The decision function computed the final estimator.\n451         \"\"\"\n452         check_is_fitted(self)\n453         return self.final_estimator_.decision_function(self.transform(X))\n454 \n455     def transform(self, X):\n456         \"\"\"Return class labels or probabilities for X for each estimator.\n457 \n458         Parameters\n459         ----------\n460         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n461             Training vectors, where `n_samples` is the number of samples and\n462             `n_features` is the number of features.\n463 \n464         Returns\n465         -------\n466         y_preds : ndarray of shape (n_samples, n_estimators) or \\\n467                 (n_samples, n_classes * n_estimators)\n468             Prediction outputs for each estimator.\n469         \"\"\"\n470         return self._transform(X)\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "sklearn/ensemble/_stacking.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/ensemble/_stacking.py",
        "start": 571,
        "end": 580,
        "class_name": "StackingRegressor",
        "method_name": "__init__",
        "code": "571     def __init__(self, estimators, final_estimator=None, cv=None, n_jobs=None,\n572                  verbose=0):\n573         super().__init__(\n574             estimators=estimators,\n575             final_estimator=final_estimator,\n576             cv=cv,\n577             stack_method=\"predict\",\n578             n_jobs=n_jobs,\n579             verbose=verbose\n580         )\n",
        "intended_behavior": "Similarly, update the __init__ method of the StackingRegressor class (if applicable, based on the issue's relevance to both classifiers and regressors) to include the use_original_features parameter, allowing users to decide whether the original features should be used in training the final estimator. This matches the functionality added to the StackingClassifier."
    },
    {
        "rel_file_path": "sklearn/ensemble/_stacking.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/ensemble/_stacking.py",
        "start": 473,
        "end": 628,
        "class_name": "StackingRegressor",
        "method_name": null,
        "code": "473 class StackingRegressor(RegressorMixin, _BaseStacking):\n474     \"\"\"Stack of estimators with a final regressor.\n475 \n476     Stacked generalization consists in stacking the output of individual\n477     estimator and use a regressor to compute the final prediction. Stacking\n478     allows to use the strength of each individual estimator by using their\n479     output as input of a final estimator.\n480 \n481     Note that `estimators_` are fitted on the full `X` while `final_estimator_`\n482     is trained using cross-validated predictions of the base estimators using\n483     `cross_val_predict`.\n484 \n485     .. versionadded:: 0.22\n486 \n487     Read more in the :ref:`User Guide <stacking>`.\n488 \n489     Parameters\n490     ----------\n491     estimators : list of (str, estimator)\n492         Base estimators which will be stacked together. Each element of the\n493         list is defined as a tuple of string (i.e. name) and an estimator\n494         instance. An estimator can be set to 'drop' using `set_params`.\n495 \n496     final_estimator : estimator, default=None\n497         A regressor which will be used to combine the base estimators.\n498         The default regressor is a `RidgeCV`.\n499 \n500     cv : int, cross-validation generator or an iterable, default=None\n501         Determines the cross-validation splitting strategy used in\n502         `cross_val_predict` to train `final_estimator`. Possible inputs for\n503         cv are:\n504 \n505         * None, to use the default 5-fold cross validation,\n506         * integer, to specify the number of folds in a (Stratified) KFold,\n507         * An object to be used as a cross-validation generator,\n508         * An iterable yielding train, test splits.\n509 \n510         For integer/None inputs, if the estimator is a classifier and y is\n511         either binary or multiclass, `StratifiedKFold` is used. In all other\n512         cases, `KFold` is used.\n513 \n514         Refer :ref:`User Guide <cross_validation>` for the various\n515         cross-validation strategies that can be used here.\n516 \n517         .. note::\n518            A larger number of split will provide no benefits if the number\n519            of training samples is large enough. Indeed, the training time\n520            will increase. ``cv`` is not used for model evaluation but for\n521            prediction.\n522 \n523     n_jobs : int, default=None\n524         The number of jobs to run in parallel for `fit` of all `estimators`.\n525         `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n526         using all processors. See Glossary for more details.\n527 \n528     Attributes\n529     ----------\n530     estimators_ : list of estimator\n531         The elements of the estimators parameter, having been fitted on the\n532         training data. If an estimator has been set to `'drop'`, it\n533         will not appear in `estimators_`.\n534 \n535     named_estimators_ : Bunch\n536         Attribute to access any fitted sub-estimators by name.\n537 \n538     final_estimator_ : estimator\n539         The regressor to stacked the base estimators fitted.\n540 \n541     References\n542     ----------\n543     .. [1] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2\n544        (1992): 241-259.\n545 \n546     Examples\n547     --------\n548     >>> from sklearn.datasets import load_diabetes\n549     >>> from sklearn.linear_model import RidgeCV\n550     >>> from sklearn.svm import LinearSVR\n551     >>> from sklearn.ensemble import RandomForestRegressor\n552     >>> from sklearn.ensemble import StackingRegressor\n553     >>> X, y = load_diabetes(return_X_y=True)\n554     >>> estimators = [\n555     ...     ('lr', RidgeCV()),\n556     ...     ('svr', LinearSVR(random_state=42))\n557     ... ]\n558     >>> reg = StackingRegressor(\n559     ...     estimators=estimators,\n560     ...     final_estimator=RandomForestRegressor(n_estimators=10,\n561     ...                                           random_state=42)\n562     ... )\n563     >>> from sklearn.model_selection import train_test_split\n564     >>> X_train, X_test, y_train, y_test = train_test_split(\n565     ...     X, y, random_state=42\n566     ... )\n567     >>> reg.fit(X_train, y_train).score(X_test, y_test)\n568     0.3...\n569 \n570     \"\"\"\n571     def __init__(self, estimators, final_estimator=None, cv=None, n_jobs=None,\n572                  verbose=0):\n573         super().__init__(\n574             estimators=estimators,\n575             final_estimator=final_estimator,\n576             cv=cv,\n577             stack_method=\"predict\",\n578             n_jobs=n_jobs,\n579             verbose=verbose\n580         )\n581 \n582     def _validate_final_estimator(self):\n583         self._clone_final_estimator(default=RidgeCV())\n584         if not is_regressor(self.final_estimator_):\n585             raise ValueError(\n586                 \"'final_estimator' parameter should be a regressor. Got {}\"\n587                 .format(self.final_estimator_)\n588             )\n589 \n590     def fit(self, X, y, sample_weight=None):\n591         \"\"\"Fit the estimators.\n592 \n593         Parameters\n594         ----------\n595         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n596             Training vectors, where n_samples is the number of samples and\n597             n_features is the number of features.\n598 \n599         y : array-like of shape (n_samples,)\n600             Target values.\n601 \n602         sample_weight : array-like of shape (n_samples,) or None\n603             Sample weights. If None, then samples are equally weighted.\n604             Note that this is supported only if all underlying estimators\n605             support sample weights.\n606 \n607         Returns\n608         -------\n609         self : object\n610         \"\"\"\n611         y = column_or_1d(y, warn=True)\n612         return super().fit(X, y, sample_weight)\n613 \n614     def transform(self, X):\n615         \"\"\"Return the predictions for X for each estimator.\n616 \n617         Parameters\n618         ----------\n619         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n620             Training vectors, where `n_samples` is the number of samples and\n621             `n_features` is the number of features.\n622 \n623         Returns\n624         -------\n625         y_preds : ndarray of shape (n_samples, n_estimators)\n626             Prediction outputs for each estimator.\n627         \"\"\"\n628         return self._transform(X)\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "sklearn/ensemble/_stacking.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/ensemble/_stacking.py",
        "start": 590,
        "end": 612,
        "class_name": "StackingRegressor",
        "method_name": "fit",
        "code": "590     def fit(self, X, y, sample_weight=None):\n591         \"\"\"Fit the estimators.\n592 \n593         Parameters\n594         ----------\n595         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n596             Training vectors, where n_samples is the number of samples and\n597             n_features is the number of features.\n598 \n599         y : array-like of shape (n_samples,)\n600             Target values.\n601 \n602         sample_weight : array-like of shape (n_samples,) or None\n603             Sample weights. If None, then samples are equally weighted.\n604             Note that this is supported only if all underlying estimators\n605             support sample weights.\n606 \n607         Returns\n608         -------\n609         self : object\n610         \"\"\"\n611         y = column_or_1d(y, warn=True)\n612         return super().fit(X, y, sample_weight)\n",
        "intended_behavior": "If the use_original_features option is deemed valuable for regression scenarios as well, modify the fit method in the StackingRegressor class to implement logic similar to that in the StackingClassifier, adjusting for regression-specific processing as necessary. This involves checking the use_original_features flag and appropriately concatenating dataset features with predictions before training the final estimator."
    },
    {
        "rel_file_path": "sklearn/ensemble/_stacking.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/ensemble/_stacking.py",
        "start": 473,
        "end": 628,
        "class_name": "StackingRegressor",
        "method_name": null,
        "code": "473 class StackingRegressor(RegressorMixin, _BaseStacking):\n474     \"\"\"Stack of estimators with a final regressor.\n475 \n476     Stacked generalization consists in stacking the output of individual\n477     estimator and use a regressor to compute the final prediction. Stacking\n478     allows to use the strength of each individual estimator by using their\n479     output as input of a final estimator.\n480 \n481     Note that `estimators_` are fitted on the full `X` while `final_estimator_`\n482     is trained using cross-validated predictions of the base estimators using\n483     `cross_val_predict`.\n484 \n485     .. versionadded:: 0.22\n486 \n487     Read more in the :ref:`User Guide <stacking>`.\n488 \n489     Parameters\n490     ----------\n491     estimators : list of (str, estimator)\n492         Base estimators which will be stacked together. Each element of the\n493         list is defined as a tuple of string (i.e. name) and an estimator\n494         instance. An estimator can be set to 'drop' using `set_params`.\n495 \n496     final_estimator : estimator, default=None\n497         A regressor which will be used to combine the base estimators.\n498         The default regressor is a `RidgeCV`.\n499 \n500     cv : int, cross-validation generator or an iterable, default=None\n501         Determines the cross-validation splitting strategy used in\n502         `cross_val_predict` to train `final_estimator`. Possible inputs for\n503         cv are:\n504 \n505         * None, to use the default 5-fold cross validation,\n506         * integer, to specify the number of folds in a (Stratified) KFold,\n507         * An object to be used as a cross-validation generator,\n508         * An iterable yielding train, test splits.\n509 \n510         For integer/None inputs, if the estimator is a classifier and y is\n511         either binary or multiclass, `StratifiedKFold` is used. In all other\n512         cases, `KFold` is used.\n513 \n514         Refer :ref:`User Guide <cross_validation>` for the various\n515         cross-validation strategies that can be used here.\n516 \n517         .. note::\n518            A larger number of split will provide no benefits if the number\n519            of training samples is large enough. Indeed, the training time\n520            will increase. ``cv`` is not used for model evaluation but for\n521            prediction.\n522 \n523     n_jobs : int, default=None\n524         The number of jobs to run in parallel for `fit` of all `estimators`.\n525         `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n526         using all processors. See Glossary for more details.\n527 \n528     Attributes\n529     ----------\n530     estimators_ : list of estimator\n531         The elements of the estimators parameter, having been fitted on the\n532         training data. If an estimator has been set to `'drop'`, it\n533         will not appear in `estimators_`.\n534 \n535     named_estimators_ : Bunch\n536         Attribute to access any fitted sub-estimators by name.\n537 \n538     final_estimator_ : estimator\n539         The regressor to stacked the base estimators fitted.\n540 \n541     References\n542     ----------\n543     .. [1] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2\n544        (1992): 241-259.\n545 \n546     Examples\n547     --------\n548     >>> from sklearn.datasets import load_diabetes\n549     >>> from sklearn.linear_model import RidgeCV\n550     >>> from sklearn.svm import LinearSVR\n551     >>> from sklearn.ensemble import RandomForestRegressor\n552     >>> from sklearn.ensemble import StackingRegressor\n553     >>> X, y = load_diabetes(return_X_y=True)\n554     >>> estimators = [\n555     ...     ('lr', RidgeCV()),\n556     ...     ('svr', LinearSVR(random_state=42))\n557     ... ]\n558     >>> reg = StackingRegressor(\n559     ...     estimators=estimators,\n560     ...     final_estimator=RandomForestRegressor(n_estimators=10,\n561     ...                                           random_state=42)\n562     ... )\n563     >>> from sklearn.model_selection import train_test_split\n564     >>> X_train, X_test, y_train, y_test = train_test_split(\n565     ...     X, y, random_state=42\n566     ... )\n567     >>> reg.fit(X_train, y_train).score(X_test, y_test)\n568     0.3...\n569 \n570     \"\"\"\n571     def __init__(self, estimators, final_estimator=None, cv=None, n_jobs=None,\n572                  verbose=0):\n573         super().__init__(\n574             estimators=estimators,\n575             final_estimator=final_estimator,\n576             cv=cv,\n577             stack_method=\"predict\",\n578             n_jobs=n_jobs,\n579             verbose=verbose\n580         )\n581 \n582     def _validate_final_estimator(self):\n583         self._clone_final_estimator(default=RidgeCV())\n584         if not is_regressor(self.final_estimator_):\n585             raise ValueError(\n586                 \"'final_estimator' parameter should be a regressor. Got {}\"\n587                 .format(self.final_estimator_)\n588             )\n589 \n590     def fit(self, X, y, sample_weight=None):\n591         \"\"\"Fit the estimators.\n592 \n593         Parameters\n594         ----------\n595         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n596             Training vectors, where n_samples is the number of samples and\n597             n_features is the number of features.\n598 \n599         y : array-like of shape (n_samples,)\n600             Target values.\n601 \n602         sample_weight : array-like of shape (n_samples,) or None\n603             Sample weights. If None, then samples are equally weighted.\n604             Note that this is supported only if all underlying estimators\n605             support sample weights.\n606 \n607         Returns\n608         -------\n609         self : object\n610         \"\"\"\n611         y = column_or_1d(y, warn=True)\n612         return super().fit(X, y, sample_weight)\n613 \n614     def transform(self, X):\n615         \"\"\"Return the predictions for X for each estimator.\n616 \n617         Parameters\n618         ----------\n619         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n620             Training vectors, where `n_samples` is the number of samples and\n621             `n_features` is the number of features.\n622 \n623         Returns\n624         -------\n625         y_preds : ndarray of shape (n_samples, n_estimators)\n626             Prediction outputs for each estimator.\n627         \"\"\"\n628         return self._transform(X)\n",
        "intended_behavior": "This class provides additional context to the issue."
    }
]