[
    {
        "rel_file_path": "django/contrib/admin/views/main.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__4.0/django/contrib/admin/views/main.py",
        "start": 442,
        "end": 494,
        "class_name": "ChangeList",
        "method_name": "get_queryset",
        "code": "442     def get_queryset(self, request):\n443         # First, we collect all the declared list filters.\n444         (\n445             self.filter_specs,\n446             self.has_filters,\n447             remaining_lookup_params,\n448             filters_use_distinct,\n449             self.has_active_filters,\n450         ) = self.get_filters(request)\n451         # Then, we let every list filter modify the queryset to its liking.\n452         qs = self.root_queryset\n453         for filter_spec in self.filter_specs:\n454             new_qs = filter_spec.queryset(request, qs)\n455             if new_qs is not None:\n456                 qs = new_qs\n457 \n458         try:\n459             # Finally, we apply the remaining lookup parameters from the query\n460             # string (i.e. those that haven't already been processed by the\n461             # filters).\n462             qs = qs.filter(**remaining_lookup_params)\n463         except (SuspiciousOperation, ImproperlyConfigured):\n464             # Allow certain types of errors to be re-raised as-is so that the\n465             # caller can treat them in a special way.\n466             raise\n467         except Exception as e:\n468             # Every other error is caught with a naked except, because we don't\n469             # have any other way of validating lookup parameters. They might be\n470             # invalid if the keyword arguments are incorrect, or if the values\n471             # are not in the correct type, so we might get FieldError,\n472             # ValueError, ValidationError, or ?.\n473             raise IncorrectLookupParameters(e)\n474 \n475         if not qs.query.select_related:\n476             qs = self.apply_select_related(qs)\n477 \n478         # Set ordering.\n479         ordering = self.get_ordering(request, qs)\n480         qs = qs.order_by(*ordering)\n481 \n482         # Apply search results\n483         qs, search_use_distinct = self.model_admin.get_search_results(request, qs, self.query)\n484 \n485         # Set query string for clearing all filters.\n486         self.clear_all_filters_qs = self.get_query_string(\n487             new_params=remaining_lookup_params,\n488             remove=self.get_filters_params(),\n489         )\n490         # Remove duplicates from results, if necessary\n491         if filters_use_distinct | search_use_distinct:\n492             return qs.distinct()\n493         else:\n494             return qs\n",
        "intended_behavior": "Ensure that any application of `.distinct()` to the queryset in the context of a deletion operation is either avoided or handled in such a way that it does not prevent the `delete()` operation from succeeding. One approach could involve conditionally applying `.distinct()` only when it's certain that the following operation is not deletion. Alternatively, before deletion, clone the queryset without `.distinct()` if its presence is detected."
    },
    {
        "rel_file_path": "django/contrib/admin/views/main.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__4.0/django/contrib/admin/views/main.py",
        "start": 48,
        "end": 526,
        "class_name": "ChangeList",
        "method_name": null,
        "code": "48 class ChangeList:\n49     search_form_class = ChangeListSearchForm\n50 \n51     def __init__(self, request, model, list_display, list_display_links,\n52                  list_filter, date_hierarchy, search_fields, list_select_related,\n53                  list_per_page, list_max_show_all, list_editable, model_admin, sortable_by):\n54         self.model = model\n55         self.opts = model._meta\n56         self.lookup_opts = self.opts\n57         self.root_queryset = model_admin.get_queryset(request)\n58         self.list_display = list_display\n59         self.list_display_links = list_display_links\n60         self.list_filter = list_filter\n61         self.has_filters = None\n62         self.has_active_filters = None\n63         self.clear_all_filters_qs = None\n64         self.date_hierarchy = date_hierarchy\n65         self.search_fields = search_fields\n66         self.list_select_related = list_select_related\n67         self.list_per_page = list_per_page\n68         self.list_max_show_all = list_max_show_all\n69         self.model_admin = model_admin\n70         self.preserved_filters = model_admin.get_preserved_filters(request)\n71         self.sortable_by = sortable_by\n72 \n73         # Get search parameters from the query string.\n74         _search_form = self.search_form_class(request.GET)\n75         if not _search_form.is_valid():\n76             for error in _search_form.errors.values():\n77                 messages.error(request, ', '.join(error))\n78         self.query = _search_form.cleaned_data.get(SEARCH_VAR) or ''\n79         try:\n80             self.page_num = int(request.GET.get(PAGE_VAR, 1))\n81         except ValueError:\n82             self.page_num = 1\n83         self.show_all = ALL_VAR in request.GET\n84         self.is_popup = IS_POPUP_VAR in request.GET\n85         to_field = request.GET.get(TO_FIELD_VAR)\n86         if to_field and not model_admin.to_field_allowed(request, to_field):\n87             raise DisallowedModelAdminToField(\"The field %s cannot be referenced.\" % to_field)\n88         self.to_field = to_field\n89         self.params = dict(request.GET.items())\n90         if PAGE_VAR in self.params:\n91             del self.params[PAGE_VAR]\n92         if ERROR_FLAG in self.params:\n93             del self.params[ERROR_FLAG]\n94 \n95         if self.is_popup:\n96             self.list_editable = ()\n97         else:\n98             self.list_editable = list_editable\n99         self.queryset = self.get_queryset(request)\n100         self.get_results(request)\n101         if self.is_popup:\n102             title = gettext('Select %s')\n103         elif self.model_admin.has_change_permission(request):\n104             title = gettext('Select %s to change')\n105         else:\n106             title = gettext('Select %s to view')\n107         self.title = title % self.opts.verbose_name\n108         self.pk_attname = self.lookup_opts.pk.attname\n109 \n110     def get_filters_params(self, params=None):\n111         \"\"\"\n112         Return all params except IGNORED_PARAMS.\n113         \"\"\"\n114         params = params or self.params\n115         lookup_params = params.copy()  # a dictionary of the query string\n116         # Remove all the parameters that are globally and systematically\n117         # ignored.\n118         for ignored in IGNORED_PARAMS:\n119             if ignored in lookup_params:\n120                 del lookup_params[ignored]\n121         return lookup_params\n122 \n123     def get_filters(self, request):\n124         lookup_params = self.get_filters_params()\n125         use_distinct = False\n126         has_active_filters = False\n127 \n128         for key, value in lookup_params.items():\n129             if not self.model_admin.lookup_allowed(key, value):\n130                 raise DisallowedModelAdminLookup(\"Filtering by %s not allowed\" % key)\n131 \n132         filter_specs = []\n133         for list_filter in self.list_filter:\n134             lookup_params_count = len(lookup_params)\n135             if callable(list_filter):\n136                 # This is simply a custom list filter class.\n137                 spec = list_filter(request, lookup_params, self.model, self.model_admin)\n138             else:\n139                 field_path = None\n140                 if isinstance(list_filter, (tuple, list)):\n141                     # This is a custom FieldListFilter class for a given field.\n142                     field, field_list_filter_class = list_filter\n143                 else:\n144                     # This is simply a field name, so use the default\n145                     # FieldListFilter class that has been registered for the\n146                     # type of the given field.\n147                     field, field_list_filter_class = list_filter, FieldListFilter.create\n148                 if not isinstance(field, Field):\n149                     field_path = field\n150                     field = get_fields_from_path(self.model, field_path)[-1]\n151 \n152                 spec = field_list_filter_class(\n153                     field, request, lookup_params,\n154                     self.model, self.model_admin, field_path=field_path,\n155                 )\n156                 # field_list_filter_class removes any lookup_params it\n157                 # processes. If that happened, check if distinct() is needed to\n158                 # remove duplicate results.\n159                 if lookup_params_count > len(lookup_params):\n160                     use_distinct = use_distinct or lookup_needs_distinct(self.lookup_opts, field_path)\n161             if spec and spec.has_output():\n162                 filter_specs.append(spec)\n163                 if lookup_params_count > len(lookup_params):\n164                     has_active_filters = True\n165 \n166         if self.date_hierarchy:\n167             # Create bounded lookup parameters so that the query is more\n168             # efficient.\n169             year = lookup_params.pop('%s__year' % self.date_hierarchy, None)\n170             if year is not None:\n171                 month = lookup_params.pop('%s__month' % self.date_hierarchy, None)\n172                 day = lookup_params.pop('%s__day' % self.date_hierarchy, None)\n173                 try:\n174                     from_date = datetime(\n175                         int(year),\n176                         int(month if month is not None else 1),\n177                         int(day if day is not None else 1),\n178                     )\n179                 except ValueError as e:\n180                     raise IncorrectLookupParameters(e) from e\n181                 if day:\n182                     to_date = from_date + timedelta(days=1)\n183                 elif month:\n184                     # In this branch, from_date will always be the first of a\n185                     # month, so advancing 32 days gives the next month.\n186                     to_date = (from_date + timedelta(days=32)).replace(day=1)\n187                 else:\n188                     to_date = from_date.replace(year=from_date.year + 1)\n189                 if settings.USE_TZ:\n190                     from_date = make_aware(from_date)\n191                     to_date = make_aware(to_date)\n192                 lookup_params.update({\n193                     '%s__gte' % self.date_hierarchy: from_date,\n194                     '%s__lt' % self.date_hierarchy: to_date,\n195                 })\n196 \n197         # At this point, all the parameters used by the various ListFilters\n198         # have been removed from lookup_params, which now only contains other\n199         # parameters passed via the query string. We now loop through the\n200         # remaining parameters both to ensure that all the parameters are valid\n201         # fields and to determine if at least one of them needs distinct(). If\n202         # the lookup parameters aren't real fields, then bail out.\n203         try:\n204             for key, value in lookup_params.items():\n205                 lookup_params[key] = prepare_lookup_value(key, value)\n206                 use_distinct = use_distinct or lookup_needs_distinct(self.lookup_opts, key)\n207             return (\n208                 filter_specs, bool(filter_specs), lookup_params, use_distinct,\n209                 has_active_filters,\n210             )\n211         except FieldDoesNotExist as e:\n212             raise IncorrectLookupParameters(e) from e\n213 \n214     def get_query_string(self, new_params=None, remove=None):\n215         if new_params is None:\n216             new_params = {}\n217         if remove is None:\n218             remove = []\n219         p = self.params.copy()\n220         for r in remove:\n221             for k in list(p):\n222                 if k.startswith(r):\n223                     del p[k]\n224         for k, v in new_params.items():\n225             if v is None:\n226                 if k in p:\n227                     del p[k]\n228             else:\n229                 p[k] = v\n230         return '?%s' % urlencode(sorted(p.items()))\n231 \n232     def get_results(self, request):\n233         paginator = self.model_admin.get_paginator(request, self.queryset, self.list_per_page)\n234         # Get the number of objects, with admin filters applied.\n235         result_count = paginator.count\n236 \n237         # Get the total number of objects, with no admin filters applied.\n238         if self.model_admin.show_full_result_count:\n239             full_result_count = self.root_queryset.count()\n240         else:\n241             full_result_count = None\n242         can_show_all = result_count <= self.list_max_show_all\n243         multi_page = result_count > self.list_per_page\n244 \n245         # Get the list of objects to display on this page.\n246         if (self.show_all and can_show_all) or not multi_page:\n247             result_list = self.queryset._clone()\n248         else:\n249             try:\n250                 result_list = paginator.page(self.page_num).object_list\n251             except InvalidPage:\n252                 raise IncorrectLookupParameters\n253 \n254         self.result_count = result_count\n255         self.show_full_result_count = self.model_admin.show_full_result_count\n256         # Admin actions are shown if there is at least one entry\n257         # or if entries are not counted because show_full_result_count is disabled\n258         self.show_admin_actions = not self.show_full_result_count or bool(full_result_count)\n259         self.full_result_count = full_result_count\n260         self.result_list = result_list\n261         self.can_show_all = can_show_all\n262         self.multi_page = multi_page\n263         self.paginator = paginator\n264 \n265     def _get_default_ordering(self):\n266         ordering = []\n267         if self.model_admin.ordering:\n268             ordering = self.model_admin.ordering\n269         elif self.lookup_opts.ordering:\n270             ordering = self.lookup_opts.ordering\n271         return ordering\n272 \n273     def get_ordering_field(self, field_name):\n274         \"\"\"\n275         Return the proper model field name corresponding to the given\n276         field_name to use for ordering. field_name may either be the name of a\n277         proper model field or the name of a method (on the admin or model) or a\n278         callable with the 'admin_order_field' attribute. Return None if no\n279         proper model field name can be matched.\n280         \"\"\"\n281         try:\n282             field = self.lookup_opts.get_field(field_name)\n283             return field.name\n284         except FieldDoesNotExist:\n285             # See whether field_name is a name of a non-field\n286             # that allows sorting.\n287             if callable(field_name):\n288                 attr = field_name\n289             elif hasattr(self.model_admin, field_name):\n290                 attr = getattr(self.model_admin, field_name)\n291             else:\n292                 attr = getattr(self.model, field_name)\n293             if isinstance(attr, property) and hasattr(attr, 'fget'):\n294                 attr = attr.fget\n295             return getattr(attr, 'admin_order_field', None)\n296 \n297     def get_ordering(self, request, queryset):\n298         \"\"\"\n299         Return the list of ordering fields for the change list.\n300         First check the get_ordering() method in model admin, then check\n301         the object's default ordering. Then, any manually-specified ordering\n302         from the query string overrides anything. Finally, a deterministic\n303         order is guaranteed by calling _get_deterministic_ordering() with the\n304         constructed ordering.\n305         \"\"\"\n306         params = self.params\n307         ordering = list(self.model_admin.get_ordering(request) or self._get_default_ordering())\n308         if ORDER_VAR in params:\n309             # Clear ordering and used params\n310             ordering = []\n311             order_params = params[ORDER_VAR].split('.')\n312             for p in order_params:\n313                 try:\n314                     none, pfx, idx = p.rpartition('-')\n315                     field_name = self.list_display[int(idx)]\n316                     order_field = self.get_ordering_field(field_name)\n317                     if not order_field:\n318                         continue  # No 'admin_order_field', skip it\n319                     if isinstance(order_field, OrderBy):\n320                         if pfx == '-':\n321                             order_field = order_field.copy()\n322                             order_field.reverse_ordering()\n323                         ordering.append(order_field)\n324                     elif hasattr(order_field, 'resolve_expression'):\n325                         # order_field is an expression.\n326                         ordering.append(order_field.desc() if pfx == '-' else order_field.asc())\n327                     # reverse order if order_field has already \"-\" as prefix\n328                     elif order_field.startswith('-') and pfx == '-':\n329                         ordering.append(order_field[1:])\n330                     else:\n331                         ordering.append(pfx + order_field)\n332                 except (IndexError, ValueError):\n333                     continue  # Invalid ordering specified, skip it.\n334 \n335         # Add the given query's ordering fields, if any.\n336         ordering.extend(queryset.query.order_by)\n337 \n338         return self._get_deterministic_ordering(ordering)\n339 \n340     def _get_deterministic_ordering(self, ordering):\n341         \"\"\"\n342         Ensure a deterministic order across all database backends. Search for a\n343         single field or unique together set of fields providing a total\n344         ordering. If these are missing, augment the ordering with a descendant\n345         primary key.\n346         \"\"\"\n347         ordering = list(ordering)\n348         ordering_fields = set()\n349         total_ordering_fields = {'pk'} | {\n350             field.attname for field in self.lookup_opts.fields\n351             if field.unique and not field.null\n352         }\n353         for part in ordering:\n354             # Search for single field providing a total ordering.\n355             field_name = None\n356             if isinstance(part, str):\n357                 field_name = part.lstrip('-')\n358             elif isinstance(part, F):\n359                 field_name = part.name\n360             elif isinstance(part, OrderBy) and isinstance(part.expression, F):\n361                 field_name = part.expression.name\n362             if field_name:\n363                 # Normalize attname references by using get_field().\n364                 try:\n365                     field = self.lookup_opts.get_field(field_name)\n366                 except FieldDoesNotExist:\n367                     # Could be \"?\" for random ordering or a related field\n368                     # lookup. Skip this part of introspection for now.\n369                     continue\n370                 # Ordering by a related field name orders by the referenced\n371                 # model's ordering. Skip this part of introspection for now.\n372                 if field.remote_field and field_name == field.name:\n373                     continue\n374                 if field.attname in total_ordering_fields:\n375                     break\n376                 ordering_fields.add(field.attname)\n377         else:\n378             # No single total ordering field, try unique_together and total\n379             # unique constraints.\n380             constraint_field_names = (\n381                 *self.lookup_opts.unique_together,\n382                 *(\n383                     constraint.fields\n384                     for constraint in self.lookup_opts.total_unique_constraints\n385                 ),\n386             )\n387             for field_names in constraint_field_names:\n388                 # Normalize attname references by using get_field().\n389                 fields = [self.lookup_opts.get_field(field_name) for field_name in field_names]\n390                 # Composite unique constraints containing a nullable column\n391                 # cannot ensure total ordering.\n392                 if any(field.null for field in fields):\n393                     continue\n394                 if ordering_fields.issuperset(field.attname for field in fields):\n395                     break\n396             else:\n397                 # If no set of unique fields is present in the ordering, rely\n398                 # on the primary key to provide total ordering.\n399                 ordering.append('-pk')\n400         return ordering\n401 \n402     def get_ordering_field_columns(self):\n403         \"\"\"\n404         Return a dictionary of ordering field column numbers and asc/desc.\n405         \"\"\"\n406         # We must cope with more than one column having the same underlying sort\n407         # field, so we base things on column numbers.\n408         ordering = self._get_default_ordering()\n409         ordering_fields = {}\n410         if ORDER_VAR not in self.params:\n411             # for ordering specified on ModelAdmin or model Meta, we don't know\n412             # the right column numbers absolutely, because there might be more\n413             # than one column associated with that ordering, so we guess.\n414             for field in ordering:\n415                 if isinstance(field, (Combinable, OrderBy)):\n416                     if not isinstance(field, OrderBy):\n417                         field = field.asc()\n418                     if isinstance(field.expression, F):\n419                         order_type = 'desc' if field.descending else 'asc'\n420                         field = field.expression.name\n421                     else:\n422                         continue\n423                 elif field.startswith('-'):\n424                     field = field[1:]\n425                     order_type = 'desc'\n426                 else:\n427                     order_type = 'asc'\n428                 for index, attr in enumerate(self.list_display):\n429                     if self.get_ordering_field(attr) == field:\n430                         ordering_fields[index] = order_type\n431                         break\n432         else:\n433             for p in self.params[ORDER_VAR].split('.'):\n434                 none, pfx, idx = p.rpartition('-')\n435                 try:\n436                     idx = int(idx)\n437                 except ValueError:\n438                     continue  # skip it\n439                 ordering_fields[idx] = 'desc' if pfx == '-' else 'asc'\n440         return ordering_fields\n441 \n442     def get_queryset(self, request):\n443         # First, we collect all the declared list filters.\n444         (\n445             self.filter_specs,\n446             self.has_filters,\n447             remaining_lookup_params,\n448             filters_use_distinct,\n449             self.has_active_filters,\n450         ) = self.get_filters(request)\n451         # Then, we let every list filter modify the queryset to its liking.\n452         qs = self.root_queryset\n453         for filter_spec in self.filter_specs:\n454             new_qs = filter_spec.queryset(request, qs)\n455             if new_qs is not None:\n456                 qs = new_qs\n457 \n458         try:\n459             # Finally, we apply the remaining lookup parameters from the query\n460             # string (i.e. those that haven't already been processed by the\n461             # filters).\n462             qs = qs.filter(**remaining_lookup_params)\n463         except (SuspiciousOperation, ImproperlyConfigured):\n464             # Allow certain types of errors to be re-raised as-is so that the\n465             # caller can treat them in a special way.\n466             raise\n467         except Exception as e:\n468             # Every other error is caught with a naked except, because we don't\n469             # have any other way of validating lookup parameters. They might be\n470             # invalid if the keyword arguments are incorrect, or if the values\n471             # are not in the correct type, so we might get FieldError,\n472             # ValueError, ValidationError, or ?.\n473             raise IncorrectLookupParameters(e)\n474 \n475         if not qs.query.select_related:\n476             qs = self.apply_select_related(qs)\n477 \n478         # Set ordering.\n479         ordering = self.get_ordering(request, qs)\n480         qs = qs.order_by(*ordering)\n481 \n482         # Apply search results\n483         qs, search_use_distinct = self.model_admin.get_search_results(request, qs, self.query)\n484 \n485         # Set query string for clearing all filters.\n486         self.clear_all_filters_qs = self.get_query_string(\n487             new_params=remaining_lookup_params,\n488             remove=self.get_filters_params(),\n489         )\n490         # Remove duplicates from results, if necessary\n491         if filters_use_distinct | search_use_distinct:\n492             return qs.distinct()\n493         else:\n494             return qs\n495 \n496     def apply_select_related(self, qs):\n497         if self.list_select_related is True:\n498             return qs.select_related()\n499 \n500         if self.list_select_related is False:\n501             if self.has_related_field_in_list_display():\n502                 return qs.select_related()\n503 \n504         if self.list_select_related:\n505             return qs.select_related(*self.list_select_related)\n506         return qs\n507 \n508     def has_related_field_in_list_display(self):\n509         for field_name in self.list_display:\n510             try:\n511                 field = self.lookup_opts.get_field(field_name)\n512             except FieldDoesNotExist:\n513                 pass\n514             else:\n515                 if isinstance(field.remote_field, ManyToOneRel):\n516                     # <FK>_id field names don't require a join.\n517                     if field_name != field.get_attname():\n518                         return True\n519         return False\n520 \n521     def url_for_result(self, result):\n522         pk = getattr(result, self.pk_attname)\n523         return reverse('admin:%s_%s_change' % (self.opts.app_label,\n524                                                self.opts.model_name),\n525                        args=(quote(pk),),\n526                        current_app=self.model_admin.admin_site.name)\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "django/db/models/query.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__4.0/django/db/models/query.py",
        "start": 722,
        "end": 750,
        "class_name": "QuerySet",
        "method_name": "delete",
        "code": "722     def delete(self):\n723         \"\"\"Delete the records in the current QuerySet.\"\"\"\n724         self._not_support_combined_queries('delete')\n725         if self.query.is_sliced:\n726             raise TypeError(\"Cannot use 'limit' or 'offset' with delete().\")\n727         if self.query.distinct or self.query.distinct_fields:\n728             raise TypeError('Cannot call delete() after .distinct().')\n729         if self._fields is not None:\n730             raise TypeError(\"Cannot call delete() after .values() or .values_list()\")\n731 \n732         del_query = self._chain()\n733 \n734         # The delete is actually 2 queries - one to find related objects,\n735         # and one to delete. Make sure that the discovery of related\n736         # objects is performed on the same database as the deletion.\n737         del_query._for_write = True\n738 \n739         # Disable non-supported fields.\n740         del_query.query.select_for_update = False\n741         del_query.query.select_related = False\n742         del_query.query.clear_ordering(force_empty=True)\n743 \n744         collector = Collector(using=del_query.db)\n745         collector.collect(del_query)\n746         deleted, _rows_count = collector.delete()\n747 \n748         # Clear the result cache, in case this QuerySet gets reused.\n749         self._result_cache = None\n750         return deleted, _rows_count\n",
        "intended_behavior": "Before the actual deletion operation, check if the queryset has been altered by `.distinct()`, and if so, reconstruct the queryset from the primary keys of the current queryset's results. This will ensure that the deletion operation proceeds on a queryset that is compatible with deletion. Code modifications might involve a pre-deletion check that explicitly handles or warns about issues arising from attempting deletion on querysets with `.distinct()`. This pre-check could offer a pathway to handle such cases more gracefully, possibly by re-fetching objects based on their IDs alone for deletion."
    },
    {
        "rel_file_path": "django/db/models/query.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__4.0/django/db/models/query.py",
        "start": 175,
        "end": 1420,
        "class_name": "QuerySet",
        "method_name": null,
        "code": "175 class QuerySet:\n176     \"\"\"Represent a lazy database lookup for a set of objects.\"\"\"\n177 \n178     def __init__(self, model=None, query=None, using=None, hints=None):\n179         self.model = model\n180         self._db = using\n181         self._hints = hints or {}\n182         self._query = query or sql.Query(self.model)\n183         self._result_cache = None\n184         self._sticky_filter = False\n185         self._for_write = False\n186         self._prefetch_related_lookups = ()\n187         self._prefetch_done = False\n188         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}\n189         self._iterable_class = ModelIterable\n190         self._fields = None\n191         self._defer_next_filter = False\n192         self._deferred_filter = None\n193 \n194     @property\n195     def query(self):\n196         if self._deferred_filter:\n197             negate, args, kwargs = self._deferred_filter\n198             self._filter_or_exclude_inplace(negate, args, kwargs)\n199             self._deferred_filter = None\n200         return self._query\n201 \n202     @query.setter\n203     def query(self, value):\n204         if value.values_select:\n205             self._iterable_class = ValuesIterable\n206         self._query = value\n207 \n208     def as_manager(cls):\n209         # Address the circular dependency between `Queryset` and `Manager`.\n210         from django.db.models.manager import Manager\n211         manager = Manager.from_queryset(cls)()\n212         manager._built_with_as_manager = True\n213         return manager\n214     as_manager.queryset_only = True\n215     as_manager = classmethod(as_manager)\n216 \n217     ########################\n218     # PYTHON MAGIC METHODS #\n219     ########################\n220 \n221     def __deepcopy__(self, memo):\n222         \"\"\"Don't populate the QuerySet's cache.\"\"\"\n223         obj = self.__class__()\n224         for k, v in self.__dict__.items():\n225             if k == '_result_cache':\n226                 obj.__dict__[k] = None\n227             else:\n228                 obj.__dict__[k] = copy.deepcopy(v, memo)\n229         return obj\n230 \n231     def __getstate__(self):\n232         # Force the cache to be fully populated.\n233         self._fetch_all()\n234         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}\n235 \n236     def __setstate__(self, state):\n237         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)\n238         if pickled_version:\n239             if pickled_version != django.__version__:\n240                 warnings.warn(\n241                     \"Pickled queryset instance's Django version %s does not \"\n242                     \"match the current version %s.\"\n243                     % (pickled_version, django.__version__),\n244                     RuntimeWarning,\n245                     stacklevel=2,\n246                 )\n247         else:\n248             warnings.warn(\n249                 \"Pickled queryset instance's Django version is not specified.\",\n250                 RuntimeWarning,\n251                 stacklevel=2,\n252             )\n253         self.__dict__.update(state)\n254 \n255     def __repr__(self):\n256         data = list(self[:REPR_OUTPUT_SIZE + 1])\n257         if len(data) > REPR_OUTPUT_SIZE:\n258             data[-1] = \"...(remaining elements truncated)...\"\n259         return '<%s %r>' % (self.__class__.__name__, data)\n260 \n261     def __len__(self):\n262         self._fetch_all()\n263         return len(self._result_cache)\n264 \n265     def __iter__(self):\n266         \"\"\"\n267         The queryset iterator protocol uses three nested iterators in the\n268         default case:\n269             1. sql.compiler.execute_sql()\n270                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)\n271                  using cursor.fetchmany(). This part is responsible for\n272                  doing some column masking, and returning the rows in chunks.\n273             2. sql.compiler.results_iter()\n274                - Returns one row at time. At this point the rows are still just\n275                  tuples. In some cases the return values are converted to\n276                  Python values at this location.\n277             3. self.iterator()\n278                - Responsible for turning the rows into model objects.\n279         \"\"\"\n280         self._fetch_all()\n281         return iter(self._result_cache)\n282 \n283     def __bool__(self):\n284         self._fetch_all()\n285         return bool(self._result_cache)\n286 \n287     def __getitem__(self, k):\n288         \"\"\"Retrieve an item or slice from the set of results.\"\"\"\n289         if not isinstance(k, (int, slice)):\n290             raise TypeError(\n291                 'QuerySet indices must be integers or slices, not %s.'\n292                 % type(k).__name__\n293             )\n294         assert ((not isinstance(k, slice) and (k >= 0)) or\n295                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and\n296                  (k.stop is None or k.stop >= 0))), \\\n297             \"Negative indexing is not supported.\"\n298 \n299         if self._result_cache is not None:\n300             return self._result_cache[k]\n301 \n302         if isinstance(k, slice):\n303             qs = self._chain()\n304             if k.start is not None:\n305                 start = int(k.start)\n306             else:\n307                 start = None\n308             if k.stop is not None:\n309                 stop = int(k.stop)\n310             else:\n311                 stop = None\n312             qs.query.set_limits(start, stop)\n313             return list(qs)[::k.step] if k.step else qs\n314 \n315         qs = self._chain()\n316         qs.query.set_limits(k, k + 1)\n317         qs._fetch_all()\n318         return qs._result_cache[0]\n319 \n320     def __class_getitem__(cls, *args, **kwargs):\n321         return cls\n322 \n323     def __and__(self, other):\n324         self._merge_sanity_check(other)\n325         if isinstance(other, EmptyQuerySet):\n326             return other\n327         if isinstance(self, EmptyQuerySet):\n328             return self\n329         combined = self._chain()\n330         combined._merge_known_related_objects(other)\n331         combined.query.combine(other.query, sql.AND)\n332         return combined\n333 \n334     def __or__(self, other):\n335         self._merge_sanity_check(other)\n336         if isinstance(self, EmptyQuerySet):\n337             return other\n338         if isinstance(other, EmptyQuerySet):\n339             return self\n340         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))\n341         combined = query._chain()\n342         combined._merge_known_related_objects(other)\n343         if not other.query.can_filter():\n344             other = other.model._base_manager.filter(pk__in=other.values('pk'))\n345         combined.query.combine(other.query, sql.OR)\n346         return combined\n347 \n348     ####################################\n349     # METHODS THAT DO DATABASE QUERIES #\n350     ####################################\n351 \n352     def _iterator(self, use_chunked_fetch, chunk_size):\n353         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)\n354 \n355     def iterator(self, chunk_size=2000):\n356         \"\"\"\n357         An iterator over the results from applying this QuerySet to the\n358         database.\n359         \"\"\"\n360         if chunk_size <= 0:\n361             raise ValueError('Chunk size must be strictly positive.')\n362         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')\n363         return self._iterator(use_chunked_fetch, chunk_size)\n364 \n365     def aggregate(self, *args, **kwargs):\n366         \"\"\"\n367         Return a dictionary containing the calculations (aggregation)\n368         over the current queryset.\n369 \n370         If args is present the expression is passed as a kwarg using\n371         the Aggregate object's default alias.\n372         \"\"\"\n373         if self.query.distinct_fields:\n374             raise NotImplementedError(\"aggregate() + distinct(fields) not implemented.\")\n375         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')\n376         for arg in args:\n377             # The default_alias property raises TypeError if default_alias\n378             # can't be set automatically or AttributeError if it isn't an\n379             # attribute.\n380             try:\n381                 arg.default_alias\n382             except (AttributeError, TypeError):\n383                 raise TypeError(\"Complex aggregates require an alias\")\n384             kwargs[arg.default_alias] = arg\n385 \n386         query = self.query.chain()\n387         for (alias, aggregate_expr) in kwargs.items():\n388             query.add_annotation(aggregate_expr, alias, is_summary=True)\n389             annotation = query.annotations[alias]\n390             if not annotation.contains_aggregate:\n391                 raise TypeError(\"%s is not an aggregate expression\" % alias)\n392             for expr in annotation.get_source_expressions():\n393                 if expr.contains_aggregate and isinstance(expr, Ref) and expr.refs in kwargs:\n394                     name = expr.refs\n395                     raise exceptions.FieldError(\n396                         \"Cannot compute %s('%s'): '%s' is an aggregate\"\n397                         % (annotation.name, name, name)\n398                     )\n399         return query.get_aggregation(self.db, kwargs)\n400 \n401     def count(self):\n402         \"\"\"\n403         Perform a SELECT COUNT() and return the number of records as an\n404         integer.\n405 \n406         If the QuerySet is already fully cached, return the length of the\n407         cached results set to avoid multiple SELECT COUNT(*) calls.\n408         \"\"\"\n409         if self._result_cache is not None:\n410             return len(self._result_cache)\n411 \n412         return self.query.get_count(using=self.db)\n413 \n414     def get(self, *args, **kwargs):\n415         \"\"\"\n416         Perform the query and return a single object matching the given\n417         keyword arguments.\n418         \"\"\"\n419         if self.query.combinator and (args or kwargs):\n420             raise NotSupportedError(\n421                 'Calling QuerySet.get(...) with filters after %s() is not '\n422                 'supported.' % self.query.combinator\n423             )\n424         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)\n425         if self.query.can_filter() and not self.query.distinct_fields:\n426             clone = clone.order_by()\n427         limit = None\n428         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:\n429             limit = MAX_GET_RESULTS\n430             clone.query.set_limits(high=limit)\n431         num = len(clone)\n432         if num == 1:\n433             return clone._result_cache[0]\n434         if not num:\n435             raise self.model.DoesNotExist(\n436                 \"%s matching query does not exist.\" %\n437                 self.model._meta.object_name\n438             )\n439         raise self.model.MultipleObjectsReturned(\n440             'get() returned more than one %s -- it returned %s!' % (\n441                 self.model._meta.object_name,\n442                 num if not limit or num < limit else 'more than %s' % (limit - 1),\n443             )\n444         )\n445 \n446     def create(self, **kwargs):\n447         \"\"\"\n448         Create a new object with the given kwargs, saving it to the database\n449         and returning the created object.\n450         \"\"\"\n451         obj = self.model(**kwargs)\n452         self._for_write = True\n453         obj.save(force_insert=True, using=self.db)\n454         return obj\n455 \n456     def _prepare_for_bulk_create(self, objs):\n457         for obj in objs:\n458             if obj.pk is None:\n459                 # Populate new PK values.\n460                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)\n461             obj._prepare_related_fields_for_save(operation_name='bulk_create')\n462 \n463     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):\n464         \"\"\"\n465         Insert each of the instances into the database. Do *not* call\n466         save() on each of the instances, do not send any pre/post_save\n467         signals, and do not set the primary key attribute if it is an\n468         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).\n469         Multi-table models are not supported.\n470         \"\"\"\n471         # When you bulk insert you don't get the primary keys back (if it's an\n472         # autoincrement, except if can_return_rows_from_bulk_insert=True), so\n473         # you can't insert into the child tables which references this. There\n474         # are two workarounds:\n475         # 1) This could be implemented if you didn't have an autoincrement pk\n476         # 2) You could do it by doing O(n) normal inserts into the parent\n477         #    tables to get the primary keys back and then doing a single bulk\n478         #    insert into the childmost table.\n479         # We currently set the primary keys on the objects when using\n480         # PostgreSQL via the RETURNING ID clause. It should be possible for\n481         # Oracle as well, but the semantics for extracting the primary keys is\n482         # trickier so it's not done yet.\n483         assert batch_size is None or batch_size > 0\n484         # Check that the parents share the same concrete model with the our\n485         # model to detect the inheritance pattern ConcreteGrandParent ->\n486         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy\n487         # would not identify that case as involving multiple tables.\n488         for parent in self.model._meta.get_parent_list():\n489             if parent._meta.concrete_model is not self.model._meta.concrete_model:\n490                 raise ValueError(\"Can't bulk create a multi-table inherited model\")\n491         if not objs:\n492             return objs\n493         self._for_write = True\n494         connection = connections[self.db]\n495         opts = self.model._meta\n496         fields = opts.concrete_fields\n497         objs = list(objs)\n498         self._prepare_for_bulk_create(objs)\n499         with transaction.atomic(using=self.db, savepoint=False):\n500             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)\n501             if objs_with_pk:\n502                 returned_columns = self._batched_insert(\n503                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,\n504                 )\n505                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):\n506                     for result, field in zip(results, opts.db_returning_fields):\n507                         if field != opts.pk:\n508                             setattr(obj_with_pk, field.attname, result)\n509                 for obj_with_pk in objs_with_pk:\n510                     obj_with_pk._state.adding = False\n511                     obj_with_pk._state.db = self.db\n512             if objs_without_pk:\n513                 fields = [f for f in fields if not isinstance(f, AutoField)]\n514                 returned_columns = self._batched_insert(\n515                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,\n516                 )\n517                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:\n518                     assert len(returned_columns) == len(objs_without_pk)\n519                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):\n520                     for result, field in zip(results, opts.db_returning_fields):\n521                         setattr(obj_without_pk, field.attname, result)\n522                     obj_without_pk._state.adding = False\n523                     obj_without_pk._state.db = self.db\n524 \n525         return objs\n526 \n527     def bulk_update(self, objs, fields, batch_size=None):\n528         \"\"\"\n529         Update the given fields in each of the given objects in the database.\n530         \"\"\"\n531         if batch_size is not None and batch_size < 0:\n532             raise ValueError('Batch size must be a positive integer.')\n533         if not fields:\n534             raise ValueError('Field names must be given to bulk_update().')\n535         objs = tuple(objs)\n536         if any(obj.pk is None for obj in objs):\n537             raise ValueError('All bulk_update() objects must have a primary key set.')\n538         fields = [self.model._meta.get_field(name) for name in fields]\n539         if any(not f.concrete or f.many_to_many for f in fields):\n540             raise ValueError('bulk_update() can only be used with concrete fields.')\n541         if any(f.primary_key for f in fields):\n542             raise ValueError('bulk_update() cannot be used with primary key fields.')\n543         if not objs:\n544             return\n545         # PK is used twice in the resulting update query, once in the filter\n546         # and once in the WHEN. Each field will also have one CAST.\n547         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)\n548         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size\n549         requires_casting = connections[self.db].features.requires_casted_case_in_updates\n550         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))\n551         updates = []\n552         for batch_objs in batches:\n553             update_kwargs = {}\n554             for field in fields:\n555                 when_statements = []\n556                 for obj in batch_objs:\n557                     attr = getattr(obj, field.attname)\n558                     if not isinstance(attr, Expression):\n559                         attr = Value(attr, output_field=field)\n560                     when_statements.append(When(pk=obj.pk, then=attr))\n561                 case_statement = Case(*when_statements, output_field=field)\n562                 if requires_casting:\n563                     case_statement = Cast(case_statement, output_field=field)\n564                 update_kwargs[field.attname] = case_statement\n565             updates.append(([obj.pk for obj in batch_objs], update_kwargs))\n566         with transaction.atomic(using=self.db, savepoint=False):\n567             for pks, update_kwargs in updates:\n568                 self.filter(pk__in=pks).update(**update_kwargs)\n569     bulk_update.alters_data = True\n570 \n571     def get_or_create(self, defaults=None, **kwargs):\n572         \"\"\"\n573         Look up an object with the given kwargs, creating one if necessary.\n574         Return a tuple of (object, created), where created is a boolean\n575         specifying whether an object was created.\n576         \"\"\"\n577         # The get() needs to be targeted at the write database in order\n578         # to avoid potential transaction consistency problems.\n579         self._for_write = True\n580         try:\n581             return self.get(**kwargs), False\n582         except self.model.DoesNotExist:\n583             params = self._extract_model_params(defaults, **kwargs)\n584             # Try to create an object using passed params.\n585             try:\n586                 with transaction.atomic(using=self.db):\n587                     params = dict(resolve_callables(params))\n588                     return self.create(**params), True\n589             except IntegrityError:\n590                 try:\n591                     return self.get(**kwargs), False\n592                 except self.model.DoesNotExist:\n593                     pass\n594                 raise\n595 \n596     def update_or_create(self, defaults=None, **kwargs):\n597         \"\"\"\n598         Look up an object with the given kwargs, updating one with defaults\n599         if it exists, otherwise create a new one.\n600         Return a tuple (object, created), where created is a boolean\n601         specifying whether an object was created.\n602         \"\"\"\n603         defaults = defaults or {}\n604         self._for_write = True\n605         with transaction.atomic(using=self.db):\n606             # Lock the row so that a concurrent update is blocked until\n607             # update_or_create() has performed its save.\n608             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)\n609             if created:\n610                 return obj, created\n611             for k, v in resolve_callables(defaults):\n612                 setattr(obj, k, v)\n613             obj.save(using=self.db)\n614         return obj, False\n615 \n616     def _extract_model_params(self, defaults, **kwargs):\n617         \"\"\"\n618         Prepare `params` for creating a model instance based on the given\n619         kwargs; for use by get_or_create().\n620         \"\"\"\n621         defaults = defaults or {}\n622         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}\n623         params.update(defaults)\n624         property_names = self.model._meta._property_names\n625         invalid_params = []\n626         for param in params:\n627             try:\n628                 self.model._meta.get_field(param)\n629             except exceptions.FieldDoesNotExist:\n630                 # It's okay to use a model's property if it has a setter.\n631                 if not (param in property_names and getattr(self.model, param).fset):\n632                     invalid_params.append(param)\n633         if invalid_params:\n634             raise exceptions.FieldError(\n635                 \"Invalid field name(s) for model %s: '%s'.\" % (\n636                     self.model._meta.object_name,\n637                     \"', '\".join(sorted(invalid_params)),\n638                 ))\n639         return params\n640 \n641     def _earliest(self, *fields):\n642         \"\"\"\n643         Return the earliest object according to fields (if given) or by the\n644         model's Meta.get_latest_by.\n645         \"\"\"\n646         if fields:\n647             order_by = fields\n648         else:\n649             order_by = getattr(self.model._meta, 'get_latest_by')\n650             if order_by and not isinstance(order_by, (tuple, list)):\n651                 order_by = (order_by,)\n652         if order_by is None:\n653             raise ValueError(\n654                 \"earliest() and latest() require either fields as positional \"\n655                 \"arguments or 'get_latest_by' in the model's Meta.\"\n656             )\n657         obj = self._chain()\n658         obj.query.set_limits(high=1)\n659         obj.query.clear_ordering(force_empty=True)\n660         obj.query.add_ordering(*order_by)\n661         return obj.get()\n662 \n663     def earliest(self, *fields):\n664         if self.query.is_sliced:\n665             raise TypeError('Cannot change a query once a slice has been taken.')\n666         return self._earliest(*fields)\n667 \n668     def latest(self, *fields):\n669         if self.query.is_sliced:\n670             raise TypeError('Cannot change a query once a slice has been taken.')\n671         return self.reverse()._earliest(*fields)\n672 \n673     def first(self):\n674         \"\"\"Return the first object of a query or None if no match is found.\"\"\"\n675         for obj in (self if self.ordered else self.order_by('pk'))[:1]:\n676             return obj\n677 \n678     def last(self):\n679         \"\"\"Return the last object of a query or None if no match is found.\"\"\"\n680         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:\n681             return obj\n682 \n683     def in_bulk(self, id_list=None, *, field_name='pk'):\n684         \"\"\"\n685         Return a dictionary mapping each of the given IDs to the object with\n686         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.\n687         \"\"\"\n688         if self.query.is_sliced:\n689             raise TypeError(\"Cannot use 'limit' or 'offset' with in_bulk().\")\n690         opts = self.model._meta\n691         unique_fields = [\n692             constraint.fields[0]\n693             for constraint in opts.total_unique_constraints\n694             if len(constraint.fields) == 1\n695         ]\n696         if (\n697             field_name != 'pk' and\n698             not opts.get_field(field_name).unique and\n699             field_name not in unique_fields and\n700             self.query.distinct_fields != (field_name,)\n701         ):\n702             raise ValueError(\"in_bulk()'s field_name must be a unique field but %r isn't.\" % field_name)\n703         if id_list is not None:\n704             if not id_list:\n705                 return {}\n706             filter_key = '{}__in'.format(field_name)\n707             batch_size = connections[self.db].features.max_query_params\n708             id_list = tuple(id_list)\n709             # If the database has a limit on the number of query parameters\n710             # (e.g. SQLite), retrieve objects in batches if necessary.\n711             if batch_size and batch_size < len(id_list):\n712                 qs = ()\n713                 for offset in range(0, len(id_list), batch_size):\n714                     batch = id_list[offset:offset + batch_size]\n715                     qs += tuple(self.filter(**{filter_key: batch}).order_by())\n716             else:\n717                 qs = self.filter(**{filter_key: id_list}).order_by()\n718         else:\n719             qs = self._chain()\n720         return {getattr(obj, field_name): obj for obj in qs}\n721 \n722     def delete(self):\n723         \"\"\"Delete the records in the current QuerySet.\"\"\"\n724         self._not_support_combined_queries('delete')\n725         if self.query.is_sliced:\n726             raise TypeError(\"Cannot use 'limit' or 'offset' with delete().\")\n727         if self.query.distinct or self.query.distinct_fields:\n728             raise TypeError('Cannot call delete() after .distinct().')\n729         if self._fields is not None:\n730             raise TypeError(\"Cannot call delete() after .values() or .values_list()\")\n731 \n732         del_query = self._chain()\n733 \n734         # The delete is actually 2 queries - one to find related objects,\n735         # and one to delete. Make sure that the discovery of related\n736         # objects is performed on the same database as the deletion.\n737         del_query._for_write = True\n738 \n739         # Disable non-supported fields.\n740         del_query.query.select_for_update = False\n741         del_query.query.select_related = False\n742         del_query.query.clear_ordering(force_empty=True)\n743 \n744         collector = Collector(using=del_query.db)\n745         collector.collect(del_query)\n746         deleted, _rows_count = collector.delete()\n747 \n748         # Clear the result cache, in case this QuerySet gets reused.\n749         self._result_cache = None\n750         return deleted, _rows_count\n751 \n752     delete.alters_data = True\n753     delete.queryset_only = True\n754 \n755     def _raw_delete(self, using):\n756         \"\"\"\n757         Delete objects found from the given queryset in single direct SQL\n758         query. No signals are sent and there is no protection for cascades.\n759         \"\"\"\n760         query = self.query.clone()\n761         query.__class__ = sql.DeleteQuery\n762         cursor = query.get_compiler(using).execute_sql(CURSOR)\n763         if cursor:\n764             with cursor:\n765                 return cursor.rowcount\n766         return 0\n767     _raw_delete.alters_data = True\n768 \n769     def update(self, **kwargs):\n770         \"\"\"\n771         Update all elements in the current QuerySet, setting all the given\n772         fields to the appropriate values.\n773         \"\"\"\n774         self._not_support_combined_queries('update')\n775         if self.query.is_sliced:\n776             raise TypeError('Cannot update a query once a slice has been taken.')\n777         self._for_write = True\n778         query = self.query.chain(sql.UpdateQuery)\n779         query.add_update_values(kwargs)\n780         # Clear any annotations so that they won't be present in subqueries.\n781         query.annotations = {}\n782         with transaction.mark_for_rollback_on_error(using=self.db):\n783             rows = query.get_compiler(self.db).execute_sql(CURSOR)\n784         self._result_cache = None\n785         return rows\n786     update.alters_data = True\n787 \n788     def _update(self, values):\n789         \"\"\"\n790         A version of update() that accepts field objects instead of field names.\n791         Used primarily for model saving and not intended for use by general\n792         code (it requires too much poking around at model internals to be\n793         useful at that level).\n794         \"\"\"\n795         if self.query.is_sliced:\n796             raise TypeError('Cannot update a query once a slice has been taken.')\n797         query = self.query.chain(sql.UpdateQuery)\n798         query.add_update_fields(values)\n799         # Clear any annotations so that they won't be present in subqueries.\n800         query.annotations = {}\n801         self._result_cache = None\n802         return query.get_compiler(self.db).execute_sql(CURSOR)\n803     _update.alters_data = True\n804     _update.queryset_only = False\n805 \n806     def exists(self):\n807         if self._result_cache is None:\n808             return self.query.has_results(using=self.db)\n809         return bool(self._result_cache)\n810 \n811     def contains(self, obj):\n812         \"\"\"Return True if the queryset contains an object.\"\"\"\n813         self._not_support_combined_queries('contains')\n814         if self._fields is not None:\n815             raise TypeError(\n816                 'Cannot call QuerySet.contains() after .values() or '\n817                 '.values_list().'\n818             )\n819         try:\n820             if obj._meta.concrete_model != self.model._meta.concrete_model:\n821                 return False\n822         except AttributeError:\n823             raise TypeError(\"'obj' must be a model instance.\")\n824         if obj.pk is None:\n825             raise ValueError(\n826                 'QuerySet.contains() cannot be used on unsaved objects.'\n827             )\n828         if self._result_cache is not None:\n829             return obj in self._result_cache\n830         return self.filter(pk=obj.pk).exists()\n831 \n832     def _prefetch_related_objects(self):\n833         # This method can only be called once the result cache has been filled.\n834         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)\n835         self._prefetch_done = True\n836 \n837     def explain(self, *, format=None, **options):\n838         return self.query.explain(using=self.db, format=format, **options)\n839 \n840     ##################################################\n841     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #\n842     ##################################################\n843 \n844     def raw(self, raw_query, params=(), translations=None, using=None):\n845         if using is None:\n846             using = self.db\n847         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)\n848         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]\n849         return qs\n850 \n851     def _values(self, *fields, **expressions):\n852         clone = self._chain()\n853         if expressions:\n854             clone = clone.annotate(**expressions)\n855         clone._fields = fields\n856         clone.query.set_values(fields)\n857         return clone\n858 \n859     def values(self, *fields, **expressions):\n860         fields += tuple(expressions)\n861         clone = self._values(*fields, **expressions)\n862         clone._iterable_class = ValuesIterable\n863         return clone\n864 \n865     def values_list(self, *fields, flat=False, named=False):\n866         if flat and named:\n867             raise TypeError(\"'flat' and 'named' can't be used together.\")\n868         if flat and len(fields) > 1:\n869             raise TypeError(\"'flat' is not valid when values_list is called with more than one field.\")\n870 \n871         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}\n872         _fields = []\n873         expressions = {}\n874         counter = 1\n875         for field in fields:\n876             if hasattr(field, 'resolve_expression'):\n877                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())\n878                 while True:\n879                     field_id = field_id_prefix + str(counter)\n880                     counter += 1\n881                     if field_id not in field_names:\n882                         break\n883                 expressions[field_id] = field\n884                 _fields.append(field_id)\n885             else:\n886                 _fields.append(field)\n887 \n888         clone = self._values(*_fields, **expressions)\n889         clone._iterable_class = (\n890             NamedValuesListIterable if named\n891             else FlatValuesListIterable if flat\n892             else ValuesListIterable\n893         )\n894         return clone\n895 \n896     def dates(self, field_name, kind, order='ASC'):\n897         \"\"\"\n898         Return a list of date objects representing all available dates for\n899         the given field_name, scoped to 'kind'.\n900         \"\"\"\n901         assert kind in ('year', 'month', 'week', 'day'), \\\n902             \"'kind' must be one of 'year', 'month', 'week', or 'day'.\"\n903         assert order in ('ASC', 'DESC'), \\\n904             \"'order' must be either 'ASC' or 'DESC'.\"\n905         return self.annotate(\n906             datefield=Trunc(field_name, kind, output_field=DateField()),\n907             plain_field=F(field_name)\n908         ).values_list(\n909             'datefield', flat=True\n910         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')\n911 \n912     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):\n913         \"\"\"\n914         Return a list of datetime objects representing all available\n915         datetimes for the given field_name, scoped to 'kind'.\n916         \"\"\"\n917         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \\\n918             \"'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'.\"\n919         assert order in ('ASC', 'DESC'), \\\n920             \"'order' must be either 'ASC' or 'DESC'.\"\n921         if settings.USE_TZ:\n922             if tzinfo is None:\n923                 tzinfo = timezone.get_current_timezone()\n924         else:\n925             tzinfo = None\n926         return self.annotate(\n927             datetimefield=Trunc(\n928                 field_name,\n929                 kind,\n930                 output_field=DateTimeField(),\n931                 tzinfo=tzinfo,\n932                 is_dst=is_dst,\n933             ),\n934             plain_field=F(field_name)\n935         ).values_list(\n936             'datetimefield', flat=True\n937         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')\n938 \n939     def none(self):\n940         \"\"\"Return an empty QuerySet.\"\"\"\n941         clone = self._chain()\n942         clone.query.set_empty()\n943         return clone\n944 \n945     ##################################################################\n946     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #\n947     ##################################################################\n948 \n949     def all(self):\n950         \"\"\"\n951         Return a new QuerySet that is a copy of the current one. This allows a\n952         QuerySet to proxy for a model manager in some cases.\n953         \"\"\"\n954         return self._chain()\n955 \n956     def filter(self, *args, **kwargs):\n957         \"\"\"\n958         Return a new QuerySet instance with the args ANDed to the existing\n959         set.\n960         \"\"\"\n961         self._not_support_combined_queries('filter')\n962         return self._filter_or_exclude(False, args, kwargs)\n963 \n964     def exclude(self, *args, **kwargs):\n965         \"\"\"\n966         Return a new QuerySet instance with NOT (args) ANDed to the existing\n967         set.\n968         \"\"\"\n969         self._not_support_combined_queries('exclude')\n970         return self._filter_or_exclude(True, args, kwargs)\n971 \n972     def _filter_or_exclude(self, negate, args, kwargs):\n973         if (args or kwargs) and self.query.is_sliced:\n974             raise TypeError('Cannot filter a query once a slice has been taken.')\n975         clone = self._chain()\n976         if self._defer_next_filter:\n977             self._defer_next_filter = False\n978             clone._deferred_filter = negate, args, kwargs\n979         else:\n980             clone._filter_or_exclude_inplace(negate, args, kwargs)\n981         return clone\n982 \n983     def _filter_or_exclude_inplace(self, negate, args, kwargs):\n984         if negate:\n985             self._query.add_q(~Q(*args, **kwargs))\n986         else:\n987             self._query.add_q(Q(*args, **kwargs))\n988 \n989     def complex_filter(self, filter_obj):\n990         \"\"\"\n991         Return a new QuerySet instance with filter_obj added to the filters.\n992 \n993         filter_obj can be a Q object or a dictionary of keyword lookup\n994         arguments.\n995 \n996         This exists to support framework features such as 'limit_choices_to',\n997         and usually it will be more natural to use other methods.\n998         \"\"\"\n999         if isinstance(filter_obj, Q):\n1000             clone = self._chain()\n1001             clone.query.add_q(filter_obj)\n1002             return clone\n1003         else:\n1004             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)\n1005 \n1006     def _combinator_query(self, combinator, *other_qs, all=False):\n1007         # Clone the query to inherit the select list and everything\n1008         clone = self._chain()\n1009         # Clear limits and ordering so they can be reapplied\n1010         clone.query.clear_ordering(True)\n1011         clone.query.clear_limits()\n1012         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)\n1013         clone.query.combinator = combinator\n1014         clone.query.combinator_all = all\n1015         return clone\n1016 \n1017     def union(self, *other_qs, all=False):\n1018         # If the query is an EmptyQuerySet, combine all nonempty querysets.\n1019         if isinstance(self, EmptyQuerySet):\n1020             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]\n1021             if not qs:\n1022                 return self\n1023             if len(qs) == 1:\n1024                 return qs[0]\n1025             return qs[0]._combinator_query('union', *qs[1:], all=all)\n1026         return self._combinator_query('union', *other_qs, all=all)\n1027 \n1028     def intersection(self, *other_qs):\n1029         # If any query is an EmptyQuerySet, return it.\n1030         if isinstance(self, EmptyQuerySet):\n1031             return self\n1032         for other in other_qs:\n1033             if isinstance(other, EmptyQuerySet):\n1034                 return other\n1035         return self._combinator_query('intersection', *other_qs)\n1036 \n1037     def difference(self, *other_qs):\n1038         # If the query is an EmptyQuerySet, return it.\n1039         if isinstance(self, EmptyQuerySet):\n1040             return self\n1041         return self._combinator_query('difference', *other_qs)\n1042 \n1043     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):\n1044         \"\"\"\n1045         Return a new QuerySet instance that will select objects with a\n1046         FOR UPDATE lock.\n1047         \"\"\"\n1048         if nowait and skip_locked:\n1049             raise ValueError('The nowait option cannot be used with skip_locked.')\n1050         obj = self._chain()\n1051         obj._for_write = True\n1052         obj.query.select_for_update = True\n1053         obj.query.select_for_update_nowait = nowait\n1054         obj.query.select_for_update_skip_locked = skip_locked\n1055         obj.query.select_for_update_of = of\n1056         obj.query.select_for_no_key_update = no_key\n1057         return obj\n1058 \n1059     def select_related(self, *fields):\n1060         \"\"\"\n1061         Return a new QuerySet instance that will select related objects.\n1062 \n1063         If fields are specified, they must be ForeignKey fields and only those\n1064         related objects are included in the selection.\n1065 \n1066         If select_related(None) is called, clear the list.\n1067         \"\"\"\n1068         self._not_support_combined_queries('select_related')\n1069         if self._fields is not None:\n1070             raise TypeError(\"Cannot call select_related() after .values() or .values_list()\")\n1071 \n1072         obj = self._chain()\n1073         if fields == (None,):\n1074             obj.query.select_related = False\n1075         elif fields:\n1076             obj.query.add_select_related(fields)\n1077         else:\n1078             obj.query.select_related = True\n1079         return obj\n1080 \n1081     def prefetch_related(self, *lookups):\n1082         \"\"\"\n1083         Return a new QuerySet instance that will prefetch the specified\n1084         Many-To-One and Many-To-Many related objects when the QuerySet is\n1085         evaluated.\n1086 \n1087         When prefetch_related() is called more than once, append to the list of\n1088         prefetch lookups. If prefetch_related(None) is called, clear the list.\n1089         \"\"\"\n1090         self._not_support_combined_queries('prefetch_related')\n1091         clone = self._chain()\n1092         if lookups == (None,):\n1093             clone._prefetch_related_lookups = ()\n1094         else:\n1095             for lookup in lookups:\n1096                 if isinstance(lookup, Prefetch):\n1097                     lookup = lookup.prefetch_to\n1098                 lookup = lookup.split(LOOKUP_SEP, 1)[0]\n1099                 if lookup in self.query._filtered_relations:\n1100                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')\n1101             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups\n1102         return clone\n1103 \n1104     def annotate(self, *args, **kwargs):\n1105         \"\"\"\n1106         Return a query set in which the returned objects have been annotated\n1107         with extra data or aggregations.\n1108         \"\"\"\n1109         self._not_support_combined_queries('annotate')\n1110         return self._annotate(args, kwargs, select=True)\n1111 \n1112     def alias(self, *args, **kwargs):\n1113         \"\"\"\n1114         Return a query set with added aliases for extra data or aggregations.\n1115         \"\"\"\n1116         self._not_support_combined_queries('alias')\n1117         return self._annotate(args, kwargs, select=False)\n1118 \n1119     def _annotate(self, args, kwargs, select=True):\n1120         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')\n1121         annotations = {}\n1122         for arg in args:\n1123             # The default_alias property may raise a TypeError.\n1124             try:\n1125                 if arg.default_alias in kwargs:\n1126                     raise ValueError(\"The named annotation '%s' conflicts with the \"\n1127                                      \"default name for another annotation.\"\n1128                                      % arg.default_alias)\n1129             except TypeError:\n1130                 raise TypeError(\"Complex annotations require an alias\")\n1131             annotations[arg.default_alias] = arg\n1132         annotations.update(kwargs)\n1133 \n1134         clone = self._chain()\n1135         names = self._fields\n1136         if names is None:\n1137             names = set(chain.from_iterable(\n1138                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)\n1139                 for field in self.model._meta.get_fields()\n1140             ))\n1141 \n1142         for alias, annotation in annotations.items():\n1143             if alias in names:\n1144                 raise ValueError(\"The annotation '%s' conflicts with a field on \"\n1145                                  \"the model.\" % alias)\n1146             if isinstance(annotation, FilteredRelation):\n1147                 clone.query.add_filtered_relation(annotation, alias)\n1148             else:\n1149                 clone.query.add_annotation(\n1150                     annotation, alias, is_summary=False, select=select,\n1151                 )\n1152         for alias, annotation in clone.query.annotations.items():\n1153             if alias in annotations and annotation.contains_aggregate:\n1154                 if clone._fields is None:\n1155                     clone.query.group_by = True\n1156                 else:\n1157                     clone.query.set_group_by()\n1158                 break\n1159 \n1160         return clone\n1161 \n1162     def order_by(self, *field_names):\n1163         \"\"\"Return a new QuerySet instance with the ordering changed.\"\"\"\n1164         if self.query.is_sliced:\n1165             raise TypeError('Cannot reorder a query once a slice has been taken.')\n1166         obj = self._chain()\n1167         obj.query.clear_ordering(force_empty=False)\n1168         obj.query.add_ordering(*field_names)\n1169         return obj\n1170 \n1171     def distinct(self, *field_names):\n1172         \"\"\"\n1173         Return a new QuerySet instance that will select only distinct results.\n1174         \"\"\"\n1175         self._not_support_combined_queries('distinct')\n1176         if self.query.is_sliced:\n1177             raise TypeError('Cannot create distinct fields once a slice has been taken.')\n1178         obj = self._chain()\n1179         obj.query.add_distinct_fields(*field_names)\n1180         return obj\n1181 \n1182     def extra(self, select=None, where=None, params=None, tables=None,\n1183               order_by=None, select_params=None):\n1184         \"\"\"Add extra SQL fragments to the query.\"\"\"\n1185         self._not_support_combined_queries('extra')\n1186         if self.query.is_sliced:\n1187             raise TypeError('Cannot change a query once a slice has been taken.')\n1188         clone = self._chain()\n1189         clone.query.add_extra(select, select_params, where, params, tables, order_by)\n1190         return clone\n1191 \n1192     def reverse(self):\n1193         \"\"\"Reverse the ordering of the QuerySet.\"\"\"\n1194         if self.query.is_sliced:\n1195             raise TypeError('Cannot reverse a query once a slice has been taken.')\n1196         clone = self._chain()\n1197         clone.query.standard_ordering = not clone.query.standard_ordering\n1198         return clone\n1199 \n1200     def defer(self, *fields):\n1201         \"\"\"\n1202         Defer the loading of data for certain fields until they are accessed.\n1203         Add the set of deferred fields to any existing set of deferred fields.\n1204         The only exception to this is if None is passed in as the only\n1205         parameter, in which case removal all deferrals.\n1206         \"\"\"\n1207         self._not_support_combined_queries('defer')\n1208         if self._fields is not None:\n1209             raise TypeError(\"Cannot call defer() after .values() or .values_list()\")\n1210         clone = self._chain()\n1211         if fields == (None,):\n1212             clone.query.clear_deferred_loading()\n1213         else:\n1214             clone.query.add_deferred_loading(fields)\n1215         return clone\n1216 \n1217     def only(self, *fields):\n1218         \"\"\"\n1219         Essentially, the opposite of defer(). Only the fields passed into this\n1220         method and that are not already specified as deferred are loaded\n1221         immediately when the queryset is evaluated.\n1222         \"\"\"\n1223         self._not_support_combined_queries('only')\n1224         if self._fields is not None:\n1225             raise TypeError(\"Cannot call only() after .values() or .values_list()\")\n1226         if fields == (None,):\n1227             # Can only pass None to defer(), not only(), as the rest option.\n1228             # That won't stop people trying to do this, so let's be explicit.\n1229             raise TypeError(\"Cannot pass None as an argument to only().\")\n1230         for field in fields:\n1231             field = field.split(LOOKUP_SEP, 1)[0]\n1232             if field in self.query._filtered_relations:\n1233                 raise ValueError('only() is not supported with FilteredRelation.')\n1234         clone = self._chain()\n1235         clone.query.add_immediate_loading(fields)\n1236         return clone\n1237 \n1238     def using(self, alias):\n1239         \"\"\"Select which database this QuerySet should execute against.\"\"\"\n1240         clone = self._chain()\n1241         clone._db = alias\n1242         return clone\n1243 \n1244     ###################################\n1245     # PUBLIC INTROSPECTION ATTRIBUTES #\n1246     ###################################\n1247 \n1248     @property\n1249     def ordered(self):\n1250         \"\"\"\n1251         Return True if the QuerySet is ordered -- i.e. has an order_by()\n1252         clause or a default ordering on the model (or is empty).\n1253         \"\"\"\n1254         if isinstance(self, EmptyQuerySet):\n1255             return True\n1256         if self.query.extra_order_by or self.query.order_by:\n1257             return True\n1258         elif (\n1259             self.query.default_ordering and\n1260             self.query.get_meta().ordering and\n1261             # A default ordering doesn't affect GROUP BY queries.\n1262             not self.query.group_by\n1263         ):\n1264             return True\n1265         else:\n1266             return False\n1267 \n1268     @property\n1269     def db(self):\n1270         \"\"\"Return the database used if this query is executed now.\"\"\"\n1271         if self._for_write:\n1272             return self._db or router.db_for_write(self.model, **self._hints)\n1273         return self._db or router.db_for_read(self.model, **self._hints)\n1274 \n1275     ###################\n1276     # PRIVATE METHODS #\n1277     ###################\n1278 \n1279     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):\n1280         \"\"\"\n1281         Insert a new record for the given model. This provides an interface to\n1282         the InsertQuery class and is how Model.save() is implemented.\n1283         \"\"\"\n1284         self._for_write = True\n1285         if using is None:\n1286             using = self.db\n1287         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)\n1288         query.insert_values(fields, objs, raw=raw)\n1289         return query.get_compiler(using=using).execute_sql(returning_fields)\n1290     _insert.alters_data = True\n1291     _insert.queryset_only = False\n1292 \n1293     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):\n1294         \"\"\"\n1295         Helper method for bulk_create() to insert objs one batch at a time.\n1296         \"\"\"\n1297         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:\n1298             raise NotSupportedError('This database backend does not support ignoring conflicts.')\n1299         ops = connections[self.db].ops\n1300         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)\n1301         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size\n1302         inserted_rows = []\n1303         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert\n1304         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:\n1305             if bulk_return and not ignore_conflicts:\n1306                 inserted_rows.extend(self._insert(\n1307                     item, fields=fields, using=self.db,\n1308                     returning_fields=self.model._meta.db_returning_fields,\n1309                     ignore_conflicts=ignore_conflicts,\n1310                 ))\n1311             else:\n1312                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)\n1313         return inserted_rows\n1314 \n1315     def _chain(self, **kwargs):\n1316         \"\"\"\n1317         Return a copy of the current QuerySet that's ready for another\n1318         operation.\n1319         \"\"\"\n1320         obj = self._clone()\n1321         if obj._sticky_filter:\n1322             obj.query.filter_is_sticky = True\n1323             obj._sticky_filter = False\n1324         obj.__dict__.update(kwargs)\n1325         return obj\n1326 \n1327     def _clone(self):\n1328         \"\"\"\n1329         Return a copy of the current QuerySet. A lightweight alternative\n1330         to deepcopy().\n1331         \"\"\"\n1332         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)\n1333         c._sticky_filter = self._sticky_filter\n1334         c._for_write = self._for_write\n1335         c._prefetch_related_lookups = self._prefetch_related_lookups[:]\n1336         c._known_related_objects = self._known_related_objects\n1337         c._iterable_class = self._iterable_class\n1338         c._fields = self._fields\n1339         return c\n1340 \n1341     def _fetch_all(self):\n1342         if self._result_cache is None:\n1343             self._result_cache = list(self._iterable_class(self))\n1344         if self._prefetch_related_lookups and not self._prefetch_done:\n1345             self._prefetch_related_objects()\n1346 \n1347     def _next_is_sticky(self):\n1348         \"\"\"\n1349         Indicate that the next filter call and the one following that should\n1350         be treated as a single filter. This is only important when it comes to\n1351         determining when to reuse tables for many-to-many filters. Required so\n1352         that we can filter naturally on the results of related managers.\n1353 \n1354         This doesn't return a clone of the current QuerySet (it returns\n1355         \"self\"). The method is only used internally and should be immediately\n1356         followed by a filter() that does create a clone.\n1357         \"\"\"\n1358         self._sticky_filter = True\n1359         return self\n1360 \n1361     def _merge_sanity_check(self, other):\n1362         \"\"\"Check that two QuerySet classes may be merged.\"\"\"\n1363         if self._fields is not None and (\n1364                 set(self.query.values_select) != set(other.query.values_select) or\n1365                 set(self.query.extra_select) != set(other.query.extra_select) or\n1366                 set(self.query.annotation_select) != set(other.query.annotation_select)):\n1367             raise TypeError(\n1368                 \"Merging '%s' classes must involve the same values in each case.\"\n1369                 % self.__class__.__name__\n1370             )\n1371 \n1372     def _merge_known_related_objects(self, other):\n1373         \"\"\"\n1374         Keep track of all known related objects from either QuerySet instance.\n1375         \"\"\"\n1376         for field, objects in other._known_related_objects.items():\n1377             self._known_related_objects.setdefault(field, {}).update(objects)\n1378 \n1379     def resolve_expression(self, *args, **kwargs):\n1380         if self._fields and len(self._fields) > 1:\n1381             # values() queryset can only be used as nested queries\n1382             # if they are set up to select only a single field.\n1383             raise TypeError('Cannot use multi-field values as a filter value.')\n1384         query = self.query.resolve_expression(*args, **kwargs)\n1385         query._db = self._db\n1386         return query\n1387     resolve_expression.queryset_only = True\n1388 \n1389     def _add_hints(self, **hints):\n1390         \"\"\"\n1391         Update hinting information for use by routers. Add new key/values or\n1392         overwrite existing key/values.\n1393         \"\"\"\n1394         self._hints.update(hints)\n1395 \n1396     def _has_filters(self):\n1397         \"\"\"\n1398         Check if this QuerySet has any filtering going on. This isn't\n1399         equivalent with checking if all objects are present in results, for\n1400         example, qs[1:]._has_filters() -> False.\n1401         \"\"\"\n1402         return self.query.has_filters()\n1403 \n1404     @staticmethod\n1405     def _validate_values_are_expressions(values, method_name):\n1406         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))\n1407         if invalid_args:\n1408             raise TypeError(\n1409                 'QuerySet.%s() received non-expression(s): %s.' % (\n1410                     method_name,\n1411                     ', '.join(invalid_args),\n1412                 )\n1413             )\n1414 \n1415     def _not_support_combined_queries(self, operation_name):\n1416         if self.query.combinator:\n1417             raise NotSupportedError(\n1418                 'Calling QuerySet.%s() after %s() is not supported.'\n1419                 % (operation_name, self.query.combinator)\n1420             )\n",
        "intended_behavior": "This class provides additional context to the issue."
    }
]