2025-10-18 12:43:38.085 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 12:43:40.758 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=839, output_tokens=12, cost=0.008750
2025-10-18 12:44:01.515 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1019, output_tokens=963, cost=0.039080
2025-10-18 12:44:02.808 | INFO     | registered non reproducing test 0
2025-10-18 12:44:21.075 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2599, output_tokens=849, cost=0.051460
2025-10-18 12:44:22.207 | INFO     | registered non reproducing test 1
2025-10-18 12:44:37.940 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2236, output_tokens=744, cost=0.044680
2025-10-18 12:44:39.091 | INFO     | registered non reproducing test 2
2025-10-18 12:44:39.091 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-18 12:44:39.097 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 12:44:48.532 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1148, output_tokens=399, cost=0.023450
2025-10-18 12:44:48.540 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 12:44:52.521 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=753, output_tokens=97, cost=0.010440
2025-10-18 12:44:52.522 | DEBUG    | Extracted a valid json.
2025-10-18 12:44:52.523 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"Post\")",
        "search_class(\"User\")",
        "search_class_in_file(\"PostAdmin\", \"admin.py\")",
        "search_method_in_class(\"delete_queryset\", \"PostAdmin\")",
        "search_method(\"delete_selected\")",
        "search_method_in_class(\"changelist_view\", \"PostAdmin\")",
        "search_method(\"get_queryset\")"
    ],
    "bug_locations": []
}
2025-10-18 12:44:52.558 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 12:44:52.559 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 12:45:08.800 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2675, output_tokens=703, cost=0.047840
2025-10-18 12:45:08.814 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 12:45:20.488 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3594, output_tokens=528, cost=0.051780
2025-10-18 12:45:20.498 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 12:45:27.227 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=882, output_tokens=268, cost=0.016860
2025-10-18 12:45:27.228 | DEBUG    | Extracted a valid json.
2025-10-18 12:45:27.228 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_code(\"class Post(models.Model)\")",
        "search_code(\"search_fields = ('title', 'authors__email')\")"
    ],
    "bug_locations": [
        {
            "file": "admin.py",
            "class": "PostAdmin",
            "method": "",
            "intended_behavior": "The method handling deletions in the PostAdmin class must ensure that any queryset modifications, especially .distinct() calls made for searching or filtering based on related fields, do not interfere with the ability to delete queried items. Specifically, ensure that any .distinct() calls are only applied in contexts where deletion is not the subsequent action, or, alternatively, find and apply a method for reassimilating the queryset to a deletable state."
        },
        {
            "file": "",
            "class": "",
            "method": "",
            "intended_behavior": "If a more general Django admin issue related to handling deletions from a queryset that includes a .distinct() call is identified, the resolution would involve ensuring that the admin's deletion process can handle or bypass the unique constraints imposed by .distinct(), perhaps by reverting to a less constrained queryset or ensuring that deletion logic aligns with the unique handling required for post-search action situations."
        }
    ]
}
2025-10-18 12:45:27.291 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 12:45:27.291 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 12:45:41.176 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=4296, output_tokens=607, cost=0.061170
2025-10-18 12:45:41.186 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 12:45:55.305 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=5119, output_tokens=629, cost=0.070060
2025-10-18 12:45:55.316 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 12:47:47.572 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=983, output_tokens=14, cost=0.010250
2025-10-18 12:47:47.573 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 12:47:47.573 | DEBUG    | Trying to convert API calls/bug locations into json. Try 2 of 5.
2025-10-18 12:47:49.035 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=983, output_tokens=14, cost=0.010250
2025-10-18 12:47:49.036 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 12:47:49.036 | DEBUG    | Trying to convert API calls/bug locations into json. Try 3 of 5.
2025-10-18 12:47:52.037 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=983, output_tokens=109, cost=0.013100
2025-10-18 12:47:52.038 | DEBUG    | Bug location not detailed enough. Each location must contain at least a class or a method or a file.. Will retry.
2025-10-18 12:47:52.039 | DEBUG    | Trying to convert API calls/bug locations into json. Try 4 of 5.
2025-10-18 12:47:55.763 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=983, output_tokens=134, cost=0.013850
2025-10-18 12:47:55.764 | DEBUG    | Bug location not detailed enough. Each location must contain at least a class or a method or a file.. Will retry.
2025-10-18 12:47:55.764 | DEBUG    | Trying to convert API calls/bug locations into json. Try 5 of 5.
2025-10-18 12:47:56.564 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=983, output_tokens=14, cost=0.010250
2025-10-18 12:47:56.565 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 12:47:56.565 | DEBUG    | Agent proxy return the following json: None
2025-10-18 12:47:56.565 | DEBUG    | Could not extract API calls from agent search response, asking search agent to re-generate response.
2025-10-18 12:47:56.566 | DEBUG    | <Agent search> Downstream could not consume our last response. Will retry.
2025-10-18 12:47:56.566 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 12:48:14.805 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=5775, output_tokens=693, cost=0.078540
2025-10-18 12:48:14.821 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 12:48:18.676 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1047, output_tokens=152, cost=0.015030
2025-10-18 12:48:18.677 | DEBUG    | Extracted a valid json.
2025-10-18 12:48:18.677 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "django/contrib/admin/options.py or django/contrib/admin/actions.py",
            "class": "ModelAdmin or a custom PostAdmin class",
            "method": "the custom deletion method or the overridden get_queryset method",
            "intended_behavior": "Before performing the delete operation, ensure that any .distinct() call applied to the queryset as part of search operations is handled in such a way that it doesn't interfere with deletion. This could involve creating a new queryset based purely on the IDs collected from the .distinct() queryset, thereby bypassing the issue since the new queryset wouldn't inherently carry over the .distinct() call."
        }
    ]
}
2025-10-18 12:48:18.681 | DEBUG    | Failed to retrieve code from all bug locations. Asking search agent to re-generate response.
2025-10-18 12:48:18.682 | DEBUG    | <Agent search> Downstream could not consume our last response. Will retry.
2025-10-18 12:48:18.682 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 12:48:35.042 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=6502, output_tokens=631, cost=0.083950
2025-10-18 12:48:35.053 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 12:48:35.984 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=985, output_tokens=14, cost=0.010270
2025-10-18 12:48:35.985 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 12:48:35.985 | DEBUG    | Trying to convert API calls/bug locations into json. Try 2 of 5.
2025-10-18 12:48:38.375 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=985, output_tokens=14, cost=0.010270
2025-10-18 12:48:38.376 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 12:48:38.376 | DEBUG    | Trying to convert API calls/bug locations into json. Try 3 of 5.
2025-10-18 12:48:39.322 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=985, output_tokens=14, cost=0.010270
2025-10-18 12:48:39.323 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 12:48:39.323 | DEBUG    | Trying to convert API calls/bug locations into json. Try 4 of 5.
2025-10-18 12:48:40.303 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=985, output_tokens=14, cost=0.010270
2025-10-18 12:48:40.304 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 12:48:40.310 | DEBUG    | Trying to convert API calls/bug locations into json. Try 5 of 5.
2025-10-18 12:48:41.838 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=985, output_tokens=14, cost=0.010270
2025-10-18 12:48:41.839 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 12:48:41.839 | DEBUG    | Agent proxy return the following json: None
2025-10-18 12:48:41.840 | DEBUG    | Could not extract API calls from agent search response, asking search agent to re-generate response.
2025-10-18 12:48:41.841 | DEBUG    | <Agent search> Downstream could not consume our last response. Will retry.
2025-10-18 12:48:41.841 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 12:48:56.795 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=7160, output_tokens=623, cost=0.090290
2025-10-18 12:48:56.806 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 12:48:58.817 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=977, output_tokens=14, cost=0.010190
2025-10-18 12:48:58.818 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 12:48:58.818 | DEBUG    | Trying to convert API calls/bug locations into json. Try 2 of 5.
2025-10-18 12:48:59.784 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=977, output_tokens=14, cost=0.010190
2025-10-18 12:48:59.785 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 12:48:59.785 | DEBUG    | Trying to convert API calls/bug locations into json. Try 3 of 5.
2025-10-18 12:49:01.024 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=977, output_tokens=14, cost=0.010190
2025-10-18 12:49:01.025 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 12:49:01.025 | DEBUG    | Trying to convert API calls/bug locations into json. Try 4 of 5.
2025-10-18 12:49:03.539 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=977, output_tokens=14, cost=0.010190
2025-10-18 12:49:03.540 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 12:49:03.541 | DEBUG    | Trying to convert API calls/bug locations into json. Try 5 of 5.
2025-10-18 12:49:04.539 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=977, output_tokens=14, cost=0.010190
2025-10-18 12:49:04.540 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 12:49:04.540 | DEBUG    | Agent proxy return the following json: None
2025-10-18 12:49:04.541 | DEBUG    | Could not extract API calls from agent search response, asking search agent to re-generate response.
2025-10-18 12:49:04.542 | DEBUG    | <Agent search> Downstream could not consume our last response. Will retry.
2025-10-18 12:49:04.542 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 12:49:19.267 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=7810, output_tokens=633, cost=0.097090
2025-10-18 12:49:19.279 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 12:49:20.565 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=987, output_tokens=14, cost=0.010290
2025-10-18 12:49:20.566 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 12:49:20.567 | DEBUG    | Trying to convert API calls/bug locations into json. Try 2 of 5.
2025-10-18 12:49:23.656 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=987, output_tokens=64, cost=0.011790
2025-10-18 12:49:23.657 | DEBUG    | Extracted a valid json.
2025-10-18 12:49:23.657 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_code(\".distinct()\")",
        "search_class(\"Admin\")",
        "search_method(\"delete\")",
        "search_code(\"delete()\")",
        "search_method_in_class(\"delete\", \"Admin\")"
    ],
    "bug_locations": []
}
2025-10-18 12:49:23.754 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 12:49:23.755 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 12:49:43.210 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=9476, output_tokens=782, cost=0.118220
2025-10-18 12:49:43.217 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 12:49:56.365 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=10474, output_tokens=518, cost=0.120280
2025-10-18 12:49:56.376 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 12:50:03.072 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=872, output_tokens=283, cost=0.017210
2025-10-18 12:50:03.073 | DEBUG    | Extracted a valid json.
2025-10-18 12:50:03.073 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "django/contrib/admin/views/main.py",
            "class": "ChangeList",
            "method": "get_queryset",
            "intended_behavior": "Ensure that any application of `.distinct()` to the queryset in the context of a deletion operation is either avoided or handled in such a way that it does not prevent the `delete()` operation from succeeding. One approach could involve conditionally applying `.distinct()` only when it's certain that the following operation is not deletion. Alternatively, before deletion, clone the queryset without `.distinct()` if its presence is detected."
        },
        {
            "file": "django/db/models/query.py",
            "class": "QuerySet",
            "method": "delete",
            "intended_behavior": "Before the actual deletion operation, check if the queryset has been altered by `.distinct()`, and if so, reconstruct the queryset from the primary keys of the current queryset's results. This will ensure that the deletion operation proceeds on a queryset that is compatible with deletion. Code modifications might involve a pre-deletion check that explicitly handles or warns about issues arising from attempting deletion on querysets with `.distinct()`. This pre-check could offer a pathway to handle such cases more gracefully, possibly by re-fetching objects based on their IDs alone for deletion."
        }
    ]
}
2025-10-18 12:50:03.086 | DEBUG    | Bug location extracted successfully: [<file>django/contrib/admin/views/main.py</file>
<class>ChangeList</class>
<method>get_queryset</method>
<code>
442     def get_queryset(self, request):
443         # First, we collect all the declared list filters.
444         (
445             self.filter_specs,
446             self.has_filters,
447             remaining_lookup_params,
448             filters_use_distinct,
449             self.has_active_filters,
450         ) = self.get_filters(request)
451         # Then, we let every list filter modify the queryset to its liking.
452         qs = self.root_queryset
453         for filter_spec in self.filter_specs:
454             new_qs = filter_spec.queryset(request, qs)
455             if new_qs is not None:
456                 qs = new_qs
457 
458         try:
459             # Finally, we apply the remaining lookup parameters from the query
460             # string (i.e. those that haven't already been processed by the
461             # filters).
462             qs = qs.filter(**remaining_lookup_params)
463         except (SuspiciousOperation, ImproperlyConfigured):
464             # Allow certain types of errors to be re-raised as-is so that the
465             # caller can treat them in a special way.
466             raise
467         except Exception as e:
468             # Every other error is caught with a naked except, because we don't
469             # have any other way of validating lookup parameters. They might be
470             # invalid if the keyword arguments are incorrect, or if the values
471             # are not in the correct type, so we might get FieldError,
472             # ValueError, ValidationError, or ?.
473             raise IncorrectLookupParameters(e)
474 
475         if not qs.query.select_related:
476             qs = self.apply_select_related(qs)
477 
478         # Set ordering.
479         ordering = self.get_ordering(request, qs)
480         qs = qs.order_by(*ordering)
481 
482         # Apply search results
483         qs, search_use_distinct = self.model_admin.get_search_results(request, qs, self.query)
484 
485         # Set query string for clearing all filters.
486         self.clear_all_filters_qs = self.get_query_string(
487             new_params=remaining_lookup_params,
488             remove=self.get_filters_params(),
489         )
490         # Remove duplicates from results, if necessary
491         if filters_use_distinct | search_use_distinct:
492             return qs.distinct()
493         else:
494             return qs

</code><intended_behavior>Ensure that any application of `.distinct()` to the queryset in the context of a deletion operation is either avoided or handled in such a way that it does not prevent the `delete()` operation from succeeding. One approach could involve conditionally applying `.distinct()` only when it's certain that the following operation is not deletion. Alternatively, before deletion, clone the queryset without `.distinct()` if its presence is detected.</intended_behavior>, <file>django/contrib/admin/views/main.py</file>
<class>ChangeList</class>
<method>None</method>
<code>
48 class ChangeList:
49     search_form_class = ChangeListSearchForm
50 
51     def __init__(self, request, model, list_display, list_display_links,
52                  list_filter, date_hierarchy, search_fields, list_select_related,
53                  list_per_page, list_max_show_all, list_editable, model_admin, sortable_by):
54         self.model = model
55         self.opts = model._meta
56         self.lookup_opts = self.opts
57         self.root_queryset = model_admin.get_queryset(request)
58         self.list_display = list_display
59         self.list_display_links = list_display_links
60         self.list_filter = list_filter
61         self.has_filters = None
62         self.has_active_filters = None
63         self.clear_all_filters_qs = None
64         self.date_hierarchy = date_hierarchy
65         self.search_fields = search_fields
66         self.list_select_related = list_select_related
67         self.list_per_page = list_per_page
68         self.list_max_show_all = list_max_show_all
69         self.model_admin = model_admin
70         self.preserved_filters = model_admin.get_preserved_filters(request)
71         self.sortable_by = sortable_by
72 
73         # Get search parameters from the query string.
74         _search_form = self.search_form_class(request.GET)
75         if not _search_form.is_valid():
76             for error in _search_form.errors.values():
77                 messages.error(request, ', '.join(error))
78         self.query = _search_form.cleaned_data.get(SEARCH_VAR) or ''
79         try:
80             self.page_num = int(request.GET.get(PAGE_VAR, 1))
81         except ValueError:
82             self.page_num = 1
83         self.show_all = ALL_VAR in request.GET
84         self.is_popup = IS_POPUP_VAR in request.GET
85         to_field = request.GET.get(TO_FIELD_VAR)
86         if to_field and not model_admin.to_field_allowed(request, to_field):
87             raise DisallowedModelAdminToField("The field %s cannot be referenced." % to_field)
88         self.to_field = to_field
89         self.params = dict(request.GET.items())
90         if PAGE_VAR in self.params:
91             del self.params[PAGE_VAR]
92         if ERROR_FLAG in self.params:
93             del self.params[ERROR_FLAG]
94 
95         if self.is_popup:
96             self.list_editable = ()
97         else:
98             self.list_editable = list_editable
99         self.queryset = self.get_queryset(request)
100         self.get_results(request)
101         if self.is_popup:
102             title = gettext('Select %s')
103         elif self.model_admin.has_change_permission(request):
104             title = gettext('Select %s to change')
105         else:
106             title = gettext('Select %s to view')
107         self.title = title % self.opts.verbose_name
108         self.pk_attname = self.lookup_opts.pk.attname
109 
110     def get_filters_params(self, params=None):
111         """
112         Return all params except IGNORED_PARAMS.
113         """
114         params = params or self.params
115         lookup_params = params.copy()  # a dictionary of the query string
116         # Remove all the parameters that are globally and systematically
117         # ignored.
118         for ignored in IGNORED_PARAMS:
119             if ignored in lookup_params:
120                 del lookup_params[ignored]
121         return lookup_params
122 
123     def get_filters(self, request):
124         lookup_params = self.get_filters_params()
125         use_distinct = False
126         has_active_filters = False
127 
128         for key, value in lookup_params.items():
129             if not self.model_admin.lookup_allowed(key, value):
130                 raise DisallowedModelAdminLookup("Filtering by %s not allowed" % key)
131 
132         filter_specs = []
133         for list_filter in self.list_filter:
134             lookup_params_count = len(lookup_params)
135             if callable(list_filter):
136                 # This is simply a custom list filter class.
137                 spec = list_filter(request, lookup_params, self.model, self.model_admin)
138             else:
139                 field_path = None
140                 if isinstance(list_filter, (tuple, list)):
141                     # This is a custom FieldListFilter class for a given field.
142                     field, field_list_filter_class = list_filter
143                 else:
144                     # This is simply a field name, so use the default
145                     # FieldListFilter class that has been registered for the
146                     # type of the given field.
147                     field, field_list_filter_class = list_filter, FieldListFilter.create
148                 if not isinstance(field, Field):
149                     field_path = field
150                     field = get_fields_from_path(self.model, field_path)[-1]
151 
152                 spec = field_list_filter_class(
153                     field, request, lookup_params,
154                     self.model, self.model_admin, field_path=field_path,
155                 )
156                 # field_list_filter_class removes any lookup_params it
157                 # processes. If that happened, check if distinct() is needed to
158                 # remove duplicate results.
159                 if lookup_params_count > len(lookup_params):
160                     use_distinct = use_distinct or lookup_needs_distinct(self.lookup_opts, field_path)
161             if spec and spec.has_output():
162                 filter_specs.append(spec)
163                 if lookup_params_count > len(lookup_params):
164                     has_active_filters = True
165 
166         if self.date_hierarchy:
167             # Create bounded lookup parameters so that the query is more
168             # efficient.
169             year = lookup_params.pop('%s__year' % self.date_hierarchy, None)
170             if year is not None:
171                 month = lookup_params.pop('%s__month' % self.date_hierarchy, None)
172                 day = lookup_params.pop('%s__day' % self.date_hierarchy, None)
173                 try:
174                     from_date = datetime(
175                         int(year),
176                         int(month if month is not None else 1),
177                         int(day if day is not None else 1),
178                     )
179                 except ValueError as e:
180                     raise IncorrectLookupParameters(e) from e
181                 if day:
182                     to_date = from_date + timedelta(days=1)
183                 elif month:
184                     # In this branch, from_date will always be the first of a
185                     # month, so advancing 32 days gives the next month.
186                     to_date = (from_date + timedelta(days=32)).replace(day=1)
187                 else:
188                     to_date = from_date.replace(year=from_date.year + 1)
189                 if settings.USE_TZ:
190                     from_date = make_aware(from_date)
191                     to_date = make_aware(to_date)
192                 lookup_params.update({
193                     '%s__gte' % self.date_hierarchy: from_date,
194                     '%s__lt' % self.date_hierarchy: to_date,
195                 })
196 
197         # At this point, all the parameters used by the various ListFilters
198         # have been removed from lookup_params, which now only contains other
199         # parameters passed via the query string. We now loop through the
200         # remaining parameters both to ensure that all the parameters are valid
201         # fields and to determine if at least one of them needs distinct(). If
202         # the lookup parameters aren't real fields, then bail out.
203         try:
204             for key, value in lookup_params.items():
205                 lookup_params[key] = prepare_lookup_value(key, value)
206                 use_distinct = use_distinct or lookup_needs_distinct(self.lookup_opts, key)
207             return (
208                 filter_specs, bool(filter_specs), lookup_params, use_distinct,
209                 has_active_filters,
210             )
211         except FieldDoesNotExist as e:
212             raise IncorrectLookupParameters(e) from e
213 
214     def get_query_string(self, new_params=None, remove=None):
215         if new_params is None:
216             new_params = {}
217         if remove is None:
218             remove = []
219         p = self.params.copy()
220         for r in remove:
221             for k in list(p):
222                 if k.startswith(r):
223                     del p[k]
224         for k, v in new_params.items():
225             if v is None:
226                 if k in p:
227                     del p[k]
228             else:
229                 p[k] = v
230         return '?%s' % urlencode(sorted(p.items()))
231 
232     def get_results(self, request):
233         paginator = self.model_admin.get_paginator(request, self.queryset, self.list_per_page)
234         # Get the number of objects, with admin filters applied.
235         result_count = paginator.count
236 
237         # Get the total number of objects, with no admin filters applied.
238         if self.model_admin.show_full_result_count:
239             full_result_count = self.root_queryset.count()
240         else:
241             full_result_count = None
242         can_show_all = result_count <= self.list_max_show_all
243         multi_page = result_count > self.list_per_page
244 
245         # Get the list of objects to display on this page.
246         if (self.show_all and can_show_all) or not multi_page:
247             result_list = self.queryset._clone()
248         else:
249             try:
250                 result_list = paginator.page(self.page_num).object_list
251             except InvalidPage:
252                 raise IncorrectLookupParameters
253 
254         self.result_count = result_count
255         self.show_full_result_count = self.model_admin.show_full_result_count
256         # Admin actions are shown if there is at least one entry
257         # or if entries are not counted because show_full_result_count is disabled
258         self.show_admin_actions = not self.show_full_result_count or bool(full_result_count)
259         self.full_result_count = full_result_count
260         self.result_list = result_list
261         self.can_show_all = can_show_all
262         self.multi_page = multi_page
263         self.paginator = paginator
264 
265     def _get_default_ordering(self):
266         ordering = []
267         if self.model_admin.ordering:
268             ordering = self.model_admin.ordering
269         elif self.lookup_opts.ordering:
270             ordering = self.lookup_opts.ordering
271         return ordering
272 
273     def get_ordering_field(self, field_name):
274         """
275         Return the proper model field name corresponding to the given
276         field_name to use for ordering. field_name may either be the name of a
277         proper model field or the name of a method (on the admin or model) or a
278         callable with the 'admin_order_field' attribute. Return None if no
279         proper model field name can be matched.
280         """
281         try:
282             field = self.lookup_opts.get_field(field_name)
283             return field.name
284         except FieldDoesNotExist:
285             # See whether field_name is a name of a non-field
286             # that allows sorting.
287             if callable(field_name):
288                 attr = field_name
289             elif hasattr(self.model_admin, field_name):
290                 attr = getattr(self.model_admin, field_name)
291             else:
292                 attr = getattr(self.model, field_name)
293             if isinstance(attr, property) and hasattr(attr, 'fget'):
294                 attr = attr.fget
295             return getattr(attr, 'admin_order_field', None)
296 
297     def get_ordering(self, request, queryset):
298         """
299         Return the list of ordering fields for the change list.
300         First check the get_ordering() method in model admin, then check
301         the object's default ordering. Then, any manually-specified ordering
302         from the query string overrides anything. Finally, a deterministic
303         order is guaranteed by calling _get_deterministic_ordering() with the
304         constructed ordering.
305         """
306         params = self.params
307         ordering = list(self.model_admin.get_ordering(request) or self._get_default_ordering())
308         if ORDER_VAR in params:
309             # Clear ordering and used params
310             ordering = []
311             order_params = params[ORDER_VAR].split('.')
312             for p in order_params:
313                 try:
314                     none, pfx, idx = p.rpartition('-')
315                     field_name = self.list_display[int(idx)]
316                     order_field = self.get_ordering_field(field_name)
317                     if not order_field:
318                         continue  # No 'admin_order_field', skip it
319                     if isinstance(order_field, OrderBy):
320                         if pfx == '-':
321                             order_field = order_field.copy()
322                             order_field.reverse_ordering()
323                         ordering.append(order_field)
324                     elif hasattr(order_field, 'resolve_expression'):
325                         # order_field is an expression.
326                         ordering.append(order_field.desc() if pfx == '-' else order_field.asc())
327                     # reverse order if order_field has already "-" as prefix
328                     elif order_field.startswith('-') and pfx == '-':
329                         ordering.append(order_field[1:])
330                     else:
331                         ordering.append(pfx + order_field)
332                 except (IndexError, ValueError):
333                     continue  # Invalid ordering specified, skip it.
334 
335         # Add the given query's ordering fields, if any.
336         ordering.extend(queryset.query.order_by)
337 
338         return self._get_deterministic_ordering(ordering)
339 
340     def _get_deterministic_ordering(self, ordering):
341         """
342         Ensure a deterministic order across all database backends. Search for a
343         single field or unique together set of fields providing a total
344         ordering. If these are missing, augment the ordering with a descendant
345         primary key.
346         """
347         ordering = list(ordering)
348         ordering_fields = set()
349         total_ordering_fields = {'pk'} | {
350             field.attname for field in self.lookup_opts.fields
351             if field.unique and not field.null
352         }
353         for part in ordering:
354             # Search for single field providing a total ordering.
355             field_name = None
356             if isinstance(part, str):
357                 field_name = part.lstrip('-')
358             elif isinstance(part, F):
359                 field_name = part.name
360             elif isinstance(part, OrderBy) and isinstance(part.expression, F):
361                 field_name = part.expression.name
362             if field_name:
363                 # Normalize attname references by using get_field().
364                 try:
365                     field = self.lookup_opts.get_field(field_name)
366                 except FieldDoesNotExist:
367                     # Could be "?" for random ordering or a related field
368                     # lookup. Skip this part of introspection for now.
369                     continue
370                 # Ordering by a related field name orders by the referenced
371                 # model's ordering. Skip this part of introspection for now.
372                 if field.remote_field and field_name == field.name:
373                     continue
374                 if field.attname in total_ordering_fields:
375                     break
376                 ordering_fields.add(field.attname)
377         else:
378             # No single total ordering field, try unique_together and total
379             # unique constraints.
380             constraint_field_names = (
381                 *self.lookup_opts.unique_together,
382                 *(
383                     constraint.fields
384                     for constraint in self.lookup_opts.total_unique_constraints
385                 ),
386             )
387             for field_names in constraint_field_names:
388                 # Normalize attname references by using get_field().
389                 fields = [self.lookup_opts.get_field(field_name) for field_name in field_names]
390                 # Composite unique constraints containing a nullable column
391                 # cannot ensure total ordering.
392                 if any(field.null for field in fields):
393                     continue
394                 if ordering_fields.issuperset(field.attname for field in fields):
395                     break
396             else:
397                 # If no set of unique fields is present in the ordering, rely
398                 # on the primary key to provide total ordering.
399                 ordering.append('-pk')
400         return ordering
401 
402     def get_ordering_field_columns(self):
403         """
404         Return a dictionary of ordering field column numbers and asc/desc.
405         """
406         # We must cope with more than one column having the same underlying sort
407         # field, so we base things on column numbers.
408         ordering = self._get_default_ordering()
409         ordering_fields = {}
410         if ORDER_VAR not in self.params:
411             # for ordering specified on ModelAdmin or model Meta, we don't know
412             # the right column numbers absolutely, because there might be more
413             # than one column associated with that ordering, so we guess.
414             for field in ordering:
415                 if isinstance(field, (Combinable, OrderBy)):
416                     if not isinstance(field, OrderBy):
417                         field = field.asc()
418                     if isinstance(field.expression, F):
419                         order_type = 'desc' if field.descending else 'asc'
420                         field = field.expression.name
421                     else:
422                         continue
423                 elif field.startswith('-'):
424                     field = field[1:]
425                     order_type = 'desc'
426                 else:
427                     order_type = 'asc'
428                 for index, attr in enumerate(self.list_display):
429                     if self.get_ordering_field(attr) == field:
430                         ordering_fields[index] = order_type
431                         break
432         else:
433             for p in self.params[ORDER_VAR].split('.'):
434                 none, pfx, idx = p.rpartition('-')
435                 try:
436                     idx = int(idx)
437                 except ValueError:
438                     continue  # skip it
439                 ordering_fields[idx] = 'desc' if pfx == '-' else 'asc'
440         return ordering_fields
441 
442     def get_queryset(self, request):
443         # First, we collect all the declared list filters.
444         (
445             self.filter_specs,
446             self.has_filters,
447             remaining_lookup_params,
448             filters_use_distinct,
449             self.has_active_filters,
450         ) = self.get_filters(request)
451         # Then, we let every list filter modify the queryset to its liking.
452         qs = self.root_queryset
453         for filter_spec in self.filter_specs:
454             new_qs = filter_spec.queryset(request, qs)
455             if new_qs is not None:
456                 qs = new_qs
457 
458         try:
459             # Finally, we apply the remaining lookup parameters from the query
460             # string (i.e. those that haven't already been processed by the
461             # filters).
462             qs = qs.filter(**remaining_lookup_params)
463         except (SuspiciousOperation, ImproperlyConfigured):
464             # Allow certain types of errors to be re-raised as-is so that the
465             # caller can treat them in a special way.
466             raise
467         except Exception as e:
468             # Every other error is caught with a naked except, because we don't
469             # have any other way of validating lookup parameters. They might be
470             # invalid if the keyword arguments are incorrect, or if the values
471             # are not in the correct type, so we might get FieldError,
472             # ValueError, ValidationError, or ?.
473             raise IncorrectLookupParameters(e)
474 
475         if not qs.query.select_related:
476             qs = self.apply_select_related(qs)
477 
478         # Set ordering.
479         ordering = self.get_ordering(request, qs)
480         qs = qs.order_by(*ordering)
481 
482         # Apply search results
483         qs, search_use_distinct = self.model_admin.get_search_results(request, qs, self.query)
484 
485         # Set query string for clearing all filters.
486         self.clear_all_filters_qs = self.get_query_string(
487             new_params=remaining_lookup_params,
488             remove=self.get_filters_params(),
489         )
490         # Remove duplicates from results, if necessary
491         if filters_use_distinct | search_use_distinct:
492             return qs.distinct()
493         else:
494             return qs
495 
496     def apply_select_related(self, qs):
497         if self.list_select_related is True:
498             return qs.select_related()
499 
500         if self.list_select_related is False:
501             if self.has_related_field_in_list_display():
502                 return qs.select_related()
503 
504         if self.list_select_related:
505             return qs.select_related(*self.list_select_related)
506         return qs
507 
508     def has_related_field_in_list_display(self):
509         for field_name in self.list_display:
510             try:
511                 field = self.lookup_opts.get_field(field_name)
512             except FieldDoesNotExist:
513                 pass
514             else:
515                 if isinstance(field.remote_field, ManyToOneRel):
516                     # <FK>_id field names don't require a join.
517                     if field_name != field.get_attname():
518                         return True
519         return False
520 
521     def url_for_result(self, result):
522         pk = getattr(result, self.pk_attname)
523         return reverse('admin:%s_%s_change' % (self.opts.app_label,
524                                                self.opts.model_name),
525                        args=(quote(pk),),
526                        current_app=self.model_admin.admin_site.name)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>delete</method>
<code>
722     def delete(self):
723         """Delete the records in the current QuerySet."""
724         self._not_support_combined_queries('delete')
725         if self.query.is_sliced:
726             raise TypeError("Cannot use 'limit' or 'offset' with delete().")
727         if self.query.distinct or self.query.distinct_fields:
728             raise TypeError('Cannot call delete() after .distinct().')
729         if self._fields is not None:
730             raise TypeError("Cannot call delete() after .values() or .values_list()")
731 
732         del_query = self._chain()
733 
734         # The delete is actually 2 queries - one to find related objects,
735         # and one to delete. Make sure that the discovery of related
736         # objects is performed on the same database as the deletion.
737         del_query._for_write = True
738 
739         # Disable non-supported fields.
740         del_query.query.select_for_update = False
741         del_query.query.select_related = False
742         del_query.query.clear_ordering(force_empty=True)
743 
744         collector = Collector(using=del_query.db)
745         collector.collect(del_query)
746         deleted, _rows_count = collector.delete()
747 
748         # Clear the result cache, in case this QuerySet gets reused.
749         self._result_cache = None
750         return deleted, _rows_count

</code><intended_behavior>Before the actual deletion operation, check if the queryset has been altered by `.distinct()`, and if so, reconstruct the queryset from the primary keys of the current queryset's results. This will ensure that the deletion operation proceeds on a queryset that is compatible with deletion. Code modifications might involve a pre-deletion check that explicitly handles or warns about issues arising from attempting deletion on querysets with `.distinct()`. This pre-check could offer a pathway to handle such cases more gracefully, possibly by re-fetching objects based on their IDs alone for deletion.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
175 class QuerySet:
176     """Represent a lazy database lookup for a set of objects."""
177 
178     def __init__(self, model=None, query=None, using=None, hints=None):
179         self.model = model
180         self._db = using
181         self._hints = hints or {}
182         self._query = query or sql.Query(self.model)
183         self._result_cache = None
184         self._sticky_filter = False
185         self._for_write = False
186         self._prefetch_related_lookups = ()
187         self._prefetch_done = False
188         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
189         self._iterable_class = ModelIterable
190         self._fields = None
191         self._defer_next_filter = False
192         self._deferred_filter = None
193 
194     @property
195     def query(self):
196         if self._deferred_filter:
197             negate, args, kwargs = self._deferred_filter
198             self._filter_or_exclude_inplace(negate, args, kwargs)
199             self._deferred_filter = None
200         return self._query
201 
202     @query.setter
203     def query(self, value):
204         if value.values_select:
205             self._iterable_class = ValuesIterable
206         self._query = value
207 
208     def as_manager(cls):
209         # Address the circular dependency between `Queryset` and `Manager`.
210         from django.db.models.manager import Manager
211         manager = Manager.from_queryset(cls)()
212         manager._built_with_as_manager = True
213         return manager
214     as_manager.queryset_only = True
215     as_manager = classmethod(as_manager)
216 
217     ########################
218     # PYTHON MAGIC METHODS #
219     ########################
220 
221     def __deepcopy__(self, memo):
222         """Don't populate the QuerySet's cache."""
223         obj = self.__class__()
224         for k, v in self.__dict__.items():
225             if k == '_result_cache':
226                 obj.__dict__[k] = None
227             else:
228                 obj.__dict__[k] = copy.deepcopy(v, memo)
229         return obj
230 
231     def __getstate__(self):
232         # Force the cache to be fully populated.
233         self._fetch_all()
234         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
235 
236     def __setstate__(self, state):
237         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
238         if pickled_version:
239             if pickled_version != django.__version__:
240                 warnings.warn(
241                     "Pickled queryset instance's Django version %s does not "
242                     "match the current version %s."
243                     % (pickled_version, django.__version__),
244                     RuntimeWarning,
245                     stacklevel=2,
246                 )
247         else:
248             warnings.warn(
249                 "Pickled queryset instance's Django version is not specified.",
250                 RuntimeWarning,
251                 stacklevel=2,
252             )
253         self.__dict__.update(state)
254 
255     def __repr__(self):
256         data = list(self[:REPR_OUTPUT_SIZE + 1])
257         if len(data) > REPR_OUTPUT_SIZE:
258             data[-1] = "...(remaining elements truncated)..."
259         return '<%s %r>' % (self.__class__.__name__, data)
260 
261     def __len__(self):
262         self._fetch_all()
263         return len(self._result_cache)
264 
265     def __iter__(self):
266         """
267         The queryset iterator protocol uses three nested iterators in the
268         default case:
269             1. sql.compiler.execute_sql()
270                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
271                  using cursor.fetchmany(). This part is responsible for
272                  doing some column masking, and returning the rows in chunks.
273             2. sql.compiler.results_iter()
274                - Returns one row at time. At this point the rows are still just
275                  tuples. In some cases the return values are converted to
276                  Python values at this location.
277             3. self.iterator()
278                - Responsible for turning the rows into model objects.
279         """
280         self._fetch_all()
281         return iter(self._result_cache)
282 
283     def __bool__(self):
284         self._fetch_all()
285         return bool(self._result_cache)
286 
287     def __getitem__(self, k):
288         """Retrieve an item or slice from the set of results."""
289         if not isinstance(k, (int, slice)):
290             raise TypeError(
291                 'QuerySet indices must be integers or slices, not %s.'
292                 % type(k).__name__
293             )
294         assert ((not isinstance(k, slice) and (k >= 0)) or
295                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
296                  (k.stop is None or k.stop >= 0))), \
297             "Negative indexing is not supported."
298 
299         if self._result_cache is not None:
300             return self._result_cache[k]
301 
302         if isinstance(k, slice):
303             qs = self._chain()
304             if k.start is not None:
305                 start = int(k.start)
306             else:
307                 start = None
308             if k.stop is not None:
309                 stop = int(k.stop)
310             else:
311                 stop = None
312             qs.query.set_limits(start, stop)
313             return list(qs)[::k.step] if k.step else qs
314 
315         qs = self._chain()
316         qs.query.set_limits(k, k + 1)
317         qs._fetch_all()
318         return qs._result_cache[0]
319 
320     def __class_getitem__(cls, *args, **kwargs):
321         return cls
322 
323     def __and__(self, other):
324         self._merge_sanity_check(other)
325         if isinstance(other, EmptyQuerySet):
326             return other
327         if isinstance(self, EmptyQuerySet):
328             return self
329         combined = self._chain()
330         combined._merge_known_related_objects(other)
331         combined.query.combine(other.query, sql.AND)
332         return combined
333 
334     def __or__(self, other):
335         self._merge_sanity_check(other)
336         if isinstance(self, EmptyQuerySet):
337             return other
338         if isinstance(other, EmptyQuerySet):
339             return self
340         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
341         combined = query._chain()
342         combined._merge_known_related_objects(other)
343         if not other.query.can_filter():
344             other = other.model._base_manager.filter(pk__in=other.values('pk'))
345         combined.query.combine(other.query, sql.OR)
346         return combined
347 
348     ####################################
349     # METHODS THAT DO DATABASE QUERIES #
350     ####################################
351 
352     def _iterator(self, use_chunked_fetch, chunk_size):
353         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
354 
355     def iterator(self, chunk_size=2000):
356         """
357         An iterator over the results from applying this QuerySet to the
358         database.
359         """
360         if chunk_size <= 0:
361             raise ValueError('Chunk size must be strictly positive.')
362         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
363         return self._iterator(use_chunked_fetch, chunk_size)
364 
365     def aggregate(self, *args, **kwargs):
366         """
367         Return a dictionary containing the calculations (aggregation)
368         over the current queryset.
369 
370         If args is present the expression is passed as a kwarg using
371         the Aggregate object's default alias.
372         """
373         if self.query.distinct_fields:
374             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
375         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
376         for arg in args:
377             # The default_alias property raises TypeError if default_alias
378             # can't be set automatically or AttributeError if it isn't an
379             # attribute.
380             try:
381                 arg.default_alias
382             except (AttributeError, TypeError):
383                 raise TypeError("Complex aggregates require an alias")
384             kwargs[arg.default_alias] = arg
385 
386         query = self.query.chain()
387         for (alias, aggregate_expr) in kwargs.items():
388             query.add_annotation(aggregate_expr, alias, is_summary=True)
389             annotation = query.annotations[alias]
390             if not annotation.contains_aggregate:
391                 raise TypeError("%s is not an aggregate expression" % alias)
392             for expr in annotation.get_source_expressions():
393                 if expr.contains_aggregate and isinstance(expr, Ref) and expr.refs in kwargs:
394                     name = expr.refs
395                     raise exceptions.FieldError(
396                         "Cannot compute %s('%s'): '%s' is an aggregate"
397                         % (annotation.name, name, name)
398                     )
399         return query.get_aggregation(self.db, kwargs)
400 
401     def count(self):
402         """
403         Perform a SELECT COUNT() and return the number of records as an
404         integer.
405 
406         If the QuerySet is already fully cached, return the length of the
407         cached results set to avoid multiple SELECT COUNT(*) calls.
408         """
409         if self._result_cache is not None:
410             return len(self._result_cache)
411 
412         return self.query.get_count(using=self.db)
413 
414     def get(self, *args, **kwargs):
415         """
416         Perform the query and return a single object matching the given
417         keyword arguments.
418         """
419         if self.query.combinator and (args or kwargs):
420             raise NotSupportedError(
421                 'Calling QuerySet.get(...) with filters after %s() is not '
422                 'supported.' % self.query.combinator
423             )
424         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
425         if self.query.can_filter() and not self.query.distinct_fields:
426             clone = clone.order_by()
427         limit = None
428         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
429             limit = MAX_GET_RESULTS
430             clone.query.set_limits(high=limit)
431         num = len(clone)
432         if num == 1:
433             return clone._result_cache[0]
434         if not num:
435             raise self.model.DoesNotExist(
436                 "%s matching query does not exist." %
437                 self.model._meta.object_name
438             )
439         raise self.model.MultipleObjectsReturned(
440             'get() returned more than one %s -- it returned %s!' % (
441                 self.model._meta.object_name,
442                 num if not limit or num < limit else 'more than %s' % (limit - 1),
443             )
444         )
445 
446     def create(self, **kwargs):
447         """
448         Create a new object with the given kwargs, saving it to the database
449         and returning the created object.
450         """
451         obj = self.model(**kwargs)
452         self._for_write = True
453         obj.save(force_insert=True, using=self.db)
454         return obj
455 
456     def _prepare_for_bulk_create(self, objs):
457         for obj in objs:
458             if obj.pk is None:
459                 # Populate new PK values.
460                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
461             obj._prepare_related_fields_for_save(operation_name='bulk_create')
462 
463     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
464         """
465         Insert each of the instances into the database. Do *not* call
466         save() on each of the instances, do not send any pre/post_save
467         signals, and do not set the primary key attribute if it is an
468         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
469         Multi-table models are not supported.
470         """
471         # When you bulk insert you don't get the primary keys back (if it's an
472         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
473         # you can't insert into the child tables which references this. There
474         # are two workarounds:
475         # 1) This could be implemented if you didn't have an autoincrement pk
476         # 2) You could do it by doing O(n) normal inserts into the parent
477         #    tables to get the primary keys back and then doing a single bulk
478         #    insert into the childmost table.
479         # We currently set the primary keys on the objects when using
480         # PostgreSQL via the RETURNING ID clause. It should be possible for
481         # Oracle as well, but the semantics for extracting the primary keys is
482         # trickier so it's not done yet.
483         assert batch_size is None or batch_size > 0
484         # Check that the parents share the same concrete model with the our
485         # model to detect the inheritance pattern ConcreteGrandParent ->
486         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
487         # would not identify that case as involving multiple tables.
488         for parent in self.model._meta.get_parent_list():
489             if parent._meta.concrete_model is not self.model._meta.concrete_model:
490                 raise ValueError("Can't bulk create a multi-table inherited model")
491         if not objs:
492             return objs
493         self._for_write = True
494         connection = connections[self.db]
495         opts = self.model._meta
496         fields = opts.concrete_fields
497         objs = list(objs)
498         self._prepare_for_bulk_create(objs)
499         with transaction.atomic(using=self.db, savepoint=False):
500             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
501             if objs_with_pk:
502                 returned_columns = self._batched_insert(
503                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
504                 )
505                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
506                     for result, field in zip(results, opts.db_returning_fields):
507                         if field != opts.pk:
508                             setattr(obj_with_pk, field.attname, result)
509                 for obj_with_pk in objs_with_pk:
510                     obj_with_pk._state.adding = False
511                     obj_with_pk._state.db = self.db
512             if objs_without_pk:
513                 fields = [f for f in fields if not isinstance(f, AutoField)]
514                 returned_columns = self._batched_insert(
515                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
516                 )
517                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
518                     assert len(returned_columns) == len(objs_without_pk)
519                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
520                     for result, field in zip(results, opts.db_returning_fields):
521                         setattr(obj_without_pk, field.attname, result)
522                     obj_without_pk._state.adding = False
523                     obj_without_pk._state.db = self.db
524 
525         return objs
526 
527     def bulk_update(self, objs, fields, batch_size=None):
528         """
529         Update the given fields in each of the given objects in the database.
530         """
531         if batch_size is not None and batch_size < 0:
532             raise ValueError('Batch size must be a positive integer.')
533         if not fields:
534             raise ValueError('Field names must be given to bulk_update().')
535         objs = tuple(objs)
536         if any(obj.pk is None for obj in objs):
537             raise ValueError('All bulk_update() objects must have a primary key set.')
538         fields = [self.model._meta.get_field(name) for name in fields]
539         if any(not f.concrete or f.many_to_many for f in fields):
540             raise ValueError('bulk_update() can only be used with concrete fields.')
541         if any(f.primary_key for f in fields):
542             raise ValueError('bulk_update() cannot be used with primary key fields.')
543         if not objs:
544             return
545         # PK is used twice in the resulting update query, once in the filter
546         # and once in the WHEN. Each field will also have one CAST.
547         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
548         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
549         requires_casting = connections[self.db].features.requires_casted_case_in_updates
550         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
551         updates = []
552         for batch_objs in batches:
553             update_kwargs = {}
554             for field in fields:
555                 when_statements = []
556                 for obj in batch_objs:
557                     attr = getattr(obj, field.attname)
558                     if not isinstance(attr, Expression):
559                         attr = Value(attr, output_field=field)
560                     when_statements.append(When(pk=obj.pk, then=attr))
561                 case_statement = Case(*when_statements, output_field=field)
562                 if requires_casting:
563                     case_statement = Cast(case_statement, output_field=field)
564                 update_kwargs[field.attname] = case_statement
565             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
566         with transaction.atomic(using=self.db, savepoint=False):
567             for pks, update_kwargs in updates:
568                 self.filter(pk__in=pks).update(**update_kwargs)
569     bulk_update.alters_data = True
570 
571     def get_or_create(self, defaults=None, **kwargs):
572         """
573         Look up an object with the given kwargs, creating one if necessary.
574         Return a tuple of (object, created), where created is a boolean
575         specifying whether an object was created.
576         """
577         # The get() needs to be targeted at the write database in order
578         # to avoid potential transaction consistency problems.
579         self._for_write = True
580         try:
581             return self.get(**kwargs), False
582         except self.model.DoesNotExist:
583             params = self._extract_model_params(defaults, **kwargs)
584             # Try to create an object using passed params.
585             try:
586                 with transaction.atomic(using=self.db):
587                     params = dict(resolve_callables(params))
588                     return self.create(**params), True
589             except IntegrityError:
590                 try:
591                     return self.get(**kwargs), False
592                 except self.model.DoesNotExist:
593                     pass
594                 raise
595 
596     def update_or_create(self, defaults=None, **kwargs):
597         """
598         Look up an object with the given kwargs, updating one with defaults
599         if it exists, otherwise create a new one.
600         Return a tuple (object, created), where created is a boolean
601         specifying whether an object was created.
602         """
603         defaults = defaults or {}
604         self._for_write = True
605         with transaction.atomic(using=self.db):
606             # Lock the row so that a concurrent update is blocked until
607             # update_or_create() has performed its save.
608             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)
609             if created:
610                 return obj, created
611             for k, v in resolve_callables(defaults):
612                 setattr(obj, k, v)
613             obj.save(using=self.db)
614         return obj, False
615 
616     def _extract_model_params(self, defaults, **kwargs):
617         """
618         Prepare `params` for creating a model instance based on the given
619         kwargs; for use by get_or_create().
620         """
621         defaults = defaults or {}
622         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
623         params.update(defaults)
624         property_names = self.model._meta._property_names
625         invalid_params = []
626         for param in params:
627             try:
628                 self.model._meta.get_field(param)
629             except exceptions.FieldDoesNotExist:
630                 # It's okay to use a model's property if it has a setter.
631                 if not (param in property_names and getattr(self.model, param).fset):
632                     invalid_params.append(param)
633         if invalid_params:
634             raise exceptions.FieldError(
635                 "Invalid field name(s) for model %s: '%s'." % (
636                     self.model._meta.object_name,
637                     "', '".join(sorted(invalid_params)),
638                 ))
639         return params
640 
641     def _earliest(self, *fields):
642         """
643         Return the earliest object according to fields (if given) or by the
644         model's Meta.get_latest_by.
645         """
646         if fields:
647             order_by = fields
648         else:
649             order_by = getattr(self.model._meta, 'get_latest_by')
650             if order_by and not isinstance(order_by, (tuple, list)):
651                 order_by = (order_by,)
652         if order_by is None:
653             raise ValueError(
654                 "earliest() and latest() require either fields as positional "
655                 "arguments or 'get_latest_by' in the model's Meta."
656             )
657         obj = self._chain()
658         obj.query.set_limits(high=1)
659         obj.query.clear_ordering(force_empty=True)
660         obj.query.add_ordering(*order_by)
661         return obj.get()
662 
663     def earliest(self, *fields):
664         if self.query.is_sliced:
665             raise TypeError('Cannot change a query once a slice has been taken.')
666         return self._earliest(*fields)
667 
668     def latest(self, *fields):
669         if self.query.is_sliced:
670             raise TypeError('Cannot change a query once a slice has been taken.')
671         return self.reverse()._earliest(*fields)
672 
673     def first(self):
674         """Return the first object of a query or None if no match is found."""
675         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
676             return obj
677 
678     def last(self):
679         """Return the last object of a query or None if no match is found."""
680         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
681             return obj
682 
683     def in_bulk(self, id_list=None, *, field_name='pk'):
684         """
685         Return a dictionary mapping each of the given IDs to the object with
686         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
687         """
688         if self.query.is_sliced:
689             raise TypeError("Cannot use 'limit' or 'offset' with in_bulk().")
690         opts = self.model._meta
691         unique_fields = [
692             constraint.fields[0]
693             for constraint in opts.total_unique_constraints
694             if len(constraint.fields) == 1
695         ]
696         if (
697             field_name != 'pk' and
698             not opts.get_field(field_name).unique and
699             field_name not in unique_fields and
700             self.query.distinct_fields != (field_name,)
701         ):
702             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
703         if id_list is not None:
704             if not id_list:
705                 return {}
706             filter_key = '{}__in'.format(field_name)
707             batch_size = connections[self.db].features.max_query_params
708             id_list = tuple(id_list)
709             # If the database has a limit on the number of query parameters
710             # (e.g. SQLite), retrieve objects in batches if necessary.
711             if batch_size and batch_size < len(id_list):
712                 qs = ()
713                 for offset in range(0, len(id_list), batch_size):
714                     batch = id_list[offset:offset + batch_size]
715                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
716             else:
717                 qs = self.filter(**{filter_key: id_list}).order_by()
718         else:
719             qs = self._chain()
720         return {getattr(obj, field_name): obj for obj in qs}
721 
722     def delete(self):
723         """Delete the records in the current QuerySet."""
724         self._not_support_combined_queries('delete')
725         if self.query.is_sliced:
726             raise TypeError("Cannot use 'limit' or 'offset' with delete().")
727         if self.query.distinct or self.query.distinct_fields:
728             raise TypeError('Cannot call delete() after .distinct().')
729         if self._fields is not None:
730             raise TypeError("Cannot call delete() after .values() or .values_list()")
731 
732         del_query = self._chain()
733 
734         # The delete is actually 2 queries - one to find related objects,
735         # and one to delete. Make sure that the discovery of related
736         # objects is performed on the same database as the deletion.
737         del_query._for_write = True
738 
739         # Disable non-supported fields.
740         del_query.query.select_for_update = False
741         del_query.query.select_related = False
742         del_query.query.clear_ordering(force_empty=True)
743 
744         collector = Collector(using=del_query.db)
745         collector.collect(del_query)
746         deleted, _rows_count = collector.delete()
747 
748         # Clear the result cache, in case this QuerySet gets reused.
749         self._result_cache = None
750         return deleted, _rows_count
751 
752     delete.alters_data = True
753     delete.queryset_only = True
754 
755     def _raw_delete(self, using):
756         """
757         Delete objects found from the given queryset in single direct SQL
758         query. No signals are sent and there is no protection for cascades.
759         """
760         query = self.query.clone()
761         query.__class__ = sql.DeleteQuery
762         cursor = query.get_compiler(using).execute_sql(CURSOR)
763         if cursor:
764             with cursor:
765                 return cursor.rowcount
766         return 0
767     _raw_delete.alters_data = True
768 
769     def update(self, **kwargs):
770         """
771         Update all elements in the current QuerySet, setting all the given
772         fields to the appropriate values.
773         """
774         self._not_support_combined_queries('update')
775         if self.query.is_sliced:
776             raise TypeError('Cannot update a query once a slice has been taken.')
777         self._for_write = True
778         query = self.query.chain(sql.UpdateQuery)
779         query.add_update_values(kwargs)
780         # Clear any annotations so that they won't be present in subqueries.
781         query.annotations = {}
782         with transaction.mark_for_rollback_on_error(using=self.db):
783             rows = query.get_compiler(self.db).execute_sql(CURSOR)
784         self._result_cache = None
785         return rows
786     update.alters_data = True
787 
788     def _update(self, values):
789         """
790         A version of update() that accepts field objects instead of field names.
791         Used primarily for model saving and not intended for use by general
792         code (it requires too much poking around at model internals to be
793         useful at that level).
794         """
795         if self.query.is_sliced:
796             raise TypeError('Cannot update a query once a slice has been taken.')
797         query = self.query.chain(sql.UpdateQuery)
798         query.add_update_fields(values)
799         # Clear any annotations so that they won't be present in subqueries.
800         query.annotations = {}
801         self._result_cache = None
802         return query.get_compiler(self.db).execute_sql(CURSOR)
803     _update.alters_data = True
804     _update.queryset_only = False
805 
806     def exists(self):
807         if self._result_cache is None:
808             return self.query.has_results(using=self.db)
809         return bool(self._result_cache)
810 
811     def contains(self, obj):
812         """Return True if the queryset contains an object."""
813         self._not_support_combined_queries('contains')
814         if self._fields is not None:
815             raise TypeError(
816                 'Cannot call QuerySet.contains() after .values() or '
817                 '.values_list().'
818             )
819         try:
820             if obj._meta.concrete_model != self.model._meta.concrete_model:
821                 return False
822         except AttributeError:
823             raise TypeError("'obj' must be a model instance.")
824         if obj.pk is None:
825             raise ValueError(
826                 'QuerySet.contains() cannot be used on unsaved objects.'
827             )
828         if self._result_cache is not None:
829             return obj in self._result_cache
830         return self.filter(pk=obj.pk).exists()
831 
832     def _prefetch_related_objects(self):
833         # This method can only be called once the result cache has been filled.
834         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
835         self._prefetch_done = True
836 
837     def explain(self, *, format=None, **options):
838         return self.query.explain(using=self.db, format=format, **options)
839 
840     ##################################################
841     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
842     ##################################################
843 
844     def raw(self, raw_query, params=(), translations=None, using=None):
845         if using is None:
846             using = self.db
847         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
848         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
849         return qs
850 
851     def _values(self, *fields, **expressions):
852         clone = self._chain()
853         if expressions:
854             clone = clone.annotate(**expressions)
855         clone._fields = fields
856         clone.query.set_values(fields)
857         return clone
858 
859     def values(self, *fields, **expressions):
860         fields += tuple(expressions)
861         clone = self._values(*fields, **expressions)
862         clone._iterable_class = ValuesIterable
863         return clone
864 
865     def values_list(self, *fields, flat=False, named=False):
866         if flat and named:
867             raise TypeError("'flat' and 'named' can't be used together.")
868         if flat and len(fields) > 1:
869             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
870 
871         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
872         _fields = []
873         expressions = {}
874         counter = 1
875         for field in fields:
876             if hasattr(field, 'resolve_expression'):
877                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
878                 while True:
879                     field_id = field_id_prefix + str(counter)
880                     counter += 1
881                     if field_id not in field_names:
882                         break
883                 expressions[field_id] = field
884                 _fields.append(field_id)
885             else:
886                 _fields.append(field)
887 
888         clone = self._values(*_fields, **expressions)
889         clone._iterable_class = (
890             NamedValuesListIterable if named
891             else FlatValuesListIterable if flat
892             else ValuesListIterable
893         )
894         return clone
895 
896     def dates(self, field_name, kind, order='ASC'):
897         """
898         Return a list of date objects representing all available dates for
899         the given field_name, scoped to 'kind'.
900         """
901         assert kind in ('year', 'month', 'week', 'day'), \
902             "'kind' must be one of 'year', 'month', 'week', or 'day'."
903         assert order in ('ASC', 'DESC'), \
904             "'order' must be either 'ASC' or 'DESC'."
905         return self.annotate(
906             datefield=Trunc(field_name, kind, output_field=DateField()),
907             plain_field=F(field_name)
908         ).values_list(
909             'datefield', flat=True
910         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
911 
912     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):
913         """
914         Return a list of datetime objects representing all available
915         datetimes for the given field_name, scoped to 'kind'.
916         """
917         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
918             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
919         assert order in ('ASC', 'DESC'), \
920             "'order' must be either 'ASC' or 'DESC'."
921         if settings.USE_TZ:
922             if tzinfo is None:
923                 tzinfo = timezone.get_current_timezone()
924         else:
925             tzinfo = None
926         return self.annotate(
927             datetimefield=Trunc(
928                 field_name,
929                 kind,
930                 output_field=DateTimeField(),
931                 tzinfo=tzinfo,
932                 is_dst=is_dst,
933             ),
934             plain_field=F(field_name)
935         ).values_list(
936             'datetimefield', flat=True
937         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
938 
939     def none(self):
940         """Return an empty QuerySet."""
941         clone = self._chain()
942         clone.query.set_empty()
943         return clone
944 
945     ##################################################################
946     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
947     ##################################################################
948 
949     def all(self):
950         """
951         Return a new QuerySet that is a copy of the current one. This allows a
952         QuerySet to proxy for a model manager in some cases.
953         """
954         return self._chain()
955 
956     def filter(self, *args, **kwargs):
957         """
958         Return a new QuerySet instance with the args ANDed to the existing
959         set.
960         """
961         self._not_support_combined_queries('filter')
962         return self._filter_or_exclude(False, args, kwargs)
963 
964     def exclude(self, *args, **kwargs):
965         """
966         Return a new QuerySet instance with NOT (args) ANDed to the existing
967         set.
968         """
969         self._not_support_combined_queries('exclude')
970         return self._filter_or_exclude(True, args, kwargs)
971 
972     def _filter_or_exclude(self, negate, args, kwargs):
973         if (args or kwargs) and self.query.is_sliced:
974             raise TypeError('Cannot filter a query once a slice has been taken.')
975         clone = self._chain()
976         if self._defer_next_filter:
977             self._defer_next_filter = False
978             clone._deferred_filter = negate, args, kwargs
979         else:
980             clone._filter_or_exclude_inplace(negate, args, kwargs)
981         return clone
982 
983     def _filter_or_exclude_inplace(self, negate, args, kwargs):
984         if negate:
985             self._query.add_q(~Q(*args, **kwargs))
986         else:
987             self._query.add_q(Q(*args, **kwargs))
988 
989     def complex_filter(self, filter_obj):
990         """
991         Return a new QuerySet instance with filter_obj added to the filters.
992 
993         filter_obj can be a Q object or a dictionary of keyword lookup
994         arguments.
995 
996         This exists to support framework features such as 'limit_choices_to',
997         and usually it will be more natural to use other methods.
998         """
999         if isinstance(filter_obj, Q):
1000             clone = self._chain()
1001             clone.query.add_q(filter_obj)
1002             return clone
1003         else:
1004             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
1005 
1006     def _combinator_query(self, combinator, *other_qs, all=False):
1007         # Clone the query to inherit the select list and everything
1008         clone = self._chain()
1009         # Clear limits and ordering so they can be reapplied
1010         clone.query.clear_ordering(True)
1011         clone.query.clear_limits()
1012         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
1013         clone.query.combinator = combinator
1014         clone.query.combinator_all = all
1015         return clone
1016 
1017     def union(self, *other_qs, all=False):
1018         # If the query is an EmptyQuerySet, combine all nonempty querysets.
1019         if isinstance(self, EmptyQuerySet):
1020             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1021             if not qs:
1022                 return self
1023             if len(qs) == 1:
1024                 return qs[0]
1025             return qs[0]._combinator_query('union', *qs[1:], all=all)
1026         return self._combinator_query('union', *other_qs, all=all)
1027 
1028     def intersection(self, *other_qs):
1029         # If any query is an EmptyQuerySet, return it.
1030         if isinstance(self, EmptyQuerySet):
1031             return self
1032         for other in other_qs:
1033             if isinstance(other, EmptyQuerySet):
1034                 return other
1035         return self._combinator_query('intersection', *other_qs)
1036 
1037     def difference(self, *other_qs):
1038         # If the query is an EmptyQuerySet, return it.
1039         if isinstance(self, EmptyQuerySet):
1040             return self
1041         return self._combinator_query('difference', *other_qs)
1042 
1043     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1044         """
1045         Return a new QuerySet instance that will select objects with a
1046         FOR UPDATE lock.
1047         """
1048         if nowait and skip_locked:
1049             raise ValueError('The nowait option cannot be used with skip_locked.')
1050         obj = self._chain()
1051         obj._for_write = True
1052         obj.query.select_for_update = True
1053         obj.query.select_for_update_nowait = nowait
1054         obj.query.select_for_update_skip_locked = skip_locked
1055         obj.query.select_for_update_of = of
1056         obj.query.select_for_no_key_update = no_key
1057         return obj
1058 
1059     def select_related(self, *fields):
1060         """
1061         Return a new QuerySet instance that will select related objects.
1062 
1063         If fields are specified, they must be ForeignKey fields and only those
1064         related objects are included in the selection.
1065 
1066         If select_related(None) is called, clear the list.
1067         """
1068         self._not_support_combined_queries('select_related')
1069         if self._fields is not None:
1070             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1071 
1072         obj = self._chain()
1073         if fields == (None,):
1074             obj.query.select_related = False
1075         elif fields:
1076             obj.query.add_select_related(fields)
1077         else:
1078             obj.query.select_related = True
1079         return obj
1080 
1081     def prefetch_related(self, *lookups):
1082         """
1083         Return a new QuerySet instance that will prefetch the specified
1084         Many-To-One and Many-To-Many related objects when the QuerySet is
1085         evaluated.
1086 
1087         When prefetch_related() is called more than once, append to the list of
1088         prefetch lookups. If prefetch_related(None) is called, clear the list.
1089         """
1090         self._not_support_combined_queries('prefetch_related')
1091         clone = self._chain()
1092         if lookups == (None,):
1093             clone._prefetch_related_lookups = ()
1094         else:
1095             for lookup in lookups:
1096                 if isinstance(lookup, Prefetch):
1097                     lookup = lookup.prefetch_to
1098                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1099                 if lookup in self.query._filtered_relations:
1100                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1101             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1102         return clone
1103 
1104     def annotate(self, *args, **kwargs):
1105         """
1106         Return a query set in which the returned objects have been annotated
1107         with extra data or aggregations.
1108         """
1109         self._not_support_combined_queries('annotate')
1110         return self._annotate(args, kwargs, select=True)
1111 
1112     def alias(self, *args, **kwargs):
1113         """
1114         Return a query set with added aliases for extra data or aggregations.
1115         """
1116         self._not_support_combined_queries('alias')
1117         return self._annotate(args, kwargs, select=False)
1118 
1119     def _annotate(self, args, kwargs, select=True):
1120         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1121         annotations = {}
1122         for arg in args:
1123             # The default_alias property may raise a TypeError.
1124             try:
1125                 if arg.default_alias in kwargs:
1126                     raise ValueError("The named annotation '%s' conflicts with the "
1127                                      "default name for another annotation."
1128                                      % arg.default_alias)
1129             except TypeError:
1130                 raise TypeError("Complex annotations require an alias")
1131             annotations[arg.default_alias] = arg
1132         annotations.update(kwargs)
1133 
1134         clone = self._chain()
1135         names = self._fields
1136         if names is None:
1137             names = set(chain.from_iterable(
1138                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1139                 for field in self.model._meta.get_fields()
1140             ))
1141 
1142         for alias, annotation in annotations.items():
1143             if alias in names:
1144                 raise ValueError("The annotation '%s' conflicts with a field on "
1145                                  "the model." % alias)
1146             if isinstance(annotation, FilteredRelation):
1147                 clone.query.add_filtered_relation(annotation, alias)
1148             else:
1149                 clone.query.add_annotation(
1150                     annotation, alias, is_summary=False, select=select,
1151                 )
1152         for alias, annotation in clone.query.annotations.items():
1153             if alias in annotations and annotation.contains_aggregate:
1154                 if clone._fields is None:
1155                     clone.query.group_by = True
1156                 else:
1157                     clone.query.set_group_by()
1158                 break
1159 
1160         return clone
1161 
1162     def order_by(self, *field_names):
1163         """Return a new QuerySet instance with the ordering changed."""
1164         if self.query.is_sliced:
1165             raise TypeError('Cannot reorder a query once a slice has been taken.')
1166         obj = self._chain()
1167         obj.query.clear_ordering(force_empty=False)
1168         obj.query.add_ordering(*field_names)
1169         return obj
1170 
1171     def distinct(self, *field_names):
1172         """
1173         Return a new QuerySet instance that will select only distinct results.
1174         """
1175         self._not_support_combined_queries('distinct')
1176         if self.query.is_sliced:
1177             raise TypeError('Cannot create distinct fields once a slice has been taken.')
1178         obj = self._chain()
1179         obj.query.add_distinct_fields(*field_names)
1180         return obj
1181 
1182     def extra(self, select=None, where=None, params=None, tables=None,
1183               order_by=None, select_params=None):
1184         """Add extra SQL fragments to the query."""
1185         self._not_support_combined_queries('extra')
1186         if self.query.is_sliced:
1187             raise TypeError('Cannot change a query once a slice has been taken.')
1188         clone = self._chain()
1189         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1190         return clone
1191 
1192     def reverse(self):
1193         """Reverse the ordering of the QuerySet."""
1194         if self.query.is_sliced:
1195             raise TypeError('Cannot reverse a query once a slice has been taken.')
1196         clone = self._chain()
1197         clone.query.standard_ordering = not clone.query.standard_ordering
1198         return clone
1199 
1200     def defer(self, *fields):
1201         """
1202         Defer the loading of data for certain fields until they are accessed.
1203         Add the set of deferred fields to any existing set of deferred fields.
1204         The only exception to this is if None is passed in as the only
1205         parameter, in which case removal all deferrals.
1206         """
1207         self._not_support_combined_queries('defer')
1208         if self._fields is not None:
1209             raise TypeError("Cannot call defer() after .values() or .values_list()")
1210         clone = self._chain()
1211         if fields == (None,):
1212             clone.query.clear_deferred_loading()
1213         else:
1214             clone.query.add_deferred_loading(fields)
1215         return clone
1216 
1217     def only(self, *fields):
1218         """
1219         Essentially, the opposite of defer(). Only the fields passed into this
1220         method and that are not already specified as deferred are loaded
1221         immediately when the queryset is evaluated.
1222         """
1223         self._not_support_combined_queries('only')
1224         if self._fields is not None:
1225             raise TypeError("Cannot call only() after .values() or .values_list()")
1226         if fields == (None,):
1227             # Can only pass None to defer(), not only(), as the rest option.
1228             # That won't stop people trying to do this, so let's be explicit.
1229             raise TypeError("Cannot pass None as an argument to only().")
1230         for field in fields:
1231             field = field.split(LOOKUP_SEP, 1)[0]
1232             if field in self.query._filtered_relations:
1233                 raise ValueError('only() is not supported with FilteredRelation.')
1234         clone = self._chain()
1235         clone.query.add_immediate_loading(fields)
1236         return clone
1237 
1238     def using(self, alias):
1239         """Select which database this QuerySet should execute against."""
1240         clone = self._chain()
1241         clone._db = alias
1242         return clone
1243 
1244     ###################################
1245     # PUBLIC INTROSPECTION ATTRIBUTES #
1246     ###################################
1247 
1248     @property
1249     def ordered(self):
1250         """
1251         Return True if the QuerySet is ordered -- i.e. has an order_by()
1252         clause or a default ordering on the model (or is empty).
1253         """
1254         if isinstance(self, EmptyQuerySet):
1255             return True
1256         if self.query.extra_order_by or self.query.order_by:
1257             return True
1258         elif (
1259             self.query.default_ordering and
1260             self.query.get_meta().ordering and
1261             # A default ordering doesn't affect GROUP BY queries.
1262             not self.query.group_by
1263         ):
1264             return True
1265         else:
1266             return False
1267 
1268     @property
1269     def db(self):
1270         """Return the database used if this query is executed now."""
1271         if self._for_write:
1272             return self._db or router.db_for_write(self.model, **self._hints)
1273         return self._db or router.db_for_read(self.model, **self._hints)
1274 
1275     ###################
1276     # PRIVATE METHODS #
1277     ###################
1278 
1279     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1280         """
1281         Insert a new record for the given model. This provides an interface to
1282         the InsertQuery class and is how Model.save() is implemented.
1283         """
1284         self._for_write = True
1285         if using is None:
1286             using = self.db
1287         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1288         query.insert_values(fields, objs, raw=raw)
1289         return query.get_compiler(using=using).execute_sql(returning_fields)
1290     _insert.alters_data = True
1291     _insert.queryset_only = False
1292 
1293     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1294         """
1295         Helper method for bulk_create() to insert objs one batch at a time.
1296         """
1297         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1298             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1299         ops = connections[self.db].ops
1300         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1301         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1302         inserted_rows = []
1303         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1304         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1305             if bulk_return and not ignore_conflicts:
1306                 inserted_rows.extend(self._insert(
1307                     item, fields=fields, using=self.db,
1308                     returning_fields=self.model._meta.db_returning_fields,
1309                     ignore_conflicts=ignore_conflicts,
1310                 ))
1311             else:
1312                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1313         return inserted_rows
1314 
1315     def _chain(self, **kwargs):
1316         """
1317         Return a copy of the current QuerySet that's ready for another
1318         operation.
1319         """
1320         obj = self._clone()
1321         if obj._sticky_filter:
1322             obj.query.filter_is_sticky = True
1323             obj._sticky_filter = False
1324         obj.__dict__.update(kwargs)
1325         return obj
1326 
1327     def _clone(self):
1328         """
1329         Return a copy of the current QuerySet. A lightweight alternative
1330         to deepcopy().
1331         """
1332         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1333         c._sticky_filter = self._sticky_filter
1334         c._for_write = self._for_write
1335         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1336         c._known_related_objects = self._known_related_objects
1337         c._iterable_class = self._iterable_class
1338         c._fields = self._fields
1339         return c
1340 
1341     def _fetch_all(self):
1342         if self._result_cache is None:
1343             self._result_cache = list(self._iterable_class(self))
1344         if self._prefetch_related_lookups and not self._prefetch_done:
1345             self._prefetch_related_objects()
1346 
1347     def _next_is_sticky(self):
1348         """
1349         Indicate that the next filter call and the one following that should
1350         be treated as a single filter. This is only important when it comes to
1351         determining when to reuse tables for many-to-many filters. Required so
1352         that we can filter naturally on the results of related managers.
1353 
1354         This doesn't return a clone of the current QuerySet (it returns
1355         "self"). The method is only used internally and should be immediately
1356         followed by a filter() that does create a clone.
1357         """
1358         self._sticky_filter = True
1359         return self
1360 
1361     def _merge_sanity_check(self, other):
1362         """Check that two QuerySet classes may be merged."""
1363         if self._fields is not None and (
1364                 set(self.query.values_select) != set(other.query.values_select) or
1365                 set(self.query.extra_select) != set(other.query.extra_select) or
1366                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1367             raise TypeError(
1368                 "Merging '%s' classes must involve the same values in each case."
1369                 % self.__class__.__name__
1370             )
1371 
1372     def _merge_known_related_objects(self, other):
1373         """
1374         Keep track of all known related objects from either QuerySet instance.
1375         """
1376         for field, objects in other._known_related_objects.items():
1377             self._known_related_objects.setdefault(field, {}).update(objects)
1378 
1379     def resolve_expression(self, *args, **kwargs):
1380         if self._fields and len(self._fields) > 1:
1381             # values() queryset can only be used as nested queries
1382             # if they are set up to select only a single field.
1383             raise TypeError('Cannot use multi-field values as a filter value.')
1384         query = self.query.resolve_expression(*args, **kwargs)
1385         query._db = self._db
1386         return query
1387     resolve_expression.queryset_only = True
1388 
1389     def _add_hints(self, **hints):
1390         """
1391         Update hinting information for use by routers. Add new key/values or
1392         overwrite existing key/values.
1393         """
1394         self._hints.update(hints)
1395 
1396     def _has_filters(self):
1397         """
1398         Check if this QuerySet has any filtering going on. This isn't
1399         equivalent with checking if all objects are present in results, for
1400         example, qs[1:]._has_filters() -> False.
1401         """
1402         return self.query.has_filters()
1403 
1404     @staticmethod
1405     def _validate_values_are_expressions(values, method_name):
1406         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1407         if invalid_args:
1408             raise TypeError(
1409                 'QuerySet.%s() received non-expression(s): %s.' % (
1410                     method_name,
1411                     ', '.join(invalid_args),
1412                 )
1413             )
1414 
1415     def _not_support_combined_queries(self, operation_name):
1416         if self.query.combinator:
1417             raise NotSupportedError(
1418                 'Calling QuerySet.%s() after %s() is not supported.'
1419                 % (operation_name, self.query.combinator)
1420             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 12:50:03.086 | INFO     | Search completed. Bug locations: [<file>django/contrib/admin/views/main.py</file>
<class>ChangeList</class>
<method>get_queryset</method>
<code>
442     def get_queryset(self, request):
443         # First, we collect all the declared list filters.
444         (
445             self.filter_specs,
446             self.has_filters,
447             remaining_lookup_params,
448             filters_use_distinct,
449             self.has_active_filters,
450         ) = self.get_filters(request)
451         # Then, we let every list filter modify the queryset to its liking.
452         qs = self.root_queryset
453         for filter_spec in self.filter_specs:
454             new_qs = filter_spec.queryset(request, qs)
455             if new_qs is not None:
456                 qs = new_qs
457 
458         try:
459             # Finally, we apply the remaining lookup parameters from the query
460             # string (i.e. those that haven't already been processed by the
461             # filters).
462             qs = qs.filter(**remaining_lookup_params)
463         except (SuspiciousOperation, ImproperlyConfigured):
464             # Allow certain types of errors to be re-raised as-is so that the
465             # caller can treat them in a special way.
466             raise
467         except Exception as e:
468             # Every other error is caught with a naked except, because we don't
469             # have any other way of validating lookup parameters. They might be
470             # invalid if the keyword arguments are incorrect, or if the values
471             # are not in the correct type, so we might get FieldError,
472             # ValueError, ValidationError, or ?.
473             raise IncorrectLookupParameters(e)
474 
475         if not qs.query.select_related:
476             qs = self.apply_select_related(qs)
477 
478         # Set ordering.
479         ordering = self.get_ordering(request, qs)
480         qs = qs.order_by(*ordering)
481 
482         # Apply search results
483         qs, search_use_distinct = self.model_admin.get_search_results(request, qs, self.query)
484 
485         # Set query string for clearing all filters.
486         self.clear_all_filters_qs = self.get_query_string(
487             new_params=remaining_lookup_params,
488             remove=self.get_filters_params(),
489         )
490         # Remove duplicates from results, if necessary
491         if filters_use_distinct | search_use_distinct:
492             return qs.distinct()
493         else:
494             return qs

</code><intended_behavior>Ensure that any application of `.distinct()` to the queryset in the context of a deletion operation is either avoided or handled in such a way that it does not prevent the `delete()` operation from succeeding. One approach could involve conditionally applying `.distinct()` only when it's certain that the following operation is not deletion. Alternatively, before deletion, clone the queryset without `.distinct()` if its presence is detected.</intended_behavior>, <file>django/contrib/admin/views/main.py</file>
<class>ChangeList</class>
<method>None</method>
<code>
48 class ChangeList:
49     search_form_class = ChangeListSearchForm
50 
51     def __init__(self, request, model, list_display, list_display_links,
52                  list_filter, date_hierarchy, search_fields, list_select_related,
53                  list_per_page, list_max_show_all, list_editable, model_admin, sortable_by):
54         self.model = model
55         self.opts = model._meta
56         self.lookup_opts = self.opts
57         self.root_queryset = model_admin.get_queryset(request)
58         self.list_display = list_display
59         self.list_display_links = list_display_links
60         self.list_filter = list_filter
61         self.has_filters = None
62         self.has_active_filters = None
63         self.clear_all_filters_qs = None
64         self.date_hierarchy = date_hierarchy
65         self.search_fields = search_fields
66         self.list_select_related = list_select_related
67         self.list_per_page = list_per_page
68         self.list_max_show_all = list_max_show_all
69         self.model_admin = model_admin
70         self.preserved_filters = model_admin.get_preserved_filters(request)
71         self.sortable_by = sortable_by
72 
73         # Get search parameters from the query string.
74         _search_form = self.search_form_class(request.GET)
75         if not _search_form.is_valid():
76             for error in _search_form.errors.values():
77                 messages.error(request, ', '.join(error))
78         self.query = _search_form.cleaned_data.get(SEARCH_VAR) or ''
79         try:
80             self.page_num = int(request.GET.get(PAGE_VAR, 1))
81         except ValueError:
82             self.page_num = 1
83         self.show_all = ALL_VAR in request.GET
84         self.is_popup = IS_POPUP_VAR in request.GET
85         to_field = request.GET.get(TO_FIELD_VAR)
86         if to_field and not model_admin.to_field_allowed(request, to_field):
87             raise DisallowedModelAdminToField("The field %s cannot be referenced." % to_field)
88         self.to_field = to_field
89         self.params = dict(request.GET.items())
90         if PAGE_VAR in self.params:
91             del self.params[PAGE_VAR]
92         if ERROR_FLAG in self.params:
93             del self.params[ERROR_FLAG]
94 
95         if self.is_popup:
96             self.list_editable = ()
97         else:
98             self.list_editable = list_editable
99         self.queryset = self.get_queryset(request)
100         self.get_results(request)
101         if self.is_popup:
102             title = gettext('Select %s')
103         elif self.model_admin.has_change_permission(request):
104             title = gettext('Select %s to change')
105         else:
106             title = gettext('Select %s to view')
107         self.title = title % self.opts.verbose_name
108         self.pk_attname = self.lookup_opts.pk.attname
109 
110     def get_filters_params(self, params=None):
111         """
112         Return all params except IGNORED_PARAMS.
113         """
114         params = params or self.params
115         lookup_params = params.copy()  # a dictionary of the query string
116         # Remove all the parameters that are globally and systematically
117         # ignored.
118         for ignored in IGNORED_PARAMS:
119             if ignored in lookup_params:
120                 del lookup_params[ignored]
121         return lookup_params
122 
123     def get_filters(self, request):
124         lookup_params = self.get_filters_params()
125         use_distinct = False
126         has_active_filters = False
127 
128         for key, value in lookup_params.items():
129             if not self.model_admin.lookup_allowed(key, value):
130                 raise DisallowedModelAdminLookup("Filtering by %s not allowed" % key)
131 
132         filter_specs = []
133         for list_filter in self.list_filter:
134             lookup_params_count = len(lookup_params)
135             if callable(list_filter):
136                 # This is simply a custom list filter class.
137                 spec = list_filter(request, lookup_params, self.model, self.model_admin)
138             else:
139                 field_path = None
140                 if isinstance(list_filter, (tuple, list)):
141                     # This is a custom FieldListFilter class for a given field.
142                     field, field_list_filter_class = list_filter
143                 else:
144                     # This is simply a field name, so use the default
145                     # FieldListFilter class that has been registered for the
146                     # type of the given field.
147                     field, field_list_filter_class = list_filter, FieldListFilter.create
148                 if not isinstance(field, Field):
149                     field_path = field
150                     field = get_fields_from_path(self.model, field_path)[-1]
151 
152                 spec = field_list_filter_class(
153                     field, request, lookup_params,
154                     self.model, self.model_admin, field_path=field_path,
155                 )
156                 # field_list_filter_class removes any lookup_params it
157                 # processes. If that happened, check if distinct() is needed to
158                 # remove duplicate results.
159                 if lookup_params_count > len(lookup_params):
160                     use_distinct = use_distinct or lookup_needs_distinct(self.lookup_opts, field_path)
161             if spec and spec.has_output():
162                 filter_specs.append(spec)
163                 if lookup_params_count > len(lookup_params):
164                     has_active_filters = True
165 
166         if self.date_hierarchy:
167             # Create bounded lookup parameters so that the query is more
168             # efficient.
169             year = lookup_params.pop('%s__year' % self.date_hierarchy, None)
170             if year is not None:
171                 month = lookup_params.pop('%s__month' % self.date_hierarchy, None)
172                 day = lookup_params.pop('%s__day' % self.date_hierarchy, None)
173                 try:
174                     from_date = datetime(
175                         int(year),
176                         int(month if month is not None else 1),
177                         int(day if day is not None else 1),
178                     )
179                 except ValueError as e:
180                     raise IncorrectLookupParameters(e) from e
181                 if day:
182                     to_date = from_date + timedelta(days=1)
183                 elif month:
184                     # In this branch, from_date will always be the first of a
185                     # month, so advancing 32 days gives the next month.
186                     to_date = (from_date + timedelta(days=32)).replace(day=1)
187                 else:
188                     to_date = from_date.replace(year=from_date.year + 1)
189                 if settings.USE_TZ:
190                     from_date = make_aware(from_date)
191                     to_date = make_aware(to_date)
192                 lookup_params.update({
193                     '%s__gte' % self.date_hierarchy: from_date,
194                     '%s__lt' % self.date_hierarchy: to_date,
195                 })
196 
197         # At this point, all the parameters used by the various ListFilters
198         # have been removed from lookup_params, which now only contains other
199         # parameters passed via the query string. We now loop through the
200         # remaining parameters both to ensure that all the parameters are valid
201         # fields and to determine if at least one of them needs distinct(). If
202         # the lookup parameters aren't real fields, then bail out.
203         try:
204             for key, value in lookup_params.items():
205                 lookup_params[key] = prepare_lookup_value(key, value)
206                 use_distinct = use_distinct or lookup_needs_distinct(self.lookup_opts, key)
207             return (
208                 filter_specs, bool(filter_specs), lookup_params, use_distinct,
209                 has_active_filters,
210             )
211         except FieldDoesNotExist as e:
212             raise IncorrectLookupParameters(e) from e
213 
214     def get_query_string(self, new_params=None, remove=None):
215         if new_params is None:
216             new_params = {}
217         if remove is None:
218             remove = []
219         p = self.params.copy()
220         for r in remove:
221             for k in list(p):
222                 if k.startswith(r):
223                     del p[k]
224         for k, v in new_params.items():
225             if v is None:
226                 if k in p:
227                     del p[k]
228             else:
229                 p[k] = v
230         return '?%s' % urlencode(sorted(p.items()))
231 
232     def get_results(self, request):
233         paginator = self.model_admin.get_paginator(request, self.queryset, self.list_per_page)
234         # Get the number of objects, with admin filters applied.
235         result_count = paginator.count
236 
237         # Get the total number of objects, with no admin filters applied.
238         if self.model_admin.show_full_result_count:
239             full_result_count = self.root_queryset.count()
240         else:
241             full_result_count = None
242         can_show_all = result_count <= self.list_max_show_all
243         multi_page = result_count > self.list_per_page
244 
245         # Get the list of objects to display on this page.
246         if (self.show_all and can_show_all) or not multi_page:
247             result_list = self.queryset._clone()
248         else:
249             try:
250                 result_list = paginator.page(self.page_num).object_list
251             except InvalidPage:
252                 raise IncorrectLookupParameters
253 
254         self.result_count = result_count
255         self.show_full_result_count = self.model_admin.show_full_result_count
256         # Admin actions are shown if there is at least one entry
257         # or if entries are not counted because show_full_result_count is disabled
258         self.show_admin_actions = not self.show_full_result_count or bool(full_result_count)
259         self.full_result_count = full_result_count
260         self.result_list = result_list
261         self.can_show_all = can_show_all
262         self.multi_page = multi_page
263         self.paginator = paginator
264 
265     def _get_default_ordering(self):
266         ordering = []
267         if self.model_admin.ordering:
268             ordering = self.model_admin.ordering
269         elif self.lookup_opts.ordering:
270             ordering = self.lookup_opts.ordering
271         return ordering
272 
273     def get_ordering_field(self, field_name):
274         """
275         Return the proper model field name corresponding to the given
276         field_name to use for ordering. field_name may either be the name of a
277         proper model field or the name of a method (on the admin or model) or a
278         callable with the 'admin_order_field' attribute. Return None if no
279         proper model field name can be matched.
280         """
281         try:
282             field = self.lookup_opts.get_field(field_name)
283             return field.name
284         except FieldDoesNotExist:
285             # See whether field_name is a name of a non-field
286             # that allows sorting.
287             if callable(field_name):
288                 attr = field_name
289             elif hasattr(self.model_admin, field_name):
290                 attr = getattr(self.model_admin, field_name)
291             else:
292                 attr = getattr(self.model, field_name)
293             if isinstance(attr, property) and hasattr(attr, 'fget'):
294                 attr = attr.fget
295             return getattr(attr, 'admin_order_field', None)
296 
297     def get_ordering(self, request, queryset):
298         """
299         Return the list of ordering fields for the change list.
300         First check the get_ordering() method in model admin, then check
301         the object's default ordering. Then, any manually-specified ordering
302         from the query string overrides anything. Finally, a deterministic
303         order is guaranteed by calling _get_deterministic_ordering() with the
304         constructed ordering.
305         """
306         params = self.params
307         ordering = list(self.model_admin.get_ordering(request) or self._get_default_ordering())
308         if ORDER_VAR in params:
309             # Clear ordering and used params
310             ordering = []
311             order_params = params[ORDER_VAR].split('.')
312             for p in order_params:
313                 try:
314                     none, pfx, idx = p.rpartition('-')
315                     field_name = self.list_display[int(idx)]
316                     order_field = self.get_ordering_field(field_name)
317                     if not order_field:
318                         continue  # No 'admin_order_field', skip it
319                     if isinstance(order_field, OrderBy):
320                         if pfx == '-':
321                             order_field = order_field.copy()
322                             order_field.reverse_ordering()
323                         ordering.append(order_field)
324                     elif hasattr(order_field, 'resolve_expression'):
325                         # order_field is an expression.
326                         ordering.append(order_field.desc() if pfx == '-' else order_field.asc())
327                     # reverse order if order_field has already "-" as prefix
328                     elif order_field.startswith('-') and pfx == '-':
329                         ordering.append(order_field[1:])
330                     else:
331                         ordering.append(pfx + order_field)
332                 except (IndexError, ValueError):
333                     continue  # Invalid ordering specified, skip it.
334 
335         # Add the given query's ordering fields, if any.
336         ordering.extend(queryset.query.order_by)
337 
338         return self._get_deterministic_ordering(ordering)
339 
340     def _get_deterministic_ordering(self, ordering):
341         """
342         Ensure a deterministic order across all database backends. Search for a
343         single field or unique together set of fields providing a total
344         ordering. If these are missing, augment the ordering with a descendant
345         primary key.
346         """
347         ordering = list(ordering)
348         ordering_fields = set()
349         total_ordering_fields = {'pk'} | {
350             field.attname for field in self.lookup_opts.fields
351             if field.unique and not field.null
352         }
353         for part in ordering:
354             # Search for single field providing a total ordering.
355             field_name = None
356             if isinstance(part, str):
357                 field_name = part.lstrip('-')
358             elif isinstance(part, F):
359                 field_name = part.name
360             elif isinstance(part, OrderBy) and isinstance(part.expression, F):
361                 field_name = part.expression.name
362             if field_name:
363                 # Normalize attname references by using get_field().
364                 try:
365                     field = self.lookup_opts.get_field(field_name)
366                 except FieldDoesNotExist:
367                     # Could be "?" for random ordering or a related field
368                     # lookup. Skip this part of introspection for now.
369                     continue
370                 # Ordering by a related field name orders by the referenced
371                 # model's ordering. Skip this part of introspection for now.
372                 if field.remote_field and field_name == field.name:
373                     continue
374                 if field.attname in total_ordering_fields:
375                     break
376                 ordering_fields.add(field.attname)
377         else:
378             # No single total ordering field, try unique_together and total
379             # unique constraints.
380             constraint_field_names = (
381                 *self.lookup_opts.unique_together,
382                 *(
383                     constraint.fields
384                     for constraint in self.lookup_opts.total_unique_constraints
385                 ),
386             )
387             for field_names in constraint_field_names:
388                 # Normalize attname references by using get_field().
389                 fields = [self.lookup_opts.get_field(field_name) for field_name in field_names]
390                 # Composite unique constraints containing a nullable column
391                 # cannot ensure total ordering.
392                 if any(field.null for field in fields):
393                     continue
394                 if ordering_fields.issuperset(field.attname for field in fields):
395                     break
396             else:
397                 # If no set of unique fields is present in the ordering, rely
398                 # on the primary key to provide total ordering.
399                 ordering.append('-pk')
400         return ordering
401 
402     def get_ordering_field_columns(self):
403         """
404         Return a dictionary of ordering field column numbers and asc/desc.
405         """
406         # We must cope with more than one column having the same underlying sort
407         # field, so we base things on column numbers.
408         ordering = self._get_default_ordering()
409         ordering_fields = {}
410         if ORDER_VAR not in self.params:
411             # for ordering specified on ModelAdmin or model Meta, we don't know
412             # the right column numbers absolutely, because there might be more
413             # than one column associated with that ordering, so we guess.
414             for field in ordering:
415                 if isinstance(field, (Combinable, OrderBy)):
416                     if not isinstance(field, OrderBy):
417                         field = field.asc()
418                     if isinstance(field.expression, F):
419                         order_type = 'desc' if field.descending else 'asc'
420                         field = field.expression.name
421                     else:
422                         continue
423                 elif field.startswith('-'):
424                     field = field[1:]
425                     order_type = 'desc'
426                 else:
427                     order_type = 'asc'
428                 for index, attr in enumerate(self.list_display):
429                     if self.get_ordering_field(attr) == field:
430                         ordering_fields[index] = order_type
431                         break
432         else:
433             for p in self.params[ORDER_VAR].split('.'):
434                 none, pfx, idx = p.rpartition('-')
435                 try:
436                     idx = int(idx)
437                 except ValueError:
438                     continue  # skip it
439                 ordering_fields[idx] = 'desc' if pfx == '-' else 'asc'
440         return ordering_fields
441 
442     def get_queryset(self, request):
443         # First, we collect all the declared list filters.
444         (
445             self.filter_specs,
446             self.has_filters,
447             remaining_lookup_params,
448             filters_use_distinct,
449             self.has_active_filters,
450         ) = self.get_filters(request)
451         # Then, we let every list filter modify the queryset to its liking.
452         qs = self.root_queryset
453         for filter_spec in self.filter_specs:
454             new_qs = filter_spec.queryset(request, qs)
455             if new_qs is not None:
456                 qs = new_qs
457 
458         try:
459             # Finally, we apply the remaining lookup parameters from the query
460             # string (i.e. those that haven't already been processed by the
461             # filters).
462             qs = qs.filter(**remaining_lookup_params)
463         except (SuspiciousOperation, ImproperlyConfigured):
464             # Allow certain types of errors to be re-raised as-is so that the
465             # caller can treat them in a special way.
466             raise
467         except Exception as e:
468             # Every other error is caught with a naked except, because we don't
469             # have any other way of validating lookup parameters. They might be
470             # invalid if the keyword arguments are incorrect, or if the values
471             # are not in the correct type, so we might get FieldError,
472             # ValueError, ValidationError, or ?.
473             raise IncorrectLookupParameters(e)
474 
475         if not qs.query.select_related:
476             qs = self.apply_select_related(qs)
477 
478         # Set ordering.
479         ordering = self.get_ordering(request, qs)
480         qs = qs.order_by(*ordering)
481 
482         # Apply search results
483         qs, search_use_distinct = self.model_admin.get_search_results(request, qs, self.query)
484 
485         # Set query string for clearing all filters.
486         self.clear_all_filters_qs = self.get_query_string(
487             new_params=remaining_lookup_params,
488             remove=self.get_filters_params(),
489         )
490         # Remove duplicates from results, if necessary
491         if filters_use_distinct | search_use_distinct:
492             return qs.distinct()
493         else:
494             return qs
495 
496     def apply_select_related(self, qs):
497         if self.list_select_related is True:
498             return qs.select_related()
499 
500         if self.list_select_related is False:
501             if self.has_related_field_in_list_display():
502                 return qs.select_related()
503 
504         if self.list_select_related:
505             return qs.select_related(*self.list_select_related)
506         return qs
507 
508     def has_related_field_in_list_display(self):
509         for field_name in self.list_display:
510             try:
511                 field = self.lookup_opts.get_field(field_name)
512             except FieldDoesNotExist:
513                 pass
514             else:
515                 if isinstance(field.remote_field, ManyToOneRel):
516                     # <FK>_id field names don't require a join.
517                     if field_name != field.get_attname():
518                         return True
519         return False
520 
521     def url_for_result(self, result):
522         pk = getattr(result, self.pk_attname)
523         return reverse('admin:%s_%s_change' % (self.opts.app_label,
524                                                self.opts.model_name),
525                        args=(quote(pk),),
526                        current_app=self.model_admin.admin_site.name)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>delete</method>
<code>
722     def delete(self):
723         """Delete the records in the current QuerySet."""
724         self._not_support_combined_queries('delete')
725         if self.query.is_sliced:
726             raise TypeError("Cannot use 'limit' or 'offset' with delete().")
727         if self.query.distinct or self.query.distinct_fields:
728             raise TypeError('Cannot call delete() after .distinct().')
729         if self._fields is not None:
730             raise TypeError("Cannot call delete() after .values() or .values_list()")
731 
732         del_query = self._chain()
733 
734         # The delete is actually 2 queries - one to find related objects,
735         # and one to delete. Make sure that the discovery of related
736         # objects is performed on the same database as the deletion.
737         del_query._for_write = True
738 
739         # Disable non-supported fields.
740         del_query.query.select_for_update = False
741         del_query.query.select_related = False
742         del_query.query.clear_ordering(force_empty=True)
743 
744         collector = Collector(using=del_query.db)
745         collector.collect(del_query)
746         deleted, _rows_count = collector.delete()
747 
748         # Clear the result cache, in case this QuerySet gets reused.
749         self._result_cache = None
750         return deleted, _rows_count

</code><intended_behavior>Before the actual deletion operation, check if the queryset has been altered by `.distinct()`, and if so, reconstruct the queryset from the primary keys of the current queryset's results. This will ensure that the deletion operation proceeds on a queryset that is compatible with deletion. Code modifications might involve a pre-deletion check that explicitly handles or warns about issues arising from attempting deletion on querysets with `.distinct()`. This pre-check could offer a pathway to handle such cases more gracefully, possibly by re-fetching objects based on their IDs alone for deletion.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
175 class QuerySet:
176     """Represent a lazy database lookup for a set of objects."""
177 
178     def __init__(self, model=None, query=None, using=None, hints=None):
179         self.model = model
180         self._db = using
181         self._hints = hints or {}
182         self._query = query or sql.Query(self.model)
183         self._result_cache = None
184         self._sticky_filter = False
185         self._for_write = False
186         self._prefetch_related_lookups = ()
187         self._prefetch_done = False
188         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
189         self._iterable_class = ModelIterable
190         self._fields = None
191         self._defer_next_filter = False
192         self._deferred_filter = None
193 
194     @property
195     def query(self):
196         if self._deferred_filter:
197             negate, args, kwargs = self._deferred_filter
198             self._filter_or_exclude_inplace(negate, args, kwargs)
199             self._deferred_filter = None
200         return self._query
201 
202     @query.setter
203     def query(self, value):
204         if value.values_select:
205             self._iterable_class = ValuesIterable
206         self._query = value
207 
208     def as_manager(cls):
209         # Address the circular dependency between `Queryset` and `Manager`.
210         from django.db.models.manager import Manager
211         manager = Manager.from_queryset(cls)()
212         manager._built_with_as_manager = True
213         return manager
214     as_manager.queryset_only = True
215     as_manager = classmethod(as_manager)
216 
217     ########################
218     # PYTHON MAGIC METHODS #
219     ########################
220 
221     def __deepcopy__(self, memo):
222         """Don't populate the QuerySet's cache."""
223         obj = self.__class__()
224         for k, v in self.__dict__.items():
225             if k == '_result_cache':
226                 obj.__dict__[k] = None
227             else:
228                 obj.__dict__[k] = copy.deepcopy(v, memo)
229         return obj
230 
231     def __getstate__(self):
232         # Force the cache to be fully populated.
233         self._fetch_all()
234         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
235 
236     def __setstate__(self, state):
237         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
238         if pickled_version:
239             if pickled_version != django.__version__:
240                 warnings.warn(
241                     "Pickled queryset instance's Django version %s does not "
242                     "match the current version %s."
243                     % (pickled_version, django.__version__),
244                     RuntimeWarning,
245                     stacklevel=2,
246                 )
247         else:
248             warnings.warn(
249                 "Pickled queryset instance's Django version is not specified.",
250                 RuntimeWarning,
251                 stacklevel=2,
252             )
253         self.__dict__.update(state)
254 
255     def __repr__(self):
256         data = list(self[:REPR_OUTPUT_SIZE + 1])
257         if len(data) > REPR_OUTPUT_SIZE:
258             data[-1] = "...(remaining elements truncated)..."
259         return '<%s %r>' % (self.__class__.__name__, data)
260 
261     def __len__(self):
262         self._fetch_all()
263         return len(self._result_cache)
264 
265     def __iter__(self):
266         """
267         The queryset iterator protocol uses three nested iterators in the
268         default case:
269             1. sql.compiler.execute_sql()
270                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
271                  using cursor.fetchmany(). This part is responsible for
272                  doing some column masking, and returning the rows in chunks.
273             2. sql.compiler.results_iter()
274                - Returns one row at time. At this point the rows are still just
275                  tuples. In some cases the return values are converted to
276                  Python values at this location.
277             3. self.iterator()
278                - Responsible for turning the rows into model objects.
279         """
280         self._fetch_all()
281         return iter(self._result_cache)
282 
283     def __bool__(self):
284         self._fetch_all()
285         return bool(self._result_cache)
286 
287     def __getitem__(self, k):
288         """Retrieve an item or slice from the set of results."""
289         if not isinstance(k, (int, slice)):
290             raise TypeError(
291                 'QuerySet indices must be integers or slices, not %s.'
292                 % type(k).__name__
293             )
294         assert ((not isinstance(k, slice) and (k >= 0)) or
295                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
296                  (k.stop is None or k.stop >= 0))), \
297             "Negative indexing is not supported."
298 
299         if self._result_cache is not None:
300             return self._result_cache[k]
301 
302         if isinstance(k, slice):
303             qs = self._chain()
304             if k.start is not None:
305                 start = int(k.start)
306             else:
307                 start = None
308             if k.stop is not None:
309                 stop = int(k.stop)
310             else:
311                 stop = None
312             qs.query.set_limits(start, stop)
313             return list(qs)[::k.step] if k.step else qs
314 
315         qs = self._chain()
316         qs.query.set_limits(k, k + 1)
317         qs._fetch_all()
318         return qs._result_cache[0]
319 
320     def __class_getitem__(cls, *args, **kwargs):
321         return cls
322 
323     def __and__(self, other):
324         self._merge_sanity_check(other)
325         if isinstance(other, EmptyQuerySet):
326             return other
327         if isinstance(self, EmptyQuerySet):
328             return self
329         combined = self._chain()
330         combined._merge_known_related_objects(other)
331         combined.query.combine(other.query, sql.AND)
332         return combined
333 
334     def __or__(self, other):
335         self._merge_sanity_check(other)
336         if isinstance(self, EmptyQuerySet):
337             return other
338         if isinstance(other, EmptyQuerySet):
339             return self
340         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
341         combined = query._chain()
342         combined._merge_known_related_objects(other)
343         if not other.query.can_filter():
344             other = other.model._base_manager.filter(pk__in=other.values('pk'))
345         combined.query.combine(other.query, sql.OR)
346         return combined
347 
348     ####################################
349     # METHODS THAT DO DATABASE QUERIES #
350     ####################################
351 
352     def _iterator(self, use_chunked_fetch, chunk_size):
353         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
354 
355     def iterator(self, chunk_size=2000):
356         """
357         An iterator over the results from applying this QuerySet to the
358         database.
359         """
360         if chunk_size <= 0:
361             raise ValueError('Chunk size must be strictly positive.')
362         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
363         return self._iterator(use_chunked_fetch, chunk_size)
364 
365     def aggregate(self, *args, **kwargs):
366         """
367         Return a dictionary containing the calculations (aggregation)
368         over the current queryset.
369 
370         If args is present the expression is passed as a kwarg using
371         the Aggregate object's default alias.
372         """
373         if self.query.distinct_fields:
374             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
375         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
376         for arg in args:
377             # The default_alias property raises TypeError if default_alias
378             # can't be set automatically or AttributeError if it isn't an
379             # attribute.
380             try:
381                 arg.default_alias
382             except (AttributeError, TypeError):
383                 raise TypeError("Complex aggregates require an alias")
384             kwargs[arg.default_alias] = arg
385 
386         query = self.query.chain()
387         for (alias, aggregate_expr) in kwargs.items():
388             query.add_annotation(aggregate_expr, alias, is_summary=True)
389             annotation = query.annotations[alias]
390             if not annotation.contains_aggregate:
391                 raise TypeError("%s is not an aggregate expression" % alias)
392             for expr in annotation.get_source_expressions():
393                 if expr.contains_aggregate and isinstance(expr, Ref) and expr.refs in kwargs:
394                     name = expr.refs
395                     raise exceptions.FieldError(
396                         "Cannot compute %s('%s'): '%s' is an aggregate"
397                         % (annotation.name, name, name)
398                     )
399         return query.get_aggregation(self.db, kwargs)
400 
401     def count(self):
402         """
403         Perform a SELECT COUNT() and return the number of records as an
404         integer.
405 
406         If the QuerySet is already fully cached, return the length of the
407         cached results set to avoid multiple SELECT COUNT(*) calls.
408         """
409         if self._result_cache is not None:
410             return len(self._result_cache)
411 
412         return self.query.get_count(using=self.db)
413 
414     def get(self, *args, **kwargs):
415         """
416         Perform the query and return a single object matching the given
417         keyword arguments.
418         """
419         if self.query.combinator and (args or kwargs):
420             raise NotSupportedError(
421                 'Calling QuerySet.get(...) with filters after %s() is not '
422                 'supported.' % self.query.combinator
423             )
424         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
425         if self.query.can_filter() and not self.query.distinct_fields:
426             clone = clone.order_by()
427         limit = None
428         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
429             limit = MAX_GET_RESULTS
430             clone.query.set_limits(high=limit)
431         num = len(clone)
432         if num == 1:
433             return clone._result_cache[0]
434         if not num:
435             raise self.model.DoesNotExist(
436                 "%s matching query does not exist." %
437                 self.model._meta.object_name
438             )
439         raise self.model.MultipleObjectsReturned(
440             'get() returned more than one %s -- it returned %s!' % (
441                 self.model._meta.object_name,
442                 num if not limit or num < limit else 'more than %s' % (limit - 1),
443             )
444         )
445 
446     def create(self, **kwargs):
447         """
448         Create a new object with the given kwargs, saving it to the database
449         and returning the created object.
450         """
451         obj = self.model(**kwargs)
452         self._for_write = True
453         obj.save(force_insert=True, using=self.db)
454         return obj
455 
456     def _prepare_for_bulk_create(self, objs):
457         for obj in objs:
458             if obj.pk is None:
459                 # Populate new PK values.
460                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
461             obj._prepare_related_fields_for_save(operation_name='bulk_create')
462 
463     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
464         """
465         Insert each of the instances into the database. Do *not* call
466         save() on each of the instances, do not send any pre/post_save
467         signals, and do not set the primary key attribute if it is an
468         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
469         Multi-table models are not supported.
470         """
471         # When you bulk insert you don't get the primary keys back (if it's an
472         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
473         # you can't insert into the child tables which references this. There
474         # are two workarounds:
475         # 1) This could be implemented if you didn't have an autoincrement pk
476         # 2) You could do it by doing O(n) normal inserts into the parent
477         #    tables to get the primary keys back and then doing a single bulk
478         #    insert into the childmost table.
479         # We currently set the primary keys on the objects when using
480         # PostgreSQL via the RETURNING ID clause. It should be possible for
481         # Oracle as well, but the semantics for extracting the primary keys is
482         # trickier so it's not done yet.
483         assert batch_size is None or batch_size > 0
484         # Check that the parents share the same concrete model with the our
485         # model to detect the inheritance pattern ConcreteGrandParent ->
486         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
487         # would not identify that case as involving multiple tables.
488         for parent in self.model._meta.get_parent_list():
489             if parent._meta.concrete_model is not self.model._meta.concrete_model:
490                 raise ValueError("Can't bulk create a multi-table inherited model")
491         if not objs:
492             return objs
493         self._for_write = True
494         connection = connections[self.db]
495         opts = self.model._meta
496         fields = opts.concrete_fields
497         objs = list(objs)
498         self._prepare_for_bulk_create(objs)
499         with transaction.atomic(using=self.db, savepoint=False):
500             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
501             if objs_with_pk:
502                 returned_columns = self._batched_insert(
503                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
504                 )
505                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
506                     for result, field in zip(results, opts.db_returning_fields):
507                         if field != opts.pk:
508                             setattr(obj_with_pk, field.attname, result)
509                 for obj_with_pk in objs_with_pk:
510                     obj_with_pk._state.adding = False
511                     obj_with_pk._state.db = self.db
512             if objs_without_pk:
513                 fields = [f for f in fields if not isinstance(f, AutoField)]
514                 returned_columns = self._batched_insert(
515                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
516                 )
517                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
518                     assert len(returned_columns) == len(objs_without_pk)
519                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
520                     for result, field in zip(results, opts.db_returning_fields):
521                         setattr(obj_without_pk, field.attname, result)
522                     obj_without_pk._state.adding = False
523                     obj_without_pk._state.db = self.db
524 
525         return objs
526 
527     def bulk_update(self, objs, fields, batch_size=None):
528         """
529         Update the given fields in each of the given objects in the database.
530         """
531         if batch_size is not None and batch_size < 0:
532             raise ValueError('Batch size must be a positive integer.')
533         if not fields:
534             raise ValueError('Field names must be given to bulk_update().')
535         objs = tuple(objs)
536         if any(obj.pk is None for obj in objs):
537             raise ValueError('All bulk_update() objects must have a primary key set.')
538         fields = [self.model._meta.get_field(name) for name in fields]
539         if any(not f.concrete or f.many_to_many for f in fields):
540             raise ValueError('bulk_update() can only be used with concrete fields.')
541         if any(f.primary_key for f in fields):
542             raise ValueError('bulk_update() cannot be used with primary key fields.')
543         if not objs:
544             return
545         # PK is used twice in the resulting update query, once in the filter
546         # and once in the WHEN. Each field will also have one CAST.
547         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
548         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
549         requires_casting = connections[self.db].features.requires_casted_case_in_updates
550         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
551         updates = []
552         for batch_objs in batches:
553             update_kwargs = {}
554             for field in fields:
555                 when_statements = []
556                 for obj in batch_objs:
557                     attr = getattr(obj, field.attname)
558                     if not isinstance(attr, Expression):
559                         attr = Value(attr, output_field=field)
560                     when_statements.append(When(pk=obj.pk, then=attr))
561                 case_statement = Case(*when_statements, output_field=field)
562                 if requires_casting:
563                     case_statement = Cast(case_statement, output_field=field)
564                 update_kwargs[field.attname] = case_statement
565             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
566         with transaction.atomic(using=self.db, savepoint=False):
567             for pks, update_kwargs in updates:
568                 self.filter(pk__in=pks).update(**update_kwargs)
569     bulk_update.alters_data = True
570 
571     def get_or_create(self, defaults=None, **kwargs):
572         """
573         Look up an object with the given kwargs, creating one if necessary.
574         Return a tuple of (object, created), where created is a boolean
575         specifying whether an object was created.
576         """
577         # The get() needs to be targeted at the write database in order
578         # to avoid potential transaction consistency problems.
579         self._for_write = True
580         try:
581             return self.get(**kwargs), False
582         except self.model.DoesNotExist:
583             params = self._extract_model_params(defaults, **kwargs)
584             # Try to create an object using passed params.
585             try:
586                 with transaction.atomic(using=self.db):
587                     params = dict(resolve_callables(params))
588                     return self.create(**params), True
589             except IntegrityError:
590                 try:
591                     return self.get(**kwargs), False
592                 except self.model.DoesNotExist:
593                     pass
594                 raise
595 
596     def update_or_create(self, defaults=None, **kwargs):
597         """
598         Look up an object with the given kwargs, updating one with defaults
599         if it exists, otherwise create a new one.
600         Return a tuple (object, created), where created is a boolean
601         specifying whether an object was created.
602         """
603         defaults = defaults or {}
604         self._for_write = True
605         with transaction.atomic(using=self.db):
606             # Lock the row so that a concurrent update is blocked until
607             # update_or_create() has performed its save.
608             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)
609             if created:
610                 return obj, created
611             for k, v in resolve_callables(defaults):
612                 setattr(obj, k, v)
613             obj.save(using=self.db)
614         return obj, False
615 
616     def _extract_model_params(self, defaults, **kwargs):
617         """
618         Prepare `params` for creating a model instance based on the given
619         kwargs; for use by get_or_create().
620         """
621         defaults = defaults or {}
622         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
623         params.update(defaults)
624         property_names = self.model._meta._property_names
625         invalid_params = []
626         for param in params:
627             try:
628                 self.model._meta.get_field(param)
629             except exceptions.FieldDoesNotExist:
630                 # It's okay to use a model's property if it has a setter.
631                 if not (param in property_names and getattr(self.model, param).fset):
632                     invalid_params.append(param)
633         if invalid_params:
634             raise exceptions.FieldError(
635                 "Invalid field name(s) for model %s: '%s'." % (
636                     self.model._meta.object_name,
637                     "', '".join(sorted(invalid_params)),
638                 ))
639         return params
640 
641     def _earliest(self, *fields):
642         """
643         Return the earliest object according to fields (if given) or by the
644         model's Meta.get_latest_by.
645         """
646         if fields:
647             order_by = fields
648         else:
649             order_by = getattr(self.model._meta, 'get_latest_by')
650             if order_by and not isinstance(order_by, (tuple, list)):
651                 order_by = (order_by,)
652         if order_by is None:
653             raise ValueError(
654                 "earliest() and latest() require either fields as positional "
655                 "arguments or 'get_latest_by' in the model's Meta."
656             )
657         obj = self._chain()
658         obj.query.set_limits(high=1)
659         obj.query.clear_ordering(force_empty=True)
660         obj.query.add_ordering(*order_by)
661         return obj.get()
662 
663     def earliest(self, *fields):
664         if self.query.is_sliced:
665             raise TypeError('Cannot change a query once a slice has been taken.')
666         return self._earliest(*fields)
667 
668     def latest(self, *fields):
669         if self.query.is_sliced:
670             raise TypeError('Cannot change a query once a slice has been taken.')
671         return self.reverse()._earliest(*fields)
672 
673     def first(self):
674         """Return the first object of a query or None if no match is found."""
675         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
676             return obj
677 
678     def last(self):
679         """Return the last object of a query or None if no match is found."""
680         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
681             return obj
682 
683     def in_bulk(self, id_list=None, *, field_name='pk'):
684         """
685         Return a dictionary mapping each of the given IDs to the object with
686         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
687         """
688         if self.query.is_sliced:
689             raise TypeError("Cannot use 'limit' or 'offset' with in_bulk().")
690         opts = self.model._meta
691         unique_fields = [
692             constraint.fields[0]
693             for constraint in opts.total_unique_constraints
694             if len(constraint.fields) == 1
695         ]
696         if (
697             field_name != 'pk' and
698             not opts.get_field(field_name).unique and
699             field_name not in unique_fields and
700             self.query.distinct_fields != (field_name,)
701         ):
702             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
703         if id_list is not None:
704             if not id_list:
705                 return {}
706             filter_key = '{}__in'.format(field_name)
707             batch_size = connections[self.db].features.max_query_params
708             id_list = tuple(id_list)
709             # If the database has a limit on the number of query parameters
710             # (e.g. SQLite), retrieve objects in batches if necessary.
711             if batch_size and batch_size < len(id_list):
712                 qs = ()
713                 for offset in range(0, len(id_list), batch_size):
714                     batch = id_list[offset:offset + batch_size]
715                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
716             else:
717                 qs = self.filter(**{filter_key: id_list}).order_by()
718         else:
719             qs = self._chain()
720         return {getattr(obj, field_name): obj for obj in qs}
721 
722     def delete(self):
723         """Delete the records in the current QuerySet."""
724         self._not_support_combined_queries('delete')
725         if self.query.is_sliced:
726             raise TypeError("Cannot use 'limit' or 'offset' with delete().")
727         if self.query.distinct or self.query.distinct_fields:
728             raise TypeError('Cannot call delete() after .distinct().')
729         if self._fields is not None:
730             raise TypeError("Cannot call delete() after .values() or .values_list()")
731 
732         del_query = self._chain()
733 
734         # The delete is actually 2 queries - one to find related objects,
735         # and one to delete. Make sure that the discovery of related
736         # objects is performed on the same database as the deletion.
737         del_query._for_write = True
738 
739         # Disable non-supported fields.
740         del_query.query.select_for_update = False
741         del_query.query.select_related = False
742         del_query.query.clear_ordering(force_empty=True)
743 
744         collector = Collector(using=del_query.db)
745         collector.collect(del_query)
746         deleted, _rows_count = collector.delete()
747 
748         # Clear the result cache, in case this QuerySet gets reused.
749         self._result_cache = None
750         return deleted, _rows_count
751 
752     delete.alters_data = True
753     delete.queryset_only = True
754 
755     def _raw_delete(self, using):
756         """
757         Delete objects found from the given queryset in single direct SQL
758         query. No signals are sent and there is no protection for cascades.
759         """
760         query = self.query.clone()
761         query.__class__ = sql.DeleteQuery
762         cursor = query.get_compiler(using).execute_sql(CURSOR)
763         if cursor:
764             with cursor:
765                 return cursor.rowcount
766         return 0
767     _raw_delete.alters_data = True
768 
769     def update(self, **kwargs):
770         """
771         Update all elements in the current QuerySet, setting all the given
772         fields to the appropriate values.
773         """
774         self._not_support_combined_queries('update')
775         if self.query.is_sliced:
776             raise TypeError('Cannot update a query once a slice has been taken.')
777         self._for_write = True
778         query = self.query.chain(sql.UpdateQuery)
779         query.add_update_values(kwargs)
780         # Clear any annotations so that they won't be present in subqueries.
781         query.annotations = {}
782         with transaction.mark_for_rollback_on_error(using=self.db):
783             rows = query.get_compiler(self.db).execute_sql(CURSOR)
784         self._result_cache = None
785         return rows
786     update.alters_data = True
787 
788     def _update(self, values):
789         """
790         A version of update() that accepts field objects instead of field names.
791         Used primarily for model saving and not intended for use by general
792         code (it requires too much poking around at model internals to be
793         useful at that level).
794         """
795         if self.query.is_sliced:
796             raise TypeError('Cannot update a query once a slice has been taken.')
797         query = self.query.chain(sql.UpdateQuery)
798         query.add_update_fields(values)
799         # Clear any annotations so that they won't be present in subqueries.
800         query.annotations = {}
801         self._result_cache = None
802         return query.get_compiler(self.db).execute_sql(CURSOR)
803     _update.alters_data = True
804     _update.queryset_only = False
805 
806     def exists(self):
807         if self._result_cache is None:
808             return self.query.has_results(using=self.db)
809         return bool(self._result_cache)
810 
811     def contains(self, obj):
812         """Return True if the queryset contains an object."""
813         self._not_support_combined_queries('contains')
814         if self._fields is not None:
815             raise TypeError(
816                 'Cannot call QuerySet.contains() after .values() or '
817                 '.values_list().'
818             )
819         try:
820             if obj._meta.concrete_model != self.model._meta.concrete_model:
821                 return False
822         except AttributeError:
823             raise TypeError("'obj' must be a model instance.")
824         if obj.pk is None:
825             raise ValueError(
826                 'QuerySet.contains() cannot be used on unsaved objects.'
827             )
828         if self._result_cache is not None:
829             return obj in self._result_cache
830         return self.filter(pk=obj.pk).exists()
831 
832     def _prefetch_related_objects(self):
833         # This method can only be called once the result cache has been filled.
834         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
835         self._prefetch_done = True
836 
837     def explain(self, *, format=None, **options):
838         return self.query.explain(using=self.db, format=format, **options)
839 
840     ##################################################
841     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
842     ##################################################
843 
844     def raw(self, raw_query, params=(), translations=None, using=None):
845         if using is None:
846             using = self.db
847         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
848         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
849         return qs
850 
851     def _values(self, *fields, **expressions):
852         clone = self._chain()
853         if expressions:
854             clone = clone.annotate(**expressions)
855         clone._fields = fields
856         clone.query.set_values(fields)
857         return clone
858 
859     def values(self, *fields, **expressions):
860         fields += tuple(expressions)
861         clone = self._values(*fields, **expressions)
862         clone._iterable_class = ValuesIterable
863         return clone
864 
865     def values_list(self, *fields, flat=False, named=False):
866         if flat and named:
867             raise TypeError("'flat' and 'named' can't be used together.")
868         if flat and len(fields) > 1:
869             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
870 
871         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
872         _fields = []
873         expressions = {}
874         counter = 1
875         for field in fields:
876             if hasattr(field, 'resolve_expression'):
877                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
878                 while True:
879                     field_id = field_id_prefix + str(counter)
880                     counter += 1
881                     if field_id not in field_names:
882                         break
883                 expressions[field_id] = field
884                 _fields.append(field_id)
885             else:
886                 _fields.append(field)
887 
888         clone = self._values(*_fields, **expressions)
889         clone._iterable_class = (
890             NamedValuesListIterable if named
891             else FlatValuesListIterable if flat
892             else ValuesListIterable
893         )
894         return clone
895 
896     def dates(self, field_name, kind, order='ASC'):
897         """
898         Return a list of date objects representing all available dates for
899         the given field_name, scoped to 'kind'.
900         """
901         assert kind in ('year', 'month', 'week', 'day'), \
902             "'kind' must be one of 'year', 'month', 'week', or 'day'."
903         assert order in ('ASC', 'DESC'), \
904             "'order' must be either 'ASC' or 'DESC'."
905         return self.annotate(
906             datefield=Trunc(field_name, kind, output_field=DateField()),
907             plain_field=F(field_name)
908         ).values_list(
909             'datefield', flat=True
910         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
911 
912     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):
913         """
914         Return a list of datetime objects representing all available
915         datetimes for the given field_name, scoped to 'kind'.
916         """
917         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
918             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
919         assert order in ('ASC', 'DESC'), \
920             "'order' must be either 'ASC' or 'DESC'."
921         if settings.USE_TZ:
922             if tzinfo is None:
923                 tzinfo = timezone.get_current_timezone()
924         else:
925             tzinfo = None
926         return self.annotate(
927             datetimefield=Trunc(
928                 field_name,
929                 kind,
930                 output_field=DateTimeField(),
931                 tzinfo=tzinfo,
932                 is_dst=is_dst,
933             ),
934             plain_field=F(field_name)
935         ).values_list(
936             'datetimefield', flat=True
937         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
938 
939     def none(self):
940         """Return an empty QuerySet."""
941         clone = self._chain()
942         clone.query.set_empty()
943         return clone
944 
945     ##################################################################
946     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
947     ##################################################################
948 
949     def all(self):
950         """
951         Return a new QuerySet that is a copy of the current one. This allows a
952         QuerySet to proxy for a model manager in some cases.
953         """
954         return self._chain()
955 
956     def filter(self, *args, **kwargs):
957         """
958         Return a new QuerySet instance with the args ANDed to the existing
959         set.
960         """
961         self._not_support_combined_queries('filter')
962         return self._filter_or_exclude(False, args, kwargs)
963 
964     def exclude(self, *args, **kwargs):
965         """
966         Return a new QuerySet instance with NOT (args) ANDed to the existing
967         set.
968         """
969         self._not_support_combined_queries('exclude')
970         return self._filter_or_exclude(True, args, kwargs)
971 
972     def _filter_or_exclude(self, negate, args, kwargs):
973         if (args or kwargs) and self.query.is_sliced:
974             raise TypeError('Cannot filter a query once a slice has been taken.')
975         clone = self._chain()
976         if self._defer_next_filter:
977             self._defer_next_filter = False
978             clone._deferred_filter = negate, args, kwargs
979         else:
980             clone._filter_or_exclude_inplace(negate, args, kwargs)
981         return clone
982 
983     def _filter_or_exclude_inplace(self, negate, args, kwargs):
984         if negate:
985             self._query.add_q(~Q(*args, **kwargs))
986         else:
987             self._query.add_q(Q(*args, **kwargs))
988 
989     def complex_filter(self, filter_obj):
990         """
991         Return a new QuerySet instance with filter_obj added to the filters.
992 
993         filter_obj can be a Q object or a dictionary of keyword lookup
994         arguments.
995 
996         This exists to support framework features such as 'limit_choices_to',
997         and usually it will be more natural to use other methods.
998         """
999         if isinstance(filter_obj, Q):
1000             clone = self._chain()
1001             clone.query.add_q(filter_obj)
1002             return clone
1003         else:
1004             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
1005 
1006     def _combinator_query(self, combinator, *other_qs, all=False):
1007         # Clone the query to inherit the select list and everything
1008         clone = self._chain()
1009         # Clear limits and ordering so they can be reapplied
1010         clone.query.clear_ordering(True)
1011         clone.query.clear_limits()
1012         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
1013         clone.query.combinator = combinator
1014         clone.query.combinator_all = all
1015         return clone
1016 
1017     def union(self, *other_qs, all=False):
1018         # If the query is an EmptyQuerySet, combine all nonempty querysets.
1019         if isinstance(self, EmptyQuerySet):
1020             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1021             if not qs:
1022                 return self
1023             if len(qs) == 1:
1024                 return qs[0]
1025             return qs[0]._combinator_query('union', *qs[1:], all=all)
1026         return self._combinator_query('union', *other_qs, all=all)
1027 
1028     def intersection(self, *other_qs):
1029         # If any query is an EmptyQuerySet, return it.
1030         if isinstance(self, EmptyQuerySet):
1031             return self
1032         for other in other_qs:
1033             if isinstance(other, EmptyQuerySet):
1034                 return other
1035         return self._combinator_query('intersection', *other_qs)
1036 
1037     def difference(self, *other_qs):
1038         # If the query is an EmptyQuerySet, return it.
1039         if isinstance(self, EmptyQuerySet):
1040             return self
1041         return self._combinator_query('difference', *other_qs)
1042 
1043     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1044         """
1045         Return a new QuerySet instance that will select objects with a
1046         FOR UPDATE lock.
1047         """
1048         if nowait and skip_locked:
1049             raise ValueError('The nowait option cannot be used with skip_locked.')
1050         obj = self._chain()
1051         obj._for_write = True
1052         obj.query.select_for_update = True
1053         obj.query.select_for_update_nowait = nowait
1054         obj.query.select_for_update_skip_locked = skip_locked
1055         obj.query.select_for_update_of = of
1056         obj.query.select_for_no_key_update = no_key
1057         return obj
1058 
1059     def select_related(self, *fields):
1060         """
1061         Return a new QuerySet instance that will select related objects.
1062 
1063         If fields are specified, they must be ForeignKey fields and only those
1064         related objects are included in the selection.
1065 
1066         If select_related(None) is called, clear the list.
1067         """
1068         self._not_support_combined_queries('select_related')
1069         if self._fields is not None:
1070             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1071 
1072         obj = self._chain()
1073         if fields == (None,):
1074             obj.query.select_related = False
1075         elif fields:
1076             obj.query.add_select_related(fields)
1077         else:
1078             obj.query.select_related = True
1079         return obj
1080 
1081     def prefetch_related(self, *lookups):
1082         """
1083         Return a new QuerySet instance that will prefetch the specified
1084         Many-To-One and Many-To-Many related objects when the QuerySet is
1085         evaluated.
1086 
1087         When prefetch_related() is called more than once, append to the list of
1088         prefetch lookups. If prefetch_related(None) is called, clear the list.
1089         """
1090         self._not_support_combined_queries('prefetch_related')
1091         clone = self._chain()
1092         if lookups == (None,):
1093             clone._prefetch_related_lookups = ()
1094         else:
1095             for lookup in lookups:
1096                 if isinstance(lookup, Prefetch):
1097                     lookup = lookup.prefetch_to
1098                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1099                 if lookup in self.query._filtered_relations:
1100                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1101             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1102         return clone
1103 
1104     def annotate(self, *args, **kwargs):
1105         """
1106         Return a query set in which the returned objects have been annotated
1107         with extra data or aggregations.
1108         """
1109         self._not_support_combined_queries('annotate')
1110         return self._annotate(args, kwargs, select=True)
1111 
1112     def alias(self, *args, **kwargs):
1113         """
1114         Return a query set with added aliases for extra data or aggregations.
1115         """
1116         self._not_support_combined_queries('alias')
1117         return self._annotate(args, kwargs, select=False)
1118 
1119     def _annotate(self, args, kwargs, select=True):
1120         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1121         annotations = {}
1122         for arg in args:
1123             # The default_alias property may raise a TypeError.
1124             try:
1125                 if arg.default_alias in kwargs:
1126                     raise ValueError("The named annotation '%s' conflicts with the "
1127                                      "default name for another annotation."
1128                                      % arg.default_alias)
1129             except TypeError:
1130                 raise TypeError("Complex annotations require an alias")
1131             annotations[arg.default_alias] = arg
1132         annotations.update(kwargs)
1133 
1134         clone = self._chain()
1135         names = self._fields
1136         if names is None:
1137             names = set(chain.from_iterable(
1138                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1139                 for field in self.model._meta.get_fields()
1140             ))
1141 
1142         for alias, annotation in annotations.items():
1143             if alias in names:
1144                 raise ValueError("The annotation '%s' conflicts with a field on "
1145                                  "the model." % alias)
1146             if isinstance(annotation, FilteredRelation):
1147                 clone.query.add_filtered_relation(annotation, alias)
1148             else:
1149                 clone.query.add_annotation(
1150                     annotation, alias, is_summary=False, select=select,
1151                 )
1152         for alias, annotation in clone.query.annotations.items():
1153             if alias in annotations and annotation.contains_aggregate:
1154                 if clone._fields is None:
1155                     clone.query.group_by = True
1156                 else:
1157                     clone.query.set_group_by()
1158                 break
1159 
1160         return clone
1161 
1162     def order_by(self, *field_names):
1163         """Return a new QuerySet instance with the ordering changed."""
1164         if self.query.is_sliced:
1165             raise TypeError('Cannot reorder a query once a slice has been taken.')
1166         obj = self._chain()
1167         obj.query.clear_ordering(force_empty=False)
1168         obj.query.add_ordering(*field_names)
1169         return obj
1170 
1171     def distinct(self, *field_names):
1172         """
1173         Return a new QuerySet instance that will select only distinct results.
1174         """
1175         self._not_support_combined_queries('distinct')
1176         if self.query.is_sliced:
1177             raise TypeError('Cannot create distinct fields once a slice has been taken.')
1178         obj = self._chain()
1179         obj.query.add_distinct_fields(*field_names)
1180         return obj
1181 
1182     def extra(self, select=None, where=None, params=None, tables=None,
1183               order_by=None, select_params=None):
1184         """Add extra SQL fragments to the query."""
1185         self._not_support_combined_queries('extra')
1186         if self.query.is_sliced:
1187             raise TypeError('Cannot change a query once a slice has been taken.')
1188         clone = self._chain()
1189         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1190         return clone
1191 
1192     def reverse(self):
1193         """Reverse the ordering of the QuerySet."""
1194         if self.query.is_sliced:
1195             raise TypeError('Cannot reverse a query once a slice has been taken.')
1196         clone = self._chain()
1197         clone.query.standard_ordering = not clone.query.standard_ordering
1198         return clone
1199 
1200     def defer(self, *fields):
1201         """
1202         Defer the loading of data for certain fields until they are accessed.
1203         Add the set of deferred fields to any existing set of deferred fields.
1204         The only exception to this is if None is passed in as the only
1205         parameter, in which case removal all deferrals.
1206         """
1207         self._not_support_combined_queries('defer')
1208         if self._fields is not None:
1209             raise TypeError("Cannot call defer() after .values() or .values_list()")
1210         clone = self._chain()
1211         if fields == (None,):
1212             clone.query.clear_deferred_loading()
1213         else:
1214             clone.query.add_deferred_loading(fields)
1215         return clone
1216 
1217     def only(self, *fields):
1218         """
1219         Essentially, the opposite of defer(). Only the fields passed into this
1220         method and that are not already specified as deferred are loaded
1221         immediately when the queryset is evaluated.
1222         """
1223         self._not_support_combined_queries('only')
1224         if self._fields is not None:
1225             raise TypeError("Cannot call only() after .values() or .values_list()")
1226         if fields == (None,):
1227             # Can only pass None to defer(), not only(), as the rest option.
1228             # That won't stop people trying to do this, so let's be explicit.
1229             raise TypeError("Cannot pass None as an argument to only().")
1230         for field in fields:
1231             field = field.split(LOOKUP_SEP, 1)[0]
1232             if field in self.query._filtered_relations:
1233                 raise ValueError('only() is not supported with FilteredRelation.')
1234         clone = self._chain()
1235         clone.query.add_immediate_loading(fields)
1236         return clone
1237 
1238     def using(self, alias):
1239         """Select which database this QuerySet should execute against."""
1240         clone = self._chain()
1241         clone._db = alias
1242         return clone
1243 
1244     ###################################
1245     # PUBLIC INTROSPECTION ATTRIBUTES #
1246     ###################################
1247 
1248     @property
1249     def ordered(self):
1250         """
1251         Return True if the QuerySet is ordered -- i.e. has an order_by()
1252         clause or a default ordering on the model (or is empty).
1253         """
1254         if isinstance(self, EmptyQuerySet):
1255             return True
1256         if self.query.extra_order_by or self.query.order_by:
1257             return True
1258         elif (
1259             self.query.default_ordering and
1260             self.query.get_meta().ordering and
1261             # A default ordering doesn't affect GROUP BY queries.
1262             not self.query.group_by
1263         ):
1264             return True
1265         else:
1266             return False
1267 
1268     @property
1269     def db(self):
1270         """Return the database used if this query is executed now."""
1271         if self._for_write:
1272             return self._db or router.db_for_write(self.model, **self._hints)
1273         return self._db or router.db_for_read(self.model, **self._hints)
1274 
1275     ###################
1276     # PRIVATE METHODS #
1277     ###################
1278 
1279     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1280         """
1281         Insert a new record for the given model. This provides an interface to
1282         the InsertQuery class and is how Model.save() is implemented.
1283         """
1284         self._for_write = True
1285         if using is None:
1286             using = self.db
1287         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1288         query.insert_values(fields, objs, raw=raw)
1289         return query.get_compiler(using=using).execute_sql(returning_fields)
1290     _insert.alters_data = True
1291     _insert.queryset_only = False
1292 
1293     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1294         """
1295         Helper method for bulk_create() to insert objs one batch at a time.
1296         """
1297         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1298             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1299         ops = connections[self.db].ops
1300         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1301         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1302         inserted_rows = []
1303         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1304         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1305             if bulk_return and not ignore_conflicts:
1306                 inserted_rows.extend(self._insert(
1307                     item, fields=fields, using=self.db,
1308                     returning_fields=self.model._meta.db_returning_fields,
1309                     ignore_conflicts=ignore_conflicts,
1310                 ))
1311             else:
1312                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1313         return inserted_rows
1314 
1315     def _chain(self, **kwargs):
1316         """
1317         Return a copy of the current QuerySet that's ready for another
1318         operation.
1319         """
1320         obj = self._clone()
1321         if obj._sticky_filter:
1322             obj.query.filter_is_sticky = True
1323             obj._sticky_filter = False
1324         obj.__dict__.update(kwargs)
1325         return obj
1326 
1327     def _clone(self):
1328         """
1329         Return a copy of the current QuerySet. A lightweight alternative
1330         to deepcopy().
1331         """
1332         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1333         c._sticky_filter = self._sticky_filter
1334         c._for_write = self._for_write
1335         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1336         c._known_related_objects = self._known_related_objects
1337         c._iterable_class = self._iterable_class
1338         c._fields = self._fields
1339         return c
1340 
1341     def _fetch_all(self):
1342         if self._result_cache is None:
1343             self._result_cache = list(self._iterable_class(self))
1344         if self._prefetch_related_lookups and not self._prefetch_done:
1345             self._prefetch_related_objects()
1346 
1347     def _next_is_sticky(self):
1348         """
1349         Indicate that the next filter call and the one following that should
1350         be treated as a single filter. This is only important when it comes to
1351         determining when to reuse tables for many-to-many filters. Required so
1352         that we can filter naturally on the results of related managers.
1353 
1354         This doesn't return a clone of the current QuerySet (it returns
1355         "self"). The method is only used internally and should be immediately
1356         followed by a filter() that does create a clone.
1357         """
1358         self._sticky_filter = True
1359         return self
1360 
1361     def _merge_sanity_check(self, other):
1362         """Check that two QuerySet classes may be merged."""
1363         if self._fields is not None and (
1364                 set(self.query.values_select) != set(other.query.values_select) or
1365                 set(self.query.extra_select) != set(other.query.extra_select) or
1366                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1367             raise TypeError(
1368                 "Merging '%s' classes must involve the same values in each case."
1369                 % self.__class__.__name__
1370             )
1371 
1372     def _merge_known_related_objects(self, other):
1373         """
1374         Keep track of all known related objects from either QuerySet instance.
1375         """
1376         for field, objects in other._known_related_objects.items():
1377             self._known_related_objects.setdefault(field, {}).update(objects)
1378 
1379     def resolve_expression(self, *args, **kwargs):
1380         if self._fields and len(self._fields) > 1:
1381             # values() queryset can only be used as nested queries
1382             # if they are set up to select only a single field.
1383             raise TypeError('Cannot use multi-field values as a filter value.')
1384         query = self.query.resolve_expression(*args, **kwargs)
1385         query._db = self._db
1386         return query
1387     resolve_expression.queryset_only = True
1388 
1389     def _add_hints(self, **hints):
1390         """
1391         Update hinting information for use by routers. Add new key/values or
1392         overwrite existing key/values.
1393         """
1394         self._hints.update(hints)
1395 
1396     def _has_filters(self):
1397         """
1398         Check if this QuerySet has any filtering going on. This isn't
1399         equivalent with checking if all objects are present in results, for
1400         example, qs[1:]._has_filters() -> False.
1401         """
1402         return self.query.has_filters()
1403 
1404     @staticmethod
1405     def _validate_values_are_expressions(values, method_name):
1406         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1407         if invalid_args:
1408             raise TypeError(
1409                 'QuerySet.%s() received non-expression(s): %s.' % (
1410                     method_name,
1411                     ', '.join(invalid_args),
1412                 )
1413             )
1414 
1415     def _not_support_combined_queries(self, operation_name):
1416         if self.query.combinator:
1417             raise NotSupportedError(
1418                 'Calling QuerySet.%s() after %s() is not supported.'
1419                 % (operation_name, self.query.combinator)
1420             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 12:50:03.087 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 12:50:03.087 | INFO     | Search-only: skipping patch selection
2025-10-18 12:50:03.217 | INFO     | Task django__django-14313 completed successfully.
2025-10-18 12:50:03.218 | INFO     | No patch generated. You can try running ACR again.
