2025-10-18 15:26:09.015 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 15:26:11.443 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=611, output_tokens=12, cost=0.006470
2025-10-18 15:26:17.329 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=791, output_tokens=267, cost=0.015920
2025-10-18 15:26:19.045 | INFO     | registered non reproducing test 0
2025-10-18 15:26:34.114 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1931, output_tokens=692, cost=0.040070
2025-10-18 15:26:35.828 | INFO     | registered non reproducing test 1
2025-10-18 15:26:49.076 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2350, output_tokens=593, cost=0.041290
2025-10-18 15:26:50.781 | INFO     | registered non reproducing test 2
2025-10-18 15:26:50.781 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-18 15:26:50.784 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 15:26:56.756 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=918, output_tokens=263, cost=0.017070
2025-10-18 15:26:56.762 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 15:27:00.829 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=617, output_tokens=48, cost=0.007610
2025-10-18 15:27:00.830 | DEBUG    | Extracted a valid json.
2025-10-18 15:27:00.830 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"DataArray\")",
        "search_method_in_class(\"astype\", \"DataArray\")",
        "search_code(\"order parameter\")"
    ],
    "bug_locations": []
}
2025-10-18 15:27:00.922 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 15:27:00.923 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 15:27:16.238 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=5484, output_tokens=644, cost=0.074160
2025-10-18 15:27:16.252 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 15:27:29.592 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=6344, output_tokens=534, cost=0.079460
2025-10-18 15:27:29.602 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 15:27:33.247 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=888, output_tokens=148, cost=0.013320
2025-10-18 15:27:33.248 | DEBUG    | Extracted a valid json.
2025-10-18 15:27:33.248 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "xarray/core/dataarray.py",
            "class": "DataArray",
            "method": "astype",
            "intended_behavior": "The `astype` method should allow for type conversion of the data contained within a `DataArray` instance, mirroring the functionality seen in NumPy arrays. This method should include the `order` parameter to control the memory layout of the result. The intended fix would involve reinstating this parameter if it was removed, or providing a compatible interface that re-enables control over the memory layout as part of type conversion, ensuring consistency with NumPy and preserving existing functionality."
        }
    ]
}
2025-10-18 15:27:33.269 | DEBUG    | Bug location extracted successfully: [<file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
216 class DataArray(AbstractArray, DataWithCoords):
217     """N-dimensional array with labeled coordinates and dimensions.
218 
219     DataArray provides a wrapper around numpy ndarrays that uses
220     labeled dimensions and coordinates to support metadata aware
221     operations. The API is similar to that for the pandas Series or
222     DataFrame, but DataArray objects can have any number of dimensions,
223     and their contents have fixed data types.
224 
225     Additional features over raw numpy arrays:
226 
227     - Apply operations over dimensions by name: ``x.sum('time')``.
228     - Select or assign values by integer location (like numpy):
229       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or
230       ``x.sel(time='2014-01-01')``.
231     - Mathematical operations (e.g., ``x - y``) vectorize across
232       multiple dimensions (known in numpy as "broadcasting") based on
233       dimension names, regardless of their original order.
234     - Keep track of arbitrary metadata in the form of a Python
235       dictionary: ``x.attrs``
236     - Convert to a pandas Series: ``x.to_series()``.
237 
238     Getting items from or doing mathematical operations with a
239     DataArray always returns another DataArray.
240 
241     Parameters
242     ----------
243     data : array_like
244         Values for this array. Must be an ``numpy.ndarray``, ndarray
245         like, or castable to an ``ndarray``. If a self-described xarray
246         or pandas object, attempts are made to use this array's
247         metadata to fill in other unspecified arguments. A view of the
248         array's data is used instead of a copy if possible.
249     coords : sequence or dict of array_like, optional
250         Coordinates (tick labels) to use for indexing along each
251         dimension. The following notations are accepted:
252 
253         - mapping {dimension name: array-like}
254         - sequence of tuples that are valid arguments for
255           ``xarray.Variable()``
256           - (dims, data)
257           - (dims, data, attrs)
258           - (dims, data, attrs, encoding)
259 
260         Additionally, it is possible to define a coord whose name
261         does not match the dimension name, or a coord based on multiple
262         dimensions, with one of the following notations:
263 
264         - mapping {coord name: DataArray}
265         - mapping {coord name: Variable}
266         - mapping {coord name: (dimension name, array-like)}
267         - mapping {coord name: (tuple of dimension names, array-like)}
268 
269     dims : hashable or sequence of hashable, optional
270         Name(s) of the data dimension(s). Must be either a hashable
271         (only for 1D data) or a sequence of hashables with length equal
272         to the number of dimensions. If this argument is omitted,
273         dimension names default to ``['dim_0', ... 'dim_n']``.
274     name : str or None, optional
275         Name of this array.
276     attrs : dict_like or None, optional
277         Attributes to assign to the new instance. By default, an empty
278         attribute dictionary is initialized.
279 
280     Examples
281     --------
282     Create data:
283 
284     >>> np.random.seed(0)
285     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
286     >>> precipitation = 10 * np.random.rand(2, 2, 3)
287     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
288     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
289     >>> time = pd.date_range("2014-09-06", periods=3)
290     >>> reference_time = pd.Timestamp("2014-09-05")
291 
292     Initialize a dataarray with multiple dimensions:
293 
294     >>> da = xr.DataArray(
295     ...     data=temperature,
296     ...     dims=["x", "y", "time"],
297     ...     coords=dict(
298     ...         lon=(["x", "y"], lon),
299     ...         lat=(["x", "y"], lat),
300     ...         time=time,
301     ...         reference_time=reference_time,
302     ...     ),
303     ...     attrs=dict(
304     ...         description="Ambient temperature.",
305     ...         units="degC",
306     ...     ),
307     ... )
308     >>> da
309     <xarray.DataArray (x: 2, y: 2, time: 3)>
310     array([[[29.11241877, 18.20125767, 22.82990387],
311             [32.92714559, 29.94046392,  7.18177696]],
312     <BLANKLINE>
313            [[22.60070734, 13.78914233, 14.17424919],
314             [18.28478802, 16.15234857, 26.63418806]]])
315     Coordinates:
316         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
317         lat             (x, y) float64 42.25 42.21 42.63 42.59
318       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
319         reference_time  datetime64[ns] 2014-09-05
320     Dimensions without coordinates: x, y
321     Attributes:
322         description:  Ambient temperature.
323         units:        degC
324 
325     Find out where the coldest temperature was:
326 
327     >>> da.isel(da.argmin(...))
328     <xarray.DataArray ()>
329     array(7.18177696)
330     Coordinates:
331         lon             float64 -99.32
332         lat             float64 42.21
333         time            datetime64[ns] 2014-09-08
334         reference_time  datetime64[ns] 2014-09-05
335     Attributes:
336         description:  Ambient temperature.
337         units:        degC
338     """
339 
340     _cache: Dict[str, Any]
341     _coords: Dict[Any, Variable]
342     _indexes: Optional[Dict[Hashable, pd.Index]]
343     _name: Optional[Hashable]
344     _variable: Variable
345 
346     __slots__ = (
347         "_cache",
348         "_coords",
349         "_file_obj",
350         "_indexes",
351         "_name",
352         "_variable",
353         "__weakref__",
354     )
355 
356     _groupby_cls = groupby.DataArrayGroupBy
357     _rolling_cls = rolling.DataArrayRolling
358     _coarsen_cls = rolling.DataArrayCoarsen
359     _resample_cls = resample.DataArrayResample
360     _weighted_cls = weighted.DataArrayWeighted
361 
362     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor)
363 
364     def __init__(
365         self,
366         data: Any = dtypes.NA,
367         coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None] = None,
368         dims: Union[Hashable, Sequence[Hashable], None] = None,
369         name: Hashable = None,
370         attrs: Mapping = None,
371         # internal parameters
372         indexes: Dict[Hashable, pd.Index] = None,
373         fastpath: bool = False,
374     ):
375         if fastpath:
376             variable = data
377             assert dims is None
378             assert attrs is None
379         else:
380             # try to fill in arguments from data if they weren't supplied
381             if coords is None:
382 
383                 if isinstance(data, DataArray):
384                     coords = data.coords
385                 elif isinstance(data, pd.Series):
386                     coords = [data.index]
387                 elif isinstance(data, pd.DataFrame):
388                     coords = [data.index, data.columns]
389                 elif isinstance(data, (pd.Index, IndexVariable)):
390                     coords = [data]
391                 elif isinstance(data, pdcompat.Panel):
392                     coords = [data.items, data.major_axis, data.minor_axis]
393 
394             if dims is None:
395                 dims = getattr(data, "dims", getattr(coords, "dims", None))
396             if name is None:
397                 name = getattr(data, "name", None)
398             if attrs is None and not isinstance(data, PANDAS_TYPES):
399                 attrs = getattr(data, "attrs", None)
400 
401             data = _check_data_shape(data, coords, dims)
402             data = as_compatible_data(data)
403             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
404             variable = Variable(dims, data, attrs, fastpath=True)
405             indexes = dict(
406                 _extract_indexes_from_coords(coords)
407             )  # needed for to_dataset
408 
409         # These fully describe a DataArray
410         self._variable = variable
411         assert isinstance(coords, dict)
412         self._coords = coords
413         self._name = name
414 
415         # TODO(shoyer): document this argument, once it becomes part of the
416         # public interface.
417         self._indexes = indexes
418 
419         self._file_obj = None
420 
421     def _replace(
422         self,
423         variable: Variable = None,
424         coords=None,
425         name: Union[Hashable, None, Default] = _default,
426         indexes=None,
427     ) -> "DataArray":
428         if variable is None:
429             variable = self.variable
430         if coords is None:
431             coords = self._coords
432         if name is _default:
433             name = self.name
434         return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)
435 
436     def _replace_maybe_drop_dims(
437         self, variable: Variable, name: Union[Hashable, None, Default] = _default
438     ) -> "DataArray":
439         if variable.dims == self.dims and variable.shape == self.shape:
440             coords = self._coords.copy()
441             indexes = self._indexes
442         elif variable.dims == self.dims:
443             # Shape has changed (e.g. from reduce(..., keepdims=True)
444             new_sizes = dict(zip(self.dims, variable.shape))
445             coords = {
446                 k: v
447                 for k, v in self._coords.items()
448                 if v.shape == tuple(new_sizes[d] for d in v.dims)
449             }
450             changed_dims = [
451                 k for k in variable.dims if variable.sizes[k] != self.sizes[k]
452             ]
453             indexes = propagate_indexes(self._indexes, exclude=changed_dims)
454         else:
455             allowed_dims = set(variable.dims)
456             coords = {
457                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
458             }
459             indexes = propagate_indexes(
460                 self._indexes, exclude=(set(self.dims) - allowed_dims)
461             )
462         return self._replace(variable, coords, name, indexes=indexes)
463 
464     def _overwrite_indexes(self, indexes: Mapping[Hashable, Any]) -> "DataArray":
465         if not len(indexes):
466             return self
467         coords = self._coords.copy()
468         for name, idx in indexes.items():
469             coords[name] = IndexVariable(name, idx)
470         obj = self._replace(coords=coords)
471 
472         # switch from dimension to level names, if necessary
473         dim_names: Dict[Any, str] = {}
474         for dim, idx in indexes.items():
475             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:
476                 dim_names[dim] = idx.name
477         if dim_names:
478             obj = obj.rename(dim_names)
479         return obj
480 
481     def _to_temp_dataset(self) -> Dataset:
482         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
483 
484     def _from_temp_dataset(
485         self, dataset: Dataset, name: Union[Hashable, None, Default] = _default
486     ) -> "DataArray":
487         variable = dataset._variables.pop(_THIS_ARRAY)
488         coords = dataset._variables
489         indexes = dataset._indexes
490         return self._replace(variable, coords, name, indexes=indexes)
491 
492     def _to_dataset_split(self, dim: Hashable) -> Dataset:
493         """ splits dataarray along dimension 'dim' """
494 
495         def subset(dim, label):
496             array = self.loc[{dim: label}]
497             array.attrs = {}
498             return as_variable(array)
499 
500         variables = {label: subset(dim, label) for label in self.get_index(dim)}
501         variables.update({k: v for k, v in self._coords.items() if k != dim})
502         indexes = propagate_indexes(self._indexes, exclude=dim)
503         coord_names = set(self._coords) - {dim}
504         dataset = Dataset._construct_direct(
505             variables, coord_names, indexes=indexes, attrs=self.attrs
506         )
507         return dataset
508 
509     def _to_dataset_whole(
510         self, name: Hashable = None, shallow_copy: bool = True
511     ) -> Dataset:
512         if name is None:
513             name = self.name
514         if name is None:
515             raise ValueError(
516                 "unable to convert unnamed DataArray to a "
517                 "Dataset without providing an explicit name"
518             )
519         if name in self.coords:
520             raise ValueError(
521                 "cannot create a Dataset from a DataArray with "
522                 "the same name as one of its coordinates"
523             )
524         # use private APIs for speed: this is called by _to_temp_dataset(),
525         # which is used in the guts of a lot of operations (e.g., reindex)
526         variables = self._coords.copy()
527         variables[name] = self.variable
528         if shallow_copy:
529             for k in variables:
530                 variables[k] = variables[k].copy(deep=False)
531         indexes = self._indexes
532 
533         coord_names = set(self._coords)
534         dataset = Dataset._construct_direct(variables, coord_names, indexes=indexes)
535         return dataset
536 
537     def to_dataset(
538         self,
539         dim: Hashable = None,
540         *,
541         name: Hashable = None,
542         promote_attrs: bool = False,
543     ) -> Dataset:
544         """Convert a DataArray to a Dataset.
545 
546         Parameters
547         ----------
548         dim : hashable, optional
549             Name of the dimension on this array along which to split this array
550             into separate variables. If not provided, this array is converted
551             into a Dataset of one variable.
552         name : hashable, optional
553             Name to substitute for this array's name. Only valid if ``dim`` is
554             not provided.
555         promote_attrs : bool, default: False
556             Set to True to shallow copy attrs of DataArray to returned Dataset.
557 
558         Returns
559         -------
560         dataset : Dataset
561         """
562         if dim is not None and dim not in self.dims:
563             raise TypeError(
564                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
565             )
566 
567         if dim is not None:
568             if name is not None:
569                 raise TypeError("cannot supply both dim and name arguments")
570             result = self._to_dataset_split(dim)
571         else:
572             result = self._to_dataset_whole(name)
573 
574         if promote_attrs:
575             result.attrs = dict(self.attrs)
576 
577         return result
578 
579     @property
580     def name(self) -> Optional[Hashable]:
581         """The name of this array."""
582         return self._name
583 
584     @name.setter
585     def name(self, value: Optional[Hashable]) -> None:
586         self._name = value
587 
588     @property
589     def variable(self) -> Variable:
590         """Low level interface to the Variable object for this DataArray."""
591         return self._variable
592 
593     @property
594     def dtype(self) -> np.dtype:
595         return self.variable.dtype
596 
597     @property
598     def shape(self) -> Tuple[int, ...]:
599         return self.variable.shape
600 
601     @property
602     def size(self) -> int:
603         return self.variable.size
604 
605     @property
606     def nbytes(self) -> int:
607         return self.variable.nbytes
608 
609     @property
610     def ndim(self) -> int:
611         return self.variable.ndim
612 
613     def __len__(self) -> int:
614         return len(self.variable)
615 
616     @property
617     def data(self) -> Any:
618         """The array's data as a dask or numpy array"""
619         return self.variable.data
620 
621     @data.setter
622     def data(self, value: Any) -> None:
623         self.variable.data = value
624 
625     @property
626     def values(self) -> np.ndarray:
627         """The array's data as a numpy.ndarray"""
628         return self.variable.values
629 
630     @values.setter
631     def values(self, value: Any) -> None:
632         self.variable.values = value
633 
634     @property
635     def _in_memory(self) -> bool:
636         return self.variable._in_memory
637 
638     def to_index(self) -> pd.Index:
639         """Convert this variable to a pandas.Index. Only possible for 1D
640         arrays.
641         """
642         return self.variable.to_index()
643 
644     @property
645     def dims(self) -> Tuple[Hashable, ...]:
646         """Tuple of dimension names associated with this array.
647 
648         Note that the type of this property is inconsistent with
649         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
650         consistently named properties.
651         """
652         return self.variable.dims
653 
654     @dims.setter
655     def dims(self, value):
656         raise AttributeError(
657             "you cannot assign dims on a DataArray. Use "
658             ".rename() or .swap_dims() instead."
659         )
660 
661     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
662         if utils.is_dict_like(key):
663             return key
664         else:
665             key = indexing.expanded_indexer(key, self.ndim)
666             return dict(zip(self.dims, key))
667 
668     @property
669     def _level_coords(self) -> Dict[Hashable, Hashable]:
670         """Return a mapping of all MultiIndex levels and their corresponding
671         coordinate name.
672         """
673         level_coords: Dict[Hashable, Hashable] = {}
674 
675         for cname, var in self._coords.items():
676             if var.ndim == 1 and isinstance(var, IndexVariable):
677                 level_names = var.level_names
678                 if level_names is not None:
679                     (dim,) = var.dims
680                     level_coords.update({lname: dim for lname in level_names})
681         return level_coords
682 
683     def _getitem_coord(self, key):
684         from .dataset import _get_virtual_variable
685 
686         try:
687             var = self._coords[key]
688         except KeyError:
689             dim_sizes = dict(zip(self.dims, self.shape))
690             _, key, var = _get_virtual_variable(
691                 self._coords, key, self._level_coords, dim_sizes
692             )
693 
694         return self._replace_maybe_drop_dims(var, name=key)
695 
696     def __getitem__(self, key: Any) -> "DataArray":
697         if isinstance(key, str):
698             return self._getitem_coord(key)
699         else:
700             # xarray-style array indexing
701             return self.isel(indexers=self._item_key_to_dict(key))
702 
703     def __setitem__(self, key: Any, value: Any) -> None:
704         if isinstance(key, str):
705             self.coords[key] = value
706         else:
707             # Coordinates in key, value and self[key] should be consistent.
708             # TODO Coordinate consistency in key is checked here, but it
709             # causes unnecessary indexing. It should be optimized.
710             obj = self[key]
711             if isinstance(value, DataArray):
712                 assert_coordinate_consistent(value, obj.coords.variables)
713             # DataArray key -> Variable key
714             key = {
715                 k: v.variable if isinstance(v, DataArray) else v
716                 for k, v in self._item_key_to_dict(key).items()
717             }
718             self.variable[key] = value
719 
720     def __delitem__(self, key: Any) -> None:
721         del self.coords[key]
722 
723     @property
724     def _attr_sources(self) -> List[Mapping[Hashable, Any]]:
725         """List of places to look-up items for attribute-style access"""
726         return self._item_sources + [self.attrs]
727 
728     @property
729     def _item_sources(self) -> List[Mapping[Hashable, Any]]:
730         """List of places to look-up items for key-completion"""
731         return [
732             self.coords,
733             {d: self.coords[d] for d in self.dims},
734             LevelCoordinatesSource(self),
735         ]
736 
737     def __contains__(self, key: Any) -> bool:
738         return key in self.data
739 
740     @property
741     def loc(self) -> _LocIndexer:
742         """Attribute for location based indexing like pandas."""
743         return _LocIndexer(self)
744 
745     @property
746     def attrs(self) -> Dict[Hashable, Any]:
747         """Dictionary storing arbitrary metadata with this array."""
748         return self.variable.attrs
749 
750     @attrs.setter
751     def attrs(self, value: Mapping[Hashable, Any]) -> None:
752         # Disable type checking to work around mypy bug - see mypy#4167
753         self.variable.attrs = value  # type: ignore
754 
755     @property
756     def encoding(self) -> Dict[Hashable, Any]:
757         """Dictionary of format-specific settings for how this array should be
758         serialized."""
759         return self.variable.encoding
760 
761     @encoding.setter
762     def encoding(self, value: Mapping[Hashable, Any]) -> None:
763         self.variable.encoding = value
764 
765     @property
766     def indexes(self) -> Indexes:
767         """Mapping of pandas.Index objects used for label based indexing"""
768         if self._indexes is None:
769             self._indexes = default_indexes(self._coords, self.dims)
770         return Indexes(self._indexes)
771 
772     @property
773     def coords(self) -> DataArrayCoordinates:
774         """Dictionary-like container of coordinate arrays."""
775         return DataArrayCoordinates(self)
776 
777     def reset_coords(
778         self,
779         names: Union[Iterable[Hashable], Hashable, None] = None,
780         drop: bool = False,
781     ) -> Union[None, "DataArray", Dataset]:
782         """Given names of coordinates, reset them to become variables.
783 
784         Parameters
785         ----------
786         names : hashable or iterable of hashable, optional
787             Name(s) of non-index coordinates in this dataset to reset into
788             variables. By default, all non-index coordinates are reset.
789         drop : bool, optional
790             If True, remove coordinates instead of converting them into
791             variables.
792 
793         Returns
794         -------
795         Dataset, or DataArray if ``drop == True``
796         """
797         if names is None:
798             names = set(self.coords) - set(self.dims)
799         dataset = self.coords.to_dataset().reset_coords(names, drop)
800         if drop:
801             return self._replace(coords=dataset._variables)
802         else:
803             if self.name is None:
804                 raise ValueError(
805                     "cannot reset_coords with drop=False on an unnamed DataArrray"
806                 )
807             dataset[self.name] = self.variable
808             return dataset
809 
810     def __dask_tokenize__(self):
811         from dask.base import normalize_token
812 
813         return normalize_token((type(self), self._variable, self._coords, self._name))
814 
815     def __dask_graph__(self):
816         return self._to_temp_dataset().__dask_graph__()
817 
818     def __dask_keys__(self):
819         return self._to_temp_dataset().__dask_keys__()
820 
821     def __dask_layers__(self):
822         return self._to_temp_dataset().__dask_layers__()
823 
824     @property
825     def __dask_optimize__(self):
826         return self._to_temp_dataset().__dask_optimize__
827 
828     @property
829     def __dask_scheduler__(self):
830         return self._to_temp_dataset().__dask_scheduler__
831 
832     def __dask_postcompute__(self):
833         func, args = self._to_temp_dataset().__dask_postcompute__()
834         return self._dask_finalize, (func, args, self.name)
835 
836     def __dask_postpersist__(self):
837         func, args = self._to_temp_dataset().__dask_postpersist__()
838         return self._dask_finalize, (func, args, self.name)
839 
840     @staticmethod
841     def _dask_finalize(results, func, args, name):
842         ds = func(results, *args)
843         variable = ds._variables.pop(_THIS_ARRAY)
844         coords = ds._variables
845         return DataArray(variable, coords, name=name, fastpath=True)
846 
847     def load(self, **kwargs) -> "DataArray":
848         """Manually trigger loading of this array's data from disk or a
849         remote source into memory and return this array.
850 
851         Normally, it should not be necessary to call this method in user code,
852         because all xarray functions should either work on deferred data or
853         load data automatically. However, this method can be necessary when
854         working with many file objects on disk.
855 
856         Parameters
857         ----------
858         **kwargs : dict
859             Additional keyword arguments passed on to ``dask.array.compute``.
860 
861         See Also
862         --------
863         dask.array.compute
864         """
865         ds = self._to_temp_dataset().load(**kwargs)
866         new = self._from_temp_dataset(ds)
867         self._variable = new._variable
868         self._coords = new._coords
869         return self
870 
871     def compute(self, **kwargs) -> "DataArray":
872         """Manually trigger loading of this array's data from disk or a
873         remote source into memory and return a new array. The original is
874         left unaltered.
875 
876         Normally, it should not be necessary to call this method in user code,
877         because all xarray functions should either work on deferred data or
878         load data automatically. However, this method can be necessary when
879         working with many file objects on disk.
880 
881         Parameters
882         ----------
883         **kwargs : dict
884             Additional keyword arguments passed on to ``dask.array.compute``.
885 
886         See Also
887         --------
888         dask.array.compute
889         """
890         new = self.copy(deep=False)
891         return new.load(**kwargs)
892 
893     def persist(self, **kwargs) -> "DataArray":
894         """Trigger computation in constituent dask arrays
895 
896         This keeps them as dask arrays but encourages them to keep data in
897         memory.  This is particularly useful when on a distributed machine.
898         When on a single machine consider using ``.compute()`` instead.
899 
900         Parameters
901         ----------
902         **kwargs : dict
903             Additional keyword arguments passed on to ``dask.persist``.
904 
905         See Also
906         --------
907         dask.persist
908         """
909         ds = self._to_temp_dataset().persist(**kwargs)
910         return self._from_temp_dataset(ds)
911 
912     def copy(self, deep: bool = True, data: Any = None) -> "DataArray":
913         """Returns a copy of this array.
914 
915         If `deep=True`, a deep copy is made of the data array.
916         Otherwise, a shallow copy is made, and the returned data array's
917         values are a new view of this data array's values.
918 
919         Use `data` to create a new object with the same structure as
920         original but entirely new data.
921 
922         Parameters
923         ----------
924         deep : bool, optional
925             Whether the data array and its coordinates are loaded into memory
926             and copied onto the new object. Default is True.
927         data : array_like, optional
928             Data to use in the new object. Must have same shape as original.
929             When `data` is used, `deep` is ignored for all data variables,
930             and only used for coords.
931 
932         Returns
933         -------
934         object : DataArray
935             New object with dimensions, attributes, coordinates, name,
936             encoding, and optionally data copied from original.
937 
938         Examples
939         --------
940 
941         Shallow versus deep copy
942 
943         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
944         >>> array.copy()
945         <xarray.DataArray (x: 3)>
946         array([1, 2, 3])
947         Coordinates:
948           * x        (x) <U1 'a' 'b' 'c'
949         >>> array_0 = array.copy(deep=False)
950         >>> array_0[0] = 7
951         >>> array_0
952         <xarray.DataArray (x: 3)>
953         array([7, 2, 3])
954         Coordinates:
955           * x        (x) <U1 'a' 'b' 'c'
956         >>> array
957         <xarray.DataArray (x: 3)>
958         array([7, 2, 3])
959         Coordinates:
960           * x        (x) <U1 'a' 'b' 'c'
961 
962         Changing the data using the ``data`` argument maintains the
963         structure of the original object, but with the new data. Original
964         object is unaffected.
965 
966         >>> array.copy(data=[0.1, 0.2, 0.3])
967         <xarray.DataArray (x: 3)>
968         array([0.1, 0.2, 0.3])
969         Coordinates:
970           * x        (x) <U1 'a' 'b' 'c'
971         >>> array
972         <xarray.DataArray (x: 3)>
973         array([7, 2, 3])
974         Coordinates:
975           * x        (x) <U1 'a' 'b' 'c'
976 
977         See Also
978         --------
979         pandas.DataFrame.copy
980         """
981         variable = self.variable.copy(deep=deep, data=data)
982         coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}
983         if self._indexes is None:
984             indexes = self._indexes
985         else:
986             indexes = {k: v.copy(deep=deep) for k, v in self._indexes.items()}
987         return self._replace(variable, coords, indexes=indexes)
988 
989     def __copy__(self) -> "DataArray":
990         return self.copy(deep=False)
991 
992     def __deepcopy__(self, memo=None) -> "DataArray":
993         # memo does nothing but is required for compatibility with
994         # copy.deepcopy
995         return self.copy(deep=True)
996 
997     # mutable objects should not be hashable
998     # https://github.com/python/mypy/issues/4266
999     __hash__ = None  # type: ignore
1000 
1001     @property
1002     def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:
1003         """Block dimensions for this array's data or None if it's not a dask
1004         array.
1005         """
1006         return self.variable.chunks
1007 
1008     def chunk(
1009         self,
1010         chunks: Union[
1011             Number,
1012             Tuple[Number, ...],
1013             Tuple[Tuple[Number, ...], ...],
1014             Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]],
1015         ] = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1016         name_prefix: str = "xarray-",
1017         token: str = None,
1018         lock: bool = False,
1019     ) -> "DataArray":
1020         """Coerce this array's data into a dask arrays with the given chunks.
1021 
1022         If this variable is a non-dask array, it will be converted to dask
1023         array. If it's a dask array, it will be rechunked to the given chunk
1024         sizes.
1025 
1026         If neither chunks is not provided for one or more dimensions, chunk
1027         sizes along that dimension will not be updated; non-dask arrays will be
1028         converted into dask arrays with a single block.
1029 
1030         Parameters
1031         ----------
1032         chunks : int, tuple of int or mapping of hashable to int, optional
1033             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
1034             ``{'x': 5, 'y': 5}``.
1035         name_prefix : str, optional
1036             Prefix for the name of the new dask array.
1037         token : str, optional
1038             Token uniquely identifying this array.
1039         lock : optional
1040             Passed on to :py:func:`dask.array.from_array`, if the array is not
1041             already as dask array.
1042 
1043         Returns
1044         -------
1045         chunked : xarray.DataArray
1046         """
1047         if isinstance(chunks, (tuple, list)):
1048             chunks = dict(zip(self.dims, chunks))
1049 
1050         ds = self._to_temp_dataset().chunk(
1051             chunks, name_prefix=name_prefix, token=token, lock=lock
1052         )
1053         return self._from_temp_dataset(ds)
1054 
1055     def isel(
1056         self,
1057         indexers: Mapping[Hashable, Any] = None,
1058         drop: bool = False,
1059         missing_dims: str = "raise",
1060         **indexers_kwargs: Any,
1061     ) -> "DataArray":
1062         """Return a new DataArray whose data is given by integer indexing
1063         along the specified dimension(s).
1064 
1065         Parameters
1066         ----------
1067         indexers : dict, optional
1068             A dict with keys matching dimensions and values given
1069             by integers, slice objects or arrays.
1070             indexer can be a integer, slice, array-like or DataArray.
1071             If DataArrays are passed as indexers, xarray-style indexing will be
1072             carried out. See :ref:`indexing` for the details.
1073             One of indexers or indexers_kwargs must be provided.
1074         drop : bool, optional
1075             If ``drop=True``, drop coordinates variables indexed by integers
1076             instead of making them scalar.
1077         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1078             What to do if dimensions that should be selected from are not present in the
1079             DataArray:
1080             - "raise": raise an exception
1081             - "warning": raise a warning, and ignore the missing dimensions
1082             - "ignore": ignore the missing dimensions
1083         **indexers_kwargs : {dim: indexer, ...}, optional
1084             The keyword arguments form of ``indexers``.
1085 
1086         See Also
1087         --------
1088         Dataset.isel
1089         DataArray.sel
1090         """
1091 
1092         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1093 
1094         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1095             ds = self._to_temp_dataset()._isel_fancy(
1096                 indexers, drop=drop, missing_dims=missing_dims
1097             )
1098             return self._from_temp_dataset(ds)
1099 
1100         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1101         # lists, or zero or one-dimensional np.ndarray's
1102 
1103         variable = self._variable.isel(indexers, missing_dims=missing_dims)
1104 
1105         coords = {}
1106         for coord_name, coord_value in self._coords.items():
1107             coord_indexers = {
1108                 k: v for k, v in indexers.items() if k in coord_value.dims
1109             }
1110             if coord_indexers:
1111                 coord_value = coord_value.isel(coord_indexers)
1112                 if drop and coord_value.ndim == 0:
1113                     continue
1114             coords[coord_name] = coord_value
1115 
1116         return self._replace(variable=variable, coords=coords)
1117 
1118     def sel(
1119         self,
1120         indexers: Mapping[Hashable, Any] = None,
1121         method: str = None,
1122         tolerance=None,
1123         drop: bool = False,
1124         **indexers_kwargs: Any,
1125     ) -> "DataArray":
1126         """Return a new DataArray whose data is given by selecting index
1127         labels along the specified dimension(s).
1128 
1129         In contrast to `DataArray.isel`, indexers for this method should use
1130         labels instead of integers.
1131 
1132         Under the hood, this method is powered by using pandas's powerful Index
1133         objects. This makes label based indexing essentially just as fast as
1134         using integer indexing.
1135 
1136         It also means this method uses pandas's (well documented) logic for
1137         indexing. This means you can use string shortcuts for datetime indexes
1138         (e.g., '2000-01' to select all values in January 2000). It also means
1139         that slices are treated as inclusive of both the start and stop values,
1140         unlike normal Python indexing.
1141 
1142         .. warning::
1143 
1144           Do not try to assign values when using any of the indexing methods
1145           ``isel`` or ``sel``::
1146 
1147             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1148             # DO NOT do this
1149             da.isel(x=[0, 1, 2])[1] = -1
1150 
1151           Assigning values with the chained indexing using ``.sel`` or
1152           ``.isel`` fails silently.
1153 
1154         Parameters
1155         ----------
1156         indexers : dict, optional
1157             A dict with keys matching dimensions and values given
1158             by scalars, slices or arrays of tick labels. For dimensions with
1159             multi-index, the indexer may also be a dict-like object with keys
1160             matching index level names.
1161             If DataArrays are passed as indexers, xarray-style indexing will be
1162             carried out. See :ref:`indexing` for the details.
1163             One of indexers or indexers_kwargs must be provided.
1164         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1165             Method to use for inexact matches:
1166 
1167             * None (default): only exact matches
1168             * pad / ffill: propagate last valid index value forward
1169             * backfill / bfill: propagate next valid index value backward
1170             * nearest: use nearest valid index value
1171         tolerance : optional
1172             Maximum distance between original and new labels for inexact
1173             matches. The values of the index at the matching locations must
1174             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1175         drop : bool, optional
1176             If ``drop=True``, drop coordinates variables in `indexers` instead
1177             of making them scalar.
1178         **indexers_kwargs : {dim: indexer, ...}, optional
1179             The keyword arguments form of ``indexers``.
1180             One of indexers or indexers_kwargs must be provided.
1181 
1182         Returns
1183         -------
1184         obj : DataArray
1185             A new DataArray with the same contents as this DataArray, except the
1186             data and each dimension is indexed by the appropriate indexers.
1187             If indexer DataArrays have coordinates that do not conflict with
1188             this object, then these coordinates will be attached.
1189             In general, each array's data will be a view of the array's data
1190             in this DataArray, unless vectorized indexing was triggered by using
1191             an array indexer, in which case the data will be a copy.
1192 
1193         See Also
1194         --------
1195         Dataset.sel
1196         DataArray.isel
1197 
1198         """
1199         ds = self._to_temp_dataset().sel(
1200             indexers=indexers,
1201             drop=drop,
1202             method=method,
1203             tolerance=tolerance,
1204             **indexers_kwargs,
1205         )
1206         return self._from_temp_dataset(ds)
1207 
1208     def head(
1209         self,
1210         indexers: Union[Mapping[Hashable, int], int] = None,
1211         **indexers_kwargs: Any,
1212     ) -> "DataArray":
1213         """Return a new DataArray whose data is given by the the first `n`
1214         values along the specified dimension(s). Default `n` = 5
1215 
1216         See Also
1217         --------
1218         Dataset.head
1219         DataArray.tail
1220         DataArray.thin
1221         """
1222         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1223         return self._from_temp_dataset(ds)
1224 
1225     def tail(
1226         self,
1227         indexers: Union[Mapping[Hashable, int], int] = None,
1228         **indexers_kwargs: Any,
1229     ) -> "DataArray":
1230         """Return a new DataArray whose data is given by the the last `n`
1231         values along the specified dimension(s). Default `n` = 5
1232 
1233         See Also
1234         --------
1235         Dataset.tail
1236         DataArray.head
1237         DataArray.thin
1238         """
1239         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1240         return self._from_temp_dataset(ds)
1241 
1242     def thin(
1243         self,
1244         indexers: Union[Mapping[Hashable, int], int] = None,
1245         **indexers_kwargs: Any,
1246     ) -> "DataArray":
1247         """Return a new DataArray whose data is given by each `n` value
1248         along the specified dimension(s).
1249 
1250         See Also
1251         --------
1252         Dataset.thin
1253         DataArray.head
1254         DataArray.tail
1255         """
1256         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1257         return self._from_temp_dataset(ds)
1258 
1259     def broadcast_like(
1260         self, other: Union["DataArray", Dataset], exclude: Iterable[Hashable] = None
1261     ) -> "DataArray":
1262         """Broadcast this DataArray against another Dataset or DataArray.
1263 
1264         This is equivalent to xr.broadcast(other, self)[1]
1265 
1266         xarray objects are broadcast against each other in arithmetic
1267         operations, so this method is not be necessary for most uses.
1268 
1269         If no change is needed, the input data is returned to the output
1270         without being copied.
1271 
1272         If new coords are added by the broadcast, their values are
1273         NaN filled.
1274 
1275         Parameters
1276         ----------
1277         other : Dataset or DataArray
1278             Object against which to broadcast this array.
1279         exclude : iterable of hashable, optional
1280             Dimensions that must not be broadcasted
1281 
1282         Returns
1283         -------
1284         new_da : DataArray
1285             The caller broadcasted against ``other``.
1286 
1287         Examples
1288         --------
1289 
1290         >>> arr1 = xr.DataArray(
1291         ...     np.random.randn(2, 3),
1292         ...     dims=("x", "y"),
1293         ...     coords={"x": ["a", "b"], "y": ["a", "b", "c"]},
1294         ... )
1295         >>> arr2 = xr.DataArray(
1296         ...     np.random.randn(3, 2),
1297         ...     dims=("x", "y"),
1298         ...     coords={"x": ["a", "b", "c"], "y": ["a", "b"]},
1299         ... )
1300         >>> arr1
1301         <xarray.DataArray (x: 2, y: 3)>
1302         array([[ 1.76405235,  0.40015721,  0.97873798],
1303                [ 2.2408932 ,  1.86755799, -0.97727788]])
1304         Coordinates:
1305           * x        (x) <U1 'a' 'b'
1306           * y        (y) <U1 'a' 'b' 'c'
1307         >>> arr2
1308         <xarray.DataArray (x: 3, y: 2)>
1309         array([[ 0.95008842, -0.15135721],
1310                [-0.10321885,  0.4105985 ],
1311                [ 0.14404357,  1.45427351]])
1312         Coordinates:
1313           * x        (x) <U1 'a' 'b' 'c'
1314           * y        (y) <U1 'a' 'b'
1315         >>> arr1.broadcast_like(arr2)
1316         <xarray.DataArray (x: 3, y: 3)>
1317         array([[ 1.76405235,  0.40015721,  0.97873798],
1318                [ 2.2408932 ,  1.86755799, -0.97727788],
1319                [        nan,         nan,         nan]])
1320         Coordinates:
1321           * x        (x) object 'a' 'b' 'c'
1322           * y        (y) object 'a' 'b' 'c'
1323         """
1324         if exclude is None:
1325             exclude = set()
1326         else:
1327             exclude = set(exclude)
1328         args = align(other, self, join="outer", copy=False, exclude=exclude)
1329 
1330         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1331 
1332         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
1333 
1334     def reindex_like(
1335         self,
1336         other: Union["DataArray", Dataset],
1337         method: str = None,
1338         tolerance=None,
1339         copy: bool = True,
1340         fill_value=dtypes.NA,
1341     ) -> "DataArray":
1342         """Conform this object onto the indexes of another object, filling in
1343         missing values with ``fill_value``. The default fill value is NaN.
1344 
1345         Parameters
1346         ----------
1347         other : Dataset or DataArray
1348             Object with an 'indexes' attribute giving a mapping from dimension
1349             names to pandas.Index objects, which provides coordinates upon
1350             which to index the variables in this dataset. The indexes on this
1351             other object need not be the same as the indexes on this
1352             dataset. Any mis-matched index values will be filled in with
1353             NaN, and any mis-matched dimension names will simply be ignored.
1354         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1355             Method to use for filling index values from other not found on this
1356             data array:
1357 
1358             * None (default): don't fill gaps
1359             * pad / ffill: propagate last valid index value forward
1360             * backfill / bfill: propagate next valid index value backward
1361             * nearest: use nearest valid index value
1362         tolerance : optional
1363             Maximum distance between original and new labels for inexact
1364             matches. The values of the index at the matching locations must
1365             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1366         copy : bool, optional
1367             If ``copy=True``, data in the return value is always copied. If
1368             ``copy=False`` and reindexing is unnecessary, or can be performed
1369             with only slice operations, then the output may share memory with
1370             the input. In either case, a new xarray object is always returned.
1371         fill_value : scalar or dict-like, optional
1372             Value to use for newly missing values. If a dict-like, maps
1373             variable names (including coordinates) to fill values. Use this
1374             data array's name to refer to the data array's values.
1375 
1376         Returns
1377         -------
1378         reindexed : DataArray
1379             Another dataset array, with this array's data but coordinates from
1380             the other object.
1381 
1382         See Also
1383         --------
1384         DataArray.reindex
1385         align
1386         """
1387         indexers = reindex_like_indexers(self, other)
1388         return self.reindex(
1389             indexers=indexers,
1390             method=method,
1391             tolerance=tolerance,
1392             copy=copy,
1393             fill_value=fill_value,
1394         )
1395 
1396     def reindex(
1397         self,
1398         indexers: Mapping[Hashable, Any] = None,
1399         method: str = None,
1400         tolerance=None,
1401         copy: bool = True,
1402         fill_value=dtypes.NA,
1403         **indexers_kwargs: Any,
1404     ) -> "DataArray":
1405         """Conform this object onto the indexes of another object, filling in
1406         missing values with ``fill_value``. The default fill value is NaN.
1407 
1408         Parameters
1409         ----------
1410         indexers : dict, optional
1411             Dictionary with keys given by dimension names and values given by
1412             arrays of coordinates tick labels. Any mis-matched coordinate
1413             values will be filled in with NaN, and any mis-matched dimension
1414             names will simply be ignored.
1415             One of indexers or indexers_kwargs must be provided.
1416         copy : bool, optional
1417             If ``copy=True``, data in the return value is always copied. If
1418             ``copy=False`` and reindexing is unnecessary, or can be performed
1419             with only slice operations, then the output may share memory with
1420             the input. In either case, a new xarray object is always returned.
1421         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1422             Method to use for filling index values in ``indexers`` not found on
1423             this data array:
1424 
1425             * None (default): don't fill gaps
1426             * pad / ffill: propagate last valid index value forward
1427             * backfill / bfill: propagate next valid index value backward
1428             * nearest: use nearest valid index value
1429         tolerance : optional
1430             Maximum distance between original and new labels for inexact
1431             matches. The values of the index at the matching locations must
1432             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1433         fill_value : scalar or dict-like, optional
1434             Value to use for newly missing values. If a dict-like, maps
1435             variable names (including coordinates) to fill values. Use this
1436             data array's name to refer to the data array's values.
1437         **indexers_kwargs : {dim: indexer, ...}, optional
1438             The keyword arguments form of ``indexers``.
1439             One of indexers or indexers_kwargs must be provided.
1440 
1441         Returns
1442         -------
1443         reindexed : DataArray
1444             Another dataset array, with this array's data but replaced
1445             coordinates.
1446 
1447         See Also
1448         --------
1449         DataArray.reindex_like
1450         align
1451         """
1452         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
1453         if isinstance(fill_value, dict):
1454             fill_value = fill_value.copy()
1455             sentinel = object()
1456             value = fill_value.pop(self.name, sentinel)
1457             if value is not sentinel:
1458                 fill_value[_THIS_ARRAY] = value
1459 
1460         ds = self._to_temp_dataset().reindex(
1461             indexers=indexers,
1462             method=method,
1463             tolerance=tolerance,
1464             copy=copy,
1465             fill_value=fill_value,
1466         )
1467         return self._from_temp_dataset(ds)
1468 
1469     def interp(
1470         self,
1471         coords: Mapping[Hashable, Any] = None,
1472         method: str = "linear",
1473         assume_sorted: bool = False,
1474         kwargs: Mapping[str, Any] = None,
1475         **coords_kwargs: Any,
1476     ) -> "DataArray":
1477         """Multidimensional interpolation of variables.
1478 
1479         Parameters
1480         ----------
1481         coords : dict, optional
1482             Mapping from dimension names to the new coordinates.
1483             New coordinate can be an scalar, array-like or DataArray.
1484             If DataArrays are passed as new coordinates, their dimensions are
1485             used for the broadcasting. Missing values are skipped.
1486         method : str, default: "linear"
1487             The method used to interpolate. Choose from
1488 
1489             - {"linear", "nearest"} for multidimensional array,
1490             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"} for 1-dimensional array.
1491         assume_sorted : bool, optional
1492             If False, values of x can be in any order and they are sorted
1493             first. If True, x has to be an array of monotonically increasing
1494             values.
1495         kwargs : dict
1496             Additional keyword arguments passed to scipy's interpolator. Valid
1497             options and their behavior depend on if 1-dimensional or
1498             multi-dimensional interpolation is used.
1499         **coords_kwargs : {dim: coordinate, ...}, optional
1500             The keyword arguments form of ``coords``.
1501             One of coords or coords_kwargs must be provided.
1502 
1503         Returns
1504         -------
1505         interpolated : DataArray
1506             New dataarray on the new coordinates.
1507 
1508         Notes
1509         -----
1510         scipy is required.
1511 
1512         See Also
1513         --------
1514         scipy.interpolate.interp1d
1515         scipy.interpolate.interpn
1516 
1517         Examples
1518         --------
1519         >>> da = xr.DataArray(
1520         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
1521         ...     dims=("x", "y"),
1522         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
1523         ... )
1524         >>> da
1525         <xarray.DataArray (x: 3, y: 4)>
1526         array([[ 1.,  4.,  2.,  9.],
1527                [ 2.,  7.,  6., nan],
1528                [ 6., nan,  5.,  8.]])
1529         Coordinates:
1530           * x        (x) int64 0 1 2
1531           * y        (y) int64 10 12 14 16
1532 
1533         1D linear interpolation (the default):
1534 
1535         >>> da.interp(x=[0, 0.75, 1.25, 1.75])
1536         <xarray.DataArray (x: 4, y: 4)>
1537         array([[1.  , 4.  , 2.  ,  nan],
1538                [1.75, 6.25, 5.  ,  nan],
1539                [3.  ,  nan, 5.75,  nan],
1540                [5.  ,  nan, 5.25,  nan]])
1541         Coordinates:
1542           * y        (y) int64 10 12 14 16
1543           * x        (x) float64 0.0 0.75 1.25 1.75
1544 
1545         1D nearest interpolation:
1546 
1547         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
1548         <xarray.DataArray (x: 4, y: 4)>
1549         array([[ 1.,  4.,  2.,  9.],
1550                [ 2.,  7.,  6., nan],
1551                [ 2.,  7.,  6., nan],
1552                [ 6., nan,  5.,  8.]])
1553         Coordinates:
1554           * y        (y) int64 10 12 14 16
1555           * x        (x) float64 0.0 0.75 1.25 1.75
1556 
1557         1D linear extrapolation:
1558 
1559         >>> da.interp(
1560         ...     x=[1, 1.5, 2.5, 3.5],
1561         ...     method="linear",
1562         ...     kwargs={"fill_value": "extrapolate"},
1563         ... )
1564         <xarray.DataArray (x: 4, y: 4)>
1565         array([[ 2. ,  7. ,  6. ,  nan],
1566                [ 4. ,  nan,  5.5,  nan],
1567                [ 8. ,  nan,  4.5,  nan],
1568                [12. ,  nan,  3.5,  nan]])
1569         Coordinates:
1570           * y        (y) int64 10 12 14 16
1571           * x        (x) float64 1.0 1.5 2.5 3.5
1572 
1573         2D linear interpolation:
1574 
1575         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
1576         <xarray.DataArray (x: 4, y: 3)>
1577         array([[2.5  , 3.   ,   nan],
1578                [4.   , 5.625,   nan],
1579                [  nan,   nan,   nan],
1580                [  nan,   nan,   nan]])
1581         Coordinates:
1582           * x        (x) float64 0.0 0.75 1.25 1.75
1583           * y        (y) int64 11 13 15
1584         """
1585         if self.dtype.kind not in "uifc":
1586             raise TypeError(
1587                 "interp only works for a numeric type array. "
1588                 "Given {}.".format(self.dtype)
1589             )
1590         ds = self._to_temp_dataset().interp(
1591             coords,
1592             method=method,
1593             kwargs=kwargs,
1594             assume_sorted=assume_sorted,
1595             **coords_kwargs,
1596         )
1597         return self._from_temp_dataset(ds)
1598 
1599     def interp_like(
1600         self,
1601         other: Union["DataArray", Dataset],
1602         method: str = "linear",
1603         assume_sorted: bool = False,
1604         kwargs: Mapping[str, Any] = None,
1605     ) -> "DataArray":
1606         """Interpolate this object onto the coordinates of another object,
1607         filling out of range values with NaN.
1608 
1609         Parameters
1610         ----------
1611         other : Dataset or DataArray
1612             Object with an 'indexes' attribute giving a mapping from dimension
1613             names to an 1d array-like, which provides coordinates upon
1614             which to index the variables in this dataset. Missing values are skipped.
1615         method : str, default: "linear"
1616             The method used to interpolate. Choose from
1617 
1618             - {"linear", "nearest"} for multidimensional array,
1619             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"} for 1-dimensional array.
1620         assume_sorted : bool, optional
1621             If False, values of coordinates that are interpolated over can be
1622             in any order and they are sorted first. If True, interpolated
1623             coordinates are assumed to be an array of monotonically increasing
1624             values.
1625         kwargs : dict, optional
1626             Additional keyword passed to scipy's interpolator.
1627 
1628         Returns
1629         -------
1630         interpolated : DataArray
1631             Another dataarray by interpolating this dataarray's data along the
1632             coordinates of the other object.
1633 
1634         Notes
1635         -----
1636         scipy is required.
1637         If the dataarray has object-type coordinates, reindex is used for these
1638         coordinates instead of the interpolation.
1639 
1640         See Also
1641         --------
1642         DataArray.interp
1643         DataArray.reindex_like
1644         """
1645         if self.dtype.kind not in "uifc":
1646             raise TypeError(
1647                 "interp only works for a numeric type array. "
1648                 "Given {}.".format(self.dtype)
1649             )
1650         ds = self._to_temp_dataset().interp_like(
1651             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
1652         )
1653         return self._from_temp_dataset(ds)
1654 
1655     def rename(
1656         self,
1657         new_name_or_name_dict: Union[Hashable, Mapping[Hashable, Hashable]] = None,
1658         **names: Hashable,
1659     ) -> "DataArray":
1660         """Returns a new DataArray with renamed coordinates or a new name.
1661 
1662         Parameters
1663         ----------
1664         new_name_or_name_dict : str or dict-like, optional
1665             If the argument is dict-like, it used as a mapping from old
1666             names to new names for coordinates. Otherwise, use the argument
1667             as the new name for this array.
1668         **names : hashable, optional
1669             The keyword arguments form of a mapping from old names to
1670             new names for coordinates.
1671             One of new_name_or_name_dict or names must be provided.
1672 
1673         Returns
1674         -------
1675         renamed : DataArray
1676             Renamed array or array with renamed coordinates.
1677 
1678         See Also
1679         --------
1680         Dataset.rename
1681         DataArray.swap_dims
1682         """
1683         if names or utils.is_dict_like(new_name_or_name_dict):
1684             new_name_or_name_dict = cast(
1685                 Mapping[Hashable, Hashable], new_name_or_name_dict
1686             )
1687             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
1688             dataset = self._to_temp_dataset().rename(name_dict)
1689             return self._from_temp_dataset(dataset)
1690         else:
1691             new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)
1692             return self._replace(name=new_name_or_name_dict)
1693 
1694     def swap_dims(self, dims_dict: Mapping[Hashable, Hashable]) -> "DataArray":
1695         """Returns a new DataArray with swapped dimensions.
1696 
1697         Parameters
1698         ----------
1699         dims_dict : dict-like
1700             Dictionary whose keys are current dimension names and whose values
1701             are new names.
1702 
1703         Returns
1704         -------
1705         swapped : DataArray
1706             DataArray with swapped dimensions.
1707 
1708         Examples
1709         --------
1710 
1711         >>> arr = xr.DataArray(
1712         ...     data=[0, 1],
1713         ...     dims="x",
1714         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
1715         ... )
1716         >>> arr
1717         <xarray.DataArray (x: 2)>
1718         array([0, 1])
1719         Coordinates:
1720           * x        (x) <U1 'a' 'b'
1721             y        (x) int64 0 1
1722 
1723         >>> arr.swap_dims({"x": "y"})
1724         <xarray.DataArray (y: 2)>
1725         array([0, 1])
1726         Coordinates:
1727             x        (y) <U1 'a' 'b'
1728           * y        (y) int64 0 1
1729 
1730         >>> arr.swap_dims({"x": "z"})
1731         <xarray.DataArray (z: 2)>
1732         array([0, 1])
1733         Coordinates:
1734             x        (z) <U1 'a' 'b'
1735             y        (z) int64 0 1
1736         Dimensions without coordinates: z
1737 
1738         See Also
1739         --------
1740 
1741         DataArray.rename
1742         Dataset.swap_dims
1743         """
1744         ds = self._to_temp_dataset().swap_dims(dims_dict)
1745         return self._from_temp_dataset(ds)
1746 
1747     def expand_dims(
1748         self,
1749         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,
1750         axis=None,
1751         **dim_kwargs: Any,
1752     ) -> "DataArray":
1753         """Return a new object with an additional axis (or axes) inserted at
1754         the corresponding position in the array shape. The new object is a
1755         view into the underlying array, not a copy.
1756 
1757 
1758         If dim is already a scalar coordinate, it will be promoted to a 1D
1759         coordinate consisting of a single value.
1760 
1761         Parameters
1762         ----------
1763         dim : hashable, sequence of hashable, dict, or None, optional
1764             Dimensions to include on the new variable.
1765             If provided as str or sequence of str, then dimensions are inserted
1766             with length 1. If provided as a dict, then the keys are the new
1767             dimensions and the values are either integers (giving the length of
1768             the new dimensions) or sequence/ndarray (giving the coordinates of
1769             the new dimensions).
1770         axis : int, list of int or tuple of int, or None, default: None
1771             Axis position(s) where new axis is to be inserted (position(s) on
1772             the result array). If a list (or tuple) of integers is passed,
1773             multiple axes are inserted. In this case, dim arguments should be
1774             same length list. If axis=None is passed, all the axes will be
1775             inserted to the start of the result array.
1776         **dim_kwargs : int or sequence or ndarray
1777             The keywords are arbitrary dimensions being inserted and the values
1778             are either the lengths of the new dims (if int is given), or their
1779             coordinates. Note, this is an alternative to passing a dict to the
1780             dim kwarg and will only be used if dim is None.
1781 
1782         Returns
1783         -------
1784         expanded : same type as caller
1785             This object, but with an additional dimension(s).
1786         """
1787         if isinstance(dim, int):
1788             raise TypeError("dim should be hashable or sequence/mapping of hashables")
1789         elif isinstance(dim, Sequence) and not isinstance(dim, str):
1790             if len(dim) != len(set(dim)):
1791                 raise ValueError("dims should not contain duplicate values.")
1792             dim = dict.fromkeys(dim, 1)
1793         elif dim is not None and not isinstance(dim, Mapping):
1794             dim = {cast(Hashable, dim): 1}
1795 
1796         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
1797         ds = self._to_temp_dataset().expand_dims(dim, axis)
1798         return self._from_temp_dataset(ds)
1799 
1800     def set_index(
1801         self,
1802         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
1803         append: bool = False,
1804         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],
1805     ) -> Optional["DataArray"]:
1806         """Set DataArray (multi-)indexes using one or more existing
1807         coordinates.
1808 
1809         Parameters
1810         ----------
1811         indexes : {dim: index, ...}
1812             Mapping from names matching dimensions and values given
1813             by (lists of) the names of existing coordinates or variables to set
1814             as new (multi-)index.
1815         append : bool, optional
1816             If True, append the supplied index(es) to the existing index(es).
1817             Otherwise replace the existing index(es) (default).
1818         **indexes_kwargs : optional
1819             The keyword arguments form of ``indexes``.
1820             One of indexes or indexes_kwargs must be provided.
1821 
1822         Returns
1823         -------
1824         obj : DataArray
1825             Another DataArray, with this data but replaced coordinates.
1826 
1827         Examples
1828         --------
1829         >>> arr = xr.DataArray(
1830         ...     data=np.ones((2, 3)),
1831         ...     dims=["x", "y"],
1832         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
1833         ... )
1834         >>> arr
1835         <xarray.DataArray (x: 2, y: 3)>
1836         array([[1., 1., 1.],
1837                [1., 1., 1.]])
1838         Coordinates:
1839           * x        (x) int64 0 1
1840           * y        (y) int64 0 1 2
1841             a        (x) int64 3 4
1842         >>> arr.set_index(x="a")
1843         <xarray.DataArray (x: 2, y: 3)>
1844         array([[1., 1., 1.],
1845                [1., 1., 1.]])
1846         Coordinates:
1847           * x        (x) int64 3 4
1848           * y        (y) int64 0 1 2
1849 
1850         See Also
1851         --------
1852         DataArray.reset_index
1853         """
1854         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)
1855         return self._from_temp_dataset(ds)
1856 
1857     def reset_index(
1858         self,
1859         dims_or_levels: Union[Hashable, Sequence[Hashable]],
1860         drop: bool = False,
1861     ) -> Optional["DataArray"]:
1862         """Reset the specified index(es) or multi-index level(s).
1863 
1864         Parameters
1865         ----------
1866         dims_or_levels : hashable or sequence of hashable
1867             Name(s) of the dimension(s) and/or multi-index level(s) that will
1868             be reset.
1869         drop : bool, optional
1870             If True, remove the specified indexes and/or multi-index levels
1871             instead of extracting them as new coordinates (default: False).
1872 
1873         Returns
1874         -------
1875         obj : DataArray
1876             Another dataarray, with this dataarray's data but replaced
1877             coordinates.
1878 
1879         See Also
1880         --------
1881         DataArray.set_index
1882         """
1883         coords, _ = split_indexes(
1884             dims_or_levels, self._coords, set(), self._level_coords, drop=drop
1885         )
1886         return self._replace(coords=coords)
1887 
1888     def reorder_levels(
1889         self,
1890         dim_order: Mapping[Hashable, Sequence[int]] = None,
1891         **dim_order_kwargs: Sequence[int],
1892     ) -> "DataArray":
1893         """Rearrange index levels using input order.
1894 
1895         Parameters
1896         ----------
1897         dim_order : optional
1898             Mapping from names matching dimensions and values given
1899             by lists representing new level orders. Every given dimension
1900             must have a multi-index.
1901         **dim_order_kwargs : optional
1902             The keyword arguments form of ``dim_order``.
1903             One of dim_order or dim_order_kwargs must be provided.
1904 
1905         Returns
1906         -------
1907         obj : DataArray
1908             Another dataarray, with this dataarray's data but replaced
1909             coordinates.
1910         """
1911         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
1912         replace_coords = {}
1913         for dim, order in dim_order.items():
1914             coord = self._coords[dim]
1915             index = coord.to_index()
1916             if not isinstance(index, pd.MultiIndex):
1917                 raise ValueError("coordinate %r has no MultiIndex" % dim)
1918             replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))
1919         coords = self._coords.copy()
1920         coords.update(replace_coords)
1921         return self._replace(coords=coords)
1922 
1923     def stack(
1924         self,
1925         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
1926         **dimensions_kwargs: Sequence[Hashable],
1927     ) -> "DataArray":
1928         """
1929         Stack any number of existing dimensions into a single new dimension.
1930 
1931         New dimensions will be added at the end, and the corresponding
1932         coordinate variables will be combined into a MultiIndex.
1933 
1934         Parameters
1935         ----------
1936         dimensions : mapping of hashable to sequence of hashable
1937             Mapping of the form `new_name=(dim1, dim2, ...)`.
1938             Names of new dimensions, and the existing dimensions that they
1939             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
1940             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
1941             all dimensions.
1942         **dimensions_kwargs
1943             The keyword arguments form of ``dimensions``.
1944             One of dimensions or dimensions_kwargs must be provided.
1945 
1946         Returns
1947         -------
1948         stacked : DataArray
1949             DataArray with stacked data.
1950 
1951         Examples
1952         --------
1953 
1954         >>> arr = xr.DataArray(
1955         ...     np.arange(6).reshape(2, 3),
1956         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
1957         ... )
1958         >>> arr
1959         <xarray.DataArray (x: 2, y: 3)>
1960         array([[0, 1, 2],
1961                [3, 4, 5]])
1962         Coordinates:
1963           * x        (x) <U1 'a' 'b'
1964           * y        (y) int64 0 1 2
1965         >>> stacked = arr.stack(z=("x", "y"))
1966         >>> stacked.indexes["z"]
1967         MultiIndex([('a', 0),
1968                     ('a', 1),
1969                     ('a', 2),
1970                     ('b', 0),
1971                     ('b', 1),
1972                     ('b', 2)],
1973                    names=['x', 'y'])
1974 
1975         See Also
1976         --------
1977         DataArray.unstack
1978         """
1979         ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)
1980         return self._from_temp_dataset(ds)
1981 
1982     def unstack(
1983         self,
1984         dim: Union[Hashable, Sequence[Hashable], None] = None,
1985         fill_value: Any = dtypes.NA,
1986         sparse: bool = False,
1987     ) -> "DataArray":
1988         """
1989         Unstack existing dimensions corresponding to MultiIndexes into
1990         multiple new dimensions.
1991 
1992         New dimensions will be added at the end.
1993 
1994         Parameters
1995         ----------
1996         dim : hashable or sequence of hashable, optional
1997             Dimension(s) over which to unstack. By default unstacks all
1998             MultiIndexes.
1999         fill_value : scalar or dict-like, default: nan
2000             value to be filled. If a dict-like, maps variable names to
2001             fill values. Use the data array's name to refer to its
2002             name. If not provided or if the dict-like does not contain
2003             all variables, the dtype's NA value will be used.
2004         sparse : bool, default: False
2005             use sparse-array if True
2006 
2007         Returns
2008         -------
2009         unstacked : DataArray
2010             Array with unstacked data.
2011 
2012         Examples
2013         --------
2014 
2015         >>> arr = xr.DataArray(
2016         ...     np.arange(6).reshape(2, 3),
2017         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2018         ... )
2019         >>> arr
2020         <xarray.DataArray (x: 2, y: 3)>
2021         array([[0, 1, 2],
2022                [3, 4, 5]])
2023         Coordinates:
2024           * x        (x) <U1 'a' 'b'
2025           * y        (y) int64 0 1 2
2026         >>> stacked = arr.stack(z=("x", "y"))
2027         >>> stacked.indexes["z"]
2028         MultiIndex([('a', 0),
2029                     ('a', 1),
2030                     ('a', 2),
2031                     ('b', 0),
2032                     ('b', 1),
2033                     ('b', 2)],
2034                    names=['x', 'y'])
2035         >>> roundtripped = stacked.unstack()
2036         >>> arr.identical(roundtripped)
2037         True
2038 
2039         See Also
2040         --------
2041         DataArray.stack
2042         """
2043         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
2044         return self._from_temp_dataset(ds)
2045 
2046     def to_unstacked_dataset(self, dim, level=0):
2047         """Unstack DataArray expanding to Dataset along a given level of a
2048         stacked coordinate.
2049 
2050         This is the inverse operation of Dataset.to_stacked_array.
2051 
2052         Parameters
2053         ----------
2054         dim : str
2055             Name of existing dimension to unstack
2056         level : int or str
2057             The MultiIndex level to expand to a dataset along. Can either be
2058             the integer index of the level or its name.
2059         label : int, default: 0
2060             Label of the level to expand dataset along. Overrides the label
2061             argument if given.
2062 
2063         Returns
2064         -------
2065         unstacked: Dataset
2066 
2067         Examples
2068         --------
2069         >>> import xarray as xr
2070         >>> arr = xr.DataArray(
2071         ...     np.arange(6).reshape(2, 3),
2072         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2073         ... )
2074         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
2075         >>> data
2076         <xarray.Dataset>
2077         Dimensions:  (x: 2, y: 3)
2078         Coordinates:
2079           * x        (x) <U1 'a' 'b'
2080           * y        (y) int64 0 1 2
2081         Data variables:
2082             a        (x, y) int64 0 1 2 3 4 5
2083             b        (x) int64 0 3
2084         >>> stacked = data.to_stacked_array("z", ["x"])
2085         >>> stacked.indexes["z"]
2086         MultiIndex([('a', 0.0),
2087                     ('a', 1.0),
2088                     ('a', 2.0),
2089                     ('b', nan)],
2090                    names=['variable', 'y'])
2091         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
2092         >>> data.identical(roundtripped)
2093         True
2094 
2095         See Also
2096         --------
2097         Dataset.to_stacked_array
2098         """
2099 
2100         idx = self.indexes[dim]
2101         if not isinstance(idx, pd.MultiIndex):
2102             raise ValueError(f"'{dim}' is not a stacked coordinate")
2103 
2104         level_number = idx._get_level_number(level)
2105         variables = idx.levels[level_number]
2106         variable_dim = idx.names[level_number]
2107 
2108         # pull variables out of datarray
2109         data_dict = {}
2110         for k in variables:
2111             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)
2112 
2113         # unstacked dataset
2114         return Dataset(data_dict)
2115 
2116     def transpose(self, *dims: Hashable, transpose_coords: bool = True) -> "DataArray":
2117         """Return a new DataArray object with transposed dimensions.
2118 
2119         Parameters
2120         ----------
2121         *dims : hashable, optional
2122             By default, reverse the dimensions. Otherwise, reorder the
2123             dimensions to this order.
2124         transpose_coords : bool, default: True
2125             If True, also transpose the coordinates of this DataArray.
2126 
2127         Returns
2128         -------
2129         transposed : DataArray
2130             The returned DataArray's array is transposed.
2131 
2132         Notes
2133         -----
2134         This operation returns a view of this array's data. It is
2135         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
2136         -- the data will be fully loaded.
2137 
2138         See Also
2139         --------
2140         numpy.transpose
2141         Dataset.transpose
2142         """
2143         if dims:
2144             dims = tuple(utils.infix_dims(dims, self.dims))
2145         variable = self.variable.transpose(*dims)
2146         if transpose_coords:
2147             coords: Dict[Hashable, Variable] = {}
2148             for name, coord in self.coords.items():
2149                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
2150                 coords[name] = coord.variable.transpose(*coord_dims)
2151             return self._replace(variable, coords)
2152         else:
2153             return self._replace(variable)
2154 
2155     @property
2156     def T(self) -> "DataArray":
2157         return self.transpose()
2158 
2159     def drop_vars(
2160         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
2161     ) -> "DataArray":
2162         """Returns an array with dropped variables.
2163 
2164         Parameters
2165         ----------
2166         names : hashable or iterable of hashable
2167             Name(s) of variables to drop.
2168         errors: {"raise", "ignore"}, optional
2169             If 'raise' (default), raises a ValueError error if any of the variable
2170             passed are not in the dataset. If 'ignore', any given names that are in the
2171             DataArray are dropped and no error is raised.
2172 
2173         Returns
2174         -------
2175         dropped : Dataset
2176             New Dataset copied from `self` with variables removed.
2177         """
2178         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
2179         return self._from_temp_dataset(ds)
2180 
2181     def drop(
2182         self,
2183         labels: Mapping = None,
2184         dim: Hashable = None,
2185         *,
2186         errors: str = "raise",
2187         **labels_kwargs,
2188     ) -> "DataArray":
2189         """Backward compatible method based on `drop_vars` and `drop_sel`
2190 
2191         Using either `drop_vars` or `drop_sel` is encouraged
2192 
2193         See Also
2194         --------
2195         DataArray.drop_vars
2196         DataArray.drop_sel
2197         """
2198         ds = self._to_temp_dataset().drop(labels, dim, errors=errors)
2199         return self._from_temp_dataset(ds)
2200 
2201     def drop_sel(
2202         self,
2203         labels: Mapping[Hashable, Any] = None,
2204         *,
2205         errors: str = "raise",
2206         **labels_kwargs,
2207     ) -> "DataArray":
2208         """Drop index labels from this DataArray.
2209 
2210         Parameters
2211         ----------
2212         labels : mapping of hashable to Any
2213             Index labels to drop
2214         errors : {"raise", "ignore"}, optional
2215             If 'raise' (default), raises a ValueError error if
2216             any of the index labels passed are not
2217             in the dataset. If 'ignore', any given labels that are in the
2218             dataset are dropped and no error is raised.
2219         **labels_kwargs : {dim: label, ...}, optional
2220             The keyword arguments form of ``dim`` and ``labels``
2221 
2222         Returns
2223         -------
2224         dropped : DataArray
2225         """
2226         if labels_kwargs or isinstance(labels, dict):
2227             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
2228 
2229         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
2230         return self._from_temp_dataset(ds)
2231 
2232     def dropna(
2233         self, dim: Hashable, how: str = "any", thresh: int = None
2234     ) -> "DataArray":
2235         """Returns a new array with dropped labels for missing values along
2236         the provided dimension.
2237 
2238         Parameters
2239         ----------
2240         dim : hashable
2241             Dimension along which to drop missing values. Dropping along
2242             multiple dimensions simultaneously is not yet supported.
2243         how : {"any", "all"}, optional
2244             * any : if any NA values are present, drop that label
2245             * all : if all values are NA, drop that label
2246         thresh : int, default: None
2247             If supplied, require this many non-NA values.
2248 
2249         Returns
2250         -------
2251         DataArray
2252         """
2253         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
2254         return self._from_temp_dataset(ds)
2255 
2256     def fillna(self, value: Any) -> "DataArray":
2257         """Fill missing values in this object.
2258 
2259         This operation follows the normal broadcasting and alignment rules that
2260         xarray uses for binary arithmetic, except the result is aligned to this
2261         object (``join='left'``) instead of aligned to the intersection of
2262         index coordinates (``join='inner'``).
2263 
2264         Parameters
2265         ----------
2266         value : scalar, ndarray or DataArray
2267             Used to fill all matching missing values in this array. If the
2268             argument is a DataArray, it is first aligned with (reindexed to)
2269             this array.
2270 
2271         Returns
2272         -------
2273         DataArray
2274         """
2275         if utils.is_dict_like(value):
2276             raise TypeError(
2277                 "cannot provide fill value as a dictionary with "
2278                 "fillna on a DataArray"
2279             )
2280         out = ops.fillna(self, value)
2281         return out
2282 
2283     def interpolate_na(
2284         self,
2285         dim: Hashable = None,
2286         method: str = "linear",
2287         limit: int = None,
2288         use_coordinate: Union[bool, str] = True,
2289         max_gap: Union[
2290             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta
2291         ] = None,
2292         keep_attrs: bool = None,
2293         **kwargs: Any,
2294     ) -> "DataArray":
2295         """Fill in NaNs by interpolating according to different methods.
2296 
2297         Parameters
2298         ----------
2299         dim : str
2300             Specifies the dimension along which to interpolate.
2301         method : str, optional
2302             String indicating which method to use for interpolation:
2303 
2304             - 'linear': linear interpolation (Default). Additional keyword
2305               arguments are passed to :py:func:`numpy.interp`
2306             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
2307               are passed to :py:func:`scipy.interpolate.interp1d`. If
2308               ``method='polynomial'``, the ``order`` keyword argument must also be
2309               provided.
2310             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
2311               respective :py:class:`scipy.interpolate` classes.
2312 
2313         use_coordinate : bool or str, default: True
2314             Specifies which index to use as the x values in the interpolation
2315             formulated as `y = f(x)`. If False, values are treated as if
2316             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
2317             used. If ``use_coordinate`` is a string, it specifies the name of a
2318             coordinate variariable to use as the index.
2319         limit : int, default: None
2320             Maximum number of consecutive NaNs to fill. Must be greater than 0
2321             or None for no limit. This filling is done regardless of the size of
2322             the gap in the data. To only interpolate over gaps less than a given length,
2323             see ``max_gap``.
2324         max_gap: int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
2325             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
2326             Use None for no limit. When interpolating along a datetime64 dimension
2327             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
2328 
2329             - a string that is valid input for pandas.to_timedelta
2330             - a :py:class:`numpy.timedelta64` object
2331             - a :py:class:`pandas.Timedelta` object
2332             - a :py:class:`datetime.timedelta` object
2333 
2334             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
2335             dimensions has not been implemented yet. Gap length is defined as the difference
2336             between coordinate values at the first data point after a gap and the last value
2337             before a gap. For gaps at the beginning (end), gap length is defined as the difference
2338             between coordinate values at the first (last) valid data point and the first (last) NaN.
2339             For example, consider::
2340 
2341                 <xarray.DataArray (x: 9)>
2342                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
2343                 Coordinates:
2344                   * x        (x) int64 0 1 2 3 4 5 6 7 8
2345 
2346             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
2347         keep_attrs : bool, default: True
2348             If True, the dataarray's attributes (`attrs`) will be copied from
2349             the original object to the new one.  If False, the new
2350             object will be returned without attributes.
2351         kwargs : dict, optional
2352             parameters passed verbatim to the underlying interpolation function
2353 
2354         Returns
2355         -------
2356         interpolated: DataArray
2357             Filled in DataArray.
2358 
2359         See also
2360         --------
2361         numpy.interp
2362         scipy.interpolate
2363 
2364         Examples
2365         --------
2366         >>> da = xr.DataArray(
2367         ...     [np.nan, 2, 3, np.nan, 0], dims="x", coords={"x": [0, 1, 2, 3, 4]}
2368         ... )
2369         >>> da
2370         <xarray.DataArray (x: 5)>
2371         array([nan,  2.,  3., nan,  0.])
2372         Coordinates:
2373           * x        (x) int64 0 1 2 3 4
2374 
2375         >>> da.interpolate_na(dim="x", method="linear")
2376         <xarray.DataArray (x: 5)>
2377         array([nan, 2. , 3. , 1.5, 0. ])
2378         Coordinates:
2379           * x        (x) int64 0 1 2 3 4
2380 
2381         >>> da.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
2382         <xarray.DataArray (x: 5)>
2383         array([1. , 2. , 3. , 1.5, 0. ])
2384         Coordinates:
2385           * x        (x) int64 0 1 2 3 4
2386         """
2387         from .missing import interp_na
2388 
2389         return interp_na(
2390             self,
2391             dim=dim,
2392             method=method,
2393             limit=limit,
2394             use_coordinate=use_coordinate,
2395             max_gap=max_gap,
2396             keep_attrs=keep_attrs,
2397             **kwargs,
2398         )
2399 
2400     def ffill(self, dim: Hashable, limit: int = None) -> "DataArray":
2401         """Fill NaN values by propogating values forward
2402 
2403         *Requires bottleneck.*
2404 
2405         Parameters
2406         ----------
2407         dim : hashable
2408             Specifies the dimension along which to propagate values when
2409             filling.
2410         limit : int, default: None
2411             The maximum number of consecutive NaN values to forward fill. In
2412             other words, if there is a gap with more than this number of
2413             consecutive NaNs, it will only be partially filled. Must be greater
2414             than 0 or None for no limit.
2415 
2416         Returns
2417         -------
2418         DataArray
2419         """
2420         from .missing import ffill
2421 
2422         return ffill(self, dim, limit=limit)
2423 
2424     def bfill(self, dim: Hashable, limit: int = None) -> "DataArray":
2425         """Fill NaN values by propogating values backward
2426 
2427         *Requires bottleneck.*
2428 
2429         Parameters
2430         ----------
2431         dim : str
2432             Specifies the dimension along which to propagate values when
2433             filling.
2434         limit : int, default: None
2435             The maximum number of consecutive NaN values to backward fill. In
2436             other words, if there is a gap with more than this number of
2437             consecutive NaNs, it will only be partially filled. Must be greater
2438             than 0 or None for no limit.
2439 
2440         Returns
2441         -------
2442         DataArray
2443         """
2444         from .missing import bfill
2445 
2446         return bfill(self, dim, limit=limit)
2447 
2448     def combine_first(self, other: "DataArray") -> "DataArray":
2449         """Combine two DataArray objects, with union of coordinates.
2450 
2451         This operation follows the normal broadcasting and alignment rules of
2452         ``join='outer'``.  Default to non-null values of array calling the
2453         method.  Use np.nan to fill in vacant cells after alignment.
2454 
2455         Parameters
2456         ----------
2457         other : DataArray
2458             Used to fill all matching missing values in this array.
2459 
2460         Returns
2461         -------
2462         DataArray
2463         """
2464         return ops.fillna(self, other, join="outer")
2465 
2466     def reduce(
2467         self,
2468         func: Callable[..., Any],
2469         dim: Union[None, Hashable, Sequence[Hashable]] = None,
2470         axis: Union[None, int, Sequence[int]] = None,
2471         keep_attrs: bool = None,
2472         keepdims: bool = False,
2473         **kwargs: Any,
2474     ) -> "DataArray":
2475         """Reduce this array by applying `func` along some dimension(s).
2476 
2477         Parameters
2478         ----------
2479         func : callable
2480             Function which can be called in the form
2481             `f(x, axis=axis, **kwargs)` to return the result of reducing an
2482             np.ndarray over an integer valued axis.
2483         dim : hashable or sequence of hashable, optional
2484             Dimension(s) over which to apply `func`.
2485         axis : int or sequence of int, optional
2486             Axis(es) over which to repeatedly apply `func`. Only one of the
2487             'dim' and 'axis' arguments can be supplied. If neither are
2488             supplied, then the reduction is calculated over the flattened array
2489             (by calling `f(x)` without an axis argument).
2490         keep_attrs : bool, optional
2491             If True, the variable's attributes (`attrs`) will be copied from
2492             the original object to the new one.  If False (default), the new
2493             object will be returned without attributes.
2494         keepdims : bool, default: False
2495             If True, the dimensions which are reduced are left in the result
2496             as dimensions of size one. Coordinates that use these dimensions
2497             are removed.
2498         **kwargs : dict
2499             Additional keyword arguments passed on to `func`.
2500 
2501         Returns
2502         -------
2503         reduced : DataArray
2504             DataArray with this object's array replaced with an array with
2505             summarized data and the indicated dimension(s) removed.
2506         """
2507 
2508         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
2509         return self._replace_maybe_drop_dims(var)
2510 
2511     def to_pandas(self) -> Union["DataArray", pd.Series, pd.DataFrame]:
2512         """Convert this array into a pandas object with the same shape.
2513 
2514         The type of the returned object depends on the number of DataArray
2515         dimensions:
2516 
2517         * 0D -> `xarray.DataArray`
2518         * 1D -> `pandas.Series`
2519         * 2D -> `pandas.DataFrame`
2520 
2521         Only works for arrays with 2 or fewer dimensions.
2522 
2523         The DataArray constructor performs the inverse transformation.
2524         """
2525         # TODO: consolidate the info about pandas constructors and the
2526         # attributes that correspond to their indexes into a separate module?
2527         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
2528         try:
2529             constructor = constructors[self.ndim]
2530         except KeyError:
2531             raise ValueError(
2532                 "cannot convert arrays with %s dimensions into "
2533                 "pandas objects" % self.ndim
2534             )
2535         indexes = [self.get_index(dim) for dim in self.dims]
2536         return constructor(self.values, *indexes)
2537 
2538     def to_dataframe(
2539         self, name: Hashable = None, dim_order: List[Hashable] = None
2540     ) -> pd.DataFrame:
2541         """Convert this array and its coordinates into a tidy pandas.DataFrame.
2542 
2543         The DataFrame is indexed by the Cartesian product of index coordinates
2544         (in the form of a :py:class:`pandas.MultiIndex`).
2545 
2546         Other coordinates are included as columns in the DataFrame.
2547 
2548         Parameters
2549         ----------
2550         name
2551             Name to give to this array (required if unnamed).
2552         dim_order
2553             Hierarchical dimension order for the resulting dataframe.
2554             Array content is transposed to this order and then written out as flat
2555             vectors in contiguous order, so the last dimension in this list
2556             will be contiguous in the resulting DataFrame. This has a major
2557             influence on which operations are efficient on the resulting
2558             dataframe.
2559 
2560             If provided, must include all dimensions of this DataArray. By default,
2561             dimensions are sorted according to the DataArray dimensions order.
2562 
2563         Returns
2564         -------
2565         result
2566             DataArray as a pandas DataFrame.
2567 
2568         """
2569         if name is None:
2570             name = self.name
2571         if name is None:
2572             raise ValueError(
2573                 "cannot convert an unnamed DataArray to a "
2574                 "DataFrame: use the ``name`` parameter"
2575             )
2576         if self.ndim == 0:
2577             raise ValueError("cannot convert a scalar to a DataFrame")
2578 
2579         # By using a unique name, we can convert a DataArray into a DataFrame
2580         # even if it shares a name with one of its coordinates.
2581         # I would normally use unique_name = object() but that results in a
2582         # dataframe with columns in the wrong order, for reasons I have not
2583         # been able to debug (possibly a pandas bug?).
2584         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
2585         ds = self._to_dataset_whole(name=unique_name)
2586 
2587         if dim_order is None:
2588             ordered_dims = dict(zip(self.dims, self.shape))
2589         else:
2590             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)
2591 
2592         df = ds._to_dataframe(ordered_dims)
2593         df.columns = [name if c == unique_name else c for c in df.columns]
2594         return df
2595 
2596     def to_series(self) -> pd.Series:
2597         """Convert this array into a pandas.Series.
2598 
2599         The Series is indexed by the Cartesian product of index coordinates
2600         (in the form of a :py:class:`pandas.MultiIndex`).
2601         """
2602         index = self.coords.to_index()
2603         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
2604 
2605     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
2606         """Convert this array into a numpy.ma.MaskedArray
2607 
2608         Parameters
2609         ----------
2610         copy : bool, default: True
2611             If True make a copy of the array in the result. If False,
2612             a MaskedArray view of DataArray.values is returned.
2613 
2614         Returns
2615         -------
2616         result : MaskedArray
2617             Masked where invalid values (nan or inf) occur.
2618         """
2619         values = self.values  # only compute lazy arrays once
2620         isnull = pd.isnull(values)
2621         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
2622 
2623     def to_netcdf(self, *args, **kwargs) -> Union[bytes, "Delayed", None]:
2624         """Write DataArray contents to a netCDF file.
2625 
2626         All parameters are passed directly to :py:meth:`xarray.Dataset.to_netcdf`.
2627 
2628         Notes
2629         -----
2630         Only xarray.Dataset objects can be written to netCDF files, so
2631         the xarray.DataArray is converted to a xarray.Dataset object
2632         containing a single variable. If the DataArray has no name, or if the
2633         name is the same as a coordinate name, then it is given the name
2634         ``"__xarray_dataarray_variable__"``.
2635 
2636         See Also
2637         --------
2638         Dataset.to_netcdf
2639         """
2640         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE
2641 
2642         if self.name is None:
2643             # If no name is set then use a generic xarray name
2644             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2645         elif self.name in self.coords or self.name in self.dims:
2646             # The name is the same as one of the coords names, which netCDF
2647             # doesn't support, so rename it but keep track of the old name
2648             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2649             dataset.attrs[DATAARRAY_NAME] = self.name
2650         else:
2651             # No problems with the name - so we're fine!
2652             dataset = self.to_dataset()
2653 
2654         return dataset.to_netcdf(*args, **kwargs)
2655 
2656     def to_dict(self, data: bool = True) -> dict:
2657         """
2658         Convert this xarray.DataArray into a dictionary following xarray
2659         naming conventions.
2660 
2661         Converts all variables and attributes to native Python objects.
2662         Useful for converting to json. To avoid datetime incompatibility
2663         use decode_times=False kwarg in xarray.open_dataset.
2664 
2665         Parameters
2666         ----------
2667         data : bool, optional
2668             Whether to include the actual data in the dictionary. When set to
2669             False, returns just the schema.
2670 
2671         See also
2672         --------
2673         DataArray.from_dict
2674         """
2675         d = self.variable.to_dict(data=data)
2676         d.update({"coords": {}, "name": self.name})
2677         for k in self.coords:
2678             d["coords"][k] = self.coords[k].variable.to_dict(data=data)
2679         return d
2680 
2681     @classmethod
2682     def from_dict(cls, d: dict) -> "DataArray":
2683         """
2684         Convert a dictionary into an xarray.DataArray
2685 
2686         Input dict can take several forms:
2687 
2688         .. code:: python
2689 
2690             d = {"dims": ("t"), "data": x}
2691 
2692             d = {
2693                 "coords": {"t": {"dims": "t", "data": t, "attrs": {"units": "s"}}},
2694                 "attrs": {"title": "air temperature"},
2695                 "dims": "t",
2696                 "data": x,
2697                 "name": "a",
2698             }
2699 
2700         where "t" is the name of the dimesion, "a" is the name of the array,
2701         and x and t are lists, numpy.arrays, or pandas objects.
2702 
2703         Parameters
2704         ----------
2705         d : dict
2706             Mapping with a minimum structure of {"dims": [...], "data": [...]}
2707 
2708         Returns
2709         -------
2710         obj : xarray.DataArray
2711 
2712         See also
2713         --------
2714         DataArray.to_dict
2715         Dataset.from_dict
2716         """
2717         coords = None
2718         if "coords" in d:
2719             try:
2720                 coords = {
2721                     k: (v["dims"], v["data"], v.get("attrs"))
2722                     for k, v in d["coords"].items()
2723                 }
2724             except KeyError as e:
2725                 raise ValueError(
2726                     "cannot convert dict when coords are missing the key "
2727                     "'{dims_data}'".format(dims_data=str(e.args[0]))
2728                 )
2729         try:
2730             data = d["data"]
2731         except KeyError:
2732             raise ValueError("cannot convert dict without the key 'data''")
2733         else:
2734             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
2735         return obj
2736 
2737     @classmethod
2738     def from_series(cls, series: pd.Series, sparse: bool = False) -> "DataArray":
2739         """Convert a pandas.Series into an xarray.DataArray.
2740 
2741         If the series's index is a MultiIndex, it will be expanded into a
2742         tensor product of one-dimensional coordinates (filling in missing
2743         values with NaN). Thus this operation should be the inverse of the
2744         `to_series` method.
2745 
2746         If sparse=True, creates a sparse array instead of a dense NumPy array.
2747         Requires the pydata/sparse package.
2748 
2749         See also
2750         --------
2751         xarray.Dataset.from_dataframe
2752         """
2753         temp_name = "__temporary_name"
2754         df = pd.DataFrame({temp_name: series})
2755         ds = Dataset.from_dataframe(df, sparse=sparse)
2756         result = cast(DataArray, ds[temp_name])
2757         result.name = series.name
2758         return result
2759 
2760     def to_cdms2(self) -> "cdms2_Variable":
2761         """Convert this array into a cdms2.Variable"""
2762         from ..convert import to_cdms2
2763 
2764         return to_cdms2(self)
2765 
2766     @classmethod
2767     def from_cdms2(cls, variable: "cdms2_Variable") -> "DataArray":
2768         """Convert a cdms2.Variable into an xarray.DataArray"""
2769         from ..convert import from_cdms2
2770 
2771         return from_cdms2(variable)
2772 
2773     def to_iris(self) -> "iris_Cube":
2774         """Convert this array into a iris.cube.Cube"""
2775         from ..convert import to_iris
2776 
2777         return to_iris(self)
2778 
2779     @classmethod
2780     def from_iris(cls, cube: "iris_Cube") -> "DataArray":
2781         """Convert a iris.cube.Cube into an xarray.DataArray"""
2782         from ..convert import from_iris
2783 
2784         return from_iris(cube)
2785 
2786     def _all_compat(self, other: "DataArray", compat_str: str) -> bool:
2787         """Helper function for equals, broadcast_equals, and identical"""
2788 
2789         def compat(x, y):
2790             return getattr(x.variable, compat_str)(y.variable)
2791 
2792         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
2793             self, other
2794         )
2795 
2796     def broadcast_equals(self, other: "DataArray") -> bool:
2797         """Two DataArrays are broadcast equal if they are equal after
2798         broadcasting them against each other such that they have the same
2799         dimensions.
2800 
2801         See Also
2802         --------
2803         DataArray.equals
2804         DataArray.identical
2805         """
2806         try:
2807             return self._all_compat(other, "broadcast_equals")
2808         except (TypeError, AttributeError):
2809             return False
2810 
2811     def equals(self, other: "DataArray") -> bool:
2812         """True if two DataArrays have the same dimensions, coordinates and
2813         values; otherwise False.
2814 
2815         DataArrays can still be equal (like pandas objects) if they have NaN
2816         values in the same locations.
2817 
2818         This method is necessary because `v1 == v2` for ``DataArray``
2819         does element-wise comparisons (like numpy.ndarrays).
2820 
2821         See Also
2822         --------
2823         DataArray.broadcast_equals
2824         DataArray.identical
2825         """
2826         try:
2827             return self._all_compat(other, "equals")
2828         except (TypeError, AttributeError):
2829             return False
2830 
2831     def identical(self, other: "DataArray") -> bool:
2832         """Like equals, but also checks the array name and attributes, and
2833         attributes on all coordinates.
2834 
2835         See Also
2836         --------
2837         DataArray.broadcast_equals
2838         DataArray.equals
2839         """
2840         try:
2841             return self.name == other.name and self._all_compat(other, "identical")
2842         except (TypeError, AttributeError):
2843             return False
2844 
2845     def _result_name(self, other: Any = None) -> Optional[Hashable]:
2846         # use the same naming heuristics as pandas:
2847         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
2848         other_name = getattr(other, "name", _default)
2849         if other_name is _default or other_name == self.name:
2850             return self.name
2851         else:
2852             return None
2853 
2854     def __array_wrap__(self, obj, context=None) -> "DataArray":
2855         new_var = self.variable.__array_wrap__(obj, context)
2856         return self._replace(new_var)
2857 
2858     def __matmul__(self, obj):
2859         return self.dot(obj)
2860 
2861     def __rmatmul__(self, other):
2862         # currently somewhat duplicative, as only other DataArrays are
2863         # compatible with matmul
2864         return computation.dot(other, self)
2865 
2866     @staticmethod
2867     def _unary_op(f: Callable[..., Any]) -> Callable[..., "DataArray"]:
2868         @functools.wraps(f)
2869         def func(self, *args, **kwargs):
2870             keep_attrs = kwargs.pop("keep_attrs", None)
2871             if keep_attrs is None:
2872                 keep_attrs = _get_keep_attrs(default=True)
2873             with warnings.catch_warnings():
2874                 warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
2875                 warnings.filterwarnings(
2876                     "ignore", r"Mean of empty slice", category=RuntimeWarning
2877                 )
2878                 with np.errstate(all="ignore"):
2879                     da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))
2880                 if keep_attrs:
2881                     da.attrs = self.attrs
2882                 return da
2883 
2884         return func
2885 
2886     @staticmethod
2887     def _binary_op(
2888         f: Callable[..., Any],
2889         reflexive: bool = False,
2890         join: str = None,  # see xarray.align
2891         **ignored_kwargs,
2892     ) -> Callable[..., "DataArray"]:
2893         @functools.wraps(f)
2894         def func(self, other):
2895             if isinstance(other, (Dataset, groupby.GroupBy)):
2896                 return NotImplemented
2897             if isinstance(other, DataArray):
2898                 align_type = OPTIONS["arithmetic_join"] if join is None else join
2899                 self, other = align(self, other, join=align_type, copy=False)
2900             other_variable = getattr(other, "variable", other)
2901             other_coords = getattr(other, "coords", None)
2902 
2903             variable = (
2904                 f(self.variable, other_variable)
2905                 if not reflexive
2906                 else f(other_variable, self.variable)
2907             )
2908             coords, indexes = self.coords._merge_raw(other_coords)
2909             name = self._result_name(other)
2910 
2911             return self._replace(variable, coords, name, indexes=indexes)
2912 
2913         return func
2914 
2915     @staticmethod
2916     def _inplace_binary_op(f: Callable) -> Callable[..., "DataArray"]:
2917         @functools.wraps(f)
2918         def func(self, other):
2919             if isinstance(other, groupby.GroupBy):
2920                 raise TypeError(
2921                     "in-place operations between a DataArray and "
2922                     "a grouped object are not permitted"
2923                 )
2924             # n.b. we can't align other to self (with other.reindex_like(self))
2925             # because `other` may be converted into floats, which would cause
2926             # in-place arithmetic to fail unpredictably. Instead, we simply
2927             # don't support automatic alignment with in-place arithmetic.
2928             other_coords = getattr(other, "coords", None)
2929             other_variable = getattr(other, "variable", other)
2930             try:
2931                 with self.coords._merge_inplace(other_coords):
2932                     f(self.variable, other_variable)
2933             except MergeError as exc:
2934                 raise MergeError(
2935                     "Automatic alignment is not supported for in-place operations.\n"
2936                     "Consider aligning the indices manually or using a not-in-place operation.\n"
2937                     "See https://github.com/pydata/xarray/issues/3910 for more explanations."
2938                 ) from exc
2939             return self
2940 
2941         return func
2942 
2943     def _copy_attrs_from(self, other: Union["DataArray", Dataset, Variable]) -> None:
2944         self.attrs = other.attrs
2945 
2946     plot = utils.UncachedAccessor(_PlotMethods)
2947 
2948     def _title_for_slice(self, truncate: int = 50) -> str:
2949         """
2950         If the dataarray has 1 dimensional coordinates or comes from a slice
2951         we can show that info in the title
2952 
2953         Parameters
2954         ----------
2955         truncate : int, default: 50
2956             maximum number of characters for title
2957 
2958         Returns
2959         -------
2960         title : string
2961             Can be used for plot titles
2962 
2963         """
2964         one_dims = []
2965         for dim, coord in self.coords.items():
2966             if coord.size == 1:
2967                 one_dims.append(
2968                     "{dim} = {v}".format(dim=dim, v=format_item(coord.values))
2969                 )
2970 
2971         title = ", ".join(one_dims)
2972         if len(title) > truncate:
2973             title = title[: (truncate - 3)] + "..."
2974 
2975         return title
2976 
2977     def diff(self, dim: Hashable, n: int = 1, label: Hashable = "upper") -> "DataArray":
2978         """Calculate the n-th order discrete difference along given axis.
2979 
2980         Parameters
2981         ----------
2982         dim : hashable
2983             Dimension over which to calculate the finite difference.
2984         n : int, optional
2985             The number of times values are differenced.
2986         label : hashable, optional
2987             The new coordinate in dimension ``dim`` will have the
2988             values of either the minuend's or subtrahend's coordinate
2989             for values 'upper' and 'lower', respectively.  Other
2990             values are not supported.
2991 
2992         Returns
2993         -------
2994         difference : same type as caller
2995             The n-th order finite difference of this object.
2996 
2997         .. note::
2998 
2999             `n` matches numpy's behavior and is different from pandas' first
3000             argument named `periods`.
3001 
3002 
3003         Examples
3004         --------
3005         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
3006         >>> arr.diff("x")
3007         <xarray.DataArray (x: 3)>
3008         array([0, 1, 0])
3009         Coordinates:
3010           * x        (x) int64 2 3 4
3011         >>> arr.diff("x", 2)
3012         <xarray.DataArray (x: 2)>
3013         array([ 1, -1])
3014         Coordinates:
3015           * x        (x) int64 3 4
3016 
3017         See Also
3018         --------
3019         DataArray.differentiate
3020         """
3021         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
3022         return self._from_temp_dataset(ds)
3023 
3024     def shift(
3025         self,
3026         shifts: Mapping[Hashable, int] = None,
3027         fill_value: Any = dtypes.NA,
3028         **shifts_kwargs: int,
3029     ) -> "DataArray":
3030         """Shift this array by an offset along one or more dimensions.
3031 
3032         Only the data is moved; coordinates stay in place. Values shifted from
3033         beyond array bounds are replaced by NaN. This is consistent with the
3034         behavior of ``shift`` in pandas.
3035 
3036         Parameters
3037         ----------
3038         shifts : mapping of hashable to int, optional
3039             Integer offset to shift along each of the given dimensions.
3040             Positive offsets shift to the right; negative offsets shift to the
3041             left.
3042         fill_value: scalar, optional
3043             Value to use for newly missing values
3044         **shifts_kwargs
3045             The keyword arguments form of ``shifts``.
3046             One of shifts or shifts_kwargs must be provided.
3047 
3048         Returns
3049         -------
3050         shifted : DataArray
3051             DataArray with the same coordinates and attributes but shifted
3052             data.
3053 
3054         See also
3055         --------
3056         roll
3057 
3058         Examples
3059         --------
3060 
3061         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3062         >>> arr.shift(x=1)
3063         <xarray.DataArray (x: 3)>
3064         array([nan,  5.,  6.])
3065         Dimensions without coordinates: x
3066         """
3067         variable = self.variable.shift(
3068             shifts=shifts, fill_value=fill_value, **shifts_kwargs
3069         )
3070         return self._replace(variable=variable)
3071 
3072     def roll(
3073         self,
3074         shifts: Mapping[Hashable, int] = None,
3075         roll_coords: bool = None,
3076         **shifts_kwargs: int,
3077     ) -> "DataArray":
3078         """Roll this array by an offset along one or more dimensions.
3079 
3080         Unlike shift, roll may rotate all variables, including coordinates
3081         if specified. The direction of rotation is consistent with
3082         :py:func:`numpy.roll`.
3083 
3084         Parameters
3085         ----------
3086         shifts : mapping of hashable to int, optional
3087             Integer offset to rotate each of the given dimensions.
3088             Positive offsets roll to the right; negative offsets roll to the
3089             left.
3090         roll_coords : bool
3091             Indicates whether to roll the coordinates by the offset
3092             The current default of roll_coords (None, equivalent to True) is
3093             deprecated and will change to False in a future version.
3094             Explicitly pass roll_coords to silence the warning.
3095         **shifts_kwargs
3096             The keyword arguments form of ``shifts``.
3097             One of shifts or shifts_kwargs must be provided.
3098 
3099         Returns
3100         -------
3101         rolled : DataArray
3102             DataArray with the same attributes but rolled data and coordinates.
3103 
3104         See also
3105         --------
3106         shift
3107 
3108         Examples
3109         --------
3110 
3111         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3112         >>> arr.roll(x=1)
3113         <xarray.DataArray (x: 3)>
3114         array([7, 5, 6])
3115         Dimensions without coordinates: x
3116         """
3117         ds = self._to_temp_dataset().roll(
3118             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
3119         )
3120         return self._from_temp_dataset(ds)
3121 
3122     @property
3123     def real(self) -> "DataArray":
3124         return self._replace(self.variable.real)
3125 
3126     @property
3127     def imag(self) -> "DataArray":
3128         return self._replace(self.variable.imag)
3129 
3130     def dot(
3131         self, other: "DataArray", dims: Union[Hashable, Sequence[Hashable], None] = None
3132     ) -> "DataArray":
3133         """Perform dot product of two DataArrays along their shared dims.
3134 
3135         Equivalent to taking taking tensordot over all shared dims.
3136 
3137         Parameters
3138         ----------
3139         other : DataArray
3140             The other array with which the dot product is performed.
3141         dims : ..., hashable or sequence of hashable, optional
3142             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.
3143             If not specified, then all the common dimensions are summed over.
3144 
3145         Returns
3146         -------
3147         result : DataArray
3148             Array resulting from the dot product over all shared dimensions.
3149 
3150         See also
3151         --------
3152         dot
3153         numpy.tensordot
3154 
3155         Examples
3156         --------
3157 
3158         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
3159         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
3160         >>> dm_vals = np.arange(4)
3161         >>> dm = xr.DataArray(dm_vals, dims=["z"])
3162 
3163         >>> dm.dims
3164         ('z',)
3165 
3166         >>> da.dims
3167         ('x', 'y', 'z')
3168 
3169         >>> dot_result = da.dot(dm)
3170         >>> dot_result.dims
3171         ('x', 'y')
3172 
3173         """
3174         if isinstance(other, Dataset):
3175             raise NotImplementedError(
3176                 "dot products are not yet supported with Dataset objects."
3177             )
3178         if not isinstance(other, DataArray):
3179             raise TypeError("dot only operates on DataArrays.")
3180 
3181         return computation.dot(self, other, dims=dims)
3182 
3183     def sortby(
3184         self,
3185         variables: Union[Hashable, "DataArray", Sequence[Union[Hashable, "DataArray"]]],
3186         ascending: bool = True,
3187     ) -> "DataArray":
3188         """Sort object by labels or values (along an axis).
3189 
3190         Sorts the dataarray, either along specified dimensions,
3191         or according to values of 1-D dataarrays that share dimension
3192         with calling object.
3193 
3194         If the input variables are dataarrays, then the dataarrays are aligned
3195         (via left-join) to the calling object prior to sorting by cell values.
3196         NaNs are sorted to the end, following Numpy convention.
3197 
3198         If multiple sorts along the same dimension is
3199         given, numpy's lexsort is performed along that dimension:
3200         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
3201         and the FIRST key in the sequence is used as the primary sort key,
3202         followed by the 2nd key, etc.
3203 
3204         Parameters
3205         ----------
3206         variables : hashable, DataArray, or sequence of hashable or DataArray
3207             1D DataArray objects or name(s) of 1D variable(s) in
3208             coords whose values are used to sort this array.
3209         ascending : bool, optional
3210             Whether to sort by ascending or descending order.
3211 
3212         Returns
3213         -------
3214         sorted : DataArray
3215             A new dataarray where all the specified dims are sorted by dim
3216             labels.
3217 
3218         Examples
3219         --------
3220 
3221         >>> da = xr.DataArray(
3222         ...     np.random.rand(5),
3223         ...     coords=[pd.date_range("1/1/2000", periods=5)],
3224         ...     dims="time",
3225         ... )
3226         >>> da
3227         <xarray.DataArray (time: 5)>
3228         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])
3229         Coordinates:
3230           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05
3231 
3232         >>> da.sortby(da)
3233         <xarray.DataArray (time: 5)>
3234         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])
3235         Coordinates:
3236           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02
3237         """
3238         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
3239         return self._from_temp_dataset(ds)
3240 
3241     def quantile(
3242         self,
3243         q: Any,
3244         dim: Union[Hashable, Sequence[Hashable], None] = None,
3245         interpolation: str = "linear",
3246         keep_attrs: bool = None,
3247         skipna: bool = True,
3248     ) -> "DataArray":
3249         """Compute the qth quantile of the data along the specified dimension.
3250 
3251         Returns the qth quantiles(s) of the array elements.
3252 
3253         Parameters
3254         ----------
3255         q : float or array-like of float
3256             Quantile to compute, which must be between 0 and 1 inclusive.
3257         dim : hashable or sequence of hashable, optional
3258             Dimension(s) over which to apply quantile.
3259         interpolation : {"linear", "lower", "higher", "midpoint", "nearest"}, default: "linear"
3260             This optional parameter specifies the interpolation method to
3261             use when the desired quantile lies between two data points
3262             ``i < j``:
3263 
3264                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is
3265                   the fractional part of the index surrounded by ``i`` and
3266                   ``j``.
3267                 - lower: ``i``.
3268                 - higher: ``j``.
3269                 - nearest: ``i`` or ``j``, whichever is nearest.
3270                 - midpoint: ``(i + j) / 2``.
3271         keep_attrs : bool, optional
3272             If True, the dataset's attributes (`attrs`) will be copied from
3273             the original object to the new one.  If False (default), the new
3274             object will be returned without attributes.
3275         skipna : bool, optional
3276             Whether to skip missing values when aggregating.
3277 
3278         Returns
3279         -------
3280         quantiles : DataArray
3281             If `q` is a single quantile, then the result
3282             is a scalar. If multiple percentiles are given, first axis of
3283             the result corresponds to the quantile and a quantile dimension
3284             is added to the return array. The other dimensions are the
3285             dimensions that remain after the reduction of the array.
3286 
3287         See Also
3288         --------
3289         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
3290 
3291         Examples
3292         --------
3293 
3294         >>> da = xr.DataArray(
3295         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
3296         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
3297         ...     dims=("x", "y"),
3298         ... )
3299         >>> da.quantile(0)  # or da.quantile(0, dim=...)
3300         <xarray.DataArray ()>
3301         array(0.7)
3302         Coordinates:
3303             quantile  float64 0.0
3304         >>> da.quantile(0, dim="x")
3305         <xarray.DataArray (y: 4)>
3306         array([0.7, 4.2, 2.6, 1.5])
3307         Coordinates:
3308           * y         (y) float64 1.0 1.5 2.0 2.5
3309             quantile  float64 0.0
3310         >>> da.quantile([0, 0.5, 1])
3311         <xarray.DataArray (quantile: 3)>
3312         array([0.7, 3.4, 9.4])
3313         Coordinates:
3314           * quantile  (quantile) float64 0.0 0.5 1.0
3315         >>> da.quantile([0, 0.5, 1], dim="x")
3316         <xarray.DataArray (quantile: 3, y: 4)>
3317         array([[0.7 , 4.2 , 2.6 , 1.5 ],
3318                [3.6 , 5.75, 6.  , 1.7 ],
3319                [6.5 , 7.3 , 9.4 , 1.9 ]])
3320         Coordinates:
3321           * y         (y) float64 1.0 1.5 2.0 2.5
3322           * quantile  (quantile) float64 0.0 0.5 1.0
3323         """
3324 
3325         ds = self._to_temp_dataset().quantile(
3326             q,
3327             dim=dim,
3328             keep_attrs=keep_attrs,
3329             interpolation=interpolation,
3330             skipna=skipna,
3331         )
3332         return self._from_temp_dataset(ds)
3333 
3334     def rank(
3335         self, dim: Hashable, pct: bool = False, keep_attrs: bool = None
3336     ) -> "DataArray":
3337         """Ranks the data.
3338 
3339         Equal values are assigned a rank that is the average of the ranks that
3340         would have been otherwise assigned to all of the values within that
3341         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
3342 
3343         NaNs in the input array are returned as NaNs.
3344 
3345         The `bottleneck` library is required.
3346 
3347         Parameters
3348         ----------
3349         dim : hashable
3350             Dimension over which to compute rank.
3351         pct : bool, optional
3352             If True, compute percentage ranks, otherwise compute integer ranks.
3353         keep_attrs : bool, optional
3354             If True, the dataset's attributes (`attrs`) will be copied from
3355             the original object to the new one.  If False (default), the new
3356             object will be returned without attributes.
3357 
3358         Returns
3359         -------
3360         ranked : DataArray
3361             DataArray with the same coordinates and dtype 'float64'.
3362 
3363         Examples
3364         --------
3365 
3366         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3367         >>> arr.rank("x")
3368         <xarray.DataArray (x: 3)>
3369         array([1., 2., 3.])
3370         Dimensions without coordinates: x
3371         """
3372 
3373         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
3374         return self._from_temp_dataset(ds)
3375 
3376     def differentiate(
3377         self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None
3378     ) -> "DataArray":
3379         """ Differentiate the array with the second order accurate central
3380         differences.
3381 
3382         .. note::
3383             This feature is limited to simple cartesian geometry, i.e. coord
3384             must be one dimensional.
3385 
3386         Parameters
3387         ----------
3388         coord : hashable
3389             The coordinate to be used to compute the gradient.
3390         edge_order : {1, 2}, default: 1
3391             N-th order accurate differences at the boundaries.
3392         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", \
3393                          "us", "ns", "ps", "fs", "as"} or None, optional
3394             Unit to compute gradient. Only valid for datetime coordinate.
3395 
3396         Returns
3397         -------
3398         differentiated: DataArray
3399 
3400         See also
3401         --------
3402         numpy.gradient: corresponding numpy function
3403 
3404         Examples
3405         --------
3406 
3407         >>> da = xr.DataArray(
3408         ...     np.arange(12).reshape(4, 3),
3409         ...     dims=["x", "y"],
3410         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3411         ... )
3412         >>> da
3413         <xarray.DataArray (x: 4, y: 3)>
3414         array([[ 0,  1,  2],
3415                [ 3,  4,  5],
3416                [ 6,  7,  8],
3417                [ 9, 10, 11]])
3418         Coordinates:
3419           * x        (x) float64 0.0 0.1 1.1 1.2
3420         Dimensions without coordinates: y
3421         >>>
3422         >>> da.differentiate("x")
3423         <xarray.DataArray (x: 4, y: 3)>
3424         array([[30.        , 30.        , 30.        ],
3425                [27.54545455, 27.54545455, 27.54545455],
3426                [27.54545455, 27.54545455, 27.54545455],
3427                [30.        , 30.        , 30.        ]])
3428         Coordinates:
3429           * x        (x) float64 0.0 0.1 1.1 1.2
3430         Dimensions without coordinates: y
3431         """
3432         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
3433         return self._from_temp_dataset(ds)
3434 
3435     def integrate(
3436         self, dim: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None
3437     ) -> "DataArray":
3438         """ integrate the array with the trapezoidal rule.
3439 
3440         .. note::
3441             This feature is limited to simple cartesian geometry, i.e. dim
3442             must be one dimensional.
3443 
3444         Parameters
3445         ----------
3446         dim : hashable, or sequence of hashable
3447             Coordinate(s) used for the integration.
3448         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", "us", "ns", \
3449                          "ps", "fs", "as"}, optional
3450             Can be used to specify the unit if datetime coordinate is used.
3451 
3452         Returns
3453         -------
3454         integrated: DataArray
3455 
3456         See also
3457         --------
3458         numpy.trapz: corresponding numpy function
3459 
3460         Examples
3461         --------
3462 
3463         >>> da = xr.DataArray(
3464         ...     np.arange(12).reshape(4, 3),
3465         ...     dims=["x", "y"],
3466         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3467         ... )
3468         >>> da
3469         <xarray.DataArray (x: 4, y: 3)>
3470         array([[ 0,  1,  2],
3471                [ 3,  4,  5],
3472                [ 6,  7,  8],
3473                [ 9, 10, 11]])
3474         Coordinates:
3475           * x        (x) float64 0.0 0.1 1.1 1.2
3476         Dimensions without coordinates: y
3477         >>>
3478         >>> da.integrate("x")
3479         <xarray.DataArray (y: 3)>
3480         array([5.4, 6.6, 7.8])
3481         Dimensions without coordinates: y
3482         """
3483         ds = self._to_temp_dataset().integrate(dim, datetime_unit)
3484         return self._from_temp_dataset(ds)
3485 
3486     def unify_chunks(self) -> "DataArray":
3487         """Unify chunk size along all chunked dimensions of this DataArray.
3488 
3489         Returns
3490         -------
3491 
3492         DataArray with consistent chunk sizes for all dask-array variables
3493 
3494         See Also
3495         --------
3496 
3497         dask.array.core.unify_chunks
3498         """
3499         ds = self._to_temp_dataset().unify_chunks()
3500         return self._from_temp_dataset(ds)
3501 
3502     def map_blocks(
3503         self,
3504         func: "Callable[..., T_DSorDA]",
3505         args: Sequence[Any] = (),
3506         kwargs: Mapping[str, Any] = None,
3507         template: Union["DataArray", "Dataset"] = None,
3508     ) -> "T_DSorDA":
3509         """
3510         Apply a function to each block of this DataArray.
3511 
3512         .. warning::
3513             This method is experimental and its signature may change.
3514 
3515         Parameters
3516         ----------
3517         func : callable
3518             User-provided function that accepts a DataArray as its first
3519             parameter. The function will receive a subset or 'block' of this DataArray (see below),
3520             corresponding to one chunk along each chunked dimension. ``func`` will be
3521             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.
3522 
3523             This function must return either a single DataArray or a single Dataset.
3524 
3525             This function cannot add a new chunked dimension.
3526         args : sequence
3527             Passed to func after unpacking and subsetting any xarray objects by blocks.
3528             xarray objects in args must be aligned with this object, otherwise an error is raised.
3529         kwargs : mapping
3530             Passed verbatim to func after unpacking. xarray objects, if any, will not be
3531             subset to blocks. Passing dask collections in kwargs is not allowed.
3532         template : DataArray or Dataset, optional
3533             xarray object representing the final result after compute is called. If not provided,
3534             the function will be first run on mocked-up data, that looks like this object but
3535             has sizes 0, to determine properties of the returned object such as dtype,
3536             variable names, attributes, new dimensions and new indexes (if any).
3537             ``template`` must be provided if the function changes the size of existing dimensions.
3538             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
3539             ``attrs`` set by ``func`` will be ignored.
3540 
3541         Returns
3542         -------
3543         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
3544         function.
3545 
3546         Notes
3547         -----
3548         This function is designed for when ``func`` needs to manipulate a whole xarray object
3549         subset to each block. In the more common case where ``func`` can work on numpy arrays, it is
3550         recommended to use ``apply_ufunc``.
3551 
3552         If none of the variables in this object is backed by dask arrays, calling this function is
3553         equivalent to calling ``func(obj, *args, **kwargs)``.
3554 
3555         See Also
3556         --------
3557         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks,
3558         xarray.DataArray.map_blocks
3559 
3560         Examples
3561         --------
3562 
3563         Calculate an anomaly from climatology using ``.groupby()``. Using
3564         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
3565         its indices, and its methods like ``.groupby()``.
3566 
3567         >>> def calculate_anomaly(da, groupby_type="time.month"):
3568         ...     gb = da.groupby(groupby_type)
3569         ...     clim = gb.mean(dim="time")
3570         ...     return gb - clim
3571         ...
3572         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
3573         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
3574         >>> np.random.seed(123)
3575         >>> array = xr.DataArray(
3576         ...     np.random.rand(len(time)),
3577         ...     dims=["time"],
3578         ...     coords={"time": time, "month": month},
3579         ... ).chunk()
3580         >>> array.map_blocks(calculate_anomaly, template=array).compute()
3581         <xarray.DataArray (time: 24)>
3582         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,
3583                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,
3584                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,
3585                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,
3586                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])
3587         Coordinates:
3588           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
3589             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
3590 
3591         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
3592         to the function being applied in ``xr.map_blocks()``:
3593 
3594         >>> array.map_blocks(
3595         ...     calculate_anomaly, kwargs={"groupby_type": "time.year"}, template=array
3596         ... )  # doctest: +ELLIPSIS
3597         <xarray.DataArray (time: 24)>
3598         dask.array<calculate_anomaly-...-<this, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>
3599         Coordinates:
3600           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
3601             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
3602         """
3603         from .parallel import map_blocks
3604 
3605         return map_blocks(func, self, args, kwargs, template)
3606 
3607     def polyfit(
3608         self,
3609         dim: Hashable,
3610         deg: int,
3611         skipna: bool = None,
3612         rcond: float = None,
3613         w: Union[Hashable, Any] = None,
3614         full: bool = False,
3615         cov: bool = False,
3616     ):
3617         """
3618         Least squares polynomial fit.
3619 
3620         This replicates the behaviour of `numpy.polyfit` but differs by skipping
3621         invalid values when `skipna = True`.
3622 
3623         Parameters
3624         ----------
3625         dim : hashable
3626             Coordinate along which to fit the polynomials.
3627         deg : int
3628             Degree of the fitting polynomial.
3629         skipna : bool, optional
3630             If True, removes all invalid values before fitting each 1D slices of the array.
3631             Default is True if data is stored in a dask.array or if there is any
3632             invalid values, False otherwise.
3633         rcond : float, optional
3634             Relative condition number to the fit.
3635         w : hashable or array-like, optional
3636             Weights to apply to the y-coordinate of the sample points.
3637             Can be an array-like object or the name of a coordinate in the dataset.
3638         full : bool, optional
3639             Whether to return the residuals, matrix rank and singular values in addition
3640             to the coefficients.
3641         cov : bool or str, optional
3642             Whether to return to the covariance matrix in addition to the coefficients.
3643             The matrix is not scaled if `cov='unscaled'`.
3644 
3645         Returns
3646         -------
3647         polyfit_results : Dataset
3648             A single dataset which contains:
3649 
3650             polyfit_coefficients
3651                 The coefficients of the best fit.
3652             polyfit_residuals
3653                 The residuals of the least-square computation (only included if `full=True`).
3654                 When the matrix rank is deficient, np.nan is returned.
3655             [dim]_matrix_rank
3656                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
3657             [dim]_singular_value
3658                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
3659             polyfit_covariance
3660                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
3661 
3662         See also
3663         --------
3664         numpy.polyfit
3665         """
3666         return self._to_temp_dataset().polyfit(
3667             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov
3668         )
3669 
3670     def pad(
3671         self,
3672         pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,
3673         mode: str = "constant",
3674         stat_length: Union[
3675             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3676         ] = None,
3677         constant_values: Union[
3678             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3679         ] = None,
3680         end_values: Union[
3681             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3682         ] = None,
3683         reflect_type: str = None,
3684         **pad_width_kwargs: Any,
3685     ) -> "DataArray":
3686         """Pad this array along one or more dimensions.
3687 
3688         .. warning::
3689             This function is experimental and its behaviour is likely to change
3690             especially regarding padding of dimension coordinates (or IndexVariables).
3691 
3692         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
3693         coordinates will be padded with the same mode, otherwise coordinates
3694         are padded using the "constant" mode with fill_value dtypes.NA.
3695 
3696         Parameters
3697         ----------
3698         pad_width : mapping of hashable to tuple of int
3699             Mapping with the form of {dim: (pad_before, pad_after)}
3700             describing the number of values padded along each dimension.
3701             {dim: pad} is a shortcut for pad_before = pad_after = pad
3702         mode : str, default: "constant"
3703             One of the following string values (taken from numpy docs)
3704 
3705             'constant' (default)
3706                 Pads with a constant value.
3707             'edge'
3708                 Pads with the edge values of array.
3709             'linear_ramp'
3710                 Pads with the linear ramp between end_value and the
3711                 array edge value.
3712             'maximum'
3713                 Pads with the maximum value of all or part of the
3714                 vector along each axis.
3715             'mean'
3716                 Pads with the mean value of all or part of the
3717                 vector along each axis.
3718             'median'
3719                 Pads with the median value of all or part of the
3720                 vector along each axis.
3721             'minimum'
3722                 Pads with the minimum value of all or part of the
3723                 vector along each axis.
3724             'reflect'
3725                 Pads with the reflection of the vector mirrored on
3726                 the first and last values of the vector along each
3727                 axis.
3728             'symmetric'
3729                 Pads with the reflection of the vector mirrored
3730                 along the edge of the array.
3731             'wrap'
3732                 Pads with the wrap of the vector along the axis.
3733                 The first values are used to pad the end and the
3734                 end values are used to pad the beginning.
3735         stat_length : int, tuple or mapping of hashable to tuple, default: None
3736             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
3737             values at edge of each axis used to calculate the statistic value.
3738             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
3739             statistic lengths along each dimension.
3740             ((before, after),) yields same before and after statistic lengths
3741             for each dimension.
3742             (stat_length,) or int is a shortcut for before = after = statistic
3743             length for all axes.
3744             Default is ``None``, to use the entire axis.
3745         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
3746             Used in 'constant'.  The values to set the padded values for each
3747             axis.
3748             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
3749             pad constants along each dimension.
3750             ``((before, after),)`` yields same before and after constants for each
3751             dimension.
3752             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
3753             all dimensions.
3754             Default is 0.
3755         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
3756             Used in 'linear_ramp'.  The values used for the ending value of the
3757             linear_ramp and that will form the edge of the padded array.
3758             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
3759             end values along each dimension.
3760             ``((before, after),)`` yields same before and after end values for each
3761             axis.
3762             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
3763             all axes.
3764             Default is 0.
3765         reflect_type : {"even", "odd"}, optional
3766             Used in "reflect", and "symmetric".  The "even" style is the
3767             default with an unaltered reflection around the edge value.  For
3768             the "odd" style, the extended part of the array is created by
3769             subtracting the reflected values from two times the edge value.
3770         **pad_width_kwargs
3771             The keyword arguments form of ``pad_width``.
3772             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
3773 
3774         Returns
3775         -------
3776         padded : DataArray
3777             DataArray with the padded coordinates and data.
3778 
3779         See also
3780         --------
3781         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad
3782 
3783         Notes
3784         -----
3785         By default when ``mode="constant"`` and ``constant_values=None``, integer types will be
3786         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion
3787         specify ``constant_values=np.nan``
3788 
3789         Examples
3790         --------
3791 
3792         >>> arr = xr.DataArray([5, 6, 7], coords=[("x", [0, 1, 2])])
3793         >>> arr.pad(x=(1, 2), constant_values=0)
3794         <xarray.DataArray (x: 6)>
3795         array([0, 5, 6, 7, 0, 0])
3796         Coordinates:
3797           * x        (x) float64 nan 0.0 1.0 2.0 nan nan
3798 
3799         >>> da = xr.DataArray(
3800         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],
3801         ...     dims=["x", "y"],
3802         ...     coords={"x": [0, 1], "y": [10, 20, 30, 40], "z": ("x", [100, 200])},
3803         ... )
3804         >>> da.pad(x=1)
3805         <xarray.DataArray (x: 4, y: 4)>
3806         array([[nan, nan, nan, nan],
3807                [ 0.,  1.,  2.,  3.],
3808                [10., 11., 12., 13.],
3809                [nan, nan, nan, nan]])
3810         Coordinates:
3811           * x        (x) float64 nan 0.0 1.0 nan
3812           * y        (y) int64 10 20 30 40
3813             z        (x) float64 nan 100.0 200.0 nan
3814         >>> da.pad(x=1, constant_values=np.nan)
3815         <xarray.DataArray (x: 4, y: 4)>
3816         array([[-9223372036854775808, -9223372036854775808, -9223372036854775808,
3817                 -9223372036854775808],
3818                [                   0,                    1,                    2,
3819                                    3],
3820                [                  10,                   11,                   12,
3821                                   13],
3822                [-9223372036854775808, -9223372036854775808, -9223372036854775808,
3823                 -9223372036854775808]])
3824         Coordinates:
3825           * x        (x) float64 nan 0.0 1.0 nan
3826           * y        (y) int64 10 20 30 40
3827             z        (x) float64 nan 100.0 200.0 nan
3828         """
3829         ds = self._to_temp_dataset().pad(
3830             pad_width=pad_width,
3831             mode=mode,
3832             stat_length=stat_length,
3833             constant_values=constant_values,
3834             end_values=end_values,
3835             reflect_type=reflect_type,
3836             **pad_width_kwargs,
3837         )
3838         return self._from_temp_dataset(ds)
3839 
3840     def idxmin(
3841         self,
3842         dim: Hashable = None,
3843         skipna: bool = None,
3844         fill_value: Any = dtypes.NA,
3845         keep_attrs: bool = None,
3846     ) -> "DataArray":
3847         """Return the coordinate label of the minimum value along a dimension.
3848 
3849         Returns a new `DataArray` named after the dimension with the values of
3850         the coordinate labels along that dimension corresponding to minimum
3851         values along that dimension.
3852 
3853         In comparison to :py:meth:`~DataArray.argmin`, this returns the
3854         coordinate label while :py:meth:`~DataArray.argmin` returns the index.
3855 
3856         Parameters
3857         ----------
3858         dim : str, optional
3859             Dimension over which to apply `idxmin`.  This is optional for 1D
3860             arrays, but required for arrays with 2 or more dimensions.
3861         skipna : bool or None, default: None
3862             If True, skip missing values (as marked by NaN). By default, only
3863             skips missing values for ``float``, ``complex``, and ``object``
3864             dtypes; other dtypes either do not have a sentinel missing value
3865             (``int``) or ``skipna=True`` has not been implemented
3866             (``datetime64`` or ``timedelta64``).
3867         fill_value : Any, default: NaN
3868             Value to be filled in case all of the values along a dimension are
3869             null.  By default this is NaN.  The fill value and result are
3870             automatically converted to a compatible dtype if possible.
3871             Ignored if ``skipna`` is False.
3872         keep_attrs : bool, default: False
3873             If True, the attributes (``attrs``) will be copied from the
3874             original object to the new one.  If False (default), the new object
3875             will be returned without attributes.
3876 
3877         Returns
3878         -------
3879         reduced : DataArray
3880             New `DataArray` object with `idxmin` applied to its data and the
3881             indicated dimension removed.
3882 
3883         See also
3884         --------
3885         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin
3886 
3887         Examples
3888         --------
3889 
3890         >>> array = xr.DataArray(
3891         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
3892         ... )
3893         >>> array.min()
3894         <xarray.DataArray ()>
3895         array(-2)
3896         >>> array.argmin()
3897         <xarray.DataArray ()>
3898         array(4)
3899         >>> array.idxmin()
3900         <xarray.DataArray 'x' ()>
3901         array('e', dtype='<U1')
3902 
3903         >>> array = xr.DataArray(
3904         ...     [
3905         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
3906         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
3907         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
3908         ...     ],
3909         ...     dims=["y", "x"],
3910         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
3911         ... )
3912         >>> array.min(dim="x")
3913         <xarray.DataArray (y: 3)>
3914         array([-2., -4.,  1.])
3915         Coordinates:
3916           * y        (y) int64 -1 0 1
3917         >>> array.argmin(dim="x")
3918         <xarray.DataArray (y: 3)>
3919         array([4, 0, 2])
3920         Coordinates:
3921           * y        (y) int64 -1 0 1
3922         >>> array.idxmin(dim="x")
3923         <xarray.DataArray 'x' (y: 3)>
3924         array([16.,  0.,  4.])
3925         Coordinates:
3926           * y        (y) int64 -1 0 1
3927         """
3928         return computation._calc_idxminmax(
3929             array=self,
3930             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),
3931             dim=dim,
3932             skipna=skipna,
3933             fill_value=fill_value,
3934             keep_attrs=keep_attrs,
3935         )
3936 
3937     def idxmax(
3938         self,
3939         dim: Hashable = None,
3940         skipna: bool = None,
3941         fill_value: Any = dtypes.NA,
3942         keep_attrs: bool = None,
3943     ) -> "DataArray":
3944         """Return the coordinate label of the maximum value along a dimension.
3945 
3946         Returns a new `DataArray` named after the dimension with the values of
3947         the coordinate labels along that dimension corresponding to maximum
3948         values along that dimension.
3949 
3950         In comparison to :py:meth:`~DataArray.argmax`, this returns the
3951         coordinate label while :py:meth:`~DataArray.argmax` returns the index.
3952 
3953         Parameters
3954         ----------
3955         dim : hashable, optional
3956             Dimension over which to apply `idxmax`.  This is optional for 1D
3957             arrays, but required for arrays with 2 or more dimensions.
3958         skipna : bool or None, default: None
3959             If True, skip missing values (as marked by NaN). By default, only
3960             skips missing values for ``float``, ``complex``, and ``object``
3961             dtypes; other dtypes either do not have a sentinel missing value
3962             (``int``) or ``skipna=True`` has not been implemented
3963             (``datetime64`` or ``timedelta64``).
3964         fill_value : Any, default: NaN
3965             Value to be filled in case all of the values along a dimension are
3966             null.  By default this is NaN.  The fill value and result are
3967             automatically converted to a compatible dtype if possible.
3968             Ignored if ``skipna`` is False.
3969         keep_attrs : bool, default: False
3970             If True, the attributes (``attrs``) will be copied from the
3971             original object to the new one.  If False (default), the new object
3972             will be returned without attributes.
3973 
3974         Returns
3975         -------
3976         reduced : DataArray
3977             New `DataArray` object with `idxmax` applied to its data and the
3978             indicated dimension removed.
3979 
3980         See also
3981         --------
3982         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax
3983 
3984         Examples
3985         --------
3986 
3987         >>> array = xr.DataArray(
3988         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
3989         ... )
3990         >>> array.max()
3991         <xarray.DataArray ()>
3992         array(2)
3993         >>> array.argmax()
3994         <xarray.DataArray ()>
3995         array(1)
3996         >>> array.idxmax()
3997         <xarray.DataArray 'x' ()>
3998         array('b', dtype='<U1')
3999 
4000         >>> array = xr.DataArray(
4001         ...     [
4002         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
4003         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
4004         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
4005         ...     ],
4006         ...     dims=["y", "x"],
4007         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
4008         ... )
4009         >>> array.max(dim="x")
4010         <xarray.DataArray (y: 3)>
4011         array([2., 2., 1.])
4012         Coordinates:
4013           * y        (y) int64 -1 0 1
4014         >>> array.argmax(dim="x")
4015         <xarray.DataArray (y: 3)>
4016         array([0, 2, 2])
4017         Coordinates:
4018           * y        (y) int64 -1 0 1
4019         >>> array.idxmax(dim="x")
4020         <xarray.DataArray 'x' (y: 3)>
4021         array([0., 4., 4.])
4022         Coordinates:
4023           * y        (y) int64 -1 0 1
4024         """
4025         return computation._calc_idxminmax(
4026             array=self,
4027             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),
4028             dim=dim,
4029             skipna=skipna,
4030             fill_value=fill_value,
4031             keep_attrs=keep_attrs,
4032         )
4033 
4034     def argmin(
4035         self,
4036         dim: Union[Hashable, Sequence[Hashable]] = None,
4037         axis: int = None,
4038         keep_attrs: bool = None,
4039         skipna: bool = None,
4040     ) -> Union["DataArray", Dict[Hashable, "DataArray"]]:
4041         """Index or indices of the minimum of the DataArray over one or more dimensions.
4042 
4043         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4044         which can be passed directly to isel(). If a single str is passed to 'dim' then
4045         returns a DataArray with dtype int.
4046 
4047         If there are multiple minima, the indices of the first one found will be
4048         returned.
4049 
4050         Parameters
4051         ----------
4052         dim : hashable, sequence of hashable or ..., optional
4053             The dimensions over which to find the minimum. By default, finds minimum over
4054             all dimensions - for now returning an int for backward compatibility, but
4055             this is deprecated, in future will return a dict with indices for all
4056             dimensions; to return a dict with all dimensions now, pass '...'.
4057         axis : int, optional
4058             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
4059             can be supplied.
4060         keep_attrs : bool, optional
4061             If True, the attributes (`attrs`) will be copied from the original
4062             object to the new one.  If False (default), the new object will be
4063             returned without attributes.
4064         skipna : bool, optional
4065             If True, skip missing values (as marked by NaN). By default, only
4066             skips missing values for float dtypes; other dtypes either do not
4067             have a sentinel missing value (int) or skipna=True has not been
4068             implemented (object, datetime64 or timedelta64).
4069 
4070         Returns
4071         -------
4072         result : DataArray or dict of DataArray
4073 
4074         See also
4075         --------
4076         Variable.argmin, DataArray.idxmin
4077 
4078         Examples
4079         --------
4080         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4081         >>> array.min()
4082         <xarray.DataArray ()>
4083         array(-1)
4084         >>> array.argmin()
4085         <xarray.DataArray ()>
4086         array(2)
4087         >>> array.argmin(...)
4088         {'x': <xarray.DataArray ()>
4089         array(2)}
4090         >>> array.isel(array.argmin(...))
4091         <xarray.DataArray ()>
4092         array(-1)
4093 
4094         >>> array = xr.DataArray(
4095         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],
4096         ...     dims=("x", "y", "z"),
4097         ... )
4098         >>> array.min(dim="x")
4099         <xarray.DataArray (y: 3, z: 3)>
4100         array([[ 1,  2,  1],
4101                [ 2, -5,  1],
4102                [ 2,  1,  1]])
4103         Dimensions without coordinates: y, z
4104         >>> array.argmin(dim="x")
4105         <xarray.DataArray (y: 3, z: 3)>
4106         array([[1, 0, 0],
4107                [1, 1, 1],
4108                [0, 0, 1]])
4109         Dimensions without coordinates: y, z
4110         >>> array.argmin(dim=["x"])
4111         {'x': <xarray.DataArray (y: 3, z: 3)>
4112         array([[1, 0, 0],
4113                [1, 1, 1],
4114                [0, 0, 1]])
4115         Dimensions without coordinates: y, z}
4116         >>> array.min(dim=("x", "z"))
4117         <xarray.DataArray (y: 3)>
4118         array([ 1, -5,  1])
4119         Dimensions without coordinates: y
4120         >>> array.argmin(dim=["x", "z"])
4121         {'x': <xarray.DataArray (y: 3)>
4122         array([0, 1, 0])
4123         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4124         array([2, 1, 1])
4125         Dimensions without coordinates: y}
4126         >>> array.isel(array.argmin(dim=["x", "z"]))
4127         <xarray.DataArray (y: 3)>
4128         array([ 1, -5,  1])
4129         Dimensions without coordinates: y
4130         """
4131         result = self.variable.argmin(dim, axis, keep_attrs, skipna)
4132         if isinstance(result, dict):
4133             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4134         else:
4135             return self._replace_maybe_drop_dims(result)
4136 
4137     def argmax(
4138         self,
4139         dim: Union[Hashable, Sequence[Hashable]] = None,
4140         axis: int = None,
4141         keep_attrs: bool = None,
4142         skipna: bool = None,
4143     ) -> Union["DataArray", Dict[Hashable, "DataArray"]]:
4144         """Index or indices of the maximum of the DataArray over one or more dimensions.
4145 
4146         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4147         which can be passed directly to isel(). If a single str is passed to 'dim' then
4148         returns a DataArray with dtype int.
4149 
4150         If there are multiple maxima, the indices of the first one found will be
4151         returned.
4152 
4153         Parameters
4154         ----------
4155         dim : hashable, sequence of hashable or ..., optional
4156             The dimensions over which to find the maximum. By default, finds maximum over
4157             all dimensions - for now returning an int for backward compatibility, but
4158             this is deprecated, in future will return a dict with indices for all
4159             dimensions; to return a dict with all dimensions now, pass '...'.
4160         axis : int, optional
4161             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
4162             can be supplied.
4163         keep_attrs : bool, optional
4164             If True, the attributes (`attrs`) will be copied from the original
4165             object to the new one.  If False (default), the new object will be
4166             returned without attributes.
4167         skipna : bool, optional
4168             If True, skip missing values (as marked by NaN). By default, only
4169             skips missing values for float dtypes; other dtypes either do not
4170             have a sentinel missing value (int) or skipna=True has not been
4171             implemented (object, datetime64 or timedelta64).
4172 
4173         Returns
4174         -------
4175         result : DataArray or dict of DataArray
4176 
4177         See also
4178         --------
4179         Variable.argmax, DataArray.idxmax
4180 
4181         Examples
4182         --------
4183         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4184         >>> array.max()
4185         <xarray.DataArray ()>
4186         array(3)
4187         >>> array.argmax()
4188         <xarray.DataArray ()>
4189         array(3)
4190         >>> array.argmax(...)
4191         {'x': <xarray.DataArray ()>
4192         array(3)}
4193         >>> array.isel(array.argmax(...))
4194         <xarray.DataArray ()>
4195         array(3)
4196 
4197         >>> array = xr.DataArray(
4198         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],
4199         ...     dims=("x", "y", "z"),
4200         ... )
4201         >>> array.max(dim="x")
4202         <xarray.DataArray (y: 3, z: 3)>
4203         array([[3, 3, 2],
4204                [3, 5, 2],
4205                [2, 3, 3]])
4206         Dimensions without coordinates: y, z
4207         >>> array.argmax(dim="x")
4208         <xarray.DataArray (y: 3, z: 3)>
4209         array([[0, 1, 1],
4210                [0, 1, 0],
4211                [0, 1, 0]])
4212         Dimensions without coordinates: y, z
4213         >>> array.argmax(dim=["x"])
4214         {'x': <xarray.DataArray (y: 3, z: 3)>
4215         array([[0, 1, 1],
4216                [0, 1, 0],
4217                [0, 1, 0]])
4218         Dimensions without coordinates: y, z}
4219         >>> array.max(dim=("x", "z"))
4220         <xarray.DataArray (y: 3)>
4221         array([3, 5, 3])
4222         Dimensions without coordinates: y
4223         >>> array.argmax(dim=["x", "z"])
4224         {'x': <xarray.DataArray (y: 3)>
4225         array([0, 1, 0])
4226         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4227         array([0, 1, 2])
4228         Dimensions without coordinates: y}
4229         >>> array.isel(array.argmax(dim=["x", "z"]))
4230         <xarray.DataArray (y: 3)>
4231         array([3, 5, 3])
4232         Dimensions without coordinates: y
4233         """
4234         result = self.variable.argmax(dim, axis, keep_attrs, skipna)
4235         if isinstance(result, dict):
4236             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4237         else:
4238             return self._replace_maybe_drop_dims(result)
4239 
4240     # this needs to be at the end, or mypy will confuse with `str`
4241     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
4242     str = utils.UncachedAccessor(StringAccessor)

</code><intended_behavior>The `astype` method should allow for type conversion of the data contained within a `DataArray` instance, mirroring the functionality seen in NumPy arrays. This method should include the `order` parameter to control the memory layout of the result. The intended fix would involve reinstating this parameter if it was removed, or providing a compatible interface that re-enables control over the memory layout as part of type conversion, ensuring consistency with NumPy and preserving existing functionality.</intended_behavior>]
2025-10-18 15:27:33.270 | INFO     | Search completed. Bug locations: [<file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
216 class DataArray(AbstractArray, DataWithCoords):
217     """N-dimensional array with labeled coordinates and dimensions.
218 
219     DataArray provides a wrapper around numpy ndarrays that uses
220     labeled dimensions and coordinates to support metadata aware
221     operations. The API is similar to that for the pandas Series or
222     DataFrame, but DataArray objects can have any number of dimensions,
223     and their contents have fixed data types.
224 
225     Additional features over raw numpy arrays:
226 
227     - Apply operations over dimensions by name: ``x.sum('time')``.
228     - Select or assign values by integer location (like numpy):
229       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or
230       ``x.sel(time='2014-01-01')``.
231     - Mathematical operations (e.g., ``x - y``) vectorize across
232       multiple dimensions (known in numpy as "broadcasting") based on
233       dimension names, regardless of their original order.
234     - Keep track of arbitrary metadata in the form of a Python
235       dictionary: ``x.attrs``
236     - Convert to a pandas Series: ``x.to_series()``.
237 
238     Getting items from or doing mathematical operations with a
239     DataArray always returns another DataArray.
240 
241     Parameters
242     ----------
243     data : array_like
244         Values for this array. Must be an ``numpy.ndarray``, ndarray
245         like, or castable to an ``ndarray``. If a self-described xarray
246         or pandas object, attempts are made to use this array's
247         metadata to fill in other unspecified arguments. A view of the
248         array's data is used instead of a copy if possible.
249     coords : sequence or dict of array_like, optional
250         Coordinates (tick labels) to use for indexing along each
251         dimension. The following notations are accepted:
252 
253         - mapping {dimension name: array-like}
254         - sequence of tuples that are valid arguments for
255           ``xarray.Variable()``
256           - (dims, data)
257           - (dims, data, attrs)
258           - (dims, data, attrs, encoding)
259 
260         Additionally, it is possible to define a coord whose name
261         does not match the dimension name, or a coord based on multiple
262         dimensions, with one of the following notations:
263 
264         - mapping {coord name: DataArray}
265         - mapping {coord name: Variable}
266         - mapping {coord name: (dimension name, array-like)}
267         - mapping {coord name: (tuple of dimension names, array-like)}
268 
269     dims : hashable or sequence of hashable, optional
270         Name(s) of the data dimension(s). Must be either a hashable
271         (only for 1D data) or a sequence of hashables with length equal
272         to the number of dimensions. If this argument is omitted,
273         dimension names default to ``['dim_0', ... 'dim_n']``.
274     name : str or None, optional
275         Name of this array.
276     attrs : dict_like or None, optional
277         Attributes to assign to the new instance. By default, an empty
278         attribute dictionary is initialized.
279 
280     Examples
281     --------
282     Create data:
283 
284     >>> np.random.seed(0)
285     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
286     >>> precipitation = 10 * np.random.rand(2, 2, 3)
287     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
288     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
289     >>> time = pd.date_range("2014-09-06", periods=3)
290     >>> reference_time = pd.Timestamp("2014-09-05")
291 
292     Initialize a dataarray with multiple dimensions:
293 
294     >>> da = xr.DataArray(
295     ...     data=temperature,
296     ...     dims=["x", "y", "time"],
297     ...     coords=dict(
298     ...         lon=(["x", "y"], lon),
299     ...         lat=(["x", "y"], lat),
300     ...         time=time,
301     ...         reference_time=reference_time,
302     ...     ),
303     ...     attrs=dict(
304     ...         description="Ambient temperature.",
305     ...         units="degC",
306     ...     ),
307     ... )
308     >>> da
309     <xarray.DataArray (x: 2, y: 2, time: 3)>
310     array([[[29.11241877, 18.20125767, 22.82990387],
311             [32.92714559, 29.94046392,  7.18177696]],
312     <BLANKLINE>
313            [[22.60070734, 13.78914233, 14.17424919],
314             [18.28478802, 16.15234857, 26.63418806]]])
315     Coordinates:
316         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
317         lat             (x, y) float64 42.25 42.21 42.63 42.59
318       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
319         reference_time  datetime64[ns] 2014-09-05
320     Dimensions without coordinates: x, y
321     Attributes:
322         description:  Ambient temperature.
323         units:        degC
324 
325     Find out where the coldest temperature was:
326 
327     >>> da.isel(da.argmin(...))
328     <xarray.DataArray ()>
329     array(7.18177696)
330     Coordinates:
331         lon             float64 -99.32
332         lat             float64 42.21
333         time            datetime64[ns] 2014-09-08
334         reference_time  datetime64[ns] 2014-09-05
335     Attributes:
336         description:  Ambient temperature.
337         units:        degC
338     """
339 
340     _cache: Dict[str, Any]
341     _coords: Dict[Any, Variable]
342     _indexes: Optional[Dict[Hashable, pd.Index]]
343     _name: Optional[Hashable]
344     _variable: Variable
345 
346     __slots__ = (
347         "_cache",
348         "_coords",
349         "_file_obj",
350         "_indexes",
351         "_name",
352         "_variable",
353         "__weakref__",
354     )
355 
356     _groupby_cls = groupby.DataArrayGroupBy
357     _rolling_cls = rolling.DataArrayRolling
358     _coarsen_cls = rolling.DataArrayCoarsen
359     _resample_cls = resample.DataArrayResample
360     _weighted_cls = weighted.DataArrayWeighted
361 
362     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor)
363 
364     def __init__(
365         self,
366         data: Any = dtypes.NA,
367         coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None] = None,
368         dims: Union[Hashable, Sequence[Hashable], None] = None,
369         name: Hashable = None,
370         attrs: Mapping = None,
371         # internal parameters
372         indexes: Dict[Hashable, pd.Index] = None,
373         fastpath: bool = False,
374     ):
375         if fastpath:
376             variable = data
377             assert dims is None
378             assert attrs is None
379         else:
380             # try to fill in arguments from data if they weren't supplied
381             if coords is None:
382 
383                 if isinstance(data, DataArray):
384                     coords = data.coords
385                 elif isinstance(data, pd.Series):
386                     coords = [data.index]
387                 elif isinstance(data, pd.DataFrame):
388                     coords = [data.index, data.columns]
389                 elif isinstance(data, (pd.Index, IndexVariable)):
390                     coords = [data]
391                 elif isinstance(data, pdcompat.Panel):
392                     coords = [data.items, data.major_axis, data.minor_axis]
393 
394             if dims is None:
395                 dims = getattr(data, "dims", getattr(coords, "dims", None))
396             if name is None:
397                 name = getattr(data, "name", None)
398             if attrs is None and not isinstance(data, PANDAS_TYPES):
399                 attrs = getattr(data, "attrs", None)
400 
401             data = _check_data_shape(data, coords, dims)
402             data = as_compatible_data(data)
403             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
404             variable = Variable(dims, data, attrs, fastpath=True)
405             indexes = dict(
406                 _extract_indexes_from_coords(coords)
407             )  # needed for to_dataset
408 
409         # These fully describe a DataArray
410         self._variable = variable
411         assert isinstance(coords, dict)
412         self._coords = coords
413         self._name = name
414 
415         # TODO(shoyer): document this argument, once it becomes part of the
416         # public interface.
417         self._indexes = indexes
418 
419         self._file_obj = None
420 
421     def _replace(
422         self,
423         variable: Variable = None,
424         coords=None,
425         name: Union[Hashable, None, Default] = _default,
426         indexes=None,
427     ) -> "DataArray":
428         if variable is None:
429             variable = self.variable
430         if coords is None:
431             coords = self._coords
432         if name is _default:
433             name = self.name
434         return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)
435 
436     def _replace_maybe_drop_dims(
437         self, variable: Variable, name: Union[Hashable, None, Default] = _default
438     ) -> "DataArray":
439         if variable.dims == self.dims and variable.shape == self.shape:
440             coords = self._coords.copy()
441             indexes = self._indexes
442         elif variable.dims == self.dims:
443             # Shape has changed (e.g. from reduce(..., keepdims=True)
444             new_sizes = dict(zip(self.dims, variable.shape))
445             coords = {
446                 k: v
447                 for k, v in self._coords.items()
448                 if v.shape == tuple(new_sizes[d] for d in v.dims)
449             }
450             changed_dims = [
451                 k for k in variable.dims if variable.sizes[k] != self.sizes[k]
452             ]
453             indexes = propagate_indexes(self._indexes, exclude=changed_dims)
454         else:
455             allowed_dims = set(variable.dims)
456             coords = {
457                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
458             }
459             indexes = propagate_indexes(
460                 self._indexes, exclude=(set(self.dims) - allowed_dims)
461             )
462         return self._replace(variable, coords, name, indexes=indexes)
463 
464     def _overwrite_indexes(self, indexes: Mapping[Hashable, Any]) -> "DataArray":
465         if not len(indexes):
466             return self
467         coords = self._coords.copy()
468         for name, idx in indexes.items():
469             coords[name] = IndexVariable(name, idx)
470         obj = self._replace(coords=coords)
471 
472         # switch from dimension to level names, if necessary
473         dim_names: Dict[Any, str] = {}
474         for dim, idx in indexes.items():
475             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:
476                 dim_names[dim] = idx.name
477         if dim_names:
478             obj = obj.rename(dim_names)
479         return obj
480 
481     def _to_temp_dataset(self) -> Dataset:
482         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
483 
484     def _from_temp_dataset(
485         self, dataset: Dataset, name: Union[Hashable, None, Default] = _default
486     ) -> "DataArray":
487         variable = dataset._variables.pop(_THIS_ARRAY)
488         coords = dataset._variables
489         indexes = dataset._indexes
490         return self._replace(variable, coords, name, indexes=indexes)
491 
492     def _to_dataset_split(self, dim: Hashable) -> Dataset:
493         """ splits dataarray along dimension 'dim' """
494 
495         def subset(dim, label):
496             array = self.loc[{dim: label}]
497             array.attrs = {}
498             return as_variable(array)
499 
500         variables = {label: subset(dim, label) for label in self.get_index(dim)}
501         variables.update({k: v for k, v in self._coords.items() if k != dim})
502         indexes = propagate_indexes(self._indexes, exclude=dim)
503         coord_names = set(self._coords) - {dim}
504         dataset = Dataset._construct_direct(
505             variables, coord_names, indexes=indexes, attrs=self.attrs
506         )
507         return dataset
508 
509     def _to_dataset_whole(
510         self, name: Hashable = None, shallow_copy: bool = True
511     ) -> Dataset:
512         if name is None:
513             name = self.name
514         if name is None:
515             raise ValueError(
516                 "unable to convert unnamed DataArray to a "
517                 "Dataset without providing an explicit name"
518             )
519         if name in self.coords:
520             raise ValueError(
521                 "cannot create a Dataset from a DataArray with "
522                 "the same name as one of its coordinates"
523             )
524         # use private APIs for speed: this is called by _to_temp_dataset(),
525         # which is used in the guts of a lot of operations (e.g., reindex)
526         variables = self._coords.copy()
527         variables[name] = self.variable
528         if shallow_copy:
529             for k in variables:
530                 variables[k] = variables[k].copy(deep=False)
531         indexes = self._indexes
532 
533         coord_names = set(self._coords)
534         dataset = Dataset._construct_direct(variables, coord_names, indexes=indexes)
535         return dataset
536 
537     def to_dataset(
538         self,
539         dim: Hashable = None,
540         *,
541         name: Hashable = None,
542         promote_attrs: bool = False,
543     ) -> Dataset:
544         """Convert a DataArray to a Dataset.
545 
546         Parameters
547         ----------
548         dim : hashable, optional
549             Name of the dimension on this array along which to split this array
550             into separate variables. If not provided, this array is converted
551             into a Dataset of one variable.
552         name : hashable, optional
553             Name to substitute for this array's name. Only valid if ``dim`` is
554             not provided.
555         promote_attrs : bool, default: False
556             Set to True to shallow copy attrs of DataArray to returned Dataset.
557 
558         Returns
559         -------
560         dataset : Dataset
561         """
562         if dim is not None and dim not in self.dims:
563             raise TypeError(
564                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
565             )
566 
567         if dim is not None:
568             if name is not None:
569                 raise TypeError("cannot supply both dim and name arguments")
570             result = self._to_dataset_split(dim)
571         else:
572             result = self._to_dataset_whole(name)
573 
574         if promote_attrs:
575             result.attrs = dict(self.attrs)
576 
577         return result
578 
579     @property
580     def name(self) -> Optional[Hashable]:
581         """The name of this array."""
582         return self._name
583 
584     @name.setter
585     def name(self, value: Optional[Hashable]) -> None:
586         self._name = value
587 
588     @property
589     def variable(self) -> Variable:
590         """Low level interface to the Variable object for this DataArray."""
591         return self._variable
592 
593     @property
594     def dtype(self) -> np.dtype:
595         return self.variable.dtype
596 
597     @property
598     def shape(self) -> Tuple[int, ...]:
599         return self.variable.shape
600 
601     @property
602     def size(self) -> int:
603         return self.variable.size
604 
605     @property
606     def nbytes(self) -> int:
607         return self.variable.nbytes
608 
609     @property
610     def ndim(self) -> int:
611         return self.variable.ndim
612 
613     def __len__(self) -> int:
614         return len(self.variable)
615 
616     @property
617     def data(self) -> Any:
618         """The array's data as a dask or numpy array"""
619         return self.variable.data
620 
621     @data.setter
622     def data(self, value: Any) -> None:
623         self.variable.data = value
624 
625     @property
626     def values(self) -> np.ndarray:
627         """The array's data as a numpy.ndarray"""
628         return self.variable.values
629 
630     @values.setter
631     def values(self, value: Any) -> None:
632         self.variable.values = value
633 
634     @property
635     def _in_memory(self) -> bool:
636         return self.variable._in_memory
637 
638     def to_index(self) -> pd.Index:
639         """Convert this variable to a pandas.Index. Only possible for 1D
640         arrays.
641         """
642         return self.variable.to_index()
643 
644     @property
645     def dims(self) -> Tuple[Hashable, ...]:
646         """Tuple of dimension names associated with this array.
647 
648         Note that the type of this property is inconsistent with
649         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
650         consistently named properties.
651         """
652         return self.variable.dims
653 
654     @dims.setter
655     def dims(self, value):
656         raise AttributeError(
657             "you cannot assign dims on a DataArray. Use "
658             ".rename() or .swap_dims() instead."
659         )
660 
661     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
662         if utils.is_dict_like(key):
663             return key
664         else:
665             key = indexing.expanded_indexer(key, self.ndim)
666             return dict(zip(self.dims, key))
667 
668     @property
669     def _level_coords(self) -> Dict[Hashable, Hashable]:
670         """Return a mapping of all MultiIndex levels and their corresponding
671         coordinate name.
672         """
673         level_coords: Dict[Hashable, Hashable] = {}
674 
675         for cname, var in self._coords.items():
676             if var.ndim == 1 and isinstance(var, IndexVariable):
677                 level_names = var.level_names
678                 if level_names is not None:
679                     (dim,) = var.dims
680                     level_coords.update({lname: dim for lname in level_names})
681         return level_coords
682 
683     def _getitem_coord(self, key):
684         from .dataset import _get_virtual_variable
685 
686         try:
687             var = self._coords[key]
688         except KeyError:
689             dim_sizes = dict(zip(self.dims, self.shape))
690             _, key, var = _get_virtual_variable(
691                 self._coords, key, self._level_coords, dim_sizes
692             )
693 
694         return self._replace_maybe_drop_dims(var, name=key)
695 
696     def __getitem__(self, key: Any) -> "DataArray":
697         if isinstance(key, str):
698             return self._getitem_coord(key)
699         else:
700             # xarray-style array indexing
701             return self.isel(indexers=self._item_key_to_dict(key))
702 
703     def __setitem__(self, key: Any, value: Any) -> None:
704         if isinstance(key, str):
705             self.coords[key] = value
706         else:
707             # Coordinates in key, value and self[key] should be consistent.
708             # TODO Coordinate consistency in key is checked here, but it
709             # causes unnecessary indexing. It should be optimized.
710             obj = self[key]
711             if isinstance(value, DataArray):
712                 assert_coordinate_consistent(value, obj.coords.variables)
713             # DataArray key -> Variable key
714             key = {
715                 k: v.variable if isinstance(v, DataArray) else v
716                 for k, v in self._item_key_to_dict(key).items()
717             }
718             self.variable[key] = value
719 
720     def __delitem__(self, key: Any) -> None:
721         del self.coords[key]
722 
723     @property
724     def _attr_sources(self) -> List[Mapping[Hashable, Any]]:
725         """List of places to look-up items for attribute-style access"""
726         return self._item_sources + [self.attrs]
727 
728     @property
729     def _item_sources(self) -> List[Mapping[Hashable, Any]]:
730         """List of places to look-up items for key-completion"""
731         return [
732             self.coords,
733             {d: self.coords[d] for d in self.dims},
734             LevelCoordinatesSource(self),
735         ]
736 
737     def __contains__(self, key: Any) -> bool:
738         return key in self.data
739 
740     @property
741     def loc(self) -> _LocIndexer:
742         """Attribute for location based indexing like pandas."""
743         return _LocIndexer(self)
744 
745     @property
746     def attrs(self) -> Dict[Hashable, Any]:
747         """Dictionary storing arbitrary metadata with this array."""
748         return self.variable.attrs
749 
750     @attrs.setter
751     def attrs(self, value: Mapping[Hashable, Any]) -> None:
752         # Disable type checking to work around mypy bug - see mypy#4167
753         self.variable.attrs = value  # type: ignore
754 
755     @property
756     def encoding(self) -> Dict[Hashable, Any]:
757         """Dictionary of format-specific settings for how this array should be
758         serialized."""
759         return self.variable.encoding
760 
761     @encoding.setter
762     def encoding(self, value: Mapping[Hashable, Any]) -> None:
763         self.variable.encoding = value
764 
765     @property
766     def indexes(self) -> Indexes:
767         """Mapping of pandas.Index objects used for label based indexing"""
768         if self._indexes is None:
769             self._indexes = default_indexes(self._coords, self.dims)
770         return Indexes(self._indexes)
771 
772     @property
773     def coords(self) -> DataArrayCoordinates:
774         """Dictionary-like container of coordinate arrays."""
775         return DataArrayCoordinates(self)
776 
777     def reset_coords(
778         self,
779         names: Union[Iterable[Hashable], Hashable, None] = None,
780         drop: bool = False,
781     ) -> Union[None, "DataArray", Dataset]:
782         """Given names of coordinates, reset them to become variables.
783 
784         Parameters
785         ----------
786         names : hashable or iterable of hashable, optional
787             Name(s) of non-index coordinates in this dataset to reset into
788             variables. By default, all non-index coordinates are reset.
789         drop : bool, optional
790             If True, remove coordinates instead of converting them into
791             variables.
792 
793         Returns
794         -------
795         Dataset, or DataArray if ``drop == True``
796         """
797         if names is None:
798             names = set(self.coords) - set(self.dims)
799         dataset = self.coords.to_dataset().reset_coords(names, drop)
800         if drop:
801             return self._replace(coords=dataset._variables)
802         else:
803             if self.name is None:
804                 raise ValueError(
805                     "cannot reset_coords with drop=False on an unnamed DataArrray"
806                 )
807             dataset[self.name] = self.variable
808             return dataset
809 
810     def __dask_tokenize__(self):
811         from dask.base import normalize_token
812 
813         return normalize_token((type(self), self._variable, self._coords, self._name))
814 
815     def __dask_graph__(self):
816         return self._to_temp_dataset().__dask_graph__()
817 
818     def __dask_keys__(self):
819         return self._to_temp_dataset().__dask_keys__()
820 
821     def __dask_layers__(self):
822         return self._to_temp_dataset().__dask_layers__()
823 
824     @property
825     def __dask_optimize__(self):
826         return self._to_temp_dataset().__dask_optimize__
827 
828     @property
829     def __dask_scheduler__(self):
830         return self._to_temp_dataset().__dask_scheduler__
831 
832     def __dask_postcompute__(self):
833         func, args = self._to_temp_dataset().__dask_postcompute__()
834         return self._dask_finalize, (func, args, self.name)
835 
836     def __dask_postpersist__(self):
837         func, args = self._to_temp_dataset().__dask_postpersist__()
838         return self._dask_finalize, (func, args, self.name)
839 
840     @staticmethod
841     def _dask_finalize(results, func, args, name):
842         ds = func(results, *args)
843         variable = ds._variables.pop(_THIS_ARRAY)
844         coords = ds._variables
845         return DataArray(variable, coords, name=name, fastpath=True)
846 
847     def load(self, **kwargs) -> "DataArray":
848         """Manually trigger loading of this array's data from disk or a
849         remote source into memory and return this array.
850 
851         Normally, it should not be necessary to call this method in user code,
852         because all xarray functions should either work on deferred data or
853         load data automatically. However, this method can be necessary when
854         working with many file objects on disk.
855 
856         Parameters
857         ----------
858         **kwargs : dict
859             Additional keyword arguments passed on to ``dask.array.compute``.
860 
861         See Also
862         --------
863         dask.array.compute
864         """
865         ds = self._to_temp_dataset().load(**kwargs)
866         new = self._from_temp_dataset(ds)
867         self._variable = new._variable
868         self._coords = new._coords
869         return self
870 
871     def compute(self, **kwargs) -> "DataArray":
872         """Manually trigger loading of this array's data from disk or a
873         remote source into memory and return a new array. The original is
874         left unaltered.
875 
876         Normally, it should not be necessary to call this method in user code,
877         because all xarray functions should either work on deferred data or
878         load data automatically. However, this method can be necessary when
879         working with many file objects on disk.
880 
881         Parameters
882         ----------
883         **kwargs : dict
884             Additional keyword arguments passed on to ``dask.array.compute``.
885 
886         See Also
887         --------
888         dask.array.compute
889         """
890         new = self.copy(deep=False)
891         return new.load(**kwargs)
892 
893     def persist(self, **kwargs) -> "DataArray":
894         """Trigger computation in constituent dask arrays
895 
896         This keeps them as dask arrays but encourages them to keep data in
897         memory.  This is particularly useful when on a distributed machine.
898         When on a single machine consider using ``.compute()`` instead.
899 
900         Parameters
901         ----------
902         **kwargs : dict
903             Additional keyword arguments passed on to ``dask.persist``.
904 
905         See Also
906         --------
907         dask.persist
908         """
909         ds = self._to_temp_dataset().persist(**kwargs)
910         return self._from_temp_dataset(ds)
911 
912     def copy(self, deep: bool = True, data: Any = None) -> "DataArray":
913         """Returns a copy of this array.
914 
915         If `deep=True`, a deep copy is made of the data array.
916         Otherwise, a shallow copy is made, and the returned data array's
917         values are a new view of this data array's values.
918 
919         Use `data` to create a new object with the same structure as
920         original but entirely new data.
921 
922         Parameters
923         ----------
924         deep : bool, optional
925             Whether the data array and its coordinates are loaded into memory
926             and copied onto the new object. Default is True.
927         data : array_like, optional
928             Data to use in the new object. Must have same shape as original.
929             When `data` is used, `deep` is ignored for all data variables,
930             and only used for coords.
931 
932         Returns
933         -------
934         object : DataArray
935             New object with dimensions, attributes, coordinates, name,
936             encoding, and optionally data copied from original.
937 
938         Examples
939         --------
940 
941         Shallow versus deep copy
942 
943         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
944         >>> array.copy()
945         <xarray.DataArray (x: 3)>
946         array([1, 2, 3])
947         Coordinates:
948           * x        (x) <U1 'a' 'b' 'c'
949         >>> array_0 = array.copy(deep=False)
950         >>> array_0[0] = 7
951         >>> array_0
952         <xarray.DataArray (x: 3)>
953         array([7, 2, 3])
954         Coordinates:
955           * x        (x) <U1 'a' 'b' 'c'
956         >>> array
957         <xarray.DataArray (x: 3)>
958         array([7, 2, 3])
959         Coordinates:
960           * x        (x) <U1 'a' 'b' 'c'
961 
962         Changing the data using the ``data`` argument maintains the
963         structure of the original object, but with the new data. Original
964         object is unaffected.
965 
966         >>> array.copy(data=[0.1, 0.2, 0.3])
967         <xarray.DataArray (x: 3)>
968         array([0.1, 0.2, 0.3])
969         Coordinates:
970           * x        (x) <U1 'a' 'b' 'c'
971         >>> array
972         <xarray.DataArray (x: 3)>
973         array([7, 2, 3])
974         Coordinates:
975           * x        (x) <U1 'a' 'b' 'c'
976 
977         See Also
978         --------
979         pandas.DataFrame.copy
980         """
981         variable = self.variable.copy(deep=deep, data=data)
982         coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}
983         if self._indexes is None:
984             indexes = self._indexes
985         else:
986             indexes = {k: v.copy(deep=deep) for k, v in self._indexes.items()}
987         return self._replace(variable, coords, indexes=indexes)
988 
989     def __copy__(self) -> "DataArray":
990         return self.copy(deep=False)
991 
992     def __deepcopy__(self, memo=None) -> "DataArray":
993         # memo does nothing but is required for compatibility with
994         # copy.deepcopy
995         return self.copy(deep=True)
996 
997     # mutable objects should not be hashable
998     # https://github.com/python/mypy/issues/4266
999     __hash__ = None  # type: ignore
1000 
1001     @property
1002     def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:
1003         """Block dimensions for this array's data or None if it's not a dask
1004         array.
1005         """
1006         return self.variable.chunks
1007 
1008     def chunk(
1009         self,
1010         chunks: Union[
1011             Number,
1012             Tuple[Number, ...],
1013             Tuple[Tuple[Number, ...], ...],
1014             Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]],
1015         ] = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1016         name_prefix: str = "xarray-",
1017         token: str = None,
1018         lock: bool = False,
1019     ) -> "DataArray":
1020         """Coerce this array's data into a dask arrays with the given chunks.
1021 
1022         If this variable is a non-dask array, it will be converted to dask
1023         array. If it's a dask array, it will be rechunked to the given chunk
1024         sizes.
1025 
1026         If neither chunks is not provided for one or more dimensions, chunk
1027         sizes along that dimension will not be updated; non-dask arrays will be
1028         converted into dask arrays with a single block.
1029 
1030         Parameters
1031         ----------
1032         chunks : int, tuple of int or mapping of hashable to int, optional
1033             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
1034             ``{'x': 5, 'y': 5}``.
1035         name_prefix : str, optional
1036             Prefix for the name of the new dask array.
1037         token : str, optional
1038             Token uniquely identifying this array.
1039         lock : optional
1040             Passed on to :py:func:`dask.array.from_array`, if the array is not
1041             already as dask array.
1042 
1043         Returns
1044         -------
1045         chunked : xarray.DataArray
1046         """
1047         if isinstance(chunks, (tuple, list)):
1048             chunks = dict(zip(self.dims, chunks))
1049 
1050         ds = self._to_temp_dataset().chunk(
1051             chunks, name_prefix=name_prefix, token=token, lock=lock
1052         )
1053         return self._from_temp_dataset(ds)
1054 
1055     def isel(
1056         self,
1057         indexers: Mapping[Hashable, Any] = None,
1058         drop: bool = False,
1059         missing_dims: str = "raise",
1060         **indexers_kwargs: Any,
1061     ) -> "DataArray":
1062         """Return a new DataArray whose data is given by integer indexing
1063         along the specified dimension(s).
1064 
1065         Parameters
1066         ----------
1067         indexers : dict, optional
1068             A dict with keys matching dimensions and values given
1069             by integers, slice objects or arrays.
1070             indexer can be a integer, slice, array-like or DataArray.
1071             If DataArrays are passed as indexers, xarray-style indexing will be
1072             carried out. See :ref:`indexing` for the details.
1073             One of indexers or indexers_kwargs must be provided.
1074         drop : bool, optional
1075             If ``drop=True``, drop coordinates variables indexed by integers
1076             instead of making them scalar.
1077         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1078             What to do if dimensions that should be selected from are not present in the
1079             DataArray:
1080             - "raise": raise an exception
1081             - "warning": raise a warning, and ignore the missing dimensions
1082             - "ignore": ignore the missing dimensions
1083         **indexers_kwargs : {dim: indexer, ...}, optional
1084             The keyword arguments form of ``indexers``.
1085 
1086         See Also
1087         --------
1088         Dataset.isel
1089         DataArray.sel
1090         """
1091 
1092         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1093 
1094         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1095             ds = self._to_temp_dataset()._isel_fancy(
1096                 indexers, drop=drop, missing_dims=missing_dims
1097             )
1098             return self._from_temp_dataset(ds)
1099 
1100         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1101         # lists, or zero or one-dimensional np.ndarray's
1102 
1103         variable = self._variable.isel(indexers, missing_dims=missing_dims)
1104 
1105         coords = {}
1106         for coord_name, coord_value in self._coords.items():
1107             coord_indexers = {
1108                 k: v for k, v in indexers.items() if k in coord_value.dims
1109             }
1110             if coord_indexers:
1111                 coord_value = coord_value.isel(coord_indexers)
1112                 if drop and coord_value.ndim == 0:
1113                     continue
1114             coords[coord_name] = coord_value
1115 
1116         return self._replace(variable=variable, coords=coords)
1117 
1118     def sel(
1119         self,
1120         indexers: Mapping[Hashable, Any] = None,
1121         method: str = None,
1122         tolerance=None,
1123         drop: bool = False,
1124         **indexers_kwargs: Any,
1125     ) -> "DataArray":
1126         """Return a new DataArray whose data is given by selecting index
1127         labels along the specified dimension(s).
1128 
1129         In contrast to `DataArray.isel`, indexers for this method should use
1130         labels instead of integers.
1131 
1132         Under the hood, this method is powered by using pandas's powerful Index
1133         objects. This makes label based indexing essentially just as fast as
1134         using integer indexing.
1135 
1136         It also means this method uses pandas's (well documented) logic for
1137         indexing. This means you can use string shortcuts for datetime indexes
1138         (e.g., '2000-01' to select all values in January 2000). It also means
1139         that slices are treated as inclusive of both the start and stop values,
1140         unlike normal Python indexing.
1141 
1142         .. warning::
1143 
1144           Do not try to assign values when using any of the indexing methods
1145           ``isel`` or ``sel``::
1146 
1147             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1148             # DO NOT do this
1149             da.isel(x=[0, 1, 2])[1] = -1
1150 
1151           Assigning values with the chained indexing using ``.sel`` or
1152           ``.isel`` fails silently.
1153 
1154         Parameters
1155         ----------
1156         indexers : dict, optional
1157             A dict with keys matching dimensions and values given
1158             by scalars, slices or arrays of tick labels. For dimensions with
1159             multi-index, the indexer may also be a dict-like object with keys
1160             matching index level names.
1161             If DataArrays are passed as indexers, xarray-style indexing will be
1162             carried out. See :ref:`indexing` for the details.
1163             One of indexers or indexers_kwargs must be provided.
1164         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1165             Method to use for inexact matches:
1166 
1167             * None (default): only exact matches
1168             * pad / ffill: propagate last valid index value forward
1169             * backfill / bfill: propagate next valid index value backward
1170             * nearest: use nearest valid index value
1171         tolerance : optional
1172             Maximum distance between original and new labels for inexact
1173             matches. The values of the index at the matching locations must
1174             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1175         drop : bool, optional
1176             If ``drop=True``, drop coordinates variables in `indexers` instead
1177             of making them scalar.
1178         **indexers_kwargs : {dim: indexer, ...}, optional
1179             The keyword arguments form of ``indexers``.
1180             One of indexers or indexers_kwargs must be provided.
1181 
1182         Returns
1183         -------
1184         obj : DataArray
1185             A new DataArray with the same contents as this DataArray, except the
1186             data and each dimension is indexed by the appropriate indexers.
1187             If indexer DataArrays have coordinates that do not conflict with
1188             this object, then these coordinates will be attached.
1189             In general, each array's data will be a view of the array's data
1190             in this DataArray, unless vectorized indexing was triggered by using
1191             an array indexer, in which case the data will be a copy.
1192 
1193         See Also
1194         --------
1195         Dataset.sel
1196         DataArray.isel
1197 
1198         """
1199         ds = self._to_temp_dataset().sel(
1200             indexers=indexers,
1201             drop=drop,
1202             method=method,
1203             tolerance=tolerance,
1204             **indexers_kwargs,
1205         )
1206         return self._from_temp_dataset(ds)
1207 
1208     def head(
1209         self,
1210         indexers: Union[Mapping[Hashable, int], int] = None,
1211         **indexers_kwargs: Any,
1212     ) -> "DataArray":
1213         """Return a new DataArray whose data is given by the the first `n`
1214         values along the specified dimension(s). Default `n` = 5
1215 
1216         See Also
1217         --------
1218         Dataset.head
1219         DataArray.tail
1220         DataArray.thin
1221         """
1222         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1223         return self._from_temp_dataset(ds)
1224 
1225     def tail(
1226         self,
1227         indexers: Union[Mapping[Hashable, int], int] = None,
1228         **indexers_kwargs: Any,
1229     ) -> "DataArray":
1230         """Return a new DataArray whose data is given by the the last `n`
1231         values along the specified dimension(s). Default `n` = 5
1232 
1233         See Also
1234         --------
1235         Dataset.tail
1236         DataArray.head
1237         DataArray.thin
1238         """
1239         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1240         return self._from_temp_dataset(ds)
1241 
1242     def thin(
1243         self,
1244         indexers: Union[Mapping[Hashable, int], int] = None,
1245         **indexers_kwargs: Any,
1246     ) -> "DataArray":
1247         """Return a new DataArray whose data is given by each `n` value
1248         along the specified dimension(s).
1249 
1250         See Also
1251         --------
1252         Dataset.thin
1253         DataArray.head
1254         DataArray.tail
1255         """
1256         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1257         return self._from_temp_dataset(ds)
1258 
1259     def broadcast_like(
1260         self, other: Union["DataArray", Dataset], exclude: Iterable[Hashable] = None
1261     ) -> "DataArray":
1262         """Broadcast this DataArray against another Dataset or DataArray.
1263 
1264         This is equivalent to xr.broadcast(other, self)[1]
1265 
1266         xarray objects are broadcast against each other in arithmetic
1267         operations, so this method is not be necessary for most uses.
1268 
1269         If no change is needed, the input data is returned to the output
1270         without being copied.
1271 
1272         If new coords are added by the broadcast, their values are
1273         NaN filled.
1274 
1275         Parameters
1276         ----------
1277         other : Dataset or DataArray
1278             Object against which to broadcast this array.
1279         exclude : iterable of hashable, optional
1280             Dimensions that must not be broadcasted
1281 
1282         Returns
1283         -------
1284         new_da : DataArray
1285             The caller broadcasted against ``other``.
1286 
1287         Examples
1288         --------
1289 
1290         >>> arr1 = xr.DataArray(
1291         ...     np.random.randn(2, 3),
1292         ...     dims=("x", "y"),
1293         ...     coords={"x": ["a", "b"], "y": ["a", "b", "c"]},
1294         ... )
1295         >>> arr2 = xr.DataArray(
1296         ...     np.random.randn(3, 2),
1297         ...     dims=("x", "y"),
1298         ...     coords={"x": ["a", "b", "c"], "y": ["a", "b"]},
1299         ... )
1300         >>> arr1
1301         <xarray.DataArray (x: 2, y: 3)>
1302         array([[ 1.76405235,  0.40015721,  0.97873798],
1303                [ 2.2408932 ,  1.86755799, -0.97727788]])
1304         Coordinates:
1305           * x        (x) <U1 'a' 'b'
1306           * y        (y) <U1 'a' 'b' 'c'
1307         >>> arr2
1308         <xarray.DataArray (x: 3, y: 2)>
1309         array([[ 0.95008842, -0.15135721],
1310                [-0.10321885,  0.4105985 ],
1311                [ 0.14404357,  1.45427351]])
1312         Coordinates:
1313           * x        (x) <U1 'a' 'b' 'c'
1314           * y        (y) <U1 'a' 'b'
1315         >>> arr1.broadcast_like(arr2)
1316         <xarray.DataArray (x: 3, y: 3)>
1317         array([[ 1.76405235,  0.40015721,  0.97873798],
1318                [ 2.2408932 ,  1.86755799, -0.97727788],
1319                [        nan,         nan,         nan]])
1320         Coordinates:
1321           * x        (x) object 'a' 'b' 'c'
1322           * y        (y) object 'a' 'b' 'c'
1323         """
1324         if exclude is None:
1325             exclude = set()
1326         else:
1327             exclude = set(exclude)
1328         args = align(other, self, join="outer", copy=False, exclude=exclude)
1329 
1330         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1331 
1332         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
1333 
1334     def reindex_like(
1335         self,
1336         other: Union["DataArray", Dataset],
1337         method: str = None,
1338         tolerance=None,
1339         copy: bool = True,
1340         fill_value=dtypes.NA,
1341     ) -> "DataArray":
1342         """Conform this object onto the indexes of another object, filling in
1343         missing values with ``fill_value``. The default fill value is NaN.
1344 
1345         Parameters
1346         ----------
1347         other : Dataset or DataArray
1348             Object with an 'indexes' attribute giving a mapping from dimension
1349             names to pandas.Index objects, which provides coordinates upon
1350             which to index the variables in this dataset. The indexes on this
1351             other object need not be the same as the indexes on this
1352             dataset. Any mis-matched index values will be filled in with
1353             NaN, and any mis-matched dimension names will simply be ignored.
1354         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1355             Method to use for filling index values from other not found on this
1356             data array:
1357 
1358             * None (default): don't fill gaps
1359             * pad / ffill: propagate last valid index value forward
1360             * backfill / bfill: propagate next valid index value backward
1361             * nearest: use nearest valid index value
1362         tolerance : optional
1363             Maximum distance between original and new labels for inexact
1364             matches. The values of the index at the matching locations must
1365             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1366         copy : bool, optional
1367             If ``copy=True``, data in the return value is always copied. If
1368             ``copy=False`` and reindexing is unnecessary, or can be performed
1369             with only slice operations, then the output may share memory with
1370             the input. In either case, a new xarray object is always returned.
1371         fill_value : scalar or dict-like, optional
1372             Value to use for newly missing values. If a dict-like, maps
1373             variable names (including coordinates) to fill values. Use this
1374             data array's name to refer to the data array's values.
1375 
1376         Returns
1377         -------
1378         reindexed : DataArray
1379             Another dataset array, with this array's data but coordinates from
1380             the other object.
1381 
1382         See Also
1383         --------
1384         DataArray.reindex
1385         align
1386         """
1387         indexers = reindex_like_indexers(self, other)
1388         return self.reindex(
1389             indexers=indexers,
1390             method=method,
1391             tolerance=tolerance,
1392             copy=copy,
1393             fill_value=fill_value,
1394         )
1395 
1396     def reindex(
1397         self,
1398         indexers: Mapping[Hashable, Any] = None,
1399         method: str = None,
1400         tolerance=None,
1401         copy: bool = True,
1402         fill_value=dtypes.NA,
1403         **indexers_kwargs: Any,
1404     ) -> "DataArray":
1405         """Conform this object onto the indexes of another object, filling in
1406         missing values with ``fill_value``. The default fill value is NaN.
1407 
1408         Parameters
1409         ----------
1410         indexers : dict, optional
1411             Dictionary with keys given by dimension names and values given by
1412             arrays of coordinates tick labels. Any mis-matched coordinate
1413             values will be filled in with NaN, and any mis-matched dimension
1414             names will simply be ignored.
1415             One of indexers or indexers_kwargs must be provided.
1416         copy : bool, optional
1417             If ``copy=True``, data in the return value is always copied. If
1418             ``copy=False`` and reindexing is unnecessary, or can be performed
1419             with only slice operations, then the output may share memory with
1420             the input. In either case, a new xarray object is always returned.
1421         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1422             Method to use for filling index values in ``indexers`` not found on
1423             this data array:
1424 
1425             * None (default): don't fill gaps
1426             * pad / ffill: propagate last valid index value forward
1427             * backfill / bfill: propagate next valid index value backward
1428             * nearest: use nearest valid index value
1429         tolerance : optional
1430             Maximum distance between original and new labels for inexact
1431             matches. The values of the index at the matching locations must
1432             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1433         fill_value : scalar or dict-like, optional
1434             Value to use for newly missing values. If a dict-like, maps
1435             variable names (including coordinates) to fill values. Use this
1436             data array's name to refer to the data array's values.
1437         **indexers_kwargs : {dim: indexer, ...}, optional
1438             The keyword arguments form of ``indexers``.
1439             One of indexers or indexers_kwargs must be provided.
1440 
1441         Returns
1442         -------
1443         reindexed : DataArray
1444             Another dataset array, with this array's data but replaced
1445             coordinates.
1446 
1447         See Also
1448         --------
1449         DataArray.reindex_like
1450         align
1451         """
1452         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
1453         if isinstance(fill_value, dict):
1454             fill_value = fill_value.copy()
1455             sentinel = object()
1456             value = fill_value.pop(self.name, sentinel)
1457             if value is not sentinel:
1458                 fill_value[_THIS_ARRAY] = value
1459 
1460         ds = self._to_temp_dataset().reindex(
1461             indexers=indexers,
1462             method=method,
1463             tolerance=tolerance,
1464             copy=copy,
1465             fill_value=fill_value,
1466         )
1467         return self._from_temp_dataset(ds)
1468 
1469     def interp(
1470         self,
1471         coords: Mapping[Hashable, Any] = None,
1472         method: str = "linear",
1473         assume_sorted: bool = False,
1474         kwargs: Mapping[str, Any] = None,
1475         **coords_kwargs: Any,
1476     ) -> "DataArray":
1477         """Multidimensional interpolation of variables.
1478 
1479         Parameters
1480         ----------
1481         coords : dict, optional
1482             Mapping from dimension names to the new coordinates.
1483             New coordinate can be an scalar, array-like or DataArray.
1484             If DataArrays are passed as new coordinates, their dimensions are
1485             used for the broadcasting. Missing values are skipped.
1486         method : str, default: "linear"
1487             The method used to interpolate. Choose from
1488 
1489             - {"linear", "nearest"} for multidimensional array,
1490             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"} for 1-dimensional array.
1491         assume_sorted : bool, optional
1492             If False, values of x can be in any order and they are sorted
1493             first. If True, x has to be an array of monotonically increasing
1494             values.
1495         kwargs : dict
1496             Additional keyword arguments passed to scipy's interpolator. Valid
1497             options and their behavior depend on if 1-dimensional or
1498             multi-dimensional interpolation is used.
1499         **coords_kwargs : {dim: coordinate, ...}, optional
1500             The keyword arguments form of ``coords``.
1501             One of coords or coords_kwargs must be provided.
1502 
1503         Returns
1504         -------
1505         interpolated : DataArray
1506             New dataarray on the new coordinates.
1507 
1508         Notes
1509         -----
1510         scipy is required.
1511 
1512         See Also
1513         --------
1514         scipy.interpolate.interp1d
1515         scipy.interpolate.interpn
1516 
1517         Examples
1518         --------
1519         >>> da = xr.DataArray(
1520         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
1521         ...     dims=("x", "y"),
1522         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
1523         ... )
1524         >>> da
1525         <xarray.DataArray (x: 3, y: 4)>
1526         array([[ 1.,  4.,  2.,  9.],
1527                [ 2.,  7.,  6., nan],
1528                [ 6., nan,  5.,  8.]])
1529         Coordinates:
1530           * x        (x) int64 0 1 2
1531           * y        (y) int64 10 12 14 16
1532 
1533         1D linear interpolation (the default):
1534 
1535         >>> da.interp(x=[0, 0.75, 1.25, 1.75])
1536         <xarray.DataArray (x: 4, y: 4)>
1537         array([[1.  , 4.  , 2.  ,  nan],
1538                [1.75, 6.25, 5.  ,  nan],
1539                [3.  ,  nan, 5.75,  nan],
1540                [5.  ,  nan, 5.25,  nan]])
1541         Coordinates:
1542           * y        (y) int64 10 12 14 16
1543           * x        (x) float64 0.0 0.75 1.25 1.75
1544 
1545         1D nearest interpolation:
1546 
1547         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
1548         <xarray.DataArray (x: 4, y: 4)>
1549         array([[ 1.,  4.,  2.,  9.],
1550                [ 2.,  7.,  6., nan],
1551                [ 2.,  7.,  6., nan],
1552                [ 6., nan,  5.,  8.]])
1553         Coordinates:
1554           * y        (y) int64 10 12 14 16
1555           * x        (x) float64 0.0 0.75 1.25 1.75
1556 
1557         1D linear extrapolation:
1558 
1559         >>> da.interp(
1560         ...     x=[1, 1.5, 2.5, 3.5],
1561         ...     method="linear",
1562         ...     kwargs={"fill_value": "extrapolate"},
1563         ... )
1564         <xarray.DataArray (x: 4, y: 4)>
1565         array([[ 2. ,  7. ,  6. ,  nan],
1566                [ 4. ,  nan,  5.5,  nan],
1567                [ 8. ,  nan,  4.5,  nan],
1568                [12. ,  nan,  3.5,  nan]])
1569         Coordinates:
1570           * y        (y) int64 10 12 14 16
1571           * x        (x) float64 1.0 1.5 2.5 3.5
1572 
1573         2D linear interpolation:
1574 
1575         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
1576         <xarray.DataArray (x: 4, y: 3)>
1577         array([[2.5  , 3.   ,   nan],
1578                [4.   , 5.625,   nan],
1579                [  nan,   nan,   nan],
1580                [  nan,   nan,   nan]])
1581         Coordinates:
1582           * x        (x) float64 0.0 0.75 1.25 1.75
1583           * y        (y) int64 11 13 15
1584         """
1585         if self.dtype.kind not in "uifc":
1586             raise TypeError(
1587                 "interp only works for a numeric type array. "
1588                 "Given {}.".format(self.dtype)
1589             )
1590         ds = self._to_temp_dataset().interp(
1591             coords,
1592             method=method,
1593             kwargs=kwargs,
1594             assume_sorted=assume_sorted,
1595             **coords_kwargs,
1596         )
1597         return self._from_temp_dataset(ds)
1598 
1599     def interp_like(
1600         self,
1601         other: Union["DataArray", Dataset],
1602         method: str = "linear",
1603         assume_sorted: bool = False,
1604         kwargs: Mapping[str, Any] = None,
1605     ) -> "DataArray":
1606         """Interpolate this object onto the coordinates of another object,
1607         filling out of range values with NaN.
1608 
1609         Parameters
1610         ----------
1611         other : Dataset or DataArray
1612             Object with an 'indexes' attribute giving a mapping from dimension
1613             names to an 1d array-like, which provides coordinates upon
1614             which to index the variables in this dataset. Missing values are skipped.
1615         method : str, default: "linear"
1616             The method used to interpolate. Choose from
1617 
1618             - {"linear", "nearest"} for multidimensional array,
1619             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"} for 1-dimensional array.
1620         assume_sorted : bool, optional
1621             If False, values of coordinates that are interpolated over can be
1622             in any order and they are sorted first. If True, interpolated
1623             coordinates are assumed to be an array of monotonically increasing
1624             values.
1625         kwargs : dict, optional
1626             Additional keyword passed to scipy's interpolator.
1627 
1628         Returns
1629         -------
1630         interpolated : DataArray
1631             Another dataarray by interpolating this dataarray's data along the
1632             coordinates of the other object.
1633 
1634         Notes
1635         -----
1636         scipy is required.
1637         If the dataarray has object-type coordinates, reindex is used for these
1638         coordinates instead of the interpolation.
1639 
1640         See Also
1641         --------
1642         DataArray.interp
1643         DataArray.reindex_like
1644         """
1645         if self.dtype.kind not in "uifc":
1646             raise TypeError(
1647                 "interp only works for a numeric type array. "
1648                 "Given {}.".format(self.dtype)
1649             )
1650         ds = self._to_temp_dataset().interp_like(
1651             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
1652         )
1653         return self._from_temp_dataset(ds)
1654 
1655     def rename(
1656         self,
1657         new_name_or_name_dict: Union[Hashable, Mapping[Hashable, Hashable]] = None,
1658         **names: Hashable,
1659     ) -> "DataArray":
1660         """Returns a new DataArray with renamed coordinates or a new name.
1661 
1662         Parameters
1663         ----------
1664         new_name_or_name_dict : str or dict-like, optional
1665             If the argument is dict-like, it used as a mapping from old
1666             names to new names for coordinates. Otherwise, use the argument
1667             as the new name for this array.
1668         **names : hashable, optional
1669             The keyword arguments form of a mapping from old names to
1670             new names for coordinates.
1671             One of new_name_or_name_dict or names must be provided.
1672 
1673         Returns
1674         -------
1675         renamed : DataArray
1676             Renamed array or array with renamed coordinates.
1677 
1678         See Also
1679         --------
1680         Dataset.rename
1681         DataArray.swap_dims
1682         """
1683         if names or utils.is_dict_like(new_name_or_name_dict):
1684             new_name_or_name_dict = cast(
1685                 Mapping[Hashable, Hashable], new_name_or_name_dict
1686             )
1687             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
1688             dataset = self._to_temp_dataset().rename(name_dict)
1689             return self._from_temp_dataset(dataset)
1690         else:
1691             new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)
1692             return self._replace(name=new_name_or_name_dict)
1693 
1694     def swap_dims(self, dims_dict: Mapping[Hashable, Hashable]) -> "DataArray":
1695         """Returns a new DataArray with swapped dimensions.
1696 
1697         Parameters
1698         ----------
1699         dims_dict : dict-like
1700             Dictionary whose keys are current dimension names and whose values
1701             are new names.
1702 
1703         Returns
1704         -------
1705         swapped : DataArray
1706             DataArray with swapped dimensions.
1707 
1708         Examples
1709         --------
1710 
1711         >>> arr = xr.DataArray(
1712         ...     data=[0, 1],
1713         ...     dims="x",
1714         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
1715         ... )
1716         >>> arr
1717         <xarray.DataArray (x: 2)>
1718         array([0, 1])
1719         Coordinates:
1720           * x        (x) <U1 'a' 'b'
1721             y        (x) int64 0 1
1722 
1723         >>> arr.swap_dims({"x": "y"})
1724         <xarray.DataArray (y: 2)>
1725         array([0, 1])
1726         Coordinates:
1727             x        (y) <U1 'a' 'b'
1728           * y        (y) int64 0 1
1729 
1730         >>> arr.swap_dims({"x": "z"})
1731         <xarray.DataArray (z: 2)>
1732         array([0, 1])
1733         Coordinates:
1734             x        (z) <U1 'a' 'b'
1735             y        (z) int64 0 1
1736         Dimensions without coordinates: z
1737 
1738         See Also
1739         --------
1740 
1741         DataArray.rename
1742         Dataset.swap_dims
1743         """
1744         ds = self._to_temp_dataset().swap_dims(dims_dict)
1745         return self._from_temp_dataset(ds)
1746 
1747     def expand_dims(
1748         self,
1749         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,
1750         axis=None,
1751         **dim_kwargs: Any,
1752     ) -> "DataArray":
1753         """Return a new object with an additional axis (or axes) inserted at
1754         the corresponding position in the array shape. The new object is a
1755         view into the underlying array, not a copy.
1756 
1757 
1758         If dim is already a scalar coordinate, it will be promoted to a 1D
1759         coordinate consisting of a single value.
1760 
1761         Parameters
1762         ----------
1763         dim : hashable, sequence of hashable, dict, or None, optional
1764             Dimensions to include on the new variable.
1765             If provided as str or sequence of str, then dimensions are inserted
1766             with length 1. If provided as a dict, then the keys are the new
1767             dimensions and the values are either integers (giving the length of
1768             the new dimensions) or sequence/ndarray (giving the coordinates of
1769             the new dimensions).
1770         axis : int, list of int or tuple of int, or None, default: None
1771             Axis position(s) where new axis is to be inserted (position(s) on
1772             the result array). If a list (or tuple) of integers is passed,
1773             multiple axes are inserted. In this case, dim arguments should be
1774             same length list. If axis=None is passed, all the axes will be
1775             inserted to the start of the result array.
1776         **dim_kwargs : int or sequence or ndarray
1777             The keywords are arbitrary dimensions being inserted and the values
1778             are either the lengths of the new dims (if int is given), or their
1779             coordinates. Note, this is an alternative to passing a dict to the
1780             dim kwarg and will only be used if dim is None.
1781 
1782         Returns
1783         -------
1784         expanded : same type as caller
1785             This object, but with an additional dimension(s).
1786         """
1787         if isinstance(dim, int):
1788             raise TypeError("dim should be hashable or sequence/mapping of hashables")
1789         elif isinstance(dim, Sequence) and not isinstance(dim, str):
1790             if len(dim) != len(set(dim)):
1791                 raise ValueError("dims should not contain duplicate values.")
1792             dim = dict.fromkeys(dim, 1)
1793         elif dim is not None and not isinstance(dim, Mapping):
1794             dim = {cast(Hashable, dim): 1}
1795 
1796         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
1797         ds = self._to_temp_dataset().expand_dims(dim, axis)
1798         return self._from_temp_dataset(ds)
1799 
1800     def set_index(
1801         self,
1802         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
1803         append: bool = False,
1804         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],
1805     ) -> Optional["DataArray"]:
1806         """Set DataArray (multi-)indexes using one or more existing
1807         coordinates.
1808 
1809         Parameters
1810         ----------
1811         indexes : {dim: index, ...}
1812             Mapping from names matching dimensions and values given
1813             by (lists of) the names of existing coordinates or variables to set
1814             as new (multi-)index.
1815         append : bool, optional
1816             If True, append the supplied index(es) to the existing index(es).
1817             Otherwise replace the existing index(es) (default).
1818         **indexes_kwargs : optional
1819             The keyword arguments form of ``indexes``.
1820             One of indexes or indexes_kwargs must be provided.
1821 
1822         Returns
1823         -------
1824         obj : DataArray
1825             Another DataArray, with this data but replaced coordinates.
1826 
1827         Examples
1828         --------
1829         >>> arr = xr.DataArray(
1830         ...     data=np.ones((2, 3)),
1831         ...     dims=["x", "y"],
1832         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
1833         ... )
1834         >>> arr
1835         <xarray.DataArray (x: 2, y: 3)>
1836         array([[1., 1., 1.],
1837                [1., 1., 1.]])
1838         Coordinates:
1839           * x        (x) int64 0 1
1840           * y        (y) int64 0 1 2
1841             a        (x) int64 3 4
1842         >>> arr.set_index(x="a")
1843         <xarray.DataArray (x: 2, y: 3)>
1844         array([[1., 1., 1.],
1845                [1., 1., 1.]])
1846         Coordinates:
1847           * x        (x) int64 3 4
1848           * y        (y) int64 0 1 2
1849 
1850         See Also
1851         --------
1852         DataArray.reset_index
1853         """
1854         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)
1855         return self._from_temp_dataset(ds)
1856 
1857     def reset_index(
1858         self,
1859         dims_or_levels: Union[Hashable, Sequence[Hashable]],
1860         drop: bool = False,
1861     ) -> Optional["DataArray"]:
1862         """Reset the specified index(es) or multi-index level(s).
1863 
1864         Parameters
1865         ----------
1866         dims_or_levels : hashable or sequence of hashable
1867             Name(s) of the dimension(s) and/or multi-index level(s) that will
1868             be reset.
1869         drop : bool, optional
1870             If True, remove the specified indexes and/or multi-index levels
1871             instead of extracting them as new coordinates (default: False).
1872 
1873         Returns
1874         -------
1875         obj : DataArray
1876             Another dataarray, with this dataarray's data but replaced
1877             coordinates.
1878 
1879         See Also
1880         --------
1881         DataArray.set_index
1882         """
1883         coords, _ = split_indexes(
1884             dims_or_levels, self._coords, set(), self._level_coords, drop=drop
1885         )
1886         return self._replace(coords=coords)
1887 
1888     def reorder_levels(
1889         self,
1890         dim_order: Mapping[Hashable, Sequence[int]] = None,
1891         **dim_order_kwargs: Sequence[int],
1892     ) -> "DataArray":
1893         """Rearrange index levels using input order.
1894 
1895         Parameters
1896         ----------
1897         dim_order : optional
1898             Mapping from names matching dimensions and values given
1899             by lists representing new level orders. Every given dimension
1900             must have a multi-index.
1901         **dim_order_kwargs : optional
1902             The keyword arguments form of ``dim_order``.
1903             One of dim_order or dim_order_kwargs must be provided.
1904 
1905         Returns
1906         -------
1907         obj : DataArray
1908             Another dataarray, with this dataarray's data but replaced
1909             coordinates.
1910         """
1911         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
1912         replace_coords = {}
1913         for dim, order in dim_order.items():
1914             coord = self._coords[dim]
1915             index = coord.to_index()
1916             if not isinstance(index, pd.MultiIndex):
1917                 raise ValueError("coordinate %r has no MultiIndex" % dim)
1918             replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))
1919         coords = self._coords.copy()
1920         coords.update(replace_coords)
1921         return self._replace(coords=coords)
1922 
1923     def stack(
1924         self,
1925         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
1926         **dimensions_kwargs: Sequence[Hashable],
1927     ) -> "DataArray":
1928         """
1929         Stack any number of existing dimensions into a single new dimension.
1930 
1931         New dimensions will be added at the end, and the corresponding
1932         coordinate variables will be combined into a MultiIndex.
1933 
1934         Parameters
1935         ----------
1936         dimensions : mapping of hashable to sequence of hashable
1937             Mapping of the form `new_name=(dim1, dim2, ...)`.
1938             Names of new dimensions, and the existing dimensions that they
1939             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
1940             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
1941             all dimensions.
1942         **dimensions_kwargs
1943             The keyword arguments form of ``dimensions``.
1944             One of dimensions or dimensions_kwargs must be provided.
1945 
1946         Returns
1947         -------
1948         stacked : DataArray
1949             DataArray with stacked data.
1950 
1951         Examples
1952         --------
1953 
1954         >>> arr = xr.DataArray(
1955         ...     np.arange(6).reshape(2, 3),
1956         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
1957         ... )
1958         >>> arr
1959         <xarray.DataArray (x: 2, y: 3)>
1960         array([[0, 1, 2],
1961                [3, 4, 5]])
1962         Coordinates:
1963           * x        (x) <U1 'a' 'b'
1964           * y        (y) int64 0 1 2
1965         >>> stacked = arr.stack(z=("x", "y"))
1966         >>> stacked.indexes["z"]
1967         MultiIndex([('a', 0),
1968                     ('a', 1),
1969                     ('a', 2),
1970                     ('b', 0),
1971                     ('b', 1),
1972                     ('b', 2)],
1973                    names=['x', 'y'])
1974 
1975         See Also
1976         --------
1977         DataArray.unstack
1978         """
1979         ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)
1980         return self._from_temp_dataset(ds)
1981 
1982     def unstack(
1983         self,
1984         dim: Union[Hashable, Sequence[Hashable], None] = None,
1985         fill_value: Any = dtypes.NA,
1986         sparse: bool = False,
1987     ) -> "DataArray":
1988         """
1989         Unstack existing dimensions corresponding to MultiIndexes into
1990         multiple new dimensions.
1991 
1992         New dimensions will be added at the end.
1993 
1994         Parameters
1995         ----------
1996         dim : hashable or sequence of hashable, optional
1997             Dimension(s) over which to unstack. By default unstacks all
1998             MultiIndexes.
1999         fill_value : scalar or dict-like, default: nan
2000             value to be filled. If a dict-like, maps variable names to
2001             fill values. Use the data array's name to refer to its
2002             name. If not provided or if the dict-like does not contain
2003             all variables, the dtype's NA value will be used.
2004         sparse : bool, default: False
2005             use sparse-array if True
2006 
2007         Returns
2008         -------
2009         unstacked : DataArray
2010             Array with unstacked data.
2011 
2012         Examples
2013         --------
2014 
2015         >>> arr = xr.DataArray(
2016         ...     np.arange(6).reshape(2, 3),
2017         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2018         ... )
2019         >>> arr
2020         <xarray.DataArray (x: 2, y: 3)>
2021         array([[0, 1, 2],
2022                [3, 4, 5]])
2023         Coordinates:
2024           * x        (x) <U1 'a' 'b'
2025           * y        (y) int64 0 1 2
2026         >>> stacked = arr.stack(z=("x", "y"))
2027         >>> stacked.indexes["z"]
2028         MultiIndex([('a', 0),
2029                     ('a', 1),
2030                     ('a', 2),
2031                     ('b', 0),
2032                     ('b', 1),
2033                     ('b', 2)],
2034                    names=['x', 'y'])
2035         >>> roundtripped = stacked.unstack()
2036         >>> arr.identical(roundtripped)
2037         True
2038 
2039         See Also
2040         --------
2041         DataArray.stack
2042         """
2043         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
2044         return self._from_temp_dataset(ds)
2045 
2046     def to_unstacked_dataset(self, dim, level=0):
2047         """Unstack DataArray expanding to Dataset along a given level of a
2048         stacked coordinate.
2049 
2050         This is the inverse operation of Dataset.to_stacked_array.
2051 
2052         Parameters
2053         ----------
2054         dim : str
2055             Name of existing dimension to unstack
2056         level : int or str
2057             The MultiIndex level to expand to a dataset along. Can either be
2058             the integer index of the level or its name.
2059         label : int, default: 0
2060             Label of the level to expand dataset along. Overrides the label
2061             argument if given.
2062 
2063         Returns
2064         -------
2065         unstacked: Dataset
2066 
2067         Examples
2068         --------
2069         >>> import xarray as xr
2070         >>> arr = xr.DataArray(
2071         ...     np.arange(6).reshape(2, 3),
2072         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2073         ... )
2074         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
2075         >>> data
2076         <xarray.Dataset>
2077         Dimensions:  (x: 2, y: 3)
2078         Coordinates:
2079           * x        (x) <U1 'a' 'b'
2080           * y        (y) int64 0 1 2
2081         Data variables:
2082             a        (x, y) int64 0 1 2 3 4 5
2083             b        (x) int64 0 3
2084         >>> stacked = data.to_stacked_array("z", ["x"])
2085         >>> stacked.indexes["z"]
2086         MultiIndex([('a', 0.0),
2087                     ('a', 1.0),
2088                     ('a', 2.0),
2089                     ('b', nan)],
2090                    names=['variable', 'y'])
2091         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
2092         >>> data.identical(roundtripped)
2093         True
2094 
2095         See Also
2096         --------
2097         Dataset.to_stacked_array
2098         """
2099 
2100         idx = self.indexes[dim]
2101         if not isinstance(idx, pd.MultiIndex):
2102             raise ValueError(f"'{dim}' is not a stacked coordinate")
2103 
2104         level_number = idx._get_level_number(level)
2105         variables = idx.levels[level_number]
2106         variable_dim = idx.names[level_number]
2107 
2108         # pull variables out of datarray
2109         data_dict = {}
2110         for k in variables:
2111             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)
2112 
2113         # unstacked dataset
2114         return Dataset(data_dict)
2115 
2116     def transpose(self, *dims: Hashable, transpose_coords: bool = True) -> "DataArray":
2117         """Return a new DataArray object with transposed dimensions.
2118 
2119         Parameters
2120         ----------
2121         *dims : hashable, optional
2122             By default, reverse the dimensions. Otherwise, reorder the
2123             dimensions to this order.
2124         transpose_coords : bool, default: True
2125             If True, also transpose the coordinates of this DataArray.
2126 
2127         Returns
2128         -------
2129         transposed : DataArray
2130             The returned DataArray's array is transposed.
2131 
2132         Notes
2133         -----
2134         This operation returns a view of this array's data. It is
2135         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
2136         -- the data will be fully loaded.
2137 
2138         See Also
2139         --------
2140         numpy.transpose
2141         Dataset.transpose
2142         """
2143         if dims:
2144             dims = tuple(utils.infix_dims(dims, self.dims))
2145         variable = self.variable.transpose(*dims)
2146         if transpose_coords:
2147             coords: Dict[Hashable, Variable] = {}
2148             for name, coord in self.coords.items():
2149                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
2150                 coords[name] = coord.variable.transpose(*coord_dims)
2151             return self._replace(variable, coords)
2152         else:
2153             return self._replace(variable)
2154 
2155     @property
2156     def T(self) -> "DataArray":
2157         return self.transpose()
2158 
2159     def drop_vars(
2160         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
2161     ) -> "DataArray":
2162         """Returns an array with dropped variables.
2163 
2164         Parameters
2165         ----------
2166         names : hashable or iterable of hashable
2167             Name(s) of variables to drop.
2168         errors: {"raise", "ignore"}, optional
2169             If 'raise' (default), raises a ValueError error if any of the variable
2170             passed are not in the dataset. If 'ignore', any given names that are in the
2171             DataArray are dropped and no error is raised.
2172 
2173         Returns
2174         -------
2175         dropped : Dataset
2176             New Dataset copied from `self` with variables removed.
2177         """
2178         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
2179         return self._from_temp_dataset(ds)
2180 
2181     def drop(
2182         self,
2183         labels: Mapping = None,
2184         dim: Hashable = None,
2185         *,
2186         errors: str = "raise",
2187         **labels_kwargs,
2188     ) -> "DataArray":
2189         """Backward compatible method based on `drop_vars` and `drop_sel`
2190 
2191         Using either `drop_vars` or `drop_sel` is encouraged
2192 
2193         See Also
2194         --------
2195         DataArray.drop_vars
2196         DataArray.drop_sel
2197         """
2198         ds = self._to_temp_dataset().drop(labels, dim, errors=errors)
2199         return self._from_temp_dataset(ds)
2200 
2201     def drop_sel(
2202         self,
2203         labels: Mapping[Hashable, Any] = None,
2204         *,
2205         errors: str = "raise",
2206         **labels_kwargs,
2207     ) -> "DataArray":
2208         """Drop index labels from this DataArray.
2209 
2210         Parameters
2211         ----------
2212         labels : mapping of hashable to Any
2213             Index labels to drop
2214         errors : {"raise", "ignore"}, optional
2215             If 'raise' (default), raises a ValueError error if
2216             any of the index labels passed are not
2217             in the dataset. If 'ignore', any given labels that are in the
2218             dataset are dropped and no error is raised.
2219         **labels_kwargs : {dim: label, ...}, optional
2220             The keyword arguments form of ``dim`` and ``labels``
2221 
2222         Returns
2223         -------
2224         dropped : DataArray
2225         """
2226         if labels_kwargs or isinstance(labels, dict):
2227             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
2228 
2229         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
2230         return self._from_temp_dataset(ds)
2231 
2232     def dropna(
2233         self, dim: Hashable, how: str = "any", thresh: int = None
2234     ) -> "DataArray":
2235         """Returns a new array with dropped labels for missing values along
2236         the provided dimension.
2237 
2238         Parameters
2239         ----------
2240         dim : hashable
2241             Dimension along which to drop missing values. Dropping along
2242             multiple dimensions simultaneously is not yet supported.
2243         how : {"any", "all"}, optional
2244             * any : if any NA values are present, drop that label
2245             * all : if all values are NA, drop that label
2246         thresh : int, default: None
2247             If supplied, require this many non-NA values.
2248 
2249         Returns
2250         -------
2251         DataArray
2252         """
2253         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
2254         return self._from_temp_dataset(ds)
2255 
2256     def fillna(self, value: Any) -> "DataArray":
2257         """Fill missing values in this object.
2258 
2259         This operation follows the normal broadcasting and alignment rules that
2260         xarray uses for binary arithmetic, except the result is aligned to this
2261         object (``join='left'``) instead of aligned to the intersection of
2262         index coordinates (``join='inner'``).
2263 
2264         Parameters
2265         ----------
2266         value : scalar, ndarray or DataArray
2267             Used to fill all matching missing values in this array. If the
2268             argument is a DataArray, it is first aligned with (reindexed to)
2269             this array.
2270 
2271         Returns
2272         -------
2273         DataArray
2274         """
2275         if utils.is_dict_like(value):
2276             raise TypeError(
2277                 "cannot provide fill value as a dictionary with "
2278                 "fillna on a DataArray"
2279             )
2280         out = ops.fillna(self, value)
2281         return out
2282 
2283     def interpolate_na(
2284         self,
2285         dim: Hashable = None,
2286         method: str = "linear",
2287         limit: int = None,
2288         use_coordinate: Union[bool, str] = True,
2289         max_gap: Union[
2290             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta
2291         ] = None,
2292         keep_attrs: bool = None,
2293         **kwargs: Any,
2294     ) -> "DataArray":
2295         """Fill in NaNs by interpolating according to different methods.
2296 
2297         Parameters
2298         ----------
2299         dim : str
2300             Specifies the dimension along which to interpolate.
2301         method : str, optional
2302             String indicating which method to use for interpolation:
2303 
2304             - 'linear': linear interpolation (Default). Additional keyword
2305               arguments are passed to :py:func:`numpy.interp`
2306             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
2307               are passed to :py:func:`scipy.interpolate.interp1d`. If
2308               ``method='polynomial'``, the ``order`` keyword argument must also be
2309               provided.
2310             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
2311               respective :py:class:`scipy.interpolate` classes.
2312 
2313         use_coordinate : bool or str, default: True
2314             Specifies which index to use as the x values in the interpolation
2315             formulated as `y = f(x)`. If False, values are treated as if
2316             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
2317             used. If ``use_coordinate`` is a string, it specifies the name of a
2318             coordinate variariable to use as the index.
2319         limit : int, default: None
2320             Maximum number of consecutive NaNs to fill. Must be greater than 0
2321             or None for no limit. This filling is done regardless of the size of
2322             the gap in the data. To only interpolate over gaps less than a given length,
2323             see ``max_gap``.
2324         max_gap: int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
2325             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
2326             Use None for no limit. When interpolating along a datetime64 dimension
2327             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
2328 
2329             - a string that is valid input for pandas.to_timedelta
2330             - a :py:class:`numpy.timedelta64` object
2331             - a :py:class:`pandas.Timedelta` object
2332             - a :py:class:`datetime.timedelta` object
2333 
2334             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
2335             dimensions has not been implemented yet. Gap length is defined as the difference
2336             between coordinate values at the first data point after a gap and the last value
2337             before a gap. For gaps at the beginning (end), gap length is defined as the difference
2338             between coordinate values at the first (last) valid data point and the first (last) NaN.
2339             For example, consider::
2340 
2341                 <xarray.DataArray (x: 9)>
2342                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
2343                 Coordinates:
2344                   * x        (x) int64 0 1 2 3 4 5 6 7 8
2345 
2346             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
2347         keep_attrs : bool, default: True
2348             If True, the dataarray's attributes (`attrs`) will be copied from
2349             the original object to the new one.  If False, the new
2350             object will be returned without attributes.
2351         kwargs : dict, optional
2352             parameters passed verbatim to the underlying interpolation function
2353 
2354         Returns
2355         -------
2356         interpolated: DataArray
2357             Filled in DataArray.
2358 
2359         See also
2360         --------
2361         numpy.interp
2362         scipy.interpolate
2363 
2364         Examples
2365         --------
2366         >>> da = xr.DataArray(
2367         ...     [np.nan, 2, 3, np.nan, 0], dims="x", coords={"x": [0, 1, 2, 3, 4]}
2368         ... )
2369         >>> da
2370         <xarray.DataArray (x: 5)>
2371         array([nan,  2.,  3., nan,  0.])
2372         Coordinates:
2373           * x        (x) int64 0 1 2 3 4
2374 
2375         >>> da.interpolate_na(dim="x", method="linear")
2376         <xarray.DataArray (x: 5)>
2377         array([nan, 2. , 3. , 1.5, 0. ])
2378         Coordinates:
2379           * x        (x) int64 0 1 2 3 4
2380 
2381         >>> da.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
2382         <xarray.DataArray (x: 5)>
2383         array([1. , 2. , 3. , 1.5, 0. ])
2384         Coordinates:
2385           * x        (x) int64 0 1 2 3 4
2386         """
2387         from .missing import interp_na
2388 
2389         return interp_na(
2390             self,
2391             dim=dim,
2392             method=method,
2393             limit=limit,
2394             use_coordinate=use_coordinate,
2395             max_gap=max_gap,
2396             keep_attrs=keep_attrs,
2397             **kwargs,
2398         )
2399 
2400     def ffill(self, dim: Hashable, limit: int = None) -> "DataArray":
2401         """Fill NaN values by propogating values forward
2402 
2403         *Requires bottleneck.*
2404 
2405         Parameters
2406         ----------
2407         dim : hashable
2408             Specifies the dimension along which to propagate values when
2409             filling.
2410         limit : int, default: None
2411             The maximum number of consecutive NaN values to forward fill. In
2412             other words, if there is a gap with more than this number of
2413             consecutive NaNs, it will only be partially filled. Must be greater
2414             than 0 or None for no limit.
2415 
2416         Returns
2417         -------
2418         DataArray
2419         """
2420         from .missing import ffill
2421 
2422         return ffill(self, dim, limit=limit)
2423 
2424     def bfill(self, dim: Hashable, limit: int = None) -> "DataArray":
2425         """Fill NaN values by propogating values backward
2426 
2427         *Requires bottleneck.*
2428 
2429         Parameters
2430         ----------
2431         dim : str
2432             Specifies the dimension along which to propagate values when
2433             filling.
2434         limit : int, default: None
2435             The maximum number of consecutive NaN values to backward fill. In
2436             other words, if there is a gap with more than this number of
2437             consecutive NaNs, it will only be partially filled. Must be greater
2438             than 0 or None for no limit.
2439 
2440         Returns
2441         -------
2442         DataArray
2443         """
2444         from .missing import bfill
2445 
2446         return bfill(self, dim, limit=limit)
2447 
2448     def combine_first(self, other: "DataArray") -> "DataArray":
2449         """Combine two DataArray objects, with union of coordinates.
2450 
2451         This operation follows the normal broadcasting and alignment rules of
2452         ``join='outer'``.  Default to non-null values of array calling the
2453         method.  Use np.nan to fill in vacant cells after alignment.
2454 
2455         Parameters
2456         ----------
2457         other : DataArray
2458             Used to fill all matching missing values in this array.
2459 
2460         Returns
2461         -------
2462         DataArray
2463         """
2464         return ops.fillna(self, other, join="outer")
2465 
2466     def reduce(
2467         self,
2468         func: Callable[..., Any],
2469         dim: Union[None, Hashable, Sequence[Hashable]] = None,
2470         axis: Union[None, int, Sequence[int]] = None,
2471         keep_attrs: bool = None,
2472         keepdims: bool = False,
2473         **kwargs: Any,
2474     ) -> "DataArray":
2475         """Reduce this array by applying `func` along some dimension(s).
2476 
2477         Parameters
2478         ----------
2479         func : callable
2480             Function which can be called in the form
2481             `f(x, axis=axis, **kwargs)` to return the result of reducing an
2482             np.ndarray over an integer valued axis.
2483         dim : hashable or sequence of hashable, optional
2484             Dimension(s) over which to apply `func`.
2485         axis : int or sequence of int, optional
2486             Axis(es) over which to repeatedly apply `func`. Only one of the
2487             'dim' and 'axis' arguments can be supplied. If neither are
2488             supplied, then the reduction is calculated over the flattened array
2489             (by calling `f(x)` without an axis argument).
2490         keep_attrs : bool, optional
2491             If True, the variable's attributes (`attrs`) will be copied from
2492             the original object to the new one.  If False (default), the new
2493             object will be returned without attributes.
2494         keepdims : bool, default: False
2495             If True, the dimensions which are reduced are left in the result
2496             as dimensions of size one. Coordinates that use these dimensions
2497             are removed.
2498         **kwargs : dict
2499             Additional keyword arguments passed on to `func`.
2500 
2501         Returns
2502         -------
2503         reduced : DataArray
2504             DataArray with this object's array replaced with an array with
2505             summarized data and the indicated dimension(s) removed.
2506         """
2507 
2508         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
2509         return self._replace_maybe_drop_dims(var)
2510 
2511     def to_pandas(self) -> Union["DataArray", pd.Series, pd.DataFrame]:
2512         """Convert this array into a pandas object with the same shape.
2513 
2514         The type of the returned object depends on the number of DataArray
2515         dimensions:
2516 
2517         * 0D -> `xarray.DataArray`
2518         * 1D -> `pandas.Series`
2519         * 2D -> `pandas.DataFrame`
2520 
2521         Only works for arrays with 2 or fewer dimensions.
2522 
2523         The DataArray constructor performs the inverse transformation.
2524         """
2525         # TODO: consolidate the info about pandas constructors and the
2526         # attributes that correspond to their indexes into a separate module?
2527         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
2528         try:
2529             constructor = constructors[self.ndim]
2530         except KeyError:
2531             raise ValueError(
2532                 "cannot convert arrays with %s dimensions into "
2533                 "pandas objects" % self.ndim
2534             )
2535         indexes = [self.get_index(dim) for dim in self.dims]
2536         return constructor(self.values, *indexes)
2537 
2538     def to_dataframe(
2539         self, name: Hashable = None, dim_order: List[Hashable] = None
2540     ) -> pd.DataFrame:
2541         """Convert this array and its coordinates into a tidy pandas.DataFrame.
2542 
2543         The DataFrame is indexed by the Cartesian product of index coordinates
2544         (in the form of a :py:class:`pandas.MultiIndex`).
2545 
2546         Other coordinates are included as columns in the DataFrame.
2547 
2548         Parameters
2549         ----------
2550         name
2551             Name to give to this array (required if unnamed).
2552         dim_order
2553             Hierarchical dimension order for the resulting dataframe.
2554             Array content is transposed to this order and then written out as flat
2555             vectors in contiguous order, so the last dimension in this list
2556             will be contiguous in the resulting DataFrame. This has a major
2557             influence on which operations are efficient on the resulting
2558             dataframe.
2559 
2560             If provided, must include all dimensions of this DataArray. By default,
2561             dimensions are sorted according to the DataArray dimensions order.
2562 
2563         Returns
2564         -------
2565         result
2566             DataArray as a pandas DataFrame.
2567 
2568         """
2569         if name is None:
2570             name = self.name
2571         if name is None:
2572             raise ValueError(
2573                 "cannot convert an unnamed DataArray to a "
2574                 "DataFrame: use the ``name`` parameter"
2575             )
2576         if self.ndim == 0:
2577             raise ValueError("cannot convert a scalar to a DataFrame")
2578 
2579         # By using a unique name, we can convert a DataArray into a DataFrame
2580         # even if it shares a name with one of its coordinates.
2581         # I would normally use unique_name = object() but that results in a
2582         # dataframe with columns in the wrong order, for reasons I have not
2583         # been able to debug (possibly a pandas bug?).
2584         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
2585         ds = self._to_dataset_whole(name=unique_name)
2586 
2587         if dim_order is None:
2588             ordered_dims = dict(zip(self.dims, self.shape))
2589         else:
2590             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)
2591 
2592         df = ds._to_dataframe(ordered_dims)
2593         df.columns = [name if c == unique_name else c for c in df.columns]
2594         return df
2595 
2596     def to_series(self) -> pd.Series:
2597         """Convert this array into a pandas.Series.
2598 
2599         The Series is indexed by the Cartesian product of index coordinates
2600         (in the form of a :py:class:`pandas.MultiIndex`).
2601         """
2602         index = self.coords.to_index()
2603         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
2604 
2605     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
2606         """Convert this array into a numpy.ma.MaskedArray
2607 
2608         Parameters
2609         ----------
2610         copy : bool, default: True
2611             If True make a copy of the array in the result. If False,
2612             a MaskedArray view of DataArray.values is returned.
2613 
2614         Returns
2615         -------
2616         result : MaskedArray
2617             Masked where invalid values (nan or inf) occur.
2618         """
2619         values = self.values  # only compute lazy arrays once
2620         isnull = pd.isnull(values)
2621         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
2622 
2623     def to_netcdf(self, *args, **kwargs) -> Union[bytes, "Delayed", None]:
2624         """Write DataArray contents to a netCDF file.
2625 
2626         All parameters are passed directly to :py:meth:`xarray.Dataset.to_netcdf`.
2627 
2628         Notes
2629         -----
2630         Only xarray.Dataset objects can be written to netCDF files, so
2631         the xarray.DataArray is converted to a xarray.Dataset object
2632         containing a single variable. If the DataArray has no name, or if the
2633         name is the same as a coordinate name, then it is given the name
2634         ``"__xarray_dataarray_variable__"``.
2635 
2636         See Also
2637         --------
2638         Dataset.to_netcdf
2639         """
2640         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE
2641 
2642         if self.name is None:
2643             # If no name is set then use a generic xarray name
2644             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2645         elif self.name in self.coords or self.name in self.dims:
2646             # The name is the same as one of the coords names, which netCDF
2647             # doesn't support, so rename it but keep track of the old name
2648             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2649             dataset.attrs[DATAARRAY_NAME] = self.name
2650         else:
2651             # No problems with the name - so we're fine!
2652             dataset = self.to_dataset()
2653 
2654         return dataset.to_netcdf(*args, **kwargs)
2655 
2656     def to_dict(self, data: bool = True) -> dict:
2657         """
2658         Convert this xarray.DataArray into a dictionary following xarray
2659         naming conventions.
2660 
2661         Converts all variables and attributes to native Python objects.
2662         Useful for converting to json. To avoid datetime incompatibility
2663         use decode_times=False kwarg in xarray.open_dataset.
2664 
2665         Parameters
2666         ----------
2667         data : bool, optional
2668             Whether to include the actual data in the dictionary. When set to
2669             False, returns just the schema.
2670 
2671         See also
2672         --------
2673         DataArray.from_dict
2674         """
2675         d = self.variable.to_dict(data=data)
2676         d.update({"coords": {}, "name": self.name})
2677         for k in self.coords:
2678             d["coords"][k] = self.coords[k].variable.to_dict(data=data)
2679         return d
2680 
2681     @classmethod
2682     def from_dict(cls, d: dict) -> "DataArray":
2683         """
2684         Convert a dictionary into an xarray.DataArray
2685 
2686         Input dict can take several forms:
2687 
2688         .. code:: python
2689 
2690             d = {"dims": ("t"), "data": x}
2691 
2692             d = {
2693                 "coords": {"t": {"dims": "t", "data": t, "attrs": {"units": "s"}}},
2694                 "attrs": {"title": "air temperature"},
2695                 "dims": "t",
2696                 "data": x,
2697                 "name": "a",
2698             }
2699 
2700         where "t" is the name of the dimesion, "a" is the name of the array,
2701         and x and t are lists, numpy.arrays, or pandas objects.
2702 
2703         Parameters
2704         ----------
2705         d : dict
2706             Mapping with a minimum structure of {"dims": [...], "data": [...]}
2707 
2708         Returns
2709         -------
2710         obj : xarray.DataArray
2711 
2712         See also
2713         --------
2714         DataArray.to_dict
2715         Dataset.from_dict
2716         """
2717         coords = None
2718         if "coords" in d:
2719             try:
2720                 coords = {
2721                     k: (v["dims"], v["data"], v.get("attrs"))
2722                     for k, v in d["coords"].items()
2723                 }
2724             except KeyError as e:
2725                 raise ValueError(
2726                     "cannot convert dict when coords are missing the key "
2727                     "'{dims_data}'".format(dims_data=str(e.args[0]))
2728                 )
2729         try:
2730             data = d["data"]
2731         except KeyError:
2732             raise ValueError("cannot convert dict without the key 'data''")
2733         else:
2734             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
2735         return obj
2736 
2737     @classmethod
2738     def from_series(cls, series: pd.Series, sparse: bool = False) -> "DataArray":
2739         """Convert a pandas.Series into an xarray.DataArray.
2740 
2741         If the series's index is a MultiIndex, it will be expanded into a
2742         tensor product of one-dimensional coordinates (filling in missing
2743         values with NaN). Thus this operation should be the inverse of the
2744         `to_series` method.
2745 
2746         If sparse=True, creates a sparse array instead of a dense NumPy array.
2747         Requires the pydata/sparse package.
2748 
2749         See also
2750         --------
2751         xarray.Dataset.from_dataframe
2752         """
2753         temp_name = "__temporary_name"
2754         df = pd.DataFrame({temp_name: series})
2755         ds = Dataset.from_dataframe(df, sparse=sparse)
2756         result = cast(DataArray, ds[temp_name])
2757         result.name = series.name
2758         return result
2759 
2760     def to_cdms2(self) -> "cdms2_Variable":
2761         """Convert this array into a cdms2.Variable"""
2762         from ..convert import to_cdms2
2763 
2764         return to_cdms2(self)
2765 
2766     @classmethod
2767     def from_cdms2(cls, variable: "cdms2_Variable") -> "DataArray":
2768         """Convert a cdms2.Variable into an xarray.DataArray"""
2769         from ..convert import from_cdms2
2770 
2771         return from_cdms2(variable)
2772 
2773     def to_iris(self) -> "iris_Cube":
2774         """Convert this array into a iris.cube.Cube"""
2775         from ..convert import to_iris
2776 
2777         return to_iris(self)
2778 
2779     @classmethod
2780     def from_iris(cls, cube: "iris_Cube") -> "DataArray":
2781         """Convert a iris.cube.Cube into an xarray.DataArray"""
2782         from ..convert import from_iris
2783 
2784         return from_iris(cube)
2785 
2786     def _all_compat(self, other: "DataArray", compat_str: str) -> bool:
2787         """Helper function for equals, broadcast_equals, and identical"""
2788 
2789         def compat(x, y):
2790             return getattr(x.variable, compat_str)(y.variable)
2791 
2792         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
2793             self, other
2794         )
2795 
2796     def broadcast_equals(self, other: "DataArray") -> bool:
2797         """Two DataArrays are broadcast equal if they are equal after
2798         broadcasting them against each other such that they have the same
2799         dimensions.
2800 
2801         See Also
2802         --------
2803         DataArray.equals
2804         DataArray.identical
2805         """
2806         try:
2807             return self._all_compat(other, "broadcast_equals")
2808         except (TypeError, AttributeError):
2809             return False
2810 
2811     def equals(self, other: "DataArray") -> bool:
2812         """True if two DataArrays have the same dimensions, coordinates and
2813         values; otherwise False.
2814 
2815         DataArrays can still be equal (like pandas objects) if they have NaN
2816         values in the same locations.
2817 
2818         This method is necessary because `v1 == v2` for ``DataArray``
2819         does element-wise comparisons (like numpy.ndarrays).
2820 
2821         See Also
2822         --------
2823         DataArray.broadcast_equals
2824         DataArray.identical
2825         """
2826         try:
2827             return self._all_compat(other, "equals")
2828         except (TypeError, AttributeError):
2829             return False
2830 
2831     def identical(self, other: "DataArray") -> bool:
2832         """Like equals, but also checks the array name and attributes, and
2833         attributes on all coordinates.
2834 
2835         See Also
2836         --------
2837         DataArray.broadcast_equals
2838         DataArray.equals
2839         """
2840         try:
2841             return self.name == other.name and self._all_compat(other, "identical")
2842         except (TypeError, AttributeError):
2843             return False
2844 
2845     def _result_name(self, other: Any = None) -> Optional[Hashable]:
2846         # use the same naming heuristics as pandas:
2847         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
2848         other_name = getattr(other, "name", _default)
2849         if other_name is _default or other_name == self.name:
2850             return self.name
2851         else:
2852             return None
2853 
2854     def __array_wrap__(self, obj, context=None) -> "DataArray":
2855         new_var = self.variable.__array_wrap__(obj, context)
2856         return self._replace(new_var)
2857 
2858     def __matmul__(self, obj):
2859         return self.dot(obj)
2860 
2861     def __rmatmul__(self, other):
2862         # currently somewhat duplicative, as only other DataArrays are
2863         # compatible with matmul
2864         return computation.dot(other, self)
2865 
2866     @staticmethod
2867     def _unary_op(f: Callable[..., Any]) -> Callable[..., "DataArray"]:
2868         @functools.wraps(f)
2869         def func(self, *args, **kwargs):
2870             keep_attrs = kwargs.pop("keep_attrs", None)
2871             if keep_attrs is None:
2872                 keep_attrs = _get_keep_attrs(default=True)
2873             with warnings.catch_warnings():
2874                 warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
2875                 warnings.filterwarnings(
2876                     "ignore", r"Mean of empty slice", category=RuntimeWarning
2877                 )
2878                 with np.errstate(all="ignore"):
2879                     da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))
2880                 if keep_attrs:
2881                     da.attrs = self.attrs
2882                 return da
2883 
2884         return func
2885 
2886     @staticmethod
2887     def _binary_op(
2888         f: Callable[..., Any],
2889         reflexive: bool = False,
2890         join: str = None,  # see xarray.align
2891         **ignored_kwargs,
2892     ) -> Callable[..., "DataArray"]:
2893         @functools.wraps(f)
2894         def func(self, other):
2895             if isinstance(other, (Dataset, groupby.GroupBy)):
2896                 return NotImplemented
2897             if isinstance(other, DataArray):
2898                 align_type = OPTIONS["arithmetic_join"] if join is None else join
2899                 self, other = align(self, other, join=align_type, copy=False)
2900             other_variable = getattr(other, "variable", other)
2901             other_coords = getattr(other, "coords", None)
2902 
2903             variable = (
2904                 f(self.variable, other_variable)
2905                 if not reflexive
2906                 else f(other_variable, self.variable)
2907             )
2908             coords, indexes = self.coords._merge_raw(other_coords)
2909             name = self._result_name(other)
2910 
2911             return self._replace(variable, coords, name, indexes=indexes)
2912 
2913         return func
2914 
2915     @staticmethod
2916     def _inplace_binary_op(f: Callable) -> Callable[..., "DataArray"]:
2917         @functools.wraps(f)
2918         def func(self, other):
2919             if isinstance(other, groupby.GroupBy):
2920                 raise TypeError(
2921                     "in-place operations between a DataArray and "
2922                     "a grouped object are not permitted"
2923                 )
2924             # n.b. we can't align other to self (with other.reindex_like(self))
2925             # because `other` may be converted into floats, which would cause
2926             # in-place arithmetic to fail unpredictably. Instead, we simply
2927             # don't support automatic alignment with in-place arithmetic.
2928             other_coords = getattr(other, "coords", None)
2929             other_variable = getattr(other, "variable", other)
2930             try:
2931                 with self.coords._merge_inplace(other_coords):
2932                     f(self.variable, other_variable)
2933             except MergeError as exc:
2934                 raise MergeError(
2935                     "Automatic alignment is not supported for in-place operations.\n"
2936                     "Consider aligning the indices manually or using a not-in-place operation.\n"
2937                     "See https://github.com/pydata/xarray/issues/3910 for more explanations."
2938                 ) from exc
2939             return self
2940 
2941         return func
2942 
2943     def _copy_attrs_from(self, other: Union["DataArray", Dataset, Variable]) -> None:
2944         self.attrs = other.attrs
2945 
2946     plot = utils.UncachedAccessor(_PlotMethods)
2947 
2948     def _title_for_slice(self, truncate: int = 50) -> str:
2949         """
2950         If the dataarray has 1 dimensional coordinates or comes from a slice
2951         we can show that info in the title
2952 
2953         Parameters
2954         ----------
2955         truncate : int, default: 50
2956             maximum number of characters for title
2957 
2958         Returns
2959         -------
2960         title : string
2961             Can be used for plot titles
2962 
2963         """
2964         one_dims = []
2965         for dim, coord in self.coords.items():
2966             if coord.size == 1:
2967                 one_dims.append(
2968                     "{dim} = {v}".format(dim=dim, v=format_item(coord.values))
2969                 )
2970 
2971         title = ", ".join(one_dims)
2972         if len(title) > truncate:
2973             title = title[: (truncate - 3)] + "..."
2974 
2975         return title
2976 
2977     def diff(self, dim: Hashable, n: int = 1, label: Hashable = "upper") -> "DataArray":
2978         """Calculate the n-th order discrete difference along given axis.
2979 
2980         Parameters
2981         ----------
2982         dim : hashable
2983             Dimension over which to calculate the finite difference.
2984         n : int, optional
2985             The number of times values are differenced.
2986         label : hashable, optional
2987             The new coordinate in dimension ``dim`` will have the
2988             values of either the minuend's or subtrahend's coordinate
2989             for values 'upper' and 'lower', respectively.  Other
2990             values are not supported.
2991 
2992         Returns
2993         -------
2994         difference : same type as caller
2995             The n-th order finite difference of this object.
2996 
2997         .. note::
2998 
2999             `n` matches numpy's behavior and is different from pandas' first
3000             argument named `periods`.
3001 
3002 
3003         Examples
3004         --------
3005         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
3006         >>> arr.diff("x")
3007         <xarray.DataArray (x: 3)>
3008         array([0, 1, 0])
3009         Coordinates:
3010           * x        (x) int64 2 3 4
3011         >>> arr.diff("x", 2)
3012         <xarray.DataArray (x: 2)>
3013         array([ 1, -1])
3014         Coordinates:
3015           * x        (x) int64 3 4
3016 
3017         See Also
3018         --------
3019         DataArray.differentiate
3020         """
3021         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
3022         return self._from_temp_dataset(ds)
3023 
3024     def shift(
3025         self,
3026         shifts: Mapping[Hashable, int] = None,
3027         fill_value: Any = dtypes.NA,
3028         **shifts_kwargs: int,
3029     ) -> "DataArray":
3030         """Shift this array by an offset along one or more dimensions.
3031 
3032         Only the data is moved; coordinates stay in place. Values shifted from
3033         beyond array bounds are replaced by NaN. This is consistent with the
3034         behavior of ``shift`` in pandas.
3035 
3036         Parameters
3037         ----------
3038         shifts : mapping of hashable to int, optional
3039             Integer offset to shift along each of the given dimensions.
3040             Positive offsets shift to the right; negative offsets shift to the
3041             left.
3042         fill_value: scalar, optional
3043             Value to use for newly missing values
3044         **shifts_kwargs
3045             The keyword arguments form of ``shifts``.
3046             One of shifts or shifts_kwargs must be provided.
3047 
3048         Returns
3049         -------
3050         shifted : DataArray
3051             DataArray with the same coordinates and attributes but shifted
3052             data.
3053 
3054         See also
3055         --------
3056         roll
3057 
3058         Examples
3059         --------
3060 
3061         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3062         >>> arr.shift(x=1)
3063         <xarray.DataArray (x: 3)>
3064         array([nan,  5.,  6.])
3065         Dimensions without coordinates: x
3066         """
3067         variable = self.variable.shift(
3068             shifts=shifts, fill_value=fill_value, **shifts_kwargs
3069         )
3070         return self._replace(variable=variable)
3071 
3072     def roll(
3073         self,
3074         shifts: Mapping[Hashable, int] = None,
3075         roll_coords: bool = None,
3076         **shifts_kwargs: int,
3077     ) -> "DataArray":
3078         """Roll this array by an offset along one or more dimensions.
3079 
3080         Unlike shift, roll may rotate all variables, including coordinates
3081         if specified. The direction of rotation is consistent with
3082         :py:func:`numpy.roll`.
3083 
3084         Parameters
3085         ----------
3086         shifts : mapping of hashable to int, optional
3087             Integer offset to rotate each of the given dimensions.
3088             Positive offsets roll to the right; negative offsets roll to the
3089             left.
3090         roll_coords : bool
3091             Indicates whether to roll the coordinates by the offset
3092             The current default of roll_coords (None, equivalent to True) is
3093             deprecated and will change to False in a future version.
3094             Explicitly pass roll_coords to silence the warning.
3095         **shifts_kwargs
3096             The keyword arguments form of ``shifts``.
3097             One of shifts or shifts_kwargs must be provided.
3098 
3099         Returns
3100         -------
3101         rolled : DataArray
3102             DataArray with the same attributes but rolled data and coordinates.
3103 
3104         See also
3105         --------
3106         shift
3107 
3108         Examples
3109         --------
3110 
3111         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3112         >>> arr.roll(x=1)
3113         <xarray.DataArray (x: 3)>
3114         array([7, 5, 6])
3115         Dimensions without coordinates: x
3116         """
3117         ds = self._to_temp_dataset().roll(
3118             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
3119         )
3120         return self._from_temp_dataset(ds)
3121 
3122     @property
3123     def real(self) -> "DataArray":
3124         return self._replace(self.variable.real)
3125 
3126     @property
3127     def imag(self) -> "DataArray":
3128         return self._replace(self.variable.imag)
3129 
3130     def dot(
3131         self, other: "DataArray", dims: Union[Hashable, Sequence[Hashable], None] = None
3132     ) -> "DataArray":
3133         """Perform dot product of two DataArrays along their shared dims.
3134 
3135         Equivalent to taking taking tensordot over all shared dims.
3136 
3137         Parameters
3138         ----------
3139         other : DataArray
3140             The other array with which the dot product is performed.
3141         dims : ..., hashable or sequence of hashable, optional
3142             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.
3143             If not specified, then all the common dimensions are summed over.
3144 
3145         Returns
3146         -------
3147         result : DataArray
3148             Array resulting from the dot product over all shared dimensions.
3149 
3150         See also
3151         --------
3152         dot
3153         numpy.tensordot
3154 
3155         Examples
3156         --------
3157 
3158         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
3159         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
3160         >>> dm_vals = np.arange(4)
3161         >>> dm = xr.DataArray(dm_vals, dims=["z"])
3162 
3163         >>> dm.dims
3164         ('z',)
3165 
3166         >>> da.dims
3167         ('x', 'y', 'z')
3168 
3169         >>> dot_result = da.dot(dm)
3170         >>> dot_result.dims
3171         ('x', 'y')
3172 
3173         """
3174         if isinstance(other, Dataset):
3175             raise NotImplementedError(
3176                 "dot products are not yet supported with Dataset objects."
3177             )
3178         if not isinstance(other, DataArray):
3179             raise TypeError("dot only operates on DataArrays.")
3180 
3181         return computation.dot(self, other, dims=dims)
3182 
3183     def sortby(
3184         self,
3185         variables: Union[Hashable, "DataArray", Sequence[Union[Hashable, "DataArray"]]],
3186         ascending: bool = True,
3187     ) -> "DataArray":
3188         """Sort object by labels or values (along an axis).
3189 
3190         Sorts the dataarray, either along specified dimensions,
3191         or according to values of 1-D dataarrays that share dimension
3192         with calling object.
3193 
3194         If the input variables are dataarrays, then the dataarrays are aligned
3195         (via left-join) to the calling object prior to sorting by cell values.
3196         NaNs are sorted to the end, following Numpy convention.
3197 
3198         If multiple sorts along the same dimension is
3199         given, numpy's lexsort is performed along that dimension:
3200         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
3201         and the FIRST key in the sequence is used as the primary sort key,
3202         followed by the 2nd key, etc.
3203 
3204         Parameters
3205         ----------
3206         variables : hashable, DataArray, or sequence of hashable or DataArray
3207             1D DataArray objects or name(s) of 1D variable(s) in
3208             coords whose values are used to sort this array.
3209         ascending : bool, optional
3210             Whether to sort by ascending or descending order.
3211 
3212         Returns
3213         -------
3214         sorted : DataArray
3215             A new dataarray where all the specified dims are sorted by dim
3216             labels.
3217 
3218         Examples
3219         --------
3220 
3221         >>> da = xr.DataArray(
3222         ...     np.random.rand(5),
3223         ...     coords=[pd.date_range("1/1/2000", periods=5)],
3224         ...     dims="time",
3225         ... )
3226         >>> da
3227         <xarray.DataArray (time: 5)>
3228         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])
3229         Coordinates:
3230           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05
3231 
3232         >>> da.sortby(da)
3233         <xarray.DataArray (time: 5)>
3234         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])
3235         Coordinates:
3236           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02
3237         """
3238         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
3239         return self._from_temp_dataset(ds)
3240 
3241     def quantile(
3242         self,
3243         q: Any,
3244         dim: Union[Hashable, Sequence[Hashable], None] = None,
3245         interpolation: str = "linear",
3246         keep_attrs: bool = None,
3247         skipna: bool = True,
3248     ) -> "DataArray":
3249         """Compute the qth quantile of the data along the specified dimension.
3250 
3251         Returns the qth quantiles(s) of the array elements.
3252 
3253         Parameters
3254         ----------
3255         q : float or array-like of float
3256             Quantile to compute, which must be between 0 and 1 inclusive.
3257         dim : hashable or sequence of hashable, optional
3258             Dimension(s) over which to apply quantile.
3259         interpolation : {"linear", "lower", "higher", "midpoint", "nearest"}, default: "linear"
3260             This optional parameter specifies the interpolation method to
3261             use when the desired quantile lies between two data points
3262             ``i < j``:
3263 
3264                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is
3265                   the fractional part of the index surrounded by ``i`` and
3266                   ``j``.
3267                 - lower: ``i``.
3268                 - higher: ``j``.
3269                 - nearest: ``i`` or ``j``, whichever is nearest.
3270                 - midpoint: ``(i + j) / 2``.
3271         keep_attrs : bool, optional
3272             If True, the dataset's attributes (`attrs`) will be copied from
3273             the original object to the new one.  If False (default), the new
3274             object will be returned without attributes.
3275         skipna : bool, optional
3276             Whether to skip missing values when aggregating.
3277 
3278         Returns
3279         -------
3280         quantiles : DataArray
3281             If `q` is a single quantile, then the result
3282             is a scalar. If multiple percentiles are given, first axis of
3283             the result corresponds to the quantile and a quantile dimension
3284             is added to the return array. The other dimensions are the
3285             dimensions that remain after the reduction of the array.
3286 
3287         See Also
3288         --------
3289         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
3290 
3291         Examples
3292         --------
3293 
3294         >>> da = xr.DataArray(
3295         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
3296         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
3297         ...     dims=("x", "y"),
3298         ... )
3299         >>> da.quantile(0)  # or da.quantile(0, dim=...)
3300         <xarray.DataArray ()>
3301         array(0.7)
3302         Coordinates:
3303             quantile  float64 0.0
3304         >>> da.quantile(0, dim="x")
3305         <xarray.DataArray (y: 4)>
3306         array([0.7, 4.2, 2.6, 1.5])
3307         Coordinates:
3308           * y         (y) float64 1.0 1.5 2.0 2.5
3309             quantile  float64 0.0
3310         >>> da.quantile([0, 0.5, 1])
3311         <xarray.DataArray (quantile: 3)>
3312         array([0.7, 3.4, 9.4])
3313         Coordinates:
3314           * quantile  (quantile) float64 0.0 0.5 1.0
3315         >>> da.quantile([0, 0.5, 1], dim="x")
3316         <xarray.DataArray (quantile: 3, y: 4)>
3317         array([[0.7 , 4.2 , 2.6 , 1.5 ],
3318                [3.6 , 5.75, 6.  , 1.7 ],
3319                [6.5 , 7.3 , 9.4 , 1.9 ]])
3320         Coordinates:
3321           * y         (y) float64 1.0 1.5 2.0 2.5
3322           * quantile  (quantile) float64 0.0 0.5 1.0
3323         """
3324 
3325         ds = self._to_temp_dataset().quantile(
3326             q,
3327             dim=dim,
3328             keep_attrs=keep_attrs,
3329             interpolation=interpolation,
3330             skipna=skipna,
3331         )
3332         return self._from_temp_dataset(ds)
3333 
3334     def rank(
3335         self, dim: Hashable, pct: bool = False, keep_attrs: bool = None
3336     ) -> "DataArray":
3337         """Ranks the data.
3338 
3339         Equal values are assigned a rank that is the average of the ranks that
3340         would have been otherwise assigned to all of the values within that
3341         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
3342 
3343         NaNs in the input array are returned as NaNs.
3344 
3345         The `bottleneck` library is required.
3346 
3347         Parameters
3348         ----------
3349         dim : hashable
3350             Dimension over which to compute rank.
3351         pct : bool, optional
3352             If True, compute percentage ranks, otherwise compute integer ranks.
3353         keep_attrs : bool, optional
3354             If True, the dataset's attributes (`attrs`) will be copied from
3355             the original object to the new one.  If False (default), the new
3356             object will be returned without attributes.
3357 
3358         Returns
3359         -------
3360         ranked : DataArray
3361             DataArray with the same coordinates and dtype 'float64'.
3362 
3363         Examples
3364         --------
3365 
3366         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3367         >>> arr.rank("x")
3368         <xarray.DataArray (x: 3)>
3369         array([1., 2., 3.])
3370         Dimensions without coordinates: x
3371         """
3372 
3373         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
3374         return self._from_temp_dataset(ds)
3375 
3376     def differentiate(
3377         self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None
3378     ) -> "DataArray":
3379         """ Differentiate the array with the second order accurate central
3380         differences.
3381 
3382         .. note::
3383             This feature is limited to simple cartesian geometry, i.e. coord
3384             must be one dimensional.
3385 
3386         Parameters
3387         ----------
3388         coord : hashable
3389             The coordinate to be used to compute the gradient.
3390         edge_order : {1, 2}, default: 1
3391             N-th order accurate differences at the boundaries.
3392         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", \
3393                          "us", "ns", "ps", "fs", "as"} or None, optional
3394             Unit to compute gradient. Only valid for datetime coordinate.
3395 
3396         Returns
3397         -------
3398         differentiated: DataArray
3399 
3400         See also
3401         --------
3402         numpy.gradient: corresponding numpy function
3403 
3404         Examples
3405         --------
3406 
3407         >>> da = xr.DataArray(
3408         ...     np.arange(12).reshape(4, 3),
3409         ...     dims=["x", "y"],
3410         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3411         ... )
3412         >>> da
3413         <xarray.DataArray (x: 4, y: 3)>
3414         array([[ 0,  1,  2],
3415                [ 3,  4,  5],
3416                [ 6,  7,  8],
3417                [ 9, 10, 11]])
3418         Coordinates:
3419           * x        (x) float64 0.0 0.1 1.1 1.2
3420         Dimensions without coordinates: y
3421         >>>
3422         >>> da.differentiate("x")
3423         <xarray.DataArray (x: 4, y: 3)>
3424         array([[30.        , 30.        , 30.        ],
3425                [27.54545455, 27.54545455, 27.54545455],
3426                [27.54545455, 27.54545455, 27.54545455],
3427                [30.        , 30.        , 30.        ]])
3428         Coordinates:
3429           * x        (x) float64 0.0 0.1 1.1 1.2
3430         Dimensions without coordinates: y
3431         """
3432         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
3433         return self._from_temp_dataset(ds)
3434 
3435     def integrate(
3436         self, dim: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None
3437     ) -> "DataArray":
3438         """ integrate the array with the trapezoidal rule.
3439 
3440         .. note::
3441             This feature is limited to simple cartesian geometry, i.e. dim
3442             must be one dimensional.
3443 
3444         Parameters
3445         ----------
3446         dim : hashable, or sequence of hashable
3447             Coordinate(s) used for the integration.
3448         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", "us", "ns", \
3449                          "ps", "fs", "as"}, optional
3450             Can be used to specify the unit if datetime coordinate is used.
3451 
3452         Returns
3453         -------
3454         integrated: DataArray
3455 
3456         See also
3457         --------
3458         numpy.trapz: corresponding numpy function
3459 
3460         Examples
3461         --------
3462 
3463         >>> da = xr.DataArray(
3464         ...     np.arange(12).reshape(4, 3),
3465         ...     dims=["x", "y"],
3466         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3467         ... )
3468         >>> da
3469         <xarray.DataArray (x: 4, y: 3)>
3470         array([[ 0,  1,  2],
3471                [ 3,  4,  5],
3472                [ 6,  7,  8],
3473                [ 9, 10, 11]])
3474         Coordinates:
3475           * x        (x) float64 0.0 0.1 1.1 1.2
3476         Dimensions without coordinates: y
3477         >>>
3478         >>> da.integrate("x")
3479         <xarray.DataArray (y: 3)>
3480         array([5.4, 6.6, 7.8])
3481         Dimensions without coordinates: y
3482         """
3483         ds = self._to_temp_dataset().integrate(dim, datetime_unit)
3484         return self._from_temp_dataset(ds)
3485 
3486     def unify_chunks(self) -> "DataArray":
3487         """Unify chunk size along all chunked dimensions of this DataArray.
3488 
3489         Returns
3490         -------
3491 
3492         DataArray with consistent chunk sizes for all dask-array variables
3493 
3494         See Also
3495         --------
3496 
3497         dask.array.core.unify_chunks
3498         """
3499         ds = self._to_temp_dataset().unify_chunks()
3500         return self._from_temp_dataset(ds)
3501 
3502     def map_blocks(
3503         self,
3504         func: "Callable[..., T_DSorDA]",
3505         args: Sequence[Any] = (),
3506         kwargs: Mapping[str, Any] = None,
3507         template: Union["DataArray", "Dataset"] = None,
3508     ) -> "T_DSorDA":
3509         """
3510         Apply a function to each block of this DataArray.
3511 
3512         .. warning::
3513             This method is experimental and its signature may change.
3514 
3515         Parameters
3516         ----------
3517         func : callable
3518             User-provided function that accepts a DataArray as its first
3519             parameter. The function will receive a subset or 'block' of this DataArray (see below),
3520             corresponding to one chunk along each chunked dimension. ``func`` will be
3521             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.
3522 
3523             This function must return either a single DataArray or a single Dataset.
3524 
3525             This function cannot add a new chunked dimension.
3526         args : sequence
3527             Passed to func after unpacking and subsetting any xarray objects by blocks.
3528             xarray objects in args must be aligned with this object, otherwise an error is raised.
3529         kwargs : mapping
3530             Passed verbatim to func after unpacking. xarray objects, if any, will not be
3531             subset to blocks. Passing dask collections in kwargs is not allowed.
3532         template : DataArray or Dataset, optional
3533             xarray object representing the final result after compute is called. If not provided,
3534             the function will be first run on mocked-up data, that looks like this object but
3535             has sizes 0, to determine properties of the returned object such as dtype,
3536             variable names, attributes, new dimensions and new indexes (if any).
3537             ``template`` must be provided if the function changes the size of existing dimensions.
3538             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
3539             ``attrs`` set by ``func`` will be ignored.
3540 
3541         Returns
3542         -------
3543         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
3544         function.
3545 
3546         Notes
3547         -----
3548         This function is designed for when ``func`` needs to manipulate a whole xarray object
3549         subset to each block. In the more common case where ``func`` can work on numpy arrays, it is
3550         recommended to use ``apply_ufunc``.
3551 
3552         If none of the variables in this object is backed by dask arrays, calling this function is
3553         equivalent to calling ``func(obj, *args, **kwargs)``.
3554 
3555         See Also
3556         --------
3557         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks,
3558         xarray.DataArray.map_blocks
3559 
3560         Examples
3561         --------
3562 
3563         Calculate an anomaly from climatology using ``.groupby()``. Using
3564         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
3565         its indices, and its methods like ``.groupby()``.
3566 
3567         >>> def calculate_anomaly(da, groupby_type="time.month"):
3568         ...     gb = da.groupby(groupby_type)
3569         ...     clim = gb.mean(dim="time")
3570         ...     return gb - clim
3571         ...
3572         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
3573         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
3574         >>> np.random.seed(123)
3575         >>> array = xr.DataArray(
3576         ...     np.random.rand(len(time)),
3577         ...     dims=["time"],
3578         ...     coords={"time": time, "month": month},
3579         ... ).chunk()
3580         >>> array.map_blocks(calculate_anomaly, template=array).compute()
3581         <xarray.DataArray (time: 24)>
3582         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,
3583                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,
3584                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,
3585                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,
3586                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])
3587         Coordinates:
3588           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
3589             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
3590 
3591         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
3592         to the function being applied in ``xr.map_blocks()``:
3593 
3594         >>> array.map_blocks(
3595         ...     calculate_anomaly, kwargs={"groupby_type": "time.year"}, template=array
3596         ... )  # doctest: +ELLIPSIS
3597         <xarray.DataArray (time: 24)>
3598         dask.array<calculate_anomaly-...-<this, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>
3599         Coordinates:
3600           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
3601             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
3602         """
3603         from .parallel import map_blocks
3604 
3605         return map_blocks(func, self, args, kwargs, template)
3606 
3607     def polyfit(
3608         self,
3609         dim: Hashable,
3610         deg: int,
3611         skipna: bool = None,
3612         rcond: float = None,
3613         w: Union[Hashable, Any] = None,
3614         full: bool = False,
3615         cov: bool = False,
3616     ):
3617         """
3618         Least squares polynomial fit.
3619 
3620         This replicates the behaviour of `numpy.polyfit` but differs by skipping
3621         invalid values when `skipna = True`.
3622 
3623         Parameters
3624         ----------
3625         dim : hashable
3626             Coordinate along which to fit the polynomials.
3627         deg : int
3628             Degree of the fitting polynomial.
3629         skipna : bool, optional
3630             If True, removes all invalid values before fitting each 1D slices of the array.
3631             Default is True if data is stored in a dask.array or if there is any
3632             invalid values, False otherwise.
3633         rcond : float, optional
3634             Relative condition number to the fit.
3635         w : hashable or array-like, optional
3636             Weights to apply to the y-coordinate of the sample points.
3637             Can be an array-like object or the name of a coordinate in the dataset.
3638         full : bool, optional
3639             Whether to return the residuals, matrix rank and singular values in addition
3640             to the coefficients.
3641         cov : bool or str, optional
3642             Whether to return to the covariance matrix in addition to the coefficients.
3643             The matrix is not scaled if `cov='unscaled'`.
3644 
3645         Returns
3646         -------
3647         polyfit_results : Dataset
3648             A single dataset which contains:
3649 
3650             polyfit_coefficients
3651                 The coefficients of the best fit.
3652             polyfit_residuals
3653                 The residuals of the least-square computation (only included if `full=True`).
3654                 When the matrix rank is deficient, np.nan is returned.
3655             [dim]_matrix_rank
3656                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
3657             [dim]_singular_value
3658                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
3659             polyfit_covariance
3660                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
3661 
3662         See also
3663         --------
3664         numpy.polyfit
3665         """
3666         return self._to_temp_dataset().polyfit(
3667             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov
3668         )
3669 
3670     def pad(
3671         self,
3672         pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,
3673         mode: str = "constant",
3674         stat_length: Union[
3675             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3676         ] = None,
3677         constant_values: Union[
3678             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3679         ] = None,
3680         end_values: Union[
3681             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3682         ] = None,
3683         reflect_type: str = None,
3684         **pad_width_kwargs: Any,
3685     ) -> "DataArray":
3686         """Pad this array along one or more dimensions.
3687 
3688         .. warning::
3689             This function is experimental and its behaviour is likely to change
3690             especially regarding padding of dimension coordinates (or IndexVariables).
3691 
3692         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
3693         coordinates will be padded with the same mode, otherwise coordinates
3694         are padded using the "constant" mode with fill_value dtypes.NA.
3695 
3696         Parameters
3697         ----------
3698         pad_width : mapping of hashable to tuple of int
3699             Mapping with the form of {dim: (pad_before, pad_after)}
3700             describing the number of values padded along each dimension.
3701             {dim: pad} is a shortcut for pad_before = pad_after = pad
3702         mode : str, default: "constant"
3703             One of the following string values (taken from numpy docs)
3704 
3705             'constant' (default)
3706                 Pads with a constant value.
3707             'edge'
3708                 Pads with the edge values of array.
3709             'linear_ramp'
3710                 Pads with the linear ramp between end_value and the
3711                 array edge value.
3712             'maximum'
3713                 Pads with the maximum value of all or part of the
3714                 vector along each axis.
3715             'mean'
3716                 Pads with the mean value of all or part of the
3717                 vector along each axis.
3718             'median'
3719                 Pads with the median value of all or part of the
3720                 vector along each axis.
3721             'minimum'
3722                 Pads with the minimum value of all or part of the
3723                 vector along each axis.
3724             'reflect'
3725                 Pads with the reflection of the vector mirrored on
3726                 the first and last values of the vector along each
3727                 axis.
3728             'symmetric'
3729                 Pads with the reflection of the vector mirrored
3730                 along the edge of the array.
3731             'wrap'
3732                 Pads with the wrap of the vector along the axis.
3733                 The first values are used to pad the end and the
3734                 end values are used to pad the beginning.
3735         stat_length : int, tuple or mapping of hashable to tuple, default: None
3736             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
3737             values at edge of each axis used to calculate the statistic value.
3738             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
3739             statistic lengths along each dimension.
3740             ((before, after),) yields same before and after statistic lengths
3741             for each dimension.
3742             (stat_length,) or int is a shortcut for before = after = statistic
3743             length for all axes.
3744             Default is ``None``, to use the entire axis.
3745         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
3746             Used in 'constant'.  The values to set the padded values for each
3747             axis.
3748             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
3749             pad constants along each dimension.
3750             ``((before, after),)`` yields same before and after constants for each
3751             dimension.
3752             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
3753             all dimensions.
3754             Default is 0.
3755         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
3756             Used in 'linear_ramp'.  The values used for the ending value of the
3757             linear_ramp and that will form the edge of the padded array.
3758             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
3759             end values along each dimension.
3760             ``((before, after),)`` yields same before and after end values for each
3761             axis.
3762             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
3763             all axes.
3764             Default is 0.
3765         reflect_type : {"even", "odd"}, optional
3766             Used in "reflect", and "symmetric".  The "even" style is the
3767             default with an unaltered reflection around the edge value.  For
3768             the "odd" style, the extended part of the array is created by
3769             subtracting the reflected values from two times the edge value.
3770         **pad_width_kwargs
3771             The keyword arguments form of ``pad_width``.
3772             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
3773 
3774         Returns
3775         -------
3776         padded : DataArray
3777             DataArray with the padded coordinates and data.
3778 
3779         See also
3780         --------
3781         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad
3782 
3783         Notes
3784         -----
3785         By default when ``mode="constant"`` and ``constant_values=None``, integer types will be
3786         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion
3787         specify ``constant_values=np.nan``
3788 
3789         Examples
3790         --------
3791 
3792         >>> arr = xr.DataArray([5, 6, 7], coords=[("x", [0, 1, 2])])
3793         >>> arr.pad(x=(1, 2), constant_values=0)
3794         <xarray.DataArray (x: 6)>
3795         array([0, 5, 6, 7, 0, 0])
3796         Coordinates:
3797           * x        (x) float64 nan 0.0 1.0 2.0 nan nan
3798 
3799         >>> da = xr.DataArray(
3800         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],
3801         ...     dims=["x", "y"],
3802         ...     coords={"x": [0, 1], "y": [10, 20, 30, 40], "z": ("x", [100, 200])},
3803         ... )
3804         >>> da.pad(x=1)
3805         <xarray.DataArray (x: 4, y: 4)>
3806         array([[nan, nan, nan, nan],
3807                [ 0.,  1.,  2.,  3.],
3808                [10., 11., 12., 13.],
3809                [nan, nan, nan, nan]])
3810         Coordinates:
3811           * x        (x) float64 nan 0.0 1.0 nan
3812           * y        (y) int64 10 20 30 40
3813             z        (x) float64 nan 100.0 200.0 nan
3814         >>> da.pad(x=1, constant_values=np.nan)
3815         <xarray.DataArray (x: 4, y: 4)>
3816         array([[-9223372036854775808, -9223372036854775808, -9223372036854775808,
3817                 -9223372036854775808],
3818                [                   0,                    1,                    2,
3819                                    3],
3820                [                  10,                   11,                   12,
3821                                   13],
3822                [-9223372036854775808, -9223372036854775808, -9223372036854775808,
3823                 -9223372036854775808]])
3824         Coordinates:
3825           * x        (x) float64 nan 0.0 1.0 nan
3826           * y        (y) int64 10 20 30 40
3827             z        (x) float64 nan 100.0 200.0 nan
3828         """
3829         ds = self._to_temp_dataset().pad(
3830             pad_width=pad_width,
3831             mode=mode,
3832             stat_length=stat_length,
3833             constant_values=constant_values,
3834             end_values=end_values,
3835             reflect_type=reflect_type,
3836             **pad_width_kwargs,
3837         )
3838         return self._from_temp_dataset(ds)
3839 
3840     def idxmin(
3841         self,
3842         dim: Hashable = None,
3843         skipna: bool = None,
3844         fill_value: Any = dtypes.NA,
3845         keep_attrs: bool = None,
3846     ) -> "DataArray":
3847         """Return the coordinate label of the minimum value along a dimension.
3848 
3849         Returns a new `DataArray` named after the dimension with the values of
3850         the coordinate labels along that dimension corresponding to minimum
3851         values along that dimension.
3852 
3853         In comparison to :py:meth:`~DataArray.argmin`, this returns the
3854         coordinate label while :py:meth:`~DataArray.argmin` returns the index.
3855 
3856         Parameters
3857         ----------
3858         dim : str, optional
3859             Dimension over which to apply `idxmin`.  This is optional for 1D
3860             arrays, but required for arrays with 2 or more dimensions.
3861         skipna : bool or None, default: None
3862             If True, skip missing values (as marked by NaN). By default, only
3863             skips missing values for ``float``, ``complex``, and ``object``
3864             dtypes; other dtypes either do not have a sentinel missing value
3865             (``int``) or ``skipna=True`` has not been implemented
3866             (``datetime64`` or ``timedelta64``).
3867         fill_value : Any, default: NaN
3868             Value to be filled in case all of the values along a dimension are
3869             null.  By default this is NaN.  The fill value and result are
3870             automatically converted to a compatible dtype if possible.
3871             Ignored if ``skipna`` is False.
3872         keep_attrs : bool, default: False
3873             If True, the attributes (``attrs``) will be copied from the
3874             original object to the new one.  If False (default), the new object
3875             will be returned without attributes.
3876 
3877         Returns
3878         -------
3879         reduced : DataArray
3880             New `DataArray` object with `idxmin` applied to its data and the
3881             indicated dimension removed.
3882 
3883         See also
3884         --------
3885         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin
3886 
3887         Examples
3888         --------
3889 
3890         >>> array = xr.DataArray(
3891         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
3892         ... )
3893         >>> array.min()
3894         <xarray.DataArray ()>
3895         array(-2)
3896         >>> array.argmin()
3897         <xarray.DataArray ()>
3898         array(4)
3899         >>> array.idxmin()
3900         <xarray.DataArray 'x' ()>
3901         array('e', dtype='<U1')
3902 
3903         >>> array = xr.DataArray(
3904         ...     [
3905         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
3906         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
3907         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
3908         ...     ],
3909         ...     dims=["y", "x"],
3910         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
3911         ... )
3912         >>> array.min(dim="x")
3913         <xarray.DataArray (y: 3)>
3914         array([-2., -4.,  1.])
3915         Coordinates:
3916           * y        (y) int64 -1 0 1
3917         >>> array.argmin(dim="x")
3918         <xarray.DataArray (y: 3)>
3919         array([4, 0, 2])
3920         Coordinates:
3921           * y        (y) int64 -1 0 1
3922         >>> array.idxmin(dim="x")
3923         <xarray.DataArray 'x' (y: 3)>
3924         array([16.,  0.,  4.])
3925         Coordinates:
3926           * y        (y) int64 -1 0 1
3927         """
3928         return computation._calc_idxminmax(
3929             array=self,
3930             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),
3931             dim=dim,
3932             skipna=skipna,
3933             fill_value=fill_value,
3934             keep_attrs=keep_attrs,
3935         )
3936 
3937     def idxmax(
3938         self,
3939         dim: Hashable = None,
3940         skipna: bool = None,
3941         fill_value: Any = dtypes.NA,
3942         keep_attrs: bool = None,
3943     ) -> "DataArray":
3944         """Return the coordinate label of the maximum value along a dimension.
3945 
3946         Returns a new `DataArray` named after the dimension with the values of
3947         the coordinate labels along that dimension corresponding to maximum
3948         values along that dimension.
3949 
3950         In comparison to :py:meth:`~DataArray.argmax`, this returns the
3951         coordinate label while :py:meth:`~DataArray.argmax` returns the index.
3952 
3953         Parameters
3954         ----------
3955         dim : hashable, optional
3956             Dimension over which to apply `idxmax`.  This is optional for 1D
3957             arrays, but required for arrays with 2 or more dimensions.
3958         skipna : bool or None, default: None
3959             If True, skip missing values (as marked by NaN). By default, only
3960             skips missing values for ``float``, ``complex``, and ``object``
3961             dtypes; other dtypes either do not have a sentinel missing value
3962             (``int``) or ``skipna=True`` has not been implemented
3963             (``datetime64`` or ``timedelta64``).
3964         fill_value : Any, default: NaN
3965             Value to be filled in case all of the values along a dimension are
3966             null.  By default this is NaN.  The fill value and result are
3967             automatically converted to a compatible dtype if possible.
3968             Ignored if ``skipna`` is False.
3969         keep_attrs : bool, default: False
3970             If True, the attributes (``attrs``) will be copied from the
3971             original object to the new one.  If False (default), the new object
3972             will be returned without attributes.
3973 
3974         Returns
3975         -------
3976         reduced : DataArray
3977             New `DataArray` object with `idxmax` applied to its data and the
3978             indicated dimension removed.
3979 
3980         See also
3981         --------
3982         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax
3983 
3984         Examples
3985         --------
3986 
3987         >>> array = xr.DataArray(
3988         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
3989         ... )
3990         >>> array.max()
3991         <xarray.DataArray ()>
3992         array(2)
3993         >>> array.argmax()
3994         <xarray.DataArray ()>
3995         array(1)
3996         >>> array.idxmax()
3997         <xarray.DataArray 'x' ()>
3998         array('b', dtype='<U1')
3999 
4000         >>> array = xr.DataArray(
4001         ...     [
4002         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
4003         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
4004         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
4005         ...     ],
4006         ...     dims=["y", "x"],
4007         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
4008         ... )
4009         >>> array.max(dim="x")
4010         <xarray.DataArray (y: 3)>
4011         array([2., 2., 1.])
4012         Coordinates:
4013           * y        (y) int64 -1 0 1
4014         >>> array.argmax(dim="x")
4015         <xarray.DataArray (y: 3)>
4016         array([0, 2, 2])
4017         Coordinates:
4018           * y        (y) int64 -1 0 1
4019         >>> array.idxmax(dim="x")
4020         <xarray.DataArray 'x' (y: 3)>
4021         array([0., 4., 4.])
4022         Coordinates:
4023           * y        (y) int64 -1 0 1
4024         """
4025         return computation._calc_idxminmax(
4026             array=self,
4027             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),
4028             dim=dim,
4029             skipna=skipna,
4030             fill_value=fill_value,
4031             keep_attrs=keep_attrs,
4032         )
4033 
4034     def argmin(
4035         self,
4036         dim: Union[Hashable, Sequence[Hashable]] = None,
4037         axis: int = None,
4038         keep_attrs: bool = None,
4039         skipna: bool = None,
4040     ) -> Union["DataArray", Dict[Hashable, "DataArray"]]:
4041         """Index or indices of the minimum of the DataArray over one or more dimensions.
4042 
4043         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4044         which can be passed directly to isel(). If a single str is passed to 'dim' then
4045         returns a DataArray with dtype int.
4046 
4047         If there are multiple minima, the indices of the first one found will be
4048         returned.
4049 
4050         Parameters
4051         ----------
4052         dim : hashable, sequence of hashable or ..., optional
4053             The dimensions over which to find the minimum. By default, finds minimum over
4054             all dimensions - for now returning an int for backward compatibility, but
4055             this is deprecated, in future will return a dict with indices for all
4056             dimensions; to return a dict with all dimensions now, pass '...'.
4057         axis : int, optional
4058             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
4059             can be supplied.
4060         keep_attrs : bool, optional
4061             If True, the attributes (`attrs`) will be copied from the original
4062             object to the new one.  If False (default), the new object will be
4063             returned without attributes.
4064         skipna : bool, optional
4065             If True, skip missing values (as marked by NaN). By default, only
4066             skips missing values for float dtypes; other dtypes either do not
4067             have a sentinel missing value (int) or skipna=True has not been
4068             implemented (object, datetime64 or timedelta64).
4069 
4070         Returns
4071         -------
4072         result : DataArray or dict of DataArray
4073 
4074         See also
4075         --------
4076         Variable.argmin, DataArray.idxmin
4077 
4078         Examples
4079         --------
4080         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4081         >>> array.min()
4082         <xarray.DataArray ()>
4083         array(-1)
4084         >>> array.argmin()
4085         <xarray.DataArray ()>
4086         array(2)
4087         >>> array.argmin(...)
4088         {'x': <xarray.DataArray ()>
4089         array(2)}
4090         >>> array.isel(array.argmin(...))
4091         <xarray.DataArray ()>
4092         array(-1)
4093 
4094         >>> array = xr.DataArray(
4095         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],
4096         ...     dims=("x", "y", "z"),
4097         ... )
4098         >>> array.min(dim="x")
4099         <xarray.DataArray (y: 3, z: 3)>
4100         array([[ 1,  2,  1],
4101                [ 2, -5,  1],
4102                [ 2,  1,  1]])
4103         Dimensions without coordinates: y, z
4104         >>> array.argmin(dim="x")
4105         <xarray.DataArray (y: 3, z: 3)>
4106         array([[1, 0, 0],
4107                [1, 1, 1],
4108                [0, 0, 1]])
4109         Dimensions without coordinates: y, z
4110         >>> array.argmin(dim=["x"])
4111         {'x': <xarray.DataArray (y: 3, z: 3)>
4112         array([[1, 0, 0],
4113                [1, 1, 1],
4114                [0, 0, 1]])
4115         Dimensions without coordinates: y, z}
4116         >>> array.min(dim=("x", "z"))
4117         <xarray.DataArray (y: 3)>
4118         array([ 1, -5,  1])
4119         Dimensions without coordinates: y
4120         >>> array.argmin(dim=["x", "z"])
4121         {'x': <xarray.DataArray (y: 3)>
4122         array([0, 1, 0])
4123         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4124         array([2, 1, 1])
4125         Dimensions without coordinates: y}
4126         >>> array.isel(array.argmin(dim=["x", "z"]))
4127         <xarray.DataArray (y: 3)>
4128         array([ 1, -5,  1])
4129         Dimensions without coordinates: y
4130         """
4131         result = self.variable.argmin(dim, axis, keep_attrs, skipna)
4132         if isinstance(result, dict):
4133             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4134         else:
4135             return self._replace_maybe_drop_dims(result)
4136 
4137     def argmax(
4138         self,
4139         dim: Union[Hashable, Sequence[Hashable]] = None,
4140         axis: int = None,
4141         keep_attrs: bool = None,
4142         skipna: bool = None,
4143     ) -> Union["DataArray", Dict[Hashable, "DataArray"]]:
4144         """Index or indices of the maximum of the DataArray over one or more dimensions.
4145 
4146         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4147         which can be passed directly to isel(). If a single str is passed to 'dim' then
4148         returns a DataArray with dtype int.
4149 
4150         If there are multiple maxima, the indices of the first one found will be
4151         returned.
4152 
4153         Parameters
4154         ----------
4155         dim : hashable, sequence of hashable or ..., optional
4156             The dimensions over which to find the maximum. By default, finds maximum over
4157             all dimensions - for now returning an int for backward compatibility, but
4158             this is deprecated, in future will return a dict with indices for all
4159             dimensions; to return a dict with all dimensions now, pass '...'.
4160         axis : int, optional
4161             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
4162             can be supplied.
4163         keep_attrs : bool, optional
4164             If True, the attributes (`attrs`) will be copied from the original
4165             object to the new one.  If False (default), the new object will be
4166             returned without attributes.
4167         skipna : bool, optional
4168             If True, skip missing values (as marked by NaN). By default, only
4169             skips missing values for float dtypes; other dtypes either do not
4170             have a sentinel missing value (int) or skipna=True has not been
4171             implemented (object, datetime64 or timedelta64).
4172 
4173         Returns
4174         -------
4175         result : DataArray or dict of DataArray
4176 
4177         See also
4178         --------
4179         Variable.argmax, DataArray.idxmax
4180 
4181         Examples
4182         --------
4183         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4184         >>> array.max()
4185         <xarray.DataArray ()>
4186         array(3)
4187         >>> array.argmax()
4188         <xarray.DataArray ()>
4189         array(3)
4190         >>> array.argmax(...)
4191         {'x': <xarray.DataArray ()>
4192         array(3)}
4193         >>> array.isel(array.argmax(...))
4194         <xarray.DataArray ()>
4195         array(3)
4196 
4197         >>> array = xr.DataArray(
4198         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],
4199         ...     dims=("x", "y", "z"),
4200         ... )
4201         >>> array.max(dim="x")
4202         <xarray.DataArray (y: 3, z: 3)>
4203         array([[3, 3, 2],
4204                [3, 5, 2],
4205                [2, 3, 3]])
4206         Dimensions without coordinates: y, z
4207         >>> array.argmax(dim="x")
4208         <xarray.DataArray (y: 3, z: 3)>
4209         array([[0, 1, 1],
4210                [0, 1, 0],
4211                [0, 1, 0]])
4212         Dimensions without coordinates: y, z
4213         >>> array.argmax(dim=["x"])
4214         {'x': <xarray.DataArray (y: 3, z: 3)>
4215         array([[0, 1, 1],
4216                [0, 1, 0],
4217                [0, 1, 0]])
4218         Dimensions without coordinates: y, z}
4219         >>> array.max(dim=("x", "z"))
4220         <xarray.DataArray (y: 3)>
4221         array([3, 5, 3])
4222         Dimensions without coordinates: y
4223         >>> array.argmax(dim=["x", "z"])
4224         {'x': <xarray.DataArray (y: 3)>
4225         array([0, 1, 0])
4226         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4227         array([0, 1, 2])
4228         Dimensions without coordinates: y}
4229         >>> array.isel(array.argmax(dim=["x", "z"]))
4230         <xarray.DataArray (y: 3)>
4231         array([3, 5, 3])
4232         Dimensions without coordinates: y
4233         """
4234         result = self.variable.argmax(dim, axis, keep_attrs, skipna)
4235         if isinstance(result, dict):
4236             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4237         else:
4238             return self._replace_maybe_drop_dims(result)
4239 
4240     # this needs to be at the end, or mypy will confuse with `str`
4241     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
4242     str = utils.UncachedAccessor(StringAccessor)

</code><intended_behavior>The `astype` method should allow for type conversion of the data contained within a `DataArray` instance, mirroring the functionality seen in NumPy arrays. This method should include the `order` parameter to control the memory layout of the result. The intended fix would involve reinstating this parameter if it was removed, or providing a compatible interface that re-enables control over the memory layout as part of type conversion, ensuring consistency with NumPy and preserving existing functionality.</intended_behavior>]
2025-10-18 15:27:33.270 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 15:27:33.270 | INFO     | Search-only: skipping patch selection
2025-10-18 15:27:33.335 | INFO     | Task pydata__xarray-4683 completed successfully.
2025-10-18 15:27:33.335 | INFO     | No patch generated. You can try running ACR again.
