[
    {
        "rel_file_path": "sklearn/calibration.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/calibration.py",
        "start": 114,
        "end": 201,
        "class_name": "CalibratedClassifierCV",
        "method_name": "fit",
        "code": "114     def fit(self, X, y, sample_weight=None):\n115         \"\"\"Fit the calibrated model\n116 \n117         Parameters\n118         ----------\n119         X : array-like, shape (n_samples, n_features)\n120             Training data.\n121 \n122         y : array-like, shape (n_samples,)\n123             Target values.\n124 \n125         sample_weight : array-like of shape (n_samples,), default=None\n126             Sample weights. If None, then samples are equally weighted.\n127 \n128         Returns\n129         -------\n130         self : object\n131             Returns an instance of self.\n132         \"\"\"\n133         X, y = check_X_y(X, y, accept_sparse=['csc', 'csr', 'coo'],\n134                          force_all_finite=False, allow_nd=True)\n135         X, y = indexable(X, y)\n136         le = LabelBinarizer().fit(y)\n137         self.classes_ = le.classes_\n138 \n139         # Check that each cross-validation fold can have at least one\n140         # example per class\n141         n_folds = self.cv if isinstance(self.cv, int) \\\n142             else self.cv.n_folds if hasattr(self.cv, \"n_folds\") else None\n143         if n_folds and \\\n144                 np.any([np.sum(y == class_) < n_folds for class_ in\n145                         self.classes_]):\n146             raise ValueError(\"Requesting %d-fold cross-validation but provided\"\n147                              \" less than %d examples for at least one class.\"\n148                              % (n_folds, n_folds))\n149 \n150         self.calibrated_classifiers_ = []\n151         if self.base_estimator is None:\n152             # we want all classifiers that don't expose a random_state\n153             # to be deterministic (and we don't want to expose this one).\n154             base_estimator = LinearSVC(random_state=0)\n155         else:\n156             base_estimator = self.base_estimator\n157 \n158         if self.cv == \"prefit\":\n159             calibrated_classifier = _CalibratedClassifier(\n160                 base_estimator, method=self.method)\n161             if sample_weight is not None:\n162                 calibrated_classifier.fit(X, y, sample_weight)\n163             else:\n164                 calibrated_classifier.fit(X, y)\n165             self.calibrated_classifiers_.append(calibrated_classifier)\n166         else:\n167             cv = check_cv(self.cv, y, classifier=True)\n168             fit_parameters = signature(base_estimator.fit).parameters\n169             estimator_name = type(base_estimator).__name__\n170             if (sample_weight is not None\n171                     and \"sample_weight\" not in fit_parameters):\n172                 warnings.warn(\"%s does not support sample_weight. Samples\"\n173                               \" weights are only used for the calibration\"\n174                               \" itself.\" % estimator_name)\n175                 sample_weight = check_array(sample_weight, ensure_2d=False)\n176                 base_estimator_sample_weight = None\n177             else:\n178                 if sample_weight is not None:\n179                     sample_weight = check_array(sample_weight, ensure_2d=False)\n180                     check_consistent_length(y, sample_weight)\n181                 base_estimator_sample_weight = sample_weight\n182             for train, test in cv.split(X, y):\n183                 this_estimator = clone(base_estimator)\n184                 if base_estimator_sample_weight is not None:\n185                     this_estimator.fit(\n186                         X[train], y[train],\n187                         sample_weight=base_estimator_sample_weight[train])\n188                 else:\n189                     this_estimator.fit(X[train], y[train])\n190 \n191                 calibrated_classifier = _CalibratedClassifier(\n192                     this_estimator, method=self.method,\n193                     classes=self.classes_)\n194                 if sample_weight is not None:\n195                     calibrated_classifier.fit(X[test], y[test],\n196                                               sample_weight[test])\n197                 else:\n198                     calibrated_classifier.fit(X[test], y[test])\n199                 self.calibrated_classifiers_.append(calibrated_classifier)\n200 \n201         return self\n",
        "intended_behavior": "The fit method should utilize _check_sample_weight to validate and handle sample_weight efficiently and consistently with the rest of the sklearn library, ensuring any custom or ad-hoc handling is replaced or integrated with the standard utility method."
    },
    {
        "rel_file_path": "sklearn/calibration.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/calibration.py",
        "start": 31,
        "end": 248,
        "class_name": "CalibratedClassifierCV",
        "method_name": null,
        "code": "31 class CalibratedClassifierCV(BaseEstimator, ClassifierMixin,\n32                              MetaEstimatorMixin):\n33     \"\"\"Probability calibration with isotonic regression or sigmoid.\n34 \n35     See glossary entry for :term:`cross-validation estimator`.\n36 \n37     With this class, the base_estimator is fit on the train set of the\n38     cross-validation generator and the test set is used for calibration.\n39     The probabilities for each of the folds are then averaged\n40     for prediction. In case that cv=\"prefit\" is passed to __init__,\n41     it is assumed that base_estimator has been fitted already and all\n42     data is used for calibration. Note that data for fitting the\n43     classifier and for calibrating it must be disjoint.\n44 \n45     Read more in the :ref:`User Guide <calibration>`.\n46 \n47     Parameters\n48     ----------\n49     base_estimator : instance BaseEstimator\n50         The classifier whose output decision function needs to be calibrated\n51         to offer more accurate predict_proba outputs. If cv=prefit, the\n52         classifier must have been fit already on data.\n53 \n54     method : 'sigmoid' or 'isotonic'\n55         The method to use for calibration. Can be 'sigmoid' which\n56         corresponds to Platt's method or 'isotonic' which is a\n57         non-parametric approach. It is not advised to use isotonic calibration\n58         with too few calibration samples ``(<<1000)`` since it tends to\n59         overfit.\n60         Use sigmoids (Platt's calibration) in this case.\n61 \n62     cv : integer, cross-validation generator, iterable or \"prefit\", optional\n63         Determines the cross-validation splitting strategy.\n64         Possible inputs for cv are:\n65 \n66         - None, to use the default 5-fold cross-validation,\n67         - integer, to specify the number of folds.\n68         - :term:`CV splitter`,\n69         - An iterable yielding (train, test) splits as arrays of indices.\n70 \n71         For integer/None inputs, if ``y`` is binary or multiclass,\n72         :class:`sklearn.model_selection.StratifiedKFold` is used. If ``y`` is\n73         neither binary nor multiclass, :class:`sklearn.model_selection.KFold`\n74         is used.\n75 \n76         Refer :ref:`User Guide <cross_validation>` for the various\n77         cross-validation strategies that can be used here.\n78 \n79         If \"prefit\" is passed, it is assumed that base_estimator has been\n80         fitted already and all data is used for calibration.\n81 \n82         .. versionchanged:: 0.22\n83             ``cv`` default value if None changed from 3-fold to 5-fold.\n84 \n85     Attributes\n86     ----------\n87     classes_ : array, shape (n_classes)\n88         The class labels.\n89 \n90     calibrated_classifiers_ : list (len() equal to cv or 1 if cv == \"prefit\")\n91         The list of calibrated classifiers, one for each crossvalidation fold,\n92         which has been fitted on all but the validation fold and calibrated\n93         on the validation fold.\n94 \n95     References\n96     ----------\n97     .. [1] Obtaining calibrated probability estimates from decision trees\n98            and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001\n99 \n100     .. [2] Transforming Classifier Scores into Accurate Multiclass\n101            Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)\n102 \n103     .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to\n104            Regularized Likelihood Methods, J. Platt, (1999)\n105 \n106     .. [4] Predicting Good Probabilities with Supervised Learning,\n107            A. Niculescu-Mizil & R. Caruana, ICML 2005\n108     \"\"\"\n109     def __init__(self, base_estimator=None, method='sigmoid', cv=None):\n110         self.base_estimator = base_estimator\n111         self.method = method\n112         self.cv = cv\n113 \n114     def fit(self, X, y, sample_weight=None):\n115         \"\"\"Fit the calibrated model\n116 \n117         Parameters\n118         ----------\n119         X : array-like, shape (n_samples, n_features)\n120             Training data.\n121 \n122         y : array-like, shape (n_samples,)\n123             Target values.\n124 \n125         sample_weight : array-like of shape (n_samples,), default=None\n126             Sample weights. If None, then samples are equally weighted.\n127 \n128         Returns\n129         -------\n130         self : object\n131             Returns an instance of self.\n132         \"\"\"\n133         X, y = check_X_y(X, y, accept_sparse=['csc', 'csr', 'coo'],\n134                          force_all_finite=False, allow_nd=True)\n135         X, y = indexable(X, y)\n136         le = LabelBinarizer().fit(y)\n137         self.classes_ = le.classes_\n138 \n139         # Check that each cross-validation fold can have at least one\n140         # example per class\n141         n_folds = self.cv if isinstance(self.cv, int) \\\n142             else self.cv.n_folds if hasattr(self.cv, \"n_folds\") else None\n143         if n_folds and \\\n144                 np.any([np.sum(y == class_) < n_folds for class_ in\n145                         self.classes_]):\n146             raise ValueError(\"Requesting %d-fold cross-validation but provided\"\n147                              \" less than %d examples for at least one class.\"\n148                              % (n_folds, n_folds))\n149 \n150         self.calibrated_classifiers_ = []\n151         if self.base_estimator is None:\n152             # we want all classifiers that don't expose a random_state\n153             # to be deterministic (and we don't want to expose this one).\n154             base_estimator = LinearSVC(random_state=0)\n155         else:\n156             base_estimator = self.base_estimator\n157 \n158         if self.cv == \"prefit\":\n159             calibrated_classifier = _CalibratedClassifier(\n160                 base_estimator, method=self.method)\n161             if sample_weight is not None:\n162                 calibrated_classifier.fit(X, y, sample_weight)\n163             else:\n164                 calibrated_classifier.fit(X, y)\n165             self.calibrated_classifiers_.append(calibrated_classifier)\n166         else:\n167             cv = check_cv(self.cv, y, classifier=True)\n168             fit_parameters = signature(base_estimator.fit).parameters\n169             estimator_name = type(base_estimator).__name__\n170             if (sample_weight is not None\n171                     and \"sample_weight\" not in fit_parameters):\n172                 warnings.warn(\"%s does not support sample_weight. Samples\"\n173                               \" weights are only used for the calibration\"\n174                               \" itself.\" % estimator_name)\n175                 sample_weight = check_array(sample_weight, ensure_2d=False)\n176                 base_estimator_sample_weight = None\n177             else:\n178                 if sample_weight is not None:\n179                     sample_weight = check_array(sample_weight, ensure_2d=False)\n180                     check_consistent_length(y, sample_weight)\n181                 base_estimator_sample_weight = sample_weight\n182             for train, test in cv.split(X, y):\n183                 this_estimator = clone(base_estimator)\n184                 if base_estimator_sample_weight is not None:\n185                     this_estimator.fit(\n186                         X[train], y[train],\n187                         sample_weight=base_estimator_sample_weight[train])\n188                 else:\n189                     this_estimator.fit(X[train], y[train])\n190 \n191                 calibrated_classifier = _CalibratedClassifier(\n192                     this_estimator, method=self.method,\n193                     classes=self.classes_)\n194                 if sample_weight is not None:\n195                     calibrated_classifier.fit(X[test], y[test],\n196                                               sample_weight[test])\n197                 else:\n198                     calibrated_classifier.fit(X[test], y[test])\n199                 self.calibrated_classifiers_.append(calibrated_classifier)\n200 \n201         return self\n202 \n203     def predict_proba(self, X):\n204         \"\"\"Posterior probabilities of classification\n205 \n206         This function returns posterior probabilities of classification\n207         according to each class on an array of test vectors X.\n208 \n209         Parameters\n210         ----------\n211         X : array-like, shape (n_samples, n_features)\n212             The samples.\n213 \n214         Returns\n215         -------\n216         C : array, shape (n_samples, n_classes)\n217             The predicted probas.\n218         \"\"\"\n219         check_is_fitted(self)\n220         X = check_array(X, accept_sparse=['csc', 'csr', 'coo'],\n221                         force_all_finite=False)\n222         # Compute the arithmetic mean of the predictions of the calibrated\n223         # classifiers\n224         mean_proba = np.zeros((X.shape[0], len(self.classes_)))\n225         for calibrated_classifier in self.calibrated_classifiers_:\n226             proba = calibrated_classifier.predict_proba(X)\n227             mean_proba += proba\n228 \n229         mean_proba /= len(self.calibrated_classifiers_)\n230 \n231         return mean_proba\n232 \n233     def predict(self, X):\n234         \"\"\"Predict the target of new samples. Can be different from the\n235         prediction of the uncalibrated classifier.\n236 \n237         Parameters\n238         ----------\n239         X : array-like, shape (n_samples, n_features)\n240             The samples.\n241 \n242         Returns\n243         -------\n244         C : array, shape (n_samples,)\n245             The predicted class.\n246         \"\"\"\n247         check_is_fitted(self)\n248         return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "sklearn/cluster/_dbscan.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/cluster/_dbscan.py",
        "start": 284,
        "end": 361,
        "class_name": "DBSCAN",
        "method_name": "fit",
        "code": "284     def fit(self, X, y=None, sample_weight=None):\n285         \"\"\"Perform DBSCAN clustering from features, or distance matrix.\n286 \n287         Parameters\n288         ----------\n289         X : array-like or sparse matrix, shape (n_samples, n_features), or \\\n290             (n_samples, n_samples)\n291             Training instances to cluster, or distances between instances if\n292             ``metric='precomputed'``. If a sparse matrix is provided, it will\n293             be converted into a sparse ``csr_matrix``.\n294 \n295         sample_weight : array, shape (n_samples,), optional\n296             Weight of each sample, such that a sample with a weight of at least\n297             ``min_samples`` is by itself a core sample; a sample with a\n298             negative weight may inhibit its eps-neighbor from being core.\n299             Note that weights are absolute, and default to 1.\n300 \n301         y : Ignored\n302             Not used, present here for API consistency by convention.\n303 \n304         Returns\n305         -------\n306         self\n307 \n308         \"\"\"\n309         X = check_array(X, accept_sparse='csr')\n310 \n311         if not self.eps > 0.0:\n312             raise ValueError(\"eps must be positive.\")\n313 \n314         if sample_weight is not None:\n315             sample_weight = np.asarray(sample_weight)\n316             check_consistent_length(X, sample_weight)\n317 \n318         # Calculate neighborhood for all samples. This leaves the original\n319         # point in, which needs to be considered later (i.e. point i is in the\n320         # neighborhood of point i. While True, its useless information)\n321         if self.metric == 'precomputed' and sparse.issparse(X):\n322             # set the diagonal to explicit values, as a point is its own\n323             # neighbor\n324             with warnings.catch_warnings():\n325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)\n326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place\n327 \n328         neighbors_model = NearestNeighbors(\n329             radius=self.eps, algorithm=self.algorithm,\n330             leaf_size=self.leaf_size, metric=self.metric,\n331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)\n332         neighbors_model.fit(X)\n333         # This has worst case O(n^2) memory complexity\n334         neighborhoods = neighbors_model.radius_neighbors(X,\n335                                                          return_distance=False)\n336 \n337         if sample_weight is None:\n338             n_neighbors = np.array([len(neighbors)\n339                                     for neighbors in neighborhoods])\n340         else:\n341             n_neighbors = np.array([np.sum(sample_weight[neighbors])\n342                                     for neighbors in neighborhoods])\n343 \n344         # Initially, all samples are noise.\n345         labels = np.full(X.shape[0], -1, dtype=np.intp)\n346 \n347         # A list of all core samples found.\n348         core_samples = np.asarray(n_neighbors >= self.min_samples,\n349                                   dtype=np.uint8)\n350         dbscan_inner(core_samples, neighborhoods, labels)\n351 \n352         self.core_sample_indices_ = np.where(core_samples)[0]\n353         self.labels_ = labels\n354 \n355         if len(self.core_sample_indices_):\n356             # fix for scipy sparse indexing issue\n357             self.components_ = X[self.core_sample_indices_].copy()\n358         else:\n359             # no core samples\n360             self.components_ = np.empty((0, X.shape[1]))\n361         return self\n",
        "intended_behavior": "While DBSCAN may not explicitly validate sample_weight in the same way other estimators do due to its unique implementation, ensuring any internal handling of sample_weight is consistent or verified through _check_sample_weight could improve uniformity across the library."
    },
    {
        "rel_file_path": "sklearn/cluster/_dbscan.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/cluster/_dbscan.py",
        "start": 147,
        "end": 390,
        "class_name": "DBSCAN",
        "method_name": null,
        "code": "147 class DBSCAN(ClusterMixin, BaseEstimator):\n148     \"\"\"Perform DBSCAN clustering from vector array or distance matrix.\n149 \n150     DBSCAN - Density-Based Spatial Clustering of Applications with Noise.\n151     Finds core samples of high density and expands clusters from them.\n152     Good for data which contains clusters of similar density.\n153 \n154     Read more in the :ref:`User Guide <dbscan>`.\n155 \n156     Parameters\n157     ----------\n158     eps : float, optional\n159         The maximum distance between two samples for one to be considered\n160         as in the neighborhood of the other. This is not a maximum bound\n161         on the distances of points within a cluster. This is the most\n162         important DBSCAN parameter to choose appropriately for your data set\n163         and distance function.\n164 \n165     min_samples : int, optional\n166         The number of samples (or total weight) in a neighborhood for a point\n167         to be considered as a core point. This includes the point itself.\n168 \n169     metric : string, or callable\n170         The metric to use when calculating distance between instances in a\n171         feature array. If metric is a string or callable, it must be one of\n172         the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n173         its metric parameter.\n174         If metric is \"precomputed\", X is assumed to be a distance matrix and\n175         must be square. X may be a :term:`Glossary <sparse graph>`, in which\n176         case only \"nonzero\" elements may be considered neighbors for DBSCAN.\n177 \n178         .. versionadded:: 0.17\n179            metric *precomputed* to accept precomputed sparse matrix.\n180 \n181     metric_params : dict, optional\n182         Additional keyword arguments for the metric function.\n183 \n184         .. versionadded:: 0.19\n185 \n186     algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n187         The algorithm to be used by the NearestNeighbors module\n188         to compute pointwise distances and find nearest neighbors.\n189         See NearestNeighbors module documentation for details.\n190 \n191     leaf_size : int, optional (default = 30)\n192         Leaf size passed to BallTree or cKDTree. This can affect the speed\n193         of the construction and query, as well as the memory required\n194         to store the tree. The optimal value depends\n195         on the nature of the problem.\n196 \n197     p : float, optional\n198         The power of the Minkowski metric to be used to calculate distance\n199         between points.\n200 \n201     n_jobs : int or None, optional (default=None)\n202         The number of parallel jobs to run.\n203         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n204         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n205         for more details.\n206 \n207     Attributes\n208     ----------\n209     core_sample_indices_ : array, shape = [n_core_samples]\n210         Indices of core samples.\n211 \n212     components_ : array, shape = [n_core_samples, n_features]\n213         Copy of each core sample found by training.\n214 \n215     labels_ : array, shape = [n_samples]\n216         Cluster labels for each point in the dataset given to fit().\n217         Noisy samples are given the label -1.\n218 \n219     Examples\n220     --------\n221     >>> from sklearn.cluster import DBSCAN\n222     >>> import numpy as np\n223     >>> X = np.array([[1, 2], [2, 2], [2, 3],\n224     ...               [8, 7], [8, 8], [25, 80]])\n225     >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n226     >>> clustering.labels_\n227     array([ 0,  0,  0,  1,  1, -1])\n228     >>> clustering\n229     DBSCAN(eps=3, min_samples=2)\n230 \n231     See also\n232     --------\n233     OPTICS\n234         A similar clustering at multiple values of eps. Our implementation\n235         is optimized for memory usage.\n236 \n237     Notes\n238     -----\n239     For an example, see :ref:`examples/cluster/plot_dbscan.py\n240     <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.\n241 \n242     This implementation bulk-computes all neighborhood queries, which increases\n243     the memory complexity to O(n.d) where d is the average number of neighbors,\n244     while original DBSCAN had memory complexity O(n). It may attract a higher\n245     memory complexity when querying these nearest neighborhoods, depending\n246     on the ``algorithm``.\n247 \n248     One way to avoid the query complexity is to pre-compute sparse\n249     neighborhoods in chunks using\n250     :func:`NearestNeighbors.radius_neighbors_graph\n251     <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n252     ``mode='distance'``, then using ``metric='precomputed'`` here.\n253 \n254     Another way to reduce memory and computation time is to remove\n255     (near-)duplicate points and use ``sample_weight`` instead.\n256 \n257     :class:`cluster.OPTICS` provides a similar clustering with lower memory\n258     usage.\n259 \n260     References\n261     ----------\n262     Ester, M., H. P. Kriegel, J. Sander, and X. Xu, \"A Density-Based\n263     Algorithm for Discovering Clusters in Large Spatial Databases with Noise\".\n264     In: Proceedings of the 2nd International Conference on Knowledge Discovery\n265     and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n266 \n267     Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n268     DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\n269     ACM Transactions on Database Systems (TODS), 42(3), 19.\n270     \"\"\"\n271 \n272     def __init__(self, eps=0.5, min_samples=5, metric='euclidean',\n273                  metric_params=None, algorithm='auto', leaf_size=30, p=None,\n274                  n_jobs=None):\n275         self.eps = eps\n276         self.min_samples = min_samples\n277         self.metric = metric\n278         self.metric_params = metric_params\n279         self.algorithm = algorithm\n280         self.leaf_size = leaf_size\n281         self.p = p\n282         self.n_jobs = n_jobs\n283 \n284     def fit(self, X, y=None, sample_weight=None):\n285         \"\"\"Perform DBSCAN clustering from features, or distance matrix.\n286 \n287         Parameters\n288         ----------\n289         X : array-like or sparse matrix, shape (n_samples, n_features), or \\\n290             (n_samples, n_samples)\n291             Training instances to cluster, or distances between instances if\n292             ``metric='precomputed'``. If a sparse matrix is provided, it will\n293             be converted into a sparse ``csr_matrix``.\n294 \n295         sample_weight : array, shape (n_samples,), optional\n296             Weight of each sample, such that a sample with a weight of at least\n297             ``min_samples`` is by itself a core sample; a sample with a\n298             negative weight may inhibit its eps-neighbor from being core.\n299             Note that weights are absolute, and default to 1.\n300 \n301         y : Ignored\n302             Not used, present here for API consistency by convention.\n303 \n304         Returns\n305         -------\n306         self\n307 \n308         \"\"\"\n309         X = check_array(X, accept_sparse='csr')\n310 \n311         if not self.eps > 0.0:\n312             raise ValueError(\"eps must be positive.\")\n313 \n314         if sample_weight is not None:\n315             sample_weight = np.asarray(sample_weight)\n316             check_consistent_length(X, sample_weight)\n317 \n318         # Calculate neighborhood for all samples. This leaves the original\n319         # point in, which needs to be considered later (i.e. point i is in the\n320         # neighborhood of point i. While True, its useless information)\n321         if self.metric == 'precomputed' and sparse.issparse(X):\n322             # set the diagonal to explicit values, as a point is its own\n323             # neighbor\n324             with warnings.catch_warnings():\n325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)\n326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place\n327 \n328         neighbors_model = NearestNeighbors(\n329             radius=self.eps, algorithm=self.algorithm,\n330             leaf_size=self.leaf_size, metric=self.metric,\n331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)\n332         neighbors_model.fit(X)\n333         # This has worst case O(n^2) memory complexity\n334         neighborhoods = neighbors_model.radius_neighbors(X,\n335                                                          return_distance=False)\n336 \n337         if sample_weight is None:\n338             n_neighbors = np.array([len(neighbors)\n339                                     for neighbors in neighborhoods])\n340         else:\n341             n_neighbors = np.array([np.sum(sample_weight[neighbors])\n342                                     for neighbors in neighborhoods])\n343 \n344         # Initially, all samples are noise.\n345         labels = np.full(X.shape[0], -1, dtype=np.intp)\n346 \n347         # A list of all core samples found.\n348         core_samples = np.asarray(n_neighbors >= self.min_samples,\n349                                   dtype=np.uint8)\n350         dbscan_inner(core_samples, neighborhoods, labels)\n351 \n352         self.core_sample_indices_ = np.where(core_samples)[0]\n353         self.labels_ = labels\n354 \n355         if len(self.core_sample_indices_):\n356             # fix for scipy sparse indexing issue\n357             self.components_ = X[self.core_sample_indices_].copy()\n358         else:\n359             # no core samples\n360             self.components_ = np.empty((0, X.shape[1]))\n361         return self\n362 \n363     def fit_predict(self, X, y=None, sample_weight=None):\n364         \"\"\"Perform DBSCAN clustering from features or distance matrix,\n365         and return cluster labels.\n366 \n367         Parameters\n368         ----------\n369         X : array-like or sparse matrix, shape (n_samples, n_features), or \\\n370             (n_samples, n_samples)\n371             Training instances to cluster, or distances between instances if\n372             ``metric='precomputed'``. If a sparse matrix is provided, it will\n373             be converted into a sparse ``csr_matrix``.\n374 \n375         sample_weight : array, shape (n_samples,), optional\n376             Weight of each sample, such that a sample with a weight of at least\n377             ``min_samples`` is by itself a core sample; a sample with a\n378             negative weight may inhibit its eps-neighbor from being core.\n379             Note that weights are absolute, and default to 1.\n380 \n381         y : Ignored\n382             Not used, present here for API consistency by convention.\n383 \n384         Returns\n385         -------\n386         labels : ndarray, shape (n_samples,)\n387             Cluster labels. Noisy samples are given the label -1.\n388         \"\"\"\n389         self.fit(X, sample_weight=sample_weight)\n390         return self.labels_\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "sklearn/dummy.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/dummy.py",
        "start": 101,
        "end": 176,
        "class_name": "DummyClassifier",
        "method_name": "fit",
        "code": "101     def fit(self, X, y, sample_weight=None):\n102         \"\"\"Fit the random classifier.\n103 \n104         Parameters\n105         ----------\n106         X : {array-like, object with finite length or shape}\n107             Training data, requires length = n_samples\n108 \n109         y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n110             Target values.\n111 \n112         sample_weight : array-like of shape (n_samples,), default=None\n113             Sample weights.\n114 \n115         Returns\n116         -------\n117         self : object\n118         \"\"\"\n119         allowed_strategies = (\"most_frequent\", \"stratified\", \"uniform\",\n120                               \"constant\", \"prior\")\n121         if self.strategy not in allowed_strategies:\n122             raise ValueError(\"Unknown strategy type: %s, expected one of %s.\"\n123                              % (self.strategy, allowed_strategies))\n124 \n125         if self.strategy == \"uniform\" and sp.issparse(y):\n126             y = y.toarray()\n127             warnings.warn('A local copy of the target data has been converted '\n128                           'to a numpy array. Predicting on sparse target data '\n129                           'with the uniform strategy would not save memory '\n130                           'and would be slower.',\n131                           UserWarning)\n132 \n133         self.sparse_output_ = sp.issparse(y)\n134 \n135         if not self.sparse_output_:\n136             y = np.asarray(y)\n137             y = np.atleast_1d(y)\n138 \n139         if y.ndim == 1:\n140             y = np.reshape(y, (-1, 1))\n141 \n142         self.n_outputs_ = y.shape[1]\n143 \n144         check_consistent_length(X, y, sample_weight)\n145 \n146         if self.strategy == \"constant\":\n147             if self.constant is None:\n148                 raise ValueError(\"Constant target value has to be specified \"\n149                                  \"when the constant strategy is used.\")\n150             else:\n151                 constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))\n152                 if constant.shape[0] != self.n_outputs_:\n153                     raise ValueError(\"Constant target value should have \"\n154                                      \"shape (%d, 1).\" % self.n_outputs_)\n155 \n156         (self.classes_,\n157          self.n_classes_,\n158          self.class_prior_) = class_distribution(y, sample_weight)\n159 \n160         if self.strategy == \"constant\":\n161             for k in range(self.n_outputs_):\n162                 if not any(constant[k][0] == c for c in self.classes_[k]):\n163                     # Checking in case of constant strategy if the constant\n164                     # provided by the user is in y.\n165                     err_msg = (\"The constant target value must be present in \"\n166                                \"the training data. You provided constant={}. \"\n167                                \"Possible values are: {}.\"\n168                                .format(self.constant, list(self.classes_[k])))\n169                     raise ValueError(err_msg)\n170 \n171         if self.n_outputs_ == 1:\n172             self.n_classes_ = self.n_classes_[0]\n173             self.classes_ = self.classes_[0]\n174             self.class_prior_ = self.class_prior_[0]\n175 \n176         return self\n",
        "intended_behavior": "The fit method of DummyClassifier should incorporate _check_sample_weight for consistent sample weight validation and processing, replacing any custom logic currently implemented."
    },
    {
        "rel_file_path": "sklearn/dummy.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/dummy.py",
        "start": 23,
        "end": 379,
        "class_name": "DummyClassifier",
        "method_name": null,
        "code": "23 class DummyClassifier(MultiOutputMixin, ClassifierMixin, BaseEstimator):\n24     \"\"\"\n25     DummyClassifier is a classifier that makes predictions using simple rules.\n26 \n27     This classifier is useful as a simple baseline to compare with other\n28     (real) classifiers. Do not use it for real problems.\n29 \n30     Read more in the :ref:`User Guide <dummy_estimators>`.\n31 \n32     Parameters\n33     ----------\n34     strategy : str, default=\"stratified\"\n35         Strategy to use to generate predictions.\n36 \n37         * \"stratified\": generates predictions by respecting the training\n38           set's class distribution.\n39         * \"most_frequent\": always predicts the most frequent label in the\n40           training set.\n41         * \"prior\": always predicts the class that maximizes the class prior\n42           (like \"most_frequent\") and ``predict_proba`` returns the class prior.\n43         * \"uniform\": generates predictions uniformly at random.\n44         * \"constant\": always predicts a constant label that is provided by\n45           the user. This is useful for metrics that evaluate a non-majority\n46           class\n47 \n48           .. versionadded:: 0.17\n49              Dummy Classifier now supports prior fitting strategy using\n50              parameter *prior*.\n51 \n52     random_state : int, RandomState instance or None, optional, default=None\n53         If int, random_state is the seed used by the random number generator;\n54         If RandomState instance, random_state is the random number generator;\n55         If None, the random number generator is the RandomState instance used\n56         by `np.random`.\n57 \n58     constant : int or str or array-like of shape (n_outputs,)\n59         The explicit constant as predicted by the \"constant\" strategy. This\n60         parameter is useful only for the \"constant\" strategy.\n61 \n62     Attributes\n63     ----------\n64     classes_ : array or list of array of shape (n_classes,)\n65         Class labels for each output.\n66 \n67     n_classes_ : array or list of array of shape (n_classes,)\n68         Number of label for each output.\n69 \n70     class_prior_ : array or list of array of shape (n_classes,)\n71         Probability of each class for each output.\n72 \n73     n_outputs_ : int,\n74         Number of outputs.\n75 \n76     sparse_output_ : bool,\n77         True if the array returned from predict is to be in sparse CSC format.\n78         Is automatically set to True if the input y is passed in sparse format.\n79 \n80     Examples\n81     --------\n82     >>> import numpy as np\n83     >>> from sklearn.dummy import DummyClassifier\n84     >>> X = np.array([-1, 1, 1, 1])\n85     >>> y = np.array([0, 1, 1, 1])\n86     >>> dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n87     >>> dummy_clf.fit(X, y)\n88     DummyClassifier(strategy='most_frequent')\n89     >>> dummy_clf.predict(X)\n90     array([1, 1, 1, 1])\n91     >>> dummy_clf.score(X, y)\n92     0.75\n93     \"\"\"\n94 \n95     def __init__(self, strategy=\"stratified\", random_state=None,\n96                  constant=None):\n97         self.strategy = strategy\n98         self.random_state = random_state\n99         self.constant = constant\n100 \n101     def fit(self, X, y, sample_weight=None):\n102         \"\"\"Fit the random classifier.\n103 \n104         Parameters\n105         ----------\n106         X : {array-like, object with finite length or shape}\n107             Training data, requires length = n_samples\n108 \n109         y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n110             Target values.\n111 \n112         sample_weight : array-like of shape (n_samples,), default=None\n113             Sample weights.\n114 \n115         Returns\n116         -------\n117         self : object\n118         \"\"\"\n119         allowed_strategies = (\"most_frequent\", \"stratified\", \"uniform\",\n120                               \"constant\", \"prior\")\n121         if self.strategy not in allowed_strategies:\n122             raise ValueError(\"Unknown strategy type: %s, expected one of %s.\"\n123                              % (self.strategy, allowed_strategies))\n124 \n125         if self.strategy == \"uniform\" and sp.issparse(y):\n126             y = y.toarray()\n127             warnings.warn('A local copy of the target data has been converted '\n128                           'to a numpy array. Predicting on sparse target data '\n129                           'with the uniform strategy would not save memory '\n130                           'and would be slower.',\n131                           UserWarning)\n132 \n133         self.sparse_output_ = sp.issparse(y)\n134 \n135         if not self.sparse_output_:\n136             y = np.asarray(y)\n137             y = np.atleast_1d(y)\n138 \n139         if y.ndim == 1:\n140             y = np.reshape(y, (-1, 1))\n141 \n142         self.n_outputs_ = y.shape[1]\n143 \n144         check_consistent_length(X, y, sample_weight)\n145 \n146         if self.strategy == \"constant\":\n147             if self.constant is None:\n148                 raise ValueError(\"Constant target value has to be specified \"\n149                                  \"when the constant strategy is used.\")\n150             else:\n151                 constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))\n152                 if constant.shape[0] != self.n_outputs_:\n153                     raise ValueError(\"Constant target value should have \"\n154                                      \"shape (%d, 1).\" % self.n_outputs_)\n155 \n156         (self.classes_,\n157          self.n_classes_,\n158          self.class_prior_) = class_distribution(y, sample_weight)\n159 \n160         if self.strategy == \"constant\":\n161             for k in range(self.n_outputs_):\n162                 if not any(constant[k][0] == c for c in self.classes_[k]):\n163                     # Checking in case of constant strategy if the constant\n164                     # provided by the user is in y.\n165                     err_msg = (\"The constant target value must be present in \"\n166                                \"the training data. You provided constant={}. \"\n167                                \"Possible values are: {}.\"\n168                                .format(self.constant, list(self.classes_[k])))\n169                     raise ValueError(err_msg)\n170 \n171         if self.n_outputs_ == 1:\n172             self.n_classes_ = self.n_classes_[0]\n173             self.classes_ = self.classes_[0]\n174             self.class_prior_ = self.class_prior_[0]\n175 \n176         return self\n177 \n178     def predict(self, X):\n179         \"\"\"Perform classification on test vectors X.\n180 \n181         Parameters\n182         ----------\n183         X : {array-like, object with finite length or shape}\n184             Training data, requires length = n_samples\n185 \n186         Returns\n187         -------\n188         y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n189             Predicted target values for X.\n190         \"\"\"\n191         check_is_fitted(self)\n192 \n193         # numpy random_state expects Python int and not long as size argument\n194         # under Windows\n195         n_samples = _num_samples(X)\n196         rs = check_random_state(self.random_state)\n197 \n198         n_classes_ = self.n_classes_\n199         classes_ = self.classes_\n200         class_prior_ = self.class_prior_\n201         constant = self.constant\n202         if self.n_outputs_ == 1:\n203             # Get same type even for self.n_outputs_ == 1\n204             n_classes_ = [n_classes_]\n205             classes_ = [classes_]\n206             class_prior_ = [class_prior_]\n207             constant = [constant]\n208         # Compute probability only once\n209         if self.strategy == \"stratified\":\n210             proba = self.predict_proba(X)\n211             if self.n_outputs_ == 1:\n212                 proba = [proba]\n213 \n214         if self.sparse_output_:\n215             class_prob = None\n216             if self.strategy in (\"most_frequent\", \"prior\"):\n217                 classes_ = [np.array([cp.argmax()]) for cp in class_prior_]\n218 \n219             elif self.strategy == \"stratified\":\n220                 class_prob = class_prior_\n221 \n222             elif self.strategy == \"uniform\":\n223                 raise ValueError(\"Sparse target prediction is not \"\n224                                  \"supported with the uniform strategy\")\n225 \n226             elif self.strategy == \"constant\":\n227                 classes_ = [np.array([c]) for c in constant]\n228 \n229             y = _random_choice_csc(n_samples, classes_, class_prob,\n230                                   self.random_state)\n231         else:\n232             if self.strategy in (\"most_frequent\", \"prior\"):\n233                 y = np.tile([classes_[k][class_prior_[k].argmax()] for\n234                              k in range(self.n_outputs_)], [n_samples, 1])\n235 \n236             elif self.strategy == \"stratified\":\n237                 y = np.vstack([classes_[k][proba[k].argmax(axis=1)] for\n238                                k in range(self.n_outputs_)]).T\n239 \n240             elif self.strategy == \"uniform\":\n241                 ret = [classes_[k][rs.randint(n_classes_[k], size=n_samples)]\n242                        for k in range(self.n_outputs_)]\n243                 y = np.vstack(ret).T\n244 \n245             elif self.strategy == \"constant\":\n246                 y = np.tile(self.constant, (n_samples, 1))\n247 \n248             if self.n_outputs_ == 1:\n249                 y = np.ravel(y)\n250 \n251         return y\n252 \n253     def predict_proba(self, X):\n254         \"\"\"\n255         Return probability estimates for the test vectors X.\n256 \n257         Parameters\n258         ----------\n259         X : {array-like, object with finite length or shape}\n260             Training data, requires length = n_samples\n261 \n262         Returns\n263         -------\n264         P : array-like or list of array-lke of shape (n_samples, n_classes)\n265             Returns the probability of the sample for each class in\n266             the model, where classes are ordered arithmetically, for each\n267             output.\n268         \"\"\"\n269         check_is_fitted(self)\n270 \n271         # numpy random_state expects Python int and not long as size argument\n272         # under Windows\n273         n_samples = _num_samples(X)\n274         rs = check_random_state(self.random_state)\n275 \n276         n_classes_ = self.n_classes_\n277         classes_ = self.classes_\n278         class_prior_ = self.class_prior_\n279         constant = self.constant\n280         if self.n_outputs_ == 1:\n281             # Get same type even for self.n_outputs_ == 1\n282             n_classes_ = [n_classes_]\n283             classes_ = [classes_]\n284             class_prior_ = [class_prior_]\n285             constant = [constant]\n286 \n287         P = []\n288         for k in range(self.n_outputs_):\n289             if self.strategy == \"most_frequent\":\n290                 ind = class_prior_[k].argmax()\n291                 out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)\n292                 out[:, ind] = 1.0\n293             elif self.strategy == \"prior\":\n294                 out = np.ones((n_samples, 1)) * class_prior_[k]\n295 \n296             elif self.strategy == \"stratified\":\n297                 out = rs.multinomial(1, class_prior_[k], size=n_samples)\n298                 out = out.astype(np.float64)\n299 \n300             elif self.strategy == \"uniform\":\n301                 out = np.ones((n_samples, n_classes_[k]), dtype=np.float64)\n302                 out /= n_classes_[k]\n303 \n304             elif self.strategy == \"constant\":\n305                 ind = np.where(classes_[k] == constant[k])\n306                 out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)\n307                 out[:, ind] = 1.0\n308 \n309             P.append(out)\n310 \n311         if self.n_outputs_ == 1:\n312             P = P[0]\n313 \n314         return P\n315 \n316     def predict_log_proba(self, X):\n317         \"\"\"\n318         Return log probability estimates for the test vectors X.\n319 \n320         Parameters\n321         ----------\n322         X : {array-like, object with finite length or shape}\n323             Training data, requires length = n_samples\n324 \n325         Returns\n326         -------\n327         P : array-like or list of array-like of shape (n_samples, n_classes)\n328             Returns the log probability of the sample for each class in\n329             the model, where classes are ordered arithmetically for each\n330             output.\n331         \"\"\"\n332         proba = self.predict_proba(X)\n333         if self.n_outputs_ == 1:\n334             return np.log(proba)\n335         else:\n336             return [np.log(p) for p in proba]\n337 \n338     def _more_tags(self):\n339         return {'poor_score': True, 'no_validation': True}\n340 \n341     def score(self, X, y, sample_weight=None):\n342         \"\"\"Returns the mean accuracy on the given test data and labels.\n343 \n344         In multi-label classification, this is the subset accuracy\n345         which is a harsh metric since you require for each sample that\n346         each label set be correctly predicted.\n347 \n348         Parameters\n349         ----------\n350         X : {array-like, None}\n351             Test samples with shape = (n_samples, n_features) or\n352             None. Passing None as test samples gives the same result\n353             as passing real test samples, since DummyClassifier\n354             operates independently of the sampled observations.\n355 \n356         y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n357             True labels for X.\n358 \n359         sample_weight : array-like of shape (n_samples,), default=None\n360             Sample weights.\n361 \n362         Returns\n363         -------\n364         score : float\n365             Mean accuracy of self.predict(X) wrt. y.\n366 \n367         \"\"\"\n368         if X is None:\n369             X = np.zeros(shape=(len(y), 1))\n370         return super().score(X, y, sample_weight)\n371 \n372     @deprecated(\n373         \"The outputs_2d_ attribute is deprecated in version 0.22 \"\n374         \"and will be removed in version 0.24. It is equivalent to \"\n375         \"n_outputs_ > 1.\"\n376     )\n377     @property\n378     def outputs_2d_(self):\n379         return self.n_outputs_ != 1\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "sklearn/dummy.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/dummy.py",
        "start": 442,
        "end": 518,
        "class_name": "DummyRegressor",
        "method_name": "fit",
        "code": "442     def fit(self, X, y, sample_weight=None):\n443         \"\"\"Fit the random regressor.\n444 \n445         Parameters\n446         ----------\n447         X : {array-like, object with finite length or shape}\n448             Training data, requires length = n_samples\n449 \n450         y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n451             Target values.\n452 \n453         sample_weight : array-like of shape (n_samples,), default=None\n454             Sample weights.\n455 \n456         Returns\n457         -------\n458         self : object\n459         \"\"\"\n460         allowed_strategies = (\"mean\", \"median\", \"quantile\", \"constant\")\n461         if self.strategy not in allowed_strategies:\n462             raise ValueError(\"Unknown strategy type: %s, expected one of %s.\"\n463                              % (self.strategy, allowed_strategies))\n464 \n465         y = check_array(y, ensure_2d=False)\n466         if len(y) == 0:\n467             raise ValueError(\"y must not be empty.\")\n468 \n469         if y.ndim == 1:\n470             y = np.reshape(y, (-1, 1))\n471         self.n_outputs_ = y.shape[1]\n472 \n473         check_consistent_length(X, y, sample_weight)\n474         if sample_weight is not None:\n475             sample_weight = np.asarray(sample_weight)\n476 \n477         if self.strategy == \"mean\":\n478             self.constant_ = np.average(y, axis=0, weights=sample_weight)\n479 \n480         elif self.strategy == \"median\":\n481             if sample_weight is None:\n482                 self.constant_ = np.median(y, axis=0)\n483             else:\n484                 self.constant_ = [_weighted_percentile(y[:, k], sample_weight,\n485                                                        percentile=50.)\n486                                   for k in range(self.n_outputs_)]\n487 \n488         elif self.strategy == \"quantile\":\n489             if self.quantile is None or not np.isscalar(self.quantile):\n490                 raise ValueError(\"Quantile must be a scalar in the range \"\n491                                  \"[0.0, 1.0], but got %s.\" % self.quantile)\n492 \n493             percentile = self.quantile * 100.0\n494             if sample_weight is None:\n495                 self.constant_ = np.percentile(y, axis=0, q=percentile)\n496             else:\n497                 self.constant_ = [_weighted_percentile(y[:, k], sample_weight,\n498                                                        percentile=percentile)\n499                                   for k in range(self.n_outputs_)]\n500 \n501         elif self.strategy == \"constant\":\n502             if self.constant is None:\n503                 raise TypeError(\"Constant target value has to be specified \"\n504                                 \"when the constant strategy is used.\")\n505 \n506             self.constant = check_array(self.constant,\n507                                         accept_sparse=['csr', 'csc', 'coo'],\n508                                         ensure_2d=False, ensure_min_samples=0)\n509 \n510             if self.n_outputs_ != 1 and self.constant.shape[0] != y.shape[1]:\n511                 raise ValueError(\n512                     \"Constant target value should have \"\n513                     \"shape (%d, 1).\" % y.shape[1])\n514 \n515             self.constant_ = self.constant\n516 \n517         self.constant_ = np.reshape(self.constant_, (1, -1))\n518         return self\n",
        "intended_behavior": "Similar to DummyClassifier, DummyRegressor's fit method should also adopt _check_sample_weight to ensure standardized handling of sample_weight."
    },
    {
        "rel_file_path": "sklearn/dummy.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/dummy.py",
        "start": 382,
        "end": 602,
        "class_name": "DummyRegressor",
        "method_name": null,
        "code": "382 class DummyRegressor(MultiOutputMixin, RegressorMixin, BaseEstimator):\n383     \"\"\"\n384     DummyRegressor is a regressor that makes predictions using\n385     simple rules.\n386 \n387     This regressor is useful as a simple baseline to compare with other\n388     (real) regressors. Do not use it for real problems.\n389 \n390     Read more in the :ref:`User Guide <dummy_estimators>`.\n391 \n392     Parameters\n393     ----------\n394     strategy : str\n395         Strategy to use to generate predictions.\n396 \n397         * \"mean\": always predicts the mean of the training set\n398         * \"median\": always predicts the median of the training set\n399         * \"quantile\": always predicts a specified quantile of the training set,\n400           provided with the quantile parameter.\n401         * \"constant\": always predicts a constant value that is provided by\n402           the user.\n403 \n404     constant : int or float or array-like of shape (n_outputs,)\n405         The explicit constant as predicted by the \"constant\" strategy. This\n406         parameter is useful only for the \"constant\" strategy.\n407 \n408     quantile : float in [0.0, 1.0]\n409         The quantile to predict using the \"quantile\" strategy. A quantile of\n410         0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the\n411         maximum.\n412 \n413     Attributes\n414     ----------\n415     constant_ : array, shape (1, n_outputs)\n416         Mean or median or quantile of the training targets or constant value\n417         given by the user.\n418 \n419     n_outputs_ : int,\n420         Number of outputs.\n421 \n422     Examples\n423     --------\n424     >>> import numpy as np\n425     >>> from sklearn.dummy import DummyRegressor\n426     >>> X = np.array([1.0, 2.0, 3.0, 4.0])\n427     >>> y = np.array([2.0, 3.0, 5.0, 10.0])\n428     >>> dummy_regr = DummyRegressor(strategy=\"mean\")\n429     >>> dummy_regr.fit(X, y)\n430     DummyRegressor()\n431     >>> dummy_regr.predict(X)\n432     array([5., 5., 5., 5.])\n433     >>> dummy_regr.score(X, y)\n434     0.0\n435     \"\"\"\n436 \n437     def __init__(self, strategy=\"mean\", constant=None, quantile=None):\n438         self.strategy = strategy\n439         self.constant = constant\n440         self.quantile = quantile\n441 \n442     def fit(self, X, y, sample_weight=None):\n443         \"\"\"Fit the random regressor.\n444 \n445         Parameters\n446         ----------\n447         X : {array-like, object with finite length or shape}\n448             Training data, requires length = n_samples\n449 \n450         y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n451             Target values.\n452 \n453         sample_weight : array-like of shape (n_samples,), default=None\n454             Sample weights.\n455 \n456         Returns\n457         -------\n458         self : object\n459         \"\"\"\n460         allowed_strategies = (\"mean\", \"median\", \"quantile\", \"constant\")\n461         if self.strategy not in allowed_strategies:\n462             raise ValueError(\"Unknown strategy type: %s, expected one of %s.\"\n463                              % (self.strategy, allowed_strategies))\n464 \n465         y = check_array(y, ensure_2d=False)\n466         if len(y) == 0:\n467             raise ValueError(\"y must not be empty.\")\n468 \n469         if y.ndim == 1:\n470             y = np.reshape(y, (-1, 1))\n471         self.n_outputs_ = y.shape[1]\n472 \n473         check_consistent_length(X, y, sample_weight)\n474         if sample_weight is not None:\n475             sample_weight = np.asarray(sample_weight)\n476 \n477         if self.strategy == \"mean\":\n478             self.constant_ = np.average(y, axis=0, weights=sample_weight)\n479 \n480         elif self.strategy == \"median\":\n481             if sample_weight is None:\n482                 self.constant_ = np.median(y, axis=0)\n483             else:\n484                 self.constant_ = [_weighted_percentile(y[:, k], sample_weight,\n485                                                        percentile=50.)\n486                                   for k in range(self.n_outputs_)]\n487 \n488         elif self.strategy == \"quantile\":\n489             if self.quantile is None or not np.isscalar(self.quantile):\n490                 raise ValueError(\"Quantile must be a scalar in the range \"\n491                                  \"[0.0, 1.0], but got %s.\" % self.quantile)\n492 \n493             percentile = self.quantile * 100.0\n494             if sample_weight is None:\n495                 self.constant_ = np.percentile(y, axis=0, q=percentile)\n496             else:\n497                 self.constant_ = [_weighted_percentile(y[:, k], sample_weight,\n498                                                        percentile=percentile)\n499                                   for k in range(self.n_outputs_)]\n500 \n501         elif self.strategy == \"constant\":\n502             if self.constant is None:\n503                 raise TypeError(\"Constant target value has to be specified \"\n504                                 \"when the constant strategy is used.\")\n505 \n506             self.constant = check_array(self.constant,\n507                                         accept_sparse=['csr', 'csc', 'coo'],\n508                                         ensure_2d=False, ensure_min_samples=0)\n509 \n510             if self.n_outputs_ != 1 and self.constant.shape[0] != y.shape[1]:\n511                 raise ValueError(\n512                     \"Constant target value should have \"\n513                     \"shape (%d, 1).\" % y.shape[1])\n514 \n515             self.constant_ = self.constant\n516 \n517         self.constant_ = np.reshape(self.constant_, (1, -1))\n518         return self\n519 \n520     def predict(self, X, return_std=False):\n521         \"\"\"\n522         Perform classification on test vectors X.\n523 \n524         Parameters\n525         ----------\n526         X : {array-like, object with finite length or shape}\n527             Training data, requires length = n_samples\n528 \n529         return_std : boolean, optional\n530             Whether to return the standard deviation of posterior prediction.\n531             All zeros in this case.\n532 \n533         Returns\n534         -------\n535         y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n536             Predicted target values for X.\n537 \n538         y_std : array-like of shape (n_samples,) or (n_samples, n_outputs)\n539             Standard deviation of predictive distribution of query points.\n540         \"\"\"\n541         check_is_fitted(self)\n542         n_samples = _num_samples(X)\n543 \n544         y = np.full((n_samples, self.n_outputs_), self.constant_,\n545                     dtype=np.array(self.constant_).dtype)\n546         y_std = np.zeros((n_samples, self.n_outputs_))\n547 \n548         if self.n_outputs_ == 1:\n549             y = np.ravel(y)\n550             y_std = np.ravel(y_std)\n551 \n552         return (y, y_std) if return_std else y\n553 \n554     def _more_tags(self):\n555         return {'poor_score': True, 'no_validation': True}\n556 \n557     def score(self, X, y, sample_weight=None):\n558         \"\"\"Returns the coefficient of determination R^2 of the prediction.\n559 \n560         The coefficient R^2 is defined as (1 - u/v), where u is the residual\n561         sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n562         sum of squares ((y_true - y_true.mean()) ** 2).sum().\n563         The best possible score is 1.0 and it can be negative (because the\n564         model can be arbitrarily worse). A constant model that always\n565         predicts the expected value of y, disregarding the input features,\n566         would get a R^2 score of 0.0.\n567 \n568         Parameters\n569         ----------\n570         X : {array-like, None}\n571             Test samples with shape = (n_samples, n_features) or None.\n572             For some estimators this may be a\n573             precomputed kernel matrix instead, shape = (n_samples,\n574             n_samples_fitted], where n_samples_fitted is the number of\n575             samples used in the fitting for the estimator.\n576             Passing None as test samples gives the same result\n577             as passing real test samples, since DummyRegressor\n578             operates independently of the sampled observations.\n579 \n580         y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n581             True values for X.\n582 \n583         sample_weight : array-like of shape (n_samples,), default=None\n584             Sample weights.\n585 \n586         Returns\n587         -------\n588         score : float\n589             R^2 of self.predict(X) wrt. y.\n590         \"\"\"\n591         if X is None:\n592             X = np.zeros(shape=(len(y), 1))\n593         return super().score(X, y, sample_weight)\n594 \n595     @deprecated(\n596         \"The outputs_2d_ attribute is deprecated in version 0.22 \"\n597         \"and will be removed in version 0.24. It is equivalent to \"\n598         \"n_outputs_ > 1.\"\n599     )\n600     @property\n601     def outputs_2d_(self):\n602         return self.n_outputs_ != 1\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "sklearn/ensemble/_bagging.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/ensemble/_bagging.py",
        "start": 219,
        "end": 242,
        "class_name": "BaseBagging",
        "method_name": "fit",
        "code": "219     def fit(self, X, y, sample_weight=None):\n220         \"\"\"Build a Bagging ensemble of estimators from the training\n221            set (X, y).\n222 \n223         Parameters\n224         ----------\n225         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n226             The training input samples. Sparse matrices are accepted only if\n227             they are supported by the base estimator.\n228 \n229         y : array-like of shape (n_samples,)\n230             The target values (class labels in classification, real numbers in\n231             regression).\n232 \n233         sample_weight : array-like of shape (n_samples,), default=None\n234             Sample weights. If None, then samples are equally weighted.\n235             Note that this is supported only if the base estimator supports\n236             sample weighting.\n237 \n238         Returns\n239         -------\n240         self : object\n241         \"\"\"\n242         return self._fit(X, y, self.max_samples, sample_weight=sample_weight)\n",
        "intended_behavior": "Adapt the fit method to employ _check_sample_weight for validating and processing sample_weight. This will maintain the feature's existing functionality while ensuring that weight handling adheres to the standardized approach used across the library."
    },
    {
        "rel_file_path": "sklearn/ensemble/_bagging.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/ensemble/_bagging.py",
        "start": 185,
        "end": 430,
        "class_name": "BaseBagging",
        "method_name": null,
        "code": "185 class BaseBagging(BaseEnsemble, metaclass=ABCMeta):\n186     \"\"\"Base class for Bagging meta-estimator.\n187 \n188     Warning: This class should not be used directly. Use derived classes\n189     instead.\n190     \"\"\"\n191 \n192     @abstractmethod\n193     def __init__(self,\n194                  base_estimator=None,\n195                  n_estimators=10,\n196                  max_samples=1.0,\n197                  max_features=1.0,\n198                  bootstrap=True,\n199                  bootstrap_features=False,\n200                  oob_score=False,\n201                  warm_start=False,\n202                  n_jobs=None,\n203                  random_state=None,\n204                  verbose=0):\n205         super().__init__(\n206             base_estimator=base_estimator,\n207             n_estimators=n_estimators)\n208 \n209         self.max_samples = max_samples\n210         self.max_features = max_features\n211         self.bootstrap = bootstrap\n212         self.bootstrap_features = bootstrap_features\n213         self.oob_score = oob_score\n214         self.warm_start = warm_start\n215         self.n_jobs = n_jobs\n216         self.random_state = random_state\n217         self.verbose = verbose\n218 \n219     def fit(self, X, y, sample_weight=None):\n220         \"\"\"Build a Bagging ensemble of estimators from the training\n221            set (X, y).\n222 \n223         Parameters\n224         ----------\n225         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n226             The training input samples. Sparse matrices are accepted only if\n227             they are supported by the base estimator.\n228 \n229         y : array-like of shape (n_samples,)\n230             The target values (class labels in classification, real numbers in\n231             regression).\n232 \n233         sample_weight : array-like of shape (n_samples,), default=None\n234             Sample weights. If None, then samples are equally weighted.\n235             Note that this is supported only if the base estimator supports\n236             sample weighting.\n237 \n238         Returns\n239         -------\n240         self : object\n241         \"\"\"\n242         return self._fit(X, y, self.max_samples, sample_weight=sample_weight)\n243 \n244     def _parallel_args(self):\n245         return {}\n246 \n247     def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):\n248         \"\"\"Build a Bagging ensemble of estimators from the training\n249            set (X, y).\n250 \n251         Parameters\n252         ----------\n253         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n254             The training input samples. Sparse matrices are accepted only if\n255             they are supported by the base estimator.\n256 \n257         y : array-like of shape (n_samples,)\n258             The target values (class labels in classification, real numbers in\n259             regression).\n260 \n261         max_samples : int or float, optional (default=None)\n262             Argument to use instead of self.max_samples.\n263 \n264         max_depth : int, optional (default=None)\n265             Override value used when constructing base estimator. Only\n266             supported if the base estimator has a max_depth parameter.\n267 \n268         sample_weight : array-like of shape (n_samples,), default=None\n269             Sample weights. If None, then samples are equally weighted.\n270             Note that this is supported only if the base estimator supports\n271             sample weighting.\n272 \n273         Returns\n274         -------\n275         self : object\n276         \"\"\"\n277         random_state = check_random_state(self.random_state)\n278 \n279         # Convert data (X is required to be 2d and indexable)\n280         X, y = check_X_y(\n281             X, y, ['csr', 'csc'], dtype=None, force_all_finite=False,\n282             multi_output=True\n283         )\n284         if sample_weight is not None:\n285             sample_weight = check_array(sample_weight, ensure_2d=False)\n286             check_consistent_length(y, sample_weight)\n287 \n288         # Remap output\n289         n_samples, self.n_features_ = X.shape\n290         self._n_samples = n_samples\n291         y = self._validate_y(y)\n292 \n293         # Check parameters\n294         self._validate_estimator()\n295 \n296         if max_depth is not None:\n297             self.base_estimator_.max_depth = max_depth\n298 \n299         # Validate max_samples\n300         if max_samples is None:\n301             max_samples = self.max_samples\n302         elif not isinstance(max_samples, numbers.Integral):\n303             max_samples = int(max_samples * X.shape[0])\n304 \n305         if not (0 < max_samples <= X.shape[0]):\n306             raise ValueError(\"max_samples must be in (0, n_samples]\")\n307 \n308         # Store validated integer row sampling value\n309         self._max_samples = max_samples\n310 \n311         # Validate max_features\n312         if isinstance(self.max_features, numbers.Integral):\n313             max_features = self.max_features\n314         elif isinstance(self.max_features, np.float):\n315             max_features = self.max_features * self.n_features_\n316         else:\n317             raise ValueError(\"max_features must be int or float\")\n318 \n319         if not (0 < max_features <= self.n_features_):\n320             raise ValueError(\"max_features must be in (0, n_features]\")\n321 \n322         max_features = max(1, int(max_features))\n323 \n324         # Store validated integer feature sampling value\n325         self._max_features = max_features\n326 \n327         # Other checks\n328         if not self.bootstrap and self.oob_score:\n329             raise ValueError(\"Out of bag estimation only available\"\n330                              \" if bootstrap=True\")\n331 \n332         if self.warm_start and self.oob_score:\n333             raise ValueError(\"Out of bag estimate only available\"\n334                              \" if warm_start=False\")\n335 \n336         if hasattr(self, \"oob_score_\") and self.warm_start:\n337             del self.oob_score_\n338 \n339         if not self.warm_start or not hasattr(self, 'estimators_'):\n340             # Free allocated memory, if any\n341             self.estimators_ = []\n342             self.estimators_features_ = []\n343 \n344         n_more_estimators = self.n_estimators - len(self.estimators_)\n345 \n346         if n_more_estimators < 0:\n347             raise ValueError('n_estimators=%d must be larger or equal to '\n348                              'len(estimators_)=%d when warm_start==True'\n349                              % (self.n_estimators, len(self.estimators_)))\n350 \n351         elif n_more_estimators == 0:\n352             warn(\"Warm-start fitting without increasing n_estimators does not \"\n353                  \"fit new trees.\")\n354             return self\n355 \n356         # Parallel loop\n357         n_jobs, n_estimators, starts = _partition_estimators(n_more_estimators,\n358                                                              self.n_jobs)\n359         total_n_estimators = sum(n_estimators)\n360 \n361         # Advance random state to state after training\n362         # the first n_estimators\n363         if self.warm_start and len(self.estimators_) > 0:\n364             random_state.randint(MAX_INT, size=len(self.estimators_))\n365 \n366         seeds = random_state.randint(MAX_INT, size=n_more_estimators)\n367         self._seeds = seeds\n368 \n369         all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose,\n370                                **self._parallel_args())(\n371             delayed(_parallel_build_estimators)(\n372                 n_estimators[i],\n373                 self,\n374                 X,\n375                 y,\n376                 sample_weight,\n377                 seeds[starts[i]:starts[i + 1]],\n378                 total_n_estimators,\n379                 verbose=self.verbose)\n380             for i in range(n_jobs))\n381 \n382         # Reduce\n383         self.estimators_ += list(itertools.chain.from_iterable(\n384             t[0] for t in all_results))\n385         self.estimators_features_ += list(itertools.chain.from_iterable(\n386             t[1] for t in all_results))\n387 \n388         if self.oob_score:\n389             self._set_oob_score(X, y)\n390 \n391         return self\n392 \n393     @abstractmethod\n394     def _set_oob_score(self, X, y):\n395         \"\"\"Calculate out of bag predictions and score.\"\"\"\n396 \n397     def _validate_y(self, y):\n398         if len(y.shape) == 1 or y.shape[1] == 1:\n399             return column_or_1d(y, warn=True)\n400         else:\n401             return y\n402 \n403     def _get_estimators_indices(self):\n404         # Get drawn indices along both sample and feature axes\n405         for seed in self._seeds:\n406             # Operations accessing random_state must be performed identically\n407             # to those in `_parallel_build_estimators()`\n408             random_state = np.random.RandomState(seed)\n409             feature_indices, sample_indices = _generate_bagging_indices(\n410                 random_state, self.bootstrap_features, self.bootstrap,\n411                 self.n_features_, self._n_samples, self._max_features,\n412                 self._max_samples)\n413 \n414             yield feature_indices, sample_indices\n415 \n416     @property\n417     def estimators_samples_(self):\n418         \"\"\"\n419         The subset of drawn samples for each base estimator.\n420 \n421         Returns a dynamically generated list of indices identifying\n422         the samples used for fitting each member of the ensemble, i.e.,\n423         the in-bag samples.\n424 \n425         Note: the list is re-created at each call to the property in order\n426         to reduce the object memory footprint by not storing the sampling\n427         data. Thus fetching the property may be slower than expected.\n428         \"\"\"\n429         return [sample_indices\n430                 for _, sample_indices in self._get_estimators_indices()]\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "sklearn/ensemble/_forest.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/ensemble/_forest.py",
        "start": 269,
        "end": 397,
        "class_name": "BaseForest",
        "method_name": "fit",
        "code": "269     def fit(self, X, y, sample_weight=None):\n270         \"\"\"\n271         Build a forest of trees from the training set (X, y).\n272 \n273         Parameters\n274         ----------\n275         X : array-like or sparse matrix of shape (n_samples, n_features)\n276             The training input samples. Internally, its dtype will be converted\n277             to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n278             converted into a sparse ``csc_matrix``.\n279 \n280         y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n281             The target values (class labels in classification, real numbers in\n282             regression).\n283 \n284         sample_weight : array-like of shape (n_samples,), default=None\n285             Sample weights. If None, then samples are equally weighted. Splits\n286             that would create child nodes with net zero or negative weight are\n287             ignored while searching for a split in each node. In the case of\n288             classification, splits are also ignored if they would result in any\n289             single class carrying a negative weight in either child node.\n290 \n291         Returns\n292         -------\n293         self : object\n294         \"\"\"\n295         # Validate or convert input data\n296         X = check_array(X, accept_sparse=\"csc\", dtype=DTYPE)\n297         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)\n298         if sample_weight is not None:\n299             sample_weight = check_array(sample_weight, ensure_2d=False)\n300         if issparse(X):\n301             # Pre-sort indices to avoid that each individual tree of the\n302             # ensemble sorts the indices.\n303             X.sort_indices()\n304 \n305         # Remap output\n306         self.n_features_ = X.shape[1]\n307 \n308         y = np.atleast_1d(y)\n309         if y.ndim == 2 and y.shape[1] == 1:\n310             warn(\"A column-vector y was passed when a 1d array was\"\n311                  \" expected. Please change the shape of y to \"\n312                  \"(n_samples,), for example using ravel().\",\n313                  DataConversionWarning, stacklevel=2)\n314 \n315         if y.ndim == 1:\n316             # reshape is necessary to preserve the data contiguity against vs\n317             # [:, np.newaxis] that does not.\n318             y = np.reshape(y, (-1, 1))\n319 \n320         self.n_outputs_ = y.shape[1]\n321 \n322         y, expanded_class_weight = self._validate_y_class_weight(y)\n323 \n324         if getattr(y, \"dtype\", None) != DOUBLE or not y.flags.contiguous:\n325             y = np.ascontiguousarray(y, dtype=DOUBLE)\n326 \n327         if expanded_class_weight is not None:\n328             if sample_weight is not None:\n329                 sample_weight = sample_weight * expanded_class_weight\n330             else:\n331                 sample_weight = expanded_class_weight\n332 \n333         # Get bootstrap sample size\n334         n_samples_bootstrap = _get_n_samples_bootstrap(\n335             n_samples=X.shape[0],\n336             max_samples=self.max_samples\n337         )\n338 \n339         # Check parameters\n340         self._validate_estimator()\n341 \n342         if not self.bootstrap and self.oob_score:\n343             raise ValueError(\"Out of bag estimation only available\"\n344                              \" if bootstrap=True\")\n345 \n346         random_state = check_random_state(self.random_state)\n347 \n348         if not self.warm_start or not hasattr(self, \"estimators_\"):\n349             # Free allocated memory, if any\n350             self.estimators_ = []\n351 \n352         n_more_estimators = self.n_estimators - len(self.estimators_)\n353 \n354         if n_more_estimators < 0:\n355             raise ValueError('n_estimators=%d must be larger or equal to '\n356                              'len(estimators_)=%d when warm_start==True'\n357                              % (self.n_estimators, len(self.estimators_)))\n358 \n359         elif n_more_estimators == 0:\n360             warn(\"Warm-start fitting without increasing n_estimators does not \"\n361                  \"fit new trees.\")\n362         else:\n363             if self.warm_start and len(self.estimators_) > 0:\n364                 # We draw from the random state to get the random state we\n365                 # would have got if we hadn't used a warm_start.\n366                 random_state.randint(MAX_INT, size=len(self.estimators_))\n367 \n368             trees = [self._make_estimator(append=False,\n369                                           random_state=random_state)\n370                      for i in range(n_more_estimators)]\n371 \n372             # Parallel loop: we prefer the threading backend as the Cython code\n373             # for fitting the trees is internally releasing the Python GIL\n374             # making threading more efficient than multiprocessing in\n375             # that case. However, for joblib 0.12+ we respect any\n376             # parallel_backend contexts set at a higher level,\n377             # since correctness does not rely on using threads.\n378             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n379                              **_joblib_parallel_args(prefer='threads'))(\n380                 delayed(_parallel_build_trees)(\n381                     t, self, X, y, sample_weight, i, len(trees),\n382                     verbose=self.verbose, class_weight=self.class_weight,\n383                     n_samples_bootstrap=n_samples_bootstrap)\n384                 for i, t in enumerate(trees))\n385 \n386             # Collect newly grown trees\n387             self.estimators_.extend(trees)\n388 \n389         if self.oob_score:\n390             self._set_oob_score(X, y)\n391 \n392         # Decapsulate classes_ attributes\n393         if hasattr(self, \"classes_\") and self.n_outputs_ == 1:\n394             self.n_classes_ = self.n_classes_[0]\n395             self.classes_ = self.classes_[0]\n396 \n397         return self\n",
        "intended_behavior": "Ensure that fit method uses _check_sample_weight for validating and processing sample_weight in line with sklearn's standardized practices, ensuring consistency, and possibly enhancing error checking and input validation."
    },
    {
        "rel_file_path": "sklearn/ensemble/_forest.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/ensemble/_forest.py",
        "start": 172,
        "end": 440,
        "class_name": "BaseForest",
        "method_name": null,
        "code": "172 class BaseForest(MultiOutputMixin, BaseEnsemble, metaclass=ABCMeta):\n173     \"\"\"\n174     Base class for forests of trees.\n175 \n176     Warning: This class should not be used directly. Use derived classes\n177     instead.\n178     \"\"\"\n179 \n180     @abstractmethod\n181     def __init__(self,\n182                  base_estimator,\n183                  n_estimators=100,\n184                  estimator_params=tuple(),\n185                  bootstrap=False,\n186                  oob_score=False,\n187                  n_jobs=None,\n188                  random_state=None,\n189                  verbose=0,\n190                  warm_start=False,\n191                  class_weight=None,\n192                  max_samples=None):\n193         super().__init__(\n194             base_estimator=base_estimator,\n195             n_estimators=n_estimators,\n196             estimator_params=estimator_params)\n197 \n198         self.bootstrap = bootstrap\n199         self.oob_score = oob_score\n200         self.n_jobs = n_jobs\n201         self.random_state = random_state\n202         self.verbose = verbose\n203         self.warm_start = warm_start\n204         self.class_weight = class_weight\n205         self.max_samples = max_samples\n206 \n207     def apply(self, X):\n208         \"\"\"\n209         Apply trees in the forest to X, return leaf indices.\n210 \n211         Parameters\n212         ----------\n213         X : {array-like or sparse matrix} of shape (n_samples, n_features)\n214             The input samples. Internally, its dtype will be converted to\n215             ``dtype=np.float32``. If a sparse matrix is provided, it will be\n216             converted into a sparse ``csr_matrix``.\n217 \n218         Returns\n219         -------\n220         X_leaves : array_like, shape = [n_samples, n_estimators]\n221             For each datapoint x in X and for each tree in the forest,\n222             return the index of the leaf x ends up in.\n223         \"\"\"\n224         X = self._validate_X_predict(X)\n225         results = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n226                            **_joblib_parallel_args(prefer=\"threads\"))(\n227             delayed(tree.apply)(X, check_input=False)\n228             for tree in self.estimators_)\n229 \n230         return np.array(results).T\n231 \n232     def decision_path(self, X):\n233         \"\"\"\n234         Return the decision path in the forest.\n235 \n236         .. versionadded:: 0.18\n237 \n238         Parameters\n239         ----------\n240         X : {array-like or sparse matrix} of shape (n_samples, n_features)\n241             The input samples. Internally, its dtype will be converted to\n242             ``dtype=np.float32``. If a sparse matrix is provided, it will be\n243             converted into a sparse ``csr_matrix``.\n244 \n245         Returns\n246         -------\n247         indicator : sparse csr array, shape = [n_samples, n_nodes]\n248             Return a node indicator matrix where non zero elements\n249             indicates that the samples goes through the nodes.\n250 \n251         n_nodes_ptr : array of size (n_estimators + 1, )\n252             The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n253             gives the indicator value for the i-th estimator.\n254 \n255         \"\"\"\n256         X = self._validate_X_predict(X)\n257         indicators = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n258                               **_joblib_parallel_args(prefer='threads'))(\n259             delayed(tree.decision_path)(X,\n260                                      check_input=False)\n261             for tree in self.estimators_)\n262 \n263         n_nodes = [0]\n264         n_nodes.extend([i.shape[1] for i in indicators])\n265         n_nodes_ptr = np.array(n_nodes).cumsum()\n266 \n267         return sparse_hstack(indicators).tocsr(), n_nodes_ptr\n268 \n269     def fit(self, X, y, sample_weight=None):\n270         \"\"\"\n271         Build a forest of trees from the training set (X, y).\n272 \n273         Parameters\n274         ----------\n275         X : array-like or sparse matrix of shape (n_samples, n_features)\n276             The training input samples. Internally, its dtype will be converted\n277             to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n278             converted into a sparse ``csc_matrix``.\n279 \n280         y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n281             The target values (class labels in classification, real numbers in\n282             regression).\n283 \n284         sample_weight : array-like of shape (n_samples,), default=None\n285             Sample weights. If None, then samples are equally weighted. Splits\n286             that would create child nodes with net zero or negative weight are\n287             ignored while searching for a split in each node. In the case of\n288             classification, splits are also ignored if they would result in any\n289             single class carrying a negative weight in either child node.\n290 \n291         Returns\n292         -------\n293         self : object\n294         \"\"\"\n295         # Validate or convert input data\n296         X = check_array(X, accept_sparse=\"csc\", dtype=DTYPE)\n297         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)\n298         if sample_weight is not None:\n299             sample_weight = check_array(sample_weight, ensure_2d=False)\n300         if issparse(X):\n301             # Pre-sort indices to avoid that each individual tree of the\n302             # ensemble sorts the indices.\n303             X.sort_indices()\n304 \n305         # Remap output\n306         self.n_features_ = X.shape[1]\n307 \n308         y = np.atleast_1d(y)\n309         if y.ndim == 2 and y.shape[1] == 1:\n310             warn(\"A column-vector y was passed when a 1d array was\"\n311                  \" expected. Please change the shape of y to \"\n312                  \"(n_samples,), for example using ravel().\",\n313                  DataConversionWarning, stacklevel=2)\n314 \n315         if y.ndim == 1:\n316             # reshape is necessary to preserve the data contiguity against vs\n317             # [:, np.newaxis] that does not.\n318             y = np.reshape(y, (-1, 1))\n319 \n320         self.n_outputs_ = y.shape[1]\n321 \n322         y, expanded_class_weight = self._validate_y_class_weight(y)\n323 \n324         if getattr(y, \"dtype\", None) != DOUBLE or not y.flags.contiguous:\n325             y = np.ascontiguousarray(y, dtype=DOUBLE)\n326 \n327         if expanded_class_weight is not None:\n328             if sample_weight is not None:\n329                 sample_weight = sample_weight * expanded_class_weight\n330             else:\n331                 sample_weight = expanded_class_weight\n332 \n333         # Get bootstrap sample size\n334         n_samples_bootstrap = _get_n_samples_bootstrap(\n335             n_samples=X.shape[0],\n336             max_samples=self.max_samples\n337         )\n338 \n339         # Check parameters\n340         self._validate_estimator()\n341 \n342         if not self.bootstrap and self.oob_score:\n343             raise ValueError(\"Out of bag estimation only available\"\n344                              \" if bootstrap=True\")\n345 \n346         random_state = check_random_state(self.random_state)\n347 \n348         if not self.warm_start or not hasattr(self, \"estimators_\"):\n349             # Free allocated memory, if any\n350             self.estimators_ = []\n351 \n352         n_more_estimators = self.n_estimators - len(self.estimators_)\n353 \n354         if n_more_estimators < 0:\n355             raise ValueError('n_estimators=%d must be larger or equal to '\n356                              'len(estimators_)=%d when warm_start==True'\n357                              % (self.n_estimators, len(self.estimators_)))\n358 \n359         elif n_more_estimators == 0:\n360             warn(\"Warm-start fitting without increasing n_estimators does not \"\n361                  \"fit new trees.\")\n362         else:\n363             if self.warm_start and len(self.estimators_) > 0:\n364                 # We draw from the random state to get the random state we\n365                 # would have got if we hadn't used a warm_start.\n366                 random_state.randint(MAX_INT, size=len(self.estimators_))\n367 \n368             trees = [self._make_estimator(append=False,\n369                                           random_state=random_state)\n370                      for i in range(n_more_estimators)]\n371 \n372             # Parallel loop: we prefer the threading backend as the Cython code\n373             # for fitting the trees is internally releasing the Python GIL\n374             # making threading more efficient than multiprocessing in\n375             # that case. However, for joblib 0.12+ we respect any\n376             # parallel_backend contexts set at a higher level,\n377             # since correctness does not rely on using threads.\n378             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n379                              **_joblib_parallel_args(prefer='threads'))(\n380                 delayed(_parallel_build_trees)(\n381                     t, self, X, y, sample_weight, i, len(trees),\n382                     verbose=self.verbose, class_weight=self.class_weight,\n383                     n_samples_bootstrap=n_samples_bootstrap)\n384                 for i, t in enumerate(trees))\n385 \n386             # Collect newly grown trees\n387             self.estimators_.extend(trees)\n388 \n389         if self.oob_score:\n390             self._set_oob_score(X, y)\n391 \n392         # Decapsulate classes_ attributes\n393         if hasattr(self, \"classes_\") and self.n_outputs_ == 1:\n394             self.n_classes_ = self.n_classes_[0]\n395             self.classes_ = self.classes_[0]\n396 \n397         return self\n398 \n399     @abstractmethod\n400     def _set_oob_score(self, X, y):\n401         \"\"\"\n402         Calculate out of bag predictions and score.\"\"\"\n403 \n404     def _validate_y_class_weight(self, y):\n405         # Default implementation\n406         return y, None\n407 \n408     def _validate_X_predict(self, X):\n409         \"\"\"\n410         Validate X whenever one tries to predict, apply, predict_proba.\"\"\"\n411         check_is_fitted(self)\n412 \n413         return self.estimators_[0]._validate_X_predict(X, check_input=True)\n414 \n415     @property\n416     def feature_importances_(self):\n417         \"\"\"\n418         Return the feature importances (the higher, the more important the\n419            feature).\n420 \n421         Returns\n422         -------\n423         feature_importances_ : array, shape = [n_features]\n424             The values of this array sum to 1, unless all trees are single node\n425             trees consisting of only the root node, in which case it will be an\n426             array of zeros.\n427         \"\"\"\n428         check_is_fitted(self)\n429 \n430         all_importances = Parallel(n_jobs=self.n_jobs,\n431                                    **_joblib_parallel_args(prefer='threads'))(\n432             delayed(getattr)(tree, 'feature_importances_')\n433             for tree in self.estimators_ if tree.tree_.node_count > 1)\n434 \n435         if not all_importances:\n436             return np.zeros(self.n_features_, dtype=np.float64)\n437 \n438         all_importances = np.mean(all_importances,\n439                                   axis=0, dtype=np.float64)\n440         return all_importances / np.sum(all_importances)\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "sklearn/ensemble/_gb.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/ensemble/_gb.py",
        "start": 1399,
        "end": 1547,
        "class_name": "BaseGradientBoosting",
        "method_name": "fit",
        "code": "1399     def fit(self, X, y, sample_weight=None, monitor=None):\n1400         \"\"\"Fit the gradient boosting model.\n1401 \n1402         Parameters\n1403         ----------\n1404         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n1405             The input samples. Internally, it will be converted to\n1406             ``dtype=np.float32`` and if a sparse matrix is provided\n1407             to a sparse ``csr_matrix``.\n1408 \n1409         y : array-like, shape (n_samples,)\n1410             Target values (strings or integers in classification, real numbers\n1411             in regression)\n1412             For classification, labels must correspond to classes.\n1413 \n1414         sample_weight : array-like, shape (n_samples,) or None\n1415             Sample weights. If None, then samples are equally weighted. Splits\n1416             that would create child nodes with net zero or negative weight are\n1417             ignored while searching for a split in each node. In the case of\n1418             classification, splits are also ignored if they would result in any\n1419             single class carrying a negative weight in either child node.\n1420 \n1421         monitor : callable, optional\n1422             The monitor is called after each iteration with the current\n1423             iteration, a reference to the estimator and the local variables of\n1424             ``_fit_stages`` as keyword arguments ``callable(i, self,\n1425             locals())``. If the callable returns ``True`` the fitting procedure\n1426             is stopped. The monitor can be used for various things such as\n1427             computing held-out estimates, early stopping, model introspect, and\n1428             snapshoting.\n1429 \n1430         Returns\n1431         -------\n1432         self : object\n1433         \"\"\"\n1434         # if not warmstart - clear the estimator state\n1435         if not self.warm_start:\n1436             self._clear_state()\n1437 \n1438         # Check input\n1439         # Since check_array converts both X and y to the same dtype, but the\n1440         # trees use different types for X and y, checking them separately.\n1441         X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)\n1442         n_samples, self.n_features_ = X.shape\n1443 \n1444         sample_weight_is_none = sample_weight is None\n1445         if sample_weight_is_none:\n1446             sample_weight = np.ones(n_samples, dtype=np.float32)\n1447         else:\n1448             sample_weight = column_or_1d(sample_weight, warn=True)\n1449             sample_weight_is_none = False\n1450 \n1451         check_consistent_length(X, y, sample_weight)\n1452 \n1453         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)\n1454         y = column_or_1d(y, warn=True)\n1455         y = self._validate_y(y, sample_weight)\n1456 \n1457         if self.n_iter_no_change is not None:\n1458             stratify = y if is_classifier(self) else None\n1459             X, X_val, y, y_val, sample_weight, sample_weight_val = (\n1460                 train_test_split(X, y, sample_weight,\n1461                                  random_state=self.random_state,\n1462                                  test_size=self.validation_fraction,\n1463                                  stratify=stratify))\n1464             if is_classifier(self):\n1465                 if self.n_classes_ != np.unique(y).shape[0]:\n1466                     # We choose to error here. The problem is that the init\n1467                     # estimator would be trained on y, which has some missing\n1468                     # classes now, so its predictions would not have the\n1469                     # correct shape.\n1470                     raise ValueError(\n1471                         'The training data after the early stopping split '\n1472                         'is missing some classes. Try using another random '\n1473                         'seed.'\n1474                     )\n1475         else:\n1476             X_val = y_val = sample_weight_val = None\n1477 \n1478         self._check_params()\n1479 \n1480         if not self._is_initialized():\n1481             # init state\n1482             self._init_state()\n1483 \n1484             # fit initial model and initialize raw predictions\n1485             if self.init_ == 'zero':\n1486                 raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),\n1487                                            dtype=np.float64)\n1488             else:\n1489                 # XXX clean this once we have a support_sample_weight tag\n1490                 if sample_weight_is_none:\n1491                     self.init_.fit(X, y)\n1492                 else:\n1493                     msg = (\"The initial estimator {} does not support sample \"\n1494                            \"weights.\".format(self.init_.__class__.__name__))\n1495                     try:\n1496                         self.init_.fit(X, y, sample_weight=sample_weight)\n1497                     except TypeError:  # regular estimator without SW support\n1498                         raise ValueError(msg)\n1499                     except ValueError as e:\n1500                         if \"pass parameters to specific steps of \"\\\n1501                            \"your pipeline using the \"\\\n1502                            \"stepname__parameter\" in str(e):  # pipeline\n1503                             raise ValueError(msg) from e\n1504                         else:  # regular estimator whose input checking failed\n1505                             raise\n1506 \n1507                 raw_predictions = \\\n1508                     self.loss_.get_init_raw_predictions(X, self.init_)\n1509 \n1510             begin_at_stage = 0\n1511 \n1512             # The rng state must be preserved if warm_start is True\n1513             self._rng = check_random_state(self.random_state)\n1514 \n1515         else:\n1516             # add more estimators to fitted model\n1517             # invariant: warm_start = True\n1518             if self.n_estimators < self.estimators_.shape[0]:\n1519                 raise ValueError('n_estimators=%d must be larger or equal to '\n1520                                  'estimators_.shape[0]=%d when '\n1521                                  'warm_start==True'\n1522                                  % (self.n_estimators,\n1523                                     self.estimators_.shape[0]))\n1524             begin_at_stage = self.estimators_.shape[0]\n1525             # The requirements of _decision_function (called in two lines\n1526             # below) are more constrained than fit. It accepts only CSR\n1527             # matrices.\n1528             X = check_array(X, dtype=DTYPE, order=\"C\", accept_sparse='csr')\n1529             raw_predictions = self._raw_predict(X)\n1530             self._resize_state()\n1531 \n1532         X_idx_sorted = None\n1533 \n1534         # fit the boosting stages\n1535         n_stages = self._fit_stages(\n1536             X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,\n1537             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n1538 \n1539         # change shape of arrays after fit (early-stopping or additional ests)\n1540         if n_stages != self.estimators_.shape[0]:\n1541             self.estimators_ = self.estimators_[:n_stages]\n1542             self.train_score_ = self.train_score_[:n_stages]\n1543             if hasattr(self, 'oob_improvement_'):\n1544                 self.oob_improvement_ = self.oob_improvement_[:n_stages]\n1545 \n1546         self.n_estimators_ = n_stages\n1547         return self\n",
        "intended_behavior": "The fit function should be updated to utilize _check_sample_weight for handling sample_weight, replacing or complementing any specific logic currently in place with the standardized utility function."
    },
    {
        "rel_file_path": "sklearn/ensemble/_gb.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/ensemble/_gb.py",
        "start": 1166,
        "end": 1799,
        "class_name": "BaseGradientBoosting",
        "method_name": null,
        "code": "1166 class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):\n1167     \"\"\"Abstract base class for Gradient Boosting. \"\"\"\n1168 \n1169     @abstractmethod\n1170     def __init__(self, loss, learning_rate, n_estimators, criterion,\n1171                  min_samples_split, min_samples_leaf, min_weight_fraction_leaf,\n1172                  max_depth, min_impurity_decrease, min_impurity_split,\n1173                  init, subsample, max_features, ccp_alpha,\n1174                  random_state, alpha=0.9, verbose=0, max_leaf_nodes=None,\n1175                  warm_start=False, presort='deprecated',\n1176                  validation_fraction=0.1, n_iter_no_change=None,\n1177                  tol=1e-4):\n1178 \n1179         self.n_estimators = n_estimators\n1180         self.learning_rate = learning_rate\n1181         self.loss = loss\n1182         self.criterion = criterion\n1183         self.min_samples_split = min_samples_split\n1184         self.min_samples_leaf = min_samples_leaf\n1185         self.min_weight_fraction_leaf = min_weight_fraction_leaf\n1186         self.subsample = subsample\n1187         self.max_features = max_features\n1188         self.max_depth = max_depth\n1189         self.min_impurity_decrease = min_impurity_decrease\n1190         self.min_impurity_split = min_impurity_split\n1191         self.ccp_alpha = ccp_alpha\n1192         self.init = init\n1193         self.random_state = random_state\n1194         self.alpha = alpha\n1195         self.verbose = verbose\n1196         self.max_leaf_nodes = max_leaf_nodes\n1197         self.warm_start = warm_start\n1198         self.presort = presort\n1199         self.validation_fraction = validation_fraction\n1200         self.n_iter_no_change = n_iter_no_change\n1201         self.tol = tol\n1202 \n1203     def _fit_stage(self, i, X, y, raw_predictions, sample_weight, sample_mask,\n1204                    random_state, X_idx_sorted, X_csc=None, X_csr=None):\n1205         \"\"\"Fit another stage of ``n_classes_`` trees to the boosting model. \"\"\"\n1206 \n1207         assert sample_mask.dtype == np.bool\n1208         loss = self.loss_\n1209         original_y = y\n1210 \n1211         # Need to pass a copy of raw_predictions to negative_gradient()\n1212         # because raw_predictions is partially updated at the end of the loop\n1213         # in update_terminal_regions(), and gradients need to be evaluated at\n1214         # iteration i - 1.\n1215         raw_predictions_copy = raw_predictions.copy()\n1216 \n1217         for k in range(loss.K):\n1218             if loss.is_multi_class:\n1219                 y = np.array(original_y == k, dtype=np.float64)\n1220 \n1221             residual = loss.negative_gradient(y, raw_predictions_copy, k=k,\n1222                                               sample_weight=sample_weight)\n1223 \n1224             # induce regression tree on residuals\n1225             tree = DecisionTreeRegressor(\n1226                 criterion=self.criterion,\n1227                 splitter='best',\n1228                 max_depth=self.max_depth,\n1229                 min_samples_split=self.min_samples_split,\n1230                 min_samples_leaf=self.min_samples_leaf,\n1231                 min_weight_fraction_leaf=self.min_weight_fraction_leaf,\n1232                 min_impurity_decrease=self.min_impurity_decrease,\n1233                 min_impurity_split=self.min_impurity_split,\n1234                 max_features=self.max_features,\n1235                 max_leaf_nodes=self.max_leaf_nodes,\n1236                 random_state=random_state,\n1237                 ccp_alpha=self.ccp_alpha)\n1238 \n1239             if self.subsample < 1.0:\n1240                 # no inplace multiplication!\n1241                 sample_weight = sample_weight * sample_mask.astype(np.float64)\n1242 \n1243             X = X_csr if X_csr is not None else X\n1244             tree.fit(X, residual, sample_weight=sample_weight,\n1245                      check_input=False, X_idx_sorted=X_idx_sorted)\n1246 \n1247             # update tree leaves\n1248             loss.update_terminal_regions(\n1249                 tree.tree_, X, y, residual, raw_predictions, sample_weight,\n1250                 sample_mask, learning_rate=self.learning_rate, k=k)\n1251 \n1252             # add tree to ensemble\n1253             self.estimators_[i, k] = tree\n1254 \n1255         return raw_predictions\n1256 \n1257     def _check_params(self):\n1258         \"\"\"Check validity of parameters and raise ValueError if not valid. \"\"\"\n1259         if self.n_estimators <= 0:\n1260             raise ValueError(\"n_estimators must be greater than 0 but \"\n1261                              \"was %r\" % self.n_estimators)\n1262 \n1263         if self.learning_rate <= 0.0:\n1264             raise ValueError(\"learning_rate must be greater than 0 but \"\n1265                              \"was %r\" % self.learning_rate)\n1266 \n1267         if (self.loss not in self._SUPPORTED_LOSS\n1268                 or self.loss not in _gb_losses.LOSS_FUNCTIONS):\n1269             raise ValueError(\"Loss '{0:s}' not supported. \".format(self.loss))\n1270 \n1271         if self.loss == 'deviance':\n1272             loss_class = (_gb_losses.MultinomialDeviance\n1273                           if len(self.classes_) > 2\n1274                           else _gb_losses.BinomialDeviance)\n1275         else:\n1276             loss_class = _gb_losses.LOSS_FUNCTIONS[self.loss]\n1277 \n1278         if self.loss in ('huber', 'quantile'):\n1279             self.loss_ = loss_class(self.n_classes_, self.alpha)\n1280         else:\n1281             self.loss_ = loss_class(self.n_classes_)\n1282 \n1283         if not (0.0 < self.subsample <= 1.0):\n1284             raise ValueError(\"subsample must be in (0,1] but \"\n1285                              \"was %r\" % self.subsample)\n1286 \n1287         if self.init is not None:\n1288             # init must be an estimator or 'zero'\n1289             if isinstance(self.init, BaseEstimator):\n1290                 self.loss_.check_init_estimator(self.init)\n1291             elif not (isinstance(self.init, str) and self.init == 'zero'):\n1292                 raise ValueError(\n1293                     \"The init parameter must be an estimator or 'zero'. \"\n1294                     \"Got init={}\".format(self.init)\n1295                 )\n1296 \n1297         if not (0.0 < self.alpha < 1.0):\n1298             raise ValueError(\"alpha must be in (0.0, 1.0) but \"\n1299                              \"was %r\" % self.alpha)\n1300 \n1301         if isinstance(self.max_features, str):\n1302             if self.max_features == \"auto\":\n1303                 # if is_classification\n1304                 if self.n_classes_ > 1:\n1305                     max_features = max(1, int(np.sqrt(self.n_features_)))\n1306                 else:\n1307                     # is regression\n1308                     max_features = self.n_features_\n1309             elif self.max_features == \"sqrt\":\n1310                 max_features = max(1, int(np.sqrt(self.n_features_)))\n1311             elif self.max_features == \"log2\":\n1312                 max_features = max(1, int(np.log2(self.n_features_)))\n1313             else:\n1314                 raise ValueError(\"Invalid value for max_features: %r. \"\n1315                                  \"Allowed string values are 'auto', 'sqrt' \"\n1316                                  \"or 'log2'.\" % self.max_features)\n1317         elif self.max_features is None:\n1318             max_features = self.n_features_\n1319         elif isinstance(self.max_features, numbers.Integral):\n1320             max_features = self.max_features\n1321         else:  # float\n1322             if 0. < self.max_features <= 1.:\n1323                 max_features = max(int(self.max_features *\n1324                                        self.n_features_), 1)\n1325             else:\n1326                 raise ValueError(\"max_features must be in (0, n_features]\")\n1327 \n1328         self.max_features_ = max_features\n1329 \n1330         if not isinstance(self.n_iter_no_change,\n1331                           (numbers.Integral, type(None))):\n1332             raise ValueError(\"n_iter_no_change should either be None or an \"\n1333                              \"integer. %r was passed\"\n1334                              % self.n_iter_no_change)\n1335 \n1336         if self.presort != 'deprecated':\n1337             warnings.warn(\"The parameter 'presort' is deprecated and has no \"\n1338                           \"effect. It will be removed in v0.24. You can \"\n1339                           \"suppress this warning by not passing any value \"\n1340                           \"to the 'presort' parameter. We also recommend \"\n1341                           \"using HistGradientBoosting models instead.\",\n1342                           FutureWarning)\n1343 \n1344     def _init_state(self):\n1345         \"\"\"Initialize model state and allocate model state data structures. \"\"\"\n1346 \n1347         self.init_ = self.init\n1348         if self.init_ is None:\n1349             self.init_ = self.loss_.init_estimator()\n1350 \n1351         self.estimators_ = np.empty((self.n_estimators, self.loss_.K),\n1352                                     dtype=np.object)\n1353         self.train_score_ = np.zeros((self.n_estimators,), dtype=np.float64)\n1354         # do oob?\n1355         if self.subsample < 1.0:\n1356             self.oob_improvement_ = np.zeros((self.n_estimators),\n1357                                              dtype=np.float64)\n1358 \n1359     def _clear_state(self):\n1360         \"\"\"Clear the state of the gradient boosting model. \"\"\"\n1361         if hasattr(self, 'estimators_'):\n1362             self.estimators_ = np.empty((0, 0), dtype=np.object)\n1363         if hasattr(self, 'train_score_'):\n1364             del self.train_score_\n1365         if hasattr(self, 'oob_improvement_'):\n1366             del self.oob_improvement_\n1367         if hasattr(self, 'init_'):\n1368             del self.init_\n1369         if hasattr(self, '_rng'):\n1370             del self._rng\n1371 \n1372     def _resize_state(self):\n1373         \"\"\"Add additional ``n_estimators`` entries to all attributes. \"\"\"\n1374         # self.n_estimators is the number of additional est to fit\n1375         total_n_estimators = self.n_estimators\n1376         if total_n_estimators < self.estimators_.shape[0]:\n1377             raise ValueError('resize with smaller n_estimators %d < %d' %\n1378                              (total_n_estimators, self.estimators_[0]))\n1379 \n1380         self.estimators_ = np.resize(self.estimators_,\n1381                                      (total_n_estimators, self.loss_.K))\n1382         self.train_score_ = np.resize(self.train_score_, total_n_estimators)\n1383         if (self.subsample < 1 or hasattr(self, 'oob_improvement_')):\n1384             # if do oob resize arrays or create new if not available\n1385             if hasattr(self, 'oob_improvement_'):\n1386                 self.oob_improvement_ = np.resize(self.oob_improvement_,\n1387                                                   total_n_estimators)\n1388             else:\n1389                 self.oob_improvement_ = np.zeros((total_n_estimators,),\n1390                                                  dtype=np.float64)\n1391 \n1392     def _is_initialized(self):\n1393         return len(getattr(self, 'estimators_', [])) > 0\n1394 \n1395     def _check_initialized(self):\n1396         \"\"\"Check that the estimator is initialized, raising an error if not.\"\"\"\n1397         check_is_fitted(self)\n1398 \n1399     def fit(self, X, y, sample_weight=None, monitor=None):\n1400         \"\"\"Fit the gradient boosting model.\n1401 \n1402         Parameters\n1403         ----------\n1404         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n1405             The input samples. Internally, it will be converted to\n1406             ``dtype=np.float32`` and if a sparse matrix is provided\n1407             to a sparse ``csr_matrix``.\n1408 \n1409         y : array-like, shape (n_samples,)\n1410             Target values (strings or integers in classification, real numbers\n1411             in regression)\n1412             For classification, labels must correspond to classes.\n1413 \n1414         sample_weight : array-like, shape (n_samples,) or None\n1415             Sample weights. If None, then samples are equally weighted. Splits\n1416             that would create child nodes with net zero or negative weight are\n1417             ignored while searching for a split in each node. In the case of\n1418             classification, splits are also ignored if they would result in any\n1419             single class carrying a negative weight in either child node.\n1420 \n1421         monitor : callable, optional\n1422             The monitor is called after each iteration with the current\n1423             iteration, a reference to the estimator and the local variables of\n1424             ``_fit_stages`` as keyword arguments ``callable(i, self,\n1425             locals())``. If the callable returns ``True`` the fitting procedure\n1426             is stopped. The monitor can be used for various things such as\n1427             computing held-out estimates, early stopping, model introspect, and\n1428             snapshoting.\n1429 \n1430         Returns\n1431         -------\n1432         self : object\n1433         \"\"\"\n1434         # if not warmstart - clear the estimator state\n1435         if not self.warm_start:\n1436             self._clear_state()\n1437 \n1438         # Check input\n1439         # Since check_array converts both X and y to the same dtype, but the\n1440         # trees use different types for X and y, checking them separately.\n1441         X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)\n1442         n_samples, self.n_features_ = X.shape\n1443 \n1444         sample_weight_is_none = sample_weight is None\n1445         if sample_weight_is_none:\n1446             sample_weight = np.ones(n_samples, dtype=np.float32)\n1447         else:\n1448             sample_weight = column_or_1d(sample_weight, warn=True)\n1449             sample_weight_is_none = False\n1450 \n1451         check_consistent_length(X, y, sample_weight)\n1452 \n1453         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)\n1454         y = column_or_1d(y, warn=True)\n1455         y = self._validate_y(y, sample_weight)\n1456 \n1457         if self.n_iter_no_change is not None:\n1458             stratify = y if is_classifier(self) else None\n1459             X, X_val, y, y_val, sample_weight, sample_weight_val = (\n1460                 train_test_split(X, y, sample_weight,\n1461                                  random_state=self.random_state,\n1462                                  test_size=self.validation_fraction,\n1463                                  stratify=stratify))\n1464             if is_classifier(self):\n1465                 if self.n_classes_ != np.unique(y).shape[0]:\n1466                     # We choose to error here. The problem is that the init\n1467                     # estimator would be trained on y, which has some missing\n1468                     # classes now, so its predictions would not have the\n1469                     # correct shape.\n1470                     raise ValueError(\n1471                         'The training data after the early stopping split '\n1472                         'is missing some classes. Try using another random '\n1473                         'seed.'\n1474                     )\n1475         else:\n1476             X_val = y_val = sample_weight_val = None\n1477 \n1478         self._check_params()\n1479 \n1480         if not self._is_initialized():\n1481             # init state\n1482             self._init_state()\n1483 \n1484             # fit initial model and initialize raw predictions\n1485             if self.init_ == 'zero':\n1486                 raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),\n1487                                            dtype=np.float64)\n1488             else:\n1489                 # XXX clean this once we have a support_sample_weight tag\n1490                 if sample_weight_is_none:\n1491                     self.init_.fit(X, y)\n1492                 else:\n1493                     msg = (\"The initial estimator {} does not support sample \"\n1494                            \"weights.\".format(self.init_.__class__.__name__))\n1495                     try:\n1496                         self.init_.fit(X, y, sample_weight=sample_weight)\n1497                     except TypeError:  # regular estimator without SW support\n1498                         raise ValueError(msg)\n1499                     except ValueError as e:\n1500                         if \"pass parameters to specific steps of \"\\\n1501                            \"your pipeline using the \"\\\n1502                            \"stepname__parameter\" in str(e):  # pipeline\n1503                             raise ValueError(msg) from e\n1504                         else:  # regular estimator whose input checking failed\n1505                             raise\n1506 \n1507                 raw_predictions = \\\n1508                     self.loss_.get_init_raw_predictions(X, self.init_)\n1509 \n1510             begin_at_stage = 0\n1511 \n1512             # The rng state must be preserved if warm_start is True\n1513             self._rng = check_random_state(self.random_state)\n1514 \n1515         else:\n1516             # add more estimators to fitted model\n1517             # invariant: warm_start = True\n1518             if self.n_estimators < self.estimators_.shape[0]:\n1519                 raise ValueError('n_estimators=%d must be larger or equal to '\n1520                                  'estimators_.shape[0]=%d when '\n1521                                  'warm_start==True'\n1522                                  % (self.n_estimators,\n1523                                     self.estimators_.shape[0]))\n1524             begin_at_stage = self.estimators_.shape[0]\n1525             # The requirements of _decision_function (called in two lines\n1526             # below) are more constrained than fit. It accepts only CSR\n1527             # matrices.\n1528             X = check_array(X, dtype=DTYPE, order=\"C\", accept_sparse='csr')\n1529             raw_predictions = self._raw_predict(X)\n1530             self._resize_state()\n1531 \n1532         X_idx_sorted = None\n1533 \n1534         # fit the boosting stages\n1535         n_stages = self._fit_stages(\n1536             X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,\n1537             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n1538 \n1539         # change shape of arrays after fit (early-stopping or additional ests)\n1540         if n_stages != self.estimators_.shape[0]:\n1541             self.estimators_ = self.estimators_[:n_stages]\n1542             self.train_score_ = self.train_score_[:n_stages]\n1543             if hasattr(self, 'oob_improvement_'):\n1544                 self.oob_improvement_ = self.oob_improvement_[:n_stages]\n1545 \n1546         self.n_estimators_ = n_stages\n1547         return self\n1548 \n1549     def _fit_stages(self, X, y, raw_predictions, sample_weight, random_state,\n1550                     X_val, y_val, sample_weight_val,\n1551                     begin_at_stage=0, monitor=None, X_idx_sorted=None):\n1552         \"\"\"Iteratively fits the stages.\n1553 \n1554         For each stage it computes the progress (OOB, train score)\n1555         and delegates to ``_fit_stage``.\n1556         Returns the number of stages fit; might differ from ``n_estimators``\n1557         due to early stopping.\n1558         \"\"\"\n1559         n_samples = X.shape[0]\n1560         do_oob = self.subsample < 1.0\n1561         sample_mask = np.ones((n_samples, ), dtype=np.bool)\n1562         n_inbag = max(1, int(self.subsample * n_samples))\n1563         loss_ = self.loss_\n1564 \n1565         if self.verbose:\n1566             verbose_reporter = VerboseReporter(self.verbose)\n1567             verbose_reporter.init(self, begin_at_stage)\n1568 \n1569         X_csc = csc_matrix(X) if issparse(X) else None\n1570         X_csr = csr_matrix(X) if issparse(X) else None\n1571 \n1572         if self.n_iter_no_change is not None:\n1573             loss_history = np.full(self.n_iter_no_change, np.inf)\n1574             # We create a generator to get the predictions for X_val after\n1575             # the addition of each successive stage\n1576             y_val_pred_iter = self._staged_raw_predict(X_val)\n1577 \n1578         # perform boosting iterations\n1579         i = begin_at_stage\n1580         for i in range(begin_at_stage, self.n_estimators):\n1581 \n1582             # subsampling\n1583             if do_oob:\n1584                 sample_mask = _random_sample_mask(n_samples, n_inbag,\n1585                                                   random_state)\n1586                 # OOB score before adding this stage\n1587                 old_oob_score = loss_(y[~sample_mask],\n1588                                       raw_predictions[~sample_mask],\n1589                                       sample_weight[~sample_mask])\n1590 \n1591             # fit next stage of trees\n1592             raw_predictions = self._fit_stage(\n1593                 i, X, y, raw_predictions, sample_weight, sample_mask,\n1594                 random_state, X_idx_sorted, X_csc, X_csr)\n1595 \n1596             # track deviance (= loss)\n1597             if do_oob:\n1598                 self.train_score_[i] = loss_(y[sample_mask],\n1599                                              raw_predictions[sample_mask],\n1600                                              sample_weight[sample_mask])\n1601                 self.oob_improvement_[i] = (\n1602                     old_oob_score - loss_(y[~sample_mask],\n1603                                           raw_predictions[~sample_mask],\n1604                                           sample_weight[~sample_mask]))\n1605             else:\n1606                 # no need to fancy index w/ no subsampling\n1607                 self.train_score_[i] = loss_(y, raw_predictions, sample_weight)\n1608 \n1609             if self.verbose > 0:\n1610                 verbose_reporter.update(i, self)\n1611 \n1612             if monitor is not None:\n1613                 early_stopping = monitor(i, self, locals())\n1614                 if early_stopping:\n1615                     break\n1616 \n1617             # We also provide an early stopping based on the score from\n1618             # validation set (X_val, y_val), if n_iter_no_change is set\n1619             if self.n_iter_no_change is not None:\n1620                 # By calling next(y_val_pred_iter), we get the predictions\n1621                 # for X_val after the addition of the current stage\n1622                 validation_loss = loss_(y_val, next(y_val_pred_iter),\n1623                                         sample_weight_val)\n1624 \n1625                 # Require validation_score to be better (less) than at least\n1626                 # one of the last n_iter_no_change evaluations\n1627                 if np.any(validation_loss + self.tol < loss_history):\n1628                     loss_history[i % len(loss_history)] = validation_loss\n1629                 else:\n1630                     break\n1631 \n1632         return i + 1\n1633 \n1634     def _make_estimator(self, append=True):\n1635         # we don't need _make_estimator\n1636         raise NotImplementedError()\n1637 \n1638     def _raw_predict_init(self, X):\n1639         \"\"\"Check input and compute raw predictions of the init estimtor.\"\"\"\n1640         self._check_initialized()\n1641         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)\n1642         if X.shape[1] != self.n_features_:\n1643             raise ValueError(\"X.shape[1] should be {0:d}, not {1:d}.\".format(\n1644                 self.n_features_, X.shape[1]))\n1645         if self.init_ == 'zero':\n1646             raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),\n1647                                        dtype=np.float64)\n1648         else:\n1649             raw_predictions = self.loss_.get_init_raw_predictions(\n1650                 X, self.init_).astype(np.float64)\n1651         return raw_predictions\n1652 \n1653     def _raw_predict(self, X):\n1654         \"\"\"Return the sum of the trees raw predictions (+ init estimator).\"\"\"\n1655         raw_predictions = self._raw_predict_init(X)\n1656         predict_stages(self.estimators_, X, self.learning_rate,\n1657                        raw_predictions)\n1658         return raw_predictions\n1659 \n1660     def _staged_raw_predict(self, X):\n1661         \"\"\"Compute raw predictions of ``X`` for each iteration.\n1662 \n1663         This method allows monitoring (i.e. determine error on testing set)\n1664         after each stage.\n1665 \n1666         Parameters\n1667         ----------\n1668         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n1669             The input samples. Internally, it will be converted to\n1670             ``dtype=np.float32`` and if a sparse matrix is provided\n1671             to a sparse ``csr_matrix``.\n1672 \n1673         Returns\n1674         -------\n1675         raw_predictions : generator of array, shape (n_samples, k)\n1676             The raw predictions of the input samples. The order of the\n1677             classes corresponds to that in the attribute :term:`classes_`.\n1678             Regression and binary classification are special cases with\n1679             ``k == 1``, otherwise ``k==n_classes``.\n1680         \"\"\"\n1681         X = check_array(X, dtype=DTYPE, order=\"C\", accept_sparse='csr')\n1682         raw_predictions = self._raw_predict_init(X)\n1683         for i in range(self.estimators_.shape[0]):\n1684             predict_stage(self.estimators_, i, X, self.learning_rate,\n1685                           raw_predictions)\n1686             yield raw_predictions.copy()\n1687 \n1688     @property\n1689     def feature_importances_(self):\n1690         \"\"\"Return the feature importances (the higher, the more important the\n1691            feature).\n1692 \n1693         Returns\n1694         -------\n1695         feature_importances_ : array, shape (n_features,)\n1696             The values of this array sum to 1, unless all trees are single node\n1697             trees consisting of only the root node, in which case it will be an\n1698             array of zeros.\n1699         \"\"\"\n1700         self._check_initialized()\n1701 \n1702         relevant_trees = [tree\n1703                           for stage in self.estimators_ for tree in stage\n1704                           if tree.tree_.node_count > 1]\n1705         if not relevant_trees:\n1706             # degenerate case where all trees have only one node\n1707             return np.zeros(shape=self.n_features_, dtype=np.float64)\n1708 \n1709         relevant_feature_importances = [\n1710             tree.tree_.compute_feature_importances(normalize=False)\n1711             for tree in relevant_trees\n1712         ]\n1713         avg_feature_importances = np.mean(relevant_feature_importances,\n1714                                           axis=0, dtype=np.float64)\n1715         return avg_feature_importances / np.sum(avg_feature_importances)\n1716 \n1717     def _compute_partial_dependence_recursion(self, grid, target_features):\n1718         \"\"\"Fast partial dependence computation.\n1719 \n1720         Parameters\n1721         ----------\n1722         grid : ndarray, shape (n_samples, n_target_features)\n1723             The grid points on which the partial dependence should be\n1724             evaluated.\n1725         target_features : ndarray, shape (n_target_features)\n1726             The set of target features for which the partial dependence\n1727             should be evaluated.\n1728 \n1729         Returns\n1730         -------\n1731         averaged_predictions : ndarray, shape \\\n1732                 (n_trees_per_iteration, n_samples)\n1733             The value of the partial dependence function on each grid point.\n1734         \"\"\"\n1735         check_is_fitted(self,\n1736                         msg=\"'estimator' parameter must be a fitted estimator\")\n1737         if self.init is not None:\n1738             warnings.warn(\n1739                 'Using recursion method with a non-constant init predictor '\n1740                 'will lead to incorrect partial dependence values. '\n1741                 'Got init=%s.' % self.init,\n1742                 UserWarning\n1743             )\n1744         grid = np.asarray(grid, dtype=DTYPE, order='C')\n1745         n_estimators, n_trees_per_stage = self.estimators_.shape\n1746         averaged_predictions = np.zeros((n_trees_per_stage, grid.shape[0]),\n1747                                         dtype=np.float64, order='C')\n1748         for stage in range(n_estimators):\n1749             for k in range(n_trees_per_stage):\n1750                 tree = self.estimators_[stage, k].tree_\n1751                 tree.compute_partial_dependence(grid, target_features,\n1752                                                 averaged_predictions[k])\n1753         averaged_predictions *= self.learning_rate\n1754 \n1755         return averaged_predictions\n1756 \n1757     def _validate_y(self, y, sample_weight):\n1758         # 'sample_weight' is not utilised but is used for\n1759         # consistency with similar method _validate_y of GBC\n1760         self.n_classes_ = 1\n1761         if y.dtype.kind == 'O':\n1762             y = y.astype(DOUBLE)\n1763         # Default implementation\n1764         return y\n1765 \n1766     def apply(self, X):\n1767         \"\"\"Apply trees in the ensemble to X, return leaf indices.\n1768 \n1769         .. versionadded:: 0.17\n1770 \n1771         Parameters\n1772         ----------\n1773         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n1774             The input samples. Internally, its dtype will be converted to\n1775             ``dtype=np.float32``. If a sparse matrix is provided, it will\n1776             be converted to a sparse ``csr_matrix``.\n1777 \n1778         Returns\n1779         -------\n1780         X_leaves : array-like, shape (n_samples, n_estimators, n_classes)\n1781             For each datapoint x in X and for each tree in the ensemble,\n1782             return the index of the leaf x ends up in each estimator.\n1783             In the case of binary classification n_classes is 1.\n1784         \"\"\"\n1785 \n1786         self._check_initialized()\n1787         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)\n1788 \n1789         # n_classes will be equal to 1 in the binary classification or the\n1790         # regression case.\n1791         n_estimators, n_classes = self.estimators_.shape\n1792         leaves = np.zeros((X.shape[0], n_estimators, n_classes))\n1793 \n1794         for i in range(n_estimators):\n1795             for j in range(n_classes):\n1796                 estimator = self.estimators_[i, j]\n1797                 leaves[:, i, j] = estimator.apply(X, check_input=False)\n1798 \n1799         return leaves\n",
        "intended_behavior": "This class provides additional context to the issue."
    }
]