2025-10-18 17:28:19.571 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 17:28:22.139 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=339, output_tokens=12, cost=0.003750
2025-10-18 17:28:22.146 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-18 17:28:22.150 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 17:28:32.084 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=647, output_tokens=432, cost=0.019430
2025-10-18 17:28:32.095 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 17:28:40.182 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=786, output_tokens=340, cost=0.018060
2025-10-18 17:28:40.185 | DEBUG    | Extracted a valid json.
2025-10-18 17:28:40.185 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"CalibratedClassifierCV\")",
        "search_class(\"DBSCAN\")",
        "search_class(\"DummyClassifier\")",
        "search_class(\"DummyRegressor\")",
        "search_class(\"BaseBagging\")",
        "search_class(\"BaseForest\")",
        "search_class(\"BaseGradientBoosting\")",
        "search_class(\"IsotonicRegression\")",
        "search_class(\"KernelRidge\")",
        "search_class(\"GaussianNB\")",
        "search_class(\"BaseDiscreteNB\")",
        "search_class(\"KernelDensity\")",
        "search_class(\"BaseDecisionTree\")",
        "search_method_in_class(\"fit\", \"CalibratedClassifierCV\")",
        "search_method_in_class(\"partial_fit\", \"DBSCAN\")",
        "search_method_in_class(\"fit\", \"DummyClassifier\")",
        "search_method_in_class(\"fit\", \"DummyRegressor\")",
        "search_method_in_class(\"fit\", \"BaseBagging\")",
        "search_method_in_class(\"fit\", \"BaseForest\")",
        "search_method_in_class(\"fit\", \"BaseGradientBoosting\")",
        "search_method_in_class(\"fit\", \"IsotonicRegression\")",
        "search_method_in_class(\"fit\", \"KernelRidge\")",
        "search_method_in_class(\"fit\", \"GaussianNB\")",
        "search_method_in_class(\"fit\", \"BaseDiscreteNB\")",
        "search_method_in_class(\"fit\", \"KernelDensity\")",
        "search_method_in_class(\"fit\", \"BaseDecisionTree\")"
    ],
    "bug_locations": []
}
2025-10-18 17:28:40.596 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 17:28:40.597 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 17:29:01.479 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=14474, output_tokens=819, cost=0.169310
2025-10-18 17:29:01.497 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 17:29:23.563 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=15509, output_tokens=699, cost=0.176060
2025-10-18 17:29:23.580 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 17:29:35.602 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1053, output_tokens=544, cost=0.026850
2025-10-18 17:29:35.603 | DEBUG    | Extracted a valid json.
2025-10-18 17:29:35.603 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "sklearn/calibration.py",
            "class": "CalibratedClassifierCV",
            "method": "fit",
            "intended_behavior": "The fit method should utilize _check_sample_weight to validate and handle sample_weight efficiently and consistently with the rest of the sklearn library, ensuring any custom or ad-hoc handling is replaced or integrated with the standard utility method."
        },
        {
            "file": "sklearn/cluster/_dbscan.py",
            "class": "DBSCAN",
            "method": "fit",
            "intended_behavior": "While DBSCAN may not explicitly validate sample_weight in the same way other estimators do due to its unique implementation, ensuring any internal handling of sample_weight is consistent or verified through _check_sample_weight could improve uniformity across the library."
        },
        {
            "file": "sklearn/dummy.py",
            "class": "DummyClassifier",
            "method": "fit",
            "intended_behavior": "The fit method of DummyClassifier should incorporate _check_sample_weight for consistent sample weight validation and processing, replacing any custom logic currently implemented."
        },
        {
            "file": "sklearn/dummy.py",
            "class": "DummyRegressor",
            "method": "fit",
            "intended_behavior": "Similar to DummyClassifier, DummyRegressor's fit method should also adopt _check_sample_weight to ensure standardized handling of sample_weight."
        },
        {
            "file": "sklearn/ensemble/_bagging.py",
            "class": "BaseBagging",
            "method": "fit",
            "intended_behavior": "Adapt the fit method to employ _check_sample_weight for validating and processing sample_weight. This will maintain the feature's existing functionality while ensuring that weight handling adheres to the standardized approach used across the library."
        },
        {
            "file": "sklearn/ensemble/_forest.py",
            "class": "BaseForest",
            "method": "fit",
            "intended_behavior": "Ensure that fit method uses _check_sample_weight for validating and processing sample_weight in line with sklearn's standardized practices, ensuring consistency, and possibly enhancing error checking and input validation."
        },
        {
            "file": "sklearn/ensemble/_gb.py",
            "class": "BaseGradientBoosting",
            "method": "fit",
            "intended_behavior": "The fit function should be updated to utilize _check_sample_weight for handling sample_weight, replacing or complementing any specific logic currently in place with the standardized utility function."
        }
    ]
}
2025-10-18 17:29:35.624 | DEBUG    | Bug location extracted successfully: [<file>sklearn/calibration.py</file>
<class>CalibratedClassifierCV</class>
<method>fit</method>
<code>
114     def fit(self, X, y, sample_weight=None):
115         """Fit the calibrated model
116 
117         Parameters
118         ----------
119         X : array-like, shape (n_samples, n_features)
120             Training data.
121 
122         y : array-like, shape (n_samples,)
123             Target values.
124 
125         sample_weight : array-like of shape (n_samples,), default=None
126             Sample weights. If None, then samples are equally weighted.
127 
128         Returns
129         -------
130         self : object
131             Returns an instance of self.
132         """
133         X, y = check_X_y(X, y, accept_sparse=['csc', 'csr', 'coo'],
134                          force_all_finite=False, allow_nd=True)
135         X, y = indexable(X, y)
136         le = LabelBinarizer().fit(y)
137         self.classes_ = le.classes_
138 
139         # Check that each cross-validation fold can have at least one
140         # example per class
141         n_folds = self.cv if isinstance(self.cv, int) \
142             else self.cv.n_folds if hasattr(self.cv, "n_folds") else None
143         if n_folds and \
144                 np.any([np.sum(y == class_) < n_folds for class_ in
145                         self.classes_]):
146             raise ValueError("Requesting %d-fold cross-validation but provided"
147                              " less than %d examples for at least one class."
148                              % (n_folds, n_folds))
149 
150         self.calibrated_classifiers_ = []
151         if self.base_estimator is None:
152             # we want all classifiers that don't expose a random_state
153             # to be deterministic (and we don't want to expose this one).
154             base_estimator = LinearSVC(random_state=0)
155         else:
156             base_estimator = self.base_estimator
157 
158         if self.cv == "prefit":
159             calibrated_classifier = _CalibratedClassifier(
160                 base_estimator, method=self.method)
161             if sample_weight is not None:
162                 calibrated_classifier.fit(X, y, sample_weight)
163             else:
164                 calibrated_classifier.fit(X, y)
165             self.calibrated_classifiers_.append(calibrated_classifier)
166         else:
167             cv = check_cv(self.cv, y, classifier=True)
168             fit_parameters = signature(base_estimator.fit).parameters
169             estimator_name = type(base_estimator).__name__
170             if (sample_weight is not None
171                     and "sample_weight" not in fit_parameters):
172                 warnings.warn("%s does not support sample_weight. Samples"
173                               " weights are only used for the calibration"
174                               " itself." % estimator_name)
175                 sample_weight = check_array(sample_weight, ensure_2d=False)
176                 base_estimator_sample_weight = None
177             else:
178                 if sample_weight is not None:
179                     sample_weight = check_array(sample_weight, ensure_2d=False)
180                     check_consistent_length(y, sample_weight)
181                 base_estimator_sample_weight = sample_weight
182             for train, test in cv.split(X, y):
183                 this_estimator = clone(base_estimator)
184                 if base_estimator_sample_weight is not None:
185                     this_estimator.fit(
186                         X[train], y[train],
187                         sample_weight=base_estimator_sample_weight[train])
188                 else:
189                     this_estimator.fit(X[train], y[train])
190 
191                 calibrated_classifier = _CalibratedClassifier(
192                     this_estimator, method=self.method,
193                     classes=self.classes_)
194                 if sample_weight is not None:
195                     calibrated_classifier.fit(X[test], y[test],
196                                               sample_weight[test])
197                 else:
198                     calibrated_classifier.fit(X[test], y[test])
199                 self.calibrated_classifiers_.append(calibrated_classifier)
200 
201         return self

</code><intended_behavior>The fit method should utilize _check_sample_weight to validate and handle sample_weight efficiently and consistently with the rest of the sklearn library, ensuring any custom or ad-hoc handling is replaced or integrated with the standard utility method.</intended_behavior>, <file>sklearn/calibration.py</file>
<class>CalibratedClassifierCV</class>
<method>None</method>
<code>
31 class CalibratedClassifierCV(BaseEstimator, ClassifierMixin,
32                              MetaEstimatorMixin):
33     """Probability calibration with isotonic regression or sigmoid.
34 
35     See glossary entry for :term:`cross-validation estimator`.
36 
37     With this class, the base_estimator is fit on the train set of the
38     cross-validation generator and the test set is used for calibration.
39     The probabilities for each of the folds are then averaged
40     for prediction. In case that cv="prefit" is passed to __init__,
41     it is assumed that base_estimator has been fitted already and all
42     data is used for calibration. Note that data for fitting the
43     classifier and for calibrating it must be disjoint.
44 
45     Read more in the :ref:`User Guide <calibration>`.
46 
47     Parameters
48     ----------
49     base_estimator : instance BaseEstimator
50         The classifier whose output decision function needs to be calibrated
51         to offer more accurate predict_proba outputs. If cv=prefit, the
52         classifier must have been fit already on data.
53 
54     method : 'sigmoid' or 'isotonic'
55         The method to use for calibration. Can be 'sigmoid' which
56         corresponds to Platt's method or 'isotonic' which is a
57         non-parametric approach. It is not advised to use isotonic calibration
58         with too few calibration samples ``(<<1000)`` since it tends to
59         overfit.
60         Use sigmoids (Platt's calibration) in this case.
61 
62     cv : integer, cross-validation generator, iterable or "prefit", optional
63         Determines the cross-validation splitting strategy.
64         Possible inputs for cv are:
65 
66         - None, to use the default 5-fold cross-validation,
67         - integer, to specify the number of folds.
68         - :term:`CV splitter`,
69         - An iterable yielding (train, test) splits as arrays of indices.
70 
71         For integer/None inputs, if ``y`` is binary or multiclass,
72         :class:`sklearn.model_selection.StratifiedKFold` is used. If ``y`` is
73         neither binary nor multiclass, :class:`sklearn.model_selection.KFold`
74         is used.
75 
76         Refer :ref:`User Guide <cross_validation>` for the various
77         cross-validation strategies that can be used here.
78 
79         If "prefit" is passed, it is assumed that base_estimator has been
80         fitted already and all data is used for calibration.
81 
82         .. versionchanged:: 0.22
83             ``cv`` default value if None changed from 3-fold to 5-fold.
84 
85     Attributes
86     ----------
87     classes_ : array, shape (n_classes)
88         The class labels.
89 
90     calibrated_classifiers_ : list (len() equal to cv or 1 if cv == "prefit")
91         The list of calibrated classifiers, one for each crossvalidation fold,
92         which has been fitted on all but the validation fold and calibrated
93         on the validation fold.
94 
95     References
96     ----------
97     .. [1] Obtaining calibrated probability estimates from decision trees
98            and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001
99 
100     .. [2] Transforming Classifier Scores into Accurate Multiclass
101            Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)
102 
103     .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to
104            Regularized Likelihood Methods, J. Platt, (1999)
105 
106     .. [4] Predicting Good Probabilities with Supervised Learning,
107            A. Niculescu-Mizil & R. Caruana, ICML 2005
108     """
109     def __init__(self, base_estimator=None, method='sigmoid', cv=None):
110         self.base_estimator = base_estimator
111         self.method = method
112         self.cv = cv
113 
114     def fit(self, X, y, sample_weight=None):
115         """Fit the calibrated model
116 
117         Parameters
118         ----------
119         X : array-like, shape (n_samples, n_features)
120             Training data.
121 
122         y : array-like, shape (n_samples,)
123             Target values.
124 
125         sample_weight : array-like of shape (n_samples,), default=None
126             Sample weights. If None, then samples are equally weighted.
127 
128         Returns
129         -------
130         self : object
131             Returns an instance of self.
132         """
133         X, y = check_X_y(X, y, accept_sparse=['csc', 'csr', 'coo'],
134                          force_all_finite=False, allow_nd=True)
135         X, y = indexable(X, y)
136         le = LabelBinarizer().fit(y)
137         self.classes_ = le.classes_
138 
139         # Check that each cross-validation fold can have at least one
140         # example per class
141         n_folds = self.cv if isinstance(self.cv, int) \
142             else self.cv.n_folds if hasattr(self.cv, "n_folds") else None
143         if n_folds and \
144                 np.any([np.sum(y == class_) < n_folds for class_ in
145                         self.classes_]):
146             raise ValueError("Requesting %d-fold cross-validation but provided"
147                              " less than %d examples for at least one class."
148                              % (n_folds, n_folds))
149 
150         self.calibrated_classifiers_ = []
151         if self.base_estimator is None:
152             # we want all classifiers that don't expose a random_state
153             # to be deterministic (and we don't want to expose this one).
154             base_estimator = LinearSVC(random_state=0)
155         else:
156             base_estimator = self.base_estimator
157 
158         if self.cv == "prefit":
159             calibrated_classifier = _CalibratedClassifier(
160                 base_estimator, method=self.method)
161             if sample_weight is not None:
162                 calibrated_classifier.fit(X, y, sample_weight)
163             else:
164                 calibrated_classifier.fit(X, y)
165             self.calibrated_classifiers_.append(calibrated_classifier)
166         else:
167             cv = check_cv(self.cv, y, classifier=True)
168             fit_parameters = signature(base_estimator.fit).parameters
169             estimator_name = type(base_estimator).__name__
170             if (sample_weight is not None
171                     and "sample_weight" not in fit_parameters):
172                 warnings.warn("%s does not support sample_weight. Samples"
173                               " weights are only used for the calibration"
174                               " itself." % estimator_name)
175                 sample_weight = check_array(sample_weight, ensure_2d=False)
176                 base_estimator_sample_weight = None
177             else:
178                 if sample_weight is not None:
179                     sample_weight = check_array(sample_weight, ensure_2d=False)
180                     check_consistent_length(y, sample_weight)
181                 base_estimator_sample_weight = sample_weight
182             for train, test in cv.split(X, y):
183                 this_estimator = clone(base_estimator)
184                 if base_estimator_sample_weight is not None:
185                     this_estimator.fit(
186                         X[train], y[train],
187                         sample_weight=base_estimator_sample_weight[train])
188                 else:
189                     this_estimator.fit(X[train], y[train])
190 
191                 calibrated_classifier = _CalibratedClassifier(
192                     this_estimator, method=self.method,
193                     classes=self.classes_)
194                 if sample_weight is not None:
195                     calibrated_classifier.fit(X[test], y[test],
196                                               sample_weight[test])
197                 else:
198                     calibrated_classifier.fit(X[test], y[test])
199                 self.calibrated_classifiers_.append(calibrated_classifier)
200 
201         return self
202 
203     def predict_proba(self, X):
204         """Posterior probabilities of classification
205 
206         This function returns posterior probabilities of classification
207         according to each class on an array of test vectors X.
208 
209         Parameters
210         ----------
211         X : array-like, shape (n_samples, n_features)
212             The samples.
213 
214         Returns
215         -------
216         C : array, shape (n_samples, n_classes)
217             The predicted probas.
218         """
219         check_is_fitted(self)
220         X = check_array(X, accept_sparse=['csc', 'csr', 'coo'],
221                         force_all_finite=False)
222         # Compute the arithmetic mean of the predictions of the calibrated
223         # classifiers
224         mean_proba = np.zeros((X.shape[0], len(self.classes_)))
225         for calibrated_classifier in self.calibrated_classifiers_:
226             proba = calibrated_classifier.predict_proba(X)
227             mean_proba += proba
228 
229         mean_proba /= len(self.calibrated_classifiers_)
230 
231         return mean_proba
232 
233     def predict(self, X):
234         """Predict the target of new samples. Can be different from the
235         prediction of the uncalibrated classifier.
236 
237         Parameters
238         ----------
239         X : array-like, shape (n_samples, n_features)
240             The samples.
241 
242         Returns
243         -------
244         C : array, shape (n_samples,)
245             The predicted class.
246         """
247         check_is_fitted(self)
248         return self.classes_[np.argmax(self.predict_proba(X), axis=1)]

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/cluster/_dbscan.py</file>
<class>DBSCAN</class>
<method>fit</method>
<code>
284     def fit(self, X, y=None, sample_weight=None):
285         """Perform DBSCAN clustering from features, or distance matrix.
286 
287         Parameters
288         ----------
289         X : array-like or sparse matrix, shape (n_samples, n_features), or \
290             (n_samples, n_samples)
291             Training instances to cluster, or distances between instances if
292             ``metric='precomputed'``. If a sparse matrix is provided, it will
293             be converted into a sparse ``csr_matrix``.
294 
295         sample_weight : array, shape (n_samples,), optional
296             Weight of each sample, such that a sample with a weight of at least
297             ``min_samples`` is by itself a core sample; a sample with a
298             negative weight may inhibit its eps-neighbor from being core.
299             Note that weights are absolute, and default to 1.
300 
301         y : Ignored
302             Not used, present here for API consistency by convention.
303 
304         Returns
305         -------
306         self
307 
308         """
309         X = check_array(X, accept_sparse='csr')
310 
311         if not self.eps > 0.0:
312             raise ValueError("eps must be positive.")
313 
314         if sample_weight is not None:
315             sample_weight = np.asarray(sample_weight)
316             check_consistent_length(X, sample_weight)
317 
318         # Calculate neighborhood for all samples. This leaves the original
319         # point in, which needs to be considered later (i.e. point i is in the
320         # neighborhood of point i. While True, its useless information)
321         if self.metric == 'precomputed' and sparse.issparse(X):
322             # set the diagonal to explicit values, as a point is its own
323             # neighbor
324             with warnings.catch_warnings():
325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)
326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place
327 
328         neighbors_model = NearestNeighbors(
329             radius=self.eps, algorithm=self.algorithm,
330             leaf_size=self.leaf_size, metric=self.metric,
331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)
332         neighbors_model.fit(X)
333         # This has worst case O(n^2) memory complexity
334         neighborhoods = neighbors_model.radius_neighbors(X,
335                                                          return_distance=False)
336 
337         if sample_weight is None:
338             n_neighbors = np.array([len(neighbors)
339                                     for neighbors in neighborhoods])
340         else:
341             n_neighbors = np.array([np.sum(sample_weight[neighbors])
342                                     for neighbors in neighborhoods])
343 
344         # Initially, all samples are noise.
345         labels = np.full(X.shape[0], -1, dtype=np.intp)
346 
347         # A list of all core samples found.
348         core_samples = np.asarray(n_neighbors >= self.min_samples,
349                                   dtype=np.uint8)
350         dbscan_inner(core_samples, neighborhoods, labels)
351 
352         self.core_sample_indices_ = np.where(core_samples)[0]
353         self.labels_ = labels
354 
355         if len(self.core_sample_indices_):
356             # fix for scipy sparse indexing issue
357             self.components_ = X[self.core_sample_indices_].copy()
358         else:
359             # no core samples
360             self.components_ = np.empty((0, X.shape[1]))
361         return self

</code><intended_behavior>While DBSCAN may not explicitly validate sample_weight in the same way other estimators do due to its unique implementation, ensuring any internal handling of sample_weight is consistent or verified through _check_sample_weight could improve uniformity across the library.</intended_behavior>, <file>sklearn/cluster/_dbscan.py</file>
<class>DBSCAN</class>
<method>None</method>
<code>
147 class DBSCAN(ClusterMixin, BaseEstimator):
148     """Perform DBSCAN clustering from vector array or distance matrix.
149 
150     DBSCAN - Density-Based Spatial Clustering of Applications with Noise.
151     Finds core samples of high density and expands clusters from them.
152     Good for data which contains clusters of similar density.
153 
154     Read more in the :ref:`User Guide <dbscan>`.
155 
156     Parameters
157     ----------
158     eps : float, optional
159         The maximum distance between two samples for one to be considered
160         as in the neighborhood of the other. This is not a maximum bound
161         on the distances of points within a cluster. This is the most
162         important DBSCAN parameter to choose appropriately for your data set
163         and distance function.
164 
165     min_samples : int, optional
166         The number of samples (or total weight) in a neighborhood for a point
167         to be considered as a core point. This includes the point itself.
168 
169     metric : string, or callable
170         The metric to use when calculating distance between instances in a
171         feature array. If metric is a string or callable, it must be one of
172         the options allowed by :func:`sklearn.metrics.pairwise_distances` for
173         its metric parameter.
174         If metric is "precomputed", X is assumed to be a distance matrix and
175         must be square. X may be a :term:`Glossary <sparse graph>`, in which
176         case only "nonzero" elements may be considered neighbors for DBSCAN.
177 
178         .. versionadded:: 0.17
179            metric *precomputed* to accept precomputed sparse matrix.
180 
181     metric_params : dict, optional
182         Additional keyword arguments for the metric function.
183 
184         .. versionadded:: 0.19
185 
186     algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional
187         The algorithm to be used by the NearestNeighbors module
188         to compute pointwise distances and find nearest neighbors.
189         See NearestNeighbors module documentation for details.
190 
191     leaf_size : int, optional (default = 30)
192         Leaf size passed to BallTree or cKDTree. This can affect the speed
193         of the construction and query, as well as the memory required
194         to store the tree. The optimal value depends
195         on the nature of the problem.
196 
197     p : float, optional
198         The power of the Minkowski metric to be used to calculate distance
199         between points.
200 
201     n_jobs : int or None, optional (default=None)
202         The number of parallel jobs to run.
203         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
204         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
205         for more details.
206 
207     Attributes
208     ----------
209     core_sample_indices_ : array, shape = [n_core_samples]
210         Indices of core samples.
211 
212     components_ : array, shape = [n_core_samples, n_features]
213         Copy of each core sample found by training.
214 
215     labels_ : array, shape = [n_samples]
216         Cluster labels for each point in the dataset given to fit().
217         Noisy samples are given the label -1.
218 
219     Examples
220     --------
221     >>> from sklearn.cluster import DBSCAN
222     >>> import numpy as np
223     >>> X = np.array([[1, 2], [2, 2], [2, 3],
224     ...               [8, 7], [8, 8], [25, 80]])
225     >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)
226     >>> clustering.labels_
227     array([ 0,  0,  0,  1,  1, -1])
228     >>> clustering
229     DBSCAN(eps=3, min_samples=2)
230 
231     See also
232     --------
233     OPTICS
234         A similar clustering at multiple values of eps. Our implementation
235         is optimized for memory usage.
236 
237     Notes
238     -----
239     For an example, see :ref:`examples/cluster/plot_dbscan.py
240     <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.
241 
242     This implementation bulk-computes all neighborhood queries, which increases
243     the memory complexity to O(n.d) where d is the average number of neighbors,
244     while original DBSCAN had memory complexity O(n). It may attract a higher
245     memory complexity when querying these nearest neighborhoods, depending
246     on the ``algorithm``.
247 
248     One way to avoid the query complexity is to pre-compute sparse
249     neighborhoods in chunks using
250     :func:`NearestNeighbors.radius_neighbors_graph
251     <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with
252     ``mode='distance'``, then using ``metric='precomputed'`` here.
253 
254     Another way to reduce memory and computation time is to remove
255     (near-)duplicate points and use ``sample_weight`` instead.
256 
257     :class:`cluster.OPTICS` provides a similar clustering with lower memory
258     usage.
259 
260     References
261     ----------
262     Ester, M., H. P. Kriegel, J. Sander, and X. Xu, "A Density-Based
263     Algorithm for Discovering Clusters in Large Spatial Databases with Noise".
264     In: Proceedings of the 2nd International Conference on Knowledge Discovery
265     and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996
266 
267     Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).
268     DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.
269     ACM Transactions on Database Systems (TODS), 42(3), 19.
270     """
271 
272     def __init__(self, eps=0.5, min_samples=5, metric='euclidean',
273                  metric_params=None, algorithm='auto', leaf_size=30, p=None,
274                  n_jobs=None):
275         self.eps = eps
276         self.min_samples = min_samples
277         self.metric = metric
278         self.metric_params = metric_params
279         self.algorithm = algorithm
280         self.leaf_size = leaf_size
281         self.p = p
282         self.n_jobs = n_jobs
283 
284     def fit(self, X, y=None, sample_weight=None):
285         """Perform DBSCAN clustering from features, or distance matrix.
286 
287         Parameters
288         ----------
289         X : array-like or sparse matrix, shape (n_samples, n_features), or \
290             (n_samples, n_samples)
291             Training instances to cluster, or distances between instances if
292             ``metric='precomputed'``. If a sparse matrix is provided, it will
293             be converted into a sparse ``csr_matrix``.
294 
295         sample_weight : array, shape (n_samples,), optional
296             Weight of each sample, such that a sample with a weight of at least
297             ``min_samples`` is by itself a core sample; a sample with a
298             negative weight may inhibit its eps-neighbor from being core.
299             Note that weights are absolute, and default to 1.
300 
301         y : Ignored
302             Not used, present here for API consistency by convention.
303 
304         Returns
305         -------
306         self
307 
308         """
309         X = check_array(X, accept_sparse='csr')
310 
311         if not self.eps > 0.0:
312             raise ValueError("eps must be positive.")
313 
314         if sample_weight is not None:
315             sample_weight = np.asarray(sample_weight)
316             check_consistent_length(X, sample_weight)
317 
318         # Calculate neighborhood for all samples. This leaves the original
319         # point in, which needs to be considered later (i.e. point i is in the
320         # neighborhood of point i. While True, its useless information)
321         if self.metric == 'precomputed' and sparse.issparse(X):
322             # set the diagonal to explicit values, as a point is its own
323             # neighbor
324             with warnings.catch_warnings():
325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)
326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place
327 
328         neighbors_model = NearestNeighbors(
329             radius=self.eps, algorithm=self.algorithm,
330             leaf_size=self.leaf_size, metric=self.metric,
331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)
332         neighbors_model.fit(X)
333         # This has worst case O(n^2) memory complexity
334         neighborhoods = neighbors_model.radius_neighbors(X,
335                                                          return_distance=False)
336 
337         if sample_weight is None:
338             n_neighbors = np.array([len(neighbors)
339                                     for neighbors in neighborhoods])
340         else:
341             n_neighbors = np.array([np.sum(sample_weight[neighbors])
342                                     for neighbors in neighborhoods])
343 
344         # Initially, all samples are noise.
345         labels = np.full(X.shape[0], -1, dtype=np.intp)
346 
347         # A list of all core samples found.
348         core_samples = np.asarray(n_neighbors >= self.min_samples,
349                                   dtype=np.uint8)
350         dbscan_inner(core_samples, neighborhoods, labels)
351 
352         self.core_sample_indices_ = np.where(core_samples)[0]
353         self.labels_ = labels
354 
355         if len(self.core_sample_indices_):
356             # fix for scipy sparse indexing issue
357             self.components_ = X[self.core_sample_indices_].copy()
358         else:
359             # no core samples
360             self.components_ = np.empty((0, X.shape[1]))
361         return self
362 
363     def fit_predict(self, X, y=None, sample_weight=None):
364         """Perform DBSCAN clustering from features or distance matrix,
365         and return cluster labels.
366 
367         Parameters
368         ----------
369         X : array-like or sparse matrix, shape (n_samples, n_features), or \
370             (n_samples, n_samples)
371             Training instances to cluster, or distances between instances if
372             ``metric='precomputed'``. If a sparse matrix is provided, it will
373             be converted into a sparse ``csr_matrix``.
374 
375         sample_weight : array, shape (n_samples,), optional
376             Weight of each sample, such that a sample with a weight of at least
377             ``min_samples`` is by itself a core sample; a sample with a
378             negative weight may inhibit its eps-neighbor from being core.
379             Note that weights are absolute, and default to 1.
380 
381         y : Ignored
382             Not used, present here for API consistency by convention.
383 
384         Returns
385         -------
386         labels : ndarray, shape (n_samples,)
387             Cluster labels. Noisy samples are given the label -1.
388         """
389         self.fit(X, sample_weight=sample_weight)
390         return self.labels_

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/dummy.py</file>
<class>DummyClassifier</class>
<method>fit</method>
<code>
101     def fit(self, X, y, sample_weight=None):
102         """Fit the random classifier.
103 
104         Parameters
105         ----------
106         X : {array-like, object with finite length or shape}
107             Training data, requires length = n_samples
108 
109         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
110             Target values.
111 
112         sample_weight : array-like of shape (n_samples,), default=None
113             Sample weights.
114 
115         Returns
116         -------
117         self : object
118         """
119         allowed_strategies = ("most_frequent", "stratified", "uniform",
120                               "constant", "prior")
121         if self.strategy not in allowed_strategies:
122             raise ValueError("Unknown strategy type: %s, expected one of %s."
123                              % (self.strategy, allowed_strategies))
124 
125         if self.strategy == "uniform" and sp.issparse(y):
126             y = y.toarray()
127             warnings.warn('A local copy of the target data has been converted '
128                           'to a numpy array. Predicting on sparse target data '
129                           'with the uniform strategy would not save memory '
130                           'and would be slower.',
131                           UserWarning)
132 
133         self.sparse_output_ = sp.issparse(y)
134 
135         if not self.sparse_output_:
136             y = np.asarray(y)
137             y = np.atleast_1d(y)
138 
139         if y.ndim == 1:
140             y = np.reshape(y, (-1, 1))
141 
142         self.n_outputs_ = y.shape[1]
143 
144         check_consistent_length(X, y, sample_weight)
145 
146         if self.strategy == "constant":
147             if self.constant is None:
148                 raise ValueError("Constant target value has to be specified "
149                                  "when the constant strategy is used.")
150             else:
151                 constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))
152                 if constant.shape[0] != self.n_outputs_:
153                     raise ValueError("Constant target value should have "
154                                      "shape (%d, 1)." % self.n_outputs_)
155 
156         (self.classes_,
157          self.n_classes_,
158          self.class_prior_) = class_distribution(y, sample_weight)
159 
160         if self.strategy == "constant":
161             for k in range(self.n_outputs_):
162                 if not any(constant[k][0] == c for c in self.classes_[k]):
163                     # Checking in case of constant strategy if the constant
164                     # provided by the user is in y.
165                     err_msg = ("The constant target value must be present in "
166                                "the training data. You provided constant={}. "
167                                "Possible values are: {}."
168                                .format(self.constant, list(self.classes_[k])))
169                     raise ValueError(err_msg)
170 
171         if self.n_outputs_ == 1:
172             self.n_classes_ = self.n_classes_[0]
173             self.classes_ = self.classes_[0]
174             self.class_prior_ = self.class_prior_[0]
175 
176         return self

</code><intended_behavior>The fit method of DummyClassifier should incorporate _check_sample_weight for consistent sample weight validation and processing, replacing any custom logic currently implemented.</intended_behavior>, <file>sklearn/dummy.py</file>
<class>DummyClassifier</class>
<method>None</method>
<code>
23 class DummyClassifier(MultiOutputMixin, ClassifierMixin, BaseEstimator):
24     """
25     DummyClassifier is a classifier that makes predictions using simple rules.
26 
27     This classifier is useful as a simple baseline to compare with other
28     (real) classifiers. Do not use it for real problems.
29 
30     Read more in the :ref:`User Guide <dummy_estimators>`.
31 
32     Parameters
33     ----------
34     strategy : str, default="stratified"
35         Strategy to use to generate predictions.
36 
37         * "stratified": generates predictions by respecting the training
38           set's class distribution.
39         * "most_frequent": always predicts the most frequent label in the
40           training set.
41         * "prior": always predicts the class that maximizes the class prior
42           (like "most_frequent") and ``predict_proba`` returns the class prior.
43         * "uniform": generates predictions uniformly at random.
44         * "constant": always predicts a constant label that is provided by
45           the user. This is useful for metrics that evaluate a non-majority
46           class
47 
48           .. versionadded:: 0.17
49              Dummy Classifier now supports prior fitting strategy using
50              parameter *prior*.
51 
52     random_state : int, RandomState instance or None, optional, default=None
53         If int, random_state is the seed used by the random number generator;
54         If RandomState instance, random_state is the random number generator;
55         If None, the random number generator is the RandomState instance used
56         by `np.random`.
57 
58     constant : int or str or array-like of shape (n_outputs,)
59         The explicit constant as predicted by the "constant" strategy. This
60         parameter is useful only for the "constant" strategy.
61 
62     Attributes
63     ----------
64     classes_ : array or list of array of shape (n_classes,)
65         Class labels for each output.
66 
67     n_classes_ : array or list of array of shape (n_classes,)
68         Number of label for each output.
69 
70     class_prior_ : array or list of array of shape (n_classes,)
71         Probability of each class for each output.
72 
73     n_outputs_ : int,
74         Number of outputs.
75 
76     sparse_output_ : bool,
77         True if the array returned from predict is to be in sparse CSC format.
78         Is automatically set to True if the input y is passed in sparse format.
79 
80     Examples
81     --------
82     >>> import numpy as np
83     >>> from sklearn.dummy import DummyClassifier
84     >>> X = np.array([-1, 1, 1, 1])
85     >>> y = np.array([0, 1, 1, 1])
86     >>> dummy_clf = DummyClassifier(strategy="most_frequent")
87     >>> dummy_clf.fit(X, y)
88     DummyClassifier(strategy='most_frequent')
89     >>> dummy_clf.predict(X)
90     array([1, 1, 1, 1])
91     >>> dummy_clf.score(X, y)
92     0.75
93     """
94 
95     def __init__(self, strategy="stratified", random_state=None,
96                  constant=None):
97         self.strategy = strategy
98         self.random_state = random_state
99         self.constant = constant
100 
101     def fit(self, X, y, sample_weight=None):
102         """Fit the random classifier.
103 
104         Parameters
105         ----------
106         X : {array-like, object with finite length or shape}
107             Training data, requires length = n_samples
108 
109         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
110             Target values.
111 
112         sample_weight : array-like of shape (n_samples,), default=None
113             Sample weights.
114 
115         Returns
116         -------
117         self : object
118         """
119         allowed_strategies = ("most_frequent", "stratified", "uniform",
120                               "constant", "prior")
121         if self.strategy not in allowed_strategies:
122             raise ValueError("Unknown strategy type: %s, expected one of %s."
123                              % (self.strategy, allowed_strategies))
124 
125         if self.strategy == "uniform" and sp.issparse(y):
126             y = y.toarray()
127             warnings.warn('A local copy of the target data has been converted '
128                           'to a numpy array. Predicting on sparse target data '
129                           'with the uniform strategy would not save memory '
130                           'and would be slower.',
131                           UserWarning)
132 
133         self.sparse_output_ = sp.issparse(y)
134 
135         if not self.sparse_output_:
136             y = np.asarray(y)
137             y = np.atleast_1d(y)
138 
139         if y.ndim == 1:
140             y = np.reshape(y, (-1, 1))
141 
142         self.n_outputs_ = y.shape[1]
143 
144         check_consistent_length(X, y, sample_weight)
145 
146         if self.strategy == "constant":
147             if self.constant is None:
148                 raise ValueError("Constant target value has to be specified "
149                                  "when the constant strategy is used.")
150             else:
151                 constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))
152                 if constant.shape[0] != self.n_outputs_:
153                     raise ValueError("Constant target value should have "
154                                      "shape (%d, 1)." % self.n_outputs_)
155 
156         (self.classes_,
157          self.n_classes_,
158          self.class_prior_) = class_distribution(y, sample_weight)
159 
160         if self.strategy == "constant":
161             for k in range(self.n_outputs_):
162                 if not any(constant[k][0] == c for c in self.classes_[k]):
163                     # Checking in case of constant strategy if the constant
164                     # provided by the user is in y.
165                     err_msg = ("The constant target value must be present in "
166                                "the training data. You provided constant={}. "
167                                "Possible values are: {}."
168                                .format(self.constant, list(self.classes_[k])))
169                     raise ValueError(err_msg)
170 
171         if self.n_outputs_ == 1:
172             self.n_classes_ = self.n_classes_[0]
173             self.classes_ = self.classes_[0]
174             self.class_prior_ = self.class_prior_[0]
175 
176         return self
177 
178     def predict(self, X):
179         """Perform classification on test vectors X.
180 
181         Parameters
182         ----------
183         X : {array-like, object with finite length or shape}
184             Training data, requires length = n_samples
185 
186         Returns
187         -------
188         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
189             Predicted target values for X.
190         """
191         check_is_fitted(self)
192 
193         # numpy random_state expects Python int and not long as size argument
194         # under Windows
195         n_samples = _num_samples(X)
196         rs = check_random_state(self.random_state)
197 
198         n_classes_ = self.n_classes_
199         classes_ = self.classes_
200         class_prior_ = self.class_prior_
201         constant = self.constant
202         if self.n_outputs_ == 1:
203             # Get same type even for self.n_outputs_ == 1
204             n_classes_ = [n_classes_]
205             classes_ = [classes_]
206             class_prior_ = [class_prior_]
207             constant = [constant]
208         # Compute probability only once
209         if self.strategy == "stratified":
210             proba = self.predict_proba(X)
211             if self.n_outputs_ == 1:
212                 proba = [proba]
213 
214         if self.sparse_output_:
215             class_prob = None
216             if self.strategy in ("most_frequent", "prior"):
217                 classes_ = [np.array([cp.argmax()]) for cp in class_prior_]
218 
219             elif self.strategy == "stratified":
220                 class_prob = class_prior_
221 
222             elif self.strategy == "uniform":
223                 raise ValueError("Sparse target prediction is not "
224                                  "supported with the uniform strategy")
225 
226             elif self.strategy == "constant":
227                 classes_ = [np.array([c]) for c in constant]
228 
229             y = _random_choice_csc(n_samples, classes_, class_prob,
230                                   self.random_state)
231         else:
232             if self.strategy in ("most_frequent", "prior"):
233                 y = np.tile([classes_[k][class_prior_[k].argmax()] for
234                              k in range(self.n_outputs_)], [n_samples, 1])
235 
236             elif self.strategy == "stratified":
237                 y = np.vstack([classes_[k][proba[k].argmax(axis=1)] for
238                                k in range(self.n_outputs_)]).T
239 
240             elif self.strategy == "uniform":
241                 ret = [classes_[k][rs.randint(n_classes_[k], size=n_samples)]
242                        for k in range(self.n_outputs_)]
243                 y = np.vstack(ret).T
244 
245             elif self.strategy == "constant":
246                 y = np.tile(self.constant, (n_samples, 1))
247 
248             if self.n_outputs_ == 1:
249                 y = np.ravel(y)
250 
251         return y
252 
253     def predict_proba(self, X):
254         """
255         Return probability estimates for the test vectors X.
256 
257         Parameters
258         ----------
259         X : {array-like, object with finite length or shape}
260             Training data, requires length = n_samples
261 
262         Returns
263         -------
264         P : array-like or list of array-lke of shape (n_samples, n_classes)
265             Returns the probability of the sample for each class in
266             the model, where classes are ordered arithmetically, for each
267             output.
268         """
269         check_is_fitted(self)
270 
271         # numpy random_state expects Python int and not long as size argument
272         # under Windows
273         n_samples = _num_samples(X)
274         rs = check_random_state(self.random_state)
275 
276         n_classes_ = self.n_classes_
277         classes_ = self.classes_
278         class_prior_ = self.class_prior_
279         constant = self.constant
280         if self.n_outputs_ == 1:
281             # Get same type even for self.n_outputs_ == 1
282             n_classes_ = [n_classes_]
283             classes_ = [classes_]
284             class_prior_ = [class_prior_]
285             constant = [constant]
286 
287         P = []
288         for k in range(self.n_outputs_):
289             if self.strategy == "most_frequent":
290                 ind = class_prior_[k].argmax()
291                 out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)
292                 out[:, ind] = 1.0
293             elif self.strategy == "prior":
294                 out = np.ones((n_samples, 1)) * class_prior_[k]
295 
296             elif self.strategy == "stratified":
297                 out = rs.multinomial(1, class_prior_[k], size=n_samples)
298                 out = out.astype(np.float64)
299 
300             elif self.strategy == "uniform":
301                 out = np.ones((n_samples, n_classes_[k]), dtype=np.float64)
302                 out /= n_classes_[k]
303 
304             elif self.strategy == "constant":
305                 ind = np.where(classes_[k] == constant[k])
306                 out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)
307                 out[:, ind] = 1.0
308 
309             P.append(out)
310 
311         if self.n_outputs_ == 1:
312             P = P[0]
313 
314         return P
315 
316     def predict_log_proba(self, X):
317         """
318         Return log probability estimates for the test vectors X.
319 
320         Parameters
321         ----------
322         X : {array-like, object with finite length or shape}
323             Training data, requires length = n_samples
324 
325         Returns
326         -------
327         P : array-like or list of array-like of shape (n_samples, n_classes)
328             Returns the log probability of the sample for each class in
329             the model, where classes are ordered arithmetically for each
330             output.
331         """
332         proba = self.predict_proba(X)
333         if self.n_outputs_ == 1:
334             return np.log(proba)
335         else:
336             return [np.log(p) for p in proba]
337 
338     def _more_tags(self):
339         return {'poor_score': True, 'no_validation': True}
340 
341     def score(self, X, y, sample_weight=None):
342         """Returns the mean accuracy on the given test data and labels.
343 
344         In multi-label classification, this is the subset accuracy
345         which is a harsh metric since you require for each sample that
346         each label set be correctly predicted.
347 
348         Parameters
349         ----------
350         X : {array-like, None}
351             Test samples with shape = (n_samples, n_features) or
352             None. Passing None as test samples gives the same result
353             as passing real test samples, since DummyClassifier
354             operates independently of the sampled observations.
355 
356         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
357             True labels for X.
358 
359         sample_weight : array-like of shape (n_samples,), default=None
360             Sample weights.
361 
362         Returns
363         -------
364         score : float
365             Mean accuracy of self.predict(X) wrt. y.
366 
367         """
368         if X is None:
369             X = np.zeros(shape=(len(y), 1))
370         return super().score(X, y, sample_weight)
371 
372     @deprecated(
373         "The outputs_2d_ attribute is deprecated in version 0.22 "
374         "and will be removed in version 0.24. It is equivalent to "
375         "n_outputs_ > 1."
376     )
377     @property
378     def outputs_2d_(self):
379         return self.n_outputs_ != 1

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/dummy.py</file>
<class>DummyRegressor</class>
<method>fit</method>
<code>
442     def fit(self, X, y, sample_weight=None):
443         """Fit the random regressor.
444 
445         Parameters
446         ----------
447         X : {array-like, object with finite length or shape}
448             Training data, requires length = n_samples
449 
450         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
451             Target values.
452 
453         sample_weight : array-like of shape (n_samples,), default=None
454             Sample weights.
455 
456         Returns
457         -------
458         self : object
459         """
460         allowed_strategies = ("mean", "median", "quantile", "constant")
461         if self.strategy not in allowed_strategies:
462             raise ValueError("Unknown strategy type: %s, expected one of %s."
463                              % (self.strategy, allowed_strategies))
464 
465         y = check_array(y, ensure_2d=False)
466         if len(y) == 0:
467             raise ValueError("y must not be empty.")
468 
469         if y.ndim == 1:
470             y = np.reshape(y, (-1, 1))
471         self.n_outputs_ = y.shape[1]
472 
473         check_consistent_length(X, y, sample_weight)
474         if sample_weight is not None:
475             sample_weight = np.asarray(sample_weight)
476 
477         if self.strategy == "mean":
478             self.constant_ = np.average(y, axis=0, weights=sample_weight)
479 
480         elif self.strategy == "median":
481             if sample_weight is None:
482                 self.constant_ = np.median(y, axis=0)
483             else:
484                 self.constant_ = [_weighted_percentile(y[:, k], sample_weight,
485                                                        percentile=50.)
486                                   for k in range(self.n_outputs_)]
487 
488         elif self.strategy == "quantile":
489             if self.quantile is None or not np.isscalar(self.quantile):
490                 raise ValueError("Quantile must be a scalar in the range "
491                                  "[0.0, 1.0], but got %s." % self.quantile)
492 
493             percentile = self.quantile * 100.0
494             if sample_weight is None:
495                 self.constant_ = np.percentile(y, axis=0, q=percentile)
496             else:
497                 self.constant_ = [_weighted_percentile(y[:, k], sample_weight,
498                                                        percentile=percentile)
499                                   for k in range(self.n_outputs_)]
500 
501         elif self.strategy == "constant":
502             if self.constant is None:
503                 raise TypeError("Constant target value has to be specified "
504                                 "when the constant strategy is used.")
505 
506             self.constant = check_array(self.constant,
507                                         accept_sparse=['csr', 'csc', 'coo'],
508                                         ensure_2d=False, ensure_min_samples=0)
509 
510             if self.n_outputs_ != 1 and self.constant.shape[0] != y.shape[1]:
511                 raise ValueError(
512                     "Constant target value should have "
513                     "shape (%d, 1)." % y.shape[1])
514 
515             self.constant_ = self.constant
516 
517         self.constant_ = np.reshape(self.constant_, (1, -1))
518         return self

</code><intended_behavior>Similar to DummyClassifier, DummyRegressor's fit method should also adopt _check_sample_weight to ensure standardized handling of sample_weight.</intended_behavior>, <file>sklearn/dummy.py</file>
<class>DummyRegressor</class>
<method>None</method>
<code>
382 class DummyRegressor(MultiOutputMixin, RegressorMixin, BaseEstimator):
383     """
384     DummyRegressor is a regressor that makes predictions using
385     simple rules.
386 
387     This regressor is useful as a simple baseline to compare with other
388     (real) regressors. Do not use it for real problems.
389 
390     Read more in the :ref:`User Guide <dummy_estimators>`.
391 
392     Parameters
393     ----------
394     strategy : str
395         Strategy to use to generate predictions.
396 
397         * "mean": always predicts the mean of the training set
398         * "median": always predicts the median of the training set
399         * "quantile": always predicts a specified quantile of the training set,
400           provided with the quantile parameter.
401         * "constant": always predicts a constant value that is provided by
402           the user.
403 
404     constant : int or float or array-like of shape (n_outputs,)
405         The explicit constant as predicted by the "constant" strategy. This
406         parameter is useful only for the "constant" strategy.
407 
408     quantile : float in [0.0, 1.0]
409         The quantile to predict using the "quantile" strategy. A quantile of
410         0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the
411         maximum.
412 
413     Attributes
414     ----------
415     constant_ : array, shape (1, n_outputs)
416         Mean or median or quantile of the training targets or constant value
417         given by the user.
418 
419     n_outputs_ : int,
420         Number of outputs.
421 
422     Examples
423     --------
424     >>> import numpy as np
425     >>> from sklearn.dummy import DummyRegressor
426     >>> X = np.array([1.0, 2.0, 3.0, 4.0])
427     >>> y = np.array([2.0, 3.0, 5.0, 10.0])
428     >>> dummy_regr = DummyRegressor(strategy="mean")
429     >>> dummy_regr.fit(X, y)
430     DummyRegressor()
431     >>> dummy_regr.predict(X)
432     array([5., 5., 5., 5.])
433     >>> dummy_regr.score(X, y)
434     0.0
435     """
436 
437     def __init__(self, strategy="mean", constant=None, quantile=None):
438         self.strategy = strategy
439         self.constant = constant
440         self.quantile = quantile
441 
442     def fit(self, X, y, sample_weight=None):
443         """Fit the random regressor.
444 
445         Parameters
446         ----------
447         X : {array-like, object with finite length or shape}
448             Training data, requires length = n_samples
449 
450         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
451             Target values.
452 
453         sample_weight : array-like of shape (n_samples,), default=None
454             Sample weights.
455 
456         Returns
457         -------
458         self : object
459         """
460         allowed_strategies = ("mean", "median", "quantile", "constant")
461         if self.strategy not in allowed_strategies:
462             raise ValueError("Unknown strategy type: %s, expected one of %s."
463                              % (self.strategy, allowed_strategies))
464 
465         y = check_array(y, ensure_2d=False)
466         if len(y) == 0:
467             raise ValueError("y must not be empty.")
468 
469         if y.ndim == 1:
470             y = np.reshape(y, (-1, 1))
471         self.n_outputs_ = y.shape[1]
472 
473         check_consistent_length(X, y, sample_weight)
474         if sample_weight is not None:
475             sample_weight = np.asarray(sample_weight)
476 
477         if self.strategy == "mean":
478             self.constant_ = np.average(y, axis=0, weights=sample_weight)
479 
480         elif self.strategy == "median":
481             if sample_weight is None:
482                 self.constant_ = np.median(y, axis=0)
483             else:
484                 self.constant_ = [_weighted_percentile(y[:, k], sample_weight,
485                                                        percentile=50.)
486                                   for k in range(self.n_outputs_)]
487 
488         elif self.strategy == "quantile":
489             if self.quantile is None or not np.isscalar(self.quantile):
490                 raise ValueError("Quantile must be a scalar in the range "
491                                  "[0.0, 1.0], but got %s." % self.quantile)
492 
493             percentile = self.quantile * 100.0
494             if sample_weight is None:
495                 self.constant_ = np.percentile(y, axis=0, q=percentile)
496             else:
497                 self.constant_ = [_weighted_percentile(y[:, k], sample_weight,
498                                                        percentile=percentile)
499                                   for k in range(self.n_outputs_)]
500 
501         elif self.strategy == "constant":
502             if self.constant is None:
503                 raise TypeError("Constant target value has to be specified "
504                                 "when the constant strategy is used.")
505 
506             self.constant = check_array(self.constant,
507                                         accept_sparse=['csr', 'csc', 'coo'],
508                                         ensure_2d=False, ensure_min_samples=0)
509 
510             if self.n_outputs_ != 1 and self.constant.shape[0] != y.shape[1]:
511                 raise ValueError(
512                     "Constant target value should have "
513                     "shape (%d, 1)." % y.shape[1])
514 
515             self.constant_ = self.constant
516 
517         self.constant_ = np.reshape(self.constant_, (1, -1))
518         return self
519 
520     def predict(self, X, return_std=False):
521         """
522         Perform classification on test vectors X.
523 
524         Parameters
525         ----------
526         X : {array-like, object with finite length or shape}
527             Training data, requires length = n_samples
528 
529         return_std : boolean, optional
530             Whether to return the standard deviation of posterior prediction.
531             All zeros in this case.
532 
533         Returns
534         -------
535         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
536             Predicted target values for X.
537 
538         y_std : array-like of shape (n_samples,) or (n_samples, n_outputs)
539             Standard deviation of predictive distribution of query points.
540         """
541         check_is_fitted(self)
542         n_samples = _num_samples(X)
543 
544         y = np.full((n_samples, self.n_outputs_), self.constant_,
545                     dtype=np.array(self.constant_).dtype)
546         y_std = np.zeros((n_samples, self.n_outputs_))
547 
548         if self.n_outputs_ == 1:
549             y = np.ravel(y)
550             y_std = np.ravel(y_std)
551 
552         return (y, y_std) if return_std else y
553 
554     def _more_tags(self):
555         return {'poor_score': True, 'no_validation': True}
556 
557     def score(self, X, y, sample_weight=None):
558         """Returns the coefficient of determination R^2 of the prediction.
559 
560         The coefficient R^2 is defined as (1 - u/v), where u is the residual
561         sum of squares ((y_true - y_pred) ** 2).sum() and v is the total
562         sum of squares ((y_true - y_true.mean()) ** 2).sum().
563         The best possible score is 1.0 and it can be negative (because the
564         model can be arbitrarily worse). A constant model that always
565         predicts the expected value of y, disregarding the input features,
566         would get a R^2 score of 0.0.
567 
568         Parameters
569         ----------
570         X : {array-like, None}
571             Test samples with shape = (n_samples, n_features) or None.
572             For some estimators this may be a
573             precomputed kernel matrix instead, shape = (n_samples,
574             n_samples_fitted], where n_samples_fitted is the number of
575             samples used in the fitting for the estimator.
576             Passing None as test samples gives the same result
577             as passing real test samples, since DummyRegressor
578             operates independently of the sampled observations.
579 
580         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
581             True values for X.
582 
583         sample_weight : array-like of shape (n_samples,), default=None
584             Sample weights.
585 
586         Returns
587         -------
588         score : float
589             R^2 of self.predict(X) wrt. y.
590         """
591         if X is None:
592             X = np.zeros(shape=(len(y), 1))
593         return super().score(X, y, sample_weight)
594 
595     @deprecated(
596         "The outputs_2d_ attribute is deprecated in version 0.22 "
597         "and will be removed in version 0.24. It is equivalent to "
598         "n_outputs_ > 1."
599     )
600     @property
601     def outputs_2d_(self):
602         return self.n_outputs_ != 1

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/ensemble/_bagging.py</file>
<class>BaseBagging</class>
<method>fit</method>
<code>
219     def fit(self, X, y, sample_weight=None):
220         """Build a Bagging ensemble of estimators from the training
221            set (X, y).
222 
223         Parameters
224         ----------
225         X : {array-like, sparse matrix} of shape (n_samples, n_features)
226             The training input samples. Sparse matrices are accepted only if
227             they are supported by the base estimator.
228 
229         y : array-like of shape (n_samples,)
230             The target values (class labels in classification, real numbers in
231             regression).
232 
233         sample_weight : array-like of shape (n_samples,), default=None
234             Sample weights. If None, then samples are equally weighted.
235             Note that this is supported only if the base estimator supports
236             sample weighting.
237 
238         Returns
239         -------
240         self : object
241         """
242         return self._fit(X, y, self.max_samples, sample_weight=sample_weight)

</code><intended_behavior>Adapt the fit method to employ _check_sample_weight for validating and processing sample_weight. This will maintain the feature's existing functionality while ensuring that weight handling adheres to the standardized approach used across the library.</intended_behavior>, <file>sklearn/ensemble/_bagging.py</file>
<class>BaseBagging</class>
<method>None</method>
<code>
185 class BaseBagging(BaseEnsemble, metaclass=ABCMeta):
186     """Base class for Bagging meta-estimator.
187 
188     Warning: This class should not be used directly. Use derived classes
189     instead.
190     """
191 
192     @abstractmethod
193     def __init__(self,
194                  base_estimator=None,
195                  n_estimators=10,
196                  max_samples=1.0,
197                  max_features=1.0,
198                  bootstrap=True,
199                  bootstrap_features=False,
200                  oob_score=False,
201                  warm_start=False,
202                  n_jobs=None,
203                  random_state=None,
204                  verbose=0):
205         super().__init__(
206             base_estimator=base_estimator,
207             n_estimators=n_estimators)
208 
209         self.max_samples = max_samples
210         self.max_features = max_features
211         self.bootstrap = bootstrap
212         self.bootstrap_features = bootstrap_features
213         self.oob_score = oob_score
214         self.warm_start = warm_start
215         self.n_jobs = n_jobs
216         self.random_state = random_state
217         self.verbose = verbose
218 
219     def fit(self, X, y, sample_weight=None):
220         """Build a Bagging ensemble of estimators from the training
221            set (X, y).
222 
223         Parameters
224         ----------
225         X : {array-like, sparse matrix} of shape (n_samples, n_features)
226             The training input samples. Sparse matrices are accepted only if
227             they are supported by the base estimator.
228 
229         y : array-like of shape (n_samples,)
230             The target values (class labels in classification, real numbers in
231             regression).
232 
233         sample_weight : array-like of shape (n_samples,), default=None
234             Sample weights. If None, then samples are equally weighted.
235             Note that this is supported only if the base estimator supports
236             sample weighting.
237 
238         Returns
239         -------
240         self : object
241         """
242         return self._fit(X, y, self.max_samples, sample_weight=sample_weight)
243 
244     def _parallel_args(self):
245         return {}
246 
247     def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):
248         """Build a Bagging ensemble of estimators from the training
249            set (X, y).
250 
251         Parameters
252         ----------
253         X : {array-like, sparse matrix} of shape (n_samples, n_features)
254             The training input samples. Sparse matrices are accepted only if
255             they are supported by the base estimator.
256 
257         y : array-like of shape (n_samples,)
258             The target values (class labels in classification, real numbers in
259             regression).
260 
261         max_samples : int or float, optional (default=None)
262             Argument to use instead of self.max_samples.
263 
264         max_depth : int, optional (default=None)
265             Override value used when constructing base estimator. Only
266             supported if the base estimator has a max_depth parameter.
267 
268         sample_weight : array-like of shape (n_samples,), default=None
269             Sample weights. If None, then samples are equally weighted.
270             Note that this is supported only if the base estimator supports
271             sample weighting.
272 
273         Returns
274         -------
275         self : object
276         """
277         random_state = check_random_state(self.random_state)
278 
279         # Convert data (X is required to be 2d and indexable)
280         X, y = check_X_y(
281             X, y, ['csr', 'csc'], dtype=None, force_all_finite=False,
282             multi_output=True
283         )
284         if sample_weight is not None:
285             sample_weight = check_array(sample_weight, ensure_2d=False)
286             check_consistent_length(y, sample_weight)
287 
288         # Remap output
289         n_samples, self.n_features_ = X.shape
290         self._n_samples = n_samples
291         y = self._validate_y(y)
292 
293         # Check parameters
294         self._validate_estimator()
295 
296         if max_depth is not None:
297             self.base_estimator_.max_depth = max_depth
298 
299         # Validate max_samples
300         if max_samples is None:
301             max_samples = self.max_samples
302         elif not isinstance(max_samples, numbers.Integral):
303             max_samples = int(max_samples * X.shape[0])
304 
305         if not (0 < max_samples <= X.shape[0]):
306             raise ValueError("max_samples must be in (0, n_samples]")
307 
308         # Store validated integer row sampling value
309         self._max_samples = max_samples
310 
311         # Validate max_features
312         if isinstance(self.max_features, numbers.Integral):
313             max_features = self.max_features
314         elif isinstance(self.max_features, np.float):
315             max_features = self.max_features * self.n_features_
316         else:
317             raise ValueError("max_features must be int or float")
318 
319         if not (0 < max_features <= self.n_features_):
320             raise ValueError("max_features must be in (0, n_features]")
321 
322         max_features = max(1, int(max_features))
323 
324         # Store validated integer feature sampling value
325         self._max_features = max_features
326 
327         # Other checks
328         if not self.bootstrap and self.oob_score:
329             raise ValueError("Out of bag estimation only available"
330                              " if bootstrap=True")
331 
332         if self.warm_start and self.oob_score:
333             raise ValueError("Out of bag estimate only available"
334                              " if warm_start=False")
335 
336         if hasattr(self, "oob_score_") and self.warm_start:
337             del self.oob_score_
338 
339         if not self.warm_start or not hasattr(self, 'estimators_'):
340             # Free allocated memory, if any
341             self.estimators_ = []
342             self.estimators_features_ = []
343 
344         n_more_estimators = self.n_estimators - len(self.estimators_)
345 
346         if n_more_estimators < 0:
347             raise ValueError('n_estimators=%d must be larger or equal to '
348                              'len(estimators_)=%d when warm_start==True'
349                              % (self.n_estimators, len(self.estimators_)))
350 
351         elif n_more_estimators == 0:
352             warn("Warm-start fitting without increasing n_estimators does not "
353                  "fit new trees.")
354             return self
355 
356         # Parallel loop
357         n_jobs, n_estimators, starts = _partition_estimators(n_more_estimators,
358                                                              self.n_jobs)
359         total_n_estimators = sum(n_estimators)
360 
361         # Advance random state to state after training
362         # the first n_estimators
363         if self.warm_start and len(self.estimators_) > 0:
364             random_state.randint(MAX_INT, size=len(self.estimators_))
365 
366         seeds = random_state.randint(MAX_INT, size=n_more_estimators)
367         self._seeds = seeds
368 
369         all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose,
370                                **self._parallel_args())(
371             delayed(_parallel_build_estimators)(
372                 n_estimators[i],
373                 self,
374                 X,
375                 y,
376                 sample_weight,
377                 seeds[starts[i]:starts[i + 1]],
378                 total_n_estimators,
379                 verbose=self.verbose)
380             for i in range(n_jobs))
381 
382         # Reduce
383         self.estimators_ += list(itertools.chain.from_iterable(
384             t[0] for t in all_results))
385         self.estimators_features_ += list(itertools.chain.from_iterable(
386             t[1] for t in all_results))
387 
388         if self.oob_score:
389             self._set_oob_score(X, y)
390 
391         return self
392 
393     @abstractmethod
394     def _set_oob_score(self, X, y):
395         """Calculate out of bag predictions and score."""
396 
397     def _validate_y(self, y):
398         if len(y.shape) == 1 or y.shape[1] == 1:
399             return column_or_1d(y, warn=True)
400         else:
401             return y
402 
403     def _get_estimators_indices(self):
404         # Get drawn indices along both sample and feature axes
405         for seed in self._seeds:
406             # Operations accessing random_state must be performed identically
407             # to those in `_parallel_build_estimators()`
408             random_state = np.random.RandomState(seed)
409             feature_indices, sample_indices = _generate_bagging_indices(
410                 random_state, self.bootstrap_features, self.bootstrap,
411                 self.n_features_, self._n_samples, self._max_features,
412                 self._max_samples)
413 
414             yield feature_indices, sample_indices
415 
416     @property
417     def estimators_samples_(self):
418         """
419         The subset of drawn samples for each base estimator.
420 
421         Returns a dynamically generated list of indices identifying
422         the samples used for fitting each member of the ensemble, i.e.,
423         the in-bag samples.
424 
425         Note: the list is re-created at each call to the property in order
426         to reduce the object memory footprint by not storing the sampling
427         data. Thus fetching the property may be slower than expected.
428         """
429         return [sample_indices
430                 for _, sample_indices in self._get_estimators_indices()]

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/ensemble/_forest.py</file>
<class>BaseForest</class>
<method>fit</method>
<code>
269     def fit(self, X, y, sample_weight=None):
270         """
271         Build a forest of trees from the training set (X, y).
272 
273         Parameters
274         ----------
275         X : array-like or sparse matrix of shape (n_samples, n_features)
276             The training input samples. Internally, its dtype will be converted
277             to ``dtype=np.float32``. If a sparse matrix is provided, it will be
278             converted into a sparse ``csc_matrix``.
279 
280         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
281             The target values (class labels in classification, real numbers in
282             regression).
283 
284         sample_weight : array-like of shape (n_samples,), default=None
285             Sample weights. If None, then samples are equally weighted. Splits
286             that would create child nodes with net zero or negative weight are
287             ignored while searching for a split in each node. In the case of
288             classification, splits are also ignored if they would result in any
289             single class carrying a negative weight in either child node.
290 
291         Returns
292         -------
293         self : object
294         """
295         # Validate or convert input data
296         X = check_array(X, accept_sparse="csc", dtype=DTYPE)
297         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
298         if sample_weight is not None:
299             sample_weight = check_array(sample_weight, ensure_2d=False)
300         if issparse(X):
301             # Pre-sort indices to avoid that each individual tree of the
302             # ensemble sorts the indices.
303             X.sort_indices()
304 
305         # Remap output
306         self.n_features_ = X.shape[1]
307 
308         y = np.atleast_1d(y)
309         if y.ndim == 2 and y.shape[1] == 1:
310             warn("A column-vector y was passed when a 1d array was"
311                  " expected. Please change the shape of y to "
312                  "(n_samples,), for example using ravel().",
313                  DataConversionWarning, stacklevel=2)
314 
315         if y.ndim == 1:
316             # reshape is necessary to preserve the data contiguity against vs
317             # [:, np.newaxis] that does not.
318             y = np.reshape(y, (-1, 1))
319 
320         self.n_outputs_ = y.shape[1]
321 
322         y, expanded_class_weight = self._validate_y_class_weight(y)
323 
324         if getattr(y, "dtype", None) != DOUBLE or not y.flags.contiguous:
325             y = np.ascontiguousarray(y, dtype=DOUBLE)
326 
327         if expanded_class_weight is not None:
328             if sample_weight is not None:
329                 sample_weight = sample_weight * expanded_class_weight
330             else:
331                 sample_weight = expanded_class_weight
332 
333         # Get bootstrap sample size
334         n_samples_bootstrap = _get_n_samples_bootstrap(
335             n_samples=X.shape[0],
336             max_samples=self.max_samples
337         )
338 
339         # Check parameters
340         self._validate_estimator()
341 
342         if not self.bootstrap and self.oob_score:
343             raise ValueError("Out of bag estimation only available"
344                              " if bootstrap=True")
345 
346         random_state = check_random_state(self.random_state)
347 
348         if not self.warm_start or not hasattr(self, "estimators_"):
349             # Free allocated memory, if any
350             self.estimators_ = []
351 
352         n_more_estimators = self.n_estimators - len(self.estimators_)
353 
354         if n_more_estimators < 0:
355             raise ValueError('n_estimators=%d must be larger or equal to '
356                              'len(estimators_)=%d when warm_start==True'
357                              % (self.n_estimators, len(self.estimators_)))
358 
359         elif n_more_estimators == 0:
360             warn("Warm-start fitting without increasing n_estimators does not "
361                  "fit new trees.")
362         else:
363             if self.warm_start and len(self.estimators_) > 0:
364                 # We draw from the random state to get the random state we
365                 # would have got if we hadn't used a warm_start.
366                 random_state.randint(MAX_INT, size=len(self.estimators_))
367 
368             trees = [self._make_estimator(append=False,
369                                           random_state=random_state)
370                      for i in range(n_more_estimators)]
371 
372             # Parallel loop: we prefer the threading backend as the Cython code
373             # for fitting the trees is internally releasing the Python GIL
374             # making threading more efficient than multiprocessing in
375             # that case. However, for joblib 0.12+ we respect any
376             # parallel_backend contexts set at a higher level,
377             # since correctness does not rely on using threads.
378             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
379                              **_joblib_parallel_args(prefer='threads'))(
380                 delayed(_parallel_build_trees)(
381                     t, self, X, y, sample_weight, i, len(trees),
382                     verbose=self.verbose, class_weight=self.class_weight,
383                     n_samples_bootstrap=n_samples_bootstrap)
384                 for i, t in enumerate(trees))
385 
386             # Collect newly grown trees
387             self.estimators_.extend(trees)
388 
389         if self.oob_score:
390             self._set_oob_score(X, y)
391 
392         # Decapsulate classes_ attributes
393         if hasattr(self, "classes_") and self.n_outputs_ == 1:
394             self.n_classes_ = self.n_classes_[0]
395             self.classes_ = self.classes_[0]
396 
397         return self

</code><intended_behavior>Ensure that fit method uses _check_sample_weight for validating and processing sample_weight in line with sklearn's standardized practices, ensuring consistency, and possibly enhancing error checking and input validation.</intended_behavior>, <file>sklearn/ensemble/_forest.py</file>
<class>BaseForest</class>
<method>None</method>
<code>
172 class BaseForest(MultiOutputMixin, BaseEnsemble, metaclass=ABCMeta):
173     """
174     Base class for forests of trees.
175 
176     Warning: This class should not be used directly. Use derived classes
177     instead.
178     """
179 
180     @abstractmethod
181     def __init__(self,
182                  base_estimator,
183                  n_estimators=100,
184                  estimator_params=tuple(),
185                  bootstrap=False,
186                  oob_score=False,
187                  n_jobs=None,
188                  random_state=None,
189                  verbose=0,
190                  warm_start=False,
191                  class_weight=None,
192                  max_samples=None):
193         super().__init__(
194             base_estimator=base_estimator,
195             n_estimators=n_estimators,
196             estimator_params=estimator_params)
197 
198         self.bootstrap = bootstrap
199         self.oob_score = oob_score
200         self.n_jobs = n_jobs
201         self.random_state = random_state
202         self.verbose = verbose
203         self.warm_start = warm_start
204         self.class_weight = class_weight
205         self.max_samples = max_samples
206 
207     def apply(self, X):
208         """
209         Apply trees in the forest to X, return leaf indices.
210 
211         Parameters
212         ----------
213         X : {array-like or sparse matrix} of shape (n_samples, n_features)
214             The input samples. Internally, its dtype will be converted to
215             ``dtype=np.float32``. If a sparse matrix is provided, it will be
216             converted into a sparse ``csr_matrix``.
217 
218         Returns
219         -------
220         X_leaves : array_like, shape = [n_samples, n_estimators]
221             For each datapoint x in X and for each tree in the forest,
222             return the index of the leaf x ends up in.
223         """
224         X = self._validate_X_predict(X)
225         results = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
226                            **_joblib_parallel_args(prefer="threads"))(
227             delayed(tree.apply)(X, check_input=False)
228             for tree in self.estimators_)
229 
230         return np.array(results).T
231 
232     def decision_path(self, X):
233         """
234         Return the decision path in the forest.
235 
236         .. versionadded:: 0.18
237 
238         Parameters
239         ----------
240         X : {array-like or sparse matrix} of shape (n_samples, n_features)
241             The input samples. Internally, its dtype will be converted to
242             ``dtype=np.float32``. If a sparse matrix is provided, it will be
243             converted into a sparse ``csr_matrix``.
244 
245         Returns
246         -------
247         indicator : sparse csr array, shape = [n_samples, n_nodes]
248             Return a node indicator matrix where non zero elements
249             indicates that the samples goes through the nodes.
250 
251         n_nodes_ptr : array of size (n_estimators + 1, )
252             The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]
253             gives the indicator value for the i-th estimator.
254 
255         """
256         X = self._validate_X_predict(X)
257         indicators = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
258                               **_joblib_parallel_args(prefer='threads'))(
259             delayed(tree.decision_path)(X,
260                                      check_input=False)
261             for tree in self.estimators_)
262 
263         n_nodes = [0]
264         n_nodes.extend([i.shape[1] for i in indicators])
265         n_nodes_ptr = np.array(n_nodes).cumsum()
266 
267         return sparse_hstack(indicators).tocsr(), n_nodes_ptr
268 
269     def fit(self, X, y, sample_weight=None):
270         """
271         Build a forest of trees from the training set (X, y).
272 
273         Parameters
274         ----------
275         X : array-like or sparse matrix of shape (n_samples, n_features)
276             The training input samples. Internally, its dtype will be converted
277             to ``dtype=np.float32``. If a sparse matrix is provided, it will be
278             converted into a sparse ``csc_matrix``.
279 
280         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
281             The target values (class labels in classification, real numbers in
282             regression).
283 
284         sample_weight : array-like of shape (n_samples,), default=None
285             Sample weights. If None, then samples are equally weighted. Splits
286             that would create child nodes with net zero or negative weight are
287             ignored while searching for a split in each node. In the case of
288             classification, splits are also ignored if they would result in any
289             single class carrying a negative weight in either child node.
290 
291         Returns
292         -------
293         self : object
294         """
295         # Validate or convert input data
296         X = check_array(X, accept_sparse="csc", dtype=DTYPE)
297         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
298         if sample_weight is not None:
299             sample_weight = check_array(sample_weight, ensure_2d=False)
300         if issparse(X):
301             # Pre-sort indices to avoid that each individual tree of the
302             # ensemble sorts the indices.
303             X.sort_indices()
304 
305         # Remap output
306         self.n_features_ = X.shape[1]
307 
308         y = np.atleast_1d(y)
309         if y.ndim == 2 and y.shape[1] == 1:
310             warn("A column-vector y was passed when a 1d array was"
311                  " expected. Please change the shape of y to "
312                  "(n_samples,), for example using ravel().",
313                  DataConversionWarning, stacklevel=2)
314 
315         if y.ndim == 1:
316             # reshape is necessary to preserve the data contiguity against vs
317             # [:, np.newaxis] that does not.
318             y = np.reshape(y, (-1, 1))
319 
320         self.n_outputs_ = y.shape[1]
321 
322         y, expanded_class_weight = self._validate_y_class_weight(y)
323 
324         if getattr(y, "dtype", None) != DOUBLE or not y.flags.contiguous:
325             y = np.ascontiguousarray(y, dtype=DOUBLE)
326 
327         if expanded_class_weight is not None:
328             if sample_weight is not None:
329                 sample_weight = sample_weight * expanded_class_weight
330             else:
331                 sample_weight = expanded_class_weight
332 
333         # Get bootstrap sample size
334         n_samples_bootstrap = _get_n_samples_bootstrap(
335             n_samples=X.shape[0],
336             max_samples=self.max_samples
337         )
338 
339         # Check parameters
340         self._validate_estimator()
341 
342         if not self.bootstrap and self.oob_score:
343             raise ValueError("Out of bag estimation only available"
344                              " if bootstrap=True")
345 
346         random_state = check_random_state(self.random_state)
347 
348         if not self.warm_start or not hasattr(self, "estimators_"):
349             # Free allocated memory, if any
350             self.estimators_ = []
351 
352         n_more_estimators = self.n_estimators - len(self.estimators_)
353 
354         if n_more_estimators < 0:
355             raise ValueError('n_estimators=%d must be larger or equal to '
356                              'len(estimators_)=%d when warm_start==True'
357                              % (self.n_estimators, len(self.estimators_)))
358 
359         elif n_more_estimators == 0:
360             warn("Warm-start fitting without increasing n_estimators does not "
361                  "fit new trees.")
362         else:
363             if self.warm_start and len(self.estimators_) > 0:
364                 # We draw from the random state to get the random state we
365                 # would have got if we hadn't used a warm_start.
366                 random_state.randint(MAX_INT, size=len(self.estimators_))
367 
368             trees = [self._make_estimator(append=False,
369                                           random_state=random_state)
370                      for i in range(n_more_estimators)]
371 
372             # Parallel loop: we prefer the threading backend as the Cython code
373             # for fitting the trees is internally releasing the Python GIL
374             # making threading more efficient than multiprocessing in
375             # that case. However, for joblib 0.12+ we respect any
376             # parallel_backend contexts set at a higher level,
377             # since correctness does not rely on using threads.
378             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
379                              **_joblib_parallel_args(prefer='threads'))(
380                 delayed(_parallel_build_trees)(
381                     t, self, X, y, sample_weight, i, len(trees),
382                     verbose=self.verbose, class_weight=self.class_weight,
383                     n_samples_bootstrap=n_samples_bootstrap)
384                 for i, t in enumerate(trees))
385 
386             # Collect newly grown trees
387             self.estimators_.extend(trees)
388 
389         if self.oob_score:
390             self._set_oob_score(X, y)
391 
392         # Decapsulate classes_ attributes
393         if hasattr(self, "classes_") and self.n_outputs_ == 1:
394             self.n_classes_ = self.n_classes_[0]
395             self.classes_ = self.classes_[0]
396 
397         return self
398 
399     @abstractmethod
400     def _set_oob_score(self, X, y):
401         """
402         Calculate out of bag predictions and score."""
403 
404     def _validate_y_class_weight(self, y):
405         # Default implementation
406         return y, None
407 
408     def _validate_X_predict(self, X):
409         """
410         Validate X whenever one tries to predict, apply, predict_proba."""
411         check_is_fitted(self)
412 
413         return self.estimators_[0]._validate_X_predict(X, check_input=True)
414 
415     @property
416     def feature_importances_(self):
417         """
418         Return the feature importances (the higher, the more important the
419            feature).
420 
421         Returns
422         -------
423         feature_importances_ : array, shape = [n_features]
424             The values of this array sum to 1, unless all trees are single node
425             trees consisting of only the root node, in which case it will be an
426             array of zeros.
427         """
428         check_is_fitted(self)
429 
430         all_importances = Parallel(n_jobs=self.n_jobs,
431                                    **_joblib_parallel_args(prefer='threads'))(
432             delayed(getattr)(tree, 'feature_importances_')
433             for tree in self.estimators_ if tree.tree_.node_count > 1)
434 
435         if not all_importances:
436             return np.zeros(self.n_features_, dtype=np.float64)
437 
438         all_importances = np.mean(all_importances,
439                                   axis=0, dtype=np.float64)
440         return all_importances / np.sum(all_importances)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/ensemble/_gb.py</file>
<class>BaseGradientBoosting</class>
<method>fit</method>
<code>
1399     def fit(self, X, y, sample_weight=None, monitor=None):
1400         """Fit the gradient boosting model.
1401 
1402         Parameters
1403         ----------
1404         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1405             The input samples. Internally, it will be converted to
1406             ``dtype=np.float32`` and if a sparse matrix is provided
1407             to a sparse ``csr_matrix``.
1408 
1409         y : array-like, shape (n_samples,)
1410             Target values (strings or integers in classification, real numbers
1411             in regression)
1412             For classification, labels must correspond to classes.
1413 
1414         sample_weight : array-like, shape (n_samples,) or None
1415             Sample weights. If None, then samples are equally weighted. Splits
1416             that would create child nodes with net zero or negative weight are
1417             ignored while searching for a split in each node. In the case of
1418             classification, splits are also ignored if they would result in any
1419             single class carrying a negative weight in either child node.
1420 
1421         monitor : callable, optional
1422             The monitor is called after each iteration with the current
1423             iteration, a reference to the estimator and the local variables of
1424             ``_fit_stages`` as keyword arguments ``callable(i, self,
1425             locals())``. If the callable returns ``True`` the fitting procedure
1426             is stopped. The monitor can be used for various things such as
1427             computing held-out estimates, early stopping, model introspect, and
1428             snapshoting.
1429 
1430         Returns
1431         -------
1432         self : object
1433         """
1434         # if not warmstart - clear the estimator state
1435         if not self.warm_start:
1436             self._clear_state()
1437 
1438         # Check input
1439         # Since check_array converts both X and y to the same dtype, but the
1440         # trees use different types for X and y, checking them separately.
1441         X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
1442         n_samples, self.n_features_ = X.shape
1443 
1444         sample_weight_is_none = sample_weight is None
1445         if sample_weight_is_none:
1446             sample_weight = np.ones(n_samples, dtype=np.float32)
1447         else:
1448             sample_weight = column_or_1d(sample_weight, warn=True)
1449             sample_weight_is_none = False
1450 
1451         check_consistent_length(X, y, sample_weight)
1452 
1453         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
1454         y = column_or_1d(y, warn=True)
1455         y = self._validate_y(y, sample_weight)
1456 
1457         if self.n_iter_no_change is not None:
1458             stratify = y if is_classifier(self) else None
1459             X, X_val, y, y_val, sample_weight, sample_weight_val = (
1460                 train_test_split(X, y, sample_weight,
1461                                  random_state=self.random_state,
1462                                  test_size=self.validation_fraction,
1463                                  stratify=stratify))
1464             if is_classifier(self):
1465                 if self.n_classes_ != np.unique(y).shape[0]:
1466                     # We choose to error here. The problem is that the init
1467                     # estimator would be trained on y, which has some missing
1468                     # classes now, so its predictions would not have the
1469                     # correct shape.
1470                     raise ValueError(
1471                         'The training data after the early stopping split '
1472                         'is missing some classes. Try using another random '
1473                         'seed.'
1474                     )
1475         else:
1476             X_val = y_val = sample_weight_val = None
1477 
1478         self._check_params()
1479 
1480         if not self._is_initialized():
1481             # init state
1482             self._init_state()
1483 
1484             # fit initial model and initialize raw predictions
1485             if self.init_ == 'zero':
1486                 raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1487                                            dtype=np.float64)
1488             else:
1489                 # XXX clean this once we have a support_sample_weight tag
1490                 if sample_weight_is_none:
1491                     self.init_.fit(X, y)
1492                 else:
1493                     msg = ("The initial estimator {} does not support sample "
1494                            "weights.".format(self.init_.__class__.__name__))
1495                     try:
1496                         self.init_.fit(X, y, sample_weight=sample_weight)
1497                     except TypeError:  # regular estimator without SW support
1498                         raise ValueError(msg)
1499                     except ValueError as e:
1500                         if "pass parameters to specific steps of "\
1501                            "your pipeline using the "\
1502                            "stepname__parameter" in str(e):  # pipeline
1503                             raise ValueError(msg) from e
1504                         else:  # regular estimator whose input checking failed
1505                             raise
1506 
1507                 raw_predictions = \
1508                     self.loss_.get_init_raw_predictions(X, self.init_)
1509 
1510             begin_at_stage = 0
1511 
1512             # The rng state must be preserved if warm_start is True
1513             self._rng = check_random_state(self.random_state)
1514 
1515         else:
1516             # add more estimators to fitted model
1517             # invariant: warm_start = True
1518             if self.n_estimators < self.estimators_.shape[0]:
1519                 raise ValueError('n_estimators=%d must be larger or equal to '
1520                                  'estimators_.shape[0]=%d when '
1521                                  'warm_start==True'
1522                                  % (self.n_estimators,
1523                                     self.estimators_.shape[0]))
1524             begin_at_stage = self.estimators_.shape[0]
1525             # The requirements of _decision_function (called in two lines
1526             # below) are more constrained than fit. It accepts only CSR
1527             # matrices.
1528             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1529             raw_predictions = self._raw_predict(X)
1530             self._resize_state()
1531 
1532         X_idx_sorted = None
1533 
1534         # fit the boosting stages
1535         n_stages = self._fit_stages(
1536             X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,
1537             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)
1538 
1539         # change shape of arrays after fit (early-stopping or additional ests)
1540         if n_stages != self.estimators_.shape[0]:
1541             self.estimators_ = self.estimators_[:n_stages]
1542             self.train_score_ = self.train_score_[:n_stages]
1543             if hasattr(self, 'oob_improvement_'):
1544                 self.oob_improvement_ = self.oob_improvement_[:n_stages]
1545 
1546         self.n_estimators_ = n_stages
1547         return self

</code><intended_behavior>The fit function should be updated to utilize _check_sample_weight for handling sample_weight, replacing or complementing any specific logic currently in place with the standardized utility function.</intended_behavior>, <file>sklearn/ensemble/_gb.py</file>
<class>BaseGradientBoosting</class>
<method>None</method>
<code>
1166 class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
1167     """Abstract base class for Gradient Boosting. """
1168 
1169     @abstractmethod
1170     def __init__(self, loss, learning_rate, n_estimators, criterion,
1171                  min_samples_split, min_samples_leaf, min_weight_fraction_leaf,
1172                  max_depth, min_impurity_decrease, min_impurity_split,
1173                  init, subsample, max_features, ccp_alpha,
1174                  random_state, alpha=0.9, verbose=0, max_leaf_nodes=None,
1175                  warm_start=False, presort='deprecated',
1176                  validation_fraction=0.1, n_iter_no_change=None,
1177                  tol=1e-4):
1178 
1179         self.n_estimators = n_estimators
1180         self.learning_rate = learning_rate
1181         self.loss = loss
1182         self.criterion = criterion
1183         self.min_samples_split = min_samples_split
1184         self.min_samples_leaf = min_samples_leaf
1185         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1186         self.subsample = subsample
1187         self.max_features = max_features
1188         self.max_depth = max_depth
1189         self.min_impurity_decrease = min_impurity_decrease
1190         self.min_impurity_split = min_impurity_split
1191         self.ccp_alpha = ccp_alpha
1192         self.init = init
1193         self.random_state = random_state
1194         self.alpha = alpha
1195         self.verbose = verbose
1196         self.max_leaf_nodes = max_leaf_nodes
1197         self.warm_start = warm_start
1198         self.presort = presort
1199         self.validation_fraction = validation_fraction
1200         self.n_iter_no_change = n_iter_no_change
1201         self.tol = tol
1202 
1203     def _fit_stage(self, i, X, y, raw_predictions, sample_weight, sample_mask,
1204                    random_state, X_idx_sorted, X_csc=None, X_csr=None):
1205         """Fit another stage of ``n_classes_`` trees to the boosting model. """
1206 
1207         assert sample_mask.dtype == np.bool
1208         loss = self.loss_
1209         original_y = y
1210 
1211         # Need to pass a copy of raw_predictions to negative_gradient()
1212         # because raw_predictions is partially updated at the end of the loop
1213         # in update_terminal_regions(), and gradients need to be evaluated at
1214         # iteration i - 1.
1215         raw_predictions_copy = raw_predictions.copy()
1216 
1217         for k in range(loss.K):
1218             if loss.is_multi_class:
1219                 y = np.array(original_y == k, dtype=np.float64)
1220 
1221             residual = loss.negative_gradient(y, raw_predictions_copy, k=k,
1222                                               sample_weight=sample_weight)
1223 
1224             # induce regression tree on residuals
1225             tree = DecisionTreeRegressor(
1226                 criterion=self.criterion,
1227                 splitter='best',
1228                 max_depth=self.max_depth,
1229                 min_samples_split=self.min_samples_split,
1230                 min_samples_leaf=self.min_samples_leaf,
1231                 min_weight_fraction_leaf=self.min_weight_fraction_leaf,
1232                 min_impurity_decrease=self.min_impurity_decrease,
1233                 min_impurity_split=self.min_impurity_split,
1234                 max_features=self.max_features,
1235                 max_leaf_nodes=self.max_leaf_nodes,
1236                 random_state=random_state,
1237                 ccp_alpha=self.ccp_alpha)
1238 
1239             if self.subsample < 1.0:
1240                 # no inplace multiplication!
1241                 sample_weight = sample_weight * sample_mask.astype(np.float64)
1242 
1243             X = X_csr if X_csr is not None else X
1244             tree.fit(X, residual, sample_weight=sample_weight,
1245                      check_input=False, X_idx_sorted=X_idx_sorted)
1246 
1247             # update tree leaves
1248             loss.update_terminal_regions(
1249                 tree.tree_, X, y, residual, raw_predictions, sample_weight,
1250                 sample_mask, learning_rate=self.learning_rate, k=k)
1251 
1252             # add tree to ensemble
1253             self.estimators_[i, k] = tree
1254 
1255         return raw_predictions
1256 
1257     def _check_params(self):
1258         """Check validity of parameters and raise ValueError if not valid. """
1259         if self.n_estimators <= 0:
1260             raise ValueError("n_estimators must be greater than 0 but "
1261                              "was %r" % self.n_estimators)
1262 
1263         if self.learning_rate <= 0.0:
1264             raise ValueError("learning_rate must be greater than 0 but "
1265                              "was %r" % self.learning_rate)
1266 
1267         if (self.loss not in self._SUPPORTED_LOSS
1268                 or self.loss not in _gb_losses.LOSS_FUNCTIONS):
1269             raise ValueError("Loss '{0:s}' not supported. ".format(self.loss))
1270 
1271         if self.loss == 'deviance':
1272             loss_class = (_gb_losses.MultinomialDeviance
1273                           if len(self.classes_) > 2
1274                           else _gb_losses.BinomialDeviance)
1275         else:
1276             loss_class = _gb_losses.LOSS_FUNCTIONS[self.loss]
1277 
1278         if self.loss in ('huber', 'quantile'):
1279             self.loss_ = loss_class(self.n_classes_, self.alpha)
1280         else:
1281             self.loss_ = loss_class(self.n_classes_)
1282 
1283         if not (0.0 < self.subsample <= 1.0):
1284             raise ValueError("subsample must be in (0,1] but "
1285                              "was %r" % self.subsample)
1286 
1287         if self.init is not None:
1288             # init must be an estimator or 'zero'
1289             if isinstance(self.init, BaseEstimator):
1290                 self.loss_.check_init_estimator(self.init)
1291             elif not (isinstance(self.init, str) and self.init == 'zero'):
1292                 raise ValueError(
1293                     "The init parameter must be an estimator or 'zero'. "
1294                     "Got init={}".format(self.init)
1295                 )
1296 
1297         if not (0.0 < self.alpha < 1.0):
1298             raise ValueError("alpha must be in (0.0, 1.0) but "
1299                              "was %r" % self.alpha)
1300 
1301         if isinstance(self.max_features, str):
1302             if self.max_features == "auto":
1303                 # if is_classification
1304                 if self.n_classes_ > 1:
1305                     max_features = max(1, int(np.sqrt(self.n_features_)))
1306                 else:
1307                     # is regression
1308                     max_features = self.n_features_
1309             elif self.max_features == "sqrt":
1310                 max_features = max(1, int(np.sqrt(self.n_features_)))
1311             elif self.max_features == "log2":
1312                 max_features = max(1, int(np.log2(self.n_features_)))
1313             else:
1314                 raise ValueError("Invalid value for max_features: %r. "
1315                                  "Allowed string values are 'auto', 'sqrt' "
1316                                  "or 'log2'." % self.max_features)
1317         elif self.max_features is None:
1318             max_features = self.n_features_
1319         elif isinstance(self.max_features, numbers.Integral):
1320             max_features = self.max_features
1321         else:  # float
1322             if 0. < self.max_features <= 1.:
1323                 max_features = max(int(self.max_features *
1324                                        self.n_features_), 1)
1325             else:
1326                 raise ValueError("max_features must be in (0, n_features]")
1327 
1328         self.max_features_ = max_features
1329 
1330         if not isinstance(self.n_iter_no_change,
1331                           (numbers.Integral, type(None))):
1332             raise ValueError("n_iter_no_change should either be None or an "
1333                              "integer. %r was passed"
1334                              % self.n_iter_no_change)
1335 
1336         if self.presort != 'deprecated':
1337             warnings.warn("The parameter 'presort' is deprecated and has no "
1338                           "effect. It will be removed in v0.24. You can "
1339                           "suppress this warning by not passing any value "
1340                           "to the 'presort' parameter. We also recommend "
1341                           "using HistGradientBoosting models instead.",
1342                           FutureWarning)
1343 
1344     def _init_state(self):
1345         """Initialize model state and allocate model state data structures. """
1346 
1347         self.init_ = self.init
1348         if self.init_ is None:
1349             self.init_ = self.loss_.init_estimator()
1350 
1351         self.estimators_ = np.empty((self.n_estimators, self.loss_.K),
1352                                     dtype=np.object)
1353         self.train_score_ = np.zeros((self.n_estimators,), dtype=np.float64)
1354         # do oob?
1355         if self.subsample < 1.0:
1356             self.oob_improvement_ = np.zeros((self.n_estimators),
1357                                              dtype=np.float64)
1358 
1359     def _clear_state(self):
1360         """Clear the state of the gradient boosting model. """
1361         if hasattr(self, 'estimators_'):
1362             self.estimators_ = np.empty((0, 0), dtype=np.object)
1363         if hasattr(self, 'train_score_'):
1364             del self.train_score_
1365         if hasattr(self, 'oob_improvement_'):
1366             del self.oob_improvement_
1367         if hasattr(self, 'init_'):
1368             del self.init_
1369         if hasattr(self, '_rng'):
1370             del self._rng
1371 
1372     def _resize_state(self):
1373         """Add additional ``n_estimators`` entries to all attributes. """
1374         # self.n_estimators is the number of additional est to fit
1375         total_n_estimators = self.n_estimators
1376         if total_n_estimators < self.estimators_.shape[0]:
1377             raise ValueError('resize with smaller n_estimators %d < %d' %
1378                              (total_n_estimators, self.estimators_[0]))
1379 
1380         self.estimators_ = np.resize(self.estimators_,
1381                                      (total_n_estimators, self.loss_.K))
1382         self.train_score_ = np.resize(self.train_score_, total_n_estimators)
1383         if (self.subsample < 1 or hasattr(self, 'oob_improvement_')):
1384             # if do oob resize arrays or create new if not available
1385             if hasattr(self, 'oob_improvement_'):
1386                 self.oob_improvement_ = np.resize(self.oob_improvement_,
1387                                                   total_n_estimators)
1388             else:
1389                 self.oob_improvement_ = np.zeros((total_n_estimators,),
1390                                                  dtype=np.float64)
1391 
1392     def _is_initialized(self):
1393         return len(getattr(self, 'estimators_', [])) > 0
1394 
1395     def _check_initialized(self):
1396         """Check that the estimator is initialized, raising an error if not."""
1397         check_is_fitted(self)
1398 
1399     def fit(self, X, y, sample_weight=None, monitor=None):
1400         """Fit the gradient boosting model.
1401 
1402         Parameters
1403         ----------
1404         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1405             The input samples. Internally, it will be converted to
1406             ``dtype=np.float32`` and if a sparse matrix is provided
1407             to a sparse ``csr_matrix``.
1408 
1409         y : array-like, shape (n_samples,)
1410             Target values (strings or integers in classification, real numbers
1411             in regression)
1412             For classification, labels must correspond to classes.
1413 
1414         sample_weight : array-like, shape (n_samples,) or None
1415             Sample weights. If None, then samples are equally weighted. Splits
1416             that would create child nodes with net zero or negative weight are
1417             ignored while searching for a split in each node. In the case of
1418             classification, splits are also ignored if they would result in any
1419             single class carrying a negative weight in either child node.
1420 
1421         monitor : callable, optional
1422             The monitor is called after each iteration with the current
1423             iteration, a reference to the estimator and the local variables of
1424             ``_fit_stages`` as keyword arguments ``callable(i, self,
1425             locals())``. If the callable returns ``True`` the fitting procedure
1426             is stopped. The monitor can be used for various things such as
1427             computing held-out estimates, early stopping, model introspect, and
1428             snapshoting.
1429 
1430         Returns
1431         -------
1432         self : object
1433         """
1434         # if not warmstart - clear the estimator state
1435         if not self.warm_start:
1436             self._clear_state()
1437 
1438         # Check input
1439         # Since check_array converts both X and y to the same dtype, but the
1440         # trees use different types for X and y, checking them separately.
1441         X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
1442         n_samples, self.n_features_ = X.shape
1443 
1444         sample_weight_is_none = sample_weight is None
1445         if sample_weight_is_none:
1446             sample_weight = np.ones(n_samples, dtype=np.float32)
1447         else:
1448             sample_weight = column_or_1d(sample_weight, warn=True)
1449             sample_weight_is_none = False
1450 
1451         check_consistent_length(X, y, sample_weight)
1452 
1453         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
1454         y = column_or_1d(y, warn=True)
1455         y = self._validate_y(y, sample_weight)
1456 
1457         if self.n_iter_no_change is not None:
1458             stratify = y if is_classifier(self) else None
1459             X, X_val, y, y_val, sample_weight, sample_weight_val = (
1460                 train_test_split(X, y, sample_weight,
1461                                  random_state=self.random_state,
1462                                  test_size=self.validation_fraction,
1463                                  stratify=stratify))
1464             if is_classifier(self):
1465                 if self.n_classes_ != np.unique(y).shape[0]:
1466                     # We choose to error here. The problem is that the init
1467                     # estimator would be trained on y, which has some missing
1468                     # classes now, so its predictions would not have the
1469                     # correct shape.
1470                     raise ValueError(
1471                         'The training data after the early stopping split '
1472                         'is missing some classes. Try using another random '
1473                         'seed.'
1474                     )
1475         else:
1476             X_val = y_val = sample_weight_val = None
1477 
1478         self._check_params()
1479 
1480         if not self._is_initialized():
1481             # init state
1482             self._init_state()
1483 
1484             # fit initial model and initialize raw predictions
1485             if self.init_ == 'zero':
1486                 raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1487                                            dtype=np.float64)
1488             else:
1489                 # XXX clean this once we have a support_sample_weight tag
1490                 if sample_weight_is_none:
1491                     self.init_.fit(X, y)
1492                 else:
1493                     msg = ("The initial estimator {} does not support sample "
1494                            "weights.".format(self.init_.__class__.__name__))
1495                     try:
1496                         self.init_.fit(X, y, sample_weight=sample_weight)
1497                     except TypeError:  # regular estimator without SW support
1498                         raise ValueError(msg)
1499                     except ValueError as e:
1500                         if "pass parameters to specific steps of "\
1501                            "your pipeline using the "\
1502                            "stepname__parameter" in str(e):  # pipeline
1503                             raise ValueError(msg) from e
1504                         else:  # regular estimator whose input checking failed
1505                             raise
1506 
1507                 raw_predictions = \
1508                     self.loss_.get_init_raw_predictions(X, self.init_)
1509 
1510             begin_at_stage = 0
1511 
1512             # The rng state must be preserved if warm_start is True
1513             self._rng = check_random_state(self.random_state)
1514 
1515         else:
1516             # add more estimators to fitted model
1517             # invariant: warm_start = True
1518             if self.n_estimators < self.estimators_.shape[0]:
1519                 raise ValueError('n_estimators=%d must be larger or equal to '
1520                                  'estimators_.shape[0]=%d when '
1521                                  'warm_start==True'
1522                                  % (self.n_estimators,
1523                                     self.estimators_.shape[0]))
1524             begin_at_stage = self.estimators_.shape[0]
1525             # The requirements of _decision_function (called in two lines
1526             # below) are more constrained than fit. It accepts only CSR
1527             # matrices.
1528             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1529             raw_predictions = self._raw_predict(X)
1530             self._resize_state()
1531 
1532         X_idx_sorted = None
1533 
1534         # fit the boosting stages
1535         n_stages = self._fit_stages(
1536             X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,
1537             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)
1538 
1539         # change shape of arrays after fit (early-stopping or additional ests)
1540         if n_stages != self.estimators_.shape[0]:
1541             self.estimators_ = self.estimators_[:n_stages]
1542             self.train_score_ = self.train_score_[:n_stages]
1543             if hasattr(self, 'oob_improvement_'):
1544                 self.oob_improvement_ = self.oob_improvement_[:n_stages]
1545 
1546         self.n_estimators_ = n_stages
1547         return self
1548 
1549     def _fit_stages(self, X, y, raw_predictions, sample_weight, random_state,
1550                     X_val, y_val, sample_weight_val,
1551                     begin_at_stage=0, monitor=None, X_idx_sorted=None):
1552         """Iteratively fits the stages.
1553 
1554         For each stage it computes the progress (OOB, train score)
1555         and delegates to ``_fit_stage``.
1556         Returns the number of stages fit; might differ from ``n_estimators``
1557         due to early stopping.
1558         """
1559         n_samples = X.shape[0]
1560         do_oob = self.subsample < 1.0
1561         sample_mask = np.ones((n_samples, ), dtype=np.bool)
1562         n_inbag = max(1, int(self.subsample * n_samples))
1563         loss_ = self.loss_
1564 
1565         if self.verbose:
1566             verbose_reporter = VerboseReporter(self.verbose)
1567             verbose_reporter.init(self, begin_at_stage)
1568 
1569         X_csc = csc_matrix(X) if issparse(X) else None
1570         X_csr = csr_matrix(X) if issparse(X) else None
1571 
1572         if self.n_iter_no_change is not None:
1573             loss_history = np.full(self.n_iter_no_change, np.inf)
1574             # We create a generator to get the predictions for X_val after
1575             # the addition of each successive stage
1576             y_val_pred_iter = self._staged_raw_predict(X_val)
1577 
1578         # perform boosting iterations
1579         i = begin_at_stage
1580         for i in range(begin_at_stage, self.n_estimators):
1581 
1582             # subsampling
1583             if do_oob:
1584                 sample_mask = _random_sample_mask(n_samples, n_inbag,
1585                                                   random_state)
1586                 # OOB score before adding this stage
1587                 old_oob_score = loss_(y[~sample_mask],
1588                                       raw_predictions[~sample_mask],
1589                                       sample_weight[~sample_mask])
1590 
1591             # fit next stage of trees
1592             raw_predictions = self._fit_stage(
1593                 i, X, y, raw_predictions, sample_weight, sample_mask,
1594                 random_state, X_idx_sorted, X_csc, X_csr)
1595 
1596             # track deviance (= loss)
1597             if do_oob:
1598                 self.train_score_[i] = loss_(y[sample_mask],
1599                                              raw_predictions[sample_mask],
1600                                              sample_weight[sample_mask])
1601                 self.oob_improvement_[i] = (
1602                     old_oob_score - loss_(y[~sample_mask],
1603                                           raw_predictions[~sample_mask],
1604                                           sample_weight[~sample_mask]))
1605             else:
1606                 # no need to fancy index w/ no subsampling
1607                 self.train_score_[i] = loss_(y, raw_predictions, sample_weight)
1608 
1609             if self.verbose > 0:
1610                 verbose_reporter.update(i, self)
1611 
1612             if monitor is not None:
1613                 early_stopping = monitor(i, self, locals())
1614                 if early_stopping:
1615                     break
1616 
1617             # We also provide an early stopping based on the score from
1618             # validation set (X_val, y_val), if n_iter_no_change is set
1619             if self.n_iter_no_change is not None:
1620                 # By calling next(y_val_pred_iter), we get the predictions
1621                 # for X_val after the addition of the current stage
1622                 validation_loss = loss_(y_val, next(y_val_pred_iter),
1623                                         sample_weight_val)
1624 
1625                 # Require validation_score to be better (less) than at least
1626                 # one of the last n_iter_no_change evaluations
1627                 if np.any(validation_loss + self.tol < loss_history):
1628                     loss_history[i % len(loss_history)] = validation_loss
1629                 else:
1630                     break
1631 
1632         return i + 1
1633 
1634     def _make_estimator(self, append=True):
1635         # we don't need _make_estimator
1636         raise NotImplementedError()
1637 
1638     def _raw_predict_init(self, X):
1639         """Check input and compute raw predictions of the init estimtor."""
1640         self._check_initialized()
1641         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1642         if X.shape[1] != self.n_features_:
1643             raise ValueError("X.shape[1] should be {0:d}, not {1:d}.".format(
1644                 self.n_features_, X.shape[1]))
1645         if self.init_ == 'zero':
1646             raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1647                                        dtype=np.float64)
1648         else:
1649             raw_predictions = self.loss_.get_init_raw_predictions(
1650                 X, self.init_).astype(np.float64)
1651         return raw_predictions
1652 
1653     def _raw_predict(self, X):
1654         """Return the sum of the trees raw predictions (+ init estimator)."""
1655         raw_predictions = self._raw_predict_init(X)
1656         predict_stages(self.estimators_, X, self.learning_rate,
1657                        raw_predictions)
1658         return raw_predictions
1659 
1660     def _staged_raw_predict(self, X):
1661         """Compute raw predictions of ``X`` for each iteration.
1662 
1663         This method allows monitoring (i.e. determine error on testing set)
1664         after each stage.
1665 
1666         Parameters
1667         ----------
1668         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1669             The input samples. Internally, it will be converted to
1670             ``dtype=np.float32`` and if a sparse matrix is provided
1671             to a sparse ``csr_matrix``.
1672 
1673         Returns
1674         -------
1675         raw_predictions : generator of array, shape (n_samples, k)
1676             The raw predictions of the input samples. The order of the
1677             classes corresponds to that in the attribute :term:`classes_`.
1678             Regression and binary classification are special cases with
1679             ``k == 1``, otherwise ``k==n_classes``.
1680         """
1681         X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1682         raw_predictions = self._raw_predict_init(X)
1683         for i in range(self.estimators_.shape[0]):
1684             predict_stage(self.estimators_, i, X, self.learning_rate,
1685                           raw_predictions)
1686             yield raw_predictions.copy()
1687 
1688     @property
1689     def feature_importances_(self):
1690         """Return the feature importances (the higher, the more important the
1691            feature).
1692 
1693         Returns
1694         -------
1695         feature_importances_ : array, shape (n_features,)
1696             The values of this array sum to 1, unless all trees are single node
1697             trees consisting of only the root node, in which case it will be an
1698             array of zeros.
1699         """
1700         self._check_initialized()
1701 
1702         relevant_trees = [tree
1703                           for stage in self.estimators_ for tree in stage
1704                           if tree.tree_.node_count > 1]
1705         if not relevant_trees:
1706             # degenerate case where all trees have only one node
1707             return np.zeros(shape=self.n_features_, dtype=np.float64)
1708 
1709         relevant_feature_importances = [
1710             tree.tree_.compute_feature_importances(normalize=False)
1711             for tree in relevant_trees
1712         ]
1713         avg_feature_importances = np.mean(relevant_feature_importances,
1714                                           axis=0, dtype=np.float64)
1715         return avg_feature_importances / np.sum(avg_feature_importances)
1716 
1717     def _compute_partial_dependence_recursion(self, grid, target_features):
1718         """Fast partial dependence computation.
1719 
1720         Parameters
1721         ----------
1722         grid : ndarray, shape (n_samples, n_target_features)
1723             The grid points on which the partial dependence should be
1724             evaluated.
1725         target_features : ndarray, shape (n_target_features)
1726             The set of target features for which the partial dependence
1727             should be evaluated.
1728 
1729         Returns
1730         -------
1731         averaged_predictions : ndarray, shape \
1732                 (n_trees_per_iteration, n_samples)
1733             The value of the partial dependence function on each grid point.
1734         """
1735         check_is_fitted(self,
1736                         msg="'estimator' parameter must be a fitted estimator")
1737         if self.init is not None:
1738             warnings.warn(
1739                 'Using recursion method with a non-constant init predictor '
1740                 'will lead to incorrect partial dependence values. '
1741                 'Got init=%s.' % self.init,
1742                 UserWarning
1743             )
1744         grid = np.asarray(grid, dtype=DTYPE, order='C')
1745         n_estimators, n_trees_per_stage = self.estimators_.shape
1746         averaged_predictions = np.zeros((n_trees_per_stage, grid.shape[0]),
1747                                         dtype=np.float64, order='C')
1748         for stage in range(n_estimators):
1749             for k in range(n_trees_per_stage):
1750                 tree = self.estimators_[stage, k].tree_
1751                 tree.compute_partial_dependence(grid, target_features,
1752                                                 averaged_predictions[k])
1753         averaged_predictions *= self.learning_rate
1754 
1755         return averaged_predictions
1756 
1757     def _validate_y(self, y, sample_weight):
1758         # 'sample_weight' is not utilised but is used for
1759         # consistency with similar method _validate_y of GBC
1760         self.n_classes_ = 1
1761         if y.dtype.kind == 'O':
1762             y = y.astype(DOUBLE)
1763         # Default implementation
1764         return y
1765 
1766     def apply(self, X):
1767         """Apply trees in the ensemble to X, return leaf indices.
1768 
1769         .. versionadded:: 0.17
1770 
1771         Parameters
1772         ----------
1773         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1774             The input samples. Internally, its dtype will be converted to
1775             ``dtype=np.float32``. If a sparse matrix is provided, it will
1776             be converted to a sparse ``csr_matrix``.
1777 
1778         Returns
1779         -------
1780         X_leaves : array-like, shape (n_samples, n_estimators, n_classes)
1781             For each datapoint x in X and for each tree in the ensemble,
1782             return the index of the leaf x ends up in each estimator.
1783             In the case of binary classification n_classes is 1.
1784         """
1785 
1786         self._check_initialized()
1787         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1788 
1789         # n_classes will be equal to 1 in the binary classification or the
1790         # regression case.
1791         n_estimators, n_classes = self.estimators_.shape
1792         leaves = np.zeros((X.shape[0], n_estimators, n_classes))
1793 
1794         for i in range(n_estimators):
1795             for j in range(n_classes):
1796                 estimator = self.estimators_[i, j]
1797                 leaves[:, i, j] = estimator.apply(X, check_input=False)
1798 
1799         return leaves

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 17:29:35.625 | INFO     | Search completed. Bug locations: [<file>sklearn/calibration.py</file>
<class>CalibratedClassifierCV</class>
<method>fit</method>
<code>
114     def fit(self, X, y, sample_weight=None):
115         """Fit the calibrated model
116 
117         Parameters
118         ----------
119         X : array-like, shape (n_samples, n_features)
120             Training data.
121 
122         y : array-like, shape (n_samples,)
123             Target values.
124 
125         sample_weight : array-like of shape (n_samples,), default=None
126             Sample weights. If None, then samples are equally weighted.
127 
128         Returns
129         -------
130         self : object
131             Returns an instance of self.
132         """
133         X, y = check_X_y(X, y, accept_sparse=['csc', 'csr', 'coo'],
134                          force_all_finite=False, allow_nd=True)
135         X, y = indexable(X, y)
136         le = LabelBinarizer().fit(y)
137         self.classes_ = le.classes_
138 
139         # Check that each cross-validation fold can have at least one
140         # example per class
141         n_folds = self.cv if isinstance(self.cv, int) \
142             else self.cv.n_folds if hasattr(self.cv, "n_folds") else None
143         if n_folds and \
144                 np.any([np.sum(y == class_) < n_folds for class_ in
145                         self.classes_]):
146             raise ValueError("Requesting %d-fold cross-validation but provided"
147                              " less than %d examples for at least one class."
148                              % (n_folds, n_folds))
149 
150         self.calibrated_classifiers_ = []
151         if self.base_estimator is None:
152             # we want all classifiers that don't expose a random_state
153             # to be deterministic (and we don't want to expose this one).
154             base_estimator = LinearSVC(random_state=0)
155         else:
156             base_estimator = self.base_estimator
157 
158         if self.cv == "prefit":
159             calibrated_classifier = _CalibratedClassifier(
160                 base_estimator, method=self.method)
161             if sample_weight is not None:
162                 calibrated_classifier.fit(X, y, sample_weight)
163             else:
164                 calibrated_classifier.fit(X, y)
165             self.calibrated_classifiers_.append(calibrated_classifier)
166         else:
167             cv = check_cv(self.cv, y, classifier=True)
168             fit_parameters = signature(base_estimator.fit).parameters
169             estimator_name = type(base_estimator).__name__
170             if (sample_weight is not None
171                     and "sample_weight" not in fit_parameters):
172                 warnings.warn("%s does not support sample_weight. Samples"
173                               " weights are only used for the calibration"
174                               " itself." % estimator_name)
175                 sample_weight = check_array(sample_weight, ensure_2d=False)
176                 base_estimator_sample_weight = None
177             else:
178                 if sample_weight is not None:
179                     sample_weight = check_array(sample_weight, ensure_2d=False)
180                     check_consistent_length(y, sample_weight)
181                 base_estimator_sample_weight = sample_weight
182             for train, test in cv.split(X, y):
183                 this_estimator = clone(base_estimator)
184                 if base_estimator_sample_weight is not None:
185                     this_estimator.fit(
186                         X[train], y[train],
187                         sample_weight=base_estimator_sample_weight[train])
188                 else:
189                     this_estimator.fit(X[train], y[train])
190 
191                 calibrated_classifier = _CalibratedClassifier(
192                     this_estimator, method=self.method,
193                     classes=self.classes_)
194                 if sample_weight is not None:
195                     calibrated_classifier.fit(X[test], y[test],
196                                               sample_weight[test])
197                 else:
198                     calibrated_classifier.fit(X[test], y[test])
199                 self.calibrated_classifiers_.append(calibrated_classifier)
200 
201         return self

</code><intended_behavior>The fit method should utilize _check_sample_weight to validate and handle sample_weight efficiently and consistently with the rest of the sklearn library, ensuring any custom or ad-hoc handling is replaced or integrated with the standard utility method.</intended_behavior>, <file>sklearn/calibration.py</file>
<class>CalibratedClassifierCV</class>
<method>None</method>
<code>
31 class CalibratedClassifierCV(BaseEstimator, ClassifierMixin,
32                              MetaEstimatorMixin):
33     """Probability calibration with isotonic regression or sigmoid.
34 
35     See glossary entry for :term:`cross-validation estimator`.
36 
37     With this class, the base_estimator is fit on the train set of the
38     cross-validation generator and the test set is used for calibration.
39     The probabilities for each of the folds are then averaged
40     for prediction. In case that cv="prefit" is passed to __init__,
41     it is assumed that base_estimator has been fitted already and all
42     data is used for calibration. Note that data for fitting the
43     classifier and for calibrating it must be disjoint.
44 
45     Read more in the :ref:`User Guide <calibration>`.
46 
47     Parameters
48     ----------
49     base_estimator : instance BaseEstimator
50         The classifier whose output decision function needs to be calibrated
51         to offer more accurate predict_proba outputs. If cv=prefit, the
52         classifier must have been fit already on data.
53 
54     method : 'sigmoid' or 'isotonic'
55         The method to use for calibration. Can be 'sigmoid' which
56         corresponds to Platt's method or 'isotonic' which is a
57         non-parametric approach. It is not advised to use isotonic calibration
58         with too few calibration samples ``(<<1000)`` since it tends to
59         overfit.
60         Use sigmoids (Platt's calibration) in this case.
61 
62     cv : integer, cross-validation generator, iterable or "prefit", optional
63         Determines the cross-validation splitting strategy.
64         Possible inputs for cv are:
65 
66         - None, to use the default 5-fold cross-validation,
67         - integer, to specify the number of folds.
68         - :term:`CV splitter`,
69         - An iterable yielding (train, test) splits as arrays of indices.
70 
71         For integer/None inputs, if ``y`` is binary or multiclass,
72         :class:`sklearn.model_selection.StratifiedKFold` is used. If ``y`` is
73         neither binary nor multiclass, :class:`sklearn.model_selection.KFold`
74         is used.
75 
76         Refer :ref:`User Guide <cross_validation>` for the various
77         cross-validation strategies that can be used here.
78 
79         If "prefit" is passed, it is assumed that base_estimator has been
80         fitted already and all data is used for calibration.
81 
82         .. versionchanged:: 0.22
83             ``cv`` default value if None changed from 3-fold to 5-fold.
84 
85     Attributes
86     ----------
87     classes_ : array, shape (n_classes)
88         The class labels.
89 
90     calibrated_classifiers_ : list (len() equal to cv or 1 if cv == "prefit")
91         The list of calibrated classifiers, one for each crossvalidation fold,
92         which has been fitted on all but the validation fold and calibrated
93         on the validation fold.
94 
95     References
96     ----------
97     .. [1] Obtaining calibrated probability estimates from decision trees
98            and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001
99 
100     .. [2] Transforming Classifier Scores into Accurate Multiclass
101            Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)
102 
103     .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to
104            Regularized Likelihood Methods, J. Platt, (1999)
105 
106     .. [4] Predicting Good Probabilities with Supervised Learning,
107            A. Niculescu-Mizil & R. Caruana, ICML 2005
108     """
109     def __init__(self, base_estimator=None, method='sigmoid', cv=None):
110         self.base_estimator = base_estimator
111         self.method = method
112         self.cv = cv
113 
114     def fit(self, X, y, sample_weight=None):
115         """Fit the calibrated model
116 
117         Parameters
118         ----------
119         X : array-like, shape (n_samples, n_features)
120             Training data.
121 
122         y : array-like, shape (n_samples,)
123             Target values.
124 
125         sample_weight : array-like of shape (n_samples,), default=None
126             Sample weights. If None, then samples are equally weighted.
127 
128         Returns
129         -------
130         self : object
131             Returns an instance of self.
132         """
133         X, y = check_X_y(X, y, accept_sparse=['csc', 'csr', 'coo'],
134                          force_all_finite=False, allow_nd=True)
135         X, y = indexable(X, y)
136         le = LabelBinarizer().fit(y)
137         self.classes_ = le.classes_
138 
139         # Check that each cross-validation fold can have at least one
140         # example per class
141         n_folds = self.cv if isinstance(self.cv, int) \
142             else self.cv.n_folds if hasattr(self.cv, "n_folds") else None
143         if n_folds and \
144                 np.any([np.sum(y == class_) < n_folds for class_ in
145                         self.classes_]):
146             raise ValueError("Requesting %d-fold cross-validation but provided"
147                              " less than %d examples for at least one class."
148                              % (n_folds, n_folds))
149 
150         self.calibrated_classifiers_ = []
151         if self.base_estimator is None:
152             # we want all classifiers that don't expose a random_state
153             # to be deterministic (and we don't want to expose this one).
154             base_estimator = LinearSVC(random_state=0)
155         else:
156             base_estimator = self.base_estimator
157 
158         if self.cv == "prefit":
159             calibrated_classifier = _CalibratedClassifier(
160                 base_estimator, method=self.method)
161             if sample_weight is not None:
162                 calibrated_classifier.fit(X, y, sample_weight)
163             else:
164                 calibrated_classifier.fit(X, y)
165             self.calibrated_classifiers_.append(calibrated_classifier)
166         else:
167             cv = check_cv(self.cv, y, classifier=True)
168             fit_parameters = signature(base_estimator.fit).parameters
169             estimator_name = type(base_estimator).__name__
170             if (sample_weight is not None
171                     and "sample_weight" not in fit_parameters):
172                 warnings.warn("%s does not support sample_weight. Samples"
173                               " weights are only used for the calibration"
174                               " itself." % estimator_name)
175                 sample_weight = check_array(sample_weight, ensure_2d=False)
176                 base_estimator_sample_weight = None
177             else:
178                 if sample_weight is not None:
179                     sample_weight = check_array(sample_weight, ensure_2d=False)
180                     check_consistent_length(y, sample_weight)
181                 base_estimator_sample_weight = sample_weight
182             for train, test in cv.split(X, y):
183                 this_estimator = clone(base_estimator)
184                 if base_estimator_sample_weight is not None:
185                     this_estimator.fit(
186                         X[train], y[train],
187                         sample_weight=base_estimator_sample_weight[train])
188                 else:
189                     this_estimator.fit(X[train], y[train])
190 
191                 calibrated_classifier = _CalibratedClassifier(
192                     this_estimator, method=self.method,
193                     classes=self.classes_)
194                 if sample_weight is not None:
195                     calibrated_classifier.fit(X[test], y[test],
196                                               sample_weight[test])
197                 else:
198                     calibrated_classifier.fit(X[test], y[test])
199                 self.calibrated_classifiers_.append(calibrated_classifier)
200 
201         return self
202 
203     def predict_proba(self, X):
204         """Posterior probabilities of classification
205 
206         This function returns posterior probabilities of classification
207         according to each class on an array of test vectors X.
208 
209         Parameters
210         ----------
211         X : array-like, shape (n_samples, n_features)
212             The samples.
213 
214         Returns
215         -------
216         C : array, shape (n_samples, n_classes)
217             The predicted probas.
218         """
219         check_is_fitted(self)
220         X = check_array(X, accept_sparse=['csc', 'csr', 'coo'],
221                         force_all_finite=False)
222         # Compute the arithmetic mean of the predictions of the calibrated
223         # classifiers
224         mean_proba = np.zeros((X.shape[0], len(self.classes_)))
225         for calibrated_classifier in self.calibrated_classifiers_:
226             proba = calibrated_classifier.predict_proba(X)
227             mean_proba += proba
228 
229         mean_proba /= len(self.calibrated_classifiers_)
230 
231         return mean_proba
232 
233     def predict(self, X):
234         """Predict the target of new samples. Can be different from the
235         prediction of the uncalibrated classifier.
236 
237         Parameters
238         ----------
239         X : array-like, shape (n_samples, n_features)
240             The samples.
241 
242         Returns
243         -------
244         C : array, shape (n_samples,)
245             The predicted class.
246         """
247         check_is_fitted(self)
248         return self.classes_[np.argmax(self.predict_proba(X), axis=1)]

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/cluster/_dbscan.py</file>
<class>DBSCAN</class>
<method>fit</method>
<code>
284     def fit(self, X, y=None, sample_weight=None):
285         """Perform DBSCAN clustering from features, or distance matrix.
286 
287         Parameters
288         ----------
289         X : array-like or sparse matrix, shape (n_samples, n_features), or \
290             (n_samples, n_samples)
291             Training instances to cluster, or distances between instances if
292             ``metric='precomputed'``. If a sparse matrix is provided, it will
293             be converted into a sparse ``csr_matrix``.
294 
295         sample_weight : array, shape (n_samples,), optional
296             Weight of each sample, such that a sample with a weight of at least
297             ``min_samples`` is by itself a core sample; a sample with a
298             negative weight may inhibit its eps-neighbor from being core.
299             Note that weights are absolute, and default to 1.
300 
301         y : Ignored
302             Not used, present here for API consistency by convention.
303 
304         Returns
305         -------
306         self
307 
308         """
309         X = check_array(X, accept_sparse='csr')
310 
311         if not self.eps > 0.0:
312             raise ValueError("eps must be positive.")
313 
314         if sample_weight is not None:
315             sample_weight = np.asarray(sample_weight)
316             check_consistent_length(X, sample_weight)
317 
318         # Calculate neighborhood for all samples. This leaves the original
319         # point in, which needs to be considered later (i.e. point i is in the
320         # neighborhood of point i. While True, its useless information)
321         if self.metric == 'precomputed' and sparse.issparse(X):
322             # set the diagonal to explicit values, as a point is its own
323             # neighbor
324             with warnings.catch_warnings():
325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)
326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place
327 
328         neighbors_model = NearestNeighbors(
329             radius=self.eps, algorithm=self.algorithm,
330             leaf_size=self.leaf_size, metric=self.metric,
331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)
332         neighbors_model.fit(X)
333         # This has worst case O(n^2) memory complexity
334         neighborhoods = neighbors_model.radius_neighbors(X,
335                                                          return_distance=False)
336 
337         if sample_weight is None:
338             n_neighbors = np.array([len(neighbors)
339                                     for neighbors in neighborhoods])
340         else:
341             n_neighbors = np.array([np.sum(sample_weight[neighbors])
342                                     for neighbors in neighborhoods])
343 
344         # Initially, all samples are noise.
345         labels = np.full(X.shape[0], -1, dtype=np.intp)
346 
347         # A list of all core samples found.
348         core_samples = np.asarray(n_neighbors >= self.min_samples,
349                                   dtype=np.uint8)
350         dbscan_inner(core_samples, neighborhoods, labels)
351 
352         self.core_sample_indices_ = np.where(core_samples)[0]
353         self.labels_ = labels
354 
355         if len(self.core_sample_indices_):
356             # fix for scipy sparse indexing issue
357             self.components_ = X[self.core_sample_indices_].copy()
358         else:
359             # no core samples
360             self.components_ = np.empty((0, X.shape[1]))
361         return self

</code><intended_behavior>While DBSCAN may not explicitly validate sample_weight in the same way other estimators do due to its unique implementation, ensuring any internal handling of sample_weight is consistent or verified through _check_sample_weight could improve uniformity across the library.</intended_behavior>, <file>sklearn/cluster/_dbscan.py</file>
<class>DBSCAN</class>
<method>None</method>
<code>
147 class DBSCAN(ClusterMixin, BaseEstimator):
148     """Perform DBSCAN clustering from vector array or distance matrix.
149 
150     DBSCAN - Density-Based Spatial Clustering of Applications with Noise.
151     Finds core samples of high density and expands clusters from them.
152     Good for data which contains clusters of similar density.
153 
154     Read more in the :ref:`User Guide <dbscan>`.
155 
156     Parameters
157     ----------
158     eps : float, optional
159         The maximum distance between two samples for one to be considered
160         as in the neighborhood of the other. This is not a maximum bound
161         on the distances of points within a cluster. This is the most
162         important DBSCAN parameter to choose appropriately for your data set
163         and distance function.
164 
165     min_samples : int, optional
166         The number of samples (or total weight) in a neighborhood for a point
167         to be considered as a core point. This includes the point itself.
168 
169     metric : string, or callable
170         The metric to use when calculating distance between instances in a
171         feature array. If metric is a string or callable, it must be one of
172         the options allowed by :func:`sklearn.metrics.pairwise_distances` for
173         its metric parameter.
174         If metric is "precomputed", X is assumed to be a distance matrix and
175         must be square. X may be a :term:`Glossary <sparse graph>`, in which
176         case only "nonzero" elements may be considered neighbors for DBSCAN.
177 
178         .. versionadded:: 0.17
179            metric *precomputed* to accept precomputed sparse matrix.
180 
181     metric_params : dict, optional
182         Additional keyword arguments for the metric function.
183 
184         .. versionadded:: 0.19
185 
186     algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional
187         The algorithm to be used by the NearestNeighbors module
188         to compute pointwise distances and find nearest neighbors.
189         See NearestNeighbors module documentation for details.
190 
191     leaf_size : int, optional (default = 30)
192         Leaf size passed to BallTree or cKDTree. This can affect the speed
193         of the construction and query, as well as the memory required
194         to store the tree. The optimal value depends
195         on the nature of the problem.
196 
197     p : float, optional
198         The power of the Minkowski metric to be used to calculate distance
199         between points.
200 
201     n_jobs : int or None, optional (default=None)
202         The number of parallel jobs to run.
203         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
204         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
205         for more details.
206 
207     Attributes
208     ----------
209     core_sample_indices_ : array, shape = [n_core_samples]
210         Indices of core samples.
211 
212     components_ : array, shape = [n_core_samples, n_features]
213         Copy of each core sample found by training.
214 
215     labels_ : array, shape = [n_samples]
216         Cluster labels for each point in the dataset given to fit().
217         Noisy samples are given the label -1.
218 
219     Examples
220     --------
221     >>> from sklearn.cluster import DBSCAN
222     >>> import numpy as np
223     >>> X = np.array([[1, 2], [2, 2], [2, 3],
224     ...               [8, 7], [8, 8], [25, 80]])
225     >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)
226     >>> clustering.labels_
227     array([ 0,  0,  0,  1,  1, -1])
228     >>> clustering
229     DBSCAN(eps=3, min_samples=2)
230 
231     See also
232     --------
233     OPTICS
234         A similar clustering at multiple values of eps. Our implementation
235         is optimized for memory usage.
236 
237     Notes
238     -----
239     For an example, see :ref:`examples/cluster/plot_dbscan.py
240     <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.
241 
242     This implementation bulk-computes all neighborhood queries, which increases
243     the memory complexity to O(n.d) where d is the average number of neighbors,
244     while original DBSCAN had memory complexity O(n). It may attract a higher
245     memory complexity when querying these nearest neighborhoods, depending
246     on the ``algorithm``.
247 
248     One way to avoid the query complexity is to pre-compute sparse
249     neighborhoods in chunks using
250     :func:`NearestNeighbors.radius_neighbors_graph
251     <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with
252     ``mode='distance'``, then using ``metric='precomputed'`` here.
253 
254     Another way to reduce memory and computation time is to remove
255     (near-)duplicate points and use ``sample_weight`` instead.
256 
257     :class:`cluster.OPTICS` provides a similar clustering with lower memory
258     usage.
259 
260     References
261     ----------
262     Ester, M., H. P. Kriegel, J. Sander, and X. Xu, "A Density-Based
263     Algorithm for Discovering Clusters in Large Spatial Databases with Noise".
264     In: Proceedings of the 2nd International Conference on Knowledge Discovery
265     and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996
266 
267     Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).
268     DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.
269     ACM Transactions on Database Systems (TODS), 42(3), 19.
270     """
271 
272     def __init__(self, eps=0.5, min_samples=5, metric='euclidean',
273                  metric_params=None, algorithm='auto', leaf_size=30, p=None,
274                  n_jobs=None):
275         self.eps = eps
276         self.min_samples = min_samples
277         self.metric = metric
278         self.metric_params = metric_params
279         self.algorithm = algorithm
280         self.leaf_size = leaf_size
281         self.p = p
282         self.n_jobs = n_jobs
283 
284     def fit(self, X, y=None, sample_weight=None):
285         """Perform DBSCAN clustering from features, or distance matrix.
286 
287         Parameters
288         ----------
289         X : array-like or sparse matrix, shape (n_samples, n_features), or \
290             (n_samples, n_samples)
291             Training instances to cluster, or distances between instances if
292             ``metric='precomputed'``. If a sparse matrix is provided, it will
293             be converted into a sparse ``csr_matrix``.
294 
295         sample_weight : array, shape (n_samples,), optional
296             Weight of each sample, such that a sample with a weight of at least
297             ``min_samples`` is by itself a core sample; a sample with a
298             negative weight may inhibit its eps-neighbor from being core.
299             Note that weights are absolute, and default to 1.
300 
301         y : Ignored
302             Not used, present here for API consistency by convention.
303 
304         Returns
305         -------
306         self
307 
308         """
309         X = check_array(X, accept_sparse='csr')
310 
311         if not self.eps > 0.0:
312             raise ValueError("eps must be positive.")
313 
314         if sample_weight is not None:
315             sample_weight = np.asarray(sample_weight)
316             check_consistent_length(X, sample_weight)
317 
318         # Calculate neighborhood for all samples. This leaves the original
319         # point in, which needs to be considered later (i.e. point i is in the
320         # neighborhood of point i. While True, its useless information)
321         if self.metric == 'precomputed' and sparse.issparse(X):
322             # set the diagonal to explicit values, as a point is its own
323             # neighbor
324             with warnings.catch_warnings():
325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)
326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place
327 
328         neighbors_model = NearestNeighbors(
329             radius=self.eps, algorithm=self.algorithm,
330             leaf_size=self.leaf_size, metric=self.metric,
331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)
332         neighbors_model.fit(X)
333         # This has worst case O(n^2) memory complexity
334         neighborhoods = neighbors_model.radius_neighbors(X,
335                                                          return_distance=False)
336 
337         if sample_weight is None:
338             n_neighbors = np.array([len(neighbors)
339                                     for neighbors in neighborhoods])
340         else:
341             n_neighbors = np.array([np.sum(sample_weight[neighbors])
342                                     for neighbors in neighborhoods])
343 
344         # Initially, all samples are noise.
345         labels = np.full(X.shape[0], -1, dtype=np.intp)
346 
347         # A list of all core samples found.
348         core_samples = np.asarray(n_neighbors >= self.min_samples,
349                                   dtype=np.uint8)
350         dbscan_inner(core_samples, neighborhoods, labels)
351 
352         self.core_sample_indices_ = np.where(core_samples)[0]
353         self.labels_ = labels
354 
355         if len(self.core_sample_indices_):
356             # fix for scipy sparse indexing issue
357             self.components_ = X[self.core_sample_indices_].copy()
358         else:
359             # no core samples
360             self.components_ = np.empty((0, X.shape[1]))
361         return self
362 
363     def fit_predict(self, X, y=None, sample_weight=None):
364         """Perform DBSCAN clustering from features or distance matrix,
365         and return cluster labels.
366 
367         Parameters
368         ----------
369         X : array-like or sparse matrix, shape (n_samples, n_features), or \
370             (n_samples, n_samples)
371             Training instances to cluster, or distances between instances if
372             ``metric='precomputed'``. If a sparse matrix is provided, it will
373             be converted into a sparse ``csr_matrix``.
374 
375         sample_weight : array, shape (n_samples,), optional
376             Weight of each sample, such that a sample with a weight of at least
377             ``min_samples`` is by itself a core sample; a sample with a
378             negative weight may inhibit its eps-neighbor from being core.
379             Note that weights are absolute, and default to 1.
380 
381         y : Ignored
382             Not used, present here for API consistency by convention.
383 
384         Returns
385         -------
386         labels : ndarray, shape (n_samples,)
387             Cluster labels. Noisy samples are given the label -1.
388         """
389         self.fit(X, sample_weight=sample_weight)
390         return self.labels_

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/dummy.py</file>
<class>DummyClassifier</class>
<method>fit</method>
<code>
101     def fit(self, X, y, sample_weight=None):
102         """Fit the random classifier.
103 
104         Parameters
105         ----------
106         X : {array-like, object with finite length or shape}
107             Training data, requires length = n_samples
108 
109         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
110             Target values.
111 
112         sample_weight : array-like of shape (n_samples,), default=None
113             Sample weights.
114 
115         Returns
116         -------
117         self : object
118         """
119         allowed_strategies = ("most_frequent", "stratified", "uniform",
120                               "constant", "prior")
121         if self.strategy not in allowed_strategies:
122             raise ValueError("Unknown strategy type: %s, expected one of %s."
123                              % (self.strategy, allowed_strategies))
124 
125         if self.strategy == "uniform" and sp.issparse(y):
126             y = y.toarray()
127             warnings.warn('A local copy of the target data has been converted '
128                           'to a numpy array. Predicting on sparse target data '
129                           'with the uniform strategy would not save memory '
130                           'and would be slower.',
131                           UserWarning)
132 
133         self.sparse_output_ = sp.issparse(y)
134 
135         if not self.sparse_output_:
136             y = np.asarray(y)
137             y = np.atleast_1d(y)
138 
139         if y.ndim == 1:
140             y = np.reshape(y, (-1, 1))
141 
142         self.n_outputs_ = y.shape[1]
143 
144         check_consistent_length(X, y, sample_weight)
145 
146         if self.strategy == "constant":
147             if self.constant is None:
148                 raise ValueError("Constant target value has to be specified "
149                                  "when the constant strategy is used.")
150             else:
151                 constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))
152                 if constant.shape[0] != self.n_outputs_:
153                     raise ValueError("Constant target value should have "
154                                      "shape (%d, 1)." % self.n_outputs_)
155 
156         (self.classes_,
157          self.n_classes_,
158          self.class_prior_) = class_distribution(y, sample_weight)
159 
160         if self.strategy == "constant":
161             for k in range(self.n_outputs_):
162                 if not any(constant[k][0] == c for c in self.classes_[k]):
163                     # Checking in case of constant strategy if the constant
164                     # provided by the user is in y.
165                     err_msg = ("The constant target value must be present in "
166                                "the training data. You provided constant={}. "
167                                "Possible values are: {}."
168                                .format(self.constant, list(self.classes_[k])))
169                     raise ValueError(err_msg)
170 
171         if self.n_outputs_ == 1:
172             self.n_classes_ = self.n_classes_[0]
173             self.classes_ = self.classes_[0]
174             self.class_prior_ = self.class_prior_[0]
175 
176         return self

</code><intended_behavior>The fit method of DummyClassifier should incorporate _check_sample_weight for consistent sample weight validation and processing, replacing any custom logic currently implemented.</intended_behavior>, <file>sklearn/dummy.py</file>
<class>DummyClassifier</class>
<method>None</method>
<code>
23 class DummyClassifier(MultiOutputMixin, ClassifierMixin, BaseEstimator):
24     """
25     DummyClassifier is a classifier that makes predictions using simple rules.
26 
27     This classifier is useful as a simple baseline to compare with other
28     (real) classifiers. Do not use it for real problems.
29 
30     Read more in the :ref:`User Guide <dummy_estimators>`.
31 
32     Parameters
33     ----------
34     strategy : str, default="stratified"
35         Strategy to use to generate predictions.
36 
37         * "stratified": generates predictions by respecting the training
38           set's class distribution.
39         * "most_frequent": always predicts the most frequent label in the
40           training set.
41         * "prior": always predicts the class that maximizes the class prior
42           (like "most_frequent") and ``predict_proba`` returns the class prior.
43         * "uniform": generates predictions uniformly at random.
44         * "constant": always predicts a constant label that is provided by
45           the user. This is useful for metrics that evaluate a non-majority
46           class
47 
48           .. versionadded:: 0.17
49              Dummy Classifier now supports prior fitting strategy using
50              parameter *prior*.
51 
52     random_state : int, RandomState instance or None, optional, default=None
53         If int, random_state is the seed used by the random number generator;
54         If RandomState instance, random_state is the random number generator;
55         If None, the random number generator is the RandomState instance used
56         by `np.random`.
57 
58     constant : int or str or array-like of shape (n_outputs,)
59         The explicit constant as predicted by the "constant" strategy. This
60         parameter is useful only for the "constant" strategy.
61 
62     Attributes
63     ----------
64     classes_ : array or list of array of shape (n_classes,)
65         Class labels for each output.
66 
67     n_classes_ : array or list of array of shape (n_classes,)
68         Number of label for each output.
69 
70     class_prior_ : array or list of array of shape (n_classes,)
71         Probability of each class for each output.
72 
73     n_outputs_ : int,
74         Number of outputs.
75 
76     sparse_output_ : bool,
77         True if the array returned from predict is to be in sparse CSC format.
78         Is automatically set to True if the input y is passed in sparse format.
79 
80     Examples
81     --------
82     >>> import numpy as np
83     >>> from sklearn.dummy import DummyClassifier
84     >>> X = np.array([-1, 1, 1, 1])
85     >>> y = np.array([0, 1, 1, 1])
86     >>> dummy_clf = DummyClassifier(strategy="most_frequent")
87     >>> dummy_clf.fit(X, y)
88     DummyClassifier(strategy='most_frequent')
89     >>> dummy_clf.predict(X)
90     array([1, 1, 1, 1])
91     >>> dummy_clf.score(X, y)
92     0.75
93     """
94 
95     def __init__(self, strategy="stratified", random_state=None,
96                  constant=None):
97         self.strategy = strategy
98         self.random_state = random_state
99         self.constant = constant
100 
101     def fit(self, X, y, sample_weight=None):
102         """Fit the random classifier.
103 
104         Parameters
105         ----------
106         X : {array-like, object with finite length or shape}
107             Training data, requires length = n_samples
108 
109         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
110             Target values.
111 
112         sample_weight : array-like of shape (n_samples,), default=None
113             Sample weights.
114 
115         Returns
116         -------
117         self : object
118         """
119         allowed_strategies = ("most_frequent", "stratified", "uniform",
120                               "constant", "prior")
121         if self.strategy not in allowed_strategies:
122             raise ValueError("Unknown strategy type: %s, expected one of %s."
123                              % (self.strategy, allowed_strategies))
124 
125         if self.strategy == "uniform" and sp.issparse(y):
126             y = y.toarray()
127             warnings.warn('A local copy of the target data has been converted '
128                           'to a numpy array. Predicting on sparse target data '
129                           'with the uniform strategy would not save memory '
130                           'and would be slower.',
131                           UserWarning)
132 
133         self.sparse_output_ = sp.issparse(y)
134 
135         if not self.sparse_output_:
136             y = np.asarray(y)
137             y = np.atleast_1d(y)
138 
139         if y.ndim == 1:
140             y = np.reshape(y, (-1, 1))
141 
142         self.n_outputs_ = y.shape[1]
143 
144         check_consistent_length(X, y, sample_weight)
145 
146         if self.strategy == "constant":
147             if self.constant is None:
148                 raise ValueError("Constant target value has to be specified "
149                                  "when the constant strategy is used.")
150             else:
151                 constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))
152                 if constant.shape[0] != self.n_outputs_:
153                     raise ValueError("Constant target value should have "
154                                      "shape (%d, 1)." % self.n_outputs_)
155 
156         (self.classes_,
157          self.n_classes_,
158          self.class_prior_) = class_distribution(y, sample_weight)
159 
160         if self.strategy == "constant":
161             for k in range(self.n_outputs_):
162                 if not any(constant[k][0] == c for c in self.classes_[k]):
163                     # Checking in case of constant strategy if the constant
164                     # provided by the user is in y.
165                     err_msg = ("The constant target value must be present in "
166                                "the training data. You provided constant={}. "
167                                "Possible values are: {}."
168                                .format(self.constant, list(self.classes_[k])))
169                     raise ValueError(err_msg)
170 
171         if self.n_outputs_ == 1:
172             self.n_classes_ = self.n_classes_[0]
173             self.classes_ = self.classes_[0]
174             self.class_prior_ = self.class_prior_[0]
175 
176         return self
177 
178     def predict(self, X):
179         """Perform classification on test vectors X.
180 
181         Parameters
182         ----------
183         X : {array-like, object with finite length or shape}
184             Training data, requires length = n_samples
185 
186         Returns
187         -------
188         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
189             Predicted target values for X.
190         """
191         check_is_fitted(self)
192 
193         # numpy random_state expects Python int and not long as size argument
194         # under Windows
195         n_samples = _num_samples(X)
196         rs = check_random_state(self.random_state)
197 
198         n_classes_ = self.n_classes_
199         classes_ = self.classes_
200         class_prior_ = self.class_prior_
201         constant = self.constant
202         if self.n_outputs_ == 1:
203             # Get same type even for self.n_outputs_ == 1
204             n_classes_ = [n_classes_]
205             classes_ = [classes_]
206             class_prior_ = [class_prior_]
207             constant = [constant]
208         # Compute probability only once
209         if self.strategy == "stratified":
210             proba = self.predict_proba(X)
211             if self.n_outputs_ == 1:
212                 proba = [proba]
213 
214         if self.sparse_output_:
215             class_prob = None
216             if self.strategy in ("most_frequent", "prior"):
217                 classes_ = [np.array([cp.argmax()]) for cp in class_prior_]
218 
219             elif self.strategy == "stratified":
220                 class_prob = class_prior_
221 
222             elif self.strategy == "uniform":
223                 raise ValueError("Sparse target prediction is not "
224                                  "supported with the uniform strategy")
225 
226             elif self.strategy == "constant":
227                 classes_ = [np.array([c]) for c in constant]
228 
229             y = _random_choice_csc(n_samples, classes_, class_prob,
230                                   self.random_state)
231         else:
232             if self.strategy in ("most_frequent", "prior"):
233                 y = np.tile([classes_[k][class_prior_[k].argmax()] for
234                              k in range(self.n_outputs_)], [n_samples, 1])
235 
236             elif self.strategy == "stratified":
237                 y = np.vstack([classes_[k][proba[k].argmax(axis=1)] for
238                                k in range(self.n_outputs_)]).T
239 
240             elif self.strategy == "uniform":
241                 ret = [classes_[k][rs.randint(n_classes_[k], size=n_samples)]
242                        for k in range(self.n_outputs_)]
243                 y = np.vstack(ret).T
244 
245             elif self.strategy == "constant":
246                 y = np.tile(self.constant, (n_samples, 1))
247 
248             if self.n_outputs_ == 1:
249                 y = np.ravel(y)
250 
251         return y
252 
253     def predict_proba(self, X):
254         """
255         Return probability estimates for the test vectors X.
256 
257         Parameters
258         ----------
259         X : {array-like, object with finite length or shape}
260             Training data, requires length = n_samples
261 
262         Returns
263         -------
264         P : array-like or list of array-lke of shape (n_samples, n_classes)
265             Returns the probability of the sample for each class in
266             the model, where classes are ordered arithmetically, for each
267             output.
268         """
269         check_is_fitted(self)
270 
271         # numpy random_state expects Python int and not long as size argument
272         # under Windows
273         n_samples = _num_samples(X)
274         rs = check_random_state(self.random_state)
275 
276         n_classes_ = self.n_classes_
277         classes_ = self.classes_
278         class_prior_ = self.class_prior_
279         constant = self.constant
280         if self.n_outputs_ == 1:
281             # Get same type even for self.n_outputs_ == 1
282             n_classes_ = [n_classes_]
283             classes_ = [classes_]
284             class_prior_ = [class_prior_]
285             constant = [constant]
286 
287         P = []
288         for k in range(self.n_outputs_):
289             if self.strategy == "most_frequent":
290                 ind = class_prior_[k].argmax()
291                 out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)
292                 out[:, ind] = 1.0
293             elif self.strategy == "prior":
294                 out = np.ones((n_samples, 1)) * class_prior_[k]
295 
296             elif self.strategy == "stratified":
297                 out = rs.multinomial(1, class_prior_[k], size=n_samples)
298                 out = out.astype(np.float64)
299 
300             elif self.strategy == "uniform":
301                 out = np.ones((n_samples, n_classes_[k]), dtype=np.float64)
302                 out /= n_classes_[k]
303 
304             elif self.strategy == "constant":
305                 ind = np.where(classes_[k] == constant[k])
306                 out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)
307                 out[:, ind] = 1.0
308 
309             P.append(out)
310 
311         if self.n_outputs_ == 1:
312             P = P[0]
313 
314         return P
315 
316     def predict_log_proba(self, X):
317         """
318         Return log probability estimates for the test vectors X.
319 
320         Parameters
321         ----------
322         X : {array-like, object with finite length or shape}
323             Training data, requires length = n_samples
324 
325         Returns
326         -------
327         P : array-like or list of array-like of shape (n_samples, n_classes)
328             Returns the log probability of the sample for each class in
329             the model, where classes are ordered arithmetically for each
330             output.
331         """
332         proba = self.predict_proba(X)
333         if self.n_outputs_ == 1:
334             return np.log(proba)
335         else:
336             return [np.log(p) for p in proba]
337 
338     def _more_tags(self):
339         return {'poor_score': True, 'no_validation': True}
340 
341     def score(self, X, y, sample_weight=None):
342         """Returns the mean accuracy on the given test data and labels.
343 
344         In multi-label classification, this is the subset accuracy
345         which is a harsh metric since you require for each sample that
346         each label set be correctly predicted.
347 
348         Parameters
349         ----------
350         X : {array-like, None}
351             Test samples with shape = (n_samples, n_features) or
352             None. Passing None as test samples gives the same result
353             as passing real test samples, since DummyClassifier
354             operates independently of the sampled observations.
355 
356         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
357             True labels for X.
358 
359         sample_weight : array-like of shape (n_samples,), default=None
360             Sample weights.
361 
362         Returns
363         -------
364         score : float
365             Mean accuracy of self.predict(X) wrt. y.
366 
367         """
368         if X is None:
369             X = np.zeros(shape=(len(y), 1))
370         return super().score(X, y, sample_weight)
371 
372     @deprecated(
373         "The outputs_2d_ attribute is deprecated in version 0.22 "
374         "and will be removed in version 0.24. It is equivalent to "
375         "n_outputs_ > 1."
376     )
377     @property
378     def outputs_2d_(self):
379         return self.n_outputs_ != 1

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/dummy.py</file>
<class>DummyRegressor</class>
<method>fit</method>
<code>
442     def fit(self, X, y, sample_weight=None):
443         """Fit the random regressor.
444 
445         Parameters
446         ----------
447         X : {array-like, object with finite length or shape}
448             Training data, requires length = n_samples
449 
450         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
451             Target values.
452 
453         sample_weight : array-like of shape (n_samples,), default=None
454             Sample weights.
455 
456         Returns
457         -------
458         self : object
459         """
460         allowed_strategies = ("mean", "median", "quantile", "constant")
461         if self.strategy not in allowed_strategies:
462             raise ValueError("Unknown strategy type: %s, expected one of %s."
463                              % (self.strategy, allowed_strategies))
464 
465         y = check_array(y, ensure_2d=False)
466         if len(y) == 0:
467             raise ValueError("y must not be empty.")
468 
469         if y.ndim == 1:
470             y = np.reshape(y, (-1, 1))
471         self.n_outputs_ = y.shape[1]
472 
473         check_consistent_length(X, y, sample_weight)
474         if sample_weight is not None:
475             sample_weight = np.asarray(sample_weight)
476 
477         if self.strategy == "mean":
478             self.constant_ = np.average(y, axis=0, weights=sample_weight)
479 
480         elif self.strategy == "median":
481             if sample_weight is None:
482                 self.constant_ = np.median(y, axis=0)
483             else:
484                 self.constant_ = [_weighted_percentile(y[:, k], sample_weight,
485                                                        percentile=50.)
486                                   for k in range(self.n_outputs_)]
487 
488         elif self.strategy == "quantile":
489             if self.quantile is None or not np.isscalar(self.quantile):
490                 raise ValueError("Quantile must be a scalar in the range "
491                                  "[0.0, 1.0], but got %s." % self.quantile)
492 
493             percentile = self.quantile * 100.0
494             if sample_weight is None:
495                 self.constant_ = np.percentile(y, axis=0, q=percentile)
496             else:
497                 self.constant_ = [_weighted_percentile(y[:, k], sample_weight,
498                                                        percentile=percentile)
499                                   for k in range(self.n_outputs_)]
500 
501         elif self.strategy == "constant":
502             if self.constant is None:
503                 raise TypeError("Constant target value has to be specified "
504                                 "when the constant strategy is used.")
505 
506             self.constant = check_array(self.constant,
507                                         accept_sparse=['csr', 'csc', 'coo'],
508                                         ensure_2d=False, ensure_min_samples=0)
509 
510             if self.n_outputs_ != 1 and self.constant.shape[0] != y.shape[1]:
511                 raise ValueError(
512                     "Constant target value should have "
513                     "shape (%d, 1)." % y.shape[1])
514 
515             self.constant_ = self.constant
516 
517         self.constant_ = np.reshape(self.constant_, (1, -1))
518         return self

</code><intended_behavior>Similar to DummyClassifier, DummyRegressor's fit method should also adopt _check_sample_weight to ensure standardized handling of sample_weight.</intended_behavior>, <file>sklearn/dummy.py</file>
<class>DummyRegressor</class>
<method>None</method>
<code>
382 class DummyRegressor(MultiOutputMixin, RegressorMixin, BaseEstimator):
383     """
384     DummyRegressor is a regressor that makes predictions using
385     simple rules.
386 
387     This regressor is useful as a simple baseline to compare with other
388     (real) regressors. Do not use it for real problems.
389 
390     Read more in the :ref:`User Guide <dummy_estimators>`.
391 
392     Parameters
393     ----------
394     strategy : str
395         Strategy to use to generate predictions.
396 
397         * "mean": always predicts the mean of the training set
398         * "median": always predicts the median of the training set
399         * "quantile": always predicts a specified quantile of the training set,
400           provided with the quantile parameter.
401         * "constant": always predicts a constant value that is provided by
402           the user.
403 
404     constant : int or float or array-like of shape (n_outputs,)
405         The explicit constant as predicted by the "constant" strategy. This
406         parameter is useful only for the "constant" strategy.
407 
408     quantile : float in [0.0, 1.0]
409         The quantile to predict using the "quantile" strategy. A quantile of
410         0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the
411         maximum.
412 
413     Attributes
414     ----------
415     constant_ : array, shape (1, n_outputs)
416         Mean or median or quantile of the training targets or constant value
417         given by the user.
418 
419     n_outputs_ : int,
420         Number of outputs.
421 
422     Examples
423     --------
424     >>> import numpy as np
425     >>> from sklearn.dummy import DummyRegressor
426     >>> X = np.array([1.0, 2.0, 3.0, 4.0])
427     >>> y = np.array([2.0, 3.0, 5.0, 10.0])
428     >>> dummy_regr = DummyRegressor(strategy="mean")
429     >>> dummy_regr.fit(X, y)
430     DummyRegressor()
431     >>> dummy_regr.predict(X)
432     array([5., 5., 5., 5.])
433     >>> dummy_regr.score(X, y)
434     0.0
435     """
436 
437     def __init__(self, strategy="mean", constant=None, quantile=None):
438         self.strategy = strategy
439         self.constant = constant
440         self.quantile = quantile
441 
442     def fit(self, X, y, sample_weight=None):
443         """Fit the random regressor.
444 
445         Parameters
446         ----------
447         X : {array-like, object with finite length or shape}
448             Training data, requires length = n_samples
449 
450         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
451             Target values.
452 
453         sample_weight : array-like of shape (n_samples,), default=None
454             Sample weights.
455 
456         Returns
457         -------
458         self : object
459         """
460         allowed_strategies = ("mean", "median", "quantile", "constant")
461         if self.strategy not in allowed_strategies:
462             raise ValueError("Unknown strategy type: %s, expected one of %s."
463                              % (self.strategy, allowed_strategies))
464 
465         y = check_array(y, ensure_2d=False)
466         if len(y) == 0:
467             raise ValueError("y must not be empty.")
468 
469         if y.ndim == 1:
470             y = np.reshape(y, (-1, 1))
471         self.n_outputs_ = y.shape[1]
472 
473         check_consistent_length(X, y, sample_weight)
474         if sample_weight is not None:
475             sample_weight = np.asarray(sample_weight)
476 
477         if self.strategy == "mean":
478             self.constant_ = np.average(y, axis=0, weights=sample_weight)
479 
480         elif self.strategy == "median":
481             if sample_weight is None:
482                 self.constant_ = np.median(y, axis=0)
483             else:
484                 self.constant_ = [_weighted_percentile(y[:, k], sample_weight,
485                                                        percentile=50.)
486                                   for k in range(self.n_outputs_)]
487 
488         elif self.strategy == "quantile":
489             if self.quantile is None or not np.isscalar(self.quantile):
490                 raise ValueError("Quantile must be a scalar in the range "
491                                  "[0.0, 1.0], but got %s." % self.quantile)
492 
493             percentile = self.quantile * 100.0
494             if sample_weight is None:
495                 self.constant_ = np.percentile(y, axis=0, q=percentile)
496             else:
497                 self.constant_ = [_weighted_percentile(y[:, k], sample_weight,
498                                                        percentile=percentile)
499                                   for k in range(self.n_outputs_)]
500 
501         elif self.strategy == "constant":
502             if self.constant is None:
503                 raise TypeError("Constant target value has to be specified "
504                                 "when the constant strategy is used.")
505 
506             self.constant = check_array(self.constant,
507                                         accept_sparse=['csr', 'csc', 'coo'],
508                                         ensure_2d=False, ensure_min_samples=0)
509 
510             if self.n_outputs_ != 1 and self.constant.shape[0] != y.shape[1]:
511                 raise ValueError(
512                     "Constant target value should have "
513                     "shape (%d, 1)." % y.shape[1])
514 
515             self.constant_ = self.constant
516 
517         self.constant_ = np.reshape(self.constant_, (1, -1))
518         return self
519 
520     def predict(self, X, return_std=False):
521         """
522         Perform classification on test vectors X.
523 
524         Parameters
525         ----------
526         X : {array-like, object with finite length or shape}
527             Training data, requires length = n_samples
528 
529         return_std : boolean, optional
530             Whether to return the standard deviation of posterior prediction.
531             All zeros in this case.
532 
533         Returns
534         -------
535         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
536             Predicted target values for X.
537 
538         y_std : array-like of shape (n_samples,) or (n_samples, n_outputs)
539             Standard deviation of predictive distribution of query points.
540         """
541         check_is_fitted(self)
542         n_samples = _num_samples(X)
543 
544         y = np.full((n_samples, self.n_outputs_), self.constant_,
545                     dtype=np.array(self.constant_).dtype)
546         y_std = np.zeros((n_samples, self.n_outputs_))
547 
548         if self.n_outputs_ == 1:
549             y = np.ravel(y)
550             y_std = np.ravel(y_std)
551 
552         return (y, y_std) if return_std else y
553 
554     def _more_tags(self):
555         return {'poor_score': True, 'no_validation': True}
556 
557     def score(self, X, y, sample_weight=None):
558         """Returns the coefficient of determination R^2 of the prediction.
559 
560         The coefficient R^2 is defined as (1 - u/v), where u is the residual
561         sum of squares ((y_true - y_pred) ** 2).sum() and v is the total
562         sum of squares ((y_true - y_true.mean()) ** 2).sum().
563         The best possible score is 1.0 and it can be negative (because the
564         model can be arbitrarily worse). A constant model that always
565         predicts the expected value of y, disregarding the input features,
566         would get a R^2 score of 0.0.
567 
568         Parameters
569         ----------
570         X : {array-like, None}
571             Test samples with shape = (n_samples, n_features) or None.
572             For some estimators this may be a
573             precomputed kernel matrix instead, shape = (n_samples,
574             n_samples_fitted], where n_samples_fitted is the number of
575             samples used in the fitting for the estimator.
576             Passing None as test samples gives the same result
577             as passing real test samples, since DummyRegressor
578             operates independently of the sampled observations.
579 
580         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
581             True values for X.
582 
583         sample_weight : array-like of shape (n_samples,), default=None
584             Sample weights.
585 
586         Returns
587         -------
588         score : float
589             R^2 of self.predict(X) wrt. y.
590         """
591         if X is None:
592             X = np.zeros(shape=(len(y), 1))
593         return super().score(X, y, sample_weight)
594 
595     @deprecated(
596         "The outputs_2d_ attribute is deprecated in version 0.22 "
597         "and will be removed in version 0.24. It is equivalent to "
598         "n_outputs_ > 1."
599     )
600     @property
601     def outputs_2d_(self):
602         return self.n_outputs_ != 1

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/ensemble/_bagging.py</file>
<class>BaseBagging</class>
<method>fit</method>
<code>
219     def fit(self, X, y, sample_weight=None):
220         """Build a Bagging ensemble of estimators from the training
221            set (X, y).
222 
223         Parameters
224         ----------
225         X : {array-like, sparse matrix} of shape (n_samples, n_features)
226             The training input samples. Sparse matrices are accepted only if
227             they are supported by the base estimator.
228 
229         y : array-like of shape (n_samples,)
230             The target values (class labels in classification, real numbers in
231             regression).
232 
233         sample_weight : array-like of shape (n_samples,), default=None
234             Sample weights. If None, then samples are equally weighted.
235             Note that this is supported only if the base estimator supports
236             sample weighting.
237 
238         Returns
239         -------
240         self : object
241         """
242         return self._fit(X, y, self.max_samples, sample_weight=sample_weight)

</code><intended_behavior>Adapt the fit method to employ _check_sample_weight for validating and processing sample_weight. This will maintain the feature's existing functionality while ensuring that weight handling adheres to the standardized approach used across the library.</intended_behavior>, <file>sklearn/ensemble/_bagging.py</file>
<class>BaseBagging</class>
<method>None</method>
<code>
185 class BaseBagging(BaseEnsemble, metaclass=ABCMeta):
186     """Base class for Bagging meta-estimator.
187 
188     Warning: This class should not be used directly. Use derived classes
189     instead.
190     """
191 
192     @abstractmethod
193     def __init__(self,
194                  base_estimator=None,
195                  n_estimators=10,
196                  max_samples=1.0,
197                  max_features=1.0,
198                  bootstrap=True,
199                  bootstrap_features=False,
200                  oob_score=False,
201                  warm_start=False,
202                  n_jobs=None,
203                  random_state=None,
204                  verbose=0):
205         super().__init__(
206             base_estimator=base_estimator,
207             n_estimators=n_estimators)
208 
209         self.max_samples = max_samples
210         self.max_features = max_features
211         self.bootstrap = bootstrap
212         self.bootstrap_features = bootstrap_features
213         self.oob_score = oob_score
214         self.warm_start = warm_start
215         self.n_jobs = n_jobs
216         self.random_state = random_state
217         self.verbose = verbose
218 
219     def fit(self, X, y, sample_weight=None):
220         """Build a Bagging ensemble of estimators from the training
221            set (X, y).
222 
223         Parameters
224         ----------
225         X : {array-like, sparse matrix} of shape (n_samples, n_features)
226             The training input samples. Sparse matrices are accepted only if
227             they are supported by the base estimator.
228 
229         y : array-like of shape (n_samples,)
230             The target values (class labels in classification, real numbers in
231             regression).
232 
233         sample_weight : array-like of shape (n_samples,), default=None
234             Sample weights. If None, then samples are equally weighted.
235             Note that this is supported only if the base estimator supports
236             sample weighting.
237 
238         Returns
239         -------
240         self : object
241         """
242         return self._fit(X, y, self.max_samples, sample_weight=sample_weight)
243 
244     def _parallel_args(self):
245         return {}
246 
247     def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):
248         """Build a Bagging ensemble of estimators from the training
249            set (X, y).
250 
251         Parameters
252         ----------
253         X : {array-like, sparse matrix} of shape (n_samples, n_features)
254             The training input samples. Sparse matrices are accepted only if
255             they are supported by the base estimator.
256 
257         y : array-like of shape (n_samples,)
258             The target values (class labels in classification, real numbers in
259             regression).
260 
261         max_samples : int or float, optional (default=None)
262             Argument to use instead of self.max_samples.
263 
264         max_depth : int, optional (default=None)
265             Override value used when constructing base estimator. Only
266             supported if the base estimator has a max_depth parameter.
267 
268         sample_weight : array-like of shape (n_samples,), default=None
269             Sample weights. If None, then samples are equally weighted.
270             Note that this is supported only if the base estimator supports
271             sample weighting.
272 
273         Returns
274         -------
275         self : object
276         """
277         random_state = check_random_state(self.random_state)
278 
279         # Convert data (X is required to be 2d and indexable)
280         X, y = check_X_y(
281             X, y, ['csr', 'csc'], dtype=None, force_all_finite=False,
282             multi_output=True
283         )
284         if sample_weight is not None:
285             sample_weight = check_array(sample_weight, ensure_2d=False)
286             check_consistent_length(y, sample_weight)
287 
288         # Remap output
289         n_samples, self.n_features_ = X.shape
290         self._n_samples = n_samples
291         y = self._validate_y(y)
292 
293         # Check parameters
294         self._validate_estimator()
295 
296         if max_depth is not None:
297             self.base_estimator_.max_depth = max_depth
298 
299         # Validate max_samples
300         if max_samples is None:
301             max_samples = self.max_samples
302         elif not isinstance(max_samples, numbers.Integral):
303             max_samples = int(max_samples * X.shape[0])
304 
305         if not (0 < max_samples <= X.shape[0]):
306             raise ValueError("max_samples must be in (0, n_samples]")
307 
308         # Store validated integer row sampling value
309         self._max_samples = max_samples
310 
311         # Validate max_features
312         if isinstance(self.max_features, numbers.Integral):
313             max_features = self.max_features
314         elif isinstance(self.max_features, np.float):
315             max_features = self.max_features * self.n_features_
316         else:
317             raise ValueError("max_features must be int or float")
318 
319         if not (0 < max_features <= self.n_features_):
320             raise ValueError("max_features must be in (0, n_features]")
321 
322         max_features = max(1, int(max_features))
323 
324         # Store validated integer feature sampling value
325         self._max_features = max_features
326 
327         # Other checks
328         if not self.bootstrap and self.oob_score:
329             raise ValueError("Out of bag estimation only available"
330                              " if bootstrap=True")
331 
332         if self.warm_start and self.oob_score:
333             raise ValueError("Out of bag estimate only available"
334                              " if warm_start=False")
335 
336         if hasattr(self, "oob_score_") and self.warm_start:
337             del self.oob_score_
338 
339         if not self.warm_start or not hasattr(self, 'estimators_'):
340             # Free allocated memory, if any
341             self.estimators_ = []
342             self.estimators_features_ = []
343 
344         n_more_estimators = self.n_estimators - len(self.estimators_)
345 
346         if n_more_estimators < 0:
347             raise ValueError('n_estimators=%d must be larger or equal to '
348                              'len(estimators_)=%d when warm_start==True'
349                              % (self.n_estimators, len(self.estimators_)))
350 
351         elif n_more_estimators == 0:
352             warn("Warm-start fitting without increasing n_estimators does not "
353                  "fit new trees.")
354             return self
355 
356         # Parallel loop
357         n_jobs, n_estimators, starts = _partition_estimators(n_more_estimators,
358                                                              self.n_jobs)
359         total_n_estimators = sum(n_estimators)
360 
361         # Advance random state to state after training
362         # the first n_estimators
363         if self.warm_start and len(self.estimators_) > 0:
364             random_state.randint(MAX_INT, size=len(self.estimators_))
365 
366         seeds = random_state.randint(MAX_INT, size=n_more_estimators)
367         self._seeds = seeds
368 
369         all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose,
370                                **self._parallel_args())(
371             delayed(_parallel_build_estimators)(
372                 n_estimators[i],
373                 self,
374                 X,
375                 y,
376                 sample_weight,
377                 seeds[starts[i]:starts[i + 1]],
378                 total_n_estimators,
379                 verbose=self.verbose)
380             for i in range(n_jobs))
381 
382         # Reduce
383         self.estimators_ += list(itertools.chain.from_iterable(
384             t[0] for t in all_results))
385         self.estimators_features_ += list(itertools.chain.from_iterable(
386             t[1] for t in all_results))
387 
388         if self.oob_score:
389             self._set_oob_score(X, y)
390 
391         return self
392 
393     @abstractmethod
394     def _set_oob_score(self, X, y):
395         """Calculate out of bag predictions and score."""
396 
397     def _validate_y(self, y):
398         if len(y.shape) == 1 or y.shape[1] == 1:
399             return column_or_1d(y, warn=True)
400         else:
401             return y
402 
403     def _get_estimators_indices(self):
404         # Get drawn indices along both sample and feature axes
405         for seed in self._seeds:
406             # Operations accessing random_state must be performed identically
407             # to those in `_parallel_build_estimators()`
408             random_state = np.random.RandomState(seed)
409             feature_indices, sample_indices = _generate_bagging_indices(
410                 random_state, self.bootstrap_features, self.bootstrap,
411                 self.n_features_, self._n_samples, self._max_features,
412                 self._max_samples)
413 
414             yield feature_indices, sample_indices
415 
416     @property
417     def estimators_samples_(self):
418         """
419         The subset of drawn samples for each base estimator.
420 
421         Returns a dynamically generated list of indices identifying
422         the samples used for fitting each member of the ensemble, i.e.,
423         the in-bag samples.
424 
425         Note: the list is re-created at each call to the property in order
426         to reduce the object memory footprint by not storing the sampling
427         data. Thus fetching the property may be slower than expected.
428         """
429         return [sample_indices
430                 for _, sample_indices in self._get_estimators_indices()]

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/ensemble/_forest.py</file>
<class>BaseForest</class>
<method>fit</method>
<code>
269     def fit(self, X, y, sample_weight=None):
270         """
271         Build a forest of trees from the training set (X, y).
272 
273         Parameters
274         ----------
275         X : array-like or sparse matrix of shape (n_samples, n_features)
276             The training input samples. Internally, its dtype will be converted
277             to ``dtype=np.float32``. If a sparse matrix is provided, it will be
278             converted into a sparse ``csc_matrix``.
279 
280         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
281             The target values (class labels in classification, real numbers in
282             regression).
283 
284         sample_weight : array-like of shape (n_samples,), default=None
285             Sample weights. If None, then samples are equally weighted. Splits
286             that would create child nodes with net zero or negative weight are
287             ignored while searching for a split in each node. In the case of
288             classification, splits are also ignored if they would result in any
289             single class carrying a negative weight in either child node.
290 
291         Returns
292         -------
293         self : object
294         """
295         # Validate or convert input data
296         X = check_array(X, accept_sparse="csc", dtype=DTYPE)
297         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
298         if sample_weight is not None:
299             sample_weight = check_array(sample_weight, ensure_2d=False)
300         if issparse(X):
301             # Pre-sort indices to avoid that each individual tree of the
302             # ensemble sorts the indices.
303             X.sort_indices()
304 
305         # Remap output
306         self.n_features_ = X.shape[1]
307 
308         y = np.atleast_1d(y)
309         if y.ndim == 2 and y.shape[1] == 1:
310             warn("A column-vector y was passed when a 1d array was"
311                  " expected. Please change the shape of y to "
312                  "(n_samples,), for example using ravel().",
313                  DataConversionWarning, stacklevel=2)
314 
315         if y.ndim == 1:
316             # reshape is necessary to preserve the data contiguity against vs
317             # [:, np.newaxis] that does not.
318             y = np.reshape(y, (-1, 1))
319 
320         self.n_outputs_ = y.shape[1]
321 
322         y, expanded_class_weight = self._validate_y_class_weight(y)
323 
324         if getattr(y, "dtype", None) != DOUBLE or not y.flags.contiguous:
325             y = np.ascontiguousarray(y, dtype=DOUBLE)
326 
327         if expanded_class_weight is not None:
328             if sample_weight is not None:
329                 sample_weight = sample_weight * expanded_class_weight
330             else:
331                 sample_weight = expanded_class_weight
332 
333         # Get bootstrap sample size
334         n_samples_bootstrap = _get_n_samples_bootstrap(
335             n_samples=X.shape[0],
336             max_samples=self.max_samples
337         )
338 
339         # Check parameters
340         self._validate_estimator()
341 
342         if not self.bootstrap and self.oob_score:
343             raise ValueError("Out of bag estimation only available"
344                              " if bootstrap=True")
345 
346         random_state = check_random_state(self.random_state)
347 
348         if not self.warm_start or not hasattr(self, "estimators_"):
349             # Free allocated memory, if any
350             self.estimators_ = []
351 
352         n_more_estimators = self.n_estimators - len(self.estimators_)
353 
354         if n_more_estimators < 0:
355             raise ValueError('n_estimators=%d must be larger or equal to '
356                              'len(estimators_)=%d when warm_start==True'
357                              % (self.n_estimators, len(self.estimators_)))
358 
359         elif n_more_estimators == 0:
360             warn("Warm-start fitting without increasing n_estimators does not "
361                  "fit new trees.")
362         else:
363             if self.warm_start and len(self.estimators_) > 0:
364                 # We draw from the random state to get the random state we
365                 # would have got if we hadn't used a warm_start.
366                 random_state.randint(MAX_INT, size=len(self.estimators_))
367 
368             trees = [self._make_estimator(append=False,
369                                           random_state=random_state)
370                      for i in range(n_more_estimators)]
371 
372             # Parallel loop: we prefer the threading backend as the Cython code
373             # for fitting the trees is internally releasing the Python GIL
374             # making threading more efficient than multiprocessing in
375             # that case. However, for joblib 0.12+ we respect any
376             # parallel_backend contexts set at a higher level,
377             # since correctness does not rely on using threads.
378             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
379                              **_joblib_parallel_args(prefer='threads'))(
380                 delayed(_parallel_build_trees)(
381                     t, self, X, y, sample_weight, i, len(trees),
382                     verbose=self.verbose, class_weight=self.class_weight,
383                     n_samples_bootstrap=n_samples_bootstrap)
384                 for i, t in enumerate(trees))
385 
386             # Collect newly grown trees
387             self.estimators_.extend(trees)
388 
389         if self.oob_score:
390             self._set_oob_score(X, y)
391 
392         # Decapsulate classes_ attributes
393         if hasattr(self, "classes_") and self.n_outputs_ == 1:
394             self.n_classes_ = self.n_classes_[0]
395             self.classes_ = self.classes_[0]
396 
397         return self

</code><intended_behavior>Ensure that fit method uses _check_sample_weight for validating and processing sample_weight in line with sklearn's standardized practices, ensuring consistency, and possibly enhancing error checking and input validation.</intended_behavior>, <file>sklearn/ensemble/_forest.py</file>
<class>BaseForest</class>
<method>None</method>
<code>
172 class BaseForest(MultiOutputMixin, BaseEnsemble, metaclass=ABCMeta):
173     """
174     Base class for forests of trees.
175 
176     Warning: This class should not be used directly. Use derived classes
177     instead.
178     """
179 
180     @abstractmethod
181     def __init__(self,
182                  base_estimator,
183                  n_estimators=100,
184                  estimator_params=tuple(),
185                  bootstrap=False,
186                  oob_score=False,
187                  n_jobs=None,
188                  random_state=None,
189                  verbose=0,
190                  warm_start=False,
191                  class_weight=None,
192                  max_samples=None):
193         super().__init__(
194             base_estimator=base_estimator,
195             n_estimators=n_estimators,
196             estimator_params=estimator_params)
197 
198         self.bootstrap = bootstrap
199         self.oob_score = oob_score
200         self.n_jobs = n_jobs
201         self.random_state = random_state
202         self.verbose = verbose
203         self.warm_start = warm_start
204         self.class_weight = class_weight
205         self.max_samples = max_samples
206 
207     def apply(self, X):
208         """
209         Apply trees in the forest to X, return leaf indices.
210 
211         Parameters
212         ----------
213         X : {array-like or sparse matrix} of shape (n_samples, n_features)
214             The input samples. Internally, its dtype will be converted to
215             ``dtype=np.float32``. If a sparse matrix is provided, it will be
216             converted into a sparse ``csr_matrix``.
217 
218         Returns
219         -------
220         X_leaves : array_like, shape = [n_samples, n_estimators]
221             For each datapoint x in X and for each tree in the forest,
222             return the index of the leaf x ends up in.
223         """
224         X = self._validate_X_predict(X)
225         results = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
226                            **_joblib_parallel_args(prefer="threads"))(
227             delayed(tree.apply)(X, check_input=False)
228             for tree in self.estimators_)
229 
230         return np.array(results).T
231 
232     def decision_path(self, X):
233         """
234         Return the decision path in the forest.
235 
236         .. versionadded:: 0.18
237 
238         Parameters
239         ----------
240         X : {array-like or sparse matrix} of shape (n_samples, n_features)
241             The input samples. Internally, its dtype will be converted to
242             ``dtype=np.float32``. If a sparse matrix is provided, it will be
243             converted into a sparse ``csr_matrix``.
244 
245         Returns
246         -------
247         indicator : sparse csr array, shape = [n_samples, n_nodes]
248             Return a node indicator matrix where non zero elements
249             indicates that the samples goes through the nodes.
250 
251         n_nodes_ptr : array of size (n_estimators + 1, )
252             The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]
253             gives the indicator value for the i-th estimator.
254 
255         """
256         X = self._validate_X_predict(X)
257         indicators = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
258                               **_joblib_parallel_args(prefer='threads'))(
259             delayed(tree.decision_path)(X,
260                                      check_input=False)
261             for tree in self.estimators_)
262 
263         n_nodes = [0]
264         n_nodes.extend([i.shape[1] for i in indicators])
265         n_nodes_ptr = np.array(n_nodes).cumsum()
266 
267         return sparse_hstack(indicators).tocsr(), n_nodes_ptr
268 
269     def fit(self, X, y, sample_weight=None):
270         """
271         Build a forest of trees from the training set (X, y).
272 
273         Parameters
274         ----------
275         X : array-like or sparse matrix of shape (n_samples, n_features)
276             The training input samples. Internally, its dtype will be converted
277             to ``dtype=np.float32``. If a sparse matrix is provided, it will be
278             converted into a sparse ``csc_matrix``.
279 
280         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
281             The target values (class labels in classification, real numbers in
282             regression).
283 
284         sample_weight : array-like of shape (n_samples,), default=None
285             Sample weights. If None, then samples are equally weighted. Splits
286             that would create child nodes with net zero or negative weight are
287             ignored while searching for a split in each node. In the case of
288             classification, splits are also ignored if they would result in any
289             single class carrying a negative weight in either child node.
290 
291         Returns
292         -------
293         self : object
294         """
295         # Validate or convert input data
296         X = check_array(X, accept_sparse="csc", dtype=DTYPE)
297         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
298         if sample_weight is not None:
299             sample_weight = check_array(sample_weight, ensure_2d=False)
300         if issparse(X):
301             # Pre-sort indices to avoid that each individual tree of the
302             # ensemble sorts the indices.
303             X.sort_indices()
304 
305         # Remap output
306         self.n_features_ = X.shape[1]
307 
308         y = np.atleast_1d(y)
309         if y.ndim == 2 and y.shape[1] == 1:
310             warn("A column-vector y was passed when a 1d array was"
311                  " expected. Please change the shape of y to "
312                  "(n_samples,), for example using ravel().",
313                  DataConversionWarning, stacklevel=2)
314 
315         if y.ndim == 1:
316             # reshape is necessary to preserve the data contiguity against vs
317             # [:, np.newaxis] that does not.
318             y = np.reshape(y, (-1, 1))
319 
320         self.n_outputs_ = y.shape[1]
321 
322         y, expanded_class_weight = self._validate_y_class_weight(y)
323 
324         if getattr(y, "dtype", None) != DOUBLE or not y.flags.contiguous:
325             y = np.ascontiguousarray(y, dtype=DOUBLE)
326 
327         if expanded_class_weight is not None:
328             if sample_weight is not None:
329                 sample_weight = sample_weight * expanded_class_weight
330             else:
331                 sample_weight = expanded_class_weight
332 
333         # Get bootstrap sample size
334         n_samples_bootstrap = _get_n_samples_bootstrap(
335             n_samples=X.shape[0],
336             max_samples=self.max_samples
337         )
338 
339         # Check parameters
340         self._validate_estimator()
341 
342         if not self.bootstrap and self.oob_score:
343             raise ValueError("Out of bag estimation only available"
344                              " if bootstrap=True")
345 
346         random_state = check_random_state(self.random_state)
347 
348         if not self.warm_start or not hasattr(self, "estimators_"):
349             # Free allocated memory, if any
350             self.estimators_ = []
351 
352         n_more_estimators = self.n_estimators - len(self.estimators_)
353 
354         if n_more_estimators < 0:
355             raise ValueError('n_estimators=%d must be larger or equal to '
356                              'len(estimators_)=%d when warm_start==True'
357                              % (self.n_estimators, len(self.estimators_)))
358 
359         elif n_more_estimators == 0:
360             warn("Warm-start fitting without increasing n_estimators does not "
361                  "fit new trees.")
362         else:
363             if self.warm_start and len(self.estimators_) > 0:
364                 # We draw from the random state to get the random state we
365                 # would have got if we hadn't used a warm_start.
366                 random_state.randint(MAX_INT, size=len(self.estimators_))
367 
368             trees = [self._make_estimator(append=False,
369                                           random_state=random_state)
370                      for i in range(n_more_estimators)]
371 
372             # Parallel loop: we prefer the threading backend as the Cython code
373             # for fitting the trees is internally releasing the Python GIL
374             # making threading more efficient than multiprocessing in
375             # that case. However, for joblib 0.12+ we respect any
376             # parallel_backend contexts set at a higher level,
377             # since correctness does not rely on using threads.
378             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
379                              **_joblib_parallel_args(prefer='threads'))(
380                 delayed(_parallel_build_trees)(
381                     t, self, X, y, sample_weight, i, len(trees),
382                     verbose=self.verbose, class_weight=self.class_weight,
383                     n_samples_bootstrap=n_samples_bootstrap)
384                 for i, t in enumerate(trees))
385 
386             # Collect newly grown trees
387             self.estimators_.extend(trees)
388 
389         if self.oob_score:
390             self._set_oob_score(X, y)
391 
392         # Decapsulate classes_ attributes
393         if hasattr(self, "classes_") and self.n_outputs_ == 1:
394             self.n_classes_ = self.n_classes_[0]
395             self.classes_ = self.classes_[0]
396 
397         return self
398 
399     @abstractmethod
400     def _set_oob_score(self, X, y):
401         """
402         Calculate out of bag predictions and score."""
403 
404     def _validate_y_class_weight(self, y):
405         # Default implementation
406         return y, None
407 
408     def _validate_X_predict(self, X):
409         """
410         Validate X whenever one tries to predict, apply, predict_proba."""
411         check_is_fitted(self)
412 
413         return self.estimators_[0]._validate_X_predict(X, check_input=True)
414 
415     @property
416     def feature_importances_(self):
417         """
418         Return the feature importances (the higher, the more important the
419            feature).
420 
421         Returns
422         -------
423         feature_importances_ : array, shape = [n_features]
424             The values of this array sum to 1, unless all trees are single node
425             trees consisting of only the root node, in which case it will be an
426             array of zeros.
427         """
428         check_is_fitted(self)
429 
430         all_importances = Parallel(n_jobs=self.n_jobs,
431                                    **_joblib_parallel_args(prefer='threads'))(
432             delayed(getattr)(tree, 'feature_importances_')
433             for tree in self.estimators_ if tree.tree_.node_count > 1)
434 
435         if not all_importances:
436             return np.zeros(self.n_features_, dtype=np.float64)
437 
438         all_importances = np.mean(all_importances,
439                                   axis=0, dtype=np.float64)
440         return all_importances / np.sum(all_importances)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/ensemble/_gb.py</file>
<class>BaseGradientBoosting</class>
<method>fit</method>
<code>
1399     def fit(self, X, y, sample_weight=None, monitor=None):
1400         """Fit the gradient boosting model.
1401 
1402         Parameters
1403         ----------
1404         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1405             The input samples. Internally, it will be converted to
1406             ``dtype=np.float32`` and if a sparse matrix is provided
1407             to a sparse ``csr_matrix``.
1408 
1409         y : array-like, shape (n_samples,)
1410             Target values (strings or integers in classification, real numbers
1411             in regression)
1412             For classification, labels must correspond to classes.
1413 
1414         sample_weight : array-like, shape (n_samples,) or None
1415             Sample weights. If None, then samples are equally weighted. Splits
1416             that would create child nodes with net zero or negative weight are
1417             ignored while searching for a split in each node. In the case of
1418             classification, splits are also ignored if they would result in any
1419             single class carrying a negative weight in either child node.
1420 
1421         monitor : callable, optional
1422             The monitor is called after each iteration with the current
1423             iteration, a reference to the estimator and the local variables of
1424             ``_fit_stages`` as keyword arguments ``callable(i, self,
1425             locals())``. If the callable returns ``True`` the fitting procedure
1426             is stopped. The monitor can be used for various things such as
1427             computing held-out estimates, early stopping, model introspect, and
1428             snapshoting.
1429 
1430         Returns
1431         -------
1432         self : object
1433         """
1434         # if not warmstart - clear the estimator state
1435         if not self.warm_start:
1436             self._clear_state()
1437 
1438         # Check input
1439         # Since check_array converts both X and y to the same dtype, but the
1440         # trees use different types for X and y, checking them separately.
1441         X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
1442         n_samples, self.n_features_ = X.shape
1443 
1444         sample_weight_is_none = sample_weight is None
1445         if sample_weight_is_none:
1446             sample_weight = np.ones(n_samples, dtype=np.float32)
1447         else:
1448             sample_weight = column_or_1d(sample_weight, warn=True)
1449             sample_weight_is_none = False
1450 
1451         check_consistent_length(X, y, sample_weight)
1452 
1453         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
1454         y = column_or_1d(y, warn=True)
1455         y = self._validate_y(y, sample_weight)
1456 
1457         if self.n_iter_no_change is not None:
1458             stratify = y if is_classifier(self) else None
1459             X, X_val, y, y_val, sample_weight, sample_weight_val = (
1460                 train_test_split(X, y, sample_weight,
1461                                  random_state=self.random_state,
1462                                  test_size=self.validation_fraction,
1463                                  stratify=stratify))
1464             if is_classifier(self):
1465                 if self.n_classes_ != np.unique(y).shape[0]:
1466                     # We choose to error here. The problem is that the init
1467                     # estimator would be trained on y, which has some missing
1468                     # classes now, so its predictions would not have the
1469                     # correct shape.
1470                     raise ValueError(
1471                         'The training data after the early stopping split '
1472                         'is missing some classes. Try using another random '
1473                         'seed.'
1474                     )
1475         else:
1476             X_val = y_val = sample_weight_val = None
1477 
1478         self._check_params()
1479 
1480         if not self._is_initialized():
1481             # init state
1482             self._init_state()
1483 
1484             # fit initial model and initialize raw predictions
1485             if self.init_ == 'zero':
1486                 raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1487                                            dtype=np.float64)
1488             else:
1489                 # XXX clean this once we have a support_sample_weight tag
1490                 if sample_weight_is_none:
1491                     self.init_.fit(X, y)
1492                 else:
1493                     msg = ("The initial estimator {} does not support sample "
1494                            "weights.".format(self.init_.__class__.__name__))
1495                     try:
1496                         self.init_.fit(X, y, sample_weight=sample_weight)
1497                     except TypeError:  # regular estimator without SW support
1498                         raise ValueError(msg)
1499                     except ValueError as e:
1500                         if "pass parameters to specific steps of "\
1501                            "your pipeline using the "\
1502                            "stepname__parameter" in str(e):  # pipeline
1503                             raise ValueError(msg) from e
1504                         else:  # regular estimator whose input checking failed
1505                             raise
1506 
1507                 raw_predictions = \
1508                     self.loss_.get_init_raw_predictions(X, self.init_)
1509 
1510             begin_at_stage = 0
1511 
1512             # The rng state must be preserved if warm_start is True
1513             self._rng = check_random_state(self.random_state)
1514 
1515         else:
1516             # add more estimators to fitted model
1517             # invariant: warm_start = True
1518             if self.n_estimators < self.estimators_.shape[0]:
1519                 raise ValueError('n_estimators=%d must be larger or equal to '
1520                                  'estimators_.shape[0]=%d when '
1521                                  'warm_start==True'
1522                                  % (self.n_estimators,
1523                                     self.estimators_.shape[0]))
1524             begin_at_stage = self.estimators_.shape[0]
1525             # The requirements of _decision_function (called in two lines
1526             # below) are more constrained than fit. It accepts only CSR
1527             # matrices.
1528             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1529             raw_predictions = self._raw_predict(X)
1530             self._resize_state()
1531 
1532         X_idx_sorted = None
1533 
1534         # fit the boosting stages
1535         n_stages = self._fit_stages(
1536             X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,
1537             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)
1538 
1539         # change shape of arrays after fit (early-stopping or additional ests)
1540         if n_stages != self.estimators_.shape[0]:
1541             self.estimators_ = self.estimators_[:n_stages]
1542             self.train_score_ = self.train_score_[:n_stages]
1543             if hasattr(self, 'oob_improvement_'):
1544                 self.oob_improvement_ = self.oob_improvement_[:n_stages]
1545 
1546         self.n_estimators_ = n_stages
1547         return self

</code><intended_behavior>The fit function should be updated to utilize _check_sample_weight for handling sample_weight, replacing or complementing any specific logic currently in place with the standardized utility function.</intended_behavior>, <file>sklearn/ensemble/_gb.py</file>
<class>BaseGradientBoosting</class>
<method>None</method>
<code>
1166 class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
1167     """Abstract base class for Gradient Boosting. """
1168 
1169     @abstractmethod
1170     def __init__(self, loss, learning_rate, n_estimators, criterion,
1171                  min_samples_split, min_samples_leaf, min_weight_fraction_leaf,
1172                  max_depth, min_impurity_decrease, min_impurity_split,
1173                  init, subsample, max_features, ccp_alpha,
1174                  random_state, alpha=0.9, verbose=0, max_leaf_nodes=None,
1175                  warm_start=False, presort='deprecated',
1176                  validation_fraction=0.1, n_iter_no_change=None,
1177                  tol=1e-4):
1178 
1179         self.n_estimators = n_estimators
1180         self.learning_rate = learning_rate
1181         self.loss = loss
1182         self.criterion = criterion
1183         self.min_samples_split = min_samples_split
1184         self.min_samples_leaf = min_samples_leaf
1185         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1186         self.subsample = subsample
1187         self.max_features = max_features
1188         self.max_depth = max_depth
1189         self.min_impurity_decrease = min_impurity_decrease
1190         self.min_impurity_split = min_impurity_split
1191         self.ccp_alpha = ccp_alpha
1192         self.init = init
1193         self.random_state = random_state
1194         self.alpha = alpha
1195         self.verbose = verbose
1196         self.max_leaf_nodes = max_leaf_nodes
1197         self.warm_start = warm_start
1198         self.presort = presort
1199         self.validation_fraction = validation_fraction
1200         self.n_iter_no_change = n_iter_no_change
1201         self.tol = tol
1202 
1203     def _fit_stage(self, i, X, y, raw_predictions, sample_weight, sample_mask,
1204                    random_state, X_idx_sorted, X_csc=None, X_csr=None):
1205         """Fit another stage of ``n_classes_`` trees to the boosting model. """
1206 
1207         assert sample_mask.dtype == np.bool
1208         loss = self.loss_
1209         original_y = y
1210 
1211         # Need to pass a copy of raw_predictions to negative_gradient()
1212         # because raw_predictions is partially updated at the end of the loop
1213         # in update_terminal_regions(), and gradients need to be evaluated at
1214         # iteration i - 1.
1215         raw_predictions_copy = raw_predictions.copy()
1216 
1217         for k in range(loss.K):
1218             if loss.is_multi_class:
1219                 y = np.array(original_y == k, dtype=np.float64)
1220 
1221             residual = loss.negative_gradient(y, raw_predictions_copy, k=k,
1222                                               sample_weight=sample_weight)
1223 
1224             # induce regression tree on residuals
1225             tree = DecisionTreeRegressor(
1226                 criterion=self.criterion,
1227                 splitter='best',
1228                 max_depth=self.max_depth,
1229                 min_samples_split=self.min_samples_split,
1230                 min_samples_leaf=self.min_samples_leaf,
1231                 min_weight_fraction_leaf=self.min_weight_fraction_leaf,
1232                 min_impurity_decrease=self.min_impurity_decrease,
1233                 min_impurity_split=self.min_impurity_split,
1234                 max_features=self.max_features,
1235                 max_leaf_nodes=self.max_leaf_nodes,
1236                 random_state=random_state,
1237                 ccp_alpha=self.ccp_alpha)
1238 
1239             if self.subsample < 1.0:
1240                 # no inplace multiplication!
1241                 sample_weight = sample_weight * sample_mask.astype(np.float64)
1242 
1243             X = X_csr if X_csr is not None else X
1244             tree.fit(X, residual, sample_weight=sample_weight,
1245                      check_input=False, X_idx_sorted=X_idx_sorted)
1246 
1247             # update tree leaves
1248             loss.update_terminal_regions(
1249                 tree.tree_, X, y, residual, raw_predictions, sample_weight,
1250                 sample_mask, learning_rate=self.learning_rate, k=k)
1251 
1252             # add tree to ensemble
1253             self.estimators_[i, k] = tree
1254 
1255         return raw_predictions
1256 
1257     def _check_params(self):
1258         """Check validity of parameters and raise ValueError if not valid. """
1259         if self.n_estimators <= 0:
1260             raise ValueError("n_estimators must be greater than 0 but "
1261                              "was %r" % self.n_estimators)
1262 
1263         if self.learning_rate <= 0.0:
1264             raise ValueError("learning_rate must be greater than 0 but "
1265                              "was %r" % self.learning_rate)
1266 
1267         if (self.loss not in self._SUPPORTED_LOSS
1268                 or self.loss not in _gb_losses.LOSS_FUNCTIONS):
1269             raise ValueError("Loss '{0:s}' not supported. ".format(self.loss))
1270 
1271         if self.loss == 'deviance':
1272             loss_class = (_gb_losses.MultinomialDeviance
1273                           if len(self.classes_) > 2
1274                           else _gb_losses.BinomialDeviance)
1275         else:
1276             loss_class = _gb_losses.LOSS_FUNCTIONS[self.loss]
1277 
1278         if self.loss in ('huber', 'quantile'):
1279             self.loss_ = loss_class(self.n_classes_, self.alpha)
1280         else:
1281             self.loss_ = loss_class(self.n_classes_)
1282 
1283         if not (0.0 < self.subsample <= 1.0):
1284             raise ValueError("subsample must be in (0,1] but "
1285                              "was %r" % self.subsample)
1286 
1287         if self.init is not None:
1288             # init must be an estimator or 'zero'
1289             if isinstance(self.init, BaseEstimator):
1290                 self.loss_.check_init_estimator(self.init)
1291             elif not (isinstance(self.init, str) and self.init == 'zero'):
1292                 raise ValueError(
1293                     "The init parameter must be an estimator or 'zero'. "
1294                     "Got init={}".format(self.init)
1295                 )
1296 
1297         if not (0.0 < self.alpha < 1.0):
1298             raise ValueError("alpha must be in (0.0, 1.0) but "
1299                              "was %r" % self.alpha)
1300 
1301         if isinstance(self.max_features, str):
1302             if self.max_features == "auto":
1303                 # if is_classification
1304                 if self.n_classes_ > 1:
1305                     max_features = max(1, int(np.sqrt(self.n_features_)))
1306                 else:
1307                     # is regression
1308                     max_features = self.n_features_
1309             elif self.max_features == "sqrt":
1310                 max_features = max(1, int(np.sqrt(self.n_features_)))
1311             elif self.max_features == "log2":
1312                 max_features = max(1, int(np.log2(self.n_features_)))
1313             else:
1314                 raise ValueError("Invalid value for max_features: %r. "
1315                                  "Allowed string values are 'auto', 'sqrt' "
1316                                  "or 'log2'." % self.max_features)
1317         elif self.max_features is None:
1318             max_features = self.n_features_
1319         elif isinstance(self.max_features, numbers.Integral):
1320             max_features = self.max_features
1321         else:  # float
1322             if 0. < self.max_features <= 1.:
1323                 max_features = max(int(self.max_features *
1324                                        self.n_features_), 1)
1325             else:
1326                 raise ValueError("max_features must be in (0, n_features]")
1327 
1328         self.max_features_ = max_features
1329 
1330         if not isinstance(self.n_iter_no_change,
1331                           (numbers.Integral, type(None))):
1332             raise ValueError("n_iter_no_change should either be None or an "
1333                              "integer. %r was passed"
1334                              % self.n_iter_no_change)
1335 
1336         if self.presort != 'deprecated':
1337             warnings.warn("The parameter 'presort' is deprecated and has no "
1338                           "effect. It will be removed in v0.24. You can "
1339                           "suppress this warning by not passing any value "
1340                           "to the 'presort' parameter. We also recommend "
1341                           "using HistGradientBoosting models instead.",
1342                           FutureWarning)
1343 
1344     def _init_state(self):
1345         """Initialize model state and allocate model state data structures. """
1346 
1347         self.init_ = self.init
1348         if self.init_ is None:
1349             self.init_ = self.loss_.init_estimator()
1350 
1351         self.estimators_ = np.empty((self.n_estimators, self.loss_.K),
1352                                     dtype=np.object)
1353         self.train_score_ = np.zeros((self.n_estimators,), dtype=np.float64)
1354         # do oob?
1355         if self.subsample < 1.0:
1356             self.oob_improvement_ = np.zeros((self.n_estimators),
1357                                              dtype=np.float64)
1358 
1359     def _clear_state(self):
1360         """Clear the state of the gradient boosting model. """
1361         if hasattr(self, 'estimators_'):
1362             self.estimators_ = np.empty((0, 0), dtype=np.object)
1363         if hasattr(self, 'train_score_'):
1364             del self.train_score_
1365         if hasattr(self, 'oob_improvement_'):
1366             del self.oob_improvement_
1367         if hasattr(self, 'init_'):
1368             del self.init_
1369         if hasattr(self, '_rng'):
1370             del self._rng
1371 
1372     def _resize_state(self):
1373         """Add additional ``n_estimators`` entries to all attributes. """
1374         # self.n_estimators is the number of additional est to fit
1375         total_n_estimators = self.n_estimators
1376         if total_n_estimators < self.estimators_.shape[0]:
1377             raise ValueError('resize with smaller n_estimators %d < %d' %
1378                              (total_n_estimators, self.estimators_[0]))
1379 
1380         self.estimators_ = np.resize(self.estimators_,
1381                                      (total_n_estimators, self.loss_.K))
1382         self.train_score_ = np.resize(self.train_score_, total_n_estimators)
1383         if (self.subsample < 1 or hasattr(self, 'oob_improvement_')):
1384             # if do oob resize arrays or create new if not available
1385             if hasattr(self, 'oob_improvement_'):
1386                 self.oob_improvement_ = np.resize(self.oob_improvement_,
1387                                                   total_n_estimators)
1388             else:
1389                 self.oob_improvement_ = np.zeros((total_n_estimators,),
1390                                                  dtype=np.float64)
1391 
1392     def _is_initialized(self):
1393         return len(getattr(self, 'estimators_', [])) > 0
1394 
1395     def _check_initialized(self):
1396         """Check that the estimator is initialized, raising an error if not."""
1397         check_is_fitted(self)
1398 
1399     def fit(self, X, y, sample_weight=None, monitor=None):
1400         """Fit the gradient boosting model.
1401 
1402         Parameters
1403         ----------
1404         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1405             The input samples. Internally, it will be converted to
1406             ``dtype=np.float32`` and if a sparse matrix is provided
1407             to a sparse ``csr_matrix``.
1408 
1409         y : array-like, shape (n_samples,)
1410             Target values (strings or integers in classification, real numbers
1411             in regression)
1412             For classification, labels must correspond to classes.
1413 
1414         sample_weight : array-like, shape (n_samples,) or None
1415             Sample weights. If None, then samples are equally weighted. Splits
1416             that would create child nodes with net zero or negative weight are
1417             ignored while searching for a split in each node. In the case of
1418             classification, splits are also ignored if they would result in any
1419             single class carrying a negative weight in either child node.
1420 
1421         monitor : callable, optional
1422             The monitor is called after each iteration with the current
1423             iteration, a reference to the estimator and the local variables of
1424             ``_fit_stages`` as keyword arguments ``callable(i, self,
1425             locals())``. If the callable returns ``True`` the fitting procedure
1426             is stopped. The monitor can be used for various things such as
1427             computing held-out estimates, early stopping, model introspect, and
1428             snapshoting.
1429 
1430         Returns
1431         -------
1432         self : object
1433         """
1434         # if not warmstart - clear the estimator state
1435         if not self.warm_start:
1436             self._clear_state()
1437 
1438         # Check input
1439         # Since check_array converts both X and y to the same dtype, but the
1440         # trees use different types for X and y, checking them separately.
1441         X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
1442         n_samples, self.n_features_ = X.shape
1443 
1444         sample_weight_is_none = sample_weight is None
1445         if sample_weight_is_none:
1446             sample_weight = np.ones(n_samples, dtype=np.float32)
1447         else:
1448             sample_weight = column_or_1d(sample_weight, warn=True)
1449             sample_weight_is_none = False
1450 
1451         check_consistent_length(X, y, sample_weight)
1452 
1453         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
1454         y = column_or_1d(y, warn=True)
1455         y = self._validate_y(y, sample_weight)
1456 
1457         if self.n_iter_no_change is not None:
1458             stratify = y if is_classifier(self) else None
1459             X, X_val, y, y_val, sample_weight, sample_weight_val = (
1460                 train_test_split(X, y, sample_weight,
1461                                  random_state=self.random_state,
1462                                  test_size=self.validation_fraction,
1463                                  stratify=stratify))
1464             if is_classifier(self):
1465                 if self.n_classes_ != np.unique(y).shape[0]:
1466                     # We choose to error here. The problem is that the init
1467                     # estimator would be trained on y, which has some missing
1468                     # classes now, so its predictions would not have the
1469                     # correct shape.
1470                     raise ValueError(
1471                         'The training data after the early stopping split '
1472                         'is missing some classes. Try using another random '
1473                         'seed.'
1474                     )
1475         else:
1476             X_val = y_val = sample_weight_val = None
1477 
1478         self._check_params()
1479 
1480         if not self._is_initialized():
1481             # init state
1482             self._init_state()
1483 
1484             # fit initial model and initialize raw predictions
1485             if self.init_ == 'zero':
1486                 raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1487                                            dtype=np.float64)
1488             else:
1489                 # XXX clean this once we have a support_sample_weight tag
1490                 if sample_weight_is_none:
1491                     self.init_.fit(X, y)
1492                 else:
1493                     msg = ("The initial estimator {} does not support sample "
1494                            "weights.".format(self.init_.__class__.__name__))
1495                     try:
1496                         self.init_.fit(X, y, sample_weight=sample_weight)
1497                     except TypeError:  # regular estimator without SW support
1498                         raise ValueError(msg)
1499                     except ValueError as e:
1500                         if "pass parameters to specific steps of "\
1501                            "your pipeline using the "\
1502                            "stepname__parameter" in str(e):  # pipeline
1503                             raise ValueError(msg) from e
1504                         else:  # regular estimator whose input checking failed
1505                             raise
1506 
1507                 raw_predictions = \
1508                     self.loss_.get_init_raw_predictions(X, self.init_)
1509 
1510             begin_at_stage = 0
1511 
1512             # The rng state must be preserved if warm_start is True
1513             self._rng = check_random_state(self.random_state)
1514 
1515         else:
1516             # add more estimators to fitted model
1517             # invariant: warm_start = True
1518             if self.n_estimators < self.estimators_.shape[0]:
1519                 raise ValueError('n_estimators=%d must be larger or equal to '
1520                                  'estimators_.shape[0]=%d when '
1521                                  'warm_start==True'
1522                                  % (self.n_estimators,
1523                                     self.estimators_.shape[0]))
1524             begin_at_stage = self.estimators_.shape[0]
1525             # The requirements of _decision_function (called in two lines
1526             # below) are more constrained than fit. It accepts only CSR
1527             # matrices.
1528             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1529             raw_predictions = self._raw_predict(X)
1530             self._resize_state()
1531 
1532         X_idx_sorted = None
1533 
1534         # fit the boosting stages
1535         n_stages = self._fit_stages(
1536             X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,
1537             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)
1538 
1539         # change shape of arrays after fit (early-stopping or additional ests)
1540         if n_stages != self.estimators_.shape[0]:
1541             self.estimators_ = self.estimators_[:n_stages]
1542             self.train_score_ = self.train_score_[:n_stages]
1543             if hasattr(self, 'oob_improvement_'):
1544                 self.oob_improvement_ = self.oob_improvement_[:n_stages]
1545 
1546         self.n_estimators_ = n_stages
1547         return self
1548 
1549     def _fit_stages(self, X, y, raw_predictions, sample_weight, random_state,
1550                     X_val, y_val, sample_weight_val,
1551                     begin_at_stage=0, monitor=None, X_idx_sorted=None):
1552         """Iteratively fits the stages.
1553 
1554         For each stage it computes the progress (OOB, train score)
1555         and delegates to ``_fit_stage``.
1556         Returns the number of stages fit; might differ from ``n_estimators``
1557         due to early stopping.
1558         """
1559         n_samples = X.shape[0]
1560         do_oob = self.subsample < 1.0
1561         sample_mask = np.ones((n_samples, ), dtype=np.bool)
1562         n_inbag = max(1, int(self.subsample * n_samples))
1563         loss_ = self.loss_
1564 
1565         if self.verbose:
1566             verbose_reporter = VerboseReporter(self.verbose)
1567             verbose_reporter.init(self, begin_at_stage)
1568 
1569         X_csc = csc_matrix(X) if issparse(X) else None
1570         X_csr = csr_matrix(X) if issparse(X) else None
1571 
1572         if self.n_iter_no_change is not None:
1573             loss_history = np.full(self.n_iter_no_change, np.inf)
1574             # We create a generator to get the predictions for X_val after
1575             # the addition of each successive stage
1576             y_val_pred_iter = self._staged_raw_predict(X_val)
1577 
1578         # perform boosting iterations
1579         i = begin_at_stage
1580         for i in range(begin_at_stage, self.n_estimators):
1581 
1582             # subsampling
1583             if do_oob:
1584                 sample_mask = _random_sample_mask(n_samples, n_inbag,
1585                                                   random_state)
1586                 # OOB score before adding this stage
1587                 old_oob_score = loss_(y[~sample_mask],
1588                                       raw_predictions[~sample_mask],
1589                                       sample_weight[~sample_mask])
1590 
1591             # fit next stage of trees
1592             raw_predictions = self._fit_stage(
1593                 i, X, y, raw_predictions, sample_weight, sample_mask,
1594                 random_state, X_idx_sorted, X_csc, X_csr)
1595 
1596             # track deviance (= loss)
1597             if do_oob:
1598                 self.train_score_[i] = loss_(y[sample_mask],
1599                                              raw_predictions[sample_mask],
1600                                              sample_weight[sample_mask])
1601                 self.oob_improvement_[i] = (
1602                     old_oob_score - loss_(y[~sample_mask],
1603                                           raw_predictions[~sample_mask],
1604                                           sample_weight[~sample_mask]))
1605             else:
1606                 # no need to fancy index w/ no subsampling
1607                 self.train_score_[i] = loss_(y, raw_predictions, sample_weight)
1608 
1609             if self.verbose > 0:
1610                 verbose_reporter.update(i, self)
1611 
1612             if monitor is not None:
1613                 early_stopping = monitor(i, self, locals())
1614                 if early_stopping:
1615                     break
1616 
1617             # We also provide an early stopping based on the score from
1618             # validation set (X_val, y_val), if n_iter_no_change is set
1619             if self.n_iter_no_change is not None:
1620                 # By calling next(y_val_pred_iter), we get the predictions
1621                 # for X_val after the addition of the current stage
1622                 validation_loss = loss_(y_val, next(y_val_pred_iter),
1623                                         sample_weight_val)
1624 
1625                 # Require validation_score to be better (less) than at least
1626                 # one of the last n_iter_no_change evaluations
1627                 if np.any(validation_loss + self.tol < loss_history):
1628                     loss_history[i % len(loss_history)] = validation_loss
1629                 else:
1630                     break
1631 
1632         return i + 1
1633 
1634     def _make_estimator(self, append=True):
1635         # we don't need _make_estimator
1636         raise NotImplementedError()
1637 
1638     def _raw_predict_init(self, X):
1639         """Check input and compute raw predictions of the init estimtor."""
1640         self._check_initialized()
1641         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1642         if X.shape[1] != self.n_features_:
1643             raise ValueError("X.shape[1] should be {0:d}, not {1:d}.".format(
1644                 self.n_features_, X.shape[1]))
1645         if self.init_ == 'zero':
1646             raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1647                                        dtype=np.float64)
1648         else:
1649             raw_predictions = self.loss_.get_init_raw_predictions(
1650                 X, self.init_).astype(np.float64)
1651         return raw_predictions
1652 
1653     def _raw_predict(self, X):
1654         """Return the sum of the trees raw predictions (+ init estimator)."""
1655         raw_predictions = self._raw_predict_init(X)
1656         predict_stages(self.estimators_, X, self.learning_rate,
1657                        raw_predictions)
1658         return raw_predictions
1659 
1660     def _staged_raw_predict(self, X):
1661         """Compute raw predictions of ``X`` for each iteration.
1662 
1663         This method allows monitoring (i.e. determine error on testing set)
1664         after each stage.
1665 
1666         Parameters
1667         ----------
1668         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1669             The input samples. Internally, it will be converted to
1670             ``dtype=np.float32`` and if a sparse matrix is provided
1671             to a sparse ``csr_matrix``.
1672 
1673         Returns
1674         -------
1675         raw_predictions : generator of array, shape (n_samples, k)
1676             The raw predictions of the input samples. The order of the
1677             classes corresponds to that in the attribute :term:`classes_`.
1678             Regression and binary classification are special cases with
1679             ``k == 1``, otherwise ``k==n_classes``.
1680         """
1681         X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1682         raw_predictions = self._raw_predict_init(X)
1683         for i in range(self.estimators_.shape[0]):
1684             predict_stage(self.estimators_, i, X, self.learning_rate,
1685                           raw_predictions)
1686             yield raw_predictions.copy()
1687 
1688     @property
1689     def feature_importances_(self):
1690         """Return the feature importances (the higher, the more important the
1691            feature).
1692 
1693         Returns
1694         -------
1695         feature_importances_ : array, shape (n_features,)
1696             The values of this array sum to 1, unless all trees are single node
1697             trees consisting of only the root node, in which case it will be an
1698             array of zeros.
1699         """
1700         self._check_initialized()
1701 
1702         relevant_trees = [tree
1703                           for stage in self.estimators_ for tree in stage
1704                           if tree.tree_.node_count > 1]
1705         if not relevant_trees:
1706             # degenerate case where all trees have only one node
1707             return np.zeros(shape=self.n_features_, dtype=np.float64)
1708 
1709         relevant_feature_importances = [
1710             tree.tree_.compute_feature_importances(normalize=False)
1711             for tree in relevant_trees
1712         ]
1713         avg_feature_importances = np.mean(relevant_feature_importances,
1714                                           axis=0, dtype=np.float64)
1715         return avg_feature_importances / np.sum(avg_feature_importances)
1716 
1717     def _compute_partial_dependence_recursion(self, grid, target_features):
1718         """Fast partial dependence computation.
1719 
1720         Parameters
1721         ----------
1722         grid : ndarray, shape (n_samples, n_target_features)
1723             The grid points on which the partial dependence should be
1724             evaluated.
1725         target_features : ndarray, shape (n_target_features)
1726             The set of target features for which the partial dependence
1727             should be evaluated.
1728 
1729         Returns
1730         -------
1731         averaged_predictions : ndarray, shape \
1732                 (n_trees_per_iteration, n_samples)
1733             The value of the partial dependence function on each grid point.
1734         """
1735         check_is_fitted(self,
1736                         msg="'estimator' parameter must be a fitted estimator")
1737         if self.init is not None:
1738             warnings.warn(
1739                 'Using recursion method with a non-constant init predictor '
1740                 'will lead to incorrect partial dependence values. '
1741                 'Got init=%s.' % self.init,
1742                 UserWarning
1743             )
1744         grid = np.asarray(grid, dtype=DTYPE, order='C')
1745         n_estimators, n_trees_per_stage = self.estimators_.shape
1746         averaged_predictions = np.zeros((n_trees_per_stage, grid.shape[0]),
1747                                         dtype=np.float64, order='C')
1748         for stage in range(n_estimators):
1749             for k in range(n_trees_per_stage):
1750                 tree = self.estimators_[stage, k].tree_
1751                 tree.compute_partial_dependence(grid, target_features,
1752                                                 averaged_predictions[k])
1753         averaged_predictions *= self.learning_rate
1754 
1755         return averaged_predictions
1756 
1757     def _validate_y(self, y, sample_weight):
1758         # 'sample_weight' is not utilised but is used for
1759         # consistency with similar method _validate_y of GBC
1760         self.n_classes_ = 1
1761         if y.dtype.kind == 'O':
1762             y = y.astype(DOUBLE)
1763         # Default implementation
1764         return y
1765 
1766     def apply(self, X):
1767         """Apply trees in the ensemble to X, return leaf indices.
1768 
1769         .. versionadded:: 0.17
1770 
1771         Parameters
1772         ----------
1773         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1774             The input samples. Internally, its dtype will be converted to
1775             ``dtype=np.float32``. If a sparse matrix is provided, it will
1776             be converted to a sparse ``csr_matrix``.
1777 
1778         Returns
1779         -------
1780         X_leaves : array-like, shape (n_samples, n_estimators, n_classes)
1781             For each datapoint x in X and for each tree in the ensemble,
1782             return the index of the leaf x ends up in each estimator.
1783             In the case of binary classification n_classes is 1.
1784         """
1785 
1786         self._check_initialized()
1787         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1788 
1789         # n_classes will be equal to 1 in the binary classification or the
1790         # regression case.
1791         n_estimators, n_classes = self.estimators_.shape
1792         leaves = np.zeros((X.shape[0], n_estimators, n_classes))
1793 
1794         for i in range(n_estimators):
1795             for j in range(n_classes):
1796                 estimator = self.estimators_[i, j]
1797                 leaves[:, i, j] = estimator.apply(X, check_input=False)
1798 
1799         return leaves

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 17:29:35.625 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 17:29:35.625 | INFO     | Search-only: skipping patch selection
2025-10-18 17:29:35.720 | INFO     | Task scikit-learn__scikit-learn-15495 completed successfully.
2025-10-18 17:29:35.721 | INFO     | No patch generated. You can try running ACR again.
