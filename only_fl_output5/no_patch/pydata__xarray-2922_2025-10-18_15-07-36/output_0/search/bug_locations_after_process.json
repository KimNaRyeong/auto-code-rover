[
    {
        "rel_file_path": "xarray/core/dataset.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/dataset.py",
        "start": 422,
        "end": 5744,
        "class_name": "Dataset",
        "method_name": null,
        "code": "422 class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n423     \"\"\"A multi-dimensional, in memory, array database.\n424 \n425     A dataset resembles an in-memory representation of a NetCDF file, and\n426     consists of variables, coordinates and attributes which together form a\n427     self describing dataset.\n428 \n429     Dataset implements the mapping interface with keys given by variable names\n430     and values given by DataArray objects for each variable name.\n431 \n432     One dimensional variables with name equal to their dimension are index\n433     coordinates used for label based indexing.\n434     \"\"\"\n435 \n436     _attrs: Optional[Dict[Hashable, Any]]\n437     _cache: Dict[str, Any]\n438     _coord_names: Set[Hashable]\n439     _dims: Dict[Hashable, int]\n440     _encoding: Optional[Dict[Hashable, Any]]\n441     _indexes: Optional[Dict[Hashable, pd.Index]]\n442     _variables: Dict[Hashable, Variable]\n443 \n444     __slots__ = (\n445         \"_attrs\",\n446         \"_cache\",\n447         \"_coord_names\",\n448         \"_dims\",\n449         \"_encoding\",\n450         \"_file_obj\",\n451         \"_indexes\",\n452         \"_variables\",\n453         \"__weakref__\",\n454     )\n455 \n456     _groupby_cls = groupby.DatasetGroupBy\n457     _rolling_cls = rolling.DatasetRolling\n458     _coarsen_cls = rolling.DatasetCoarsen\n459     _resample_cls = resample.DatasetResample\n460 \n461     def __init__(\n462         self,\n463         # could make a VariableArgs to use more generally, and refine these\n464         # categories\n465         data_vars: Mapping[Hashable, Any] = None,\n466         coords: Mapping[Hashable, Any] = None,\n467         attrs: Mapping[Hashable, Any] = None,\n468     ):\n469         \"\"\"To load data from a file or file-like object, use the `open_dataset`\n470         function.\n471 \n472         Parameters\n473         ----------\n474         data_vars : dict-like, optional\n475             A mapping from variable names to :py:class:`~xarray.DataArray`\n476             objects, :py:class:`~xarray.Variable` objects or to tuples of the\n477             form ``(dims, data[, attrs])`` which can be used as arguments to\n478             create a new ``Variable``. Each dimension must have the same length\n479             in all variables in which it appears.\n480 \n481             The following notations are accepted:\n482 \n483             - mapping {var name: DataArray}\n484             - mapping {var name: Variable}\n485             - mapping {var name: (dimension name, array-like)}\n486             - mapping {var name: (tuple of dimension names, array-like)}\n487             - mapping {dimension name: array-like}\n488               (it will be automatically moved to coords, see below)\n489 \n490             Each dimension must have the same length in all variables in which\n491             it appears.\n492         coords : dict-like, optional\n493             Another mapping in similar form as the `data_vars` argument,\n494             except the each item is saved on the dataset as a \"coordinate\".\n495             These variables have an associated meaning: they describe\n496             constant/fixed/independent quantities, unlike the\n497             varying/measured/dependent quantities that belong in `variables`.\n498             Coordinates values may be given by 1-dimensional arrays or scalars,\n499             in which case `dims` do not need to be supplied: 1D arrays will be\n500             assumed to give index values along the dimension with the same\n501             name.\n502 \n503             The following notations are accepted:\n504 \n505             - mapping {coord name: DataArray}\n506             - mapping {coord name: Variable}\n507             - mapping {coord name: (dimension name, array-like)}\n508             - mapping {coord name: (tuple of dimension names, array-like)}\n509             - mapping {dimension name: array-like}\n510               (the dimension name is implicitly set to be the same as the coord name)\n511 \n512             The last notation implies that the coord name is the same as the\n513             dimension name.\n514 \n515         attrs : dict-like, optional\n516             Global attributes to save on this dataset.\n517         \"\"\"\n518 \n519         # TODO(shoyer): expose indexes as a public argument in __init__\n520 \n521         if data_vars is None:\n522             data_vars = {}\n523         if coords is None:\n524             coords = {}\n525 \n526         both_data_and_coords = set(data_vars) & set(coords)\n527         if both_data_and_coords:\n528             raise ValueError(\n529                 \"variables %r are found in both data_vars and coords\"\n530                 % both_data_and_coords\n531             )\n532 \n533         if isinstance(coords, Dataset):\n534             coords = coords.variables\n535 \n536         variables, coord_names, dims, indexes = merge_data_and_coords(\n537             data_vars, coords, compat=\"broadcast_equals\"\n538         )\n539 \n540         self._attrs = dict(attrs) if attrs is not None else None\n541         self._file_obj = None\n542         self._encoding = None\n543         self._variables = variables\n544         self._coord_names = coord_names\n545         self._dims = dims\n546         self._indexes = indexes\n547 \n548     @classmethod\n549     def load_store(cls, store, decoder=None) -> \"Dataset\":\n550         \"\"\"Create a new dataset from the contents of a backends.*DataStore\n551         object\n552         \"\"\"\n553         variables, attributes = store.load()\n554         if decoder:\n555             variables, attributes = decoder(variables, attributes)\n556         obj = cls(variables, attrs=attributes)\n557         obj._file_obj = store\n558         return obj\n559 \n560     @property\n561     def variables(self) -> Mapping[Hashable, Variable]:\n562         \"\"\"Low level interface to Dataset contents as dict of Variable objects.\n563 \n564         This ordered dictionary is frozen to prevent mutation that could\n565         violate Dataset invariants. It contains all variable objects\n566         constituting the Dataset, including both data variables and\n567         coordinates.\n568         \"\"\"\n569         return Frozen(self._variables)\n570 \n571     @property\n572     def attrs(self) -> Dict[Hashable, Any]:\n573         \"\"\"Dictionary of global attributes on this dataset\n574         \"\"\"\n575         if self._attrs is None:\n576             self._attrs = {}\n577         return self._attrs\n578 \n579     @attrs.setter\n580     def attrs(self, value: Mapping[Hashable, Any]) -> None:\n581         self._attrs = dict(value)\n582 \n583     @property\n584     def encoding(self) -> Dict:\n585         \"\"\"Dictionary of global encoding attributes on this dataset\n586         \"\"\"\n587         if self._encoding is None:\n588             self._encoding = {}\n589         return self._encoding\n590 \n591     @encoding.setter\n592     def encoding(self, value: Mapping) -> None:\n593         self._encoding = dict(value)\n594 \n595     @property\n596     def dims(self) -> Mapping[Hashable, int]:\n597         \"\"\"Mapping from dimension names to lengths.\n598 \n599         Cannot be modified directly, but is updated when adding new variables.\n600 \n601         Note that type of this object differs from `DataArray.dims`.\n602         See `Dataset.sizes` and `DataArray.sizes` for consistently named\n603         properties.\n604         \"\"\"\n605         return Frozen(SortedKeysDict(self._dims))\n606 \n607     @property\n608     def sizes(self) -> Mapping[Hashable, int]:\n609         \"\"\"Mapping from dimension names to lengths.\n610 \n611         Cannot be modified directly, but is updated when adding new variables.\n612 \n613         This is an alias for `Dataset.dims` provided for the benefit of\n614         consistency with `DataArray.sizes`.\n615 \n616         See also\n617         --------\n618         DataArray.sizes\n619         \"\"\"\n620         return self.dims\n621 \n622     def load(self, **kwargs) -> \"Dataset\":\n623         \"\"\"Manually trigger loading and/or computation of this dataset's data\n624         from disk or a remote source into memory and return this dataset.\n625         Unlike compute, the original dataset is modified and returned.\n626 \n627         Normally, it should not be necessary to call this method in user code,\n628         because all xarray functions should either work on deferred data or\n629         load data automatically. However, this method can be necessary when\n630         working with many file objects on disk.\n631 \n632         Parameters\n633         ----------\n634         **kwargs : dict\n635             Additional keyword arguments passed on to ``dask.array.compute``.\n636 \n637         See Also\n638         --------\n639         dask.array.compute\n640         \"\"\"\n641         # access .data to coerce everything to numpy or dask arrays\n642         lazy_data = {\n643             k: v._data\n644             for k, v in self.variables.items()\n645             if isinstance(v._data, dask_array_type)\n646         }\n647         if lazy_data:\n648             import dask.array as da\n649 \n650             # evaluate all the dask arrays simultaneously\n651             evaluated_data = da.compute(*lazy_data.values(), **kwargs)\n652 \n653             for k, data in zip(lazy_data, evaluated_data):\n654                 self.variables[k].data = data\n655 \n656         # load everything else sequentially\n657         for k, v in self.variables.items():\n658             if k not in lazy_data:\n659                 v.load()\n660 \n661         return self\n662 \n663     def __dask_tokenize__(self):\n664         from dask.base import normalize_token\n665 \n666         return normalize_token(\n667             (type(self), self._variables, self._coord_names, self._attrs)\n668         )\n669 \n670     def __dask_graph__(self):\n671         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}\n672         graphs = {k: v for k, v in graphs.items() if v is not None}\n673         if not graphs:\n674             return None\n675         else:\n676             try:\n677                 from dask.highlevelgraph import HighLevelGraph\n678 \n679                 return HighLevelGraph.merge(*graphs.values())\n680             except ImportError:\n681                 from dask import sharedict\n682 \n683                 return sharedict.merge(*graphs.values())\n684 \n685     def __dask_keys__(self):\n686         import dask\n687 \n688         return [\n689             v.__dask_keys__()\n690             for v in self.variables.values()\n691             if dask.is_dask_collection(v)\n692         ]\n693 \n694     def __dask_layers__(self):\n695         import dask\n696 \n697         return sum(\n698             [\n699                 v.__dask_layers__()\n700                 for v in self.variables.values()\n701                 if dask.is_dask_collection(v)\n702             ],\n703             (),\n704         )\n705 \n706     @property\n707     def __dask_optimize__(self):\n708         import dask.array as da\n709 \n710         return da.Array.__dask_optimize__\n711 \n712     @property\n713     def __dask_scheduler__(self):\n714         import dask.array as da\n715 \n716         return da.Array.__dask_scheduler__\n717 \n718     def __dask_postcompute__(self):\n719         import dask\n720 \n721         info = [\n722             (True, k, v.__dask_postcompute__())\n723             if dask.is_dask_collection(v)\n724             else (False, k, v)\n725             for k, v in self._variables.items()\n726         ]\n727         args = (\n728             info,\n729             self._coord_names,\n730             self._dims,\n731             self._attrs,\n732             self._indexes,\n733             self._encoding,\n734             self._file_obj,\n735         )\n736         return self._dask_postcompute, args\n737 \n738     def __dask_postpersist__(self):\n739         import dask\n740 \n741         info = [\n742             (True, k, v.__dask_postpersist__())\n743             if dask.is_dask_collection(v)\n744             else (False, k, v)\n745             for k, v in self._variables.items()\n746         ]\n747         args = (\n748             info,\n749             self._coord_names,\n750             self._dims,\n751             self._attrs,\n752             self._indexes,\n753             self._encoding,\n754             self._file_obj,\n755         )\n756         return self._dask_postpersist, args\n757 \n758     @staticmethod\n759     def _dask_postcompute(results, info, *args):\n760         variables = {}\n761         results2 = list(results[::-1])\n762         for is_dask, k, v in info:\n763             if is_dask:\n764                 func, args2 = v\n765                 r = results2.pop()\n766                 result = func(r, *args2)\n767             else:\n768                 result = v\n769             variables[k] = result\n770 \n771         final = Dataset._construct_direct(variables, *args)\n772         return final\n773 \n774     @staticmethod\n775     def _dask_postpersist(dsk, info, *args):\n776         variables = {}\n777         for is_dask, k, v in info:\n778             if is_dask:\n779                 func, args2 = v\n780                 result = func(dsk, *args2)\n781             else:\n782                 result = v\n783             variables[k] = result\n784 \n785         return Dataset._construct_direct(variables, *args)\n786 \n787     def compute(self, **kwargs) -> \"Dataset\":\n788         \"\"\"Manually trigger loading and/or computation of this dataset's data\n789         from disk or a remote source into memory and return a new dataset.\n790         Unlike load, the original dataset is left unaltered.\n791 \n792         Normally, it should not be necessary to call this method in user code,\n793         because all xarray functions should either work on deferred data or\n794         load data automatically. However, this method can be necessary when\n795         working with many file objects on disk.\n796 \n797         Parameters\n798         ----------\n799         **kwargs : dict\n800             Additional keyword arguments passed on to ``dask.array.compute``.\n801 \n802         See Also\n803         --------\n804         dask.array.compute\n805         \"\"\"\n806         new = self.copy(deep=False)\n807         return new.load(**kwargs)\n808 \n809     def _persist_inplace(self, **kwargs) -> \"Dataset\":\n810         \"\"\"Persist all Dask arrays in memory\n811         \"\"\"\n812         # access .data to coerce everything to numpy or dask arrays\n813         lazy_data = {\n814             k: v._data\n815             for k, v in self.variables.items()\n816             if isinstance(v._data, dask_array_type)\n817         }\n818         if lazy_data:\n819             import dask\n820 \n821             # evaluate all the dask arrays simultaneously\n822             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)\n823 \n824             for k, data in zip(lazy_data, evaluated_data):\n825                 self.variables[k].data = data\n826 \n827         return self\n828 \n829     def persist(self, **kwargs) -> \"Dataset\":\n830         \"\"\" Trigger computation, keeping data as dask arrays\n831 \n832         This operation can be used to trigger computation on underlying dask\n833         arrays, similar to ``.compute()`` or ``.load()``.  However this\n834         operation keeps the data as dask arrays. This is particularly useful\n835         when using the dask.distributed scheduler and you want to load a large\n836         amount of data into distributed memory.\n837 \n838         Parameters\n839         ----------\n840         **kwargs : dict\n841             Additional keyword arguments passed on to ``dask.persist``.\n842 \n843         See Also\n844         --------\n845         dask.persist\n846         \"\"\"\n847         new = self.copy(deep=False)\n848         return new._persist_inplace(**kwargs)\n849 \n850     @classmethod\n851     def _construct_direct(\n852         cls,\n853         variables,\n854         coord_names,\n855         dims=None,\n856         attrs=None,\n857         indexes=None,\n858         encoding=None,\n859         file_obj=None,\n860     ):\n861         \"\"\"Shortcut around __init__ for internal use when we want to skip\n862         costly validation\n863         \"\"\"\n864         if dims is None:\n865             dims = calculate_dimensions(variables)\n866         obj = object.__new__(cls)\n867         obj._variables = variables\n868         obj._coord_names = coord_names\n869         obj._dims = dims\n870         obj._indexes = indexes\n871         obj._attrs = attrs\n872         obj._file_obj = file_obj\n873         obj._encoding = encoding\n874         return obj\n875 \n876     def _replace(\n877         self,\n878         variables: Dict[Hashable, Variable] = None,\n879         coord_names: Set[Hashable] = None,\n880         dims: Dict[Any, int] = None,\n881         attrs: Union[Dict[Hashable, Any], None, Default] = _default,\n882         indexes: Union[Dict[Any, pd.Index], None, Default] = _default,\n883         encoding: Union[dict, None, Default] = _default,\n884         inplace: bool = False,\n885     ) -> \"Dataset\":\n886         \"\"\"Fastpath constructor for internal use.\n887 \n888         Returns an object with optionally with replaced attributes.\n889 \n890         Explicitly passed arguments are *not* copied when placed on the new\n891         dataset. It is up to the caller to ensure that they have the right type\n892         and are not used elsewhere.\n893         \"\"\"\n894         if inplace:\n895             if variables is not None:\n896                 self._variables = variables\n897             if coord_names is not None:\n898                 self._coord_names = coord_names\n899             if dims is not None:\n900                 self._dims = dims\n901             if attrs is not _default:\n902                 self._attrs = attrs\n903             if indexes is not _default:\n904                 self._indexes = indexes\n905             if encoding is not _default:\n906                 self._encoding = encoding\n907             obj = self\n908         else:\n909             if variables is None:\n910                 variables = self._variables.copy()\n911             if coord_names is None:\n912                 coord_names = self._coord_names.copy()\n913             if dims is None:\n914                 dims = self._dims.copy()\n915             if attrs is _default:\n916                 attrs = copy.copy(self._attrs)\n917             if indexes is _default:\n918                 indexes = copy.copy(self._indexes)\n919             if encoding is _default:\n920                 encoding = copy.copy(self._encoding)\n921             obj = self._construct_direct(\n922                 variables, coord_names, dims, attrs, indexes, encoding\n923             )\n924         return obj\n925 \n926     def _replace_with_new_dims(\n927         self,\n928         variables: Dict[Hashable, Variable],\n929         coord_names: set = None,\n930         attrs: Union[Dict[Hashable, Any], None, Default] = _default,\n931         indexes: Union[Dict[Hashable, pd.Index], None, Default] = _default,\n932         inplace: bool = False,\n933     ) -> \"Dataset\":\n934         \"\"\"Replace variables with recalculated dimensions.\"\"\"\n935         dims = calculate_dimensions(variables)\n936         return self._replace(\n937             variables, coord_names, dims, attrs, indexes, inplace=inplace\n938         )\n939 \n940     def _replace_vars_and_dims(\n941         self,\n942         variables: Dict[Hashable, Variable],\n943         coord_names: set = None,\n944         dims: Dict[Hashable, int] = None,\n945         attrs: Union[Dict[Hashable, Any], None, Default] = _default,\n946         inplace: bool = False,\n947     ) -> \"Dataset\":\n948         \"\"\"Deprecated version of _replace_with_new_dims().\n949 \n950         Unlike _replace_with_new_dims(), this method always recalculates\n951         indexes from variables.\n952         \"\"\"\n953         if dims is None:\n954             dims = calculate_dimensions(variables)\n955         return self._replace(\n956             variables, coord_names, dims, attrs, indexes=None, inplace=inplace\n957         )\n958 \n959     def _overwrite_indexes(self, indexes: Mapping[Any, pd.Index]) -> \"Dataset\":\n960         if not indexes:\n961             return self\n962 \n963         variables = self._variables.copy()\n964         new_indexes = dict(self.indexes)\n965         for name, idx in indexes.items():\n966             variables[name] = IndexVariable(name, idx)\n967             new_indexes[name] = idx\n968         obj = self._replace(variables, indexes=new_indexes)\n969 \n970         # switch from dimension to level names, if necessary\n971         dim_names: Dict[Hashable, str] = {}\n972         for dim, idx in indexes.items():\n973             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:\n974                 dim_names[dim] = idx.name\n975         if dim_names:\n976             obj = obj.rename(dim_names)\n977         return obj\n978 \n979     def copy(self, deep: bool = False, data: Mapping = None) -> \"Dataset\":\n980         \"\"\"Returns a copy of this dataset.\n981 \n982         If `deep=True`, a deep copy is made of each of the component variables.\n983         Otherwise, a shallow copy of each of the component variable is made, so\n984         that the underlying memory region of the new dataset is the same as in\n985         the original dataset.\n986 \n987         Use `data` to create a new object with the same structure as\n988         original but entirely new data.\n989 \n990         Parameters\n991         ----------\n992         deep : bool, optional\n993             Whether each component variable is loaded into memory and copied onto\n994             the new object. Default is False.\n995         data : dict-like, optional\n996             Data to use in the new object. Each item in `data` must have same\n997             shape as corresponding data variable in original. When `data` is\n998             used, `deep` is ignored for the data variables and only used for\n999             coords.\n1000 \n1001         Returns\n1002         -------\n1003         object : Dataset\n1004             New object with dimensions, attributes, coordinates, name, encoding,\n1005             and optionally data copied from original.\n1006 \n1007         Examples\n1008         --------\n1009 \n1010         Shallow copy versus deep copy\n1011 \n1012         >>> da = xr.DataArray(np.random.randn(2, 3))\n1013         >>> ds = xr.Dataset(\n1014         ...     {\"foo\": da, \"bar\": (\"x\", [-1, 2])}, coords={\"x\": [\"one\", \"two\"]},\n1015         ... )\n1016         >>> ds.copy()\n1017         <xarray.Dataset>\n1018         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1019         Coordinates:\n1020         * x        (x) <U3 'one' 'two'\n1021         Dimensions without coordinates: dim_0, dim_1\n1022         Data variables:\n1023             foo      (dim_0, dim_1) float64 -0.8079 0.3897 -1.862 -0.6091 -1.051 -0.3003\n1024             bar      (x) int64 -1 2\n1025 \n1026         >>> ds_0 = ds.copy(deep=False)\n1027         >>> ds_0[\"foo\"][0, 0] = 7\n1028         >>> ds_0\n1029         <xarray.Dataset>\n1030         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1031         Coordinates:\n1032         * x        (x) <U3 'one' 'two'\n1033         Dimensions without coordinates: dim_0, dim_1\n1034         Data variables:\n1035             foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003\n1036             bar      (x) int64 -1 2\n1037 \n1038         >>> ds\n1039         <xarray.Dataset>\n1040         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1041         Coordinates:\n1042         * x        (x) <U3 'one' 'two'\n1043         Dimensions without coordinates: dim_0, dim_1\n1044         Data variables:\n1045             foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003\n1046             bar      (x) int64 -1 2\n1047 \n1048         Changing the data using the ``data`` argument maintains the\n1049         structure of the original object, but with the new data. Original\n1050         object is unaffected.\n1051 \n1052         >>> ds.copy(\n1053         ...     data={\"foo\": np.arange(6).reshape(2, 3), \"bar\": [\"a\", \"b\"]}\n1054         ... )\n1055         <xarray.Dataset>\n1056         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1057         Coordinates:\n1058         * x        (x) <U3 'one' 'two'\n1059         Dimensions without coordinates: dim_0, dim_1\n1060         Data variables:\n1061             foo      (dim_0, dim_1) int64 0 1 2 3 4 5\n1062             bar      (x) <U1 'a' 'b'\n1063 \n1064         >>> ds\n1065         <xarray.Dataset>\n1066         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1067         Coordinates:\n1068         * x        (x) <U3 'one' 'two'\n1069         Dimensions without coordinates: dim_0, dim_1\n1070         Data variables:\n1071             foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003\n1072             bar      (x) int64 -1 2\n1073 \n1074         See Also\n1075         --------\n1076         pandas.DataFrame.copy\n1077         \"\"\"\n1078         if data is None:\n1079             variables = {k: v.copy(deep=deep) for k, v in self._variables.items()}\n1080         elif not utils.is_dict_like(data):\n1081             raise ValueError(\"Data must be dict-like\")\n1082         else:\n1083             var_keys = set(self.data_vars.keys())\n1084             data_keys = set(data.keys())\n1085             keys_not_in_vars = data_keys - var_keys\n1086             if keys_not_in_vars:\n1087                 raise ValueError(\n1088                     \"Data must only contain variables in original \"\n1089                     \"dataset. Extra variables: {}\".format(keys_not_in_vars)\n1090                 )\n1091             keys_missing_from_data = var_keys - data_keys\n1092             if keys_missing_from_data:\n1093                 raise ValueError(\n1094                     \"Data must contain all variables in original \"\n1095                     \"dataset. Data is missing {}\".format(keys_missing_from_data)\n1096                 )\n1097             variables = {\n1098                 k: v.copy(deep=deep, data=data.get(k))\n1099                 for k, v in self._variables.items()\n1100             }\n1101 \n1102         attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)\n1103 \n1104         return self._replace(variables, attrs=attrs)\n1105 \n1106     @property\n1107     def _level_coords(self) -> Dict[str, Hashable]:\n1108         \"\"\"Return a mapping of all MultiIndex levels and their corresponding\n1109         coordinate name.\n1110         \"\"\"\n1111         level_coords: Dict[str, Hashable] = {}\n1112         for name, index in self.indexes.items():\n1113             if isinstance(index, pd.MultiIndex):\n1114                 level_names = index.names\n1115                 (dim,) = self.variables[name].dims\n1116                 level_coords.update({lname: dim for lname in level_names})\n1117         return level_coords\n1118 \n1119     def _copy_listed(self, names: Iterable[Hashable]) -> \"Dataset\":\n1120         \"\"\"Create a new Dataset with the listed variables from this dataset and\n1121         the all relevant coordinates. Skips all validation.\n1122         \"\"\"\n1123         variables: Dict[Hashable, Variable] = {}\n1124         coord_names = set()\n1125         indexes: Dict[Hashable, pd.Index] = {}\n1126 \n1127         for name in names:\n1128             try:\n1129                 variables[name] = self._variables[name]\n1130             except KeyError:\n1131                 ref_name, var_name, var = _get_virtual_variable(\n1132                     self._variables, name, self._level_coords, self.dims\n1133                 )\n1134                 variables[var_name] = var\n1135                 if ref_name in self._coord_names or ref_name in self.dims:\n1136                     coord_names.add(var_name)\n1137                 if (var_name,) == var.dims:\n1138                     indexes[var_name] = var.to_index()\n1139 \n1140         needed_dims: Set[Hashable] = set()\n1141         for v in variables.values():\n1142             needed_dims.update(v.dims)\n1143 \n1144         dims = {k: self.dims[k] for k in needed_dims}\n1145 \n1146         for k in self._coord_names:\n1147             if set(self.variables[k].dims) <= needed_dims:\n1148                 variables[k] = self._variables[k]\n1149                 coord_names.add(k)\n1150                 if k in self.indexes:\n1151                     indexes[k] = self.indexes[k]\n1152 \n1153         return self._replace(variables, coord_names, dims, indexes=indexes)\n1154 \n1155     def _construct_dataarray(self, name: Hashable) -> \"DataArray\":\n1156         \"\"\"Construct a DataArray by indexing this dataset\n1157         \"\"\"\n1158         from .dataarray import DataArray\n1159 \n1160         try:\n1161             variable = self._variables[name]\n1162         except KeyError:\n1163             _, name, variable = _get_virtual_variable(\n1164                 self._variables, name, self._level_coords, self.dims\n1165             )\n1166 \n1167         needed_dims = set(variable.dims)\n1168 \n1169         coords: Dict[Hashable, Variable] = {}\n1170         for k in self.coords:\n1171             if set(self.variables[k].dims) <= needed_dims:\n1172                 coords[k] = self.variables[k]\n1173 \n1174         if self._indexes is None:\n1175             indexes = None\n1176         else:\n1177             indexes = {k: v for k, v in self._indexes.items() if k in coords}\n1178 \n1179         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)\n1180 \n1181     def __copy__(self) -> \"Dataset\":\n1182         return self.copy(deep=False)\n1183 \n1184     def __deepcopy__(self, memo=None) -> \"Dataset\":\n1185         # memo does nothing but is required for compatibility with\n1186         # copy.deepcopy\n1187         return self.copy(deep=True)\n1188 \n1189     @property\n1190     def _attr_sources(self) -> List[Mapping[Hashable, Any]]:\n1191         \"\"\"List of places to look-up items for attribute-style access\n1192         \"\"\"\n1193         return self._item_sources + [self.attrs]\n1194 \n1195     @property\n1196     def _item_sources(self) -> List[Mapping[Hashable, Any]]:\n1197         \"\"\"List of places to look-up items for key-completion\n1198         \"\"\"\n1199         return [\n1200             self.data_vars,\n1201             self.coords,\n1202             {d: self[d] for d in self.dims},\n1203             LevelCoordinatesSource(self),\n1204         ]\n1205 \n1206     def __contains__(self, key: object) -> bool:\n1207         \"\"\"The 'in' operator will return true or false depending on whether\n1208         'key' is an array in the dataset or not.\n1209         \"\"\"\n1210         return key in self._variables\n1211 \n1212     def __len__(self) -> int:\n1213         return len(self.data_vars)\n1214 \n1215     def __bool__(self) -> bool:\n1216         return bool(self.data_vars)\n1217 \n1218     def __iter__(self) -> Iterator[Hashable]:\n1219         return iter(self.data_vars)\n1220 \n1221     def __array__(self, dtype=None):\n1222         raise TypeError(\n1223             \"cannot directly convert an xarray.Dataset into a \"\n1224             \"numpy array. Instead, create an xarray.DataArray \"\n1225             \"first, either with indexing on the Dataset or by \"\n1226             \"invoking the `to_array()` method.\"\n1227         )\n1228 \n1229     @property\n1230     def nbytes(self) -> int:\n1231         return sum(v.nbytes for v in self.variables.values())\n1232 \n1233     @property\n1234     def loc(self) -> _LocIndexer:\n1235         \"\"\"Attribute for location based indexing. Only supports __getitem__,\n1236         and only when the key is a dict of the form {dim: labels}.\n1237         \"\"\"\n1238         return _LocIndexer(self)\n1239 \n1240     def __getitem__(self, key: Any) -> \"Union[DataArray, Dataset]\":\n1241         \"\"\"Access variables or coordinates this dataset as a\n1242         :py:class:`~xarray.DataArray`.\n1243 \n1244         Indexing with a list of names will return a new ``Dataset`` object.\n1245         \"\"\"\n1246         # TODO(shoyer): type this properly: https://github.com/python/mypy/issues/7328\n1247         if utils.is_dict_like(key):\n1248             return self.isel(**cast(Mapping, key))\n1249 \n1250         if hashable(key):\n1251             return self._construct_dataarray(key)\n1252         else:\n1253             return self._copy_listed(np.asarray(key))\n1254 \n1255     def __setitem__(self, key: Hashable, value) -> None:\n1256         \"\"\"Add an array to this dataset.\n1257 \n1258         If value is a `DataArray`, call its `select_vars()` method, rename it\n1259         to `key` and merge the contents of the resulting dataset into this\n1260         dataset.\n1261 \n1262         If value is an `Variable` object (or tuple of form\n1263         ``(dims, data[, attrs])``), add it to this dataset as a new\n1264         variable.\n1265         \"\"\"\n1266         if utils.is_dict_like(key):\n1267             raise NotImplementedError(\n1268                 \"cannot yet use a dictionary as a key \" \"to set Dataset values\"\n1269             )\n1270 \n1271         self.update({key: value})\n1272 \n1273     def __delitem__(self, key: Hashable) -> None:\n1274         \"\"\"Remove a variable from this dataset.\n1275         \"\"\"\n1276         del self._variables[key]\n1277         self._coord_names.discard(key)\n1278         if key in self.indexes:\n1279             assert self._indexes is not None\n1280             del self._indexes[key]\n1281         self._dims = calculate_dimensions(self._variables)\n1282 \n1283     # mutable objects should not be hashable\n1284     # https://github.com/python/mypy/issues/4266\n1285     __hash__ = None  # type: ignore\n1286 \n1287     def _all_compat(self, other: \"Dataset\", compat_str: str) -> bool:\n1288         \"\"\"Helper function for equals and identical\n1289         \"\"\"\n1290 \n1291         # some stores (e.g., scipy) do not seem to preserve order, so don't\n1292         # require matching order for equality\n1293         def compat(x: Variable, y: Variable) -> bool:\n1294             return getattr(x, compat_str)(y)\n1295 \n1296         return self._coord_names == other._coord_names and utils.dict_equiv(\n1297             self._variables, other._variables, compat=compat\n1298         )\n1299 \n1300     def broadcast_equals(self, other: \"Dataset\") -> bool:\n1301         \"\"\"Two Datasets are broadcast equal if they are equal after\n1302         broadcasting all variables against each other.\n1303 \n1304         For example, variables that are scalar in one dataset but non-scalar in\n1305         the other dataset can still be broadcast equal if the the non-scalar\n1306         variable is a constant.\n1307 \n1308         See Also\n1309         --------\n1310         Dataset.equals\n1311         Dataset.identical\n1312         \"\"\"\n1313         try:\n1314             return self._all_compat(other, \"broadcast_equals\")\n1315         except (TypeError, AttributeError):\n1316             return False\n1317 \n1318     def equals(self, other: \"Dataset\") -> bool:\n1319         \"\"\"Two Datasets are equal if they have matching variables and\n1320         coordinates, all of which are equal.\n1321 \n1322         Datasets can still be equal (like pandas objects) if they have NaN\n1323         values in the same locations.\n1324 \n1325         This method is necessary because `v1 == v2` for ``Dataset``\n1326         does element-wise comparisons (like numpy.ndarrays).\n1327 \n1328         See Also\n1329         --------\n1330         Dataset.broadcast_equals\n1331         Dataset.identical\n1332         \"\"\"\n1333         try:\n1334             return self._all_compat(other, \"equals\")\n1335         except (TypeError, AttributeError):\n1336             return False\n1337 \n1338     def identical(self, other: \"Dataset\") -> bool:\n1339         \"\"\"Like equals, but also checks all dataset attributes and the\n1340         attributes on all variables and coordinates.\n1341 \n1342         See Also\n1343         --------\n1344         Dataset.broadcast_equals\n1345         Dataset.equals\n1346         \"\"\"\n1347         try:\n1348             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(\n1349                 other, \"identical\"\n1350             )\n1351         except (TypeError, AttributeError):\n1352             return False\n1353 \n1354     @property\n1355     def indexes(self) -> Indexes:\n1356         \"\"\"Mapping of pandas.Index objects used for label based indexing\n1357         \"\"\"\n1358         if self._indexes is None:\n1359             self._indexes = default_indexes(self._variables, self._dims)\n1360         return Indexes(self._indexes)\n1361 \n1362     @property\n1363     def coords(self) -> DatasetCoordinates:\n1364         \"\"\"Dictionary of xarray.DataArray objects corresponding to coordinate\n1365         variables\n1366         \"\"\"\n1367         return DatasetCoordinates(self)\n1368 \n1369     @property\n1370     def data_vars(self) -> DataVariables:\n1371         \"\"\"Dictionary of DataArray objects corresponding to data variables\n1372         \"\"\"\n1373         return DataVariables(self)\n1374 \n1375     def set_coords(\n1376         self, names: \"Union[Hashable, Iterable[Hashable]]\", inplace: bool = None\n1377     ) -> \"Dataset\":\n1378         \"\"\"Given names of one or more variables, set them as coordinates\n1379 \n1380         Parameters\n1381         ----------\n1382         names : hashable or iterable of hashables\n1383             Name(s) of variables in this dataset to convert into coordinates.\n1384 \n1385         Returns\n1386         -------\n1387         Dataset\n1388 \n1389         See also\n1390         --------\n1391         Dataset.swap_dims\n1392         \"\"\"\n1393         # TODO: allow inserting new coordinates with this method, like\n1394         # DataFrame.set_index?\n1395         # nb. check in self._variables, not self.data_vars to insure that the\n1396         # operation is idempotent\n1397         _check_inplace(inplace)\n1398         if isinstance(names, str) or not isinstance(names, Iterable):\n1399             names = [names]\n1400         else:\n1401             names = list(names)\n1402         self._assert_all_in_dataset(names)\n1403         obj = self.copy()\n1404         obj._coord_names.update(names)\n1405         return obj\n1406 \n1407     def reset_coords(\n1408         self,\n1409         names: \"Union[Hashable, Iterable[Hashable], None]\" = None,\n1410         drop: bool = False,\n1411         inplace: bool = None,\n1412     ) -> \"Dataset\":\n1413         \"\"\"Given names of coordinates, reset them to become variables\n1414 \n1415         Parameters\n1416         ----------\n1417         names : hashable or iterable of hashables, optional\n1418             Name(s) of non-index coordinates in this dataset to reset into\n1419             variables. By default, all non-index coordinates are reset.\n1420         drop : bool, optional\n1421             If True, remove coordinates instead of converting them into\n1422             variables.\n1423 \n1424         Returns\n1425         -------\n1426         Dataset\n1427         \"\"\"\n1428         _check_inplace(inplace)\n1429         if names is None:\n1430             names = self._coord_names - set(self.dims)\n1431         else:\n1432             if isinstance(names, str) or not isinstance(names, Iterable):\n1433                 names = [names]\n1434             else:\n1435                 names = list(names)\n1436             self._assert_all_in_dataset(names)\n1437             bad_coords = set(names) & set(self.dims)\n1438             if bad_coords:\n1439                 raise ValueError(\n1440                     \"cannot remove index coordinates with reset_coords: %s\" % bad_coords\n1441                 )\n1442         obj = self.copy()\n1443         obj._coord_names.difference_update(names)\n1444         if drop:\n1445             for name in names:\n1446                 del obj._variables[name]\n1447         return obj\n1448 \n1449     def dump_to_store(self, store: \"AbstractDataStore\", **kwargs) -> None:\n1450         \"\"\"Store dataset contents to a backends.*DataStore object.\n1451         \"\"\"\n1452         from ..backends.api import dump_to_store\n1453 \n1454         # TODO: rename and/or cleanup this method to make it more consistent\n1455         # with to_netcdf()\n1456         dump_to_store(self, store, **kwargs)\n1457 \n1458     def to_netcdf(\n1459         self,\n1460         path=None,\n1461         mode: str = \"w\",\n1462         format: str = None,\n1463         group: str = None,\n1464         engine: str = None,\n1465         encoding: Mapping = None,\n1466         unlimited_dims: Iterable[Hashable] = None,\n1467         compute: bool = True,\n1468         invalid_netcdf: bool = False,\n1469     ) -> Union[bytes, \"Delayed\", None]:\n1470         \"\"\"Write dataset contents to a netCDF file.\n1471 \n1472         Parameters\n1473         ----------\n1474         path : str, Path or file-like object, optional\n1475             Path to which to save this dataset. File-like objects are only\n1476             supported by the scipy engine. If no path is provided, this\n1477             function returns the resulting netCDF file as bytes; in this case,\n1478             we need to use scipy, which does not support netCDF version 4 (the\n1479             default format becomes NETCDF3_64BIT).\n1480         mode : {'w', 'a'}, optional\n1481             Write ('w') or append ('a') mode. If mode='w', any existing file at\n1482             this location will be overwritten. If mode='a', existing variables\n1483             will be overwritten.\n1484         format : {'NETCDF4', 'NETCDF4_CLASSIC', 'NETCDF3_64BIT',\n1485                   'NETCDF3_CLASSIC'}, optional\n1486             File format for the resulting netCDF file:\n1487 \n1488             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API\n1489               features.\n1490             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only\n1491               netCDF 3 compatible API features.\n1492             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,\n1493               which fully supports 2+ GB files, but is only compatible with\n1494               clients linked against netCDF version 3.6.0 or later.\n1495             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not\n1496               handle 2+ GB files very well.\n1497 \n1498             All formats are supported by the netCDF4-python library.\n1499             scipy.io.netcdf only supports the last two formats.\n1500 \n1501             The default format is NETCDF4 if you are saving a file to disk and\n1502             have the netCDF4-python library available. Otherwise, xarray falls\n1503             back to using scipy to write netCDF files and defaults to the\n1504             NETCDF3_64BIT format (scipy does not support netCDF4).\n1505         group : str, optional\n1506             Path to the netCDF4 group in the given file to open (only works for\n1507             format='NETCDF4'). The group(s) will be created if necessary.\n1508         engine : {'netcdf4', 'scipy', 'h5netcdf'}, optional\n1509             Engine to use when writing netCDF files. If not provided, the\n1510             default engine is chosen based on available dependencies, with a\n1511             preference for 'netcdf4' if writing to a file on disk.\n1512         encoding : dict, optional\n1513             Nested dictionary with variable names as keys and dictionaries of\n1514             variable specific encodings as values, e.g.,\n1515             ``{'my_variable': {'dtype': 'int16', 'scale_factor': 0.1,\n1516             'zlib': True}, ...}``\n1517 \n1518             The `h5netcdf` engine supports both the NetCDF4-style compression\n1519             encoding parameters ``{'zlib': True, 'complevel': 9}`` and the h5py\n1520             ones ``{'compression': 'gzip', 'compression_opts': 9}``.\n1521             This allows using any compression plugin installed in the HDF5\n1522             library, e.g. LZF.\n1523 \n1524         unlimited_dims : iterable of hashable, optional\n1525             Dimension(s) that should be serialized as unlimited dimensions.\n1526             By default, no dimensions are treated as unlimited dimensions.\n1527             Note that unlimited_dims may also be set via\n1528             ``dataset.encoding['unlimited_dims']``.\n1529         compute: boolean\n1530             If true compute immediately, otherwise return a\n1531             ``dask.delayed.Delayed`` object that can be computed later.\n1532         invalid_netcdf: boolean\n1533             Only valid along with engine='h5netcdf'. If True, allow writing\n1534             hdf5 files which are valid netcdf as described in\n1535             https://github.com/shoyer/h5netcdf. Default: False.\n1536         \"\"\"\n1537         if encoding is None:\n1538             encoding = {}\n1539         from ..backends.api import to_netcdf\n1540 \n1541         return to_netcdf(\n1542             self,\n1543             path,\n1544             mode,\n1545             format=format,\n1546             group=group,\n1547             engine=engine,\n1548             encoding=encoding,\n1549             unlimited_dims=unlimited_dims,\n1550             compute=compute,\n1551             invalid_netcdf=invalid_netcdf,\n1552         )\n1553 \n1554     def to_zarr(\n1555         self,\n1556         store: Union[MutableMapping, str, Path] = None,\n1557         mode: str = None,\n1558         synchronizer=None,\n1559         group: str = None,\n1560         encoding: Mapping = None,\n1561         compute: bool = True,\n1562         consolidated: bool = False,\n1563         append_dim: Hashable = None,\n1564     ) -> \"ZarrStore\":\n1565         \"\"\"Write dataset contents to a zarr group.\n1566 \n1567         .. note:: Experimental\n1568                   The Zarr backend is new and experimental. Please report any\n1569                   unexpected behavior via github issues.\n1570 \n1571         Parameters\n1572         ----------\n1573         store : MutableMapping, str or Path, optional\n1574             Store or path to directory in file system.\n1575         mode : {'w', 'w-', 'a', None}\n1576             Persistence mode: 'w' means create (overwrite if exists);\n1577             'w-' means create (fail if exists);\n1578             'a' means append (create if does not exist).\n1579             If ``append_dim`` is set, ``mode`` can be omitted as it is\n1580             internally set to ``'a'``. Otherwise, ``mode`` will default to\n1581             `w-` if not set.\n1582         synchronizer : object, optional\n1583             Array synchronizer\n1584         group : str, optional\n1585             Group path. (a.k.a. `path` in zarr terminology.)\n1586         encoding : dict, optional\n1587             Nested dictionary with variable names as keys and dictionaries of\n1588             variable specific encodings as values, e.g.,\n1589             ``{'my_variable': {'dtype': 'int16', 'scale_factor': 0.1,}, ...}``\n1590         compute: bool, optional\n1591             If True compute immediately, otherwise return a\n1592             ``dask.delayed.Delayed`` object that can be computed later.\n1593         consolidated: bool, optional\n1594             If True, apply zarr's `consolidate_metadata` function to the store\n1595             after writing.\n1596         append_dim: hashable, optional\n1597             If set, the dimension on which the data will be appended.\n1598 \n1599         References\n1600         ----------\n1601         https://zarr.readthedocs.io/\n1602         \"\"\"\n1603         if encoding is None:\n1604             encoding = {}\n1605         if (mode == \"a\") or (append_dim is not None):\n1606             if mode is None:\n1607                 mode = \"a\"\n1608             elif mode != \"a\":\n1609                 raise ValueError(\n1610                     \"append_dim was set along with mode='{}', either set \"\n1611                     \"mode='a' or don't set it.\".format(mode)\n1612                 )\n1613         elif mode is None:\n1614             mode = \"w-\"\n1615         if mode not in [\"w\", \"w-\", \"a\"]:\n1616             # TODO: figure out how to handle 'r+'\n1617             raise ValueError(\n1618                 \"The only supported options for mode are 'w',\" \"'w-' and 'a'.\"\n1619             )\n1620         from ..backends.api import to_zarr\n1621 \n1622         return to_zarr(\n1623             self,\n1624             store=store,\n1625             mode=mode,\n1626             synchronizer=synchronizer,\n1627             group=group,\n1628             encoding=encoding,\n1629             compute=compute,\n1630             consolidated=consolidated,\n1631             append_dim=append_dim,\n1632         )\n1633 \n1634     def __repr__(self) -> str:\n1635         return formatting.dataset_repr(self)\n1636 \n1637     def _repr_html_(self):\n1638         if OPTIONS[\"display_style\"] == \"text\":\n1639             return f\"<pre>{escape(repr(self))}</pre>\"\n1640         return formatting_html.dataset_repr(self)\n1641 \n1642     def info(self, buf=None) -> None:\n1643         \"\"\"\n1644         Concise summary of a Dataset variables and attributes.\n1645 \n1646         Parameters\n1647         ----------\n1648         buf : writable buffer, defaults to sys.stdout\n1649 \n1650         See Also\n1651         --------\n1652         pandas.DataFrame.assign\n1653         ncdump: netCDF's ncdump\n1654         \"\"\"\n1655         if buf is None:  # pragma: no cover\n1656             buf = sys.stdout\n1657 \n1658         lines = []\n1659         lines.append(\"xarray.Dataset {\")\n1660         lines.append(\"dimensions:\")\n1661         for name, size in self.dims.items():\n1662             lines.append(f\"\\t{name} = {size} ;\")\n1663         lines.append(\"\\nvariables:\")\n1664         for name, da in self.variables.items():\n1665             dims = \", \".join(da.dims)\n1666             lines.append(f\"\\t{da.dtype} {name}({dims}) ;\")\n1667             for k, v in da.attrs.items():\n1668                 lines.append(f\"\\t\\t{name}:{k} = {v} ;\")\n1669         lines.append(\"\\n// global attributes:\")\n1670         for k, v in self.attrs.items():\n1671             lines.append(f\"\\t:{k} = {v} ;\")\n1672         lines.append(\"}\")\n1673 \n1674         buf.write(\"\\n\".join(lines))\n1675 \n1676     @property\n1677     def chunks(self) -> Mapping[Hashable, Tuple[int, ...]]:\n1678         \"\"\"Block dimensions for this dataset's data or None if it's not a dask\n1679         array.\n1680         \"\"\"\n1681         chunks: Dict[Hashable, Tuple[int, ...]] = {}\n1682         for v in self.variables.values():\n1683             if v.chunks is not None:\n1684                 for dim, c in zip(v.dims, v.chunks):\n1685                     if dim in chunks and c != chunks[dim]:\n1686                         raise ValueError(\n1687                             f\"Object has inconsistent chunks along dimension {dim}. \"\n1688                             \"This can be fixed by calling unify_chunks().\"\n1689                         )\n1690                     chunks[dim] = c\n1691         return Frozen(SortedKeysDict(chunks))\n1692 \n1693     def chunk(\n1694         self,\n1695         chunks: Union[\n1696             None, Number, Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]]\n1697         ] = None,\n1698         name_prefix: str = \"xarray-\",\n1699         token: str = None,\n1700         lock: bool = False,\n1701     ) -> \"Dataset\":\n1702         \"\"\"Coerce all arrays in this dataset into dask arrays with the given\n1703         chunks.\n1704 \n1705         Non-dask arrays in this dataset will be converted to dask arrays. Dask\n1706         arrays will be rechunked to the given chunk sizes.\n1707 \n1708         If neither chunks is not provided for one or more dimensions, chunk\n1709         sizes along that dimension will not be updated; non-dask arrays will be\n1710         converted into dask arrays with a single block.\n1711 \n1712         Parameters\n1713         ----------\n1714         chunks : int or mapping, optional\n1715             Chunk sizes along each dimension, e.g., ``5`` or\n1716             ``{'x': 5, 'y': 5}``.\n1717         name_prefix : str, optional\n1718             Prefix for the name of any new dask arrays.\n1719         token : str, optional\n1720             Token uniquely identifying this dataset.\n1721         lock : optional\n1722             Passed on to :py:func:`dask.array.from_array`, if the array is not\n1723             already as dask array.\n1724 \n1725         Returns\n1726         -------\n1727         chunked : xarray.Dataset\n1728         \"\"\"\n1729         from dask.base import tokenize\n1730 \n1731         if isinstance(chunks, Number):\n1732             chunks = dict.fromkeys(self.dims, chunks)\n1733 \n1734         if chunks is not None:\n1735             bad_dims = chunks.keys() - self.dims.keys()\n1736             if bad_dims:\n1737                 raise ValueError(\n1738                     \"some chunks keys are not dimensions on this \"\n1739                     \"object: %s\" % bad_dims\n1740                 )\n1741 \n1742         def selkeys(dict_, keys):\n1743             if dict_ is None:\n1744                 return None\n1745             return {d: dict_[d] for d in keys if d in dict_}\n1746 \n1747         def maybe_chunk(name, var, chunks):\n1748             chunks = selkeys(chunks, var.dims)\n1749             if not chunks:\n1750                 chunks = None\n1751             if var.ndim > 0:\n1752                 # when rechunking by different amounts, make sure dask names change\n1753                 # by provinding chunks as an input to tokenize.\n1754                 # subtle bugs result otherwise. see GH3350\n1755                 token2 = tokenize(name, token if token else var._data, chunks)\n1756                 name2 = f\"{name_prefix}{name}-{token2}\"\n1757                 return var.chunk(chunks, name=name2, lock=lock)\n1758             else:\n1759                 return var\n1760 \n1761         variables = {k: maybe_chunk(k, v, chunks) for k, v in self.variables.items()}\n1762         return self._replace(variables)\n1763 \n1764     def _validate_indexers(\n1765         self, indexers: Mapping[Hashable, Any]\n1766     ) -> Iterator[Tuple[Hashable, Union[int, slice, np.ndarray, Variable]]]:\n1767         \"\"\" Here we make sure\n1768         + indexer has a valid keys\n1769         + indexer is in a valid data type\n1770         + string indexers are cast to the appropriate date type if the\n1771           associated index is a DatetimeIndex or CFTimeIndex\n1772         \"\"\"\n1773         from .dataarray import DataArray\n1774 \n1775         invalid = indexers.keys() - self.dims.keys()\n1776         if invalid:\n1777             raise ValueError(\"dimensions %r do not exist\" % invalid)\n1778 \n1779         # all indexers should be int, slice, np.ndarrays, or Variable\n1780         for k, v in indexers.items():\n1781             if isinstance(v, (int, slice, Variable)):\n1782                 yield k, v\n1783             elif isinstance(v, DataArray):\n1784                 yield k, v.variable\n1785             elif isinstance(v, tuple):\n1786                 yield k, as_variable(v)\n1787             elif isinstance(v, Dataset):\n1788                 raise TypeError(\"cannot use a Dataset as an indexer\")\n1789             elif isinstance(v, Sequence) and len(v) == 0:\n1790                 yield k, np.empty((0,), dtype=\"int64\")\n1791             else:\n1792                 v = np.asarray(v)\n1793 \n1794                 if v.dtype.kind in \"US\":\n1795                     index = self.indexes[k]\n1796                     if isinstance(index, pd.DatetimeIndex):\n1797                         v = v.astype(\"datetime64[ns]\")\n1798                     elif isinstance(index, xr.CFTimeIndex):\n1799                         v = _parse_array_of_cftime_strings(v, index.date_type)\n1800 \n1801                 if v.ndim > 1:\n1802                     raise IndexError(\n1803                         \"Unlabeled multi-dimensional array cannot be \"\n1804                         \"used for indexing: {}\".format(k)\n1805                     )\n1806                 yield k, v\n1807 \n1808     def _validate_interp_indexers(\n1809         self, indexers: Mapping[Hashable, Any]\n1810     ) -> Iterator[Tuple[Hashable, Variable]]:\n1811         \"\"\"Variant of _validate_indexers to be used for interpolation\n1812         \"\"\"\n1813         for k, v in self._validate_indexers(indexers):\n1814             if isinstance(v, Variable):\n1815                 if v.ndim == 1:\n1816                     yield k, v.to_index_variable()\n1817                 else:\n1818                     yield k, v\n1819             elif isinstance(v, int):\n1820                 yield k, Variable((), v)\n1821             elif isinstance(v, np.ndarray):\n1822                 if v.ndim == 0:\n1823                     yield k, Variable((), v)\n1824                 elif v.ndim == 1:\n1825                     yield k, IndexVariable((k,), v)\n1826                 else:\n1827                     raise AssertionError()  # Already tested by _validate_indexers\n1828             else:\n1829                 raise TypeError(type(v))\n1830 \n1831     def _get_indexers_coords_and_indexes(self, indexers):\n1832         \"\"\"Extract coordinates and indexes from indexers.\n1833 \n1834         Only coordinate with a name different from any of self.variables will\n1835         be attached.\n1836         \"\"\"\n1837         from .dataarray import DataArray\n1838 \n1839         coords_list = []\n1840         for k, v in indexers.items():\n1841             if isinstance(v, DataArray):\n1842                 if v.dtype.kind == \"b\":\n1843                     if v.ndim != 1:  # we only support 1-d boolean array\n1844                         raise ValueError(\n1845                             \"{:d}d-boolean array is used for indexing along \"\n1846                             \"dimension {!r}, but only 1d boolean arrays are \"\n1847                             \"supported.\".format(v.ndim, k)\n1848                         )\n1849                     # Make sure in case of boolean DataArray, its\n1850                     # coordinate also should be indexed.\n1851                     v_coords = v[v.values.nonzero()[0]].coords\n1852                 else:\n1853                     v_coords = v.coords\n1854                 coords_list.append(v_coords)\n1855 \n1856         # we don't need to call align() explicitly or check indexes for\n1857         # alignment, because merge_variables already checks for exact alignment\n1858         # between dimension coordinates\n1859         coords, indexes = merge_coordinates_without_align(coords_list)\n1860         assert_coordinate_consistent(self, coords)\n1861 \n1862         # silently drop the conflicted variables.\n1863         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}\n1864         attached_indexes = {\n1865             k: v for k, v in indexes.items() if k not in self._variables\n1866         }\n1867         return attached_coords, attached_indexes\n1868 \n1869     def isel(\n1870         self,\n1871         indexers: Mapping[Hashable, Any] = None,\n1872         drop: bool = False,\n1873         **indexers_kwargs: Any,\n1874     ) -> \"Dataset\":\n1875         \"\"\"Returns a new dataset with each array indexed along the specified\n1876         dimension(s).\n1877 \n1878         This method selects values from each array using its `__getitem__`\n1879         method, except this method does not require knowing the order of\n1880         each array's dimensions.\n1881 \n1882         Parameters\n1883         ----------\n1884         indexers : dict, optional\n1885             A dict with keys matching dimensions and values given\n1886             by integers, slice objects or arrays.\n1887             indexer can be a integer, slice, array-like or DataArray.\n1888             If DataArrays are passed as indexers, xarray-style indexing will be\n1889             carried out. See :ref:`indexing` for the details.\n1890             One of indexers or indexers_kwargs must be provided.\n1891         drop : bool, optional\n1892             If ``drop=True``, drop coordinates variables indexed by integers\n1893             instead of making them scalar.\n1894         **indexers_kwargs : {dim: indexer, ...}, optional\n1895             The keyword arguments form of ``indexers``.\n1896             One of indexers or indexers_kwargs must be provided.\n1897 \n1898         Returns\n1899         -------\n1900         obj : Dataset\n1901             A new Dataset with the same contents as this dataset, except each\n1902             array and dimension is indexed by the appropriate indexers.\n1903             If indexer DataArrays have coordinates that do not conflict with\n1904             this object, then these coordinates will be attached.\n1905             In general, each array's data will be a view of the array's data\n1906             in this dataset, unless vectorized indexing was triggered by using\n1907             an array indexer, in which case the data will be a copy.\n1908 \n1909         See Also\n1910         --------\n1911         Dataset.sel\n1912         DataArray.isel\n1913         \"\"\"\n1914         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"isel\")\n1915         if any(is_fancy_indexer(idx) for idx in indexers.values()):\n1916             return self._isel_fancy(indexers, drop=drop)\n1917 \n1918         # Much faster algorithm for when all indexers are ints, slices, one-dimensional\n1919         # lists, or zero or one-dimensional np.ndarray's\n1920         invalid = indexers.keys() - self.dims.keys()\n1921         if invalid:\n1922             raise ValueError(\"dimensions %r do not exist\" % invalid)\n1923 \n1924         variables = {}\n1925         dims: Dict[Hashable, Tuple[int, ...]] = {}\n1926         coord_names = self._coord_names.copy()\n1927         indexes = self._indexes.copy() if self._indexes is not None else None\n1928 \n1929         for var_name, var_value in self._variables.items():\n1930             var_indexers = {k: v for k, v in indexers.items() if k in var_value.dims}\n1931             if var_indexers:\n1932                 var_value = var_value.isel(var_indexers)\n1933                 if drop and var_value.ndim == 0 and var_name in coord_names:\n1934                     coord_names.remove(var_name)\n1935                     if indexes:\n1936                         indexes.pop(var_name, None)\n1937                     continue\n1938                 if indexes and var_name in indexes:\n1939                     if var_value.ndim == 1:\n1940                         indexes[var_name] = var_value.to_index()\n1941                     else:\n1942                         del indexes[var_name]\n1943             variables[var_name] = var_value\n1944             dims.update(zip(var_value.dims, var_value.shape))\n1945 \n1946         return self._construct_direct(\n1947             variables=variables,\n1948             coord_names=coord_names,\n1949             dims=dims,\n1950             attrs=self._attrs,\n1951             indexes=indexes,\n1952             encoding=self._encoding,\n1953             file_obj=self._file_obj,\n1954         )\n1955 \n1956     def _isel_fancy(self, indexers: Mapping[Hashable, Any], *, drop: bool) -> \"Dataset\":\n1957         # Note: we need to preserve the original indexers variable in order to merge the\n1958         # coords below\n1959         indexers_list = list(self._validate_indexers(indexers))\n1960 \n1961         variables: Dict[Hashable, Variable] = {}\n1962         indexes: Dict[Hashable, pd.Index] = {}\n1963 \n1964         for name, var in self.variables.items():\n1965             var_indexers = {k: v for k, v in indexers_list if k in var.dims}\n1966             if drop and name in var_indexers:\n1967                 continue  # drop this variable\n1968 \n1969             if name in self.indexes:\n1970                 new_var, new_index = isel_variable_and_index(\n1971                     name, var, self.indexes[name], var_indexers\n1972                 )\n1973                 if new_index is not None:\n1974                     indexes[name] = new_index\n1975             elif var_indexers:\n1976                 new_var = var.isel(indexers=var_indexers)\n1977             else:\n1978                 new_var = var.copy(deep=False)\n1979 \n1980             variables[name] = new_var\n1981 \n1982         coord_names = self._coord_names & variables.keys()\n1983         selected = self._replace_with_new_dims(variables, coord_names, indexes)\n1984 \n1985         # Extract coordinates from indexers\n1986         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)\n1987         variables.update(coord_vars)\n1988         indexes.update(new_indexes)\n1989         coord_names = self._coord_names & variables.keys() | coord_vars.keys()\n1990         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)\n1991 \n1992     def sel(\n1993         self,\n1994         indexers: Mapping[Hashable, Any] = None,\n1995         method: str = None,\n1996         tolerance: Number = None,\n1997         drop: bool = False,\n1998         **indexers_kwargs: Any,\n1999     ) -> \"Dataset\":\n2000         \"\"\"Returns a new dataset with each array indexed by tick labels\n2001         along the specified dimension(s).\n2002 \n2003         In contrast to `Dataset.isel`, indexers for this method should use\n2004         labels instead of integers.\n2005 \n2006         Under the hood, this method is powered by using pandas's powerful Index\n2007         objects. This makes label based indexing essentially just as fast as\n2008         using integer indexing.\n2009 \n2010         It also means this method uses pandas's (well documented) logic for\n2011         indexing. This means you can use string shortcuts for datetime indexes\n2012         (e.g., '2000-01' to select all values in January 2000). It also means\n2013         that slices are treated as inclusive of both the start and stop values,\n2014         unlike normal Python indexing.\n2015 \n2016         Parameters\n2017         ----------\n2018         indexers : dict, optional\n2019             A dict with keys matching dimensions and values given\n2020             by scalars, slices or arrays of tick labels. For dimensions with\n2021             multi-index, the indexer may also be a dict-like object with keys\n2022             matching index level names.\n2023             If DataArrays are passed as indexers, xarray-style indexing will be\n2024             carried out. See :ref:`indexing` for the details.\n2025             One of indexers or indexers_kwargs must be provided.\n2026         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n2027             Method to use for inexact matches:\n2028 \n2029             * None (default): only exact matches\n2030             * pad / ffill: propagate last valid index value forward\n2031             * backfill / bfill: propagate next valid index value backward\n2032             * nearest: use nearest valid index value\n2033         tolerance : optional\n2034             Maximum distance between original and new labels for inexact\n2035             matches. The values of the index at the matching locations must\n2036             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n2037         drop : bool, optional\n2038             If ``drop=True``, drop coordinates variables in `indexers` instead\n2039             of making them scalar.\n2040         **indexers_kwargs : {dim: indexer, ...}, optional\n2041             The keyword arguments form of ``indexers``.\n2042             One of indexers or indexers_kwargs must be provided.\n2043 \n2044         Returns\n2045         -------\n2046         obj : Dataset\n2047             A new Dataset with the same contents as this dataset, except each\n2048             variable and dimension is indexed by the appropriate indexers.\n2049             If indexer DataArrays have coordinates that do not conflict with\n2050             this object, then these coordinates will be attached.\n2051             In general, each array's data will be a view of the array's data\n2052             in this dataset, unless vectorized indexing was triggered by using\n2053             an array indexer, in which case the data will be a copy.\n2054 \n2055 \n2056         See Also\n2057         --------\n2058         Dataset.isel\n2059         DataArray.sel\n2060         \"\"\"\n2061         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"sel\")\n2062         pos_indexers, new_indexes = remap_label_indexers(\n2063             self, indexers=indexers, method=method, tolerance=tolerance\n2064         )\n2065         result = self.isel(indexers=pos_indexers, drop=drop)\n2066         return result._overwrite_indexes(new_indexes)\n2067 \n2068     def head(\n2069         self,\n2070         indexers: Union[Mapping[Hashable, int], int] = None,\n2071         **indexers_kwargs: Any,\n2072     ) -> \"Dataset\":\n2073         \"\"\"Returns a new dataset with the first `n` values of each array\n2074         for the specified dimension(s).\n2075 \n2076         Parameters\n2077         ----------\n2078         indexers : dict or int, default: 5\n2079             A dict with keys matching dimensions and integer values `n`\n2080             or a single integer `n` applied over all dimensions.\n2081             One of indexers or indexers_kwargs must be provided.\n2082         **indexers_kwargs : {dim: n, ...}, optional\n2083             The keyword arguments form of ``indexers``.\n2084             One of indexers or indexers_kwargs must be provided.\n2085 \n2086 \n2087         See Also\n2088         --------\n2089         Dataset.tail\n2090         Dataset.thin\n2091         DataArray.head\n2092         \"\"\"\n2093         if not indexers_kwargs:\n2094             if indexers is None:\n2095                 indexers = 5\n2096             if not isinstance(indexers, int) and not is_dict_like(indexers):\n2097                 raise TypeError(\"indexers must be either dict-like or a single integer\")\n2098         if isinstance(indexers, int):\n2099             indexers = {dim: indexers for dim in self.dims}\n2100         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"head\")\n2101         for k, v in indexers.items():\n2102             if not isinstance(v, int):\n2103                 raise TypeError(\n2104                     \"expected integer type indexer for \"\n2105                     \"dimension %r, found %r\" % (k, type(v))\n2106                 )\n2107             elif v < 0:\n2108                 raise ValueError(\n2109                     \"expected positive integer as indexer \"\n2110                     \"for dimension %r, found %s\" % (k, v)\n2111                 )\n2112         indexers_slices = {k: slice(val) for k, val in indexers.items()}\n2113         return self.isel(indexers_slices)\n2114 \n2115     def tail(\n2116         self,\n2117         indexers: Union[Mapping[Hashable, int], int] = None,\n2118         **indexers_kwargs: Any,\n2119     ) -> \"Dataset\":\n2120         \"\"\"Returns a new dataset with the last `n` values of each array\n2121         for the specified dimension(s).\n2122 \n2123         Parameters\n2124         ----------\n2125         indexers : dict or int, default: 5\n2126             A dict with keys matching dimensions and integer values `n`\n2127             or a single integer `n` applied over all dimensions.\n2128             One of indexers or indexers_kwargs must be provided.\n2129         **indexers_kwargs : {dim: n, ...}, optional\n2130             The keyword arguments form of ``indexers``.\n2131             One of indexers or indexers_kwargs must be provided.\n2132 \n2133 \n2134         See Also\n2135         --------\n2136         Dataset.head\n2137         Dataset.thin\n2138         DataArray.tail\n2139         \"\"\"\n2140         if not indexers_kwargs:\n2141             if indexers is None:\n2142                 indexers = 5\n2143             if not isinstance(indexers, int) and not is_dict_like(indexers):\n2144                 raise TypeError(\"indexers must be either dict-like or a single integer\")\n2145         if isinstance(indexers, int):\n2146             indexers = {dim: indexers for dim in self.dims}\n2147         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"tail\")\n2148         for k, v in indexers.items():\n2149             if not isinstance(v, int):\n2150                 raise TypeError(\n2151                     \"expected integer type indexer for \"\n2152                     \"dimension %r, found %r\" % (k, type(v))\n2153                 )\n2154             elif v < 0:\n2155                 raise ValueError(\n2156                     \"expected positive integer as indexer \"\n2157                     \"for dimension %r, found %s\" % (k, v)\n2158                 )\n2159         indexers_slices = {\n2160             k: slice(-val, None) if val != 0 else slice(val)\n2161             for k, val in indexers.items()\n2162         }\n2163         return self.isel(indexers_slices)\n2164 \n2165     def thin(\n2166         self,\n2167         indexers: Union[Mapping[Hashable, int], int] = None,\n2168         **indexers_kwargs: Any,\n2169     ) -> \"Dataset\":\n2170         \"\"\"Returns a new dataset with each array indexed along every `n`-th\n2171         value for the specified dimension(s)\n2172 \n2173         Parameters\n2174         ----------\n2175         indexers : dict or int\n2176             A dict with keys matching dimensions and integer values `n`\n2177             or a single integer `n` applied over all dimensions.\n2178             One of indexers or indexers_kwargs must be provided.\n2179         ``**indexers_kwargs`` : {dim: n, ...}, optional\n2180             The keyword arguments form of ``indexers``.\n2181             One of indexers or indexers_kwargs must be provided.\n2182 \n2183 \n2184         See Also\n2185         --------\n2186         Dataset.head\n2187         Dataset.tail\n2188         DataArray.thin\n2189         \"\"\"\n2190         if (\n2191             not indexers_kwargs\n2192             and not isinstance(indexers, int)\n2193             and not is_dict_like(indexers)\n2194         ):\n2195             raise TypeError(\"indexers must be either dict-like or a single integer\")\n2196         if isinstance(indexers, int):\n2197             indexers = {dim: indexers for dim in self.dims}\n2198         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"thin\")\n2199         for k, v in indexers.items():\n2200             if not isinstance(v, int):\n2201                 raise TypeError(\n2202                     \"expected integer type indexer for \"\n2203                     \"dimension %r, found %r\" % (k, type(v))\n2204                 )\n2205             elif v < 0:\n2206                 raise ValueError(\n2207                     \"expected positive integer as indexer \"\n2208                     \"for dimension %r, found %s\" % (k, v)\n2209                 )\n2210             elif v == 0:\n2211                 raise ValueError(\"step cannot be zero\")\n2212         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}\n2213         return self.isel(indexers_slices)\n2214 \n2215     def broadcast_like(\n2216         self, other: Union[\"Dataset\", \"DataArray\"], exclude: Iterable[Hashable] = None\n2217     ) -> \"Dataset\":\n2218         \"\"\"Broadcast this DataArray against another Dataset or DataArray.\n2219         This is equivalent to xr.broadcast(other, self)[1]\n2220 \n2221         Parameters\n2222         ----------\n2223         other : Dataset or DataArray\n2224             Object against which to broadcast this array.\n2225         exclude : iterable of hashable, optional\n2226             Dimensions that must not be broadcasted\n2227 \n2228         \"\"\"\n2229         if exclude is None:\n2230             exclude = set()\n2231         else:\n2232             exclude = set(exclude)\n2233         args = align(other, self, join=\"outer\", copy=False, exclude=exclude)\n2234 \n2235         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)\n2236 \n2237         return _broadcast_helper(args[1], exclude, dims_map, common_coords)\n2238 \n2239     def reindex_like(\n2240         self,\n2241         other: Union[\"Dataset\", \"DataArray\"],\n2242         method: str = None,\n2243         tolerance: Number = None,\n2244         copy: bool = True,\n2245         fill_value: Any = dtypes.NA,\n2246     ) -> \"Dataset\":\n2247         \"\"\"Conform this object onto the indexes of another object, filling in\n2248         missing values with ``fill_value``. The default fill value is NaN.\n2249 \n2250         Parameters\n2251         ----------\n2252         other : Dataset or DataArray\n2253             Object with an 'indexes' attribute giving a mapping from dimension\n2254             names to pandas.Index objects, which provides coordinates upon\n2255             which to index the variables in this dataset. The indexes on this\n2256             other object need not be the same as the indexes on this\n2257             dataset. Any mis-matched index values will be filled in with\n2258             NaN, and any mis-matched dimension names will simply be ignored.\n2259         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n2260             Method to use for filling index values from other not found in this\n2261             dataset:\n2262 \n2263             * None (default): don't fill gaps\n2264             * pad / ffill: propagate last valid index value forward\n2265             * backfill / bfill: propagate next valid index value backward\n2266             * nearest: use nearest valid index value\n2267         tolerance : optional\n2268             Maximum distance between original and new labels for inexact\n2269             matches. The values of the index at the matching locations must\n2270             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n2271         copy : bool, optional\n2272             If ``copy=True``, data in the return value is always copied. If\n2273             ``copy=False`` and reindexing is unnecessary, or can be performed\n2274             with only slice operations, then the output may share memory with\n2275             the input. In either case, a new xarray object is always returned.\n2276         fill_value : scalar, optional\n2277             Value to use for newly missing values\n2278 \n2279         Returns\n2280         -------\n2281         reindexed : Dataset\n2282             Another dataset, with this dataset's data but coordinates from the\n2283             other object.\n2284 \n2285         See Also\n2286         --------\n2287         Dataset.reindex\n2288         align\n2289         \"\"\"\n2290         indexers = alignment.reindex_like_indexers(self, other)\n2291         return self.reindex(\n2292             indexers=indexers,\n2293             method=method,\n2294             copy=copy,\n2295             fill_value=fill_value,\n2296             tolerance=tolerance,\n2297         )\n2298 \n2299     def reindex(\n2300         self,\n2301         indexers: Mapping[Hashable, Any] = None,\n2302         method: str = None,\n2303         tolerance: Number = None,\n2304         copy: bool = True,\n2305         fill_value: Any = dtypes.NA,\n2306         **indexers_kwargs: Any,\n2307     ) -> \"Dataset\":\n2308         \"\"\"Conform this object onto a new set of indexes, filling in\n2309         missing values with ``fill_value``. The default fill value is NaN.\n2310 \n2311         Parameters\n2312         ----------\n2313         indexers : dict. optional\n2314             Dictionary with keys given by dimension names and values given by\n2315             arrays of coordinates tick labels. Any mis-matched coordinate\n2316             values will be filled in with NaN, and any mis-matched dimension\n2317             names will simply be ignored.\n2318             One of indexers or indexers_kwargs must be provided.\n2319         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n2320             Method to use for filling index values in ``indexers`` not found in\n2321             this dataset:\n2322 \n2323             * None (default): don't fill gaps\n2324             * pad / ffill: propagate last valid index value forward\n2325             * backfill / bfill: propagate next valid index value backward\n2326             * nearest: use nearest valid index value\n2327         tolerance : optional\n2328             Maximum distance between original and new labels for inexact\n2329             matches. The values of the index at the matching locations must\n2330             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n2331         copy : bool, optional\n2332             If ``copy=True``, data in the return value is always copied. If\n2333             ``copy=False`` and reindexing is unnecessary, or can be performed\n2334             with only slice operations, then the output may share memory with\n2335             the input. In either case, a new xarray object is always returned.\n2336         fill_value : scalar, optional\n2337             Value to use for newly missing values\n2338         sparse: use sparse-array. By default, False\n2339         **indexers_kwargs : {dim: indexer, ...}, optional\n2340             Keyword arguments in the same form as ``indexers``.\n2341             One of indexers or indexers_kwargs must be provided.\n2342 \n2343         Returns\n2344         -------\n2345         reindexed : Dataset\n2346             Another dataset, with this dataset's data but replaced coordinates.\n2347 \n2348         See Also\n2349         --------\n2350         Dataset.reindex_like\n2351         align\n2352         pandas.Index.get_indexer\n2353 \n2354         Examples\n2355         --------\n2356 \n2357         Create a dataset with some fictional data.\n2358 \n2359         >>> import xarray as xr\n2360         >>> import pandas as pd\n2361         >>> x = xr.Dataset(\n2362         ...     {\n2363         ...         \"temperature\": (\"station\", 20 * np.random.rand(4)),\n2364         ...         \"pressure\": (\"station\", 500 * np.random.rand(4)),\n2365         ...     },\n2366         ...     coords={\"station\": [\"boston\", \"nyc\", \"seattle\", \"denver\"]},\n2367         ... )\n2368         >>> x\n2369         <xarray.Dataset>\n2370         Dimensions:      (station: 4)\n2371         Coordinates:\n2372         * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'\n2373         Data variables:\n2374             temperature  (station) float64 18.84 14.59 19.22 17.16\n2375             pressure     (station) float64 324.1 194.3 122.8 244.3\n2376         >>> x.indexes\n2377         station: Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')\n2378 \n2379         Create a new index and reindex the dataset. By default values in the new index that\n2380         do not have corresponding records in the dataset are assigned `NaN`.\n2381 \n2382         >>> new_index = [\"boston\", \"austin\", \"seattle\", \"lincoln\"]\n2383         >>> x.reindex({\"station\": new_index})\n2384         <xarray.Dataset>\n2385         Dimensions:      (station: 4)\n2386         Coordinates:\n2387         * station      (station) object 'boston' 'austin' 'seattle' 'lincoln'\n2388         Data variables:\n2389             temperature  (station) float64 18.84 nan 19.22 nan\n2390             pressure     (station) float64 324.1 nan 122.8 nan\n2391 \n2392         We can fill in the missing values by passing a value to the keyword `fill_value`.\n2393 \n2394         >>> x.reindex({\"station\": new_index}, fill_value=0)\n2395         <xarray.Dataset>\n2396         Dimensions:      (station: 4)\n2397         Coordinates:\n2398         * station      (station) object 'boston' 'austin' 'seattle' 'lincoln'\n2399         Data variables:\n2400             temperature  (station) float64 18.84 0.0 19.22 0.0\n2401             pressure     (station) float64 324.1 0.0 122.8 0.0\n2402 \n2403         Because the index is not monotonically increasing or decreasing, we cannot use arguments\n2404         to the keyword method to fill the `NaN` values.\n2405 \n2406         >>> x.reindex({\"station\": new_index}, method=\"nearest\")\n2407         Traceback (most recent call last):\n2408         ...\n2409             raise ValueError('index must be monotonic increasing or decreasing')\n2410         ValueError: index must be monotonic increasing or decreasing\n2411 \n2412         To further illustrate the filling functionality in reindex, we will create a\n2413         dataset with a monotonically increasing index (for example, a sequence of dates).\n2414 \n2415         >>> x2 = xr.Dataset(\n2416         ...     {\n2417         ...         \"temperature\": (\n2418         ...             \"time\",\n2419         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],\n2420         ...         ),\n2421         ...         \"pressure\": (\"time\", 500 * np.random.rand(6)),\n2422         ...     },\n2423         ...     coords={\"time\": pd.date_range(\"01/01/2019\", periods=6, freq=\"D\")},\n2424         ... )\n2425         >>> x2\n2426         <xarray.Dataset>\n2427         Dimensions:      (time: 6)\n2428         Coordinates:\n2429         * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06\n2430         Data variables:\n2431             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12\n2432             pressure     (time) float64 103.4 122.7 452.0 444.0 399.2 486.0\n2433 \n2434         Suppose we decide to expand the dataset to cover a wider date range.\n2435 \n2436         >>> time_index2 = pd.date_range(\"12/29/2018\", periods=10, freq=\"D\")\n2437         >>> x2.reindex({\"time\": time_index2})\n2438         <xarray.Dataset>\n2439         Dimensions:      (time: 10)\n2440         Coordinates:\n2441         * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07\n2442         Data variables:\n2443             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan\n2444             pressure     (time) float64 nan nan nan 103.4 ... 444.0 399.2 486.0 nan\n2445 \n2446         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)\n2447         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.\n2448 \n2449         For example, to back-propagate the last valid value to fill the `NaN` values,\n2450         pass `bfill` as an argument to the `method` keyword.\n2451 \n2452         >>> x3 = x2.reindex({\"time\": time_index2}, method=\"bfill\")\n2453         >>> x3\n2454         <xarray.Dataset>\n2455         Dimensions:      (time: 10)\n2456         Coordinates:\n2457         * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07\n2458         Data variables:\n2459             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan\n2460             pressure     (time) float64 103.4 103.4 103.4 103.4 ... 399.2 486.0 nan\n2461 \n2462         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)\n2463         will not be filled by any of the value propagation schemes.\n2464 \n2465         >>> x2.where(x2.temperature.isnull(), drop=True)\n2466         <xarray.Dataset>\n2467         Dimensions:      (time: 1)\n2468         Coordinates:\n2469         * time         (time) datetime64[ns] 2019-01-03\n2470         Data variables:\n2471             temperature  (time) float64 nan\n2472             pressure     (time) float64 452.0\n2473         >>> x3.where(x3.temperature.isnull(), drop=True)\n2474         <xarray.Dataset>\n2475         Dimensions:      (time: 2)\n2476         Coordinates:\n2477         * time         (time) datetime64[ns] 2019-01-03 2019-01-07\n2478         Data variables:\n2479             temperature  (time) float64 nan nan\n2480             pressure     (time) float64 452.0 nan\n2481 \n2482         This is because filling while reindexing does not look at dataset values, but only compares\n2483         the original and desired indexes. If you do want to fill in the `NaN` values present in the\n2484         original dataset, use the :py:meth:`~Dataset.fillna()` method.\n2485 \n2486         \"\"\"\n2487         return self._reindex(\n2488             indexers,\n2489             method,\n2490             tolerance,\n2491             copy,\n2492             fill_value,\n2493             sparse=False,\n2494             **indexers_kwargs,\n2495         )\n2496 \n2497     def _reindex(\n2498         self,\n2499         indexers: Mapping[Hashable, Any] = None,\n2500         method: str = None,\n2501         tolerance: Number = None,\n2502         copy: bool = True,\n2503         fill_value: Any = dtypes.NA,\n2504         sparse: bool = False,\n2505         **indexers_kwargs: Any,\n2506     ) -> \"Dataset\":\n2507         \"\"\"\n2508         same to _reindex but support sparse option\n2509         \"\"\"\n2510         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, \"reindex\")\n2511 \n2512         bad_dims = [d for d in indexers if d not in self.dims]\n2513         if bad_dims:\n2514             raise ValueError(\"invalid reindex dimensions: %s\" % bad_dims)\n2515 \n2516         variables, indexes = alignment.reindex_variables(\n2517             self.variables,\n2518             self.sizes,\n2519             self.indexes,\n2520             indexers,\n2521             method,\n2522             tolerance,\n2523             copy=copy,\n2524             fill_value=fill_value,\n2525             sparse=sparse,\n2526         )\n2527         coord_names = set(self._coord_names)\n2528         coord_names.update(indexers)\n2529         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)\n2530 \n2531     def interp(\n2532         self,\n2533         coords: Mapping[Hashable, Any] = None,\n2534         method: str = \"linear\",\n2535         assume_sorted: bool = False,\n2536         kwargs: Mapping[str, Any] = None,\n2537         **coords_kwargs: Any,\n2538     ) -> \"Dataset\":\n2539         \"\"\" Multidimensional interpolation of Dataset.\n2540 \n2541         Parameters\n2542         ----------\n2543         coords : dict, optional\n2544             Mapping from dimension names to the new coordinates.\n2545             New coordinate can be a scalar, array-like or DataArray.\n2546             If DataArrays are passed as new coordates, their dimensions are\n2547             used for the broadcasting.\n2548         method: string, optional.\n2549             {'linear', 'nearest'} for multidimensional array,\n2550             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}\n2551             for 1-dimensional array. 'linear' is used by default.\n2552         assume_sorted: boolean, optional\n2553             If False, values of coordinates that are interpolated over can be\n2554             in any order and they are sorted first. If True, interpolated\n2555             coordinates are assumed to be an array of monotonically increasing\n2556             values.\n2557         kwargs: dictionary, optional\n2558             Additional keyword passed to scipy's interpolator.\n2559         **coords_kwargs : {dim: coordinate, ...}, optional\n2560             The keyword arguments form of ``coords``.\n2561             One of coords or coords_kwargs must be provided.\n2562 \n2563         Returns\n2564         -------\n2565         interpolated: xr.Dataset\n2566             New dataset on the new coordinates.\n2567 \n2568         Notes\n2569         -----\n2570         scipy is required.\n2571 \n2572         See Also\n2573         --------\n2574         scipy.interpolate.interp1d\n2575         scipy.interpolate.interpn\n2576         \"\"\"\n2577         from . import missing\n2578 \n2579         if kwargs is None:\n2580             kwargs = {}\n2581 \n2582         coords = either_dict_or_kwargs(coords, coords_kwargs, \"interp\")\n2583         indexers = dict(self._validate_interp_indexers(coords))\n2584 \n2585         if coords:\n2586             # This avoids broadcasting over coordinates that are both in\n2587             # the original array AND in the indexing array. It essentially\n2588             # forces interpolation along the shared coordinates.\n2589             sdims = (\n2590                 set(self.dims)\n2591                 .intersection(*[set(nx.dims) for nx in indexers.values()])\n2592                 .difference(coords.keys())\n2593             )\n2594             indexers.update({d: self.variables[d] for d in sdims})\n2595 \n2596         obj = self if assume_sorted else self.sortby([k for k in coords])\n2597 \n2598         def maybe_variable(obj, k):\n2599             # workaround to get variable for dimension without coordinate.\n2600             try:\n2601                 return obj._variables[k]\n2602             except KeyError:\n2603                 return as_variable((k, range(obj.dims[k])))\n2604 \n2605         def _validate_interp_indexer(x, new_x):\n2606             # In the case of datetimes, the restrictions placed on indexers\n2607             # used with interp are stronger than those which are placed on\n2608             # isel, so we need an additional check after _validate_indexers.\n2609             if _contains_datetime_like_objects(\n2610                 x\n2611             ) and not _contains_datetime_like_objects(new_x):\n2612                 raise TypeError(\n2613                     \"When interpolating over a datetime-like \"\n2614                     \"coordinate, the coordinates to \"\n2615                     \"interpolate to must be either datetime \"\n2616                     \"strings or datetimes. \"\n2617                     \"Instead got\\n{}\".format(new_x)\n2618                 )\n2619             return x, new_x\n2620 \n2621         variables: Dict[Hashable, Variable] = {}\n2622         for name, var in obj._variables.items():\n2623             if name in indexers:\n2624                 continue\n2625 \n2626             if var.dtype.kind in \"uifc\":\n2627                 var_indexers = {\n2628                     k: _validate_interp_indexer(maybe_variable(obj, k), v)\n2629                     for k, v in indexers.items()\n2630                     if k in var.dims\n2631                 }\n2632                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)\n2633             elif all(d not in indexers for d in var.dims):\n2634                 # keep unrelated object array\n2635                 variables[name] = var\n2636 \n2637         coord_names = obj._coord_names & variables.keys()\n2638         indexes = {k: v for k, v in obj.indexes.items() if k not in indexers}\n2639         selected = self._replace_with_new_dims(\n2640             variables.copy(), coord_names, indexes=indexes\n2641         )\n2642 \n2643         # attach indexer as coordinate\n2644         variables.update(indexers)\n2645         for k, v in indexers.items():\n2646             assert isinstance(v, Variable)\n2647             if v.dims == (k,):\n2648                 indexes[k] = v.to_index()\n2649 \n2650         # Extract coordinates from indexers\n2651         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)\n2652         variables.update(coord_vars)\n2653         indexes.update(new_indexes)\n2654 \n2655         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()\n2656         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)\n2657 \n2658     def interp_like(\n2659         self,\n2660         other: Union[\"Dataset\", \"DataArray\"],\n2661         method: str = \"linear\",\n2662         assume_sorted: bool = False,\n2663         kwargs: Mapping[str, Any] = None,\n2664     ) -> \"Dataset\":\n2665         \"\"\"Interpolate this object onto the coordinates of another object,\n2666         filling the out of range values with NaN.\n2667 \n2668         Parameters\n2669         ----------\n2670         other : Dataset or DataArray\n2671             Object with an 'indexes' attribute giving a mapping from dimension\n2672             names to an 1d array-like, which provides coordinates upon\n2673             which to index the variables in this dataset.\n2674         method: string, optional.\n2675             {'linear', 'nearest'} for multidimensional array,\n2676             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}\n2677             for 1-dimensional array. 'linear' is used by default.\n2678         assume_sorted: boolean, optional\n2679             If False, values of coordinates that are interpolated over can be\n2680             in any order and they are sorted first. If True, interpolated\n2681             coordinates are assumed to be an array of monotonically increasing\n2682             values.\n2683         kwargs: dictionary, optional\n2684             Additional keyword passed to scipy's interpolator.\n2685 \n2686         Returns\n2687         -------\n2688         interpolated: xr.Dataset\n2689             Another dataset by interpolating this dataset's data along the\n2690             coordinates of the other object.\n2691 \n2692         Notes\n2693         -----\n2694         scipy is required.\n2695         If the dataset has object-type coordinates, reindex is used for these\n2696         coordinates instead of the interpolation.\n2697 \n2698         See Also\n2699         --------\n2700         Dataset.interp\n2701         Dataset.reindex_like\n2702         \"\"\"\n2703         if kwargs is None:\n2704             kwargs = {}\n2705         coords = alignment.reindex_like_indexers(self, other)\n2706 \n2707         numeric_coords: Dict[Hashable, pd.Index] = {}\n2708         object_coords: Dict[Hashable, pd.Index] = {}\n2709         for k, v in coords.items():\n2710             if v.dtype.kind in \"uifcMm\":\n2711                 numeric_coords[k] = v\n2712             else:\n2713                 object_coords[k] = v\n2714 \n2715         ds = self\n2716         if object_coords:\n2717             # We do not support interpolation along object coordinate.\n2718             # reindex instead.\n2719             ds = self.reindex(object_coords)\n2720         return ds.interp(numeric_coords, method, assume_sorted, kwargs)\n2721 \n2722     # Helper methods for rename()\n2723     def _rename_vars(self, name_dict, dims_dict):\n2724         variables = {}\n2725         coord_names = set()\n2726         for k, v in self.variables.items():\n2727             var = v.copy(deep=False)\n2728             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)\n2729             name = name_dict.get(k, k)\n2730             if name in variables:\n2731                 raise ValueError(f\"the new name {name!r} conflicts\")\n2732             variables[name] = var\n2733             if k in self._coord_names:\n2734                 coord_names.add(name)\n2735         return variables, coord_names\n2736 \n2737     def _rename_dims(self, name_dict):\n2738         return {name_dict.get(k, k): v for k, v in self.dims.items()}\n2739 \n2740     def _rename_indexes(self, name_dict, dims_set):\n2741         if self._indexes is None:\n2742             return None\n2743         indexes = {}\n2744         for k, v in self.indexes.items():\n2745             new_name = name_dict.get(k, k)\n2746             if new_name not in dims_set:\n2747                 continue\n2748             if isinstance(v, pd.MultiIndex):\n2749                 new_names = [name_dict.get(k, k) for k in v.names]\n2750                 index = v.rename(names=new_names)\n2751             else:\n2752                 index = v.rename(new_name)\n2753             indexes[new_name] = index\n2754         return indexes\n2755 \n2756     def _rename_all(self, name_dict, dims_dict):\n2757         variables, coord_names = self._rename_vars(name_dict, dims_dict)\n2758         dims = self._rename_dims(dims_dict)\n2759         indexes = self._rename_indexes(name_dict, dims.keys())\n2760         return variables, coord_names, dims, indexes\n2761 \n2762     def rename(\n2763         self,\n2764         name_dict: Mapping[Hashable, Hashable] = None,\n2765         inplace: bool = None,\n2766         **names: Hashable,\n2767     ) -> \"Dataset\":\n2768         \"\"\"Returns a new object with renamed variables and dimensions.\n2769 \n2770         Parameters\n2771         ----------\n2772         name_dict : dict-like, optional\n2773             Dictionary whose keys are current variable or dimension names and\n2774             whose values are the desired names.\n2775         **names, optional\n2776             Keyword form of ``name_dict``.\n2777             One of name_dict or names must be provided.\n2778 \n2779         Returns\n2780         -------\n2781         renamed : Dataset\n2782             Dataset with renamed variables and dimensions.\n2783 \n2784         See Also\n2785         --------\n2786         Dataset.swap_dims\n2787         Dataset.rename_vars\n2788         Dataset.rename_dims\n2789         DataArray.rename\n2790         \"\"\"\n2791         _check_inplace(inplace)\n2792         name_dict = either_dict_or_kwargs(name_dict, names, \"rename\")\n2793         for k in name_dict.keys():\n2794             if k not in self and k not in self.dims:\n2795                 raise ValueError(\n2796                     \"cannot rename %r because it is not a \"\n2797                     \"variable or dimension in this dataset\" % k\n2798                 )\n2799 \n2800         variables, coord_names, dims, indexes = self._rename_all(\n2801             name_dict=name_dict, dims_dict=name_dict\n2802         )\n2803         assert_unique_multiindex_level_names(variables)\n2804         return self._replace(variables, coord_names, dims=dims, indexes=indexes)\n2805 \n2806     def rename_dims(\n2807         self, dims_dict: Mapping[Hashable, Hashable] = None, **dims: Hashable\n2808     ) -> \"Dataset\":\n2809         \"\"\"Returns a new object with renamed dimensions only.\n2810 \n2811         Parameters\n2812         ----------\n2813         dims_dict : dict-like, optional\n2814             Dictionary whose keys are current dimension names and\n2815             whose values are the desired names. The desired names must\n2816             not be the name of an existing dimension or Variable in the Dataset.\n2817         **dims, optional\n2818             Keyword form of ``dims_dict``.\n2819             One of dims_dict or dims must be provided.\n2820 \n2821         Returns\n2822         -------\n2823         renamed : Dataset\n2824             Dataset with renamed dimensions.\n2825 \n2826         See Also\n2827         --------\n2828         Dataset.swap_dims\n2829         Dataset.rename\n2830         Dataset.rename_vars\n2831         DataArray.rename\n2832         \"\"\"\n2833         dims_dict = either_dict_or_kwargs(dims_dict, dims, \"rename_dims\")\n2834         for k, v in dims_dict.items():\n2835             if k not in self.dims:\n2836                 raise ValueError(\n2837                     \"cannot rename %r because it is not a \"\n2838                     \"dimension in this dataset\" % k\n2839                 )\n2840             if v in self.dims or v in self:\n2841                 raise ValueError(\n2842                     f\"Cannot rename {k} to {v} because {v} already exists. \"\n2843                     \"Try using swap_dims instead.\"\n2844                 )\n2845 \n2846         variables, coord_names, sizes, indexes = self._rename_all(\n2847             name_dict={}, dims_dict=dims_dict\n2848         )\n2849         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)\n2850 \n2851     def rename_vars(\n2852         self, name_dict: Mapping[Hashable, Hashable] = None, **names: Hashable\n2853     ) -> \"Dataset\":\n2854         \"\"\"Returns a new object with renamed variables including coordinates\n2855 \n2856         Parameters\n2857         ----------\n2858         name_dict : dict-like, optional\n2859             Dictionary whose keys are current variable or coordinate names and\n2860             whose values are the desired names.\n2861         **names, optional\n2862             Keyword form of ``name_dict``.\n2863             One of name_dict or names must be provided.\n2864 \n2865         Returns\n2866         -------\n2867         renamed : Dataset\n2868             Dataset with renamed variables including coordinates\n2869 \n2870         See Also\n2871         --------\n2872         Dataset.swap_dims\n2873         Dataset.rename\n2874         Dataset.rename_dims\n2875         DataArray.rename\n2876         \"\"\"\n2877         name_dict = either_dict_or_kwargs(name_dict, names, \"rename_vars\")\n2878         for k in name_dict:\n2879             if k not in self:\n2880                 raise ValueError(\n2881                     \"cannot rename %r because it is not a \"\n2882                     \"variable or coordinate in this dataset\" % k\n2883                 )\n2884         variables, coord_names, dims, indexes = self._rename_all(\n2885             name_dict=name_dict, dims_dict={}\n2886         )\n2887         return self._replace(variables, coord_names, dims=dims, indexes=indexes)\n2888 \n2889     def swap_dims(\n2890         self, dims_dict: Mapping[Hashable, Hashable], inplace: bool = None\n2891     ) -> \"Dataset\":\n2892         \"\"\"Returns a new object with swapped dimensions.\n2893 \n2894         Parameters\n2895         ----------\n2896         dims_dict : dict-like\n2897             Dictionary whose keys are current dimension names and whose values\n2898             are new names.\n2899 \n2900         Returns\n2901         -------\n2902         swapped : Dataset\n2903             Dataset with swapped dimensions.\n2904 \n2905         Examples\n2906         --------\n2907         >>> ds = xr.Dataset(\n2908         ...     data_vars={\"a\": (\"x\", [5, 7]), \"b\": (\"x\", [0.1, 2.4])},\n2909         ...     coords={\"x\": [\"a\", \"b\"], \"y\": (\"x\", [0, 1])},\n2910         ... )\n2911         >>> ds\n2912         <xarray.Dataset>\n2913         Dimensions:  (x: 2)\n2914         Coordinates:\n2915           * x        (x) <U1 'a' 'b'\n2916             y        (x) int64 0 1\n2917         Data variables:\n2918             a        (x) int64 5 7\n2919             b        (x) float64 0.1 2.4\n2920 \n2921         >>> ds.swap_dims({\"x\": \"y\"})\n2922         <xarray.Dataset>\n2923         Dimensions:  (y: 2)\n2924         Coordinates:\n2925             x        (y) <U1 'a' 'b'\n2926           * y        (y) int64 0 1\n2927         Data variables:\n2928             a        (y) int64 5 7\n2929             b        (y) float64 0.1 2.4\n2930 \n2931         >>> ds.swap_dims({\"x\": \"z\"})\n2932         <xarray.Dataset>\n2933         Dimensions:  (z: 2)\n2934         Coordinates:\n2935             x        (z) <U1 'a' 'b'\n2936             y        (z) int64 0 1\n2937         Dimensions without coordinates: z\n2938         Data variables:\n2939             a        (z) int64 5 7\n2940             b        (z) float64 0.1 2.4\n2941 \n2942         See Also\n2943         --------\n2944 \n2945         Dataset.rename\n2946         DataArray.swap_dims\n2947         \"\"\"\n2948         # TODO: deprecate this method in favor of a (less confusing)\n2949         # rename_dims() method that only renames dimensions.\n2950         _check_inplace(inplace)\n2951         for k, v in dims_dict.items():\n2952             if k not in self.dims:\n2953                 raise ValueError(\n2954                     \"cannot swap from dimension %r because it is \"\n2955                     \"not an existing dimension\" % k\n2956                 )\n2957             if v in self.variables and self.variables[v].dims != (k,):\n2958                 raise ValueError(\n2959                     \"replacement dimension %r is not a 1D \"\n2960                     \"variable along the old dimension %r\" % (v, k)\n2961                 )\n2962 \n2963         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}\n2964 \n2965         coord_names = self._coord_names.copy()\n2966         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})\n2967 \n2968         variables: Dict[Hashable, Variable] = {}\n2969         indexes: Dict[Hashable, pd.Index] = {}\n2970         for k, v in self.variables.items():\n2971             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)\n2972             if k in result_dims:\n2973                 var = v.to_index_variable()\n2974                 if k in self.indexes:\n2975                     indexes[k] = self.indexes[k]\n2976                 else:\n2977                     new_index = var.to_index()\n2978                     if new_index.nlevels == 1:\n2979                         # make sure index name matches dimension name\n2980                         new_index = new_index.rename(k)\n2981                     indexes[k] = new_index\n2982             else:\n2983                 var = v.to_base_variable()\n2984             var.dims = dims\n2985             variables[k] = var\n2986 \n2987         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)\n2988 \n2989     def expand_dims(\n2990         self,\n2991         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,\n2992         axis: Union[None, int, Sequence[int]] = None,\n2993         **dim_kwargs: Any,\n2994     ) -> \"Dataset\":\n2995         \"\"\"Return a new object with an additional axis (or axes) inserted at\n2996         the corresponding position in the array shape.  The new object is a\n2997         view into the underlying array, not a copy.\n2998 \n2999         If dim is already a scalar coordinate, it will be promoted to a 1D\n3000         coordinate consisting of a single value.\n3001 \n3002         Parameters\n3003         ----------\n3004         dim : hashable, sequence of hashable, mapping, or None\n3005             Dimensions to include on the new variable. If provided as hashable\n3006             or sequence of hashable, then dimensions are inserted with length\n3007             1. If provided as a mapping, then the keys are the new dimensions\n3008             and the values are either integers (giving the length of the new\n3009             dimensions) or array-like (giving the coordinates of the new\n3010             dimensions).\n3011         axis : integer, sequence of integers, or None\n3012             Axis position(s) where new axis is to be inserted (position(s) on\n3013             the result array). If a list (or tuple) of integers is passed,\n3014             multiple axes are inserted. In this case, dim arguments should be\n3015             same length list. If axis=None is passed, all the axes will be\n3016             inserted to the start of the result array.\n3017         **dim_kwargs : int or sequence/ndarray\n3018             The keywords are arbitrary dimensions being inserted and the values\n3019             are either the lengths of the new dims (if int is given), or their\n3020             coordinates. Note, this is an alternative to passing a dict to the\n3021             dim kwarg and will only be used if dim is None.\n3022 \n3023         Returns\n3024         -------\n3025         expanded : same type as caller\n3026             This object, but with an additional dimension(s).\n3027         \"\"\"\n3028         if dim is None:\n3029             pass\n3030         elif isinstance(dim, Mapping):\n3031             # We're later going to modify dim in place; don't tamper with\n3032             # the input\n3033             dim = dict(dim)\n3034         elif isinstance(dim, int):\n3035             raise TypeError(\n3036                 \"dim should be hashable or sequence of hashables or mapping\"\n3037             )\n3038         elif isinstance(dim, str) or not isinstance(dim, Sequence):\n3039             dim = {dim: 1}\n3040         elif isinstance(dim, Sequence):\n3041             if len(dim) != len(set(dim)):\n3042                 raise ValueError(\"dims should not contain duplicate values.\")\n3043             dim = {d: 1 for d in dim}\n3044 \n3045         dim = either_dict_or_kwargs(dim, dim_kwargs, \"expand_dims\")\n3046         assert isinstance(dim, MutableMapping)\n3047 \n3048         if axis is None:\n3049             axis = list(range(len(dim)))\n3050         elif not isinstance(axis, Sequence):\n3051             axis = [axis]\n3052 \n3053         if len(dim) != len(axis):\n3054             raise ValueError(\"lengths of dim and axis should be identical.\")\n3055         for d in dim:\n3056             if d in self.dims:\n3057                 raise ValueError(f\"Dimension {d} already exists.\")\n3058             if d in self._variables and not utils.is_scalar(self._variables[d]):\n3059                 raise ValueError(\n3060                     \"{dim} already exists as coordinate or\"\n3061                     \" variable name.\".format(dim=d)\n3062                 )\n3063 \n3064         variables: Dict[Hashable, Variable] = {}\n3065         coord_names = self._coord_names.copy()\n3066         # If dim is a dict, then ensure that the values are either integers\n3067         # or iterables.\n3068         for k, v in dim.items():\n3069             if hasattr(v, \"__iter__\"):\n3070                 # If the value for the new dimension is an iterable, then\n3071                 # save the coordinates to the variables dict, and set the\n3072                 # value within the dim dict to the length of the iterable\n3073                 # for later use.\n3074                 variables[k] = xr.IndexVariable((k,), v)\n3075                 coord_names.add(k)\n3076                 dim[k] = variables[k].size\n3077             elif isinstance(v, int):\n3078                 pass  # Do nothing if the dimensions value is just an int\n3079             else:\n3080                 raise TypeError(\n3081                     \"The value of new dimension {k} must be \"\n3082                     \"an iterable or an int\".format(k=k)\n3083                 )\n3084 \n3085         for k, v in self._variables.items():\n3086             if k not in dim:\n3087                 if k in coord_names:  # Do not change coordinates\n3088                     variables[k] = v\n3089                 else:\n3090                     result_ndim = len(v.dims) + len(axis)\n3091                     for a in axis:\n3092                         if a < -result_ndim or result_ndim - 1 < a:\n3093                             raise IndexError(\n3094                                 f\"Axis {a} of variable {k} is out of bounds of the \"\n3095                                 f\"expanded dimension size {result_ndim}\"\n3096                             )\n3097 \n3098                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]\n3099                     if len(axis_pos) != len(set(axis_pos)):\n3100                         raise ValueError(\"axis should not contain duplicate values\")\n3101                     # We need to sort them to make sure `axis` equals to the\n3102                     # axis positions of the result array.\n3103                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))\n3104 \n3105                     all_dims = list(zip(v.dims, v.shape))\n3106                     for d, c in zip_axis_dim:\n3107                         all_dims.insert(d, c)\n3108                     variables[k] = v.set_dims(dict(all_dims))\n3109             else:\n3110                 # If dims includes a label of a non-dimension coordinate,\n3111                 # it will be promoted to a 1D coordinate with a single value.\n3112                 variables[k] = v.set_dims(k).to_index_variable()\n3113 \n3114         new_dims = self._dims.copy()\n3115         new_dims.update(dim)\n3116 \n3117         return self._replace_vars_and_dims(\n3118             variables, dims=new_dims, coord_names=coord_names\n3119         )\n3120 \n3121     def set_index(\n3122         self,\n3123         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,\n3124         append: bool = False,\n3125         inplace: bool = None,\n3126         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],\n3127     ) -> \"Dataset\":\n3128         \"\"\"Set Dataset (multi-)indexes using one or more existing coordinates\n3129         or variables.\n3130 \n3131         Parameters\n3132         ----------\n3133         indexes : {dim: index, ...}\n3134             Mapping from names matching dimensions and values given\n3135             by (lists of) the names of existing coordinates or variables to set\n3136             as new (multi-)index.\n3137         append : bool, optional\n3138             If True, append the supplied index(es) to the existing index(es).\n3139             Otherwise replace the existing index(es) (default).\n3140         **indexes_kwargs: optional\n3141             The keyword arguments form of ``indexes``.\n3142             One of indexes or indexes_kwargs must be provided.\n3143 \n3144         Returns\n3145         -------\n3146         obj : Dataset\n3147             Another dataset, with this dataset's data but replaced coordinates.\n3148 \n3149         Examples\n3150         --------\n3151         >>> arr = xr.DataArray(\n3152         ...     data=np.ones((2, 3)),\n3153         ...     dims=[\"x\", \"y\"],\n3154         ...     coords={\"x\": range(2), \"y\": range(3), \"a\": (\"x\", [3, 4])},\n3155         ... )\n3156         >>> ds = xr.Dataset({\"v\": arr})\n3157         >>> ds\n3158         <xarray.Dataset>\n3159         Dimensions:  (x: 2, y: 3)\n3160         Coordinates:\n3161           * x        (x) int64 0 1\n3162           * y        (y) int64 0 1 2\n3163             a        (x) int64 3 4\n3164         Data variables:\n3165             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0\n3166         >>> ds.set_index(x=\"a\")\n3167         <xarray.Dataset>\n3168         Dimensions:  (x: 2, y: 3)\n3169         Coordinates:\n3170           * x        (x) int64 3 4\n3171           * y        (y) int64 0 1 2\n3172         Data variables:\n3173             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0\n3174 \n3175         See Also\n3176         --------\n3177         Dataset.reset_index\n3178         Dataset.swap_dims\n3179         \"\"\"\n3180         _check_inplace(inplace)\n3181         indexes = either_dict_or_kwargs(indexes, indexes_kwargs, \"set_index\")\n3182         variables, coord_names = merge_indexes(\n3183             indexes, self._variables, self._coord_names, append=append\n3184         )\n3185         return self._replace_vars_and_dims(variables, coord_names=coord_names)\n3186 \n3187     def reset_index(\n3188         self,\n3189         dims_or_levels: Union[Hashable, Sequence[Hashable]],\n3190         drop: bool = False,\n3191         inplace: bool = None,\n3192     ) -> \"Dataset\":\n3193         \"\"\"Reset the specified index(es) or multi-index level(s).\n3194 \n3195         Parameters\n3196         ----------\n3197         dims_or_levels : str or list\n3198             Name(s) of the dimension(s) and/or multi-index level(s) that will\n3199             be reset.\n3200         drop : bool, optional\n3201             If True, remove the specified indexes and/or multi-index levels\n3202             instead of extracting them as new coordinates (default: False).\n3203 \n3204         Returns\n3205         -------\n3206         obj : Dataset\n3207             Another dataset, with this dataset's data but replaced coordinates.\n3208 \n3209         See Also\n3210         --------\n3211         Dataset.set_index\n3212         \"\"\"\n3213         _check_inplace(inplace)\n3214         variables, coord_names = split_indexes(\n3215             dims_or_levels,\n3216             self._variables,\n3217             self._coord_names,\n3218             cast(Mapping[Hashable, Hashable], self._level_coords),\n3219             drop=drop,\n3220         )\n3221         return self._replace_vars_and_dims(variables, coord_names=coord_names)\n3222 \n3223     def reorder_levels(\n3224         self,\n3225         dim_order: Mapping[Hashable, Sequence[int]] = None,\n3226         inplace: bool = None,\n3227         **dim_order_kwargs: Sequence[int],\n3228     ) -> \"Dataset\":\n3229         \"\"\"Rearrange index levels using input order.\n3230 \n3231         Parameters\n3232         ----------\n3233         dim_order : optional\n3234             Mapping from names matching dimensions and values given\n3235             by lists representing new level orders. Every given dimension\n3236             must have a multi-index.\n3237         **dim_order_kwargs: optional\n3238             The keyword arguments form of ``dim_order``.\n3239             One of dim_order or dim_order_kwargs must be provided.\n3240 \n3241         Returns\n3242         -------\n3243         obj : Dataset\n3244             Another dataset, with this dataset's data but replaced\n3245             coordinates.\n3246         \"\"\"\n3247         _check_inplace(inplace)\n3248         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, \"reorder_levels\")\n3249         variables = self._variables.copy()\n3250         indexes = dict(self.indexes)\n3251         for dim, order in dim_order.items():\n3252             coord = self._variables[dim]\n3253             index = self.indexes[dim]\n3254             if not isinstance(index, pd.MultiIndex):\n3255                 raise ValueError(f\"coordinate {dim} has no MultiIndex\")\n3256             new_index = index.reorder_levels(order)\n3257             variables[dim] = IndexVariable(coord.dims, new_index)\n3258             indexes[dim] = new_index\n3259 \n3260         return self._replace(variables, indexes=indexes)\n3261 \n3262     def _stack_once(self, dims, new_dim):\n3263         variables = {}\n3264         for name, var in self.variables.items():\n3265             if name not in dims:\n3266                 if any(d in var.dims for d in dims):\n3267                     add_dims = [d for d in dims if d not in var.dims]\n3268                     vdims = list(var.dims) + add_dims\n3269                     shape = [self.dims[d] for d in vdims]\n3270                     exp_var = var.set_dims(vdims, shape)\n3271                     stacked_var = exp_var.stack(**{new_dim: dims})\n3272                     variables[name] = stacked_var\n3273                 else:\n3274                     variables[name] = var.copy(deep=False)\n3275 \n3276         # consider dropping levels that are unused?\n3277         levels = [self.get_index(dim) for dim in dims]\n3278         idx = utils.multiindex_from_product_levels(levels, names=dims)\n3279         variables[new_dim] = IndexVariable(new_dim, idx)\n3280 \n3281         coord_names = set(self._coord_names) - set(dims) | {new_dim}\n3282 \n3283         indexes = {k: v for k, v in self.indexes.items() if k not in dims}\n3284         indexes[new_dim] = idx\n3285 \n3286         return self._replace_with_new_dims(\n3287             variables, coord_names=coord_names, indexes=indexes\n3288         )\n3289 \n3290     def stack(\n3291         self,\n3292         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,\n3293         **dimensions_kwargs: Sequence[Hashable],\n3294     ) -> \"Dataset\":\n3295         \"\"\"\n3296         Stack any number of existing dimensions into a single new dimension.\n3297 \n3298         New dimensions will be added at the end, and the corresponding\n3299         coordinate variables will be combined into a MultiIndex.\n3300 \n3301         Parameters\n3302         ----------\n3303         dimensions : Mapping of the form new_name=(dim1, dim2, ...)\n3304             Names of new dimensions, and the existing dimensions that they\n3305             replace.\n3306         **dimensions_kwargs:\n3307             The keyword arguments form of ``dimensions``.\n3308             One of dimensions or dimensions_kwargs must be provided.\n3309 \n3310         Returns\n3311         -------\n3312         stacked : Dataset\n3313             Dataset with stacked data.\n3314 \n3315         See also\n3316         --------\n3317         Dataset.unstack\n3318         \"\"\"\n3319         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, \"stack\")\n3320         result = self\n3321         for new_dim, dims in dimensions.items():\n3322             result = result._stack_once(dims, new_dim)\n3323         return result\n3324 \n3325     def to_stacked_array(\n3326         self,\n3327         new_dim: Hashable,\n3328         sample_dims: Sequence[Hashable],\n3329         variable_dim: str = \"variable\",\n3330         name: Hashable = None,\n3331     ) -> \"DataArray\":\n3332         \"\"\"Combine variables of differing dimensionality into a DataArray\n3333         without broadcasting.\n3334 \n3335         This method is similar to Dataset.to_array but does not broadcast the\n3336         variables.\n3337 \n3338         Parameters\n3339         ----------\n3340         new_dim : Hashable\n3341             Name of the new stacked coordinate\n3342         sample_dims : Sequence[Hashable]\n3343             Dimensions that **will not** be stacked. Each array in the dataset\n3344             must share these dimensions. For machine learning applications,\n3345             these define the dimensions over which samples are drawn.\n3346         variable_dim : str, optional\n3347             Name of the level in the stacked coordinate which corresponds to\n3348             the variables.\n3349         name : str, optional\n3350             Name of the new data array.\n3351 \n3352         Returns\n3353         -------\n3354         stacked : DataArray\n3355             DataArray with the specified dimensions and data variables\n3356             stacked together. The stacked coordinate is named ``new_dim``\n3357             and represented by a MultiIndex object with a level containing the\n3358             data variable names. The name of this level is controlled using\n3359             the ``variable_dim`` argument.\n3360 \n3361         See Also\n3362         --------\n3363         Dataset.to_array\n3364         Dataset.stack\n3365         DataArray.to_unstacked_dataset\n3366 \n3367         Examples\n3368         --------\n3369         >>> data = xr.Dataset(\n3370         ...     data_vars={\n3371         ...         \"a\": ((\"x\", \"y\"), [[0, 1, 2], [3, 4, 5]]),\n3372         ...         \"b\": (\"x\", [6, 7]),\n3373         ...     },\n3374         ...     coords={\"y\": [\"u\", \"v\", \"w\"]},\n3375         ... )\n3376 \n3377         >>> data\n3378         <xarray.Dataset>\n3379         Dimensions:  (x: 2, y: 3)\n3380         Coordinates:\n3381         * y        (y) <U1 'u' 'v' 'w'\n3382         Dimensions without coordinates: x\n3383         Data variables:\n3384             a        (x, y) int64 0 1 2 3 4 5\n3385             b        (x) int64 6 7\n3386 \n3387         >>> data.to_stacked_array(\"z\", sample_dims=[\"x\"])\n3388         <xarray.DataArray (x: 2, z: 4)>\n3389         array([[0, 1, 2, 6],\n3390             [3, 4, 5, 7]])\n3391         Coordinates:\n3392         * z         (z) MultiIndex\n3393         - variable  (z) object 'a' 'a' 'a' 'b'\n3394         - y         (z) object 'u' 'v' 'w' nan\n3395         Dimensions without coordinates: x\n3396 \n3397         \"\"\"\n3398         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)\n3399 \n3400         for variable in self:\n3401             dims = self[variable].dims\n3402             dims_include_sample_dims = set(sample_dims) <= set(dims)\n3403             if not dims_include_sample_dims:\n3404                 raise ValueError(\n3405                     \"All variables in the dataset must contain the \"\n3406                     \"dimensions {}.\".format(dims)\n3407                 )\n3408 \n3409         def ensure_stackable(val):\n3410             assign_coords = {variable_dim: val.name}\n3411             for dim in stacking_dims:\n3412                 if dim not in val.dims:\n3413                     assign_coords[dim] = None\n3414 \n3415             expand_dims = set(stacking_dims).difference(set(val.dims))\n3416             expand_dims.add(variable_dim)\n3417             # must be list for .expand_dims\n3418             expand_dims = list(expand_dims)\n3419 \n3420             return (\n3421                 val.assign_coords(**assign_coords)\n3422                 .expand_dims(expand_dims)\n3423                 .stack({new_dim: (variable_dim,) + stacking_dims})\n3424             )\n3425 \n3426         # concatenate the arrays\n3427         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]\n3428         data_array = xr.concat(stackable_vars, dim=new_dim)\n3429 \n3430         # coerce the levels of the MultiIndex to have the same type as the\n3431         # input dimensions. This code is messy, so it might be better to just\n3432         # input a dummy value for the singleton dimension.\n3433         idx = data_array.indexes[new_dim]\n3434         levels = [idx.levels[0]] + [\n3435             level.astype(self[level.name].dtype) for level in idx.levels[1:]\n3436         ]\n3437         new_idx = idx.set_levels(levels)\n3438         data_array[new_dim] = IndexVariable(new_dim, new_idx)\n3439 \n3440         if name is not None:\n3441             data_array.name = name\n3442 \n3443         return data_array\n3444 \n3445     def _unstack_once(self, dim: Hashable, fill_value, sparse) -> \"Dataset\":\n3446         index = self.get_index(dim)\n3447         index = remove_unused_levels_categories(index)\n3448         full_idx = pd.MultiIndex.from_product(index.levels, names=index.names)\n3449 \n3450         # take a shortcut in case the MultiIndex was not modified.\n3451         if index.equals(full_idx):\n3452             obj = self\n3453         else:\n3454             obj = self._reindex(\n3455                 {dim: full_idx}, copy=False, fill_value=fill_value, sparse=sparse\n3456             )\n3457 \n3458         new_dim_names = index.names\n3459         new_dim_sizes = [lev.size for lev in index.levels]\n3460 \n3461         variables: Dict[Hashable, Variable] = {}\n3462         indexes = {k: v for k, v in self.indexes.items() if k != dim}\n3463 \n3464         for name, var in obj.variables.items():\n3465             if name != dim:\n3466                 if dim in var.dims:\n3467                     new_dims = dict(zip(new_dim_names, new_dim_sizes))\n3468                     variables[name] = var.unstack({dim: new_dims})\n3469                 else:\n3470                     variables[name] = var\n3471 \n3472         for name, lev in zip(new_dim_names, index.levels):\n3473             variables[name] = IndexVariable(name, lev)\n3474             indexes[name] = lev\n3475 \n3476         coord_names = set(self._coord_names) - {dim} | set(new_dim_names)\n3477 \n3478         return self._replace_with_new_dims(\n3479             variables, coord_names=coord_names, indexes=indexes\n3480         )\n3481 \n3482     def unstack(\n3483         self,\n3484         dim: Union[Hashable, Iterable[Hashable]] = None,\n3485         fill_value: Any = dtypes.NA,\n3486         sparse: bool = False,\n3487     ) -> \"Dataset\":\n3488         \"\"\"\n3489         Unstack existing dimensions corresponding to MultiIndexes into\n3490         multiple new dimensions.\n3491 \n3492         New dimensions will be added at the end.\n3493 \n3494         Parameters\n3495         ----------\n3496         dim : Hashable or iterable of Hashable, optional\n3497             Dimension(s) over which to unstack. By default unstacks all\n3498             MultiIndexes.\n3499         fill_value: value to be filled. By default, np.nan\n3500         sparse: use sparse-array if True\n3501 \n3502         Returns\n3503         -------\n3504         unstacked : Dataset\n3505             Dataset with unstacked data.\n3506 \n3507         See also\n3508         --------\n3509         Dataset.stack\n3510         \"\"\"\n3511         if dim is None:\n3512             dims = [\n3513                 d for d in self.dims if isinstance(self.get_index(d), pd.MultiIndex)\n3514             ]\n3515         else:\n3516             if isinstance(dim, str) or not isinstance(dim, Iterable):\n3517                 dims = [dim]\n3518             else:\n3519                 dims = list(dim)\n3520 \n3521             missing_dims = [d for d in dims if d not in self.dims]\n3522             if missing_dims:\n3523                 raise ValueError(\n3524                     \"Dataset does not contain the dimensions: %s\" % missing_dims\n3525                 )\n3526 \n3527             non_multi_dims = [\n3528                 d for d in dims if not isinstance(self.get_index(d), pd.MultiIndex)\n3529             ]\n3530             if non_multi_dims:\n3531                 raise ValueError(\n3532                     \"cannot unstack dimensions that do not \"\n3533                     \"have a MultiIndex: %s\" % non_multi_dims\n3534                 )\n3535 \n3536         result = self.copy(deep=False)\n3537         for dim in dims:\n3538             result = result._unstack_once(dim, fill_value, sparse)\n3539         return result\n3540 \n3541     def update(self, other: \"CoercibleMapping\", inplace: bool = None) -> \"Dataset\":\n3542         \"\"\"Update this dataset's variables with those from another dataset.\n3543 \n3544         Parameters\n3545         ----------\n3546         other : Dataset or castable to Dataset\n3547             Variables with which to update this dataset. One of:\n3548 \n3549             - Dataset\n3550             - mapping {var name: DataArray}\n3551             - mapping {var name: Variable}\n3552             - mapping {var name: (dimension name, array-like)}\n3553             - mapping {var name: (tuple of dimension names, array-like)}\n3554 \n3555 \n3556         Returns\n3557         -------\n3558         updated : Dataset\n3559             Updated dataset.\n3560 \n3561         Raises\n3562         ------\n3563         ValueError\n3564             If any dimensions would have inconsistent sizes in the updated\n3565             dataset.\n3566         \"\"\"\n3567         _check_inplace(inplace)\n3568         merge_result = dataset_update_method(self, other)\n3569         return self._replace(inplace=True, **merge_result._asdict())\n3570 \n3571     def merge(\n3572         self,\n3573         other: Union[\"CoercibleMapping\", \"DataArray\"],\n3574         inplace: bool = None,\n3575         overwrite_vars: Union[Hashable, Iterable[Hashable]] = frozenset(),\n3576         compat: str = \"no_conflicts\",\n3577         join: str = \"outer\",\n3578         fill_value: Any = dtypes.NA,\n3579     ) -> \"Dataset\":\n3580         \"\"\"Merge the arrays of two datasets into a single dataset.\n3581 \n3582         This method generally does not allow for overriding data, with the\n3583         exception of attributes, which are ignored on the second dataset.\n3584         Variables with the same name are checked for conflicts via the equals\n3585         or identical methods.\n3586 \n3587         Parameters\n3588         ----------\n3589         other : Dataset or castable to Dataset\n3590             Dataset or variables to merge with this dataset.\n3591         overwrite_vars : Hashable or iterable of Hashable, optional\n3592             If provided, update variables of these name(s) without checking for\n3593             conflicts in this dataset.\n3594         compat : {'broadcast_equals', 'equals', 'identical',\n3595                   'no_conflicts'}, optional\n3596             String indicating how to compare variables of the same name for\n3597             potential conflicts:\n3598 \n3599             - 'broadcast_equals': all values must be equal when variables are\n3600               broadcast against each other to ensure common dimensions.\n3601             - 'equals': all values and dimensions must be the same.\n3602             - 'identical': all values, dimensions and attributes must be the\n3603               same.\n3604             - 'no_conflicts': only values which are not null in both datasets\n3605               must be equal. The returned dataset then contains the combination\n3606               of all non-null values.\n3607 \n3608         join : {'outer', 'inner', 'left', 'right', 'exact'}, optional\n3609             Method for joining ``self`` and ``other`` along shared dimensions:\n3610 \n3611             - 'outer': use the union of the indexes\n3612             - 'inner': use the intersection of the indexes\n3613             - 'left': use indexes from ``self``\n3614             - 'right': use indexes from ``other``\n3615             - 'exact': error instead of aligning non-equal indexes\n3616         fill_value: scalar, optional\n3617             Value to use for newly missing values\n3618 \n3619         Returns\n3620         -------\n3621         merged : Dataset\n3622             Merged dataset.\n3623 \n3624         Raises\n3625         ------\n3626         MergeError\n3627             If any variables conflict (see ``compat``).\n3628         \"\"\"\n3629         _check_inplace(inplace)\n3630         other = other.to_dataset() if isinstance(other, xr.DataArray) else other\n3631         merge_result = dataset_merge_method(\n3632             self,\n3633             other,\n3634             overwrite_vars=overwrite_vars,\n3635             compat=compat,\n3636             join=join,\n3637             fill_value=fill_value,\n3638         )\n3639         return self._replace(**merge_result._asdict())\n3640 \n3641     def _assert_all_in_dataset(\n3642         self, names: Iterable[Hashable], virtual_okay: bool = False\n3643     ) -> None:\n3644         bad_names = set(names) - set(self._variables)\n3645         if virtual_okay:\n3646             bad_names -= self.virtual_variables\n3647         if bad_names:\n3648             raise ValueError(\n3649                 \"One or more of the specified variables \"\n3650                 \"cannot be found in this dataset\"\n3651             )\n3652 \n3653     def drop_vars(\n3654         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = \"raise\"\n3655     ) -> \"Dataset\":\n3656         \"\"\"Drop variables from this dataset.\n3657 \n3658         Parameters\n3659         ----------\n3660         names : hashable or iterable of hashables\n3661             Name(s) of variables to drop.\n3662         errors: {'raise', 'ignore'}, optional\n3663             If 'raise' (default), raises a ValueError error if any of the variable\n3664             passed are not in the dataset. If 'ignore', any given names that are in the\n3665             dataset are dropped and no error is raised.\n3666 \n3667         Returns\n3668         -------\n3669         dropped : Dataset\n3670 \n3671         \"\"\"\n3672         # the Iterable check is required for mypy\n3673         if is_scalar(names) or not isinstance(names, Iterable):\n3674             names = {names}\n3675         else:\n3676             names = set(names)\n3677         if errors == \"raise\":\n3678             self._assert_all_in_dataset(names)\n3679 \n3680         variables = {k: v for k, v in self._variables.items() if k not in names}\n3681         coord_names = {k for k in self._coord_names if k in variables}\n3682         indexes = {k: v for k, v in self.indexes.items() if k not in names}\n3683         return self._replace_with_new_dims(\n3684             variables, coord_names=coord_names, indexes=indexes\n3685         )\n3686 \n3687     def drop(self, labels=None, dim=None, *, errors=\"raise\", **labels_kwargs):\n3688         \"\"\"Backward compatible method based on `drop_vars` and `drop_sel`\n3689 \n3690         Using either `drop_vars` or `drop_sel` is encouraged\n3691 \n3692         See Also\n3693         --------\n3694         Dataset.drop_vars\n3695         Dataset.drop_sel\n3696         \"\"\"\n3697         if errors not in [\"raise\", \"ignore\"]:\n3698             raise ValueError('errors must be either \"raise\" or \"ignore\"')\n3699 \n3700         if is_dict_like(labels) and not isinstance(labels, dict):\n3701             warnings.warn(\n3702                 \"dropping coordinates using `drop` is be deprecated; use drop_vars.\",\n3703                 FutureWarning,\n3704                 stacklevel=2,\n3705             )\n3706             return self.drop_vars(labels, errors=errors)\n3707 \n3708         if labels_kwargs or isinstance(labels, dict):\n3709             if dim is not None:\n3710                 raise ValueError(\"cannot specify dim and dict-like arguments.\")\n3711             labels = either_dict_or_kwargs(labels, labels_kwargs, \"drop\")\n3712 \n3713         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):\n3714             warnings.warn(\n3715                 \"dropping variables using `drop` will be deprecated; using drop_vars is encouraged.\",\n3716                 PendingDeprecationWarning,\n3717                 stacklevel=2,\n3718             )\n3719             return self.drop_vars(labels, errors=errors)\n3720         if dim is not None:\n3721             warnings.warn(\n3722                 \"dropping labels using list-like labels is deprecated; using \"\n3723                 \"dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).\",\n3724                 DeprecationWarning,\n3725                 stacklevel=2,\n3726             )\n3727             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)\n3728 \n3729         warnings.warn(\n3730             \"dropping labels using `drop` will be deprecated; using drop_sel is encouraged.\",\n3731             PendingDeprecationWarning,\n3732             stacklevel=2,\n3733         )\n3734         return self.drop_sel(labels, errors=errors)\n3735 \n3736     def drop_sel(self, labels=None, *, errors=\"raise\", **labels_kwargs):\n3737         \"\"\"Drop index labels from this dataset.\n3738 \n3739         Parameters\n3740         ----------\n3741         labels : Mapping[Hashable, Any]\n3742             Index labels to drop\n3743         errors: {'raise', 'ignore'}, optional\n3744             If 'raise' (default), raises a ValueError error if\n3745             any of the index labels passed are not\n3746             in the dataset. If 'ignore', any given labels that are in the\n3747             dataset are dropped and no error is raised.\n3748         **labels_kwargs : {dim: label, ...}, optional\n3749             The keyword arguments form of ``dim`` and ``labels``\n3750 \n3751         Returns\n3752         -------\n3753         dropped : Dataset\n3754 \n3755         Examples\n3756         --------\n3757         >>> data = np.random.randn(2, 3)\n3758         >>> labels = [\"a\", \"b\", \"c\"]\n3759         >>> ds = xr.Dataset({\"A\": ([\"x\", \"y\"], data), \"y\": labels})\n3760         >>> ds.drop_sel(y=[\"a\", \"c\"])\n3761         <xarray.Dataset>\n3762         Dimensions:  (x: 2, y: 1)\n3763         Coordinates:\n3764           * y        (y) <U1 'b'\n3765         Dimensions without coordinates: x\n3766         Data variables:\n3767             A        (x, y) float64 -0.3454 0.1734\n3768         >>> ds.drop_sel(y=\"b\")\n3769         <xarray.Dataset>\n3770         Dimensions:  (x: 2, y: 2)\n3771         Coordinates:\n3772           * y        (y) <U1 'a' 'c'\n3773         Dimensions without coordinates: x\n3774         Data variables:\n3775             A        (x, y) float64 -0.3944 -1.418 1.423 -1.041\n3776         \"\"\"\n3777         if errors not in [\"raise\", \"ignore\"]:\n3778             raise ValueError('errors must be either \"raise\" or \"ignore\"')\n3779 \n3780         labels = either_dict_or_kwargs(labels, labels_kwargs, \"drop\")\n3781 \n3782         ds = self\n3783         for dim, labels_for_dim in labels.items():\n3784             # Don't cast to set, as it would harm performance when labels\n3785             # is a large numpy array\n3786             if utils.is_scalar(labels_for_dim):\n3787                 labels_for_dim = [labels_for_dim]\n3788             labels_for_dim = np.asarray(labels_for_dim)\n3789             try:\n3790                 index = self.indexes[dim]\n3791             except KeyError:\n3792                 raise ValueError(\"dimension %r does not have coordinate labels\" % dim)\n3793             new_index = index.drop(labels_for_dim, errors=errors)\n3794             ds = ds.loc[{dim: new_index}]\n3795         return ds\n3796 \n3797     def drop_dims(\n3798         self, drop_dims: Union[Hashable, Iterable[Hashable]], *, errors: str = \"raise\"\n3799     ) -> \"Dataset\":\n3800         \"\"\"Drop dimensions and associated variables from this dataset.\n3801 \n3802         Parameters\n3803         ----------\n3804         drop_dims : hashable or iterable of hashable\n3805             Dimension or dimensions to drop.\n3806         errors: {'raise', 'ignore'}, optional\n3807             If 'raise' (default), raises a ValueError error if any of the\n3808             dimensions passed are not in the dataset. If 'ignore', any given\n3809             labels that are in the dataset are dropped and no error is raised.\n3810 \n3811         Returns\n3812         -------\n3813         obj : Dataset\n3814             The dataset without the given dimensions (or any variables\n3815             containing those dimensions)\n3816         errors: {'raise', 'ignore'}, optional\n3817             If 'raise' (default), raises a ValueError error if\n3818             any of the dimensions passed are not\n3819             in the dataset. If 'ignore', any given dimensions that are in the\n3820             dataset are dropped and no error is raised.\n3821         \"\"\"\n3822         if errors not in [\"raise\", \"ignore\"]:\n3823             raise ValueError('errors must be either \"raise\" or \"ignore\"')\n3824 \n3825         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):\n3826             drop_dims = {drop_dims}\n3827         else:\n3828             drop_dims = set(drop_dims)\n3829 \n3830         if errors == \"raise\":\n3831             missing_dims = drop_dims - set(self.dims)\n3832             if missing_dims:\n3833                 raise ValueError(\n3834                     \"Dataset does not contain the dimensions: %s\" % missing_dims\n3835                 )\n3836 \n3837         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}\n3838         return self.drop_vars(drop_vars)\n3839 \n3840     def transpose(self, *dims: Hashable) -> \"Dataset\":\n3841         \"\"\"Return a new Dataset object with all array dimensions transposed.\n3842 \n3843         Although the order of dimensions on each array will change, the dataset\n3844         dimensions themselves will remain in fixed (sorted) order.\n3845 \n3846         Parameters\n3847         ----------\n3848         *dims : Hashable, optional\n3849             By default, reverse the dimensions on each array. Otherwise,\n3850             reorder the dimensions to this order.\n3851 \n3852         Returns\n3853         -------\n3854         transposed : Dataset\n3855             Each array in the dataset (including) coordinates will be\n3856             transposed to the given order.\n3857 \n3858         Notes\n3859         -----\n3860         This operation returns a view of each array's data. It is\n3861         lazy for dask-backed DataArrays but not for numpy-backed DataArrays\n3862         -- the data will be fully loaded into memory.\n3863 \n3864         See Also\n3865         --------\n3866         numpy.transpose\n3867         DataArray.transpose\n3868         \"\"\"\n3869         if dims:\n3870             if set(dims) ^ set(self.dims) and ... not in dims:\n3871                 raise ValueError(\n3872                     \"arguments to transpose (%s) must be \"\n3873                     \"permuted dataset dimensions (%s)\" % (dims, tuple(self.dims))\n3874                 )\n3875         ds = self.copy()\n3876         for name, var in self._variables.items():\n3877             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))\n3878             ds._variables[name] = var.transpose(*var_dims)\n3879         return ds\n3880 \n3881     def dropna(\n3882         self,\n3883         dim: Hashable,\n3884         how: str = \"any\",\n3885         thresh: int = None,\n3886         subset: Iterable[Hashable] = None,\n3887     ):\n3888         \"\"\"Returns a new dataset with dropped labels for missing values along\n3889         the provided dimension.\n3890 \n3891         Parameters\n3892         ----------\n3893         dim : Hashable\n3894             Dimension along which to drop missing values. Dropping along\n3895             multiple dimensions simultaneously is not yet supported.\n3896         how : {'any', 'all'}, optional\n3897             * any : if any NA values are present, drop that label\n3898             * all : if all values are NA, drop that label\n3899         thresh : int, default None\n3900             If supplied, require this many non-NA values.\n3901         subset : iterable of hashable, optional\n3902             Which variables to check for missing values. By default, all\n3903             variables in the dataset are checked.\n3904 \n3905         Returns\n3906         -------\n3907         Dataset\n3908         \"\"\"\n3909         # TODO: consider supporting multiple dimensions? Or not, given that\n3910         # there are some ugly edge cases, e.g., pandas's dropna differs\n3911         # depending on the order of the supplied axes.\n3912 \n3913         if dim not in self.dims:\n3914             raise ValueError(\"%s must be a single dataset dimension\" % dim)\n3915 \n3916         if subset is None:\n3917             subset = iter(self.data_vars)\n3918 \n3919         count = np.zeros(self.dims[dim], dtype=np.int64)\n3920         size = 0\n3921 \n3922         for k in subset:\n3923             array = self._variables[k]\n3924             if dim in array.dims:\n3925                 dims = [d for d in array.dims if d != dim]\n3926                 count += np.asarray(array.count(dims))  # type: ignore\n3927                 size += np.prod([self.dims[d] for d in dims])\n3928 \n3929         if thresh is not None:\n3930             mask = count >= thresh\n3931         elif how == \"any\":\n3932             mask = count == size\n3933         elif how == \"all\":\n3934             mask = count > 0\n3935         elif how is not None:\n3936             raise ValueError(\"invalid how option: %s\" % how)\n3937         else:\n3938             raise TypeError(\"must specify how or thresh\")\n3939 \n3940         return self.isel({dim: mask})\n3941 \n3942     def fillna(self, value: Any) -> \"Dataset\":\n3943         \"\"\"Fill missing values in this object.\n3944 \n3945         This operation follows the normal broadcasting and alignment rules that\n3946         xarray uses for binary arithmetic, except the result is aligned to this\n3947         object (``join='left'``) instead of aligned to the intersection of\n3948         index coordinates (``join='inner'``).\n3949 \n3950         Parameters\n3951         ----------\n3952         value : scalar, ndarray, DataArray, dict or Dataset\n3953             Used to fill all matching missing values in this dataset's data\n3954             variables. Scalars, ndarrays or DataArrays arguments are used to\n3955             fill all data with aligned coordinates (for DataArrays).\n3956             Dictionaries or datasets match data variables and then align\n3957             coordinates if necessary.\n3958 \n3959         Returns\n3960         -------\n3961         Dataset\n3962 \n3963         Examples\n3964         --------\n3965 \n3966         >>> import numpy as np\n3967         >>> import xarray as xr\n3968         >>> ds = xr.Dataset(\n3969         ...     {\n3970         ...         \"A\": (\"x\", [np.nan, 2, np.nan, 0]),\n3971         ...         \"B\": (\"x\", [3, 4, np.nan, 1]),\n3972         ...         \"C\": (\"x\", [np.nan, np.nan, np.nan, 5]),\n3973         ...         \"D\": (\"x\", [np.nan, 3, np.nan, 4]),\n3974         ...     },\n3975         ...     coords={\"x\": [0, 1, 2, 3]},\n3976         ... )\n3977         >>> ds\n3978         <xarray.Dataset>\n3979         Dimensions:  (x: 4)\n3980         Coordinates:\n3981         * x        (x) int64 0 1 2 3\n3982         Data variables:\n3983             A        (x) float64 nan 2.0 nan 0.0\n3984             B        (x) float64 3.0 4.0 nan 1.0\n3985             C        (x) float64 nan nan nan 5.0\n3986             D        (x) float64 nan 3.0 nan 4.0\n3987 \n3988         Replace all `NaN` values with 0s.\n3989 \n3990         >>> ds.fillna(0)\n3991         <xarray.Dataset>\n3992         Dimensions:  (x: 4)\n3993         Coordinates:\n3994         * x        (x) int64 0 1 2 3\n3995         Data variables:\n3996             A        (x) float64 0.0 2.0 0.0 0.0\n3997             B        (x) float64 3.0 4.0 0.0 1.0\n3998             C        (x) float64 0.0 0.0 0.0 5.0\n3999             D        (x) float64 0.0 3.0 0.0 4.0\n4000 \n4001         Replace all `NaN` elements in column \u2018A\u2019, \u2018B\u2019, \u2018C\u2019, and \u2018D\u2019, with 0, 1, 2, and 3 respectively.\n4002 \n4003         >>> values = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n4004         >>> ds.fillna(value=values)\n4005         <xarray.Dataset>\n4006         Dimensions:  (x: 4)\n4007         Coordinates:\n4008         * x        (x) int64 0 1 2 3\n4009         Data variables:\n4010             A        (x) float64 0.0 2.0 0.0 0.0\n4011             B        (x) float64 3.0 4.0 1.0 1.0\n4012             C        (x) float64 2.0 2.0 2.0 5.0\n4013             D        (x) float64 3.0 3.0 3.0 4.0\n4014         \"\"\"\n4015         if utils.is_dict_like(value):\n4016             value_keys = getattr(value, \"data_vars\", value).keys()\n4017             if not set(value_keys) <= set(self.data_vars.keys()):\n4018                 raise ValueError(\n4019                     \"all variables in the argument to `fillna` \"\n4020                     \"must be contained in the original dataset\"\n4021                 )\n4022         out = ops.fillna(self, value)\n4023         return out\n4024 \n4025     def interpolate_na(\n4026         self,\n4027         dim: Hashable = None,\n4028         method: str = \"linear\",\n4029         limit: int = None,\n4030         use_coordinate: Union[bool, Hashable] = True,\n4031         max_gap: Union[\n4032             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta\n4033         ] = None,\n4034         **kwargs: Any,\n4035     ) -> \"Dataset\":\n4036         \"\"\"Fill in NaNs by interpolating according to different methods.\n4037 \n4038         Parameters\n4039         ----------\n4040         dim : str\n4041             Specifies the dimension along which to interpolate.\n4042 \n4043         method : str, optional\n4044             String indicating which method to use for interpolation:\n4045 \n4046             - 'linear': linear interpolation (Default). Additional keyword\n4047               arguments are passed to :py:func:`numpy.interp`\n4048             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n4049               are passed to :py:func:`scipy.interpolate.interp1d`. If\n4050               ``method='polynomial'``, the ``order`` keyword argument must also be\n4051               provided.\n4052             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n4053               respective :py:class:`scipy.interpolate` classes.\n4054 \n4055         use_coordinate : bool, str, default True\n4056             Specifies which index to use as the x values in the interpolation\n4057             formulated as `y = f(x)`. If False, values are treated as if\n4058             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is\n4059             used. If ``use_coordinate`` is a string, it specifies the name of a\n4060             coordinate variariable to use as the index.\n4061         limit : int, default None\n4062             Maximum number of consecutive NaNs to fill. Must be greater than 0\n4063             or None for no limit. This filling is done regardless of the size of\n4064             the gap in the data. To only interpolate over gaps less than a given length,\n4065             see ``max_gap``.\n4066         max_gap: int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default None.\n4067             Maximum size of gap, a continuous sequence of NaNs, that will be filled.\n4068             Use None for no limit. When interpolating along a datetime64 dimension\n4069             and ``use_coordinate=True``, ``max_gap`` can be one of the following:\n4070 \n4071             - a string that is valid input for pandas.to_timedelta\n4072             - a :py:class:`numpy.timedelta64` object\n4073             - a :py:class:`pandas.Timedelta` object\n4074             - a :py:class:`datetime.timedelta` object\n4075 \n4076             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled\n4077             dimensions has not been implemented yet. Gap length is defined as the difference\n4078             between coordinate values at the first data point after a gap and the last value\n4079             before a gap. For gaps at the beginning (end), gap length is defined as the difference\n4080             between coordinate values at the first (last) valid data point and the first (last) NaN.\n4081             For example, consider::\n4082 \n4083                 <xarray.DataArray (x: 9)>\n4084                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])\n4085                 Coordinates:\n4086                   * x        (x) int64 0 1 2 3 4 5 6 7 8\n4087 \n4088             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively\n4089         kwargs : dict, optional\n4090             parameters passed verbatim to the underlying interpolation function\n4091 \n4092         Returns\n4093         -------\n4094         interpolated: Dataset\n4095             Filled in Dataset.\n4096 \n4097         See also\n4098         --------\n4099         numpy.interp\n4100         scipy.interpolate\n4101         \"\"\"\n4102         from .missing import interp_na, _apply_over_vars_with_dim\n4103 \n4104         new = _apply_over_vars_with_dim(\n4105             interp_na,\n4106             self,\n4107             dim=dim,\n4108             method=method,\n4109             limit=limit,\n4110             use_coordinate=use_coordinate,\n4111             max_gap=max_gap,\n4112             **kwargs,\n4113         )\n4114         return new\n4115 \n4116     def ffill(self, dim: Hashable, limit: int = None) -> \"Dataset\":\n4117         \"\"\"Fill NaN values by propogating values forward\n4118 \n4119         *Requires bottleneck.*\n4120 \n4121         Parameters\n4122         ----------\n4123         dim : Hashable\n4124             Specifies the dimension along which to propagate values when\n4125             filling.\n4126         limit : int, default None\n4127             The maximum number of consecutive NaN values to forward fill. In\n4128             other words, if there is a gap with more than this number of\n4129             consecutive NaNs, it will only be partially filled. Must be greater\n4130             than 0 or None for no limit.\n4131 \n4132         Returns\n4133         -------\n4134         Dataset\n4135         \"\"\"\n4136         from .missing import ffill, _apply_over_vars_with_dim\n4137 \n4138         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)\n4139         return new\n4140 \n4141     def bfill(self, dim: Hashable, limit: int = None) -> \"Dataset\":\n4142         \"\"\"Fill NaN values by propogating values backward\n4143 \n4144         *Requires bottleneck.*\n4145 \n4146         Parameters\n4147         ----------\n4148         dim : str\n4149             Specifies the dimension along which to propagate values when\n4150             filling.\n4151         limit : int, default None\n4152             The maximum number of consecutive NaN values to backward fill. In\n4153             other words, if there is a gap with more than this number of\n4154             consecutive NaNs, it will only be partially filled. Must be greater\n4155             than 0 or None for no limit.\n4156 \n4157         Returns\n4158         -------\n4159         Dataset\n4160         \"\"\"\n4161         from .missing import bfill, _apply_over_vars_with_dim\n4162 \n4163         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)\n4164         return new\n4165 \n4166     def combine_first(self, other: \"Dataset\") -> \"Dataset\":\n4167         \"\"\"Combine two Datasets, default to data_vars of self.\n4168 \n4169         The new coordinates follow the normal broadcasting and alignment rules\n4170         of ``join='outer'``.  Vacant cells in the expanded coordinates are\n4171         filled with np.nan.\n4172 \n4173         Parameters\n4174         ----------\n4175         other : Dataset\n4176             Used to fill all matching missing values in this array.\n4177 \n4178         Returns\n4179         -------\n4180         Dataset\n4181         \"\"\"\n4182         out = ops.fillna(self, other, join=\"outer\", dataset_join=\"outer\")\n4183         return out\n4184 \n4185     def reduce(\n4186         self,\n4187         func: Callable,\n4188         dim: Union[Hashable, Iterable[Hashable]] = None,\n4189         keep_attrs: bool = None,\n4190         keepdims: bool = False,\n4191         numeric_only: bool = False,\n4192         allow_lazy: bool = None,\n4193         **kwargs: Any,\n4194     ) -> \"Dataset\":\n4195         \"\"\"Reduce this dataset by applying `func` along some dimension(s).\n4196 \n4197         Parameters\n4198         ----------\n4199         func : callable\n4200             Function which can be called in the form\n4201             `f(x, axis=axis, **kwargs)` to return the result of reducing an\n4202             np.ndarray over an integer valued axis.\n4203         dim : str or sequence of str, optional\n4204             Dimension(s) over which to apply `func`.  By default `func` is\n4205             applied over all dimensions.\n4206         keep_attrs : bool, optional\n4207             If True, the dataset's attributes (`attrs`) will be copied from\n4208             the original object to the new one.  If False (default), the new\n4209             object will be returned without attributes.\n4210         keepdims : bool, default False\n4211             If True, the dimensions which are reduced are left in the result\n4212             as dimensions of size one. Coordinates that use these dimensions\n4213             are removed.\n4214         numeric_only : bool, optional\n4215             If True, only apply ``func`` to variables with a numeric dtype.\n4216         **kwargs : Any\n4217             Additional keyword arguments passed on to ``func``.\n4218 \n4219         Returns\n4220         -------\n4221         reduced : Dataset\n4222             Dataset with this object's DataArrays replaced with new DataArrays\n4223             of summarized data and the indicated dimension(s) removed.\n4224         \"\"\"\n4225         if dim is None or dim is ...:\n4226             dims = set(self.dims)\n4227         elif isinstance(dim, str) or not isinstance(dim, Iterable):\n4228             dims = {dim}\n4229         else:\n4230             dims = set(dim)\n4231 \n4232         missing_dimensions = [d for d in dims if d not in self.dims]\n4233         if missing_dimensions:\n4234             raise ValueError(\n4235                 \"Dataset does not contain the dimensions: %s\" % missing_dimensions\n4236             )\n4237 \n4238         if keep_attrs is None:\n4239             keep_attrs = _get_keep_attrs(default=False)\n4240 \n4241         variables: Dict[Hashable, Variable] = {}\n4242         for name, var in self._variables.items():\n4243             reduce_dims = [d for d in var.dims if d in dims]\n4244             if name in self.coords:\n4245                 if not reduce_dims:\n4246                     variables[name] = var\n4247             else:\n4248                 if (\n4249                     not numeric_only\n4250                     or np.issubdtype(var.dtype, np.number)\n4251                     or (var.dtype == np.bool_)\n4252                 ):\n4253                     if len(reduce_dims) == 1:\n4254                         # unpack dimensions for the benefit of functions\n4255                         # like np.argmin which can't handle tuple arguments\n4256                         (reduce_dims,) = reduce_dims\n4257                     elif len(reduce_dims) == var.ndim:\n4258                         # prefer to aggregate over axis=None rather than\n4259                         # axis=(0, 1) if they will be equivalent, because\n4260                         # the former is often more efficient\n4261                         reduce_dims = None  # type: ignore\n4262                     variables[name] = var.reduce(\n4263                         func,\n4264                         dim=reduce_dims,\n4265                         keep_attrs=keep_attrs,\n4266                         keepdims=keepdims,\n4267                         allow_lazy=allow_lazy,\n4268                         **kwargs,\n4269                     )\n4270 \n4271         coord_names = {k for k in self.coords if k in variables}\n4272         indexes = {k: v for k, v in self.indexes.items() if k in variables}\n4273         attrs = self.attrs if keep_attrs else None\n4274         return self._replace_with_new_dims(\n4275             variables, coord_names=coord_names, attrs=attrs, indexes=indexes\n4276         )\n4277 \n4278     def map(\n4279         self,\n4280         func: Callable,\n4281         keep_attrs: bool = None,\n4282         args: Iterable[Any] = (),\n4283         **kwargs: Any,\n4284     ) -> \"Dataset\":\n4285         \"\"\"Apply a function to each variable in this dataset\n4286 \n4287         Parameters\n4288         ----------\n4289         func : callable\n4290             Function which can be called in the form `func(x, *args, **kwargs)`\n4291             to transform each DataArray `x` in this dataset into another\n4292             DataArray.\n4293         keep_attrs : bool, optional\n4294             If True, the dataset's attributes (`attrs`) will be copied from\n4295             the original object to the new one. If False, the new object will\n4296             be returned without attributes.\n4297         args : tuple, optional\n4298             Positional arguments passed on to `func`.\n4299         **kwargs : Any\n4300             Keyword arguments passed on to `func`.\n4301 \n4302         Returns\n4303         -------\n4304         applied : Dataset\n4305             Resulting dataset from applying ``func`` to each data variable.\n4306 \n4307         Examples\n4308         --------\n4309         >>> da = xr.DataArray(np.random.randn(2, 3))\n4310         >>> ds = xr.Dataset({\"foo\": da, \"bar\": (\"x\", [-1, 2])})\n4311         >>> ds\n4312         <xarray.Dataset>\n4313         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n4314         Dimensions without coordinates: dim_0, dim_1, x\n4315         Data variables:\n4316             foo      (dim_0, dim_1) float64 -0.3751 -1.951 -1.945 0.2948 0.711 -0.3948\n4317             bar      (x) int64 -1 2\n4318         >>> ds.map(np.fabs)\n4319         <xarray.Dataset>\n4320         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n4321         Dimensions without coordinates: dim_0, dim_1, x\n4322         Data variables:\n4323             foo      (dim_0, dim_1) float64 0.3751 1.951 1.945 0.2948 0.711 0.3948\n4324             bar      (x) float64 1.0 2.0\n4325         \"\"\"\n4326         variables = {\n4327             k: maybe_wrap_array(v, func(v, *args, **kwargs))\n4328             for k, v in self.data_vars.items()\n4329         }\n4330         if keep_attrs is None:\n4331             keep_attrs = _get_keep_attrs(default=False)\n4332         attrs = self.attrs if keep_attrs else None\n4333         return type(self)(variables, attrs=attrs)\n4334 \n4335     def apply(\n4336         self,\n4337         func: Callable,\n4338         keep_attrs: bool = None,\n4339         args: Iterable[Any] = (),\n4340         **kwargs: Any,\n4341     ) -> \"Dataset\":\n4342         \"\"\"\n4343         Backward compatible implementation of ``map``\n4344 \n4345         See Also\n4346         --------\n4347         Dataset.map\n4348         \"\"\"\n4349         warnings.warn(\n4350             \"Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged\",\n4351             PendingDeprecationWarning,\n4352             stacklevel=2,\n4353         )\n4354         return self.map(func, keep_attrs, args, **kwargs)\n4355 \n4356     def assign(\n4357         self, variables: Mapping[Hashable, Any] = None, **variables_kwargs: Hashable\n4358     ) -> \"Dataset\":\n4359         \"\"\"Assign new data variables to a Dataset, returning a new object\n4360         with all the original variables in addition to the new ones.\n4361 \n4362         Parameters\n4363         ----------\n4364         variables : mapping, value pairs\n4365             Mapping from variables names to the new values. If the new values\n4366             are callable, they are computed on the Dataset and assigned to new\n4367             data variables. If the values are not callable, (e.g. a DataArray,\n4368             scalar, or array), they are simply assigned.\n4369         **variables_kwargs:\n4370             The keyword arguments form of ``variables``.\n4371             One of variables or variables_kwargs must be provided.\n4372 \n4373         Returns\n4374         -------\n4375         ds : Dataset\n4376             A new Dataset with the new variables in addition to all the\n4377             existing variables.\n4378 \n4379         Notes\n4380         -----\n4381         Since ``kwargs`` is a dictionary, the order of your arguments may not\n4382         be preserved, and so the order of the new variables is not well\n4383         defined. Assigning multiple variables within the same ``assign`` is\n4384         possible, but you cannot reference other variables created within the\n4385         same ``assign`` call.\n4386 \n4387         See Also\n4388         --------\n4389         pandas.DataFrame.assign\n4390 \n4391         Examples\n4392         --------\n4393         >>> import numpy as np\n4394         >>> import xarray as xr\n4395         >>> x = xr.Dataset(\n4396         ...     {\n4397         ...         \"temperature_c\": (\n4398         ...             (\"lat\", \"lon\"),\n4399         ...             20 * np.random.rand(4).reshape(2, 2),\n4400         ...         ),\n4401         ...         \"precipitation\": ((\"lat\", \"lon\"), np.random.rand(4).reshape(2, 2)),\n4402         ...     },\n4403         ...     coords={\"lat\": [10, 20], \"lon\": [150, 160]},\n4404         ... )\n4405         >>> x\n4406         <xarray.Dataset>\n4407         Dimensions:        (lat: 2, lon: 2)\n4408         Coordinates:\n4409         * lat            (lat) int64 10 20\n4410         * lon            (lon) int64 150 160\n4411         Data variables:\n4412             temperature_c  (lat, lon) float64 18.04 12.51 17.64 9.313\n4413             precipitation  (lat, lon) float64 0.4751 0.6827 0.3697 0.03524\n4414 \n4415         Where the value is a callable, evaluated on dataset:\n4416 \n4417         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)\n4418         <xarray.Dataset>\n4419         Dimensions:        (lat: 2, lon: 2)\n4420         Coordinates:\n4421         * lat            (lat) int64 10 20\n4422         * lon            (lon) int64 150 160\n4423         Data variables:\n4424             temperature_c  (lat, lon) float64 18.04 12.51 17.64 9.313\n4425             precipitation  (lat, lon) float64 0.4751 0.6827 0.3697 0.03524\n4426             temperature_f  (lat, lon) float64 64.47 54.51 63.75 48.76\n4427 \n4428         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:\n4429 \n4430         >>> x.assign(temperature_f=x[\"temperature_c\"] * 9 / 5 + 32)\n4431         <xarray.Dataset>\n4432         Dimensions:        (lat: 2, lon: 2)\n4433         Coordinates:\n4434         * lat            (lat) int64 10 20\n4435         * lon            (lon) int64 150 160\n4436         Data variables:\n4437             temperature_c  (lat, lon) float64 18.04 12.51 17.64 9.313\n4438             precipitation  (lat, lon) float64 0.4751 0.6827 0.3697 0.03524\n4439             temperature_f  (lat, lon) float64 64.47 54.51 63.75 48.76\n4440 \n4441         \"\"\"\n4442         variables = either_dict_or_kwargs(variables, variables_kwargs, \"assign\")\n4443         data = self.copy()\n4444         # do all calculations first...\n4445         results = data._calc_assign_results(variables)\n4446         # ... and then assign\n4447         data.update(results)\n4448         return data\n4449 \n4450     def to_array(self, dim=\"variable\", name=None):\n4451         \"\"\"Convert this dataset into an xarray.DataArray\n4452 \n4453         The data variables of this dataset will be broadcast against each other\n4454         and stacked along the first axis of the new array. All coordinates of\n4455         this dataset will remain coordinates.\n4456 \n4457         Parameters\n4458         ----------\n4459         dim : str, optional\n4460             Name of the new dimension.\n4461         name : str, optional\n4462             Name of the new data array.\n4463 \n4464         Returns\n4465         -------\n4466         array : xarray.DataArray\n4467         \"\"\"\n4468         from .dataarray import DataArray\n4469 \n4470         data_vars = [self.variables[k] for k in self.data_vars]\n4471         broadcast_vars = broadcast_variables(*data_vars)\n4472         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)\n4473 \n4474         coords = dict(self.coords)\n4475         coords[dim] = list(self.data_vars)\n4476         indexes = propagate_indexes(self._indexes)\n4477 \n4478         dims = (dim,) + broadcast_vars[0].dims\n4479 \n4480         return DataArray(\n4481             data, coords, dims, attrs=self.attrs, name=name, indexes=indexes\n4482         )\n4483 \n4484     def _to_dataframe(self, ordered_dims):\n4485         columns = [k for k in self.variables if k not in self.dims]\n4486         data = [\n4487             self._variables[k].set_dims(ordered_dims).values.reshape(-1)\n4488             for k in columns\n4489         ]\n4490         index = self.coords.to_index(ordered_dims)\n4491         return pd.DataFrame(dict(zip(columns, data)), index=index)\n4492 \n4493     def to_dataframe(self):\n4494         \"\"\"Convert this dataset into a pandas.DataFrame.\n4495 \n4496         Non-index variables in this dataset form the columns of the\n4497         DataFrame. The DataFrame is be indexed by the Cartesian product of\n4498         this dataset's indices.\n4499         \"\"\"\n4500         return self._to_dataframe(self.dims)\n4501 \n4502     def _set_sparse_data_from_dataframe(\n4503         self, dataframe: pd.DataFrame, dims: tuple\n4504     ) -> None:\n4505         from sparse import COO\n4506 \n4507         idx = dataframe.index\n4508         if isinstance(idx, pd.MultiIndex):\n4509             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)\n4510             is_sorted = idx.is_lexsorted\n4511             shape = tuple(lev.size for lev in idx.levels)\n4512         else:\n4513             coords = np.arange(idx.size).reshape(1, -1)\n4514             is_sorted = True\n4515             shape = (idx.size,)\n4516 \n4517         for name, series in dataframe.items():\n4518             # Cast to a NumPy array first, in case the Series is a pandas\n4519             # Extension array (which doesn't have a valid NumPy dtype)\n4520             values = np.asarray(series)\n4521 \n4522             # In virtually all real use cases, the sparse array will now have\n4523             # missing values and needs a fill_value. For consistency, don't\n4524             # special case the rare exceptions (e.g., dtype=int without a\n4525             # MultiIndex).\n4526             dtype, fill_value = dtypes.maybe_promote(values.dtype)\n4527             values = np.asarray(values, dtype=dtype)\n4528 \n4529             data = COO(\n4530                 coords,\n4531                 values,\n4532                 shape,\n4533                 has_duplicates=False,\n4534                 sorted=is_sorted,\n4535                 fill_value=fill_value,\n4536             )\n4537             self[name] = (dims, data)\n4538 \n4539     def _set_numpy_data_from_dataframe(\n4540         self, dataframe: pd.DataFrame, dims: tuple\n4541     ) -> None:\n4542         idx = dataframe.index\n4543         if isinstance(idx, pd.MultiIndex):\n4544             # expand the DataFrame to include the product of all levels\n4545             full_idx = pd.MultiIndex.from_product(idx.levels, names=idx.names)\n4546             dataframe = dataframe.reindex(full_idx)\n4547             shape = tuple(lev.size for lev in idx.levels)\n4548         else:\n4549             shape = (idx.size,)\n4550         for name, series in dataframe.items():\n4551             data = np.asarray(series).reshape(shape)\n4552             self[name] = (dims, data)\n4553 \n4554     @classmethod\n4555     def from_dataframe(cls, dataframe: pd.DataFrame, sparse: bool = False) -> \"Dataset\":\n4556         \"\"\"Convert a pandas.DataFrame into an xarray.Dataset\n4557 \n4558         Each column will be converted into an independent variable in the\n4559         Dataset. If the dataframe's index is a MultiIndex, it will be expanded\n4560         into a tensor product of one-dimensional indices (filling in missing\n4561         values with NaN). This method will produce a Dataset very similar to\n4562         that on which the 'to_dataframe' method was called, except with\n4563         possibly redundant dimensions (since all dataset variables will have\n4564         the same dimensionality)\n4565 \n4566         Parameters\n4567         ----------\n4568         dataframe : pandas.DataFrame\n4569             DataFrame from which to copy data and indices.\n4570         sparse : bool\n4571             If true, create a sparse arrays instead of dense numpy arrays. This\n4572             can potentially save a large amount of memory if the DataFrame has\n4573             a MultiIndex. Requires the sparse package (sparse.pydata.org).\n4574 \n4575         Returns\n4576         -------\n4577         New Dataset.\n4578 \n4579         See also\n4580         --------\n4581         xarray.DataArray.from_series\n4582         \"\"\"\n4583         # TODO: Add an option to remove dimensions along which the variables\n4584         # are constant, to enable consistent serialization to/from a dataframe,\n4585         # even if some variables have different dimensionality.\n4586 \n4587         if not dataframe.columns.is_unique:\n4588             raise ValueError(\"cannot convert DataFrame with non-unique columns\")\n4589 \n4590         idx = remove_unused_levels_categories(dataframe.index)\n4591         dataframe = dataframe.set_index(idx)\n4592         obj = cls()\n4593 \n4594         if isinstance(idx, pd.MultiIndex):\n4595             dims = tuple(\n4596                 name if name is not None else \"level_%i\" % n\n4597                 for n, name in enumerate(idx.names)\n4598             )\n4599             for dim, lev in zip(dims, idx.levels):\n4600                 obj[dim] = (dim, lev)\n4601         else:\n4602             index_name = idx.name if idx.name is not None else \"index\"\n4603             dims = (index_name,)\n4604             obj[index_name] = (dims, idx)\n4605 \n4606         if sparse:\n4607             obj._set_sparse_data_from_dataframe(dataframe, dims)\n4608         else:\n4609             obj._set_numpy_data_from_dataframe(dataframe, dims)\n4610         return obj\n4611 \n4612     def to_dask_dataframe(self, dim_order=None, set_index=False):\n4613         \"\"\"\n4614         Convert this dataset into a dask.dataframe.DataFrame.\n4615 \n4616         The dimensions, coordinates and data variables in this dataset form\n4617         the columns of the DataFrame.\n4618 \n4619         Parameters\n4620         ----------\n4621         dim_order : list, optional\n4622             Hierarchical dimension order for the resulting dataframe. All\n4623             arrays are transposed to this order and then written out as flat\n4624             vectors in contiguous order, so the last dimension in this list\n4625             will be contiguous in the resulting DataFrame. This has a major\n4626             influence on which operations are efficient on the resulting dask\n4627             dataframe.\n4628 \n4629             If provided, must include all dimensions on this dataset. By\n4630             default, dimensions are sorted alphabetically.\n4631         set_index : bool, optional\n4632             If set_index=True, the dask DataFrame is indexed by this dataset's\n4633             coordinate. Since dask DataFrames to not support multi-indexes,\n4634             set_index only works if the dataset only contains one dimension.\n4635 \n4636         Returns\n4637         -------\n4638         dask.dataframe.DataFrame\n4639         \"\"\"\n4640 \n4641         import dask.array as da\n4642         import dask.dataframe as dd\n4643 \n4644         if dim_order is None:\n4645             dim_order = list(self.dims)\n4646         elif set(dim_order) != set(self.dims):\n4647             raise ValueError(\n4648                 \"dim_order {} does not match the set of dimensions on this \"\n4649                 \"Dataset: {}\".format(dim_order, list(self.dims))\n4650             )\n4651 \n4652         ordered_dims = {k: self.dims[k] for k in dim_order}\n4653 \n4654         columns = list(ordered_dims)\n4655         columns.extend(k for k in self.coords if k not in self.dims)\n4656         columns.extend(self.data_vars)\n4657 \n4658         series_list = []\n4659         for name in columns:\n4660             try:\n4661                 var = self.variables[name]\n4662             except KeyError:\n4663                 # dimension without a matching coordinate\n4664                 size = self.dims[name]\n4665                 data = da.arange(size, chunks=size, dtype=np.int64)\n4666                 var = Variable((name,), data)\n4667 \n4668             # IndexVariable objects have a dummy .chunk() method\n4669             if isinstance(var, IndexVariable):\n4670                 var = var.to_base_variable()\n4671 \n4672             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data\n4673             series = dd.from_array(dask_array.reshape(-1), columns=[name])\n4674             series_list.append(series)\n4675 \n4676         df = dd.concat(series_list, axis=1)\n4677 \n4678         if set_index:\n4679             if len(dim_order) == 1:\n4680                 (dim,) = dim_order\n4681                 df = df.set_index(dim)\n4682             else:\n4683                 # triggers an error about multi-indexes, even if only one\n4684                 # dimension is passed\n4685                 df = df.set_index(dim_order)\n4686 \n4687         return df\n4688 \n4689     def to_dict(self, data=True):\n4690         \"\"\"\n4691         Convert this dataset to a dictionary following xarray naming\n4692         conventions.\n4693 \n4694         Converts all variables and attributes to native Python objects\n4695         Useful for converting to json. To avoid datetime incompatibility\n4696         use decode_times=False kwarg in xarrray.open_dataset.\n4697 \n4698         Parameters\n4699         ----------\n4700         data : bool, optional\n4701             Whether to include the actual data in the dictionary. When set to\n4702             False, returns just the schema.\n4703 \n4704         See also\n4705         --------\n4706         Dataset.from_dict\n4707         \"\"\"\n4708         d = {\n4709             \"coords\": {},\n4710             \"attrs\": decode_numpy_dict_values(self.attrs),\n4711             \"dims\": dict(self.dims),\n4712             \"data_vars\": {},\n4713         }\n4714         for k in self.coords:\n4715             d[\"coords\"].update({k: self[k].variable.to_dict(data=data)})\n4716         for k in self.data_vars:\n4717             d[\"data_vars\"].update({k: self[k].variable.to_dict(data=data)})\n4718         return d\n4719 \n4720     @classmethod\n4721     def from_dict(cls, d):\n4722         \"\"\"\n4723         Convert a dictionary into an xarray.Dataset.\n4724 \n4725         Input dict can take several forms::\n4726 \n4727             d = {'t': {'dims': ('t'), 'data': t},\n4728                  'a': {'dims': ('t'), 'data': x},\n4729                  'b': {'dims': ('t'), 'data': y}}\n4730 \n4731             d = {'coords': {'t': {'dims': 't', 'data': t,\n4732                                   'attrs': {'units':'s'}}},\n4733                  'attrs': {'title': 'air temperature'},\n4734                  'dims': 't',\n4735                  'data_vars': {'a': {'dims': 't', 'data': x, },\n4736                                'b': {'dims': 't', 'data': y}}}\n4737 \n4738         where 't' is the name of the dimesion, 'a' and 'b' are names of data\n4739         variables and t, x, and y are lists, numpy.arrays or pandas objects.\n4740 \n4741         Parameters\n4742         ----------\n4743         d : dict, with a minimum structure of {'var_0': {'dims': [..], \\\n4744                                                          'data': [..]}, \\\n4745                                                ...}\n4746 \n4747         Returns\n4748         -------\n4749         obj : xarray.Dataset\n4750 \n4751         See also\n4752         --------\n4753         Dataset.to_dict\n4754         DataArray.from_dict\n4755         \"\"\"\n4756 \n4757         if not {\"coords\", \"data_vars\"}.issubset(set(d)):\n4758             variables = d.items()\n4759         else:\n4760             import itertools\n4761 \n4762             variables = itertools.chain(\n4763                 d.get(\"coords\", {}).items(), d.get(\"data_vars\", {}).items()\n4764             )\n4765         try:\n4766             variable_dict = {\n4767                 k: (v[\"dims\"], v[\"data\"], v.get(\"attrs\")) for k, v in variables\n4768             }\n4769         except KeyError as e:\n4770             raise ValueError(\n4771                 \"cannot convert dict without the key \"\n4772                 \"'{dims_data}'\".format(dims_data=str(e.args[0]))\n4773             )\n4774         obj = cls(variable_dict)\n4775 \n4776         # what if coords aren't dims?\n4777         coords = set(d.get(\"coords\", {})) - set(d.get(\"dims\", {}))\n4778         obj = obj.set_coords(coords)\n4779 \n4780         obj.attrs.update(d.get(\"attrs\", {}))\n4781 \n4782         return obj\n4783 \n4784     @staticmethod\n4785     def _unary_op(f, keep_attrs=False):\n4786         @functools.wraps(f)\n4787         def func(self, *args, **kwargs):\n4788             variables = {}\n4789             for k, v in self._variables.items():\n4790                 if k in self._coord_names:\n4791                     variables[k] = v\n4792                 else:\n4793                     variables[k] = f(v, *args, **kwargs)\n4794             attrs = self._attrs if keep_attrs else None\n4795             return self._replace_with_new_dims(variables, attrs=attrs)\n4796 \n4797         return func\n4798 \n4799     @staticmethod\n4800     def _binary_op(f, reflexive=False, join=None):\n4801         @functools.wraps(f)\n4802         def func(self, other):\n4803             from .dataarray import DataArray\n4804 \n4805             if isinstance(other, groupby.GroupBy):\n4806                 return NotImplemented\n4807             align_type = OPTIONS[\"arithmetic_join\"] if join is None else join\n4808             if isinstance(other, (DataArray, Dataset)):\n4809                 self, other = align(self, other, join=align_type, copy=False)\n4810             g = f if not reflexive else lambda x, y: f(y, x)\n4811             ds = self._calculate_binary_op(g, other, join=align_type)\n4812             return ds\n4813 \n4814         return func\n4815 \n4816     @staticmethod\n4817     def _inplace_binary_op(f):\n4818         @functools.wraps(f)\n4819         def func(self, other):\n4820             from .dataarray import DataArray\n4821 \n4822             if isinstance(other, groupby.GroupBy):\n4823                 raise TypeError(\n4824                     \"in-place operations between a Dataset and \"\n4825                     \"a grouped object are not permitted\"\n4826                 )\n4827             # we don't actually modify arrays in-place with in-place Dataset\n4828             # arithmetic -- this lets us automatically align things\n4829             if isinstance(other, (DataArray, Dataset)):\n4830                 other = other.reindex_like(self, copy=False)\n4831             g = ops.inplace_to_noninplace_op(f)\n4832             ds = self._calculate_binary_op(g, other, inplace=True)\n4833             self._replace_with_new_dims(\n4834                 ds._variables,\n4835                 ds._coord_names,\n4836                 attrs=ds._attrs,\n4837                 indexes=ds._indexes,\n4838                 inplace=True,\n4839             )\n4840             return self\n4841 \n4842         return func\n4843 \n4844     def _calculate_binary_op(self, f, other, join=\"inner\", inplace=False):\n4845         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):\n4846             if inplace and set(lhs_data_vars) != set(rhs_data_vars):\n4847                 raise ValueError(\n4848                     \"datasets must have the same data variables \"\n4849                     \"for in-place arithmetic operations: %s, %s\"\n4850                     % (list(lhs_data_vars), list(rhs_data_vars))\n4851                 )\n4852 \n4853             dest_vars = {}\n4854 \n4855             for k in lhs_data_vars:\n4856                 if k in rhs_data_vars:\n4857                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])\n4858                 elif join in [\"left\", \"outer\"]:\n4859                     dest_vars[k] = f(lhs_vars[k], np.nan)\n4860             for k in rhs_data_vars:\n4861                 if k not in dest_vars and join in [\"right\", \"outer\"]:\n4862                     dest_vars[k] = f(rhs_vars[k], np.nan)\n4863             return dest_vars\n4864 \n4865         if utils.is_dict_like(other) and not isinstance(other, Dataset):\n4866             # can't use our shortcut of doing the binary operation with\n4867             # Variable objects, so apply over our data vars instead.\n4868             new_data_vars = apply_over_both(\n4869                 self.data_vars, other, self.data_vars, other\n4870             )\n4871             return Dataset(new_data_vars)\n4872 \n4873         other_coords = getattr(other, \"coords\", None)\n4874         ds = self.coords.merge(other_coords)\n4875 \n4876         if isinstance(other, Dataset):\n4877             new_vars = apply_over_both(\n4878                 self.data_vars, other.data_vars, self.variables, other.variables\n4879             )\n4880         else:\n4881             other_variable = getattr(other, \"variable\", other)\n4882             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}\n4883         ds._variables.update(new_vars)\n4884         ds._dims = calculate_dimensions(ds._variables)\n4885         return ds\n4886 \n4887     def _copy_attrs_from(self, other):\n4888         self.attrs = other.attrs\n4889         for v in other.variables:\n4890             if v in self.variables:\n4891                 self.variables[v].attrs = other.variables[v].attrs\n4892 \n4893     def diff(self, dim, n=1, label=\"upper\"):\n4894         \"\"\"Calculate the n-th order discrete difference along given axis.\n4895 \n4896         Parameters\n4897         ----------\n4898         dim : str, optional\n4899             Dimension over which to calculate the finite difference.\n4900         n : int, optional\n4901             The number of times values are differenced.\n4902         label : str, optional\n4903             The new coordinate in dimension ``dim`` will have the\n4904             values of either the minuend's or subtrahend's coordinate\n4905             for values 'upper' and 'lower', respectively.  Other\n4906             values are not supported.\n4907 \n4908         Returns\n4909         -------\n4910         difference : same type as caller\n4911             The n-th order finite difference of this object.\n4912 \n4913         .. note::\n4914 \n4915             `n` matches numpy's behavior and is different from pandas' first\n4916             argument named `periods`.\n4917 \n4918         Examples\n4919         --------\n4920         >>> ds = xr.Dataset({\"foo\": (\"x\", [5, 5, 6, 6])})\n4921         >>> ds.diff(\"x\")\n4922         <xarray.Dataset>\n4923         Dimensions:  (x: 3)\n4924         Coordinates:\n4925           * x        (x) int64 1 2 3\n4926         Data variables:\n4927             foo      (x) int64 0 1 0\n4928         >>> ds.diff(\"x\", 2)\n4929         <xarray.Dataset>\n4930         Dimensions:  (x: 2)\n4931         Coordinates:\n4932         * x        (x) int64 2 3\n4933         Data variables:\n4934         foo      (x) int64 1 -1\n4935 \n4936         See Also\n4937         --------\n4938         Dataset.differentiate\n4939         \"\"\"\n4940         if n == 0:\n4941             return self\n4942         if n < 0:\n4943             raise ValueError(f\"order `n` must be non-negative but got {n}\")\n4944 \n4945         # prepare slices\n4946         kwargs_start = {dim: slice(None, -1)}\n4947         kwargs_end = {dim: slice(1, None)}\n4948 \n4949         # prepare new coordinate\n4950         if label == \"upper\":\n4951             kwargs_new = kwargs_end\n4952         elif label == \"lower\":\n4953             kwargs_new = kwargs_start\n4954         else:\n4955             raise ValueError(\n4956                 \"The 'label' argument has to be either \" \"'upper' or 'lower'\"\n4957             )\n4958 \n4959         variables = {}\n4960 \n4961         for name, var in self.variables.items():\n4962             if dim in var.dims:\n4963                 if name in self.data_vars:\n4964                     variables[name] = var.isel(**kwargs_end) - var.isel(**kwargs_start)\n4965                 else:\n4966                     variables[name] = var.isel(**kwargs_new)\n4967             else:\n4968                 variables[name] = var\n4969 \n4970         indexes = dict(self.indexes)\n4971         if dim in indexes:\n4972             indexes[dim] = indexes[dim][kwargs_new[dim]]\n4973 \n4974         difference = self._replace_with_new_dims(variables, indexes=indexes)\n4975 \n4976         if n > 1:\n4977             return difference.diff(dim, n - 1)\n4978         else:\n4979             return difference\n4980 \n4981     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):\n4982         \"\"\"Shift this dataset by an offset along one or more dimensions.\n4983 \n4984         Only data variables are moved; coordinates stay in place. This is\n4985         consistent with the behavior of ``shift`` in pandas.\n4986 \n4987         Parameters\n4988         ----------\n4989         shifts : Mapping with the form of {dim: offset}\n4990             Integer offset to shift along each of the given dimensions.\n4991             Positive offsets shift to the right; negative offsets shift to the\n4992             left.\n4993         fill_value: scalar, optional\n4994             Value to use for newly missing values\n4995         **shifts_kwargs:\n4996             The keyword arguments form of ``shifts``.\n4997             One of shifts or shifts_kwargs must be provided.\n4998 \n4999         Returns\n5000         -------\n5001         shifted : Dataset\n5002             Dataset with the same coordinates and attributes but shifted data\n5003             variables.\n5004 \n5005         See also\n5006         --------\n5007         roll\n5008 \n5009         Examples\n5010         --------\n5011 \n5012         >>> ds = xr.Dataset({\"foo\": (\"x\", list(\"abcde\"))})\n5013         >>> ds.shift(x=2)\n5014         <xarray.Dataset>\n5015         Dimensions:  (x: 5)\n5016         Coordinates:\n5017           * x        (x) int64 0 1 2 3 4\n5018         Data variables:\n5019             foo      (x) object nan nan 'a' 'b' 'c'\n5020         \"\"\"\n5021         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"shift\")\n5022         invalid = [k for k in shifts if k not in self.dims]\n5023         if invalid:\n5024             raise ValueError(\"dimensions %r do not exist\" % invalid)\n5025 \n5026         variables = {}\n5027         for name, var in self.variables.items():\n5028             if name in self.data_vars:\n5029                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}\n5030                 variables[name] = var.shift(fill_value=fill_value, shifts=var_shifts)\n5031             else:\n5032                 variables[name] = var\n5033 \n5034         return self._replace(variables)\n5035 \n5036     def roll(self, shifts=None, roll_coords=None, **shifts_kwargs):\n5037         \"\"\"Roll this dataset by an offset along one or more dimensions.\n5038 \n5039         Unlike shift, roll may rotate all variables, including coordinates\n5040         if specified. The direction of rotation is consistent with\n5041         :py:func:`numpy.roll`.\n5042 \n5043         Parameters\n5044         ----------\n5045 \n5046         shifts : dict, optional\n5047             A dict with keys matching dimensions and values given\n5048             by integers to rotate each of the given dimensions. Positive\n5049             offsets roll to the right; negative offsets roll to the left.\n5050         roll_coords : bool\n5051             Indicates whether to  roll the coordinates by the offset\n5052             The current default of roll_coords (None, equivalent to True) is\n5053             deprecated and will change to False in a future version.\n5054             Explicitly pass roll_coords to silence the warning.\n5055         **shifts_kwargs : {dim: offset, ...}, optional\n5056             The keyword arguments form of ``shifts``.\n5057             One of shifts or shifts_kwargs must be provided.\n5058         Returns\n5059         -------\n5060         rolled : Dataset\n5061             Dataset with the same coordinates and attributes but rolled\n5062             variables.\n5063 \n5064         See also\n5065         --------\n5066         shift\n5067 \n5068         Examples\n5069         --------\n5070 \n5071         >>> ds = xr.Dataset({\"foo\": (\"x\", list(\"abcde\"))})\n5072         >>> ds.roll(x=2)\n5073         <xarray.Dataset>\n5074         Dimensions:  (x: 5)\n5075         Coordinates:\n5076           * x        (x) int64 3 4 0 1 2\n5077         Data variables:\n5078             foo      (x) object 'd' 'e' 'a' 'b' 'c'\n5079         \"\"\"\n5080         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"roll\")\n5081         invalid = [k for k in shifts if k not in self.dims]\n5082         if invalid:\n5083             raise ValueError(\"dimensions %r do not exist\" % invalid)\n5084 \n5085         if roll_coords is None:\n5086             warnings.warn(\n5087                 \"roll_coords will be set to False in the future.\"\n5088                 \" Explicitly set roll_coords to silence warning.\",\n5089                 FutureWarning,\n5090                 stacklevel=2,\n5091             )\n5092             roll_coords = True\n5093 \n5094         unrolled_vars = () if roll_coords else self.coords\n5095 \n5096         variables = {}\n5097         for k, v in self.variables.items():\n5098             if k not in unrolled_vars:\n5099                 variables[k] = v.roll(\n5100                     **{k: s for k, s in shifts.items() if k in v.dims}\n5101                 )\n5102             else:\n5103                 variables[k] = v\n5104 \n5105         if roll_coords:\n5106             indexes = {}\n5107             for k, v in self.indexes.items():\n5108                 (dim,) = self.variables[k].dims\n5109                 if dim in shifts:\n5110                     indexes[k] = roll_index(v, shifts[dim])\n5111                 else:\n5112                     indexes[k] = v\n5113         else:\n5114             indexes = dict(self.indexes)\n5115 \n5116         return self._replace(variables, indexes=indexes)\n5117 \n5118     def sortby(self, variables, ascending=True):\n5119         \"\"\"\n5120         Sort object by labels or values (along an axis).\n5121 \n5122         Sorts the dataset, either along specified dimensions,\n5123         or according to values of 1-D dataarrays that share dimension\n5124         with calling object.\n5125 \n5126         If the input variables are dataarrays, then the dataarrays are aligned\n5127         (via left-join) to the calling object prior to sorting by cell values.\n5128         NaNs are sorted to the end, following Numpy convention.\n5129 \n5130         If multiple sorts along the same dimension is\n5131         given, numpy's lexsort is performed along that dimension:\n5132         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html\n5133         and the FIRST key in the sequence is used as the primary sort key,\n5134         followed by the 2nd key, etc.\n5135 \n5136         Parameters\n5137         ----------\n5138         variables: str, DataArray, or list of either\n5139             1D DataArray objects or name(s) of 1D variable(s) in\n5140             coords/data_vars whose values are used to sort the dataset.\n5141         ascending: boolean, optional\n5142             Whether to sort by ascending or descending order.\n5143 \n5144         Returns\n5145         -------\n5146         sorted: Dataset\n5147             A new dataset where all the specified dims are sorted by dim\n5148             labels.\n5149         \"\"\"\n5150         from .dataarray import DataArray\n5151 \n5152         if not isinstance(variables, list):\n5153             variables = [variables]\n5154         else:\n5155             variables = variables\n5156         variables = [v if isinstance(v, DataArray) else self[v] for v in variables]\n5157         aligned_vars = align(self, *variables, join=\"left\")\n5158         aligned_self = aligned_vars[0]\n5159         aligned_other_vars = aligned_vars[1:]\n5160         vars_by_dim = defaultdict(list)\n5161         for data_array in aligned_other_vars:\n5162             if data_array.ndim != 1:\n5163                 raise ValueError(\"Input DataArray is not 1-D.\")\n5164             (key,) = data_array.dims\n5165             vars_by_dim[key].append(data_array)\n5166 \n5167         indices = {}\n5168         for key, arrays in vars_by_dim.items():\n5169             order = np.lexsort(tuple(reversed(arrays)))\n5170             indices[key] = order if ascending else order[::-1]\n5171         return aligned_self.isel(**indices)\n5172 \n5173     def quantile(\n5174         self,\n5175         q,\n5176         dim=None,\n5177         interpolation=\"linear\",\n5178         numeric_only=False,\n5179         keep_attrs=None,\n5180         skipna=True,\n5181     ):\n5182         \"\"\"Compute the qth quantile of the data along the specified dimension.\n5183 \n5184         Returns the qth quantiles(s) of the array elements for each variable\n5185         in the Dataset.\n5186 \n5187         Parameters\n5188         ----------\n5189         q : float in range of [0,1] or array-like of floats\n5190             Quantile to compute, which must be between 0 and 1 inclusive.\n5191         dim : str or sequence of str, optional\n5192             Dimension(s) over which to apply quantile.\n5193         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n5194             This optional parameter specifies the interpolation method to\n5195             use when the desired quantile lies between two data points\n5196             ``i < j``:\n5197 \n5198                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is\n5199                   the fractional part of the index surrounded by ``i`` and\n5200                   ``j``.\n5201                 * lower: ``i``.\n5202                 * higher: ``j``.\n5203                 * nearest: ``i`` or ``j``, whichever is nearest.\n5204                 * midpoint: ``(i + j) / 2``.\n5205         keep_attrs : bool, optional\n5206             If True, the dataset's attributes (`attrs`) will be copied from\n5207             the original object to the new one.  If False (default), the new\n5208             object will be returned without attributes.\n5209         numeric_only : bool, optional\n5210             If True, only apply ``func`` to variables with a numeric dtype.\n5211         skipna : bool, optional\n5212             Whether to skip missing values when aggregating.\n5213 \n5214         Returns\n5215         -------\n5216         quantiles : Dataset\n5217             If `q` is a single quantile, then the result is a scalar for each\n5218             variable in data_vars. If multiple percentiles are given, first\n5219             axis of the result corresponds to the quantile and a quantile\n5220             dimension is added to the return Dataset. The other dimensions are\n5221             the dimensions that remain after the reduction of the array.\n5222 \n5223         See Also\n5224         --------\n5225         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile\n5226 \n5227         Examples\n5228         --------\n5229 \n5230         >>> ds = xr.Dataset(\n5231         ...     {\"a\": ((\"x\", \"y\"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},\n5232         ...     coords={\"x\": [7, 9], \"y\": [1, 1.5, 2, 2.5]},\n5233         ... )\n5234         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)\n5235         <xarray.Dataset>\n5236         Dimensions:   ()\n5237         Coordinates:\n5238             quantile  float64 0.0\n5239         Data variables:\n5240             a         float64 0.7\n5241         >>> ds.quantile(0, dim=\"x\")\n5242         <xarray.Dataset>\n5243         Dimensions:   (y: 4)\n5244         Coordinates:\n5245           * y         (y) float64 1.0 1.5 2.0 2.5\n5246             quantile  float64 0.0\n5247         Data variables:\n5248             a         (y) float64 0.7 4.2 2.6 1.5\n5249         >>> ds.quantile([0, 0.5, 1])\n5250         <xarray.Dataset>\n5251         Dimensions:   (quantile: 3)\n5252         Coordinates:\n5253           * quantile  (quantile) float64 0.0 0.5 1.0\n5254         Data variables:\n5255             a         (quantile) float64 0.7 3.4 9.4\n5256         >>> ds.quantile([0, 0.5, 1], dim=\"x\")\n5257         <xarray.Dataset>\n5258         Dimensions:   (quantile: 3, y: 4)\n5259         Coordinates:\n5260           * y         (y) float64 1.0 1.5 2.0 2.5\n5261           * quantile  (quantile) float64 0.0 0.5 1.0\n5262         Data variables:\n5263             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9\n5264         \"\"\"\n5265 \n5266         if isinstance(dim, str):\n5267             dims = {dim}\n5268         elif dim in [None, ...]:\n5269             dims = set(self.dims)\n5270         else:\n5271             dims = set(dim)\n5272 \n5273         _assert_empty(\n5274             [d for d in dims if d not in self.dims],\n5275             \"Dataset does not contain the dimensions: %s\",\n5276         )\n5277 \n5278         q = np.asarray(q, dtype=np.float64)\n5279 \n5280         variables = {}\n5281         for name, var in self.variables.items():\n5282             reduce_dims = [d for d in var.dims if d in dims]\n5283             if reduce_dims or not var.dims:\n5284                 if name not in self.coords:\n5285                     if (\n5286                         not numeric_only\n5287                         or np.issubdtype(var.dtype, np.number)\n5288                         or var.dtype == np.bool_\n5289                     ):\n5290                         if len(reduce_dims) == var.ndim:\n5291                             # prefer to aggregate over axis=None rather than\n5292                             # axis=(0, 1) if they will be equivalent, because\n5293                             # the former is often more efficient\n5294                             reduce_dims = None\n5295                         variables[name] = var.quantile(\n5296                             q,\n5297                             dim=reduce_dims,\n5298                             interpolation=interpolation,\n5299                             keep_attrs=keep_attrs,\n5300                             skipna=skipna,\n5301                         )\n5302 \n5303             else:\n5304                 variables[name] = var\n5305 \n5306         # construct the new dataset\n5307         coord_names = {k for k in self.coords if k in variables}\n5308         indexes = {k: v for k, v in self.indexes.items() if k in variables}\n5309         if keep_attrs is None:\n5310             keep_attrs = _get_keep_attrs(default=False)\n5311         attrs = self.attrs if keep_attrs else None\n5312         new = self._replace_with_new_dims(\n5313             variables, coord_names=coord_names, attrs=attrs, indexes=indexes\n5314         )\n5315         return new.assign_coords(quantile=q)\n5316 \n5317     def rank(self, dim, pct=False, keep_attrs=None):\n5318         \"\"\"Ranks the data.\n5319 \n5320         Equal values are assigned a rank that is the average of the ranks that\n5321         would have been otherwise assigned to all of the values within\n5322         that set.\n5323         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.\n5324 \n5325         NaNs in the input array are returned as NaNs.\n5326 \n5327         The `bottleneck` library is required.\n5328 \n5329         Parameters\n5330         ----------\n5331         dim : str\n5332             Dimension over which to compute rank.\n5333         pct : bool, optional\n5334             If True, compute percentage ranks, otherwise compute integer ranks.\n5335         keep_attrs : bool, optional\n5336             If True, the dataset's attributes (`attrs`) will be copied from\n5337             the original object to the new one.  If False (default), the new\n5338             object will be returned without attributes.\n5339 \n5340         Returns\n5341         -------\n5342         ranked : Dataset\n5343             Variables that do not depend on `dim` are dropped.\n5344         \"\"\"\n5345         if dim not in self.dims:\n5346             raise ValueError(\"Dataset does not contain the dimension: %s\" % dim)\n5347 \n5348         variables = {}\n5349         for name, var in self.variables.items():\n5350             if name in self.data_vars:\n5351                 if dim in var.dims:\n5352                     variables[name] = var.rank(dim, pct=pct)\n5353             else:\n5354                 variables[name] = var\n5355 \n5356         coord_names = set(self.coords)\n5357         if keep_attrs is None:\n5358             keep_attrs = _get_keep_attrs(default=False)\n5359         attrs = self.attrs if keep_attrs else None\n5360         return self._replace(variables, coord_names, attrs=attrs)\n5361 \n5362     def differentiate(self, coord, edge_order=1, datetime_unit=None):\n5363         \"\"\" Differentiate with the second order accurate central\n5364         differences.\n5365 \n5366         .. note::\n5367             This feature is limited to simple cartesian geometry, i.e. coord\n5368             must be one dimensional.\n5369 \n5370         Parameters\n5371         ----------\n5372         coord: str\n5373             The coordinate to be used to compute the gradient.\n5374         edge_order: 1 or 2. Default 1\n5375             N-th order accurate differences at the boundaries.\n5376         datetime_unit: None or any of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms',\n5377             'us', 'ns', 'ps', 'fs', 'as'}\n5378             Unit to compute gradient. Only valid for datetime coordinate.\n5379 \n5380         Returns\n5381         -------\n5382         differentiated: Dataset\n5383 \n5384         See also\n5385         --------\n5386         numpy.gradient: corresponding numpy function\n5387         \"\"\"\n5388         from .variable import Variable\n5389 \n5390         if coord not in self.variables and coord not in self.dims:\n5391             raise ValueError(f\"Coordinate {coord} does not exist.\")\n5392 \n5393         coord_var = self[coord].variable\n5394         if coord_var.ndim != 1:\n5395             raise ValueError(\n5396                 \"Coordinate {} must be 1 dimensional but is {}\"\n5397                 \" dimensional\".format(coord, coord_var.ndim)\n5398             )\n5399 \n5400         dim = coord_var.dims[0]\n5401         if _contains_datetime_like_objects(coord_var):\n5402             if coord_var.dtype.kind in \"mM\" and datetime_unit is None:\n5403                 datetime_unit, _ = np.datetime_data(coord_var.dtype)\n5404             elif datetime_unit is None:\n5405                 datetime_unit = \"s\"  # Default to seconds for cftime objects\n5406             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)\n5407 \n5408         variables = {}\n5409         for k, v in self.variables.items():\n5410             if k in self.data_vars and dim in v.dims and k not in self.coords:\n5411                 if _contains_datetime_like_objects(v):\n5412                     v = v._to_numeric(datetime_unit=datetime_unit)\n5413                 grad = duck_array_ops.gradient(\n5414                     v.data, coord_var, edge_order=edge_order, axis=v.get_axis_num(dim)\n5415                 )\n5416                 variables[k] = Variable(v.dims, grad)\n5417             else:\n5418                 variables[k] = v\n5419         return self._replace(variables)\n5420 \n5421     def integrate(self, coord, datetime_unit=None):\n5422         \"\"\" integrate the array with the trapezoidal rule.\n5423 \n5424         .. note::\n5425             This feature is limited to simple cartesian geometry, i.e. coord\n5426             must be one dimensional.\n5427 \n5428         Parameters\n5429         ----------\n5430         coord: str, or a sequence of str\n5431             Coordinate(s) used for the integration.\n5432         datetime_unit\n5433             Can be specify the unit if datetime coordinate is used. One of\n5434             {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', 'ps', 'fs',\n5435             'as'}\n5436 \n5437         Returns\n5438         -------\n5439         integrated: Dataset\n5440 \n5441         See also\n5442         --------\n5443         DataArray.integrate\n5444         numpy.trapz: corresponding numpy function\n5445 \n5446         Examples\n5447         --------\n5448         >>> ds = xr.Dataset(\n5449         ...     data_vars={\"a\": (\"x\", [5, 5, 6, 6]), \"b\": (\"x\", [1, 2, 1, 0])},\n5450         ...     coords={\"x\": [0, 1, 2, 3], \"y\": (\"x\", [1, 7, 3, 5])},\n5451         ... )\n5452         >>> ds\n5453         <xarray.Dataset>\n5454         Dimensions:  (x: 4)\n5455         Coordinates:\n5456           * x        (x) int64 0 1 2 3\n5457             y        (x) int64 1 7 3 5\n5458         Data variables:\n5459             a        (x) int64 5 5 6 6\n5460             b        (x) int64 1 2 1 0\n5461         >>> ds.integrate(\"x\")\n5462         <xarray.Dataset>\n5463         Dimensions:  ()\n5464         Data variables:\n5465             a        float64 16.5\n5466             b        float64 3.5\n5467         >>> ds.integrate(\"y\")\n5468         <xarray.Dataset>\n5469         Dimensions:  ()\n5470         Data variables:\n5471             a        float64 20.0\n5472             b        float64 4.0\n5473         \"\"\"\n5474         if not isinstance(coord, (list, tuple)):\n5475             coord = (coord,)\n5476         result = self\n5477         for c in coord:\n5478             result = result._integrate_one(c, datetime_unit=datetime_unit)\n5479         return result\n5480 \n5481     def _integrate_one(self, coord, datetime_unit=None):\n5482         from .variable import Variable\n5483 \n5484         if coord not in self.variables and coord not in self.dims:\n5485             raise ValueError(f\"Coordinate {coord} does not exist.\")\n5486 \n5487         coord_var = self[coord].variable\n5488         if coord_var.ndim != 1:\n5489             raise ValueError(\n5490                 \"Coordinate {} must be 1 dimensional but is {}\"\n5491                 \" dimensional\".format(coord, coord_var.ndim)\n5492             )\n5493 \n5494         dim = coord_var.dims[0]\n5495         if _contains_datetime_like_objects(coord_var):\n5496             if coord_var.dtype.kind in \"mM\" and datetime_unit is None:\n5497                 datetime_unit, _ = np.datetime_data(coord_var.dtype)\n5498             elif datetime_unit is None:\n5499                 datetime_unit = \"s\"  # Default to seconds for cftime objects\n5500             coord_var = coord_var._replace(\n5501                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)\n5502             )\n5503 \n5504         variables = {}\n5505         coord_names = set()\n5506         for k, v in self.variables.items():\n5507             if k in self.coords:\n5508                 if dim not in v.dims:\n5509                     variables[k] = v\n5510                     coord_names.add(k)\n5511             else:\n5512                 if k in self.data_vars and dim in v.dims:\n5513                     if _contains_datetime_like_objects(v):\n5514                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)\n5515                     integ = duck_array_ops.trapz(\n5516                         v.data, coord_var.data, axis=v.get_axis_num(dim)\n5517                     )\n5518                     v_dims = list(v.dims)\n5519                     v_dims.remove(dim)\n5520                     variables[k] = Variable(v_dims, integ)\n5521                 else:\n5522                     variables[k] = v\n5523         indexes = {k: v for k, v in self.indexes.items() if k in variables}\n5524         return self._replace_with_new_dims(\n5525             variables, coord_names=coord_names, indexes=indexes\n5526         )\n5527 \n5528     @property\n5529     def real(self):\n5530         return self._unary_op(lambda x: x.real, keep_attrs=True)(self)\n5531 \n5532     @property\n5533     def imag(self):\n5534         return self._unary_op(lambda x: x.imag, keep_attrs=True)(self)\n5535 \n5536     @property\n5537     def plot(self):\n5538         \"\"\"\n5539         Access plotting functions for Datasets.\n5540         Use it as a namespace to use xarray.plot functions as Dataset methods\n5541 \n5542         >>> ds.plot.scatter(...)  # equivalent to xarray.plot.scatter(ds,...)\n5543 \n5544         \"\"\"\n5545         return _Dataset_PlotMethods(self)\n5546 \n5547     def filter_by_attrs(self, **kwargs):\n5548         \"\"\"Returns a ``Dataset`` with variables that match specific conditions.\n5549 \n5550         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned\n5551         containing only the variables for which all the filter tests pass.\n5552         These tests are either ``key=value`` for which the attribute ``key``\n5553         has the exact value ``value`` or the callable passed into\n5554         ``key=callable`` returns True. The callable will be passed a single\n5555         value, either the value of the attribute ``key`` or ``None`` if the\n5556         DataArray does not have an attribute with the name ``key``.\n5557 \n5558         Parameters\n5559         ----------\n5560         **kwargs : key=value\n5561             key : str\n5562                 Attribute name.\n5563             value : callable or obj\n5564                 If value is a callable, it should return a boolean in the form\n5565                 of bool = func(attr) where attr is da.attrs[key].\n5566                 Otherwise, value will be compared to the each\n5567                 DataArray's attrs[key].\n5568 \n5569         Returns\n5570         -------\n5571         new : Dataset\n5572             New dataset with variables filtered by attribute.\n5573 \n5574         Examples\n5575         --------\n5576         >>> # Create an example dataset:\n5577         >>> import numpy as np\n5578         >>> import pandas as pd\n5579         >>> import xarray as xr\n5580         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)\n5581         >>> precip = 10 * np.random.rand(2, 2, 3)\n5582         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]\n5583         >>> lat = [[42.25, 42.21], [42.63, 42.59]]\n5584         >>> dims = [\"x\", \"y\", \"time\"]\n5585         >>> temp_attr = dict(standard_name=\"air_potential_temperature\")\n5586         >>> precip_attr = dict(standard_name=\"convective_precipitation_flux\")\n5587         >>> ds = xr.Dataset(\n5588         ...     {\n5589         ...         \"temperature\": (dims, temp, temp_attr),\n5590         ...         \"precipitation\": (dims, precip, precip_attr),\n5591         ...     },\n5592         ...     coords={\n5593         ...         \"lon\": ([\"x\", \"y\"], lon),\n5594         ...         \"lat\": ([\"x\", \"y\"], lat),\n5595         ...         \"time\": pd.date_range(\"2014-09-06\", periods=3),\n5596         ...         \"reference_time\": pd.Timestamp(\"2014-09-05\"),\n5597         ...     },\n5598         ... )\n5599         >>> # Get variables matching a specific standard_name.\n5600         >>> ds.filter_by_attrs(standard_name=\"convective_precipitation_flux\")\n5601         <xarray.Dataset>\n5602         Dimensions:         (time: 3, x: 2, y: 2)\n5603         Coordinates:\n5604           * x               (x) int64 0 1\n5605           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n5606             lat             (x, y) float64 42.25 42.21 42.63 42.59\n5607           * y               (y) int64 0 1\n5608             reference_time  datetime64[ns] 2014-09-05\n5609             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n5610         Data variables:\n5611             precipitation   (x, y, time) float64 4.178 2.307 6.041 6.046 0.06648 ...\n5612         >>> # Get all variables that have a standard_name attribute.\n5613         >>> standard_name = lambda v: v is not None\n5614         >>> ds.filter_by_attrs(standard_name=standard_name)\n5615         <xarray.Dataset>\n5616         Dimensions:         (time: 3, x: 2, y: 2)\n5617         Coordinates:\n5618             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n5619             lat             (x, y) float64 42.25 42.21 42.63 42.59\n5620           * x               (x) int64 0 1\n5621           * y               (y) int64 0 1\n5622           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n5623             reference_time  datetime64[ns] 2014-09-05\n5624         Data variables:\n5625             temperature     (x, y, time) float64 25.86 20.82 6.954 23.13 10.25 11.68 ...\n5626             precipitation   (x, y, time) float64 5.702 0.9422 2.075 1.178 3.284 ...\n5627 \n5628         \"\"\"\n5629         selection = []\n5630         for var_name, variable in self.variables.items():\n5631             has_value_flag = False\n5632             for attr_name, pattern in kwargs.items():\n5633                 attr_value = variable.attrs.get(attr_name)\n5634                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:\n5635                     has_value_flag = True\n5636                 else:\n5637                     has_value_flag = False\n5638                     break\n5639             if has_value_flag is True:\n5640                 selection.append(var_name)\n5641         return self[selection]\n5642 \n5643     def unify_chunks(self) -> \"Dataset\":\n5644         \"\"\" Unify chunk size along all chunked dimensions of this Dataset.\n5645 \n5646         Returns\n5647         -------\n5648 \n5649         Dataset with consistent chunk sizes for all dask-array variables\n5650 \n5651         See Also\n5652         --------\n5653 \n5654         dask.array.core.unify_chunks\n5655         \"\"\"\n5656 \n5657         try:\n5658             self.chunks\n5659         except ValueError:  # \"inconsistent chunks\"\n5660             pass\n5661         else:\n5662             # No variables with dask backend, or all chunks are already aligned\n5663             return self.copy()\n5664 \n5665         # import dask is placed after the quick exit test above to allow\n5666         # running this method if dask isn't installed and there are no chunks\n5667         import dask.array\n5668 \n5669         ds = self.copy()\n5670 \n5671         dims_pos_map = {dim: index for index, dim in enumerate(ds.dims)}\n5672 \n5673         dask_array_names = []\n5674         dask_unify_args = []\n5675         for name, variable in ds.variables.items():\n5676             if isinstance(variable.data, dask.array.Array):\n5677                 dims_tuple = [dims_pos_map[dim] for dim in variable.dims]\n5678                 dask_array_names.append(name)\n5679                 dask_unify_args.append(variable.data)\n5680                 dask_unify_args.append(dims_tuple)\n5681 \n5682         _, rechunked_arrays = dask.array.core.unify_chunks(*dask_unify_args)\n5683 \n5684         for name, new_array in zip(dask_array_names, rechunked_arrays):\n5685             ds.variables[name]._data = new_array\n5686 \n5687         return ds\n5688 \n5689     def map_blocks(\n5690         self,\n5691         func: \"Callable[..., T_DSorDA]\",\n5692         args: Sequence[Any] = (),\n5693         kwargs: Mapping[str, Any] = None,\n5694     ) -> \"T_DSorDA\":\n5695         \"\"\"\n5696         Apply a function to each chunk of this Dataset. This method is experimental and\n5697         its signature may change.\n5698 \n5699         Parameters\n5700         ----------\n5701         func: callable\n5702             User-provided function that accepts a Dataset as its first parameter. The\n5703             function will receive a subset of this Dataset, corresponding to one chunk\n5704             along each chunked dimension. ``func`` will be executed as\n5705             ``func(obj_subset, *args, **kwargs)``.\n5706 \n5707             The function will be first run on mocked-up data, that looks like this\n5708             Dataset but has sizes 0, to determine properties of the returned object such\n5709             as dtype, variable names, new dimensions and new indexes (if any).\n5710 \n5711             This function must return either a single DataArray or a single Dataset.\n5712 \n5713             This function cannot change size of existing dimensions, or add new chunked\n5714             dimensions.\n5715         args: Sequence\n5716             Passed verbatim to func after unpacking, after the sliced DataArray. xarray\n5717             objects, if any, will not be split by chunks. Passing dask collections is\n5718             not allowed.\n5719         kwargs: Mapping\n5720             Passed verbatim to func after unpacking. xarray objects, if any, will not be\n5721             split by chunks. Passing dask collections is not allowed.\n5722 \n5723         Returns\n5724         -------\n5725         A single DataArray or Dataset with dask backend, reassembled from the outputs of\n5726         the function.\n5727 \n5728         Notes\n5729         -----\n5730         This method is designed for when one needs to manipulate a whole xarray object\n5731         within each chunk. In the more common case where one can work on numpy arrays,\n5732         it is recommended to use apply_ufunc.\n5733 \n5734         If none of the variables in this Dataset is backed by dask, calling this method\n5735         is equivalent to calling ``func(self, *args, **kwargs)``.\n5736 \n5737         See Also\n5738         --------\n5739         dask.array.map_blocks, xarray.apply_ufunc, xarray.map_blocks,\n5740         xarray.DataArray.map_blocks\n5741         \"\"\"\n5742         from .parallel import map_blocks\n5743 \n5744         return map_blocks(func, self, args, kwargs)\n",
        "intended_behavior": "Implement a new method named 'average' (or 'weighted_average' for clarity) that allows for computing weighted averages across specified dimensions of the dataset. This method should accept parameters for specifying the axes along which to compute the averages and an optional weight parameter for each dimension. It should handle missing data gracefully, likely reusing or extending patterns seen in 'interpolate_na'."
    },
    {
        "rel_file_path": "xarray/core/dataarray.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/dataarray.py",
        "start": 215,
        "end": 3263,
        "class_name": "DataArray",
        "method_name": null,
        "code": "215 class DataArray(AbstractArray, DataWithCoords):\n216     \"\"\"N-dimensional array with labeled coordinates and dimensions.\n217 \n218     DataArray provides a wrapper around numpy ndarrays that uses labeled\n219     dimensions and coordinates to support metadata aware operations. The API is\n220     similar to that for the pandas Series or DataFrame, but DataArray objects\n221     can have any number of dimensions, and their contents have fixed data\n222     types.\n223 \n224     Additional features over raw numpy arrays:\n225 \n226     - Apply operations over dimensions by name: ``x.sum('time')``.\n227     - Select or assign values by integer location (like numpy): ``x[:10]``\n228       or by label (like pandas): ``x.loc['2014-01-01']`` or\n229       ``x.sel(time='2014-01-01')``.\n230     - Mathematical operations (e.g., ``x - y``) vectorize across multiple\n231       dimensions (known in numpy as \"broadcasting\") based on dimension names,\n232       regardless of their original order.\n233     - Keep track of arbitrary metadata in the form of a Python dictionary:\n234       ``x.attrs``\n235     - Convert to a pandas Series: ``x.to_series()``.\n236 \n237     Getting items from or doing mathematical operations with a DataArray\n238     always returns another DataArray.\n239     \"\"\"\n240 \n241     _cache: Dict[str, Any]\n242     _coords: Dict[Any, Variable]\n243     _indexes: Optional[Dict[Hashable, pd.Index]]\n244     _name: Optional[Hashable]\n245     _variable: Variable\n246 \n247     __slots__ = (\n248         \"_cache\",\n249         \"_coords\",\n250         \"_file_obj\",\n251         \"_indexes\",\n252         \"_name\",\n253         \"_variable\",\n254         \"__weakref__\",\n255     )\n256 \n257     _groupby_cls = groupby.DataArrayGroupBy\n258     _rolling_cls = rolling.DataArrayRolling\n259     _coarsen_cls = rolling.DataArrayCoarsen\n260     _resample_cls = resample.DataArrayResample\n261 \n262     dt = property(CombinedDatetimelikeAccessor)\n263 \n264     def __init__(\n265         self,\n266         data: Any = dtypes.NA,\n267         coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None] = None,\n268         dims: Union[Hashable, Sequence[Hashable], None] = None,\n269         name: Hashable = None,\n270         attrs: Mapping = None,\n271         # internal parameters\n272         indexes: Dict[Hashable, pd.Index] = None,\n273         fastpath: bool = False,\n274     ):\n275         \"\"\"\n276         Parameters\n277         ----------\n278         data : array_like\n279             Values for this array. Must be an ``numpy.ndarray``, ndarray like,\n280             or castable to an ``ndarray``. If a self-described xarray or pandas\n281             object, attempts are made to use this array's metadata to fill in\n282             other unspecified arguments. A view of the array's data is used\n283             instead of a copy if possible.\n284         coords : sequence or dict of array_like objects, optional\n285             Coordinates (tick labels) to use for indexing along each dimension.\n286             The following notations are accepted:\n287 \n288             - mapping {dimension name: array-like}\n289             - sequence of tuples that are valid arguments for xarray.Variable()\n290               - (dims, data)\n291               - (dims, data, attrs)\n292               - (dims, data, attrs, encoding)\n293 \n294             Additionally, it is possible to define a coord whose name\n295             does not match the dimension name, or a coord based on multiple\n296             dimensions, with one of the following notations:\n297 \n298             - mapping {coord name: DataArray}\n299             - mapping {coord name: Variable}\n300             - mapping {coord name: (dimension name, array-like)}\n301             - mapping {coord name: (tuple of dimension names, array-like)}\n302 \n303         dims : hashable or sequence of hashable, optional\n304             Name(s) of the data dimension(s). Must be either a hashable (only\n305             for 1D data) or a sequence of hashables with length equal to the\n306             number of dimensions. If this argument is omitted, dimension names\n307             default to ``['dim_0', ... 'dim_n']``.\n308         name : str or None, optional\n309             Name of this array.\n310         attrs : dict_like or None, optional\n311             Attributes to assign to the new instance. By default, an empty\n312             attribute dictionary is initialized.\n313         \"\"\"\n314         if fastpath:\n315             variable = data\n316             assert dims is None\n317             assert attrs is None\n318         else:\n319             # try to fill in arguments from data if they weren't supplied\n320             if coords is None:\n321 \n322                 if isinstance(data, DataArray):\n323                     coords = data.coords\n324                 elif isinstance(data, pd.Series):\n325                     coords = [data.index]\n326                 elif isinstance(data, pd.DataFrame):\n327                     coords = [data.index, data.columns]\n328                 elif isinstance(data, (pd.Index, IndexVariable)):\n329                     coords = [data]\n330                 elif isinstance(data, pdcompat.Panel):\n331                     coords = [data.items, data.major_axis, data.minor_axis]\n332 \n333             if dims is None:\n334                 dims = getattr(data, \"dims\", getattr(coords, \"dims\", None))\n335             if name is None:\n336                 name = getattr(data, \"name\", None)\n337             if attrs is None and not isinstance(data, PANDAS_TYPES):\n338                 attrs = getattr(data, \"attrs\", None)\n339 \n340             data = _check_data_shape(data, coords, dims)\n341             data = as_compatible_data(data)\n342             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)\n343             variable = Variable(dims, data, attrs, fastpath=True)\n344             indexes = dict(\n345                 _extract_indexes_from_coords(coords)\n346             )  # needed for to_dataset\n347 \n348         # These fully describe a DataArray\n349         self._variable = variable\n350         assert isinstance(coords, dict)\n351         self._coords = coords\n352         self._name = name\n353 \n354         # TODO(shoyer): document this argument, once it becomes part of the\n355         # public interface.\n356         self._indexes = indexes\n357 \n358         self._file_obj = None\n359 \n360     def _replace(\n361         self,\n362         variable: Variable = None,\n363         coords=None,\n364         name: Union[Hashable, None, Default] = _default,\n365         indexes=None,\n366     ) -> \"DataArray\":\n367         if variable is None:\n368             variable = self.variable\n369         if coords is None:\n370             coords = self._coords\n371         if name is _default:\n372             name = self.name\n373         return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)\n374 \n375     def _replace_maybe_drop_dims(\n376         self, variable: Variable, name: Union[Hashable, None, Default] = _default\n377     ) -> \"DataArray\":\n378         if variable.dims == self.dims and variable.shape == self.shape:\n379             coords = self._coords.copy()\n380             indexes = self._indexes\n381         elif variable.dims == self.dims:\n382             # Shape has changed (e.g. from reduce(..., keepdims=True)\n383             new_sizes = dict(zip(self.dims, variable.shape))\n384             coords = {\n385                 k: v\n386                 for k, v in self._coords.items()\n387                 if v.shape == tuple(new_sizes[d] for d in v.dims)\n388             }\n389             changed_dims = [\n390                 k for k in variable.dims if variable.sizes[k] != self.sizes[k]\n391             ]\n392             indexes = propagate_indexes(self._indexes, exclude=changed_dims)\n393         else:\n394             allowed_dims = set(variable.dims)\n395             coords = {\n396                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims\n397             }\n398             indexes = propagate_indexes(\n399                 self._indexes, exclude=(set(self.dims) - allowed_dims)\n400             )\n401         return self._replace(variable, coords, name, indexes=indexes)\n402 \n403     def _overwrite_indexes(self, indexes: Mapping[Hashable, Any]) -> \"DataArray\":\n404         if not len(indexes):\n405             return self\n406         coords = self._coords.copy()\n407         for name, idx in indexes.items():\n408             coords[name] = IndexVariable(name, idx)\n409         obj = self._replace(coords=coords)\n410 \n411         # switch from dimension to level names, if necessary\n412         dim_names: Dict[Any, str] = {}\n413         for dim, idx in indexes.items():\n414             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:\n415                 dim_names[dim] = idx.name\n416         if dim_names:\n417             obj = obj.rename(dim_names)\n418         return obj\n419 \n420     def _to_temp_dataset(self) -> Dataset:\n421         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)\n422 \n423     def _from_temp_dataset(\n424         self, dataset: Dataset, name: Hashable = _default\n425     ) -> \"DataArray\":\n426         variable = dataset._variables.pop(_THIS_ARRAY)\n427         coords = dataset._variables\n428         indexes = dataset._indexes\n429         return self._replace(variable, coords, name, indexes=indexes)\n430 \n431     def _to_dataset_split(self, dim: Hashable) -> Dataset:\n432         \"\"\" splits dataarray along dimension 'dim' \"\"\"\n433 \n434         def subset(dim, label):\n435             array = self.loc[{dim: label}]\n436             array.attrs = {}\n437             return as_variable(array)\n438 \n439         variables = {label: subset(dim, label) for label in self.get_index(dim)}\n440         variables.update({k: v for k, v in self._coords.items() if k != dim})\n441         indexes = propagate_indexes(self._indexes, exclude=dim)\n442         coord_names = set(self._coords) - set([dim])\n443         dataset = Dataset._construct_direct(\n444             variables, coord_names, indexes=indexes, attrs=self.attrs\n445         )\n446         return dataset\n447 \n448     def _to_dataset_whole(\n449         self, name: Hashable = None, shallow_copy: bool = True\n450     ) -> Dataset:\n451         if name is None:\n452             name = self.name\n453         if name is None:\n454             raise ValueError(\n455                 \"unable to convert unnamed DataArray to a \"\n456                 \"Dataset without providing an explicit name\"\n457             )\n458         if name in self.coords:\n459             raise ValueError(\n460                 \"cannot create a Dataset from a DataArray with \"\n461                 \"the same name as one of its coordinates\"\n462             )\n463         # use private APIs for speed: this is called by _to_temp_dataset(),\n464         # which is used in the guts of a lot of operations (e.g., reindex)\n465         variables = self._coords.copy()\n466         variables[name] = self.variable\n467         if shallow_copy:\n468             for k in variables:\n469                 variables[k] = variables[k].copy(deep=False)\n470         indexes = self._indexes\n471 \n472         coord_names = set(self._coords)\n473         dataset = Dataset._construct_direct(variables, coord_names, indexes=indexes)\n474         return dataset\n475 \n476     def to_dataset(self, dim: Hashable = None, *, name: Hashable = None) -> Dataset:\n477         \"\"\"Convert a DataArray to a Dataset.\n478 \n479         Parameters\n480         ----------\n481         dim : hashable, optional\n482             Name of the dimension on this array along which to split this array\n483             into separate variables. If not provided, this array is converted\n484             into a Dataset of one variable.\n485         name : hashable, optional\n486             Name to substitute for this array's name. Only valid if ``dim`` is\n487             not provided.\n488 \n489         Returns\n490         -------\n491         dataset : Dataset\n492         \"\"\"\n493         if dim is not None and dim not in self.dims:\n494             raise TypeError(\n495                 f\"{dim} is not a dim. If supplying a ``name``, pass as a kwarg.\"\n496             )\n497 \n498         if dim is not None:\n499             if name is not None:\n500                 raise TypeError(\"cannot supply both dim and name arguments\")\n501             return self._to_dataset_split(dim)\n502         else:\n503             return self._to_dataset_whole(name)\n504 \n505     @property\n506     def name(self) -> Optional[Hashable]:\n507         \"\"\"The name of this array.\n508         \"\"\"\n509         return self._name\n510 \n511     @name.setter\n512     def name(self, value: Optional[Hashable]) -> None:\n513         self._name = value\n514 \n515     @property\n516     def variable(self) -> Variable:\n517         \"\"\"Low level interface to the Variable object for this DataArray.\"\"\"\n518         return self._variable\n519 \n520     @property\n521     def dtype(self) -> np.dtype:\n522         return self.variable.dtype\n523 \n524     @property\n525     def shape(self) -> Tuple[int, ...]:\n526         return self.variable.shape\n527 \n528     @property\n529     def size(self) -> int:\n530         return self.variable.size\n531 \n532     @property\n533     def nbytes(self) -> int:\n534         return self.variable.nbytes\n535 \n536     @property\n537     def ndim(self) -> int:\n538         return self.variable.ndim\n539 \n540     def __len__(self) -> int:\n541         return len(self.variable)\n542 \n543     @property\n544     def data(self) -> Any:\n545         \"\"\"The array's data as a dask or numpy array\n546         \"\"\"\n547         return self.variable.data\n548 \n549     @data.setter\n550     def data(self, value: Any) -> None:\n551         self.variable.data = value\n552 \n553     @property\n554     def values(self) -> np.ndarray:\n555         \"\"\"The array's data as a numpy.ndarray\"\"\"\n556         return self.variable.values\n557 \n558     @values.setter\n559     def values(self, value: Any) -> None:\n560         self.variable.values = value\n561 \n562     @property\n563     def _in_memory(self) -> bool:\n564         return self.variable._in_memory\n565 \n566     def to_index(self) -> pd.Index:\n567         \"\"\"Convert this variable to a pandas.Index. Only possible for 1D\n568         arrays.\n569         \"\"\"\n570         return self.variable.to_index()\n571 \n572     @property\n573     def dims(self) -> Tuple[Hashable, ...]:\n574         \"\"\"Tuple of dimension names associated with this array.\n575 \n576         Note that the type of this property is inconsistent with\n577         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for\n578         consistently named properties.\n579         \"\"\"\n580         return self.variable.dims\n581 \n582     @dims.setter\n583     def dims(self, value):\n584         raise AttributeError(\n585             \"you cannot assign dims on a DataArray. Use \"\n586             \".rename() or .swap_dims() instead.\"\n587         )\n588 \n589     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:\n590         if utils.is_dict_like(key):\n591             return key\n592         else:\n593             key = indexing.expanded_indexer(key, self.ndim)\n594             return dict(zip(self.dims, key))\n595 \n596     @property\n597     def _level_coords(self) -> Dict[Hashable, Hashable]:\n598         \"\"\"Return a mapping of all MultiIndex levels and their corresponding\n599         coordinate name.\n600         \"\"\"\n601         level_coords: Dict[Hashable, Hashable] = {}\n602 \n603         for cname, var in self._coords.items():\n604             if var.ndim == 1 and isinstance(var, IndexVariable):\n605                 level_names = var.level_names\n606                 if level_names is not None:\n607                     (dim,) = var.dims\n608                     level_coords.update({lname: dim for lname in level_names})\n609         return level_coords\n610 \n611     def _getitem_coord(self, key):\n612         from .dataset import _get_virtual_variable\n613 \n614         try:\n615             var = self._coords[key]\n616         except KeyError:\n617             dim_sizes = dict(zip(self.dims, self.shape))\n618             _, key, var = _get_virtual_variable(\n619                 self._coords, key, self._level_coords, dim_sizes\n620             )\n621 \n622         return self._replace_maybe_drop_dims(var, name=key)\n623 \n624     def __getitem__(self, key: Any) -> \"DataArray\":\n625         if isinstance(key, str):\n626             return self._getitem_coord(key)\n627         else:\n628             # xarray-style array indexing\n629             return self.isel(indexers=self._item_key_to_dict(key))\n630 \n631     def __setitem__(self, key: Any, value: Any) -> None:\n632         if isinstance(key, str):\n633             self.coords[key] = value\n634         else:\n635             # Coordinates in key, value and self[key] should be consistent.\n636             # TODO Coordinate consistency in key is checked here, but it\n637             # causes unnecessary indexing. It should be optimized.\n638             obj = self[key]\n639             if isinstance(value, DataArray):\n640                 assert_coordinate_consistent(value, obj.coords.variables)\n641             # DataArray key -> Variable key\n642             key = {\n643                 k: v.variable if isinstance(v, DataArray) else v\n644                 for k, v in self._item_key_to_dict(key).items()\n645             }\n646             self.variable[key] = value\n647 \n648     def __delitem__(self, key: Any) -> None:\n649         del self.coords[key]\n650 \n651     @property\n652     def _attr_sources(self) -> List[Mapping[Hashable, Any]]:\n653         \"\"\"List of places to look-up items for attribute-style access\n654         \"\"\"\n655         return self._item_sources + [self.attrs]\n656 \n657     @property\n658     def _item_sources(self) -> List[Mapping[Hashable, Any]]:\n659         \"\"\"List of places to look-up items for key-completion\n660         \"\"\"\n661         return [\n662             self.coords,\n663             {d: self.coords[d] for d in self.dims},\n664             LevelCoordinatesSource(self),\n665         ]\n666 \n667     def __contains__(self, key: Any) -> bool:\n668         return key in self.data\n669 \n670     @property\n671     def loc(self) -> _LocIndexer:\n672         \"\"\"Attribute for location based indexing like pandas.\n673         \"\"\"\n674         return _LocIndexer(self)\n675 \n676     @property\n677     def attrs(self) -> Dict[Hashable, Any]:\n678         \"\"\"Dictionary storing arbitrary metadata with this array.\"\"\"\n679         return self.variable.attrs\n680 \n681     @attrs.setter\n682     def attrs(self, value: Mapping[Hashable, Any]) -> None:\n683         # Disable type checking to work around mypy bug - see mypy#4167\n684         self.variable.attrs = value  # type: ignore\n685 \n686     @property\n687     def encoding(self) -> Dict[Hashable, Any]:\n688         \"\"\"Dictionary of format-specific settings for how this array should be\n689         serialized.\"\"\"\n690         return self.variable.encoding\n691 \n692     @encoding.setter\n693     def encoding(self, value: Mapping[Hashable, Any]) -> None:\n694         self.variable.encoding = value\n695 \n696     @property\n697     def indexes(self) -> Indexes:\n698         \"\"\"Mapping of pandas.Index objects used for label based indexing\n699         \"\"\"\n700         if self._indexes is None:\n701             self._indexes = default_indexes(self._coords, self.dims)\n702         return Indexes(self._indexes)\n703 \n704     @property\n705     def coords(self) -> DataArrayCoordinates:\n706         \"\"\"Dictionary-like container of coordinate arrays.\n707         \"\"\"\n708         return DataArrayCoordinates(self)\n709 \n710     def reset_coords(\n711         self,\n712         names: Union[Iterable[Hashable], Hashable, None] = None,\n713         drop: bool = False,\n714         inplace: bool = None,\n715     ) -> Union[None, \"DataArray\", Dataset]:\n716         \"\"\"Given names of coordinates, reset them to become variables.\n717 \n718         Parameters\n719         ----------\n720         names : hashable or iterable of hashables, optional\n721             Name(s) of non-index coordinates in this dataset to reset into\n722             variables. By default, all non-index coordinates are reset.\n723         drop : bool, optional\n724             If True, remove coordinates instead of converting them into\n725             variables.\n726 \n727         Returns\n728         -------\n729         Dataset, or DataArray if ``drop == True``\n730         \"\"\"\n731         _check_inplace(inplace)\n732         if names is None:\n733             names = set(self.coords) - set(self.dims)\n734         dataset = self.coords.to_dataset().reset_coords(names, drop)\n735         if drop:\n736             return self._replace(coords=dataset._variables)\n737         else:\n738             if self.name is None:\n739                 raise ValueError(\n740                     \"cannot reset_coords with drop=False on an unnamed DataArrray\"\n741                 )\n742             dataset[self.name] = self.variable\n743             return dataset\n744 \n745     def __dask_tokenize__(self):\n746         from dask.base import normalize_token\n747 \n748         return normalize_token((type(self), self._variable, self._coords, self._name))\n749 \n750     def __dask_graph__(self):\n751         return self._to_temp_dataset().__dask_graph__()\n752 \n753     def __dask_keys__(self):\n754         return self._to_temp_dataset().__dask_keys__()\n755 \n756     def __dask_layers__(self):\n757         return self._to_temp_dataset().__dask_layers__()\n758 \n759     @property\n760     def __dask_optimize__(self):\n761         return self._to_temp_dataset().__dask_optimize__\n762 \n763     @property\n764     def __dask_scheduler__(self):\n765         return self._to_temp_dataset().__dask_scheduler__\n766 \n767     def __dask_postcompute__(self):\n768         func, args = self._to_temp_dataset().__dask_postcompute__()\n769         return self._dask_finalize, (func, args, self.name)\n770 \n771     def __dask_postpersist__(self):\n772         func, args = self._to_temp_dataset().__dask_postpersist__()\n773         return self._dask_finalize, (func, args, self.name)\n774 \n775     @staticmethod\n776     def _dask_finalize(results, func, args, name):\n777         ds = func(results, *args)\n778         variable = ds._variables.pop(_THIS_ARRAY)\n779         coords = ds._variables\n780         return DataArray(variable, coords, name=name, fastpath=True)\n781 \n782     def load(self, **kwargs) -> \"DataArray\":\n783         \"\"\"Manually trigger loading of this array's data from disk or a\n784         remote source into memory and return this array.\n785 \n786         Normally, it should not be necessary to call this method in user code,\n787         because all xarray functions should either work on deferred data or\n788         load data automatically. However, this method can be necessary when\n789         working with many file objects on disk.\n790 \n791         Parameters\n792         ----------\n793         **kwargs : dict\n794             Additional keyword arguments passed on to ``dask.array.compute``.\n795 \n796         See Also\n797         --------\n798         dask.array.compute\n799         \"\"\"\n800         ds = self._to_temp_dataset().load(**kwargs)\n801         new = self._from_temp_dataset(ds)\n802         self._variable = new._variable\n803         self._coords = new._coords\n804         return self\n805 \n806     def compute(self, **kwargs) -> \"DataArray\":\n807         \"\"\"Manually trigger loading of this array's data from disk or a\n808         remote source into memory and return a new array. The original is\n809         left unaltered.\n810 \n811         Normally, it should not be necessary to call this method in user code,\n812         because all xarray functions should either work on deferred data or\n813         load data automatically. However, this method can be necessary when\n814         working with many file objects on disk.\n815 \n816         Parameters\n817         ----------\n818         **kwargs : dict\n819             Additional keyword arguments passed on to ``dask.array.compute``.\n820 \n821         See Also\n822         --------\n823         dask.array.compute\n824         \"\"\"\n825         new = self.copy(deep=False)\n826         return new.load(**kwargs)\n827 \n828     def persist(self, **kwargs) -> \"DataArray\":\n829         \"\"\" Trigger computation in constituent dask arrays\n830 \n831         This keeps them as dask arrays but encourages them to keep data in\n832         memory.  This is particularly useful when on a distributed machine.\n833         When on a single machine consider using ``.compute()`` instead.\n834 \n835         Parameters\n836         ----------\n837         **kwargs : dict\n838             Additional keyword arguments passed on to ``dask.persist``.\n839 \n840         See Also\n841         --------\n842         dask.persist\n843         \"\"\"\n844         ds = self._to_temp_dataset().persist(**kwargs)\n845         return self._from_temp_dataset(ds)\n846 \n847     def copy(self, deep: bool = True, data: Any = None) -> \"DataArray\":\n848         \"\"\"Returns a copy of this array.\n849 \n850         If `deep=True`, a deep copy is made of the data array.\n851         Otherwise, a shallow copy is made, so each variable in the new\n852         array's dataset is also a variable in this array's dataset.\n853 \n854         Use `data` to create a new object with the same structure as\n855         original but entirely new data.\n856 \n857         Parameters\n858         ----------\n859         deep : bool, optional\n860             Whether the data array and its coordinates are loaded into memory\n861             and copied onto the new object. Default is True.\n862         data : array_like, optional\n863             Data to use in the new object. Must have same shape as original.\n864             When `data` is used, `deep` is ignored for all data variables,\n865             and only used for coords.\n866 \n867         Returns\n868         -------\n869         object : DataArray\n870             New object with dimensions, attributes, coordinates, name,\n871             encoding, and optionally data copied from original.\n872 \n873         Examples\n874         --------\n875 \n876         Shallow versus deep copy\n877 \n878         >>> array = xr.DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\"]})\n879         >>> array.copy()\n880         <xarray.DataArray (x: 3)>\n881         array([1, 2, 3])\n882         Coordinates:\n883         * x        (x) <U1 'a' 'b' 'c'\n884         >>> array_0 = array.copy(deep=False)\n885         >>> array_0[0] = 7\n886         >>> array_0\n887         <xarray.DataArray (x: 3)>\n888         array([7, 2, 3])\n889         Coordinates:\n890         * x        (x) <U1 'a' 'b' 'c'\n891         >>> array\n892         <xarray.DataArray (x: 3)>\n893         array([7, 2, 3])\n894         Coordinates:\n895         * x        (x) <U1 'a' 'b' 'c'\n896 \n897         Changing the data using the ``data`` argument maintains the\n898         structure of the original object, but with the new data. Original\n899         object is unaffected.\n900 \n901         >>> array.copy(data=[0.1, 0.2, 0.3])\n902         <xarray.DataArray (x: 3)>\n903         array([ 0.1,  0.2,  0.3])\n904         Coordinates:\n905         * x        (x) <U1 'a' 'b' 'c'\n906         >>> array\n907         <xarray.DataArray (x: 3)>\n908         array([1, 2, 3])\n909         Coordinates:\n910         * x        (x) <U1 'a' 'b' 'c'\n911 \n912         See Also\n913         --------\n914         pandas.DataFrame.copy\n915         \"\"\"\n916         variable = self.variable.copy(deep=deep, data=data)\n917         coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}\n918         indexes = self._indexes\n919         return self._replace(variable, coords, indexes=indexes)\n920 \n921     def __copy__(self) -> \"DataArray\":\n922         return self.copy(deep=False)\n923 \n924     def __deepcopy__(self, memo=None) -> \"DataArray\":\n925         # memo does nothing but is required for compatibility with\n926         # copy.deepcopy\n927         return self.copy(deep=True)\n928 \n929     # mutable objects should not be hashable\n930     # https://github.com/python/mypy/issues/4266\n931     __hash__ = None  # type: ignore\n932 \n933     @property\n934     def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:\n935         \"\"\"Block dimensions for this array's data or None if it's not a dask\n936         array.\n937         \"\"\"\n938         return self.variable.chunks\n939 \n940     def chunk(\n941         self,\n942         chunks: Union[\n943             None,\n944             Number,\n945             Tuple[Number, ...],\n946             Tuple[Tuple[Number, ...], ...],\n947             Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]],\n948         ] = None,\n949         name_prefix: str = \"xarray-\",\n950         token: str = None,\n951         lock: bool = False,\n952     ) -> \"DataArray\":\n953         \"\"\"Coerce this array's data into a dask arrays with the given chunks.\n954 \n955         If this variable is a non-dask array, it will be converted to dask\n956         array. If it's a dask array, it will be rechunked to the given chunk\n957         sizes.\n958 \n959         If neither chunks is not provided for one or more dimensions, chunk\n960         sizes along that dimension will not be updated; non-dask arrays will be\n961         converted into dask arrays with a single block.\n962 \n963         Parameters\n964         ----------\n965         chunks : int, tuple or mapping, optional\n966             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or\n967             ``{'x': 5, 'y': 5}``.\n968         name_prefix : str, optional\n969             Prefix for the name of the new dask array.\n970         token : str, optional\n971             Token uniquely identifying this array.\n972         lock : optional\n973             Passed on to :py:func:`dask.array.from_array`, if the array is not\n974             already as dask array.\n975 \n976         Returns\n977         -------\n978         chunked : xarray.DataArray\n979         \"\"\"\n980         if isinstance(chunks, (tuple, list)):\n981             chunks = dict(zip(self.dims, chunks))\n982 \n983         ds = self._to_temp_dataset().chunk(\n984             chunks, name_prefix=name_prefix, token=token, lock=lock\n985         )\n986         return self._from_temp_dataset(ds)\n987 \n988     def isel(\n989         self,\n990         indexers: Mapping[Hashable, Any] = None,\n991         drop: bool = False,\n992         **indexers_kwargs: Any,\n993     ) -> \"DataArray\":\n994         \"\"\"Return a new DataArray whose data is given by integer indexing\n995         along the specified dimension(s).\n996 \n997         See Also\n998         --------\n999         Dataset.isel\n1000         DataArray.sel\n1001         \"\"\"\n1002         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"isel\")\n1003         if any(is_fancy_indexer(idx) for idx in indexers.values()):\n1004             ds = self._to_temp_dataset()._isel_fancy(indexers, drop=drop)\n1005             return self._from_temp_dataset(ds)\n1006 \n1007         # Much faster algorithm for when all indexers are ints, slices, one-dimensional\n1008         # lists, or zero or one-dimensional np.ndarray's\n1009 \n1010         variable = self._variable.isel(indexers)\n1011 \n1012         coords = {}\n1013         for coord_name, coord_value in self._coords.items():\n1014             coord_indexers = {\n1015                 k: v for k, v in indexers.items() if k in coord_value.dims\n1016             }\n1017             if coord_indexers:\n1018                 coord_value = coord_value.isel(coord_indexers)\n1019                 if drop and coord_value.ndim == 0:\n1020                     continue\n1021             coords[coord_name] = coord_value\n1022 \n1023         return self._replace(variable=variable, coords=coords)\n1024 \n1025     def sel(\n1026         self,\n1027         indexers: Mapping[Hashable, Any] = None,\n1028         method: str = None,\n1029         tolerance=None,\n1030         drop: bool = False,\n1031         **indexers_kwargs: Any,\n1032     ) -> \"DataArray\":\n1033         \"\"\"Return a new DataArray whose data is given by selecting index\n1034         labels along the specified dimension(s).\n1035 \n1036         .. warning::\n1037 \n1038           Do not try to assign values when using any of the indexing methods\n1039           ``isel`` or ``sel``::\n1040 \n1041             da = xr.DataArray([0, 1, 2, 3], dims=['x'])\n1042             # DO NOT do this\n1043             da.isel(x=[0, 1, 2])[1] = -1\n1044 \n1045           Assigning values with the chained indexing using ``.sel`` or\n1046           ``.isel`` fails silently.\n1047 \n1048         See Also\n1049         --------\n1050         Dataset.sel\n1051         DataArray.isel\n1052 \n1053         \"\"\"\n1054         ds = self._to_temp_dataset().sel(\n1055             indexers=indexers,\n1056             drop=drop,\n1057             method=method,\n1058             tolerance=tolerance,\n1059             **indexers_kwargs,\n1060         )\n1061         return self._from_temp_dataset(ds)\n1062 \n1063     def head(\n1064         self,\n1065         indexers: Union[Mapping[Hashable, int], int] = None,\n1066         **indexers_kwargs: Any,\n1067     ) -> \"DataArray\":\n1068         \"\"\"Return a new DataArray whose data is given by the the first `n`\n1069         values along the specified dimension(s). Default `n` = 5\n1070 \n1071         See Also\n1072         --------\n1073         Dataset.head\n1074         DataArray.tail\n1075         DataArray.thin\n1076         \"\"\"\n1077         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)\n1078         return self._from_temp_dataset(ds)\n1079 \n1080     def tail(\n1081         self,\n1082         indexers: Union[Mapping[Hashable, int], int] = None,\n1083         **indexers_kwargs: Any,\n1084     ) -> \"DataArray\":\n1085         \"\"\"Return a new DataArray whose data is given by the the last `n`\n1086         values along the specified dimension(s). Default `n` = 5\n1087 \n1088         See Also\n1089         --------\n1090         Dataset.tail\n1091         DataArray.head\n1092         DataArray.thin\n1093         \"\"\"\n1094         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)\n1095         return self._from_temp_dataset(ds)\n1096 \n1097     def thin(\n1098         self,\n1099         indexers: Union[Mapping[Hashable, int], int] = None,\n1100         **indexers_kwargs: Any,\n1101     ) -> \"DataArray\":\n1102         \"\"\"Return a new DataArray whose data is given by each `n` value\n1103         along the specified dimension(s).\n1104 \n1105         See Also\n1106         --------\n1107         Dataset.thin\n1108         DataArray.head\n1109         DataArray.tail\n1110         \"\"\"\n1111         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)\n1112         return self._from_temp_dataset(ds)\n1113 \n1114     def broadcast_like(\n1115         self, other: Union[\"DataArray\", Dataset], exclude: Iterable[Hashable] = None\n1116     ) -> \"DataArray\":\n1117         \"\"\"Broadcast this DataArray against another Dataset or DataArray.\n1118 \n1119         This is equivalent to xr.broadcast(other, self)[1]\n1120 \n1121         xarray objects are broadcast against each other in arithmetic\n1122         operations, so this method is not be necessary for most uses.\n1123 \n1124         If no change is needed, the input data is returned to the output\n1125         without being copied.\n1126 \n1127         If new coords are added by the broadcast, their values are\n1128         NaN filled.\n1129 \n1130         Parameters\n1131         ----------\n1132         other : Dataset or DataArray\n1133             Object against which to broadcast this array.\n1134         exclude : iterable of hashable, optional\n1135             Dimensions that must not be broadcasted\n1136 \n1137         Returns\n1138         -------\n1139         new_da: xr.DataArray\n1140 \n1141         Examples\n1142         --------\n1143 \n1144         >>> arr1\n1145         <xarray.DataArray (x: 2, y: 3)>\n1146         array([[0.840235, 0.215216, 0.77917 ],\n1147                [0.726351, 0.543824, 0.875115]])\n1148         Coordinates:\n1149           * x        (x) <U1 'a' 'b'\n1150           * y        (y) <U1 'a' 'b' 'c'\n1151         >>> arr2\n1152         <xarray.DataArray (x: 3, y: 2)>\n1153         array([[0.612611, 0.125753],\n1154                [0.853181, 0.948818],\n1155                [0.180885, 0.33363 ]])\n1156         Coordinates:\n1157           * x        (x) <U1 'a' 'b' 'c'\n1158           * y        (y) <U1 'a' 'b'\n1159         >>> arr1.broadcast_like(arr2)\n1160         <xarray.DataArray (x: 3, y: 3)>\n1161         array([[0.840235, 0.215216, 0.77917 ],\n1162                [0.726351, 0.543824, 0.875115],\n1163                [     nan,      nan,      nan]])\n1164         Coordinates:\n1165           * x        (x) object 'a' 'b' 'c'\n1166           * y        (y) object 'a' 'b' 'c'\n1167         \"\"\"\n1168         if exclude is None:\n1169             exclude = set()\n1170         else:\n1171             exclude = set(exclude)\n1172         args = align(other, self, join=\"outer\", copy=False, exclude=exclude)\n1173 \n1174         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)\n1175 \n1176         return _broadcast_helper(args[1], exclude, dims_map, common_coords)\n1177 \n1178     def reindex_like(\n1179         self,\n1180         other: Union[\"DataArray\", Dataset],\n1181         method: str = None,\n1182         tolerance=None,\n1183         copy: bool = True,\n1184         fill_value=dtypes.NA,\n1185     ) -> \"DataArray\":\n1186         \"\"\"Conform this object onto the indexes of another object, filling in\n1187         missing values with ``fill_value``. The default fill value is NaN.\n1188 \n1189         Parameters\n1190         ----------\n1191         other : Dataset or DataArray\n1192             Object with an 'indexes' attribute giving a mapping from dimension\n1193             names to pandas.Index objects, which provides coordinates upon\n1194             which to index the variables in this dataset. The indexes on this\n1195             other object need not be the same as the indexes on this\n1196             dataset. Any mis-matched index values will be filled in with\n1197             NaN, and any mis-matched dimension names will simply be ignored.\n1198         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n1199             Method to use for filling index values from other not found on this\n1200             data array:\n1201 \n1202             * None (default): don't fill gaps\n1203             * pad / ffill: propagate last valid index value forward\n1204             * backfill / bfill: propagate next valid index value backward\n1205             * nearest: use nearest valid index value\n1206         tolerance : optional\n1207             Maximum distance between original and new labels for inexact\n1208             matches. The values of the index at the matching locations must\n1209             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1210         copy : bool, optional\n1211             If ``copy=True``, data in the return value is always copied. If\n1212             ``copy=False`` and reindexing is unnecessary, or can be performed\n1213             with only slice operations, then the output may share memory with\n1214             the input. In either case, a new xarray object is always returned.\n1215         fill_value : scalar, optional\n1216             Value to use for newly missing values\n1217 \n1218         Returns\n1219         -------\n1220         reindexed : DataArray\n1221             Another dataset array, with this array's data but coordinates from\n1222             the other object.\n1223 \n1224         See Also\n1225         --------\n1226         DataArray.reindex\n1227         align\n1228         \"\"\"\n1229         indexers = reindex_like_indexers(self, other)\n1230         return self.reindex(\n1231             indexers=indexers,\n1232             method=method,\n1233             tolerance=tolerance,\n1234             copy=copy,\n1235             fill_value=fill_value,\n1236         )\n1237 \n1238     def reindex(\n1239         self,\n1240         indexers: Mapping[Hashable, Any] = None,\n1241         method: str = None,\n1242         tolerance=None,\n1243         copy: bool = True,\n1244         fill_value=dtypes.NA,\n1245         **indexers_kwargs: Any,\n1246     ) -> \"DataArray\":\n1247         \"\"\"Conform this object onto the indexes of another object, filling in\n1248         missing values with ``fill_value``. The default fill value is NaN.\n1249 \n1250         Parameters\n1251         ----------\n1252         indexers : dict, optional\n1253             Dictionary with keys given by dimension names and values given by\n1254             arrays of coordinates tick labels. Any mis-matched coordinate\n1255             values will be filled in with NaN, and any mis-matched dimension\n1256             names will simply be ignored.\n1257             One of indexers or indexers_kwargs must be provided.\n1258         copy : bool, optional\n1259             If ``copy=True``, data in the return value is always copied. If\n1260             ``copy=False`` and reindexing is unnecessary, or can be performed\n1261             with only slice operations, then the output may share memory with\n1262             the input. In either case, a new xarray object is always returned.\n1263         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n1264             Method to use for filling index values in ``indexers`` not found on\n1265             this data array:\n1266 \n1267             * None (default): don't fill gaps\n1268             * pad / ffill: propagate last valid index value forward\n1269             * backfill / bfill: propagate next valid index value backward\n1270             * nearest: use nearest valid index value\n1271         tolerance : optional\n1272             Maximum distance between original and new labels for inexact\n1273             matches. The values of the index at the matching locations must\n1274             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1275         fill_value : scalar, optional\n1276             Value to use for newly missing values\n1277         **indexers_kwargs : {dim: indexer, ...}, optional\n1278             The keyword arguments form of ``indexers``.\n1279             One of indexers or indexers_kwargs must be provided.\n1280 \n1281         Returns\n1282         -------\n1283         reindexed : DataArray\n1284             Another dataset array, with this array's data but replaced\n1285             coordinates.\n1286 \n1287         See Also\n1288         --------\n1289         DataArray.reindex_like\n1290         align\n1291         \"\"\"\n1292         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"reindex\")\n1293         ds = self._to_temp_dataset().reindex(\n1294             indexers=indexers,\n1295             method=method,\n1296             tolerance=tolerance,\n1297             copy=copy,\n1298             fill_value=fill_value,\n1299         )\n1300         return self._from_temp_dataset(ds)\n1301 \n1302     def interp(\n1303         self,\n1304         coords: Mapping[Hashable, Any] = None,\n1305         method: str = \"linear\",\n1306         assume_sorted: bool = False,\n1307         kwargs: Mapping[str, Any] = None,\n1308         **coords_kwargs: Any,\n1309     ) -> \"DataArray\":\n1310         \"\"\" Multidimensional interpolation of variables.\n1311 \n1312         coords : dict, optional\n1313             Mapping from dimension names to the new coordinates.\n1314             new coordinate can be an scalar, array-like or DataArray.\n1315             If DataArrays are passed as new coordates, their dimensions are\n1316             used for the broadcasting.\n1317         method: {'linear', 'nearest'} for multidimensional array,\n1318             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}\n1319             for 1-dimensional array.\n1320         assume_sorted: boolean, optional\n1321             If False, values of x can be in any order and they are sorted\n1322             first. If True, x has to be an array of monotonically increasing\n1323             values.\n1324         kwargs: dictionary\n1325             Additional keyword passed to scipy's interpolator.\n1326         ``**coords_kwargs`` : {dim: coordinate, ...}, optional\n1327             The keyword arguments form of ``coords``.\n1328             One of coords or coords_kwargs must be provided.\n1329 \n1330         Returns\n1331         -------\n1332         interpolated: xr.DataArray\n1333             New dataarray on the new coordinates.\n1334 \n1335         Notes\n1336         -----\n1337         scipy is required.\n1338 \n1339         See Also\n1340         --------\n1341         scipy.interpolate.interp1d\n1342         scipy.interpolate.interpn\n1343 \n1344         Examples\n1345         --------\n1346         >>> da = xr.DataArray([1, 3], [(\"x\", np.arange(2))])\n1347         >>> da.interp(x=0.5)\n1348         <xarray.DataArray ()>\n1349         array(2.0)\n1350         Coordinates:\n1351             x        float64 0.5\n1352         \"\"\"\n1353         if self.dtype.kind not in \"uifc\":\n1354             raise TypeError(\n1355                 \"interp only works for a numeric type array. \"\n1356                 \"Given {}.\".format(self.dtype)\n1357             )\n1358         ds = self._to_temp_dataset().interp(\n1359             coords,\n1360             method=method,\n1361             kwargs=kwargs,\n1362             assume_sorted=assume_sorted,\n1363             **coords_kwargs,\n1364         )\n1365         return self._from_temp_dataset(ds)\n1366 \n1367     def interp_like(\n1368         self,\n1369         other: Union[\"DataArray\", Dataset],\n1370         method: str = \"linear\",\n1371         assume_sorted: bool = False,\n1372         kwargs: Mapping[str, Any] = None,\n1373     ) -> \"DataArray\":\n1374         \"\"\"Interpolate this object onto the coordinates of another object,\n1375         filling out of range values with NaN.\n1376 \n1377         Parameters\n1378         ----------\n1379         other : Dataset or DataArray\n1380             Object with an 'indexes' attribute giving a mapping from dimension\n1381             names to an 1d array-like, which provides coordinates upon\n1382             which to index the variables in this dataset.\n1383         method: string, optional.\n1384             {'linear', 'nearest'} for multidimensional array,\n1385             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}\n1386             for 1-dimensional array. 'linear' is used by default.\n1387         assume_sorted: boolean, optional\n1388             If False, values of coordinates that are interpolated over can be\n1389             in any order and they are sorted first. If True, interpolated\n1390             coordinates are assumed to be an array of monotonically increasing\n1391             values.\n1392         kwargs: dictionary, optional\n1393             Additional keyword passed to scipy's interpolator.\n1394 \n1395         Returns\n1396         -------\n1397         interpolated: xr.DataArray\n1398             Another dataarray by interpolating this dataarray's data along the\n1399             coordinates of the other object.\n1400 \n1401         Notes\n1402         -----\n1403         scipy is required.\n1404         If the dataarray has object-type coordinates, reindex is used for these\n1405         coordinates instead of the interpolation.\n1406 \n1407         See Also\n1408         --------\n1409         DataArray.interp\n1410         DataArray.reindex_like\n1411         \"\"\"\n1412         if self.dtype.kind not in \"uifc\":\n1413             raise TypeError(\n1414                 \"interp only works for a numeric type array. \"\n1415                 \"Given {}.\".format(self.dtype)\n1416             )\n1417         ds = self._to_temp_dataset().interp_like(\n1418             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted\n1419         )\n1420         return self._from_temp_dataset(ds)\n1421 \n1422     def rename(\n1423         self,\n1424         new_name_or_name_dict: Union[Hashable, Mapping[Hashable, Hashable]] = None,\n1425         **names: Hashable,\n1426     ) -> \"DataArray\":\n1427         \"\"\"Returns a new DataArray with renamed coordinates or a new name.\n1428 \n1429         Parameters\n1430         ----------\n1431         new_name_or_name_dict : str or dict-like, optional\n1432             If the argument is dict-like, it used as a mapping from old\n1433             names to new names for coordinates. Otherwise, use the argument\n1434             as the new name for this array.\n1435         **names: hashable, optional\n1436             The keyword arguments form of a mapping from old names to\n1437             new names for coordinates.\n1438             One of new_name_or_name_dict or names must be provided.\n1439 \n1440         Returns\n1441         -------\n1442         renamed : DataArray\n1443             Renamed array or array with renamed coordinates.\n1444 \n1445         See Also\n1446         --------\n1447         Dataset.rename\n1448         DataArray.swap_dims\n1449         \"\"\"\n1450         if names or utils.is_dict_like(new_name_or_name_dict):\n1451             new_name_or_name_dict = cast(\n1452                 Mapping[Hashable, Hashable], new_name_or_name_dict\n1453             )\n1454             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, \"rename\")\n1455             dataset = self._to_temp_dataset().rename(name_dict)\n1456             return self._from_temp_dataset(dataset)\n1457         else:\n1458             new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)\n1459             return self._replace(name=new_name_or_name_dict)\n1460 \n1461     def swap_dims(self, dims_dict: Mapping[Hashable, Hashable]) -> \"DataArray\":\n1462         \"\"\"Returns a new DataArray with swapped dimensions.\n1463 \n1464         Parameters\n1465         ----------\n1466         dims_dict : dict-like\n1467             Dictionary whose keys are current dimension names and whose values\n1468             are new names.\n1469 \n1470         Returns\n1471         -------\n1472         swapped : DataArray\n1473             DataArray with swapped dimensions.\n1474 \n1475         Examples\n1476         --------\n1477 \n1478         >>> arr = xr.DataArray(\n1479         ...     data=[0, 1], dims=\"x\", coords={\"x\": [\"a\", \"b\"], \"y\": (\"x\", [0, 1])},\n1480         ... )\n1481         >>> arr\n1482         <xarray.DataArray (x: 2)>\n1483         array([0, 1])\n1484         Coordinates:\n1485           * x        (x) <U1 'a' 'b'\n1486             y        (x) int64 0 1\n1487 \n1488         >>> arr.swap_dims({\"x\": \"y\"})\n1489         <xarray.DataArray (y: 2)>\n1490         array([0, 1])\n1491         Coordinates:\n1492             x        (y) <U1 'a' 'b'\n1493           * y        (y) int64 0 1\n1494 \n1495         >>> arr.swap_dims({\"x\": \"z\"})\n1496         <xarray.DataArray (z: 2)>\n1497         array([0, 1])\n1498         Coordinates:\n1499             x        (z) <U1 'a' 'b'\n1500             y        (z) int64 0 1\n1501         Dimensions without coordinates: z\n1502 \n1503         See Also\n1504         --------\n1505 \n1506         DataArray.rename\n1507         Dataset.swap_dims\n1508         \"\"\"\n1509         ds = self._to_temp_dataset().swap_dims(dims_dict)\n1510         return self._from_temp_dataset(ds)\n1511 \n1512     def expand_dims(\n1513         self,\n1514         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,\n1515         axis=None,\n1516         **dim_kwargs: Any,\n1517     ) -> \"DataArray\":\n1518         \"\"\"Return a new object with an additional axis (or axes) inserted at\n1519         the corresponding position in the array shape. The new object is a\n1520         view into the underlying array, not a copy.\n1521 \n1522 \n1523         If dim is already a scalar coordinate, it will be promoted to a 1D\n1524         coordinate consisting of a single value.\n1525 \n1526         Parameters\n1527         ----------\n1528         dim : hashable, sequence of hashable, dict, or None\n1529             Dimensions to include on the new variable.\n1530             If provided as str or sequence of str, then dimensions are inserted\n1531             with length 1. If provided as a dict, then the keys are the new\n1532             dimensions and the values are either integers (giving the length of\n1533             the new dimensions) or sequence/ndarray (giving the coordinates of\n1534             the new dimensions).\n1535         axis : integer, list (or tuple) of integers, or None\n1536             Axis position(s) where new axis is to be inserted (position(s) on\n1537             the result array). If a list (or tuple) of integers is passed,\n1538             multiple axes are inserted. In this case, dim arguments should be\n1539             same length list. If axis=None is passed, all the axes will be\n1540             inserted to the start of the result array.\n1541         **dim_kwargs : int or sequence/ndarray\n1542             The keywords are arbitrary dimensions being inserted and the values\n1543             are either the lengths of the new dims (if int is given), or their\n1544             coordinates. Note, this is an alternative to passing a dict to the\n1545             dim kwarg and will only be used if dim is None.\n1546 \n1547         Returns\n1548         -------\n1549         expanded : same type as caller\n1550             This object, but with an additional dimension(s).\n1551         \"\"\"\n1552         if isinstance(dim, int):\n1553             raise TypeError(\"dim should be hashable or sequence/mapping of hashables\")\n1554         elif isinstance(dim, Sequence) and not isinstance(dim, str):\n1555             if len(dim) != len(set(dim)):\n1556                 raise ValueError(\"dims should not contain duplicate values.\")\n1557             dim = dict.fromkeys(dim, 1)\n1558         elif dim is not None and not isinstance(dim, Mapping):\n1559             dim = {cast(Hashable, dim): 1}\n1560 \n1561         dim = either_dict_or_kwargs(dim, dim_kwargs, \"expand_dims\")\n1562         ds = self._to_temp_dataset().expand_dims(dim, axis)\n1563         return self._from_temp_dataset(ds)\n1564 \n1565     def set_index(\n1566         self,\n1567         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,\n1568         append: bool = False,\n1569         inplace: bool = None,\n1570         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],\n1571     ) -> Optional[\"DataArray\"]:\n1572         \"\"\"Set DataArray (multi-)indexes using one or more existing\n1573         coordinates.\n1574 \n1575         Parameters\n1576         ----------\n1577         indexes : {dim: index, ...}\n1578             Mapping from names matching dimensions and values given\n1579             by (lists of) the names of existing coordinates or variables to set\n1580             as new (multi-)index.\n1581         append : bool, optional\n1582             If True, append the supplied index(es) to the existing index(es).\n1583             Otherwise replace the existing index(es) (default).\n1584         **indexes_kwargs: optional\n1585             The keyword arguments form of ``indexes``.\n1586             One of indexes or indexes_kwargs must be provided.\n1587 \n1588         Returns\n1589         -------\n1590         obj : DataArray\n1591             Another DataArray, with this data but replaced coordinates.\n1592 \n1593         Examples\n1594         --------\n1595         >>> arr = xr.DataArray(\n1596         ...     data=np.ones((2, 3)),\n1597         ...     dims=[\"x\", \"y\"],\n1598         ...     coords={\"x\": range(2), \"y\": range(3), \"a\": (\"x\", [3, 4])},\n1599         ... )\n1600         >>> arr\n1601         <xarray.DataArray (x: 2, y: 3)>\n1602         array([[1., 1., 1.],\n1603                [1., 1., 1.]])\n1604         Coordinates:\n1605           * x        (x) int64 0 1\n1606           * y        (y) int64 0 1 2\n1607             a        (x) int64 3 4\n1608         >>> arr.set_index(x=\"a\")\n1609         <xarray.DataArray (x: 2, y: 3)>\n1610         array([[1., 1., 1.],\n1611                [1., 1., 1.]])\n1612         Coordinates:\n1613           * x        (x) int64 3 4\n1614           * y        (y) int64 0 1 2\n1615 \n1616         See Also\n1617         --------\n1618         DataArray.reset_index\n1619         \"\"\"\n1620         ds = self._to_temp_dataset().set_index(\n1621             indexes, append=append, inplace=inplace, **indexes_kwargs\n1622         )\n1623         return self._from_temp_dataset(ds)\n1624 \n1625     def reset_index(\n1626         self,\n1627         dims_or_levels: Union[Hashable, Sequence[Hashable]],\n1628         drop: bool = False,\n1629         inplace: bool = None,\n1630     ) -> Optional[\"DataArray\"]:\n1631         \"\"\"Reset the specified index(es) or multi-index level(s).\n1632 \n1633         Parameters\n1634         ----------\n1635         dims_or_levels : hashable or sequence of hashables\n1636             Name(s) of the dimension(s) and/or multi-index level(s) that will\n1637             be reset.\n1638         drop : bool, optional\n1639             If True, remove the specified indexes and/or multi-index levels\n1640             instead of extracting them as new coordinates (default: False).\n1641 \n1642         Returns\n1643         -------\n1644         obj : DataArray\n1645             Another dataarray, with this dataarray's data but replaced\n1646             coordinates.\n1647 \n1648         See Also\n1649         --------\n1650         DataArray.set_index\n1651         \"\"\"\n1652         _check_inplace(inplace)\n1653         coords, _ = split_indexes(\n1654             dims_or_levels, self._coords, set(), self._level_coords, drop=drop\n1655         )\n1656         return self._replace(coords=coords)\n1657 \n1658     def reorder_levels(\n1659         self,\n1660         dim_order: Mapping[Hashable, Sequence[int]] = None,\n1661         inplace: bool = None,\n1662         **dim_order_kwargs: Sequence[int],\n1663     ) -> \"DataArray\":\n1664         \"\"\"Rearrange index levels using input order.\n1665 \n1666         Parameters\n1667         ----------\n1668         dim_order : optional\n1669             Mapping from names matching dimensions and values given\n1670             by lists representing new level orders. Every given dimension\n1671             must have a multi-index.\n1672         **dim_order_kwargs: optional\n1673             The keyword arguments form of ``dim_order``.\n1674             One of dim_order or dim_order_kwargs must be provided.\n1675 \n1676         Returns\n1677         -------\n1678         obj : DataArray\n1679             Another dataarray, with this dataarray's data but replaced\n1680             coordinates.\n1681         \"\"\"\n1682         _check_inplace(inplace)\n1683         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, \"reorder_levels\")\n1684         replace_coords = {}\n1685         for dim, order in dim_order.items():\n1686             coord = self._coords[dim]\n1687             index = coord.to_index()\n1688             if not isinstance(index, pd.MultiIndex):\n1689                 raise ValueError(\"coordinate %r has no MultiIndex\" % dim)\n1690             replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))\n1691         coords = self._coords.copy()\n1692         coords.update(replace_coords)\n1693         return self._replace(coords=coords)\n1694 \n1695     def stack(\n1696         self,\n1697         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,\n1698         **dimensions_kwargs: Sequence[Hashable],\n1699     ) -> \"DataArray\":\n1700         \"\"\"\n1701         Stack any number of existing dimensions into a single new dimension.\n1702 \n1703         New dimensions will be added at the end, and the corresponding\n1704         coordinate variables will be combined into a MultiIndex.\n1705 \n1706         Parameters\n1707         ----------\n1708         dimensions : Mapping of the form new_name=(dim1, dim2, ...)\n1709             Names of new dimensions, and the existing dimensions that they\n1710             replace.\n1711         **dimensions_kwargs:\n1712             The keyword arguments form of ``dimensions``.\n1713             One of dimensions or dimensions_kwargs must be provided.\n1714 \n1715         Returns\n1716         -------\n1717         stacked : DataArray\n1718             DataArray with stacked data.\n1719 \n1720         Examples\n1721         --------\n1722 \n1723         >>> arr = xr.DataArray(\n1724         ...     np.arange(6).reshape(2, 3),\n1725         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n1726         ... )\n1727         >>> arr\n1728         <xarray.DataArray (x: 2, y: 3)>\n1729         array([[0, 1, 2],\n1730                [3, 4, 5]])\n1731         Coordinates:\n1732           * x        (x) |S1 'a' 'b'\n1733           * y        (y) int64 0 1 2\n1734         >>> stacked = arr.stack(z=(\"x\", \"y\"))\n1735         >>> stacked.indexes[\"z\"]\n1736         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],\n1737                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],\n1738                    names=['x', 'y'])\n1739 \n1740         See Also\n1741         --------\n1742         DataArray.unstack\n1743         \"\"\"\n1744         ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)\n1745         return self._from_temp_dataset(ds)\n1746 \n1747     def unstack(\n1748         self,\n1749         dim: Union[Hashable, Sequence[Hashable], None] = None,\n1750         fill_value: Any = dtypes.NA,\n1751         sparse: bool = False,\n1752     ) -> \"DataArray\":\n1753         \"\"\"\n1754         Unstack existing dimensions corresponding to MultiIndexes into\n1755         multiple new dimensions.\n1756 \n1757         New dimensions will be added at the end.\n1758 \n1759         Parameters\n1760         ----------\n1761         dim : hashable or sequence of hashable, optional\n1762             Dimension(s) over which to unstack. By default unstacks all\n1763             MultiIndexes.\n1764         fill_value: value to be filled. By default, np.nan\n1765         sparse: use sparse-array if True\n1766 \n1767         Returns\n1768         -------\n1769         unstacked : DataArray\n1770             Array with unstacked data.\n1771 \n1772         Examples\n1773         --------\n1774 \n1775         >>> arr = xr.DataArray(\n1776         ...     np.arange(6).reshape(2, 3),\n1777         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n1778         ... )\n1779         >>> arr\n1780         <xarray.DataArray (x: 2, y: 3)>\n1781         array([[0, 1, 2],\n1782                [3, 4, 5]])\n1783         Coordinates:\n1784           * x        (x) |S1 'a' 'b'\n1785           * y        (y) int64 0 1 2\n1786         >>> stacked = arr.stack(z=(\"x\", \"y\"))\n1787         >>> stacked.indexes[\"z\"]\n1788         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],\n1789                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],\n1790                    names=['x', 'y'])\n1791         >>> roundtripped = stacked.unstack()\n1792         >>> arr.identical(roundtripped)\n1793         True\n1794 \n1795         See Also\n1796         --------\n1797         DataArray.stack\n1798         \"\"\"\n1799         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)\n1800         return self._from_temp_dataset(ds)\n1801 \n1802     def to_unstacked_dataset(self, dim, level=0):\n1803         \"\"\"Unstack DataArray expanding to Dataset along a given level of a\n1804         stacked coordinate.\n1805 \n1806         This is the inverse operation of Dataset.to_stacked_array.\n1807 \n1808         Parameters\n1809         ----------\n1810         dim : str\n1811             Name of existing dimension to unstack\n1812         level : int or str\n1813             The MultiIndex level to expand to a dataset along. Can either be\n1814             the integer index of the level or its name.\n1815         label : int, default 0\n1816             Label of the level to expand dataset along. Overrides the label\n1817             argument if given.\n1818 \n1819         Returns\n1820         -------\n1821         unstacked: Dataset\n1822 \n1823         Examples\n1824         --------\n1825         >>> import xarray as xr\n1826         >>> arr = xr.DataArray(\n1827         ...     np.arange(6).reshape(2, 3),\n1828         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n1829         ... )\n1830         >>> data = xr.Dataset({\"a\": arr, \"b\": arr.isel(y=0)})\n1831         >>> data\n1832         <xarray.Dataset>\n1833         Dimensions:  (x: 2, y: 3)\n1834         Coordinates:\n1835           * x        (x) <U1 'a' 'b'\n1836           * y        (y) int64 0 1 2\n1837         Data variables:\n1838             a        (x, y) int64 0 1 2 3 4 5\n1839             b        (x) int64 0 3\n1840         >>> stacked = data.to_stacked_array(\"z\", [\"y\"])\n1841         >>> stacked.indexes[\"z\"]\n1842         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],\n1843                 labels=[[0, 0, 0, 1], [0, 1, 2, -1]],\n1844                 names=['variable', 'y'])\n1845         >>> roundtripped = stacked.to_unstacked_dataset(dim=\"z\")\n1846         >>> data.identical(roundtripped)\n1847         True\n1848 \n1849         See Also\n1850         --------\n1851         Dataset.to_stacked_array\n1852         \"\"\"\n1853 \n1854         idx = self.indexes[dim]\n1855         if not isinstance(idx, pd.MultiIndex):\n1856             raise ValueError(f\"'{dim}' is not a stacked coordinate\")\n1857 \n1858         level_number = idx._get_level_number(level)\n1859         variables = idx.levels[level_number]\n1860         variable_dim = idx.names[level_number]\n1861 \n1862         # pull variables out of datarray\n1863         data_dict = {}\n1864         for k in variables:\n1865             data_dict[k] = self.sel({variable_dim: k}).squeeze(drop=True)\n1866 \n1867         # unstacked dataset\n1868         return Dataset(data_dict)\n1869 \n1870     def transpose(self, *dims: Hashable, transpose_coords: bool = None) -> \"DataArray\":\n1871         \"\"\"Return a new DataArray object with transposed dimensions.\n1872 \n1873         Parameters\n1874         ----------\n1875         *dims : hashable, optional\n1876             By default, reverse the dimensions. Otherwise, reorder the\n1877             dimensions to this order.\n1878         transpose_coords : boolean, optional\n1879             If True, also transpose the coordinates of this DataArray.\n1880 \n1881         Returns\n1882         -------\n1883         transposed : DataArray\n1884             The returned DataArray's array is transposed.\n1885 \n1886         Notes\n1887         -----\n1888         This operation returns a view of this array's data. It is\n1889         lazy for dask-backed DataArrays but not for numpy-backed DataArrays\n1890         -- the data will be fully loaded.\n1891 \n1892         See Also\n1893         --------\n1894         numpy.transpose\n1895         Dataset.transpose\n1896         \"\"\"\n1897         if dims:\n1898             dims = tuple(utils.infix_dims(dims, self.dims))\n1899         variable = self.variable.transpose(*dims)\n1900         if transpose_coords:\n1901             coords: Dict[Hashable, Variable] = {}\n1902             for name, coord in self.coords.items():\n1903                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)\n1904                 coords[name] = coord.variable.transpose(*coord_dims)\n1905             return self._replace(variable, coords)\n1906         else:\n1907             if transpose_coords is None and any(self[c].ndim > 1 for c in self.coords):\n1908                 warnings.warn(\n1909                     \"This DataArray contains multi-dimensional \"\n1910                     \"coordinates. In the future, these coordinates \"\n1911                     \"will be transposed as well unless you specify \"\n1912                     \"transpose_coords=False.\",\n1913                     FutureWarning,\n1914                     stacklevel=2,\n1915                 )\n1916             return self._replace(variable)\n1917 \n1918     @property\n1919     def T(self) -> \"DataArray\":\n1920         return self.transpose()\n1921 \n1922     def drop_vars(\n1923         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = \"raise\"\n1924     ) -> \"DataArray\":\n1925         \"\"\"Drop variables from this DataArray.\n1926 \n1927         Parameters\n1928         ----------\n1929         names : hashable or iterable of hashables\n1930             Name(s) of variables to drop.\n1931         errors: {'raise', 'ignore'}, optional\n1932             If 'raise' (default), raises a ValueError error if any of the variable\n1933             passed are not in the dataset. If 'ignore', any given names that are in the\n1934             DataArray are dropped and no error is raised.\n1935 \n1936         Returns\n1937         -------\n1938         dropped : Dataset\n1939 \n1940         \"\"\"\n1941         ds = self._to_temp_dataset().drop_vars(names, errors=errors)\n1942         return self._from_temp_dataset(ds)\n1943 \n1944     def drop(\n1945         self,\n1946         labels: Mapping = None,\n1947         dim: Hashable = None,\n1948         *,\n1949         errors: str = \"raise\",\n1950         **labels_kwargs,\n1951     ) -> \"DataArray\":\n1952         \"\"\"Backward compatible method based on `drop_vars` and `drop_sel`\n1953 \n1954         Using either `drop_vars` or `drop_sel` is encouraged\n1955 \n1956         See Also\n1957         --------\n1958         DataArray.drop_vars\n1959         DataArray.drop_sel\n1960         \"\"\"\n1961         ds = self._to_temp_dataset().drop(labels, dim, errors=errors)\n1962         return self._from_temp_dataset(ds)\n1963 \n1964     def drop_sel(\n1965         self,\n1966         labels: Mapping[Hashable, Any] = None,\n1967         *,\n1968         errors: str = \"raise\",\n1969         **labels_kwargs,\n1970     ) -> \"DataArray\":\n1971         \"\"\"Drop index labels from this DataArray.\n1972 \n1973         Parameters\n1974         ----------\n1975         labels : Mapping[Hashable, Any]\n1976             Index labels to drop\n1977         errors: {'raise', 'ignore'}, optional\n1978             If 'raise' (default), raises a ValueError error if\n1979             any of the index labels passed are not\n1980             in the dataset. If 'ignore', any given labels that are in the\n1981             dataset are dropped and no error is raised.\n1982         **labels_kwargs : {dim: label, ...}, optional\n1983             The keyword arguments form of ``dim`` and ``labels``\n1984 \n1985         Returns\n1986         -------\n1987         dropped : DataArray\n1988         \"\"\"\n1989         if labels_kwargs or isinstance(labels, dict):\n1990             labels = either_dict_or_kwargs(labels, labels_kwargs, \"drop\")\n1991 \n1992         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)\n1993         return self._from_temp_dataset(ds)\n1994 \n1995     def dropna(\n1996         self, dim: Hashable, how: str = \"any\", thresh: int = None\n1997     ) -> \"DataArray\":\n1998         \"\"\"Returns a new array with dropped labels for missing values along\n1999         the provided dimension.\n2000 \n2001         Parameters\n2002         ----------\n2003         dim : hashable\n2004             Dimension along which to drop missing values. Dropping along\n2005             multiple dimensions simultaneously is not yet supported.\n2006         how : {'any', 'all'}, optional\n2007             * any : if any NA values are present, drop that label\n2008             * all : if all values are NA, drop that label\n2009         thresh : int, default None\n2010             If supplied, require this many non-NA values.\n2011 \n2012         Returns\n2013         -------\n2014         DataArray\n2015         \"\"\"\n2016         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)\n2017         return self._from_temp_dataset(ds)\n2018 \n2019     def fillna(self, value: Any) -> \"DataArray\":\n2020         \"\"\"Fill missing values in this object.\n2021 \n2022         This operation follows the normal broadcasting and alignment rules that\n2023         xarray uses for binary arithmetic, except the result is aligned to this\n2024         object (``join='left'``) instead of aligned to the intersection of\n2025         index coordinates (``join='inner'``).\n2026 \n2027         Parameters\n2028         ----------\n2029         value : scalar, ndarray or DataArray\n2030             Used to fill all matching missing values in this array. If the\n2031             argument is a DataArray, it is first aligned with (reindexed to)\n2032             this array.\n2033 \n2034         Returns\n2035         -------\n2036         DataArray\n2037         \"\"\"\n2038         if utils.is_dict_like(value):\n2039             raise TypeError(\n2040                 \"cannot provide fill value as a dictionary with \"\n2041                 \"fillna on a DataArray\"\n2042             )\n2043         out = ops.fillna(self, value)\n2044         return out\n2045 \n2046     def interpolate_na(\n2047         self,\n2048         dim: Hashable = None,\n2049         method: str = \"linear\",\n2050         limit: int = None,\n2051         use_coordinate: Union[bool, str] = True,\n2052         max_gap: Union[\n2053             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta\n2054         ] = None,\n2055         **kwargs: Any,\n2056     ) -> \"DataArray\":\n2057         \"\"\"Fill in NaNs by interpolating according to different methods.\n2058 \n2059         Parameters\n2060         ----------\n2061         dim : str\n2062             Specifies the dimension along which to interpolate.\n2063         method : str, optional\n2064             String indicating which method to use for interpolation:\n2065 \n2066             - 'linear': linear interpolation (Default). Additional keyword\n2067               arguments are passed to :py:func:`numpy.interp`\n2068             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n2069               are passed to :py:func:`scipy.interpolate.interp1d`. If\n2070               ``method='polynomial'``, the ``order`` keyword argument must also be\n2071               provided.\n2072             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n2073               respective :py:class:`scipy.interpolate` classes.\n2074 \n2075         use_coordinate : bool, str, default True\n2076             Specifies which index to use as the x values in the interpolation\n2077             formulated as `y = f(x)`. If False, values are treated as if\n2078             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is\n2079             used. If ``use_coordinate`` is a string, it specifies the name of a\n2080             coordinate variariable to use as the index.\n2081         limit : int, default None\n2082             Maximum number of consecutive NaNs to fill. Must be greater than 0\n2083             or None for no limit. This filling is done regardless of the size of\n2084             the gap in the data. To only interpolate over gaps less than a given length,\n2085             see ``max_gap``.\n2086         max_gap: int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default None.\n2087             Maximum size of gap, a continuous sequence of NaNs, that will be filled.\n2088             Use None for no limit. When interpolating along a datetime64 dimension\n2089             and ``use_coordinate=True``, ``max_gap`` can be one of the following:\n2090 \n2091             - a string that is valid input for pandas.to_timedelta\n2092             - a :py:class:`numpy.timedelta64` object\n2093             - a :py:class:`pandas.Timedelta` object\n2094             - a :py:class:`datetime.timedelta` object\n2095 \n2096             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled\n2097             dimensions has not been implemented yet. Gap length is defined as the difference\n2098             between coordinate values at the first data point after a gap and the last value\n2099             before a gap. For gaps at the beginning (end), gap length is defined as the difference\n2100             between coordinate values at the first (last) valid data point and the first (last) NaN.\n2101             For example, consider::\n2102 \n2103                 <xarray.DataArray (x: 9)>\n2104                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])\n2105                 Coordinates:\n2106                   * x        (x) int64 0 1 2 3 4 5 6 7 8\n2107 \n2108             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively\n2109         kwargs : dict, optional\n2110             parameters passed verbatim to the underlying interpolation function\n2111 \n2112         Returns\n2113         -------\n2114         interpolated: DataArray\n2115             Filled in DataArray.\n2116 \n2117         See also\n2118         --------\n2119         numpy.interp\n2120         scipy.interpolate\n2121         \"\"\"\n2122         from .missing import interp_na\n2123 \n2124         return interp_na(\n2125             self,\n2126             dim=dim,\n2127             method=method,\n2128             limit=limit,\n2129             use_coordinate=use_coordinate,\n2130             max_gap=max_gap,\n2131             **kwargs,\n2132         )\n2133 \n2134     def ffill(self, dim: Hashable, limit: int = None) -> \"DataArray\":\n2135         \"\"\"Fill NaN values by propogating values forward\n2136 \n2137         *Requires bottleneck.*\n2138 \n2139         Parameters\n2140         ----------\n2141         dim : hashable\n2142             Specifies the dimension along which to propagate values when\n2143             filling.\n2144         limit : int, default None\n2145             The maximum number of consecutive NaN values to forward fill. In\n2146             other words, if there is a gap with more than this number of\n2147             consecutive NaNs, it will only be partially filled. Must be greater\n2148             than 0 or None for no limit.\n2149 \n2150         Returns\n2151         -------\n2152         DataArray\n2153         \"\"\"\n2154         from .missing import ffill\n2155 \n2156         return ffill(self, dim, limit=limit)\n2157 \n2158     def bfill(self, dim: Hashable, limit: int = None) -> \"DataArray\":\n2159         \"\"\"Fill NaN values by propogating values backward\n2160 \n2161         *Requires bottleneck.*\n2162 \n2163         Parameters\n2164         ----------\n2165         dim : str\n2166             Specifies the dimension along which to propagate values when\n2167             filling.\n2168         limit : int, default None\n2169             The maximum number of consecutive NaN values to backward fill. In\n2170             other words, if there is a gap with more than this number of\n2171             consecutive NaNs, it will only be partially filled. Must be greater\n2172             than 0 or None for no limit.\n2173 \n2174         Returns\n2175         -------\n2176         DataArray\n2177         \"\"\"\n2178         from .missing import bfill\n2179 \n2180         return bfill(self, dim, limit=limit)\n2181 \n2182     def combine_first(self, other: \"DataArray\") -> \"DataArray\":\n2183         \"\"\"Combine two DataArray objects, with union of coordinates.\n2184 \n2185         This operation follows the normal broadcasting and alignment rules of\n2186         ``join='outer'``.  Default to non-null values of array calling the\n2187         method.  Use np.nan to fill in vacant cells after alignment.\n2188 \n2189         Parameters\n2190         ----------\n2191         other : DataArray\n2192             Used to fill all matching missing values in this array.\n2193 \n2194         Returns\n2195         -------\n2196         DataArray\n2197         \"\"\"\n2198         return ops.fillna(self, other, join=\"outer\")\n2199 \n2200     def reduce(\n2201         self,\n2202         func: Callable[..., Any],\n2203         dim: Union[None, Hashable, Sequence[Hashable]] = None,\n2204         axis: Union[None, int, Sequence[int]] = None,\n2205         keep_attrs: bool = None,\n2206         keepdims: bool = False,\n2207         **kwargs: Any,\n2208     ) -> \"DataArray\":\n2209         \"\"\"Reduce this array by applying `func` along some dimension(s).\n2210 \n2211         Parameters\n2212         ----------\n2213         func : function\n2214             Function which can be called in the form\n2215             `f(x, axis=axis, **kwargs)` to return the result of reducing an\n2216             np.ndarray over an integer valued axis.\n2217         dim : hashable or sequence of hashables, optional\n2218             Dimension(s) over which to apply `func`.\n2219         axis : int or sequence of int, optional\n2220             Axis(es) over which to repeatedly apply `func`. Only one of the\n2221             'dim' and 'axis' arguments can be supplied. If neither are\n2222             supplied, then the reduction is calculated over the flattened array\n2223             (by calling `f(x)` without an axis argument).\n2224         keep_attrs : bool, optional\n2225             If True, the variable's attributes (`attrs`) will be copied from\n2226             the original object to the new one.  If False (default), the new\n2227             object will be returned without attributes.\n2228         keepdims : bool, default False\n2229             If True, the dimensions which are reduced are left in the result\n2230             as dimensions of size one. Coordinates that use these dimensions\n2231             are removed.\n2232         **kwargs : dict\n2233             Additional keyword arguments passed on to `func`.\n2234 \n2235         Returns\n2236         -------\n2237         reduced : DataArray\n2238             DataArray with this object's array replaced with an array with\n2239             summarized data and the indicated dimension(s) removed.\n2240         \"\"\"\n2241 \n2242         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)\n2243         return self._replace_maybe_drop_dims(var)\n2244 \n2245     def to_pandas(self) -> Union[\"DataArray\", pd.Series, pd.DataFrame]:\n2246         \"\"\"Convert this array into a pandas object with the same shape.\n2247 \n2248         The type of the returned object depends on the number of DataArray\n2249         dimensions:\n2250 \n2251         * 0D -> `xarray.DataArray`\n2252         * 1D -> `pandas.Series`\n2253         * 2D -> `pandas.DataFrame`\n2254 \n2255         Only works for arrays with 2 or fewer dimensions.\n2256 \n2257         The DataArray constructor performs the inverse transformation.\n2258         \"\"\"\n2259         # TODO: consolidate the info about pandas constructors and the\n2260         # attributes that correspond to their indexes into a separate module?\n2261         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}\n2262         try:\n2263             constructor = constructors[self.ndim]\n2264         except KeyError:\n2265             raise ValueError(\n2266                 \"cannot convert arrays with %s dimensions into \"\n2267                 \"pandas objects\" % self.ndim\n2268             )\n2269         indexes = [self.get_index(dim) for dim in self.dims]\n2270         return constructor(self.values, *indexes)\n2271 \n2272     def to_dataframe(self, name: Hashable = None) -> pd.DataFrame:\n2273         \"\"\"Convert this array and its coordinates into a tidy pandas.DataFrame.\n2274 \n2275         The DataFrame is indexed by the Cartesian product of index coordinates\n2276         (in the form of a :py:class:`pandas.MultiIndex`).\n2277 \n2278         Other coordinates are included as columns in the DataFrame.\n2279         \"\"\"\n2280         if name is None:\n2281             name = self.name\n2282         if name is None:\n2283             raise ValueError(\n2284                 \"cannot convert an unnamed DataArray to a \"\n2285                 \"DataFrame: use the ``name`` parameter\"\n2286             )\n2287 \n2288         dims = dict(zip(self.dims, self.shape))\n2289         # By using a unique name, we can convert a DataArray into a DataFrame\n2290         # even if it shares a name with one of its coordinates.\n2291         # I would normally use unique_name = object() but that results in a\n2292         # dataframe with columns in the wrong order, for reasons I have not\n2293         # been able to debug (possibly a pandas bug?).\n2294         unique_name = \"__unique_name_identifier_z98xfz98xugfg73ho__\"\n2295         ds = self._to_dataset_whole(name=unique_name)\n2296         df = ds._to_dataframe(dims)\n2297         df.columns = [name if c == unique_name else c for c in df.columns]\n2298         return df\n2299 \n2300     def to_series(self) -> pd.Series:\n2301         \"\"\"Convert this array into a pandas.Series.\n2302 \n2303         The Series is indexed by the Cartesian product of index coordinates\n2304         (in the form of a :py:class:`pandas.MultiIndex`).\n2305         \"\"\"\n2306         index = self.coords.to_index()\n2307         return pd.Series(self.values.reshape(-1), index=index, name=self.name)\n2308 \n2309     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:\n2310         \"\"\"Convert this array into a numpy.ma.MaskedArray\n2311 \n2312         Parameters\n2313         ----------\n2314         copy : bool\n2315             If True (default) make a copy of the array in the result. If False,\n2316             a MaskedArray view of DataArray.values is returned.\n2317 \n2318         Returns\n2319         -------\n2320         result : MaskedArray\n2321             Masked where invalid values (nan or inf) occur.\n2322         \"\"\"\n2323         values = self.values  # only compute lazy arrays once\n2324         isnull = pd.isnull(values)\n2325         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)\n2326 \n2327     def to_netcdf(self, *args, **kwargs) -> Union[bytes, \"Delayed\", None]:\n2328         \"\"\"Write DataArray contents to a netCDF file.\n2329 \n2330         All parameters are passed directly to `xarray.Dataset.to_netcdf`.\n2331 \n2332         Notes\n2333         -----\n2334         Only xarray.Dataset objects can be written to netCDF files, so\n2335         the xarray.DataArray is converted to a xarray.Dataset object\n2336         containing a single variable. If the DataArray has no name, or if the\n2337         name is the same as a co-ordinate name, then it is given the name\n2338         '__xarray_dataarray_variable__'.\n2339         \"\"\"\n2340         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE\n2341 \n2342         if self.name is None:\n2343             # If no name is set then use a generic xarray name\n2344             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n2345         elif self.name in self.coords or self.name in self.dims:\n2346             # The name is the same as one of the coords names, which netCDF\n2347             # doesn't support, so rename it but keep track of the old name\n2348             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n2349             dataset.attrs[DATAARRAY_NAME] = self.name\n2350         else:\n2351             # No problems with the name - so we're fine!\n2352             dataset = self.to_dataset()\n2353 \n2354         return dataset.to_netcdf(*args, **kwargs)\n2355 \n2356     def to_dict(self, data: bool = True) -> dict:\n2357         \"\"\"\n2358         Convert this xarray.DataArray into a dictionary following xarray\n2359         naming conventions.\n2360 \n2361         Converts all variables and attributes to native Python objects.\n2362         Useful for converting to json. To avoid datetime incompatibility\n2363         use decode_times=False kwarg in xarrray.open_dataset.\n2364 \n2365         Parameters\n2366         ----------\n2367         data : bool, optional\n2368             Whether to include the actual data in the dictionary. When set to\n2369             False, returns just the schema.\n2370 \n2371         See also\n2372         --------\n2373         DataArray.from_dict\n2374         \"\"\"\n2375         d = self.variable.to_dict(data=data)\n2376         d.update({\"coords\": {}, \"name\": self.name})\n2377         for k in self.coords:\n2378             d[\"coords\"][k] = self.coords[k].variable.to_dict(data=data)\n2379         return d\n2380 \n2381     @classmethod\n2382     def from_dict(cls, d: dict) -> \"DataArray\":\n2383         \"\"\"\n2384         Convert a dictionary into an xarray.DataArray\n2385 \n2386         Input dict can take several forms::\n2387 \n2388             d = {'dims': ('t'), 'data': x}\n2389 \n2390             d = {'coords': {'t': {'dims': 't', 'data': t,\n2391                                   'attrs': {'units':'s'}}},\n2392                  'attrs': {'title': 'air temperature'},\n2393                  'dims': 't',\n2394                  'data': x,\n2395                  'name': 'a'}\n2396 \n2397         where 't' is the name of the dimesion, 'a' is the name of the array,\n2398         and  x and t are lists, numpy.arrays, or pandas objects.\n2399 \n2400         Parameters\n2401         ----------\n2402         d : dict, with a minimum structure of {'dims': [..], 'data': [..]}\n2403 \n2404         Returns\n2405         -------\n2406         obj : xarray.DataArray\n2407 \n2408         See also\n2409         --------\n2410         DataArray.to_dict\n2411         Dataset.from_dict\n2412         \"\"\"\n2413         coords = None\n2414         if \"coords\" in d:\n2415             try:\n2416                 coords = {\n2417                     k: (v[\"dims\"], v[\"data\"], v.get(\"attrs\"))\n2418                     for k, v in d[\"coords\"].items()\n2419                 }\n2420             except KeyError as e:\n2421                 raise ValueError(\n2422                     \"cannot convert dict when coords are missing the key \"\n2423                     \"'{dims_data}'\".format(dims_data=str(e.args[0]))\n2424                 )\n2425         try:\n2426             data = d[\"data\"]\n2427         except KeyError:\n2428             raise ValueError(\"cannot convert dict without the key 'data''\")\n2429         else:\n2430             obj = cls(data, coords, d.get(\"dims\"), d.get(\"name\"), d.get(\"attrs\"))\n2431         return obj\n2432 \n2433     @classmethod\n2434     def from_series(cls, series: pd.Series, sparse: bool = False) -> \"DataArray\":\n2435         \"\"\"Convert a pandas.Series into an xarray.DataArray.\n2436 \n2437         If the series's index is a MultiIndex, it will be expanded into a\n2438         tensor product of one-dimensional coordinates (filling in missing\n2439         values with NaN). Thus this operation should be the inverse of the\n2440         `to_series` method.\n2441 \n2442         If sparse=True, creates a sparse array instead of a dense NumPy array.\n2443         Requires the pydata/sparse package.\n2444 \n2445         See also\n2446         --------\n2447         xarray.Dataset.from_dataframe\n2448         \"\"\"\n2449         temp_name = \"__temporary_name\"\n2450         df = pd.DataFrame({temp_name: series})\n2451         ds = Dataset.from_dataframe(df, sparse=sparse)\n2452         result = cast(DataArray, ds[temp_name])\n2453         result.name = series.name\n2454         return result\n2455 \n2456     def to_cdms2(self) -> \"cdms2_Variable\":\n2457         \"\"\"Convert this array into a cdms2.Variable\n2458         \"\"\"\n2459         from ..convert import to_cdms2\n2460 \n2461         return to_cdms2(self)\n2462 \n2463     @classmethod\n2464     def from_cdms2(cls, variable: \"cdms2_Variable\") -> \"DataArray\":\n2465         \"\"\"Convert a cdms2.Variable into an xarray.DataArray\n2466         \"\"\"\n2467         from ..convert import from_cdms2\n2468 \n2469         return from_cdms2(variable)\n2470 \n2471     def to_iris(self) -> \"iris_Cube\":\n2472         \"\"\"Convert this array into a iris.cube.Cube\n2473         \"\"\"\n2474         from ..convert import to_iris\n2475 \n2476         return to_iris(self)\n2477 \n2478     @classmethod\n2479     def from_iris(cls, cube: \"iris_Cube\") -> \"DataArray\":\n2480         \"\"\"Convert a iris.cube.Cube into an xarray.DataArray\n2481         \"\"\"\n2482         from ..convert import from_iris\n2483 \n2484         return from_iris(cube)\n2485 \n2486     def _all_compat(self, other: \"DataArray\", compat_str: str) -> bool:\n2487         \"\"\"Helper function for equals, broadcast_equals, and identical\n2488         \"\"\"\n2489 \n2490         def compat(x, y):\n2491             return getattr(x.variable, compat_str)(y.variable)\n2492 \n2493         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(\n2494             self, other\n2495         )\n2496 \n2497     def broadcast_equals(self, other: \"DataArray\") -> bool:\n2498         \"\"\"Two DataArrays are broadcast equal if they are equal after\n2499         broadcasting them against each other such that they have the same\n2500         dimensions.\n2501 \n2502         See Also\n2503         --------\n2504         DataArray.equals\n2505         DataArray.identical\n2506         \"\"\"\n2507         try:\n2508             return self._all_compat(other, \"broadcast_equals\")\n2509         except (TypeError, AttributeError):\n2510             return False\n2511 \n2512     def equals(self, other: \"DataArray\") -> bool:\n2513         \"\"\"True if two DataArrays have the same dimensions, coordinates and\n2514         values; otherwise False.\n2515 \n2516         DataArrays can still be equal (like pandas objects) if they have NaN\n2517         values in the same locations.\n2518 \n2519         This method is necessary because `v1 == v2` for ``DataArray``\n2520         does element-wise comparisons (like numpy.ndarrays).\n2521 \n2522         See Also\n2523         --------\n2524         DataArray.broadcast_equals\n2525         DataArray.identical\n2526         \"\"\"\n2527         try:\n2528             return self._all_compat(other, \"equals\")\n2529         except (TypeError, AttributeError):\n2530             return False\n2531 \n2532     def identical(self, other: \"DataArray\") -> bool:\n2533         \"\"\"Like equals, but also checks the array name and attributes, and\n2534         attributes on all coordinates.\n2535 \n2536         See Also\n2537         --------\n2538         DataArray.broadcast_equals\n2539         DataArray.equal\n2540         \"\"\"\n2541         try:\n2542             return self.name == other.name and self._all_compat(other, \"identical\")\n2543         except (TypeError, AttributeError):\n2544             return False\n2545 \n2546     def _result_name(self, other: Any = None) -> Optional[Hashable]:\n2547         # use the same naming heuristics as pandas:\n2548         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356\n2549         other_name = getattr(other, \"name\", _default)\n2550         if other_name is _default or other_name == self.name:\n2551             return self.name\n2552         else:\n2553             return None\n2554 \n2555     def __array_wrap__(self, obj, context=None) -> \"DataArray\":\n2556         new_var = self.variable.__array_wrap__(obj, context)\n2557         return self._replace(new_var)\n2558 \n2559     def __matmul__(self, obj):\n2560         return self.dot(obj)\n2561 \n2562     def __rmatmul__(self, other):\n2563         # currently somewhat duplicative, as only other DataArrays are\n2564         # compatible with matmul\n2565         return computation.dot(other, self)\n2566 \n2567     @staticmethod\n2568     def _unary_op(f: Callable[..., Any]) -> Callable[..., \"DataArray\"]:\n2569         @functools.wraps(f)\n2570         def func(self, *args, **kwargs):\n2571             with np.errstate(all=\"ignore\"):\n2572                 return self.__array_wrap__(f(self.variable.data, *args, **kwargs))\n2573 \n2574         return func\n2575 \n2576     @staticmethod\n2577     def _binary_op(\n2578         f: Callable[..., Any],\n2579         reflexive: bool = False,\n2580         join: str = None,  # see xarray.align\n2581         **ignored_kwargs,\n2582     ) -> Callable[..., \"DataArray\"]:\n2583         @functools.wraps(f)\n2584         def func(self, other):\n2585             if isinstance(other, (Dataset, groupby.GroupBy)):\n2586                 return NotImplemented\n2587             if isinstance(other, DataArray):\n2588                 align_type = OPTIONS[\"arithmetic_join\"] if join is None else join\n2589                 self, other = align(self, other, join=align_type, copy=False)\n2590             other_variable = getattr(other, \"variable\", other)\n2591             other_coords = getattr(other, \"coords\", None)\n2592 \n2593             variable = (\n2594                 f(self.variable, other_variable)\n2595                 if not reflexive\n2596                 else f(other_variable, self.variable)\n2597             )\n2598             coords, indexes = self.coords._merge_raw(other_coords)\n2599             name = self._result_name(other)\n2600 \n2601             return self._replace(variable, coords, name, indexes=indexes)\n2602 \n2603         return func\n2604 \n2605     @staticmethod\n2606     def _inplace_binary_op(f: Callable) -> Callable[..., \"DataArray\"]:\n2607         @functools.wraps(f)\n2608         def func(self, other):\n2609             if isinstance(other, groupby.GroupBy):\n2610                 raise TypeError(\n2611                     \"in-place operations between a DataArray and \"\n2612                     \"a grouped object are not permitted\"\n2613                 )\n2614             # n.b. we can't align other to self (with other.reindex_like(self))\n2615             # because `other` may be converted into floats, which would cause\n2616             # in-place arithmetic to fail unpredictably. Instead, we simply\n2617             # don't support automatic alignment with in-place arithmetic.\n2618             other_coords = getattr(other, \"coords\", None)\n2619             other_variable = getattr(other, \"variable\", other)\n2620             with self.coords._merge_inplace(other_coords):\n2621                 f(self.variable, other_variable)\n2622             return self\n2623 \n2624         return func\n2625 \n2626     def _copy_attrs_from(self, other: Union[\"DataArray\", Dataset, Variable]) -> None:\n2627         self.attrs = other.attrs\n2628 \n2629     @property\n2630     def plot(self) -> _PlotMethods:\n2631         \"\"\"\n2632         Access plotting functions for DataArray's\n2633 \n2634         >>> d = xr.DataArray([[1, 2], [3, 4]])\n2635 \n2636         For convenience just call this directly\n2637 \n2638         >>> d.plot()\n2639 \n2640         Or use it as a namespace to use xarray.plot functions as\n2641         DataArray methods\n2642 \n2643         >>> d.plot.imshow()  # equivalent to xarray.plot.imshow(d)\n2644 \n2645         \"\"\"\n2646         return _PlotMethods(self)\n2647 \n2648     def _title_for_slice(self, truncate: int = 50) -> str:\n2649         \"\"\"\n2650         If the dataarray has 1 dimensional coordinates or comes from a slice\n2651         we can show that info in the title\n2652 \n2653         Parameters\n2654         ----------\n2655         truncate : integer\n2656             maximum number of characters for title\n2657 \n2658         Returns\n2659         -------\n2660         title : string\n2661             Can be used for plot titles\n2662 \n2663         \"\"\"\n2664         one_dims = []\n2665         for dim, coord in self.coords.items():\n2666             if coord.size == 1:\n2667                 one_dims.append(\n2668                     \"{dim} = {v}\".format(dim=dim, v=format_item(coord.values))\n2669                 )\n2670 \n2671         title = \", \".join(one_dims)\n2672         if len(title) > truncate:\n2673             title = title[: (truncate - 3)] + \"...\"\n2674 \n2675         return title\n2676 \n2677     def diff(self, dim: Hashable, n: int = 1, label: Hashable = \"upper\") -> \"DataArray\":\n2678         \"\"\"Calculate the n-th order discrete difference along given axis.\n2679 \n2680         Parameters\n2681         ----------\n2682         dim : hashable, optional\n2683             Dimension over which to calculate the finite difference.\n2684         n : int, optional\n2685             The number of times values are differenced.\n2686         label : hashable, optional\n2687             The new coordinate in dimension ``dim`` will have the\n2688             values of either the minuend's or subtrahend's coordinate\n2689             for values 'upper' and 'lower', respectively.  Other\n2690             values are not supported.\n2691 \n2692         Returns\n2693         -------\n2694         difference : same type as caller\n2695             The n-th order finite difference of this object.\n2696 \n2697         .. note::\n2698 \n2699             `n` matches numpy's behavior and is different from pandas' first\n2700             argument named `periods`.\n2701 \n2702 \n2703         Examples\n2704         --------\n2705         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], [\"x\"])\n2706         >>> arr.diff(\"x\")\n2707         <xarray.DataArray (x: 3)>\n2708         array([0, 1, 0])\n2709         Coordinates:\n2710         * x        (x) int64 2 3 4\n2711         >>> arr.diff(\"x\", 2)\n2712         <xarray.DataArray (x: 2)>\n2713         array([ 1, -1])\n2714         Coordinates:\n2715         * x        (x) int64 3 4\n2716 \n2717         See Also\n2718         --------\n2719         DataArray.differentiate\n2720         \"\"\"\n2721         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)\n2722         return self._from_temp_dataset(ds)\n2723 \n2724     def shift(\n2725         self,\n2726         shifts: Mapping[Hashable, int] = None,\n2727         fill_value: Any = dtypes.NA,\n2728         **shifts_kwargs: int,\n2729     ) -> \"DataArray\":\n2730         \"\"\"Shift this array by an offset along one or more dimensions.\n2731 \n2732         Only the data is moved; coordinates stay in place. Values shifted from\n2733         beyond array bounds are replaced by NaN. This is consistent with the\n2734         behavior of ``shift`` in pandas.\n2735 \n2736         Parameters\n2737         ----------\n2738         shifts : Mapping with the form of {dim: offset}\n2739             Integer offset to shift along each of the given dimensions.\n2740             Positive offsets shift to the right; negative offsets shift to the\n2741             left.\n2742         fill_value: scalar, optional\n2743             Value to use for newly missing values\n2744         **shifts_kwargs:\n2745             The keyword arguments form of ``shifts``.\n2746             One of shifts or shifts_kwargs must be provided.\n2747 \n2748         Returns\n2749         -------\n2750         shifted : DataArray\n2751             DataArray with the same coordinates and attributes but shifted\n2752             data.\n2753 \n2754         See also\n2755         --------\n2756         roll\n2757 \n2758         Examples\n2759         --------\n2760 \n2761         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n2762         >>> arr.shift(x=1)\n2763         <xarray.DataArray (x: 3)>\n2764         array([ nan,   5.,   6.])\n2765         Coordinates:\n2766           * x        (x) int64 0 1 2\n2767         \"\"\"\n2768         variable = self.variable.shift(\n2769             shifts=shifts, fill_value=fill_value, **shifts_kwargs\n2770         )\n2771         return self._replace(variable=variable)\n2772 \n2773     def roll(\n2774         self,\n2775         shifts: Mapping[Hashable, int] = None,\n2776         roll_coords: bool = None,\n2777         **shifts_kwargs: int,\n2778     ) -> \"DataArray\":\n2779         \"\"\"Roll this array by an offset along one or more dimensions.\n2780 \n2781         Unlike shift, roll may rotate all variables, including coordinates\n2782         if specified. The direction of rotation is consistent with\n2783         :py:func:`numpy.roll`.\n2784 \n2785         Parameters\n2786         ----------\n2787         shifts : Mapping with the form of {dim: offset}\n2788             Integer offset to rotate each of the given dimensions.\n2789             Positive offsets roll to the right; negative offsets roll to the\n2790             left.\n2791         roll_coords : bool\n2792             Indicates whether to  roll the coordinates by the offset\n2793             The current default of roll_coords (None, equivalent to True) is\n2794             deprecated and will change to False in a future version.\n2795             Explicitly pass roll_coords to silence the warning.\n2796         **shifts_kwargs : The keyword arguments form of ``shifts``.\n2797             One of shifts or shifts_kwargs must be provided.\n2798 \n2799         Returns\n2800         -------\n2801         rolled : DataArray\n2802             DataArray with the same attributes but rolled data and coordinates.\n2803 \n2804         See also\n2805         --------\n2806         shift\n2807 \n2808         Examples\n2809         --------\n2810 \n2811         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n2812         >>> arr.roll(x=1)\n2813         <xarray.DataArray (x: 3)>\n2814         array([7, 5, 6])\n2815         Coordinates:\n2816           * x        (x) int64 2 0 1\n2817         \"\"\"\n2818         ds = self._to_temp_dataset().roll(\n2819             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs\n2820         )\n2821         return self._from_temp_dataset(ds)\n2822 \n2823     @property\n2824     def real(self) -> \"DataArray\":\n2825         return self._replace(self.variable.real)\n2826 \n2827     @property\n2828     def imag(self) -> \"DataArray\":\n2829         return self._replace(self.variable.imag)\n2830 \n2831     def dot(\n2832         self, other: \"DataArray\", dims: Union[Hashable, Sequence[Hashable], None] = None\n2833     ) -> \"DataArray\":\n2834         \"\"\"Perform dot product of two DataArrays along their shared dims.\n2835 \n2836         Equivalent to taking taking tensordot over all shared dims.\n2837 \n2838         Parameters\n2839         ----------\n2840         other : DataArray\n2841             The other array with which the dot product is performed.\n2842         dims: '...', hashable or sequence of hashables, optional\n2843             Which dimensions to sum over. Ellipsis ('...') sums over all dimensions.\n2844             If not specified, then all the common dimensions are summed over.\n2845 \n2846         Returns\n2847         -------\n2848         result : DataArray\n2849             Array resulting from the dot product over all shared dimensions.\n2850 \n2851         See also\n2852         --------\n2853         dot\n2854         numpy.tensordot\n2855 \n2856         Examples\n2857         --------\n2858 \n2859         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))\n2860         >>> da = xr.DataArray(da_vals, dims=[\"x\", \"y\", \"z\"])\n2861         >>> dm_vals = np.arange(4)\n2862         >>> dm = xr.DataArray(dm_vals, dims=[\"z\"])\n2863 \n2864         >>> dm.dims\n2865         ('z')\n2866 \n2867         >>> da.dims\n2868         ('x', 'y', 'z')\n2869 \n2870         >>> dot_result = da.dot(dm)\n2871         >>> dot_result.dims\n2872         ('x', 'y')\n2873 \n2874         \"\"\"\n2875         if isinstance(other, Dataset):\n2876             raise NotImplementedError(\n2877                 \"dot products are not yet supported with Dataset objects.\"\n2878             )\n2879         if not isinstance(other, DataArray):\n2880             raise TypeError(\"dot only operates on DataArrays.\")\n2881 \n2882         return computation.dot(self, other, dims=dims)\n2883 \n2884     def sortby(\n2885         self,\n2886         variables: Union[Hashable, \"DataArray\", Sequence[Union[Hashable, \"DataArray\"]]],\n2887         ascending: bool = True,\n2888     ) -> \"DataArray\":\n2889         \"\"\"Sort object by labels or values (along an axis).\n2890 \n2891         Sorts the dataarray, either along specified dimensions,\n2892         or according to values of 1-D dataarrays that share dimension\n2893         with calling object.\n2894 \n2895         If the input variables are dataarrays, then the dataarrays are aligned\n2896         (via left-join) to the calling object prior to sorting by cell values.\n2897         NaNs are sorted to the end, following Numpy convention.\n2898 \n2899         If multiple sorts along the same dimension is\n2900         given, numpy's lexsort is performed along that dimension:\n2901         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html\n2902         and the FIRST key in the sequence is used as the primary sort key,\n2903         followed by the 2nd key, etc.\n2904 \n2905         Parameters\n2906         ----------\n2907         variables: hashable, DataArray, or sequence of either\n2908             1D DataArray objects or name(s) of 1D variable(s) in\n2909             coords whose values are used to sort this array.\n2910         ascending: boolean, optional\n2911             Whether to sort by ascending or descending order.\n2912 \n2913         Returns\n2914         -------\n2915         sorted: DataArray\n2916             A new dataarray where all the specified dims are sorted by dim\n2917             labels.\n2918 \n2919         Examples\n2920         --------\n2921 \n2922         >>> da = xr.DataArray(\n2923         ...     np.random.rand(5),\n2924         ...     coords=[pd.date_range(\"1/1/2000\", periods=5)],\n2925         ...     dims=\"time\",\n2926         ... )\n2927         >>> da\n2928         <xarray.DataArray (time: 5)>\n2929         array([ 0.965471,  0.615637,  0.26532 ,  0.270962,  0.552878])\n2930         Coordinates:\n2931           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\n2932 \n2933         >>> da.sortby(da)\n2934         <xarray.DataArray (time: 5)>\n2935         array([ 0.26532 ,  0.270962,  0.552878,  0.615637,  0.965471])\n2936         Coordinates:\n2937           * time     (time) datetime64[ns] 2000-01-03 2000-01-04 2000-01-05 ...\n2938         \"\"\"\n2939         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)\n2940         return self._from_temp_dataset(ds)\n2941 \n2942     def quantile(\n2943         self,\n2944         q: Any,\n2945         dim: Union[Hashable, Sequence[Hashable], None] = None,\n2946         interpolation: str = \"linear\",\n2947         keep_attrs: bool = None,\n2948         skipna: bool = True,\n2949     ) -> \"DataArray\":\n2950         \"\"\"Compute the qth quantile of the data along the specified dimension.\n2951 \n2952         Returns the qth quantiles(s) of the array elements.\n2953 \n2954         Parameters\n2955         ----------\n2956         q : float in range of [0,1] or array-like of floats\n2957             Quantile to compute, which must be between 0 and 1 inclusive.\n2958         dim : hashable or sequence of hashable, optional\n2959             Dimension(s) over which to apply quantile.\n2960         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n2961             This optional parameter specifies the interpolation method to\n2962             use when the desired quantile lies between two data points\n2963             ``i < j``:\n2964 \n2965                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is\n2966                   the fractional part of the index surrounded by ``i`` and\n2967                   ``j``.\n2968                 - lower: ``i``.\n2969                 - higher: ``j``.\n2970                 - nearest: ``i`` or ``j``, whichever is nearest.\n2971                 - midpoint: ``(i + j) / 2``.\n2972         keep_attrs : bool, optional\n2973             If True, the dataset's attributes (`attrs`) will be copied from\n2974             the original object to the new one.  If False (default), the new\n2975             object will be returned without attributes.\n2976         skipna : bool, optional\n2977             Whether to skip missing values when aggregating.\n2978 \n2979         Returns\n2980         -------\n2981         quantiles : DataArray\n2982             If `q` is a single quantile, then the result\n2983             is a scalar. If multiple percentiles are given, first axis of\n2984             the result corresponds to the quantile and a quantile dimension\n2985             is added to the return array. The other dimensions are the\n2986             dimensions that remain after the reduction of the array.\n2987 \n2988         See Also\n2989         --------\n2990         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile\n2991 \n2992         Examples\n2993         --------\n2994 \n2995         >>> da = xr.DataArray(\n2996         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],\n2997         ...     coords={\"x\": [7, 9], \"y\": [1, 1.5, 2, 2.5]},\n2998         ...     dims=(\"x\", \"y\"),\n2999         ... )\n3000         >>> da.quantile(0)  # or da.quantile(0, dim=...)\n3001         <xarray.DataArray ()>\n3002         array(0.7)\n3003         Coordinates:\n3004             quantile  float64 0.0\n3005         >>> da.quantile(0, dim=\"x\")\n3006         <xarray.DataArray (y: 4)>\n3007         array([0.7, 4.2, 2.6, 1.5])\n3008         Coordinates:\n3009           * y         (y) float64 1.0 1.5 2.0 2.5\n3010             quantile  float64 0.0\n3011         >>> da.quantile([0, 0.5, 1])\n3012         <xarray.DataArray (quantile: 3)>\n3013         array([0.7, 3.4, 9.4])\n3014         Coordinates:\n3015           * quantile  (quantile) float64 0.0 0.5 1.0\n3016         >>> da.quantile([0, 0.5, 1], dim=\"x\")\n3017         <xarray.DataArray (quantile: 3, y: 4)>\n3018         array([[0.7 , 4.2 , 2.6 , 1.5 ],\n3019                [3.6 , 5.75, 6.  , 1.7 ],\n3020                [6.5 , 7.3 , 9.4 , 1.9 ]])\n3021         Coordinates:\n3022           * y         (y) float64 1.0 1.5 2.0 2.5\n3023           * quantile  (quantile) float64 0.0 0.5 1.0\n3024         \"\"\"\n3025 \n3026         ds = self._to_temp_dataset().quantile(\n3027             q,\n3028             dim=dim,\n3029             keep_attrs=keep_attrs,\n3030             interpolation=interpolation,\n3031             skipna=skipna,\n3032         )\n3033         return self._from_temp_dataset(ds)\n3034 \n3035     def rank(\n3036         self, dim: Hashable, pct: bool = False, keep_attrs: bool = None\n3037     ) -> \"DataArray\":\n3038         \"\"\"Ranks the data.\n3039 \n3040         Equal values are assigned a rank that is the average of the ranks that\n3041         would have been otherwise assigned to all of the values within that\n3042         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.\n3043 \n3044         NaNs in the input array are returned as NaNs.\n3045 \n3046         The `bottleneck` library is required.\n3047 \n3048         Parameters\n3049         ----------\n3050         dim : hashable\n3051             Dimension over which to compute rank.\n3052         pct : bool, optional\n3053             If True, compute percentage ranks, otherwise compute integer ranks.\n3054         keep_attrs : bool, optional\n3055             If True, the dataset's attributes (`attrs`) will be copied from\n3056             the original object to the new one.  If False (default), the new\n3057             object will be returned without attributes.\n3058 \n3059         Returns\n3060         -------\n3061         ranked : DataArray\n3062             DataArray with the same coordinates and dtype 'float64'.\n3063 \n3064         Examples\n3065         --------\n3066 \n3067         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n3068         >>> arr.rank(\"x\")\n3069         <xarray.DataArray (x: 3)>\n3070         array([ 1.,   2.,   3.])\n3071         Dimensions without coordinates: x\n3072         \"\"\"\n3073 \n3074         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)\n3075         return self._from_temp_dataset(ds)\n3076 \n3077     def differentiate(\n3078         self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None\n3079     ) -> \"DataArray\":\n3080         \"\"\" Differentiate the array with the second order accurate central\n3081         differences.\n3082 \n3083         .. note::\n3084             This feature is limited to simple cartesian geometry, i.e. coord\n3085             must be one dimensional.\n3086 \n3087         Parameters\n3088         ----------\n3089         coord: hashable\n3090             The coordinate to be used to compute the gradient.\n3091         edge_order: 1 or 2. Default 1\n3092             N-th order accurate differences at the boundaries.\n3093         datetime_unit: None or any of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms',\n3094             'us', 'ns', 'ps', 'fs', 'as'}\n3095             Unit to compute gradient. Only valid for datetime coordinate.\n3096 \n3097         Returns\n3098         -------\n3099         differentiated: DataArray\n3100 \n3101         See also\n3102         --------\n3103         numpy.gradient: corresponding numpy function\n3104 \n3105         Examples\n3106         --------\n3107 \n3108         >>> da = xr.DataArray(\n3109         ...     np.arange(12).reshape(4, 3),\n3110         ...     dims=[\"x\", \"y\"],\n3111         ...     coords={\"x\": [0, 0.1, 1.1, 1.2]},\n3112         ... )\n3113         >>> da\n3114         <xarray.DataArray (x: 4, y: 3)>\n3115         array([[ 0,  1,  2],\n3116                [ 3,  4,  5],\n3117                [ 6,  7,  8],\n3118                [ 9, 10, 11]])\n3119         Coordinates:\n3120           * x        (x) float64 0.0 0.1 1.1 1.2\n3121         Dimensions without coordinates: y\n3122         >>>\n3123         >>> da.differentiate(\"x\")\n3124         <xarray.DataArray (x: 4, y: 3)>\n3125         array([[30.      , 30.      , 30.      ],\n3126                [27.545455, 27.545455, 27.545455],\n3127                [27.545455, 27.545455, 27.545455],\n3128                [30.      , 30.      , 30.      ]])\n3129         Coordinates:\n3130           * x        (x) float64 0.0 0.1 1.1 1.2\n3131         Dimensions without coordinates: y\n3132         \"\"\"\n3133         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)\n3134         return self._from_temp_dataset(ds)\n3135 \n3136     def integrate(\n3137         self, dim: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None\n3138     ) -> \"DataArray\":\n3139         \"\"\" integrate the array with the trapezoidal rule.\n3140 \n3141         .. note::\n3142             This feature is limited to simple cartesian geometry, i.e. dim\n3143             must be one dimensional.\n3144 \n3145         Parameters\n3146         ----------\n3147         dim: hashable, or a sequence of hashable\n3148             Coordinate(s) used for the integration.\n3149         datetime_unit: str, optional\n3150             Can be used to specify the unit if datetime coordinate is used.\n3151             One of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', 'ps',\n3152             'fs', 'as'}\n3153 \n3154         Returns\n3155         -------\n3156         integrated: DataArray\n3157 \n3158         See also\n3159         --------\n3160         numpy.trapz: corresponding numpy function\n3161 \n3162         Examples\n3163         --------\n3164 \n3165         >>> da = xr.DataArray(\n3166         ...     np.arange(12).reshape(4, 3),\n3167         ...     dims=[\"x\", \"y\"],\n3168         ...     coords={\"x\": [0, 0.1, 1.1, 1.2]},\n3169         ... )\n3170         >>> da\n3171         <xarray.DataArray (x: 4, y: 3)>\n3172         array([[ 0,  1,  2],\n3173                [ 3,  4,  5],\n3174                [ 6,  7,  8],\n3175                [ 9, 10, 11]])\n3176         Coordinates:\n3177           * x        (x) float64 0.0 0.1 1.1 1.2\n3178         Dimensions without coordinates: y\n3179         >>>\n3180         >>> da.integrate(\"x\")\n3181         <xarray.DataArray (y: 3)>\n3182         array([5.4, 6.6, 7.8])\n3183         Dimensions without coordinates: y\n3184         \"\"\"\n3185         ds = self._to_temp_dataset().integrate(dim, datetime_unit)\n3186         return self._from_temp_dataset(ds)\n3187 \n3188     def unify_chunks(self) -> \"DataArray\":\n3189         \"\"\" Unify chunk size along all chunked dimensions of this DataArray.\n3190 \n3191         Returns\n3192         -------\n3193 \n3194         DataArray with consistent chunk sizes for all dask-array variables\n3195 \n3196         See Also\n3197         --------\n3198 \n3199         dask.array.core.unify_chunks\n3200         \"\"\"\n3201         ds = self._to_temp_dataset().unify_chunks()\n3202         return self._from_temp_dataset(ds)\n3203 \n3204     def map_blocks(\n3205         self,\n3206         func: \"Callable[..., T_DSorDA]\",\n3207         args: Sequence[Any] = (),\n3208         kwargs: Mapping[str, Any] = None,\n3209     ) -> \"T_DSorDA\":\n3210         \"\"\"\n3211         Apply a function to each chunk of this DataArray. This method is experimental\n3212         and its signature may change.\n3213 \n3214         Parameters\n3215         ----------\n3216         func: callable\n3217             User-provided function that accepts a DataArray as its first parameter. The\n3218             function will receive a subset of this DataArray, corresponding to one chunk\n3219             along each chunked dimension. ``func`` will be executed as\n3220             ``func(obj_subset, *args, **kwargs)``.\n3221 \n3222             The function will be first run on mocked-up data, that looks like this array\n3223             but has sizes 0, to determine properties of the returned object such as\n3224             dtype, variable names, new dimensions and new indexes (if any).\n3225 \n3226             This function must return either a single DataArray or a single Dataset.\n3227 \n3228             This function cannot change size of existing dimensions, or add new chunked\n3229             dimensions.\n3230         args: Sequence\n3231             Passed verbatim to func after unpacking, after the sliced DataArray. xarray\n3232             objects, if any, will not be split by chunks. Passing dask collections is\n3233             not allowed.\n3234         kwargs: Mapping\n3235             Passed verbatim to func after unpacking. xarray objects, if any, will not be\n3236             split by chunks. Passing dask collections is not allowed.\n3237 \n3238         Returns\n3239         -------\n3240         A single DataArray or Dataset with dask backend, reassembled from the outputs of\n3241         the function.\n3242 \n3243         Notes\n3244         -----\n3245         This method is designed for when one needs to manipulate a whole xarray object\n3246         within each chunk. In the more common case where one can work on numpy arrays,\n3247         it is recommended to use apply_ufunc.\n3248 \n3249         If none of the variables in this DataArray is backed by dask, calling this\n3250         method is equivalent to calling ``func(self, *args, **kwargs)``.\n3251 \n3252         See Also\n3253         --------\n3254         dask.array.map_blocks, xarray.apply_ufunc, xarray.map_blocks,\n3255         xarray.Dataset.map_blocks\n3256         \"\"\"\n3257         from .parallel import map_blocks\n3258 \n3259         return map_blocks(func, self, args, kwargs)\n3260 \n3261     # this needs to be at the end, or mypy will confuse with `str`\n3262     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names\n3263     str = property(StringAccessor)\n",
        "intended_behavior": "Introduce an 'average' method that enables weighted average calculation for data arrays. This method would be very similar to the one in 'Dataset', tailored to the 'DataArray' structure, leveraging the existing data handling methods and ensuring it integrates seamlessly with the broader xarray functionality."
    },
    {
        "rel_file_path": "xarray/core/variable.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/variable.py",
        "start": 1,
        "end": 2369,
        "class_name": null,
        "method_name": null,
        "code": "1 import copy\n2 import functools\n3 import itertools\n4 import warnings\n5 from collections import defaultdict\n6 from datetime import timedelta\n7 from distutils.version import LooseVersion\n8 from typing import Any, Dict, Hashable, Mapping, TypeVar, Union\n9 \n10 import numpy as np\n11 import pandas as pd\n12 \n13 import xarray as xr  # only for Dataset and DataArray\n14 \n15 from . import arithmetic, common, dtypes, duck_array_ops, indexing, nputils, ops, utils\n16 from .indexing import (\n17     BasicIndexer,\n18     OuterIndexer,\n19     PandasIndexAdapter,\n20     VectorizedIndexer,\n21     as_indexable,\n22 )\n23 from .npcompat import IS_NEP18_ACTIVE\n24 from .options import _get_keep_attrs\n25 from .pycompat import dask_array_type, integer_types\n26 from .utils import (\n27     OrderedSet,\n28     _default,\n29     decode_numpy_dict_values,\n30     either_dict_or_kwargs,\n31     ensure_us_time_resolution,\n32     infix_dims,\n33 )\n34 \n35 try:\n36     import dask.array as da\n37 except ImportError:\n38     pass\n39 \n40 \n41 NON_NUMPY_SUPPORTED_ARRAY_TYPES = (\n42     indexing.ExplicitlyIndexed,\n43     pd.Index,\n44 ) + dask_array_type\n45 # https://github.com/python/mypy/issues/224\n46 BASIC_INDEXING_TYPES = integer_types + (slice,)  # type: ignore\n47 \n48 VariableType = TypeVar(\"VariableType\", bound=\"Variable\")\n49 \"\"\"Type annotation to be used when methods of Variable return self or a copy of self.\n50 When called from an instance of a subclass, e.g. IndexVariable, mypy identifies the\n51 output as an instance of the subclass.\n52 \n53 Usage::\n54 \n55    class Variable:\n56        def f(self: VariableType, ...) -> VariableType:\n57            ...\n58 \"\"\"\n59 \n60 \n61 class MissingDimensionsError(ValueError):\n62     \"\"\"Error class used when we can't safely guess a dimension name.\n63     \"\"\"\n64 \n65     # inherits from ValueError for backward compatibility\n66     # TODO: move this to an xarray.exceptions module?\n67 \n68 \n69 def as_variable(obj, name=None) -> \"Union[Variable, IndexVariable]\":\n70     \"\"\"Convert an object into a Variable.\n71 \n72     Parameters\n73     ----------\n74     obj : object\n75         Object to convert into a Variable.\n76 \n77         - If the object is already a Variable, return a shallow copy.\n78         - Otherwise, if the object has 'dims' and 'data' attributes, convert\n79           it into a new Variable.\n80         - If all else fails, attempt to convert the object into a Variable by\n81           unpacking it into the arguments for creating a new Variable.\n82     name : str, optional\n83         If provided:\n84 \n85         - `obj` can be a 1D array, which is assumed to label coordinate values\n86           along a dimension of this given name.\n87         - Variables with name matching one of their dimensions are converted\n88           into `IndexVariable` objects.\n89 \n90     Returns\n91     -------\n92     var : Variable\n93         The newly created variable.\n94 \n95     \"\"\"\n96     from .dataarray import DataArray\n97 \n98     # TODO: consider extending this method to automatically handle Iris and\n99     if isinstance(obj, DataArray):\n100         # extract the primary Variable from DataArrays\n101         obj = obj.variable\n102 \n103     if isinstance(obj, Variable):\n104         obj = obj.copy(deep=False)\n105     elif isinstance(obj, tuple):\n106         try:\n107             obj = Variable(*obj)\n108         except (TypeError, ValueError) as error:\n109             # use .format() instead of % because it handles tuples consistently\n110             raise error.__class__(\n111                 \"Could not convert tuple of form \"\n112                 \"(dims, data[, attrs, encoding]): \"\n113                 \"{} to Variable.\".format(obj)\n114             )\n115     elif utils.is_scalar(obj):\n116         obj = Variable([], obj)\n117     elif isinstance(obj, (pd.Index, IndexVariable)) and obj.name is not None:\n118         obj = Variable(obj.name, obj)\n119     elif isinstance(obj, (set, dict)):\n120         raise TypeError(\"variable {!r} has invalid type {!r}\".format(name, type(obj)))\n121     elif name is not None:\n122         data = as_compatible_data(obj)\n123         if data.ndim != 1:\n124             raise MissingDimensionsError(\n125                 \"cannot set variable %r with %r-dimensional data \"\n126                 \"without explicit dimension names. Pass a tuple of \"\n127                 \"(dims, data) instead.\" % (name, data.ndim)\n128             )\n129         obj = Variable(name, data, fastpath=True)\n130     else:\n131         raise TypeError(\n132             \"unable to convert object into a variable without an \"\n133             \"explicit list of dimensions: %r\" % obj\n134         )\n135 \n136     if name is not None and name in obj.dims:\n137         # convert the Variable into an Index\n138         if obj.ndim != 1:\n139             raise MissingDimensionsError(\n140                 \"%r has more than 1-dimension and the same name as one of its \"\n141                 \"dimensions %r. xarray disallows such variables because they \"\n142                 \"conflict with the coordinates used to label \"\n143                 \"dimensions.\" % (name, obj.dims)\n144             )\n145         obj = obj.to_index_variable()\n146 \n147     return obj\n148 \n149 \n150 def _maybe_wrap_data(data):\n151     \"\"\"\n152     Put pandas.Index and numpy.ndarray arguments in adapter objects to ensure\n153     they can be indexed properly.\n154 \n155     NumpyArrayAdapter, PandasIndexAdapter and LazilyOuterIndexedArray should\n156     all pass through unmodified.\n157     \"\"\"\n158     if isinstance(data, pd.Index):\n159         return PandasIndexAdapter(data)\n160     return data\n161 \n162 \n163 def _possibly_convert_objects(values):\n164     \"\"\"Convert arrays of datetime.datetime and datetime.timedelta objects into\n165     datetime64 and timedelta64, according to the pandas convention.\n166     \"\"\"\n167     return np.asarray(pd.Series(values.ravel())).reshape(values.shape)\n168 \n169 \n170 def as_compatible_data(data, fastpath=False):\n171     \"\"\"Prepare and wrap data to put in a Variable.\n172 \n173     - If data does not have the necessary attributes, convert it to ndarray.\n174     - If data has dtype=datetime64, ensure that it has ns precision. If it's a\n175       pandas.Timestamp, convert it to datetime64.\n176     - If data is already a pandas or xarray object (other than an Index), just\n177       use the values.\n178 \n179     Finally, wrap it up with an adapter if necessary.\n180     \"\"\"\n181     if fastpath and getattr(data, \"ndim\", 0) > 0:\n182         # can't use fastpath (yet) for scalars\n183         return _maybe_wrap_data(data)\n184 \n185     if isinstance(data, Variable):\n186         return data.data\n187 \n188     if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):\n189         return _maybe_wrap_data(data)\n190 \n191     if isinstance(data, tuple):\n192         data = utils.to_0d_object_array(data)\n193 \n194     if isinstance(data, pd.Timestamp):\n195         # TODO: convert, handle datetime objects, too\n196         data = np.datetime64(data.value, \"ns\")\n197 \n198     if isinstance(data, timedelta):\n199         data = np.timedelta64(getattr(data, \"value\", data), \"ns\")\n200 \n201     # we don't want nested self-described arrays\n202     data = getattr(data, \"values\", data)\n203 \n204     if isinstance(data, np.ma.MaskedArray):\n205         mask = np.ma.getmaskarray(data)\n206         if mask.any():\n207             dtype, fill_value = dtypes.maybe_promote(data.dtype)\n208             data = np.asarray(data, dtype=dtype)\n209             data[mask] = fill_value\n210         else:\n211             data = np.asarray(data)\n212 \n213     if not isinstance(data, np.ndarray):\n214         if hasattr(data, \"__array_function__\"):\n215             if IS_NEP18_ACTIVE:\n216                 return data\n217             else:\n218                 raise TypeError(\n219                     \"Got an NumPy-like array type providing the \"\n220                     \"__array_function__ protocol but NEP18 is not enabled. \"\n221                     \"Check that numpy >= v1.16 and that the environment \"\n222                     'variable \"NUMPY_EXPERIMENTAL_ARRAY_FUNCTION\" is set to '\n223                     '\"1\"'\n224                 )\n225 \n226     # validate whether the data is valid data types\n227     data = np.asarray(data)\n228 \n229     if isinstance(data, np.ndarray):\n230         if data.dtype.kind == \"O\":\n231             data = _possibly_convert_objects(data)\n232         elif data.dtype.kind == \"M\":\n233             data = np.asarray(data, \"datetime64[ns]\")\n234         elif data.dtype.kind == \"m\":\n235             data = np.asarray(data, \"timedelta64[ns]\")\n236 \n237     return _maybe_wrap_data(data)\n238 \n239 \n240 def _as_array_or_item(data):\n241     \"\"\"Return the given values as a numpy array, or as an individual item if\n242     it's a 0d datetime64 or timedelta64 array.\n243 \n244     Importantly, this function does not copy data if it is already an ndarray -\n245     otherwise, it will not be possible to update Variable values in place.\n246 \n247     This function mostly exists because 0-dimensional ndarrays with\n248     dtype=datetime64 are broken :(\n249     https://github.com/numpy/numpy/issues/4337\n250     https://github.com/numpy/numpy/issues/7619\n251 \n252     TODO: remove this (replace with np.asarray) once these issues are fixed\n253     \"\"\"\n254     data = np.asarray(data)\n255     if data.ndim == 0:\n256         if data.dtype.kind == \"M\":\n257             data = np.datetime64(data, \"ns\")\n258         elif data.dtype.kind == \"m\":\n259             data = np.timedelta64(data, \"ns\")\n260     return data\n261 \n262 \n263 class Variable(\n264     common.AbstractArray, arithmetic.SupportsArithmetic, utils.NdimSizeLenMixin\n265 ):\n266     \"\"\"A netcdf-like variable consisting of dimensions, data and attributes\n267     which describe a single Array. A single Variable object is not fully\n268     described outside the context of its parent Dataset (if you want such a\n269     fully described object, use a DataArray instead).\n270 \n271     The main functional difference between Variables and numpy arrays is that\n272     numerical operations on Variables implement array broadcasting by dimension\n273     name. For example, adding an Variable with dimensions `('time',)` to\n274     another Variable with dimensions `('space',)` results in a new Variable\n275     with dimensions `('time', 'space')`. Furthermore, numpy reduce operations\n276     like ``mean`` or ``sum`` are overwritten to take a \"dimension\" argument\n277     instead of an \"axis\".\n278 \n279     Variables are light-weight objects used as the building block for datasets.\n280     They are more primitive objects, so operations with them provide marginally\n281     higher performance than using DataArrays. However, manipulating data in the\n282     form of a Dataset or DataArray should almost always be preferred, because\n283     they can use more complete metadata in context of coordinate labels.\n284     \"\"\"\n285 \n286     __slots__ = (\"_dims\", \"_data\", \"_attrs\", \"_encoding\")\n287 \n288     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n289         \"\"\"\n290         Parameters\n291         ----------\n292         dims : str or sequence of str\n293             Name(s) of the the data dimension(s). Must be either a string (only\n294             for 1D data) or a sequence of strings with length equal to the\n295             number of dimensions.\n296         data : array_like\n297             Data array which supports numpy-like data access.\n298         attrs : dict_like or None, optional\n299             Attributes to assign to the new variable. If None (default), an\n300             empty attribute dictionary is initialized.\n301         encoding : dict_like or None, optional\n302             Dictionary specifying how to encode this array's data into a\n303             serialized format like netCDF4. Currently used keys (for netCDF)\n304             include '_FillValue', 'scale_factor', 'add_offset' and 'dtype'.\n305             Well-behaved code to serialize a Variable should ignore\n306             unrecognized encoding items.\n307         \"\"\"\n308         self._data = as_compatible_data(data, fastpath=fastpath)\n309         self._dims = self._parse_dimensions(dims)\n310         self._attrs = None\n311         self._encoding = None\n312         if attrs is not None:\n313             self.attrs = attrs\n314         if encoding is not None:\n315             self.encoding = encoding\n316 \n317     @property\n318     def dtype(self):\n319         return self._data.dtype\n320 \n321     @property\n322     def shape(self):\n323         return self._data.shape\n324 \n325     @property\n326     def nbytes(self):\n327         return self.size * self.dtype.itemsize\n328 \n329     @property\n330     def _in_memory(self):\n331         return isinstance(self._data, (np.ndarray, np.number, PandasIndexAdapter)) or (\n332             isinstance(self._data, indexing.MemoryCachedArray)\n333             and isinstance(self._data.array, indexing.NumpyIndexingAdapter)\n334         )\n335 \n336     @property\n337     def data(self):\n338         if hasattr(self._data, \"__array_function__\") or isinstance(\n339             self._data, dask_array_type\n340         ):\n341             return self._data\n342         else:\n343             return self.values\n344 \n345     @data.setter\n346     def data(self, data):\n347         data = as_compatible_data(data)\n348         if data.shape != self.shape:\n349             raise ValueError(\n350                 f\"replacement data must match the Variable's shape. \"\n351                 f\"replacement data has shape {data.shape}; Variable has shape {self.shape}\"\n352             )\n353         self._data = data\n354 \n355     def load(self, **kwargs):\n356         \"\"\"Manually trigger loading of this variable's data from disk or a\n357         remote source into memory and return this variable.\n358 \n359         Normally, it should not be necessary to call this method in user code,\n360         because all xarray functions should either work on deferred data or\n361         load data automatically.\n362 \n363         Parameters\n364         ----------\n365         **kwargs : dict\n366             Additional keyword arguments passed on to ``dask.array.compute``.\n367 \n368         See Also\n369         --------\n370         dask.array.compute\n371         \"\"\"\n372         if isinstance(self._data, dask_array_type):\n373             self._data = as_compatible_data(self._data.compute(**kwargs))\n374         elif not hasattr(self._data, \"__array_function__\"):\n375             self._data = np.asarray(self._data)\n376         return self\n377 \n378     def compute(self, **kwargs):\n379         \"\"\"Manually trigger loading of this variable's data from disk or a\n380         remote source into memory and return a new variable. The original is\n381         left unaltered.\n382 \n383         Normally, it should not be necessary to call this method in user code,\n384         because all xarray functions should either work on deferred data or\n385         load data automatically.\n386 \n387         Parameters\n388         ----------\n389         **kwargs : dict\n390             Additional keyword arguments passed on to ``dask.array.compute``.\n391 \n392         See Also\n393         --------\n394         dask.array.compute\n395         \"\"\"\n396         new = self.copy(deep=False)\n397         return new.load(**kwargs)\n398 \n399     def __dask_tokenize__(self):\n400         # Use v.data, instead of v._data, in order to cope with the wrappers\n401         # around NetCDF and the like\n402         from dask.base import normalize_token\n403 \n404         return normalize_token((type(self), self._dims, self.data, self._attrs))\n405 \n406     def __dask_graph__(self):\n407         if isinstance(self._data, dask_array_type):\n408             return self._data.__dask_graph__()\n409         else:\n410             return None\n411 \n412     def __dask_keys__(self):\n413         return self._data.__dask_keys__()\n414 \n415     def __dask_layers__(self):\n416         return self._data.__dask_layers__()\n417 \n418     @property\n419     def __dask_optimize__(self):\n420         return self._data.__dask_optimize__\n421 \n422     @property\n423     def __dask_scheduler__(self):\n424         return self._data.__dask_scheduler__\n425 \n426     def __dask_postcompute__(self):\n427         array_func, array_args = self._data.__dask_postcompute__()\n428         return (\n429             self._dask_finalize,\n430             (array_func, array_args, self._dims, self._attrs, self._encoding),\n431         )\n432 \n433     def __dask_postpersist__(self):\n434         array_func, array_args = self._data.__dask_postpersist__()\n435         return (\n436             self._dask_finalize,\n437             (array_func, array_args, self._dims, self._attrs, self._encoding),\n438         )\n439 \n440     @staticmethod\n441     def _dask_finalize(results, array_func, array_args, dims, attrs, encoding):\n442         if isinstance(results, dict):  # persist case\n443             name = array_args[0]\n444             results = {k: v for k, v in results.items() if k[0] == name}\n445         data = array_func(results, *array_args)\n446         return Variable(dims, data, attrs=attrs, encoding=encoding)\n447 \n448     @property\n449     def values(self):\n450         \"\"\"The variable's data as a numpy.ndarray\"\"\"\n451         return _as_array_or_item(self._data)\n452 \n453     @values.setter\n454     def values(self, values):\n455         self.data = values\n456 \n457     def to_base_variable(self):\n458         \"\"\"Return this variable as a base xarray.Variable\"\"\"\n459         return Variable(\n460             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True\n461         )\n462 \n463     to_variable = utils.alias(to_base_variable, \"to_variable\")\n464 \n465     def to_index_variable(self):\n466         \"\"\"Return this variable as an xarray.IndexVariable\"\"\"\n467         return IndexVariable(\n468             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True\n469         )\n470 \n471     to_coord = utils.alias(to_index_variable, \"to_coord\")\n472 \n473     def to_index(self):\n474         \"\"\"Convert this variable to a pandas.Index\"\"\"\n475         return self.to_index_variable().to_index()\n476 \n477     def to_dict(self, data=True):\n478         \"\"\"Dictionary representation of variable.\"\"\"\n479         item = {\"dims\": self.dims, \"attrs\": decode_numpy_dict_values(self.attrs)}\n480         if data:\n481             item[\"data\"] = ensure_us_time_resolution(self.values).tolist()\n482         else:\n483             item.update({\"dtype\": str(self.dtype), \"shape\": self.shape})\n484         return item\n485 \n486     @property\n487     def dims(self):\n488         \"\"\"Tuple of dimension names with which this variable is associated.\n489         \"\"\"\n490         return self._dims\n491 \n492     @dims.setter\n493     def dims(self, value):\n494         self._dims = self._parse_dimensions(value)\n495 \n496     def _parse_dimensions(self, dims):\n497         if isinstance(dims, str):\n498             dims = (dims,)\n499         dims = tuple(dims)\n500         if len(dims) != self.ndim:\n501             raise ValueError(\n502                 \"dimensions %s must have the same length as the \"\n503                 \"number of data dimensions, ndim=%s\" % (dims, self.ndim)\n504             )\n505         return dims\n506 \n507     def _item_key_to_tuple(self, key):\n508         if utils.is_dict_like(key):\n509             return tuple(key.get(dim, slice(None)) for dim in self.dims)\n510         else:\n511             return key\n512 \n513     def _broadcast_indexes(self, key):\n514         \"\"\"Prepare an indexing key for an indexing operation.\n515 \n516         Parameters\n517         -----------\n518         key: int, slice, array, dict or tuple of integer, slices and arrays\n519             Any valid input for indexing.\n520 \n521         Returns\n522         -------\n523         dims: tuple\n524             Dimension of the resultant variable.\n525         indexers: IndexingTuple subclass\n526             Tuple of integer, array-like, or slices to use when indexing\n527             self._data. The type of this argument indicates the type of\n528             indexing to perform, either basic, outer or vectorized.\n529         new_order : Optional[Sequence[int]]\n530             Optional reordering to do on the result of indexing. If not None,\n531             the first len(new_order) indexing should be moved to these\n532             positions.\n533         \"\"\"\n534         key = self._item_key_to_tuple(key)  # key is a tuple\n535         # key is a tuple of full size\n536         key = indexing.expanded_indexer(key, self.ndim)\n537         # Convert a scalar Variable to an integer\n538         key = tuple(\n539             k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k for k in key\n540         )\n541         # Convert a 0d-array to an integer\n542         key = tuple(\n543             k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key\n544         )\n545 \n546         if all(isinstance(k, BASIC_INDEXING_TYPES) for k in key):\n547             return self._broadcast_indexes_basic(key)\n548 \n549         self._validate_indexers(key)\n550         # Detect it can be mapped as an outer indexer\n551         # If all key is unlabeled, or\n552         # key can be mapped as an OuterIndexer.\n553         if all(not isinstance(k, Variable) for k in key):\n554             return self._broadcast_indexes_outer(key)\n555 \n556         # If all key is 1-dimensional and there are no duplicate labels,\n557         # key can be mapped as an OuterIndexer.\n558         dims = []\n559         for k, d in zip(key, self.dims):\n560             if isinstance(k, Variable):\n561                 if len(k.dims) > 1:\n562                     return self._broadcast_indexes_vectorized(key)\n563                 dims.append(k.dims[0])\n564             elif not isinstance(k, integer_types):\n565                 dims.append(d)\n566         if len(set(dims)) == len(dims):\n567             return self._broadcast_indexes_outer(key)\n568 \n569         return self._broadcast_indexes_vectorized(key)\n570 \n571     def _broadcast_indexes_basic(self, key):\n572         dims = tuple(\n573             dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)\n574         )\n575         return dims, BasicIndexer(key), None\n576 \n577     def _validate_indexers(self, key):\n578         \"\"\" Make sanity checks \"\"\"\n579         for dim, k in zip(self.dims, key):\n580             if isinstance(k, BASIC_INDEXING_TYPES):\n581                 pass\n582             else:\n583                 if not isinstance(k, Variable):\n584                     k = np.asarray(k)\n585                     if k.ndim > 1:\n586                         raise IndexError(\n587                             \"Unlabeled multi-dimensional array cannot be \"\n588                             \"used for indexing: {}\".format(k)\n589                         )\n590                 if k.dtype.kind == \"b\":\n591                     if self.shape[self.get_axis_num(dim)] != len(k):\n592                         raise IndexError(\n593                             \"Boolean array size {:d} is used to index array \"\n594                             \"with shape {:s}.\".format(len(k), str(self.shape))\n595                         )\n596                     if k.ndim > 1:\n597                         raise IndexError(\n598                             \"{}-dimensional boolean indexing is \"\n599                             \"not supported. \".format(k.ndim)\n600                         )\n601                     if getattr(k, \"dims\", (dim,)) != (dim,):\n602                         raise IndexError(\n603                             \"Boolean indexer should be unlabeled or on the \"\n604                             \"same dimension to the indexed array. Indexer is \"\n605                             \"on {:s} but the target dimension is {:s}.\".format(\n606                                 str(k.dims), dim\n607                             )\n608                         )\n609 \n610     def _broadcast_indexes_outer(self, key):\n611         dims = tuple(\n612             k.dims[0] if isinstance(k, Variable) else dim\n613             for k, dim in zip(key, self.dims)\n614             if not isinstance(k, integer_types)\n615         )\n616 \n617         new_key = []\n618         for k in key:\n619             if isinstance(k, Variable):\n620                 k = k.data\n621             if not isinstance(k, BASIC_INDEXING_TYPES):\n622                 k = np.asarray(k)\n623                 if k.size == 0:\n624                     # Slice by empty list; numpy could not infer the dtype\n625                     k = k.astype(int)\n626                 elif k.dtype.kind == \"b\":\n627                     (k,) = np.nonzero(k)\n628             new_key.append(k)\n629 \n630         return dims, OuterIndexer(tuple(new_key)), None\n631 \n632     def _nonzero(self):\n633         \"\"\" Equivalent numpy's nonzero but returns a tuple of Varibles. \"\"\"\n634         # TODO we should replace dask's native nonzero\n635         # after https://github.com/dask/dask/issues/1076 is implemented.\n636         nonzeros = np.nonzero(self.data)\n637         return tuple(Variable((dim), nz) for nz, dim in zip(nonzeros, self.dims))\n638 \n639     def _broadcast_indexes_vectorized(self, key):\n640         variables = []\n641         out_dims_set = OrderedSet()\n642         for dim, value in zip(self.dims, key):\n643             if isinstance(value, slice):\n644                 out_dims_set.add(dim)\n645             else:\n646                 variable = (\n647                     value\n648                     if isinstance(value, Variable)\n649                     else as_variable(value, name=dim)\n650                 )\n651                 if variable.dtype.kind == \"b\":  # boolean indexing case\n652                     (variable,) = variable._nonzero()\n653 \n654                 variables.append(variable)\n655                 out_dims_set.update(variable.dims)\n656 \n657         variable_dims = set()\n658         for variable in variables:\n659             variable_dims.update(variable.dims)\n660 \n661         slices = []\n662         for i, (dim, value) in enumerate(zip(self.dims, key)):\n663             if isinstance(value, slice):\n664                 if dim in variable_dims:\n665                     # We only convert slice objects to variables if they share\n666                     # a dimension with at least one other variable. Otherwise,\n667                     # we can equivalently leave them as slices aknd transpose\n668                     # the result. This is significantly faster/more efficient\n669                     # for most array backends.\n670                     values = np.arange(*value.indices(self.sizes[dim]))\n671                     variables.insert(i - len(slices), Variable((dim,), values))\n672                 else:\n673                     slices.append((i, value))\n674 \n675         try:\n676             variables = _broadcast_compat_variables(*variables)\n677         except ValueError:\n678             raise IndexError(f\"Dimensions of indexers mismatch: {key}\")\n679 \n680         out_key = [variable.data for variable in variables]\n681         out_dims = tuple(out_dims_set)\n682         slice_positions = set()\n683         for i, value in slices:\n684             out_key.insert(i, value)\n685             new_position = out_dims.index(self.dims[i])\n686             slice_positions.add(new_position)\n687 \n688         if slice_positions:\n689             new_order = [i for i in range(len(out_dims)) if i not in slice_positions]\n690         else:\n691             new_order = None\n692 \n693         return out_dims, VectorizedIndexer(tuple(out_key)), new_order\n694 \n695     def __getitem__(self: VariableType, key) -> VariableType:\n696         \"\"\"Return a new Variable object whose contents are consistent with\n697         getting the provided key from the underlying data.\n698 \n699         NB. __getitem__ and __setitem__ implement xarray-style indexing,\n700         where if keys are unlabeled arrays, we index the array orthogonally\n701         with them. If keys are labeled array (such as Variables), they are\n702         broadcasted with our usual scheme and then the array is indexed with\n703         the broadcasted key, like numpy's fancy indexing.\n704 \n705         If you really want to do indexing like `x[x > 0]`, manipulate the numpy\n706         array `x.values` directly.\n707         \"\"\"\n708         dims, indexer, new_order = self._broadcast_indexes(key)\n709         data = as_indexable(self._data)[indexer]\n710         if new_order:\n711             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)\n712         return self._finalize_indexing_result(dims, data)\n713 \n714     def _finalize_indexing_result(self: VariableType, dims, data) -> VariableType:\n715         \"\"\"Used by IndexVariable to return IndexVariable objects when possible.\n716         \"\"\"\n717         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)\n718 \n719     def _getitem_with_mask(self, key, fill_value=dtypes.NA):\n720         \"\"\"Index this Variable with -1 remapped to fill_value.\"\"\"\n721         # TODO(shoyer): expose this method in public API somewhere (isel?) and\n722         # use it for reindex.\n723         # TODO(shoyer): add a sanity check that all other integers are\n724         # non-negative\n725         # TODO(shoyer): add an optimization, remapping -1 to an adjacent value\n726         # that is actually indexed rather than mapping it to the last value\n727         # along each axis.\n728 \n729         if fill_value is dtypes.NA:\n730             fill_value = dtypes.get_fill_value(self.dtype)\n731 \n732         dims, indexer, new_order = self._broadcast_indexes(key)\n733 \n734         if self.size:\n735             if isinstance(self._data, dask_array_type):\n736                 # dask's indexing is faster this way; also vindex does not\n737                 # support negative indices yet:\n738                 # https://github.com/dask/dask/pull/2967\n739                 actual_indexer = indexing.posify_mask_indexer(indexer)\n740             else:\n741                 actual_indexer = indexer\n742 \n743             data = as_indexable(self._data)[actual_indexer]\n744             mask = indexing.create_mask(indexer, self.shape, data)\n745             # we need to invert the mask in order to pass data first. This helps\n746             # pint to choose the correct unit\n747             # TODO: revert after https://github.com/hgrecco/pint/issues/1019 is fixed\n748             data = duck_array_ops.where(np.logical_not(mask), data, fill_value)\n749         else:\n750             # array cannot be indexed along dimensions of size 0, so just\n751             # build the mask directly instead.\n752             mask = indexing.create_mask(indexer, self.shape)\n753             data = np.broadcast_to(fill_value, getattr(mask, \"shape\", ()))\n754 \n755         if new_order:\n756             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)\n757         return self._finalize_indexing_result(dims, data)\n758 \n759     def __setitem__(self, key, value):\n760         \"\"\"__setitem__ is overloaded to access the underlying numpy values with\n761         orthogonal indexing.\n762 \n763         See __getitem__ for more details.\n764         \"\"\"\n765         dims, index_tuple, new_order = self._broadcast_indexes(key)\n766 \n767         if not isinstance(value, Variable):\n768             value = as_compatible_data(value)\n769             if value.ndim > len(dims):\n770                 raise ValueError(\n771                     \"shape mismatch: value array of shape %s could not be \"\n772                     \"broadcast to indexing result with %s dimensions\"\n773                     % (value.shape, len(dims))\n774                 )\n775             if value.ndim == 0:\n776                 value = Variable((), value)\n777             else:\n778                 value = Variable(dims[-value.ndim :], value)\n779         # broadcast to become assignable\n780         value = value.set_dims(dims).data\n781 \n782         if new_order:\n783             value = duck_array_ops.asarray(value)\n784             value = value[(len(dims) - value.ndim) * (np.newaxis,) + (Ellipsis,)]\n785             value = duck_array_ops.moveaxis(value, new_order, range(len(new_order)))\n786 \n787         indexable = as_indexable(self._data)\n788         indexable[index_tuple] = value\n789 \n790     @property\n791     def attrs(self) -> Dict[Hashable, Any]:\n792         \"\"\"Dictionary of local attributes on this variable.\n793         \"\"\"\n794         if self._attrs is None:\n795             self._attrs = {}\n796         return self._attrs\n797 \n798     @attrs.setter\n799     def attrs(self, value: Mapping[Hashable, Any]) -> None:\n800         self._attrs = dict(value)\n801 \n802     @property\n803     def encoding(self):\n804         \"\"\"Dictionary of encodings on this variable.\n805         \"\"\"\n806         if self._encoding is None:\n807             self._encoding = {}\n808         return self._encoding\n809 \n810     @encoding.setter\n811     def encoding(self, value):\n812         try:\n813             self._encoding = dict(value)\n814         except ValueError:\n815             raise ValueError(\"encoding must be castable to a dictionary\")\n816 \n817     def copy(self, deep=True, data=None):\n818         \"\"\"Returns a copy of this object.\n819 \n820         If `deep=True`, the data array is loaded into memory and copied onto\n821         the new object. Dimensions, attributes and encodings are always copied.\n822 \n823         Use `data` to create a new object with the same structure as\n824         original but entirely new data.\n825 \n826         Parameters\n827         ----------\n828         deep : bool, optional\n829             Whether the data array is loaded into memory and copied onto\n830             the new object. Default is True.\n831         data : array_like, optional\n832             Data to use in the new object. Must have same shape as original.\n833             When `data` is used, `deep` is ignored.\n834 \n835         Returns\n836         -------\n837         object : Variable\n838             New object with dimensions, attributes, encodings, and optionally\n839             data copied from original.\n840 \n841         Examples\n842         --------\n843 \n844         Shallow copy versus deep copy\n845 \n846         >>> var = xr.Variable(data=[1, 2, 3], dims=\"x\")\n847         >>> var.copy()\n848         <xarray.Variable (x: 3)>\n849         array([1, 2, 3])\n850         >>> var_0 = var.copy(deep=False)\n851         >>> var_0[0] = 7\n852         >>> var_0\n853         <xarray.Variable (x: 3)>\n854         array([7, 2, 3])\n855         >>> var\n856         <xarray.Variable (x: 3)>\n857         array([7, 2, 3])\n858 \n859         Changing the data using the ``data`` argument maintains the\n860         structure of the original object, but with the new data. Original\n861         object is unaffected.\n862 \n863         >>> var.copy(data=[0.1, 0.2, 0.3])\n864         <xarray.Variable (x: 3)>\n865         array([ 0.1,  0.2,  0.3])\n866         >>> var\n867         <xarray.Variable (x: 3)>\n868         array([7, 2, 3])\n869 \n870         See Also\n871         --------\n872         pandas.DataFrame.copy\n873         \"\"\"\n874         if data is None:\n875             data = self._data\n876 \n877             if isinstance(data, indexing.MemoryCachedArray):\n878                 # don't share caching between copies\n879                 data = indexing.MemoryCachedArray(data.array)\n880 \n881             if deep:\n882                 if hasattr(data, \"__array_function__\") or isinstance(\n883                     data, dask_array_type\n884                 ):\n885                     data = data.copy()\n886                 elif not isinstance(data, PandasIndexAdapter):\n887                     # pandas.Index is immutable\n888                     data = np.array(data)\n889         else:\n890             data = as_compatible_data(data)\n891             if self.shape != data.shape:\n892                 raise ValueError(\n893                     \"Data shape {} must match shape of object {}\".format(\n894                         data.shape, self.shape\n895                     )\n896                 )\n897 \n898         # note:\n899         # dims is already an immutable tuple\n900         # attributes and encoding will be copied when the new Array is created\n901         return self._replace(data=data)\n902 \n903     def _replace(\n904         self, dims=_default, data=_default, attrs=_default, encoding=_default\n905     ) -> \"Variable\":\n906         if dims is _default:\n907             dims = copy.copy(self._dims)\n908         if data is _default:\n909             data = copy.copy(self.data)\n910         if attrs is _default:\n911             attrs = copy.copy(self._attrs)\n912         if encoding is _default:\n913             encoding = copy.copy(self._encoding)\n914         return type(self)(dims, data, attrs, encoding, fastpath=True)\n915 \n916     def __copy__(self):\n917         return self.copy(deep=False)\n918 \n919     def __deepcopy__(self, memo=None):\n920         # memo does nothing but is required for compatibility with\n921         # copy.deepcopy\n922         return self.copy(deep=True)\n923 \n924     # mutable objects should not be hashable\n925     # https://github.com/python/mypy/issues/4266\n926     __hash__ = None  # type: ignore\n927 \n928     @property\n929     def chunks(self):\n930         \"\"\"Block dimensions for this array's data or None if it's not a dask\n931         array.\n932         \"\"\"\n933         return getattr(self._data, \"chunks\", None)\n934 \n935     _array_counter = itertools.count()\n936 \n937     def chunk(self, chunks=None, name=None, lock=False):\n938         \"\"\"Coerce this array's data into a dask arrays with the given chunks.\n939 \n940         If this variable is a non-dask array, it will be converted to dask\n941         array. If it's a dask array, it will be rechunked to the given chunk\n942         sizes.\n943 \n944         If neither chunks is not provided for one or more dimensions, chunk\n945         sizes along that dimension will not be updated; non-dask arrays will be\n946         converted into dask arrays with a single block.\n947 \n948         Parameters\n949         ----------\n950         chunks : int, tuple or dict, optional\n951             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or\n952             ``{'x': 5, 'y': 5}``.\n953         name : str, optional\n954             Used to generate the name for this array in the internal dask\n955             graph. Does not need not be unique.\n956         lock : optional\n957             Passed on to :py:func:`dask.array.from_array`, if the array is not\n958             already as dask array.\n959 \n960         Returns\n961         -------\n962         chunked : xarray.Variable\n963         \"\"\"\n964         import dask\n965         import dask.array as da\n966 \n967         if utils.is_dict_like(chunks):\n968             chunks = {self.get_axis_num(dim): chunk for dim, chunk in chunks.items()}\n969 \n970         if chunks is None:\n971             chunks = self.chunks or self.shape\n972 \n973         data = self._data\n974         if isinstance(data, da.Array):\n975             data = data.rechunk(chunks)\n976         else:\n977             if isinstance(data, indexing.ExplicitlyIndexed):\n978                 # Unambiguously handle array storage backends (like NetCDF4 and h5py)\n979                 # that can't handle general array indexing. For example, in netCDF4 you\n980                 # can do \"outer\" indexing along two dimensions independent, which works\n981                 # differently from how NumPy handles it.\n982                 # da.from_array works by using lazy indexing with a tuple of slices.\n983                 # Using OuterIndexer is a pragmatic choice: dask does not yet handle\n984                 # different indexing types in an explicit way:\n985                 # https://github.com/dask/dask/issues/2883\n986                 data = indexing.ImplicitToExplicitIndexingAdapter(\n987                     data, indexing.OuterIndexer\n988                 )\n989                 if LooseVersion(dask.__version__) < \"2.0.0\":\n990                     kwargs = {}\n991                 else:\n992                     # All of our lazily loaded backend array classes should use NumPy\n993                     # array operations.\n994                     kwargs = {\"meta\": np.ndarray}\n995             else:\n996                 kwargs = {}\n997 \n998             if utils.is_dict_like(chunks):\n999                 chunks = tuple(chunks.get(n, s) for n, s in enumerate(self.shape))\n1000 \n1001             data = da.from_array(data, chunks, name=name, lock=lock, **kwargs)\n1002 \n1003         return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)\n1004 \n1005     def _as_sparse(self, sparse_format=_default, fill_value=dtypes.NA):\n1006         \"\"\"\n1007         use sparse-array as backend.\n1008         \"\"\"\n1009         import sparse\n1010 \n1011         # TODO  what to do if dask-backended?\n1012         if fill_value is dtypes.NA:\n1013             dtype, fill_value = dtypes.maybe_promote(self.dtype)\n1014         else:\n1015             dtype = dtypes.result_type(self.dtype, fill_value)\n1016 \n1017         if sparse_format is _default:\n1018             sparse_format = \"coo\"\n1019         try:\n1020             as_sparse = getattr(sparse, \"as_{}\".format(sparse_format.lower()))\n1021         except AttributeError:\n1022             raise ValueError(\"{} is not a valid sparse format\".format(sparse_format))\n1023 \n1024         data = as_sparse(self.data.astype(dtype), fill_value=fill_value)\n1025         return self._replace(data=data)\n1026 \n1027     def _to_dense(self):\n1028         \"\"\"\n1029         Change backend from sparse to np.array\n1030         \"\"\"\n1031         if hasattr(self._data, \"todense\"):\n1032             return self._replace(data=self._data.todense())\n1033         return self.copy(deep=False)\n1034 \n1035     def isel(\n1036         self: VariableType,\n1037         indexers: Mapping[Hashable, Any] = None,\n1038         **indexers_kwargs: Any,\n1039     ) -> VariableType:\n1040         \"\"\"Return a new array indexed along the specified dimension(s).\n1041 \n1042         Parameters\n1043         ----------\n1044         **indexers : {dim: indexer, ...}\n1045             Keyword arguments with names matching dimensions and values given\n1046             by integers, slice objects or arrays.\n1047 \n1048         Returns\n1049         -------\n1050         obj : Array object\n1051             A new Array with the selected data and dimensions. In general,\n1052             the new variable's data will be a view of this variable's data,\n1053             unless numpy fancy indexing was triggered by using an array\n1054             indexer, in which case the data will be a copy.\n1055         \"\"\"\n1056         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"isel\")\n1057 \n1058         invalid = indexers.keys() - set(self.dims)\n1059         if invalid:\n1060             raise ValueError(\n1061                 f\"dimensions {invalid} do not exist. Expected one or more of {self.dims}\"\n1062             )\n1063 \n1064         key = tuple(indexers.get(dim, slice(None)) for dim in self.dims)\n1065         return self[key]\n1066 \n1067     def squeeze(self, dim=None):\n1068         \"\"\"Return a new object with squeezed data.\n1069 \n1070         Parameters\n1071         ----------\n1072         dim : None or str or tuple of str, optional\n1073             Selects a subset of the length one dimensions. If a dimension is\n1074             selected with length greater than one, an error is raised. If\n1075             None, all length one dimensions are squeezed.\n1076 \n1077         Returns\n1078         -------\n1079         squeezed : same type as caller\n1080             This object, but with with all or a subset of the dimensions of\n1081             length 1 removed.\n1082 \n1083         See Also\n1084         --------\n1085         numpy.squeeze\n1086         \"\"\"\n1087         dims = common.get_squeeze_dims(self, dim)\n1088         return self.isel({d: 0 for d in dims})\n1089 \n1090     def _shift_one_dim(self, dim, count, fill_value=dtypes.NA):\n1091         axis = self.get_axis_num(dim)\n1092 \n1093         if count > 0:\n1094             keep = slice(None, -count)\n1095         elif count < 0:\n1096             keep = slice(-count, None)\n1097         else:\n1098             keep = slice(None)\n1099 \n1100         trimmed_data = self[(slice(None),) * axis + (keep,)].data\n1101 \n1102         if fill_value is dtypes.NA:\n1103             dtype, fill_value = dtypes.maybe_promote(self.dtype)\n1104         else:\n1105             dtype = self.dtype\n1106 \n1107         width = min(abs(count), self.shape[axis])\n1108         dim_pad = (width, 0) if count >= 0 else (0, width)\n1109         pads = [(0, 0) if d != dim else dim_pad for d in self.dims]\n1110 \n1111         data = duck_array_ops.pad(\n1112             trimmed_data.astype(dtype),\n1113             pads,\n1114             mode=\"constant\",\n1115             constant_values=fill_value,\n1116         )\n1117 \n1118         if isinstance(data, dask_array_type):\n1119             # chunked data should come out with the same chunks; this makes\n1120             # it feasible to combine shifted and unshifted data\n1121             # TODO: remove this once dask.array automatically aligns chunks\n1122             data = data.rechunk(self.data.chunks)\n1123 \n1124         return type(self)(self.dims, data, self._attrs, fastpath=True)\n1125 \n1126     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):\n1127         \"\"\"\n1128         Return a new Variable with shifted data.\n1129 \n1130         Parameters\n1131         ----------\n1132         shifts : mapping of the form {dim: offset}\n1133             Integer offset to shift along each of the given dimensions.\n1134             Positive offsets shift to the right; negative offsets shift to the\n1135             left.\n1136         fill_value: scalar, optional\n1137             Value to use for newly missing values\n1138         **shifts_kwargs:\n1139             The keyword arguments form of ``shifts``.\n1140             One of shifts or shifts_kwargs must be provided.\n1141 \n1142         Returns\n1143         -------\n1144         shifted : Variable\n1145             Variable with the same dimensions and attributes but shifted data.\n1146         \"\"\"\n1147         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"shift\")\n1148         result = self\n1149         for dim, count in shifts.items():\n1150             result = result._shift_one_dim(dim, count, fill_value=fill_value)\n1151         return result\n1152 \n1153     def pad_with_fill_value(\n1154         self, pad_widths=None, fill_value=dtypes.NA, **pad_widths_kwargs\n1155     ):\n1156         \"\"\"\n1157         Return a new Variable with paddings.\n1158 \n1159         Parameters\n1160         ----------\n1161         pad_width: Mapping of the form {dim: (before, after)}\n1162             Number of values padded to the edges of each dimension.\n1163         **pad_widths_kwargs:\n1164             Keyword argument for pad_widths\n1165         \"\"\"\n1166         pad_widths = either_dict_or_kwargs(pad_widths, pad_widths_kwargs, \"pad\")\n1167 \n1168         if fill_value is dtypes.NA:\n1169             dtype, fill_value = dtypes.maybe_promote(self.dtype)\n1170         else:\n1171             dtype = self.dtype\n1172 \n1173         if isinstance(self.data, dask_array_type):\n1174             array = self.data\n1175 \n1176             # Dask does not yet support pad. We manually implement it.\n1177             # https://github.com/dask/dask/issues/1926\n1178             for d, pad in pad_widths.items():\n1179                 axis = self.get_axis_num(d)\n1180                 before_shape = list(array.shape)\n1181                 before_shape[axis] = pad[0]\n1182                 before_chunks = list(array.chunks)\n1183                 before_chunks[axis] = (pad[0],)\n1184                 after_shape = list(array.shape)\n1185                 after_shape[axis] = pad[1]\n1186                 after_chunks = list(array.chunks)\n1187                 after_chunks[axis] = (pad[1],)\n1188 \n1189                 arrays = []\n1190                 if pad[0] > 0:\n1191                     arrays.append(\n1192                         da.full(\n1193                             before_shape, fill_value, dtype=dtype, chunks=before_chunks\n1194                         )\n1195                     )\n1196                 arrays.append(array)\n1197                 if pad[1] > 0:\n1198                     arrays.append(\n1199                         da.full(\n1200                             after_shape, fill_value, dtype=dtype, chunks=after_chunks\n1201                         )\n1202                     )\n1203                 if len(arrays) > 1:\n1204                     array = da.concatenate(arrays, axis=axis)\n1205         else:\n1206             pads = [(0, 0) if d not in pad_widths else pad_widths[d] for d in self.dims]\n1207             array = np.pad(\n1208                 self.data.astype(dtype, copy=False),\n1209                 pads,\n1210                 mode=\"constant\",\n1211                 constant_values=fill_value,\n1212             )\n1213         return type(self)(self.dims, array)\n1214 \n1215     def _roll_one_dim(self, dim, count):\n1216         axis = self.get_axis_num(dim)\n1217 \n1218         count %= self.shape[axis]\n1219         if count != 0:\n1220             indices = [slice(-count, None), slice(None, -count)]\n1221         else:\n1222             indices = [slice(None)]\n1223 \n1224         arrays = [self[(slice(None),) * axis + (idx,)].data for idx in indices]\n1225 \n1226         data = duck_array_ops.concatenate(arrays, axis)\n1227 \n1228         if isinstance(data, dask_array_type):\n1229             # chunked data should come out with the same chunks; this makes\n1230             # it feasible to combine shifted and unshifted data\n1231             # TODO: remove this once dask.array automatically aligns chunks\n1232             data = data.rechunk(self.data.chunks)\n1233 \n1234         return type(self)(self.dims, data, self._attrs, fastpath=True)\n1235 \n1236     def roll(self, shifts=None, **shifts_kwargs):\n1237         \"\"\"\n1238         Return a new Variable with rolld data.\n1239 \n1240         Parameters\n1241         ----------\n1242         shifts : mapping of the form {dim: offset}\n1243             Integer offset to roll along each of the given dimensions.\n1244             Positive offsets roll to the right; negative offsets roll to the\n1245             left.\n1246         **shifts_kwargs:\n1247             The keyword arguments form of ``shifts``.\n1248             One of shifts or shifts_kwargs must be provided.\n1249 \n1250         Returns\n1251         -------\n1252         shifted : Variable\n1253             Variable with the same dimensions and attributes but rolled data.\n1254         \"\"\"\n1255         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"roll\")\n1256 \n1257         result = self\n1258         for dim, count in shifts.items():\n1259             result = result._roll_one_dim(dim, count)\n1260         return result\n1261 \n1262     def transpose(self, *dims) -> \"Variable\":\n1263         \"\"\"Return a new Variable object with transposed dimensions.\n1264 \n1265         Parameters\n1266         ----------\n1267         *dims : str, optional\n1268             By default, reverse the dimensions. Otherwise, reorder the\n1269             dimensions to this order.\n1270 \n1271         Returns\n1272         -------\n1273         transposed : Variable\n1274             The returned object has transposed data and dimensions with the\n1275             same attributes as the original.\n1276 \n1277         Notes\n1278         -----\n1279         This operation returns a view of this variable's data. It is\n1280         lazy for dask-backed Variables but not for numpy-backed Variables.\n1281 \n1282         See Also\n1283         --------\n1284         numpy.transpose\n1285         \"\"\"\n1286         if len(dims) == 0:\n1287             dims = self.dims[::-1]\n1288         dims = tuple(infix_dims(dims, self.dims))\n1289         axes = self.get_axis_num(dims)\n1290         if len(dims) < 2 or dims == self.dims:\n1291             # no need to transpose if only one dimension\n1292             # or dims are in same order\n1293             return self.copy(deep=False)\n1294 \n1295         data = as_indexable(self._data).transpose(axes)\n1296         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)\n1297 \n1298     @property\n1299     def T(self) -> \"Variable\":\n1300         return self.transpose()\n1301 \n1302     def set_dims(self, dims, shape=None):\n1303         \"\"\"Return a new variable with given set of dimensions.\n1304         This method might be used to attach new dimension(s) to variable.\n1305 \n1306         When possible, this operation does not copy this variable's data.\n1307 \n1308         Parameters\n1309         ----------\n1310         dims : str or sequence of str or dict\n1311             Dimensions to include on the new variable. If a dict, values are\n1312             used to provide the sizes of new dimensions; otherwise, new\n1313             dimensions are inserted with length 1.\n1314 \n1315         Returns\n1316         -------\n1317         Variable\n1318         \"\"\"\n1319         if isinstance(dims, str):\n1320             dims = [dims]\n1321 \n1322         if shape is None and utils.is_dict_like(dims):\n1323             shape = dims.values()\n1324 \n1325         missing_dims = set(self.dims) - set(dims)\n1326         if missing_dims:\n1327             raise ValueError(\n1328                 \"new dimensions %r must be a superset of \"\n1329                 \"existing dimensions %r\" % (dims, self.dims)\n1330             )\n1331 \n1332         self_dims = set(self.dims)\n1333         expanded_dims = tuple(d for d in dims if d not in self_dims) + self.dims\n1334 \n1335         if self.dims == expanded_dims:\n1336             # don't use broadcast_to unless necessary so the result remains\n1337             # writeable if possible\n1338             expanded_data = self.data\n1339         elif shape is not None:\n1340             dims_map = dict(zip(dims, shape))\n1341             tmp_shape = tuple(dims_map[d] for d in expanded_dims)\n1342             expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)\n1343         else:\n1344             expanded_data = self.data[(None,) * (len(expanded_dims) - self.ndim)]\n1345 \n1346         expanded_var = Variable(\n1347             expanded_dims, expanded_data, self._attrs, self._encoding, fastpath=True\n1348         )\n1349         return expanded_var.transpose(*dims)\n1350 \n1351     def _stack_once(self, dims, new_dim):\n1352         if not set(dims) <= set(self.dims):\n1353             raise ValueError(\"invalid existing dimensions: %s\" % dims)\n1354 \n1355         if new_dim in self.dims:\n1356             raise ValueError(\n1357                 \"cannot create a new dimension with the same \"\n1358                 \"name as an existing dimension\"\n1359             )\n1360 \n1361         if len(dims) == 0:\n1362             # don't stack\n1363             return self.copy(deep=False)\n1364 \n1365         other_dims = [d for d in self.dims if d not in dims]\n1366         dim_order = other_dims + list(dims)\n1367         reordered = self.transpose(*dim_order)\n1368 \n1369         new_shape = reordered.shape[: len(other_dims)] + (-1,)\n1370         new_data = reordered.data.reshape(new_shape)\n1371         new_dims = reordered.dims[: len(other_dims)] + (new_dim,)\n1372 \n1373         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)\n1374 \n1375     def stack(self, dimensions=None, **dimensions_kwargs):\n1376         \"\"\"\n1377         Stack any number of existing dimensions into a single new dimension.\n1378 \n1379         New dimensions will be added at the end, and the order of the data\n1380         along each new dimension will be in contiguous (C) order.\n1381 \n1382         Parameters\n1383         ----------\n1384         dimensions : Mapping of form new_name=(dim1, dim2, ...)\n1385             Names of new dimensions, and the existing dimensions that they\n1386             replace.\n1387         **dimensions_kwargs:\n1388             The keyword arguments form of ``dimensions``.\n1389             One of dimensions or dimensions_kwargs must be provided.\n1390 \n1391         Returns\n1392         -------\n1393         stacked : Variable\n1394             Variable with the same attributes but stacked data.\n1395 \n1396         See also\n1397         --------\n1398         Variable.unstack\n1399         \"\"\"\n1400         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, \"stack\")\n1401         result = self\n1402         for new_dim, dims in dimensions.items():\n1403             result = result._stack_once(dims, new_dim)\n1404         return result\n1405 \n1406     def _unstack_once(self, dims, old_dim):\n1407         new_dim_names = tuple(dims.keys())\n1408         new_dim_sizes = tuple(dims.values())\n1409 \n1410         if old_dim not in self.dims:\n1411             raise ValueError(\"invalid existing dimension: %s\" % old_dim)\n1412 \n1413         if set(new_dim_names).intersection(self.dims):\n1414             raise ValueError(\n1415                 \"cannot create a new dimension with the same \"\n1416                 \"name as an existing dimension\"\n1417             )\n1418 \n1419         if np.prod(new_dim_sizes) != self.sizes[old_dim]:\n1420             raise ValueError(\n1421                 \"the product of the new dimension sizes must \"\n1422                 \"equal the size of the old dimension\"\n1423             )\n1424 \n1425         other_dims = [d for d in self.dims if d != old_dim]\n1426         dim_order = other_dims + [old_dim]\n1427         reordered = self.transpose(*dim_order)\n1428 \n1429         new_shape = reordered.shape[: len(other_dims)] + new_dim_sizes\n1430         new_data = reordered.data.reshape(new_shape)\n1431         new_dims = reordered.dims[: len(other_dims)] + new_dim_names\n1432 \n1433         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)\n1434 \n1435     def unstack(self, dimensions=None, **dimensions_kwargs):\n1436         \"\"\"\n1437         Unstack an existing dimension into multiple new dimensions.\n1438 \n1439         New dimensions will be added at the end, and the order of the data\n1440         along each new dimension will be in contiguous (C) order.\n1441 \n1442         Parameters\n1443         ----------\n1444         dimensions : mapping of the form old_dim={dim1: size1, ...}\n1445             Names of existing dimensions, and the new dimensions and sizes\n1446             that they map to.\n1447         **dimensions_kwargs:\n1448             The keyword arguments form of ``dimensions``.\n1449             One of dimensions or dimensions_kwargs must be provided.\n1450 \n1451         Returns\n1452         -------\n1453         unstacked : Variable\n1454             Variable with the same attributes but unstacked data.\n1455 \n1456         See also\n1457         --------\n1458         Variable.stack\n1459         \"\"\"\n1460         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, \"unstack\")\n1461         result = self\n1462         for old_dim, dims in dimensions.items():\n1463             result = result._unstack_once(dims, old_dim)\n1464         return result\n1465 \n1466     def fillna(self, value):\n1467         return ops.fillna(self, value)\n1468 \n1469     def where(self, cond, other=dtypes.NA):\n1470         return ops.where_method(self, cond, other)\n1471 \n1472     def reduce(\n1473         self,\n1474         func,\n1475         dim=None,\n1476         axis=None,\n1477         keep_attrs=None,\n1478         keepdims=False,\n1479         allow_lazy=None,\n1480         **kwargs,\n1481     ):\n1482         \"\"\"Reduce this array by applying `func` along some dimension(s).\n1483 \n1484         Parameters\n1485         ----------\n1486         func : function\n1487             Function which can be called in the form\n1488             `func(x, axis=axis, **kwargs)` to return the result of reducing an\n1489             np.ndarray over an integer valued axis.\n1490         dim : str or sequence of str, optional\n1491             Dimension(s) over which to apply `func`.\n1492         axis : int or sequence of int, optional\n1493             Axis(es) over which to apply `func`. Only one of the 'dim'\n1494             and 'axis' arguments can be supplied. If neither are supplied, then\n1495             the reduction is calculated over the flattened array (by calling\n1496             `func(x)` without an axis argument).\n1497         keep_attrs : bool, optional\n1498             If True, the variable's attributes (`attrs`) will be copied from\n1499             the original object to the new one.  If False (default), the new\n1500             object will be returned without attributes.\n1501         keepdims : bool, default False\n1502             If True, the dimensions which are reduced are left in the result\n1503             as dimensions of size one\n1504         **kwargs : dict\n1505             Additional keyword arguments passed on to `func`.\n1506 \n1507         Returns\n1508         -------\n1509         reduced : Array\n1510             Array with summarized data and the indicated dimension(s)\n1511             removed.\n1512         \"\"\"\n1513         if dim == ...:\n1514             dim = None\n1515         if dim is not None and axis is not None:\n1516             raise ValueError(\"cannot supply both 'axis' and 'dim' arguments\")\n1517 \n1518         if dim is not None:\n1519             axis = self.get_axis_num(dim)\n1520 \n1521         if allow_lazy is not None:\n1522             warnings.warn(\n1523                 \"allow_lazy is deprecated and will be removed in version 0.16.0. It is now True by default.\",\n1524                 DeprecationWarning,\n1525             )\n1526         else:\n1527             allow_lazy = True\n1528 \n1529         input_data = self.data if allow_lazy else self.values\n1530 \n1531         if axis is not None:\n1532             data = func(input_data, axis=axis, **kwargs)\n1533         else:\n1534             data = func(input_data, **kwargs)\n1535 \n1536         if getattr(data, \"shape\", ()) == self.shape:\n1537             dims = self.dims\n1538         else:\n1539             removed_axes = (\n1540                 range(self.ndim) if axis is None else np.atleast_1d(axis) % self.ndim\n1541             )\n1542             if keepdims:\n1543                 # Insert np.newaxis for removed dims\n1544                 slices = tuple(\n1545                     np.newaxis if i in removed_axes else slice(None, None)\n1546                     for i in range(self.ndim)\n1547                 )\n1548                 if getattr(data, \"shape\", None) is None:\n1549                     # Reduce has produced a scalar value, not an array-like\n1550                     data = np.asanyarray(data)[slices]\n1551                 else:\n1552                     data = data[slices]\n1553                 dims = self.dims\n1554             else:\n1555                 dims = [\n1556                     adim for n, adim in enumerate(self.dims) if n not in removed_axes\n1557                 ]\n1558 \n1559         if keep_attrs is None:\n1560             keep_attrs = _get_keep_attrs(default=False)\n1561         attrs = self._attrs if keep_attrs else None\n1562 \n1563         return Variable(dims, data, attrs=attrs)\n1564 \n1565     @classmethod\n1566     def concat(cls, variables, dim=\"concat_dim\", positions=None, shortcut=False):\n1567         \"\"\"Concatenate variables along a new or existing dimension.\n1568 \n1569         Parameters\n1570         ----------\n1571         variables : iterable of Array\n1572             Arrays to stack together. Each variable is expected to have\n1573             matching dimensions and shape except for along the stacked\n1574             dimension.\n1575         dim : str or DataArray, optional\n1576             Name of the dimension to stack along. This can either be a new\n1577             dimension name, in which case it is added along axis=0, or an\n1578             existing dimension name, in which case the location of the\n1579             dimension is unchanged. Where to insert the new dimension is\n1580             determined by the first variable.\n1581         positions : None or list of integer arrays, optional\n1582             List of integer arrays which specifies the integer positions to\n1583             which to assign each dataset along the concatenated dimension.\n1584             If not supplied, objects are concatenated in the provided order.\n1585         shortcut : bool, optional\n1586             This option is used internally to speed-up groupby operations.\n1587             If `shortcut` is True, some checks of internal consistency between\n1588             arrays to concatenate are skipped.\n1589 \n1590         Returns\n1591         -------\n1592         stacked : Variable\n1593             Concatenated Variable formed by stacking all the supplied variables\n1594             along the given dimension.\n1595         \"\"\"\n1596         if not isinstance(dim, str):\n1597             (dim,) = dim.dims\n1598 \n1599         # can't do this lazily: we need to loop through variables at least\n1600         # twice\n1601         variables = list(variables)\n1602         first_var = variables[0]\n1603 \n1604         arrays = [v.data for v in variables]\n1605 \n1606         if dim in first_var.dims:\n1607             axis = first_var.get_axis_num(dim)\n1608             dims = first_var.dims\n1609             data = duck_array_ops.concatenate(arrays, axis=axis)\n1610             if positions is not None:\n1611                 # TODO: deprecate this option -- we don't need it for groupby\n1612                 # any more.\n1613                 indices = nputils.inverse_permutation(np.concatenate(positions))\n1614                 data = duck_array_ops.take(data, indices, axis=axis)\n1615         else:\n1616             axis = 0\n1617             dims = (dim,) + first_var.dims\n1618             data = duck_array_ops.stack(arrays, axis=axis)\n1619 \n1620         attrs = dict(first_var.attrs)\n1621         encoding = dict(first_var.encoding)\n1622         if not shortcut:\n1623             for var in variables:\n1624                 if var.dims != first_var.dims:\n1625                     raise ValueError(\n1626                         f\"Variable has dimensions {list(var.dims)} but first Variable has dimensions {list(first_var.dims)}\"\n1627                     )\n1628 \n1629         return cls(dims, data, attrs, encoding)\n1630 \n1631     def equals(self, other, equiv=duck_array_ops.array_equiv):\n1632         \"\"\"True if two Variables have the same dimensions and values;\n1633         otherwise False.\n1634 \n1635         Variables can still be equal (like pandas objects) if they have NaN\n1636         values in the same locations.\n1637 \n1638         This method is necessary because `v1 == v2` for Variables\n1639         does element-wise comparisons (like numpy.ndarrays).\n1640         \"\"\"\n1641         other = getattr(other, \"variable\", other)\n1642         try:\n1643             return self.dims == other.dims and (\n1644                 self._data is other._data or equiv(self.data, other.data)\n1645             )\n1646         except (TypeError, AttributeError):\n1647             return False\n1648 \n1649     def broadcast_equals(self, other, equiv=duck_array_ops.array_equiv):\n1650         \"\"\"True if two Variables have the values after being broadcast against\n1651         each other; otherwise False.\n1652 \n1653         Variables can still be equal (like pandas objects) if they have NaN\n1654         values in the same locations.\n1655         \"\"\"\n1656         try:\n1657             self, other = broadcast_variables(self, other)\n1658         except (ValueError, AttributeError):\n1659             return False\n1660         return self.equals(other, equiv=equiv)\n1661 \n1662     def identical(self, other, equiv=duck_array_ops.array_equiv):\n1663         \"\"\"Like equals, but also checks attributes.\n1664         \"\"\"\n1665         try:\n1666             return utils.dict_equiv(self.attrs, other.attrs) and self.equals(\n1667                 other, equiv=equiv\n1668             )\n1669         except (TypeError, AttributeError):\n1670             return False\n1671 \n1672     def no_conflicts(self, other, equiv=duck_array_ops.array_notnull_equiv):\n1673         \"\"\"True if the intersection of two Variable's non-null data is\n1674         equal; otherwise false.\n1675 \n1676         Variables can thus still be equal if there are locations where either,\n1677         or both, contain NaN values.\n1678         \"\"\"\n1679         return self.broadcast_equals(other, equiv=equiv)\n1680 \n1681     def quantile(\n1682         self, q, dim=None, interpolation=\"linear\", keep_attrs=None, skipna=True\n1683     ):\n1684         \"\"\"Compute the qth quantile of the data along the specified dimension.\n1685 \n1686         Returns the qth quantiles(s) of the array elements.\n1687 \n1688         Parameters\n1689         ----------\n1690         q : float in range of [0,1] (or sequence of floats)\n1691             Quantile to compute, which must be between 0 and 1\n1692             inclusive.\n1693         dim : str or sequence of str, optional\n1694             Dimension(s) over which to apply quantile.\n1695         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n1696             This optional parameter specifies the interpolation method to\n1697             use when the desired quantile lies between two data points\n1698             ``i < j``:\n1699 \n1700                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is\n1701                   the fractional part of the index surrounded by ``i`` and\n1702                   ``j``.\n1703                 * lower: ``i``.\n1704                 * higher: ``j``.\n1705                 * nearest: ``i`` or ``j``, whichever is nearest.\n1706                 * midpoint: ``(i + j) / 2``.\n1707 \n1708         keep_attrs : bool, optional\n1709             If True, the variable's attributes (`attrs`) will be copied from\n1710             the original object to the new one.  If False (default), the new\n1711             object will be returned without attributes.\n1712 \n1713         Returns\n1714         -------\n1715         quantiles : Variable\n1716             If `q` is a single quantile, then the result\n1717             is a scalar. If multiple percentiles are given, first axis of\n1718             the result corresponds to the quantile and a quantile dimension\n1719             is added to the return array. The other dimensions are the\n1720             dimensions that remain after the reduction of the array.\n1721 \n1722         See Also\n1723         --------\n1724         numpy.nanquantile, pandas.Series.quantile, Dataset.quantile,\n1725         DataArray.quantile\n1726         \"\"\"\n1727 \n1728         from .computation import apply_ufunc\n1729 \n1730         _quantile_func = np.nanquantile if skipna else np.quantile\n1731 \n1732         if keep_attrs is None:\n1733             keep_attrs = _get_keep_attrs(default=False)\n1734 \n1735         scalar = utils.is_scalar(q)\n1736         q = np.atleast_1d(np.asarray(q, dtype=np.float64))\n1737 \n1738         if dim is None:\n1739             dim = self.dims\n1740 \n1741         if utils.is_scalar(dim):\n1742             dim = [dim]\n1743 \n1744         def _wrapper(npa, **kwargs):\n1745             # move quantile axis to end. required for apply_ufunc\n1746             return np.moveaxis(_quantile_func(npa, **kwargs), 0, -1)\n1747 \n1748         axis = np.arange(-1, -1 * len(dim) - 1, -1)\n1749         result = apply_ufunc(\n1750             _wrapper,\n1751             self,\n1752             input_core_dims=[dim],\n1753             exclude_dims=set(dim),\n1754             output_core_dims=[[\"quantile\"]],\n1755             output_dtypes=[np.float64],\n1756             output_sizes={\"quantile\": len(q)},\n1757             dask=\"parallelized\",\n1758             kwargs={\"q\": q, \"axis\": axis, \"interpolation\": interpolation},\n1759         )\n1760 \n1761         # for backward compatibility\n1762         result = result.transpose(\"quantile\", ...)\n1763         if scalar:\n1764             result = result.squeeze(\"quantile\")\n1765         if keep_attrs:\n1766             result.attrs = self._attrs\n1767         return result\n1768 \n1769     def rank(self, dim, pct=False):\n1770         \"\"\"Ranks the data.\n1771 \n1772         Equal values are assigned a rank that is the average of the ranks that\n1773         would have been otherwise assigned to all of the values within that\n1774         set.  Ranks begin at 1, not 0. If `pct`, computes percentage ranks.\n1775 \n1776         NaNs in the input array are returned as NaNs.\n1777 \n1778         The `bottleneck` library is required.\n1779 \n1780         Parameters\n1781         ----------\n1782         dim : str\n1783             Dimension over which to compute rank.\n1784         pct : bool, optional\n1785             If True, compute percentage ranks, otherwise compute integer ranks.\n1786 \n1787         Returns\n1788         -------\n1789         ranked : Variable\n1790 \n1791         See Also\n1792         --------\n1793         Dataset.rank, DataArray.rank\n1794         \"\"\"\n1795         import bottleneck as bn\n1796 \n1797         data = self.data\n1798 \n1799         if isinstance(data, dask_array_type):\n1800             raise TypeError(\n1801                 \"rank does not work for arrays stored as dask \"\n1802                 \"arrays. Load the data via .compute() or .load() \"\n1803                 \"prior to calling this method.\"\n1804             )\n1805         elif not isinstance(data, np.ndarray):\n1806             raise TypeError(\n1807                 \"rank is not implemented for {} objects.\".format(type(data))\n1808             )\n1809 \n1810         axis = self.get_axis_num(dim)\n1811         func = bn.nanrankdata if self.dtype.kind == \"f\" else bn.rankdata\n1812         ranked = func(data, axis=axis)\n1813         if pct:\n1814             count = np.sum(~np.isnan(data), axis=axis, keepdims=True)\n1815             ranked /= count\n1816         return Variable(self.dims, ranked)\n1817 \n1818     def rolling_window(\n1819         self, dim, window, window_dim, center=False, fill_value=dtypes.NA\n1820     ):\n1821         \"\"\"\n1822         Make a rolling_window along dim and add a new_dim to the last place.\n1823 \n1824         Parameters\n1825         ----------\n1826         dim: str\n1827             Dimension over which to compute rolling_window\n1828         window: int\n1829             Window size of the rolling\n1830         window_dim: str\n1831             New name of the window dimension.\n1832         center: boolean. default False.\n1833             If True, pad fill_value for both ends. Otherwise, pad in the head\n1834             of the axis.\n1835         fill_value:\n1836             value to be filled.\n1837 \n1838         Returns\n1839         -------\n1840         Variable that is a view of the original array with a added dimension of\n1841         size w.\n1842         The return dim: self.dims + (window_dim, )\n1843         The return shape: self.shape + (window, )\n1844 \n1845         Examples\n1846         --------\n1847         >>> v = Variable((\"a\", \"b\"), np.arange(8).reshape((2, 4)))\n1848         >>> v.rolling_window(x, \"b\", 3, \"window_dim\")\n1849         <xarray.Variable (a: 2, b: 4, window_dim: 3)>\n1850         array([[[nan, nan, 0], [nan, 0, 1], [0, 1, 2], [1, 2, 3]],\n1851                [[nan, nan, 4], [nan, 4, 5], [4, 5, 6], [5, 6, 7]]])\n1852 \n1853         >>> v.rolling_window(x, \"b\", 3, \"window_dim\", center=True)\n1854         <xarray.Variable (a: 2, b: 4, window_dim: 3)>\n1855         array([[[nan, 0, 1], [0, 1, 2], [1, 2, 3], [2, 3, nan]],\n1856                [[nan, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, nan]]])\n1857         \"\"\"\n1858         if fill_value is dtypes.NA:  # np.nan is passed\n1859             dtype, fill_value = dtypes.maybe_promote(self.dtype)\n1860             array = self.astype(dtype, copy=False).data\n1861         else:\n1862             dtype = self.dtype\n1863             array = self.data\n1864 \n1865         new_dims = self.dims + (window_dim,)\n1866         return Variable(\n1867             new_dims,\n1868             duck_array_ops.rolling_window(\n1869                 array,\n1870                 axis=self.get_axis_num(dim),\n1871                 window=window,\n1872                 center=center,\n1873                 fill_value=fill_value,\n1874             ),\n1875         )\n1876 \n1877     def coarsen(self, windows, func, boundary=\"exact\", side=\"left\", **kwargs):\n1878         \"\"\"\n1879         Apply reduction function.\n1880         \"\"\"\n1881         windows = {k: v for k, v in windows.items() if k in self.dims}\n1882         if not windows:\n1883             return self.copy()\n1884 \n1885         reshaped, axes = self._coarsen_reshape(windows, boundary, side)\n1886         if isinstance(func, str):\n1887             name = func\n1888             func = getattr(duck_array_ops, name, None)\n1889             if func is None:\n1890                 raise NameError(f\"{name} is not a valid method.\")\n1891         return self._replace(data=func(reshaped, axis=axes, **kwargs))\n1892 \n1893     def _coarsen_reshape(self, windows, boundary, side):\n1894         \"\"\"\n1895         Construct a reshaped-array for coarsen\n1896         \"\"\"\n1897         if not utils.is_dict_like(boundary):\n1898             boundary = {d: boundary for d in windows.keys()}\n1899 \n1900         if not utils.is_dict_like(side):\n1901             side = {d: side for d in windows.keys()}\n1902 \n1903         # remove unrelated dimensions\n1904         boundary = {k: v for k, v in boundary.items() if k in windows}\n1905         side = {k: v for k, v in side.items() if k in windows}\n1906 \n1907         for d, window in windows.items():\n1908             if window <= 0:\n1909                 raise ValueError(f\"window must be > 0. Given {window}\")\n1910 \n1911         variable = self\n1912         for d, window in windows.items():\n1913             # trim or pad the object\n1914             size = variable.shape[self._get_axis_num(d)]\n1915             n = int(size / window)\n1916             if boundary[d] == \"exact\":\n1917                 if n * window != size:\n1918                     raise ValueError(\n1919                         \"Could not coarsen a dimension of size {} with \"\n1920                         \"window {}\".format(size, window)\n1921                     )\n1922             elif boundary[d] == \"trim\":\n1923                 if side[d] == \"left\":\n1924                     variable = variable.isel({d: slice(0, window * n)})\n1925                 else:\n1926                     excess = size - window * n\n1927                     variable = variable.isel({d: slice(excess, None)})\n1928             elif boundary[d] == \"pad\":  # pad\n1929                 pad = window * n - size\n1930                 if pad < 0:\n1931                     pad += window\n1932                 if side[d] == \"left\":\n1933                     pad_widths = {d: (0, pad)}\n1934                 else:\n1935                     pad_widths = {d: (pad, 0)}\n1936                 variable = variable.pad_with_fill_value(pad_widths)\n1937             else:\n1938                 raise TypeError(\n1939                     \"{} is invalid for boundary. Valid option is 'exact', \"\n1940                     \"'trim' and 'pad'\".format(boundary[d])\n1941                 )\n1942 \n1943         shape = []\n1944         axes = []\n1945         axis_count = 0\n1946         for i, d in enumerate(variable.dims):\n1947             if d in windows:\n1948                 size = variable.shape[i]\n1949                 shape.append(int(size / windows[d]))\n1950                 shape.append(windows[d])\n1951                 axis_count += 1\n1952                 axes.append(i + axis_count)\n1953             else:\n1954                 shape.append(variable.shape[i])\n1955 \n1956         keep_attrs = _get_keep_attrs(default=False)\n1957         variable.attrs = variable._attrs if keep_attrs else {}\n1958 \n1959         return variable.data.reshape(shape), tuple(axes)\n1960 \n1961     @property\n1962     def real(self):\n1963         return type(self)(self.dims, self.data.real, self._attrs)\n1964 \n1965     @property\n1966     def imag(self):\n1967         return type(self)(self.dims, self.data.imag, self._attrs)\n1968 \n1969     def __array_wrap__(self, obj, context=None):\n1970         return Variable(self.dims, obj)\n1971 \n1972     @staticmethod\n1973     def _unary_op(f):\n1974         @functools.wraps(f)\n1975         def func(self, *args, **kwargs):\n1976             with np.errstate(all=\"ignore\"):\n1977                 return self.__array_wrap__(f(self.data, *args, **kwargs))\n1978 \n1979         return func\n1980 \n1981     @staticmethod\n1982     def _binary_op(f, reflexive=False, **ignored_kwargs):\n1983         @functools.wraps(f)\n1984         def func(self, other):\n1985             if isinstance(other, (xr.DataArray, xr.Dataset)):\n1986                 return NotImplemented\n1987             self_data, other_data, dims = _broadcast_compat_data(self, other)\n1988             keep_attrs = _get_keep_attrs(default=False)\n1989             attrs = self._attrs if keep_attrs else None\n1990             with np.errstate(all=\"ignore\"):\n1991                 new_data = (\n1992                     f(self_data, other_data)\n1993                     if not reflexive\n1994                     else f(other_data, self_data)\n1995                 )\n1996             result = Variable(dims, new_data, attrs=attrs)\n1997             return result\n1998 \n1999         return func\n2000 \n2001     @staticmethod\n2002     def _inplace_binary_op(f):\n2003         @functools.wraps(f)\n2004         def func(self, other):\n2005             if isinstance(other, xr.Dataset):\n2006                 raise TypeError(\"cannot add a Dataset to a Variable in-place\")\n2007             self_data, other_data, dims = _broadcast_compat_data(self, other)\n2008             if dims != self.dims:\n2009                 raise ValueError(\"dimensions cannot change for in-place \" \"operations\")\n2010             with np.errstate(all=\"ignore\"):\n2011                 self.values = f(self_data, other_data)\n2012             return self\n2013 \n2014         return func\n2015 \n2016     def _to_numeric(self, offset=None, datetime_unit=None, dtype=float):\n2017         \"\"\" A (private) method to convert datetime array to numeric dtype\n2018         See duck_array_ops.datetime_to_numeric\n2019         \"\"\"\n2020         numeric_array = duck_array_ops.datetime_to_numeric(\n2021             self.data, offset, datetime_unit, dtype\n2022         )\n2023         return type(self)(self.dims, numeric_array, self._attrs)\n2024 \n2025 \n2026 ops.inject_all_ops_and_reduce_methods(Variable)\n2027 \n2028 \n2029 class IndexVariable(Variable):\n2030     \"\"\"Wrapper for accommodating a pandas.Index in an xarray.Variable.\n2031 \n2032     IndexVariable preserve loaded values in the form of a pandas.Index instead\n2033     of a NumPy array. Hence, their values are immutable and must always be one-\n2034     dimensional.\n2035 \n2036     They also have a name property, which is the name of their sole dimension\n2037     unless another name is given.\n2038     \"\"\"\n2039 \n2040     __slots__ = ()\n2041 \n2042     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n2043         super().__init__(dims, data, attrs, encoding, fastpath)\n2044         if self.ndim != 1:\n2045             raise ValueError(\"%s objects must be 1-dimensional\" % type(self).__name__)\n2046 \n2047         # Unlike in Variable, always eagerly load values into memory\n2048         if not isinstance(self._data, PandasIndexAdapter):\n2049             self._data = PandasIndexAdapter(self._data)\n2050 \n2051     def __dask_tokenize__(self):\n2052         from dask.base import normalize_token\n2053 \n2054         # Don't waste time converting pd.Index to np.ndarray\n2055         return normalize_token((type(self), self._dims, self._data.array, self._attrs))\n2056 \n2057     def load(self):\n2058         # data is already loaded into memory for IndexVariable\n2059         return self\n2060 \n2061     # https://github.com/python/mypy/issues/1465\n2062     @Variable.data.setter  # type: ignore\n2063     def data(self, data):\n2064         Variable.data.fset(self, data)\n2065         if not isinstance(self._data, PandasIndexAdapter):\n2066             self._data = PandasIndexAdapter(self._data)\n2067 \n2068     def chunk(self, chunks=None, name=None, lock=False):\n2069         # Dummy - do not chunk. This method is invoked e.g. by Dataset.chunk()\n2070         return self.copy(deep=False)\n2071 \n2072     def _as_sparse(self, sparse_format=_default, fill_value=_default):\n2073         # Dummy\n2074         return self.copy(deep=False)\n2075 \n2076     def _to_dense(self):\n2077         # Dummy\n2078         return self.copy(deep=False)\n2079 \n2080     def _finalize_indexing_result(self, dims, data):\n2081         if getattr(data, \"ndim\", 0) != 1:\n2082             # returns Variable rather than IndexVariable if multi-dimensional\n2083             return Variable(dims, data, self._attrs, self._encoding)\n2084         else:\n2085             return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)\n2086 \n2087     def __setitem__(self, key, value):\n2088         raise TypeError(\"%s values cannot be modified\" % type(self).__name__)\n2089 \n2090     @classmethod\n2091     def concat(cls, variables, dim=\"concat_dim\", positions=None, shortcut=False):\n2092         \"\"\"Specialized version of Variable.concat for IndexVariable objects.\n2093 \n2094         This exists because we want to avoid converting Index objects to NumPy\n2095         arrays, if possible.\n2096         \"\"\"\n2097         if not isinstance(dim, str):\n2098             (dim,) = dim.dims\n2099 \n2100         variables = list(variables)\n2101         first_var = variables[0]\n2102 \n2103         if any(not isinstance(v, cls) for v in variables):\n2104             raise TypeError(\n2105                 \"IndexVariable.concat requires that all input \"\n2106                 \"variables be IndexVariable objects\"\n2107             )\n2108 \n2109         indexes = [v._data.array for v in variables]\n2110 \n2111         if not indexes:\n2112             data = []\n2113         else:\n2114             data = indexes[0].append(indexes[1:])\n2115 \n2116             if positions is not None:\n2117                 indices = nputils.inverse_permutation(np.concatenate(positions))\n2118                 data = data.take(indices)\n2119 \n2120         attrs = dict(first_var.attrs)\n2121         if not shortcut:\n2122             for var in variables:\n2123                 if var.dims != first_var.dims:\n2124                     raise ValueError(\"inconsistent dimensions\")\n2125                 utils.remove_incompatible_items(attrs, var.attrs)\n2126 \n2127         return cls(first_var.dims, data, attrs)\n2128 \n2129     def copy(self, deep=True, data=None):\n2130         \"\"\"Returns a copy of this object.\n2131 \n2132         `deep` is ignored since data is stored in the form of\n2133         pandas.Index, which is already immutable. Dimensions, attributes\n2134         and encodings are always copied.\n2135 \n2136         Use `data` to create a new object with the same structure as\n2137         original but entirely new data.\n2138 \n2139         Parameters\n2140         ----------\n2141         deep : bool, optional\n2142             Deep is ignored when data is given. Whether the data array is\n2143             loaded into memory and copied onto the new object. Default is True.\n2144         data : array_like, optional\n2145             Data to use in the new object. Must have same shape as original.\n2146 \n2147         Returns\n2148         -------\n2149         object : Variable\n2150             New object with dimensions, attributes, encodings, and optionally\n2151             data copied from original.\n2152         \"\"\"\n2153         if data is None:\n2154             data = self._data.copy(deep=deep)\n2155         else:\n2156             data = as_compatible_data(data)\n2157             if self.shape != data.shape:\n2158                 raise ValueError(\n2159                     \"Data shape {} must match shape of object {}\".format(\n2160                         data.shape, self.shape\n2161                     )\n2162                 )\n2163         return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)\n2164 \n2165     def equals(self, other, equiv=None):\n2166         # if equiv is specified, super up\n2167         if equiv is not None:\n2168             return super().equals(other, equiv)\n2169 \n2170         # otherwise use the native index equals, rather than looking at _data\n2171         other = getattr(other, \"variable\", other)\n2172         try:\n2173             return self.dims == other.dims and self._data_equals(other)\n2174         except (TypeError, AttributeError):\n2175             return False\n2176 \n2177     def _data_equals(self, other):\n2178         return self.to_index().equals(other.to_index())\n2179 \n2180     def to_index_variable(self):\n2181         \"\"\"Return this variable as an xarray.IndexVariable\"\"\"\n2182         return self\n2183 \n2184     to_coord = utils.alias(to_index_variable, \"to_coord\")\n2185 \n2186     def to_index(self):\n2187         \"\"\"Convert this variable to a pandas.Index\"\"\"\n2188         # n.b. creating a new pandas.Index from an old pandas.Index is\n2189         # basically free as pandas.Index objects are immutable\n2190         assert self.ndim == 1\n2191         index = self._data.array\n2192         if isinstance(index, pd.MultiIndex):\n2193             # set default names for multi-index unnamed levels so that\n2194             # we can safely rename dimension / coordinate later\n2195             valid_level_names = [\n2196                 name or \"{}_level_{}\".format(self.dims[0], i)\n2197                 for i, name in enumerate(index.names)\n2198             ]\n2199             index = index.set_names(valid_level_names)\n2200         else:\n2201             index = index.set_names(self.name)\n2202         return index\n2203 \n2204     @property\n2205     def level_names(self):\n2206         \"\"\"Return MultiIndex level names or None if this IndexVariable has no\n2207         MultiIndex.\n2208         \"\"\"\n2209         index = self.to_index()\n2210         if isinstance(index, pd.MultiIndex):\n2211             return index.names\n2212         else:\n2213             return None\n2214 \n2215     def get_level_variable(self, level):\n2216         \"\"\"Return a new IndexVariable from a given MultiIndex level.\"\"\"\n2217         if self.level_names is None:\n2218             raise ValueError(\"IndexVariable %r has no MultiIndex\" % self.name)\n2219         index = self.to_index()\n2220         return type(self)(self.dims, index.get_level_values(level))\n2221 \n2222     @property\n2223     def name(self):\n2224         return self.dims[0]\n2225 \n2226     @name.setter\n2227     def name(self, value):\n2228         raise AttributeError(\"cannot modify name of IndexVariable in-place\")\n2229 \n2230 \n2231 # for backwards compatibility\n2232 Coordinate = utils.alias(IndexVariable, \"Coordinate\")\n2233 \n2234 \n2235 def _unified_dims(variables):\n2236     # validate dimensions\n2237     all_dims = {}\n2238     for var in variables:\n2239         var_dims = var.dims\n2240         if len(set(var_dims)) < len(var_dims):\n2241             raise ValueError(\n2242                 \"broadcasting cannot handle duplicate \"\n2243                 \"dimensions: %r\" % list(var_dims)\n2244             )\n2245         for d, s in zip(var_dims, var.shape):\n2246             if d not in all_dims:\n2247                 all_dims[d] = s\n2248             elif all_dims[d] != s:\n2249                 raise ValueError(\n2250                     \"operands cannot be broadcast together \"\n2251                     \"with mismatched lengths for dimension %r: %s\"\n2252                     % (d, (all_dims[d], s))\n2253                 )\n2254     return all_dims\n2255 \n2256 \n2257 def _broadcast_compat_variables(*variables):\n2258     \"\"\"Create broadcast compatible variables, with the same dimensions.\n2259 \n2260     Unlike the result of broadcast_variables(), some variables may have\n2261     dimensions of size 1 instead of the the size of the broadcast dimension.\n2262     \"\"\"\n2263     dims = tuple(_unified_dims(variables))\n2264     return tuple(var.set_dims(dims) if var.dims != dims else var for var in variables)\n2265 \n2266 \n2267 def broadcast_variables(*variables):\n2268     \"\"\"Given any number of variables, return variables with matching dimensions\n2269     and broadcast data.\n2270 \n2271     The data on the returned variables will be a view of the data on the\n2272     corresponding original arrays, but dimensions will be reordered and\n2273     inserted so that both broadcast arrays have the same dimensions. The new\n2274     dimensions are sorted in order of appearance in the first variable's\n2275     dimensions followed by the second variable's dimensions.\n2276     \"\"\"\n2277     dims_map = _unified_dims(variables)\n2278     dims_tuple = tuple(dims_map)\n2279     return tuple(\n2280         var.set_dims(dims_map) if var.dims != dims_tuple else var for var in variables\n2281     )\n2282 \n2283 \n2284 def _broadcast_compat_data(self, other):\n2285     if all(hasattr(other, attr) for attr in [\"dims\", \"data\", \"shape\", \"encoding\"]):\n2286         # `other` satisfies the necessary Variable API for broadcast_variables\n2287         new_self, new_other = _broadcast_compat_variables(self, other)\n2288         self_data = new_self.data\n2289         other_data = new_other.data\n2290         dims = new_self.dims\n2291     else:\n2292         # rely on numpy broadcasting rules\n2293         self_data = self.data\n2294         other_data = other\n2295         dims = self.dims\n2296     return self_data, other_data, dims\n2297 \n2298 \n2299 def concat(variables, dim=\"concat_dim\", positions=None, shortcut=False):\n2300     \"\"\"Concatenate variables along a new or existing dimension.\n2301 \n2302     Parameters\n2303     ----------\n2304     variables : iterable of Array\n2305         Arrays to stack together. Each variable is expected to have\n2306         matching dimensions and shape except for along the stacked\n2307         dimension.\n2308     dim : str or DataArray, optional\n2309         Name of the dimension to stack along. This can either be a new\n2310         dimension name, in which case it is added along axis=0, or an\n2311         existing dimension name, in which case the location of the\n2312         dimension is unchanged. Where to insert the new dimension is\n2313         determined by the first variable.\n2314     positions : None or list of integer arrays, optional\n2315         List of integer arrays which specifies the integer positions to which\n2316         to assign each dataset along the concatenated dimension. If not\n2317         supplied, objects are concatenated in the provided order.\n2318     shortcut : bool, optional\n2319         This option is used internally to speed-up groupby operations.\n2320         If `shortcut` is True, some checks of internal consistency between\n2321         arrays to concatenate are skipped.\n2322 \n2323     Returns\n2324     -------\n2325     stacked : Variable\n2326         Concatenated Variable formed by stacking all the supplied variables\n2327         along the given dimension.\n2328     \"\"\"\n2329     variables = list(variables)\n2330     if all(isinstance(v, IndexVariable) for v in variables):\n2331         return IndexVariable.concat(variables, dim, positions, shortcut)\n2332     else:\n2333         return Variable.concat(variables, dim, positions, shortcut)\n2334 \n2335 \n2336 def assert_unique_multiindex_level_names(variables):\n2337     \"\"\"Check for uniqueness of MultiIndex level names in all given\n2338     variables.\n2339 \n2340     Not public API. Used for checking consistency of DataArray and Dataset\n2341     objects.\n2342     \"\"\"\n2343     level_names = defaultdict(list)\n2344     all_level_names = set()\n2345     for var_name, var in variables.items():\n2346         if isinstance(var._data, PandasIndexAdapter):\n2347             idx_level_names = var.to_index_variable().level_names\n2348             if idx_level_names is not None:\n2349                 for n in idx_level_names:\n2350                     level_names[n].append(f\"{n!r} ({var_name})\")\n2351             if idx_level_names:\n2352                 all_level_names.update(idx_level_names)\n2353 \n2354     for k, v in level_names.items():\n2355         if k in variables:\n2356             v.append(\"(%s)\" % k)\n2357 \n2358     duplicate_names = [v for v in level_names.values() if len(v) > 1]\n2359     if duplicate_names:\n2360         conflict_str = \"\\n\".join([\", \".join(v) for v in duplicate_names])\n2361         raise ValueError(\"conflicting MultiIndex level name(s):\\n%s\" % conflict_str)\n2362     # Check confliction between level names and dimensions GH:2299\n2363     for k, v in variables.items():\n2364         for d in v.dims:\n2365             if d in all_level_names:\n2366                 raise ValueError(\n2367                     \"conflicting level / dimension names. {} \"\n2368                     \"already exists as a level name.\".format(d)\n2369                 )\n",
        "intended_behavior": "Ensure support for the weighted average computation at the variable level, potentially introducing helper functions or methods necessary for performing the weighted average calculations that 'Dataset' and 'DataArray' methods can then utilize. This may involve processing the weights alongside the data and appropriately handling dimensions."
    }
]