2025-10-18 15:07:36.438 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 15:07:37.915 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=137, output_tokens=12, cost=0.001730
2025-10-18 15:07:37.916 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-18 15:07:37.920 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 15:07:40.893 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=446, output_tokens=120, cost=0.008060
2025-10-18 15:07:40.897 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 15:07:44.071 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=474, output_tokens=27, cost=0.005550
2025-10-18 15:07:44.072 | DEBUG    | Extracted a valid json.
2025-10-18 15:07:44.072 | DEBUG    | Agent proxy return the following json: {
    "API_calls": ["search_class(\"ds\")", "search_method(\"average\")"],
    "bug_locations": []
}
2025-10-18 15:07:44.076 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 15:07:44.077 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 15:07:48.937 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=713, output_tokens=216, cost=0.013610
2025-10-18 15:07:48.945 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 15:07:50.214 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1145, output_tokens=22, cost=0.012110
2025-10-18 15:07:50.217 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 15:07:52.260 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=376, output_tokens=41, cost=0.004990
2025-10-18 15:07:52.261 | DEBUG    | Extracted a valid json.
2025-10-18 15:07:52.261 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_code(\"geo data\")",
        "search_code(\"weights\")",
        "search_code(\"dimensions\")"
    ],
    "bug_locations": []
}
2025-10-18 15:07:52.463 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 15:07:52.463 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 15:08:06.303 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1959, output_tokens=641, cost=0.038820
2025-10-18 15:08:06.310 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 15:08:07.679 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2816, output_tokens=3, cost=0.028250
2025-10-18 15:08:07.681 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 15:08:08.656 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=357, output_tokens=14, cost=0.003990
2025-10-18 15:08:08.657 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 15:08:08.657 | DEBUG    | Trying to convert API calls/bug locations into json. Try 2 of 5.
2025-10-18 15:08:10.390 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=357, output_tokens=14, cost=0.003990
2025-10-18 15:08:10.391 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 15:08:10.392 | DEBUG    | Trying to convert API calls/bug locations into json. Try 3 of 5.
2025-10-18 15:08:11.868 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=357, output_tokens=14, cost=0.003990
2025-10-18 15:08:11.869 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 15:08:11.870 | DEBUG    | Trying to convert API calls/bug locations into json. Try 4 of 5.
2025-10-18 15:08:13.401 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=357, output_tokens=14, cost=0.003990
2025-10-18 15:08:13.402 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 15:08:13.402 | DEBUG    | Trying to convert API calls/bug locations into json. Try 5 of 5.
2025-10-18 15:08:15.136 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=357, output_tokens=14, cost=0.003990
2025-10-18 15:08:15.137 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 15:08:15.137 | DEBUG    | Agent proxy return the following json: None
2025-10-18 15:08:15.138 | DEBUG    | Could not extract API calls from agent search response, asking search agent to re-generate response.
2025-10-18 15:08:15.139 | DEBUG    | <Agent search> Downstream could not consume our last response. Will retry.
2025-10-18 15:08:15.139 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 15:08:25.097 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2846, output_tokens=395, cost=0.040310
2025-10-18 15:08:25.105 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 15:08:27.186 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=749, output_tokens=14, cost=0.007910
2025-10-18 15:08:27.187 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 15:08:27.187 | DEBUG    | Trying to convert API calls/bug locations into json. Try 2 of 5.
2025-10-18 15:08:28.651 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=749, output_tokens=14, cost=0.007910
2025-10-18 15:08:28.652 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 15:08:28.658 | DEBUG    | Trying to convert API calls/bug locations into json. Try 3 of 5.
2025-10-18 15:08:31.107 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=749, output_tokens=14, cost=0.007910
2025-10-18 15:08:31.108 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 15:08:31.108 | DEBUG    | Trying to convert API calls/bug locations into json. Try 4 of 5.
2025-10-18 15:08:32.521 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=749, output_tokens=14, cost=0.007910
2025-10-18 15:08:32.522 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 15:08:32.522 | DEBUG    | Trying to convert API calls/bug locations into json. Try 5 of 5.
2025-10-18 15:08:34.267 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=749, output_tokens=14, cost=0.007910
2025-10-18 15:08:34.268 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 15:08:34.269 | DEBUG    | Agent proxy return the following json: None
2025-10-18 15:08:34.269 | DEBUG    | Could not extract API calls from agent search response, asking search agent to re-generate response.
2025-10-18 15:08:34.270 | DEBUG    | <Agent search> Downstream could not consume our last response. Will retry.
2025-10-18 15:08:34.270 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 15:08:44.542 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3268, output_tokens=374, cost=0.043900
2025-10-18 15:08:44.549 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 15:08:46.060 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=728, output_tokens=14, cost=0.007700
2025-10-18 15:08:46.061 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 15:08:46.061 | DEBUG    | Trying to convert API calls/bug locations into json. Try 2 of 5.
2025-10-18 15:08:47.546 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=728, output_tokens=14, cost=0.007700
2025-10-18 15:08:47.547 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 15:08:47.547 | DEBUG    | Trying to convert API calls/bug locations into json. Try 3 of 5.
2025-10-18 15:08:49.588 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=728, output_tokens=14, cost=0.007700
2025-10-18 15:08:49.589 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 15:08:49.596 | DEBUG    | Trying to convert API calls/bug locations into json. Try 4 of 5.
2025-10-18 15:08:50.838 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=728, output_tokens=14, cost=0.007700
2025-10-18 15:08:50.839 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 15:08:50.839 | DEBUG    | Trying to convert API calls/bug locations into json. Try 5 of 5.
2025-10-18 15:08:52.661 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=728, output_tokens=14, cost=0.007700
2025-10-18 15:08:52.662 | DEBUG    | Both API_calls and bug_locations are empty. Will retry.
2025-10-18 15:08:52.662 | DEBUG    | Agent proxy return the following json: None
2025-10-18 15:08:52.662 | DEBUG    | Could not extract API calls from agent search response, asking search agent to re-generate response.
2025-10-18 15:08:52.663 | DEBUG    | <Agent search> Downstream could not consume our last response. Will retry.
2025-10-18 15:08:52.663 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 15:09:03.039 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3669, output_tokens=424, cost=0.049410
2025-10-18 15:09:03.048 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 15:09:05.898 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=778, output_tokens=70, cost=0.009880
2025-10-18 15:09:05.899 | DEBUG    | Extracted a valid json.
2025-10-18 15:09:05.899 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_code_in_file(\"class\", \"xarray/core/dataset.py\")",
        "search_code_in_file(\"class\", \"xarray/core/dataarray.py\")",
        "search_code_in_file(\"class\", \"xarray/core/variable.py\")"
    ],
    "bug_locations": []
}
2025-10-18 15:09:05.946 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 15:09:05.947 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 15:09:23.527 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=4632, output_tokens=784, cost=0.069840
2025-10-18 15:09:23.544 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 15:09:34.514 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=5632, output_tokens=468, cost=0.070360
2025-10-18 15:09:34.525 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 15:09:44.213 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=822, output_tokens=304, cost=0.017340
2025-10-18 15:09:44.214 | DEBUG    | Extracted a valid json.
2025-10-18 15:09:44.214 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "xarray/core/dataset.py",
            "class": "Dataset",
            "method": "",
            "intended_behavior": "Implement a new method named 'average' (or 'weighted_average' for clarity) that allows for computing weighted averages across specified dimensions of the dataset. This method should accept parameters for specifying the axes along which to compute the averages and an optional weight parameter for each dimension. It should handle missing data gracefully, likely reusing or extending patterns seen in 'interpolate_na'."
        },
        {
            "file": "xarray/core/dataarray.py",
            "class": "DataArray",
            "method": "",
            "intended_behavior": "Introduce an 'average' method that enables weighted average calculation for data arrays. This method would be very similar to the one in 'Dataset', tailored to the 'DataArray' structure, leveraging the existing data handling methods and ensuring it integrates seamlessly with the broader xarray functionality."
        },
        {
            "file": "xarray/core/variable.py",
            "class": "",
            "method": "",
            "intended_behavior": "Ensure support for the weighted average computation at the variable level, potentially introducing helper functions or methods necessary for performing the weighted average calculations that 'Dataset' and 'DataArray' methods can then utilize. This may involve processing the weights alongside the data and appropriately handling dimensions."
        }
    ]
}
2025-10-18 15:09:44.248 | DEBUG    | Bug location extracted successfully: [<file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>None</method>
<code>
422 class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
423     """A multi-dimensional, in memory, array database.
424 
425     A dataset resembles an in-memory representation of a NetCDF file, and
426     consists of variables, coordinates and attributes which together form a
427     self describing dataset.
428 
429     Dataset implements the mapping interface with keys given by variable names
430     and values given by DataArray objects for each variable name.
431 
432     One dimensional variables with name equal to their dimension are index
433     coordinates used for label based indexing.
434     """
435 
436     _attrs: Optional[Dict[Hashable, Any]]
437     _cache: Dict[str, Any]
438     _coord_names: Set[Hashable]
439     _dims: Dict[Hashable, int]
440     _encoding: Optional[Dict[Hashable, Any]]
441     _indexes: Optional[Dict[Hashable, pd.Index]]
442     _variables: Dict[Hashable, Variable]
443 
444     __slots__ = (
445         "_attrs",
446         "_cache",
447         "_coord_names",
448         "_dims",
449         "_encoding",
450         "_file_obj",
451         "_indexes",
452         "_variables",
453         "__weakref__",
454     )
455 
456     _groupby_cls = groupby.DatasetGroupBy
457     _rolling_cls = rolling.DatasetRolling
458     _coarsen_cls = rolling.DatasetCoarsen
459     _resample_cls = resample.DatasetResample
460 
461     def __init__(
462         self,
463         # could make a VariableArgs to use more generally, and refine these
464         # categories
465         data_vars: Mapping[Hashable, Any] = None,
466         coords: Mapping[Hashable, Any] = None,
467         attrs: Mapping[Hashable, Any] = None,
468     ):
469         """To load data from a file or file-like object, use the `open_dataset`
470         function.
471 
472         Parameters
473         ----------
474         data_vars : dict-like, optional
475             A mapping from variable names to :py:class:`~xarray.DataArray`
476             objects, :py:class:`~xarray.Variable` objects or to tuples of the
477             form ``(dims, data[, attrs])`` which can be used as arguments to
478             create a new ``Variable``. Each dimension must have the same length
479             in all variables in which it appears.
480 
481             The following notations are accepted:
482 
483             - mapping {var name: DataArray}
484             - mapping {var name: Variable}
485             - mapping {var name: (dimension name, array-like)}
486             - mapping {var name: (tuple of dimension names, array-like)}
487             - mapping {dimension name: array-like}
488               (it will be automatically moved to coords, see below)
489 
490             Each dimension must have the same length in all variables in which
491             it appears.
492         coords : dict-like, optional
493             Another mapping in similar form as the `data_vars` argument,
494             except the each item is saved on the dataset as a "coordinate".
495             These variables have an associated meaning: they describe
496             constant/fixed/independent quantities, unlike the
497             varying/measured/dependent quantities that belong in `variables`.
498             Coordinates values may be given by 1-dimensional arrays or scalars,
499             in which case `dims` do not need to be supplied: 1D arrays will be
500             assumed to give index values along the dimension with the same
501             name.
502 
503             The following notations are accepted:
504 
505             - mapping {coord name: DataArray}
506             - mapping {coord name: Variable}
507             - mapping {coord name: (dimension name, array-like)}
508             - mapping {coord name: (tuple of dimension names, array-like)}
509             - mapping {dimension name: array-like}
510               (the dimension name is implicitly set to be the same as the coord name)
511 
512             The last notation implies that the coord name is the same as the
513             dimension name.
514 
515         attrs : dict-like, optional
516             Global attributes to save on this dataset.
517         """
518 
519         # TODO(shoyer): expose indexes as a public argument in __init__
520 
521         if data_vars is None:
522             data_vars = {}
523         if coords is None:
524             coords = {}
525 
526         both_data_and_coords = set(data_vars) & set(coords)
527         if both_data_and_coords:
528             raise ValueError(
529                 "variables %r are found in both data_vars and coords"
530                 % both_data_and_coords
531             )
532 
533         if isinstance(coords, Dataset):
534             coords = coords.variables
535 
536         variables, coord_names, dims, indexes = merge_data_and_coords(
537             data_vars, coords, compat="broadcast_equals"
538         )
539 
540         self._attrs = dict(attrs) if attrs is not None else None
541         self._file_obj = None
542         self._encoding = None
543         self._variables = variables
544         self._coord_names = coord_names
545         self._dims = dims
546         self._indexes = indexes
547 
548     @classmethod
549     def load_store(cls, store, decoder=None) -> "Dataset":
550         """Create a new dataset from the contents of a backends.*DataStore
551         object
552         """
553         variables, attributes = store.load()
554         if decoder:
555             variables, attributes = decoder(variables, attributes)
556         obj = cls(variables, attrs=attributes)
557         obj._file_obj = store
558         return obj
559 
560     @property
561     def variables(self) -> Mapping[Hashable, Variable]:
562         """Low level interface to Dataset contents as dict of Variable objects.
563 
564         This ordered dictionary is frozen to prevent mutation that could
565         violate Dataset invariants. It contains all variable objects
566         constituting the Dataset, including both data variables and
567         coordinates.
568         """
569         return Frozen(self._variables)
570 
571     @property
572     def attrs(self) -> Dict[Hashable, Any]:
573         """Dictionary of global attributes on this dataset
574         """
575         if self._attrs is None:
576             self._attrs = {}
577         return self._attrs
578 
579     @attrs.setter
580     def attrs(self, value: Mapping[Hashable, Any]) -> None:
581         self._attrs = dict(value)
582 
583     @property
584     def encoding(self) -> Dict:
585         """Dictionary of global encoding attributes on this dataset
586         """
587         if self._encoding is None:
588             self._encoding = {}
589         return self._encoding
590 
591     @encoding.setter
592     def encoding(self, value: Mapping) -> None:
593         self._encoding = dict(value)
594 
595     @property
596     def dims(self) -> Mapping[Hashable, int]:
597         """Mapping from dimension names to lengths.
598 
599         Cannot be modified directly, but is updated when adding new variables.
600 
601         Note that type of this object differs from `DataArray.dims`.
602         See `Dataset.sizes` and `DataArray.sizes` for consistently named
603         properties.
604         """
605         return Frozen(SortedKeysDict(self._dims))
606 
607     @property
608     def sizes(self) -> Mapping[Hashable, int]:
609         """Mapping from dimension names to lengths.
610 
611         Cannot be modified directly, but is updated when adding new variables.
612 
613         This is an alias for `Dataset.dims` provided for the benefit of
614         consistency with `DataArray.sizes`.
615 
616         See also
617         --------
618         DataArray.sizes
619         """
620         return self.dims
621 
622     def load(self, **kwargs) -> "Dataset":
623         """Manually trigger loading and/or computation of this dataset's data
624         from disk or a remote source into memory and return this dataset.
625         Unlike compute, the original dataset is modified and returned.
626 
627         Normally, it should not be necessary to call this method in user code,
628         because all xarray functions should either work on deferred data or
629         load data automatically. However, this method can be necessary when
630         working with many file objects on disk.
631 
632         Parameters
633         ----------
634         **kwargs : dict
635             Additional keyword arguments passed on to ``dask.array.compute``.
636 
637         See Also
638         --------
639         dask.array.compute
640         """
641         # access .data to coerce everything to numpy or dask arrays
642         lazy_data = {
643             k: v._data
644             for k, v in self.variables.items()
645             if isinstance(v._data, dask_array_type)
646         }
647         if lazy_data:
648             import dask.array as da
649 
650             # evaluate all the dask arrays simultaneously
651             evaluated_data = da.compute(*lazy_data.values(), **kwargs)
652 
653             for k, data in zip(lazy_data, evaluated_data):
654                 self.variables[k].data = data
655 
656         # load everything else sequentially
657         for k, v in self.variables.items():
658             if k not in lazy_data:
659                 v.load()
660 
661         return self
662 
663     def __dask_tokenize__(self):
664         from dask.base import normalize_token
665 
666         return normalize_token(
667             (type(self), self._variables, self._coord_names, self._attrs)
668         )
669 
670     def __dask_graph__(self):
671         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}
672         graphs = {k: v for k, v in graphs.items() if v is not None}
673         if not graphs:
674             return None
675         else:
676             try:
677                 from dask.highlevelgraph import HighLevelGraph
678 
679                 return HighLevelGraph.merge(*graphs.values())
680             except ImportError:
681                 from dask import sharedict
682 
683                 return sharedict.merge(*graphs.values())
684 
685     def __dask_keys__(self):
686         import dask
687 
688         return [
689             v.__dask_keys__()
690             for v in self.variables.values()
691             if dask.is_dask_collection(v)
692         ]
693 
694     def __dask_layers__(self):
695         import dask
696 
697         return sum(
698             [
699                 v.__dask_layers__()
700                 for v in self.variables.values()
701                 if dask.is_dask_collection(v)
702             ],
703             (),
704         )
705 
706     @property
707     def __dask_optimize__(self):
708         import dask.array as da
709 
710         return da.Array.__dask_optimize__
711 
712     @property
713     def __dask_scheduler__(self):
714         import dask.array as da
715 
716         return da.Array.__dask_scheduler__
717 
718     def __dask_postcompute__(self):
719         import dask
720 
721         info = [
722             (True, k, v.__dask_postcompute__())
723             if dask.is_dask_collection(v)
724             else (False, k, v)
725             for k, v in self._variables.items()
726         ]
727         args = (
728             info,
729             self._coord_names,
730             self._dims,
731             self._attrs,
732             self._indexes,
733             self._encoding,
734             self._file_obj,
735         )
736         return self._dask_postcompute, args
737 
738     def __dask_postpersist__(self):
739         import dask
740 
741         info = [
742             (True, k, v.__dask_postpersist__())
743             if dask.is_dask_collection(v)
744             else (False, k, v)
745             for k, v in self._variables.items()
746         ]
747         args = (
748             info,
749             self._coord_names,
750             self._dims,
751             self._attrs,
752             self._indexes,
753             self._encoding,
754             self._file_obj,
755         )
756         return self._dask_postpersist, args
757 
758     @staticmethod
759     def _dask_postcompute(results, info, *args):
760         variables = {}
761         results2 = list(results[::-1])
762         for is_dask, k, v in info:
763             if is_dask:
764                 func, args2 = v
765                 r = results2.pop()
766                 result = func(r, *args2)
767             else:
768                 result = v
769             variables[k] = result
770 
771         final = Dataset._construct_direct(variables, *args)
772         return final
773 
774     @staticmethod
775     def _dask_postpersist(dsk, info, *args):
776         variables = {}
777         for is_dask, k, v in info:
778             if is_dask:
779                 func, args2 = v
780                 result = func(dsk, *args2)
781             else:
782                 result = v
783             variables[k] = result
784 
785         return Dataset._construct_direct(variables, *args)
786 
787     def compute(self, **kwargs) -> "Dataset":
788         """Manually trigger loading and/or computation of this dataset's data
789         from disk or a remote source into memory and return a new dataset.
790         Unlike load, the original dataset is left unaltered.
791 
792         Normally, it should not be necessary to call this method in user code,
793         because all xarray functions should either work on deferred data or
794         load data automatically. However, this method can be necessary when
795         working with many file objects on disk.
796 
797         Parameters
798         ----------
799         **kwargs : dict
800             Additional keyword arguments passed on to ``dask.array.compute``.
801 
802         See Also
803         --------
804         dask.array.compute
805         """
806         new = self.copy(deep=False)
807         return new.load(**kwargs)
808 
809     def _persist_inplace(self, **kwargs) -> "Dataset":
810         """Persist all Dask arrays in memory
811         """
812         # access .data to coerce everything to numpy or dask arrays
813         lazy_data = {
814             k: v._data
815             for k, v in self.variables.items()
816             if isinstance(v._data, dask_array_type)
817         }
818         if lazy_data:
819             import dask
820 
821             # evaluate all the dask arrays simultaneously
822             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)
823 
824             for k, data in zip(lazy_data, evaluated_data):
825                 self.variables[k].data = data
826 
827         return self
828 
829     def persist(self, **kwargs) -> "Dataset":
830         """ Trigger computation, keeping data as dask arrays
831 
832         This operation can be used to trigger computation on underlying dask
833         arrays, similar to ``.compute()`` or ``.load()``.  However this
834         operation keeps the data as dask arrays. This is particularly useful
835         when using the dask.distributed scheduler and you want to load a large
836         amount of data into distributed memory.
837 
838         Parameters
839         ----------
840         **kwargs : dict
841             Additional keyword arguments passed on to ``dask.persist``.
842 
843         See Also
844         --------
845         dask.persist
846         """
847         new = self.copy(deep=False)
848         return new._persist_inplace(**kwargs)
849 
850     @classmethod
851     def _construct_direct(
852         cls,
853         variables,
854         coord_names,
855         dims=None,
856         attrs=None,
857         indexes=None,
858         encoding=None,
859         file_obj=None,
860     ):
861         """Shortcut around __init__ for internal use when we want to skip
862         costly validation
863         """
864         if dims is None:
865             dims = calculate_dimensions(variables)
866         obj = object.__new__(cls)
867         obj._variables = variables
868         obj._coord_names = coord_names
869         obj._dims = dims
870         obj._indexes = indexes
871         obj._attrs = attrs
872         obj._file_obj = file_obj
873         obj._encoding = encoding
874         return obj
875 
876     def _replace(
877         self,
878         variables: Dict[Hashable, Variable] = None,
879         coord_names: Set[Hashable] = None,
880         dims: Dict[Any, int] = None,
881         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
882         indexes: Union[Dict[Any, pd.Index], None, Default] = _default,
883         encoding: Union[dict, None, Default] = _default,
884         inplace: bool = False,
885     ) -> "Dataset":
886         """Fastpath constructor for internal use.
887 
888         Returns an object with optionally with replaced attributes.
889 
890         Explicitly passed arguments are *not* copied when placed on the new
891         dataset. It is up to the caller to ensure that they have the right type
892         and are not used elsewhere.
893         """
894         if inplace:
895             if variables is not None:
896                 self._variables = variables
897             if coord_names is not None:
898                 self._coord_names = coord_names
899             if dims is not None:
900                 self._dims = dims
901             if attrs is not _default:
902                 self._attrs = attrs
903             if indexes is not _default:
904                 self._indexes = indexes
905             if encoding is not _default:
906                 self._encoding = encoding
907             obj = self
908         else:
909             if variables is None:
910                 variables = self._variables.copy()
911             if coord_names is None:
912                 coord_names = self._coord_names.copy()
913             if dims is None:
914                 dims = self._dims.copy()
915             if attrs is _default:
916                 attrs = copy.copy(self._attrs)
917             if indexes is _default:
918                 indexes = copy.copy(self._indexes)
919             if encoding is _default:
920                 encoding = copy.copy(self._encoding)
921             obj = self._construct_direct(
922                 variables, coord_names, dims, attrs, indexes, encoding
923             )
924         return obj
925 
926     def _replace_with_new_dims(
927         self,
928         variables: Dict[Hashable, Variable],
929         coord_names: set = None,
930         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
931         indexes: Union[Dict[Hashable, pd.Index], None, Default] = _default,
932         inplace: bool = False,
933     ) -> "Dataset":
934         """Replace variables with recalculated dimensions."""
935         dims = calculate_dimensions(variables)
936         return self._replace(
937             variables, coord_names, dims, attrs, indexes, inplace=inplace
938         )
939 
940     def _replace_vars_and_dims(
941         self,
942         variables: Dict[Hashable, Variable],
943         coord_names: set = None,
944         dims: Dict[Hashable, int] = None,
945         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
946         inplace: bool = False,
947     ) -> "Dataset":
948         """Deprecated version of _replace_with_new_dims().
949 
950         Unlike _replace_with_new_dims(), this method always recalculates
951         indexes from variables.
952         """
953         if dims is None:
954             dims = calculate_dimensions(variables)
955         return self._replace(
956             variables, coord_names, dims, attrs, indexes=None, inplace=inplace
957         )
958 
959     def _overwrite_indexes(self, indexes: Mapping[Any, pd.Index]) -> "Dataset":
960         if not indexes:
961             return self
962 
963         variables = self._variables.copy()
964         new_indexes = dict(self.indexes)
965         for name, idx in indexes.items():
966             variables[name] = IndexVariable(name, idx)
967             new_indexes[name] = idx
968         obj = self._replace(variables, indexes=new_indexes)
969 
970         # switch from dimension to level names, if necessary
971         dim_names: Dict[Hashable, str] = {}
972         for dim, idx in indexes.items():
973             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:
974                 dim_names[dim] = idx.name
975         if dim_names:
976             obj = obj.rename(dim_names)
977         return obj
978 
979     def copy(self, deep: bool = False, data: Mapping = None) -> "Dataset":
980         """Returns a copy of this dataset.
981 
982         If `deep=True`, a deep copy is made of each of the component variables.
983         Otherwise, a shallow copy of each of the component variable is made, so
984         that the underlying memory region of the new dataset is the same as in
985         the original dataset.
986 
987         Use `data` to create a new object with the same structure as
988         original but entirely new data.
989 
990         Parameters
991         ----------
992         deep : bool, optional
993             Whether each component variable is loaded into memory and copied onto
994             the new object. Default is False.
995         data : dict-like, optional
996             Data to use in the new object. Each item in `data` must have same
997             shape as corresponding data variable in original. When `data` is
998             used, `deep` is ignored for the data variables and only used for
999             coords.
1000 
1001         Returns
1002         -------
1003         object : Dataset
1004             New object with dimensions, attributes, coordinates, name, encoding,
1005             and optionally data copied from original.
1006 
1007         Examples
1008         --------
1009 
1010         Shallow copy versus deep copy
1011 
1012         >>> da = xr.DataArray(np.random.randn(2, 3))
1013         >>> ds = xr.Dataset(
1014         ...     {"foo": da, "bar": ("x", [-1, 2])}, coords={"x": ["one", "two"]},
1015         ... )
1016         >>> ds.copy()
1017         <xarray.Dataset>
1018         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1019         Coordinates:
1020         * x        (x) <U3 'one' 'two'
1021         Dimensions without coordinates: dim_0, dim_1
1022         Data variables:
1023             foo      (dim_0, dim_1) float64 -0.8079 0.3897 -1.862 -0.6091 -1.051 -0.3003
1024             bar      (x) int64 -1 2
1025 
1026         >>> ds_0 = ds.copy(deep=False)
1027         >>> ds_0["foo"][0, 0] = 7
1028         >>> ds_0
1029         <xarray.Dataset>
1030         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1031         Coordinates:
1032         * x        (x) <U3 'one' 'two'
1033         Dimensions without coordinates: dim_0, dim_1
1034         Data variables:
1035             foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003
1036             bar      (x) int64 -1 2
1037 
1038         >>> ds
1039         <xarray.Dataset>
1040         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1041         Coordinates:
1042         * x        (x) <U3 'one' 'two'
1043         Dimensions without coordinates: dim_0, dim_1
1044         Data variables:
1045             foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003
1046             bar      (x) int64 -1 2
1047 
1048         Changing the data using the ``data`` argument maintains the
1049         structure of the original object, but with the new data. Original
1050         object is unaffected.
1051 
1052         >>> ds.copy(
1053         ...     data={"foo": np.arange(6).reshape(2, 3), "bar": ["a", "b"]}
1054         ... )
1055         <xarray.Dataset>
1056         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1057         Coordinates:
1058         * x        (x) <U3 'one' 'two'
1059         Dimensions without coordinates: dim_0, dim_1
1060         Data variables:
1061             foo      (dim_0, dim_1) int64 0 1 2 3 4 5
1062             bar      (x) <U1 'a' 'b'
1063 
1064         >>> ds
1065         <xarray.Dataset>
1066         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1067         Coordinates:
1068         * x        (x) <U3 'one' 'two'
1069         Dimensions without coordinates: dim_0, dim_1
1070         Data variables:
1071             foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003
1072             bar      (x) int64 -1 2
1073 
1074         See Also
1075         --------
1076         pandas.DataFrame.copy
1077         """
1078         if data is None:
1079             variables = {k: v.copy(deep=deep) for k, v in self._variables.items()}
1080         elif not utils.is_dict_like(data):
1081             raise ValueError("Data must be dict-like")
1082         else:
1083             var_keys = set(self.data_vars.keys())
1084             data_keys = set(data.keys())
1085             keys_not_in_vars = data_keys - var_keys
1086             if keys_not_in_vars:
1087                 raise ValueError(
1088                     "Data must only contain variables in original "
1089                     "dataset. Extra variables: {}".format(keys_not_in_vars)
1090                 )
1091             keys_missing_from_data = var_keys - data_keys
1092             if keys_missing_from_data:
1093                 raise ValueError(
1094                     "Data must contain all variables in original "
1095                     "dataset. Data is missing {}".format(keys_missing_from_data)
1096                 )
1097             variables = {
1098                 k: v.copy(deep=deep, data=data.get(k))
1099                 for k, v in self._variables.items()
1100             }
1101 
1102         attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)
1103 
1104         return self._replace(variables, attrs=attrs)
1105 
1106     @property
1107     def _level_coords(self) -> Dict[str, Hashable]:
1108         """Return a mapping of all MultiIndex levels and their corresponding
1109         coordinate name.
1110         """
1111         level_coords: Dict[str, Hashable] = {}
1112         for name, index in self.indexes.items():
1113             if isinstance(index, pd.MultiIndex):
1114                 level_names = index.names
1115                 (dim,) = self.variables[name].dims
1116                 level_coords.update({lname: dim for lname in level_names})
1117         return level_coords
1118 
1119     def _copy_listed(self, names: Iterable[Hashable]) -> "Dataset":
1120         """Create a new Dataset with the listed variables from this dataset and
1121         the all relevant coordinates. Skips all validation.
1122         """
1123         variables: Dict[Hashable, Variable] = {}
1124         coord_names = set()
1125         indexes: Dict[Hashable, pd.Index] = {}
1126 
1127         for name in names:
1128             try:
1129                 variables[name] = self._variables[name]
1130             except KeyError:
1131                 ref_name, var_name, var = _get_virtual_variable(
1132                     self._variables, name, self._level_coords, self.dims
1133                 )
1134                 variables[var_name] = var
1135                 if ref_name in self._coord_names or ref_name in self.dims:
1136                     coord_names.add(var_name)
1137                 if (var_name,) == var.dims:
1138                     indexes[var_name] = var.to_index()
1139 
1140         needed_dims: Set[Hashable] = set()
1141         for v in variables.values():
1142             needed_dims.update(v.dims)
1143 
1144         dims = {k: self.dims[k] for k in needed_dims}
1145 
1146         for k in self._coord_names:
1147             if set(self.variables[k].dims) <= needed_dims:
1148                 variables[k] = self._variables[k]
1149                 coord_names.add(k)
1150                 if k in self.indexes:
1151                     indexes[k] = self.indexes[k]
1152 
1153         return self._replace(variables, coord_names, dims, indexes=indexes)
1154 
1155     def _construct_dataarray(self, name: Hashable) -> "DataArray":
1156         """Construct a DataArray by indexing this dataset
1157         """
1158         from .dataarray import DataArray
1159 
1160         try:
1161             variable = self._variables[name]
1162         except KeyError:
1163             _, name, variable = _get_virtual_variable(
1164                 self._variables, name, self._level_coords, self.dims
1165             )
1166 
1167         needed_dims = set(variable.dims)
1168 
1169         coords: Dict[Hashable, Variable] = {}
1170         for k in self.coords:
1171             if set(self.variables[k].dims) <= needed_dims:
1172                 coords[k] = self.variables[k]
1173 
1174         if self._indexes is None:
1175             indexes = None
1176         else:
1177             indexes = {k: v for k, v in self._indexes.items() if k in coords}
1178 
1179         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1180 
1181     def __copy__(self) -> "Dataset":
1182         return self.copy(deep=False)
1183 
1184     def __deepcopy__(self, memo=None) -> "Dataset":
1185         # memo does nothing but is required for compatibility with
1186         # copy.deepcopy
1187         return self.copy(deep=True)
1188 
1189     @property
1190     def _attr_sources(self) -> List[Mapping[Hashable, Any]]:
1191         """List of places to look-up items for attribute-style access
1192         """
1193         return self._item_sources + [self.attrs]
1194 
1195     @property
1196     def _item_sources(self) -> List[Mapping[Hashable, Any]]:
1197         """List of places to look-up items for key-completion
1198         """
1199         return [
1200             self.data_vars,
1201             self.coords,
1202             {d: self[d] for d in self.dims},
1203             LevelCoordinatesSource(self),
1204         ]
1205 
1206     def __contains__(self, key: object) -> bool:
1207         """The 'in' operator will return true or false depending on whether
1208         'key' is an array in the dataset or not.
1209         """
1210         return key in self._variables
1211 
1212     def __len__(self) -> int:
1213         return len(self.data_vars)
1214 
1215     def __bool__(self) -> bool:
1216         return bool(self.data_vars)
1217 
1218     def __iter__(self) -> Iterator[Hashable]:
1219         return iter(self.data_vars)
1220 
1221     def __array__(self, dtype=None):
1222         raise TypeError(
1223             "cannot directly convert an xarray.Dataset into a "
1224             "numpy array. Instead, create an xarray.DataArray "
1225             "first, either with indexing on the Dataset or by "
1226             "invoking the `to_array()` method."
1227         )
1228 
1229     @property
1230     def nbytes(self) -> int:
1231         return sum(v.nbytes for v in self.variables.values())
1232 
1233     @property
1234     def loc(self) -> _LocIndexer:
1235         """Attribute for location based indexing. Only supports __getitem__,
1236         and only when the key is a dict of the form {dim: labels}.
1237         """
1238         return _LocIndexer(self)
1239 
1240     def __getitem__(self, key: Any) -> "Union[DataArray, Dataset]":
1241         """Access variables or coordinates this dataset as a
1242         :py:class:`~xarray.DataArray`.
1243 
1244         Indexing with a list of names will return a new ``Dataset`` object.
1245         """
1246         # TODO(shoyer): type this properly: https://github.com/python/mypy/issues/7328
1247         if utils.is_dict_like(key):
1248             return self.isel(**cast(Mapping, key))
1249 
1250         if hashable(key):
1251             return self._construct_dataarray(key)
1252         else:
1253             return self._copy_listed(np.asarray(key))
1254 
1255     def __setitem__(self, key: Hashable, value) -> None:
1256         """Add an array to this dataset.
1257 
1258         If value is a `DataArray`, call its `select_vars()` method, rename it
1259         to `key` and merge the contents of the resulting dataset into this
1260         dataset.
1261 
1262         If value is an `Variable` object (or tuple of form
1263         ``(dims, data[, attrs])``), add it to this dataset as a new
1264         variable.
1265         """
1266         if utils.is_dict_like(key):
1267             raise NotImplementedError(
1268                 "cannot yet use a dictionary as a key " "to set Dataset values"
1269             )
1270 
1271         self.update({key: value})
1272 
1273     def __delitem__(self, key: Hashable) -> None:
1274         """Remove a variable from this dataset.
1275         """
1276         del self._variables[key]
1277         self._coord_names.discard(key)
1278         if key in self.indexes:
1279             assert self._indexes is not None
1280             del self._indexes[key]
1281         self._dims = calculate_dimensions(self._variables)
1282 
1283     # mutable objects should not be hashable
1284     # https://github.com/python/mypy/issues/4266
1285     __hash__ = None  # type: ignore
1286 
1287     def _all_compat(self, other: "Dataset", compat_str: str) -> bool:
1288         """Helper function for equals and identical
1289         """
1290 
1291         # some stores (e.g., scipy) do not seem to preserve order, so don't
1292         # require matching order for equality
1293         def compat(x: Variable, y: Variable) -> bool:
1294             return getattr(x, compat_str)(y)
1295 
1296         return self._coord_names == other._coord_names and utils.dict_equiv(
1297             self._variables, other._variables, compat=compat
1298         )
1299 
1300     def broadcast_equals(self, other: "Dataset") -> bool:
1301         """Two Datasets are broadcast equal if they are equal after
1302         broadcasting all variables against each other.
1303 
1304         For example, variables that are scalar in one dataset but non-scalar in
1305         the other dataset can still be broadcast equal if the the non-scalar
1306         variable is a constant.
1307 
1308         See Also
1309         --------
1310         Dataset.equals
1311         Dataset.identical
1312         """
1313         try:
1314             return self._all_compat(other, "broadcast_equals")
1315         except (TypeError, AttributeError):
1316             return False
1317 
1318     def equals(self, other: "Dataset") -> bool:
1319         """Two Datasets are equal if they have matching variables and
1320         coordinates, all of which are equal.
1321 
1322         Datasets can still be equal (like pandas objects) if they have NaN
1323         values in the same locations.
1324 
1325         This method is necessary because `v1 == v2` for ``Dataset``
1326         does element-wise comparisons (like numpy.ndarrays).
1327 
1328         See Also
1329         --------
1330         Dataset.broadcast_equals
1331         Dataset.identical
1332         """
1333         try:
1334             return self._all_compat(other, "equals")
1335         except (TypeError, AttributeError):
1336             return False
1337 
1338     def identical(self, other: "Dataset") -> bool:
1339         """Like equals, but also checks all dataset attributes and the
1340         attributes on all variables and coordinates.
1341 
1342         See Also
1343         --------
1344         Dataset.broadcast_equals
1345         Dataset.equals
1346         """
1347         try:
1348             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(
1349                 other, "identical"
1350             )
1351         except (TypeError, AttributeError):
1352             return False
1353 
1354     @property
1355     def indexes(self) -> Indexes:
1356         """Mapping of pandas.Index objects used for label based indexing
1357         """
1358         if self._indexes is None:
1359             self._indexes = default_indexes(self._variables, self._dims)
1360         return Indexes(self._indexes)
1361 
1362     @property
1363     def coords(self) -> DatasetCoordinates:
1364         """Dictionary of xarray.DataArray objects corresponding to coordinate
1365         variables
1366         """
1367         return DatasetCoordinates(self)
1368 
1369     @property
1370     def data_vars(self) -> DataVariables:
1371         """Dictionary of DataArray objects corresponding to data variables
1372         """
1373         return DataVariables(self)
1374 
1375     def set_coords(
1376         self, names: "Union[Hashable, Iterable[Hashable]]", inplace: bool = None
1377     ) -> "Dataset":
1378         """Given names of one or more variables, set them as coordinates
1379 
1380         Parameters
1381         ----------
1382         names : hashable or iterable of hashables
1383             Name(s) of variables in this dataset to convert into coordinates.
1384 
1385         Returns
1386         -------
1387         Dataset
1388 
1389         See also
1390         --------
1391         Dataset.swap_dims
1392         """
1393         # TODO: allow inserting new coordinates with this method, like
1394         # DataFrame.set_index?
1395         # nb. check in self._variables, not self.data_vars to insure that the
1396         # operation is idempotent
1397         _check_inplace(inplace)
1398         if isinstance(names, str) or not isinstance(names, Iterable):
1399             names = [names]
1400         else:
1401             names = list(names)
1402         self._assert_all_in_dataset(names)
1403         obj = self.copy()
1404         obj._coord_names.update(names)
1405         return obj
1406 
1407     def reset_coords(
1408         self,
1409         names: "Union[Hashable, Iterable[Hashable], None]" = None,
1410         drop: bool = False,
1411         inplace: bool = None,
1412     ) -> "Dataset":
1413         """Given names of coordinates, reset them to become variables
1414 
1415         Parameters
1416         ----------
1417         names : hashable or iterable of hashables, optional
1418             Name(s) of non-index coordinates in this dataset to reset into
1419             variables. By default, all non-index coordinates are reset.
1420         drop : bool, optional
1421             If True, remove coordinates instead of converting them into
1422             variables.
1423 
1424         Returns
1425         -------
1426         Dataset
1427         """
1428         _check_inplace(inplace)
1429         if names is None:
1430             names = self._coord_names - set(self.dims)
1431         else:
1432             if isinstance(names, str) or not isinstance(names, Iterable):
1433                 names = [names]
1434             else:
1435                 names = list(names)
1436             self._assert_all_in_dataset(names)
1437             bad_coords = set(names) & set(self.dims)
1438             if bad_coords:
1439                 raise ValueError(
1440                     "cannot remove index coordinates with reset_coords: %s" % bad_coords
1441                 )
1442         obj = self.copy()
1443         obj._coord_names.difference_update(names)
1444         if drop:
1445             for name in names:
1446                 del obj._variables[name]
1447         return obj
1448 
1449     def dump_to_store(self, store: "AbstractDataStore", **kwargs) -> None:
1450         """Store dataset contents to a backends.*DataStore object.
1451         """
1452         from ..backends.api import dump_to_store
1453 
1454         # TODO: rename and/or cleanup this method to make it more consistent
1455         # with to_netcdf()
1456         dump_to_store(self, store, **kwargs)
1457 
1458     def to_netcdf(
1459         self,
1460         path=None,
1461         mode: str = "w",
1462         format: str = None,
1463         group: str = None,
1464         engine: str = None,
1465         encoding: Mapping = None,
1466         unlimited_dims: Iterable[Hashable] = None,
1467         compute: bool = True,
1468         invalid_netcdf: bool = False,
1469     ) -> Union[bytes, "Delayed", None]:
1470         """Write dataset contents to a netCDF file.
1471 
1472         Parameters
1473         ----------
1474         path : str, Path or file-like object, optional
1475             Path to which to save this dataset. File-like objects are only
1476             supported by the scipy engine. If no path is provided, this
1477             function returns the resulting netCDF file as bytes; in this case,
1478             we need to use scipy, which does not support netCDF version 4 (the
1479             default format becomes NETCDF3_64BIT).
1480         mode : {'w', 'a'}, optional
1481             Write ('w') or append ('a') mode. If mode='w', any existing file at
1482             this location will be overwritten. If mode='a', existing variables
1483             will be overwritten.
1484         format : {'NETCDF4', 'NETCDF4_CLASSIC', 'NETCDF3_64BIT',
1485                   'NETCDF3_CLASSIC'}, optional
1486             File format for the resulting netCDF file:
1487 
1488             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
1489               features.
1490             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
1491               netCDF 3 compatible API features.
1492             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
1493               which fully supports 2+ GB files, but is only compatible with
1494               clients linked against netCDF version 3.6.0 or later.
1495             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
1496               handle 2+ GB files very well.
1497 
1498             All formats are supported by the netCDF4-python library.
1499             scipy.io.netcdf only supports the last two formats.
1500 
1501             The default format is NETCDF4 if you are saving a file to disk and
1502             have the netCDF4-python library available. Otherwise, xarray falls
1503             back to using scipy to write netCDF files and defaults to the
1504             NETCDF3_64BIT format (scipy does not support netCDF4).
1505         group : str, optional
1506             Path to the netCDF4 group in the given file to open (only works for
1507             format='NETCDF4'). The group(s) will be created if necessary.
1508         engine : {'netcdf4', 'scipy', 'h5netcdf'}, optional
1509             Engine to use when writing netCDF files. If not provided, the
1510             default engine is chosen based on available dependencies, with a
1511             preference for 'netcdf4' if writing to a file on disk.
1512         encoding : dict, optional
1513             Nested dictionary with variable names as keys and dictionaries of
1514             variable specific encodings as values, e.g.,
1515             ``{'my_variable': {'dtype': 'int16', 'scale_factor': 0.1,
1516             'zlib': True}, ...}``
1517 
1518             The `h5netcdf` engine supports both the NetCDF4-style compression
1519             encoding parameters ``{'zlib': True, 'complevel': 9}`` and the h5py
1520             ones ``{'compression': 'gzip', 'compression_opts': 9}``.
1521             This allows using any compression plugin installed in the HDF5
1522             library, e.g. LZF.
1523 
1524         unlimited_dims : iterable of hashable, optional
1525             Dimension(s) that should be serialized as unlimited dimensions.
1526             By default, no dimensions are treated as unlimited dimensions.
1527             Note that unlimited_dims may also be set via
1528             ``dataset.encoding['unlimited_dims']``.
1529         compute: boolean
1530             If true compute immediately, otherwise return a
1531             ``dask.delayed.Delayed`` object that can be computed later.
1532         invalid_netcdf: boolean
1533             Only valid along with engine='h5netcdf'. If True, allow writing
1534             hdf5 files which are valid netcdf as described in
1535             https://github.com/shoyer/h5netcdf. Default: False.
1536         """
1537         if encoding is None:
1538             encoding = {}
1539         from ..backends.api import to_netcdf
1540 
1541         return to_netcdf(
1542             self,
1543             path,
1544             mode,
1545             format=format,
1546             group=group,
1547             engine=engine,
1548             encoding=encoding,
1549             unlimited_dims=unlimited_dims,
1550             compute=compute,
1551             invalid_netcdf=invalid_netcdf,
1552         )
1553 
1554     def to_zarr(
1555         self,
1556         store: Union[MutableMapping, str, Path] = None,
1557         mode: str = None,
1558         synchronizer=None,
1559         group: str = None,
1560         encoding: Mapping = None,
1561         compute: bool = True,
1562         consolidated: bool = False,
1563         append_dim: Hashable = None,
1564     ) -> "ZarrStore":
1565         """Write dataset contents to a zarr group.
1566 
1567         .. note:: Experimental
1568                   The Zarr backend is new and experimental. Please report any
1569                   unexpected behavior via github issues.
1570 
1571         Parameters
1572         ----------
1573         store : MutableMapping, str or Path, optional
1574             Store or path to directory in file system.
1575         mode : {'w', 'w-', 'a', None}
1576             Persistence mode: 'w' means create (overwrite if exists);
1577             'w-' means create (fail if exists);
1578             'a' means append (create if does not exist).
1579             If ``append_dim`` is set, ``mode`` can be omitted as it is
1580             internally set to ``'a'``. Otherwise, ``mode`` will default to
1581             `w-` if not set.
1582         synchronizer : object, optional
1583             Array synchronizer
1584         group : str, optional
1585             Group path. (a.k.a. `path` in zarr terminology.)
1586         encoding : dict, optional
1587             Nested dictionary with variable names as keys and dictionaries of
1588             variable specific encodings as values, e.g.,
1589             ``{'my_variable': {'dtype': 'int16', 'scale_factor': 0.1,}, ...}``
1590         compute: bool, optional
1591             If True compute immediately, otherwise return a
1592             ``dask.delayed.Delayed`` object that can be computed later.
1593         consolidated: bool, optional
1594             If True, apply zarr's `consolidate_metadata` function to the store
1595             after writing.
1596         append_dim: hashable, optional
1597             If set, the dimension on which the data will be appended.
1598 
1599         References
1600         ----------
1601         https://zarr.readthedocs.io/
1602         """
1603         if encoding is None:
1604             encoding = {}
1605         if (mode == "a") or (append_dim is not None):
1606             if mode is None:
1607                 mode = "a"
1608             elif mode != "a":
1609                 raise ValueError(
1610                     "append_dim was set along with mode='{}', either set "
1611                     "mode='a' or don't set it.".format(mode)
1612                 )
1613         elif mode is None:
1614             mode = "w-"
1615         if mode not in ["w", "w-", "a"]:
1616             # TODO: figure out how to handle 'r+'
1617             raise ValueError(
1618                 "The only supported options for mode are 'w'," "'w-' and 'a'."
1619             )
1620         from ..backends.api import to_zarr
1621 
1622         return to_zarr(
1623             self,
1624             store=store,
1625             mode=mode,
1626             synchronizer=synchronizer,
1627             group=group,
1628             encoding=encoding,
1629             compute=compute,
1630             consolidated=consolidated,
1631             append_dim=append_dim,
1632         )
1633 
1634     def __repr__(self) -> str:
1635         return formatting.dataset_repr(self)
1636 
1637     def _repr_html_(self):
1638         if OPTIONS["display_style"] == "text":
1639             return f"<pre>{escape(repr(self))}</pre>"
1640         return formatting_html.dataset_repr(self)
1641 
1642     def info(self, buf=None) -> None:
1643         """
1644         Concise summary of a Dataset variables and attributes.
1645 
1646         Parameters
1647         ----------
1648         buf : writable buffer, defaults to sys.stdout
1649 
1650         See Also
1651         --------
1652         pandas.DataFrame.assign
1653         ncdump: netCDF's ncdump
1654         """
1655         if buf is None:  # pragma: no cover
1656             buf = sys.stdout
1657 
1658         lines = []
1659         lines.append("xarray.Dataset {")
1660         lines.append("dimensions:")
1661         for name, size in self.dims.items():
1662             lines.append(f"\t{name} = {size} ;")
1663         lines.append("\nvariables:")
1664         for name, da in self.variables.items():
1665             dims = ", ".join(da.dims)
1666             lines.append(f"\t{da.dtype} {name}({dims}) ;")
1667             for k, v in da.attrs.items():
1668                 lines.append(f"\t\t{name}:{k} = {v} ;")
1669         lines.append("\n// global attributes:")
1670         for k, v in self.attrs.items():
1671             lines.append(f"\t:{k} = {v} ;")
1672         lines.append("}")
1673 
1674         buf.write("\n".join(lines))
1675 
1676     @property
1677     def chunks(self) -> Mapping[Hashable, Tuple[int, ...]]:
1678         """Block dimensions for this dataset's data or None if it's not a dask
1679         array.
1680         """
1681         chunks: Dict[Hashable, Tuple[int, ...]] = {}
1682         for v in self.variables.values():
1683             if v.chunks is not None:
1684                 for dim, c in zip(v.dims, v.chunks):
1685                     if dim in chunks and c != chunks[dim]:
1686                         raise ValueError(
1687                             f"Object has inconsistent chunks along dimension {dim}. "
1688                             "This can be fixed by calling unify_chunks()."
1689                         )
1690                     chunks[dim] = c
1691         return Frozen(SortedKeysDict(chunks))
1692 
1693     def chunk(
1694         self,
1695         chunks: Union[
1696             None, Number, Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]]
1697         ] = None,
1698         name_prefix: str = "xarray-",
1699         token: str = None,
1700         lock: bool = False,
1701     ) -> "Dataset":
1702         """Coerce all arrays in this dataset into dask arrays with the given
1703         chunks.
1704 
1705         Non-dask arrays in this dataset will be converted to dask arrays. Dask
1706         arrays will be rechunked to the given chunk sizes.
1707 
1708         If neither chunks is not provided for one or more dimensions, chunk
1709         sizes along that dimension will not be updated; non-dask arrays will be
1710         converted into dask arrays with a single block.
1711 
1712         Parameters
1713         ----------
1714         chunks : int or mapping, optional
1715             Chunk sizes along each dimension, e.g., ``5`` or
1716             ``{'x': 5, 'y': 5}``.
1717         name_prefix : str, optional
1718             Prefix for the name of any new dask arrays.
1719         token : str, optional
1720             Token uniquely identifying this dataset.
1721         lock : optional
1722             Passed on to :py:func:`dask.array.from_array`, if the array is not
1723             already as dask array.
1724 
1725         Returns
1726         -------
1727         chunked : xarray.Dataset
1728         """
1729         from dask.base import tokenize
1730 
1731         if isinstance(chunks, Number):
1732             chunks = dict.fromkeys(self.dims, chunks)
1733 
1734         if chunks is not None:
1735             bad_dims = chunks.keys() - self.dims.keys()
1736             if bad_dims:
1737                 raise ValueError(
1738                     "some chunks keys are not dimensions on this "
1739                     "object: %s" % bad_dims
1740                 )
1741 
1742         def selkeys(dict_, keys):
1743             if dict_ is None:
1744                 return None
1745             return {d: dict_[d] for d in keys if d in dict_}
1746 
1747         def maybe_chunk(name, var, chunks):
1748             chunks = selkeys(chunks, var.dims)
1749             if not chunks:
1750                 chunks = None
1751             if var.ndim > 0:
1752                 # when rechunking by different amounts, make sure dask names change
1753                 # by provinding chunks as an input to tokenize.
1754                 # subtle bugs result otherwise. see GH3350
1755                 token2 = tokenize(name, token if token else var._data, chunks)
1756                 name2 = f"{name_prefix}{name}-{token2}"
1757                 return var.chunk(chunks, name=name2, lock=lock)
1758             else:
1759                 return var
1760 
1761         variables = {k: maybe_chunk(k, v, chunks) for k, v in self.variables.items()}
1762         return self._replace(variables)
1763 
1764     def _validate_indexers(
1765         self, indexers: Mapping[Hashable, Any]
1766     ) -> Iterator[Tuple[Hashable, Union[int, slice, np.ndarray, Variable]]]:
1767         """ Here we make sure
1768         + indexer has a valid keys
1769         + indexer is in a valid data type
1770         + string indexers are cast to the appropriate date type if the
1771           associated index is a DatetimeIndex or CFTimeIndex
1772         """
1773         from .dataarray import DataArray
1774 
1775         invalid = indexers.keys() - self.dims.keys()
1776         if invalid:
1777             raise ValueError("dimensions %r do not exist" % invalid)
1778 
1779         # all indexers should be int, slice, np.ndarrays, or Variable
1780         for k, v in indexers.items():
1781             if isinstance(v, (int, slice, Variable)):
1782                 yield k, v
1783             elif isinstance(v, DataArray):
1784                 yield k, v.variable
1785             elif isinstance(v, tuple):
1786                 yield k, as_variable(v)
1787             elif isinstance(v, Dataset):
1788                 raise TypeError("cannot use a Dataset as an indexer")
1789             elif isinstance(v, Sequence) and len(v) == 0:
1790                 yield k, np.empty((0,), dtype="int64")
1791             else:
1792                 v = np.asarray(v)
1793 
1794                 if v.dtype.kind in "US":
1795                     index = self.indexes[k]
1796                     if isinstance(index, pd.DatetimeIndex):
1797                         v = v.astype("datetime64[ns]")
1798                     elif isinstance(index, xr.CFTimeIndex):
1799                         v = _parse_array_of_cftime_strings(v, index.date_type)
1800 
1801                 if v.ndim > 1:
1802                     raise IndexError(
1803                         "Unlabeled multi-dimensional array cannot be "
1804                         "used for indexing: {}".format(k)
1805                     )
1806                 yield k, v
1807 
1808     def _validate_interp_indexers(
1809         self, indexers: Mapping[Hashable, Any]
1810     ) -> Iterator[Tuple[Hashable, Variable]]:
1811         """Variant of _validate_indexers to be used for interpolation
1812         """
1813         for k, v in self._validate_indexers(indexers):
1814             if isinstance(v, Variable):
1815                 if v.ndim == 1:
1816                     yield k, v.to_index_variable()
1817                 else:
1818                     yield k, v
1819             elif isinstance(v, int):
1820                 yield k, Variable((), v)
1821             elif isinstance(v, np.ndarray):
1822                 if v.ndim == 0:
1823                     yield k, Variable((), v)
1824                 elif v.ndim == 1:
1825                     yield k, IndexVariable((k,), v)
1826                 else:
1827                     raise AssertionError()  # Already tested by _validate_indexers
1828             else:
1829                 raise TypeError(type(v))
1830 
1831     def _get_indexers_coords_and_indexes(self, indexers):
1832         """Extract coordinates and indexes from indexers.
1833 
1834         Only coordinate with a name different from any of self.variables will
1835         be attached.
1836         """
1837         from .dataarray import DataArray
1838 
1839         coords_list = []
1840         for k, v in indexers.items():
1841             if isinstance(v, DataArray):
1842                 if v.dtype.kind == "b":
1843                     if v.ndim != 1:  # we only support 1-d boolean array
1844                         raise ValueError(
1845                             "{:d}d-boolean array is used for indexing along "
1846                             "dimension {!r}, but only 1d boolean arrays are "
1847                             "supported.".format(v.ndim, k)
1848                         )
1849                     # Make sure in case of boolean DataArray, its
1850                     # coordinate also should be indexed.
1851                     v_coords = v[v.values.nonzero()[0]].coords
1852                 else:
1853                     v_coords = v.coords
1854                 coords_list.append(v_coords)
1855 
1856         # we don't need to call align() explicitly or check indexes for
1857         # alignment, because merge_variables already checks for exact alignment
1858         # between dimension coordinates
1859         coords, indexes = merge_coordinates_without_align(coords_list)
1860         assert_coordinate_consistent(self, coords)
1861 
1862         # silently drop the conflicted variables.
1863         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}
1864         attached_indexes = {
1865             k: v for k, v in indexes.items() if k not in self._variables
1866         }
1867         return attached_coords, attached_indexes
1868 
1869     def isel(
1870         self,
1871         indexers: Mapping[Hashable, Any] = None,
1872         drop: bool = False,
1873         **indexers_kwargs: Any,
1874     ) -> "Dataset":
1875         """Returns a new dataset with each array indexed along the specified
1876         dimension(s).
1877 
1878         This method selects values from each array using its `__getitem__`
1879         method, except this method does not require knowing the order of
1880         each array's dimensions.
1881 
1882         Parameters
1883         ----------
1884         indexers : dict, optional
1885             A dict with keys matching dimensions and values given
1886             by integers, slice objects or arrays.
1887             indexer can be a integer, slice, array-like or DataArray.
1888             If DataArrays are passed as indexers, xarray-style indexing will be
1889             carried out. See :ref:`indexing` for the details.
1890             One of indexers or indexers_kwargs must be provided.
1891         drop : bool, optional
1892             If ``drop=True``, drop coordinates variables indexed by integers
1893             instead of making them scalar.
1894         **indexers_kwargs : {dim: indexer, ...}, optional
1895             The keyword arguments form of ``indexers``.
1896             One of indexers or indexers_kwargs must be provided.
1897 
1898         Returns
1899         -------
1900         obj : Dataset
1901             A new Dataset with the same contents as this dataset, except each
1902             array and dimension is indexed by the appropriate indexers.
1903             If indexer DataArrays have coordinates that do not conflict with
1904             this object, then these coordinates will be attached.
1905             In general, each array's data will be a view of the array's data
1906             in this dataset, unless vectorized indexing was triggered by using
1907             an array indexer, in which case the data will be a copy.
1908 
1909         See Also
1910         --------
1911         Dataset.sel
1912         DataArray.isel
1913         """
1914         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1915         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1916             return self._isel_fancy(indexers, drop=drop)
1917 
1918         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1919         # lists, or zero or one-dimensional np.ndarray's
1920         invalid = indexers.keys() - self.dims.keys()
1921         if invalid:
1922             raise ValueError("dimensions %r do not exist" % invalid)
1923 
1924         variables = {}
1925         dims: Dict[Hashable, Tuple[int, ...]] = {}
1926         coord_names = self._coord_names.copy()
1927         indexes = self._indexes.copy() if self._indexes is not None else None
1928 
1929         for var_name, var_value in self._variables.items():
1930             var_indexers = {k: v for k, v in indexers.items() if k in var_value.dims}
1931             if var_indexers:
1932                 var_value = var_value.isel(var_indexers)
1933                 if drop and var_value.ndim == 0 and var_name in coord_names:
1934                     coord_names.remove(var_name)
1935                     if indexes:
1936                         indexes.pop(var_name, None)
1937                     continue
1938                 if indexes and var_name in indexes:
1939                     if var_value.ndim == 1:
1940                         indexes[var_name] = var_value.to_index()
1941                     else:
1942                         del indexes[var_name]
1943             variables[var_name] = var_value
1944             dims.update(zip(var_value.dims, var_value.shape))
1945 
1946         return self._construct_direct(
1947             variables=variables,
1948             coord_names=coord_names,
1949             dims=dims,
1950             attrs=self._attrs,
1951             indexes=indexes,
1952             encoding=self._encoding,
1953             file_obj=self._file_obj,
1954         )
1955 
1956     def _isel_fancy(self, indexers: Mapping[Hashable, Any], *, drop: bool) -> "Dataset":
1957         # Note: we need to preserve the original indexers variable in order to merge the
1958         # coords below
1959         indexers_list = list(self._validate_indexers(indexers))
1960 
1961         variables: Dict[Hashable, Variable] = {}
1962         indexes: Dict[Hashable, pd.Index] = {}
1963 
1964         for name, var in self.variables.items():
1965             var_indexers = {k: v for k, v in indexers_list if k in var.dims}
1966             if drop and name in var_indexers:
1967                 continue  # drop this variable
1968 
1969             if name in self.indexes:
1970                 new_var, new_index = isel_variable_and_index(
1971                     name, var, self.indexes[name], var_indexers
1972                 )
1973                 if new_index is not None:
1974                     indexes[name] = new_index
1975             elif var_indexers:
1976                 new_var = var.isel(indexers=var_indexers)
1977             else:
1978                 new_var = var.copy(deep=False)
1979 
1980             variables[name] = new_var
1981 
1982         coord_names = self._coord_names & variables.keys()
1983         selected = self._replace_with_new_dims(variables, coord_names, indexes)
1984 
1985         # Extract coordinates from indexers
1986         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)
1987         variables.update(coord_vars)
1988         indexes.update(new_indexes)
1989         coord_names = self._coord_names & variables.keys() | coord_vars.keys()
1990         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
1991 
1992     def sel(
1993         self,
1994         indexers: Mapping[Hashable, Any] = None,
1995         method: str = None,
1996         tolerance: Number = None,
1997         drop: bool = False,
1998         **indexers_kwargs: Any,
1999     ) -> "Dataset":
2000         """Returns a new dataset with each array indexed by tick labels
2001         along the specified dimension(s).
2002 
2003         In contrast to `Dataset.isel`, indexers for this method should use
2004         labels instead of integers.
2005 
2006         Under the hood, this method is powered by using pandas's powerful Index
2007         objects. This makes label based indexing essentially just as fast as
2008         using integer indexing.
2009 
2010         It also means this method uses pandas's (well documented) logic for
2011         indexing. This means you can use string shortcuts for datetime indexes
2012         (e.g., '2000-01' to select all values in January 2000). It also means
2013         that slices are treated as inclusive of both the start and stop values,
2014         unlike normal Python indexing.
2015 
2016         Parameters
2017         ----------
2018         indexers : dict, optional
2019             A dict with keys matching dimensions and values given
2020             by scalars, slices or arrays of tick labels. For dimensions with
2021             multi-index, the indexer may also be a dict-like object with keys
2022             matching index level names.
2023             If DataArrays are passed as indexers, xarray-style indexing will be
2024             carried out. See :ref:`indexing` for the details.
2025             One of indexers or indexers_kwargs must be provided.
2026         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
2027             Method to use for inexact matches:
2028 
2029             * None (default): only exact matches
2030             * pad / ffill: propagate last valid index value forward
2031             * backfill / bfill: propagate next valid index value backward
2032             * nearest: use nearest valid index value
2033         tolerance : optional
2034             Maximum distance between original and new labels for inexact
2035             matches. The values of the index at the matching locations must
2036             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2037         drop : bool, optional
2038             If ``drop=True``, drop coordinates variables in `indexers` instead
2039             of making them scalar.
2040         **indexers_kwargs : {dim: indexer, ...}, optional
2041             The keyword arguments form of ``indexers``.
2042             One of indexers or indexers_kwargs must be provided.
2043 
2044         Returns
2045         -------
2046         obj : Dataset
2047             A new Dataset with the same contents as this dataset, except each
2048             variable and dimension is indexed by the appropriate indexers.
2049             If indexer DataArrays have coordinates that do not conflict with
2050             this object, then these coordinates will be attached.
2051             In general, each array's data will be a view of the array's data
2052             in this dataset, unless vectorized indexing was triggered by using
2053             an array indexer, in which case the data will be a copy.
2054 
2055 
2056         See Also
2057         --------
2058         Dataset.isel
2059         DataArray.sel
2060         """
2061         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "sel")
2062         pos_indexers, new_indexes = remap_label_indexers(
2063             self, indexers=indexers, method=method, tolerance=tolerance
2064         )
2065         result = self.isel(indexers=pos_indexers, drop=drop)
2066         return result._overwrite_indexes(new_indexes)
2067 
2068     def head(
2069         self,
2070         indexers: Union[Mapping[Hashable, int], int] = None,
2071         **indexers_kwargs: Any,
2072     ) -> "Dataset":
2073         """Returns a new dataset with the first `n` values of each array
2074         for the specified dimension(s).
2075 
2076         Parameters
2077         ----------
2078         indexers : dict or int, default: 5
2079             A dict with keys matching dimensions and integer values `n`
2080             or a single integer `n` applied over all dimensions.
2081             One of indexers or indexers_kwargs must be provided.
2082         **indexers_kwargs : {dim: n, ...}, optional
2083             The keyword arguments form of ``indexers``.
2084             One of indexers or indexers_kwargs must be provided.
2085 
2086 
2087         See Also
2088         --------
2089         Dataset.tail
2090         Dataset.thin
2091         DataArray.head
2092         """
2093         if not indexers_kwargs:
2094             if indexers is None:
2095                 indexers = 5
2096             if not isinstance(indexers, int) and not is_dict_like(indexers):
2097                 raise TypeError("indexers must be either dict-like or a single integer")
2098         if isinstance(indexers, int):
2099             indexers = {dim: indexers for dim in self.dims}
2100         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "head")
2101         for k, v in indexers.items():
2102             if not isinstance(v, int):
2103                 raise TypeError(
2104                     "expected integer type indexer for "
2105                     "dimension %r, found %r" % (k, type(v))
2106                 )
2107             elif v < 0:
2108                 raise ValueError(
2109                     "expected positive integer as indexer "
2110                     "for dimension %r, found %s" % (k, v)
2111                 )
2112         indexers_slices = {k: slice(val) for k, val in indexers.items()}
2113         return self.isel(indexers_slices)
2114 
2115     def tail(
2116         self,
2117         indexers: Union[Mapping[Hashable, int], int] = None,
2118         **indexers_kwargs: Any,
2119     ) -> "Dataset":
2120         """Returns a new dataset with the last `n` values of each array
2121         for the specified dimension(s).
2122 
2123         Parameters
2124         ----------
2125         indexers : dict or int, default: 5
2126             A dict with keys matching dimensions and integer values `n`
2127             or a single integer `n` applied over all dimensions.
2128             One of indexers or indexers_kwargs must be provided.
2129         **indexers_kwargs : {dim: n, ...}, optional
2130             The keyword arguments form of ``indexers``.
2131             One of indexers or indexers_kwargs must be provided.
2132 
2133 
2134         See Also
2135         --------
2136         Dataset.head
2137         Dataset.thin
2138         DataArray.tail
2139         """
2140         if not indexers_kwargs:
2141             if indexers is None:
2142                 indexers = 5
2143             if not isinstance(indexers, int) and not is_dict_like(indexers):
2144                 raise TypeError("indexers must be either dict-like or a single integer")
2145         if isinstance(indexers, int):
2146             indexers = {dim: indexers for dim in self.dims}
2147         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "tail")
2148         for k, v in indexers.items():
2149             if not isinstance(v, int):
2150                 raise TypeError(
2151                     "expected integer type indexer for "
2152                     "dimension %r, found %r" % (k, type(v))
2153                 )
2154             elif v < 0:
2155                 raise ValueError(
2156                     "expected positive integer as indexer "
2157                     "for dimension %r, found %s" % (k, v)
2158                 )
2159         indexers_slices = {
2160             k: slice(-val, None) if val != 0 else slice(val)
2161             for k, val in indexers.items()
2162         }
2163         return self.isel(indexers_slices)
2164 
2165     def thin(
2166         self,
2167         indexers: Union[Mapping[Hashable, int], int] = None,
2168         **indexers_kwargs: Any,
2169     ) -> "Dataset":
2170         """Returns a new dataset with each array indexed along every `n`-th
2171         value for the specified dimension(s)
2172 
2173         Parameters
2174         ----------
2175         indexers : dict or int
2176             A dict with keys matching dimensions and integer values `n`
2177             or a single integer `n` applied over all dimensions.
2178             One of indexers or indexers_kwargs must be provided.
2179         ``**indexers_kwargs`` : {dim: n, ...}, optional
2180             The keyword arguments form of ``indexers``.
2181             One of indexers or indexers_kwargs must be provided.
2182 
2183 
2184         See Also
2185         --------
2186         Dataset.head
2187         Dataset.tail
2188         DataArray.thin
2189         """
2190         if (
2191             not indexers_kwargs
2192             and not isinstance(indexers, int)
2193             and not is_dict_like(indexers)
2194         ):
2195             raise TypeError("indexers must be either dict-like or a single integer")
2196         if isinstance(indexers, int):
2197             indexers = {dim: indexers for dim in self.dims}
2198         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "thin")
2199         for k, v in indexers.items():
2200             if not isinstance(v, int):
2201                 raise TypeError(
2202                     "expected integer type indexer for "
2203                     "dimension %r, found %r" % (k, type(v))
2204                 )
2205             elif v < 0:
2206                 raise ValueError(
2207                     "expected positive integer as indexer "
2208                     "for dimension %r, found %s" % (k, v)
2209                 )
2210             elif v == 0:
2211                 raise ValueError("step cannot be zero")
2212         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}
2213         return self.isel(indexers_slices)
2214 
2215     def broadcast_like(
2216         self, other: Union["Dataset", "DataArray"], exclude: Iterable[Hashable] = None
2217     ) -> "Dataset":
2218         """Broadcast this DataArray against another Dataset or DataArray.
2219         This is equivalent to xr.broadcast(other, self)[1]
2220 
2221         Parameters
2222         ----------
2223         other : Dataset or DataArray
2224             Object against which to broadcast this array.
2225         exclude : iterable of hashable, optional
2226             Dimensions that must not be broadcasted
2227 
2228         """
2229         if exclude is None:
2230             exclude = set()
2231         else:
2232             exclude = set(exclude)
2233         args = align(other, self, join="outer", copy=False, exclude=exclude)
2234 
2235         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
2236 
2237         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
2238 
2239     def reindex_like(
2240         self,
2241         other: Union["Dataset", "DataArray"],
2242         method: str = None,
2243         tolerance: Number = None,
2244         copy: bool = True,
2245         fill_value: Any = dtypes.NA,
2246     ) -> "Dataset":
2247         """Conform this object onto the indexes of another object, filling in
2248         missing values with ``fill_value``. The default fill value is NaN.
2249 
2250         Parameters
2251         ----------
2252         other : Dataset or DataArray
2253             Object with an 'indexes' attribute giving a mapping from dimension
2254             names to pandas.Index objects, which provides coordinates upon
2255             which to index the variables in this dataset. The indexes on this
2256             other object need not be the same as the indexes on this
2257             dataset. Any mis-matched index values will be filled in with
2258             NaN, and any mis-matched dimension names will simply be ignored.
2259         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
2260             Method to use for filling index values from other not found in this
2261             dataset:
2262 
2263             * None (default): don't fill gaps
2264             * pad / ffill: propagate last valid index value forward
2265             * backfill / bfill: propagate next valid index value backward
2266             * nearest: use nearest valid index value
2267         tolerance : optional
2268             Maximum distance between original and new labels for inexact
2269             matches. The values of the index at the matching locations must
2270             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2271         copy : bool, optional
2272             If ``copy=True``, data in the return value is always copied. If
2273             ``copy=False`` and reindexing is unnecessary, or can be performed
2274             with only slice operations, then the output may share memory with
2275             the input. In either case, a new xarray object is always returned.
2276         fill_value : scalar, optional
2277             Value to use for newly missing values
2278 
2279         Returns
2280         -------
2281         reindexed : Dataset
2282             Another dataset, with this dataset's data but coordinates from the
2283             other object.
2284 
2285         See Also
2286         --------
2287         Dataset.reindex
2288         align
2289         """
2290         indexers = alignment.reindex_like_indexers(self, other)
2291         return self.reindex(
2292             indexers=indexers,
2293             method=method,
2294             copy=copy,
2295             fill_value=fill_value,
2296             tolerance=tolerance,
2297         )
2298 
2299     def reindex(
2300         self,
2301         indexers: Mapping[Hashable, Any] = None,
2302         method: str = None,
2303         tolerance: Number = None,
2304         copy: bool = True,
2305         fill_value: Any = dtypes.NA,
2306         **indexers_kwargs: Any,
2307     ) -> "Dataset":
2308         """Conform this object onto a new set of indexes, filling in
2309         missing values with ``fill_value``. The default fill value is NaN.
2310 
2311         Parameters
2312         ----------
2313         indexers : dict. optional
2314             Dictionary with keys given by dimension names and values given by
2315             arrays of coordinates tick labels. Any mis-matched coordinate
2316             values will be filled in with NaN, and any mis-matched dimension
2317             names will simply be ignored.
2318             One of indexers or indexers_kwargs must be provided.
2319         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
2320             Method to use for filling index values in ``indexers`` not found in
2321             this dataset:
2322 
2323             * None (default): don't fill gaps
2324             * pad / ffill: propagate last valid index value forward
2325             * backfill / bfill: propagate next valid index value backward
2326             * nearest: use nearest valid index value
2327         tolerance : optional
2328             Maximum distance between original and new labels for inexact
2329             matches. The values of the index at the matching locations must
2330             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2331         copy : bool, optional
2332             If ``copy=True``, data in the return value is always copied. If
2333             ``copy=False`` and reindexing is unnecessary, or can be performed
2334             with only slice operations, then the output may share memory with
2335             the input. In either case, a new xarray object is always returned.
2336         fill_value : scalar, optional
2337             Value to use for newly missing values
2338         sparse: use sparse-array. By default, False
2339         **indexers_kwargs : {dim: indexer, ...}, optional
2340             Keyword arguments in the same form as ``indexers``.
2341             One of indexers or indexers_kwargs must be provided.
2342 
2343         Returns
2344         -------
2345         reindexed : Dataset
2346             Another dataset, with this dataset's data but replaced coordinates.
2347 
2348         See Also
2349         --------
2350         Dataset.reindex_like
2351         align
2352         pandas.Index.get_indexer
2353 
2354         Examples
2355         --------
2356 
2357         Create a dataset with some fictional data.
2358 
2359         >>> import xarray as xr
2360         >>> import pandas as pd
2361         >>> x = xr.Dataset(
2362         ...     {
2363         ...         "temperature": ("station", 20 * np.random.rand(4)),
2364         ...         "pressure": ("station", 500 * np.random.rand(4)),
2365         ...     },
2366         ...     coords={"station": ["boston", "nyc", "seattle", "denver"]},
2367         ... )
2368         >>> x
2369         <xarray.Dataset>
2370         Dimensions:      (station: 4)
2371         Coordinates:
2372         * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'
2373         Data variables:
2374             temperature  (station) float64 18.84 14.59 19.22 17.16
2375             pressure     (station) float64 324.1 194.3 122.8 244.3
2376         >>> x.indexes
2377         station: Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')
2378 
2379         Create a new index and reindex the dataset. By default values in the new index that
2380         do not have corresponding records in the dataset are assigned `NaN`.
2381 
2382         >>> new_index = ["boston", "austin", "seattle", "lincoln"]
2383         >>> x.reindex({"station": new_index})
2384         <xarray.Dataset>
2385         Dimensions:      (station: 4)
2386         Coordinates:
2387         * station      (station) object 'boston' 'austin' 'seattle' 'lincoln'
2388         Data variables:
2389             temperature  (station) float64 18.84 nan 19.22 nan
2390             pressure     (station) float64 324.1 nan 122.8 nan
2391 
2392         We can fill in the missing values by passing a value to the keyword `fill_value`.
2393 
2394         >>> x.reindex({"station": new_index}, fill_value=0)
2395         <xarray.Dataset>
2396         Dimensions:      (station: 4)
2397         Coordinates:
2398         * station      (station) object 'boston' 'austin' 'seattle' 'lincoln'
2399         Data variables:
2400             temperature  (station) float64 18.84 0.0 19.22 0.0
2401             pressure     (station) float64 324.1 0.0 122.8 0.0
2402 
2403         Because the index is not monotonically increasing or decreasing, we cannot use arguments
2404         to the keyword method to fill the `NaN` values.
2405 
2406         >>> x.reindex({"station": new_index}, method="nearest")
2407         Traceback (most recent call last):
2408         ...
2409             raise ValueError('index must be monotonic increasing or decreasing')
2410         ValueError: index must be monotonic increasing or decreasing
2411 
2412         To further illustrate the filling functionality in reindex, we will create a
2413         dataset with a monotonically increasing index (for example, a sequence of dates).
2414 
2415         >>> x2 = xr.Dataset(
2416         ...     {
2417         ...         "temperature": (
2418         ...             "time",
2419         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],
2420         ...         ),
2421         ...         "pressure": ("time", 500 * np.random.rand(6)),
2422         ...     },
2423         ...     coords={"time": pd.date_range("01/01/2019", periods=6, freq="D")},
2424         ... )
2425         >>> x2
2426         <xarray.Dataset>
2427         Dimensions:      (time: 6)
2428         Coordinates:
2429         * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06
2430         Data variables:
2431             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12
2432             pressure     (time) float64 103.4 122.7 452.0 444.0 399.2 486.0
2433 
2434         Suppose we decide to expand the dataset to cover a wider date range.
2435 
2436         >>> time_index2 = pd.date_range("12/29/2018", periods=10, freq="D")
2437         >>> x2.reindex({"time": time_index2})
2438         <xarray.Dataset>
2439         Dimensions:      (time: 10)
2440         Coordinates:
2441         * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
2442         Data variables:
2443             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan
2444             pressure     (time) float64 nan nan nan 103.4 ... 444.0 399.2 486.0 nan
2445 
2446         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)
2447         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.
2448 
2449         For example, to back-propagate the last valid value to fill the `NaN` values,
2450         pass `bfill` as an argument to the `method` keyword.
2451 
2452         >>> x3 = x2.reindex({"time": time_index2}, method="bfill")
2453         >>> x3
2454         <xarray.Dataset>
2455         Dimensions:      (time: 10)
2456         Coordinates:
2457         * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
2458         Data variables:
2459             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan
2460             pressure     (time) float64 103.4 103.4 103.4 103.4 ... 399.2 486.0 nan
2461 
2462         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)
2463         will not be filled by any of the value propagation schemes.
2464 
2465         >>> x2.where(x2.temperature.isnull(), drop=True)
2466         <xarray.Dataset>
2467         Dimensions:      (time: 1)
2468         Coordinates:
2469         * time         (time) datetime64[ns] 2019-01-03
2470         Data variables:
2471             temperature  (time) float64 nan
2472             pressure     (time) float64 452.0
2473         >>> x3.where(x3.temperature.isnull(), drop=True)
2474         <xarray.Dataset>
2475         Dimensions:      (time: 2)
2476         Coordinates:
2477         * time         (time) datetime64[ns] 2019-01-03 2019-01-07
2478         Data variables:
2479             temperature  (time) float64 nan nan
2480             pressure     (time) float64 452.0 nan
2481 
2482         This is because filling while reindexing does not look at dataset values, but only compares
2483         the original and desired indexes. If you do want to fill in the `NaN` values present in the
2484         original dataset, use the :py:meth:`~Dataset.fillna()` method.
2485 
2486         """
2487         return self._reindex(
2488             indexers,
2489             method,
2490             tolerance,
2491             copy,
2492             fill_value,
2493             sparse=False,
2494             **indexers_kwargs,
2495         )
2496 
2497     def _reindex(
2498         self,
2499         indexers: Mapping[Hashable, Any] = None,
2500         method: str = None,
2501         tolerance: Number = None,
2502         copy: bool = True,
2503         fill_value: Any = dtypes.NA,
2504         sparse: bool = False,
2505         **indexers_kwargs: Any,
2506     ) -> "Dataset":
2507         """
2508         same to _reindex but support sparse option
2509         """
2510         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
2511 
2512         bad_dims = [d for d in indexers if d not in self.dims]
2513         if bad_dims:
2514             raise ValueError("invalid reindex dimensions: %s" % bad_dims)
2515 
2516         variables, indexes = alignment.reindex_variables(
2517             self.variables,
2518             self.sizes,
2519             self.indexes,
2520             indexers,
2521             method,
2522             tolerance,
2523             copy=copy,
2524             fill_value=fill_value,
2525             sparse=sparse,
2526         )
2527         coord_names = set(self._coord_names)
2528         coord_names.update(indexers)
2529         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2530 
2531     def interp(
2532         self,
2533         coords: Mapping[Hashable, Any] = None,
2534         method: str = "linear",
2535         assume_sorted: bool = False,
2536         kwargs: Mapping[str, Any] = None,
2537         **coords_kwargs: Any,
2538     ) -> "Dataset":
2539         """ Multidimensional interpolation of Dataset.
2540 
2541         Parameters
2542         ----------
2543         coords : dict, optional
2544             Mapping from dimension names to the new coordinates.
2545             New coordinate can be a scalar, array-like or DataArray.
2546             If DataArrays are passed as new coordates, their dimensions are
2547             used for the broadcasting.
2548         method: string, optional.
2549             {'linear', 'nearest'} for multidimensional array,
2550             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
2551             for 1-dimensional array. 'linear' is used by default.
2552         assume_sorted: boolean, optional
2553             If False, values of coordinates that are interpolated over can be
2554             in any order and they are sorted first. If True, interpolated
2555             coordinates are assumed to be an array of monotonically increasing
2556             values.
2557         kwargs: dictionary, optional
2558             Additional keyword passed to scipy's interpolator.
2559         **coords_kwargs : {dim: coordinate, ...}, optional
2560             The keyword arguments form of ``coords``.
2561             One of coords or coords_kwargs must be provided.
2562 
2563         Returns
2564         -------
2565         interpolated: xr.Dataset
2566             New dataset on the new coordinates.
2567 
2568         Notes
2569         -----
2570         scipy is required.
2571 
2572         See Also
2573         --------
2574         scipy.interpolate.interp1d
2575         scipy.interpolate.interpn
2576         """
2577         from . import missing
2578 
2579         if kwargs is None:
2580             kwargs = {}
2581 
2582         coords = either_dict_or_kwargs(coords, coords_kwargs, "interp")
2583         indexers = dict(self._validate_interp_indexers(coords))
2584 
2585         if coords:
2586             # This avoids broadcasting over coordinates that are both in
2587             # the original array AND in the indexing array. It essentially
2588             # forces interpolation along the shared coordinates.
2589             sdims = (
2590                 set(self.dims)
2591                 .intersection(*[set(nx.dims) for nx in indexers.values()])
2592                 .difference(coords.keys())
2593             )
2594             indexers.update({d: self.variables[d] for d in sdims})
2595 
2596         obj = self if assume_sorted else self.sortby([k for k in coords])
2597 
2598         def maybe_variable(obj, k):
2599             # workaround to get variable for dimension without coordinate.
2600             try:
2601                 return obj._variables[k]
2602             except KeyError:
2603                 return as_variable((k, range(obj.dims[k])))
2604 
2605         def _validate_interp_indexer(x, new_x):
2606             # In the case of datetimes, the restrictions placed on indexers
2607             # used with interp are stronger than those which are placed on
2608             # isel, so we need an additional check after _validate_indexers.
2609             if _contains_datetime_like_objects(
2610                 x
2611             ) and not _contains_datetime_like_objects(new_x):
2612                 raise TypeError(
2613                     "When interpolating over a datetime-like "
2614                     "coordinate, the coordinates to "
2615                     "interpolate to must be either datetime "
2616                     "strings or datetimes. "
2617                     "Instead got\n{}".format(new_x)
2618                 )
2619             return x, new_x
2620 
2621         variables: Dict[Hashable, Variable] = {}
2622         for name, var in obj._variables.items():
2623             if name in indexers:
2624                 continue
2625 
2626             if var.dtype.kind in "uifc":
2627                 var_indexers = {
2628                     k: _validate_interp_indexer(maybe_variable(obj, k), v)
2629                     for k, v in indexers.items()
2630                     if k in var.dims
2631                 }
2632                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)
2633             elif all(d not in indexers for d in var.dims):
2634                 # keep unrelated object array
2635                 variables[name] = var
2636 
2637         coord_names = obj._coord_names & variables.keys()
2638         indexes = {k: v for k, v in obj.indexes.items() if k not in indexers}
2639         selected = self._replace_with_new_dims(
2640             variables.copy(), coord_names, indexes=indexes
2641         )
2642 
2643         # attach indexer as coordinate
2644         variables.update(indexers)
2645         for k, v in indexers.items():
2646             assert isinstance(v, Variable)
2647             if v.dims == (k,):
2648                 indexes[k] = v.to_index()
2649 
2650         # Extract coordinates from indexers
2651         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)
2652         variables.update(coord_vars)
2653         indexes.update(new_indexes)
2654 
2655         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()
2656         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2657 
2658     def interp_like(
2659         self,
2660         other: Union["Dataset", "DataArray"],
2661         method: str = "linear",
2662         assume_sorted: bool = False,
2663         kwargs: Mapping[str, Any] = None,
2664     ) -> "Dataset":
2665         """Interpolate this object onto the coordinates of another object,
2666         filling the out of range values with NaN.
2667 
2668         Parameters
2669         ----------
2670         other : Dataset or DataArray
2671             Object with an 'indexes' attribute giving a mapping from dimension
2672             names to an 1d array-like, which provides coordinates upon
2673             which to index the variables in this dataset.
2674         method: string, optional.
2675             {'linear', 'nearest'} for multidimensional array,
2676             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
2677             for 1-dimensional array. 'linear' is used by default.
2678         assume_sorted: boolean, optional
2679             If False, values of coordinates that are interpolated over can be
2680             in any order and they are sorted first. If True, interpolated
2681             coordinates are assumed to be an array of monotonically increasing
2682             values.
2683         kwargs: dictionary, optional
2684             Additional keyword passed to scipy's interpolator.
2685 
2686         Returns
2687         -------
2688         interpolated: xr.Dataset
2689             Another dataset by interpolating this dataset's data along the
2690             coordinates of the other object.
2691 
2692         Notes
2693         -----
2694         scipy is required.
2695         If the dataset has object-type coordinates, reindex is used for these
2696         coordinates instead of the interpolation.
2697 
2698         See Also
2699         --------
2700         Dataset.interp
2701         Dataset.reindex_like
2702         """
2703         if kwargs is None:
2704             kwargs = {}
2705         coords = alignment.reindex_like_indexers(self, other)
2706 
2707         numeric_coords: Dict[Hashable, pd.Index] = {}
2708         object_coords: Dict[Hashable, pd.Index] = {}
2709         for k, v in coords.items():
2710             if v.dtype.kind in "uifcMm":
2711                 numeric_coords[k] = v
2712             else:
2713                 object_coords[k] = v
2714 
2715         ds = self
2716         if object_coords:
2717             # We do not support interpolation along object coordinate.
2718             # reindex instead.
2719             ds = self.reindex(object_coords)
2720         return ds.interp(numeric_coords, method, assume_sorted, kwargs)
2721 
2722     # Helper methods for rename()
2723     def _rename_vars(self, name_dict, dims_dict):
2724         variables = {}
2725         coord_names = set()
2726         for k, v in self.variables.items():
2727             var = v.copy(deep=False)
2728             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
2729             name = name_dict.get(k, k)
2730             if name in variables:
2731                 raise ValueError(f"the new name {name!r} conflicts")
2732             variables[name] = var
2733             if k in self._coord_names:
2734                 coord_names.add(name)
2735         return variables, coord_names
2736 
2737     def _rename_dims(self, name_dict):
2738         return {name_dict.get(k, k): v for k, v in self.dims.items()}
2739 
2740     def _rename_indexes(self, name_dict, dims_set):
2741         if self._indexes is None:
2742             return None
2743         indexes = {}
2744         for k, v in self.indexes.items():
2745             new_name = name_dict.get(k, k)
2746             if new_name not in dims_set:
2747                 continue
2748             if isinstance(v, pd.MultiIndex):
2749                 new_names = [name_dict.get(k, k) for k in v.names]
2750                 index = v.rename(names=new_names)
2751             else:
2752                 index = v.rename(new_name)
2753             indexes[new_name] = index
2754         return indexes
2755 
2756     def _rename_all(self, name_dict, dims_dict):
2757         variables, coord_names = self._rename_vars(name_dict, dims_dict)
2758         dims = self._rename_dims(dims_dict)
2759         indexes = self._rename_indexes(name_dict, dims.keys())
2760         return variables, coord_names, dims, indexes
2761 
2762     def rename(
2763         self,
2764         name_dict: Mapping[Hashable, Hashable] = None,
2765         inplace: bool = None,
2766         **names: Hashable,
2767     ) -> "Dataset":
2768         """Returns a new object with renamed variables and dimensions.
2769 
2770         Parameters
2771         ----------
2772         name_dict : dict-like, optional
2773             Dictionary whose keys are current variable or dimension names and
2774             whose values are the desired names.
2775         **names, optional
2776             Keyword form of ``name_dict``.
2777             One of name_dict or names must be provided.
2778 
2779         Returns
2780         -------
2781         renamed : Dataset
2782             Dataset with renamed variables and dimensions.
2783 
2784         See Also
2785         --------
2786         Dataset.swap_dims
2787         Dataset.rename_vars
2788         Dataset.rename_dims
2789         DataArray.rename
2790         """
2791         _check_inplace(inplace)
2792         name_dict = either_dict_or_kwargs(name_dict, names, "rename")
2793         for k in name_dict.keys():
2794             if k not in self and k not in self.dims:
2795                 raise ValueError(
2796                     "cannot rename %r because it is not a "
2797                     "variable or dimension in this dataset" % k
2798                 )
2799 
2800         variables, coord_names, dims, indexes = self._rename_all(
2801             name_dict=name_dict, dims_dict=name_dict
2802         )
2803         assert_unique_multiindex_level_names(variables)
2804         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
2805 
2806     def rename_dims(
2807         self, dims_dict: Mapping[Hashable, Hashable] = None, **dims: Hashable
2808     ) -> "Dataset":
2809         """Returns a new object with renamed dimensions only.
2810 
2811         Parameters
2812         ----------
2813         dims_dict : dict-like, optional
2814             Dictionary whose keys are current dimension names and
2815             whose values are the desired names. The desired names must
2816             not be the name of an existing dimension or Variable in the Dataset.
2817         **dims, optional
2818             Keyword form of ``dims_dict``.
2819             One of dims_dict or dims must be provided.
2820 
2821         Returns
2822         -------
2823         renamed : Dataset
2824             Dataset with renamed dimensions.
2825 
2826         See Also
2827         --------
2828         Dataset.swap_dims
2829         Dataset.rename
2830         Dataset.rename_vars
2831         DataArray.rename
2832         """
2833         dims_dict = either_dict_or_kwargs(dims_dict, dims, "rename_dims")
2834         for k, v in dims_dict.items():
2835             if k not in self.dims:
2836                 raise ValueError(
2837                     "cannot rename %r because it is not a "
2838                     "dimension in this dataset" % k
2839                 )
2840             if v in self.dims or v in self:
2841                 raise ValueError(
2842                     f"Cannot rename {k} to {v} because {v} already exists. "
2843                     "Try using swap_dims instead."
2844                 )
2845 
2846         variables, coord_names, sizes, indexes = self._rename_all(
2847             name_dict={}, dims_dict=dims_dict
2848         )
2849         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)
2850 
2851     def rename_vars(
2852         self, name_dict: Mapping[Hashable, Hashable] = None, **names: Hashable
2853     ) -> "Dataset":
2854         """Returns a new object with renamed variables including coordinates
2855 
2856         Parameters
2857         ----------
2858         name_dict : dict-like, optional
2859             Dictionary whose keys are current variable or coordinate names and
2860             whose values are the desired names.
2861         **names, optional
2862             Keyword form of ``name_dict``.
2863             One of name_dict or names must be provided.
2864 
2865         Returns
2866         -------
2867         renamed : Dataset
2868             Dataset with renamed variables including coordinates
2869 
2870         See Also
2871         --------
2872         Dataset.swap_dims
2873         Dataset.rename
2874         Dataset.rename_dims
2875         DataArray.rename
2876         """
2877         name_dict = either_dict_or_kwargs(name_dict, names, "rename_vars")
2878         for k in name_dict:
2879             if k not in self:
2880                 raise ValueError(
2881                     "cannot rename %r because it is not a "
2882                     "variable or coordinate in this dataset" % k
2883                 )
2884         variables, coord_names, dims, indexes = self._rename_all(
2885             name_dict=name_dict, dims_dict={}
2886         )
2887         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
2888 
2889     def swap_dims(
2890         self, dims_dict: Mapping[Hashable, Hashable], inplace: bool = None
2891     ) -> "Dataset":
2892         """Returns a new object with swapped dimensions.
2893 
2894         Parameters
2895         ----------
2896         dims_dict : dict-like
2897             Dictionary whose keys are current dimension names and whose values
2898             are new names.
2899 
2900         Returns
2901         -------
2902         swapped : Dataset
2903             Dataset with swapped dimensions.
2904 
2905         Examples
2906         --------
2907         >>> ds = xr.Dataset(
2908         ...     data_vars={"a": ("x", [5, 7]), "b": ("x", [0.1, 2.4])},
2909         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
2910         ... )
2911         >>> ds
2912         <xarray.Dataset>
2913         Dimensions:  (x: 2)
2914         Coordinates:
2915           * x        (x) <U1 'a' 'b'
2916             y        (x) int64 0 1
2917         Data variables:
2918             a        (x) int64 5 7
2919             b        (x) float64 0.1 2.4
2920 
2921         >>> ds.swap_dims({"x": "y"})
2922         <xarray.Dataset>
2923         Dimensions:  (y: 2)
2924         Coordinates:
2925             x        (y) <U1 'a' 'b'
2926           * y        (y) int64 0 1
2927         Data variables:
2928             a        (y) int64 5 7
2929             b        (y) float64 0.1 2.4
2930 
2931         >>> ds.swap_dims({"x": "z"})
2932         <xarray.Dataset>
2933         Dimensions:  (z: 2)
2934         Coordinates:
2935             x        (z) <U1 'a' 'b'
2936             y        (z) int64 0 1
2937         Dimensions without coordinates: z
2938         Data variables:
2939             a        (z) int64 5 7
2940             b        (z) float64 0.1 2.4
2941 
2942         See Also
2943         --------
2944 
2945         Dataset.rename
2946         DataArray.swap_dims
2947         """
2948         # TODO: deprecate this method in favor of a (less confusing)
2949         # rename_dims() method that only renames dimensions.
2950         _check_inplace(inplace)
2951         for k, v in dims_dict.items():
2952             if k not in self.dims:
2953                 raise ValueError(
2954                     "cannot swap from dimension %r because it is "
2955                     "not an existing dimension" % k
2956                 )
2957             if v in self.variables and self.variables[v].dims != (k,):
2958                 raise ValueError(
2959                     "replacement dimension %r is not a 1D "
2960                     "variable along the old dimension %r" % (v, k)
2961                 )
2962 
2963         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}
2964 
2965         coord_names = self._coord_names.copy()
2966         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})
2967 
2968         variables: Dict[Hashable, Variable] = {}
2969         indexes: Dict[Hashable, pd.Index] = {}
2970         for k, v in self.variables.items():
2971             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
2972             if k in result_dims:
2973                 var = v.to_index_variable()
2974                 if k in self.indexes:
2975                     indexes[k] = self.indexes[k]
2976                 else:
2977                     new_index = var.to_index()
2978                     if new_index.nlevels == 1:
2979                         # make sure index name matches dimension name
2980                         new_index = new_index.rename(k)
2981                     indexes[k] = new_index
2982             else:
2983                 var = v.to_base_variable()
2984             var.dims = dims
2985             variables[k] = var
2986 
2987         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2988 
2989     def expand_dims(
2990         self,
2991         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,
2992         axis: Union[None, int, Sequence[int]] = None,
2993         **dim_kwargs: Any,
2994     ) -> "Dataset":
2995         """Return a new object with an additional axis (or axes) inserted at
2996         the corresponding position in the array shape.  The new object is a
2997         view into the underlying array, not a copy.
2998 
2999         If dim is already a scalar coordinate, it will be promoted to a 1D
3000         coordinate consisting of a single value.
3001 
3002         Parameters
3003         ----------
3004         dim : hashable, sequence of hashable, mapping, or None
3005             Dimensions to include on the new variable. If provided as hashable
3006             or sequence of hashable, then dimensions are inserted with length
3007             1. If provided as a mapping, then the keys are the new dimensions
3008             and the values are either integers (giving the length of the new
3009             dimensions) or array-like (giving the coordinates of the new
3010             dimensions).
3011         axis : integer, sequence of integers, or None
3012             Axis position(s) where new axis is to be inserted (position(s) on
3013             the result array). If a list (or tuple) of integers is passed,
3014             multiple axes are inserted. In this case, dim arguments should be
3015             same length list. If axis=None is passed, all the axes will be
3016             inserted to the start of the result array.
3017         **dim_kwargs : int or sequence/ndarray
3018             The keywords are arbitrary dimensions being inserted and the values
3019             are either the lengths of the new dims (if int is given), or their
3020             coordinates. Note, this is an alternative to passing a dict to the
3021             dim kwarg and will only be used if dim is None.
3022 
3023         Returns
3024         -------
3025         expanded : same type as caller
3026             This object, but with an additional dimension(s).
3027         """
3028         if dim is None:
3029             pass
3030         elif isinstance(dim, Mapping):
3031             # We're later going to modify dim in place; don't tamper with
3032             # the input
3033             dim = dict(dim)
3034         elif isinstance(dim, int):
3035             raise TypeError(
3036                 "dim should be hashable or sequence of hashables or mapping"
3037             )
3038         elif isinstance(dim, str) or not isinstance(dim, Sequence):
3039             dim = {dim: 1}
3040         elif isinstance(dim, Sequence):
3041             if len(dim) != len(set(dim)):
3042                 raise ValueError("dims should not contain duplicate values.")
3043             dim = {d: 1 for d in dim}
3044 
3045         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
3046         assert isinstance(dim, MutableMapping)
3047 
3048         if axis is None:
3049             axis = list(range(len(dim)))
3050         elif not isinstance(axis, Sequence):
3051             axis = [axis]
3052 
3053         if len(dim) != len(axis):
3054             raise ValueError("lengths of dim and axis should be identical.")
3055         for d in dim:
3056             if d in self.dims:
3057                 raise ValueError(f"Dimension {d} already exists.")
3058             if d in self._variables and not utils.is_scalar(self._variables[d]):
3059                 raise ValueError(
3060                     "{dim} already exists as coordinate or"
3061                     " variable name.".format(dim=d)
3062                 )
3063 
3064         variables: Dict[Hashable, Variable] = {}
3065         coord_names = self._coord_names.copy()
3066         # If dim is a dict, then ensure that the values are either integers
3067         # or iterables.
3068         for k, v in dim.items():
3069             if hasattr(v, "__iter__"):
3070                 # If the value for the new dimension is an iterable, then
3071                 # save the coordinates to the variables dict, and set the
3072                 # value within the dim dict to the length of the iterable
3073                 # for later use.
3074                 variables[k] = xr.IndexVariable((k,), v)
3075                 coord_names.add(k)
3076                 dim[k] = variables[k].size
3077             elif isinstance(v, int):
3078                 pass  # Do nothing if the dimensions value is just an int
3079             else:
3080                 raise TypeError(
3081                     "The value of new dimension {k} must be "
3082                     "an iterable or an int".format(k=k)
3083                 )
3084 
3085         for k, v in self._variables.items():
3086             if k not in dim:
3087                 if k in coord_names:  # Do not change coordinates
3088                     variables[k] = v
3089                 else:
3090                     result_ndim = len(v.dims) + len(axis)
3091                     for a in axis:
3092                         if a < -result_ndim or result_ndim - 1 < a:
3093                             raise IndexError(
3094                                 f"Axis {a} of variable {k} is out of bounds of the "
3095                                 f"expanded dimension size {result_ndim}"
3096                             )
3097 
3098                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]
3099                     if len(axis_pos) != len(set(axis_pos)):
3100                         raise ValueError("axis should not contain duplicate values")
3101                     # We need to sort them to make sure `axis` equals to the
3102                     # axis positions of the result array.
3103                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))
3104 
3105                     all_dims = list(zip(v.dims, v.shape))
3106                     for d, c in zip_axis_dim:
3107                         all_dims.insert(d, c)
3108                     variables[k] = v.set_dims(dict(all_dims))
3109             else:
3110                 # If dims includes a label of a non-dimension coordinate,
3111                 # it will be promoted to a 1D coordinate with a single value.
3112                 variables[k] = v.set_dims(k).to_index_variable()
3113 
3114         new_dims = self._dims.copy()
3115         new_dims.update(dim)
3116 
3117         return self._replace_vars_and_dims(
3118             variables, dims=new_dims, coord_names=coord_names
3119         )
3120 
3121     def set_index(
3122         self,
3123         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
3124         append: bool = False,
3125         inplace: bool = None,
3126         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],
3127     ) -> "Dataset":
3128         """Set Dataset (multi-)indexes using one or more existing coordinates
3129         or variables.
3130 
3131         Parameters
3132         ----------
3133         indexes : {dim: index, ...}
3134             Mapping from names matching dimensions and values given
3135             by (lists of) the names of existing coordinates or variables to set
3136             as new (multi-)index.
3137         append : bool, optional
3138             If True, append the supplied index(es) to the existing index(es).
3139             Otherwise replace the existing index(es) (default).
3140         **indexes_kwargs: optional
3141             The keyword arguments form of ``indexes``.
3142             One of indexes or indexes_kwargs must be provided.
3143 
3144         Returns
3145         -------
3146         obj : Dataset
3147             Another dataset, with this dataset's data but replaced coordinates.
3148 
3149         Examples
3150         --------
3151         >>> arr = xr.DataArray(
3152         ...     data=np.ones((2, 3)),
3153         ...     dims=["x", "y"],
3154         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
3155         ... )
3156         >>> ds = xr.Dataset({"v": arr})
3157         >>> ds
3158         <xarray.Dataset>
3159         Dimensions:  (x: 2, y: 3)
3160         Coordinates:
3161           * x        (x) int64 0 1
3162           * y        (y) int64 0 1 2
3163             a        (x) int64 3 4
3164         Data variables:
3165             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
3166         >>> ds.set_index(x="a")
3167         <xarray.Dataset>
3168         Dimensions:  (x: 2, y: 3)
3169         Coordinates:
3170           * x        (x) int64 3 4
3171           * y        (y) int64 0 1 2
3172         Data variables:
3173             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
3174 
3175         See Also
3176         --------
3177         Dataset.reset_index
3178         Dataset.swap_dims
3179         """
3180         _check_inplace(inplace)
3181         indexes = either_dict_or_kwargs(indexes, indexes_kwargs, "set_index")
3182         variables, coord_names = merge_indexes(
3183             indexes, self._variables, self._coord_names, append=append
3184         )
3185         return self._replace_vars_and_dims(variables, coord_names=coord_names)
3186 
3187     def reset_index(
3188         self,
3189         dims_or_levels: Union[Hashable, Sequence[Hashable]],
3190         drop: bool = False,
3191         inplace: bool = None,
3192     ) -> "Dataset":
3193         """Reset the specified index(es) or multi-index level(s).
3194 
3195         Parameters
3196         ----------
3197         dims_or_levels : str or list
3198             Name(s) of the dimension(s) and/or multi-index level(s) that will
3199             be reset.
3200         drop : bool, optional
3201             If True, remove the specified indexes and/or multi-index levels
3202             instead of extracting them as new coordinates (default: False).
3203 
3204         Returns
3205         -------
3206         obj : Dataset
3207             Another dataset, with this dataset's data but replaced coordinates.
3208 
3209         See Also
3210         --------
3211         Dataset.set_index
3212         """
3213         _check_inplace(inplace)
3214         variables, coord_names = split_indexes(
3215             dims_or_levels,
3216             self._variables,
3217             self._coord_names,
3218             cast(Mapping[Hashable, Hashable], self._level_coords),
3219             drop=drop,
3220         )
3221         return self._replace_vars_and_dims(variables, coord_names=coord_names)
3222 
3223     def reorder_levels(
3224         self,
3225         dim_order: Mapping[Hashable, Sequence[int]] = None,
3226         inplace: bool = None,
3227         **dim_order_kwargs: Sequence[int],
3228     ) -> "Dataset":
3229         """Rearrange index levels using input order.
3230 
3231         Parameters
3232         ----------
3233         dim_order : optional
3234             Mapping from names matching dimensions and values given
3235             by lists representing new level orders. Every given dimension
3236             must have a multi-index.
3237         **dim_order_kwargs: optional
3238             The keyword arguments form of ``dim_order``.
3239             One of dim_order or dim_order_kwargs must be provided.
3240 
3241         Returns
3242         -------
3243         obj : Dataset
3244             Another dataset, with this dataset's data but replaced
3245             coordinates.
3246         """
3247         _check_inplace(inplace)
3248         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
3249         variables = self._variables.copy()
3250         indexes = dict(self.indexes)
3251         for dim, order in dim_order.items():
3252             coord = self._variables[dim]
3253             index = self.indexes[dim]
3254             if not isinstance(index, pd.MultiIndex):
3255                 raise ValueError(f"coordinate {dim} has no MultiIndex")
3256             new_index = index.reorder_levels(order)
3257             variables[dim] = IndexVariable(coord.dims, new_index)
3258             indexes[dim] = new_index
3259 
3260         return self._replace(variables, indexes=indexes)
3261 
3262     def _stack_once(self, dims, new_dim):
3263         variables = {}
3264         for name, var in self.variables.items():
3265             if name not in dims:
3266                 if any(d in var.dims for d in dims):
3267                     add_dims = [d for d in dims if d not in var.dims]
3268                     vdims = list(var.dims) + add_dims
3269                     shape = [self.dims[d] for d in vdims]
3270                     exp_var = var.set_dims(vdims, shape)
3271                     stacked_var = exp_var.stack(**{new_dim: dims})
3272                     variables[name] = stacked_var
3273                 else:
3274                     variables[name] = var.copy(deep=False)
3275 
3276         # consider dropping levels that are unused?
3277         levels = [self.get_index(dim) for dim in dims]
3278         idx = utils.multiindex_from_product_levels(levels, names=dims)
3279         variables[new_dim] = IndexVariable(new_dim, idx)
3280 
3281         coord_names = set(self._coord_names) - set(dims) | {new_dim}
3282 
3283         indexes = {k: v for k, v in self.indexes.items() if k not in dims}
3284         indexes[new_dim] = idx
3285 
3286         return self._replace_with_new_dims(
3287             variables, coord_names=coord_names, indexes=indexes
3288         )
3289 
3290     def stack(
3291         self,
3292         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
3293         **dimensions_kwargs: Sequence[Hashable],
3294     ) -> "Dataset":
3295         """
3296         Stack any number of existing dimensions into a single new dimension.
3297 
3298         New dimensions will be added at the end, and the corresponding
3299         coordinate variables will be combined into a MultiIndex.
3300 
3301         Parameters
3302         ----------
3303         dimensions : Mapping of the form new_name=(dim1, dim2, ...)
3304             Names of new dimensions, and the existing dimensions that they
3305             replace.
3306         **dimensions_kwargs:
3307             The keyword arguments form of ``dimensions``.
3308             One of dimensions or dimensions_kwargs must be provided.
3309 
3310         Returns
3311         -------
3312         stacked : Dataset
3313             Dataset with stacked data.
3314 
3315         See also
3316         --------
3317         Dataset.unstack
3318         """
3319         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
3320         result = self
3321         for new_dim, dims in dimensions.items():
3322             result = result._stack_once(dims, new_dim)
3323         return result
3324 
3325     def to_stacked_array(
3326         self,
3327         new_dim: Hashable,
3328         sample_dims: Sequence[Hashable],
3329         variable_dim: str = "variable",
3330         name: Hashable = None,
3331     ) -> "DataArray":
3332         """Combine variables of differing dimensionality into a DataArray
3333         without broadcasting.
3334 
3335         This method is similar to Dataset.to_array but does not broadcast the
3336         variables.
3337 
3338         Parameters
3339         ----------
3340         new_dim : Hashable
3341             Name of the new stacked coordinate
3342         sample_dims : Sequence[Hashable]
3343             Dimensions that **will not** be stacked. Each array in the dataset
3344             must share these dimensions. For machine learning applications,
3345             these define the dimensions over which samples are drawn.
3346         variable_dim : str, optional
3347             Name of the level in the stacked coordinate which corresponds to
3348             the variables.
3349         name : str, optional
3350             Name of the new data array.
3351 
3352         Returns
3353         -------
3354         stacked : DataArray
3355             DataArray with the specified dimensions and data variables
3356             stacked together. The stacked coordinate is named ``new_dim``
3357             and represented by a MultiIndex object with a level containing the
3358             data variable names. The name of this level is controlled using
3359             the ``variable_dim`` argument.
3360 
3361         See Also
3362         --------
3363         Dataset.to_array
3364         Dataset.stack
3365         DataArray.to_unstacked_dataset
3366 
3367         Examples
3368         --------
3369         >>> data = xr.Dataset(
3370         ...     data_vars={
3371         ...         "a": (("x", "y"), [[0, 1, 2], [3, 4, 5]]),
3372         ...         "b": ("x", [6, 7]),
3373         ...     },
3374         ...     coords={"y": ["u", "v", "w"]},
3375         ... )
3376 
3377         >>> data
3378         <xarray.Dataset>
3379         Dimensions:  (x: 2, y: 3)
3380         Coordinates:
3381         * y        (y) <U1 'u' 'v' 'w'
3382         Dimensions without coordinates: x
3383         Data variables:
3384             a        (x, y) int64 0 1 2 3 4 5
3385             b        (x) int64 6 7
3386 
3387         >>> data.to_stacked_array("z", sample_dims=["x"])
3388         <xarray.DataArray (x: 2, z: 4)>
3389         array([[0, 1, 2, 6],
3390             [3, 4, 5, 7]])
3391         Coordinates:
3392         * z         (z) MultiIndex
3393         - variable  (z) object 'a' 'a' 'a' 'b'
3394         - y         (z) object 'u' 'v' 'w' nan
3395         Dimensions without coordinates: x
3396 
3397         """
3398         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)
3399 
3400         for variable in self:
3401             dims = self[variable].dims
3402             dims_include_sample_dims = set(sample_dims) <= set(dims)
3403             if not dims_include_sample_dims:
3404                 raise ValueError(
3405                     "All variables in the dataset must contain the "
3406                     "dimensions {}.".format(dims)
3407                 )
3408 
3409         def ensure_stackable(val):
3410             assign_coords = {variable_dim: val.name}
3411             for dim in stacking_dims:
3412                 if dim not in val.dims:
3413                     assign_coords[dim] = None
3414 
3415             expand_dims = set(stacking_dims).difference(set(val.dims))
3416             expand_dims.add(variable_dim)
3417             # must be list for .expand_dims
3418             expand_dims = list(expand_dims)
3419 
3420             return (
3421                 val.assign_coords(**assign_coords)
3422                 .expand_dims(expand_dims)
3423                 .stack({new_dim: (variable_dim,) + stacking_dims})
3424             )
3425 
3426         # concatenate the arrays
3427         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]
3428         data_array = xr.concat(stackable_vars, dim=new_dim)
3429 
3430         # coerce the levels of the MultiIndex to have the same type as the
3431         # input dimensions. This code is messy, so it might be better to just
3432         # input a dummy value for the singleton dimension.
3433         idx = data_array.indexes[new_dim]
3434         levels = [idx.levels[0]] + [
3435             level.astype(self[level.name].dtype) for level in idx.levels[1:]
3436         ]
3437         new_idx = idx.set_levels(levels)
3438         data_array[new_dim] = IndexVariable(new_dim, new_idx)
3439 
3440         if name is not None:
3441             data_array.name = name
3442 
3443         return data_array
3444 
3445     def _unstack_once(self, dim: Hashable, fill_value, sparse) -> "Dataset":
3446         index = self.get_index(dim)
3447         index = remove_unused_levels_categories(index)
3448         full_idx = pd.MultiIndex.from_product(index.levels, names=index.names)
3449 
3450         # take a shortcut in case the MultiIndex was not modified.
3451         if index.equals(full_idx):
3452             obj = self
3453         else:
3454             obj = self._reindex(
3455                 {dim: full_idx}, copy=False, fill_value=fill_value, sparse=sparse
3456             )
3457 
3458         new_dim_names = index.names
3459         new_dim_sizes = [lev.size for lev in index.levels]
3460 
3461         variables: Dict[Hashable, Variable] = {}
3462         indexes = {k: v for k, v in self.indexes.items() if k != dim}
3463 
3464         for name, var in obj.variables.items():
3465             if name != dim:
3466                 if dim in var.dims:
3467                     new_dims = dict(zip(new_dim_names, new_dim_sizes))
3468                     variables[name] = var.unstack({dim: new_dims})
3469                 else:
3470                     variables[name] = var
3471 
3472         for name, lev in zip(new_dim_names, index.levels):
3473             variables[name] = IndexVariable(name, lev)
3474             indexes[name] = lev
3475 
3476         coord_names = set(self._coord_names) - {dim} | set(new_dim_names)
3477 
3478         return self._replace_with_new_dims(
3479             variables, coord_names=coord_names, indexes=indexes
3480         )
3481 
3482     def unstack(
3483         self,
3484         dim: Union[Hashable, Iterable[Hashable]] = None,
3485         fill_value: Any = dtypes.NA,
3486         sparse: bool = False,
3487     ) -> "Dataset":
3488         """
3489         Unstack existing dimensions corresponding to MultiIndexes into
3490         multiple new dimensions.
3491 
3492         New dimensions will be added at the end.
3493 
3494         Parameters
3495         ----------
3496         dim : Hashable or iterable of Hashable, optional
3497             Dimension(s) over which to unstack. By default unstacks all
3498             MultiIndexes.
3499         fill_value: value to be filled. By default, np.nan
3500         sparse: use sparse-array if True
3501 
3502         Returns
3503         -------
3504         unstacked : Dataset
3505             Dataset with unstacked data.
3506 
3507         See also
3508         --------
3509         Dataset.stack
3510         """
3511         if dim is None:
3512             dims = [
3513                 d for d in self.dims if isinstance(self.get_index(d), pd.MultiIndex)
3514             ]
3515         else:
3516             if isinstance(dim, str) or not isinstance(dim, Iterable):
3517                 dims = [dim]
3518             else:
3519                 dims = list(dim)
3520 
3521             missing_dims = [d for d in dims if d not in self.dims]
3522             if missing_dims:
3523                 raise ValueError(
3524                     "Dataset does not contain the dimensions: %s" % missing_dims
3525                 )
3526 
3527             non_multi_dims = [
3528                 d for d in dims if not isinstance(self.get_index(d), pd.MultiIndex)
3529             ]
3530             if non_multi_dims:
3531                 raise ValueError(
3532                     "cannot unstack dimensions that do not "
3533                     "have a MultiIndex: %s" % non_multi_dims
3534                 )
3535 
3536         result = self.copy(deep=False)
3537         for dim in dims:
3538             result = result._unstack_once(dim, fill_value, sparse)
3539         return result
3540 
3541     def update(self, other: "CoercibleMapping", inplace: bool = None) -> "Dataset":
3542         """Update this dataset's variables with those from another dataset.
3543 
3544         Parameters
3545         ----------
3546         other : Dataset or castable to Dataset
3547             Variables with which to update this dataset. One of:
3548 
3549             - Dataset
3550             - mapping {var name: DataArray}
3551             - mapping {var name: Variable}
3552             - mapping {var name: (dimension name, array-like)}
3553             - mapping {var name: (tuple of dimension names, array-like)}
3554 
3555 
3556         Returns
3557         -------
3558         updated : Dataset
3559             Updated dataset.
3560 
3561         Raises
3562         ------
3563         ValueError
3564             If any dimensions would have inconsistent sizes in the updated
3565             dataset.
3566         """
3567         _check_inplace(inplace)
3568         merge_result = dataset_update_method(self, other)
3569         return self._replace(inplace=True, **merge_result._asdict())
3570 
3571     def merge(
3572         self,
3573         other: Union["CoercibleMapping", "DataArray"],
3574         inplace: bool = None,
3575         overwrite_vars: Union[Hashable, Iterable[Hashable]] = frozenset(),
3576         compat: str = "no_conflicts",
3577         join: str = "outer",
3578         fill_value: Any = dtypes.NA,
3579     ) -> "Dataset":
3580         """Merge the arrays of two datasets into a single dataset.
3581 
3582         This method generally does not allow for overriding data, with the
3583         exception of attributes, which are ignored on the second dataset.
3584         Variables with the same name are checked for conflicts via the equals
3585         or identical methods.
3586 
3587         Parameters
3588         ----------
3589         other : Dataset or castable to Dataset
3590             Dataset or variables to merge with this dataset.
3591         overwrite_vars : Hashable or iterable of Hashable, optional
3592             If provided, update variables of these name(s) without checking for
3593             conflicts in this dataset.
3594         compat : {'broadcast_equals', 'equals', 'identical',
3595                   'no_conflicts'}, optional
3596             String indicating how to compare variables of the same name for
3597             potential conflicts:
3598 
3599             - 'broadcast_equals': all values must be equal when variables are
3600               broadcast against each other to ensure common dimensions.
3601             - 'equals': all values and dimensions must be the same.
3602             - 'identical': all values, dimensions and attributes must be the
3603               same.
3604             - 'no_conflicts': only values which are not null in both datasets
3605               must be equal. The returned dataset then contains the combination
3606               of all non-null values.
3607 
3608         join : {'outer', 'inner', 'left', 'right', 'exact'}, optional
3609             Method for joining ``self`` and ``other`` along shared dimensions:
3610 
3611             - 'outer': use the union of the indexes
3612             - 'inner': use the intersection of the indexes
3613             - 'left': use indexes from ``self``
3614             - 'right': use indexes from ``other``
3615             - 'exact': error instead of aligning non-equal indexes
3616         fill_value: scalar, optional
3617             Value to use for newly missing values
3618 
3619         Returns
3620         -------
3621         merged : Dataset
3622             Merged dataset.
3623 
3624         Raises
3625         ------
3626         MergeError
3627             If any variables conflict (see ``compat``).
3628         """
3629         _check_inplace(inplace)
3630         other = other.to_dataset() if isinstance(other, xr.DataArray) else other
3631         merge_result = dataset_merge_method(
3632             self,
3633             other,
3634             overwrite_vars=overwrite_vars,
3635             compat=compat,
3636             join=join,
3637             fill_value=fill_value,
3638         )
3639         return self._replace(**merge_result._asdict())
3640 
3641     def _assert_all_in_dataset(
3642         self, names: Iterable[Hashable], virtual_okay: bool = False
3643     ) -> None:
3644         bad_names = set(names) - set(self._variables)
3645         if virtual_okay:
3646             bad_names -= self.virtual_variables
3647         if bad_names:
3648             raise ValueError(
3649                 "One or more of the specified variables "
3650                 "cannot be found in this dataset"
3651             )
3652 
3653     def drop_vars(
3654         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
3655     ) -> "Dataset":
3656         """Drop variables from this dataset.
3657 
3658         Parameters
3659         ----------
3660         names : hashable or iterable of hashables
3661             Name(s) of variables to drop.
3662         errors: {'raise', 'ignore'}, optional
3663             If 'raise' (default), raises a ValueError error if any of the variable
3664             passed are not in the dataset. If 'ignore', any given names that are in the
3665             dataset are dropped and no error is raised.
3666 
3667         Returns
3668         -------
3669         dropped : Dataset
3670 
3671         """
3672         # the Iterable check is required for mypy
3673         if is_scalar(names) or not isinstance(names, Iterable):
3674             names = {names}
3675         else:
3676             names = set(names)
3677         if errors == "raise":
3678             self._assert_all_in_dataset(names)
3679 
3680         variables = {k: v for k, v in self._variables.items() if k not in names}
3681         coord_names = {k for k in self._coord_names if k in variables}
3682         indexes = {k: v for k, v in self.indexes.items() if k not in names}
3683         return self._replace_with_new_dims(
3684             variables, coord_names=coord_names, indexes=indexes
3685         )
3686 
3687     def drop(self, labels=None, dim=None, *, errors="raise", **labels_kwargs):
3688         """Backward compatible method based on `drop_vars` and `drop_sel`
3689 
3690         Using either `drop_vars` or `drop_sel` is encouraged
3691 
3692         See Also
3693         --------
3694         Dataset.drop_vars
3695         Dataset.drop_sel
3696         """
3697         if errors not in ["raise", "ignore"]:
3698             raise ValueError('errors must be either "raise" or "ignore"')
3699 
3700         if is_dict_like(labels) and not isinstance(labels, dict):
3701             warnings.warn(
3702                 "dropping coordinates using `drop` is be deprecated; use drop_vars.",
3703                 FutureWarning,
3704                 stacklevel=2,
3705             )
3706             return self.drop_vars(labels, errors=errors)
3707 
3708         if labels_kwargs or isinstance(labels, dict):
3709             if dim is not None:
3710                 raise ValueError("cannot specify dim and dict-like arguments.")
3711             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
3712 
3713         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):
3714             warnings.warn(
3715                 "dropping variables using `drop` will be deprecated; using drop_vars is encouraged.",
3716                 PendingDeprecationWarning,
3717                 stacklevel=2,
3718             )
3719             return self.drop_vars(labels, errors=errors)
3720         if dim is not None:
3721             warnings.warn(
3722                 "dropping labels using list-like labels is deprecated; using "
3723                 "dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).",
3724                 DeprecationWarning,
3725                 stacklevel=2,
3726             )
3727             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)
3728 
3729         warnings.warn(
3730             "dropping labels using `drop` will be deprecated; using drop_sel is encouraged.",
3731             PendingDeprecationWarning,
3732             stacklevel=2,
3733         )
3734         return self.drop_sel(labels, errors=errors)
3735 
3736     def drop_sel(self, labels=None, *, errors="raise", **labels_kwargs):
3737         """Drop index labels from this dataset.
3738 
3739         Parameters
3740         ----------
3741         labels : Mapping[Hashable, Any]
3742             Index labels to drop
3743         errors: {'raise', 'ignore'}, optional
3744             If 'raise' (default), raises a ValueError error if
3745             any of the index labels passed are not
3746             in the dataset. If 'ignore', any given labels that are in the
3747             dataset are dropped and no error is raised.
3748         **labels_kwargs : {dim: label, ...}, optional
3749             The keyword arguments form of ``dim`` and ``labels``
3750 
3751         Returns
3752         -------
3753         dropped : Dataset
3754 
3755         Examples
3756         --------
3757         >>> data = np.random.randn(2, 3)
3758         >>> labels = ["a", "b", "c"]
3759         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
3760         >>> ds.drop_sel(y=["a", "c"])
3761         <xarray.Dataset>
3762         Dimensions:  (x: 2, y: 1)
3763         Coordinates:
3764           * y        (y) <U1 'b'
3765         Dimensions without coordinates: x
3766         Data variables:
3767             A        (x, y) float64 -0.3454 0.1734
3768         >>> ds.drop_sel(y="b")
3769         <xarray.Dataset>
3770         Dimensions:  (x: 2, y: 2)
3771         Coordinates:
3772           * y        (y) <U1 'a' 'c'
3773         Dimensions without coordinates: x
3774         Data variables:
3775             A        (x, y) float64 -0.3944 -1.418 1.423 -1.041
3776         """
3777         if errors not in ["raise", "ignore"]:
3778             raise ValueError('errors must be either "raise" or "ignore"')
3779 
3780         labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
3781 
3782         ds = self
3783         for dim, labels_for_dim in labels.items():
3784             # Don't cast to set, as it would harm performance when labels
3785             # is a large numpy array
3786             if utils.is_scalar(labels_for_dim):
3787                 labels_for_dim = [labels_for_dim]
3788             labels_for_dim = np.asarray(labels_for_dim)
3789             try:
3790                 index = self.indexes[dim]
3791             except KeyError:
3792                 raise ValueError("dimension %r does not have coordinate labels" % dim)
3793             new_index = index.drop(labels_for_dim, errors=errors)
3794             ds = ds.loc[{dim: new_index}]
3795         return ds
3796 
3797     def drop_dims(
3798         self, drop_dims: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
3799     ) -> "Dataset":
3800         """Drop dimensions and associated variables from this dataset.
3801 
3802         Parameters
3803         ----------
3804         drop_dims : hashable or iterable of hashable
3805             Dimension or dimensions to drop.
3806         errors: {'raise', 'ignore'}, optional
3807             If 'raise' (default), raises a ValueError error if any of the
3808             dimensions passed are not in the dataset. If 'ignore', any given
3809             labels that are in the dataset are dropped and no error is raised.
3810 
3811         Returns
3812         -------
3813         obj : Dataset
3814             The dataset without the given dimensions (or any variables
3815             containing those dimensions)
3816         errors: {'raise', 'ignore'}, optional
3817             If 'raise' (default), raises a ValueError error if
3818             any of the dimensions passed are not
3819             in the dataset. If 'ignore', any given dimensions that are in the
3820             dataset are dropped and no error is raised.
3821         """
3822         if errors not in ["raise", "ignore"]:
3823             raise ValueError('errors must be either "raise" or "ignore"')
3824 
3825         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):
3826             drop_dims = {drop_dims}
3827         else:
3828             drop_dims = set(drop_dims)
3829 
3830         if errors == "raise":
3831             missing_dims = drop_dims - set(self.dims)
3832             if missing_dims:
3833                 raise ValueError(
3834                     "Dataset does not contain the dimensions: %s" % missing_dims
3835                 )
3836 
3837         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}
3838         return self.drop_vars(drop_vars)
3839 
3840     def transpose(self, *dims: Hashable) -> "Dataset":
3841         """Return a new Dataset object with all array dimensions transposed.
3842 
3843         Although the order of dimensions on each array will change, the dataset
3844         dimensions themselves will remain in fixed (sorted) order.
3845 
3846         Parameters
3847         ----------
3848         *dims : Hashable, optional
3849             By default, reverse the dimensions on each array. Otherwise,
3850             reorder the dimensions to this order.
3851 
3852         Returns
3853         -------
3854         transposed : Dataset
3855             Each array in the dataset (including) coordinates will be
3856             transposed to the given order.
3857 
3858         Notes
3859         -----
3860         This operation returns a view of each array's data. It is
3861         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
3862         -- the data will be fully loaded into memory.
3863 
3864         See Also
3865         --------
3866         numpy.transpose
3867         DataArray.transpose
3868         """
3869         if dims:
3870             if set(dims) ^ set(self.dims) and ... not in dims:
3871                 raise ValueError(
3872                     "arguments to transpose (%s) must be "
3873                     "permuted dataset dimensions (%s)" % (dims, tuple(self.dims))
3874                 )
3875         ds = self.copy()
3876         for name, var in self._variables.items():
3877             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))
3878             ds._variables[name] = var.transpose(*var_dims)
3879         return ds
3880 
3881     def dropna(
3882         self,
3883         dim: Hashable,
3884         how: str = "any",
3885         thresh: int = None,
3886         subset: Iterable[Hashable] = None,
3887     ):
3888         """Returns a new dataset with dropped labels for missing values along
3889         the provided dimension.
3890 
3891         Parameters
3892         ----------
3893         dim : Hashable
3894             Dimension along which to drop missing values. Dropping along
3895             multiple dimensions simultaneously is not yet supported.
3896         how : {'any', 'all'}, optional
3897             * any : if any NA values are present, drop that label
3898             * all : if all values are NA, drop that label
3899         thresh : int, default None
3900             If supplied, require this many non-NA values.
3901         subset : iterable of hashable, optional
3902             Which variables to check for missing values. By default, all
3903             variables in the dataset are checked.
3904 
3905         Returns
3906         -------
3907         Dataset
3908         """
3909         # TODO: consider supporting multiple dimensions? Or not, given that
3910         # there are some ugly edge cases, e.g., pandas's dropna differs
3911         # depending on the order of the supplied axes.
3912 
3913         if dim not in self.dims:
3914             raise ValueError("%s must be a single dataset dimension" % dim)
3915 
3916         if subset is None:
3917             subset = iter(self.data_vars)
3918 
3919         count = np.zeros(self.dims[dim], dtype=np.int64)
3920         size = 0
3921 
3922         for k in subset:
3923             array = self._variables[k]
3924             if dim in array.dims:
3925                 dims = [d for d in array.dims if d != dim]
3926                 count += np.asarray(array.count(dims))  # type: ignore
3927                 size += np.prod([self.dims[d] for d in dims])
3928 
3929         if thresh is not None:
3930             mask = count >= thresh
3931         elif how == "any":
3932             mask = count == size
3933         elif how == "all":
3934             mask = count > 0
3935         elif how is not None:
3936             raise ValueError("invalid how option: %s" % how)
3937         else:
3938             raise TypeError("must specify how or thresh")
3939 
3940         return self.isel({dim: mask})
3941 
3942     def fillna(self, value: Any) -> "Dataset":
3943         """Fill missing values in this object.
3944 
3945         This operation follows the normal broadcasting and alignment rules that
3946         xarray uses for binary arithmetic, except the result is aligned to this
3947         object (``join='left'``) instead of aligned to the intersection of
3948         index coordinates (``join='inner'``).
3949 
3950         Parameters
3951         ----------
3952         value : scalar, ndarray, DataArray, dict or Dataset
3953             Used to fill all matching missing values in this dataset's data
3954             variables. Scalars, ndarrays or DataArrays arguments are used to
3955             fill all data with aligned coordinates (for DataArrays).
3956             Dictionaries or datasets match data variables and then align
3957             coordinates if necessary.
3958 
3959         Returns
3960         -------
3961         Dataset
3962 
3963         Examples
3964         --------
3965 
3966         >>> import numpy as np
3967         >>> import xarray as xr
3968         >>> ds = xr.Dataset(
3969         ...     {
3970         ...         "A": ("x", [np.nan, 2, np.nan, 0]),
3971         ...         "B": ("x", [3, 4, np.nan, 1]),
3972         ...         "C": ("x", [np.nan, np.nan, np.nan, 5]),
3973         ...         "D": ("x", [np.nan, 3, np.nan, 4]),
3974         ...     },
3975         ...     coords={"x": [0, 1, 2, 3]},
3976         ... )
3977         >>> ds
3978         <xarray.Dataset>
3979         Dimensions:  (x: 4)
3980         Coordinates:
3981         * x        (x) int64 0 1 2 3
3982         Data variables:
3983             A        (x) float64 nan 2.0 nan 0.0
3984             B        (x) float64 3.0 4.0 nan 1.0
3985             C        (x) float64 nan nan nan 5.0
3986             D        (x) float64 nan 3.0 nan 4.0
3987 
3988         Replace all `NaN` values with 0s.
3989 
3990         >>> ds.fillna(0)
3991         <xarray.Dataset>
3992         Dimensions:  (x: 4)
3993         Coordinates:
3994         * x        (x) int64 0 1 2 3
3995         Data variables:
3996             A        (x) float64 0.0 2.0 0.0 0.0
3997             B        (x) float64 3.0 4.0 0.0 1.0
3998             C        (x) float64 0.0 0.0 0.0 5.0
3999             D        (x) float64 0.0 3.0 0.0 4.0
4000 
4001         Replace all `NaN` elements in column A, B, C, and D, with 0, 1, 2, and 3 respectively.
4002 
4003         >>> values = {"A": 0, "B": 1, "C": 2, "D": 3}
4004         >>> ds.fillna(value=values)
4005         <xarray.Dataset>
4006         Dimensions:  (x: 4)
4007         Coordinates:
4008         * x        (x) int64 0 1 2 3
4009         Data variables:
4010             A        (x) float64 0.0 2.0 0.0 0.0
4011             B        (x) float64 3.0 4.0 1.0 1.0
4012             C        (x) float64 2.0 2.0 2.0 5.0
4013             D        (x) float64 3.0 3.0 3.0 4.0
4014         """
4015         if utils.is_dict_like(value):
4016             value_keys = getattr(value, "data_vars", value).keys()
4017             if not set(value_keys) <= set(self.data_vars.keys()):
4018                 raise ValueError(
4019                     "all variables in the argument to `fillna` "
4020                     "must be contained in the original dataset"
4021                 )
4022         out = ops.fillna(self, value)
4023         return out
4024 
4025     def interpolate_na(
4026         self,
4027         dim: Hashable = None,
4028         method: str = "linear",
4029         limit: int = None,
4030         use_coordinate: Union[bool, Hashable] = True,
4031         max_gap: Union[
4032             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta
4033         ] = None,
4034         **kwargs: Any,
4035     ) -> "Dataset":
4036         """Fill in NaNs by interpolating according to different methods.
4037 
4038         Parameters
4039         ----------
4040         dim : str
4041             Specifies the dimension along which to interpolate.
4042 
4043         method : str, optional
4044             String indicating which method to use for interpolation:
4045 
4046             - 'linear': linear interpolation (Default). Additional keyword
4047               arguments are passed to :py:func:`numpy.interp`
4048             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
4049               are passed to :py:func:`scipy.interpolate.interp1d`. If
4050               ``method='polynomial'``, the ``order`` keyword argument must also be
4051               provided.
4052             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
4053               respective :py:class:`scipy.interpolate` classes.
4054 
4055         use_coordinate : bool, str, default True
4056             Specifies which index to use as the x values in the interpolation
4057             formulated as `y = f(x)`. If False, values are treated as if
4058             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
4059             used. If ``use_coordinate`` is a string, it specifies the name of a
4060             coordinate variariable to use as the index.
4061         limit : int, default None
4062             Maximum number of consecutive NaNs to fill. Must be greater than 0
4063             or None for no limit. This filling is done regardless of the size of
4064             the gap in the data. To only interpolate over gaps less than a given length,
4065             see ``max_gap``.
4066         max_gap: int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default None.
4067             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
4068             Use None for no limit. When interpolating along a datetime64 dimension
4069             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
4070 
4071             - a string that is valid input for pandas.to_timedelta
4072             - a :py:class:`numpy.timedelta64` object
4073             - a :py:class:`pandas.Timedelta` object
4074             - a :py:class:`datetime.timedelta` object
4075 
4076             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
4077             dimensions has not been implemented yet. Gap length is defined as the difference
4078             between coordinate values at the first data point after a gap and the last value
4079             before a gap. For gaps at the beginning (end), gap length is defined as the difference
4080             between coordinate values at the first (last) valid data point and the first (last) NaN.
4081             For example, consider::
4082 
4083                 <xarray.DataArray (x: 9)>
4084                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
4085                 Coordinates:
4086                   * x        (x) int64 0 1 2 3 4 5 6 7 8
4087 
4088             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
4089         kwargs : dict, optional
4090             parameters passed verbatim to the underlying interpolation function
4091 
4092         Returns
4093         -------
4094         interpolated: Dataset
4095             Filled in Dataset.
4096 
4097         See also
4098         --------
4099         numpy.interp
4100         scipy.interpolate
4101         """
4102         from .missing import interp_na, _apply_over_vars_with_dim
4103 
4104         new = _apply_over_vars_with_dim(
4105             interp_na,
4106             self,
4107             dim=dim,
4108             method=method,
4109             limit=limit,
4110             use_coordinate=use_coordinate,
4111             max_gap=max_gap,
4112             **kwargs,
4113         )
4114         return new
4115 
4116     def ffill(self, dim: Hashable, limit: int = None) -> "Dataset":
4117         """Fill NaN values by propogating values forward
4118 
4119         *Requires bottleneck.*
4120 
4121         Parameters
4122         ----------
4123         dim : Hashable
4124             Specifies the dimension along which to propagate values when
4125             filling.
4126         limit : int, default None
4127             The maximum number of consecutive NaN values to forward fill. In
4128             other words, if there is a gap with more than this number of
4129             consecutive NaNs, it will only be partially filled. Must be greater
4130             than 0 or None for no limit.
4131 
4132         Returns
4133         -------
4134         Dataset
4135         """
4136         from .missing import ffill, _apply_over_vars_with_dim
4137 
4138         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)
4139         return new
4140 
4141     def bfill(self, dim: Hashable, limit: int = None) -> "Dataset":
4142         """Fill NaN values by propogating values backward
4143 
4144         *Requires bottleneck.*
4145 
4146         Parameters
4147         ----------
4148         dim : str
4149             Specifies the dimension along which to propagate values when
4150             filling.
4151         limit : int, default None
4152             The maximum number of consecutive NaN values to backward fill. In
4153             other words, if there is a gap with more than this number of
4154             consecutive NaNs, it will only be partially filled. Must be greater
4155             than 0 or None for no limit.
4156 
4157         Returns
4158         -------
4159         Dataset
4160         """
4161         from .missing import bfill, _apply_over_vars_with_dim
4162 
4163         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)
4164         return new
4165 
4166     def combine_first(self, other: "Dataset") -> "Dataset":
4167         """Combine two Datasets, default to data_vars of self.
4168 
4169         The new coordinates follow the normal broadcasting and alignment rules
4170         of ``join='outer'``.  Vacant cells in the expanded coordinates are
4171         filled with np.nan.
4172 
4173         Parameters
4174         ----------
4175         other : Dataset
4176             Used to fill all matching missing values in this array.
4177 
4178         Returns
4179         -------
4180         Dataset
4181         """
4182         out = ops.fillna(self, other, join="outer", dataset_join="outer")
4183         return out
4184 
4185     def reduce(
4186         self,
4187         func: Callable,
4188         dim: Union[Hashable, Iterable[Hashable]] = None,
4189         keep_attrs: bool = None,
4190         keepdims: bool = False,
4191         numeric_only: bool = False,
4192         allow_lazy: bool = None,
4193         **kwargs: Any,
4194     ) -> "Dataset":
4195         """Reduce this dataset by applying `func` along some dimension(s).
4196 
4197         Parameters
4198         ----------
4199         func : callable
4200             Function which can be called in the form
4201             `f(x, axis=axis, **kwargs)` to return the result of reducing an
4202             np.ndarray over an integer valued axis.
4203         dim : str or sequence of str, optional
4204             Dimension(s) over which to apply `func`.  By default `func` is
4205             applied over all dimensions.
4206         keep_attrs : bool, optional
4207             If True, the dataset's attributes (`attrs`) will be copied from
4208             the original object to the new one.  If False (default), the new
4209             object will be returned without attributes.
4210         keepdims : bool, default False
4211             If True, the dimensions which are reduced are left in the result
4212             as dimensions of size one. Coordinates that use these dimensions
4213             are removed.
4214         numeric_only : bool, optional
4215             If True, only apply ``func`` to variables with a numeric dtype.
4216         **kwargs : Any
4217             Additional keyword arguments passed on to ``func``.
4218 
4219         Returns
4220         -------
4221         reduced : Dataset
4222             Dataset with this object's DataArrays replaced with new DataArrays
4223             of summarized data and the indicated dimension(s) removed.
4224         """
4225         if dim is None or dim is ...:
4226             dims = set(self.dims)
4227         elif isinstance(dim, str) or not isinstance(dim, Iterable):
4228             dims = {dim}
4229         else:
4230             dims = set(dim)
4231 
4232         missing_dimensions = [d for d in dims if d not in self.dims]
4233         if missing_dimensions:
4234             raise ValueError(
4235                 "Dataset does not contain the dimensions: %s" % missing_dimensions
4236             )
4237 
4238         if keep_attrs is None:
4239             keep_attrs = _get_keep_attrs(default=False)
4240 
4241         variables: Dict[Hashable, Variable] = {}
4242         for name, var in self._variables.items():
4243             reduce_dims = [d for d in var.dims if d in dims]
4244             if name in self.coords:
4245                 if not reduce_dims:
4246                     variables[name] = var
4247             else:
4248                 if (
4249                     not numeric_only
4250                     or np.issubdtype(var.dtype, np.number)
4251                     or (var.dtype == np.bool_)
4252                 ):
4253                     if len(reduce_dims) == 1:
4254                         # unpack dimensions for the benefit of functions
4255                         # like np.argmin which can't handle tuple arguments
4256                         (reduce_dims,) = reduce_dims
4257                     elif len(reduce_dims) == var.ndim:
4258                         # prefer to aggregate over axis=None rather than
4259                         # axis=(0, 1) if they will be equivalent, because
4260                         # the former is often more efficient
4261                         reduce_dims = None  # type: ignore
4262                     variables[name] = var.reduce(
4263                         func,
4264                         dim=reduce_dims,
4265                         keep_attrs=keep_attrs,
4266                         keepdims=keepdims,
4267                         allow_lazy=allow_lazy,
4268                         **kwargs,
4269                     )
4270 
4271         coord_names = {k for k in self.coords if k in variables}
4272         indexes = {k: v for k, v in self.indexes.items() if k in variables}
4273         attrs = self.attrs if keep_attrs else None
4274         return self._replace_with_new_dims(
4275             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
4276         )
4277 
4278     def map(
4279         self,
4280         func: Callable,
4281         keep_attrs: bool = None,
4282         args: Iterable[Any] = (),
4283         **kwargs: Any,
4284     ) -> "Dataset":
4285         """Apply a function to each variable in this dataset
4286 
4287         Parameters
4288         ----------
4289         func : callable
4290             Function which can be called in the form `func(x, *args, **kwargs)`
4291             to transform each DataArray `x` in this dataset into another
4292             DataArray.
4293         keep_attrs : bool, optional
4294             If True, the dataset's attributes (`attrs`) will be copied from
4295             the original object to the new one. If False, the new object will
4296             be returned without attributes.
4297         args : tuple, optional
4298             Positional arguments passed on to `func`.
4299         **kwargs : Any
4300             Keyword arguments passed on to `func`.
4301 
4302         Returns
4303         -------
4304         applied : Dataset
4305             Resulting dataset from applying ``func`` to each data variable.
4306 
4307         Examples
4308         --------
4309         >>> da = xr.DataArray(np.random.randn(2, 3))
4310         >>> ds = xr.Dataset({"foo": da, "bar": ("x", [-1, 2])})
4311         >>> ds
4312         <xarray.Dataset>
4313         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
4314         Dimensions without coordinates: dim_0, dim_1, x
4315         Data variables:
4316             foo      (dim_0, dim_1) float64 -0.3751 -1.951 -1.945 0.2948 0.711 -0.3948
4317             bar      (x) int64 -1 2
4318         >>> ds.map(np.fabs)
4319         <xarray.Dataset>
4320         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
4321         Dimensions without coordinates: dim_0, dim_1, x
4322         Data variables:
4323             foo      (dim_0, dim_1) float64 0.3751 1.951 1.945 0.2948 0.711 0.3948
4324             bar      (x) float64 1.0 2.0
4325         """
4326         variables = {
4327             k: maybe_wrap_array(v, func(v, *args, **kwargs))
4328             for k, v in self.data_vars.items()
4329         }
4330         if keep_attrs is None:
4331             keep_attrs = _get_keep_attrs(default=False)
4332         attrs = self.attrs if keep_attrs else None
4333         return type(self)(variables, attrs=attrs)
4334 
4335     def apply(
4336         self,
4337         func: Callable,
4338         keep_attrs: bool = None,
4339         args: Iterable[Any] = (),
4340         **kwargs: Any,
4341     ) -> "Dataset":
4342         """
4343         Backward compatible implementation of ``map``
4344 
4345         See Also
4346         --------
4347         Dataset.map
4348         """
4349         warnings.warn(
4350             "Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged",
4351             PendingDeprecationWarning,
4352             stacklevel=2,
4353         )
4354         return self.map(func, keep_attrs, args, **kwargs)
4355 
4356     def assign(
4357         self, variables: Mapping[Hashable, Any] = None, **variables_kwargs: Hashable
4358     ) -> "Dataset":
4359         """Assign new data variables to a Dataset, returning a new object
4360         with all the original variables in addition to the new ones.
4361 
4362         Parameters
4363         ----------
4364         variables : mapping, value pairs
4365             Mapping from variables names to the new values. If the new values
4366             are callable, they are computed on the Dataset and assigned to new
4367             data variables. If the values are not callable, (e.g. a DataArray,
4368             scalar, or array), they are simply assigned.
4369         **variables_kwargs:
4370             The keyword arguments form of ``variables``.
4371             One of variables or variables_kwargs must be provided.
4372 
4373         Returns
4374         -------
4375         ds : Dataset
4376             A new Dataset with the new variables in addition to all the
4377             existing variables.
4378 
4379         Notes
4380         -----
4381         Since ``kwargs`` is a dictionary, the order of your arguments may not
4382         be preserved, and so the order of the new variables is not well
4383         defined. Assigning multiple variables within the same ``assign`` is
4384         possible, but you cannot reference other variables created within the
4385         same ``assign`` call.
4386 
4387         See Also
4388         --------
4389         pandas.DataFrame.assign
4390 
4391         Examples
4392         --------
4393         >>> import numpy as np
4394         >>> import xarray as xr
4395         >>> x = xr.Dataset(
4396         ...     {
4397         ...         "temperature_c": (
4398         ...             ("lat", "lon"),
4399         ...             20 * np.random.rand(4).reshape(2, 2),
4400         ...         ),
4401         ...         "precipitation": (("lat", "lon"), np.random.rand(4).reshape(2, 2)),
4402         ...     },
4403         ...     coords={"lat": [10, 20], "lon": [150, 160]},
4404         ... )
4405         >>> x
4406         <xarray.Dataset>
4407         Dimensions:        (lat: 2, lon: 2)
4408         Coordinates:
4409         * lat            (lat) int64 10 20
4410         * lon            (lon) int64 150 160
4411         Data variables:
4412             temperature_c  (lat, lon) float64 18.04 12.51 17.64 9.313
4413             precipitation  (lat, lon) float64 0.4751 0.6827 0.3697 0.03524
4414 
4415         Where the value is a callable, evaluated on dataset:
4416 
4417         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)
4418         <xarray.Dataset>
4419         Dimensions:        (lat: 2, lon: 2)
4420         Coordinates:
4421         * lat            (lat) int64 10 20
4422         * lon            (lon) int64 150 160
4423         Data variables:
4424             temperature_c  (lat, lon) float64 18.04 12.51 17.64 9.313
4425             precipitation  (lat, lon) float64 0.4751 0.6827 0.3697 0.03524
4426             temperature_f  (lat, lon) float64 64.47 54.51 63.75 48.76
4427 
4428         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:
4429 
4430         >>> x.assign(temperature_f=x["temperature_c"] * 9 / 5 + 32)
4431         <xarray.Dataset>
4432         Dimensions:        (lat: 2, lon: 2)
4433         Coordinates:
4434         * lat            (lat) int64 10 20
4435         * lon            (lon) int64 150 160
4436         Data variables:
4437             temperature_c  (lat, lon) float64 18.04 12.51 17.64 9.313
4438             precipitation  (lat, lon) float64 0.4751 0.6827 0.3697 0.03524
4439             temperature_f  (lat, lon) float64 64.47 54.51 63.75 48.76
4440 
4441         """
4442         variables = either_dict_or_kwargs(variables, variables_kwargs, "assign")
4443         data = self.copy()
4444         # do all calculations first...
4445         results = data._calc_assign_results(variables)
4446         # ... and then assign
4447         data.update(results)
4448         return data
4449 
4450     def to_array(self, dim="variable", name=None):
4451         """Convert this dataset into an xarray.DataArray
4452 
4453         The data variables of this dataset will be broadcast against each other
4454         and stacked along the first axis of the new array. All coordinates of
4455         this dataset will remain coordinates.
4456 
4457         Parameters
4458         ----------
4459         dim : str, optional
4460             Name of the new dimension.
4461         name : str, optional
4462             Name of the new data array.
4463 
4464         Returns
4465         -------
4466         array : xarray.DataArray
4467         """
4468         from .dataarray import DataArray
4469 
4470         data_vars = [self.variables[k] for k in self.data_vars]
4471         broadcast_vars = broadcast_variables(*data_vars)
4472         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)
4473 
4474         coords = dict(self.coords)
4475         coords[dim] = list(self.data_vars)
4476         indexes = propagate_indexes(self._indexes)
4477 
4478         dims = (dim,) + broadcast_vars[0].dims
4479 
4480         return DataArray(
4481             data, coords, dims, attrs=self.attrs, name=name, indexes=indexes
4482         )
4483 
4484     def _to_dataframe(self, ordered_dims):
4485         columns = [k for k in self.variables if k not in self.dims]
4486         data = [
4487             self._variables[k].set_dims(ordered_dims).values.reshape(-1)
4488             for k in columns
4489         ]
4490         index = self.coords.to_index(ordered_dims)
4491         return pd.DataFrame(dict(zip(columns, data)), index=index)
4492 
4493     def to_dataframe(self):
4494         """Convert this dataset into a pandas.DataFrame.
4495 
4496         Non-index variables in this dataset form the columns of the
4497         DataFrame. The DataFrame is be indexed by the Cartesian product of
4498         this dataset's indices.
4499         """
4500         return self._to_dataframe(self.dims)
4501 
4502     def _set_sparse_data_from_dataframe(
4503         self, dataframe: pd.DataFrame, dims: tuple
4504     ) -> None:
4505         from sparse import COO
4506 
4507         idx = dataframe.index
4508         if isinstance(idx, pd.MultiIndex):
4509             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)
4510             is_sorted = idx.is_lexsorted
4511             shape = tuple(lev.size for lev in idx.levels)
4512         else:
4513             coords = np.arange(idx.size).reshape(1, -1)
4514             is_sorted = True
4515             shape = (idx.size,)
4516 
4517         for name, series in dataframe.items():
4518             # Cast to a NumPy array first, in case the Series is a pandas
4519             # Extension array (which doesn't have a valid NumPy dtype)
4520             values = np.asarray(series)
4521 
4522             # In virtually all real use cases, the sparse array will now have
4523             # missing values and needs a fill_value. For consistency, don't
4524             # special case the rare exceptions (e.g., dtype=int without a
4525             # MultiIndex).
4526             dtype, fill_value = dtypes.maybe_promote(values.dtype)
4527             values = np.asarray(values, dtype=dtype)
4528 
4529             data = COO(
4530                 coords,
4531                 values,
4532                 shape,
4533                 has_duplicates=False,
4534                 sorted=is_sorted,
4535                 fill_value=fill_value,
4536             )
4537             self[name] = (dims, data)
4538 
4539     def _set_numpy_data_from_dataframe(
4540         self, dataframe: pd.DataFrame, dims: tuple
4541     ) -> None:
4542         idx = dataframe.index
4543         if isinstance(idx, pd.MultiIndex):
4544             # expand the DataFrame to include the product of all levels
4545             full_idx = pd.MultiIndex.from_product(idx.levels, names=idx.names)
4546             dataframe = dataframe.reindex(full_idx)
4547             shape = tuple(lev.size for lev in idx.levels)
4548         else:
4549             shape = (idx.size,)
4550         for name, series in dataframe.items():
4551             data = np.asarray(series).reshape(shape)
4552             self[name] = (dims, data)
4553 
4554     @classmethod
4555     def from_dataframe(cls, dataframe: pd.DataFrame, sparse: bool = False) -> "Dataset":
4556         """Convert a pandas.DataFrame into an xarray.Dataset
4557 
4558         Each column will be converted into an independent variable in the
4559         Dataset. If the dataframe's index is a MultiIndex, it will be expanded
4560         into a tensor product of one-dimensional indices (filling in missing
4561         values with NaN). This method will produce a Dataset very similar to
4562         that on which the 'to_dataframe' method was called, except with
4563         possibly redundant dimensions (since all dataset variables will have
4564         the same dimensionality)
4565 
4566         Parameters
4567         ----------
4568         dataframe : pandas.DataFrame
4569             DataFrame from which to copy data and indices.
4570         sparse : bool
4571             If true, create a sparse arrays instead of dense numpy arrays. This
4572             can potentially save a large amount of memory if the DataFrame has
4573             a MultiIndex. Requires the sparse package (sparse.pydata.org).
4574 
4575         Returns
4576         -------
4577         New Dataset.
4578 
4579         See also
4580         --------
4581         xarray.DataArray.from_series
4582         """
4583         # TODO: Add an option to remove dimensions along which the variables
4584         # are constant, to enable consistent serialization to/from a dataframe,
4585         # even if some variables have different dimensionality.
4586 
4587         if not dataframe.columns.is_unique:
4588             raise ValueError("cannot convert DataFrame with non-unique columns")
4589 
4590         idx = remove_unused_levels_categories(dataframe.index)
4591         dataframe = dataframe.set_index(idx)
4592         obj = cls()
4593 
4594         if isinstance(idx, pd.MultiIndex):
4595             dims = tuple(
4596                 name if name is not None else "level_%i" % n
4597                 for n, name in enumerate(idx.names)
4598             )
4599             for dim, lev in zip(dims, idx.levels):
4600                 obj[dim] = (dim, lev)
4601         else:
4602             index_name = idx.name if idx.name is not None else "index"
4603             dims = (index_name,)
4604             obj[index_name] = (dims, idx)
4605 
4606         if sparse:
4607             obj._set_sparse_data_from_dataframe(dataframe, dims)
4608         else:
4609             obj._set_numpy_data_from_dataframe(dataframe, dims)
4610         return obj
4611 
4612     def to_dask_dataframe(self, dim_order=None, set_index=False):
4613         """
4614         Convert this dataset into a dask.dataframe.DataFrame.
4615 
4616         The dimensions, coordinates and data variables in this dataset form
4617         the columns of the DataFrame.
4618 
4619         Parameters
4620         ----------
4621         dim_order : list, optional
4622             Hierarchical dimension order for the resulting dataframe. All
4623             arrays are transposed to this order and then written out as flat
4624             vectors in contiguous order, so the last dimension in this list
4625             will be contiguous in the resulting DataFrame. This has a major
4626             influence on which operations are efficient on the resulting dask
4627             dataframe.
4628 
4629             If provided, must include all dimensions on this dataset. By
4630             default, dimensions are sorted alphabetically.
4631         set_index : bool, optional
4632             If set_index=True, the dask DataFrame is indexed by this dataset's
4633             coordinate. Since dask DataFrames to not support multi-indexes,
4634             set_index only works if the dataset only contains one dimension.
4635 
4636         Returns
4637         -------
4638         dask.dataframe.DataFrame
4639         """
4640 
4641         import dask.array as da
4642         import dask.dataframe as dd
4643 
4644         if dim_order is None:
4645             dim_order = list(self.dims)
4646         elif set(dim_order) != set(self.dims):
4647             raise ValueError(
4648                 "dim_order {} does not match the set of dimensions on this "
4649                 "Dataset: {}".format(dim_order, list(self.dims))
4650             )
4651 
4652         ordered_dims = {k: self.dims[k] for k in dim_order}
4653 
4654         columns = list(ordered_dims)
4655         columns.extend(k for k in self.coords if k not in self.dims)
4656         columns.extend(self.data_vars)
4657 
4658         series_list = []
4659         for name in columns:
4660             try:
4661                 var = self.variables[name]
4662             except KeyError:
4663                 # dimension without a matching coordinate
4664                 size = self.dims[name]
4665                 data = da.arange(size, chunks=size, dtype=np.int64)
4666                 var = Variable((name,), data)
4667 
4668             # IndexVariable objects have a dummy .chunk() method
4669             if isinstance(var, IndexVariable):
4670                 var = var.to_base_variable()
4671 
4672             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data
4673             series = dd.from_array(dask_array.reshape(-1), columns=[name])
4674             series_list.append(series)
4675 
4676         df = dd.concat(series_list, axis=1)
4677 
4678         if set_index:
4679             if len(dim_order) == 1:
4680                 (dim,) = dim_order
4681                 df = df.set_index(dim)
4682             else:
4683                 # triggers an error about multi-indexes, even if only one
4684                 # dimension is passed
4685                 df = df.set_index(dim_order)
4686 
4687         return df
4688 
4689     def to_dict(self, data=True):
4690         """
4691         Convert this dataset to a dictionary following xarray naming
4692         conventions.
4693 
4694         Converts all variables and attributes to native Python objects
4695         Useful for converting to json. To avoid datetime incompatibility
4696         use decode_times=False kwarg in xarrray.open_dataset.
4697 
4698         Parameters
4699         ----------
4700         data : bool, optional
4701             Whether to include the actual data in the dictionary. When set to
4702             False, returns just the schema.
4703 
4704         See also
4705         --------
4706         Dataset.from_dict
4707         """
4708         d = {
4709             "coords": {},
4710             "attrs": decode_numpy_dict_values(self.attrs),
4711             "dims": dict(self.dims),
4712             "data_vars": {},
4713         }
4714         for k in self.coords:
4715             d["coords"].update({k: self[k].variable.to_dict(data=data)})
4716         for k in self.data_vars:
4717             d["data_vars"].update({k: self[k].variable.to_dict(data=data)})
4718         return d
4719 
4720     @classmethod
4721     def from_dict(cls, d):
4722         """
4723         Convert a dictionary into an xarray.Dataset.
4724 
4725         Input dict can take several forms::
4726 
4727             d = {'t': {'dims': ('t'), 'data': t},
4728                  'a': {'dims': ('t'), 'data': x},
4729                  'b': {'dims': ('t'), 'data': y}}
4730 
4731             d = {'coords': {'t': {'dims': 't', 'data': t,
4732                                   'attrs': {'units':'s'}}},
4733                  'attrs': {'title': 'air temperature'},
4734                  'dims': 't',
4735                  'data_vars': {'a': {'dims': 't', 'data': x, },
4736                                'b': {'dims': 't', 'data': y}}}
4737 
4738         where 't' is the name of the dimesion, 'a' and 'b' are names of data
4739         variables and t, x, and y are lists, numpy.arrays or pandas objects.
4740 
4741         Parameters
4742         ----------
4743         d : dict, with a minimum structure of {'var_0': {'dims': [..], \
4744                                                          'data': [..]}, \
4745                                                ...}
4746 
4747         Returns
4748         -------
4749         obj : xarray.Dataset
4750 
4751         See also
4752         --------
4753         Dataset.to_dict
4754         DataArray.from_dict
4755         """
4756 
4757         if not {"coords", "data_vars"}.issubset(set(d)):
4758             variables = d.items()
4759         else:
4760             import itertools
4761 
4762             variables = itertools.chain(
4763                 d.get("coords", {}).items(), d.get("data_vars", {}).items()
4764             )
4765         try:
4766             variable_dict = {
4767                 k: (v["dims"], v["data"], v.get("attrs")) for k, v in variables
4768             }
4769         except KeyError as e:
4770             raise ValueError(
4771                 "cannot convert dict without the key "
4772                 "'{dims_data}'".format(dims_data=str(e.args[0]))
4773             )
4774         obj = cls(variable_dict)
4775 
4776         # what if coords aren't dims?
4777         coords = set(d.get("coords", {})) - set(d.get("dims", {}))
4778         obj = obj.set_coords(coords)
4779 
4780         obj.attrs.update(d.get("attrs", {}))
4781 
4782         return obj
4783 
4784     @staticmethod
4785     def _unary_op(f, keep_attrs=False):
4786         @functools.wraps(f)
4787         def func(self, *args, **kwargs):
4788             variables = {}
4789             for k, v in self._variables.items():
4790                 if k in self._coord_names:
4791                     variables[k] = v
4792                 else:
4793                     variables[k] = f(v, *args, **kwargs)
4794             attrs = self._attrs if keep_attrs else None
4795             return self._replace_with_new_dims(variables, attrs=attrs)
4796 
4797         return func
4798 
4799     @staticmethod
4800     def _binary_op(f, reflexive=False, join=None):
4801         @functools.wraps(f)
4802         def func(self, other):
4803             from .dataarray import DataArray
4804 
4805             if isinstance(other, groupby.GroupBy):
4806                 return NotImplemented
4807             align_type = OPTIONS["arithmetic_join"] if join is None else join
4808             if isinstance(other, (DataArray, Dataset)):
4809                 self, other = align(self, other, join=align_type, copy=False)
4810             g = f if not reflexive else lambda x, y: f(y, x)
4811             ds = self._calculate_binary_op(g, other, join=align_type)
4812             return ds
4813 
4814         return func
4815 
4816     @staticmethod
4817     def _inplace_binary_op(f):
4818         @functools.wraps(f)
4819         def func(self, other):
4820             from .dataarray import DataArray
4821 
4822             if isinstance(other, groupby.GroupBy):
4823                 raise TypeError(
4824                     "in-place operations between a Dataset and "
4825                     "a grouped object are not permitted"
4826                 )
4827             # we don't actually modify arrays in-place with in-place Dataset
4828             # arithmetic -- this lets us automatically align things
4829             if isinstance(other, (DataArray, Dataset)):
4830                 other = other.reindex_like(self, copy=False)
4831             g = ops.inplace_to_noninplace_op(f)
4832             ds = self._calculate_binary_op(g, other, inplace=True)
4833             self._replace_with_new_dims(
4834                 ds._variables,
4835                 ds._coord_names,
4836                 attrs=ds._attrs,
4837                 indexes=ds._indexes,
4838                 inplace=True,
4839             )
4840             return self
4841 
4842         return func
4843 
4844     def _calculate_binary_op(self, f, other, join="inner", inplace=False):
4845         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):
4846             if inplace and set(lhs_data_vars) != set(rhs_data_vars):
4847                 raise ValueError(
4848                     "datasets must have the same data variables "
4849                     "for in-place arithmetic operations: %s, %s"
4850                     % (list(lhs_data_vars), list(rhs_data_vars))
4851                 )
4852 
4853             dest_vars = {}
4854 
4855             for k in lhs_data_vars:
4856                 if k in rhs_data_vars:
4857                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])
4858                 elif join in ["left", "outer"]:
4859                     dest_vars[k] = f(lhs_vars[k], np.nan)
4860             for k in rhs_data_vars:
4861                 if k not in dest_vars and join in ["right", "outer"]:
4862                     dest_vars[k] = f(rhs_vars[k], np.nan)
4863             return dest_vars
4864 
4865         if utils.is_dict_like(other) and not isinstance(other, Dataset):
4866             # can't use our shortcut of doing the binary operation with
4867             # Variable objects, so apply over our data vars instead.
4868             new_data_vars = apply_over_both(
4869                 self.data_vars, other, self.data_vars, other
4870             )
4871             return Dataset(new_data_vars)
4872 
4873         other_coords = getattr(other, "coords", None)
4874         ds = self.coords.merge(other_coords)
4875 
4876         if isinstance(other, Dataset):
4877             new_vars = apply_over_both(
4878                 self.data_vars, other.data_vars, self.variables, other.variables
4879             )
4880         else:
4881             other_variable = getattr(other, "variable", other)
4882             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}
4883         ds._variables.update(new_vars)
4884         ds._dims = calculate_dimensions(ds._variables)
4885         return ds
4886 
4887     def _copy_attrs_from(self, other):
4888         self.attrs = other.attrs
4889         for v in other.variables:
4890             if v in self.variables:
4891                 self.variables[v].attrs = other.variables[v].attrs
4892 
4893     def diff(self, dim, n=1, label="upper"):
4894         """Calculate the n-th order discrete difference along given axis.
4895 
4896         Parameters
4897         ----------
4898         dim : str, optional
4899             Dimension over which to calculate the finite difference.
4900         n : int, optional
4901             The number of times values are differenced.
4902         label : str, optional
4903             The new coordinate in dimension ``dim`` will have the
4904             values of either the minuend's or subtrahend's coordinate
4905             for values 'upper' and 'lower', respectively.  Other
4906             values are not supported.
4907 
4908         Returns
4909         -------
4910         difference : same type as caller
4911             The n-th order finite difference of this object.
4912 
4913         .. note::
4914 
4915             `n` matches numpy's behavior and is different from pandas' first
4916             argument named `periods`.
4917 
4918         Examples
4919         --------
4920         >>> ds = xr.Dataset({"foo": ("x", [5, 5, 6, 6])})
4921         >>> ds.diff("x")
4922         <xarray.Dataset>
4923         Dimensions:  (x: 3)
4924         Coordinates:
4925           * x        (x) int64 1 2 3
4926         Data variables:
4927             foo      (x) int64 0 1 0
4928         >>> ds.diff("x", 2)
4929         <xarray.Dataset>
4930         Dimensions:  (x: 2)
4931         Coordinates:
4932         * x        (x) int64 2 3
4933         Data variables:
4934         foo      (x) int64 1 -1
4935 
4936         See Also
4937         --------
4938         Dataset.differentiate
4939         """
4940         if n == 0:
4941             return self
4942         if n < 0:
4943             raise ValueError(f"order `n` must be non-negative but got {n}")
4944 
4945         # prepare slices
4946         kwargs_start = {dim: slice(None, -1)}
4947         kwargs_end = {dim: slice(1, None)}
4948 
4949         # prepare new coordinate
4950         if label == "upper":
4951             kwargs_new = kwargs_end
4952         elif label == "lower":
4953             kwargs_new = kwargs_start
4954         else:
4955             raise ValueError(
4956                 "The 'label' argument has to be either " "'upper' or 'lower'"
4957             )
4958 
4959         variables = {}
4960 
4961         for name, var in self.variables.items():
4962             if dim in var.dims:
4963                 if name in self.data_vars:
4964                     variables[name] = var.isel(**kwargs_end) - var.isel(**kwargs_start)
4965                 else:
4966                     variables[name] = var.isel(**kwargs_new)
4967             else:
4968                 variables[name] = var
4969 
4970         indexes = dict(self.indexes)
4971         if dim in indexes:
4972             indexes[dim] = indexes[dim][kwargs_new[dim]]
4973 
4974         difference = self._replace_with_new_dims(variables, indexes=indexes)
4975 
4976         if n > 1:
4977             return difference.diff(dim, n - 1)
4978         else:
4979             return difference
4980 
4981     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):
4982         """Shift this dataset by an offset along one or more dimensions.
4983 
4984         Only data variables are moved; coordinates stay in place. This is
4985         consistent with the behavior of ``shift`` in pandas.
4986 
4987         Parameters
4988         ----------
4989         shifts : Mapping with the form of {dim: offset}
4990             Integer offset to shift along each of the given dimensions.
4991             Positive offsets shift to the right; negative offsets shift to the
4992             left.
4993         fill_value: scalar, optional
4994             Value to use for newly missing values
4995         **shifts_kwargs:
4996             The keyword arguments form of ``shifts``.
4997             One of shifts or shifts_kwargs must be provided.
4998 
4999         Returns
5000         -------
5001         shifted : Dataset
5002             Dataset with the same coordinates and attributes but shifted data
5003             variables.
5004 
5005         See also
5006         --------
5007         roll
5008 
5009         Examples
5010         --------
5011 
5012         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
5013         >>> ds.shift(x=2)
5014         <xarray.Dataset>
5015         Dimensions:  (x: 5)
5016         Coordinates:
5017           * x        (x) int64 0 1 2 3 4
5018         Data variables:
5019             foo      (x) object nan nan 'a' 'b' 'c'
5020         """
5021         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
5022         invalid = [k for k in shifts if k not in self.dims]
5023         if invalid:
5024             raise ValueError("dimensions %r do not exist" % invalid)
5025 
5026         variables = {}
5027         for name, var in self.variables.items():
5028             if name in self.data_vars:
5029                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}
5030                 variables[name] = var.shift(fill_value=fill_value, shifts=var_shifts)
5031             else:
5032                 variables[name] = var
5033 
5034         return self._replace(variables)
5035 
5036     def roll(self, shifts=None, roll_coords=None, **shifts_kwargs):
5037         """Roll this dataset by an offset along one or more dimensions.
5038 
5039         Unlike shift, roll may rotate all variables, including coordinates
5040         if specified. The direction of rotation is consistent with
5041         :py:func:`numpy.roll`.
5042 
5043         Parameters
5044         ----------
5045 
5046         shifts : dict, optional
5047             A dict with keys matching dimensions and values given
5048             by integers to rotate each of the given dimensions. Positive
5049             offsets roll to the right; negative offsets roll to the left.
5050         roll_coords : bool
5051             Indicates whether to  roll the coordinates by the offset
5052             The current default of roll_coords (None, equivalent to True) is
5053             deprecated and will change to False in a future version.
5054             Explicitly pass roll_coords to silence the warning.
5055         **shifts_kwargs : {dim: offset, ...}, optional
5056             The keyword arguments form of ``shifts``.
5057             One of shifts or shifts_kwargs must be provided.
5058         Returns
5059         -------
5060         rolled : Dataset
5061             Dataset with the same coordinates and attributes but rolled
5062             variables.
5063 
5064         See also
5065         --------
5066         shift
5067 
5068         Examples
5069         --------
5070 
5071         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
5072         >>> ds.roll(x=2)
5073         <xarray.Dataset>
5074         Dimensions:  (x: 5)
5075         Coordinates:
5076           * x        (x) int64 3 4 0 1 2
5077         Data variables:
5078             foo      (x) object 'd' 'e' 'a' 'b' 'c'
5079         """
5080         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
5081         invalid = [k for k in shifts if k not in self.dims]
5082         if invalid:
5083             raise ValueError("dimensions %r do not exist" % invalid)
5084 
5085         if roll_coords is None:
5086             warnings.warn(
5087                 "roll_coords will be set to False in the future."
5088                 " Explicitly set roll_coords to silence warning.",
5089                 FutureWarning,
5090                 stacklevel=2,
5091             )
5092             roll_coords = True
5093 
5094         unrolled_vars = () if roll_coords else self.coords
5095 
5096         variables = {}
5097         for k, v in self.variables.items():
5098             if k not in unrolled_vars:
5099                 variables[k] = v.roll(
5100                     **{k: s for k, s in shifts.items() if k in v.dims}
5101                 )
5102             else:
5103                 variables[k] = v
5104 
5105         if roll_coords:
5106             indexes = {}
5107             for k, v in self.indexes.items():
5108                 (dim,) = self.variables[k].dims
5109                 if dim in shifts:
5110                     indexes[k] = roll_index(v, shifts[dim])
5111                 else:
5112                     indexes[k] = v
5113         else:
5114             indexes = dict(self.indexes)
5115 
5116         return self._replace(variables, indexes=indexes)
5117 
5118     def sortby(self, variables, ascending=True):
5119         """
5120         Sort object by labels or values (along an axis).
5121 
5122         Sorts the dataset, either along specified dimensions,
5123         or according to values of 1-D dataarrays that share dimension
5124         with calling object.
5125 
5126         If the input variables are dataarrays, then the dataarrays are aligned
5127         (via left-join) to the calling object prior to sorting by cell values.
5128         NaNs are sorted to the end, following Numpy convention.
5129 
5130         If multiple sorts along the same dimension is
5131         given, numpy's lexsort is performed along that dimension:
5132         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
5133         and the FIRST key in the sequence is used as the primary sort key,
5134         followed by the 2nd key, etc.
5135 
5136         Parameters
5137         ----------
5138         variables: str, DataArray, or list of either
5139             1D DataArray objects or name(s) of 1D variable(s) in
5140             coords/data_vars whose values are used to sort the dataset.
5141         ascending: boolean, optional
5142             Whether to sort by ascending or descending order.
5143 
5144         Returns
5145         -------
5146         sorted: Dataset
5147             A new dataset where all the specified dims are sorted by dim
5148             labels.
5149         """
5150         from .dataarray import DataArray
5151 
5152         if not isinstance(variables, list):
5153             variables = [variables]
5154         else:
5155             variables = variables
5156         variables = [v if isinstance(v, DataArray) else self[v] for v in variables]
5157         aligned_vars = align(self, *variables, join="left")
5158         aligned_self = aligned_vars[0]
5159         aligned_other_vars = aligned_vars[1:]
5160         vars_by_dim = defaultdict(list)
5161         for data_array in aligned_other_vars:
5162             if data_array.ndim != 1:
5163                 raise ValueError("Input DataArray is not 1-D.")
5164             (key,) = data_array.dims
5165             vars_by_dim[key].append(data_array)
5166 
5167         indices = {}
5168         for key, arrays in vars_by_dim.items():
5169             order = np.lexsort(tuple(reversed(arrays)))
5170             indices[key] = order if ascending else order[::-1]
5171         return aligned_self.isel(**indices)
5172 
5173     def quantile(
5174         self,
5175         q,
5176         dim=None,
5177         interpolation="linear",
5178         numeric_only=False,
5179         keep_attrs=None,
5180         skipna=True,
5181     ):
5182         """Compute the qth quantile of the data along the specified dimension.
5183 
5184         Returns the qth quantiles(s) of the array elements for each variable
5185         in the Dataset.
5186 
5187         Parameters
5188         ----------
5189         q : float in range of [0,1] or array-like of floats
5190             Quantile to compute, which must be between 0 and 1 inclusive.
5191         dim : str or sequence of str, optional
5192             Dimension(s) over which to apply quantile.
5193         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
5194             This optional parameter specifies the interpolation method to
5195             use when the desired quantile lies between two data points
5196             ``i < j``:
5197 
5198                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is
5199                   the fractional part of the index surrounded by ``i`` and
5200                   ``j``.
5201                 * lower: ``i``.
5202                 * higher: ``j``.
5203                 * nearest: ``i`` or ``j``, whichever is nearest.
5204                 * midpoint: ``(i + j) / 2``.
5205         keep_attrs : bool, optional
5206             If True, the dataset's attributes (`attrs`) will be copied from
5207             the original object to the new one.  If False (default), the new
5208             object will be returned without attributes.
5209         numeric_only : bool, optional
5210             If True, only apply ``func`` to variables with a numeric dtype.
5211         skipna : bool, optional
5212             Whether to skip missing values when aggregating.
5213 
5214         Returns
5215         -------
5216         quantiles : Dataset
5217             If `q` is a single quantile, then the result is a scalar for each
5218             variable in data_vars. If multiple percentiles are given, first
5219             axis of the result corresponds to the quantile and a quantile
5220             dimension is added to the return Dataset. The other dimensions are
5221             the dimensions that remain after the reduction of the array.
5222 
5223         See Also
5224         --------
5225         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile
5226 
5227         Examples
5228         --------
5229 
5230         >>> ds = xr.Dataset(
5231         ...     {"a": (("x", "y"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},
5232         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
5233         ... )
5234         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)
5235         <xarray.Dataset>
5236         Dimensions:   ()
5237         Coordinates:
5238             quantile  float64 0.0
5239         Data variables:
5240             a         float64 0.7
5241         >>> ds.quantile(0, dim="x")
5242         <xarray.Dataset>
5243         Dimensions:   (y: 4)
5244         Coordinates:
5245           * y         (y) float64 1.0 1.5 2.0 2.5
5246             quantile  float64 0.0
5247         Data variables:
5248             a         (y) float64 0.7 4.2 2.6 1.5
5249         >>> ds.quantile([0, 0.5, 1])
5250         <xarray.Dataset>
5251         Dimensions:   (quantile: 3)
5252         Coordinates:
5253           * quantile  (quantile) float64 0.0 0.5 1.0
5254         Data variables:
5255             a         (quantile) float64 0.7 3.4 9.4
5256         >>> ds.quantile([0, 0.5, 1], dim="x")
5257         <xarray.Dataset>
5258         Dimensions:   (quantile: 3, y: 4)
5259         Coordinates:
5260           * y         (y) float64 1.0 1.5 2.0 2.5
5261           * quantile  (quantile) float64 0.0 0.5 1.0
5262         Data variables:
5263             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9
5264         """
5265 
5266         if isinstance(dim, str):
5267             dims = {dim}
5268         elif dim in [None, ...]:
5269             dims = set(self.dims)
5270         else:
5271             dims = set(dim)
5272 
5273         _assert_empty(
5274             [d for d in dims if d not in self.dims],
5275             "Dataset does not contain the dimensions: %s",
5276         )
5277 
5278         q = np.asarray(q, dtype=np.float64)
5279 
5280         variables = {}
5281         for name, var in self.variables.items():
5282             reduce_dims = [d for d in var.dims if d in dims]
5283             if reduce_dims or not var.dims:
5284                 if name not in self.coords:
5285                     if (
5286                         not numeric_only
5287                         or np.issubdtype(var.dtype, np.number)
5288                         or var.dtype == np.bool_
5289                     ):
5290                         if len(reduce_dims) == var.ndim:
5291                             # prefer to aggregate over axis=None rather than
5292                             # axis=(0, 1) if they will be equivalent, because
5293                             # the former is often more efficient
5294                             reduce_dims = None
5295                         variables[name] = var.quantile(
5296                             q,
5297                             dim=reduce_dims,
5298                             interpolation=interpolation,
5299                             keep_attrs=keep_attrs,
5300                             skipna=skipna,
5301                         )
5302 
5303             else:
5304                 variables[name] = var
5305 
5306         # construct the new dataset
5307         coord_names = {k for k in self.coords if k in variables}
5308         indexes = {k: v for k, v in self.indexes.items() if k in variables}
5309         if keep_attrs is None:
5310             keep_attrs = _get_keep_attrs(default=False)
5311         attrs = self.attrs if keep_attrs else None
5312         new = self._replace_with_new_dims(
5313             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
5314         )
5315         return new.assign_coords(quantile=q)
5316 
5317     def rank(self, dim, pct=False, keep_attrs=None):
5318         """Ranks the data.
5319 
5320         Equal values are assigned a rank that is the average of the ranks that
5321         would have been otherwise assigned to all of the values within
5322         that set.
5323         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.
5324 
5325         NaNs in the input array are returned as NaNs.
5326 
5327         The `bottleneck` library is required.
5328 
5329         Parameters
5330         ----------
5331         dim : str
5332             Dimension over which to compute rank.
5333         pct : bool, optional
5334             If True, compute percentage ranks, otherwise compute integer ranks.
5335         keep_attrs : bool, optional
5336             If True, the dataset's attributes (`attrs`) will be copied from
5337             the original object to the new one.  If False (default), the new
5338             object will be returned without attributes.
5339 
5340         Returns
5341         -------
5342         ranked : Dataset
5343             Variables that do not depend on `dim` are dropped.
5344         """
5345         if dim not in self.dims:
5346             raise ValueError("Dataset does not contain the dimension: %s" % dim)
5347 
5348         variables = {}
5349         for name, var in self.variables.items():
5350             if name in self.data_vars:
5351                 if dim in var.dims:
5352                     variables[name] = var.rank(dim, pct=pct)
5353             else:
5354                 variables[name] = var
5355 
5356         coord_names = set(self.coords)
5357         if keep_attrs is None:
5358             keep_attrs = _get_keep_attrs(default=False)
5359         attrs = self.attrs if keep_attrs else None
5360         return self._replace(variables, coord_names, attrs=attrs)
5361 
5362     def differentiate(self, coord, edge_order=1, datetime_unit=None):
5363         """ Differentiate with the second order accurate central
5364         differences.
5365 
5366         .. note::
5367             This feature is limited to simple cartesian geometry, i.e. coord
5368             must be one dimensional.
5369 
5370         Parameters
5371         ----------
5372         coord: str
5373             The coordinate to be used to compute the gradient.
5374         edge_order: 1 or 2. Default 1
5375             N-th order accurate differences at the boundaries.
5376         datetime_unit: None or any of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms',
5377             'us', 'ns', 'ps', 'fs', 'as'}
5378             Unit to compute gradient. Only valid for datetime coordinate.
5379 
5380         Returns
5381         -------
5382         differentiated: Dataset
5383 
5384         See also
5385         --------
5386         numpy.gradient: corresponding numpy function
5387         """
5388         from .variable import Variable
5389 
5390         if coord not in self.variables and coord not in self.dims:
5391             raise ValueError(f"Coordinate {coord} does not exist.")
5392 
5393         coord_var = self[coord].variable
5394         if coord_var.ndim != 1:
5395             raise ValueError(
5396                 "Coordinate {} must be 1 dimensional but is {}"
5397                 " dimensional".format(coord, coord_var.ndim)
5398             )
5399 
5400         dim = coord_var.dims[0]
5401         if _contains_datetime_like_objects(coord_var):
5402             if coord_var.dtype.kind in "mM" and datetime_unit is None:
5403                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
5404             elif datetime_unit is None:
5405                 datetime_unit = "s"  # Default to seconds for cftime objects
5406             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)
5407 
5408         variables = {}
5409         for k, v in self.variables.items():
5410             if k in self.data_vars and dim in v.dims and k not in self.coords:
5411                 if _contains_datetime_like_objects(v):
5412                     v = v._to_numeric(datetime_unit=datetime_unit)
5413                 grad = duck_array_ops.gradient(
5414                     v.data, coord_var, edge_order=edge_order, axis=v.get_axis_num(dim)
5415                 )
5416                 variables[k] = Variable(v.dims, grad)
5417             else:
5418                 variables[k] = v
5419         return self._replace(variables)
5420 
5421     def integrate(self, coord, datetime_unit=None):
5422         """ integrate the array with the trapezoidal rule.
5423 
5424         .. note::
5425             This feature is limited to simple cartesian geometry, i.e. coord
5426             must be one dimensional.
5427 
5428         Parameters
5429         ----------
5430         coord: str, or a sequence of str
5431             Coordinate(s) used for the integration.
5432         datetime_unit
5433             Can be specify the unit if datetime coordinate is used. One of
5434             {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', 'ps', 'fs',
5435             'as'}
5436 
5437         Returns
5438         -------
5439         integrated: Dataset
5440 
5441         See also
5442         --------
5443         DataArray.integrate
5444         numpy.trapz: corresponding numpy function
5445 
5446         Examples
5447         --------
5448         >>> ds = xr.Dataset(
5449         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
5450         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
5451         ... )
5452         >>> ds
5453         <xarray.Dataset>
5454         Dimensions:  (x: 4)
5455         Coordinates:
5456           * x        (x) int64 0 1 2 3
5457             y        (x) int64 1 7 3 5
5458         Data variables:
5459             a        (x) int64 5 5 6 6
5460             b        (x) int64 1 2 1 0
5461         >>> ds.integrate("x")
5462         <xarray.Dataset>
5463         Dimensions:  ()
5464         Data variables:
5465             a        float64 16.5
5466             b        float64 3.5
5467         >>> ds.integrate("y")
5468         <xarray.Dataset>
5469         Dimensions:  ()
5470         Data variables:
5471             a        float64 20.0
5472             b        float64 4.0
5473         """
5474         if not isinstance(coord, (list, tuple)):
5475             coord = (coord,)
5476         result = self
5477         for c in coord:
5478             result = result._integrate_one(c, datetime_unit=datetime_unit)
5479         return result
5480 
5481     def _integrate_one(self, coord, datetime_unit=None):
5482         from .variable import Variable
5483 
5484         if coord not in self.variables and coord not in self.dims:
5485             raise ValueError(f"Coordinate {coord} does not exist.")
5486 
5487         coord_var = self[coord].variable
5488         if coord_var.ndim != 1:
5489             raise ValueError(
5490                 "Coordinate {} must be 1 dimensional but is {}"
5491                 " dimensional".format(coord, coord_var.ndim)
5492             )
5493 
5494         dim = coord_var.dims[0]
5495         if _contains_datetime_like_objects(coord_var):
5496             if coord_var.dtype.kind in "mM" and datetime_unit is None:
5497                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
5498             elif datetime_unit is None:
5499                 datetime_unit = "s"  # Default to seconds for cftime objects
5500             coord_var = coord_var._replace(
5501                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)
5502             )
5503 
5504         variables = {}
5505         coord_names = set()
5506         for k, v in self.variables.items():
5507             if k in self.coords:
5508                 if dim not in v.dims:
5509                     variables[k] = v
5510                     coord_names.add(k)
5511             else:
5512                 if k in self.data_vars and dim in v.dims:
5513                     if _contains_datetime_like_objects(v):
5514                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)
5515                     integ = duck_array_ops.trapz(
5516                         v.data, coord_var.data, axis=v.get_axis_num(dim)
5517                     )
5518                     v_dims = list(v.dims)
5519                     v_dims.remove(dim)
5520                     variables[k] = Variable(v_dims, integ)
5521                 else:
5522                     variables[k] = v
5523         indexes = {k: v for k, v in self.indexes.items() if k in variables}
5524         return self._replace_with_new_dims(
5525             variables, coord_names=coord_names, indexes=indexes
5526         )
5527 
5528     @property
5529     def real(self):
5530         return self._unary_op(lambda x: x.real, keep_attrs=True)(self)
5531 
5532     @property
5533     def imag(self):
5534         return self._unary_op(lambda x: x.imag, keep_attrs=True)(self)
5535 
5536     @property
5537     def plot(self):
5538         """
5539         Access plotting functions for Datasets.
5540         Use it as a namespace to use xarray.plot functions as Dataset methods
5541 
5542         >>> ds.plot.scatter(...)  # equivalent to xarray.plot.scatter(ds,...)
5543 
5544         """
5545         return _Dataset_PlotMethods(self)
5546 
5547     def filter_by_attrs(self, **kwargs):
5548         """Returns a ``Dataset`` with variables that match specific conditions.
5549 
5550         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned
5551         containing only the variables for which all the filter tests pass.
5552         These tests are either ``key=value`` for which the attribute ``key``
5553         has the exact value ``value`` or the callable passed into
5554         ``key=callable`` returns True. The callable will be passed a single
5555         value, either the value of the attribute ``key`` or ``None`` if the
5556         DataArray does not have an attribute with the name ``key``.
5557 
5558         Parameters
5559         ----------
5560         **kwargs : key=value
5561             key : str
5562                 Attribute name.
5563             value : callable or obj
5564                 If value is a callable, it should return a boolean in the form
5565                 of bool = func(attr) where attr is da.attrs[key].
5566                 Otherwise, value will be compared to the each
5567                 DataArray's attrs[key].
5568 
5569         Returns
5570         -------
5571         new : Dataset
5572             New dataset with variables filtered by attribute.
5573 
5574         Examples
5575         --------
5576         >>> # Create an example dataset:
5577         >>> import numpy as np
5578         >>> import pandas as pd
5579         >>> import xarray as xr
5580         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)
5581         >>> precip = 10 * np.random.rand(2, 2, 3)
5582         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
5583         >>> lat = [[42.25, 42.21], [42.63, 42.59]]
5584         >>> dims = ["x", "y", "time"]
5585         >>> temp_attr = dict(standard_name="air_potential_temperature")
5586         >>> precip_attr = dict(standard_name="convective_precipitation_flux")
5587         >>> ds = xr.Dataset(
5588         ...     {
5589         ...         "temperature": (dims, temp, temp_attr),
5590         ...         "precipitation": (dims, precip, precip_attr),
5591         ...     },
5592         ...     coords={
5593         ...         "lon": (["x", "y"], lon),
5594         ...         "lat": (["x", "y"], lat),
5595         ...         "time": pd.date_range("2014-09-06", periods=3),
5596         ...         "reference_time": pd.Timestamp("2014-09-05"),
5597         ...     },
5598         ... )
5599         >>> # Get variables matching a specific standard_name.
5600         >>> ds.filter_by_attrs(standard_name="convective_precipitation_flux")
5601         <xarray.Dataset>
5602         Dimensions:         (time: 3, x: 2, y: 2)
5603         Coordinates:
5604           * x               (x) int64 0 1
5605           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
5606             lat             (x, y) float64 42.25 42.21 42.63 42.59
5607           * y               (y) int64 0 1
5608             reference_time  datetime64[ns] 2014-09-05
5609             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
5610         Data variables:
5611             precipitation   (x, y, time) float64 4.178 2.307 6.041 6.046 0.06648 ...
5612         >>> # Get all variables that have a standard_name attribute.
5613         >>> standard_name = lambda v: v is not None
5614         >>> ds.filter_by_attrs(standard_name=standard_name)
5615         <xarray.Dataset>
5616         Dimensions:         (time: 3, x: 2, y: 2)
5617         Coordinates:
5618             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
5619             lat             (x, y) float64 42.25 42.21 42.63 42.59
5620           * x               (x) int64 0 1
5621           * y               (y) int64 0 1
5622           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
5623             reference_time  datetime64[ns] 2014-09-05
5624         Data variables:
5625             temperature     (x, y, time) float64 25.86 20.82 6.954 23.13 10.25 11.68 ...
5626             precipitation   (x, y, time) float64 5.702 0.9422 2.075 1.178 3.284 ...
5627 
5628         """
5629         selection = []
5630         for var_name, variable in self.variables.items():
5631             has_value_flag = False
5632             for attr_name, pattern in kwargs.items():
5633                 attr_value = variable.attrs.get(attr_name)
5634                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:
5635                     has_value_flag = True
5636                 else:
5637                     has_value_flag = False
5638                     break
5639             if has_value_flag is True:
5640                 selection.append(var_name)
5641         return self[selection]
5642 
5643     def unify_chunks(self) -> "Dataset":
5644         """ Unify chunk size along all chunked dimensions of this Dataset.
5645 
5646         Returns
5647         -------
5648 
5649         Dataset with consistent chunk sizes for all dask-array variables
5650 
5651         See Also
5652         --------
5653 
5654         dask.array.core.unify_chunks
5655         """
5656 
5657         try:
5658             self.chunks
5659         except ValueError:  # "inconsistent chunks"
5660             pass
5661         else:
5662             # No variables with dask backend, or all chunks are already aligned
5663             return self.copy()
5664 
5665         # import dask is placed after the quick exit test above to allow
5666         # running this method if dask isn't installed and there are no chunks
5667         import dask.array
5668 
5669         ds = self.copy()
5670 
5671         dims_pos_map = {dim: index for index, dim in enumerate(ds.dims)}
5672 
5673         dask_array_names = []
5674         dask_unify_args = []
5675         for name, variable in ds.variables.items():
5676             if isinstance(variable.data, dask.array.Array):
5677                 dims_tuple = [dims_pos_map[dim] for dim in variable.dims]
5678                 dask_array_names.append(name)
5679                 dask_unify_args.append(variable.data)
5680                 dask_unify_args.append(dims_tuple)
5681 
5682         _, rechunked_arrays = dask.array.core.unify_chunks(*dask_unify_args)
5683 
5684         for name, new_array in zip(dask_array_names, rechunked_arrays):
5685             ds.variables[name]._data = new_array
5686 
5687         return ds
5688 
5689     def map_blocks(
5690         self,
5691         func: "Callable[..., T_DSorDA]",
5692         args: Sequence[Any] = (),
5693         kwargs: Mapping[str, Any] = None,
5694     ) -> "T_DSorDA":
5695         """
5696         Apply a function to each chunk of this Dataset. This method is experimental and
5697         its signature may change.
5698 
5699         Parameters
5700         ----------
5701         func: callable
5702             User-provided function that accepts a Dataset as its first parameter. The
5703             function will receive a subset of this Dataset, corresponding to one chunk
5704             along each chunked dimension. ``func`` will be executed as
5705             ``func(obj_subset, *args, **kwargs)``.
5706 
5707             The function will be first run on mocked-up data, that looks like this
5708             Dataset but has sizes 0, to determine properties of the returned object such
5709             as dtype, variable names, new dimensions and new indexes (if any).
5710 
5711             This function must return either a single DataArray or a single Dataset.
5712 
5713             This function cannot change size of existing dimensions, or add new chunked
5714             dimensions.
5715         args: Sequence
5716             Passed verbatim to func after unpacking, after the sliced DataArray. xarray
5717             objects, if any, will not be split by chunks. Passing dask collections is
5718             not allowed.
5719         kwargs: Mapping
5720             Passed verbatim to func after unpacking. xarray objects, if any, will not be
5721             split by chunks. Passing dask collections is not allowed.
5722 
5723         Returns
5724         -------
5725         A single DataArray or Dataset with dask backend, reassembled from the outputs of
5726         the function.
5727 
5728         Notes
5729         -----
5730         This method is designed for when one needs to manipulate a whole xarray object
5731         within each chunk. In the more common case where one can work on numpy arrays,
5732         it is recommended to use apply_ufunc.
5733 
5734         If none of the variables in this Dataset is backed by dask, calling this method
5735         is equivalent to calling ``func(self, *args, **kwargs)``.
5736 
5737         See Also
5738         --------
5739         dask.array.map_blocks, xarray.apply_ufunc, xarray.map_blocks,
5740         xarray.DataArray.map_blocks
5741         """
5742         from .parallel import map_blocks
5743 
5744         return map_blocks(func, self, args, kwargs)

</code><intended_behavior>Implement a new method named 'average' (or 'weighted_average' for clarity) that allows for computing weighted averages across specified dimensions of the dataset. This method should accept parameters for specifying the axes along which to compute the averages and an optional weight parameter for each dimension. It should handle missing data gracefully, likely reusing or extending patterns seen in 'interpolate_na'.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
215 class DataArray(AbstractArray, DataWithCoords):
216     """N-dimensional array with labeled coordinates and dimensions.
217 
218     DataArray provides a wrapper around numpy ndarrays that uses labeled
219     dimensions and coordinates to support metadata aware operations. The API is
220     similar to that for the pandas Series or DataFrame, but DataArray objects
221     can have any number of dimensions, and their contents have fixed data
222     types.
223 
224     Additional features over raw numpy arrays:
225 
226     - Apply operations over dimensions by name: ``x.sum('time')``.
227     - Select or assign values by integer location (like numpy): ``x[:10]``
228       or by label (like pandas): ``x.loc['2014-01-01']`` or
229       ``x.sel(time='2014-01-01')``.
230     - Mathematical operations (e.g., ``x - y``) vectorize across multiple
231       dimensions (known in numpy as "broadcasting") based on dimension names,
232       regardless of their original order.
233     - Keep track of arbitrary metadata in the form of a Python dictionary:
234       ``x.attrs``
235     - Convert to a pandas Series: ``x.to_series()``.
236 
237     Getting items from or doing mathematical operations with a DataArray
238     always returns another DataArray.
239     """
240 
241     _cache: Dict[str, Any]
242     _coords: Dict[Any, Variable]
243     _indexes: Optional[Dict[Hashable, pd.Index]]
244     _name: Optional[Hashable]
245     _variable: Variable
246 
247     __slots__ = (
248         "_cache",
249         "_coords",
250         "_file_obj",
251         "_indexes",
252         "_name",
253         "_variable",
254         "__weakref__",
255     )
256 
257     _groupby_cls = groupby.DataArrayGroupBy
258     _rolling_cls = rolling.DataArrayRolling
259     _coarsen_cls = rolling.DataArrayCoarsen
260     _resample_cls = resample.DataArrayResample
261 
262     dt = property(CombinedDatetimelikeAccessor)
263 
264     def __init__(
265         self,
266         data: Any = dtypes.NA,
267         coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None] = None,
268         dims: Union[Hashable, Sequence[Hashable], None] = None,
269         name: Hashable = None,
270         attrs: Mapping = None,
271         # internal parameters
272         indexes: Dict[Hashable, pd.Index] = None,
273         fastpath: bool = False,
274     ):
275         """
276         Parameters
277         ----------
278         data : array_like
279             Values for this array. Must be an ``numpy.ndarray``, ndarray like,
280             or castable to an ``ndarray``. If a self-described xarray or pandas
281             object, attempts are made to use this array's metadata to fill in
282             other unspecified arguments. A view of the array's data is used
283             instead of a copy if possible.
284         coords : sequence or dict of array_like objects, optional
285             Coordinates (tick labels) to use for indexing along each dimension.
286             The following notations are accepted:
287 
288             - mapping {dimension name: array-like}
289             - sequence of tuples that are valid arguments for xarray.Variable()
290               - (dims, data)
291               - (dims, data, attrs)
292               - (dims, data, attrs, encoding)
293 
294             Additionally, it is possible to define a coord whose name
295             does not match the dimension name, or a coord based on multiple
296             dimensions, with one of the following notations:
297 
298             - mapping {coord name: DataArray}
299             - mapping {coord name: Variable}
300             - mapping {coord name: (dimension name, array-like)}
301             - mapping {coord name: (tuple of dimension names, array-like)}
302 
303         dims : hashable or sequence of hashable, optional
304             Name(s) of the data dimension(s). Must be either a hashable (only
305             for 1D data) or a sequence of hashables with length equal to the
306             number of dimensions. If this argument is omitted, dimension names
307             default to ``['dim_0', ... 'dim_n']``.
308         name : str or None, optional
309             Name of this array.
310         attrs : dict_like or None, optional
311             Attributes to assign to the new instance. By default, an empty
312             attribute dictionary is initialized.
313         """
314         if fastpath:
315             variable = data
316             assert dims is None
317             assert attrs is None
318         else:
319             # try to fill in arguments from data if they weren't supplied
320             if coords is None:
321 
322                 if isinstance(data, DataArray):
323                     coords = data.coords
324                 elif isinstance(data, pd.Series):
325                     coords = [data.index]
326                 elif isinstance(data, pd.DataFrame):
327                     coords = [data.index, data.columns]
328                 elif isinstance(data, (pd.Index, IndexVariable)):
329                     coords = [data]
330                 elif isinstance(data, pdcompat.Panel):
331                     coords = [data.items, data.major_axis, data.minor_axis]
332 
333             if dims is None:
334                 dims = getattr(data, "dims", getattr(coords, "dims", None))
335             if name is None:
336                 name = getattr(data, "name", None)
337             if attrs is None and not isinstance(data, PANDAS_TYPES):
338                 attrs = getattr(data, "attrs", None)
339 
340             data = _check_data_shape(data, coords, dims)
341             data = as_compatible_data(data)
342             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
343             variable = Variable(dims, data, attrs, fastpath=True)
344             indexes = dict(
345                 _extract_indexes_from_coords(coords)
346             )  # needed for to_dataset
347 
348         # These fully describe a DataArray
349         self._variable = variable
350         assert isinstance(coords, dict)
351         self._coords = coords
352         self._name = name
353 
354         # TODO(shoyer): document this argument, once it becomes part of the
355         # public interface.
356         self._indexes = indexes
357 
358         self._file_obj = None
359 
360     def _replace(
361         self,
362         variable: Variable = None,
363         coords=None,
364         name: Union[Hashable, None, Default] = _default,
365         indexes=None,
366     ) -> "DataArray":
367         if variable is None:
368             variable = self.variable
369         if coords is None:
370             coords = self._coords
371         if name is _default:
372             name = self.name
373         return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)
374 
375     def _replace_maybe_drop_dims(
376         self, variable: Variable, name: Union[Hashable, None, Default] = _default
377     ) -> "DataArray":
378         if variable.dims == self.dims and variable.shape == self.shape:
379             coords = self._coords.copy()
380             indexes = self._indexes
381         elif variable.dims == self.dims:
382             # Shape has changed (e.g. from reduce(..., keepdims=True)
383             new_sizes = dict(zip(self.dims, variable.shape))
384             coords = {
385                 k: v
386                 for k, v in self._coords.items()
387                 if v.shape == tuple(new_sizes[d] for d in v.dims)
388             }
389             changed_dims = [
390                 k for k in variable.dims if variable.sizes[k] != self.sizes[k]
391             ]
392             indexes = propagate_indexes(self._indexes, exclude=changed_dims)
393         else:
394             allowed_dims = set(variable.dims)
395             coords = {
396                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
397             }
398             indexes = propagate_indexes(
399                 self._indexes, exclude=(set(self.dims) - allowed_dims)
400             )
401         return self._replace(variable, coords, name, indexes=indexes)
402 
403     def _overwrite_indexes(self, indexes: Mapping[Hashable, Any]) -> "DataArray":
404         if not len(indexes):
405             return self
406         coords = self._coords.copy()
407         for name, idx in indexes.items():
408             coords[name] = IndexVariable(name, idx)
409         obj = self._replace(coords=coords)
410 
411         # switch from dimension to level names, if necessary
412         dim_names: Dict[Any, str] = {}
413         for dim, idx in indexes.items():
414             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:
415                 dim_names[dim] = idx.name
416         if dim_names:
417             obj = obj.rename(dim_names)
418         return obj
419 
420     def _to_temp_dataset(self) -> Dataset:
421         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
422 
423     def _from_temp_dataset(
424         self, dataset: Dataset, name: Hashable = _default
425     ) -> "DataArray":
426         variable = dataset._variables.pop(_THIS_ARRAY)
427         coords = dataset._variables
428         indexes = dataset._indexes
429         return self._replace(variable, coords, name, indexes=indexes)
430 
431     def _to_dataset_split(self, dim: Hashable) -> Dataset:
432         """ splits dataarray along dimension 'dim' """
433 
434         def subset(dim, label):
435             array = self.loc[{dim: label}]
436             array.attrs = {}
437             return as_variable(array)
438 
439         variables = {label: subset(dim, label) for label in self.get_index(dim)}
440         variables.update({k: v for k, v in self._coords.items() if k != dim})
441         indexes = propagate_indexes(self._indexes, exclude=dim)
442         coord_names = set(self._coords) - set([dim])
443         dataset = Dataset._construct_direct(
444             variables, coord_names, indexes=indexes, attrs=self.attrs
445         )
446         return dataset
447 
448     def _to_dataset_whole(
449         self, name: Hashable = None, shallow_copy: bool = True
450     ) -> Dataset:
451         if name is None:
452             name = self.name
453         if name is None:
454             raise ValueError(
455                 "unable to convert unnamed DataArray to a "
456                 "Dataset without providing an explicit name"
457             )
458         if name in self.coords:
459             raise ValueError(
460                 "cannot create a Dataset from a DataArray with "
461                 "the same name as one of its coordinates"
462             )
463         # use private APIs for speed: this is called by _to_temp_dataset(),
464         # which is used in the guts of a lot of operations (e.g., reindex)
465         variables = self._coords.copy()
466         variables[name] = self.variable
467         if shallow_copy:
468             for k in variables:
469                 variables[k] = variables[k].copy(deep=False)
470         indexes = self._indexes
471 
472         coord_names = set(self._coords)
473         dataset = Dataset._construct_direct(variables, coord_names, indexes=indexes)
474         return dataset
475 
476     def to_dataset(self, dim: Hashable = None, *, name: Hashable = None) -> Dataset:
477         """Convert a DataArray to a Dataset.
478 
479         Parameters
480         ----------
481         dim : hashable, optional
482             Name of the dimension on this array along which to split this array
483             into separate variables. If not provided, this array is converted
484             into a Dataset of one variable.
485         name : hashable, optional
486             Name to substitute for this array's name. Only valid if ``dim`` is
487             not provided.
488 
489         Returns
490         -------
491         dataset : Dataset
492         """
493         if dim is not None and dim not in self.dims:
494             raise TypeError(
495                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
496             )
497 
498         if dim is not None:
499             if name is not None:
500                 raise TypeError("cannot supply both dim and name arguments")
501             return self._to_dataset_split(dim)
502         else:
503             return self._to_dataset_whole(name)
504 
505     @property
506     def name(self) -> Optional[Hashable]:
507         """The name of this array.
508         """
509         return self._name
510 
511     @name.setter
512     def name(self, value: Optional[Hashable]) -> None:
513         self._name = value
514 
515     @property
516     def variable(self) -> Variable:
517         """Low level interface to the Variable object for this DataArray."""
518         return self._variable
519 
520     @property
521     def dtype(self) -> np.dtype:
522         return self.variable.dtype
523 
524     @property
525     def shape(self) -> Tuple[int, ...]:
526         return self.variable.shape
527 
528     @property
529     def size(self) -> int:
530         return self.variable.size
531 
532     @property
533     def nbytes(self) -> int:
534         return self.variable.nbytes
535 
536     @property
537     def ndim(self) -> int:
538         return self.variable.ndim
539 
540     def __len__(self) -> int:
541         return len(self.variable)
542 
543     @property
544     def data(self) -> Any:
545         """The array's data as a dask or numpy array
546         """
547         return self.variable.data
548 
549     @data.setter
550     def data(self, value: Any) -> None:
551         self.variable.data = value
552 
553     @property
554     def values(self) -> np.ndarray:
555         """The array's data as a numpy.ndarray"""
556         return self.variable.values
557 
558     @values.setter
559     def values(self, value: Any) -> None:
560         self.variable.values = value
561 
562     @property
563     def _in_memory(self) -> bool:
564         return self.variable._in_memory
565 
566     def to_index(self) -> pd.Index:
567         """Convert this variable to a pandas.Index. Only possible for 1D
568         arrays.
569         """
570         return self.variable.to_index()
571 
572     @property
573     def dims(self) -> Tuple[Hashable, ...]:
574         """Tuple of dimension names associated with this array.
575 
576         Note that the type of this property is inconsistent with
577         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
578         consistently named properties.
579         """
580         return self.variable.dims
581 
582     @dims.setter
583     def dims(self, value):
584         raise AttributeError(
585             "you cannot assign dims on a DataArray. Use "
586             ".rename() or .swap_dims() instead."
587         )
588 
589     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
590         if utils.is_dict_like(key):
591             return key
592         else:
593             key = indexing.expanded_indexer(key, self.ndim)
594             return dict(zip(self.dims, key))
595 
596     @property
597     def _level_coords(self) -> Dict[Hashable, Hashable]:
598         """Return a mapping of all MultiIndex levels and their corresponding
599         coordinate name.
600         """
601         level_coords: Dict[Hashable, Hashable] = {}
602 
603         for cname, var in self._coords.items():
604             if var.ndim == 1 and isinstance(var, IndexVariable):
605                 level_names = var.level_names
606                 if level_names is not None:
607                     (dim,) = var.dims
608                     level_coords.update({lname: dim for lname in level_names})
609         return level_coords
610 
611     def _getitem_coord(self, key):
612         from .dataset import _get_virtual_variable
613 
614         try:
615             var = self._coords[key]
616         except KeyError:
617             dim_sizes = dict(zip(self.dims, self.shape))
618             _, key, var = _get_virtual_variable(
619                 self._coords, key, self._level_coords, dim_sizes
620             )
621 
622         return self._replace_maybe_drop_dims(var, name=key)
623 
624     def __getitem__(self, key: Any) -> "DataArray":
625         if isinstance(key, str):
626             return self._getitem_coord(key)
627         else:
628             # xarray-style array indexing
629             return self.isel(indexers=self._item_key_to_dict(key))
630 
631     def __setitem__(self, key: Any, value: Any) -> None:
632         if isinstance(key, str):
633             self.coords[key] = value
634         else:
635             # Coordinates in key, value and self[key] should be consistent.
636             # TODO Coordinate consistency in key is checked here, but it
637             # causes unnecessary indexing. It should be optimized.
638             obj = self[key]
639             if isinstance(value, DataArray):
640                 assert_coordinate_consistent(value, obj.coords.variables)
641             # DataArray key -> Variable key
642             key = {
643                 k: v.variable if isinstance(v, DataArray) else v
644                 for k, v in self._item_key_to_dict(key).items()
645             }
646             self.variable[key] = value
647 
648     def __delitem__(self, key: Any) -> None:
649         del self.coords[key]
650 
651     @property
652     def _attr_sources(self) -> List[Mapping[Hashable, Any]]:
653         """List of places to look-up items for attribute-style access
654         """
655         return self._item_sources + [self.attrs]
656 
657     @property
658     def _item_sources(self) -> List[Mapping[Hashable, Any]]:
659         """List of places to look-up items for key-completion
660         """
661         return [
662             self.coords,
663             {d: self.coords[d] for d in self.dims},
664             LevelCoordinatesSource(self),
665         ]
666 
667     def __contains__(self, key: Any) -> bool:
668         return key in self.data
669 
670     @property
671     def loc(self) -> _LocIndexer:
672         """Attribute for location based indexing like pandas.
673         """
674         return _LocIndexer(self)
675 
676     @property
677     def attrs(self) -> Dict[Hashable, Any]:
678         """Dictionary storing arbitrary metadata with this array."""
679         return self.variable.attrs
680 
681     @attrs.setter
682     def attrs(self, value: Mapping[Hashable, Any]) -> None:
683         # Disable type checking to work around mypy bug - see mypy#4167
684         self.variable.attrs = value  # type: ignore
685 
686     @property
687     def encoding(self) -> Dict[Hashable, Any]:
688         """Dictionary of format-specific settings for how this array should be
689         serialized."""
690         return self.variable.encoding
691 
692     @encoding.setter
693     def encoding(self, value: Mapping[Hashable, Any]) -> None:
694         self.variable.encoding = value
695 
696     @property
697     def indexes(self) -> Indexes:
698         """Mapping of pandas.Index objects used for label based indexing
699         """
700         if self._indexes is None:
701             self._indexes = default_indexes(self._coords, self.dims)
702         return Indexes(self._indexes)
703 
704     @property
705     def coords(self) -> DataArrayCoordinates:
706         """Dictionary-like container of coordinate arrays.
707         """
708         return DataArrayCoordinates(self)
709 
710     def reset_coords(
711         self,
712         names: Union[Iterable[Hashable], Hashable, None] = None,
713         drop: bool = False,
714         inplace: bool = None,
715     ) -> Union[None, "DataArray", Dataset]:
716         """Given names of coordinates, reset them to become variables.
717 
718         Parameters
719         ----------
720         names : hashable or iterable of hashables, optional
721             Name(s) of non-index coordinates in this dataset to reset into
722             variables. By default, all non-index coordinates are reset.
723         drop : bool, optional
724             If True, remove coordinates instead of converting them into
725             variables.
726 
727         Returns
728         -------
729         Dataset, or DataArray if ``drop == True``
730         """
731         _check_inplace(inplace)
732         if names is None:
733             names = set(self.coords) - set(self.dims)
734         dataset = self.coords.to_dataset().reset_coords(names, drop)
735         if drop:
736             return self._replace(coords=dataset._variables)
737         else:
738             if self.name is None:
739                 raise ValueError(
740                     "cannot reset_coords with drop=False on an unnamed DataArrray"
741                 )
742             dataset[self.name] = self.variable
743             return dataset
744 
745     def __dask_tokenize__(self):
746         from dask.base import normalize_token
747 
748         return normalize_token((type(self), self._variable, self._coords, self._name))
749 
750     def __dask_graph__(self):
751         return self._to_temp_dataset().__dask_graph__()
752 
753     def __dask_keys__(self):
754         return self._to_temp_dataset().__dask_keys__()
755 
756     def __dask_layers__(self):
757         return self._to_temp_dataset().__dask_layers__()
758 
759     @property
760     def __dask_optimize__(self):
761         return self._to_temp_dataset().__dask_optimize__
762 
763     @property
764     def __dask_scheduler__(self):
765         return self._to_temp_dataset().__dask_scheduler__
766 
767     def __dask_postcompute__(self):
768         func, args = self._to_temp_dataset().__dask_postcompute__()
769         return self._dask_finalize, (func, args, self.name)
770 
771     def __dask_postpersist__(self):
772         func, args = self._to_temp_dataset().__dask_postpersist__()
773         return self._dask_finalize, (func, args, self.name)
774 
775     @staticmethod
776     def _dask_finalize(results, func, args, name):
777         ds = func(results, *args)
778         variable = ds._variables.pop(_THIS_ARRAY)
779         coords = ds._variables
780         return DataArray(variable, coords, name=name, fastpath=True)
781 
782     def load(self, **kwargs) -> "DataArray":
783         """Manually trigger loading of this array's data from disk or a
784         remote source into memory and return this array.
785 
786         Normally, it should not be necessary to call this method in user code,
787         because all xarray functions should either work on deferred data or
788         load data automatically. However, this method can be necessary when
789         working with many file objects on disk.
790 
791         Parameters
792         ----------
793         **kwargs : dict
794             Additional keyword arguments passed on to ``dask.array.compute``.
795 
796         See Also
797         --------
798         dask.array.compute
799         """
800         ds = self._to_temp_dataset().load(**kwargs)
801         new = self._from_temp_dataset(ds)
802         self._variable = new._variable
803         self._coords = new._coords
804         return self
805 
806     def compute(self, **kwargs) -> "DataArray":
807         """Manually trigger loading of this array's data from disk or a
808         remote source into memory and return a new array. The original is
809         left unaltered.
810 
811         Normally, it should not be necessary to call this method in user code,
812         because all xarray functions should either work on deferred data or
813         load data automatically. However, this method can be necessary when
814         working with many file objects on disk.
815 
816         Parameters
817         ----------
818         **kwargs : dict
819             Additional keyword arguments passed on to ``dask.array.compute``.
820 
821         See Also
822         --------
823         dask.array.compute
824         """
825         new = self.copy(deep=False)
826         return new.load(**kwargs)
827 
828     def persist(self, **kwargs) -> "DataArray":
829         """ Trigger computation in constituent dask arrays
830 
831         This keeps them as dask arrays but encourages them to keep data in
832         memory.  This is particularly useful when on a distributed machine.
833         When on a single machine consider using ``.compute()`` instead.
834 
835         Parameters
836         ----------
837         **kwargs : dict
838             Additional keyword arguments passed on to ``dask.persist``.
839 
840         See Also
841         --------
842         dask.persist
843         """
844         ds = self._to_temp_dataset().persist(**kwargs)
845         return self._from_temp_dataset(ds)
846 
847     def copy(self, deep: bool = True, data: Any = None) -> "DataArray":
848         """Returns a copy of this array.
849 
850         If `deep=True`, a deep copy is made of the data array.
851         Otherwise, a shallow copy is made, so each variable in the new
852         array's dataset is also a variable in this array's dataset.
853 
854         Use `data` to create a new object with the same structure as
855         original but entirely new data.
856 
857         Parameters
858         ----------
859         deep : bool, optional
860             Whether the data array and its coordinates are loaded into memory
861             and copied onto the new object. Default is True.
862         data : array_like, optional
863             Data to use in the new object. Must have same shape as original.
864             When `data` is used, `deep` is ignored for all data variables,
865             and only used for coords.
866 
867         Returns
868         -------
869         object : DataArray
870             New object with dimensions, attributes, coordinates, name,
871             encoding, and optionally data copied from original.
872 
873         Examples
874         --------
875 
876         Shallow versus deep copy
877 
878         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
879         >>> array.copy()
880         <xarray.DataArray (x: 3)>
881         array([1, 2, 3])
882         Coordinates:
883         * x        (x) <U1 'a' 'b' 'c'
884         >>> array_0 = array.copy(deep=False)
885         >>> array_0[0] = 7
886         >>> array_0
887         <xarray.DataArray (x: 3)>
888         array([7, 2, 3])
889         Coordinates:
890         * x        (x) <U1 'a' 'b' 'c'
891         >>> array
892         <xarray.DataArray (x: 3)>
893         array([7, 2, 3])
894         Coordinates:
895         * x        (x) <U1 'a' 'b' 'c'
896 
897         Changing the data using the ``data`` argument maintains the
898         structure of the original object, but with the new data. Original
899         object is unaffected.
900 
901         >>> array.copy(data=[0.1, 0.2, 0.3])
902         <xarray.DataArray (x: 3)>
903         array([ 0.1,  0.2,  0.3])
904         Coordinates:
905         * x        (x) <U1 'a' 'b' 'c'
906         >>> array
907         <xarray.DataArray (x: 3)>
908         array([1, 2, 3])
909         Coordinates:
910         * x        (x) <U1 'a' 'b' 'c'
911 
912         See Also
913         --------
914         pandas.DataFrame.copy
915         """
916         variable = self.variable.copy(deep=deep, data=data)
917         coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}
918         indexes = self._indexes
919         return self._replace(variable, coords, indexes=indexes)
920 
921     def __copy__(self) -> "DataArray":
922         return self.copy(deep=False)
923 
924     def __deepcopy__(self, memo=None) -> "DataArray":
925         # memo does nothing but is required for compatibility with
926         # copy.deepcopy
927         return self.copy(deep=True)
928 
929     # mutable objects should not be hashable
930     # https://github.com/python/mypy/issues/4266
931     __hash__ = None  # type: ignore
932 
933     @property
934     def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:
935         """Block dimensions for this array's data or None if it's not a dask
936         array.
937         """
938         return self.variable.chunks
939 
940     def chunk(
941         self,
942         chunks: Union[
943             None,
944             Number,
945             Tuple[Number, ...],
946             Tuple[Tuple[Number, ...], ...],
947             Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]],
948         ] = None,
949         name_prefix: str = "xarray-",
950         token: str = None,
951         lock: bool = False,
952     ) -> "DataArray":
953         """Coerce this array's data into a dask arrays with the given chunks.
954 
955         If this variable is a non-dask array, it will be converted to dask
956         array. If it's a dask array, it will be rechunked to the given chunk
957         sizes.
958 
959         If neither chunks is not provided for one or more dimensions, chunk
960         sizes along that dimension will not be updated; non-dask arrays will be
961         converted into dask arrays with a single block.
962 
963         Parameters
964         ----------
965         chunks : int, tuple or mapping, optional
966             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
967             ``{'x': 5, 'y': 5}``.
968         name_prefix : str, optional
969             Prefix for the name of the new dask array.
970         token : str, optional
971             Token uniquely identifying this array.
972         lock : optional
973             Passed on to :py:func:`dask.array.from_array`, if the array is not
974             already as dask array.
975 
976         Returns
977         -------
978         chunked : xarray.DataArray
979         """
980         if isinstance(chunks, (tuple, list)):
981             chunks = dict(zip(self.dims, chunks))
982 
983         ds = self._to_temp_dataset().chunk(
984             chunks, name_prefix=name_prefix, token=token, lock=lock
985         )
986         return self._from_temp_dataset(ds)
987 
988     def isel(
989         self,
990         indexers: Mapping[Hashable, Any] = None,
991         drop: bool = False,
992         **indexers_kwargs: Any,
993     ) -> "DataArray":
994         """Return a new DataArray whose data is given by integer indexing
995         along the specified dimension(s).
996 
997         See Also
998         --------
999         Dataset.isel
1000         DataArray.sel
1001         """
1002         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1003         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1004             ds = self._to_temp_dataset()._isel_fancy(indexers, drop=drop)
1005             return self._from_temp_dataset(ds)
1006 
1007         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1008         # lists, or zero or one-dimensional np.ndarray's
1009 
1010         variable = self._variable.isel(indexers)
1011 
1012         coords = {}
1013         for coord_name, coord_value in self._coords.items():
1014             coord_indexers = {
1015                 k: v for k, v in indexers.items() if k in coord_value.dims
1016             }
1017             if coord_indexers:
1018                 coord_value = coord_value.isel(coord_indexers)
1019                 if drop and coord_value.ndim == 0:
1020                     continue
1021             coords[coord_name] = coord_value
1022 
1023         return self._replace(variable=variable, coords=coords)
1024 
1025     def sel(
1026         self,
1027         indexers: Mapping[Hashable, Any] = None,
1028         method: str = None,
1029         tolerance=None,
1030         drop: bool = False,
1031         **indexers_kwargs: Any,
1032     ) -> "DataArray":
1033         """Return a new DataArray whose data is given by selecting index
1034         labels along the specified dimension(s).
1035 
1036         .. warning::
1037 
1038           Do not try to assign values when using any of the indexing methods
1039           ``isel`` or ``sel``::
1040 
1041             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1042             # DO NOT do this
1043             da.isel(x=[0, 1, 2])[1] = -1
1044 
1045           Assigning values with the chained indexing using ``.sel`` or
1046           ``.isel`` fails silently.
1047 
1048         See Also
1049         --------
1050         Dataset.sel
1051         DataArray.isel
1052 
1053         """
1054         ds = self._to_temp_dataset().sel(
1055             indexers=indexers,
1056             drop=drop,
1057             method=method,
1058             tolerance=tolerance,
1059             **indexers_kwargs,
1060         )
1061         return self._from_temp_dataset(ds)
1062 
1063     def head(
1064         self,
1065         indexers: Union[Mapping[Hashable, int], int] = None,
1066         **indexers_kwargs: Any,
1067     ) -> "DataArray":
1068         """Return a new DataArray whose data is given by the the first `n`
1069         values along the specified dimension(s). Default `n` = 5
1070 
1071         See Also
1072         --------
1073         Dataset.head
1074         DataArray.tail
1075         DataArray.thin
1076         """
1077         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1078         return self._from_temp_dataset(ds)
1079 
1080     def tail(
1081         self,
1082         indexers: Union[Mapping[Hashable, int], int] = None,
1083         **indexers_kwargs: Any,
1084     ) -> "DataArray":
1085         """Return a new DataArray whose data is given by the the last `n`
1086         values along the specified dimension(s). Default `n` = 5
1087 
1088         See Also
1089         --------
1090         Dataset.tail
1091         DataArray.head
1092         DataArray.thin
1093         """
1094         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1095         return self._from_temp_dataset(ds)
1096 
1097     def thin(
1098         self,
1099         indexers: Union[Mapping[Hashable, int], int] = None,
1100         **indexers_kwargs: Any,
1101     ) -> "DataArray":
1102         """Return a new DataArray whose data is given by each `n` value
1103         along the specified dimension(s).
1104 
1105         See Also
1106         --------
1107         Dataset.thin
1108         DataArray.head
1109         DataArray.tail
1110         """
1111         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1112         return self._from_temp_dataset(ds)
1113 
1114     def broadcast_like(
1115         self, other: Union["DataArray", Dataset], exclude: Iterable[Hashable] = None
1116     ) -> "DataArray":
1117         """Broadcast this DataArray against another Dataset or DataArray.
1118 
1119         This is equivalent to xr.broadcast(other, self)[1]
1120 
1121         xarray objects are broadcast against each other in arithmetic
1122         operations, so this method is not be necessary for most uses.
1123 
1124         If no change is needed, the input data is returned to the output
1125         without being copied.
1126 
1127         If new coords are added by the broadcast, their values are
1128         NaN filled.
1129 
1130         Parameters
1131         ----------
1132         other : Dataset or DataArray
1133             Object against which to broadcast this array.
1134         exclude : iterable of hashable, optional
1135             Dimensions that must not be broadcasted
1136 
1137         Returns
1138         -------
1139         new_da: xr.DataArray
1140 
1141         Examples
1142         --------
1143 
1144         >>> arr1
1145         <xarray.DataArray (x: 2, y: 3)>
1146         array([[0.840235, 0.215216, 0.77917 ],
1147                [0.726351, 0.543824, 0.875115]])
1148         Coordinates:
1149           * x        (x) <U1 'a' 'b'
1150           * y        (y) <U1 'a' 'b' 'c'
1151         >>> arr2
1152         <xarray.DataArray (x: 3, y: 2)>
1153         array([[0.612611, 0.125753],
1154                [0.853181, 0.948818],
1155                [0.180885, 0.33363 ]])
1156         Coordinates:
1157           * x        (x) <U1 'a' 'b' 'c'
1158           * y        (y) <U1 'a' 'b'
1159         >>> arr1.broadcast_like(arr2)
1160         <xarray.DataArray (x: 3, y: 3)>
1161         array([[0.840235, 0.215216, 0.77917 ],
1162                [0.726351, 0.543824, 0.875115],
1163                [     nan,      nan,      nan]])
1164         Coordinates:
1165           * x        (x) object 'a' 'b' 'c'
1166           * y        (y) object 'a' 'b' 'c'
1167         """
1168         if exclude is None:
1169             exclude = set()
1170         else:
1171             exclude = set(exclude)
1172         args = align(other, self, join="outer", copy=False, exclude=exclude)
1173 
1174         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1175 
1176         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
1177 
1178     def reindex_like(
1179         self,
1180         other: Union["DataArray", Dataset],
1181         method: str = None,
1182         tolerance=None,
1183         copy: bool = True,
1184         fill_value=dtypes.NA,
1185     ) -> "DataArray":
1186         """Conform this object onto the indexes of another object, filling in
1187         missing values with ``fill_value``. The default fill value is NaN.
1188 
1189         Parameters
1190         ----------
1191         other : Dataset or DataArray
1192             Object with an 'indexes' attribute giving a mapping from dimension
1193             names to pandas.Index objects, which provides coordinates upon
1194             which to index the variables in this dataset. The indexes on this
1195             other object need not be the same as the indexes on this
1196             dataset. Any mis-matched index values will be filled in with
1197             NaN, and any mis-matched dimension names will simply be ignored.
1198         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1199             Method to use for filling index values from other not found on this
1200             data array:
1201 
1202             * None (default): don't fill gaps
1203             * pad / ffill: propagate last valid index value forward
1204             * backfill / bfill: propagate next valid index value backward
1205             * nearest: use nearest valid index value
1206         tolerance : optional
1207             Maximum distance between original and new labels for inexact
1208             matches. The values of the index at the matching locations must
1209             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1210         copy : bool, optional
1211             If ``copy=True``, data in the return value is always copied. If
1212             ``copy=False`` and reindexing is unnecessary, or can be performed
1213             with only slice operations, then the output may share memory with
1214             the input. In either case, a new xarray object is always returned.
1215         fill_value : scalar, optional
1216             Value to use for newly missing values
1217 
1218         Returns
1219         -------
1220         reindexed : DataArray
1221             Another dataset array, with this array's data but coordinates from
1222             the other object.
1223 
1224         See Also
1225         --------
1226         DataArray.reindex
1227         align
1228         """
1229         indexers = reindex_like_indexers(self, other)
1230         return self.reindex(
1231             indexers=indexers,
1232             method=method,
1233             tolerance=tolerance,
1234             copy=copy,
1235             fill_value=fill_value,
1236         )
1237 
1238     def reindex(
1239         self,
1240         indexers: Mapping[Hashable, Any] = None,
1241         method: str = None,
1242         tolerance=None,
1243         copy: bool = True,
1244         fill_value=dtypes.NA,
1245         **indexers_kwargs: Any,
1246     ) -> "DataArray":
1247         """Conform this object onto the indexes of another object, filling in
1248         missing values with ``fill_value``. The default fill value is NaN.
1249 
1250         Parameters
1251         ----------
1252         indexers : dict, optional
1253             Dictionary with keys given by dimension names and values given by
1254             arrays of coordinates tick labels. Any mis-matched coordinate
1255             values will be filled in with NaN, and any mis-matched dimension
1256             names will simply be ignored.
1257             One of indexers or indexers_kwargs must be provided.
1258         copy : bool, optional
1259             If ``copy=True``, data in the return value is always copied. If
1260             ``copy=False`` and reindexing is unnecessary, or can be performed
1261             with only slice operations, then the output may share memory with
1262             the input. In either case, a new xarray object is always returned.
1263         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1264             Method to use for filling index values in ``indexers`` not found on
1265             this data array:
1266 
1267             * None (default): don't fill gaps
1268             * pad / ffill: propagate last valid index value forward
1269             * backfill / bfill: propagate next valid index value backward
1270             * nearest: use nearest valid index value
1271         tolerance : optional
1272             Maximum distance between original and new labels for inexact
1273             matches. The values of the index at the matching locations must
1274             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1275         fill_value : scalar, optional
1276             Value to use for newly missing values
1277         **indexers_kwargs : {dim: indexer, ...}, optional
1278             The keyword arguments form of ``indexers``.
1279             One of indexers or indexers_kwargs must be provided.
1280 
1281         Returns
1282         -------
1283         reindexed : DataArray
1284             Another dataset array, with this array's data but replaced
1285             coordinates.
1286 
1287         See Also
1288         --------
1289         DataArray.reindex_like
1290         align
1291         """
1292         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
1293         ds = self._to_temp_dataset().reindex(
1294             indexers=indexers,
1295             method=method,
1296             tolerance=tolerance,
1297             copy=copy,
1298             fill_value=fill_value,
1299         )
1300         return self._from_temp_dataset(ds)
1301 
1302     def interp(
1303         self,
1304         coords: Mapping[Hashable, Any] = None,
1305         method: str = "linear",
1306         assume_sorted: bool = False,
1307         kwargs: Mapping[str, Any] = None,
1308         **coords_kwargs: Any,
1309     ) -> "DataArray":
1310         """ Multidimensional interpolation of variables.
1311 
1312         coords : dict, optional
1313             Mapping from dimension names to the new coordinates.
1314             new coordinate can be an scalar, array-like or DataArray.
1315             If DataArrays are passed as new coordates, their dimensions are
1316             used for the broadcasting.
1317         method: {'linear', 'nearest'} for multidimensional array,
1318             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
1319             for 1-dimensional array.
1320         assume_sorted: boolean, optional
1321             If False, values of x can be in any order and they are sorted
1322             first. If True, x has to be an array of monotonically increasing
1323             values.
1324         kwargs: dictionary
1325             Additional keyword passed to scipy's interpolator.
1326         ``**coords_kwargs`` : {dim: coordinate, ...}, optional
1327             The keyword arguments form of ``coords``.
1328             One of coords or coords_kwargs must be provided.
1329 
1330         Returns
1331         -------
1332         interpolated: xr.DataArray
1333             New dataarray on the new coordinates.
1334 
1335         Notes
1336         -----
1337         scipy is required.
1338 
1339         See Also
1340         --------
1341         scipy.interpolate.interp1d
1342         scipy.interpolate.interpn
1343 
1344         Examples
1345         --------
1346         >>> da = xr.DataArray([1, 3], [("x", np.arange(2))])
1347         >>> da.interp(x=0.5)
1348         <xarray.DataArray ()>
1349         array(2.0)
1350         Coordinates:
1351             x        float64 0.5
1352         """
1353         if self.dtype.kind not in "uifc":
1354             raise TypeError(
1355                 "interp only works for a numeric type array. "
1356                 "Given {}.".format(self.dtype)
1357             )
1358         ds = self._to_temp_dataset().interp(
1359             coords,
1360             method=method,
1361             kwargs=kwargs,
1362             assume_sorted=assume_sorted,
1363             **coords_kwargs,
1364         )
1365         return self._from_temp_dataset(ds)
1366 
1367     def interp_like(
1368         self,
1369         other: Union["DataArray", Dataset],
1370         method: str = "linear",
1371         assume_sorted: bool = False,
1372         kwargs: Mapping[str, Any] = None,
1373     ) -> "DataArray":
1374         """Interpolate this object onto the coordinates of another object,
1375         filling out of range values with NaN.
1376 
1377         Parameters
1378         ----------
1379         other : Dataset or DataArray
1380             Object with an 'indexes' attribute giving a mapping from dimension
1381             names to an 1d array-like, which provides coordinates upon
1382             which to index the variables in this dataset.
1383         method: string, optional.
1384             {'linear', 'nearest'} for multidimensional array,
1385             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
1386             for 1-dimensional array. 'linear' is used by default.
1387         assume_sorted: boolean, optional
1388             If False, values of coordinates that are interpolated over can be
1389             in any order and they are sorted first. If True, interpolated
1390             coordinates are assumed to be an array of monotonically increasing
1391             values.
1392         kwargs: dictionary, optional
1393             Additional keyword passed to scipy's interpolator.
1394 
1395         Returns
1396         -------
1397         interpolated: xr.DataArray
1398             Another dataarray by interpolating this dataarray's data along the
1399             coordinates of the other object.
1400 
1401         Notes
1402         -----
1403         scipy is required.
1404         If the dataarray has object-type coordinates, reindex is used for these
1405         coordinates instead of the interpolation.
1406 
1407         See Also
1408         --------
1409         DataArray.interp
1410         DataArray.reindex_like
1411         """
1412         if self.dtype.kind not in "uifc":
1413             raise TypeError(
1414                 "interp only works for a numeric type array. "
1415                 "Given {}.".format(self.dtype)
1416             )
1417         ds = self._to_temp_dataset().interp_like(
1418             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
1419         )
1420         return self._from_temp_dataset(ds)
1421 
1422     def rename(
1423         self,
1424         new_name_or_name_dict: Union[Hashable, Mapping[Hashable, Hashable]] = None,
1425         **names: Hashable,
1426     ) -> "DataArray":
1427         """Returns a new DataArray with renamed coordinates or a new name.
1428 
1429         Parameters
1430         ----------
1431         new_name_or_name_dict : str or dict-like, optional
1432             If the argument is dict-like, it used as a mapping from old
1433             names to new names for coordinates. Otherwise, use the argument
1434             as the new name for this array.
1435         **names: hashable, optional
1436             The keyword arguments form of a mapping from old names to
1437             new names for coordinates.
1438             One of new_name_or_name_dict or names must be provided.
1439 
1440         Returns
1441         -------
1442         renamed : DataArray
1443             Renamed array or array with renamed coordinates.
1444 
1445         See Also
1446         --------
1447         Dataset.rename
1448         DataArray.swap_dims
1449         """
1450         if names or utils.is_dict_like(new_name_or_name_dict):
1451             new_name_or_name_dict = cast(
1452                 Mapping[Hashable, Hashable], new_name_or_name_dict
1453             )
1454             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
1455             dataset = self._to_temp_dataset().rename(name_dict)
1456             return self._from_temp_dataset(dataset)
1457         else:
1458             new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)
1459             return self._replace(name=new_name_or_name_dict)
1460 
1461     def swap_dims(self, dims_dict: Mapping[Hashable, Hashable]) -> "DataArray":
1462         """Returns a new DataArray with swapped dimensions.
1463 
1464         Parameters
1465         ----------
1466         dims_dict : dict-like
1467             Dictionary whose keys are current dimension names and whose values
1468             are new names.
1469 
1470         Returns
1471         -------
1472         swapped : DataArray
1473             DataArray with swapped dimensions.
1474 
1475         Examples
1476         --------
1477 
1478         >>> arr = xr.DataArray(
1479         ...     data=[0, 1], dims="x", coords={"x": ["a", "b"], "y": ("x", [0, 1])},
1480         ... )
1481         >>> arr
1482         <xarray.DataArray (x: 2)>
1483         array([0, 1])
1484         Coordinates:
1485           * x        (x) <U1 'a' 'b'
1486             y        (x) int64 0 1
1487 
1488         >>> arr.swap_dims({"x": "y"})
1489         <xarray.DataArray (y: 2)>
1490         array([0, 1])
1491         Coordinates:
1492             x        (y) <U1 'a' 'b'
1493           * y        (y) int64 0 1
1494 
1495         >>> arr.swap_dims({"x": "z"})
1496         <xarray.DataArray (z: 2)>
1497         array([0, 1])
1498         Coordinates:
1499             x        (z) <U1 'a' 'b'
1500             y        (z) int64 0 1
1501         Dimensions without coordinates: z
1502 
1503         See Also
1504         --------
1505 
1506         DataArray.rename
1507         Dataset.swap_dims
1508         """
1509         ds = self._to_temp_dataset().swap_dims(dims_dict)
1510         return self._from_temp_dataset(ds)
1511 
1512     def expand_dims(
1513         self,
1514         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,
1515         axis=None,
1516         **dim_kwargs: Any,
1517     ) -> "DataArray":
1518         """Return a new object with an additional axis (or axes) inserted at
1519         the corresponding position in the array shape. The new object is a
1520         view into the underlying array, not a copy.
1521 
1522 
1523         If dim is already a scalar coordinate, it will be promoted to a 1D
1524         coordinate consisting of a single value.
1525 
1526         Parameters
1527         ----------
1528         dim : hashable, sequence of hashable, dict, or None
1529             Dimensions to include on the new variable.
1530             If provided as str or sequence of str, then dimensions are inserted
1531             with length 1. If provided as a dict, then the keys are the new
1532             dimensions and the values are either integers (giving the length of
1533             the new dimensions) or sequence/ndarray (giving the coordinates of
1534             the new dimensions).
1535         axis : integer, list (or tuple) of integers, or None
1536             Axis position(s) where new axis is to be inserted (position(s) on
1537             the result array). If a list (or tuple) of integers is passed,
1538             multiple axes are inserted. In this case, dim arguments should be
1539             same length list. If axis=None is passed, all the axes will be
1540             inserted to the start of the result array.
1541         **dim_kwargs : int or sequence/ndarray
1542             The keywords are arbitrary dimensions being inserted and the values
1543             are either the lengths of the new dims (if int is given), or their
1544             coordinates. Note, this is an alternative to passing a dict to the
1545             dim kwarg and will only be used if dim is None.
1546 
1547         Returns
1548         -------
1549         expanded : same type as caller
1550             This object, but with an additional dimension(s).
1551         """
1552         if isinstance(dim, int):
1553             raise TypeError("dim should be hashable or sequence/mapping of hashables")
1554         elif isinstance(dim, Sequence) and not isinstance(dim, str):
1555             if len(dim) != len(set(dim)):
1556                 raise ValueError("dims should not contain duplicate values.")
1557             dim = dict.fromkeys(dim, 1)
1558         elif dim is not None and not isinstance(dim, Mapping):
1559             dim = {cast(Hashable, dim): 1}
1560 
1561         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
1562         ds = self._to_temp_dataset().expand_dims(dim, axis)
1563         return self._from_temp_dataset(ds)
1564 
1565     def set_index(
1566         self,
1567         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
1568         append: bool = False,
1569         inplace: bool = None,
1570         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],
1571     ) -> Optional["DataArray"]:
1572         """Set DataArray (multi-)indexes using one or more existing
1573         coordinates.
1574 
1575         Parameters
1576         ----------
1577         indexes : {dim: index, ...}
1578             Mapping from names matching dimensions and values given
1579             by (lists of) the names of existing coordinates or variables to set
1580             as new (multi-)index.
1581         append : bool, optional
1582             If True, append the supplied index(es) to the existing index(es).
1583             Otherwise replace the existing index(es) (default).
1584         **indexes_kwargs: optional
1585             The keyword arguments form of ``indexes``.
1586             One of indexes or indexes_kwargs must be provided.
1587 
1588         Returns
1589         -------
1590         obj : DataArray
1591             Another DataArray, with this data but replaced coordinates.
1592 
1593         Examples
1594         --------
1595         >>> arr = xr.DataArray(
1596         ...     data=np.ones((2, 3)),
1597         ...     dims=["x", "y"],
1598         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
1599         ... )
1600         >>> arr
1601         <xarray.DataArray (x: 2, y: 3)>
1602         array([[1., 1., 1.],
1603                [1., 1., 1.]])
1604         Coordinates:
1605           * x        (x) int64 0 1
1606           * y        (y) int64 0 1 2
1607             a        (x) int64 3 4
1608         >>> arr.set_index(x="a")
1609         <xarray.DataArray (x: 2, y: 3)>
1610         array([[1., 1., 1.],
1611                [1., 1., 1.]])
1612         Coordinates:
1613           * x        (x) int64 3 4
1614           * y        (y) int64 0 1 2
1615 
1616         See Also
1617         --------
1618         DataArray.reset_index
1619         """
1620         ds = self._to_temp_dataset().set_index(
1621             indexes, append=append, inplace=inplace, **indexes_kwargs
1622         )
1623         return self._from_temp_dataset(ds)
1624 
1625     def reset_index(
1626         self,
1627         dims_or_levels: Union[Hashable, Sequence[Hashable]],
1628         drop: bool = False,
1629         inplace: bool = None,
1630     ) -> Optional["DataArray"]:
1631         """Reset the specified index(es) or multi-index level(s).
1632 
1633         Parameters
1634         ----------
1635         dims_or_levels : hashable or sequence of hashables
1636             Name(s) of the dimension(s) and/or multi-index level(s) that will
1637             be reset.
1638         drop : bool, optional
1639             If True, remove the specified indexes and/or multi-index levels
1640             instead of extracting them as new coordinates (default: False).
1641 
1642         Returns
1643         -------
1644         obj : DataArray
1645             Another dataarray, with this dataarray's data but replaced
1646             coordinates.
1647 
1648         See Also
1649         --------
1650         DataArray.set_index
1651         """
1652         _check_inplace(inplace)
1653         coords, _ = split_indexes(
1654             dims_or_levels, self._coords, set(), self._level_coords, drop=drop
1655         )
1656         return self._replace(coords=coords)
1657 
1658     def reorder_levels(
1659         self,
1660         dim_order: Mapping[Hashable, Sequence[int]] = None,
1661         inplace: bool = None,
1662         **dim_order_kwargs: Sequence[int],
1663     ) -> "DataArray":
1664         """Rearrange index levels using input order.
1665 
1666         Parameters
1667         ----------
1668         dim_order : optional
1669             Mapping from names matching dimensions and values given
1670             by lists representing new level orders. Every given dimension
1671             must have a multi-index.
1672         **dim_order_kwargs: optional
1673             The keyword arguments form of ``dim_order``.
1674             One of dim_order or dim_order_kwargs must be provided.
1675 
1676         Returns
1677         -------
1678         obj : DataArray
1679             Another dataarray, with this dataarray's data but replaced
1680             coordinates.
1681         """
1682         _check_inplace(inplace)
1683         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
1684         replace_coords = {}
1685         for dim, order in dim_order.items():
1686             coord = self._coords[dim]
1687             index = coord.to_index()
1688             if not isinstance(index, pd.MultiIndex):
1689                 raise ValueError("coordinate %r has no MultiIndex" % dim)
1690             replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))
1691         coords = self._coords.copy()
1692         coords.update(replace_coords)
1693         return self._replace(coords=coords)
1694 
1695     def stack(
1696         self,
1697         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
1698         **dimensions_kwargs: Sequence[Hashable],
1699     ) -> "DataArray":
1700         """
1701         Stack any number of existing dimensions into a single new dimension.
1702 
1703         New dimensions will be added at the end, and the corresponding
1704         coordinate variables will be combined into a MultiIndex.
1705 
1706         Parameters
1707         ----------
1708         dimensions : Mapping of the form new_name=(dim1, dim2, ...)
1709             Names of new dimensions, and the existing dimensions that they
1710             replace.
1711         **dimensions_kwargs:
1712             The keyword arguments form of ``dimensions``.
1713             One of dimensions or dimensions_kwargs must be provided.
1714 
1715         Returns
1716         -------
1717         stacked : DataArray
1718             DataArray with stacked data.
1719 
1720         Examples
1721         --------
1722 
1723         >>> arr = xr.DataArray(
1724         ...     np.arange(6).reshape(2, 3),
1725         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
1726         ... )
1727         >>> arr
1728         <xarray.DataArray (x: 2, y: 3)>
1729         array([[0, 1, 2],
1730                [3, 4, 5]])
1731         Coordinates:
1732           * x        (x) |S1 'a' 'b'
1733           * y        (y) int64 0 1 2
1734         >>> stacked = arr.stack(z=("x", "y"))
1735         >>> stacked.indexes["z"]
1736         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1737                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],
1738                    names=['x', 'y'])
1739 
1740         See Also
1741         --------
1742         DataArray.unstack
1743         """
1744         ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)
1745         return self._from_temp_dataset(ds)
1746 
1747     def unstack(
1748         self,
1749         dim: Union[Hashable, Sequence[Hashable], None] = None,
1750         fill_value: Any = dtypes.NA,
1751         sparse: bool = False,
1752     ) -> "DataArray":
1753         """
1754         Unstack existing dimensions corresponding to MultiIndexes into
1755         multiple new dimensions.
1756 
1757         New dimensions will be added at the end.
1758 
1759         Parameters
1760         ----------
1761         dim : hashable or sequence of hashable, optional
1762             Dimension(s) over which to unstack. By default unstacks all
1763             MultiIndexes.
1764         fill_value: value to be filled. By default, np.nan
1765         sparse: use sparse-array if True
1766 
1767         Returns
1768         -------
1769         unstacked : DataArray
1770             Array with unstacked data.
1771 
1772         Examples
1773         --------
1774 
1775         >>> arr = xr.DataArray(
1776         ...     np.arange(6).reshape(2, 3),
1777         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
1778         ... )
1779         >>> arr
1780         <xarray.DataArray (x: 2, y: 3)>
1781         array([[0, 1, 2],
1782                [3, 4, 5]])
1783         Coordinates:
1784           * x        (x) |S1 'a' 'b'
1785           * y        (y) int64 0 1 2
1786         >>> stacked = arr.stack(z=("x", "y"))
1787         >>> stacked.indexes["z"]
1788         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1789                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],
1790                    names=['x', 'y'])
1791         >>> roundtripped = stacked.unstack()
1792         >>> arr.identical(roundtripped)
1793         True
1794 
1795         See Also
1796         --------
1797         DataArray.stack
1798         """
1799         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
1800         return self._from_temp_dataset(ds)
1801 
1802     def to_unstacked_dataset(self, dim, level=0):
1803         """Unstack DataArray expanding to Dataset along a given level of a
1804         stacked coordinate.
1805 
1806         This is the inverse operation of Dataset.to_stacked_array.
1807 
1808         Parameters
1809         ----------
1810         dim : str
1811             Name of existing dimension to unstack
1812         level : int or str
1813             The MultiIndex level to expand to a dataset along. Can either be
1814             the integer index of the level or its name.
1815         label : int, default 0
1816             Label of the level to expand dataset along. Overrides the label
1817             argument if given.
1818 
1819         Returns
1820         -------
1821         unstacked: Dataset
1822 
1823         Examples
1824         --------
1825         >>> import xarray as xr
1826         >>> arr = xr.DataArray(
1827         ...     np.arange(6).reshape(2, 3),
1828         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
1829         ... )
1830         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
1831         >>> data
1832         <xarray.Dataset>
1833         Dimensions:  (x: 2, y: 3)
1834         Coordinates:
1835           * x        (x) <U1 'a' 'b'
1836           * y        (y) int64 0 1 2
1837         Data variables:
1838             a        (x, y) int64 0 1 2 3 4 5
1839             b        (x) int64 0 3
1840         >>> stacked = data.to_stacked_array("z", ["y"])
1841         >>> stacked.indexes["z"]
1842         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1843                 labels=[[0, 0, 0, 1], [0, 1, 2, -1]],
1844                 names=['variable', 'y'])
1845         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
1846         >>> data.identical(roundtripped)
1847         True
1848 
1849         See Also
1850         --------
1851         Dataset.to_stacked_array
1852         """
1853 
1854         idx = self.indexes[dim]
1855         if not isinstance(idx, pd.MultiIndex):
1856             raise ValueError(f"'{dim}' is not a stacked coordinate")
1857 
1858         level_number = idx._get_level_number(level)
1859         variables = idx.levels[level_number]
1860         variable_dim = idx.names[level_number]
1861 
1862         # pull variables out of datarray
1863         data_dict = {}
1864         for k in variables:
1865             data_dict[k] = self.sel({variable_dim: k}).squeeze(drop=True)
1866 
1867         # unstacked dataset
1868         return Dataset(data_dict)
1869 
1870     def transpose(self, *dims: Hashable, transpose_coords: bool = None) -> "DataArray":
1871         """Return a new DataArray object with transposed dimensions.
1872 
1873         Parameters
1874         ----------
1875         *dims : hashable, optional
1876             By default, reverse the dimensions. Otherwise, reorder the
1877             dimensions to this order.
1878         transpose_coords : boolean, optional
1879             If True, also transpose the coordinates of this DataArray.
1880 
1881         Returns
1882         -------
1883         transposed : DataArray
1884             The returned DataArray's array is transposed.
1885 
1886         Notes
1887         -----
1888         This operation returns a view of this array's data. It is
1889         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
1890         -- the data will be fully loaded.
1891 
1892         See Also
1893         --------
1894         numpy.transpose
1895         Dataset.transpose
1896         """
1897         if dims:
1898             dims = tuple(utils.infix_dims(dims, self.dims))
1899         variable = self.variable.transpose(*dims)
1900         if transpose_coords:
1901             coords: Dict[Hashable, Variable] = {}
1902             for name, coord in self.coords.items():
1903                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
1904                 coords[name] = coord.variable.transpose(*coord_dims)
1905             return self._replace(variable, coords)
1906         else:
1907             if transpose_coords is None and any(self[c].ndim > 1 for c in self.coords):
1908                 warnings.warn(
1909                     "This DataArray contains multi-dimensional "
1910                     "coordinates. In the future, these coordinates "
1911                     "will be transposed as well unless you specify "
1912                     "transpose_coords=False.",
1913                     FutureWarning,
1914                     stacklevel=2,
1915                 )
1916             return self._replace(variable)
1917 
1918     @property
1919     def T(self) -> "DataArray":
1920         return self.transpose()
1921 
1922     def drop_vars(
1923         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
1924     ) -> "DataArray":
1925         """Drop variables from this DataArray.
1926 
1927         Parameters
1928         ----------
1929         names : hashable or iterable of hashables
1930             Name(s) of variables to drop.
1931         errors: {'raise', 'ignore'}, optional
1932             If 'raise' (default), raises a ValueError error if any of the variable
1933             passed are not in the dataset. If 'ignore', any given names that are in the
1934             DataArray are dropped and no error is raised.
1935 
1936         Returns
1937         -------
1938         dropped : Dataset
1939 
1940         """
1941         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
1942         return self._from_temp_dataset(ds)
1943 
1944     def drop(
1945         self,
1946         labels: Mapping = None,
1947         dim: Hashable = None,
1948         *,
1949         errors: str = "raise",
1950         **labels_kwargs,
1951     ) -> "DataArray":
1952         """Backward compatible method based on `drop_vars` and `drop_sel`
1953 
1954         Using either `drop_vars` or `drop_sel` is encouraged
1955 
1956         See Also
1957         --------
1958         DataArray.drop_vars
1959         DataArray.drop_sel
1960         """
1961         ds = self._to_temp_dataset().drop(labels, dim, errors=errors)
1962         return self._from_temp_dataset(ds)
1963 
1964     def drop_sel(
1965         self,
1966         labels: Mapping[Hashable, Any] = None,
1967         *,
1968         errors: str = "raise",
1969         **labels_kwargs,
1970     ) -> "DataArray":
1971         """Drop index labels from this DataArray.
1972 
1973         Parameters
1974         ----------
1975         labels : Mapping[Hashable, Any]
1976             Index labels to drop
1977         errors: {'raise', 'ignore'}, optional
1978             If 'raise' (default), raises a ValueError error if
1979             any of the index labels passed are not
1980             in the dataset. If 'ignore', any given labels that are in the
1981             dataset are dropped and no error is raised.
1982         **labels_kwargs : {dim: label, ...}, optional
1983             The keyword arguments form of ``dim`` and ``labels``
1984 
1985         Returns
1986         -------
1987         dropped : DataArray
1988         """
1989         if labels_kwargs or isinstance(labels, dict):
1990             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
1991 
1992         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
1993         return self._from_temp_dataset(ds)
1994 
1995     def dropna(
1996         self, dim: Hashable, how: str = "any", thresh: int = None
1997     ) -> "DataArray":
1998         """Returns a new array with dropped labels for missing values along
1999         the provided dimension.
2000 
2001         Parameters
2002         ----------
2003         dim : hashable
2004             Dimension along which to drop missing values. Dropping along
2005             multiple dimensions simultaneously is not yet supported.
2006         how : {'any', 'all'}, optional
2007             * any : if any NA values are present, drop that label
2008             * all : if all values are NA, drop that label
2009         thresh : int, default None
2010             If supplied, require this many non-NA values.
2011 
2012         Returns
2013         -------
2014         DataArray
2015         """
2016         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
2017         return self._from_temp_dataset(ds)
2018 
2019     def fillna(self, value: Any) -> "DataArray":
2020         """Fill missing values in this object.
2021 
2022         This operation follows the normal broadcasting and alignment rules that
2023         xarray uses for binary arithmetic, except the result is aligned to this
2024         object (``join='left'``) instead of aligned to the intersection of
2025         index coordinates (``join='inner'``).
2026 
2027         Parameters
2028         ----------
2029         value : scalar, ndarray or DataArray
2030             Used to fill all matching missing values in this array. If the
2031             argument is a DataArray, it is first aligned with (reindexed to)
2032             this array.
2033 
2034         Returns
2035         -------
2036         DataArray
2037         """
2038         if utils.is_dict_like(value):
2039             raise TypeError(
2040                 "cannot provide fill value as a dictionary with "
2041                 "fillna on a DataArray"
2042             )
2043         out = ops.fillna(self, value)
2044         return out
2045 
2046     def interpolate_na(
2047         self,
2048         dim: Hashable = None,
2049         method: str = "linear",
2050         limit: int = None,
2051         use_coordinate: Union[bool, str] = True,
2052         max_gap: Union[
2053             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta
2054         ] = None,
2055         **kwargs: Any,
2056     ) -> "DataArray":
2057         """Fill in NaNs by interpolating according to different methods.
2058 
2059         Parameters
2060         ----------
2061         dim : str
2062             Specifies the dimension along which to interpolate.
2063         method : str, optional
2064             String indicating which method to use for interpolation:
2065 
2066             - 'linear': linear interpolation (Default). Additional keyword
2067               arguments are passed to :py:func:`numpy.interp`
2068             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
2069               are passed to :py:func:`scipy.interpolate.interp1d`. If
2070               ``method='polynomial'``, the ``order`` keyword argument must also be
2071               provided.
2072             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
2073               respective :py:class:`scipy.interpolate` classes.
2074 
2075         use_coordinate : bool, str, default True
2076             Specifies which index to use as the x values in the interpolation
2077             formulated as `y = f(x)`. If False, values are treated as if
2078             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
2079             used. If ``use_coordinate`` is a string, it specifies the name of a
2080             coordinate variariable to use as the index.
2081         limit : int, default None
2082             Maximum number of consecutive NaNs to fill. Must be greater than 0
2083             or None for no limit. This filling is done regardless of the size of
2084             the gap in the data. To only interpolate over gaps less than a given length,
2085             see ``max_gap``.
2086         max_gap: int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default None.
2087             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
2088             Use None for no limit. When interpolating along a datetime64 dimension
2089             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
2090 
2091             - a string that is valid input for pandas.to_timedelta
2092             - a :py:class:`numpy.timedelta64` object
2093             - a :py:class:`pandas.Timedelta` object
2094             - a :py:class:`datetime.timedelta` object
2095 
2096             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
2097             dimensions has not been implemented yet. Gap length is defined as the difference
2098             between coordinate values at the first data point after a gap and the last value
2099             before a gap. For gaps at the beginning (end), gap length is defined as the difference
2100             between coordinate values at the first (last) valid data point and the first (last) NaN.
2101             For example, consider::
2102 
2103                 <xarray.DataArray (x: 9)>
2104                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
2105                 Coordinates:
2106                   * x        (x) int64 0 1 2 3 4 5 6 7 8
2107 
2108             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
2109         kwargs : dict, optional
2110             parameters passed verbatim to the underlying interpolation function
2111 
2112         Returns
2113         -------
2114         interpolated: DataArray
2115             Filled in DataArray.
2116 
2117         See also
2118         --------
2119         numpy.interp
2120         scipy.interpolate
2121         """
2122         from .missing import interp_na
2123 
2124         return interp_na(
2125             self,
2126             dim=dim,
2127             method=method,
2128             limit=limit,
2129             use_coordinate=use_coordinate,
2130             max_gap=max_gap,
2131             **kwargs,
2132         )
2133 
2134     def ffill(self, dim: Hashable, limit: int = None) -> "DataArray":
2135         """Fill NaN values by propogating values forward
2136 
2137         *Requires bottleneck.*
2138 
2139         Parameters
2140         ----------
2141         dim : hashable
2142             Specifies the dimension along which to propagate values when
2143             filling.
2144         limit : int, default None
2145             The maximum number of consecutive NaN values to forward fill. In
2146             other words, if there is a gap with more than this number of
2147             consecutive NaNs, it will only be partially filled. Must be greater
2148             than 0 or None for no limit.
2149 
2150         Returns
2151         -------
2152         DataArray
2153         """
2154         from .missing import ffill
2155 
2156         return ffill(self, dim, limit=limit)
2157 
2158     def bfill(self, dim: Hashable, limit: int = None) -> "DataArray":
2159         """Fill NaN values by propogating values backward
2160 
2161         *Requires bottleneck.*
2162 
2163         Parameters
2164         ----------
2165         dim : str
2166             Specifies the dimension along which to propagate values when
2167             filling.
2168         limit : int, default None
2169             The maximum number of consecutive NaN values to backward fill. In
2170             other words, if there is a gap with more than this number of
2171             consecutive NaNs, it will only be partially filled. Must be greater
2172             than 0 or None for no limit.
2173 
2174         Returns
2175         -------
2176         DataArray
2177         """
2178         from .missing import bfill
2179 
2180         return bfill(self, dim, limit=limit)
2181 
2182     def combine_first(self, other: "DataArray") -> "DataArray":
2183         """Combine two DataArray objects, with union of coordinates.
2184 
2185         This operation follows the normal broadcasting and alignment rules of
2186         ``join='outer'``.  Default to non-null values of array calling the
2187         method.  Use np.nan to fill in vacant cells after alignment.
2188 
2189         Parameters
2190         ----------
2191         other : DataArray
2192             Used to fill all matching missing values in this array.
2193 
2194         Returns
2195         -------
2196         DataArray
2197         """
2198         return ops.fillna(self, other, join="outer")
2199 
2200     def reduce(
2201         self,
2202         func: Callable[..., Any],
2203         dim: Union[None, Hashable, Sequence[Hashable]] = None,
2204         axis: Union[None, int, Sequence[int]] = None,
2205         keep_attrs: bool = None,
2206         keepdims: bool = False,
2207         **kwargs: Any,
2208     ) -> "DataArray":
2209         """Reduce this array by applying `func` along some dimension(s).
2210 
2211         Parameters
2212         ----------
2213         func : function
2214             Function which can be called in the form
2215             `f(x, axis=axis, **kwargs)` to return the result of reducing an
2216             np.ndarray over an integer valued axis.
2217         dim : hashable or sequence of hashables, optional
2218             Dimension(s) over which to apply `func`.
2219         axis : int or sequence of int, optional
2220             Axis(es) over which to repeatedly apply `func`. Only one of the
2221             'dim' and 'axis' arguments can be supplied. If neither are
2222             supplied, then the reduction is calculated over the flattened array
2223             (by calling `f(x)` without an axis argument).
2224         keep_attrs : bool, optional
2225             If True, the variable's attributes (`attrs`) will be copied from
2226             the original object to the new one.  If False (default), the new
2227             object will be returned without attributes.
2228         keepdims : bool, default False
2229             If True, the dimensions which are reduced are left in the result
2230             as dimensions of size one. Coordinates that use these dimensions
2231             are removed.
2232         **kwargs : dict
2233             Additional keyword arguments passed on to `func`.
2234 
2235         Returns
2236         -------
2237         reduced : DataArray
2238             DataArray with this object's array replaced with an array with
2239             summarized data and the indicated dimension(s) removed.
2240         """
2241 
2242         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
2243         return self._replace_maybe_drop_dims(var)
2244 
2245     def to_pandas(self) -> Union["DataArray", pd.Series, pd.DataFrame]:
2246         """Convert this array into a pandas object with the same shape.
2247 
2248         The type of the returned object depends on the number of DataArray
2249         dimensions:
2250 
2251         * 0D -> `xarray.DataArray`
2252         * 1D -> `pandas.Series`
2253         * 2D -> `pandas.DataFrame`
2254 
2255         Only works for arrays with 2 or fewer dimensions.
2256 
2257         The DataArray constructor performs the inverse transformation.
2258         """
2259         # TODO: consolidate the info about pandas constructors and the
2260         # attributes that correspond to their indexes into a separate module?
2261         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
2262         try:
2263             constructor = constructors[self.ndim]
2264         except KeyError:
2265             raise ValueError(
2266                 "cannot convert arrays with %s dimensions into "
2267                 "pandas objects" % self.ndim
2268             )
2269         indexes = [self.get_index(dim) for dim in self.dims]
2270         return constructor(self.values, *indexes)
2271 
2272     def to_dataframe(self, name: Hashable = None) -> pd.DataFrame:
2273         """Convert this array and its coordinates into a tidy pandas.DataFrame.
2274 
2275         The DataFrame is indexed by the Cartesian product of index coordinates
2276         (in the form of a :py:class:`pandas.MultiIndex`).
2277 
2278         Other coordinates are included as columns in the DataFrame.
2279         """
2280         if name is None:
2281             name = self.name
2282         if name is None:
2283             raise ValueError(
2284                 "cannot convert an unnamed DataArray to a "
2285                 "DataFrame: use the ``name`` parameter"
2286             )
2287 
2288         dims = dict(zip(self.dims, self.shape))
2289         # By using a unique name, we can convert a DataArray into a DataFrame
2290         # even if it shares a name with one of its coordinates.
2291         # I would normally use unique_name = object() but that results in a
2292         # dataframe with columns in the wrong order, for reasons I have not
2293         # been able to debug (possibly a pandas bug?).
2294         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
2295         ds = self._to_dataset_whole(name=unique_name)
2296         df = ds._to_dataframe(dims)
2297         df.columns = [name if c == unique_name else c for c in df.columns]
2298         return df
2299 
2300     def to_series(self) -> pd.Series:
2301         """Convert this array into a pandas.Series.
2302 
2303         The Series is indexed by the Cartesian product of index coordinates
2304         (in the form of a :py:class:`pandas.MultiIndex`).
2305         """
2306         index = self.coords.to_index()
2307         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
2308 
2309     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
2310         """Convert this array into a numpy.ma.MaskedArray
2311 
2312         Parameters
2313         ----------
2314         copy : bool
2315             If True (default) make a copy of the array in the result. If False,
2316             a MaskedArray view of DataArray.values is returned.
2317 
2318         Returns
2319         -------
2320         result : MaskedArray
2321             Masked where invalid values (nan or inf) occur.
2322         """
2323         values = self.values  # only compute lazy arrays once
2324         isnull = pd.isnull(values)
2325         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
2326 
2327     def to_netcdf(self, *args, **kwargs) -> Union[bytes, "Delayed", None]:
2328         """Write DataArray contents to a netCDF file.
2329 
2330         All parameters are passed directly to `xarray.Dataset.to_netcdf`.
2331 
2332         Notes
2333         -----
2334         Only xarray.Dataset objects can be written to netCDF files, so
2335         the xarray.DataArray is converted to a xarray.Dataset object
2336         containing a single variable. If the DataArray has no name, or if the
2337         name is the same as a co-ordinate name, then it is given the name
2338         '__xarray_dataarray_variable__'.
2339         """
2340         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE
2341 
2342         if self.name is None:
2343             # If no name is set then use a generic xarray name
2344             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2345         elif self.name in self.coords or self.name in self.dims:
2346             # The name is the same as one of the coords names, which netCDF
2347             # doesn't support, so rename it but keep track of the old name
2348             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2349             dataset.attrs[DATAARRAY_NAME] = self.name
2350         else:
2351             # No problems with the name - so we're fine!
2352             dataset = self.to_dataset()
2353 
2354         return dataset.to_netcdf(*args, **kwargs)
2355 
2356     def to_dict(self, data: bool = True) -> dict:
2357         """
2358         Convert this xarray.DataArray into a dictionary following xarray
2359         naming conventions.
2360 
2361         Converts all variables and attributes to native Python objects.
2362         Useful for converting to json. To avoid datetime incompatibility
2363         use decode_times=False kwarg in xarrray.open_dataset.
2364 
2365         Parameters
2366         ----------
2367         data : bool, optional
2368             Whether to include the actual data in the dictionary. When set to
2369             False, returns just the schema.
2370 
2371         See also
2372         --------
2373         DataArray.from_dict
2374         """
2375         d = self.variable.to_dict(data=data)
2376         d.update({"coords": {}, "name": self.name})
2377         for k in self.coords:
2378             d["coords"][k] = self.coords[k].variable.to_dict(data=data)
2379         return d
2380 
2381     @classmethod
2382     def from_dict(cls, d: dict) -> "DataArray":
2383         """
2384         Convert a dictionary into an xarray.DataArray
2385 
2386         Input dict can take several forms::
2387 
2388             d = {'dims': ('t'), 'data': x}
2389 
2390             d = {'coords': {'t': {'dims': 't', 'data': t,
2391                                   'attrs': {'units':'s'}}},
2392                  'attrs': {'title': 'air temperature'},
2393                  'dims': 't',
2394                  'data': x,
2395                  'name': 'a'}
2396 
2397         where 't' is the name of the dimesion, 'a' is the name of the array,
2398         and  x and t are lists, numpy.arrays, or pandas objects.
2399 
2400         Parameters
2401         ----------
2402         d : dict, with a minimum structure of {'dims': [..], 'data': [..]}
2403 
2404         Returns
2405         -------
2406         obj : xarray.DataArray
2407 
2408         See also
2409         --------
2410         DataArray.to_dict
2411         Dataset.from_dict
2412         """
2413         coords = None
2414         if "coords" in d:
2415             try:
2416                 coords = {
2417                     k: (v["dims"], v["data"], v.get("attrs"))
2418                     for k, v in d["coords"].items()
2419                 }
2420             except KeyError as e:
2421                 raise ValueError(
2422                     "cannot convert dict when coords are missing the key "
2423                     "'{dims_data}'".format(dims_data=str(e.args[0]))
2424                 )
2425         try:
2426             data = d["data"]
2427         except KeyError:
2428             raise ValueError("cannot convert dict without the key 'data''")
2429         else:
2430             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
2431         return obj
2432 
2433     @classmethod
2434     def from_series(cls, series: pd.Series, sparse: bool = False) -> "DataArray":
2435         """Convert a pandas.Series into an xarray.DataArray.
2436 
2437         If the series's index is a MultiIndex, it will be expanded into a
2438         tensor product of one-dimensional coordinates (filling in missing
2439         values with NaN). Thus this operation should be the inverse of the
2440         `to_series` method.
2441 
2442         If sparse=True, creates a sparse array instead of a dense NumPy array.
2443         Requires the pydata/sparse package.
2444 
2445         See also
2446         --------
2447         xarray.Dataset.from_dataframe
2448         """
2449         temp_name = "__temporary_name"
2450         df = pd.DataFrame({temp_name: series})
2451         ds = Dataset.from_dataframe(df, sparse=sparse)
2452         result = cast(DataArray, ds[temp_name])
2453         result.name = series.name
2454         return result
2455 
2456     def to_cdms2(self) -> "cdms2_Variable":
2457         """Convert this array into a cdms2.Variable
2458         """
2459         from ..convert import to_cdms2
2460 
2461         return to_cdms2(self)
2462 
2463     @classmethod
2464     def from_cdms2(cls, variable: "cdms2_Variable") -> "DataArray":
2465         """Convert a cdms2.Variable into an xarray.DataArray
2466         """
2467         from ..convert import from_cdms2
2468 
2469         return from_cdms2(variable)
2470 
2471     def to_iris(self) -> "iris_Cube":
2472         """Convert this array into a iris.cube.Cube
2473         """
2474         from ..convert import to_iris
2475 
2476         return to_iris(self)
2477 
2478     @classmethod
2479     def from_iris(cls, cube: "iris_Cube") -> "DataArray":
2480         """Convert a iris.cube.Cube into an xarray.DataArray
2481         """
2482         from ..convert import from_iris
2483 
2484         return from_iris(cube)
2485 
2486     def _all_compat(self, other: "DataArray", compat_str: str) -> bool:
2487         """Helper function for equals, broadcast_equals, and identical
2488         """
2489 
2490         def compat(x, y):
2491             return getattr(x.variable, compat_str)(y.variable)
2492 
2493         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
2494             self, other
2495         )
2496 
2497     def broadcast_equals(self, other: "DataArray") -> bool:
2498         """Two DataArrays are broadcast equal if they are equal after
2499         broadcasting them against each other such that they have the same
2500         dimensions.
2501 
2502         See Also
2503         --------
2504         DataArray.equals
2505         DataArray.identical
2506         """
2507         try:
2508             return self._all_compat(other, "broadcast_equals")
2509         except (TypeError, AttributeError):
2510             return False
2511 
2512     def equals(self, other: "DataArray") -> bool:
2513         """True if two DataArrays have the same dimensions, coordinates and
2514         values; otherwise False.
2515 
2516         DataArrays can still be equal (like pandas objects) if they have NaN
2517         values in the same locations.
2518 
2519         This method is necessary because `v1 == v2` for ``DataArray``
2520         does element-wise comparisons (like numpy.ndarrays).
2521 
2522         See Also
2523         --------
2524         DataArray.broadcast_equals
2525         DataArray.identical
2526         """
2527         try:
2528             return self._all_compat(other, "equals")
2529         except (TypeError, AttributeError):
2530             return False
2531 
2532     def identical(self, other: "DataArray") -> bool:
2533         """Like equals, but also checks the array name and attributes, and
2534         attributes on all coordinates.
2535 
2536         See Also
2537         --------
2538         DataArray.broadcast_equals
2539         DataArray.equal
2540         """
2541         try:
2542             return self.name == other.name and self._all_compat(other, "identical")
2543         except (TypeError, AttributeError):
2544             return False
2545 
2546     def _result_name(self, other: Any = None) -> Optional[Hashable]:
2547         # use the same naming heuristics as pandas:
2548         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
2549         other_name = getattr(other, "name", _default)
2550         if other_name is _default or other_name == self.name:
2551             return self.name
2552         else:
2553             return None
2554 
2555     def __array_wrap__(self, obj, context=None) -> "DataArray":
2556         new_var = self.variable.__array_wrap__(obj, context)
2557         return self._replace(new_var)
2558 
2559     def __matmul__(self, obj):
2560         return self.dot(obj)
2561 
2562     def __rmatmul__(self, other):
2563         # currently somewhat duplicative, as only other DataArrays are
2564         # compatible with matmul
2565         return computation.dot(other, self)
2566 
2567     @staticmethod
2568     def _unary_op(f: Callable[..., Any]) -> Callable[..., "DataArray"]:
2569         @functools.wraps(f)
2570         def func(self, *args, **kwargs):
2571             with np.errstate(all="ignore"):
2572                 return self.__array_wrap__(f(self.variable.data, *args, **kwargs))
2573 
2574         return func
2575 
2576     @staticmethod
2577     def _binary_op(
2578         f: Callable[..., Any],
2579         reflexive: bool = False,
2580         join: str = None,  # see xarray.align
2581         **ignored_kwargs,
2582     ) -> Callable[..., "DataArray"]:
2583         @functools.wraps(f)
2584         def func(self, other):
2585             if isinstance(other, (Dataset, groupby.GroupBy)):
2586                 return NotImplemented
2587             if isinstance(other, DataArray):
2588                 align_type = OPTIONS["arithmetic_join"] if join is None else join
2589                 self, other = align(self, other, join=align_type, copy=False)
2590             other_variable = getattr(other, "variable", other)
2591             other_coords = getattr(other, "coords", None)
2592 
2593             variable = (
2594                 f(self.variable, other_variable)
2595                 if not reflexive
2596                 else f(other_variable, self.variable)
2597             )
2598             coords, indexes = self.coords._merge_raw(other_coords)
2599             name = self._result_name(other)
2600 
2601             return self._replace(variable, coords, name, indexes=indexes)
2602 
2603         return func
2604 
2605     @staticmethod
2606     def _inplace_binary_op(f: Callable) -> Callable[..., "DataArray"]:
2607         @functools.wraps(f)
2608         def func(self, other):
2609             if isinstance(other, groupby.GroupBy):
2610                 raise TypeError(
2611                     "in-place operations between a DataArray and "
2612                     "a grouped object are not permitted"
2613                 )
2614             # n.b. we can't align other to self (with other.reindex_like(self))
2615             # because `other` may be converted into floats, which would cause
2616             # in-place arithmetic to fail unpredictably. Instead, we simply
2617             # don't support automatic alignment with in-place arithmetic.
2618             other_coords = getattr(other, "coords", None)
2619             other_variable = getattr(other, "variable", other)
2620             with self.coords._merge_inplace(other_coords):
2621                 f(self.variable, other_variable)
2622             return self
2623 
2624         return func
2625 
2626     def _copy_attrs_from(self, other: Union["DataArray", Dataset, Variable]) -> None:
2627         self.attrs = other.attrs
2628 
2629     @property
2630     def plot(self) -> _PlotMethods:
2631         """
2632         Access plotting functions for DataArray's
2633 
2634         >>> d = xr.DataArray([[1, 2], [3, 4]])
2635 
2636         For convenience just call this directly
2637 
2638         >>> d.plot()
2639 
2640         Or use it as a namespace to use xarray.plot functions as
2641         DataArray methods
2642 
2643         >>> d.plot.imshow()  # equivalent to xarray.plot.imshow(d)
2644 
2645         """
2646         return _PlotMethods(self)
2647 
2648     def _title_for_slice(self, truncate: int = 50) -> str:
2649         """
2650         If the dataarray has 1 dimensional coordinates or comes from a slice
2651         we can show that info in the title
2652 
2653         Parameters
2654         ----------
2655         truncate : integer
2656             maximum number of characters for title
2657 
2658         Returns
2659         -------
2660         title : string
2661             Can be used for plot titles
2662 
2663         """
2664         one_dims = []
2665         for dim, coord in self.coords.items():
2666             if coord.size == 1:
2667                 one_dims.append(
2668                     "{dim} = {v}".format(dim=dim, v=format_item(coord.values))
2669                 )
2670 
2671         title = ", ".join(one_dims)
2672         if len(title) > truncate:
2673             title = title[: (truncate - 3)] + "..."
2674 
2675         return title
2676 
2677     def diff(self, dim: Hashable, n: int = 1, label: Hashable = "upper") -> "DataArray":
2678         """Calculate the n-th order discrete difference along given axis.
2679 
2680         Parameters
2681         ----------
2682         dim : hashable, optional
2683             Dimension over which to calculate the finite difference.
2684         n : int, optional
2685             The number of times values are differenced.
2686         label : hashable, optional
2687             The new coordinate in dimension ``dim`` will have the
2688             values of either the minuend's or subtrahend's coordinate
2689             for values 'upper' and 'lower', respectively.  Other
2690             values are not supported.
2691 
2692         Returns
2693         -------
2694         difference : same type as caller
2695             The n-th order finite difference of this object.
2696 
2697         .. note::
2698 
2699             `n` matches numpy's behavior and is different from pandas' first
2700             argument named `periods`.
2701 
2702 
2703         Examples
2704         --------
2705         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
2706         >>> arr.diff("x")
2707         <xarray.DataArray (x: 3)>
2708         array([0, 1, 0])
2709         Coordinates:
2710         * x        (x) int64 2 3 4
2711         >>> arr.diff("x", 2)
2712         <xarray.DataArray (x: 2)>
2713         array([ 1, -1])
2714         Coordinates:
2715         * x        (x) int64 3 4
2716 
2717         See Also
2718         --------
2719         DataArray.differentiate
2720         """
2721         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
2722         return self._from_temp_dataset(ds)
2723 
2724     def shift(
2725         self,
2726         shifts: Mapping[Hashable, int] = None,
2727         fill_value: Any = dtypes.NA,
2728         **shifts_kwargs: int,
2729     ) -> "DataArray":
2730         """Shift this array by an offset along one or more dimensions.
2731 
2732         Only the data is moved; coordinates stay in place. Values shifted from
2733         beyond array bounds are replaced by NaN. This is consistent with the
2734         behavior of ``shift`` in pandas.
2735 
2736         Parameters
2737         ----------
2738         shifts : Mapping with the form of {dim: offset}
2739             Integer offset to shift along each of the given dimensions.
2740             Positive offsets shift to the right; negative offsets shift to the
2741             left.
2742         fill_value: scalar, optional
2743             Value to use for newly missing values
2744         **shifts_kwargs:
2745             The keyword arguments form of ``shifts``.
2746             One of shifts or shifts_kwargs must be provided.
2747 
2748         Returns
2749         -------
2750         shifted : DataArray
2751             DataArray with the same coordinates and attributes but shifted
2752             data.
2753 
2754         See also
2755         --------
2756         roll
2757 
2758         Examples
2759         --------
2760 
2761         >>> arr = xr.DataArray([5, 6, 7], dims="x")
2762         >>> arr.shift(x=1)
2763         <xarray.DataArray (x: 3)>
2764         array([ nan,   5.,   6.])
2765         Coordinates:
2766           * x        (x) int64 0 1 2
2767         """
2768         variable = self.variable.shift(
2769             shifts=shifts, fill_value=fill_value, **shifts_kwargs
2770         )
2771         return self._replace(variable=variable)
2772 
2773     def roll(
2774         self,
2775         shifts: Mapping[Hashable, int] = None,
2776         roll_coords: bool = None,
2777         **shifts_kwargs: int,
2778     ) -> "DataArray":
2779         """Roll this array by an offset along one or more dimensions.
2780 
2781         Unlike shift, roll may rotate all variables, including coordinates
2782         if specified. The direction of rotation is consistent with
2783         :py:func:`numpy.roll`.
2784 
2785         Parameters
2786         ----------
2787         shifts : Mapping with the form of {dim: offset}
2788             Integer offset to rotate each of the given dimensions.
2789             Positive offsets roll to the right; negative offsets roll to the
2790             left.
2791         roll_coords : bool
2792             Indicates whether to  roll the coordinates by the offset
2793             The current default of roll_coords (None, equivalent to True) is
2794             deprecated and will change to False in a future version.
2795             Explicitly pass roll_coords to silence the warning.
2796         **shifts_kwargs : The keyword arguments form of ``shifts``.
2797             One of shifts or shifts_kwargs must be provided.
2798 
2799         Returns
2800         -------
2801         rolled : DataArray
2802             DataArray with the same attributes but rolled data and coordinates.
2803 
2804         See also
2805         --------
2806         shift
2807 
2808         Examples
2809         --------
2810 
2811         >>> arr = xr.DataArray([5, 6, 7], dims="x")
2812         >>> arr.roll(x=1)
2813         <xarray.DataArray (x: 3)>
2814         array([7, 5, 6])
2815         Coordinates:
2816           * x        (x) int64 2 0 1
2817         """
2818         ds = self._to_temp_dataset().roll(
2819             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
2820         )
2821         return self._from_temp_dataset(ds)
2822 
2823     @property
2824     def real(self) -> "DataArray":
2825         return self._replace(self.variable.real)
2826 
2827     @property
2828     def imag(self) -> "DataArray":
2829         return self._replace(self.variable.imag)
2830 
2831     def dot(
2832         self, other: "DataArray", dims: Union[Hashable, Sequence[Hashable], None] = None
2833     ) -> "DataArray":
2834         """Perform dot product of two DataArrays along their shared dims.
2835 
2836         Equivalent to taking taking tensordot over all shared dims.
2837 
2838         Parameters
2839         ----------
2840         other : DataArray
2841             The other array with which the dot product is performed.
2842         dims: '...', hashable or sequence of hashables, optional
2843             Which dimensions to sum over. Ellipsis ('...') sums over all dimensions.
2844             If not specified, then all the common dimensions are summed over.
2845 
2846         Returns
2847         -------
2848         result : DataArray
2849             Array resulting from the dot product over all shared dimensions.
2850 
2851         See also
2852         --------
2853         dot
2854         numpy.tensordot
2855 
2856         Examples
2857         --------
2858 
2859         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
2860         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
2861         >>> dm_vals = np.arange(4)
2862         >>> dm = xr.DataArray(dm_vals, dims=["z"])
2863 
2864         >>> dm.dims
2865         ('z')
2866 
2867         >>> da.dims
2868         ('x', 'y', 'z')
2869 
2870         >>> dot_result = da.dot(dm)
2871         >>> dot_result.dims
2872         ('x', 'y')
2873 
2874         """
2875         if isinstance(other, Dataset):
2876             raise NotImplementedError(
2877                 "dot products are not yet supported with Dataset objects."
2878             )
2879         if not isinstance(other, DataArray):
2880             raise TypeError("dot only operates on DataArrays.")
2881 
2882         return computation.dot(self, other, dims=dims)
2883 
2884     def sortby(
2885         self,
2886         variables: Union[Hashable, "DataArray", Sequence[Union[Hashable, "DataArray"]]],
2887         ascending: bool = True,
2888     ) -> "DataArray":
2889         """Sort object by labels or values (along an axis).
2890 
2891         Sorts the dataarray, either along specified dimensions,
2892         or according to values of 1-D dataarrays that share dimension
2893         with calling object.
2894 
2895         If the input variables are dataarrays, then the dataarrays are aligned
2896         (via left-join) to the calling object prior to sorting by cell values.
2897         NaNs are sorted to the end, following Numpy convention.
2898 
2899         If multiple sorts along the same dimension is
2900         given, numpy's lexsort is performed along that dimension:
2901         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
2902         and the FIRST key in the sequence is used as the primary sort key,
2903         followed by the 2nd key, etc.
2904 
2905         Parameters
2906         ----------
2907         variables: hashable, DataArray, or sequence of either
2908             1D DataArray objects or name(s) of 1D variable(s) in
2909             coords whose values are used to sort this array.
2910         ascending: boolean, optional
2911             Whether to sort by ascending or descending order.
2912 
2913         Returns
2914         -------
2915         sorted: DataArray
2916             A new dataarray where all the specified dims are sorted by dim
2917             labels.
2918 
2919         Examples
2920         --------
2921 
2922         >>> da = xr.DataArray(
2923         ...     np.random.rand(5),
2924         ...     coords=[pd.date_range("1/1/2000", periods=5)],
2925         ...     dims="time",
2926         ... )
2927         >>> da
2928         <xarray.DataArray (time: 5)>
2929         array([ 0.965471,  0.615637,  0.26532 ,  0.270962,  0.552878])
2930         Coordinates:
2931           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...
2932 
2933         >>> da.sortby(da)
2934         <xarray.DataArray (time: 5)>
2935         array([ 0.26532 ,  0.270962,  0.552878,  0.615637,  0.965471])
2936         Coordinates:
2937           * time     (time) datetime64[ns] 2000-01-03 2000-01-04 2000-01-05 ...
2938         """
2939         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
2940         return self._from_temp_dataset(ds)
2941 
2942     def quantile(
2943         self,
2944         q: Any,
2945         dim: Union[Hashable, Sequence[Hashable], None] = None,
2946         interpolation: str = "linear",
2947         keep_attrs: bool = None,
2948         skipna: bool = True,
2949     ) -> "DataArray":
2950         """Compute the qth quantile of the data along the specified dimension.
2951 
2952         Returns the qth quantiles(s) of the array elements.
2953 
2954         Parameters
2955         ----------
2956         q : float in range of [0,1] or array-like of floats
2957             Quantile to compute, which must be between 0 and 1 inclusive.
2958         dim : hashable or sequence of hashable, optional
2959             Dimension(s) over which to apply quantile.
2960         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
2961             This optional parameter specifies the interpolation method to
2962             use when the desired quantile lies between two data points
2963             ``i < j``:
2964 
2965                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is
2966                   the fractional part of the index surrounded by ``i`` and
2967                   ``j``.
2968                 - lower: ``i``.
2969                 - higher: ``j``.
2970                 - nearest: ``i`` or ``j``, whichever is nearest.
2971                 - midpoint: ``(i + j) / 2``.
2972         keep_attrs : bool, optional
2973             If True, the dataset's attributes (`attrs`) will be copied from
2974             the original object to the new one.  If False (default), the new
2975             object will be returned without attributes.
2976         skipna : bool, optional
2977             Whether to skip missing values when aggregating.
2978 
2979         Returns
2980         -------
2981         quantiles : DataArray
2982             If `q` is a single quantile, then the result
2983             is a scalar. If multiple percentiles are given, first axis of
2984             the result corresponds to the quantile and a quantile dimension
2985             is added to the return array. The other dimensions are the
2986             dimensions that remain after the reduction of the array.
2987 
2988         See Also
2989         --------
2990         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
2991 
2992         Examples
2993         --------
2994 
2995         >>> da = xr.DataArray(
2996         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
2997         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
2998         ...     dims=("x", "y"),
2999         ... )
3000         >>> da.quantile(0)  # or da.quantile(0, dim=...)
3001         <xarray.DataArray ()>
3002         array(0.7)
3003         Coordinates:
3004             quantile  float64 0.0
3005         >>> da.quantile(0, dim="x")
3006         <xarray.DataArray (y: 4)>
3007         array([0.7, 4.2, 2.6, 1.5])
3008         Coordinates:
3009           * y         (y) float64 1.0 1.5 2.0 2.5
3010             quantile  float64 0.0
3011         >>> da.quantile([0, 0.5, 1])
3012         <xarray.DataArray (quantile: 3)>
3013         array([0.7, 3.4, 9.4])
3014         Coordinates:
3015           * quantile  (quantile) float64 0.0 0.5 1.0
3016         >>> da.quantile([0, 0.5, 1], dim="x")
3017         <xarray.DataArray (quantile: 3, y: 4)>
3018         array([[0.7 , 4.2 , 2.6 , 1.5 ],
3019                [3.6 , 5.75, 6.  , 1.7 ],
3020                [6.5 , 7.3 , 9.4 , 1.9 ]])
3021         Coordinates:
3022           * y         (y) float64 1.0 1.5 2.0 2.5
3023           * quantile  (quantile) float64 0.0 0.5 1.0
3024         """
3025 
3026         ds = self._to_temp_dataset().quantile(
3027             q,
3028             dim=dim,
3029             keep_attrs=keep_attrs,
3030             interpolation=interpolation,
3031             skipna=skipna,
3032         )
3033         return self._from_temp_dataset(ds)
3034 
3035     def rank(
3036         self, dim: Hashable, pct: bool = False, keep_attrs: bool = None
3037     ) -> "DataArray":
3038         """Ranks the data.
3039 
3040         Equal values are assigned a rank that is the average of the ranks that
3041         would have been otherwise assigned to all of the values within that
3042         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
3043 
3044         NaNs in the input array are returned as NaNs.
3045 
3046         The `bottleneck` library is required.
3047 
3048         Parameters
3049         ----------
3050         dim : hashable
3051             Dimension over which to compute rank.
3052         pct : bool, optional
3053             If True, compute percentage ranks, otherwise compute integer ranks.
3054         keep_attrs : bool, optional
3055             If True, the dataset's attributes (`attrs`) will be copied from
3056             the original object to the new one.  If False (default), the new
3057             object will be returned without attributes.
3058 
3059         Returns
3060         -------
3061         ranked : DataArray
3062             DataArray with the same coordinates and dtype 'float64'.
3063 
3064         Examples
3065         --------
3066 
3067         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3068         >>> arr.rank("x")
3069         <xarray.DataArray (x: 3)>
3070         array([ 1.,   2.,   3.])
3071         Dimensions without coordinates: x
3072         """
3073 
3074         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
3075         return self._from_temp_dataset(ds)
3076 
3077     def differentiate(
3078         self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None
3079     ) -> "DataArray":
3080         """ Differentiate the array with the second order accurate central
3081         differences.
3082 
3083         .. note::
3084             This feature is limited to simple cartesian geometry, i.e. coord
3085             must be one dimensional.
3086 
3087         Parameters
3088         ----------
3089         coord: hashable
3090             The coordinate to be used to compute the gradient.
3091         edge_order: 1 or 2. Default 1
3092             N-th order accurate differences at the boundaries.
3093         datetime_unit: None or any of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms',
3094             'us', 'ns', 'ps', 'fs', 'as'}
3095             Unit to compute gradient. Only valid for datetime coordinate.
3096 
3097         Returns
3098         -------
3099         differentiated: DataArray
3100 
3101         See also
3102         --------
3103         numpy.gradient: corresponding numpy function
3104 
3105         Examples
3106         --------
3107 
3108         >>> da = xr.DataArray(
3109         ...     np.arange(12).reshape(4, 3),
3110         ...     dims=["x", "y"],
3111         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3112         ... )
3113         >>> da
3114         <xarray.DataArray (x: 4, y: 3)>
3115         array([[ 0,  1,  2],
3116                [ 3,  4,  5],
3117                [ 6,  7,  8],
3118                [ 9, 10, 11]])
3119         Coordinates:
3120           * x        (x) float64 0.0 0.1 1.1 1.2
3121         Dimensions without coordinates: y
3122         >>>
3123         >>> da.differentiate("x")
3124         <xarray.DataArray (x: 4, y: 3)>
3125         array([[30.      , 30.      , 30.      ],
3126                [27.545455, 27.545455, 27.545455],
3127                [27.545455, 27.545455, 27.545455],
3128                [30.      , 30.      , 30.      ]])
3129         Coordinates:
3130           * x        (x) float64 0.0 0.1 1.1 1.2
3131         Dimensions without coordinates: y
3132         """
3133         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
3134         return self._from_temp_dataset(ds)
3135 
3136     def integrate(
3137         self, dim: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None
3138     ) -> "DataArray":
3139         """ integrate the array with the trapezoidal rule.
3140 
3141         .. note::
3142             This feature is limited to simple cartesian geometry, i.e. dim
3143             must be one dimensional.
3144 
3145         Parameters
3146         ----------
3147         dim: hashable, or a sequence of hashable
3148             Coordinate(s) used for the integration.
3149         datetime_unit: str, optional
3150             Can be used to specify the unit if datetime coordinate is used.
3151             One of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', 'ps',
3152             'fs', 'as'}
3153 
3154         Returns
3155         -------
3156         integrated: DataArray
3157 
3158         See also
3159         --------
3160         numpy.trapz: corresponding numpy function
3161 
3162         Examples
3163         --------
3164 
3165         >>> da = xr.DataArray(
3166         ...     np.arange(12).reshape(4, 3),
3167         ...     dims=["x", "y"],
3168         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3169         ... )
3170         >>> da
3171         <xarray.DataArray (x: 4, y: 3)>
3172         array([[ 0,  1,  2],
3173                [ 3,  4,  5],
3174                [ 6,  7,  8],
3175                [ 9, 10, 11]])
3176         Coordinates:
3177           * x        (x) float64 0.0 0.1 1.1 1.2
3178         Dimensions without coordinates: y
3179         >>>
3180         >>> da.integrate("x")
3181         <xarray.DataArray (y: 3)>
3182         array([5.4, 6.6, 7.8])
3183         Dimensions without coordinates: y
3184         """
3185         ds = self._to_temp_dataset().integrate(dim, datetime_unit)
3186         return self._from_temp_dataset(ds)
3187 
3188     def unify_chunks(self) -> "DataArray":
3189         """ Unify chunk size along all chunked dimensions of this DataArray.
3190 
3191         Returns
3192         -------
3193 
3194         DataArray with consistent chunk sizes for all dask-array variables
3195 
3196         See Also
3197         --------
3198 
3199         dask.array.core.unify_chunks
3200         """
3201         ds = self._to_temp_dataset().unify_chunks()
3202         return self._from_temp_dataset(ds)
3203 
3204     def map_blocks(
3205         self,
3206         func: "Callable[..., T_DSorDA]",
3207         args: Sequence[Any] = (),
3208         kwargs: Mapping[str, Any] = None,
3209     ) -> "T_DSorDA":
3210         """
3211         Apply a function to each chunk of this DataArray. This method is experimental
3212         and its signature may change.
3213 
3214         Parameters
3215         ----------
3216         func: callable
3217             User-provided function that accepts a DataArray as its first parameter. The
3218             function will receive a subset of this DataArray, corresponding to one chunk
3219             along each chunked dimension. ``func`` will be executed as
3220             ``func(obj_subset, *args, **kwargs)``.
3221 
3222             The function will be first run on mocked-up data, that looks like this array
3223             but has sizes 0, to determine properties of the returned object such as
3224             dtype, variable names, new dimensions and new indexes (if any).
3225 
3226             This function must return either a single DataArray or a single Dataset.
3227 
3228             This function cannot change size of existing dimensions, or add new chunked
3229             dimensions.
3230         args: Sequence
3231             Passed verbatim to func after unpacking, after the sliced DataArray. xarray
3232             objects, if any, will not be split by chunks. Passing dask collections is
3233             not allowed.
3234         kwargs: Mapping
3235             Passed verbatim to func after unpacking. xarray objects, if any, will not be
3236             split by chunks. Passing dask collections is not allowed.
3237 
3238         Returns
3239         -------
3240         A single DataArray or Dataset with dask backend, reassembled from the outputs of
3241         the function.
3242 
3243         Notes
3244         -----
3245         This method is designed for when one needs to manipulate a whole xarray object
3246         within each chunk. In the more common case where one can work on numpy arrays,
3247         it is recommended to use apply_ufunc.
3248 
3249         If none of the variables in this DataArray is backed by dask, calling this
3250         method is equivalent to calling ``func(self, *args, **kwargs)``.
3251 
3252         See Also
3253         --------
3254         dask.array.map_blocks, xarray.apply_ufunc, xarray.map_blocks,
3255         xarray.Dataset.map_blocks
3256         """
3257         from .parallel import map_blocks
3258 
3259         return map_blocks(func, self, args, kwargs)
3260 
3261     # this needs to be at the end, or mypy will confuse with `str`
3262     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
3263     str = property(StringAccessor)

</code><intended_behavior>Introduce an 'average' method that enables weighted average calculation for data arrays. This method would be very similar to the one in 'Dataset', tailored to the 'DataArray' structure, leveraging the existing data handling methods and ensuring it integrates seamlessly with the broader xarray functionality.</intended_behavior>, <file>xarray/core/variable.py</file>
<class>None</class>
<method>None</method>
<code>
1 import copy
2 import functools
3 import itertools
4 import warnings
5 from collections import defaultdict
6 from datetime import timedelta
7 from distutils.version import LooseVersion
8 from typing import Any, Dict, Hashable, Mapping, TypeVar, Union
9 
10 import numpy as np
11 import pandas as pd
12 
13 import xarray as xr  # only for Dataset and DataArray
14 
15 from . import arithmetic, common, dtypes, duck_array_ops, indexing, nputils, ops, utils
16 from .indexing import (
17     BasicIndexer,
18     OuterIndexer,
19     PandasIndexAdapter,
20     VectorizedIndexer,
21     as_indexable,
22 )
23 from .npcompat import IS_NEP18_ACTIVE
24 from .options import _get_keep_attrs
25 from .pycompat import dask_array_type, integer_types
26 from .utils import (
27     OrderedSet,
28     _default,
29     decode_numpy_dict_values,
30     either_dict_or_kwargs,
31     ensure_us_time_resolution,
32     infix_dims,
33 )
34 
35 try:
36     import dask.array as da
37 except ImportError:
38     pass
39 
40 
41 NON_NUMPY_SUPPORTED_ARRAY_TYPES = (
42     indexing.ExplicitlyIndexed,
43     pd.Index,
44 ) + dask_array_type
45 # https://github.com/python/mypy/issues/224
46 BASIC_INDEXING_TYPES = integer_types + (slice,)  # type: ignore
47 
48 VariableType = TypeVar("VariableType", bound="Variable")
49 """Type annotation to be used when methods of Variable return self or a copy of self.
50 When called from an instance of a subclass, e.g. IndexVariable, mypy identifies the
51 output as an instance of the subclass.
52 
53 Usage::
54 
55    class Variable:
56        def f(self: VariableType, ...) -> VariableType:
57            ...
58 """
59 
60 
61 class MissingDimensionsError(ValueError):
62     """Error class used when we can't safely guess a dimension name.
63     """
64 
65     # inherits from ValueError for backward compatibility
66     # TODO: move this to an xarray.exceptions module?
67 
68 
69 def as_variable(obj, name=None) -> "Union[Variable, IndexVariable]":
70     """Convert an object into a Variable.
71 
72     Parameters
73     ----------
74     obj : object
75         Object to convert into a Variable.
76 
77         - If the object is already a Variable, return a shallow copy.
78         - Otherwise, if the object has 'dims' and 'data' attributes, convert
79           it into a new Variable.
80         - If all else fails, attempt to convert the object into a Variable by
81           unpacking it into the arguments for creating a new Variable.
82     name : str, optional
83         If provided:
84 
85         - `obj` can be a 1D array, which is assumed to label coordinate values
86           along a dimension of this given name.
87         - Variables with name matching one of their dimensions are converted
88           into `IndexVariable` objects.
89 
90     Returns
91     -------
92     var : Variable
93         The newly created variable.
94 
95     """
96     from .dataarray import DataArray
97 
98     # TODO: consider extending this method to automatically handle Iris and
99     if isinstance(obj, DataArray):
100         # extract the primary Variable from DataArrays
101         obj = obj.variable
102 
103     if isinstance(obj, Variable):
104         obj = obj.copy(deep=False)
105     elif isinstance(obj, tuple):
106         try:
107             obj = Variable(*obj)
108         except (TypeError, ValueError) as error:
109             # use .format() instead of % because it handles tuples consistently
110             raise error.__class__(
111                 "Could not convert tuple of form "
112                 "(dims, data[, attrs, encoding]): "
113                 "{} to Variable.".format(obj)
114             )
115     elif utils.is_scalar(obj):
116         obj = Variable([], obj)
117     elif isinstance(obj, (pd.Index, IndexVariable)) and obj.name is not None:
118         obj = Variable(obj.name, obj)
119     elif isinstance(obj, (set, dict)):
120         raise TypeError("variable {!r} has invalid type {!r}".format(name, type(obj)))
121     elif name is not None:
122         data = as_compatible_data(obj)
123         if data.ndim != 1:
124             raise MissingDimensionsError(
125                 "cannot set variable %r with %r-dimensional data "
126                 "without explicit dimension names. Pass a tuple of "
127                 "(dims, data) instead." % (name, data.ndim)
128             )
129         obj = Variable(name, data, fastpath=True)
130     else:
131         raise TypeError(
132             "unable to convert object into a variable without an "
133             "explicit list of dimensions: %r" % obj
134         )
135 
136     if name is not None and name in obj.dims:
137         # convert the Variable into an Index
138         if obj.ndim != 1:
139             raise MissingDimensionsError(
140                 "%r has more than 1-dimension and the same name as one of its "
141                 "dimensions %r. xarray disallows such variables because they "
142                 "conflict with the coordinates used to label "
143                 "dimensions." % (name, obj.dims)
144             )
145         obj = obj.to_index_variable()
146 
147     return obj
148 
149 
150 def _maybe_wrap_data(data):
151     """
152     Put pandas.Index and numpy.ndarray arguments in adapter objects to ensure
153     they can be indexed properly.
154 
155     NumpyArrayAdapter, PandasIndexAdapter and LazilyOuterIndexedArray should
156     all pass through unmodified.
157     """
158     if isinstance(data, pd.Index):
159         return PandasIndexAdapter(data)
160     return data
161 
162 
163 def _possibly_convert_objects(values):
164     """Convert arrays of datetime.datetime and datetime.timedelta objects into
165     datetime64 and timedelta64, according to the pandas convention.
166     """
167     return np.asarray(pd.Series(values.ravel())).reshape(values.shape)
168 
169 
170 def as_compatible_data(data, fastpath=False):
171     """Prepare and wrap data to put in a Variable.
172 
173     - If data does not have the necessary attributes, convert it to ndarray.
174     - If data has dtype=datetime64, ensure that it has ns precision. If it's a
175       pandas.Timestamp, convert it to datetime64.
176     - If data is already a pandas or xarray object (other than an Index), just
177       use the values.
178 
179     Finally, wrap it up with an adapter if necessary.
180     """
181     if fastpath and getattr(data, "ndim", 0) > 0:
182         # can't use fastpath (yet) for scalars
183         return _maybe_wrap_data(data)
184 
185     if isinstance(data, Variable):
186         return data.data
187 
188     if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):
189         return _maybe_wrap_data(data)
190 
191     if isinstance(data, tuple):
192         data = utils.to_0d_object_array(data)
193 
194     if isinstance(data, pd.Timestamp):
195         # TODO: convert, handle datetime objects, too
196         data = np.datetime64(data.value, "ns")
197 
198     if isinstance(data, timedelta):
199         data = np.timedelta64(getattr(data, "value", data), "ns")
200 
201     # we don't want nested self-described arrays
202     data = getattr(data, "values", data)
203 
204     if isinstance(data, np.ma.MaskedArray):
205         mask = np.ma.getmaskarray(data)
206         if mask.any():
207             dtype, fill_value = dtypes.maybe_promote(data.dtype)
208             data = np.asarray(data, dtype=dtype)
209             data[mask] = fill_value
210         else:
211             data = np.asarray(data)
212 
213     if not isinstance(data, np.ndarray):
214         if hasattr(data, "__array_function__"):
215             if IS_NEP18_ACTIVE:
216                 return data
217             else:
218                 raise TypeError(
219                     "Got an NumPy-like array type providing the "
220                     "__array_function__ protocol but NEP18 is not enabled. "
221                     "Check that numpy >= v1.16 and that the environment "
222                     'variable "NUMPY_EXPERIMENTAL_ARRAY_FUNCTION" is set to '
223                     '"1"'
224                 )
225 
226     # validate whether the data is valid data types
227     data = np.asarray(data)
228 
229     if isinstance(data, np.ndarray):
230         if data.dtype.kind == "O":
231             data = _possibly_convert_objects(data)
232         elif data.dtype.kind == "M":
233             data = np.asarray(data, "datetime64[ns]")
234         elif data.dtype.kind == "m":
235             data = np.asarray(data, "timedelta64[ns]")
236 
237     return _maybe_wrap_data(data)
238 
239 
240 def _as_array_or_item(data):
241     """Return the given values as a numpy array, or as an individual item if
242     it's a 0d datetime64 or timedelta64 array.
243 
244     Importantly, this function does not copy data if it is already an ndarray -
245     otherwise, it will not be possible to update Variable values in place.
246 
247     This function mostly exists because 0-dimensional ndarrays with
248     dtype=datetime64 are broken :(
249     https://github.com/numpy/numpy/issues/4337
250     https://github.com/numpy/numpy/issues/7619
251 
252     TODO: remove this (replace with np.asarray) once these issues are fixed
253     """
254     data = np.asarray(data)
255     if data.ndim == 0:
256         if data.dtype.kind == "M":
257             data = np.datetime64(data, "ns")
258         elif data.dtype.kind == "m":
259             data = np.timedelta64(data, "ns")
260     return data
261 
262 
263 class Variable(
264     common.AbstractArray, arithmetic.SupportsArithmetic, utils.NdimSizeLenMixin
265 ):
266     """A netcdf-like variable consisting of dimensions, data and attributes
267     which describe a single Array. A single Variable object is not fully
268     described outside the context of its parent Dataset (if you want such a
269     fully described object, use a DataArray instead).
270 
271     The main functional difference between Variables and numpy arrays is that
272     numerical operations on Variables implement array broadcasting by dimension
273     name. For example, adding an Variable with dimensions `('time',)` to
274     another Variable with dimensions `('space',)` results in a new Variable
275     with dimensions `('time', 'space')`. Furthermore, numpy reduce operations
276     like ``mean`` or ``sum`` are overwritten to take a "dimension" argument
277     instead of an "axis".
278 
279     Variables are light-weight objects used as the building block for datasets.
280     They are more primitive objects, so operations with them provide marginally
281     higher performance than using DataArrays. However, manipulating data in the
282     form of a Dataset or DataArray should almost always be preferred, because
283     they can use more complete metadata in context of coordinate labels.
284     """
285 
286     __slots__ = ("_dims", "_data", "_attrs", "_encoding")
287 
288     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):
289         """
290         Parameters
291         ----------
292         dims : str or sequence of str
293             Name(s) of the the data dimension(s). Must be either a string (only
294             for 1D data) or a sequence of strings with length equal to the
295             number of dimensions.
296         data : array_like
297             Data array which supports numpy-like data access.
298         attrs : dict_like or None, optional
299             Attributes to assign to the new variable. If None (default), an
300             empty attribute dictionary is initialized.
301         encoding : dict_like or None, optional
302             Dictionary specifying how to encode this array's data into a
303             serialized format like netCDF4. Currently used keys (for netCDF)
304             include '_FillValue', 'scale_factor', 'add_offset' and 'dtype'.
305             Well-behaved code to serialize a Variable should ignore
306             unrecognized encoding items.
307         """
308         self._data = as_compatible_data(data, fastpath=fastpath)
309         self._dims = self._parse_dimensions(dims)
310         self._attrs = None
311         self._encoding = None
312         if attrs is not None:
313             self.attrs = attrs
314         if encoding is not None:
315             self.encoding = encoding
316 
317     @property
318     def dtype(self):
319         return self._data.dtype
320 
321     @property
322     def shape(self):
323         return self._data.shape
324 
325     @property
326     def nbytes(self):
327         return self.size * self.dtype.itemsize
328 
329     @property
330     def _in_memory(self):
331         return isinstance(self._data, (np.ndarray, np.number, PandasIndexAdapter)) or (
332             isinstance(self._data, indexing.MemoryCachedArray)
333             and isinstance(self._data.array, indexing.NumpyIndexingAdapter)
334         )
335 
336     @property
337     def data(self):
338         if hasattr(self._data, "__array_function__") or isinstance(
339             self._data, dask_array_type
340         ):
341             return self._data
342         else:
343             return self.values
344 
345     @data.setter
346     def data(self, data):
347         data = as_compatible_data(data)
348         if data.shape != self.shape:
349             raise ValueError(
350                 f"replacement data must match the Variable's shape. "
351                 f"replacement data has shape {data.shape}; Variable has shape {self.shape}"
352             )
353         self._data = data
354 
355     def load(self, **kwargs):
356         """Manually trigger loading of this variable's data from disk or a
357         remote source into memory and return this variable.
358 
359         Normally, it should not be necessary to call this method in user code,
360         because all xarray functions should either work on deferred data or
361         load data automatically.
362 
363         Parameters
364         ----------
365         **kwargs : dict
366             Additional keyword arguments passed on to ``dask.array.compute``.
367 
368         See Also
369         --------
370         dask.array.compute
371         """
372         if isinstance(self._data, dask_array_type):
373             self._data = as_compatible_data(self._data.compute(**kwargs))
374         elif not hasattr(self._data, "__array_function__"):
375             self._data = np.asarray(self._data)
376         return self
377 
378     def compute(self, **kwargs):
379         """Manually trigger loading of this variable's data from disk or a
380         remote source into memory and return a new variable. The original is
381         left unaltered.
382 
383         Normally, it should not be necessary to call this method in user code,
384         because all xarray functions should either work on deferred data or
385         load data automatically.
386 
387         Parameters
388         ----------
389         **kwargs : dict
390             Additional keyword arguments passed on to ``dask.array.compute``.
391 
392         See Also
393         --------
394         dask.array.compute
395         """
396         new = self.copy(deep=False)
397         return new.load(**kwargs)
398 
399     def __dask_tokenize__(self):
400         # Use v.data, instead of v._data, in order to cope with the wrappers
401         # around NetCDF and the like
402         from dask.base import normalize_token
403 
404         return normalize_token((type(self), self._dims, self.data, self._attrs))
405 
406     def __dask_graph__(self):
407         if isinstance(self._data, dask_array_type):
408             return self._data.__dask_graph__()
409         else:
410             return None
411 
412     def __dask_keys__(self):
413         return self._data.__dask_keys__()
414 
415     def __dask_layers__(self):
416         return self._data.__dask_layers__()
417 
418     @property
419     def __dask_optimize__(self):
420         return self._data.__dask_optimize__
421 
422     @property
423     def __dask_scheduler__(self):
424         return self._data.__dask_scheduler__
425 
426     def __dask_postcompute__(self):
427         array_func, array_args = self._data.__dask_postcompute__()
428         return (
429             self._dask_finalize,
430             (array_func, array_args, self._dims, self._attrs, self._encoding),
431         )
432 
433     def __dask_postpersist__(self):
434         array_func, array_args = self._data.__dask_postpersist__()
435         return (
436             self._dask_finalize,
437             (array_func, array_args, self._dims, self._attrs, self._encoding),
438         )
439 
440     @staticmethod
441     def _dask_finalize(results, array_func, array_args, dims, attrs, encoding):
442         if isinstance(results, dict):  # persist case
443             name = array_args[0]
444             results = {k: v for k, v in results.items() if k[0] == name}
445         data = array_func(results, *array_args)
446         return Variable(dims, data, attrs=attrs, encoding=encoding)
447 
448     @property
449     def values(self):
450         """The variable's data as a numpy.ndarray"""
451         return _as_array_or_item(self._data)
452 
453     @values.setter
454     def values(self, values):
455         self.data = values
456 
457     def to_base_variable(self):
458         """Return this variable as a base xarray.Variable"""
459         return Variable(
460             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True
461         )
462 
463     to_variable = utils.alias(to_base_variable, "to_variable")
464 
465     def to_index_variable(self):
466         """Return this variable as an xarray.IndexVariable"""
467         return IndexVariable(
468             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True
469         )
470 
471     to_coord = utils.alias(to_index_variable, "to_coord")
472 
473     def to_index(self):
474         """Convert this variable to a pandas.Index"""
475         return self.to_index_variable().to_index()
476 
477     def to_dict(self, data=True):
478         """Dictionary representation of variable."""
479         item = {"dims": self.dims, "attrs": decode_numpy_dict_values(self.attrs)}
480         if data:
481             item["data"] = ensure_us_time_resolution(self.values).tolist()
482         else:
483             item.update({"dtype": str(self.dtype), "shape": self.shape})
484         return item
485 
486     @property
487     def dims(self):
488         """Tuple of dimension names with which this variable is associated.
489         """
490         return self._dims
491 
492     @dims.setter
493     def dims(self, value):
494         self._dims = self._parse_dimensions(value)
495 
496     def _parse_dimensions(self, dims):
497         if isinstance(dims, str):
498             dims = (dims,)
499         dims = tuple(dims)
500         if len(dims) != self.ndim:
501             raise ValueError(
502                 "dimensions %s must have the same length as the "
503                 "number of data dimensions, ndim=%s" % (dims, self.ndim)
504             )
505         return dims
506 
507     def _item_key_to_tuple(self, key):
508         if utils.is_dict_like(key):
509             return tuple(key.get(dim, slice(None)) for dim in self.dims)
510         else:
511             return key
512 
513     def _broadcast_indexes(self, key):
514         """Prepare an indexing key for an indexing operation.
515 
516         Parameters
517         -----------
518         key: int, slice, array, dict or tuple of integer, slices and arrays
519             Any valid input for indexing.
520 
521         Returns
522         -------
523         dims: tuple
524             Dimension of the resultant variable.
525         indexers: IndexingTuple subclass
526             Tuple of integer, array-like, or slices to use when indexing
527             self._data. The type of this argument indicates the type of
528             indexing to perform, either basic, outer or vectorized.
529         new_order : Optional[Sequence[int]]
530             Optional reordering to do on the result of indexing. If not None,
531             the first len(new_order) indexing should be moved to these
532             positions.
533         """
534         key = self._item_key_to_tuple(key)  # key is a tuple
535         # key is a tuple of full size
536         key = indexing.expanded_indexer(key, self.ndim)
537         # Convert a scalar Variable to an integer
538         key = tuple(
539             k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k for k in key
540         )
541         # Convert a 0d-array to an integer
542         key = tuple(
543             k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key
544         )
545 
546         if all(isinstance(k, BASIC_INDEXING_TYPES) for k in key):
547             return self._broadcast_indexes_basic(key)
548 
549         self._validate_indexers(key)
550         # Detect it can be mapped as an outer indexer
551         # If all key is unlabeled, or
552         # key can be mapped as an OuterIndexer.
553         if all(not isinstance(k, Variable) for k in key):
554             return self._broadcast_indexes_outer(key)
555 
556         # If all key is 1-dimensional and there are no duplicate labels,
557         # key can be mapped as an OuterIndexer.
558         dims = []
559         for k, d in zip(key, self.dims):
560             if isinstance(k, Variable):
561                 if len(k.dims) > 1:
562                     return self._broadcast_indexes_vectorized(key)
563                 dims.append(k.dims[0])
564             elif not isinstance(k, integer_types):
565                 dims.append(d)
566         if len(set(dims)) == len(dims):
567             return self._broadcast_indexes_outer(key)
568 
569         return self._broadcast_indexes_vectorized(key)
570 
571     def _broadcast_indexes_basic(self, key):
572         dims = tuple(
573             dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)
574         )
575         return dims, BasicIndexer(key), None
576 
577     def _validate_indexers(self, key):
578         """ Make sanity checks """
579         for dim, k in zip(self.dims, key):
580             if isinstance(k, BASIC_INDEXING_TYPES):
581                 pass
582             else:
583                 if not isinstance(k, Variable):
584                     k = np.asarray(k)
585                     if k.ndim > 1:
586                         raise IndexError(
587                             "Unlabeled multi-dimensional array cannot be "
588                             "used for indexing: {}".format(k)
589                         )
590                 if k.dtype.kind == "b":
591                     if self.shape[self.get_axis_num(dim)] != len(k):
592                         raise IndexError(
593                             "Boolean array size {:d} is used to index array "
594                             "with shape {:s}.".format(len(k), str(self.shape))
595                         )
596                     if k.ndim > 1:
597                         raise IndexError(
598                             "{}-dimensional boolean indexing is "
599                             "not supported. ".format(k.ndim)
600                         )
601                     if getattr(k, "dims", (dim,)) != (dim,):
602                         raise IndexError(
603                             "Boolean indexer should be unlabeled or on the "
604                             "same dimension to the indexed array. Indexer is "
605                             "on {:s} but the target dimension is {:s}.".format(
606                                 str(k.dims), dim
607                             )
608                         )
609 
610     def _broadcast_indexes_outer(self, key):
611         dims = tuple(
612             k.dims[0] if isinstance(k, Variable) else dim
613             for k, dim in zip(key, self.dims)
614             if not isinstance(k, integer_types)
615         )
616 
617         new_key = []
618         for k in key:
619             if isinstance(k, Variable):
620                 k = k.data
621             if not isinstance(k, BASIC_INDEXING_TYPES):
622                 k = np.asarray(k)
623                 if k.size == 0:
624                     # Slice by empty list; numpy could not infer the dtype
625                     k = k.astype(int)
626                 elif k.dtype.kind == "b":
627                     (k,) = np.nonzero(k)
628             new_key.append(k)
629 
630         return dims, OuterIndexer(tuple(new_key)), None
631 
632     def _nonzero(self):
633         """ Equivalent numpy's nonzero but returns a tuple of Varibles. """
634         # TODO we should replace dask's native nonzero
635         # after https://github.com/dask/dask/issues/1076 is implemented.
636         nonzeros = np.nonzero(self.data)
637         return tuple(Variable((dim), nz) for nz, dim in zip(nonzeros, self.dims))
638 
639     def _broadcast_indexes_vectorized(self, key):
640         variables = []
641         out_dims_set = OrderedSet()
642         for dim, value in zip(self.dims, key):
643             if isinstance(value, slice):
644                 out_dims_set.add(dim)
645             else:
646                 variable = (
647                     value
648                     if isinstance(value, Variable)
649                     else as_variable(value, name=dim)
650                 )
651                 if variable.dtype.kind == "b":  # boolean indexing case
652                     (variable,) = variable._nonzero()
653 
654                 variables.append(variable)
655                 out_dims_set.update(variable.dims)
656 
657         variable_dims = set()
658         for variable in variables:
659             variable_dims.update(variable.dims)
660 
661         slices = []
662         for i, (dim, value) in enumerate(zip(self.dims, key)):
663             if isinstance(value, slice):
664                 if dim in variable_dims:
665                     # We only convert slice objects to variables if they share
666                     # a dimension with at least one other variable. Otherwise,
667                     # we can equivalently leave them as slices aknd transpose
668                     # the result. This is significantly faster/more efficient
669                     # for most array backends.
670                     values = np.arange(*value.indices(self.sizes[dim]))
671                     variables.insert(i - len(slices), Variable((dim,), values))
672                 else:
673                     slices.append((i, value))
674 
675         try:
676             variables = _broadcast_compat_variables(*variables)
677         except ValueError:
678             raise IndexError(f"Dimensions of indexers mismatch: {key}")
679 
680         out_key = [variable.data for variable in variables]
681         out_dims = tuple(out_dims_set)
682         slice_positions = set()
683         for i, value in slices:
684             out_key.insert(i, value)
685             new_position = out_dims.index(self.dims[i])
686             slice_positions.add(new_position)
687 
688         if slice_positions:
689             new_order = [i for i in range(len(out_dims)) if i not in slice_positions]
690         else:
691             new_order = None
692 
693         return out_dims, VectorizedIndexer(tuple(out_key)), new_order
694 
695     def __getitem__(self: VariableType, key) -> VariableType:
696         """Return a new Variable object whose contents are consistent with
697         getting the provided key from the underlying data.
698 
699         NB. __getitem__ and __setitem__ implement xarray-style indexing,
700         where if keys are unlabeled arrays, we index the array orthogonally
701         with them. If keys are labeled array (such as Variables), they are
702         broadcasted with our usual scheme and then the array is indexed with
703         the broadcasted key, like numpy's fancy indexing.
704 
705         If you really want to do indexing like `x[x > 0]`, manipulate the numpy
706         array `x.values` directly.
707         """
708         dims, indexer, new_order = self._broadcast_indexes(key)
709         data = as_indexable(self._data)[indexer]
710         if new_order:
711             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)
712         return self._finalize_indexing_result(dims, data)
713 
714     def _finalize_indexing_result(self: VariableType, dims, data) -> VariableType:
715         """Used by IndexVariable to return IndexVariable objects when possible.
716         """
717         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)
718 
719     def _getitem_with_mask(self, key, fill_value=dtypes.NA):
720         """Index this Variable with -1 remapped to fill_value."""
721         # TODO(shoyer): expose this method in public API somewhere (isel?) and
722         # use it for reindex.
723         # TODO(shoyer): add a sanity check that all other integers are
724         # non-negative
725         # TODO(shoyer): add an optimization, remapping -1 to an adjacent value
726         # that is actually indexed rather than mapping it to the last value
727         # along each axis.
728 
729         if fill_value is dtypes.NA:
730             fill_value = dtypes.get_fill_value(self.dtype)
731 
732         dims, indexer, new_order = self._broadcast_indexes(key)
733 
734         if self.size:
735             if isinstance(self._data, dask_array_type):
736                 # dask's indexing is faster this way; also vindex does not
737                 # support negative indices yet:
738                 # https://github.com/dask/dask/pull/2967
739                 actual_indexer = indexing.posify_mask_indexer(indexer)
740             else:
741                 actual_indexer = indexer
742 
743             data = as_indexable(self._data)[actual_indexer]
744             mask = indexing.create_mask(indexer, self.shape, data)
745             # we need to invert the mask in order to pass data first. This helps
746             # pint to choose the correct unit
747             # TODO: revert after https://github.com/hgrecco/pint/issues/1019 is fixed
748             data = duck_array_ops.where(np.logical_not(mask), data, fill_value)
749         else:
750             # array cannot be indexed along dimensions of size 0, so just
751             # build the mask directly instead.
752             mask = indexing.create_mask(indexer, self.shape)
753             data = np.broadcast_to(fill_value, getattr(mask, "shape", ()))
754 
755         if new_order:
756             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)
757         return self._finalize_indexing_result(dims, data)
758 
759     def __setitem__(self, key, value):
760         """__setitem__ is overloaded to access the underlying numpy values with
761         orthogonal indexing.
762 
763         See __getitem__ for more details.
764         """
765         dims, index_tuple, new_order = self._broadcast_indexes(key)
766 
767         if not isinstance(value, Variable):
768             value = as_compatible_data(value)
769             if value.ndim > len(dims):
770                 raise ValueError(
771                     "shape mismatch: value array of shape %s could not be "
772                     "broadcast to indexing result with %s dimensions"
773                     % (value.shape, len(dims))
774                 )
775             if value.ndim == 0:
776                 value = Variable((), value)
777             else:
778                 value = Variable(dims[-value.ndim :], value)
779         # broadcast to become assignable
780         value = value.set_dims(dims).data
781 
782         if new_order:
783             value = duck_array_ops.asarray(value)
784             value = value[(len(dims) - value.ndim) * (np.newaxis,) + (Ellipsis,)]
785             value = duck_array_ops.moveaxis(value, new_order, range(len(new_order)))
786 
787         indexable = as_indexable(self._data)
788         indexable[index_tuple] = value
789 
790     @property
791     def attrs(self) -> Dict[Hashable, Any]:
792         """Dictionary of local attributes on this variable.
793         """
794         if self._attrs is None:
795             self._attrs = {}
796         return self._attrs
797 
798     @attrs.setter
799     def attrs(self, value: Mapping[Hashable, Any]) -> None:
800         self._attrs = dict(value)
801 
802     @property
803     def encoding(self):
804         """Dictionary of encodings on this variable.
805         """
806         if self._encoding is None:
807             self._encoding = {}
808         return self._encoding
809 
810     @encoding.setter
811     def encoding(self, value):
812         try:
813             self._encoding = dict(value)
814         except ValueError:
815             raise ValueError("encoding must be castable to a dictionary")
816 
817     def copy(self, deep=True, data=None):
818         """Returns a copy of this object.
819 
820         If `deep=True`, the data array is loaded into memory and copied onto
821         the new object. Dimensions, attributes and encodings are always copied.
822 
823         Use `data` to create a new object with the same structure as
824         original but entirely new data.
825 
826         Parameters
827         ----------
828         deep : bool, optional
829             Whether the data array is loaded into memory and copied onto
830             the new object. Default is True.
831         data : array_like, optional
832             Data to use in the new object. Must have same shape as original.
833             When `data` is used, `deep` is ignored.
834 
835         Returns
836         -------
837         object : Variable
838             New object with dimensions, attributes, encodings, and optionally
839             data copied from original.
840 
841         Examples
842         --------
843 
844         Shallow copy versus deep copy
845 
846         >>> var = xr.Variable(data=[1, 2, 3], dims="x")
847         >>> var.copy()
848         <xarray.Variable (x: 3)>
849         array([1, 2, 3])
850         >>> var_0 = var.copy(deep=False)
851         >>> var_0[0] = 7
852         >>> var_0
853         <xarray.Variable (x: 3)>
854         array([7, 2, 3])
855         >>> var
856         <xarray.Variable (x: 3)>
857         array([7, 2, 3])
858 
859         Changing the data using the ``data`` argument maintains the
860         structure of the original object, but with the new data. Original
861         object is unaffected.
862 
863         >>> var.copy(data=[0.1, 0.2, 0.3])
864         <xarray.Variable (x: 3)>
865         array([ 0.1,  0.2,  0.3])
866         >>> var
867         <xarray.Variable (x: 3)>
868         array([7, 2, 3])
869 
870         See Also
871         --------
872         pandas.DataFrame.copy
873         """
874         if data is None:
875             data = self._data
876 
877             if isinstance(data, indexing.MemoryCachedArray):
878                 # don't share caching between copies
879                 data = indexing.MemoryCachedArray(data.array)
880 
881             if deep:
882                 if hasattr(data, "__array_function__") or isinstance(
883                     data, dask_array_type
884                 ):
885                     data = data.copy()
886                 elif not isinstance(data, PandasIndexAdapter):
887                     # pandas.Index is immutable
888                     data = np.array(data)
889         else:
890             data = as_compatible_data(data)
891             if self.shape != data.shape:
892                 raise ValueError(
893                     "Data shape {} must match shape of object {}".format(
894                         data.shape, self.shape
895                     )
896                 )
897 
898         # note:
899         # dims is already an immutable tuple
900         # attributes and encoding will be copied when the new Array is created
901         return self._replace(data=data)
902 
903     def _replace(
904         self, dims=_default, data=_default, attrs=_default, encoding=_default
905     ) -> "Variable":
906         if dims is _default:
907             dims = copy.copy(self._dims)
908         if data is _default:
909             data = copy.copy(self.data)
910         if attrs is _default:
911             attrs = copy.copy(self._attrs)
912         if encoding is _default:
913             encoding = copy.copy(self._encoding)
914         return type(self)(dims, data, attrs, encoding, fastpath=True)
915 
916     def __copy__(self):
917         return self.copy(deep=False)
918 
919     def __deepcopy__(self, memo=None):
920         # memo does nothing but is required for compatibility with
921         # copy.deepcopy
922         return self.copy(deep=True)
923 
924     # mutable objects should not be hashable
925     # https://github.com/python/mypy/issues/4266
926     __hash__ = None  # type: ignore
927 
928     @property
929     def chunks(self):
930         """Block dimensions for this array's data or None if it's not a dask
931         array.
932         """
933         return getattr(self._data, "chunks", None)
934 
935     _array_counter = itertools.count()
936 
937     def chunk(self, chunks=None, name=None, lock=False):
938         """Coerce this array's data into a dask arrays with the given chunks.
939 
940         If this variable is a non-dask array, it will be converted to dask
941         array. If it's a dask array, it will be rechunked to the given chunk
942         sizes.
943 
944         If neither chunks is not provided for one or more dimensions, chunk
945         sizes along that dimension will not be updated; non-dask arrays will be
946         converted into dask arrays with a single block.
947 
948         Parameters
949         ----------
950         chunks : int, tuple or dict, optional
951             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
952             ``{'x': 5, 'y': 5}``.
953         name : str, optional
954             Used to generate the name for this array in the internal dask
955             graph. Does not need not be unique.
956         lock : optional
957             Passed on to :py:func:`dask.array.from_array`, if the array is not
958             already as dask array.
959 
960         Returns
961         -------
962         chunked : xarray.Variable
963         """
964         import dask
965         import dask.array as da
966 
967         if utils.is_dict_like(chunks):
968             chunks = {self.get_axis_num(dim): chunk for dim, chunk in chunks.items()}
969 
970         if chunks is None:
971             chunks = self.chunks or self.shape
972 
973         data = self._data
974         if isinstance(data, da.Array):
975             data = data.rechunk(chunks)
976         else:
977             if isinstance(data, indexing.ExplicitlyIndexed):
978                 # Unambiguously handle array storage backends (like NetCDF4 and h5py)
979                 # that can't handle general array indexing. For example, in netCDF4 you
980                 # can do "outer" indexing along two dimensions independent, which works
981                 # differently from how NumPy handles it.
982                 # da.from_array works by using lazy indexing with a tuple of slices.
983                 # Using OuterIndexer is a pragmatic choice: dask does not yet handle
984                 # different indexing types in an explicit way:
985                 # https://github.com/dask/dask/issues/2883
986                 data = indexing.ImplicitToExplicitIndexingAdapter(
987                     data, indexing.OuterIndexer
988                 )
989                 if LooseVersion(dask.__version__) < "2.0.0":
990                     kwargs = {}
991                 else:
992                     # All of our lazily loaded backend array classes should use NumPy
993                     # array operations.
994                     kwargs = {"meta": np.ndarray}
995             else:
996                 kwargs = {}
997 
998             if utils.is_dict_like(chunks):
999                 chunks = tuple(chunks.get(n, s) for n, s in enumerate(self.shape))
1000 
1001             data = da.from_array(data, chunks, name=name, lock=lock, **kwargs)
1002 
1003         return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)
1004 
1005     def _as_sparse(self, sparse_format=_default, fill_value=dtypes.NA):
1006         """
1007         use sparse-array as backend.
1008         """
1009         import sparse
1010 
1011         # TODO  what to do if dask-backended?
1012         if fill_value is dtypes.NA:
1013             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1014         else:
1015             dtype = dtypes.result_type(self.dtype, fill_value)
1016 
1017         if sparse_format is _default:
1018             sparse_format = "coo"
1019         try:
1020             as_sparse = getattr(sparse, "as_{}".format(sparse_format.lower()))
1021         except AttributeError:
1022             raise ValueError("{} is not a valid sparse format".format(sparse_format))
1023 
1024         data = as_sparse(self.data.astype(dtype), fill_value=fill_value)
1025         return self._replace(data=data)
1026 
1027     def _to_dense(self):
1028         """
1029         Change backend from sparse to np.array
1030         """
1031         if hasattr(self._data, "todense"):
1032             return self._replace(data=self._data.todense())
1033         return self.copy(deep=False)
1034 
1035     def isel(
1036         self: VariableType,
1037         indexers: Mapping[Hashable, Any] = None,
1038         **indexers_kwargs: Any,
1039     ) -> VariableType:
1040         """Return a new array indexed along the specified dimension(s).
1041 
1042         Parameters
1043         ----------
1044         **indexers : {dim: indexer, ...}
1045             Keyword arguments with names matching dimensions and values given
1046             by integers, slice objects or arrays.
1047 
1048         Returns
1049         -------
1050         obj : Array object
1051             A new Array with the selected data and dimensions. In general,
1052             the new variable's data will be a view of this variable's data,
1053             unless numpy fancy indexing was triggered by using an array
1054             indexer, in which case the data will be a copy.
1055         """
1056         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1057 
1058         invalid = indexers.keys() - set(self.dims)
1059         if invalid:
1060             raise ValueError(
1061                 f"dimensions {invalid} do not exist. Expected one or more of {self.dims}"
1062             )
1063 
1064         key = tuple(indexers.get(dim, slice(None)) for dim in self.dims)
1065         return self[key]
1066 
1067     def squeeze(self, dim=None):
1068         """Return a new object with squeezed data.
1069 
1070         Parameters
1071         ----------
1072         dim : None or str or tuple of str, optional
1073             Selects a subset of the length one dimensions. If a dimension is
1074             selected with length greater than one, an error is raised. If
1075             None, all length one dimensions are squeezed.
1076 
1077         Returns
1078         -------
1079         squeezed : same type as caller
1080             This object, but with with all or a subset of the dimensions of
1081             length 1 removed.
1082 
1083         See Also
1084         --------
1085         numpy.squeeze
1086         """
1087         dims = common.get_squeeze_dims(self, dim)
1088         return self.isel({d: 0 for d in dims})
1089 
1090     def _shift_one_dim(self, dim, count, fill_value=dtypes.NA):
1091         axis = self.get_axis_num(dim)
1092 
1093         if count > 0:
1094             keep = slice(None, -count)
1095         elif count < 0:
1096             keep = slice(-count, None)
1097         else:
1098             keep = slice(None)
1099 
1100         trimmed_data = self[(slice(None),) * axis + (keep,)].data
1101 
1102         if fill_value is dtypes.NA:
1103             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1104         else:
1105             dtype = self.dtype
1106 
1107         width = min(abs(count), self.shape[axis])
1108         dim_pad = (width, 0) if count >= 0 else (0, width)
1109         pads = [(0, 0) if d != dim else dim_pad for d in self.dims]
1110 
1111         data = duck_array_ops.pad(
1112             trimmed_data.astype(dtype),
1113             pads,
1114             mode="constant",
1115             constant_values=fill_value,
1116         )
1117 
1118         if isinstance(data, dask_array_type):
1119             # chunked data should come out with the same chunks; this makes
1120             # it feasible to combine shifted and unshifted data
1121             # TODO: remove this once dask.array automatically aligns chunks
1122             data = data.rechunk(self.data.chunks)
1123 
1124         return type(self)(self.dims, data, self._attrs, fastpath=True)
1125 
1126     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):
1127         """
1128         Return a new Variable with shifted data.
1129 
1130         Parameters
1131         ----------
1132         shifts : mapping of the form {dim: offset}
1133             Integer offset to shift along each of the given dimensions.
1134             Positive offsets shift to the right; negative offsets shift to the
1135             left.
1136         fill_value: scalar, optional
1137             Value to use for newly missing values
1138         **shifts_kwargs:
1139             The keyword arguments form of ``shifts``.
1140             One of shifts or shifts_kwargs must be provided.
1141 
1142         Returns
1143         -------
1144         shifted : Variable
1145             Variable with the same dimensions and attributes but shifted data.
1146         """
1147         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
1148         result = self
1149         for dim, count in shifts.items():
1150             result = result._shift_one_dim(dim, count, fill_value=fill_value)
1151         return result
1152 
1153     def pad_with_fill_value(
1154         self, pad_widths=None, fill_value=dtypes.NA, **pad_widths_kwargs
1155     ):
1156         """
1157         Return a new Variable with paddings.
1158 
1159         Parameters
1160         ----------
1161         pad_width: Mapping of the form {dim: (before, after)}
1162             Number of values padded to the edges of each dimension.
1163         **pad_widths_kwargs:
1164             Keyword argument for pad_widths
1165         """
1166         pad_widths = either_dict_or_kwargs(pad_widths, pad_widths_kwargs, "pad")
1167 
1168         if fill_value is dtypes.NA:
1169             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1170         else:
1171             dtype = self.dtype
1172 
1173         if isinstance(self.data, dask_array_type):
1174             array = self.data
1175 
1176             # Dask does not yet support pad. We manually implement it.
1177             # https://github.com/dask/dask/issues/1926
1178             for d, pad in pad_widths.items():
1179                 axis = self.get_axis_num(d)
1180                 before_shape = list(array.shape)
1181                 before_shape[axis] = pad[0]
1182                 before_chunks = list(array.chunks)
1183                 before_chunks[axis] = (pad[0],)
1184                 after_shape = list(array.shape)
1185                 after_shape[axis] = pad[1]
1186                 after_chunks = list(array.chunks)
1187                 after_chunks[axis] = (pad[1],)
1188 
1189                 arrays = []
1190                 if pad[0] > 0:
1191                     arrays.append(
1192                         da.full(
1193                             before_shape, fill_value, dtype=dtype, chunks=before_chunks
1194                         )
1195                     )
1196                 arrays.append(array)
1197                 if pad[1] > 0:
1198                     arrays.append(
1199                         da.full(
1200                             after_shape, fill_value, dtype=dtype, chunks=after_chunks
1201                         )
1202                     )
1203                 if len(arrays) > 1:
1204                     array = da.concatenate(arrays, axis=axis)
1205         else:
1206             pads = [(0, 0) if d not in pad_widths else pad_widths[d] for d in self.dims]
1207             array = np.pad(
1208                 self.data.astype(dtype, copy=False),
1209                 pads,
1210                 mode="constant",
1211                 constant_values=fill_value,
1212             )
1213         return type(self)(self.dims, array)
1214 
1215     def _roll_one_dim(self, dim, count):
1216         axis = self.get_axis_num(dim)
1217 
1218         count %= self.shape[axis]
1219         if count != 0:
1220             indices = [slice(-count, None), slice(None, -count)]
1221         else:
1222             indices = [slice(None)]
1223 
1224         arrays = [self[(slice(None),) * axis + (idx,)].data for idx in indices]
1225 
1226         data = duck_array_ops.concatenate(arrays, axis)
1227 
1228         if isinstance(data, dask_array_type):
1229             # chunked data should come out with the same chunks; this makes
1230             # it feasible to combine shifted and unshifted data
1231             # TODO: remove this once dask.array automatically aligns chunks
1232             data = data.rechunk(self.data.chunks)
1233 
1234         return type(self)(self.dims, data, self._attrs, fastpath=True)
1235 
1236     def roll(self, shifts=None, **shifts_kwargs):
1237         """
1238         Return a new Variable with rolld data.
1239 
1240         Parameters
1241         ----------
1242         shifts : mapping of the form {dim: offset}
1243             Integer offset to roll along each of the given dimensions.
1244             Positive offsets roll to the right; negative offsets roll to the
1245             left.
1246         **shifts_kwargs:
1247             The keyword arguments form of ``shifts``.
1248             One of shifts or shifts_kwargs must be provided.
1249 
1250         Returns
1251         -------
1252         shifted : Variable
1253             Variable with the same dimensions and attributes but rolled data.
1254         """
1255         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
1256 
1257         result = self
1258         for dim, count in shifts.items():
1259             result = result._roll_one_dim(dim, count)
1260         return result
1261 
1262     def transpose(self, *dims) -> "Variable":
1263         """Return a new Variable object with transposed dimensions.
1264 
1265         Parameters
1266         ----------
1267         *dims : str, optional
1268             By default, reverse the dimensions. Otherwise, reorder the
1269             dimensions to this order.
1270 
1271         Returns
1272         -------
1273         transposed : Variable
1274             The returned object has transposed data and dimensions with the
1275             same attributes as the original.
1276 
1277         Notes
1278         -----
1279         This operation returns a view of this variable's data. It is
1280         lazy for dask-backed Variables but not for numpy-backed Variables.
1281 
1282         See Also
1283         --------
1284         numpy.transpose
1285         """
1286         if len(dims) == 0:
1287             dims = self.dims[::-1]
1288         dims = tuple(infix_dims(dims, self.dims))
1289         axes = self.get_axis_num(dims)
1290         if len(dims) < 2 or dims == self.dims:
1291             # no need to transpose if only one dimension
1292             # or dims are in same order
1293             return self.copy(deep=False)
1294 
1295         data = as_indexable(self._data).transpose(axes)
1296         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)
1297 
1298     @property
1299     def T(self) -> "Variable":
1300         return self.transpose()
1301 
1302     def set_dims(self, dims, shape=None):
1303         """Return a new variable with given set of dimensions.
1304         This method might be used to attach new dimension(s) to variable.
1305 
1306         When possible, this operation does not copy this variable's data.
1307 
1308         Parameters
1309         ----------
1310         dims : str or sequence of str or dict
1311             Dimensions to include on the new variable. If a dict, values are
1312             used to provide the sizes of new dimensions; otherwise, new
1313             dimensions are inserted with length 1.
1314 
1315         Returns
1316         -------
1317         Variable
1318         """
1319         if isinstance(dims, str):
1320             dims = [dims]
1321 
1322         if shape is None and utils.is_dict_like(dims):
1323             shape = dims.values()
1324 
1325         missing_dims = set(self.dims) - set(dims)
1326         if missing_dims:
1327             raise ValueError(
1328                 "new dimensions %r must be a superset of "
1329                 "existing dimensions %r" % (dims, self.dims)
1330             )
1331 
1332         self_dims = set(self.dims)
1333         expanded_dims = tuple(d for d in dims if d not in self_dims) + self.dims
1334 
1335         if self.dims == expanded_dims:
1336             # don't use broadcast_to unless necessary so the result remains
1337             # writeable if possible
1338             expanded_data = self.data
1339         elif shape is not None:
1340             dims_map = dict(zip(dims, shape))
1341             tmp_shape = tuple(dims_map[d] for d in expanded_dims)
1342             expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)
1343         else:
1344             expanded_data = self.data[(None,) * (len(expanded_dims) - self.ndim)]
1345 
1346         expanded_var = Variable(
1347             expanded_dims, expanded_data, self._attrs, self._encoding, fastpath=True
1348         )
1349         return expanded_var.transpose(*dims)
1350 
1351     def _stack_once(self, dims, new_dim):
1352         if not set(dims) <= set(self.dims):
1353             raise ValueError("invalid existing dimensions: %s" % dims)
1354 
1355         if new_dim in self.dims:
1356             raise ValueError(
1357                 "cannot create a new dimension with the same "
1358                 "name as an existing dimension"
1359             )
1360 
1361         if len(dims) == 0:
1362             # don't stack
1363             return self.copy(deep=False)
1364 
1365         other_dims = [d for d in self.dims if d not in dims]
1366         dim_order = other_dims + list(dims)
1367         reordered = self.transpose(*dim_order)
1368 
1369         new_shape = reordered.shape[: len(other_dims)] + (-1,)
1370         new_data = reordered.data.reshape(new_shape)
1371         new_dims = reordered.dims[: len(other_dims)] + (new_dim,)
1372 
1373         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)
1374 
1375     def stack(self, dimensions=None, **dimensions_kwargs):
1376         """
1377         Stack any number of existing dimensions into a single new dimension.
1378 
1379         New dimensions will be added at the end, and the order of the data
1380         along each new dimension will be in contiguous (C) order.
1381 
1382         Parameters
1383         ----------
1384         dimensions : Mapping of form new_name=(dim1, dim2, ...)
1385             Names of new dimensions, and the existing dimensions that they
1386             replace.
1387         **dimensions_kwargs:
1388             The keyword arguments form of ``dimensions``.
1389             One of dimensions or dimensions_kwargs must be provided.
1390 
1391         Returns
1392         -------
1393         stacked : Variable
1394             Variable with the same attributes but stacked data.
1395 
1396         See also
1397         --------
1398         Variable.unstack
1399         """
1400         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
1401         result = self
1402         for new_dim, dims in dimensions.items():
1403             result = result._stack_once(dims, new_dim)
1404         return result
1405 
1406     def _unstack_once(self, dims, old_dim):
1407         new_dim_names = tuple(dims.keys())
1408         new_dim_sizes = tuple(dims.values())
1409 
1410         if old_dim not in self.dims:
1411             raise ValueError("invalid existing dimension: %s" % old_dim)
1412 
1413         if set(new_dim_names).intersection(self.dims):
1414             raise ValueError(
1415                 "cannot create a new dimension with the same "
1416                 "name as an existing dimension"
1417             )
1418 
1419         if np.prod(new_dim_sizes) != self.sizes[old_dim]:
1420             raise ValueError(
1421                 "the product of the new dimension sizes must "
1422                 "equal the size of the old dimension"
1423             )
1424 
1425         other_dims = [d for d in self.dims if d != old_dim]
1426         dim_order = other_dims + [old_dim]
1427         reordered = self.transpose(*dim_order)
1428 
1429         new_shape = reordered.shape[: len(other_dims)] + new_dim_sizes
1430         new_data = reordered.data.reshape(new_shape)
1431         new_dims = reordered.dims[: len(other_dims)] + new_dim_names
1432 
1433         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)
1434 
1435     def unstack(self, dimensions=None, **dimensions_kwargs):
1436         """
1437         Unstack an existing dimension into multiple new dimensions.
1438 
1439         New dimensions will be added at the end, and the order of the data
1440         along each new dimension will be in contiguous (C) order.
1441 
1442         Parameters
1443         ----------
1444         dimensions : mapping of the form old_dim={dim1: size1, ...}
1445             Names of existing dimensions, and the new dimensions and sizes
1446             that they map to.
1447         **dimensions_kwargs:
1448             The keyword arguments form of ``dimensions``.
1449             One of dimensions or dimensions_kwargs must be provided.
1450 
1451         Returns
1452         -------
1453         unstacked : Variable
1454             Variable with the same attributes but unstacked data.
1455 
1456         See also
1457         --------
1458         Variable.stack
1459         """
1460         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "unstack")
1461         result = self
1462         for old_dim, dims in dimensions.items():
1463             result = result._unstack_once(dims, old_dim)
1464         return result
1465 
1466     def fillna(self, value):
1467         return ops.fillna(self, value)
1468 
1469     def where(self, cond, other=dtypes.NA):
1470         return ops.where_method(self, cond, other)
1471 
1472     def reduce(
1473         self,
1474         func,
1475         dim=None,
1476         axis=None,
1477         keep_attrs=None,
1478         keepdims=False,
1479         allow_lazy=None,
1480         **kwargs,
1481     ):
1482         """Reduce this array by applying `func` along some dimension(s).
1483 
1484         Parameters
1485         ----------
1486         func : function
1487             Function which can be called in the form
1488             `func(x, axis=axis, **kwargs)` to return the result of reducing an
1489             np.ndarray over an integer valued axis.
1490         dim : str or sequence of str, optional
1491             Dimension(s) over which to apply `func`.
1492         axis : int or sequence of int, optional
1493             Axis(es) over which to apply `func`. Only one of the 'dim'
1494             and 'axis' arguments can be supplied. If neither are supplied, then
1495             the reduction is calculated over the flattened array (by calling
1496             `func(x)` without an axis argument).
1497         keep_attrs : bool, optional
1498             If True, the variable's attributes (`attrs`) will be copied from
1499             the original object to the new one.  If False (default), the new
1500             object will be returned without attributes.
1501         keepdims : bool, default False
1502             If True, the dimensions which are reduced are left in the result
1503             as dimensions of size one
1504         **kwargs : dict
1505             Additional keyword arguments passed on to `func`.
1506 
1507         Returns
1508         -------
1509         reduced : Array
1510             Array with summarized data and the indicated dimension(s)
1511             removed.
1512         """
1513         if dim == ...:
1514             dim = None
1515         if dim is not None and axis is not None:
1516             raise ValueError("cannot supply both 'axis' and 'dim' arguments")
1517 
1518         if dim is not None:
1519             axis = self.get_axis_num(dim)
1520 
1521         if allow_lazy is not None:
1522             warnings.warn(
1523                 "allow_lazy is deprecated and will be removed in version 0.16.0. It is now True by default.",
1524                 DeprecationWarning,
1525             )
1526         else:
1527             allow_lazy = True
1528 
1529         input_data = self.data if allow_lazy else self.values
1530 
1531         if axis is not None:
1532             data = func(input_data, axis=axis, **kwargs)
1533         else:
1534             data = func(input_data, **kwargs)
1535 
1536         if getattr(data, "shape", ()) == self.shape:
1537             dims = self.dims
1538         else:
1539             removed_axes = (
1540                 range(self.ndim) if axis is None else np.atleast_1d(axis) % self.ndim
1541             )
1542             if keepdims:
1543                 # Insert np.newaxis for removed dims
1544                 slices = tuple(
1545                     np.newaxis if i in removed_axes else slice(None, None)
1546                     for i in range(self.ndim)
1547                 )
1548                 if getattr(data, "shape", None) is None:
1549                     # Reduce has produced a scalar value, not an array-like
1550                     data = np.asanyarray(data)[slices]
1551                 else:
1552                     data = data[slices]
1553                 dims = self.dims
1554             else:
1555                 dims = [
1556                     adim for n, adim in enumerate(self.dims) if n not in removed_axes
1557                 ]
1558 
1559         if keep_attrs is None:
1560             keep_attrs = _get_keep_attrs(default=False)
1561         attrs = self._attrs if keep_attrs else None
1562 
1563         return Variable(dims, data, attrs=attrs)
1564 
1565     @classmethod
1566     def concat(cls, variables, dim="concat_dim", positions=None, shortcut=False):
1567         """Concatenate variables along a new or existing dimension.
1568 
1569         Parameters
1570         ----------
1571         variables : iterable of Array
1572             Arrays to stack together. Each variable is expected to have
1573             matching dimensions and shape except for along the stacked
1574             dimension.
1575         dim : str or DataArray, optional
1576             Name of the dimension to stack along. This can either be a new
1577             dimension name, in which case it is added along axis=0, or an
1578             existing dimension name, in which case the location of the
1579             dimension is unchanged. Where to insert the new dimension is
1580             determined by the first variable.
1581         positions : None or list of integer arrays, optional
1582             List of integer arrays which specifies the integer positions to
1583             which to assign each dataset along the concatenated dimension.
1584             If not supplied, objects are concatenated in the provided order.
1585         shortcut : bool, optional
1586             This option is used internally to speed-up groupby operations.
1587             If `shortcut` is True, some checks of internal consistency between
1588             arrays to concatenate are skipped.
1589 
1590         Returns
1591         -------
1592         stacked : Variable
1593             Concatenated Variable formed by stacking all the supplied variables
1594             along the given dimension.
1595         """
1596         if not isinstance(dim, str):
1597             (dim,) = dim.dims
1598 
1599         # can't do this lazily: we need to loop through variables at least
1600         # twice
1601         variables = list(variables)
1602         first_var = variables[0]
1603 
1604         arrays = [v.data for v in variables]
1605 
1606         if dim in first_var.dims:
1607             axis = first_var.get_axis_num(dim)
1608             dims = first_var.dims
1609             data = duck_array_ops.concatenate(arrays, axis=axis)
1610             if positions is not None:
1611                 # TODO: deprecate this option -- we don't need it for groupby
1612                 # any more.
1613                 indices = nputils.inverse_permutation(np.concatenate(positions))
1614                 data = duck_array_ops.take(data, indices, axis=axis)
1615         else:
1616             axis = 0
1617             dims = (dim,) + first_var.dims
1618             data = duck_array_ops.stack(arrays, axis=axis)
1619 
1620         attrs = dict(first_var.attrs)
1621         encoding = dict(first_var.encoding)
1622         if not shortcut:
1623             for var in variables:
1624                 if var.dims != first_var.dims:
1625                     raise ValueError(
1626                         f"Variable has dimensions {list(var.dims)} but first Variable has dimensions {list(first_var.dims)}"
1627                     )
1628 
1629         return cls(dims, data, attrs, encoding)
1630 
1631     def equals(self, other, equiv=duck_array_ops.array_equiv):
1632         """True if two Variables have the same dimensions and values;
1633         otherwise False.
1634 
1635         Variables can still be equal (like pandas objects) if they have NaN
1636         values in the same locations.
1637 
1638         This method is necessary because `v1 == v2` for Variables
1639         does element-wise comparisons (like numpy.ndarrays).
1640         """
1641         other = getattr(other, "variable", other)
1642         try:
1643             return self.dims == other.dims and (
1644                 self._data is other._data or equiv(self.data, other.data)
1645             )
1646         except (TypeError, AttributeError):
1647             return False
1648 
1649     def broadcast_equals(self, other, equiv=duck_array_ops.array_equiv):
1650         """True if two Variables have the values after being broadcast against
1651         each other; otherwise False.
1652 
1653         Variables can still be equal (like pandas objects) if they have NaN
1654         values in the same locations.
1655         """
1656         try:
1657             self, other = broadcast_variables(self, other)
1658         except (ValueError, AttributeError):
1659             return False
1660         return self.equals(other, equiv=equiv)
1661 
1662     def identical(self, other, equiv=duck_array_ops.array_equiv):
1663         """Like equals, but also checks attributes.
1664         """
1665         try:
1666             return utils.dict_equiv(self.attrs, other.attrs) and self.equals(
1667                 other, equiv=equiv
1668             )
1669         except (TypeError, AttributeError):
1670             return False
1671 
1672     def no_conflicts(self, other, equiv=duck_array_ops.array_notnull_equiv):
1673         """True if the intersection of two Variable's non-null data is
1674         equal; otherwise false.
1675 
1676         Variables can thus still be equal if there are locations where either,
1677         or both, contain NaN values.
1678         """
1679         return self.broadcast_equals(other, equiv=equiv)
1680 
1681     def quantile(
1682         self, q, dim=None, interpolation="linear", keep_attrs=None, skipna=True
1683     ):
1684         """Compute the qth quantile of the data along the specified dimension.
1685 
1686         Returns the qth quantiles(s) of the array elements.
1687 
1688         Parameters
1689         ----------
1690         q : float in range of [0,1] (or sequence of floats)
1691             Quantile to compute, which must be between 0 and 1
1692             inclusive.
1693         dim : str or sequence of str, optional
1694             Dimension(s) over which to apply quantile.
1695         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
1696             This optional parameter specifies the interpolation method to
1697             use when the desired quantile lies between two data points
1698             ``i < j``:
1699 
1700                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is
1701                   the fractional part of the index surrounded by ``i`` and
1702                   ``j``.
1703                 * lower: ``i``.
1704                 * higher: ``j``.
1705                 * nearest: ``i`` or ``j``, whichever is nearest.
1706                 * midpoint: ``(i + j) / 2``.
1707 
1708         keep_attrs : bool, optional
1709             If True, the variable's attributes (`attrs`) will be copied from
1710             the original object to the new one.  If False (default), the new
1711             object will be returned without attributes.
1712 
1713         Returns
1714         -------
1715         quantiles : Variable
1716             If `q` is a single quantile, then the result
1717             is a scalar. If multiple percentiles are given, first axis of
1718             the result corresponds to the quantile and a quantile dimension
1719             is added to the return array. The other dimensions are the
1720             dimensions that remain after the reduction of the array.
1721 
1722         See Also
1723         --------
1724         numpy.nanquantile, pandas.Series.quantile, Dataset.quantile,
1725         DataArray.quantile
1726         """
1727 
1728         from .computation import apply_ufunc
1729 
1730         _quantile_func = np.nanquantile if skipna else np.quantile
1731 
1732         if keep_attrs is None:
1733             keep_attrs = _get_keep_attrs(default=False)
1734 
1735         scalar = utils.is_scalar(q)
1736         q = np.atleast_1d(np.asarray(q, dtype=np.float64))
1737 
1738         if dim is None:
1739             dim = self.dims
1740 
1741         if utils.is_scalar(dim):
1742             dim = [dim]
1743 
1744         def _wrapper(npa, **kwargs):
1745             # move quantile axis to end. required for apply_ufunc
1746             return np.moveaxis(_quantile_func(npa, **kwargs), 0, -1)
1747 
1748         axis = np.arange(-1, -1 * len(dim) - 1, -1)
1749         result = apply_ufunc(
1750             _wrapper,
1751             self,
1752             input_core_dims=[dim],
1753             exclude_dims=set(dim),
1754             output_core_dims=[["quantile"]],
1755             output_dtypes=[np.float64],
1756             output_sizes={"quantile": len(q)},
1757             dask="parallelized",
1758             kwargs={"q": q, "axis": axis, "interpolation": interpolation},
1759         )
1760 
1761         # for backward compatibility
1762         result = result.transpose("quantile", ...)
1763         if scalar:
1764             result = result.squeeze("quantile")
1765         if keep_attrs:
1766             result.attrs = self._attrs
1767         return result
1768 
1769     def rank(self, dim, pct=False):
1770         """Ranks the data.
1771 
1772         Equal values are assigned a rank that is the average of the ranks that
1773         would have been otherwise assigned to all of the values within that
1774         set.  Ranks begin at 1, not 0. If `pct`, computes percentage ranks.
1775 
1776         NaNs in the input array are returned as NaNs.
1777 
1778         The `bottleneck` library is required.
1779 
1780         Parameters
1781         ----------
1782         dim : str
1783             Dimension over which to compute rank.
1784         pct : bool, optional
1785             If True, compute percentage ranks, otherwise compute integer ranks.
1786 
1787         Returns
1788         -------
1789         ranked : Variable
1790 
1791         See Also
1792         --------
1793         Dataset.rank, DataArray.rank
1794         """
1795         import bottleneck as bn
1796 
1797         data = self.data
1798 
1799         if isinstance(data, dask_array_type):
1800             raise TypeError(
1801                 "rank does not work for arrays stored as dask "
1802                 "arrays. Load the data via .compute() or .load() "
1803                 "prior to calling this method."
1804             )
1805         elif not isinstance(data, np.ndarray):
1806             raise TypeError(
1807                 "rank is not implemented for {} objects.".format(type(data))
1808             )
1809 
1810         axis = self.get_axis_num(dim)
1811         func = bn.nanrankdata if self.dtype.kind == "f" else bn.rankdata
1812         ranked = func(data, axis=axis)
1813         if pct:
1814             count = np.sum(~np.isnan(data), axis=axis, keepdims=True)
1815             ranked /= count
1816         return Variable(self.dims, ranked)
1817 
1818     def rolling_window(
1819         self, dim, window, window_dim, center=False, fill_value=dtypes.NA
1820     ):
1821         """
1822         Make a rolling_window along dim and add a new_dim to the last place.
1823 
1824         Parameters
1825         ----------
1826         dim: str
1827             Dimension over which to compute rolling_window
1828         window: int
1829             Window size of the rolling
1830         window_dim: str
1831             New name of the window dimension.
1832         center: boolean. default False.
1833             If True, pad fill_value for both ends. Otherwise, pad in the head
1834             of the axis.
1835         fill_value:
1836             value to be filled.
1837 
1838         Returns
1839         -------
1840         Variable that is a view of the original array with a added dimension of
1841         size w.
1842         The return dim: self.dims + (window_dim, )
1843         The return shape: self.shape + (window, )
1844 
1845         Examples
1846         --------
1847         >>> v = Variable(("a", "b"), np.arange(8).reshape((2, 4)))
1848         >>> v.rolling_window(x, "b", 3, "window_dim")
1849         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
1850         array([[[nan, nan, 0], [nan, 0, 1], [0, 1, 2], [1, 2, 3]],
1851                [[nan, nan, 4], [nan, 4, 5], [4, 5, 6], [5, 6, 7]]])
1852 
1853         >>> v.rolling_window(x, "b", 3, "window_dim", center=True)
1854         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
1855         array([[[nan, 0, 1], [0, 1, 2], [1, 2, 3], [2, 3, nan]],
1856                [[nan, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, nan]]])
1857         """
1858         if fill_value is dtypes.NA:  # np.nan is passed
1859             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1860             array = self.astype(dtype, copy=False).data
1861         else:
1862             dtype = self.dtype
1863             array = self.data
1864 
1865         new_dims = self.dims + (window_dim,)
1866         return Variable(
1867             new_dims,
1868             duck_array_ops.rolling_window(
1869                 array,
1870                 axis=self.get_axis_num(dim),
1871                 window=window,
1872                 center=center,
1873                 fill_value=fill_value,
1874             ),
1875         )
1876 
1877     def coarsen(self, windows, func, boundary="exact", side="left", **kwargs):
1878         """
1879         Apply reduction function.
1880         """
1881         windows = {k: v for k, v in windows.items() if k in self.dims}
1882         if not windows:
1883             return self.copy()
1884 
1885         reshaped, axes = self._coarsen_reshape(windows, boundary, side)
1886         if isinstance(func, str):
1887             name = func
1888             func = getattr(duck_array_ops, name, None)
1889             if func is None:
1890                 raise NameError(f"{name} is not a valid method.")
1891         return self._replace(data=func(reshaped, axis=axes, **kwargs))
1892 
1893     def _coarsen_reshape(self, windows, boundary, side):
1894         """
1895         Construct a reshaped-array for coarsen
1896         """
1897         if not utils.is_dict_like(boundary):
1898             boundary = {d: boundary for d in windows.keys()}
1899 
1900         if not utils.is_dict_like(side):
1901             side = {d: side for d in windows.keys()}
1902 
1903         # remove unrelated dimensions
1904         boundary = {k: v for k, v in boundary.items() if k in windows}
1905         side = {k: v for k, v in side.items() if k in windows}
1906 
1907         for d, window in windows.items():
1908             if window <= 0:
1909                 raise ValueError(f"window must be > 0. Given {window}")
1910 
1911         variable = self
1912         for d, window in windows.items():
1913             # trim or pad the object
1914             size = variable.shape[self._get_axis_num(d)]
1915             n = int(size / window)
1916             if boundary[d] == "exact":
1917                 if n * window != size:
1918                     raise ValueError(
1919                         "Could not coarsen a dimension of size {} with "
1920                         "window {}".format(size, window)
1921                     )
1922             elif boundary[d] == "trim":
1923                 if side[d] == "left":
1924                     variable = variable.isel({d: slice(0, window * n)})
1925                 else:
1926                     excess = size - window * n
1927                     variable = variable.isel({d: slice(excess, None)})
1928             elif boundary[d] == "pad":  # pad
1929                 pad = window * n - size
1930                 if pad < 0:
1931                     pad += window
1932                 if side[d] == "left":
1933                     pad_widths = {d: (0, pad)}
1934                 else:
1935                     pad_widths = {d: (pad, 0)}
1936                 variable = variable.pad_with_fill_value(pad_widths)
1937             else:
1938                 raise TypeError(
1939                     "{} is invalid for boundary. Valid option is 'exact', "
1940                     "'trim' and 'pad'".format(boundary[d])
1941                 )
1942 
1943         shape = []
1944         axes = []
1945         axis_count = 0
1946         for i, d in enumerate(variable.dims):
1947             if d in windows:
1948                 size = variable.shape[i]
1949                 shape.append(int(size / windows[d]))
1950                 shape.append(windows[d])
1951                 axis_count += 1
1952                 axes.append(i + axis_count)
1953             else:
1954                 shape.append(variable.shape[i])
1955 
1956         keep_attrs = _get_keep_attrs(default=False)
1957         variable.attrs = variable._attrs if keep_attrs else {}
1958 
1959         return variable.data.reshape(shape), tuple(axes)
1960 
1961     @property
1962     def real(self):
1963         return type(self)(self.dims, self.data.real, self._attrs)
1964 
1965     @property
1966     def imag(self):
1967         return type(self)(self.dims, self.data.imag, self._attrs)
1968 
1969     def __array_wrap__(self, obj, context=None):
1970         return Variable(self.dims, obj)
1971 
1972     @staticmethod
1973     def _unary_op(f):
1974         @functools.wraps(f)
1975         def func(self, *args, **kwargs):
1976             with np.errstate(all="ignore"):
1977                 return self.__array_wrap__(f(self.data, *args, **kwargs))
1978 
1979         return func
1980 
1981     @staticmethod
1982     def _binary_op(f, reflexive=False, **ignored_kwargs):
1983         @functools.wraps(f)
1984         def func(self, other):
1985             if isinstance(other, (xr.DataArray, xr.Dataset)):
1986                 return NotImplemented
1987             self_data, other_data, dims = _broadcast_compat_data(self, other)
1988             keep_attrs = _get_keep_attrs(default=False)
1989             attrs = self._attrs if keep_attrs else None
1990             with np.errstate(all="ignore"):
1991                 new_data = (
1992                     f(self_data, other_data)
1993                     if not reflexive
1994                     else f(other_data, self_data)
1995                 )
1996             result = Variable(dims, new_data, attrs=attrs)
1997             return result
1998 
1999         return func
2000 
2001     @staticmethod
2002     def _inplace_binary_op(f):
2003         @functools.wraps(f)
2004         def func(self, other):
2005             if isinstance(other, xr.Dataset):
2006                 raise TypeError("cannot add a Dataset to a Variable in-place")
2007             self_data, other_data, dims = _broadcast_compat_data(self, other)
2008             if dims != self.dims:
2009                 raise ValueError("dimensions cannot change for in-place " "operations")
2010             with np.errstate(all="ignore"):
2011                 self.values = f(self_data, other_data)
2012             return self
2013 
2014         return func
2015 
2016     def _to_numeric(self, offset=None, datetime_unit=None, dtype=float):
2017         """ A (private) method to convert datetime array to numeric dtype
2018         See duck_array_ops.datetime_to_numeric
2019         """
2020         numeric_array = duck_array_ops.datetime_to_numeric(
2021             self.data, offset, datetime_unit, dtype
2022         )
2023         return type(self)(self.dims, numeric_array, self._attrs)
2024 
2025 
2026 ops.inject_all_ops_and_reduce_methods(Variable)
2027 
2028 
2029 class IndexVariable(Variable):
2030     """Wrapper for accommodating a pandas.Index in an xarray.Variable.
2031 
2032     IndexVariable preserve loaded values in the form of a pandas.Index instead
2033     of a NumPy array. Hence, their values are immutable and must always be one-
2034     dimensional.
2035 
2036     They also have a name property, which is the name of their sole dimension
2037     unless another name is given.
2038     """
2039 
2040     __slots__ = ()
2041 
2042     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):
2043         super().__init__(dims, data, attrs, encoding, fastpath)
2044         if self.ndim != 1:
2045             raise ValueError("%s objects must be 1-dimensional" % type(self).__name__)
2046 
2047         # Unlike in Variable, always eagerly load values into memory
2048         if not isinstance(self._data, PandasIndexAdapter):
2049             self._data = PandasIndexAdapter(self._data)
2050 
2051     def __dask_tokenize__(self):
2052         from dask.base import normalize_token
2053 
2054         # Don't waste time converting pd.Index to np.ndarray
2055         return normalize_token((type(self), self._dims, self._data.array, self._attrs))
2056 
2057     def load(self):
2058         # data is already loaded into memory for IndexVariable
2059         return self
2060 
2061     # https://github.com/python/mypy/issues/1465
2062     @Variable.data.setter  # type: ignore
2063     def data(self, data):
2064         Variable.data.fset(self, data)
2065         if not isinstance(self._data, PandasIndexAdapter):
2066             self._data = PandasIndexAdapter(self._data)
2067 
2068     def chunk(self, chunks=None, name=None, lock=False):
2069         # Dummy - do not chunk. This method is invoked e.g. by Dataset.chunk()
2070         return self.copy(deep=False)
2071 
2072     def _as_sparse(self, sparse_format=_default, fill_value=_default):
2073         # Dummy
2074         return self.copy(deep=False)
2075 
2076     def _to_dense(self):
2077         # Dummy
2078         return self.copy(deep=False)
2079 
2080     def _finalize_indexing_result(self, dims, data):
2081         if getattr(data, "ndim", 0) != 1:
2082             # returns Variable rather than IndexVariable if multi-dimensional
2083             return Variable(dims, data, self._attrs, self._encoding)
2084         else:
2085             return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)
2086 
2087     def __setitem__(self, key, value):
2088         raise TypeError("%s values cannot be modified" % type(self).__name__)
2089 
2090     @classmethod
2091     def concat(cls, variables, dim="concat_dim", positions=None, shortcut=False):
2092         """Specialized version of Variable.concat for IndexVariable objects.
2093 
2094         This exists because we want to avoid converting Index objects to NumPy
2095         arrays, if possible.
2096         """
2097         if not isinstance(dim, str):
2098             (dim,) = dim.dims
2099 
2100         variables = list(variables)
2101         first_var = variables[0]
2102 
2103         if any(not isinstance(v, cls) for v in variables):
2104             raise TypeError(
2105                 "IndexVariable.concat requires that all input "
2106                 "variables be IndexVariable objects"
2107             )
2108 
2109         indexes = [v._data.array for v in variables]
2110 
2111         if not indexes:
2112             data = []
2113         else:
2114             data = indexes[0].append(indexes[1:])
2115 
2116             if positions is not None:
2117                 indices = nputils.inverse_permutation(np.concatenate(positions))
2118                 data = data.take(indices)
2119 
2120         attrs = dict(first_var.attrs)
2121         if not shortcut:
2122             for var in variables:
2123                 if var.dims != first_var.dims:
2124                     raise ValueError("inconsistent dimensions")
2125                 utils.remove_incompatible_items(attrs, var.attrs)
2126 
2127         return cls(first_var.dims, data, attrs)
2128 
2129     def copy(self, deep=True, data=None):
2130         """Returns a copy of this object.
2131 
2132         `deep` is ignored since data is stored in the form of
2133         pandas.Index, which is already immutable. Dimensions, attributes
2134         and encodings are always copied.
2135 
2136         Use `data` to create a new object with the same structure as
2137         original but entirely new data.
2138 
2139         Parameters
2140         ----------
2141         deep : bool, optional
2142             Deep is ignored when data is given. Whether the data array is
2143             loaded into memory and copied onto the new object. Default is True.
2144         data : array_like, optional
2145             Data to use in the new object. Must have same shape as original.
2146 
2147         Returns
2148         -------
2149         object : Variable
2150             New object with dimensions, attributes, encodings, and optionally
2151             data copied from original.
2152         """
2153         if data is None:
2154             data = self._data.copy(deep=deep)
2155         else:
2156             data = as_compatible_data(data)
2157             if self.shape != data.shape:
2158                 raise ValueError(
2159                     "Data shape {} must match shape of object {}".format(
2160                         data.shape, self.shape
2161                     )
2162                 )
2163         return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)
2164 
2165     def equals(self, other, equiv=None):
2166         # if equiv is specified, super up
2167         if equiv is not None:
2168             return super().equals(other, equiv)
2169 
2170         # otherwise use the native index equals, rather than looking at _data
2171         other = getattr(other, "variable", other)
2172         try:
2173             return self.dims == other.dims and self._data_equals(other)
2174         except (TypeError, AttributeError):
2175             return False
2176 
2177     def _data_equals(self, other):
2178         return self.to_index().equals(other.to_index())
2179 
2180     def to_index_variable(self):
2181         """Return this variable as an xarray.IndexVariable"""
2182         return self
2183 
2184     to_coord = utils.alias(to_index_variable, "to_coord")
2185 
2186     def to_index(self):
2187         """Convert this variable to a pandas.Index"""
2188         # n.b. creating a new pandas.Index from an old pandas.Index is
2189         # basically free as pandas.Index objects are immutable
2190         assert self.ndim == 1
2191         index = self._data.array
2192         if isinstance(index, pd.MultiIndex):
2193             # set default names for multi-index unnamed levels so that
2194             # we can safely rename dimension / coordinate later
2195             valid_level_names = [
2196                 name or "{}_level_{}".format(self.dims[0], i)
2197                 for i, name in enumerate(index.names)
2198             ]
2199             index = index.set_names(valid_level_names)
2200         else:
2201             index = index.set_names(self.name)
2202         return index
2203 
2204     @property
2205     def level_names(self):
2206         """Return MultiIndex level names or None if this IndexVariable has no
2207         MultiIndex.
2208         """
2209         index = self.to_index()
2210         if isinstance(index, pd.MultiIndex):
2211             return index.names
2212         else:
2213             return None
2214 
2215     def get_level_variable(self, level):
2216         """Return a new IndexVariable from a given MultiIndex level."""
2217         if self.level_names is None:
2218             raise ValueError("IndexVariable %r has no MultiIndex" % self.name)
2219         index = self.to_index()
2220         return type(self)(self.dims, index.get_level_values(level))
2221 
2222     @property
2223     def name(self):
2224         return self.dims[0]
2225 
2226     @name.setter
2227     def name(self, value):
2228         raise AttributeError("cannot modify name of IndexVariable in-place")
2229 
2230 
2231 # for backwards compatibility
2232 Coordinate = utils.alias(IndexVariable, "Coordinate")
2233 
2234 
2235 def _unified_dims(variables):
2236     # validate dimensions
2237     all_dims = {}
2238     for var in variables:
2239         var_dims = var.dims
2240         if len(set(var_dims)) < len(var_dims):
2241             raise ValueError(
2242                 "broadcasting cannot handle duplicate "
2243                 "dimensions: %r" % list(var_dims)
2244             )
2245         for d, s in zip(var_dims, var.shape):
2246             if d not in all_dims:
2247                 all_dims[d] = s
2248             elif all_dims[d] != s:
2249                 raise ValueError(
2250                     "operands cannot be broadcast together "
2251                     "with mismatched lengths for dimension %r: %s"
2252                     % (d, (all_dims[d], s))
2253                 )
2254     return all_dims
2255 
2256 
2257 def _broadcast_compat_variables(*variables):
2258     """Create broadcast compatible variables, with the same dimensions.
2259 
2260     Unlike the result of broadcast_variables(), some variables may have
2261     dimensions of size 1 instead of the the size of the broadcast dimension.
2262     """
2263     dims = tuple(_unified_dims(variables))
2264     return tuple(var.set_dims(dims) if var.dims != dims else var for var in variables)
2265 
2266 
2267 def broadcast_variables(*variables):
2268     """Given any number of variables, return variables with matching dimensions
2269     and broadcast data.
2270 
2271     The data on the returned variables will be a view of the data on the
2272     corresponding original arrays, but dimensions will be reordered and
2273     inserted so that both broadcast arrays have the same dimensions. The new
2274     dimensions are sorted in order of appearance in the first variable's
2275     dimensions followed by the second variable's dimensions.
2276     """
2277     dims_map = _unified_dims(variables)
2278     dims_tuple = tuple(dims_map)
2279     return tuple(
2280         var.set_dims(dims_map) if var.dims != dims_tuple else var for var in variables
2281     )
2282 
2283 
2284 def _broadcast_compat_data(self, other):
2285     if all(hasattr(other, attr) for attr in ["dims", "data", "shape", "encoding"]):
2286         # `other` satisfies the necessary Variable API for broadcast_variables
2287         new_self, new_other = _broadcast_compat_variables(self, other)
2288         self_data = new_self.data
2289         other_data = new_other.data
2290         dims = new_self.dims
2291     else:
2292         # rely on numpy broadcasting rules
2293         self_data = self.data
2294         other_data = other
2295         dims = self.dims
2296     return self_data, other_data, dims
2297 
2298 
2299 def concat(variables, dim="concat_dim", positions=None, shortcut=False):
2300     """Concatenate variables along a new or existing dimension.
2301 
2302     Parameters
2303     ----------
2304     variables : iterable of Array
2305         Arrays to stack together. Each variable is expected to have
2306         matching dimensions and shape except for along the stacked
2307         dimension.
2308     dim : str or DataArray, optional
2309         Name of the dimension to stack along. This can either be a new
2310         dimension name, in which case it is added along axis=0, or an
2311         existing dimension name, in which case the location of the
2312         dimension is unchanged. Where to insert the new dimension is
2313         determined by the first variable.
2314     positions : None or list of integer arrays, optional
2315         List of integer arrays which specifies the integer positions to which
2316         to assign each dataset along the concatenated dimension. If not
2317         supplied, objects are concatenated in the provided order.
2318     shortcut : bool, optional
2319         This option is used internally to speed-up groupby operations.
2320         If `shortcut` is True, some checks of internal consistency between
2321         arrays to concatenate are skipped.
2322 
2323     Returns
2324     -------
2325     stacked : Variable
2326         Concatenated Variable formed by stacking all the supplied variables
2327         along the given dimension.
2328     """
2329     variables = list(variables)
2330     if all(isinstance(v, IndexVariable) for v in variables):
2331         return IndexVariable.concat(variables, dim, positions, shortcut)
2332     else:
2333         return Variable.concat(variables, dim, positions, shortcut)
2334 
2335 
2336 def assert_unique_multiindex_level_names(variables):
2337     """Check for uniqueness of MultiIndex level names in all given
2338     variables.
2339 
2340     Not public API. Used for checking consistency of DataArray and Dataset
2341     objects.
2342     """
2343     level_names = defaultdict(list)
2344     all_level_names = set()
2345     for var_name, var in variables.items():
2346         if isinstance(var._data, PandasIndexAdapter):
2347             idx_level_names = var.to_index_variable().level_names
2348             if idx_level_names is not None:
2349                 for n in idx_level_names:
2350                     level_names[n].append(f"{n!r} ({var_name})")
2351             if idx_level_names:
2352                 all_level_names.update(idx_level_names)
2353 
2354     for k, v in level_names.items():
2355         if k in variables:
2356             v.append("(%s)" % k)
2357 
2358     duplicate_names = [v for v in level_names.values() if len(v) > 1]
2359     if duplicate_names:
2360         conflict_str = "\n".join([", ".join(v) for v in duplicate_names])
2361         raise ValueError("conflicting MultiIndex level name(s):\n%s" % conflict_str)
2362     # Check confliction between level names and dimensions GH:2299
2363     for k, v in variables.items():
2364         for d in v.dims:
2365             if d in all_level_names:
2366                 raise ValueError(
2367                     "conflicting level / dimension names. {} "
2368                     "already exists as a level name.".format(d)
2369                 )

</code><intended_behavior>Ensure support for the weighted average computation at the variable level, potentially introducing helper functions or methods necessary for performing the weighted average calculations that 'Dataset' and 'DataArray' methods can then utilize. This may involve processing the weights alongside the data and appropriately handling dimensions.</intended_behavior>]
2025-10-18 15:09:44.251 | INFO     | Search completed. Bug locations: [<file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>None</method>
<code>
422 class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
423     """A multi-dimensional, in memory, array database.
424 
425     A dataset resembles an in-memory representation of a NetCDF file, and
426     consists of variables, coordinates and attributes which together form a
427     self describing dataset.
428 
429     Dataset implements the mapping interface with keys given by variable names
430     and values given by DataArray objects for each variable name.
431 
432     One dimensional variables with name equal to their dimension are index
433     coordinates used for label based indexing.
434     """
435 
436     _attrs: Optional[Dict[Hashable, Any]]
437     _cache: Dict[str, Any]
438     _coord_names: Set[Hashable]
439     _dims: Dict[Hashable, int]
440     _encoding: Optional[Dict[Hashable, Any]]
441     _indexes: Optional[Dict[Hashable, pd.Index]]
442     _variables: Dict[Hashable, Variable]
443 
444     __slots__ = (
445         "_attrs",
446         "_cache",
447         "_coord_names",
448         "_dims",
449         "_encoding",
450         "_file_obj",
451         "_indexes",
452         "_variables",
453         "__weakref__",
454     )
455 
456     _groupby_cls = groupby.DatasetGroupBy
457     _rolling_cls = rolling.DatasetRolling
458     _coarsen_cls = rolling.DatasetCoarsen
459     _resample_cls = resample.DatasetResample
460 
461     def __init__(
462         self,
463         # could make a VariableArgs to use more generally, and refine these
464         # categories
465         data_vars: Mapping[Hashable, Any] = None,
466         coords: Mapping[Hashable, Any] = None,
467         attrs: Mapping[Hashable, Any] = None,
468     ):
469         """To load data from a file or file-like object, use the `open_dataset`
470         function.
471 
472         Parameters
473         ----------
474         data_vars : dict-like, optional
475             A mapping from variable names to :py:class:`~xarray.DataArray`
476             objects, :py:class:`~xarray.Variable` objects or to tuples of the
477             form ``(dims, data[, attrs])`` which can be used as arguments to
478             create a new ``Variable``. Each dimension must have the same length
479             in all variables in which it appears.
480 
481             The following notations are accepted:
482 
483             - mapping {var name: DataArray}
484             - mapping {var name: Variable}
485             - mapping {var name: (dimension name, array-like)}
486             - mapping {var name: (tuple of dimension names, array-like)}
487             - mapping {dimension name: array-like}
488               (it will be automatically moved to coords, see below)
489 
490             Each dimension must have the same length in all variables in which
491             it appears.
492         coords : dict-like, optional
493             Another mapping in similar form as the `data_vars` argument,
494             except the each item is saved on the dataset as a "coordinate".
495             These variables have an associated meaning: they describe
496             constant/fixed/independent quantities, unlike the
497             varying/measured/dependent quantities that belong in `variables`.
498             Coordinates values may be given by 1-dimensional arrays or scalars,
499             in which case `dims` do not need to be supplied: 1D arrays will be
500             assumed to give index values along the dimension with the same
501             name.
502 
503             The following notations are accepted:
504 
505             - mapping {coord name: DataArray}
506             - mapping {coord name: Variable}
507             - mapping {coord name: (dimension name, array-like)}
508             - mapping {coord name: (tuple of dimension names, array-like)}
509             - mapping {dimension name: array-like}
510               (the dimension name is implicitly set to be the same as the coord name)
511 
512             The last notation implies that the coord name is the same as the
513             dimension name.
514 
515         attrs : dict-like, optional
516             Global attributes to save on this dataset.
517         """
518 
519         # TODO(shoyer): expose indexes as a public argument in __init__
520 
521         if data_vars is None:
522             data_vars = {}
523         if coords is None:
524             coords = {}
525 
526         both_data_and_coords = set(data_vars) & set(coords)
527         if both_data_and_coords:
528             raise ValueError(
529                 "variables %r are found in both data_vars and coords"
530                 % both_data_and_coords
531             )
532 
533         if isinstance(coords, Dataset):
534             coords = coords.variables
535 
536         variables, coord_names, dims, indexes = merge_data_and_coords(
537             data_vars, coords, compat="broadcast_equals"
538         )
539 
540         self._attrs = dict(attrs) if attrs is not None else None
541         self._file_obj = None
542         self._encoding = None
543         self._variables = variables
544         self._coord_names = coord_names
545         self._dims = dims
546         self._indexes = indexes
547 
548     @classmethod
549     def load_store(cls, store, decoder=None) -> "Dataset":
550         """Create a new dataset from the contents of a backends.*DataStore
551         object
552         """
553         variables, attributes = store.load()
554         if decoder:
555             variables, attributes = decoder(variables, attributes)
556         obj = cls(variables, attrs=attributes)
557         obj._file_obj = store
558         return obj
559 
560     @property
561     def variables(self) -> Mapping[Hashable, Variable]:
562         """Low level interface to Dataset contents as dict of Variable objects.
563 
564         This ordered dictionary is frozen to prevent mutation that could
565         violate Dataset invariants. It contains all variable objects
566         constituting the Dataset, including both data variables and
567         coordinates.
568         """
569         return Frozen(self._variables)
570 
571     @property
572     def attrs(self) -> Dict[Hashable, Any]:
573         """Dictionary of global attributes on this dataset
574         """
575         if self._attrs is None:
576             self._attrs = {}
577         return self._attrs
578 
579     @attrs.setter
580     def attrs(self, value: Mapping[Hashable, Any]) -> None:
581         self._attrs = dict(value)
582 
583     @property
584     def encoding(self) -> Dict:
585         """Dictionary of global encoding attributes on this dataset
586         """
587         if self._encoding is None:
588             self._encoding = {}
589         return self._encoding
590 
591     @encoding.setter
592     def encoding(self, value: Mapping) -> None:
593         self._encoding = dict(value)
594 
595     @property
596     def dims(self) -> Mapping[Hashable, int]:
597         """Mapping from dimension names to lengths.
598 
599         Cannot be modified directly, but is updated when adding new variables.
600 
601         Note that type of this object differs from `DataArray.dims`.
602         See `Dataset.sizes` and `DataArray.sizes` for consistently named
603         properties.
604         """
605         return Frozen(SortedKeysDict(self._dims))
606 
607     @property
608     def sizes(self) -> Mapping[Hashable, int]:
609         """Mapping from dimension names to lengths.
610 
611         Cannot be modified directly, but is updated when adding new variables.
612 
613         This is an alias for `Dataset.dims` provided for the benefit of
614         consistency with `DataArray.sizes`.
615 
616         See also
617         --------
618         DataArray.sizes
619         """
620         return self.dims
621 
622     def load(self, **kwargs) -> "Dataset":
623         """Manually trigger loading and/or computation of this dataset's data
624         from disk or a remote source into memory and return this dataset.
625         Unlike compute, the original dataset is modified and returned.
626 
627         Normally, it should not be necessary to call this method in user code,
628         because all xarray functions should either work on deferred data or
629         load data automatically. However, this method can be necessary when
630         working with many file objects on disk.
631 
632         Parameters
633         ----------
634         **kwargs : dict
635             Additional keyword arguments passed on to ``dask.array.compute``.
636 
637         See Also
638         --------
639         dask.array.compute
640         """
641         # access .data to coerce everything to numpy or dask arrays
642         lazy_data = {
643             k: v._data
644             for k, v in self.variables.items()
645             if isinstance(v._data, dask_array_type)
646         }
647         if lazy_data:
648             import dask.array as da
649 
650             # evaluate all the dask arrays simultaneously
651             evaluated_data = da.compute(*lazy_data.values(), **kwargs)
652 
653             for k, data in zip(lazy_data, evaluated_data):
654                 self.variables[k].data = data
655 
656         # load everything else sequentially
657         for k, v in self.variables.items():
658             if k not in lazy_data:
659                 v.load()
660 
661         return self
662 
663     def __dask_tokenize__(self):
664         from dask.base import normalize_token
665 
666         return normalize_token(
667             (type(self), self._variables, self._coord_names, self._attrs)
668         )
669 
670     def __dask_graph__(self):
671         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}
672         graphs = {k: v for k, v in graphs.items() if v is not None}
673         if not graphs:
674             return None
675         else:
676             try:
677                 from dask.highlevelgraph import HighLevelGraph
678 
679                 return HighLevelGraph.merge(*graphs.values())
680             except ImportError:
681                 from dask import sharedict
682 
683                 return sharedict.merge(*graphs.values())
684 
685     def __dask_keys__(self):
686         import dask
687 
688         return [
689             v.__dask_keys__()
690             for v in self.variables.values()
691             if dask.is_dask_collection(v)
692         ]
693 
694     def __dask_layers__(self):
695         import dask
696 
697         return sum(
698             [
699                 v.__dask_layers__()
700                 for v in self.variables.values()
701                 if dask.is_dask_collection(v)
702             ],
703             (),
704         )
705 
706     @property
707     def __dask_optimize__(self):
708         import dask.array as da
709 
710         return da.Array.__dask_optimize__
711 
712     @property
713     def __dask_scheduler__(self):
714         import dask.array as da
715 
716         return da.Array.__dask_scheduler__
717 
718     def __dask_postcompute__(self):
719         import dask
720 
721         info = [
722             (True, k, v.__dask_postcompute__())
723             if dask.is_dask_collection(v)
724             else (False, k, v)
725             for k, v in self._variables.items()
726         ]
727         args = (
728             info,
729             self._coord_names,
730             self._dims,
731             self._attrs,
732             self._indexes,
733             self._encoding,
734             self._file_obj,
735         )
736         return self._dask_postcompute, args
737 
738     def __dask_postpersist__(self):
739         import dask
740 
741         info = [
742             (True, k, v.__dask_postpersist__())
743             if dask.is_dask_collection(v)
744             else (False, k, v)
745             for k, v in self._variables.items()
746         ]
747         args = (
748             info,
749             self._coord_names,
750             self._dims,
751             self._attrs,
752             self._indexes,
753             self._encoding,
754             self._file_obj,
755         )
756         return self._dask_postpersist, args
757 
758     @staticmethod
759     def _dask_postcompute(results, info, *args):
760         variables = {}
761         results2 = list(results[::-1])
762         for is_dask, k, v in info:
763             if is_dask:
764                 func, args2 = v
765                 r = results2.pop()
766                 result = func(r, *args2)
767             else:
768                 result = v
769             variables[k] = result
770 
771         final = Dataset._construct_direct(variables, *args)
772         return final
773 
774     @staticmethod
775     def _dask_postpersist(dsk, info, *args):
776         variables = {}
777         for is_dask, k, v in info:
778             if is_dask:
779                 func, args2 = v
780                 result = func(dsk, *args2)
781             else:
782                 result = v
783             variables[k] = result
784 
785         return Dataset._construct_direct(variables, *args)
786 
787     def compute(self, **kwargs) -> "Dataset":
788         """Manually trigger loading and/or computation of this dataset's data
789         from disk or a remote source into memory and return a new dataset.
790         Unlike load, the original dataset is left unaltered.
791 
792         Normally, it should not be necessary to call this method in user code,
793         because all xarray functions should either work on deferred data or
794         load data automatically. However, this method can be necessary when
795         working with many file objects on disk.
796 
797         Parameters
798         ----------
799         **kwargs : dict
800             Additional keyword arguments passed on to ``dask.array.compute``.
801 
802         See Also
803         --------
804         dask.array.compute
805         """
806         new = self.copy(deep=False)
807         return new.load(**kwargs)
808 
809     def _persist_inplace(self, **kwargs) -> "Dataset":
810         """Persist all Dask arrays in memory
811         """
812         # access .data to coerce everything to numpy or dask arrays
813         lazy_data = {
814             k: v._data
815             for k, v in self.variables.items()
816             if isinstance(v._data, dask_array_type)
817         }
818         if lazy_data:
819             import dask
820 
821             # evaluate all the dask arrays simultaneously
822             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)
823 
824             for k, data in zip(lazy_data, evaluated_data):
825                 self.variables[k].data = data
826 
827         return self
828 
829     def persist(self, **kwargs) -> "Dataset":
830         """ Trigger computation, keeping data as dask arrays
831 
832         This operation can be used to trigger computation on underlying dask
833         arrays, similar to ``.compute()`` or ``.load()``.  However this
834         operation keeps the data as dask arrays. This is particularly useful
835         when using the dask.distributed scheduler and you want to load a large
836         amount of data into distributed memory.
837 
838         Parameters
839         ----------
840         **kwargs : dict
841             Additional keyword arguments passed on to ``dask.persist``.
842 
843         See Also
844         --------
845         dask.persist
846         """
847         new = self.copy(deep=False)
848         return new._persist_inplace(**kwargs)
849 
850     @classmethod
851     def _construct_direct(
852         cls,
853         variables,
854         coord_names,
855         dims=None,
856         attrs=None,
857         indexes=None,
858         encoding=None,
859         file_obj=None,
860     ):
861         """Shortcut around __init__ for internal use when we want to skip
862         costly validation
863         """
864         if dims is None:
865             dims = calculate_dimensions(variables)
866         obj = object.__new__(cls)
867         obj._variables = variables
868         obj._coord_names = coord_names
869         obj._dims = dims
870         obj._indexes = indexes
871         obj._attrs = attrs
872         obj._file_obj = file_obj
873         obj._encoding = encoding
874         return obj
875 
876     def _replace(
877         self,
878         variables: Dict[Hashable, Variable] = None,
879         coord_names: Set[Hashable] = None,
880         dims: Dict[Any, int] = None,
881         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
882         indexes: Union[Dict[Any, pd.Index], None, Default] = _default,
883         encoding: Union[dict, None, Default] = _default,
884         inplace: bool = False,
885     ) -> "Dataset":
886         """Fastpath constructor for internal use.
887 
888         Returns an object with optionally with replaced attributes.
889 
890         Explicitly passed arguments are *not* copied when placed on the new
891         dataset. It is up to the caller to ensure that they have the right type
892         and are not used elsewhere.
893         """
894         if inplace:
895             if variables is not None:
896                 self._variables = variables
897             if coord_names is not None:
898                 self._coord_names = coord_names
899             if dims is not None:
900                 self._dims = dims
901             if attrs is not _default:
902                 self._attrs = attrs
903             if indexes is not _default:
904                 self._indexes = indexes
905             if encoding is not _default:
906                 self._encoding = encoding
907             obj = self
908         else:
909             if variables is None:
910                 variables = self._variables.copy()
911             if coord_names is None:
912                 coord_names = self._coord_names.copy()
913             if dims is None:
914                 dims = self._dims.copy()
915             if attrs is _default:
916                 attrs = copy.copy(self._attrs)
917             if indexes is _default:
918                 indexes = copy.copy(self._indexes)
919             if encoding is _default:
920                 encoding = copy.copy(self._encoding)
921             obj = self._construct_direct(
922                 variables, coord_names, dims, attrs, indexes, encoding
923             )
924         return obj
925 
926     def _replace_with_new_dims(
927         self,
928         variables: Dict[Hashable, Variable],
929         coord_names: set = None,
930         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
931         indexes: Union[Dict[Hashable, pd.Index], None, Default] = _default,
932         inplace: bool = False,
933     ) -> "Dataset":
934         """Replace variables with recalculated dimensions."""
935         dims = calculate_dimensions(variables)
936         return self._replace(
937             variables, coord_names, dims, attrs, indexes, inplace=inplace
938         )
939 
940     def _replace_vars_and_dims(
941         self,
942         variables: Dict[Hashable, Variable],
943         coord_names: set = None,
944         dims: Dict[Hashable, int] = None,
945         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
946         inplace: bool = False,
947     ) -> "Dataset":
948         """Deprecated version of _replace_with_new_dims().
949 
950         Unlike _replace_with_new_dims(), this method always recalculates
951         indexes from variables.
952         """
953         if dims is None:
954             dims = calculate_dimensions(variables)
955         return self._replace(
956             variables, coord_names, dims, attrs, indexes=None, inplace=inplace
957         )
958 
959     def _overwrite_indexes(self, indexes: Mapping[Any, pd.Index]) -> "Dataset":
960         if not indexes:
961             return self
962 
963         variables = self._variables.copy()
964         new_indexes = dict(self.indexes)
965         for name, idx in indexes.items():
966             variables[name] = IndexVariable(name, idx)
967             new_indexes[name] = idx
968         obj = self._replace(variables, indexes=new_indexes)
969 
970         # switch from dimension to level names, if necessary
971         dim_names: Dict[Hashable, str] = {}
972         for dim, idx in indexes.items():
973             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:
974                 dim_names[dim] = idx.name
975         if dim_names:
976             obj = obj.rename(dim_names)
977         return obj
978 
979     def copy(self, deep: bool = False, data: Mapping = None) -> "Dataset":
980         """Returns a copy of this dataset.
981 
982         If `deep=True`, a deep copy is made of each of the component variables.
983         Otherwise, a shallow copy of each of the component variable is made, so
984         that the underlying memory region of the new dataset is the same as in
985         the original dataset.
986 
987         Use `data` to create a new object with the same structure as
988         original but entirely new data.
989 
990         Parameters
991         ----------
992         deep : bool, optional
993             Whether each component variable is loaded into memory and copied onto
994             the new object. Default is False.
995         data : dict-like, optional
996             Data to use in the new object. Each item in `data` must have same
997             shape as corresponding data variable in original. When `data` is
998             used, `deep` is ignored for the data variables and only used for
999             coords.
1000 
1001         Returns
1002         -------
1003         object : Dataset
1004             New object with dimensions, attributes, coordinates, name, encoding,
1005             and optionally data copied from original.
1006 
1007         Examples
1008         --------
1009 
1010         Shallow copy versus deep copy
1011 
1012         >>> da = xr.DataArray(np.random.randn(2, 3))
1013         >>> ds = xr.Dataset(
1014         ...     {"foo": da, "bar": ("x", [-1, 2])}, coords={"x": ["one", "two"]},
1015         ... )
1016         >>> ds.copy()
1017         <xarray.Dataset>
1018         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1019         Coordinates:
1020         * x        (x) <U3 'one' 'two'
1021         Dimensions without coordinates: dim_0, dim_1
1022         Data variables:
1023             foo      (dim_0, dim_1) float64 -0.8079 0.3897 -1.862 -0.6091 -1.051 -0.3003
1024             bar      (x) int64 -1 2
1025 
1026         >>> ds_0 = ds.copy(deep=False)
1027         >>> ds_0["foo"][0, 0] = 7
1028         >>> ds_0
1029         <xarray.Dataset>
1030         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1031         Coordinates:
1032         * x        (x) <U3 'one' 'two'
1033         Dimensions without coordinates: dim_0, dim_1
1034         Data variables:
1035             foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003
1036             bar      (x) int64 -1 2
1037 
1038         >>> ds
1039         <xarray.Dataset>
1040         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1041         Coordinates:
1042         * x        (x) <U3 'one' 'two'
1043         Dimensions without coordinates: dim_0, dim_1
1044         Data variables:
1045             foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003
1046             bar      (x) int64 -1 2
1047 
1048         Changing the data using the ``data`` argument maintains the
1049         structure of the original object, but with the new data. Original
1050         object is unaffected.
1051 
1052         >>> ds.copy(
1053         ...     data={"foo": np.arange(6).reshape(2, 3), "bar": ["a", "b"]}
1054         ... )
1055         <xarray.Dataset>
1056         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1057         Coordinates:
1058         * x        (x) <U3 'one' 'two'
1059         Dimensions without coordinates: dim_0, dim_1
1060         Data variables:
1061             foo      (dim_0, dim_1) int64 0 1 2 3 4 5
1062             bar      (x) <U1 'a' 'b'
1063 
1064         >>> ds
1065         <xarray.Dataset>
1066         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1067         Coordinates:
1068         * x        (x) <U3 'one' 'two'
1069         Dimensions without coordinates: dim_0, dim_1
1070         Data variables:
1071             foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003
1072             bar      (x) int64 -1 2
1073 
1074         See Also
1075         --------
1076         pandas.DataFrame.copy
1077         """
1078         if data is None:
1079             variables = {k: v.copy(deep=deep) for k, v in self._variables.items()}
1080         elif not utils.is_dict_like(data):
1081             raise ValueError("Data must be dict-like")
1082         else:
1083             var_keys = set(self.data_vars.keys())
1084             data_keys = set(data.keys())
1085             keys_not_in_vars = data_keys - var_keys
1086             if keys_not_in_vars:
1087                 raise ValueError(
1088                     "Data must only contain variables in original "
1089                     "dataset. Extra variables: {}".format(keys_not_in_vars)
1090                 )
1091             keys_missing_from_data = var_keys - data_keys
1092             if keys_missing_from_data:
1093                 raise ValueError(
1094                     "Data must contain all variables in original "
1095                     "dataset. Data is missing {}".format(keys_missing_from_data)
1096                 )
1097             variables = {
1098                 k: v.copy(deep=deep, data=data.get(k))
1099                 for k, v in self._variables.items()
1100             }
1101 
1102         attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)
1103 
1104         return self._replace(variables, attrs=attrs)
1105 
1106     @property
1107     def _level_coords(self) -> Dict[str, Hashable]:
1108         """Return a mapping of all MultiIndex levels and their corresponding
1109         coordinate name.
1110         """
1111         level_coords: Dict[str, Hashable] = {}
1112         for name, index in self.indexes.items():
1113             if isinstance(index, pd.MultiIndex):
1114                 level_names = index.names
1115                 (dim,) = self.variables[name].dims
1116                 level_coords.update({lname: dim for lname in level_names})
1117         return level_coords
1118 
1119     def _copy_listed(self, names: Iterable[Hashable]) -> "Dataset":
1120         """Create a new Dataset with the listed variables from this dataset and
1121         the all relevant coordinates. Skips all validation.
1122         """
1123         variables: Dict[Hashable, Variable] = {}
1124         coord_names = set()
1125         indexes: Dict[Hashable, pd.Index] = {}
1126 
1127         for name in names:
1128             try:
1129                 variables[name] = self._variables[name]
1130             except KeyError:
1131                 ref_name, var_name, var = _get_virtual_variable(
1132                     self._variables, name, self._level_coords, self.dims
1133                 )
1134                 variables[var_name] = var
1135                 if ref_name in self._coord_names or ref_name in self.dims:
1136                     coord_names.add(var_name)
1137                 if (var_name,) == var.dims:
1138                     indexes[var_name] = var.to_index()
1139 
1140         needed_dims: Set[Hashable] = set()
1141         for v in variables.values():
1142             needed_dims.update(v.dims)
1143 
1144         dims = {k: self.dims[k] for k in needed_dims}
1145 
1146         for k in self._coord_names:
1147             if set(self.variables[k].dims) <= needed_dims:
1148                 variables[k] = self._variables[k]
1149                 coord_names.add(k)
1150                 if k in self.indexes:
1151                     indexes[k] = self.indexes[k]
1152 
1153         return self._replace(variables, coord_names, dims, indexes=indexes)
1154 
1155     def _construct_dataarray(self, name: Hashable) -> "DataArray":
1156         """Construct a DataArray by indexing this dataset
1157         """
1158         from .dataarray import DataArray
1159 
1160         try:
1161             variable = self._variables[name]
1162         except KeyError:
1163             _, name, variable = _get_virtual_variable(
1164                 self._variables, name, self._level_coords, self.dims
1165             )
1166 
1167         needed_dims = set(variable.dims)
1168 
1169         coords: Dict[Hashable, Variable] = {}
1170         for k in self.coords:
1171             if set(self.variables[k].dims) <= needed_dims:
1172                 coords[k] = self.variables[k]
1173 
1174         if self._indexes is None:
1175             indexes = None
1176         else:
1177             indexes = {k: v for k, v in self._indexes.items() if k in coords}
1178 
1179         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1180 
1181     def __copy__(self) -> "Dataset":
1182         return self.copy(deep=False)
1183 
1184     def __deepcopy__(self, memo=None) -> "Dataset":
1185         # memo does nothing but is required for compatibility with
1186         # copy.deepcopy
1187         return self.copy(deep=True)
1188 
1189     @property
1190     def _attr_sources(self) -> List[Mapping[Hashable, Any]]:
1191         """List of places to look-up items for attribute-style access
1192         """
1193         return self._item_sources + [self.attrs]
1194 
1195     @property
1196     def _item_sources(self) -> List[Mapping[Hashable, Any]]:
1197         """List of places to look-up items for key-completion
1198         """
1199         return [
1200             self.data_vars,
1201             self.coords,
1202             {d: self[d] for d in self.dims},
1203             LevelCoordinatesSource(self),
1204         ]
1205 
1206     def __contains__(self, key: object) -> bool:
1207         """The 'in' operator will return true or false depending on whether
1208         'key' is an array in the dataset or not.
1209         """
1210         return key in self._variables
1211 
1212     def __len__(self) -> int:
1213         return len(self.data_vars)
1214 
1215     def __bool__(self) -> bool:
1216         return bool(self.data_vars)
1217 
1218     def __iter__(self) -> Iterator[Hashable]:
1219         return iter(self.data_vars)
1220 
1221     def __array__(self, dtype=None):
1222         raise TypeError(
1223             "cannot directly convert an xarray.Dataset into a "
1224             "numpy array. Instead, create an xarray.DataArray "
1225             "first, either with indexing on the Dataset or by "
1226             "invoking the `to_array()` method."
1227         )
1228 
1229     @property
1230     def nbytes(self) -> int:
1231         return sum(v.nbytes for v in self.variables.values())
1232 
1233     @property
1234     def loc(self) -> _LocIndexer:
1235         """Attribute for location based indexing. Only supports __getitem__,
1236         and only when the key is a dict of the form {dim: labels}.
1237         """
1238         return _LocIndexer(self)
1239 
1240     def __getitem__(self, key: Any) -> "Union[DataArray, Dataset]":
1241         """Access variables or coordinates this dataset as a
1242         :py:class:`~xarray.DataArray`.
1243 
1244         Indexing with a list of names will return a new ``Dataset`` object.
1245         """
1246         # TODO(shoyer): type this properly: https://github.com/python/mypy/issues/7328
1247         if utils.is_dict_like(key):
1248             return self.isel(**cast(Mapping, key))
1249 
1250         if hashable(key):
1251             return self._construct_dataarray(key)
1252         else:
1253             return self._copy_listed(np.asarray(key))
1254 
1255     def __setitem__(self, key: Hashable, value) -> None:
1256         """Add an array to this dataset.
1257 
1258         If value is a `DataArray`, call its `select_vars()` method, rename it
1259         to `key` and merge the contents of the resulting dataset into this
1260         dataset.
1261 
1262         If value is an `Variable` object (or tuple of form
1263         ``(dims, data[, attrs])``), add it to this dataset as a new
1264         variable.
1265         """
1266         if utils.is_dict_like(key):
1267             raise NotImplementedError(
1268                 "cannot yet use a dictionary as a key " "to set Dataset values"
1269             )
1270 
1271         self.update({key: value})
1272 
1273     def __delitem__(self, key: Hashable) -> None:
1274         """Remove a variable from this dataset.
1275         """
1276         del self._variables[key]
1277         self._coord_names.discard(key)
1278         if key in self.indexes:
1279             assert self._indexes is not None
1280             del self._indexes[key]
1281         self._dims = calculate_dimensions(self._variables)
1282 
1283     # mutable objects should not be hashable
1284     # https://github.com/python/mypy/issues/4266
1285     __hash__ = None  # type: ignore
1286 
1287     def _all_compat(self, other: "Dataset", compat_str: str) -> bool:
1288         """Helper function for equals and identical
1289         """
1290 
1291         # some stores (e.g., scipy) do not seem to preserve order, so don't
1292         # require matching order for equality
1293         def compat(x: Variable, y: Variable) -> bool:
1294             return getattr(x, compat_str)(y)
1295 
1296         return self._coord_names == other._coord_names and utils.dict_equiv(
1297             self._variables, other._variables, compat=compat
1298         )
1299 
1300     def broadcast_equals(self, other: "Dataset") -> bool:
1301         """Two Datasets are broadcast equal if they are equal after
1302         broadcasting all variables against each other.
1303 
1304         For example, variables that are scalar in one dataset but non-scalar in
1305         the other dataset can still be broadcast equal if the the non-scalar
1306         variable is a constant.
1307 
1308         See Also
1309         --------
1310         Dataset.equals
1311         Dataset.identical
1312         """
1313         try:
1314             return self._all_compat(other, "broadcast_equals")
1315         except (TypeError, AttributeError):
1316             return False
1317 
1318     def equals(self, other: "Dataset") -> bool:
1319         """Two Datasets are equal if they have matching variables and
1320         coordinates, all of which are equal.
1321 
1322         Datasets can still be equal (like pandas objects) if they have NaN
1323         values in the same locations.
1324 
1325         This method is necessary because `v1 == v2` for ``Dataset``
1326         does element-wise comparisons (like numpy.ndarrays).
1327 
1328         See Also
1329         --------
1330         Dataset.broadcast_equals
1331         Dataset.identical
1332         """
1333         try:
1334             return self._all_compat(other, "equals")
1335         except (TypeError, AttributeError):
1336             return False
1337 
1338     def identical(self, other: "Dataset") -> bool:
1339         """Like equals, but also checks all dataset attributes and the
1340         attributes on all variables and coordinates.
1341 
1342         See Also
1343         --------
1344         Dataset.broadcast_equals
1345         Dataset.equals
1346         """
1347         try:
1348             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(
1349                 other, "identical"
1350             )
1351         except (TypeError, AttributeError):
1352             return False
1353 
1354     @property
1355     def indexes(self) -> Indexes:
1356         """Mapping of pandas.Index objects used for label based indexing
1357         """
1358         if self._indexes is None:
1359             self._indexes = default_indexes(self._variables, self._dims)
1360         return Indexes(self._indexes)
1361 
1362     @property
1363     def coords(self) -> DatasetCoordinates:
1364         """Dictionary of xarray.DataArray objects corresponding to coordinate
1365         variables
1366         """
1367         return DatasetCoordinates(self)
1368 
1369     @property
1370     def data_vars(self) -> DataVariables:
1371         """Dictionary of DataArray objects corresponding to data variables
1372         """
1373         return DataVariables(self)
1374 
1375     def set_coords(
1376         self, names: "Union[Hashable, Iterable[Hashable]]", inplace: bool = None
1377     ) -> "Dataset":
1378         """Given names of one or more variables, set them as coordinates
1379 
1380         Parameters
1381         ----------
1382         names : hashable or iterable of hashables
1383             Name(s) of variables in this dataset to convert into coordinates.
1384 
1385         Returns
1386         -------
1387         Dataset
1388 
1389         See also
1390         --------
1391         Dataset.swap_dims
1392         """
1393         # TODO: allow inserting new coordinates with this method, like
1394         # DataFrame.set_index?
1395         # nb. check in self._variables, not self.data_vars to insure that the
1396         # operation is idempotent
1397         _check_inplace(inplace)
1398         if isinstance(names, str) or not isinstance(names, Iterable):
1399             names = [names]
1400         else:
1401             names = list(names)
1402         self._assert_all_in_dataset(names)
1403         obj = self.copy()
1404         obj._coord_names.update(names)
1405         return obj
1406 
1407     def reset_coords(
1408         self,
1409         names: "Union[Hashable, Iterable[Hashable], None]" = None,
1410         drop: bool = False,
1411         inplace: bool = None,
1412     ) -> "Dataset":
1413         """Given names of coordinates, reset them to become variables
1414 
1415         Parameters
1416         ----------
1417         names : hashable or iterable of hashables, optional
1418             Name(s) of non-index coordinates in this dataset to reset into
1419             variables. By default, all non-index coordinates are reset.
1420         drop : bool, optional
1421             If True, remove coordinates instead of converting them into
1422             variables.
1423 
1424         Returns
1425         -------
1426         Dataset
1427         """
1428         _check_inplace(inplace)
1429         if names is None:
1430             names = self._coord_names - set(self.dims)
1431         else:
1432             if isinstance(names, str) or not isinstance(names, Iterable):
1433                 names = [names]
1434             else:
1435                 names = list(names)
1436             self._assert_all_in_dataset(names)
1437             bad_coords = set(names) & set(self.dims)
1438             if bad_coords:
1439                 raise ValueError(
1440                     "cannot remove index coordinates with reset_coords: %s" % bad_coords
1441                 )
1442         obj = self.copy()
1443         obj._coord_names.difference_update(names)
1444         if drop:
1445             for name in names:
1446                 del obj._variables[name]
1447         return obj
1448 
1449     def dump_to_store(self, store: "AbstractDataStore", **kwargs) -> None:
1450         """Store dataset contents to a backends.*DataStore object.
1451         """
1452         from ..backends.api import dump_to_store
1453 
1454         # TODO: rename and/or cleanup this method to make it more consistent
1455         # with to_netcdf()
1456         dump_to_store(self, store, **kwargs)
1457 
1458     def to_netcdf(
1459         self,
1460         path=None,
1461         mode: str = "w",
1462         format: str = None,
1463         group: str = None,
1464         engine: str = None,
1465         encoding: Mapping = None,
1466         unlimited_dims: Iterable[Hashable] = None,
1467         compute: bool = True,
1468         invalid_netcdf: bool = False,
1469     ) -> Union[bytes, "Delayed", None]:
1470         """Write dataset contents to a netCDF file.
1471 
1472         Parameters
1473         ----------
1474         path : str, Path or file-like object, optional
1475             Path to which to save this dataset. File-like objects are only
1476             supported by the scipy engine. If no path is provided, this
1477             function returns the resulting netCDF file as bytes; in this case,
1478             we need to use scipy, which does not support netCDF version 4 (the
1479             default format becomes NETCDF3_64BIT).
1480         mode : {'w', 'a'}, optional
1481             Write ('w') or append ('a') mode. If mode='w', any existing file at
1482             this location will be overwritten. If mode='a', existing variables
1483             will be overwritten.
1484         format : {'NETCDF4', 'NETCDF4_CLASSIC', 'NETCDF3_64BIT',
1485                   'NETCDF3_CLASSIC'}, optional
1486             File format for the resulting netCDF file:
1487 
1488             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
1489               features.
1490             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
1491               netCDF 3 compatible API features.
1492             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
1493               which fully supports 2+ GB files, but is only compatible with
1494               clients linked against netCDF version 3.6.0 or later.
1495             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
1496               handle 2+ GB files very well.
1497 
1498             All formats are supported by the netCDF4-python library.
1499             scipy.io.netcdf only supports the last two formats.
1500 
1501             The default format is NETCDF4 if you are saving a file to disk and
1502             have the netCDF4-python library available. Otherwise, xarray falls
1503             back to using scipy to write netCDF files and defaults to the
1504             NETCDF3_64BIT format (scipy does not support netCDF4).
1505         group : str, optional
1506             Path to the netCDF4 group in the given file to open (only works for
1507             format='NETCDF4'). The group(s) will be created if necessary.
1508         engine : {'netcdf4', 'scipy', 'h5netcdf'}, optional
1509             Engine to use when writing netCDF files. If not provided, the
1510             default engine is chosen based on available dependencies, with a
1511             preference for 'netcdf4' if writing to a file on disk.
1512         encoding : dict, optional
1513             Nested dictionary with variable names as keys and dictionaries of
1514             variable specific encodings as values, e.g.,
1515             ``{'my_variable': {'dtype': 'int16', 'scale_factor': 0.1,
1516             'zlib': True}, ...}``
1517 
1518             The `h5netcdf` engine supports both the NetCDF4-style compression
1519             encoding parameters ``{'zlib': True, 'complevel': 9}`` and the h5py
1520             ones ``{'compression': 'gzip', 'compression_opts': 9}``.
1521             This allows using any compression plugin installed in the HDF5
1522             library, e.g. LZF.
1523 
1524         unlimited_dims : iterable of hashable, optional
1525             Dimension(s) that should be serialized as unlimited dimensions.
1526             By default, no dimensions are treated as unlimited dimensions.
1527             Note that unlimited_dims may also be set via
1528             ``dataset.encoding['unlimited_dims']``.
1529         compute: boolean
1530             If true compute immediately, otherwise return a
1531             ``dask.delayed.Delayed`` object that can be computed later.
1532         invalid_netcdf: boolean
1533             Only valid along with engine='h5netcdf'. If True, allow writing
1534             hdf5 files which are valid netcdf as described in
1535             https://github.com/shoyer/h5netcdf. Default: False.
1536         """
1537         if encoding is None:
1538             encoding = {}
1539         from ..backends.api import to_netcdf
1540 
1541         return to_netcdf(
1542             self,
1543             path,
1544             mode,
1545             format=format,
1546             group=group,
1547             engine=engine,
1548             encoding=encoding,
1549             unlimited_dims=unlimited_dims,
1550             compute=compute,
1551             invalid_netcdf=invalid_netcdf,
1552         )
1553 
1554     def to_zarr(
1555         self,
1556         store: Union[MutableMapping, str, Path] = None,
1557         mode: str = None,
1558         synchronizer=None,
1559         group: str = None,
1560         encoding: Mapping = None,
1561         compute: bool = True,
1562         consolidated: bool = False,
1563         append_dim: Hashable = None,
1564     ) -> "ZarrStore":
1565         """Write dataset contents to a zarr group.
1566 
1567         .. note:: Experimental
1568                   The Zarr backend is new and experimental. Please report any
1569                   unexpected behavior via github issues.
1570 
1571         Parameters
1572         ----------
1573         store : MutableMapping, str or Path, optional
1574             Store or path to directory in file system.
1575         mode : {'w', 'w-', 'a', None}
1576             Persistence mode: 'w' means create (overwrite if exists);
1577             'w-' means create (fail if exists);
1578             'a' means append (create if does not exist).
1579             If ``append_dim`` is set, ``mode`` can be omitted as it is
1580             internally set to ``'a'``. Otherwise, ``mode`` will default to
1581             `w-` if not set.
1582         synchronizer : object, optional
1583             Array synchronizer
1584         group : str, optional
1585             Group path. (a.k.a. `path` in zarr terminology.)
1586         encoding : dict, optional
1587             Nested dictionary with variable names as keys and dictionaries of
1588             variable specific encodings as values, e.g.,
1589             ``{'my_variable': {'dtype': 'int16', 'scale_factor': 0.1,}, ...}``
1590         compute: bool, optional
1591             If True compute immediately, otherwise return a
1592             ``dask.delayed.Delayed`` object that can be computed later.
1593         consolidated: bool, optional
1594             If True, apply zarr's `consolidate_metadata` function to the store
1595             after writing.
1596         append_dim: hashable, optional
1597             If set, the dimension on which the data will be appended.
1598 
1599         References
1600         ----------
1601         https://zarr.readthedocs.io/
1602         """
1603         if encoding is None:
1604             encoding = {}
1605         if (mode == "a") or (append_dim is not None):
1606             if mode is None:
1607                 mode = "a"
1608             elif mode != "a":
1609                 raise ValueError(
1610                     "append_dim was set along with mode='{}', either set "
1611                     "mode='a' or don't set it.".format(mode)
1612                 )
1613         elif mode is None:
1614             mode = "w-"
1615         if mode not in ["w", "w-", "a"]:
1616             # TODO: figure out how to handle 'r+'
1617             raise ValueError(
1618                 "The only supported options for mode are 'w'," "'w-' and 'a'."
1619             )
1620         from ..backends.api import to_zarr
1621 
1622         return to_zarr(
1623             self,
1624             store=store,
1625             mode=mode,
1626             synchronizer=synchronizer,
1627             group=group,
1628             encoding=encoding,
1629             compute=compute,
1630             consolidated=consolidated,
1631             append_dim=append_dim,
1632         )
1633 
1634     def __repr__(self) -> str:
1635         return formatting.dataset_repr(self)
1636 
1637     def _repr_html_(self):
1638         if OPTIONS["display_style"] == "text":
1639             return f"<pre>{escape(repr(self))}</pre>"
1640         return formatting_html.dataset_repr(self)
1641 
1642     def info(self, buf=None) -> None:
1643         """
1644         Concise summary of a Dataset variables and attributes.
1645 
1646         Parameters
1647         ----------
1648         buf : writable buffer, defaults to sys.stdout
1649 
1650         See Also
1651         --------
1652         pandas.DataFrame.assign
1653         ncdump: netCDF's ncdump
1654         """
1655         if buf is None:  # pragma: no cover
1656             buf = sys.stdout
1657 
1658         lines = []
1659         lines.append("xarray.Dataset {")
1660         lines.append("dimensions:")
1661         for name, size in self.dims.items():
1662             lines.append(f"\t{name} = {size} ;")
1663         lines.append("\nvariables:")
1664         for name, da in self.variables.items():
1665             dims = ", ".join(da.dims)
1666             lines.append(f"\t{da.dtype} {name}({dims}) ;")
1667             for k, v in da.attrs.items():
1668                 lines.append(f"\t\t{name}:{k} = {v} ;")
1669         lines.append("\n// global attributes:")
1670         for k, v in self.attrs.items():
1671             lines.append(f"\t:{k} = {v} ;")
1672         lines.append("}")
1673 
1674         buf.write("\n".join(lines))
1675 
1676     @property
1677     def chunks(self) -> Mapping[Hashable, Tuple[int, ...]]:
1678         """Block dimensions for this dataset's data or None if it's not a dask
1679         array.
1680         """
1681         chunks: Dict[Hashable, Tuple[int, ...]] = {}
1682         for v in self.variables.values():
1683             if v.chunks is not None:
1684                 for dim, c in zip(v.dims, v.chunks):
1685                     if dim in chunks and c != chunks[dim]:
1686                         raise ValueError(
1687                             f"Object has inconsistent chunks along dimension {dim}. "
1688                             "This can be fixed by calling unify_chunks()."
1689                         )
1690                     chunks[dim] = c
1691         return Frozen(SortedKeysDict(chunks))
1692 
1693     def chunk(
1694         self,
1695         chunks: Union[
1696             None, Number, Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]]
1697         ] = None,
1698         name_prefix: str = "xarray-",
1699         token: str = None,
1700         lock: bool = False,
1701     ) -> "Dataset":
1702         """Coerce all arrays in this dataset into dask arrays with the given
1703         chunks.
1704 
1705         Non-dask arrays in this dataset will be converted to dask arrays. Dask
1706         arrays will be rechunked to the given chunk sizes.
1707 
1708         If neither chunks is not provided for one or more dimensions, chunk
1709         sizes along that dimension will not be updated; non-dask arrays will be
1710         converted into dask arrays with a single block.
1711 
1712         Parameters
1713         ----------
1714         chunks : int or mapping, optional
1715             Chunk sizes along each dimension, e.g., ``5`` or
1716             ``{'x': 5, 'y': 5}``.
1717         name_prefix : str, optional
1718             Prefix for the name of any new dask arrays.
1719         token : str, optional
1720             Token uniquely identifying this dataset.
1721         lock : optional
1722             Passed on to :py:func:`dask.array.from_array`, if the array is not
1723             already as dask array.
1724 
1725         Returns
1726         -------
1727         chunked : xarray.Dataset
1728         """
1729         from dask.base import tokenize
1730 
1731         if isinstance(chunks, Number):
1732             chunks = dict.fromkeys(self.dims, chunks)
1733 
1734         if chunks is not None:
1735             bad_dims = chunks.keys() - self.dims.keys()
1736             if bad_dims:
1737                 raise ValueError(
1738                     "some chunks keys are not dimensions on this "
1739                     "object: %s" % bad_dims
1740                 )
1741 
1742         def selkeys(dict_, keys):
1743             if dict_ is None:
1744                 return None
1745             return {d: dict_[d] for d in keys if d in dict_}
1746 
1747         def maybe_chunk(name, var, chunks):
1748             chunks = selkeys(chunks, var.dims)
1749             if not chunks:
1750                 chunks = None
1751             if var.ndim > 0:
1752                 # when rechunking by different amounts, make sure dask names change
1753                 # by provinding chunks as an input to tokenize.
1754                 # subtle bugs result otherwise. see GH3350
1755                 token2 = tokenize(name, token if token else var._data, chunks)
1756                 name2 = f"{name_prefix}{name}-{token2}"
1757                 return var.chunk(chunks, name=name2, lock=lock)
1758             else:
1759                 return var
1760 
1761         variables = {k: maybe_chunk(k, v, chunks) for k, v in self.variables.items()}
1762         return self._replace(variables)
1763 
1764     def _validate_indexers(
1765         self, indexers: Mapping[Hashable, Any]
1766     ) -> Iterator[Tuple[Hashable, Union[int, slice, np.ndarray, Variable]]]:
1767         """ Here we make sure
1768         + indexer has a valid keys
1769         + indexer is in a valid data type
1770         + string indexers are cast to the appropriate date type if the
1771           associated index is a DatetimeIndex or CFTimeIndex
1772         """
1773         from .dataarray import DataArray
1774 
1775         invalid = indexers.keys() - self.dims.keys()
1776         if invalid:
1777             raise ValueError("dimensions %r do not exist" % invalid)
1778 
1779         # all indexers should be int, slice, np.ndarrays, or Variable
1780         for k, v in indexers.items():
1781             if isinstance(v, (int, slice, Variable)):
1782                 yield k, v
1783             elif isinstance(v, DataArray):
1784                 yield k, v.variable
1785             elif isinstance(v, tuple):
1786                 yield k, as_variable(v)
1787             elif isinstance(v, Dataset):
1788                 raise TypeError("cannot use a Dataset as an indexer")
1789             elif isinstance(v, Sequence) and len(v) == 0:
1790                 yield k, np.empty((0,), dtype="int64")
1791             else:
1792                 v = np.asarray(v)
1793 
1794                 if v.dtype.kind in "US":
1795                     index = self.indexes[k]
1796                     if isinstance(index, pd.DatetimeIndex):
1797                         v = v.astype("datetime64[ns]")
1798                     elif isinstance(index, xr.CFTimeIndex):
1799                         v = _parse_array_of_cftime_strings(v, index.date_type)
1800 
1801                 if v.ndim > 1:
1802                     raise IndexError(
1803                         "Unlabeled multi-dimensional array cannot be "
1804                         "used for indexing: {}".format(k)
1805                     )
1806                 yield k, v
1807 
1808     def _validate_interp_indexers(
1809         self, indexers: Mapping[Hashable, Any]
1810     ) -> Iterator[Tuple[Hashable, Variable]]:
1811         """Variant of _validate_indexers to be used for interpolation
1812         """
1813         for k, v in self._validate_indexers(indexers):
1814             if isinstance(v, Variable):
1815                 if v.ndim == 1:
1816                     yield k, v.to_index_variable()
1817                 else:
1818                     yield k, v
1819             elif isinstance(v, int):
1820                 yield k, Variable((), v)
1821             elif isinstance(v, np.ndarray):
1822                 if v.ndim == 0:
1823                     yield k, Variable((), v)
1824                 elif v.ndim == 1:
1825                     yield k, IndexVariable((k,), v)
1826                 else:
1827                     raise AssertionError()  # Already tested by _validate_indexers
1828             else:
1829                 raise TypeError(type(v))
1830 
1831     def _get_indexers_coords_and_indexes(self, indexers):
1832         """Extract coordinates and indexes from indexers.
1833 
1834         Only coordinate with a name different from any of self.variables will
1835         be attached.
1836         """
1837         from .dataarray import DataArray
1838 
1839         coords_list = []
1840         for k, v in indexers.items():
1841             if isinstance(v, DataArray):
1842                 if v.dtype.kind == "b":
1843                     if v.ndim != 1:  # we only support 1-d boolean array
1844                         raise ValueError(
1845                             "{:d}d-boolean array is used for indexing along "
1846                             "dimension {!r}, but only 1d boolean arrays are "
1847                             "supported.".format(v.ndim, k)
1848                         )
1849                     # Make sure in case of boolean DataArray, its
1850                     # coordinate also should be indexed.
1851                     v_coords = v[v.values.nonzero()[0]].coords
1852                 else:
1853                     v_coords = v.coords
1854                 coords_list.append(v_coords)
1855 
1856         # we don't need to call align() explicitly or check indexes for
1857         # alignment, because merge_variables already checks for exact alignment
1858         # between dimension coordinates
1859         coords, indexes = merge_coordinates_without_align(coords_list)
1860         assert_coordinate_consistent(self, coords)
1861 
1862         # silently drop the conflicted variables.
1863         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}
1864         attached_indexes = {
1865             k: v for k, v in indexes.items() if k not in self._variables
1866         }
1867         return attached_coords, attached_indexes
1868 
1869     def isel(
1870         self,
1871         indexers: Mapping[Hashable, Any] = None,
1872         drop: bool = False,
1873         **indexers_kwargs: Any,
1874     ) -> "Dataset":
1875         """Returns a new dataset with each array indexed along the specified
1876         dimension(s).
1877 
1878         This method selects values from each array using its `__getitem__`
1879         method, except this method does not require knowing the order of
1880         each array's dimensions.
1881 
1882         Parameters
1883         ----------
1884         indexers : dict, optional
1885             A dict with keys matching dimensions and values given
1886             by integers, slice objects or arrays.
1887             indexer can be a integer, slice, array-like or DataArray.
1888             If DataArrays are passed as indexers, xarray-style indexing will be
1889             carried out. See :ref:`indexing` for the details.
1890             One of indexers or indexers_kwargs must be provided.
1891         drop : bool, optional
1892             If ``drop=True``, drop coordinates variables indexed by integers
1893             instead of making them scalar.
1894         **indexers_kwargs : {dim: indexer, ...}, optional
1895             The keyword arguments form of ``indexers``.
1896             One of indexers or indexers_kwargs must be provided.
1897 
1898         Returns
1899         -------
1900         obj : Dataset
1901             A new Dataset with the same contents as this dataset, except each
1902             array and dimension is indexed by the appropriate indexers.
1903             If indexer DataArrays have coordinates that do not conflict with
1904             this object, then these coordinates will be attached.
1905             In general, each array's data will be a view of the array's data
1906             in this dataset, unless vectorized indexing was triggered by using
1907             an array indexer, in which case the data will be a copy.
1908 
1909         See Also
1910         --------
1911         Dataset.sel
1912         DataArray.isel
1913         """
1914         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1915         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1916             return self._isel_fancy(indexers, drop=drop)
1917 
1918         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1919         # lists, or zero or one-dimensional np.ndarray's
1920         invalid = indexers.keys() - self.dims.keys()
1921         if invalid:
1922             raise ValueError("dimensions %r do not exist" % invalid)
1923 
1924         variables = {}
1925         dims: Dict[Hashable, Tuple[int, ...]] = {}
1926         coord_names = self._coord_names.copy()
1927         indexes = self._indexes.copy() if self._indexes is not None else None
1928 
1929         for var_name, var_value in self._variables.items():
1930             var_indexers = {k: v for k, v in indexers.items() if k in var_value.dims}
1931             if var_indexers:
1932                 var_value = var_value.isel(var_indexers)
1933                 if drop and var_value.ndim == 0 and var_name in coord_names:
1934                     coord_names.remove(var_name)
1935                     if indexes:
1936                         indexes.pop(var_name, None)
1937                     continue
1938                 if indexes and var_name in indexes:
1939                     if var_value.ndim == 1:
1940                         indexes[var_name] = var_value.to_index()
1941                     else:
1942                         del indexes[var_name]
1943             variables[var_name] = var_value
1944             dims.update(zip(var_value.dims, var_value.shape))
1945 
1946         return self._construct_direct(
1947             variables=variables,
1948             coord_names=coord_names,
1949             dims=dims,
1950             attrs=self._attrs,
1951             indexes=indexes,
1952             encoding=self._encoding,
1953             file_obj=self._file_obj,
1954         )
1955 
1956     def _isel_fancy(self, indexers: Mapping[Hashable, Any], *, drop: bool) -> "Dataset":
1957         # Note: we need to preserve the original indexers variable in order to merge the
1958         # coords below
1959         indexers_list = list(self._validate_indexers(indexers))
1960 
1961         variables: Dict[Hashable, Variable] = {}
1962         indexes: Dict[Hashable, pd.Index] = {}
1963 
1964         for name, var in self.variables.items():
1965             var_indexers = {k: v for k, v in indexers_list if k in var.dims}
1966             if drop and name in var_indexers:
1967                 continue  # drop this variable
1968 
1969             if name in self.indexes:
1970                 new_var, new_index = isel_variable_and_index(
1971                     name, var, self.indexes[name], var_indexers
1972                 )
1973                 if new_index is not None:
1974                     indexes[name] = new_index
1975             elif var_indexers:
1976                 new_var = var.isel(indexers=var_indexers)
1977             else:
1978                 new_var = var.copy(deep=False)
1979 
1980             variables[name] = new_var
1981 
1982         coord_names = self._coord_names & variables.keys()
1983         selected = self._replace_with_new_dims(variables, coord_names, indexes)
1984 
1985         # Extract coordinates from indexers
1986         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)
1987         variables.update(coord_vars)
1988         indexes.update(new_indexes)
1989         coord_names = self._coord_names & variables.keys() | coord_vars.keys()
1990         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
1991 
1992     def sel(
1993         self,
1994         indexers: Mapping[Hashable, Any] = None,
1995         method: str = None,
1996         tolerance: Number = None,
1997         drop: bool = False,
1998         **indexers_kwargs: Any,
1999     ) -> "Dataset":
2000         """Returns a new dataset with each array indexed by tick labels
2001         along the specified dimension(s).
2002 
2003         In contrast to `Dataset.isel`, indexers for this method should use
2004         labels instead of integers.
2005 
2006         Under the hood, this method is powered by using pandas's powerful Index
2007         objects. This makes label based indexing essentially just as fast as
2008         using integer indexing.
2009 
2010         It also means this method uses pandas's (well documented) logic for
2011         indexing. This means you can use string shortcuts for datetime indexes
2012         (e.g., '2000-01' to select all values in January 2000). It also means
2013         that slices are treated as inclusive of both the start and stop values,
2014         unlike normal Python indexing.
2015 
2016         Parameters
2017         ----------
2018         indexers : dict, optional
2019             A dict with keys matching dimensions and values given
2020             by scalars, slices or arrays of tick labels. For dimensions with
2021             multi-index, the indexer may also be a dict-like object with keys
2022             matching index level names.
2023             If DataArrays are passed as indexers, xarray-style indexing will be
2024             carried out. See :ref:`indexing` for the details.
2025             One of indexers or indexers_kwargs must be provided.
2026         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
2027             Method to use for inexact matches:
2028 
2029             * None (default): only exact matches
2030             * pad / ffill: propagate last valid index value forward
2031             * backfill / bfill: propagate next valid index value backward
2032             * nearest: use nearest valid index value
2033         tolerance : optional
2034             Maximum distance between original and new labels for inexact
2035             matches. The values of the index at the matching locations must
2036             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2037         drop : bool, optional
2038             If ``drop=True``, drop coordinates variables in `indexers` instead
2039             of making them scalar.
2040         **indexers_kwargs : {dim: indexer, ...}, optional
2041             The keyword arguments form of ``indexers``.
2042             One of indexers or indexers_kwargs must be provided.
2043 
2044         Returns
2045         -------
2046         obj : Dataset
2047             A new Dataset with the same contents as this dataset, except each
2048             variable and dimension is indexed by the appropriate indexers.
2049             If indexer DataArrays have coordinates that do not conflict with
2050             this object, then these coordinates will be attached.
2051             In general, each array's data will be a view of the array's data
2052             in this dataset, unless vectorized indexing was triggered by using
2053             an array indexer, in which case the data will be a copy.
2054 
2055 
2056         See Also
2057         --------
2058         Dataset.isel
2059         DataArray.sel
2060         """
2061         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "sel")
2062         pos_indexers, new_indexes = remap_label_indexers(
2063             self, indexers=indexers, method=method, tolerance=tolerance
2064         )
2065         result = self.isel(indexers=pos_indexers, drop=drop)
2066         return result._overwrite_indexes(new_indexes)
2067 
2068     def head(
2069         self,
2070         indexers: Union[Mapping[Hashable, int], int] = None,
2071         **indexers_kwargs: Any,
2072     ) -> "Dataset":
2073         """Returns a new dataset with the first `n` values of each array
2074         for the specified dimension(s).
2075 
2076         Parameters
2077         ----------
2078         indexers : dict or int, default: 5
2079             A dict with keys matching dimensions and integer values `n`
2080             or a single integer `n` applied over all dimensions.
2081             One of indexers or indexers_kwargs must be provided.
2082         **indexers_kwargs : {dim: n, ...}, optional
2083             The keyword arguments form of ``indexers``.
2084             One of indexers or indexers_kwargs must be provided.
2085 
2086 
2087         See Also
2088         --------
2089         Dataset.tail
2090         Dataset.thin
2091         DataArray.head
2092         """
2093         if not indexers_kwargs:
2094             if indexers is None:
2095                 indexers = 5
2096             if not isinstance(indexers, int) and not is_dict_like(indexers):
2097                 raise TypeError("indexers must be either dict-like or a single integer")
2098         if isinstance(indexers, int):
2099             indexers = {dim: indexers for dim in self.dims}
2100         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "head")
2101         for k, v in indexers.items():
2102             if not isinstance(v, int):
2103                 raise TypeError(
2104                     "expected integer type indexer for "
2105                     "dimension %r, found %r" % (k, type(v))
2106                 )
2107             elif v < 0:
2108                 raise ValueError(
2109                     "expected positive integer as indexer "
2110                     "for dimension %r, found %s" % (k, v)
2111                 )
2112         indexers_slices = {k: slice(val) for k, val in indexers.items()}
2113         return self.isel(indexers_slices)
2114 
2115     def tail(
2116         self,
2117         indexers: Union[Mapping[Hashable, int], int] = None,
2118         **indexers_kwargs: Any,
2119     ) -> "Dataset":
2120         """Returns a new dataset with the last `n` values of each array
2121         for the specified dimension(s).
2122 
2123         Parameters
2124         ----------
2125         indexers : dict or int, default: 5
2126             A dict with keys matching dimensions and integer values `n`
2127             or a single integer `n` applied over all dimensions.
2128             One of indexers or indexers_kwargs must be provided.
2129         **indexers_kwargs : {dim: n, ...}, optional
2130             The keyword arguments form of ``indexers``.
2131             One of indexers or indexers_kwargs must be provided.
2132 
2133 
2134         See Also
2135         --------
2136         Dataset.head
2137         Dataset.thin
2138         DataArray.tail
2139         """
2140         if not indexers_kwargs:
2141             if indexers is None:
2142                 indexers = 5
2143             if not isinstance(indexers, int) and not is_dict_like(indexers):
2144                 raise TypeError("indexers must be either dict-like or a single integer")
2145         if isinstance(indexers, int):
2146             indexers = {dim: indexers for dim in self.dims}
2147         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "tail")
2148         for k, v in indexers.items():
2149             if not isinstance(v, int):
2150                 raise TypeError(
2151                     "expected integer type indexer for "
2152                     "dimension %r, found %r" % (k, type(v))
2153                 )
2154             elif v < 0:
2155                 raise ValueError(
2156                     "expected positive integer as indexer "
2157                     "for dimension %r, found %s" % (k, v)
2158                 )
2159         indexers_slices = {
2160             k: slice(-val, None) if val != 0 else slice(val)
2161             for k, val in indexers.items()
2162         }
2163         return self.isel(indexers_slices)
2164 
2165     def thin(
2166         self,
2167         indexers: Union[Mapping[Hashable, int], int] = None,
2168         **indexers_kwargs: Any,
2169     ) -> "Dataset":
2170         """Returns a new dataset with each array indexed along every `n`-th
2171         value for the specified dimension(s)
2172 
2173         Parameters
2174         ----------
2175         indexers : dict or int
2176             A dict with keys matching dimensions and integer values `n`
2177             or a single integer `n` applied over all dimensions.
2178             One of indexers or indexers_kwargs must be provided.
2179         ``**indexers_kwargs`` : {dim: n, ...}, optional
2180             The keyword arguments form of ``indexers``.
2181             One of indexers or indexers_kwargs must be provided.
2182 
2183 
2184         See Also
2185         --------
2186         Dataset.head
2187         Dataset.tail
2188         DataArray.thin
2189         """
2190         if (
2191             not indexers_kwargs
2192             and not isinstance(indexers, int)
2193             and not is_dict_like(indexers)
2194         ):
2195             raise TypeError("indexers must be either dict-like or a single integer")
2196         if isinstance(indexers, int):
2197             indexers = {dim: indexers for dim in self.dims}
2198         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "thin")
2199         for k, v in indexers.items():
2200             if not isinstance(v, int):
2201                 raise TypeError(
2202                     "expected integer type indexer for "
2203                     "dimension %r, found %r" % (k, type(v))
2204                 )
2205             elif v < 0:
2206                 raise ValueError(
2207                     "expected positive integer as indexer "
2208                     "for dimension %r, found %s" % (k, v)
2209                 )
2210             elif v == 0:
2211                 raise ValueError("step cannot be zero")
2212         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}
2213         return self.isel(indexers_slices)
2214 
2215     def broadcast_like(
2216         self, other: Union["Dataset", "DataArray"], exclude: Iterable[Hashable] = None
2217     ) -> "Dataset":
2218         """Broadcast this DataArray against another Dataset or DataArray.
2219         This is equivalent to xr.broadcast(other, self)[1]
2220 
2221         Parameters
2222         ----------
2223         other : Dataset or DataArray
2224             Object against which to broadcast this array.
2225         exclude : iterable of hashable, optional
2226             Dimensions that must not be broadcasted
2227 
2228         """
2229         if exclude is None:
2230             exclude = set()
2231         else:
2232             exclude = set(exclude)
2233         args = align(other, self, join="outer", copy=False, exclude=exclude)
2234 
2235         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
2236 
2237         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
2238 
2239     def reindex_like(
2240         self,
2241         other: Union["Dataset", "DataArray"],
2242         method: str = None,
2243         tolerance: Number = None,
2244         copy: bool = True,
2245         fill_value: Any = dtypes.NA,
2246     ) -> "Dataset":
2247         """Conform this object onto the indexes of another object, filling in
2248         missing values with ``fill_value``. The default fill value is NaN.
2249 
2250         Parameters
2251         ----------
2252         other : Dataset or DataArray
2253             Object with an 'indexes' attribute giving a mapping from dimension
2254             names to pandas.Index objects, which provides coordinates upon
2255             which to index the variables in this dataset. The indexes on this
2256             other object need not be the same as the indexes on this
2257             dataset. Any mis-matched index values will be filled in with
2258             NaN, and any mis-matched dimension names will simply be ignored.
2259         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
2260             Method to use for filling index values from other not found in this
2261             dataset:
2262 
2263             * None (default): don't fill gaps
2264             * pad / ffill: propagate last valid index value forward
2265             * backfill / bfill: propagate next valid index value backward
2266             * nearest: use nearest valid index value
2267         tolerance : optional
2268             Maximum distance between original and new labels for inexact
2269             matches. The values of the index at the matching locations must
2270             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2271         copy : bool, optional
2272             If ``copy=True``, data in the return value is always copied. If
2273             ``copy=False`` and reindexing is unnecessary, or can be performed
2274             with only slice operations, then the output may share memory with
2275             the input. In either case, a new xarray object is always returned.
2276         fill_value : scalar, optional
2277             Value to use for newly missing values
2278 
2279         Returns
2280         -------
2281         reindexed : Dataset
2282             Another dataset, with this dataset's data but coordinates from the
2283             other object.
2284 
2285         See Also
2286         --------
2287         Dataset.reindex
2288         align
2289         """
2290         indexers = alignment.reindex_like_indexers(self, other)
2291         return self.reindex(
2292             indexers=indexers,
2293             method=method,
2294             copy=copy,
2295             fill_value=fill_value,
2296             tolerance=tolerance,
2297         )
2298 
2299     def reindex(
2300         self,
2301         indexers: Mapping[Hashable, Any] = None,
2302         method: str = None,
2303         tolerance: Number = None,
2304         copy: bool = True,
2305         fill_value: Any = dtypes.NA,
2306         **indexers_kwargs: Any,
2307     ) -> "Dataset":
2308         """Conform this object onto a new set of indexes, filling in
2309         missing values with ``fill_value``. The default fill value is NaN.
2310 
2311         Parameters
2312         ----------
2313         indexers : dict. optional
2314             Dictionary with keys given by dimension names and values given by
2315             arrays of coordinates tick labels. Any mis-matched coordinate
2316             values will be filled in with NaN, and any mis-matched dimension
2317             names will simply be ignored.
2318             One of indexers or indexers_kwargs must be provided.
2319         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
2320             Method to use for filling index values in ``indexers`` not found in
2321             this dataset:
2322 
2323             * None (default): don't fill gaps
2324             * pad / ffill: propagate last valid index value forward
2325             * backfill / bfill: propagate next valid index value backward
2326             * nearest: use nearest valid index value
2327         tolerance : optional
2328             Maximum distance between original and new labels for inexact
2329             matches. The values of the index at the matching locations must
2330             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2331         copy : bool, optional
2332             If ``copy=True``, data in the return value is always copied. If
2333             ``copy=False`` and reindexing is unnecessary, or can be performed
2334             with only slice operations, then the output may share memory with
2335             the input. In either case, a new xarray object is always returned.
2336         fill_value : scalar, optional
2337             Value to use for newly missing values
2338         sparse: use sparse-array. By default, False
2339         **indexers_kwargs : {dim: indexer, ...}, optional
2340             Keyword arguments in the same form as ``indexers``.
2341             One of indexers or indexers_kwargs must be provided.
2342 
2343         Returns
2344         -------
2345         reindexed : Dataset
2346             Another dataset, with this dataset's data but replaced coordinates.
2347 
2348         See Also
2349         --------
2350         Dataset.reindex_like
2351         align
2352         pandas.Index.get_indexer
2353 
2354         Examples
2355         --------
2356 
2357         Create a dataset with some fictional data.
2358 
2359         >>> import xarray as xr
2360         >>> import pandas as pd
2361         >>> x = xr.Dataset(
2362         ...     {
2363         ...         "temperature": ("station", 20 * np.random.rand(4)),
2364         ...         "pressure": ("station", 500 * np.random.rand(4)),
2365         ...     },
2366         ...     coords={"station": ["boston", "nyc", "seattle", "denver"]},
2367         ... )
2368         >>> x
2369         <xarray.Dataset>
2370         Dimensions:      (station: 4)
2371         Coordinates:
2372         * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'
2373         Data variables:
2374             temperature  (station) float64 18.84 14.59 19.22 17.16
2375             pressure     (station) float64 324.1 194.3 122.8 244.3
2376         >>> x.indexes
2377         station: Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')
2378 
2379         Create a new index and reindex the dataset. By default values in the new index that
2380         do not have corresponding records in the dataset are assigned `NaN`.
2381 
2382         >>> new_index = ["boston", "austin", "seattle", "lincoln"]
2383         >>> x.reindex({"station": new_index})
2384         <xarray.Dataset>
2385         Dimensions:      (station: 4)
2386         Coordinates:
2387         * station      (station) object 'boston' 'austin' 'seattle' 'lincoln'
2388         Data variables:
2389             temperature  (station) float64 18.84 nan 19.22 nan
2390             pressure     (station) float64 324.1 nan 122.8 nan
2391 
2392         We can fill in the missing values by passing a value to the keyword `fill_value`.
2393 
2394         >>> x.reindex({"station": new_index}, fill_value=0)
2395         <xarray.Dataset>
2396         Dimensions:      (station: 4)
2397         Coordinates:
2398         * station      (station) object 'boston' 'austin' 'seattle' 'lincoln'
2399         Data variables:
2400             temperature  (station) float64 18.84 0.0 19.22 0.0
2401             pressure     (station) float64 324.1 0.0 122.8 0.0
2402 
2403         Because the index is not monotonically increasing or decreasing, we cannot use arguments
2404         to the keyword method to fill the `NaN` values.
2405 
2406         >>> x.reindex({"station": new_index}, method="nearest")
2407         Traceback (most recent call last):
2408         ...
2409             raise ValueError('index must be monotonic increasing or decreasing')
2410         ValueError: index must be monotonic increasing or decreasing
2411 
2412         To further illustrate the filling functionality in reindex, we will create a
2413         dataset with a monotonically increasing index (for example, a sequence of dates).
2414 
2415         >>> x2 = xr.Dataset(
2416         ...     {
2417         ...         "temperature": (
2418         ...             "time",
2419         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],
2420         ...         ),
2421         ...         "pressure": ("time", 500 * np.random.rand(6)),
2422         ...     },
2423         ...     coords={"time": pd.date_range("01/01/2019", periods=6, freq="D")},
2424         ... )
2425         >>> x2
2426         <xarray.Dataset>
2427         Dimensions:      (time: 6)
2428         Coordinates:
2429         * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06
2430         Data variables:
2431             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12
2432             pressure     (time) float64 103.4 122.7 452.0 444.0 399.2 486.0
2433 
2434         Suppose we decide to expand the dataset to cover a wider date range.
2435 
2436         >>> time_index2 = pd.date_range("12/29/2018", periods=10, freq="D")
2437         >>> x2.reindex({"time": time_index2})
2438         <xarray.Dataset>
2439         Dimensions:      (time: 10)
2440         Coordinates:
2441         * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
2442         Data variables:
2443             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan
2444             pressure     (time) float64 nan nan nan 103.4 ... 444.0 399.2 486.0 nan
2445 
2446         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)
2447         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.
2448 
2449         For example, to back-propagate the last valid value to fill the `NaN` values,
2450         pass `bfill` as an argument to the `method` keyword.
2451 
2452         >>> x3 = x2.reindex({"time": time_index2}, method="bfill")
2453         >>> x3
2454         <xarray.Dataset>
2455         Dimensions:      (time: 10)
2456         Coordinates:
2457         * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
2458         Data variables:
2459             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan
2460             pressure     (time) float64 103.4 103.4 103.4 103.4 ... 399.2 486.0 nan
2461 
2462         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)
2463         will not be filled by any of the value propagation schemes.
2464 
2465         >>> x2.where(x2.temperature.isnull(), drop=True)
2466         <xarray.Dataset>
2467         Dimensions:      (time: 1)
2468         Coordinates:
2469         * time         (time) datetime64[ns] 2019-01-03
2470         Data variables:
2471             temperature  (time) float64 nan
2472             pressure     (time) float64 452.0
2473         >>> x3.where(x3.temperature.isnull(), drop=True)
2474         <xarray.Dataset>
2475         Dimensions:      (time: 2)
2476         Coordinates:
2477         * time         (time) datetime64[ns] 2019-01-03 2019-01-07
2478         Data variables:
2479             temperature  (time) float64 nan nan
2480             pressure     (time) float64 452.0 nan
2481 
2482         This is because filling while reindexing does not look at dataset values, but only compares
2483         the original and desired indexes. If you do want to fill in the `NaN` values present in the
2484         original dataset, use the :py:meth:`~Dataset.fillna()` method.
2485 
2486         """
2487         return self._reindex(
2488             indexers,
2489             method,
2490             tolerance,
2491             copy,
2492             fill_value,
2493             sparse=False,
2494             **indexers_kwargs,
2495         )
2496 
2497     def _reindex(
2498         self,
2499         indexers: Mapping[Hashable, Any] = None,
2500         method: str = None,
2501         tolerance: Number = None,
2502         copy: bool = True,
2503         fill_value: Any = dtypes.NA,
2504         sparse: bool = False,
2505         **indexers_kwargs: Any,
2506     ) -> "Dataset":
2507         """
2508         same to _reindex but support sparse option
2509         """
2510         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
2511 
2512         bad_dims = [d for d in indexers if d not in self.dims]
2513         if bad_dims:
2514             raise ValueError("invalid reindex dimensions: %s" % bad_dims)
2515 
2516         variables, indexes = alignment.reindex_variables(
2517             self.variables,
2518             self.sizes,
2519             self.indexes,
2520             indexers,
2521             method,
2522             tolerance,
2523             copy=copy,
2524             fill_value=fill_value,
2525             sparse=sparse,
2526         )
2527         coord_names = set(self._coord_names)
2528         coord_names.update(indexers)
2529         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2530 
2531     def interp(
2532         self,
2533         coords: Mapping[Hashable, Any] = None,
2534         method: str = "linear",
2535         assume_sorted: bool = False,
2536         kwargs: Mapping[str, Any] = None,
2537         **coords_kwargs: Any,
2538     ) -> "Dataset":
2539         """ Multidimensional interpolation of Dataset.
2540 
2541         Parameters
2542         ----------
2543         coords : dict, optional
2544             Mapping from dimension names to the new coordinates.
2545             New coordinate can be a scalar, array-like or DataArray.
2546             If DataArrays are passed as new coordates, their dimensions are
2547             used for the broadcasting.
2548         method: string, optional.
2549             {'linear', 'nearest'} for multidimensional array,
2550             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
2551             for 1-dimensional array. 'linear' is used by default.
2552         assume_sorted: boolean, optional
2553             If False, values of coordinates that are interpolated over can be
2554             in any order and they are sorted first. If True, interpolated
2555             coordinates are assumed to be an array of monotonically increasing
2556             values.
2557         kwargs: dictionary, optional
2558             Additional keyword passed to scipy's interpolator.
2559         **coords_kwargs : {dim: coordinate, ...}, optional
2560             The keyword arguments form of ``coords``.
2561             One of coords or coords_kwargs must be provided.
2562 
2563         Returns
2564         -------
2565         interpolated: xr.Dataset
2566             New dataset on the new coordinates.
2567 
2568         Notes
2569         -----
2570         scipy is required.
2571 
2572         See Also
2573         --------
2574         scipy.interpolate.interp1d
2575         scipy.interpolate.interpn
2576         """
2577         from . import missing
2578 
2579         if kwargs is None:
2580             kwargs = {}
2581 
2582         coords = either_dict_or_kwargs(coords, coords_kwargs, "interp")
2583         indexers = dict(self._validate_interp_indexers(coords))
2584 
2585         if coords:
2586             # This avoids broadcasting over coordinates that are both in
2587             # the original array AND in the indexing array. It essentially
2588             # forces interpolation along the shared coordinates.
2589             sdims = (
2590                 set(self.dims)
2591                 .intersection(*[set(nx.dims) for nx in indexers.values()])
2592                 .difference(coords.keys())
2593             )
2594             indexers.update({d: self.variables[d] for d in sdims})
2595 
2596         obj = self if assume_sorted else self.sortby([k for k in coords])
2597 
2598         def maybe_variable(obj, k):
2599             # workaround to get variable for dimension without coordinate.
2600             try:
2601                 return obj._variables[k]
2602             except KeyError:
2603                 return as_variable((k, range(obj.dims[k])))
2604 
2605         def _validate_interp_indexer(x, new_x):
2606             # In the case of datetimes, the restrictions placed on indexers
2607             # used with interp are stronger than those which are placed on
2608             # isel, so we need an additional check after _validate_indexers.
2609             if _contains_datetime_like_objects(
2610                 x
2611             ) and not _contains_datetime_like_objects(new_x):
2612                 raise TypeError(
2613                     "When interpolating over a datetime-like "
2614                     "coordinate, the coordinates to "
2615                     "interpolate to must be either datetime "
2616                     "strings or datetimes. "
2617                     "Instead got\n{}".format(new_x)
2618                 )
2619             return x, new_x
2620 
2621         variables: Dict[Hashable, Variable] = {}
2622         for name, var in obj._variables.items():
2623             if name in indexers:
2624                 continue
2625 
2626             if var.dtype.kind in "uifc":
2627                 var_indexers = {
2628                     k: _validate_interp_indexer(maybe_variable(obj, k), v)
2629                     for k, v in indexers.items()
2630                     if k in var.dims
2631                 }
2632                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)
2633             elif all(d not in indexers for d in var.dims):
2634                 # keep unrelated object array
2635                 variables[name] = var
2636 
2637         coord_names = obj._coord_names & variables.keys()
2638         indexes = {k: v for k, v in obj.indexes.items() if k not in indexers}
2639         selected = self._replace_with_new_dims(
2640             variables.copy(), coord_names, indexes=indexes
2641         )
2642 
2643         # attach indexer as coordinate
2644         variables.update(indexers)
2645         for k, v in indexers.items():
2646             assert isinstance(v, Variable)
2647             if v.dims == (k,):
2648                 indexes[k] = v.to_index()
2649 
2650         # Extract coordinates from indexers
2651         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)
2652         variables.update(coord_vars)
2653         indexes.update(new_indexes)
2654 
2655         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()
2656         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2657 
2658     def interp_like(
2659         self,
2660         other: Union["Dataset", "DataArray"],
2661         method: str = "linear",
2662         assume_sorted: bool = False,
2663         kwargs: Mapping[str, Any] = None,
2664     ) -> "Dataset":
2665         """Interpolate this object onto the coordinates of another object,
2666         filling the out of range values with NaN.
2667 
2668         Parameters
2669         ----------
2670         other : Dataset or DataArray
2671             Object with an 'indexes' attribute giving a mapping from dimension
2672             names to an 1d array-like, which provides coordinates upon
2673             which to index the variables in this dataset.
2674         method: string, optional.
2675             {'linear', 'nearest'} for multidimensional array,
2676             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
2677             for 1-dimensional array. 'linear' is used by default.
2678         assume_sorted: boolean, optional
2679             If False, values of coordinates that are interpolated over can be
2680             in any order and they are sorted first. If True, interpolated
2681             coordinates are assumed to be an array of monotonically increasing
2682             values.
2683         kwargs: dictionary, optional
2684             Additional keyword passed to scipy's interpolator.
2685 
2686         Returns
2687         -------
2688         interpolated: xr.Dataset
2689             Another dataset by interpolating this dataset's data along the
2690             coordinates of the other object.
2691 
2692         Notes
2693         -----
2694         scipy is required.
2695         If the dataset has object-type coordinates, reindex is used for these
2696         coordinates instead of the interpolation.
2697 
2698         See Also
2699         --------
2700         Dataset.interp
2701         Dataset.reindex_like
2702         """
2703         if kwargs is None:
2704             kwargs = {}
2705         coords = alignment.reindex_like_indexers(self, other)
2706 
2707         numeric_coords: Dict[Hashable, pd.Index] = {}
2708         object_coords: Dict[Hashable, pd.Index] = {}
2709         for k, v in coords.items():
2710             if v.dtype.kind in "uifcMm":
2711                 numeric_coords[k] = v
2712             else:
2713                 object_coords[k] = v
2714 
2715         ds = self
2716         if object_coords:
2717             # We do not support interpolation along object coordinate.
2718             # reindex instead.
2719             ds = self.reindex(object_coords)
2720         return ds.interp(numeric_coords, method, assume_sorted, kwargs)
2721 
2722     # Helper methods for rename()
2723     def _rename_vars(self, name_dict, dims_dict):
2724         variables = {}
2725         coord_names = set()
2726         for k, v in self.variables.items():
2727             var = v.copy(deep=False)
2728             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
2729             name = name_dict.get(k, k)
2730             if name in variables:
2731                 raise ValueError(f"the new name {name!r} conflicts")
2732             variables[name] = var
2733             if k in self._coord_names:
2734                 coord_names.add(name)
2735         return variables, coord_names
2736 
2737     def _rename_dims(self, name_dict):
2738         return {name_dict.get(k, k): v for k, v in self.dims.items()}
2739 
2740     def _rename_indexes(self, name_dict, dims_set):
2741         if self._indexes is None:
2742             return None
2743         indexes = {}
2744         for k, v in self.indexes.items():
2745             new_name = name_dict.get(k, k)
2746             if new_name not in dims_set:
2747                 continue
2748             if isinstance(v, pd.MultiIndex):
2749                 new_names = [name_dict.get(k, k) for k in v.names]
2750                 index = v.rename(names=new_names)
2751             else:
2752                 index = v.rename(new_name)
2753             indexes[new_name] = index
2754         return indexes
2755 
2756     def _rename_all(self, name_dict, dims_dict):
2757         variables, coord_names = self._rename_vars(name_dict, dims_dict)
2758         dims = self._rename_dims(dims_dict)
2759         indexes = self._rename_indexes(name_dict, dims.keys())
2760         return variables, coord_names, dims, indexes
2761 
2762     def rename(
2763         self,
2764         name_dict: Mapping[Hashable, Hashable] = None,
2765         inplace: bool = None,
2766         **names: Hashable,
2767     ) -> "Dataset":
2768         """Returns a new object with renamed variables and dimensions.
2769 
2770         Parameters
2771         ----------
2772         name_dict : dict-like, optional
2773             Dictionary whose keys are current variable or dimension names and
2774             whose values are the desired names.
2775         **names, optional
2776             Keyword form of ``name_dict``.
2777             One of name_dict or names must be provided.
2778 
2779         Returns
2780         -------
2781         renamed : Dataset
2782             Dataset with renamed variables and dimensions.
2783 
2784         See Also
2785         --------
2786         Dataset.swap_dims
2787         Dataset.rename_vars
2788         Dataset.rename_dims
2789         DataArray.rename
2790         """
2791         _check_inplace(inplace)
2792         name_dict = either_dict_or_kwargs(name_dict, names, "rename")
2793         for k in name_dict.keys():
2794             if k not in self and k not in self.dims:
2795                 raise ValueError(
2796                     "cannot rename %r because it is not a "
2797                     "variable or dimension in this dataset" % k
2798                 )
2799 
2800         variables, coord_names, dims, indexes = self._rename_all(
2801             name_dict=name_dict, dims_dict=name_dict
2802         )
2803         assert_unique_multiindex_level_names(variables)
2804         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
2805 
2806     def rename_dims(
2807         self, dims_dict: Mapping[Hashable, Hashable] = None, **dims: Hashable
2808     ) -> "Dataset":
2809         """Returns a new object with renamed dimensions only.
2810 
2811         Parameters
2812         ----------
2813         dims_dict : dict-like, optional
2814             Dictionary whose keys are current dimension names and
2815             whose values are the desired names. The desired names must
2816             not be the name of an existing dimension or Variable in the Dataset.
2817         **dims, optional
2818             Keyword form of ``dims_dict``.
2819             One of dims_dict or dims must be provided.
2820 
2821         Returns
2822         -------
2823         renamed : Dataset
2824             Dataset with renamed dimensions.
2825 
2826         See Also
2827         --------
2828         Dataset.swap_dims
2829         Dataset.rename
2830         Dataset.rename_vars
2831         DataArray.rename
2832         """
2833         dims_dict = either_dict_or_kwargs(dims_dict, dims, "rename_dims")
2834         for k, v in dims_dict.items():
2835             if k not in self.dims:
2836                 raise ValueError(
2837                     "cannot rename %r because it is not a "
2838                     "dimension in this dataset" % k
2839                 )
2840             if v in self.dims or v in self:
2841                 raise ValueError(
2842                     f"Cannot rename {k} to {v} because {v} already exists. "
2843                     "Try using swap_dims instead."
2844                 )
2845 
2846         variables, coord_names, sizes, indexes = self._rename_all(
2847             name_dict={}, dims_dict=dims_dict
2848         )
2849         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)
2850 
2851     def rename_vars(
2852         self, name_dict: Mapping[Hashable, Hashable] = None, **names: Hashable
2853     ) -> "Dataset":
2854         """Returns a new object with renamed variables including coordinates
2855 
2856         Parameters
2857         ----------
2858         name_dict : dict-like, optional
2859             Dictionary whose keys are current variable or coordinate names and
2860             whose values are the desired names.
2861         **names, optional
2862             Keyword form of ``name_dict``.
2863             One of name_dict or names must be provided.
2864 
2865         Returns
2866         -------
2867         renamed : Dataset
2868             Dataset with renamed variables including coordinates
2869 
2870         See Also
2871         --------
2872         Dataset.swap_dims
2873         Dataset.rename
2874         Dataset.rename_dims
2875         DataArray.rename
2876         """
2877         name_dict = either_dict_or_kwargs(name_dict, names, "rename_vars")
2878         for k in name_dict:
2879             if k not in self:
2880                 raise ValueError(
2881                     "cannot rename %r because it is not a "
2882                     "variable or coordinate in this dataset" % k
2883                 )
2884         variables, coord_names, dims, indexes = self._rename_all(
2885             name_dict=name_dict, dims_dict={}
2886         )
2887         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
2888 
2889     def swap_dims(
2890         self, dims_dict: Mapping[Hashable, Hashable], inplace: bool = None
2891     ) -> "Dataset":
2892         """Returns a new object with swapped dimensions.
2893 
2894         Parameters
2895         ----------
2896         dims_dict : dict-like
2897             Dictionary whose keys are current dimension names and whose values
2898             are new names.
2899 
2900         Returns
2901         -------
2902         swapped : Dataset
2903             Dataset with swapped dimensions.
2904 
2905         Examples
2906         --------
2907         >>> ds = xr.Dataset(
2908         ...     data_vars={"a": ("x", [5, 7]), "b": ("x", [0.1, 2.4])},
2909         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
2910         ... )
2911         >>> ds
2912         <xarray.Dataset>
2913         Dimensions:  (x: 2)
2914         Coordinates:
2915           * x        (x) <U1 'a' 'b'
2916             y        (x) int64 0 1
2917         Data variables:
2918             a        (x) int64 5 7
2919             b        (x) float64 0.1 2.4
2920 
2921         >>> ds.swap_dims({"x": "y"})
2922         <xarray.Dataset>
2923         Dimensions:  (y: 2)
2924         Coordinates:
2925             x        (y) <U1 'a' 'b'
2926           * y        (y) int64 0 1
2927         Data variables:
2928             a        (y) int64 5 7
2929             b        (y) float64 0.1 2.4
2930 
2931         >>> ds.swap_dims({"x": "z"})
2932         <xarray.Dataset>
2933         Dimensions:  (z: 2)
2934         Coordinates:
2935             x        (z) <U1 'a' 'b'
2936             y        (z) int64 0 1
2937         Dimensions without coordinates: z
2938         Data variables:
2939             a        (z) int64 5 7
2940             b        (z) float64 0.1 2.4
2941 
2942         See Also
2943         --------
2944 
2945         Dataset.rename
2946         DataArray.swap_dims
2947         """
2948         # TODO: deprecate this method in favor of a (less confusing)
2949         # rename_dims() method that only renames dimensions.
2950         _check_inplace(inplace)
2951         for k, v in dims_dict.items():
2952             if k not in self.dims:
2953                 raise ValueError(
2954                     "cannot swap from dimension %r because it is "
2955                     "not an existing dimension" % k
2956                 )
2957             if v in self.variables and self.variables[v].dims != (k,):
2958                 raise ValueError(
2959                     "replacement dimension %r is not a 1D "
2960                     "variable along the old dimension %r" % (v, k)
2961                 )
2962 
2963         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}
2964 
2965         coord_names = self._coord_names.copy()
2966         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})
2967 
2968         variables: Dict[Hashable, Variable] = {}
2969         indexes: Dict[Hashable, pd.Index] = {}
2970         for k, v in self.variables.items():
2971             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
2972             if k in result_dims:
2973                 var = v.to_index_variable()
2974                 if k in self.indexes:
2975                     indexes[k] = self.indexes[k]
2976                 else:
2977                     new_index = var.to_index()
2978                     if new_index.nlevels == 1:
2979                         # make sure index name matches dimension name
2980                         new_index = new_index.rename(k)
2981                     indexes[k] = new_index
2982             else:
2983                 var = v.to_base_variable()
2984             var.dims = dims
2985             variables[k] = var
2986 
2987         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2988 
2989     def expand_dims(
2990         self,
2991         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,
2992         axis: Union[None, int, Sequence[int]] = None,
2993         **dim_kwargs: Any,
2994     ) -> "Dataset":
2995         """Return a new object with an additional axis (or axes) inserted at
2996         the corresponding position in the array shape.  The new object is a
2997         view into the underlying array, not a copy.
2998 
2999         If dim is already a scalar coordinate, it will be promoted to a 1D
3000         coordinate consisting of a single value.
3001 
3002         Parameters
3003         ----------
3004         dim : hashable, sequence of hashable, mapping, or None
3005             Dimensions to include on the new variable. If provided as hashable
3006             or sequence of hashable, then dimensions are inserted with length
3007             1. If provided as a mapping, then the keys are the new dimensions
3008             and the values are either integers (giving the length of the new
3009             dimensions) or array-like (giving the coordinates of the new
3010             dimensions).
3011         axis : integer, sequence of integers, or None
3012             Axis position(s) where new axis is to be inserted (position(s) on
3013             the result array). If a list (or tuple) of integers is passed,
3014             multiple axes are inserted. In this case, dim arguments should be
3015             same length list. If axis=None is passed, all the axes will be
3016             inserted to the start of the result array.
3017         **dim_kwargs : int or sequence/ndarray
3018             The keywords are arbitrary dimensions being inserted and the values
3019             are either the lengths of the new dims (if int is given), or their
3020             coordinates. Note, this is an alternative to passing a dict to the
3021             dim kwarg and will only be used if dim is None.
3022 
3023         Returns
3024         -------
3025         expanded : same type as caller
3026             This object, but with an additional dimension(s).
3027         """
3028         if dim is None:
3029             pass
3030         elif isinstance(dim, Mapping):
3031             # We're later going to modify dim in place; don't tamper with
3032             # the input
3033             dim = dict(dim)
3034         elif isinstance(dim, int):
3035             raise TypeError(
3036                 "dim should be hashable or sequence of hashables or mapping"
3037             )
3038         elif isinstance(dim, str) or not isinstance(dim, Sequence):
3039             dim = {dim: 1}
3040         elif isinstance(dim, Sequence):
3041             if len(dim) != len(set(dim)):
3042                 raise ValueError("dims should not contain duplicate values.")
3043             dim = {d: 1 for d in dim}
3044 
3045         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
3046         assert isinstance(dim, MutableMapping)
3047 
3048         if axis is None:
3049             axis = list(range(len(dim)))
3050         elif not isinstance(axis, Sequence):
3051             axis = [axis]
3052 
3053         if len(dim) != len(axis):
3054             raise ValueError("lengths of dim and axis should be identical.")
3055         for d in dim:
3056             if d in self.dims:
3057                 raise ValueError(f"Dimension {d} already exists.")
3058             if d in self._variables and not utils.is_scalar(self._variables[d]):
3059                 raise ValueError(
3060                     "{dim} already exists as coordinate or"
3061                     " variable name.".format(dim=d)
3062                 )
3063 
3064         variables: Dict[Hashable, Variable] = {}
3065         coord_names = self._coord_names.copy()
3066         # If dim is a dict, then ensure that the values are either integers
3067         # or iterables.
3068         for k, v in dim.items():
3069             if hasattr(v, "__iter__"):
3070                 # If the value for the new dimension is an iterable, then
3071                 # save the coordinates to the variables dict, and set the
3072                 # value within the dim dict to the length of the iterable
3073                 # for later use.
3074                 variables[k] = xr.IndexVariable((k,), v)
3075                 coord_names.add(k)
3076                 dim[k] = variables[k].size
3077             elif isinstance(v, int):
3078                 pass  # Do nothing if the dimensions value is just an int
3079             else:
3080                 raise TypeError(
3081                     "The value of new dimension {k} must be "
3082                     "an iterable or an int".format(k=k)
3083                 )
3084 
3085         for k, v in self._variables.items():
3086             if k not in dim:
3087                 if k in coord_names:  # Do not change coordinates
3088                     variables[k] = v
3089                 else:
3090                     result_ndim = len(v.dims) + len(axis)
3091                     for a in axis:
3092                         if a < -result_ndim or result_ndim - 1 < a:
3093                             raise IndexError(
3094                                 f"Axis {a} of variable {k} is out of bounds of the "
3095                                 f"expanded dimension size {result_ndim}"
3096                             )
3097 
3098                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]
3099                     if len(axis_pos) != len(set(axis_pos)):
3100                         raise ValueError("axis should not contain duplicate values")
3101                     # We need to sort them to make sure `axis` equals to the
3102                     # axis positions of the result array.
3103                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))
3104 
3105                     all_dims = list(zip(v.dims, v.shape))
3106                     for d, c in zip_axis_dim:
3107                         all_dims.insert(d, c)
3108                     variables[k] = v.set_dims(dict(all_dims))
3109             else:
3110                 # If dims includes a label of a non-dimension coordinate,
3111                 # it will be promoted to a 1D coordinate with a single value.
3112                 variables[k] = v.set_dims(k).to_index_variable()
3113 
3114         new_dims = self._dims.copy()
3115         new_dims.update(dim)
3116 
3117         return self._replace_vars_and_dims(
3118             variables, dims=new_dims, coord_names=coord_names
3119         )
3120 
3121     def set_index(
3122         self,
3123         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
3124         append: bool = False,
3125         inplace: bool = None,
3126         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],
3127     ) -> "Dataset":
3128         """Set Dataset (multi-)indexes using one or more existing coordinates
3129         or variables.
3130 
3131         Parameters
3132         ----------
3133         indexes : {dim: index, ...}
3134             Mapping from names matching dimensions and values given
3135             by (lists of) the names of existing coordinates or variables to set
3136             as new (multi-)index.
3137         append : bool, optional
3138             If True, append the supplied index(es) to the existing index(es).
3139             Otherwise replace the existing index(es) (default).
3140         **indexes_kwargs: optional
3141             The keyword arguments form of ``indexes``.
3142             One of indexes or indexes_kwargs must be provided.
3143 
3144         Returns
3145         -------
3146         obj : Dataset
3147             Another dataset, with this dataset's data but replaced coordinates.
3148 
3149         Examples
3150         --------
3151         >>> arr = xr.DataArray(
3152         ...     data=np.ones((2, 3)),
3153         ...     dims=["x", "y"],
3154         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
3155         ... )
3156         >>> ds = xr.Dataset({"v": arr})
3157         >>> ds
3158         <xarray.Dataset>
3159         Dimensions:  (x: 2, y: 3)
3160         Coordinates:
3161           * x        (x) int64 0 1
3162           * y        (y) int64 0 1 2
3163             a        (x) int64 3 4
3164         Data variables:
3165             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
3166         >>> ds.set_index(x="a")
3167         <xarray.Dataset>
3168         Dimensions:  (x: 2, y: 3)
3169         Coordinates:
3170           * x        (x) int64 3 4
3171           * y        (y) int64 0 1 2
3172         Data variables:
3173             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
3174 
3175         See Also
3176         --------
3177         Dataset.reset_index
3178         Dataset.swap_dims
3179         """
3180         _check_inplace(inplace)
3181         indexes = either_dict_or_kwargs(indexes, indexes_kwargs, "set_index")
3182         variables, coord_names = merge_indexes(
3183             indexes, self._variables, self._coord_names, append=append
3184         )
3185         return self._replace_vars_and_dims(variables, coord_names=coord_names)
3186 
3187     def reset_index(
3188         self,
3189         dims_or_levels: Union[Hashable, Sequence[Hashable]],
3190         drop: bool = False,
3191         inplace: bool = None,
3192     ) -> "Dataset":
3193         """Reset the specified index(es) or multi-index level(s).
3194 
3195         Parameters
3196         ----------
3197         dims_or_levels : str or list
3198             Name(s) of the dimension(s) and/or multi-index level(s) that will
3199             be reset.
3200         drop : bool, optional
3201             If True, remove the specified indexes and/or multi-index levels
3202             instead of extracting them as new coordinates (default: False).
3203 
3204         Returns
3205         -------
3206         obj : Dataset
3207             Another dataset, with this dataset's data but replaced coordinates.
3208 
3209         See Also
3210         --------
3211         Dataset.set_index
3212         """
3213         _check_inplace(inplace)
3214         variables, coord_names = split_indexes(
3215             dims_or_levels,
3216             self._variables,
3217             self._coord_names,
3218             cast(Mapping[Hashable, Hashable], self._level_coords),
3219             drop=drop,
3220         )
3221         return self._replace_vars_and_dims(variables, coord_names=coord_names)
3222 
3223     def reorder_levels(
3224         self,
3225         dim_order: Mapping[Hashable, Sequence[int]] = None,
3226         inplace: bool = None,
3227         **dim_order_kwargs: Sequence[int],
3228     ) -> "Dataset":
3229         """Rearrange index levels using input order.
3230 
3231         Parameters
3232         ----------
3233         dim_order : optional
3234             Mapping from names matching dimensions and values given
3235             by lists representing new level orders. Every given dimension
3236             must have a multi-index.
3237         **dim_order_kwargs: optional
3238             The keyword arguments form of ``dim_order``.
3239             One of dim_order or dim_order_kwargs must be provided.
3240 
3241         Returns
3242         -------
3243         obj : Dataset
3244             Another dataset, with this dataset's data but replaced
3245             coordinates.
3246         """
3247         _check_inplace(inplace)
3248         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
3249         variables = self._variables.copy()
3250         indexes = dict(self.indexes)
3251         for dim, order in dim_order.items():
3252             coord = self._variables[dim]
3253             index = self.indexes[dim]
3254             if not isinstance(index, pd.MultiIndex):
3255                 raise ValueError(f"coordinate {dim} has no MultiIndex")
3256             new_index = index.reorder_levels(order)
3257             variables[dim] = IndexVariable(coord.dims, new_index)
3258             indexes[dim] = new_index
3259 
3260         return self._replace(variables, indexes=indexes)
3261 
3262     def _stack_once(self, dims, new_dim):
3263         variables = {}
3264         for name, var in self.variables.items():
3265             if name not in dims:
3266                 if any(d in var.dims for d in dims):
3267                     add_dims = [d for d in dims if d not in var.dims]
3268                     vdims = list(var.dims) + add_dims
3269                     shape = [self.dims[d] for d in vdims]
3270                     exp_var = var.set_dims(vdims, shape)
3271                     stacked_var = exp_var.stack(**{new_dim: dims})
3272                     variables[name] = stacked_var
3273                 else:
3274                     variables[name] = var.copy(deep=False)
3275 
3276         # consider dropping levels that are unused?
3277         levels = [self.get_index(dim) for dim in dims]
3278         idx = utils.multiindex_from_product_levels(levels, names=dims)
3279         variables[new_dim] = IndexVariable(new_dim, idx)
3280 
3281         coord_names = set(self._coord_names) - set(dims) | {new_dim}
3282 
3283         indexes = {k: v for k, v in self.indexes.items() if k not in dims}
3284         indexes[new_dim] = idx
3285 
3286         return self._replace_with_new_dims(
3287             variables, coord_names=coord_names, indexes=indexes
3288         )
3289 
3290     def stack(
3291         self,
3292         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
3293         **dimensions_kwargs: Sequence[Hashable],
3294     ) -> "Dataset":
3295         """
3296         Stack any number of existing dimensions into a single new dimension.
3297 
3298         New dimensions will be added at the end, and the corresponding
3299         coordinate variables will be combined into a MultiIndex.
3300 
3301         Parameters
3302         ----------
3303         dimensions : Mapping of the form new_name=(dim1, dim2, ...)
3304             Names of new dimensions, and the existing dimensions that they
3305             replace.
3306         **dimensions_kwargs:
3307             The keyword arguments form of ``dimensions``.
3308             One of dimensions or dimensions_kwargs must be provided.
3309 
3310         Returns
3311         -------
3312         stacked : Dataset
3313             Dataset with stacked data.
3314 
3315         See also
3316         --------
3317         Dataset.unstack
3318         """
3319         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
3320         result = self
3321         for new_dim, dims in dimensions.items():
3322             result = result._stack_once(dims, new_dim)
3323         return result
3324 
3325     def to_stacked_array(
3326         self,
3327         new_dim: Hashable,
3328         sample_dims: Sequence[Hashable],
3329         variable_dim: str = "variable",
3330         name: Hashable = None,
3331     ) -> "DataArray":
3332         """Combine variables of differing dimensionality into a DataArray
3333         without broadcasting.
3334 
3335         This method is similar to Dataset.to_array but does not broadcast the
3336         variables.
3337 
3338         Parameters
3339         ----------
3340         new_dim : Hashable
3341             Name of the new stacked coordinate
3342         sample_dims : Sequence[Hashable]
3343             Dimensions that **will not** be stacked. Each array in the dataset
3344             must share these dimensions. For machine learning applications,
3345             these define the dimensions over which samples are drawn.
3346         variable_dim : str, optional
3347             Name of the level in the stacked coordinate which corresponds to
3348             the variables.
3349         name : str, optional
3350             Name of the new data array.
3351 
3352         Returns
3353         -------
3354         stacked : DataArray
3355             DataArray with the specified dimensions and data variables
3356             stacked together. The stacked coordinate is named ``new_dim``
3357             and represented by a MultiIndex object with a level containing the
3358             data variable names. The name of this level is controlled using
3359             the ``variable_dim`` argument.
3360 
3361         See Also
3362         --------
3363         Dataset.to_array
3364         Dataset.stack
3365         DataArray.to_unstacked_dataset
3366 
3367         Examples
3368         --------
3369         >>> data = xr.Dataset(
3370         ...     data_vars={
3371         ...         "a": (("x", "y"), [[0, 1, 2], [3, 4, 5]]),
3372         ...         "b": ("x", [6, 7]),
3373         ...     },
3374         ...     coords={"y": ["u", "v", "w"]},
3375         ... )
3376 
3377         >>> data
3378         <xarray.Dataset>
3379         Dimensions:  (x: 2, y: 3)
3380         Coordinates:
3381         * y        (y) <U1 'u' 'v' 'w'
3382         Dimensions without coordinates: x
3383         Data variables:
3384             a        (x, y) int64 0 1 2 3 4 5
3385             b        (x) int64 6 7
3386 
3387         >>> data.to_stacked_array("z", sample_dims=["x"])
3388         <xarray.DataArray (x: 2, z: 4)>
3389         array([[0, 1, 2, 6],
3390             [3, 4, 5, 7]])
3391         Coordinates:
3392         * z         (z) MultiIndex
3393         - variable  (z) object 'a' 'a' 'a' 'b'
3394         - y         (z) object 'u' 'v' 'w' nan
3395         Dimensions without coordinates: x
3396 
3397         """
3398         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)
3399 
3400         for variable in self:
3401             dims = self[variable].dims
3402             dims_include_sample_dims = set(sample_dims) <= set(dims)
3403             if not dims_include_sample_dims:
3404                 raise ValueError(
3405                     "All variables in the dataset must contain the "
3406                     "dimensions {}.".format(dims)
3407                 )
3408 
3409         def ensure_stackable(val):
3410             assign_coords = {variable_dim: val.name}
3411             for dim in stacking_dims:
3412                 if dim not in val.dims:
3413                     assign_coords[dim] = None
3414 
3415             expand_dims = set(stacking_dims).difference(set(val.dims))
3416             expand_dims.add(variable_dim)
3417             # must be list for .expand_dims
3418             expand_dims = list(expand_dims)
3419 
3420             return (
3421                 val.assign_coords(**assign_coords)
3422                 .expand_dims(expand_dims)
3423                 .stack({new_dim: (variable_dim,) + stacking_dims})
3424             )
3425 
3426         # concatenate the arrays
3427         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]
3428         data_array = xr.concat(stackable_vars, dim=new_dim)
3429 
3430         # coerce the levels of the MultiIndex to have the same type as the
3431         # input dimensions. This code is messy, so it might be better to just
3432         # input a dummy value for the singleton dimension.
3433         idx = data_array.indexes[new_dim]
3434         levels = [idx.levels[0]] + [
3435             level.astype(self[level.name].dtype) for level in idx.levels[1:]
3436         ]
3437         new_idx = idx.set_levels(levels)
3438         data_array[new_dim] = IndexVariable(new_dim, new_idx)
3439 
3440         if name is not None:
3441             data_array.name = name
3442 
3443         return data_array
3444 
3445     def _unstack_once(self, dim: Hashable, fill_value, sparse) -> "Dataset":
3446         index = self.get_index(dim)
3447         index = remove_unused_levels_categories(index)
3448         full_idx = pd.MultiIndex.from_product(index.levels, names=index.names)
3449 
3450         # take a shortcut in case the MultiIndex was not modified.
3451         if index.equals(full_idx):
3452             obj = self
3453         else:
3454             obj = self._reindex(
3455                 {dim: full_idx}, copy=False, fill_value=fill_value, sparse=sparse
3456             )
3457 
3458         new_dim_names = index.names
3459         new_dim_sizes = [lev.size for lev in index.levels]
3460 
3461         variables: Dict[Hashable, Variable] = {}
3462         indexes = {k: v for k, v in self.indexes.items() if k != dim}
3463 
3464         for name, var in obj.variables.items():
3465             if name != dim:
3466                 if dim in var.dims:
3467                     new_dims = dict(zip(new_dim_names, new_dim_sizes))
3468                     variables[name] = var.unstack({dim: new_dims})
3469                 else:
3470                     variables[name] = var
3471 
3472         for name, lev in zip(new_dim_names, index.levels):
3473             variables[name] = IndexVariable(name, lev)
3474             indexes[name] = lev
3475 
3476         coord_names = set(self._coord_names) - {dim} | set(new_dim_names)
3477 
3478         return self._replace_with_new_dims(
3479             variables, coord_names=coord_names, indexes=indexes
3480         )
3481 
3482     def unstack(
3483         self,
3484         dim: Union[Hashable, Iterable[Hashable]] = None,
3485         fill_value: Any = dtypes.NA,
3486         sparse: bool = False,
3487     ) -> "Dataset":
3488         """
3489         Unstack existing dimensions corresponding to MultiIndexes into
3490         multiple new dimensions.
3491 
3492         New dimensions will be added at the end.
3493 
3494         Parameters
3495         ----------
3496         dim : Hashable or iterable of Hashable, optional
3497             Dimension(s) over which to unstack. By default unstacks all
3498             MultiIndexes.
3499         fill_value: value to be filled. By default, np.nan
3500         sparse: use sparse-array if True
3501 
3502         Returns
3503         -------
3504         unstacked : Dataset
3505             Dataset with unstacked data.
3506 
3507         See also
3508         --------
3509         Dataset.stack
3510         """
3511         if dim is None:
3512             dims = [
3513                 d for d in self.dims if isinstance(self.get_index(d), pd.MultiIndex)
3514             ]
3515         else:
3516             if isinstance(dim, str) or not isinstance(dim, Iterable):
3517                 dims = [dim]
3518             else:
3519                 dims = list(dim)
3520 
3521             missing_dims = [d for d in dims if d not in self.dims]
3522             if missing_dims:
3523                 raise ValueError(
3524                     "Dataset does not contain the dimensions: %s" % missing_dims
3525                 )
3526 
3527             non_multi_dims = [
3528                 d for d in dims if not isinstance(self.get_index(d), pd.MultiIndex)
3529             ]
3530             if non_multi_dims:
3531                 raise ValueError(
3532                     "cannot unstack dimensions that do not "
3533                     "have a MultiIndex: %s" % non_multi_dims
3534                 )
3535 
3536         result = self.copy(deep=False)
3537         for dim in dims:
3538             result = result._unstack_once(dim, fill_value, sparse)
3539         return result
3540 
3541     def update(self, other: "CoercibleMapping", inplace: bool = None) -> "Dataset":
3542         """Update this dataset's variables with those from another dataset.
3543 
3544         Parameters
3545         ----------
3546         other : Dataset or castable to Dataset
3547             Variables with which to update this dataset. One of:
3548 
3549             - Dataset
3550             - mapping {var name: DataArray}
3551             - mapping {var name: Variable}
3552             - mapping {var name: (dimension name, array-like)}
3553             - mapping {var name: (tuple of dimension names, array-like)}
3554 
3555 
3556         Returns
3557         -------
3558         updated : Dataset
3559             Updated dataset.
3560 
3561         Raises
3562         ------
3563         ValueError
3564             If any dimensions would have inconsistent sizes in the updated
3565             dataset.
3566         """
3567         _check_inplace(inplace)
3568         merge_result = dataset_update_method(self, other)
3569         return self._replace(inplace=True, **merge_result._asdict())
3570 
3571     def merge(
3572         self,
3573         other: Union["CoercibleMapping", "DataArray"],
3574         inplace: bool = None,
3575         overwrite_vars: Union[Hashable, Iterable[Hashable]] = frozenset(),
3576         compat: str = "no_conflicts",
3577         join: str = "outer",
3578         fill_value: Any = dtypes.NA,
3579     ) -> "Dataset":
3580         """Merge the arrays of two datasets into a single dataset.
3581 
3582         This method generally does not allow for overriding data, with the
3583         exception of attributes, which are ignored on the second dataset.
3584         Variables with the same name are checked for conflicts via the equals
3585         or identical methods.
3586 
3587         Parameters
3588         ----------
3589         other : Dataset or castable to Dataset
3590             Dataset or variables to merge with this dataset.
3591         overwrite_vars : Hashable or iterable of Hashable, optional
3592             If provided, update variables of these name(s) without checking for
3593             conflicts in this dataset.
3594         compat : {'broadcast_equals', 'equals', 'identical',
3595                   'no_conflicts'}, optional
3596             String indicating how to compare variables of the same name for
3597             potential conflicts:
3598 
3599             - 'broadcast_equals': all values must be equal when variables are
3600               broadcast against each other to ensure common dimensions.
3601             - 'equals': all values and dimensions must be the same.
3602             - 'identical': all values, dimensions and attributes must be the
3603               same.
3604             - 'no_conflicts': only values which are not null in both datasets
3605               must be equal. The returned dataset then contains the combination
3606               of all non-null values.
3607 
3608         join : {'outer', 'inner', 'left', 'right', 'exact'}, optional
3609             Method for joining ``self`` and ``other`` along shared dimensions:
3610 
3611             - 'outer': use the union of the indexes
3612             - 'inner': use the intersection of the indexes
3613             - 'left': use indexes from ``self``
3614             - 'right': use indexes from ``other``
3615             - 'exact': error instead of aligning non-equal indexes
3616         fill_value: scalar, optional
3617             Value to use for newly missing values
3618 
3619         Returns
3620         -------
3621         merged : Dataset
3622             Merged dataset.
3623 
3624         Raises
3625         ------
3626         MergeError
3627             If any variables conflict (see ``compat``).
3628         """
3629         _check_inplace(inplace)
3630         other = other.to_dataset() if isinstance(other, xr.DataArray) else other
3631         merge_result = dataset_merge_method(
3632             self,
3633             other,
3634             overwrite_vars=overwrite_vars,
3635             compat=compat,
3636             join=join,
3637             fill_value=fill_value,
3638         )
3639         return self._replace(**merge_result._asdict())
3640 
3641     def _assert_all_in_dataset(
3642         self, names: Iterable[Hashable], virtual_okay: bool = False
3643     ) -> None:
3644         bad_names = set(names) - set(self._variables)
3645         if virtual_okay:
3646             bad_names -= self.virtual_variables
3647         if bad_names:
3648             raise ValueError(
3649                 "One or more of the specified variables "
3650                 "cannot be found in this dataset"
3651             )
3652 
3653     def drop_vars(
3654         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
3655     ) -> "Dataset":
3656         """Drop variables from this dataset.
3657 
3658         Parameters
3659         ----------
3660         names : hashable or iterable of hashables
3661             Name(s) of variables to drop.
3662         errors: {'raise', 'ignore'}, optional
3663             If 'raise' (default), raises a ValueError error if any of the variable
3664             passed are not in the dataset. If 'ignore', any given names that are in the
3665             dataset are dropped and no error is raised.
3666 
3667         Returns
3668         -------
3669         dropped : Dataset
3670 
3671         """
3672         # the Iterable check is required for mypy
3673         if is_scalar(names) or not isinstance(names, Iterable):
3674             names = {names}
3675         else:
3676             names = set(names)
3677         if errors == "raise":
3678             self._assert_all_in_dataset(names)
3679 
3680         variables = {k: v for k, v in self._variables.items() if k not in names}
3681         coord_names = {k for k in self._coord_names if k in variables}
3682         indexes = {k: v for k, v in self.indexes.items() if k not in names}
3683         return self._replace_with_new_dims(
3684             variables, coord_names=coord_names, indexes=indexes
3685         )
3686 
3687     def drop(self, labels=None, dim=None, *, errors="raise", **labels_kwargs):
3688         """Backward compatible method based on `drop_vars` and `drop_sel`
3689 
3690         Using either `drop_vars` or `drop_sel` is encouraged
3691 
3692         See Also
3693         --------
3694         Dataset.drop_vars
3695         Dataset.drop_sel
3696         """
3697         if errors not in ["raise", "ignore"]:
3698             raise ValueError('errors must be either "raise" or "ignore"')
3699 
3700         if is_dict_like(labels) and not isinstance(labels, dict):
3701             warnings.warn(
3702                 "dropping coordinates using `drop` is be deprecated; use drop_vars.",
3703                 FutureWarning,
3704                 stacklevel=2,
3705             )
3706             return self.drop_vars(labels, errors=errors)
3707 
3708         if labels_kwargs or isinstance(labels, dict):
3709             if dim is not None:
3710                 raise ValueError("cannot specify dim and dict-like arguments.")
3711             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
3712 
3713         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):
3714             warnings.warn(
3715                 "dropping variables using `drop` will be deprecated; using drop_vars is encouraged.",
3716                 PendingDeprecationWarning,
3717                 stacklevel=2,
3718             )
3719             return self.drop_vars(labels, errors=errors)
3720         if dim is not None:
3721             warnings.warn(
3722                 "dropping labels using list-like labels is deprecated; using "
3723                 "dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).",
3724                 DeprecationWarning,
3725                 stacklevel=2,
3726             )
3727             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)
3728 
3729         warnings.warn(
3730             "dropping labels using `drop` will be deprecated; using drop_sel is encouraged.",
3731             PendingDeprecationWarning,
3732             stacklevel=2,
3733         )
3734         return self.drop_sel(labels, errors=errors)
3735 
3736     def drop_sel(self, labels=None, *, errors="raise", **labels_kwargs):
3737         """Drop index labels from this dataset.
3738 
3739         Parameters
3740         ----------
3741         labels : Mapping[Hashable, Any]
3742             Index labels to drop
3743         errors: {'raise', 'ignore'}, optional
3744             If 'raise' (default), raises a ValueError error if
3745             any of the index labels passed are not
3746             in the dataset. If 'ignore', any given labels that are in the
3747             dataset are dropped and no error is raised.
3748         **labels_kwargs : {dim: label, ...}, optional
3749             The keyword arguments form of ``dim`` and ``labels``
3750 
3751         Returns
3752         -------
3753         dropped : Dataset
3754 
3755         Examples
3756         --------
3757         >>> data = np.random.randn(2, 3)
3758         >>> labels = ["a", "b", "c"]
3759         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
3760         >>> ds.drop_sel(y=["a", "c"])
3761         <xarray.Dataset>
3762         Dimensions:  (x: 2, y: 1)
3763         Coordinates:
3764           * y        (y) <U1 'b'
3765         Dimensions without coordinates: x
3766         Data variables:
3767             A        (x, y) float64 -0.3454 0.1734
3768         >>> ds.drop_sel(y="b")
3769         <xarray.Dataset>
3770         Dimensions:  (x: 2, y: 2)
3771         Coordinates:
3772           * y        (y) <U1 'a' 'c'
3773         Dimensions without coordinates: x
3774         Data variables:
3775             A        (x, y) float64 -0.3944 -1.418 1.423 -1.041
3776         """
3777         if errors not in ["raise", "ignore"]:
3778             raise ValueError('errors must be either "raise" or "ignore"')
3779 
3780         labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
3781 
3782         ds = self
3783         for dim, labels_for_dim in labels.items():
3784             # Don't cast to set, as it would harm performance when labels
3785             # is a large numpy array
3786             if utils.is_scalar(labels_for_dim):
3787                 labels_for_dim = [labels_for_dim]
3788             labels_for_dim = np.asarray(labels_for_dim)
3789             try:
3790                 index = self.indexes[dim]
3791             except KeyError:
3792                 raise ValueError("dimension %r does not have coordinate labels" % dim)
3793             new_index = index.drop(labels_for_dim, errors=errors)
3794             ds = ds.loc[{dim: new_index}]
3795         return ds
3796 
3797     def drop_dims(
3798         self, drop_dims: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
3799     ) -> "Dataset":
3800         """Drop dimensions and associated variables from this dataset.
3801 
3802         Parameters
3803         ----------
3804         drop_dims : hashable or iterable of hashable
3805             Dimension or dimensions to drop.
3806         errors: {'raise', 'ignore'}, optional
3807             If 'raise' (default), raises a ValueError error if any of the
3808             dimensions passed are not in the dataset. If 'ignore', any given
3809             labels that are in the dataset are dropped and no error is raised.
3810 
3811         Returns
3812         -------
3813         obj : Dataset
3814             The dataset without the given dimensions (or any variables
3815             containing those dimensions)
3816         errors: {'raise', 'ignore'}, optional
3817             If 'raise' (default), raises a ValueError error if
3818             any of the dimensions passed are not
3819             in the dataset. If 'ignore', any given dimensions that are in the
3820             dataset are dropped and no error is raised.
3821         """
3822         if errors not in ["raise", "ignore"]:
3823             raise ValueError('errors must be either "raise" or "ignore"')
3824 
3825         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):
3826             drop_dims = {drop_dims}
3827         else:
3828             drop_dims = set(drop_dims)
3829 
3830         if errors == "raise":
3831             missing_dims = drop_dims - set(self.dims)
3832             if missing_dims:
3833                 raise ValueError(
3834                     "Dataset does not contain the dimensions: %s" % missing_dims
3835                 )
3836 
3837         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}
3838         return self.drop_vars(drop_vars)
3839 
3840     def transpose(self, *dims: Hashable) -> "Dataset":
3841         """Return a new Dataset object with all array dimensions transposed.
3842 
3843         Although the order of dimensions on each array will change, the dataset
3844         dimensions themselves will remain in fixed (sorted) order.
3845 
3846         Parameters
3847         ----------
3848         *dims : Hashable, optional
3849             By default, reverse the dimensions on each array. Otherwise,
3850             reorder the dimensions to this order.
3851 
3852         Returns
3853         -------
3854         transposed : Dataset
3855             Each array in the dataset (including) coordinates will be
3856             transposed to the given order.
3857 
3858         Notes
3859         -----
3860         This operation returns a view of each array's data. It is
3861         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
3862         -- the data will be fully loaded into memory.
3863 
3864         See Also
3865         --------
3866         numpy.transpose
3867         DataArray.transpose
3868         """
3869         if dims:
3870             if set(dims) ^ set(self.dims) and ... not in dims:
3871                 raise ValueError(
3872                     "arguments to transpose (%s) must be "
3873                     "permuted dataset dimensions (%s)" % (dims, tuple(self.dims))
3874                 )
3875         ds = self.copy()
3876         for name, var in self._variables.items():
3877             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))
3878             ds._variables[name] = var.transpose(*var_dims)
3879         return ds
3880 
3881     def dropna(
3882         self,
3883         dim: Hashable,
3884         how: str = "any",
3885         thresh: int = None,
3886         subset: Iterable[Hashable] = None,
3887     ):
3888         """Returns a new dataset with dropped labels for missing values along
3889         the provided dimension.
3890 
3891         Parameters
3892         ----------
3893         dim : Hashable
3894             Dimension along which to drop missing values. Dropping along
3895             multiple dimensions simultaneously is not yet supported.
3896         how : {'any', 'all'}, optional
3897             * any : if any NA values are present, drop that label
3898             * all : if all values are NA, drop that label
3899         thresh : int, default None
3900             If supplied, require this many non-NA values.
3901         subset : iterable of hashable, optional
3902             Which variables to check for missing values. By default, all
3903             variables in the dataset are checked.
3904 
3905         Returns
3906         -------
3907         Dataset
3908         """
3909         # TODO: consider supporting multiple dimensions? Or not, given that
3910         # there are some ugly edge cases, e.g., pandas's dropna differs
3911         # depending on the order of the supplied axes.
3912 
3913         if dim not in self.dims:
3914             raise ValueError("%s must be a single dataset dimension" % dim)
3915 
3916         if subset is None:
3917             subset = iter(self.data_vars)
3918 
3919         count = np.zeros(self.dims[dim], dtype=np.int64)
3920         size = 0
3921 
3922         for k in subset:
3923             array = self._variables[k]
3924             if dim in array.dims:
3925                 dims = [d for d in array.dims if d != dim]
3926                 count += np.asarray(array.count(dims))  # type: ignore
3927                 size += np.prod([self.dims[d] for d in dims])
3928 
3929         if thresh is not None:
3930             mask = count >= thresh
3931         elif how == "any":
3932             mask = count == size
3933         elif how == "all":
3934             mask = count > 0
3935         elif how is not None:
3936             raise ValueError("invalid how option: %s" % how)
3937         else:
3938             raise TypeError("must specify how or thresh")
3939 
3940         return self.isel({dim: mask})
3941 
3942     def fillna(self, value: Any) -> "Dataset":
3943         """Fill missing values in this object.
3944 
3945         This operation follows the normal broadcasting and alignment rules that
3946         xarray uses for binary arithmetic, except the result is aligned to this
3947         object (``join='left'``) instead of aligned to the intersection of
3948         index coordinates (``join='inner'``).
3949 
3950         Parameters
3951         ----------
3952         value : scalar, ndarray, DataArray, dict or Dataset
3953             Used to fill all matching missing values in this dataset's data
3954             variables. Scalars, ndarrays or DataArrays arguments are used to
3955             fill all data with aligned coordinates (for DataArrays).
3956             Dictionaries or datasets match data variables and then align
3957             coordinates if necessary.
3958 
3959         Returns
3960         -------
3961         Dataset
3962 
3963         Examples
3964         --------
3965 
3966         >>> import numpy as np
3967         >>> import xarray as xr
3968         >>> ds = xr.Dataset(
3969         ...     {
3970         ...         "A": ("x", [np.nan, 2, np.nan, 0]),
3971         ...         "B": ("x", [3, 4, np.nan, 1]),
3972         ...         "C": ("x", [np.nan, np.nan, np.nan, 5]),
3973         ...         "D": ("x", [np.nan, 3, np.nan, 4]),
3974         ...     },
3975         ...     coords={"x": [0, 1, 2, 3]},
3976         ... )
3977         >>> ds
3978         <xarray.Dataset>
3979         Dimensions:  (x: 4)
3980         Coordinates:
3981         * x        (x) int64 0 1 2 3
3982         Data variables:
3983             A        (x) float64 nan 2.0 nan 0.0
3984             B        (x) float64 3.0 4.0 nan 1.0
3985             C        (x) float64 nan nan nan 5.0
3986             D        (x) float64 nan 3.0 nan 4.0
3987 
3988         Replace all `NaN` values with 0s.
3989 
3990         >>> ds.fillna(0)
3991         <xarray.Dataset>
3992         Dimensions:  (x: 4)
3993         Coordinates:
3994         * x        (x) int64 0 1 2 3
3995         Data variables:
3996             A        (x) float64 0.0 2.0 0.0 0.0
3997             B        (x) float64 3.0 4.0 0.0 1.0
3998             C        (x) float64 0.0 0.0 0.0 5.0
3999             D        (x) float64 0.0 3.0 0.0 4.0
4000 
4001         Replace all `NaN` elements in column A, B, C, and D, with 0, 1, 2, and 3 respectively.
4002 
4003         >>> values = {"A": 0, "B": 1, "C": 2, "D": 3}
4004         >>> ds.fillna(value=values)
4005         <xarray.Dataset>
4006         Dimensions:  (x: 4)
4007         Coordinates:
4008         * x        (x) int64 0 1 2 3
4009         Data variables:
4010             A        (x) float64 0.0 2.0 0.0 0.0
4011             B        (x) float64 3.0 4.0 1.0 1.0
4012             C        (x) float64 2.0 2.0 2.0 5.0
4013             D        (x) float64 3.0 3.0 3.0 4.0
4014         """
4015         if utils.is_dict_like(value):
4016             value_keys = getattr(value, "data_vars", value).keys()
4017             if not set(value_keys) <= set(self.data_vars.keys()):
4018                 raise ValueError(
4019                     "all variables in the argument to `fillna` "
4020                     "must be contained in the original dataset"
4021                 )
4022         out = ops.fillna(self, value)
4023         return out
4024 
4025     def interpolate_na(
4026         self,
4027         dim: Hashable = None,
4028         method: str = "linear",
4029         limit: int = None,
4030         use_coordinate: Union[bool, Hashable] = True,
4031         max_gap: Union[
4032             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta
4033         ] = None,
4034         **kwargs: Any,
4035     ) -> "Dataset":
4036         """Fill in NaNs by interpolating according to different methods.
4037 
4038         Parameters
4039         ----------
4040         dim : str
4041             Specifies the dimension along which to interpolate.
4042 
4043         method : str, optional
4044             String indicating which method to use for interpolation:
4045 
4046             - 'linear': linear interpolation (Default). Additional keyword
4047               arguments are passed to :py:func:`numpy.interp`
4048             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
4049               are passed to :py:func:`scipy.interpolate.interp1d`. If
4050               ``method='polynomial'``, the ``order`` keyword argument must also be
4051               provided.
4052             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
4053               respective :py:class:`scipy.interpolate` classes.
4054 
4055         use_coordinate : bool, str, default True
4056             Specifies which index to use as the x values in the interpolation
4057             formulated as `y = f(x)`. If False, values are treated as if
4058             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
4059             used. If ``use_coordinate`` is a string, it specifies the name of a
4060             coordinate variariable to use as the index.
4061         limit : int, default None
4062             Maximum number of consecutive NaNs to fill. Must be greater than 0
4063             or None for no limit. This filling is done regardless of the size of
4064             the gap in the data. To only interpolate over gaps less than a given length,
4065             see ``max_gap``.
4066         max_gap: int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default None.
4067             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
4068             Use None for no limit. When interpolating along a datetime64 dimension
4069             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
4070 
4071             - a string that is valid input for pandas.to_timedelta
4072             - a :py:class:`numpy.timedelta64` object
4073             - a :py:class:`pandas.Timedelta` object
4074             - a :py:class:`datetime.timedelta` object
4075 
4076             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
4077             dimensions has not been implemented yet. Gap length is defined as the difference
4078             between coordinate values at the first data point after a gap and the last value
4079             before a gap. For gaps at the beginning (end), gap length is defined as the difference
4080             between coordinate values at the first (last) valid data point and the first (last) NaN.
4081             For example, consider::
4082 
4083                 <xarray.DataArray (x: 9)>
4084                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
4085                 Coordinates:
4086                   * x        (x) int64 0 1 2 3 4 5 6 7 8
4087 
4088             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
4089         kwargs : dict, optional
4090             parameters passed verbatim to the underlying interpolation function
4091 
4092         Returns
4093         -------
4094         interpolated: Dataset
4095             Filled in Dataset.
4096 
4097         See also
4098         --------
4099         numpy.interp
4100         scipy.interpolate
4101         """
4102         from .missing import interp_na, _apply_over_vars_with_dim
4103 
4104         new = _apply_over_vars_with_dim(
4105             interp_na,
4106             self,
4107             dim=dim,
4108             method=method,
4109             limit=limit,
4110             use_coordinate=use_coordinate,
4111             max_gap=max_gap,
4112             **kwargs,
4113         )
4114         return new
4115 
4116     def ffill(self, dim: Hashable, limit: int = None) -> "Dataset":
4117         """Fill NaN values by propogating values forward
4118 
4119         *Requires bottleneck.*
4120 
4121         Parameters
4122         ----------
4123         dim : Hashable
4124             Specifies the dimension along which to propagate values when
4125             filling.
4126         limit : int, default None
4127             The maximum number of consecutive NaN values to forward fill. In
4128             other words, if there is a gap with more than this number of
4129             consecutive NaNs, it will only be partially filled. Must be greater
4130             than 0 or None for no limit.
4131 
4132         Returns
4133         -------
4134         Dataset
4135         """
4136         from .missing import ffill, _apply_over_vars_with_dim
4137 
4138         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)
4139         return new
4140 
4141     def bfill(self, dim: Hashable, limit: int = None) -> "Dataset":
4142         """Fill NaN values by propogating values backward
4143 
4144         *Requires bottleneck.*
4145 
4146         Parameters
4147         ----------
4148         dim : str
4149             Specifies the dimension along which to propagate values when
4150             filling.
4151         limit : int, default None
4152             The maximum number of consecutive NaN values to backward fill. In
4153             other words, if there is a gap with more than this number of
4154             consecutive NaNs, it will only be partially filled. Must be greater
4155             than 0 or None for no limit.
4156 
4157         Returns
4158         -------
4159         Dataset
4160         """
4161         from .missing import bfill, _apply_over_vars_with_dim
4162 
4163         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)
4164         return new
4165 
4166     def combine_first(self, other: "Dataset") -> "Dataset":
4167         """Combine two Datasets, default to data_vars of self.
4168 
4169         The new coordinates follow the normal broadcasting and alignment rules
4170         of ``join='outer'``.  Vacant cells in the expanded coordinates are
4171         filled with np.nan.
4172 
4173         Parameters
4174         ----------
4175         other : Dataset
4176             Used to fill all matching missing values in this array.
4177 
4178         Returns
4179         -------
4180         Dataset
4181         """
4182         out = ops.fillna(self, other, join="outer", dataset_join="outer")
4183         return out
4184 
4185     def reduce(
4186         self,
4187         func: Callable,
4188         dim: Union[Hashable, Iterable[Hashable]] = None,
4189         keep_attrs: bool = None,
4190         keepdims: bool = False,
4191         numeric_only: bool = False,
4192         allow_lazy: bool = None,
4193         **kwargs: Any,
4194     ) -> "Dataset":
4195         """Reduce this dataset by applying `func` along some dimension(s).
4196 
4197         Parameters
4198         ----------
4199         func : callable
4200             Function which can be called in the form
4201             `f(x, axis=axis, **kwargs)` to return the result of reducing an
4202             np.ndarray over an integer valued axis.
4203         dim : str or sequence of str, optional
4204             Dimension(s) over which to apply `func`.  By default `func` is
4205             applied over all dimensions.
4206         keep_attrs : bool, optional
4207             If True, the dataset's attributes (`attrs`) will be copied from
4208             the original object to the new one.  If False (default), the new
4209             object will be returned without attributes.
4210         keepdims : bool, default False
4211             If True, the dimensions which are reduced are left in the result
4212             as dimensions of size one. Coordinates that use these dimensions
4213             are removed.
4214         numeric_only : bool, optional
4215             If True, only apply ``func`` to variables with a numeric dtype.
4216         **kwargs : Any
4217             Additional keyword arguments passed on to ``func``.
4218 
4219         Returns
4220         -------
4221         reduced : Dataset
4222             Dataset with this object's DataArrays replaced with new DataArrays
4223             of summarized data and the indicated dimension(s) removed.
4224         """
4225         if dim is None or dim is ...:
4226             dims = set(self.dims)
4227         elif isinstance(dim, str) or not isinstance(dim, Iterable):
4228             dims = {dim}
4229         else:
4230             dims = set(dim)
4231 
4232         missing_dimensions = [d for d in dims if d not in self.dims]
4233         if missing_dimensions:
4234             raise ValueError(
4235                 "Dataset does not contain the dimensions: %s" % missing_dimensions
4236             )
4237 
4238         if keep_attrs is None:
4239             keep_attrs = _get_keep_attrs(default=False)
4240 
4241         variables: Dict[Hashable, Variable] = {}
4242         for name, var in self._variables.items():
4243             reduce_dims = [d for d in var.dims if d in dims]
4244             if name in self.coords:
4245                 if not reduce_dims:
4246                     variables[name] = var
4247             else:
4248                 if (
4249                     not numeric_only
4250                     or np.issubdtype(var.dtype, np.number)
4251                     or (var.dtype == np.bool_)
4252                 ):
4253                     if len(reduce_dims) == 1:
4254                         # unpack dimensions for the benefit of functions
4255                         # like np.argmin which can't handle tuple arguments
4256                         (reduce_dims,) = reduce_dims
4257                     elif len(reduce_dims) == var.ndim:
4258                         # prefer to aggregate over axis=None rather than
4259                         # axis=(0, 1) if they will be equivalent, because
4260                         # the former is often more efficient
4261                         reduce_dims = None  # type: ignore
4262                     variables[name] = var.reduce(
4263                         func,
4264                         dim=reduce_dims,
4265                         keep_attrs=keep_attrs,
4266                         keepdims=keepdims,
4267                         allow_lazy=allow_lazy,
4268                         **kwargs,
4269                     )
4270 
4271         coord_names = {k for k in self.coords if k in variables}
4272         indexes = {k: v for k, v in self.indexes.items() if k in variables}
4273         attrs = self.attrs if keep_attrs else None
4274         return self._replace_with_new_dims(
4275             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
4276         )
4277 
4278     def map(
4279         self,
4280         func: Callable,
4281         keep_attrs: bool = None,
4282         args: Iterable[Any] = (),
4283         **kwargs: Any,
4284     ) -> "Dataset":
4285         """Apply a function to each variable in this dataset
4286 
4287         Parameters
4288         ----------
4289         func : callable
4290             Function which can be called in the form `func(x, *args, **kwargs)`
4291             to transform each DataArray `x` in this dataset into another
4292             DataArray.
4293         keep_attrs : bool, optional
4294             If True, the dataset's attributes (`attrs`) will be copied from
4295             the original object to the new one. If False, the new object will
4296             be returned without attributes.
4297         args : tuple, optional
4298             Positional arguments passed on to `func`.
4299         **kwargs : Any
4300             Keyword arguments passed on to `func`.
4301 
4302         Returns
4303         -------
4304         applied : Dataset
4305             Resulting dataset from applying ``func`` to each data variable.
4306 
4307         Examples
4308         --------
4309         >>> da = xr.DataArray(np.random.randn(2, 3))
4310         >>> ds = xr.Dataset({"foo": da, "bar": ("x", [-1, 2])})
4311         >>> ds
4312         <xarray.Dataset>
4313         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
4314         Dimensions without coordinates: dim_0, dim_1, x
4315         Data variables:
4316             foo      (dim_0, dim_1) float64 -0.3751 -1.951 -1.945 0.2948 0.711 -0.3948
4317             bar      (x) int64 -1 2
4318         >>> ds.map(np.fabs)
4319         <xarray.Dataset>
4320         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
4321         Dimensions without coordinates: dim_0, dim_1, x
4322         Data variables:
4323             foo      (dim_0, dim_1) float64 0.3751 1.951 1.945 0.2948 0.711 0.3948
4324             bar      (x) float64 1.0 2.0
4325         """
4326         variables = {
4327             k: maybe_wrap_array(v, func(v, *args, **kwargs))
4328             for k, v in self.data_vars.items()
4329         }
4330         if keep_attrs is None:
4331             keep_attrs = _get_keep_attrs(default=False)
4332         attrs = self.attrs if keep_attrs else None
4333         return type(self)(variables, attrs=attrs)
4334 
4335     def apply(
4336         self,
4337         func: Callable,
4338         keep_attrs: bool = None,
4339         args: Iterable[Any] = (),
4340         **kwargs: Any,
4341     ) -> "Dataset":
4342         """
4343         Backward compatible implementation of ``map``
4344 
4345         See Also
4346         --------
4347         Dataset.map
4348         """
4349         warnings.warn(
4350             "Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged",
4351             PendingDeprecationWarning,
4352             stacklevel=2,
4353         )
4354         return self.map(func, keep_attrs, args, **kwargs)
4355 
4356     def assign(
4357         self, variables: Mapping[Hashable, Any] = None, **variables_kwargs: Hashable
4358     ) -> "Dataset":
4359         """Assign new data variables to a Dataset, returning a new object
4360         with all the original variables in addition to the new ones.
4361 
4362         Parameters
4363         ----------
4364         variables : mapping, value pairs
4365             Mapping from variables names to the new values. If the new values
4366             are callable, they are computed on the Dataset and assigned to new
4367             data variables. If the values are not callable, (e.g. a DataArray,
4368             scalar, or array), they are simply assigned.
4369         **variables_kwargs:
4370             The keyword arguments form of ``variables``.
4371             One of variables or variables_kwargs must be provided.
4372 
4373         Returns
4374         -------
4375         ds : Dataset
4376             A new Dataset with the new variables in addition to all the
4377             existing variables.
4378 
4379         Notes
4380         -----
4381         Since ``kwargs`` is a dictionary, the order of your arguments may not
4382         be preserved, and so the order of the new variables is not well
4383         defined. Assigning multiple variables within the same ``assign`` is
4384         possible, but you cannot reference other variables created within the
4385         same ``assign`` call.
4386 
4387         See Also
4388         --------
4389         pandas.DataFrame.assign
4390 
4391         Examples
4392         --------
4393         >>> import numpy as np
4394         >>> import xarray as xr
4395         >>> x = xr.Dataset(
4396         ...     {
4397         ...         "temperature_c": (
4398         ...             ("lat", "lon"),
4399         ...             20 * np.random.rand(4).reshape(2, 2),
4400         ...         ),
4401         ...         "precipitation": (("lat", "lon"), np.random.rand(4).reshape(2, 2)),
4402         ...     },
4403         ...     coords={"lat": [10, 20], "lon": [150, 160]},
4404         ... )
4405         >>> x
4406         <xarray.Dataset>
4407         Dimensions:        (lat: 2, lon: 2)
4408         Coordinates:
4409         * lat            (lat) int64 10 20
4410         * lon            (lon) int64 150 160
4411         Data variables:
4412             temperature_c  (lat, lon) float64 18.04 12.51 17.64 9.313
4413             precipitation  (lat, lon) float64 0.4751 0.6827 0.3697 0.03524
4414 
4415         Where the value is a callable, evaluated on dataset:
4416 
4417         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)
4418         <xarray.Dataset>
4419         Dimensions:        (lat: 2, lon: 2)
4420         Coordinates:
4421         * lat            (lat) int64 10 20
4422         * lon            (lon) int64 150 160
4423         Data variables:
4424             temperature_c  (lat, lon) float64 18.04 12.51 17.64 9.313
4425             precipitation  (lat, lon) float64 0.4751 0.6827 0.3697 0.03524
4426             temperature_f  (lat, lon) float64 64.47 54.51 63.75 48.76
4427 
4428         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:
4429 
4430         >>> x.assign(temperature_f=x["temperature_c"] * 9 / 5 + 32)
4431         <xarray.Dataset>
4432         Dimensions:        (lat: 2, lon: 2)
4433         Coordinates:
4434         * lat            (lat) int64 10 20
4435         * lon            (lon) int64 150 160
4436         Data variables:
4437             temperature_c  (lat, lon) float64 18.04 12.51 17.64 9.313
4438             precipitation  (lat, lon) float64 0.4751 0.6827 0.3697 0.03524
4439             temperature_f  (lat, lon) float64 64.47 54.51 63.75 48.76
4440 
4441         """
4442         variables = either_dict_or_kwargs(variables, variables_kwargs, "assign")
4443         data = self.copy()
4444         # do all calculations first...
4445         results = data._calc_assign_results(variables)
4446         # ... and then assign
4447         data.update(results)
4448         return data
4449 
4450     def to_array(self, dim="variable", name=None):
4451         """Convert this dataset into an xarray.DataArray
4452 
4453         The data variables of this dataset will be broadcast against each other
4454         and stacked along the first axis of the new array. All coordinates of
4455         this dataset will remain coordinates.
4456 
4457         Parameters
4458         ----------
4459         dim : str, optional
4460             Name of the new dimension.
4461         name : str, optional
4462             Name of the new data array.
4463 
4464         Returns
4465         -------
4466         array : xarray.DataArray
4467         """
4468         from .dataarray import DataArray
4469 
4470         data_vars = [self.variables[k] for k in self.data_vars]
4471         broadcast_vars = broadcast_variables(*data_vars)
4472         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)
4473 
4474         coords = dict(self.coords)
4475         coords[dim] = list(self.data_vars)
4476         indexes = propagate_indexes(self._indexes)
4477 
4478         dims = (dim,) + broadcast_vars[0].dims
4479 
4480         return DataArray(
4481             data, coords, dims, attrs=self.attrs, name=name, indexes=indexes
4482         )
4483 
4484     def _to_dataframe(self, ordered_dims):
4485         columns = [k for k in self.variables if k not in self.dims]
4486         data = [
4487             self._variables[k].set_dims(ordered_dims).values.reshape(-1)
4488             for k in columns
4489         ]
4490         index = self.coords.to_index(ordered_dims)
4491         return pd.DataFrame(dict(zip(columns, data)), index=index)
4492 
4493     def to_dataframe(self):
4494         """Convert this dataset into a pandas.DataFrame.
4495 
4496         Non-index variables in this dataset form the columns of the
4497         DataFrame. The DataFrame is be indexed by the Cartesian product of
4498         this dataset's indices.
4499         """
4500         return self._to_dataframe(self.dims)
4501 
4502     def _set_sparse_data_from_dataframe(
4503         self, dataframe: pd.DataFrame, dims: tuple
4504     ) -> None:
4505         from sparse import COO
4506 
4507         idx = dataframe.index
4508         if isinstance(idx, pd.MultiIndex):
4509             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)
4510             is_sorted = idx.is_lexsorted
4511             shape = tuple(lev.size for lev in idx.levels)
4512         else:
4513             coords = np.arange(idx.size).reshape(1, -1)
4514             is_sorted = True
4515             shape = (idx.size,)
4516 
4517         for name, series in dataframe.items():
4518             # Cast to a NumPy array first, in case the Series is a pandas
4519             # Extension array (which doesn't have a valid NumPy dtype)
4520             values = np.asarray(series)
4521 
4522             # In virtually all real use cases, the sparse array will now have
4523             # missing values and needs a fill_value. For consistency, don't
4524             # special case the rare exceptions (e.g., dtype=int without a
4525             # MultiIndex).
4526             dtype, fill_value = dtypes.maybe_promote(values.dtype)
4527             values = np.asarray(values, dtype=dtype)
4528 
4529             data = COO(
4530                 coords,
4531                 values,
4532                 shape,
4533                 has_duplicates=False,
4534                 sorted=is_sorted,
4535                 fill_value=fill_value,
4536             )
4537             self[name] = (dims, data)
4538 
4539     def _set_numpy_data_from_dataframe(
4540         self, dataframe: pd.DataFrame, dims: tuple
4541     ) -> None:
4542         idx = dataframe.index
4543         if isinstance(idx, pd.MultiIndex):
4544             # expand the DataFrame to include the product of all levels
4545             full_idx = pd.MultiIndex.from_product(idx.levels, names=idx.names)
4546             dataframe = dataframe.reindex(full_idx)
4547             shape = tuple(lev.size for lev in idx.levels)
4548         else:
4549             shape = (idx.size,)
4550         for name, series in dataframe.items():
4551             data = np.asarray(series).reshape(shape)
4552             self[name] = (dims, data)
4553 
4554     @classmethod
4555     def from_dataframe(cls, dataframe: pd.DataFrame, sparse: bool = False) -> "Dataset":
4556         """Convert a pandas.DataFrame into an xarray.Dataset
4557 
4558         Each column will be converted into an independent variable in the
4559         Dataset. If the dataframe's index is a MultiIndex, it will be expanded
4560         into a tensor product of one-dimensional indices (filling in missing
4561         values with NaN). This method will produce a Dataset very similar to
4562         that on which the 'to_dataframe' method was called, except with
4563         possibly redundant dimensions (since all dataset variables will have
4564         the same dimensionality)
4565 
4566         Parameters
4567         ----------
4568         dataframe : pandas.DataFrame
4569             DataFrame from which to copy data and indices.
4570         sparse : bool
4571             If true, create a sparse arrays instead of dense numpy arrays. This
4572             can potentially save a large amount of memory if the DataFrame has
4573             a MultiIndex. Requires the sparse package (sparse.pydata.org).
4574 
4575         Returns
4576         -------
4577         New Dataset.
4578 
4579         See also
4580         --------
4581         xarray.DataArray.from_series
4582         """
4583         # TODO: Add an option to remove dimensions along which the variables
4584         # are constant, to enable consistent serialization to/from a dataframe,
4585         # even if some variables have different dimensionality.
4586 
4587         if not dataframe.columns.is_unique:
4588             raise ValueError("cannot convert DataFrame with non-unique columns")
4589 
4590         idx = remove_unused_levels_categories(dataframe.index)
4591         dataframe = dataframe.set_index(idx)
4592         obj = cls()
4593 
4594         if isinstance(idx, pd.MultiIndex):
4595             dims = tuple(
4596                 name if name is not None else "level_%i" % n
4597                 for n, name in enumerate(idx.names)
4598             )
4599             for dim, lev in zip(dims, idx.levels):
4600                 obj[dim] = (dim, lev)
4601         else:
4602             index_name = idx.name if idx.name is not None else "index"
4603             dims = (index_name,)
4604             obj[index_name] = (dims, idx)
4605 
4606         if sparse:
4607             obj._set_sparse_data_from_dataframe(dataframe, dims)
4608         else:
4609             obj._set_numpy_data_from_dataframe(dataframe, dims)
4610         return obj
4611 
4612     def to_dask_dataframe(self, dim_order=None, set_index=False):
4613         """
4614         Convert this dataset into a dask.dataframe.DataFrame.
4615 
4616         The dimensions, coordinates and data variables in this dataset form
4617         the columns of the DataFrame.
4618 
4619         Parameters
4620         ----------
4621         dim_order : list, optional
4622             Hierarchical dimension order for the resulting dataframe. All
4623             arrays are transposed to this order and then written out as flat
4624             vectors in contiguous order, so the last dimension in this list
4625             will be contiguous in the resulting DataFrame. This has a major
4626             influence on which operations are efficient on the resulting dask
4627             dataframe.
4628 
4629             If provided, must include all dimensions on this dataset. By
4630             default, dimensions are sorted alphabetically.
4631         set_index : bool, optional
4632             If set_index=True, the dask DataFrame is indexed by this dataset's
4633             coordinate. Since dask DataFrames to not support multi-indexes,
4634             set_index only works if the dataset only contains one dimension.
4635 
4636         Returns
4637         -------
4638         dask.dataframe.DataFrame
4639         """
4640 
4641         import dask.array as da
4642         import dask.dataframe as dd
4643 
4644         if dim_order is None:
4645             dim_order = list(self.dims)
4646         elif set(dim_order) != set(self.dims):
4647             raise ValueError(
4648                 "dim_order {} does not match the set of dimensions on this "
4649                 "Dataset: {}".format(dim_order, list(self.dims))
4650             )
4651 
4652         ordered_dims = {k: self.dims[k] for k in dim_order}
4653 
4654         columns = list(ordered_dims)
4655         columns.extend(k for k in self.coords if k not in self.dims)
4656         columns.extend(self.data_vars)
4657 
4658         series_list = []
4659         for name in columns:
4660             try:
4661                 var = self.variables[name]
4662             except KeyError:
4663                 # dimension without a matching coordinate
4664                 size = self.dims[name]
4665                 data = da.arange(size, chunks=size, dtype=np.int64)
4666                 var = Variable((name,), data)
4667 
4668             # IndexVariable objects have a dummy .chunk() method
4669             if isinstance(var, IndexVariable):
4670                 var = var.to_base_variable()
4671 
4672             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data
4673             series = dd.from_array(dask_array.reshape(-1), columns=[name])
4674             series_list.append(series)
4675 
4676         df = dd.concat(series_list, axis=1)
4677 
4678         if set_index:
4679             if len(dim_order) == 1:
4680                 (dim,) = dim_order
4681                 df = df.set_index(dim)
4682             else:
4683                 # triggers an error about multi-indexes, even if only one
4684                 # dimension is passed
4685                 df = df.set_index(dim_order)
4686 
4687         return df
4688 
4689     def to_dict(self, data=True):
4690         """
4691         Convert this dataset to a dictionary following xarray naming
4692         conventions.
4693 
4694         Converts all variables and attributes to native Python objects
4695         Useful for converting to json. To avoid datetime incompatibility
4696         use decode_times=False kwarg in xarrray.open_dataset.
4697 
4698         Parameters
4699         ----------
4700         data : bool, optional
4701             Whether to include the actual data in the dictionary. When set to
4702             False, returns just the schema.
4703 
4704         See also
4705         --------
4706         Dataset.from_dict
4707         """
4708         d = {
4709             "coords": {},
4710             "attrs": decode_numpy_dict_values(self.attrs),
4711             "dims": dict(self.dims),
4712             "data_vars": {},
4713         }
4714         for k in self.coords:
4715             d["coords"].update({k: self[k].variable.to_dict(data=data)})
4716         for k in self.data_vars:
4717             d["data_vars"].update({k: self[k].variable.to_dict(data=data)})
4718         return d
4719 
4720     @classmethod
4721     def from_dict(cls, d):
4722         """
4723         Convert a dictionary into an xarray.Dataset.
4724 
4725         Input dict can take several forms::
4726 
4727             d = {'t': {'dims': ('t'), 'data': t},
4728                  'a': {'dims': ('t'), 'data': x},
4729                  'b': {'dims': ('t'), 'data': y}}
4730 
4731             d = {'coords': {'t': {'dims': 't', 'data': t,
4732                                   'attrs': {'units':'s'}}},
4733                  'attrs': {'title': 'air temperature'},
4734                  'dims': 't',
4735                  'data_vars': {'a': {'dims': 't', 'data': x, },
4736                                'b': {'dims': 't', 'data': y}}}
4737 
4738         where 't' is the name of the dimesion, 'a' and 'b' are names of data
4739         variables and t, x, and y are lists, numpy.arrays or pandas objects.
4740 
4741         Parameters
4742         ----------
4743         d : dict, with a minimum structure of {'var_0': {'dims': [..], \
4744                                                          'data': [..]}, \
4745                                                ...}
4746 
4747         Returns
4748         -------
4749         obj : xarray.Dataset
4750 
4751         See also
4752         --------
4753         Dataset.to_dict
4754         DataArray.from_dict
4755         """
4756 
4757         if not {"coords", "data_vars"}.issubset(set(d)):
4758             variables = d.items()
4759         else:
4760             import itertools
4761 
4762             variables = itertools.chain(
4763                 d.get("coords", {}).items(), d.get("data_vars", {}).items()
4764             )
4765         try:
4766             variable_dict = {
4767                 k: (v["dims"], v["data"], v.get("attrs")) for k, v in variables
4768             }
4769         except KeyError as e:
4770             raise ValueError(
4771                 "cannot convert dict without the key "
4772                 "'{dims_data}'".format(dims_data=str(e.args[0]))
4773             )
4774         obj = cls(variable_dict)
4775 
4776         # what if coords aren't dims?
4777         coords = set(d.get("coords", {})) - set(d.get("dims", {}))
4778         obj = obj.set_coords(coords)
4779 
4780         obj.attrs.update(d.get("attrs", {}))
4781 
4782         return obj
4783 
4784     @staticmethod
4785     def _unary_op(f, keep_attrs=False):
4786         @functools.wraps(f)
4787         def func(self, *args, **kwargs):
4788             variables = {}
4789             for k, v in self._variables.items():
4790                 if k in self._coord_names:
4791                     variables[k] = v
4792                 else:
4793                     variables[k] = f(v, *args, **kwargs)
4794             attrs = self._attrs if keep_attrs else None
4795             return self._replace_with_new_dims(variables, attrs=attrs)
4796 
4797         return func
4798 
4799     @staticmethod
4800     def _binary_op(f, reflexive=False, join=None):
4801         @functools.wraps(f)
4802         def func(self, other):
4803             from .dataarray import DataArray
4804 
4805             if isinstance(other, groupby.GroupBy):
4806                 return NotImplemented
4807             align_type = OPTIONS["arithmetic_join"] if join is None else join
4808             if isinstance(other, (DataArray, Dataset)):
4809                 self, other = align(self, other, join=align_type, copy=False)
4810             g = f if not reflexive else lambda x, y: f(y, x)
4811             ds = self._calculate_binary_op(g, other, join=align_type)
4812             return ds
4813 
4814         return func
4815 
4816     @staticmethod
4817     def _inplace_binary_op(f):
4818         @functools.wraps(f)
4819         def func(self, other):
4820             from .dataarray import DataArray
4821 
4822             if isinstance(other, groupby.GroupBy):
4823                 raise TypeError(
4824                     "in-place operations between a Dataset and "
4825                     "a grouped object are not permitted"
4826                 )
4827             # we don't actually modify arrays in-place with in-place Dataset
4828             # arithmetic -- this lets us automatically align things
4829             if isinstance(other, (DataArray, Dataset)):
4830                 other = other.reindex_like(self, copy=False)
4831             g = ops.inplace_to_noninplace_op(f)
4832             ds = self._calculate_binary_op(g, other, inplace=True)
4833             self._replace_with_new_dims(
4834                 ds._variables,
4835                 ds._coord_names,
4836                 attrs=ds._attrs,
4837                 indexes=ds._indexes,
4838                 inplace=True,
4839             )
4840             return self
4841 
4842         return func
4843 
4844     def _calculate_binary_op(self, f, other, join="inner", inplace=False):
4845         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):
4846             if inplace and set(lhs_data_vars) != set(rhs_data_vars):
4847                 raise ValueError(
4848                     "datasets must have the same data variables "
4849                     "for in-place arithmetic operations: %s, %s"
4850                     % (list(lhs_data_vars), list(rhs_data_vars))
4851                 )
4852 
4853             dest_vars = {}
4854 
4855             for k in lhs_data_vars:
4856                 if k in rhs_data_vars:
4857                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])
4858                 elif join in ["left", "outer"]:
4859                     dest_vars[k] = f(lhs_vars[k], np.nan)
4860             for k in rhs_data_vars:
4861                 if k not in dest_vars and join in ["right", "outer"]:
4862                     dest_vars[k] = f(rhs_vars[k], np.nan)
4863             return dest_vars
4864 
4865         if utils.is_dict_like(other) and not isinstance(other, Dataset):
4866             # can't use our shortcut of doing the binary operation with
4867             # Variable objects, so apply over our data vars instead.
4868             new_data_vars = apply_over_both(
4869                 self.data_vars, other, self.data_vars, other
4870             )
4871             return Dataset(new_data_vars)
4872 
4873         other_coords = getattr(other, "coords", None)
4874         ds = self.coords.merge(other_coords)
4875 
4876         if isinstance(other, Dataset):
4877             new_vars = apply_over_both(
4878                 self.data_vars, other.data_vars, self.variables, other.variables
4879             )
4880         else:
4881             other_variable = getattr(other, "variable", other)
4882             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}
4883         ds._variables.update(new_vars)
4884         ds._dims = calculate_dimensions(ds._variables)
4885         return ds
4886 
4887     def _copy_attrs_from(self, other):
4888         self.attrs = other.attrs
4889         for v in other.variables:
4890             if v in self.variables:
4891                 self.variables[v].attrs = other.variables[v].attrs
4892 
4893     def diff(self, dim, n=1, label="upper"):
4894         """Calculate the n-th order discrete difference along given axis.
4895 
4896         Parameters
4897         ----------
4898         dim : str, optional
4899             Dimension over which to calculate the finite difference.
4900         n : int, optional
4901             The number of times values are differenced.
4902         label : str, optional
4903             The new coordinate in dimension ``dim`` will have the
4904             values of either the minuend's or subtrahend's coordinate
4905             for values 'upper' and 'lower', respectively.  Other
4906             values are not supported.
4907 
4908         Returns
4909         -------
4910         difference : same type as caller
4911             The n-th order finite difference of this object.
4912 
4913         .. note::
4914 
4915             `n` matches numpy's behavior and is different from pandas' first
4916             argument named `periods`.
4917 
4918         Examples
4919         --------
4920         >>> ds = xr.Dataset({"foo": ("x", [5, 5, 6, 6])})
4921         >>> ds.diff("x")
4922         <xarray.Dataset>
4923         Dimensions:  (x: 3)
4924         Coordinates:
4925           * x        (x) int64 1 2 3
4926         Data variables:
4927             foo      (x) int64 0 1 0
4928         >>> ds.diff("x", 2)
4929         <xarray.Dataset>
4930         Dimensions:  (x: 2)
4931         Coordinates:
4932         * x        (x) int64 2 3
4933         Data variables:
4934         foo      (x) int64 1 -1
4935 
4936         See Also
4937         --------
4938         Dataset.differentiate
4939         """
4940         if n == 0:
4941             return self
4942         if n < 0:
4943             raise ValueError(f"order `n` must be non-negative but got {n}")
4944 
4945         # prepare slices
4946         kwargs_start = {dim: slice(None, -1)}
4947         kwargs_end = {dim: slice(1, None)}
4948 
4949         # prepare new coordinate
4950         if label == "upper":
4951             kwargs_new = kwargs_end
4952         elif label == "lower":
4953             kwargs_new = kwargs_start
4954         else:
4955             raise ValueError(
4956                 "The 'label' argument has to be either " "'upper' or 'lower'"
4957             )
4958 
4959         variables = {}
4960 
4961         for name, var in self.variables.items():
4962             if dim in var.dims:
4963                 if name in self.data_vars:
4964                     variables[name] = var.isel(**kwargs_end) - var.isel(**kwargs_start)
4965                 else:
4966                     variables[name] = var.isel(**kwargs_new)
4967             else:
4968                 variables[name] = var
4969 
4970         indexes = dict(self.indexes)
4971         if dim in indexes:
4972             indexes[dim] = indexes[dim][kwargs_new[dim]]
4973 
4974         difference = self._replace_with_new_dims(variables, indexes=indexes)
4975 
4976         if n > 1:
4977             return difference.diff(dim, n - 1)
4978         else:
4979             return difference
4980 
4981     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):
4982         """Shift this dataset by an offset along one or more dimensions.
4983 
4984         Only data variables are moved; coordinates stay in place. This is
4985         consistent with the behavior of ``shift`` in pandas.
4986 
4987         Parameters
4988         ----------
4989         shifts : Mapping with the form of {dim: offset}
4990             Integer offset to shift along each of the given dimensions.
4991             Positive offsets shift to the right; negative offsets shift to the
4992             left.
4993         fill_value: scalar, optional
4994             Value to use for newly missing values
4995         **shifts_kwargs:
4996             The keyword arguments form of ``shifts``.
4997             One of shifts or shifts_kwargs must be provided.
4998 
4999         Returns
5000         -------
5001         shifted : Dataset
5002             Dataset with the same coordinates and attributes but shifted data
5003             variables.
5004 
5005         See also
5006         --------
5007         roll
5008 
5009         Examples
5010         --------
5011 
5012         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
5013         >>> ds.shift(x=2)
5014         <xarray.Dataset>
5015         Dimensions:  (x: 5)
5016         Coordinates:
5017           * x        (x) int64 0 1 2 3 4
5018         Data variables:
5019             foo      (x) object nan nan 'a' 'b' 'c'
5020         """
5021         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
5022         invalid = [k for k in shifts if k not in self.dims]
5023         if invalid:
5024             raise ValueError("dimensions %r do not exist" % invalid)
5025 
5026         variables = {}
5027         for name, var in self.variables.items():
5028             if name in self.data_vars:
5029                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}
5030                 variables[name] = var.shift(fill_value=fill_value, shifts=var_shifts)
5031             else:
5032                 variables[name] = var
5033 
5034         return self._replace(variables)
5035 
5036     def roll(self, shifts=None, roll_coords=None, **shifts_kwargs):
5037         """Roll this dataset by an offset along one or more dimensions.
5038 
5039         Unlike shift, roll may rotate all variables, including coordinates
5040         if specified. The direction of rotation is consistent with
5041         :py:func:`numpy.roll`.
5042 
5043         Parameters
5044         ----------
5045 
5046         shifts : dict, optional
5047             A dict with keys matching dimensions and values given
5048             by integers to rotate each of the given dimensions. Positive
5049             offsets roll to the right; negative offsets roll to the left.
5050         roll_coords : bool
5051             Indicates whether to  roll the coordinates by the offset
5052             The current default of roll_coords (None, equivalent to True) is
5053             deprecated and will change to False in a future version.
5054             Explicitly pass roll_coords to silence the warning.
5055         **shifts_kwargs : {dim: offset, ...}, optional
5056             The keyword arguments form of ``shifts``.
5057             One of shifts or shifts_kwargs must be provided.
5058         Returns
5059         -------
5060         rolled : Dataset
5061             Dataset with the same coordinates and attributes but rolled
5062             variables.
5063 
5064         See also
5065         --------
5066         shift
5067 
5068         Examples
5069         --------
5070 
5071         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
5072         >>> ds.roll(x=2)
5073         <xarray.Dataset>
5074         Dimensions:  (x: 5)
5075         Coordinates:
5076           * x        (x) int64 3 4 0 1 2
5077         Data variables:
5078             foo      (x) object 'd' 'e' 'a' 'b' 'c'
5079         """
5080         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
5081         invalid = [k for k in shifts if k not in self.dims]
5082         if invalid:
5083             raise ValueError("dimensions %r do not exist" % invalid)
5084 
5085         if roll_coords is None:
5086             warnings.warn(
5087                 "roll_coords will be set to False in the future."
5088                 " Explicitly set roll_coords to silence warning.",
5089                 FutureWarning,
5090                 stacklevel=2,
5091             )
5092             roll_coords = True
5093 
5094         unrolled_vars = () if roll_coords else self.coords
5095 
5096         variables = {}
5097         for k, v in self.variables.items():
5098             if k not in unrolled_vars:
5099                 variables[k] = v.roll(
5100                     **{k: s for k, s in shifts.items() if k in v.dims}
5101                 )
5102             else:
5103                 variables[k] = v
5104 
5105         if roll_coords:
5106             indexes = {}
5107             for k, v in self.indexes.items():
5108                 (dim,) = self.variables[k].dims
5109                 if dim in shifts:
5110                     indexes[k] = roll_index(v, shifts[dim])
5111                 else:
5112                     indexes[k] = v
5113         else:
5114             indexes = dict(self.indexes)
5115 
5116         return self._replace(variables, indexes=indexes)
5117 
5118     def sortby(self, variables, ascending=True):
5119         """
5120         Sort object by labels or values (along an axis).
5121 
5122         Sorts the dataset, either along specified dimensions,
5123         or according to values of 1-D dataarrays that share dimension
5124         with calling object.
5125 
5126         If the input variables are dataarrays, then the dataarrays are aligned
5127         (via left-join) to the calling object prior to sorting by cell values.
5128         NaNs are sorted to the end, following Numpy convention.
5129 
5130         If multiple sorts along the same dimension is
5131         given, numpy's lexsort is performed along that dimension:
5132         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
5133         and the FIRST key in the sequence is used as the primary sort key,
5134         followed by the 2nd key, etc.
5135 
5136         Parameters
5137         ----------
5138         variables: str, DataArray, or list of either
5139             1D DataArray objects or name(s) of 1D variable(s) in
5140             coords/data_vars whose values are used to sort the dataset.
5141         ascending: boolean, optional
5142             Whether to sort by ascending or descending order.
5143 
5144         Returns
5145         -------
5146         sorted: Dataset
5147             A new dataset where all the specified dims are sorted by dim
5148             labels.
5149         """
5150         from .dataarray import DataArray
5151 
5152         if not isinstance(variables, list):
5153             variables = [variables]
5154         else:
5155             variables = variables
5156         variables = [v if isinstance(v, DataArray) else self[v] for v in variables]
5157         aligned_vars = align(self, *variables, join="left")
5158         aligned_self = aligned_vars[0]
5159         aligned_other_vars = aligned_vars[1:]
5160         vars_by_dim = defaultdict(list)
5161         for data_array in aligned_other_vars:
5162             if data_array.ndim != 1:
5163                 raise ValueError("Input DataArray is not 1-D.")
5164             (key,) = data_array.dims
5165             vars_by_dim[key].append(data_array)
5166 
5167         indices = {}
5168         for key, arrays in vars_by_dim.items():
5169             order = np.lexsort(tuple(reversed(arrays)))
5170             indices[key] = order if ascending else order[::-1]
5171         return aligned_self.isel(**indices)
5172 
5173     def quantile(
5174         self,
5175         q,
5176         dim=None,
5177         interpolation="linear",
5178         numeric_only=False,
5179         keep_attrs=None,
5180         skipna=True,
5181     ):
5182         """Compute the qth quantile of the data along the specified dimension.
5183 
5184         Returns the qth quantiles(s) of the array elements for each variable
5185         in the Dataset.
5186 
5187         Parameters
5188         ----------
5189         q : float in range of [0,1] or array-like of floats
5190             Quantile to compute, which must be between 0 and 1 inclusive.
5191         dim : str or sequence of str, optional
5192             Dimension(s) over which to apply quantile.
5193         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
5194             This optional parameter specifies the interpolation method to
5195             use when the desired quantile lies between two data points
5196             ``i < j``:
5197 
5198                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is
5199                   the fractional part of the index surrounded by ``i`` and
5200                   ``j``.
5201                 * lower: ``i``.
5202                 * higher: ``j``.
5203                 * nearest: ``i`` or ``j``, whichever is nearest.
5204                 * midpoint: ``(i + j) / 2``.
5205         keep_attrs : bool, optional
5206             If True, the dataset's attributes (`attrs`) will be copied from
5207             the original object to the new one.  If False (default), the new
5208             object will be returned without attributes.
5209         numeric_only : bool, optional
5210             If True, only apply ``func`` to variables with a numeric dtype.
5211         skipna : bool, optional
5212             Whether to skip missing values when aggregating.
5213 
5214         Returns
5215         -------
5216         quantiles : Dataset
5217             If `q` is a single quantile, then the result is a scalar for each
5218             variable in data_vars. If multiple percentiles are given, first
5219             axis of the result corresponds to the quantile and a quantile
5220             dimension is added to the return Dataset. The other dimensions are
5221             the dimensions that remain after the reduction of the array.
5222 
5223         See Also
5224         --------
5225         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile
5226 
5227         Examples
5228         --------
5229 
5230         >>> ds = xr.Dataset(
5231         ...     {"a": (("x", "y"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},
5232         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
5233         ... )
5234         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)
5235         <xarray.Dataset>
5236         Dimensions:   ()
5237         Coordinates:
5238             quantile  float64 0.0
5239         Data variables:
5240             a         float64 0.7
5241         >>> ds.quantile(0, dim="x")
5242         <xarray.Dataset>
5243         Dimensions:   (y: 4)
5244         Coordinates:
5245           * y         (y) float64 1.0 1.5 2.0 2.5
5246             quantile  float64 0.0
5247         Data variables:
5248             a         (y) float64 0.7 4.2 2.6 1.5
5249         >>> ds.quantile([0, 0.5, 1])
5250         <xarray.Dataset>
5251         Dimensions:   (quantile: 3)
5252         Coordinates:
5253           * quantile  (quantile) float64 0.0 0.5 1.0
5254         Data variables:
5255             a         (quantile) float64 0.7 3.4 9.4
5256         >>> ds.quantile([0, 0.5, 1], dim="x")
5257         <xarray.Dataset>
5258         Dimensions:   (quantile: 3, y: 4)
5259         Coordinates:
5260           * y         (y) float64 1.0 1.5 2.0 2.5
5261           * quantile  (quantile) float64 0.0 0.5 1.0
5262         Data variables:
5263             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9
5264         """
5265 
5266         if isinstance(dim, str):
5267             dims = {dim}
5268         elif dim in [None, ...]:
5269             dims = set(self.dims)
5270         else:
5271             dims = set(dim)
5272 
5273         _assert_empty(
5274             [d for d in dims if d not in self.dims],
5275             "Dataset does not contain the dimensions: %s",
5276         )
5277 
5278         q = np.asarray(q, dtype=np.float64)
5279 
5280         variables = {}
5281         for name, var in self.variables.items():
5282             reduce_dims = [d for d in var.dims if d in dims]
5283             if reduce_dims or not var.dims:
5284                 if name not in self.coords:
5285                     if (
5286                         not numeric_only
5287                         or np.issubdtype(var.dtype, np.number)
5288                         or var.dtype == np.bool_
5289                     ):
5290                         if len(reduce_dims) == var.ndim:
5291                             # prefer to aggregate over axis=None rather than
5292                             # axis=(0, 1) if they will be equivalent, because
5293                             # the former is often more efficient
5294                             reduce_dims = None
5295                         variables[name] = var.quantile(
5296                             q,
5297                             dim=reduce_dims,
5298                             interpolation=interpolation,
5299                             keep_attrs=keep_attrs,
5300                             skipna=skipna,
5301                         )
5302 
5303             else:
5304                 variables[name] = var
5305 
5306         # construct the new dataset
5307         coord_names = {k for k in self.coords if k in variables}
5308         indexes = {k: v for k, v in self.indexes.items() if k in variables}
5309         if keep_attrs is None:
5310             keep_attrs = _get_keep_attrs(default=False)
5311         attrs = self.attrs if keep_attrs else None
5312         new = self._replace_with_new_dims(
5313             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
5314         )
5315         return new.assign_coords(quantile=q)
5316 
5317     def rank(self, dim, pct=False, keep_attrs=None):
5318         """Ranks the data.
5319 
5320         Equal values are assigned a rank that is the average of the ranks that
5321         would have been otherwise assigned to all of the values within
5322         that set.
5323         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.
5324 
5325         NaNs in the input array are returned as NaNs.
5326 
5327         The `bottleneck` library is required.
5328 
5329         Parameters
5330         ----------
5331         dim : str
5332             Dimension over which to compute rank.
5333         pct : bool, optional
5334             If True, compute percentage ranks, otherwise compute integer ranks.
5335         keep_attrs : bool, optional
5336             If True, the dataset's attributes (`attrs`) will be copied from
5337             the original object to the new one.  If False (default), the new
5338             object will be returned without attributes.
5339 
5340         Returns
5341         -------
5342         ranked : Dataset
5343             Variables that do not depend on `dim` are dropped.
5344         """
5345         if dim not in self.dims:
5346             raise ValueError("Dataset does not contain the dimension: %s" % dim)
5347 
5348         variables = {}
5349         for name, var in self.variables.items():
5350             if name in self.data_vars:
5351                 if dim in var.dims:
5352                     variables[name] = var.rank(dim, pct=pct)
5353             else:
5354                 variables[name] = var
5355 
5356         coord_names = set(self.coords)
5357         if keep_attrs is None:
5358             keep_attrs = _get_keep_attrs(default=False)
5359         attrs = self.attrs if keep_attrs else None
5360         return self._replace(variables, coord_names, attrs=attrs)
5361 
5362     def differentiate(self, coord, edge_order=1, datetime_unit=None):
5363         """ Differentiate with the second order accurate central
5364         differences.
5365 
5366         .. note::
5367             This feature is limited to simple cartesian geometry, i.e. coord
5368             must be one dimensional.
5369 
5370         Parameters
5371         ----------
5372         coord: str
5373             The coordinate to be used to compute the gradient.
5374         edge_order: 1 or 2. Default 1
5375             N-th order accurate differences at the boundaries.
5376         datetime_unit: None or any of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms',
5377             'us', 'ns', 'ps', 'fs', 'as'}
5378             Unit to compute gradient. Only valid for datetime coordinate.
5379 
5380         Returns
5381         -------
5382         differentiated: Dataset
5383 
5384         See also
5385         --------
5386         numpy.gradient: corresponding numpy function
5387         """
5388         from .variable import Variable
5389 
5390         if coord not in self.variables and coord not in self.dims:
5391             raise ValueError(f"Coordinate {coord} does not exist.")
5392 
5393         coord_var = self[coord].variable
5394         if coord_var.ndim != 1:
5395             raise ValueError(
5396                 "Coordinate {} must be 1 dimensional but is {}"
5397                 " dimensional".format(coord, coord_var.ndim)
5398             )
5399 
5400         dim = coord_var.dims[0]
5401         if _contains_datetime_like_objects(coord_var):
5402             if coord_var.dtype.kind in "mM" and datetime_unit is None:
5403                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
5404             elif datetime_unit is None:
5405                 datetime_unit = "s"  # Default to seconds for cftime objects
5406             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)
5407 
5408         variables = {}
5409         for k, v in self.variables.items():
5410             if k in self.data_vars and dim in v.dims and k not in self.coords:
5411                 if _contains_datetime_like_objects(v):
5412                     v = v._to_numeric(datetime_unit=datetime_unit)
5413                 grad = duck_array_ops.gradient(
5414                     v.data, coord_var, edge_order=edge_order, axis=v.get_axis_num(dim)
5415                 )
5416                 variables[k] = Variable(v.dims, grad)
5417             else:
5418                 variables[k] = v
5419         return self._replace(variables)
5420 
5421     def integrate(self, coord, datetime_unit=None):
5422         """ integrate the array with the trapezoidal rule.
5423 
5424         .. note::
5425             This feature is limited to simple cartesian geometry, i.e. coord
5426             must be one dimensional.
5427 
5428         Parameters
5429         ----------
5430         coord: str, or a sequence of str
5431             Coordinate(s) used for the integration.
5432         datetime_unit
5433             Can be specify the unit if datetime coordinate is used. One of
5434             {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', 'ps', 'fs',
5435             'as'}
5436 
5437         Returns
5438         -------
5439         integrated: Dataset
5440 
5441         See also
5442         --------
5443         DataArray.integrate
5444         numpy.trapz: corresponding numpy function
5445 
5446         Examples
5447         --------
5448         >>> ds = xr.Dataset(
5449         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
5450         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
5451         ... )
5452         >>> ds
5453         <xarray.Dataset>
5454         Dimensions:  (x: 4)
5455         Coordinates:
5456           * x        (x) int64 0 1 2 3
5457             y        (x) int64 1 7 3 5
5458         Data variables:
5459             a        (x) int64 5 5 6 6
5460             b        (x) int64 1 2 1 0
5461         >>> ds.integrate("x")
5462         <xarray.Dataset>
5463         Dimensions:  ()
5464         Data variables:
5465             a        float64 16.5
5466             b        float64 3.5
5467         >>> ds.integrate("y")
5468         <xarray.Dataset>
5469         Dimensions:  ()
5470         Data variables:
5471             a        float64 20.0
5472             b        float64 4.0
5473         """
5474         if not isinstance(coord, (list, tuple)):
5475             coord = (coord,)
5476         result = self
5477         for c in coord:
5478             result = result._integrate_one(c, datetime_unit=datetime_unit)
5479         return result
5480 
5481     def _integrate_one(self, coord, datetime_unit=None):
5482         from .variable import Variable
5483 
5484         if coord not in self.variables and coord not in self.dims:
5485             raise ValueError(f"Coordinate {coord} does not exist.")
5486 
5487         coord_var = self[coord].variable
5488         if coord_var.ndim != 1:
5489             raise ValueError(
5490                 "Coordinate {} must be 1 dimensional but is {}"
5491                 " dimensional".format(coord, coord_var.ndim)
5492             )
5493 
5494         dim = coord_var.dims[0]
5495         if _contains_datetime_like_objects(coord_var):
5496             if coord_var.dtype.kind in "mM" and datetime_unit is None:
5497                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
5498             elif datetime_unit is None:
5499                 datetime_unit = "s"  # Default to seconds for cftime objects
5500             coord_var = coord_var._replace(
5501                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)
5502             )
5503 
5504         variables = {}
5505         coord_names = set()
5506         for k, v in self.variables.items():
5507             if k in self.coords:
5508                 if dim not in v.dims:
5509                     variables[k] = v
5510                     coord_names.add(k)
5511             else:
5512                 if k in self.data_vars and dim in v.dims:
5513                     if _contains_datetime_like_objects(v):
5514                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)
5515                     integ = duck_array_ops.trapz(
5516                         v.data, coord_var.data, axis=v.get_axis_num(dim)
5517                     )
5518                     v_dims = list(v.dims)
5519                     v_dims.remove(dim)
5520                     variables[k] = Variable(v_dims, integ)
5521                 else:
5522                     variables[k] = v
5523         indexes = {k: v for k, v in self.indexes.items() if k in variables}
5524         return self._replace_with_new_dims(
5525             variables, coord_names=coord_names, indexes=indexes
5526         )
5527 
5528     @property
5529     def real(self):
5530         return self._unary_op(lambda x: x.real, keep_attrs=True)(self)
5531 
5532     @property
5533     def imag(self):
5534         return self._unary_op(lambda x: x.imag, keep_attrs=True)(self)
5535 
5536     @property
5537     def plot(self):
5538         """
5539         Access plotting functions for Datasets.
5540         Use it as a namespace to use xarray.plot functions as Dataset methods
5541 
5542         >>> ds.plot.scatter(...)  # equivalent to xarray.plot.scatter(ds,...)
5543 
5544         """
5545         return _Dataset_PlotMethods(self)
5546 
5547     def filter_by_attrs(self, **kwargs):
5548         """Returns a ``Dataset`` with variables that match specific conditions.
5549 
5550         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned
5551         containing only the variables for which all the filter tests pass.
5552         These tests are either ``key=value`` for which the attribute ``key``
5553         has the exact value ``value`` or the callable passed into
5554         ``key=callable`` returns True. The callable will be passed a single
5555         value, either the value of the attribute ``key`` or ``None`` if the
5556         DataArray does not have an attribute with the name ``key``.
5557 
5558         Parameters
5559         ----------
5560         **kwargs : key=value
5561             key : str
5562                 Attribute name.
5563             value : callable or obj
5564                 If value is a callable, it should return a boolean in the form
5565                 of bool = func(attr) where attr is da.attrs[key].
5566                 Otherwise, value will be compared to the each
5567                 DataArray's attrs[key].
5568 
5569         Returns
5570         -------
5571         new : Dataset
5572             New dataset with variables filtered by attribute.
5573 
5574         Examples
5575         --------
5576         >>> # Create an example dataset:
5577         >>> import numpy as np
5578         >>> import pandas as pd
5579         >>> import xarray as xr
5580         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)
5581         >>> precip = 10 * np.random.rand(2, 2, 3)
5582         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
5583         >>> lat = [[42.25, 42.21], [42.63, 42.59]]
5584         >>> dims = ["x", "y", "time"]
5585         >>> temp_attr = dict(standard_name="air_potential_temperature")
5586         >>> precip_attr = dict(standard_name="convective_precipitation_flux")
5587         >>> ds = xr.Dataset(
5588         ...     {
5589         ...         "temperature": (dims, temp, temp_attr),
5590         ...         "precipitation": (dims, precip, precip_attr),
5591         ...     },
5592         ...     coords={
5593         ...         "lon": (["x", "y"], lon),
5594         ...         "lat": (["x", "y"], lat),
5595         ...         "time": pd.date_range("2014-09-06", periods=3),
5596         ...         "reference_time": pd.Timestamp("2014-09-05"),
5597         ...     },
5598         ... )
5599         >>> # Get variables matching a specific standard_name.
5600         >>> ds.filter_by_attrs(standard_name="convective_precipitation_flux")
5601         <xarray.Dataset>
5602         Dimensions:         (time: 3, x: 2, y: 2)
5603         Coordinates:
5604           * x               (x) int64 0 1
5605           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
5606             lat             (x, y) float64 42.25 42.21 42.63 42.59
5607           * y               (y) int64 0 1
5608             reference_time  datetime64[ns] 2014-09-05
5609             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
5610         Data variables:
5611             precipitation   (x, y, time) float64 4.178 2.307 6.041 6.046 0.06648 ...
5612         >>> # Get all variables that have a standard_name attribute.
5613         >>> standard_name = lambda v: v is not None
5614         >>> ds.filter_by_attrs(standard_name=standard_name)
5615         <xarray.Dataset>
5616         Dimensions:         (time: 3, x: 2, y: 2)
5617         Coordinates:
5618             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
5619             lat             (x, y) float64 42.25 42.21 42.63 42.59
5620           * x               (x) int64 0 1
5621           * y               (y) int64 0 1
5622           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
5623             reference_time  datetime64[ns] 2014-09-05
5624         Data variables:
5625             temperature     (x, y, time) float64 25.86 20.82 6.954 23.13 10.25 11.68 ...
5626             precipitation   (x, y, time) float64 5.702 0.9422 2.075 1.178 3.284 ...
5627 
5628         """
5629         selection = []
5630         for var_name, variable in self.variables.items():
5631             has_value_flag = False
5632             for attr_name, pattern in kwargs.items():
5633                 attr_value = variable.attrs.get(attr_name)
5634                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:
5635                     has_value_flag = True
5636                 else:
5637                     has_value_flag = False
5638                     break
5639             if has_value_flag is True:
5640                 selection.append(var_name)
5641         return self[selection]
5642 
5643     def unify_chunks(self) -> "Dataset":
5644         """ Unify chunk size along all chunked dimensions of this Dataset.
5645 
5646         Returns
5647         -------
5648 
5649         Dataset with consistent chunk sizes for all dask-array variables
5650 
5651         See Also
5652         --------
5653 
5654         dask.array.core.unify_chunks
5655         """
5656 
5657         try:
5658             self.chunks
5659         except ValueError:  # "inconsistent chunks"
5660             pass
5661         else:
5662             # No variables with dask backend, or all chunks are already aligned
5663             return self.copy()
5664 
5665         # import dask is placed after the quick exit test above to allow
5666         # running this method if dask isn't installed and there are no chunks
5667         import dask.array
5668 
5669         ds = self.copy()
5670 
5671         dims_pos_map = {dim: index for index, dim in enumerate(ds.dims)}
5672 
5673         dask_array_names = []
5674         dask_unify_args = []
5675         for name, variable in ds.variables.items():
5676             if isinstance(variable.data, dask.array.Array):
5677                 dims_tuple = [dims_pos_map[dim] for dim in variable.dims]
5678                 dask_array_names.append(name)
5679                 dask_unify_args.append(variable.data)
5680                 dask_unify_args.append(dims_tuple)
5681 
5682         _, rechunked_arrays = dask.array.core.unify_chunks(*dask_unify_args)
5683 
5684         for name, new_array in zip(dask_array_names, rechunked_arrays):
5685             ds.variables[name]._data = new_array
5686 
5687         return ds
5688 
5689     def map_blocks(
5690         self,
5691         func: "Callable[..., T_DSorDA]",
5692         args: Sequence[Any] = (),
5693         kwargs: Mapping[str, Any] = None,
5694     ) -> "T_DSorDA":
5695         """
5696         Apply a function to each chunk of this Dataset. This method is experimental and
5697         its signature may change.
5698 
5699         Parameters
5700         ----------
5701         func: callable
5702             User-provided function that accepts a Dataset as its first parameter. The
5703             function will receive a subset of this Dataset, corresponding to one chunk
5704             along each chunked dimension. ``func`` will be executed as
5705             ``func(obj_subset, *args, **kwargs)``.
5706 
5707             The function will be first run on mocked-up data, that looks like this
5708             Dataset but has sizes 0, to determine properties of the returned object such
5709             as dtype, variable names, new dimensions and new indexes (if any).
5710 
5711             This function must return either a single DataArray or a single Dataset.
5712 
5713             This function cannot change size of existing dimensions, or add new chunked
5714             dimensions.
5715         args: Sequence
5716             Passed verbatim to func after unpacking, after the sliced DataArray. xarray
5717             objects, if any, will not be split by chunks. Passing dask collections is
5718             not allowed.
5719         kwargs: Mapping
5720             Passed verbatim to func after unpacking. xarray objects, if any, will not be
5721             split by chunks. Passing dask collections is not allowed.
5722 
5723         Returns
5724         -------
5725         A single DataArray or Dataset with dask backend, reassembled from the outputs of
5726         the function.
5727 
5728         Notes
5729         -----
5730         This method is designed for when one needs to manipulate a whole xarray object
5731         within each chunk. In the more common case where one can work on numpy arrays,
5732         it is recommended to use apply_ufunc.
5733 
5734         If none of the variables in this Dataset is backed by dask, calling this method
5735         is equivalent to calling ``func(self, *args, **kwargs)``.
5736 
5737         See Also
5738         --------
5739         dask.array.map_blocks, xarray.apply_ufunc, xarray.map_blocks,
5740         xarray.DataArray.map_blocks
5741         """
5742         from .parallel import map_blocks
5743 
5744         return map_blocks(func, self, args, kwargs)

</code><intended_behavior>Implement a new method named 'average' (or 'weighted_average' for clarity) that allows for computing weighted averages across specified dimensions of the dataset. This method should accept parameters for specifying the axes along which to compute the averages and an optional weight parameter for each dimension. It should handle missing data gracefully, likely reusing or extending patterns seen in 'interpolate_na'.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
215 class DataArray(AbstractArray, DataWithCoords):
216     """N-dimensional array with labeled coordinates and dimensions.
217 
218     DataArray provides a wrapper around numpy ndarrays that uses labeled
219     dimensions and coordinates to support metadata aware operations. The API is
220     similar to that for the pandas Series or DataFrame, but DataArray objects
221     can have any number of dimensions, and their contents have fixed data
222     types.
223 
224     Additional features over raw numpy arrays:
225 
226     - Apply operations over dimensions by name: ``x.sum('time')``.
227     - Select or assign values by integer location (like numpy): ``x[:10]``
228       or by label (like pandas): ``x.loc['2014-01-01']`` or
229       ``x.sel(time='2014-01-01')``.
230     - Mathematical operations (e.g., ``x - y``) vectorize across multiple
231       dimensions (known in numpy as "broadcasting") based on dimension names,
232       regardless of their original order.
233     - Keep track of arbitrary metadata in the form of a Python dictionary:
234       ``x.attrs``
235     - Convert to a pandas Series: ``x.to_series()``.
236 
237     Getting items from or doing mathematical operations with a DataArray
238     always returns another DataArray.
239     """
240 
241     _cache: Dict[str, Any]
242     _coords: Dict[Any, Variable]
243     _indexes: Optional[Dict[Hashable, pd.Index]]
244     _name: Optional[Hashable]
245     _variable: Variable
246 
247     __slots__ = (
248         "_cache",
249         "_coords",
250         "_file_obj",
251         "_indexes",
252         "_name",
253         "_variable",
254         "__weakref__",
255     )
256 
257     _groupby_cls = groupby.DataArrayGroupBy
258     _rolling_cls = rolling.DataArrayRolling
259     _coarsen_cls = rolling.DataArrayCoarsen
260     _resample_cls = resample.DataArrayResample
261 
262     dt = property(CombinedDatetimelikeAccessor)
263 
264     def __init__(
265         self,
266         data: Any = dtypes.NA,
267         coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None] = None,
268         dims: Union[Hashable, Sequence[Hashable], None] = None,
269         name: Hashable = None,
270         attrs: Mapping = None,
271         # internal parameters
272         indexes: Dict[Hashable, pd.Index] = None,
273         fastpath: bool = False,
274     ):
275         """
276         Parameters
277         ----------
278         data : array_like
279             Values for this array. Must be an ``numpy.ndarray``, ndarray like,
280             or castable to an ``ndarray``. If a self-described xarray or pandas
281             object, attempts are made to use this array's metadata to fill in
282             other unspecified arguments. A view of the array's data is used
283             instead of a copy if possible.
284         coords : sequence or dict of array_like objects, optional
285             Coordinates (tick labels) to use for indexing along each dimension.
286             The following notations are accepted:
287 
288             - mapping {dimension name: array-like}
289             - sequence of tuples that are valid arguments for xarray.Variable()
290               - (dims, data)
291               - (dims, data, attrs)
292               - (dims, data, attrs, encoding)
293 
294             Additionally, it is possible to define a coord whose name
295             does not match the dimension name, or a coord based on multiple
296             dimensions, with one of the following notations:
297 
298             - mapping {coord name: DataArray}
299             - mapping {coord name: Variable}
300             - mapping {coord name: (dimension name, array-like)}
301             - mapping {coord name: (tuple of dimension names, array-like)}
302 
303         dims : hashable or sequence of hashable, optional
304             Name(s) of the data dimension(s). Must be either a hashable (only
305             for 1D data) or a sequence of hashables with length equal to the
306             number of dimensions. If this argument is omitted, dimension names
307             default to ``['dim_0', ... 'dim_n']``.
308         name : str or None, optional
309             Name of this array.
310         attrs : dict_like or None, optional
311             Attributes to assign to the new instance. By default, an empty
312             attribute dictionary is initialized.
313         """
314         if fastpath:
315             variable = data
316             assert dims is None
317             assert attrs is None
318         else:
319             # try to fill in arguments from data if they weren't supplied
320             if coords is None:
321 
322                 if isinstance(data, DataArray):
323                     coords = data.coords
324                 elif isinstance(data, pd.Series):
325                     coords = [data.index]
326                 elif isinstance(data, pd.DataFrame):
327                     coords = [data.index, data.columns]
328                 elif isinstance(data, (pd.Index, IndexVariable)):
329                     coords = [data]
330                 elif isinstance(data, pdcompat.Panel):
331                     coords = [data.items, data.major_axis, data.minor_axis]
332 
333             if dims is None:
334                 dims = getattr(data, "dims", getattr(coords, "dims", None))
335             if name is None:
336                 name = getattr(data, "name", None)
337             if attrs is None and not isinstance(data, PANDAS_TYPES):
338                 attrs = getattr(data, "attrs", None)
339 
340             data = _check_data_shape(data, coords, dims)
341             data = as_compatible_data(data)
342             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
343             variable = Variable(dims, data, attrs, fastpath=True)
344             indexes = dict(
345                 _extract_indexes_from_coords(coords)
346             )  # needed for to_dataset
347 
348         # These fully describe a DataArray
349         self._variable = variable
350         assert isinstance(coords, dict)
351         self._coords = coords
352         self._name = name
353 
354         # TODO(shoyer): document this argument, once it becomes part of the
355         # public interface.
356         self._indexes = indexes
357 
358         self._file_obj = None
359 
360     def _replace(
361         self,
362         variable: Variable = None,
363         coords=None,
364         name: Union[Hashable, None, Default] = _default,
365         indexes=None,
366     ) -> "DataArray":
367         if variable is None:
368             variable = self.variable
369         if coords is None:
370             coords = self._coords
371         if name is _default:
372             name = self.name
373         return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)
374 
375     def _replace_maybe_drop_dims(
376         self, variable: Variable, name: Union[Hashable, None, Default] = _default
377     ) -> "DataArray":
378         if variable.dims == self.dims and variable.shape == self.shape:
379             coords = self._coords.copy()
380             indexes = self._indexes
381         elif variable.dims == self.dims:
382             # Shape has changed (e.g. from reduce(..., keepdims=True)
383             new_sizes = dict(zip(self.dims, variable.shape))
384             coords = {
385                 k: v
386                 for k, v in self._coords.items()
387                 if v.shape == tuple(new_sizes[d] for d in v.dims)
388             }
389             changed_dims = [
390                 k for k in variable.dims if variable.sizes[k] != self.sizes[k]
391             ]
392             indexes = propagate_indexes(self._indexes, exclude=changed_dims)
393         else:
394             allowed_dims = set(variable.dims)
395             coords = {
396                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
397             }
398             indexes = propagate_indexes(
399                 self._indexes, exclude=(set(self.dims) - allowed_dims)
400             )
401         return self._replace(variable, coords, name, indexes=indexes)
402 
403     def _overwrite_indexes(self, indexes: Mapping[Hashable, Any]) -> "DataArray":
404         if not len(indexes):
405             return self
406         coords = self._coords.copy()
407         for name, idx in indexes.items():
408             coords[name] = IndexVariable(name, idx)
409         obj = self._replace(coords=coords)
410 
411         # switch from dimension to level names, if necessary
412         dim_names: Dict[Any, str] = {}
413         for dim, idx in indexes.items():
414             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:
415                 dim_names[dim] = idx.name
416         if dim_names:
417             obj = obj.rename(dim_names)
418         return obj
419 
420     def _to_temp_dataset(self) -> Dataset:
421         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
422 
423     def _from_temp_dataset(
424         self, dataset: Dataset, name: Hashable = _default
425     ) -> "DataArray":
426         variable = dataset._variables.pop(_THIS_ARRAY)
427         coords = dataset._variables
428         indexes = dataset._indexes
429         return self._replace(variable, coords, name, indexes=indexes)
430 
431     def _to_dataset_split(self, dim: Hashable) -> Dataset:
432         """ splits dataarray along dimension 'dim' """
433 
434         def subset(dim, label):
435             array = self.loc[{dim: label}]
436             array.attrs = {}
437             return as_variable(array)
438 
439         variables = {label: subset(dim, label) for label in self.get_index(dim)}
440         variables.update({k: v for k, v in self._coords.items() if k != dim})
441         indexes = propagate_indexes(self._indexes, exclude=dim)
442         coord_names = set(self._coords) - set([dim])
443         dataset = Dataset._construct_direct(
444             variables, coord_names, indexes=indexes, attrs=self.attrs
445         )
446         return dataset
447 
448     def _to_dataset_whole(
449         self, name: Hashable = None, shallow_copy: bool = True
450     ) -> Dataset:
451         if name is None:
452             name = self.name
453         if name is None:
454             raise ValueError(
455                 "unable to convert unnamed DataArray to a "
456                 "Dataset without providing an explicit name"
457             )
458         if name in self.coords:
459             raise ValueError(
460                 "cannot create a Dataset from a DataArray with "
461                 "the same name as one of its coordinates"
462             )
463         # use private APIs for speed: this is called by _to_temp_dataset(),
464         # which is used in the guts of a lot of operations (e.g., reindex)
465         variables = self._coords.copy()
466         variables[name] = self.variable
467         if shallow_copy:
468             for k in variables:
469                 variables[k] = variables[k].copy(deep=False)
470         indexes = self._indexes
471 
472         coord_names = set(self._coords)
473         dataset = Dataset._construct_direct(variables, coord_names, indexes=indexes)
474         return dataset
475 
476     def to_dataset(self, dim: Hashable = None, *, name: Hashable = None) -> Dataset:
477         """Convert a DataArray to a Dataset.
478 
479         Parameters
480         ----------
481         dim : hashable, optional
482             Name of the dimension on this array along which to split this array
483             into separate variables. If not provided, this array is converted
484             into a Dataset of one variable.
485         name : hashable, optional
486             Name to substitute for this array's name. Only valid if ``dim`` is
487             not provided.
488 
489         Returns
490         -------
491         dataset : Dataset
492         """
493         if dim is not None and dim not in self.dims:
494             raise TypeError(
495                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
496             )
497 
498         if dim is not None:
499             if name is not None:
500                 raise TypeError("cannot supply both dim and name arguments")
501             return self._to_dataset_split(dim)
502         else:
503             return self._to_dataset_whole(name)
504 
505     @property
506     def name(self) -> Optional[Hashable]:
507         """The name of this array.
508         """
509         return self._name
510 
511     @name.setter
512     def name(self, value: Optional[Hashable]) -> None:
513         self._name = value
514 
515     @property
516     def variable(self) -> Variable:
517         """Low level interface to the Variable object for this DataArray."""
518         return self._variable
519 
520     @property
521     def dtype(self) -> np.dtype:
522         return self.variable.dtype
523 
524     @property
525     def shape(self) -> Tuple[int, ...]:
526         return self.variable.shape
527 
528     @property
529     def size(self) -> int:
530         return self.variable.size
531 
532     @property
533     def nbytes(self) -> int:
534         return self.variable.nbytes
535 
536     @property
537     def ndim(self) -> int:
538         return self.variable.ndim
539 
540     def __len__(self) -> int:
541         return len(self.variable)
542 
543     @property
544     def data(self) -> Any:
545         """The array's data as a dask or numpy array
546         """
547         return self.variable.data
548 
549     @data.setter
550     def data(self, value: Any) -> None:
551         self.variable.data = value
552 
553     @property
554     def values(self) -> np.ndarray:
555         """The array's data as a numpy.ndarray"""
556         return self.variable.values
557 
558     @values.setter
559     def values(self, value: Any) -> None:
560         self.variable.values = value
561 
562     @property
563     def _in_memory(self) -> bool:
564         return self.variable._in_memory
565 
566     def to_index(self) -> pd.Index:
567         """Convert this variable to a pandas.Index. Only possible for 1D
568         arrays.
569         """
570         return self.variable.to_index()
571 
572     @property
573     def dims(self) -> Tuple[Hashable, ...]:
574         """Tuple of dimension names associated with this array.
575 
576         Note that the type of this property is inconsistent with
577         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
578         consistently named properties.
579         """
580         return self.variable.dims
581 
582     @dims.setter
583     def dims(self, value):
584         raise AttributeError(
585             "you cannot assign dims on a DataArray. Use "
586             ".rename() or .swap_dims() instead."
587         )
588 
589     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
590         if utils.is_dict_like(key):
591             return key
592         else:
593             key = indexing.expanded_indexer(key, self.ndim)
594             return dict(zip(self.dims, key))
595 
596     @property
597     def _level_coords(self) -> Dict[Hashable, Hashable]:
598         """Return a mapping of all MultiIndex levels and their corresponding
599         coordinate name.
600         """
601         level_coords: Dict[Hashable, Hashable] = {}
602 
603         for cname, var in self._coords.items():
604             if var.ndim == 1 and isinstance(var, IndexVariable):
605                 level_names = var.level_names
606                 if level_names is not None:
607                     (dim,) = var.dims
608                     level_coords.update({lname: dim for lname in level_names})
609         return level_coords
610 
611     def _getitem_coord(self, key):
612         from .dataset import _get_virtual_variable
613 
614         try:
615             var = self._coords[key]
616         except KeyError:
617             dim_sizes = dict(zip(self.dims, self.shape))
618             _, key, var = _get_virtual_variable(
619                 self._coords, key, self._level_coords, dim_sizes
620             )
621 
622         return self._replace_maybe_drop_dims(var, name=key)
623 
624     def __getitem__(self, key: Any) -> "DataArray":
625         if isinstance(key, str):
626             return self._getitem_coord(key)
627         else:
628             # xarray-style array indexing
629             return self.isel(indexers=self._item_key_to_dict(key))
630 
631     def __setitem__(self, key: Any, value: Any) -> None:
632         if isinstance(key, str):
633             self.coords[key] = value
634         else:
635             # Coordinates in key, value and self[key] should be consistent.
636             # TODO Coordinate consistency in key is checked here, but it
637             # causes unnecessary indexing. It should be optimized.
638             obj = self[key]
639             if isinstance(value, DataArray):
640                 assert_coordinate_consistent(value, obj.coords.variables)
641             # DataArray key -> Variable key
642             key = {
643                 k: v.variable if isinstance(v, DataArray) else v
644                 for k, v in self._item_key_to_dict(key).items()
645             }
646             self.variable[key] = value
647 
648     def __delitem__(self, key: Any) -> None:
649         del self.coords[key]
650 
651     @property
652     def _attr_sources(self) -> List[Mapping[Hashable, Any]]:
653         """List of places to look-up items for attribute-style access
654         """
655         return self._item_sources + [self.attrs]
656 
657     @property
658     def _item_sources(self) -> List[Mapping[Hashable, Any]]:
659         """List of places to look-up items for key-completion
660         """
661         return [
662             self.coords,
663             {d: self.coords[d] for d in self.dims},
664             LevelCoordinatesSource(self),
665         ]
666 
667     def __contains__(self, key: Any) -> bool:
668         return key in self.data
669 
670     @property
671     def loc(self) -> _LocIndexer:
672         """Attribute for location based indexing like pandas.
673         """
674         return _LocIndexer(self)
675 
676     @property
677     def attrs(self) -> Dict[Hashable, Any]:
678         """Dictionary storing arbitrary metadata with this array."""
679         return self.variable.attrs
680 
681     @attrs.setter
682     def attrs(self, value: Mapping[Hashable, Any]) -> None:
683         # Disable type checking to work around mypy bug - see mypy#4167
684         self.variable.attrs = value  # type: ignore
685 
686     @property
687     def encoding(self) -> Dict[Hashable, Any]:
688         """Dictionary of format-specific settings for how this array should be
689         serialized."""
690         return self.variable.encoding
691 
692     @encoding.setter
693     def encoding(self, value: Mapping[Hashable, Any]) -> None:
694         self.variable.encoding = value
695 
696     @property
697     def indexes(self) -> Indexes:
698         """Mapping of pandas.Index objects used for label based indexing
699         """
700         if self._indexes is None:
701             self._indexes = default_indexes(self._coords, self.dims)
702         return Indexes(self._indexes)
703 
704     @property
705     def coords(self) -> DataArrayCoordinates:
706         """Dictionary-like container of coordinate arrays.
707         """
708         return DataArrayCoordinates(self)
709 
710     def reset_coords(
711         self,
712         names: Union[Iterable[Hashable], Hashable, None] = None,
713         drop: bool = False,
714         inplace: bool = None,
715     ) -> Union[None, "DataArray", Dataset]:
716         """Given names of coordinates, reset them to become variables.
717 
718         Parameters
719         ----------
720         names : hashable or iterable of hashables, optional
721             Name(s) of non-index coordinates in this dataset to reset into
722             variables. By default, all non-index coordinates are reset.
723         drop : bool, optional
724             If True, remove coordinates instead of converting them into
725             variables.
726 
727         Returns
728         -------
729         Dataset, or DataArray if ``drop == True``
730         """
731         _check_inplace(inplace)
732         if names is None:
733             names = set(self.coords) - set(self.dims)
734         dataset = self.coords.to_dataset().reset_coords(names, drop)
735         if drop:
736             return self._replace(coords=dataset._variables)
737         else:
738             if self.name is None:
739                 raise ValueError(
740                     "cannot reset_coords with drop=False on an unnamed DataArrray"
741                 )
742             dataset[self.name] = self.variable
743             return dataset
744 
745     def __dask_tokenize__(self):
746         from dask.base import normalize_token
747 
748         return normalize_token((type(self), self._variable, self._coords, self._name))
749 
750     def __dask_graph__(self):
751         return self._to_temp_dataset().__dask_graph__()
752 
753     def __dask_keys__(self):
754         return self._to_temp_dataset().__dask_keys__()
755 
756     def __dask_layers__(self):
757         return self._to_temp_dataset().__dask_layers__()
758 
759     @property
760     def __dask_optimize__(self):
761         return self._to_temp_dataset().__dask_optimize__
762 
763     @property
764     def __dask_scheduler__(self):
765         return self._to_temp_dataset().__dask_scheduler__
766 
767     def __dask_postcompute__(self):
768         func, args = self._to_temp_dataset().__dask_postcompute__()
769         return self._dask_finalize, (func, args, self.name)
770 
771     def __dask_postpersist__(self):
772         func, args = self._to_temp_dataset().__dask_postpersist__()
773         return self._dask_finalize, (func, args, self.name)
774 
775     @staticmethod
776     def _dask_finalize(results, func, args, name):
777         ds = func(results, *args)
778         variable = ds._variables.pop(_THIS_ARRAY)
779         coords = ds._variables
780         return DataArray(variable, coords, name=name, fastpath=True)
781 
782     def load(self, **kwargs) -> "DataArray":
783         """Manually trigger loading of this array's data from disk or a
784         remote source into memory and return this array.
785 
786         Normally, it should not be necessary to call this method in user code,
787         because all xarray functions should either work on deferred data or
788         load data automatically. However, this method can be necessary when
789         working with many file objects on disk.
790 
791         Parameters
792         ----------
793         **kwargs : dict
794             Additional keyword arguments passed on to ``dask.array.compute``.
795 
796         See Also
797         --------
798         dask.array.compute
799         """
800         ds = self._to_temp_dataset().load(**kwargs)
801         new = self._from_temp_dataset(ds)
802         self._variable = new._variable
803         self._coords = new._coords
804         return self
805 
806     def compute(self, **kwargs) -> "DataArray":
807         """Manually trigger loading of this array's data from disk or a
808         remote source into memory and return a new array. The original is
809         left unaltered.
810 
811         Normally, it should not be necessary to call this method in user code,
812         because all xarray functions should either work on deferred data or
813         load data automatically. However, this method can be necessary when
814         working with many file objects on disk.
815 
816         Parameters
817         ----------
818         **kwargs : dict
819             Additional keyword arguments passed on to ``dask.array.compute``.
820 
821         See Also
822         --------
823         dask.array.compute
824         """
825         new = self.copy(deep=False)
826         return new.load(**kwargs)
827 
828     def persist(self, **kwargs) -> "DataArray":
829         """ Trigger computation in constituent dask arrays
830 
831         This keeps them as dask arrays but encourages them to keep data in
832         memory.  This is particularly useful when on a distributed machine.
833         When on a single machine consider using ``.compute()`` instead.
834 
835         Parameters
836         ----------
837         **kwargs : dict
838             Additional keyword arguments passed on to ``dask.persist``.
839 
840         See Also
841         --------
842         dask.persist
843         """
844         ds = self._to_temp_dataset().persist(**kwargs)
845         return self._from_temp_dataset(ds)
846 
847     def copy(self, deep: bool = True, data: Any = None) -> "DataArray":
848         """Returns a copy of this array.
849 
850         If `deep=True`, a deep copy is made of the data array.
851         Otherwise, a shallow copy is made, so each variable in the new
852         array's dataset is also a variable in this array's dataset.
853 
854         Use `data` to create a new object with the same structure as
855         original but entirely new data.
856 
857         Parameters
858         ----------
859         deep : bool, optional
860             Whether the data array and its coordinates are loaded into memory
861             and copied onto the new object. Default is True.
862         data : array_like, optional
863             Data to use in the new object. Must have same shape as original.
864             When `data` is used, `deep` is ignored for all data variables,
865             and only used for coords.
866 
867         Returns
868         -------
869         object : DataArray
870             New object with dimensions, attributes, coordinates, name,
871             encoding, and optionally data copied from original.
872 
873         Examples
874         --------
875 
876         Shallow versus deep copy
877 
878         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
879         >>> array.copy()
880         <xarray.DataArray (x: 3)>
881         array([1, 2, 3])
882         Coordinates:
883         * x        (x) <U1 'a' 'b' 'c'
884         >>> array_0 = array.copy(deep=False)
885         >>> array_0[0] = 7
886         >>> array_0
887         <xarray.DataArray (x: 3)>
888         array([7, 2, 3])
889         Coordinates:
890         * x        (x) <U1 'a' 'b' 'c'
891         >>> array
892         <xarray.DataArray (x: 3)>
893         array([7, 2, 3])
894         Coordinates:
895         * x        (x) <U1 'a' 'b' 'c'
896 
897         Changing the data using the ``data`` argument maintains the
898         structure of the original object, but with the new data. Original
899         object is unaffected.
900 
901         >>> array.copy(data=[0.1, 0.2, 0.3])
902         <xarray.DataArray (x: 3)>
903         array([ 0.1,  0.2,  0.3])
904         Coordinates:
905         * x        (x) <U1 'a' 'b' 'c'
906         >>> array
907         <xarray.DataArray (x: 3)>
908         array([1, 2, 3])
909         Coordinates:
910         * x        (x) <U1 'a' 'b' 'c'
911 
912         See Also
913         --------
914         pandas.DataFrame.copy
915         """
916         variable = self.variable.copy(deep=deep, data=data)
917         coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}
918         indexes = self._indexes
919         return self._replace(variable, coords, indexes=indexes)
920 
921     def __copy__(self) -> "DataArray":
922         return self.copy(deep=False)
923 
924     def __deepcopy__(self, memo=None) -> "DataArray":
925         # memo does nothing but is required for compatibility with
926         # copy.deepcopy
927         return self.copy(deep=True)
928 
929     # mutable objects should not be hashable
930     # https://github.com/python/mypy/issues/4266
931     __hash__ = None  # type: ignore
932 
933     @property
934     def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:
935         """Block dimensions for this array's data or None if it's not a dask
936         array.
937         """
938         return self.variable.chunks
939 
940     def chunk(
941         self,
942         chunks: Union[
943             None,
944             Number,
945             Tuple[Number, ...],
946             Tuple[Tuple[Number, ...], ...],
947             Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]],
948         ] = None,
949         name_prefix: str = "xarray-",
950         token: str = None,
951         lock: bool = False,
952     ) -> "DataArray":
953         """Coerce this array's data into a dask arrays with the given chunks.
954 
955         If this variable is a non-dask array, it will be converted to dask
956         array. If it's a dask array, it will be rechunked to the given chunk
957         sizes.
958 
959         If neither chunks is not provided for one or more dimensions, chunk
960         sizes along that dimension will not be updated; non-dask arrays will be
961         converted into dask arrays with a single block.
962 
963         Parameters
964         ----------
965         chunks : int, tuple or mapping, optional
966             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
967             ``{'x': 5, 'y': 5}``.
968         name_prefix : str, optional
969             Prefix for the name of the new dask array.
970         token : str, optional
971             Token uniquely identifying this array.
972         lock : optional
973             Passed on to :py:func:`dask.array.from_array`, if the array is not
974             already as dask array.
975 
976         Returns
977         -------
978         chunked : xarray.DataArray
979         """
980         if isinstance(chunks, (tuple, list)):
981             chunks = dict(zip(self.dims, chunks))
982 
983         ds = self._to_temp_dataset().chunk(
984             chunks, name_prefix=name_prefix, token=token, lock=lock
985         )
986         return self._from_temp_dataset(ds)
987 
988     def isel(
989         self,
990         indexers: Mapping[Hashable, Any] = None,
991         drop: bool = False,
992         **indexers_kwargs: Any,
993     ) -> "DataArray":
994         """Return a new DataArray whose data is given by integer indexing
995         along the specified dimension(s).
996 
997         See Also
998         --------
999         Dataset.isel
1000         DataArray.sel
1001         """
1002         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1003         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1004             ds = self._to_temp_dataset()._isel_fancy(indexers, drop=drop)
1005             return self._from_temp_dataset(ds)
1006 
1007         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1008         # lists, or zero or one-dimensional np.ndarray's
1009 
1010         variable = self._variable.isel(indexers)
1011 
1012         coords = {}
1013         for coord_name, coord_value in self._coords.items():
1014             coord_indexers = {
1015                 k: v for k, v in indexers.items() if k in coord_value.dims
1016             }
1017             if coord_indexers:
1018                 coord_value = coord_value.isel(coord_indexers)
1019                 if drop and coord_value.ndim == 0:
1020                     continue
1021             coords[coord_name] = coord_value
1022 
1023         return self._replace(variable=variable, coords=coords)
1024 
1025     def sel(
1026         self,
1027         indexers: Mapping[Hashable, Any] = None,
1028         method: str = None,
1029         tolerance=None,
1030         drop: bool = False,
1031         **indexers_kwargs: Any,
1032     ) -> "DataArray":
1033         """Return a new DataArray whose data is given by selecting index
1034         labels along the specified dimension(s).
1035 
1036         .. warning::
1037 
1038           Do not try to assign values when using any of the indexing methods
1039           ``isel`` or ``sel``::
1040 
1041             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1042             # DO NOT do this
1043             da.isel(x=[0, 1, 2])[1] = -1
1044 
1045           Assigning values with the chained indexing using ``.sel`` or
1046           ``.isel`` fails silently.
1047 
1048         See Also
1049         --------
1050         Dataset.sel
1051         DataArray.isel
1052 
1053         """
1054         ds = self._to_temp_dataset().sel(
1055             indexers=indexers,
1056             drop=drop,
1057             method=method,
1058             tolerance=tolerance,
1059             **indexers_kwargs,
1060         )
1061         return self._from_temp_dataset(ds)
1062 
1063     def head(
1064         self,
1065         indexers: Union[Mapping[Hashable, int], int] = None,
1066         **indexers_kwargs: Any,
1067     ) -> "DataArray":
1068         """Return a new DataArray whose data is given by the the first `n`
1069         values along the specified dimension(s). Default `n` = 5
1070 
1071         See Also
1072         --------
1073         Dataset.head
1074         DataArray.tail
1075         DataArray.thin
1076         """
1077         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1078         return self._from_temp_dataset(ds)
1079 
1080     def tail(
1081         self,
1082         indexers: Union[Mapping[Hashable, int], int] = None,
1083         **indexers_kwargs: Any,
1084     ) -> "DataArray":
1085         """Return a new DataArray whose data is given by the the last `n`
1086         values along the specified dimension(s). Default `n` = 5
1087 
1088         See Also
1089         --------
1090         Dataset.tail
1091         DataArray.head
1092         DataArray.thin
1093         """
1094         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1095         return self._from_temp_dataset(ds)
1096 
1097     def thin(
1098         self,
1099         indexers: Union[Mapping[Hashable, int], int] = None,
1100         **indexers_kwargs: Any,
1101     ) -> "DataArray":
1102         """Return a new DataArray whose data is given by each `n` value
1103         along the specified dimension(s).
1104 
1105         See Also
1106         --------
1107         Dataset.thin
1108         DataArray.head
1109         DataArray.tail
1110         """
1111         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1112         return self._from_temp_dataset(ds)
1113 
1114     def broadcast_like(
1115         self, other: Union["DataArray", Dataset], exclude: Iterable[Hashable] = None
1116     ) -> "DataArray":
1117         """Broadcast this DataArray against another Dataset or DataArray.
1118 
1119         This is equivalent to xr.broadcast(other, self)[1]
1120 
1121         xarray objects are broadcast against each other in arithmetic
1122         operations, so this method is not be necessary for most uses.
1123 
1124         If no change is needed, the input data is returned to the output
1125         without being copied.
1126 
1127         If new coords are added by the broadcast, their values are
1128         NaN filled.
1129 
1130         Parameters
1131         ----------
1132         other : Dataset or DataArray
1133             Object against which to broadcast this array.
1134         exclude : iterable of hashable, optional
1135             Dimensions that must not be broadcasted
1136 
1137         Returns
1138         -------
1139         new_da: xr.DataArray
1140 
1141         Examples
1142         --------
1143 
1144         >>> arr1
1145         <xarray.DataArray (x: 2, y: 3)>
1146         array([[0.840235, 0.215216, 0.77917 ],
1147                [0.726351, 0.543824, 0.875115]])
1148         Coordinates:
1149           * x        (x) <U1 'a' 'b'
1150           * y        (y) <U1 'a' 'b' 'c'
1151         >>> arr2
1152         <xarray.DataArray (x: 3, y: 2)>
1153         array([[0.612611, 0.125753],
1154                [0.853181, 0.948818],
1155                [0.180885, 0.33363 ]])
1156         Coordinates:
1157           * x        (x) <U1 'a' 'b' 'c'
1158           * y        (y) <U1 'a' 'b'
1159         >>> arr1.broadcast_like(arr2)
1160         <xarray.DataArray (x: 3, y: 3)>
1161         array([[0.840235, 0.215216, 0.77917 ],
1162                [0.726351, 0.543824, 0.875115],
1163                [     nan,      nan,      nan]])
1164         Coordinates:
1165           * x        (x) object 'a' 'b' 'c'
1166           * y        (y) object 'a' 'b' 'c'
1167         """
1168         if exclude is None:
1169             exclude = set()
1170         else:
1171             exclude = set(exclude)
1172         args = align(other, self, join="outer", copy=False, exclude=exclude)
1173 
1174         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1175 
1176         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
1177 
1178     def reindex_like(
1179         self,
1180         other: Union["DataArray", Dataset],
1181         method: str = None,
1182         tolerance=None,
1183         copy: bool = True,
1184         fill_value=dtypes.NA,
1185     ) -> "DataArray":
1186         """Conform this object onto the indexes of another object, filling in
1187         missing values with ``fill_value``. The default fill value is NaN.
1188 
1189         Parameters
1190         ----------
1191         other : Dataset or DataArray
1192             Object with an 'indexes' attribute giving a mapping from dimension
1193             names to pandas.Index objects, which provides coordinates upon
1194             which to index the variables in this dataset. The indexes on this
1195             other object need not be the same as the indexes on this
1196             dataset. Any mis-matched index values will be filled in with
1197             NaN, and any mis-matched dimension names will simply be ignored.
1198         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1199             Method to use for filling index values from other not found on this
1200             data array:
1201 
1202             * None (default): don't fill gaps
1203             * pad / ffill: propagate last valid index value forward
1204             * backfill / bfill: propagate next valid index value backward
1205             * nearest: use nearest valid index value
1206         tolerance : optional
1207             Maximum distance between original and new labels for inexact
1208             matches. The values of the index at the matching locations must
1209             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1210         copy : bool, optional
1211             If ``copy=True``, data in the return value is always copied. If
1212             ``copy=False`` and reindexing is unnecessary, or can be performed
1213             with only slice operations, then the output may share memory with
1214             the input. In either case, a new xarray object is always returned.
1215         fill_value : scalar, optional
1216             Value to use for newly missing values
1217 
1218         Returns
1219         -------
1220         reindexed : DataArray
1221             Another dataset array, with this array's data but coordinates from
1222             the other object.
1223 
1224         See Also
1225         --------
1226         DataArray.reindex
1227         align
1228         """
1229         indexers = reindex_like_indexers(self, other)
1230         return self.reindex(
1231             indexers=indexers,
1232             method=method,
1233             tolerance=tolerance,
1234             copy=copy,
1235             fill_value=fill_value,
1236         )
1237 
1238     def reindex(
1239         self,
1240         indexers: Mapping[Hashable, Any] = None,
1241         method: str = None,
1242         tolerance=None,
1243         copy: bool = True,
1244         fill_value=dtypes.NA,
1245         **indexers_kwargs: Any,
1246     ) -> "DataArray":
1247         """Conform this object onto the indexes of another object, filling in
1248         missing values with ``fill_value``. The default fill value is NaN.
1249 
1250         Parameters
1251         ----------
1252         indexers : dict, optional
1253             Dictionary with keys given by dimension names and values given by
1254             arrays of coordinates tick labels. Any mis-matched coordinate
1255             values will be filled in with NaN, and any mis-matched dimension
1256             names will simply be ignored.
1257             One of indexers or indexers_kwargs must be provided.
1258         copy : bool, optional
1259             If ``copy=True``, data in the return value is always copied. If
1260             ``copy=False`` and reindexing is unnecessary, or can be performed
1261             with only slice operations, then the output may share memory with
1262             the input. In either case, a new xarray object is always returned.
1263         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1264             Method to use for filling index values in ``indexers`` not found on
1265             this data array:
1266 
1267             * None (default): don't fill gaps
1268             * pad / ffill: propagate last valid index value forward
1269             * backfill / bfill: propagate next valid index value backward
1270             * nearest: use nearest valid index value
1271         tolerance : optional
1272             Maximum distance between original and new labels for inexact
1273             matches. The values of the index at the matching locations must
1274             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1275         fill_value : scalar, optional
1276             Value to use for newly missing values
1277         **indexers_kwargs : {dim: indexer, ...}, optional
1278             The keyword arguments form of ``indexers``.
1279             One of indexers or indexers_kwargs must be provided.
1280 
1281         Returns
1282         -------
1283         reindexed : DataArray
1284             Another dataset array, with this array's data but replaced
1285             coordinates.
1286 
1287         See Also
1288         --------
1289         DataArray.reindex_like
1290         align
1291         """
1292         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
1293         ds = self._to_temp_dataset().reindex(
1294             indexers=indexers,
1295             method=method,
1296             tolerance=tolerance,
1297             copy=copy,
1298             fill_value=fill_value,
1299         )
1300         return self._from_temp_dataset(ds)
1301 
1302     def interp(
1303         self,
1304         coords: Mapping[Hashable, Any] = None,
1305         method: str = "linear",
1306         assume_sorted: bool = False,
1307         kwargs: Mapping[str, Any] = None,
1308         **coords_kwargs: Any,
1309     ) -> "DataArray":
1310         """ Multidimensional interpolation of variables.
1311 
1312         coords : dict, optional
1313             Mapping from dimension names to the new coordinates.
1314             new coordinate can be an scalar, array-like or DataArray.
1315             If DataArrays are passed as new coordates, their dimensions are
1316             used for the broadcasting.
1317         method: {'linear', 'nearest'} for multidimensional array,
1318             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
1319             for 1-dimensional array.
1320         assume_sorted: boolean, optional
1321             If False, values of x can be in any order and they are sorted
1322             first. If True, x has to be an array of monotonically increasing
1323             values.
1324         kwargs: dictionary
1325             Additional keyword passed to scipy's interpolator.
1326         ``**coords_kwargs`` : {dim: coordinate, ...}, optional
1327             The keyword arguments form of ``coords``.
1328             One of coords or coords_kwargs must be provided.
1329 
1330         Returns
1331         -------
1332         interpolated: xr.DataArray
1333             New dataarray on the new coordinates.
1334 
1335         Notes
1336         -----
1337         scipy is required.
1338 
1339         See Also
1340         --------
1341         scipy.interpolate.interp1d
1342         scipy.interpolate.interpn
1343 
1344         Examples
1345         --------
1346         >>> da = xr.DataArray([1, 3], [("x", np.arange(2))])
1347         >>> da.interp(x=0.5)
1348         <xarray.DataArray ()>
1349         array(2.0)
1350         Coordinates:
1351             x        float64 0.5
1352         """
1353         if self.dtype.kind not in "uifc":
1354             raise TypeError(
1355                 "interp only works for a numeric type array. "
1356                 "Given {}.".format(self.dtype)
1357             )
1358         ds = self._to_temp_dataset().interp(
1359             coords,
1360             method=method,
1361             kwargs=kwargs,
1362             assume_sorted=assume_sorted,
1363             **coords_kwargs,
1364         )
1365         return self._from_temp_dataset(ds)
1366 
1367     def interp_like(
1368         self,
1369         other: Union["DataArray", Dataset],
1370         method: str = "linear",
1371         assume_sorted: bool = False,
1372         kwargs: Mapping[str, Any] = None,
1373     ) -> "DataArray":
1374         """Interpolate this object onto the coordinates of another object,
1375         filling out of range values with NaN.
1376 
1377         Parameters
1378         ----------
1379         other : Dataset or DataArray
1380             Object with an 'indexes' attribute giving a mapping from dimension
1381             names to an 1d array-like, which provides coordinates upon
1382             which to index the variables in this dataset.
1383         method: string, optional.
1384             {'linear', 'nearest'} for multidimensional array,
1385             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
1386             for 1-dimensional array. 'linear' is used by default.
1387         assume_sorted: boolean, optional
1388             If False, values of coordinates that are interpolated over can be
1389             in any order and they are sorted first. If True, interpolated
1390             coordinates are assumed to be an array of monotonically increasing
1391             values.
1392         kwargs: dictionary, optional
1393             Additional keyword passed to scipy's interpolator.
1394 
1395         Returns
1396         -------
1397         interpolated: xr.DataArray
1398             Another dataarray by interpolating this dataarray's data along the
1399             coordinates of the other object.
1400 
1401         Notes
1402         -----
1403         scipy is required.
1404         If the dataarray has object-type coordinates, reindex is used for these
1405         coordinates instead of the interpolation.
1406 
1407         See Also
1408         --------
1409         DataArray.interp
1410         DataArray.reindex_like
1411         """
1412         if self.dtype.kind not in "uifc":
1413             raise TypeError(
1414                 "interp only works for a numeric type array. "
1415                 "Given {}.".format(self.dtype)
1416             )
1417         ds = self._to_temp_dataset().interp_like(
1418             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
1419         )
1420         return self._from_temp_dataset(ds)
1421 
1422     def rename(
1423         self,
1424         new_name_or_name_dict: Union[Hashable, Mapping[Hashable, Hashable]] = None,
1425         **names: Hashable,
1426     ) -> "DataArray":
1427         """Returns a new DataArray with renamed coordinates or a new name.
1428 
1429         Parameters
1430         ----------
1431         new_name_or_name_dict : str or dict-like, optional
1432             If the argument is dict-like, it used as a mapping from old
1433             names to new names for coordinates. Otherwise, use the argument
1434             as the new name for this array.
1435         **names: hashable, optional
1436             The keyword arguments form of a mapping from old names to
1437             new names for coordinates.
1438             One of new_name_or_name_dict or names must be provided.
1439 
1440         Returns
1441         -------
1442         renamed : DataArray
1443             Renamed array or array with renamed coordinates.
1444 
1445         See Also
1446         --------
1447         Dataset.rename
1448         DataArray.swap_dims
1449         """
1450         if names or utils.is_dict_like(new_name_or_name_dict):
1451             new_name_or_name_dict = cast(
1452                 Mapping[Hashable, Hashable], new_name_or_name_dict
1453             )
1454             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
1455             dataset = self._to_temp_dataset().rename(name_dict)
1456             return self._from_temp_dataset(dataset)
1457         else:
1458             new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)
1459             return self._replace(name=new_name_or_name_dict)
1460 
1461     def swap_dims(self, dims_dict: Mapping[Hashable, Hashable]) -> "DataArray":
1462         """Returns a new DataArray with swapped dimensions.
1463 
1464         Parameters
1465         ----------
1466         dims_dict : dict-like
1467             Dictionary whose keys are current dimension names and whose values
1468             are new names.
1469 
1470         Returns
1471         -------
1472         swapped : DataArray
1473             DataArray with swapped dimensions.
1474 
1475         Examples
1476         --------
1477 
1478         >>> arr = xr.DataArray(
1479         ...     data=[0, 1], dims="x", coords={"x": ["a", "b"], "y": ("x", [0, 1])},
1480         ... )
1481         >>> arr
1482         <xarray.DataArray (x: 2)>
1483         array([0, 1])
1484         Coordinates:
1485           * x        (x) <U1 'a' 'b'
1486             y        (x) int64 0 1
1487 
1488         >>> arr.swap_dims({"x": "y"})
1489         <xarray.DataArray (y: 2)>
1490         array([0, 1])
1491         Coordinates:
1492             x        (y) <U1 'a' 'b'
1493           * y        (y) int64 0 1
1494 
1495         >>> arr.swap_dims({"x": "z"})
1496         <xarray.DataArray (z: 2)>
1497         array([0, 1])
1498         Coordinates:
1499             x        (z) <U1 'a' 'b'
1500             y        (z) int64 0 1
1501         Dimensions without coordinates: z
1502 
1503         See Also
1504         --------
1505 
1506         DataArray.rename
1507         Dataset.swap_dims
1508         """
1509         ds = self._to_temp_dataset().swap_dims(dims_dict)
1510         return self._from_temp_dataset(ds)
1511 
1512     def expand_dims(
1513         self,
1514         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,
1515         axis=None,
1516         **dim_kwargs: Any,
1517     ) -> "DataArray":
1518         """Return a new object with an additional axis (or axes) inserted at
1519         the corresponding position in the array shape. The new object is a
1520         view into the underlying array, not a copy.
1521 
1522 
1523         If dim is already a scalar coordinate, it will be promoted to a 1D
1524         coordinate consisting of a single value.
1525 
1526         Parameters
1527         ----------
1528         dim : hashable, sequence of hashable, dict, or None
1529             Dimensions to include on the new variable.
1530             If provided as str or sequence of str, then dimensions are inserted
1531             with length 1. If provided as a dict, then the keys are the new
1532             dimensions and the values are either integers (giving the length of
1533             the new dimensions) or sequence/ndarray (giving the coordinates of
1534             the new dimensions).
1535         axis : integer, list (or tuple) of integers, or None
1536             Axis position(s) where new axis is to be inserted (position(s) on
1537             the result array). If a list (or tuple) of integers is passed,
1538             multiple axes are inserted. In this case, dim arguments should be
1539             same length list. If axis=None is passed, all the axes will be
1540             inserted to the start of the result array.
1541         **dim_kwargs : int or sequence/ndarray
1542             The keywords are arbitrary dimensions being inserted and the values
1543             are either the lengths of the new dims (if int is given), or their
1544             coordinates. Note, this is an alternative to passing a dict to the
1545             dim kwarg and will only be used if dim is None.
1546 
1547         Returns
1548         -------
1549         expanded : same type as caller
1550             This object, but with an additional dimension(s).
1551         """
1552         if isinstance(dim, int):
1553             raise TypeError("dim should be hashable or sequence/mapping of hashables")
1554         elif isinstance(dim, Sequence) and not isinstance(dim, str):
1555             if len(dim) != len(set(dim)):
1556                 raise ValueError("dims should not contain duplicate values.")
1557             dim = dict.fromkeys(dim, 1)
1558         elif dim is not None and not isinstance(dim, Mapping):
1559             dim = {cast(Hashable, dim): 1}
1560 
1561         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
1562         ds = self._to_temp_dataset().expand_dims(dim, axis)
1563         return self._from_temp_dataset(ds)
1564 
1565     def set_index(
1566         self,
1567         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
1568         append: bool = False,
1569         inplace: bool = None,
1570         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],
1571     ) -> Optional["DataArray"]:
1572         """Set DataArray (multi-)indexes using one or more existing
1573         coordinates.
1574 
1575         Parameters
1576         ----------
1577         indexes : {dim: index, ...}
1578             Mapping from names matching dimensions and values given
1579             by (lists of) the names of existing coordinates or variables to set
1580             as new (multi-)index.
1581         append : bool, optional
1582             If True, append the supplied index(es) to the existing index(es).
1583             Otherwise replace the existing index(es) (default).
1584         **indexes_kwargs: optional
1585             The keyword arguments form of ``indexes``.
1586             One of indexes or indexes_kwargs must be provided.
1587 
1588         Returns
1589         -------
1590         obj : DataArray
1591             Another DataArray, with this data but replaced coordinates.
1592 
1593         Examples
1594         --------
1595         >>> arr = xr.DataArray(
1596         ...     data=np.ones((2, 3)),
1597         ...     dims=["x", "y"],
1598         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
1599         ... )
1600         >>> arr
1601         <xarray.DataArray (x: 2, y: 3)>
1602         array([[1., 1., 1.],
1603                [1., 1., 1.]])
1604         Coordinates:
1605           * x        (x) int64 0 1
1606           * y        (y) int64 0 1 2
1607             a        (x) int64 3 4
1608         >>> arr.set_index(x="a")
1609         <xarray.DataArray (x: 2, y: 3)>
1610         array([[1., 1., 1.],
1611                [1., 1., 1.]])
1612         Coordinates:
1613           * x        (x) int64 3 4
1614           * y        (y) int64 0 1 2
1615 
1616         See Also
1617         --------
1618         DataArray.reset_index
1619         """
1620         ds = self._to_temp_dataset().set_index(
1621             indexes, append=append, inplace=inplace, **indexes_kwargs
1622         )
1623         return self._from_temp_dataset(ds)
1624 
1625     def reset_index(
1626         self,
1627         dims_or_levels: Union[Hashable, Sequence[Hashable]],
1628         drop: bool = False,
1629         inplace: bool = None,
1630     ) -> Optional["DataArray"]:
1631         """Reset the specified index(es) or multi-index level(s).
1632 
1633         Parameters
1634         ----------
1635         dims_or_levels : hashable or sequence of hashables
1636             Name(s) of the dimension(s) and/or multi-index level(s) that will
1637             be reset.
1638         drop : bool, optional
1639             If True, remove the specified indexes and/or multi-index levels
1640             instead of extracting them as new coordinates (default: False).
1641 
1642         Returns
1643         -------
1644         obj : DataArray
1645             Another dataarray, with this dataarray's data but replaced
1646             coordinates.
1647 
1648         See Also
1649         --------
1650         DataArray.set_index
1651         """
1652         _check_inplace(inplace)
1653         coords, _ = split_indexes(
1654             dims_or_levels, self._coords, set(), self._level_coords, drop=drop
1655         )
1656         return self._replace(coords=coords)
1657 
1658     def reorder_levels(
1659         self,
1660         dim_order: Mapping[Hashable, Sequence[int]] = None,
1661         inplace: bool = None,
1662         **dim_order_kwargs: Sequence[int],
1663     ) -> "DataArray":
1664         """Rearrange index levels using input order.
1665 
1666         Parameters
1667         ----------
1668         dim_order : optional
1669             Mapping from names matching dimensions and values given
1670             by lists representing new level orders. Every given dimension
1671             must have a multi-index.
1672         **dim_order_kwargs: optional
1673             The keyword arguments form of ``dim_order``.
1674             One of dim_order or dim_order_kwargs must be provided.
1675 
1676         Returns
1677         -------
1678         obj : DataArray
1679             Another dataarray, with this dataarray's data but replaced
1680             coordinates.
1681         """
1682         _check_inplace(inplace)
1683         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
1684         replace_coords = {}
1685         for dim, order in dim_order.items():
1686             coord = self._coords[dim]
1687             index = coord.to_index()
1688             if not isinstance(index, pd.MultiIndex):
1689                 raise ValueError("coordinate %r has no MultiIndex" % dim)
1690             replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))
1691         coords = self._coords.copy()
1692         coords.update(replace_coords)
1693         return self._replace(coords=coords)
1694 
1695     def stack(
1696         self,
1697         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
1698         **dimensions_kwargs: Sequence[Hashable],
1699     ) -> "DataArray":
1700         """
1701         Stack any number of existing dimensions into a single new dimension.
1702 
1703         New dimensions will be added at the end, and the corresponding
1704         coordinate variables will be combined into a MultiIndex.
1705 
1706         Parameters
1707         ----------
1708         dimensions : Mapping of the form new_name=(dim1, dim2, ...)
1709             Names of new dimensions, and the existing dimensions that they
1710             replace.
1711         **dimensions_kwargs:
1712             The keyword arguments form of ``dimensions``.
1713             One of dimensions or dimensions_kwargs must be provided.
1714 
1715         Returns
1716         -------
1717         stacked : DataArray
1718             DataArray with stacked data.
1719 
1720         Examples
1721         --------
1722 
1723         >>> arr = xr.DataArray(
1724         ...     np.arange(6).reshape(2, 3),
1725         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
1726         ... )
1727         >>> arr
1728         <xarray.DataArray (x: 2, y: 3)>
1729         array([[0, 1, 2],
1730                [3, 4, 5]])
1731         Coordinates:
1732           * x        (x) |S1 'a' 'b'
1733           * y        (y) int64 0 1 2
1734         >>> stacked = arr.stack(z=("x", "y"))
1735         >>> stacked.indexes["z"]
1736         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1737                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],
1738                    names=['x', 'y'])
1739 
1740         See Also
1741         --------
1742         DataArray.unstack
1743         """
1744         ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)
1745         return self._from_temp_dataset(ds)
1746 
1747     def unstack(
1748         self,
1749         dim: Union[Hashable, Sequence[Hashable], None] = None,
1750         fill_value: Any = dtypes.NA,
1751         sparse: bool = False,
1752     ) -> "DataArray":
1753         """
1754         Unstack existing dimensions corresponding to MultiIndexes into
1755         multiple new dimensions.
1756 
1757         New dimensions will be added at the end.
1758 
1759         Parameters
1760         ----------
1761         dim : hashable or sequence of hashable, optional
1762             Dimension(s) over which to unstack. By default unstacks all
1763             MultiIndexes.
1764         fill_value: value to be filled. By default, np.nan
1765         sparse: use sparse-array if True
1766 
1767         Returns
1768         -------
1769         unstacked : DataArray
1770             Array with unstacked data.
1771 
1772         Examples
1773         --------
1774 
1775         >>> arr = xr.DataArray(
1776         ...     np.arange(6).reshape(2, 3),
1777         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
1778         ... )
1779         >>> arr
1780         <xarray.DataArray (x: 2, y: 3)>
1781         array([[0, 1, 2],
1782                [3, 4, 5]])
1783         Coordinates:
1784           * x        (x) |S1 'a' 'b'
1785           * y        (y) int64 0 1 2
1786         >>> stacked = arr.stack(z=("x", "y"))
1787         >>> stacked.indexes["z"]
1788         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1789                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],
1790                    names=['x', 'y'])
1791         >>> roundtripped = stacked.unstack()
1792         >>> arr.identical(roundtripped)
1793         True
1794 
1795         See Also
1796         --------
1797         DataArray.stack
1798         """
1799         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
1800         return self._from_temp_dataset(ds)
1801 
1802     def to_unstacked_dataset(self, dim, level=0):
1803         """Unstack DataArray expanding to Dataset along a given level of a
1804         stacked coordinate.
1805 
1806         This is the inverse operation of Dataset.to_stacked_array.
1807 
1808         Parameters
1809         ----------
1810         dim : str
1811             Name of existing dimension to unstack
1812         level : int or str
1813             The MultiIndex level to expand to a dataset along. Can either be
1814             the integer index of the level or its name.
1815         label : int, default 0
1816             Label of the level to expand dataset along. Overrides the label
1817             argument if given.
1818 
1819         Returns
1820         -------
1821         unstacked: Dataset
1822 
1823         Examples
1824         --------
1825         >>> import xarray as xr
1826         >>> arr = xr.DataArray(
1827         ...     np.arange(6).reshape(2, 3),
1828         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
1829         ... )
1830         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
1831         >>> data
1832         <xarray.Dataset>
1833         Dimensions:  (x: 2, y: 3)
1834         Coordinates:
1835           * x        (x) <U1 'a' 'b'
1836           * y        (y) int64 0 1 2
1837         Data variables:
1838             a        (x, y) int64 0 1 2 3 4 5
1839             b        (x) int64 0 3
1840         >>> stacked = data.to_stacked_array("z", ["y"])
1841         >>> stacked.indexes["z"]
1842         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1843                 labels=[[0, 0, 0, 1], [0, 1, 2, -1]],
1844                 names=['variable', 'y'])
1845         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
1846         >>> data.identical(roundtripped)
1847         True
1848 
1849         See Also
1850         --------
1851         Dataset.to_stacked_array
1852         """
1853 
1854         idx = self.indexes[dim]
1855         if not isinstance(idx, pd.MultiIndex):
1856             raise ValueError(f"'{dim}' is not a stacked coordinate")
1857 
1858         level_number = idx._get_level_number(level)
1859         variables = idx.levels[level_number]
1860         variable_dim = idx.names[level_number]
1861 
1862         # pull variables out of datarray
1863         data_dict = {}
1864         for k in variables:
1865             data_dict[k] = self.sel({variable_dim: k}).squeeze(drop=True)
1866 
1867         # unstacked dataset
1868         return Dataset(data_dict)
1869 
1870     def transpose(self, *dims: Hashable, transpose_coords: bool = None) -> "DataArray":
1871         """Return a new DataArray object with transposed dimensions.
1872 
1873         Parameters
1874         ----------
1875         *dims : hashable, optional
1876             By default, reverse the dimensions. Otherwise, reorder the
1877             dimensions to this order.
1878         transpose_coords : boolean, optional
1879             If True, also transpose the coordinates of this DataArray.
1880 
1881         Returns
1882         -------
1883         transposed : DataArray
1884             The returned DataArray's array is transposed.
1885 
1886         Notes
1887         -----
1888         This operation returns a view of this array's data. It is
1889         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
1890         -- the data will be fully loaded.
1891 
1892         See Also
1893         --------
1894         numpy.transpose
1895         Dataset.transpose
1896         """
1897         if dims:
1898             dims = tuple(utils.infix_dims(dims, self.dims))
1899         variable = self.variable.transpose(*dims)
1900         if transpose_coords:
1901             coords: Dict[Hashable, Variable] = {}
1902             for name, coord in self.coords.items():
1903                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
1904                 coords[name] = coord.variable.transpose(*coord_dims)
1905             return self._replace(variable, coords)
1906         else:
1907             if transpose_coords is None and any(self[c].ndim > 1 for c in self.coords):
1908                 warnings.warn(
1909                     "This DataArray contains multi-dimensional "
1910                     "coordinates. In the future, these coordinates "
1911                     "will be transposed as well unless you specify "
1912                     "transpose_coords=False.",
1913                     FutureWarning,
1914                     stacklevel=2,
1915                 )
1916             return self._replace(variable)
1917 
1918     @property
1919     def T(self) -> "DataArray":
1920         return self.transpose()
1921 
1922     def drop_vars(
1923         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
1924     ) -> "DataArray":
1925         """Drop variables from this DataArray.
1926 
1927         Parameters
1928         ----------
1929         names : hashable or iterable of hashables
1930             Name(s) of variables to drop.
1931         errors: {'raise', 'ignore'}, optional
1932             If 'raise' (default), raises a ValueError error if any of the variable
1933             passed are not in the dataset. If 'ignore', any given names that are in the
1934             DataArray are dropped and no error is raised.
1935 
1936         Returns
1937         -------
1938         dropped : Dataset
1939 
1940         """
1941         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
1942         return self._from_temp_dataset(ds)
1943 
1944     def drop(
1945         self,
1946         labels: Mapping = None,
1947         dim: Hashable = None,
1948         *,
1949         errors: str = "raise",
1950         **labels_kwargs,
1951     ) -> "DataArray":
1952         """Backward compatible method based on `drop_vars` and `drop_sel`
1953 
1954         Using either `drop_vars` or `drop_sel` is encouraged
1955 
1956         See Also
1957         --------
1958         DataArray.drop_vars
1959         DataArray.drop_sel
1960         """
1961         ds = self._to_temp_dataset().drop(labels, dim, errors=errors)
1962         return self._from_temp_dataset(ds)
1963 
1964     def drop_sel(
1965         self,
1966         labels: Mapping[Hashable, Any] = None,
1967         *,
1968         errors: str = "raise",
1969         **labels_kwargs,
1970     ) -> "DataArray":
1971         """Drop index labels from this DataArray.
1972 
1973         Parameters
1974         ----------
1975         labels : Mapping[Hashable, Any]
1976             Index labels to drop
1977         errors: {'raise', 'ignore'}, optional
1978             If 'raise' (default), raises a ValueError error if
1979             any of the index labels passed are not
1980             in the dataset. If 'ignore', any given labels that are in the
1981             dataset are dropped and no error is raised.
1982         **labels_kwargs : {dim: label, ...}, optional
1983             The keyword arguments form of ``dim`` and ``labels``
1984 
1985         Returns
1986         -------
1987         dropped : DataArray
1988         """
1989         if labels_kwargs or isinstance(labels, dict):
1990             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
1991 
1992         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
1993         return self._from_temp_dataset(ds)
1994 
1995     def dropna(
1996         self, dim: Hashable, how: str = "any", thresh: int = None
1997     ) -> "DataArray":
1998         """Returns a new array with dropped labels for missing values along
1999         the provided dimension.
2000 
2001         Parameters
2002         ----------
2003         dim : hashable
2004             Dimension along which to drop missing values. Dropping along
2005             multiple dimensions simultaneously is not yet supported.
2006         how : {'any', 'all'}, optional
2007             * any : if any NA values are present, drop that label
2008             * all : if all values are NA, drop that label
2009         thresh : int, default None
2010             If supplied, require this many non-NA values.
2011 
2012         Returns
2013         -------
2014         DataArray
2015         """
2016         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
2017         return self._from_temp_dataset(ds)
2018 
2019     def fillna(self, value: Any) -> "DataArray":
2020         """Fill missing values in this object.
2021 
2022         This operation follows the normal broadcasting and alignment rules that
2023         xarray uses for binary arithmetic, except the result is aligned to this
2024         object (``join='left'``) instead of aligned to the intersection of
2025         index coordinates (``join='inner'``).
2026 
2027         Parameters
2028         ----------
2029         value : scalar, ndarray or DataArray
2030             Used to fill all matching missing values in this array. If the
2031             argument is a DataArray, it is first aligned with (reindexed to)
2032             this array.
2033 
2034         Returns
2035         -------
2036         DataArray
2037         """
2038         if utils.is_dict_like(value):
2039             raise TypeError(
2040                 "cannot provide fill value as a dictionary with "
2041                 "fillna on a DataArray"
2042             )
2043         out = ops.fillna(self, value)
2044         return out
2045 
2046     def interpolate_na(
2047         self,
2048         dim: Hashable = None,
2049         method: str = "linear",
2050         limit: int = None,
2051         use_coordinate: Union[bool, str] = True,
2052         max_gap: Union[
2053             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta
2054         ] = None,
2055         **kwargs: Any,
2056     ) -> "DataArray":
2057         """Fill in NaNs by interpolating according to different methods.
2058 
2059         Parameters
2060         ----------
2061         dim : str
2062             Specifies the dimension along which to interpolate.
2063         method : str, optional
2064             String indicating which method to use for interpolation:
2065 
2066             - 'linear': linear interpolation (Default). Additional keyword
2067               arguments are passed to :py:func:`numpy.interp`
2068             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
2069               are passed to :py:func:`scipy.interpolate.interp1d`. If
2070               ``method='polynomial'``, the ``order`` keyword argument must also be
2071               provided.
2072             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
2073               respective :py:class:`scipy.interpolate` classes.
2074 
2075         use_coordinate : bool, str, default True
2076             Specifies which index to use as the x values in the interpolation
2077             formulated as `y = f(x)`. If False, values are treated as if
2078             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
2079             used. If ``use_coordinate`` is a string, it specifies the name of a
2080             coordinate variariable to use as the index.
2081         limit : int, default None
2082             Maximum number of consecutive NaNs to fill. Must be greater than 0
2083             or None for no limit. This filling is done regardless of the size of
2084             the gap in the data. To only interpolate over gaps less than a given length,
2085             see ``max_gap``.
2086         max_gap: int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default None.
2087             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
2088             Use None for no limit. When interpolating along a datetime64 dimension
2089             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
2090 
2091             - a string that is valid input for pandas.to_timedelta
2092             - a :py:class:`numpy.timedelta64` object
2093             - a :py:class:`pandas.Timedelta` object
2094             - a :py:class:`datetime.timedelta` object
2095 
2096             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
2097             dimensions has not been implemented yet. Gap length is defined as the difference
2098             between coordinate values at the first data point after a gap and the last value
2099             before a gap. For gaps at the beginning (end), gap length is defined as the difference
2100             between coordinate values at the first (last) valid data point and the first (last) NaN.
2101             For example, consider::
2102 
2103                 <xarray.DataArray (x: 9)>
2104                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
2105                 Coordinates:
2106                   * x        (x) int64 0 1 2 3 4 5 6 7 8
2107 
2108             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
2109         kwargs : dict, optional
2110             parameters passed verbatim to the underlying interpolation function
2111 
2112         Returns
2113         -------
2114         interpolated: DataArray
2115             Filled in DataArray.
2116 
2117         See also
2118         --------
2119         numpy.interp
2120         scipy.interpolate
2121         """
2122         from .missing import interp_na
2123 
2124         return interp_na(
2125             self,
2126             dim=dim,
2127             method=method,
2128             limit=limit,
2129             use_coordinate=use_coordinate,
2130             max_gap=max_gap,
2131             **kwargs,
2132         )
2133 
2134     def ffill(self, dim: Hashable, limit: int = None) -> "DataArray":
2135         """Fill NaN values by propogating values forward
2136 
2137         *Requires bottleneck.*
2138 
2139         Parameters
2140         ----------
2141         dim : hashable
2142             Specifies the dimension along which to propagate values when
2143             filling.
2144         limit : int, default None
2145             The maximum number of consecutive NaN values to forward fill. In
2146             other words, if there is a gap with more than this number of
2147             consecutive NaNs, it will only be partially filled. Must be greater
2148             than 0 or None for no limit.
2149 
2150         Returns
2151         -------
2152         DataArray
2153         """
2154         from .missing import ffill
2155 
2156         return ffill(self, dim, limit=limit)
2157 
2158     def bfill(self, dim: Hashable, limit: int = None) -> "DataArray":
2159         """Fill NaN values by propogating values backward
2160 
2161         *Requires bottleneck.*
2162 
2163         Parameters
2164         ----------
2165         dim : str
2166             Specifies the dimension along which to propagate values when
2167             filling.
2168         limit : int, default None
2169             The maximum number of consecutive NaN values to backward fill. In
2170             other words, if there is a gap with more than this number of
2171             consecutive NaNs, it will only be partially filled. Must be greater
2172             than 0 or None for no limit.
2173 
2174         Returns
2175         -------
2176         DataArray
2177         """
2178         from .missing import bfill
2179 
2180         return bfill(self, dim, limit=limit)
2181 
2182     def combine_first(self, other: "DataArray") -> "DataArray":
2183         """Combine two DataArray objects, with union of coordinates.
2184 
2185         This operation follows the normal broadcasting and alignment rules of
2186         ``join='outer'``.  Default to non-null values of array calling the
2187         method.  Use np.nan to fill in vacant cells after alignment.
2188 
2189         Parameters
2190         ----------
2191         other : DataArray
2192             Used to fill all matching missing values in this array.
2193 
2194         Returns
2195         -------
2196         DataArray
2197         """
2198         return ops.fillna(self, other, join="outer")
2199 
2200     def reduce(
2201         self,
2202         func: Callable[..., Any],
2203         dim: Union[None, Hashable, Sequence[Hashable]] = None,
2204         axis: Union[None, int, Sequence[int]] = None,
2205         keep_attrs: bool = None,
2206         keepdims: bool = False,
2207         **kwargs: Any,
2208     ) -> "DataArray":
2209         """Reduce this array by applying `func` along some dimension(s).
2210 
2211         Parameters
2212         ----------
2213         func : function
2214             Function which can be called in the form
2215             `f(x, axis=axis, **kwargs)` to return the result of reducing an
2216             np.ndarray over an integer valued axis.
2217         dim : hashable or sequence of hashables, optional
2218             Dimension(s) over which to apply `func`.
2219         axis : int or sequence of int, optional
2220             Axis(es) over which to repeatedly apply `func`. Only one of the
2221             'dim' and 'axis' arguments can be supplied. If neither are
2222             supplied, then the reduction is calculated over the flattened array
2223             (by calling `f(x)` without an axis argument).
2224         keep_attrs : bool, optional
2225             If True, the variable's attributes (`attrs`) will be copied from
2226             the original object to the new one.  If False (default), the new
2227             object will be returned without attributes.
2228         keepdims : bool, default False
2229             If True, the dimensions which are reduced are left in the result
2230             as dimensions of size one. Coordinates that use these dimensions
2231             are removed.
2232         **kwargs : dict
2233             Additional keyword arguments passed on to `func`.
2234 
2235         Returns
2236         -------
2237         reduced : DataArray
2238             DataArray with this object's array replaced with an array with
2239             summarized data and the indicated dimension(s) removed.
2240         """
2241 
2242         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
2243         return self._replace_maybe_drop_dims(var)
2244 
2245     def to_pandas(self) -> Union["DataArray", pd.Series, pd.DataFrame]:
2246         """Convert this array into a pandas object with the same shape.
2247 
2248         The type of the returned object depends on the number of DataArray
2249         dimensions:
2250 
2251         * 0D -> `xarray.DataArray`
2252         * 1D -> `pandas.Series`
2253         * 2D -> `pandas.DataFrame`
2254 
2255         Only works for arrays with 2 or fewer dimensions.
2256 
2257         The DataArray constructor performs the inverse transformation.
2258         """
2259         # TODO: consolidate the info about pandas constructors and the
2260         # attributes that correspond to their indexes into a separate module?
2261         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
2262         try:
2263             constructor = constructors[self.ndim]
2264         except KeyError:
2265             raise ValueError(
2266                 "cannot convert arrays with %s dimensions into "
2267                 "pandas objects" % self.ndim
2268             )
2269         indexes = [self.get_index(dim) for dim in self.dims]
2270         return constructor(self.values, *indexes)
2271 
2272     def to_dataframe(self, name: Hashable = None) -> pd.DataFrame:
2273         """Convert this array and its coordinates into a tidy pandas.DataFrame.
2274 
2275         The DataFrame is indexed by the Cartesian product of index coordinates
2276         (in the form of a :py:class:`pandas.MultiIndex`).
2277 
2278         Other coordinates are included as columns in the DataFrame.
2279         """
2280         if name is None:
2281             name = self.name
2282         if name is None:
2283             raise ValueError(
2284                 "cannot convert an unnamed DataArray to a "
2285                 "DataFrame: use the ``name`` parameter"
2286             )
2287 
2288         dims = dict(zip(self.dims, self.shape))
2289         # By using a unique name, we can convert a DataArray into a DataFrame
2290         # even if it shares a name with one of its coordinates.
2291         # I would normally use unique_name = object() but that results in a
2292         # dataframe with columns in the wrong order, for reasons I have not
2293         # been able to debug (possibly a pandas bug?).
2294         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
2295         ds = self._to_dataset_whole(name=unique_name)
2296         df = ds._to_dataframe(dims)
2297         df.columns = [name if c == unique_name else c for c in df.columns]
2298         return df
2299 
2300     def to_series(self) -> pd.Series:
2301         """Convert this array into a pandas.Series.
2302 
2303         The Series is indexed by the Cartesian product of index coordinates
2304         (in the form of a :py:class:`pandas.MultiIndex`).
2305         """
2306         index = self.coords.to_index()
2307         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
2308 
2309     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
2310         """Convert this array into a numpy.ma.MaskedArray
2311 
2312         Parameters
2313         ----------
2314         copy : bool
2315             If True (default) make a copy of the array in the result. If False,
2316             a MaskedArray view of DataArray.values is returned.
2317 
2318         Returns
2319         -------
2320         result : MaskedArray
2321             Masked where invalid values (nan or inf) occur.
2322         """
2323         values = self.values  # only compute lazy arrays once
2324         isnull = pd.isnull(values)
2325         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
2326 
2327     def to_netcdf(self, *args, **kwargs) -> Union[bytes, "Delayed", None]:
2328         """Write DataArray contents to a netCDF file.
2329 
2330         All parameters are passed directly to `xarray.Dataset.to_netcdf`.
2331 
2332         Notes
2333         -----
2334         Only xarray.Dataset objects can be written to netCDF files, so
2335         the xarray.DataArray is converted to a xarray.Dataset object
2336         containing a single variable. If the DataArray has no name, or if the
2337         name is the same as a co-ordinate name, then it is given the name
2338         '__xarray_dataarray_variable__'.
2339         """
2340         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE
2341 
2342         if self.name is None:
2343             # If no name is set then use a generic xarray name
2344             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2345         elif self.name in self.coords or self.name in self.dims:
2346             # The name is the same as one of the coords names, which netCDF
2347             # doesn't support, so rename it but keep track of the old name
2348             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2349             dataset.attrs[DATAARRAY_NAME] = self.name
2350         else:
2351             # No problems with the name - so we're fine!
2352             dataset = self.to_dataset()
2353 
2354         return dataset.to_netcdf(*args, **kwargs)
2355 
2356     def to_dict(self, data: bool = True) -> dict:
2357         """
2358         Convert this xarray.DataArray into a dictionary following xarray
2359         naming conventions.
2360 
2361         Converts all variables and attributes to native Python objects.
2362         Useful for converting to json. To avoid datetime incompatibility
2363         use decode_times=False kwarg in xarrray.open_dataset.
2364 
2365         Parameters
2366         ----------
2367         data : bool, optional
2368             Whether to include the actual data in the dictionary. When set to
2369             False, returns just the schema.
2370 
2371         See also
2372         --------
2373         DataArray.from_dict
2374         """
2375         d = self.variable.to_dict(data=data)
2376         d.update({"coords": {}, "name": self.name})
2377         for k in self.coords:
2378             d["coords"][k] = self.coords[k].variable.to_dict(data=data)
2379         return d
2380 
2381     @classmethod
2382     def from_dict(cls, d: dict) -> "DataArray":
2383         """
2384         Convert a dictionary into an xarray.DataArray
2385 
2386         Input dict can take several forms::
2387 
2388             d = {'dims': ('t'), 'data': x}
2389 
2390             d = {'coords': {'t': {'dims': 't', 'data': t,
2391                                   'attrs': {'units':'s'}}},
2392                  'attrs': {'title': 'air temperature'},
2393                  'dims': 't',
2394                  'data': x,
2395                  'name': 'a'}
2396 
2397         where 't' is the name of the dimesion, 'a' is the name of the array,
2398         and  x and t are lists, numpy.arrays, or pandas objects.
2399 
2400         Parameters
2401         ----------
2402         d : dict, with a minimum structure of {'dims': [..], 'data': [..]}
2403 
2404         Returns
2405         -------
2406         obj : xarray.DataArray
2407 
2408         See also
2409         --------
2410         DataArray.to_dict
2411         Dataset.from_dict
2412         """
2413         coords = None
2414         if "coords" in d:
2415             try:
2416                 coords = {
2417                     k: (v["dims"], v["data"], v.get("attrs"))
2418                     for k, v in d["coords"].items()
2419                 }
2420             except KeyError as e:
2421                 raise ValueError(
2422                     "cannot convert dict when coords are missing the key "
2423                     "'{dims_data}'".format(dims_data=str(e.args[0]))
2424                 )
2425         try:
2426             data = d["data"]
2427         except KeyError:
2428             raise ValueError("cannot convert dict without the key 'data''")
2429         else:
2430             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
2431         return obj
2432 
2433     @classmethod
2434     def from_series(cls, series: pd.Series, sparse: bool = False) -> "DataArray":
2435         """Convert a pandas.Series into an xarray.DataArray.
2436 
2437         If the series's index is a MultiIndex, it will be expanded into a
2438         tensor product of one-dimensional coordinates (filling in missing
2439         values with NaN). Thus this operation should be the inverse of the
2440         `to_series` method.
2441 
2442         If sparse=True, creates a sparse array instead of a dense NumPy array.
2443         Requires the pydata/sparse package.
2444 
2445         See also
2446         --------
2447         xarray.Dataset.from_dataframe
2448         """
2449         temp_name = "__temporary_name"
2450         df = pd.DataFrame({temp_name: series})
2451         ds = Dataset.from_dataframe(df, sparse=sparse)
2452         result = cast(DataArray, ds[temp_name])
2453         result.name = series.name
2454         return result
2455 
2456     def to_cdms2(self) -> "cdms2_Variable":
2457         """Convert this array into a cdms2.Variable
2458         """
2459         from ..convert import to_cdms2
2460 
2461         return to_cdms2(self)
2462 
2463     @classmethod
2464     def from_cdms2(cls, variable: "cdms2_Variable") -> "DataArray":
2465         """Convert a cdms2.Variable into an xarray.DataArray
2466         """
2467         from ..convert import from_cdms2
2468 
2469         return from_cdms2(variable)
2470 
2471     def to_iris(self) -> "iris_Cube":
2472         """Convert this array into a iris.cube.Cube
2473         """
2474         from ..convert import to_iris
2475 
2476         return to_iris(self)
2477 
2478     @classmethod
2479     def from_iris(cls, cube: "iris_Cube") -> "DataArray":
2480         """Convert a iris.cube.Cube into an xarray.DataArray
2481         """
2482         from ..convert import from_iris
2483 
2484         return from_iris(cube)
2485 
2486     def _all_compat(self, other: "DataArray", compat_str: str) -> bool:
2487         """Helper function for equals, broadcast_equals, and identical
2488         """
2489 
2490         def compat(x, y):
2491             return getattr(x.variable, compat_str)(y.variable)
2492 
2493         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
2494             self, other
2495         )
2496 
2497     def broadcast_equals(self, other: "DataArray") -> bool:
2498         """Two DataArrays are broadcast equal if they are equal after
2499         broadcasting them against each other such that they have the same
2500         dimensions.
2501 
2502         See Also
2503         --------
2504         DataArray.equals
2505         DataArray.identical
2506         """
2507         try:
2508             return self._all_compat(other, "broadcast_equals")
2509         except (TypeError, AttributeError):
2510             return False
2511 
2512     def equals(self, other: "DataArray") -> bool:
2513         """True if two DataArrays have the same dimensions, coordinates and
2514         values; otherwise False.
2515 
2516         DataArrays can still be equal (like pandas objects) if they have NaN
2517         values in the same locations.
2518 
2519         This method is necessary because `v1 == v2` for ``DataArray``
2520         does element-wise comparisons (like numpy.ndarrays).
2521 
2522         See Also
2523         --------
2524         DataArray.broadcast_equals
2525         DataArray.identical
2526         """
2527         try:
2528             return self._all_compat(other, "equals")
2529         except (TypeError, AttributeError):
2530             return False
2531 
2532     def identical(self, other: "DataArray") -> bool:
2533         """Like equals, but also checks the array name and attributes, and
2534         attributes on all coordinates.
2535 
2536         See Also
2537         --------
2538         DataArray.broadcast_equals
2539         DataArray.equal
2540         """
2541         try:
2542             return self.name == other.name and self._all_compat(other, "identical")
2543         except (TypeError, AttributeError):
2544             return False
2545 
2546     def _result_name(self, other: Any = None) -> Optional[Hashable]:
2547         # use the same naming heuristics as pandas:
2548         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
2549         other_name = getattr(other, "name", _default)
2550         if other_name is _default or other_name == self.name:
2551             return self.name
2552         else:
2553             return None
2554 
2555     def __array_wrap__(self, obj, context=None) -> "DataArray":
2556         new_var = self.variable.__array_wrap__(obj, context)
2557         return self._replace(new_var)
2558 
2559     def __matmul__(self, obj):
2560         return self.dot(obj)
2561 
2562     def __rmatmul__(self, other):
2563         # currently somewhat duplicative, as only other DataArrays are
2564         # compatible with matmul
2565         return computation.dot(other, self)
2566 
2567     @staticmethod
2568     def _unary_op(f: Callable[..., Any]) -> Callable[..., "DataArray"]:
2569         @functools.wraps(f)
2570         def func(self, *args, **kwargs):
2571             with np.errstate(all="ignore"):
2572                 return self.__array_wrap__(f(self.variable.data, *args, **kwargs))
2573 
2574         return func
2575 
2576     @staticmethod
2577     def _binary_op(
2578         f: Callable[..., Any],
2579         reflexive: bool = False,
2580         join: str = None,  # see xarray.align
2581         **ignored_kwargs,
2582     ) -> Callable[..., "DataArray"]:
2583         @functools.wraps(f)
2584         def func(self, other):
2585             if isinstance(other, (Dataset, groupby.GroupBy)):
2586                 return NotImplemented
2587             if isinstance(other, DataArray):
2588                 align_type = OPTIONS["arithmetic_join"] if join is None else join
2589                 self, other = align(self, other, join=align_type, copy=False)
2590             other_variable = getattr(other, "variable", other)
2591             other_coords = getattr(other, "coords", None)
2592 
2593             variable = (
2594                 f(self.variable, other_variable)
2595                 if not reflexive
2596                 else f(other_variable, self.variable)
2597             )
2598             coords, indexes = self.coords._merge_raw(other_coords)
2599             name = self._result_name(other)
2600 
2601             return self._replace(variable, coords, name, indexes=indexes)
2602 
2603         return func
2604 
2605     @staticmethod
2606     def _inplace_binary_op(f: Callable) -> Callable[..., "DataArray"]:
2607         @functools.wraps(f)
2608         def func(self, other):
2609             if isinstance(other, groupby.GroupBy):
2610                 raise TypeError(
2611                     "in-place operations between a DataArray and "
2612                     "a grouped object are not permitted"
2613                 )
2614             # n.b. we can't align other to self (with other.reindex_like(self))
2615             # because `other` may be converted into floats, which would cause
2616             # in-place arithmetic to fail unpredictably. Instead, we simply
2617             # don't support automatic alignment with in-place arithmetic.
2618             other_coords = getattr(other, "coords", None)
2619             other_variable = getattr(other, "variable", other)
2620             with self.coords._merge_inplace(other_coords):
2621                 f(self.variable, other_variable)
2622             return self
2623 
2624         return func
2625 
2626     def _copy_attrs_from(self, other: Union["DataArray", Dataset, Variable]) -> None:
2627         self.attrs = other.attrs
2628 
2629     @property
2630     def plot(self) -> _PlotMethods:
2631         """
2632         Access plotting functions for DataArray's
2633 
2634         >>> d = xr.DataArray([[1, 2], [3, 4]])
2635 
2636         For convenience just call this directly
2637 
2638         >>> d.plot()
2639 
2640         Or use it as a namespace to use xarray.plot functions as
2641         DataArray methods
2642 
2643         >>> d.plot.imshow()  # equivalent to xarray.plot.imshow(d)
2644 
2645         """
2646         return _PlotMethods(self)
2647 
2648     def _title_for_slice(self, truncate: int = 50) -> str:
2649         """
2650         If the dataarray has 1 dimensional coordinates or comes from a slice
2651         we can show that info in the title
2652 
2653         Parameters
2654         ----------
2655         truncate : integer
2656             maximum number of characters for title
2657 
2658         Returns
2659         -------
2660         title : string
2661             Can be used for plot titles
2662 
2663         """
2664         one_dims = []
2665         for dim, coord in self.coords.items():
2666             if coord.size == 1:
2667                 one_dims.append(
2668                     "{dim} = {v}".format(dim=dim, v=format_item(coord.values))
2669                 )
2670 
2671         title = ", ".join(one_dims)
2672         if len(title) > truncate:
2673             title = title[: (truncate - 3)] + "..."
2674 
2675         return title
2676 
2677     def diff(self, dim: Hashable, n: int = 1, label: Hashable = "upper") -> "DataArray":
2678         """Calculate the n-th order discrete difference along given axis.
2679 
2680         Parameters
2681         ----------
2682         dim : hashable, optional
2683             Dimension over which to calculate the finite difference.
2684         n : int, optional
2685             The number of times values are differenced.
2686         label : hashable, optional
2687             The new coordinate in dimension ``dim`` will have the
2688             values of either the minuend's or subtrahend's coordinate
2689             for values 'upper' and 'lower', respectively.  Other
2690             values are not supported.
2691 
2692         Returns
2693         -------
2694         difference : same type as caller
2695             The n-th order finite difference of this object.
2696 
2697         .. note::
2698 
2699             `n` matches numpy's behavior and is different from pandas' first
2700             argument named `periods`.
2701 
2702 
2703         Examples
2704         --------
2705         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
2706         >>> arr.diff("x")
2707         <xarray.DataArray (x: 3)>
2708         array([0, 1, 0])
2709         Coordinates:
2710         * x        (x) int64 2 3 4
2711         >>> arr.diff("x", 2)
2712         <xarray.DataArray (x: 2)>
2713         array([ 1, -1])
2714         Coordinates:
2715         * x        (x) int64 3 4
2716 
2717         See Also
2718         --------
2719         DataArray.differentiate
2720         """
2721         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
2722         return self._from_temp_dataset(ds)
2723 
2724     def shift(
2725         self,
2726         shifts: Mapping[Hashable, int] = None,
2727         fill_value: Any = dtypes.NA,
2728         **shifts_kwargs: int,
2729     ) -> "DataArray":
2730         """Shift this array by an offset along one or more dimensions.
2731 
2732         Only the data is moved; coordinates stay in place. Values shifted from
2733         beyond array bounds are replaced by NaN. This is consistent with the
2734         behavior of ``shift`` in pandas.
2735 
2736         Parameters
2737         ----------
2738         shifts : Mapping with the form of {dim: offset}
2739             Integer offset to shift along each of the given dimensions.
2740             Positive offsets shift to the right; negative offsets shift to the
2741             left.
2742         fill_value: scalar, optional
2743             Value to use for newly missing values
2744         **shifts_kwargs:
2745             The keyword arguments form of ``shifts``.
2746             One of shifts or shifts_kwargs must be provided.
2747 
2748         Returns
2749         -------
2750         shifted : DataArray
2751             DataArray with the same coordinates and attributes but shifted
2752             data.
2753 
2754         See also
2755         --------
2756         roll
2757 
2758         Examples
2759         --------
2760 
2761         >>> arr = xr.DataArray([5, 6, 7], dims="x")
2762         >>> arr.shift(x=1)
2763         <xarray.DataArray (x: 3)>
2764         array([ nan,   5.,   6.])
2765         Coordinates:
2766           * x        (x) int64 0 1 2
2767         """
2768         variable = self.variable.shift(
2769             shifts=shifts, fill_value=fill_value, **shifts_kwargs
2770         )
2771         return self._replace(variable=variable)
2772 
2773     def roll(
2774         self,
2775         shifts: Mapping[Hashable, int] = None,
2776         roll_coords: bool = None,
2777         **shifts_kwargs: int,
2778     ) -> "DataArray":
2779         """Roll this array by an offset along one or more dimensions.
2780 
2781         Unlike shift, roll may rotate all variables, including coordinates
2782         if specified. The direction of rotation is consistent with
2783         :py:func:`numpy.roll`.
2784 
2785         Parameters
2786         ----------
2787         shifts : Mapping with the form of {dim: offset}
2788             Integer offset to rotate each of the given dimensions.
2789             Positive offsets roll to the right; negative offsets roll to the
2790             left.
2791         roll_coords : bool
2792             Indicates whether to  roll the coordinates by the offset
2793             The current default of roll_coords (None, equivalent to True) is
2794             deprecated and will change to False in a future version.
2795             Explicitly pass roll_coords to silence the warning.
2796         **shifts_kwargs : The keyword arguments form of ``shifts``.
2797             One of shifts or shifts_kwargs must be provided.
2798 
2799         Returns
2800         -------
2801         rolled : DataArray
2802             DataArray with the same attributes but rolled data and coordinates.
2803 
2804         See also
2805         --------
2806         shift
2807 
2808         Examples
2809         --------
2810 
2811         >>> arr = xr.DataArray([5, 6, 7], dims="x")
2812         >>> arr.roll(x=1)
2813         <xarray.DataArray (x: 3)>
2814         array([7, 5, 6])
2815         Coordinates:
2816           * x        (x) int64 2 0 1
2817         """
2818         ds = self._to_temp_dataset().roll(
2819             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
2820         )
2821         return self._from_temp_dataset(ds)
2822 
2823     @property
2824     def real(self) -> "DataArray":
2825         return self._replace(self.variable.real)
2826 
2827     @property
2828     def imag(self) -> "DataArray":
2829         return self._replace(self.variable.imag)
2830 
2831     def dot(
2832         self, other: "DataArray", dims: Union[Hashable, Sequence[Hashable], None] = None
2833     ) -> "DataArray":
2834         """Perform dot product of two DataArrays along their shared dims.
2835 
2836         Equivalent to taking taking tensordot over all shared dims.
2837 
2838         Parameters
2839         ----------
2840         other : DataArray
2841             The other array with which the dot product is performed.
2842         dims: '...', hashable or sequence of hashables, optional
2843             Which dimensions to sum over. Ellipsis ('...') sums over all dimensions.
2844             If not specified, then all the common dimensions are summed over.
2845 
2846         Returns
2847         -------
2848         result : DataArray
2849             Array resulting from the dot product over all shared dimensions.
2850 
2851         See also
2852         --------
2853         dot
2854         numpy.tensordot
2855 
2856         Examples
2857         --------
2858 
2859         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
2860         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
2861         >>> dm_vals = np.arange(4)
2862         >>> dm = xr.DataArray(dm_vals, dims=["z"])
2863 
2864         >>> dm.dims
2865         ('z')
2866 
2867         >>> da.dims
2868         ('x', 'y', 'z')
2869 
2870         >>> dot_result = da.dot(dm)
2871         >>> dot_result.dims
2872         ('x', 'y')
2873 
2874         """
2875         if isinstance(other, Dataset):
2876             raise NotImplementedError(
2877                 "dot products are not yet supported with Dataset objects."
2878             )
2879         if not isinstance(other, DataArray):
2880             raise TypeError("dot only operates on DataArrays.")
2881 
2882         return computation.dot(self, other, dims=dims)
2883 
2884     def sortby(
2885         self,
2886         variables: Union[Hashable, "DataArray", Sequence[Union[Hashable, "DataArray"]]],
2887         ascending: bool = True,
2888     ) -> "DataArray":
2889         """Sort object by labels or values (along an axis).
2890 
2891         Sorts the dataarray, either along specified dimensions,
2892         or according to values of 1-D dataarrays that share dimension
2893         with calling object.
2894 
2895         If the input variables are dataarrays, then the dataarrays are aligned
2896         (via left-join) to the calling object prior to sorting by cell values.
2897         NaNs are sorted to the end, following Numpy convention.
2898 
2899         If multiple sorts along the same dimension is
2900         given, numpy's lexsort is performed along that dimension:
2901         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
2902         and the FIRST key in the sequence is used as the primary sort key,
2903         followed by the 2nd key, etc.
2904 
2905         Parameters
2906         ----------
2907         variables: hashable, DataArray, or sequence of either
2908             1D DataArray objects or name(s) of 1D variable(s) in
2909             coords whose values are used to sort this array.
2910         ascending: boolean, optional
2911             Whether to sort by ascending or descending order.
2912 
2913         Returns
2914         -------
2915         sorted: DataArray
2916             A new dataarray where all the specified dims are sorted by dim
2917             labels.
2918 
2919         Examples
2920         --------
2921 
2922         >>> da = xr.DataArray(
2923         ...     np.random.rand(5),
2924         ...     coords=[pd.date_range("1/1/2000", periods=5)],
2925         ...     dims="time",
2926         ... )
2927         >>> da
2928         <xarray.DataArray (time: 5)>
2929         array([ 0.965471,  0.615637,  0.26532 ,  0.270962,  0.552878])
2930         Coordinates:
2931           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...
2932 
2933         >>> da.sortby(da)
2934         <xarray.DataArray (time: 5)>
2935         array([ 0.26532 ,  0.270962,  0.552878,  0.615637,  0.965471])
2936         Coordinates:
2937           * time     (time) datetime64[ns] 2000-01-03 2000-01-04 2000-01-05 ...
2938         """
2939         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
2940         return self._from_temp_dataset(ds)
2941 
2942     def quantile(
2943         self,
2944         q: Any,
2945         dim: Union[Hashable, Sequence[Hashable], None] = None,
2946         interpolation: str = "linear",
2947         keep_attrs: bool = None,
2948         skipna: bool = True,
2949     ) -> "DataArray":
2950         """Compute the qth quantile of the data along the specified dimension.
2951 
2952         Returns the qth quantiles(s) of the array elements.
2953 
2954         Parameters
2955         ----------
2956         q : float in range of [0,1] or array-like of floats
2957             Quantile to compute, which must be between 0 and 1 inclusive.
2958         dim : hashable or sequence of hashable, optional
2959             Dimension(s) over which to apply quantile.
2960         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
2961             This optional parameter specifies the interpolation method to
2962             use when the desired quantile lies between two data points
2963             ``i < j``:
2964 
2965                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is
2966                   the fractional part of the index surrounded by ``i`` and
2967                   ``j``.
2968                 - lower: ``i``.
2969                 - higher: ``j``.
2970                 - nearest: ``i`` or ``j``, whichever is nearest.
2971                 - midpoint: ``(i + j) / 2``.
2972         keep_attrs : bool, optional
2973             If True, the dataset's attributes (`attrs`) will be copied from
2974             the original object to the new one.  If False (default), the new
2975             object will be returned without attributes.
2976         skipna : bool, optional
2977             Whether to skip missing values when aggregating.
2978 
2979         Returns
2980         -------
2981         quantiles : DataArray
2982             If `q` is a single quantile, then the result
2983             is a scalar. If multiple percentiles are given, first axis of
2984             the result corresponds to the quantile and a quantile dimension
2985             is added to the return array. The other dimensions are the
2986             dimensions that remain after the reduction of the array.
2987 
2988         See Also
2989         --------
2990         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
2991 
2992         Examples
2993         --------
2994 
2995         >>> da = xr.DataArray(
2996         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
2997         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
2998         ...     dims=("x", "y"),
2999         ... )
3000         >>> da.quantile(0)  # or da.quantile(0, dim=...)
3001         <xarray.DataArray ()>
3002         array(0.7)
3003         Coordinates:
3004             quantile  float64 0.0
3005         >>> da.quantile(0, dim="x")
3006         <xarray.DataArray (y: 4)>
3007         array([0.7, 4.2, 2.6, 1.5])
3008         Coordinates:
3009           * y         (y) float64 1.0 1.5 2.0 2.5
3010             quantile  float64 0.0
3011         >>> da.quantile([0, 0.5, 1])
3012         <xarray.DataArray (quantile: 3)>
3013         array([0.7, 3.4, 9.4])
3014         Coordinates:
3015           * quantile  (quantile) float64 0.0 0.5 1.0
3016         >>> da.quantile([0, 0.5, 1], dim="x")
3017         <xarray.DataArray (quantile: 3, y: 4)>
3018         array([[0.7 , 4.2 , 2.6 , 1.5 ],
3019                [3.6 , 5.75, 6.  , 1.7 ],
3020                [6.5 , 7.3 , 9.4 , 1.9 ]])
3021         Coordinates:
3022           * y         (y) float64 1.0 1.5 2.0 2.5
3023           * quantile  (quantile) float64 0.0 0.5 1.0
3024         """
3025 
3026         ds = self._to_temp_dataset().quantile(
3027             q,
3028             dim=dim,
3029             keep_attrs=keep_attrs,
3030             interpolation=interpolation,
3031             skipna=skipna,
3032         )
3033         return self._from_temp_dataset(ds)
3034 
3035     def rank(
3036         self, dim: Hashable, pct: bool = False, keep_attrs: bool = None
3037     ) -> "DataArray":
3038         """Ranks the data.
3039 
3040         Equal values are assigned a rank that is the average of the ranks that
3041         would have been otherwise assigned to all of the values within that
3042         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
3043 
3044         NaNs in the input array are returned as NaNs.
3045 
3046         The `bottleneck` library is required.
3047 
3048         Parameters
3049         ----------
3050         dim : hashable
3051             Dimension over which to compute rank.
3052         pct : bool, optional
3053             If True, compute percentage ranks, otherwise compute integer ranks.
3054         keep_attrs : bool, optional
3055             If True, the dataset's attributes (`attrs`) will be copied from
3056             the original object to the new one.  If False (default), the new
3057             object will be returned without attributes.
3058 
3059         Returns
3060         -------
3061         ranked : DataArray
3062             DataArray with the same coordinates and dtype 'float64'.
3063 
3064         Examples
3065         --------
3066 
3067         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3068         >>> arr.rank("x")
3069         <xarray.DataArray (x: 3)>
3070         array([ 1.,   2.,   3.])
3071         Dimensions without coordinates: x
3072         """
3073 
3074         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
3075         return self._from_temp_dataset(ds)
3076 
3077     def differentiate(
3078         self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None
3079     ) -> "DataArray":
3080         """ Differentiate the array with the second order accurate central
3081         differences.
3082 
3083         .. note::
3084             This feature is limited to simple cartesian geometry, i.e. coord
3085             must be one dimensional.
3086 
3087         Parameters
3088         ----------
3089         coord: hashable
3090             The coordinate to be used to compute the gradient.
3091         edge_order: 1 or 2. Default 1
3092             N-th order accurate differences at the boundaries.
3093         datetime_unit: None or any of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms',
3094             'us', 'ns', 'ps', 'fs', 'as'}
3095             Unit to compute gradient. Only valid for datetime coordinate.
3096 
3097         Returns
3098         -------
3099         differentiated: DataArray
3100 
3101         See also
3102         --------
3103         numpy.gradient: corresponding numpy function
3104 
3105         Examples
3106         --------
3107 
3108         >>> da = xr.DataArray(
3109         ...     np.arange(12).reshape(4, 3),
3110         ...     dims=["x", "y"],
3111         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3112         ... )
3113         >>> da
3114         <xarray.DataArray (x: 4, y: 3)>
3115         array([[ 0,  1,  2],
3116                [ 3,  4,  5],
3117                [ 6,  7,  8],
3118                [ 9, 10, 11]])
3119         Coordinates:
3120           * x        (x) float64 0.0 0.1 1.1 1.2
3121         Dimensions without coordinates: y
3122         >>>
3123         >>> da.differentiate("x")
3124         <xarray.DataArray (x: 4, y: 3)>
3125         array([[30.      , 30.      , 30.      ],
3126                [27.545455, 27.545455, 27.545455],
3127                [27.545455, 27.545455, 27.545455],
3128                [30.      , 30.      , 30.      ]])
3129         Coordinates:
3130           * x        (x) float64 0.0 0.1 1.1 1.2
3131         Dimensions without coordinates: y
3132         """
3133         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
3134         return self._from_temp_dataset(ds)
3135 
3136     def integrate(
3137         self, dim: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None
3138     ) -> "DataArray":
3139         """ integrate the array with the trapezoidal rule.
3140 
3141         .. note::
3142             This feature is limited to simple cartesian geometry, i.e. dim
3143             must be one dimensional.
3144 
3145         Parameters
3146         ----------
3147         dim: hashable, or a sequence of hashable
3148             Coordinate(s) used for the integration.
3149         datetime_unit: str, optional
3150             Can be used to specify the unit if datetime coordinate is used.
3151             One of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', 'ps',
3152             'fs', 'as'}
3153 
3154         Returns
3155         -------
3156         integrated: DataArray
3157 
3158         See also
3159         --------
3160         numpy.trapz: corresponding numpy function
3161 
3162         Examples
3163         --------
3164 
3165         >>> da = xr.DataArray(
3166         ...     np.arange(12).reshape(4, 3),
3167         ...     dims=["x", "y"],
3168         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3169         ... )
3170         >>> da
3171         <xarray.DataArray (x: 4, y: 3)>
3172         array([[ 0,  1,  2],
3173                [ 3,  4,  5],
3174                [ 6,  7,  8],
3175                [ 9, 10, 11]])
3176         Coordinates:
3177           * x        (x) float64 0.0 0.1 1.1 1.2
3178         Dimensions without coordinates: y
3179         >>>
3180         >>> da.integrate("x")
3181         <xarray.DataArray (y: 3)>
3182         array([5.4, 6.6, 7.8])
3183         Dimensions without coordinates: y
3184         """
3185         ds = self._to_temp_dataset().integrate(dim, datetime_unit)
3186         return self._from_temp_dataset(ds)
3187 
3188     def unify_chunks(self) -> "DataArray":
3189         """ Unify chunk size along all chunked dimensions of this DataArray.
3190 
3191         Returns
3192         -------
3193 
3194         DataArray with consistent chunk sizes for all dask-array variables
3195 
3196         See Also
3197         --------
3198 
3199         dask.array.core.unify_chunks
3200         """
3201         ds = self._to_temp_dataset().unify_chunks()
3202         return self._from_temp_dataset(ds)
3203 
3204     def map_blocks(
3205         self,
3206         func: "Callable[..., T_DSorDA]",
3207         args: Sequence[Any] = (),
3208         kwargs: Mapping[str, Any] = None,
3209     ) -> "T_DSorDA":
3210         """
3211         Apply a function to each chunk of this DataArray. This method is experimental
3212         and its signature may change.
3213 
3214         Parameters
3215         ----------
3216         func: callable
3217             User-provided function that accepts a DataArray as its first parameter. The
3218             function will receive a subset of this DataArray, corresponding to one chunk
3219             along each chunked dimension. ``func`` will be executed as
3220             ``func(obj_subset, *args, **kwargs)``.
3221 
3222             The function will be first run on mocked-up data, that looks like this array
3223             but has sizes 0, to determine properties of the returned object such as
3224             dtype, variable names, new dimensions and new indexes (if any).
3225 
3226             This function must return either a single DataArray or a single Dataset.
3227 
3228             This function cannot change size of existing dimensions, or add new chunked
3229             dimensions.
3230         args: Sequence
3231             Passed verbatim to func after unpacking, after the sliced DataArray. xarray
3232             objects, if any, will not be split by chunks. Passing dask collections is
3233             not allowed.
3234         kwargs: Mapping
3235             Passed verbatim to func after unpacking. xarray objects, if any, will not be
3236             split by chunks. Passing dask collections is not allowed.
3237 
3238         Returns
3239         -------
3240         A single DataArray or Dataset with dask backend, reassembled from the outputs of
3241         the function.
3242 
3243         Notes
3244         -----
3245         This method is designed for when one needs to manipulate a whole xarray object
3246         within each chunk. In the more common case where one can work on numpy arrays,
3247         it is recommended to use apply_ufunc.
3248 
3249         If none of the variables in this DataArray is backed by dask, calling this
3250         method is equivalent to calling ``func(self, *args, **kwargs)``.
3251 
3252         See Also
3253         --------
3254         dask.array.map_blocks, xarray.apply_ufunc, xarray.map_blocks,
3255         xarray.Dataset.map_blocks
3256         """
3257         from .parallel import map_blocks
3258 
3259         return map_blocks(func, self, args, kwargs)
3260 
3261     # this needs to be at the end, or mypy will confuse with `str`
3262     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
3263     str = property(StringAccessor)

</code><intended_behavior>Introduce an 'average' method that enables weighted average calculation for data arrays. This method would be very similar to the one in 'Dataset', tailored to the 'DataArray' structure, leveraging the existing data handling methods and ensuring it integrates seamlessly with the broader xarray functionality.</intended_behavior>, <file>xarray/core/variable.py</file>
<class>None</class>
<method>None</method>
<code>
1 import copy
2 import functools
3 import itertools
4 import warnings
5 from collections import defaultdict
6 from datetime import timedelta
7 from distutils.version import LooseVersion
8 from typing import Any, Dict, Hashable, Mapping, TypeVar, Union
9 
10 import numpy as np
11 import pandas as pd
12 
13 import xarray as xr  # only for Dataset and DataArray
14 
15 from . import arithmetic, common, dtypes, duck_array_ops, indexing, nputils, ops, utils
16 from .indexing import (
17     BasicIndexer,
18     OuterIndexer,
19     PandasIndexAdapter,
20     VectorizedIndexer,
21     as_indexable,
22 )
23 from .npcompat import IS_NEP18_ACTIVE
24 from .options import _get_keep_attrs
25 from .pycompat import dask_array_type, integer_types
26 from .utils import (
27     OrderedSet,
28     _default,
29     decode_numpy_dict_values,
30     either_dict_or_kwargs,
31     ensure_us_time_resolution,
32     infix_dims,
33 )
34 
35 try:
36     import dask.array as da
37 except ImportError:
38     pass
39 
40 
41 NON_NUMPY_SUPPORTED_ARRAY_TYPES = (
42     indexing.ExplicitlyIndexed,
43     pd.Index,
44 ) + dask_array_type
45 # https://github.com/python/mypy/issues/224
46 BASIC_INDEXING_TYPES = integer_types + (slice,)  # type: ignore
47 
48 VariableType = TypeVar("VariableType", bound="Variable")
49 """Type annotation to be used when methods of Variable return self or a copy of self.
50 When called from an instance of a subclass, e.g. IndexVariable, mypy identifies the
51 output as an instance of the subclass.
52 
53 Usage::
54 
55    class Variable:
56        def f(self: VariableType, ...) -> VariableType:
57            ...
58 """
59 
60 
61 class MissingDimensionsError(ValueError):
62     """Error class used when we can't safely guess a dimension name.
63     """
64 
65     # inherits from ValueError for backward compatibility
66     # TODO: move this to an xarray.exceptions module?
67 
68 
69 def as_variable(obj, name=None) -> "Union[Variable, IndexVariable]":
70     """Convert an object into a Variable.
71 
72     Parameters
73     ----------
74     obj : object
75         Object to convert into a Variable.
76 
77         - If the object is already a Variable, return a shallow copy.
78         - Otherwise, if the object has 'dims' and 'data' attributes, convert
79           it into a new Variable.
80         - If all else fails, attempt to convert the object into a Variable by
81           unpacking it into the arguments for creating a new Variable.
82     name : str, optional
83         If provided:
84 
85         - `obj` can be a 1D array, which is assumed to label coordinate values
86           along a dimension of this given name.
87         - Variables with name matching one of their dimensions are converted
88           into `IndexVariable` objects.
89 
90     Returns
91     -------
92     var : Variable
93         The newly created variable.
94 
95     """
96     from .dataarray import DataArray
97 
98     # TODO: consider extending this method to automatically handle Iris and
99     if isinstance(obj, DataArray):
100         # extract the primary Variable from DataArrays
101         obj = obj.variable
102 
103     if isinstance(obj, Variable):
104         obj = obj.copy(deep=False)
105     elif isinstance(obj, tuple):
106         try:
107             obj = Variable(*obj)
108         except (TypeError, ValueError) as error:
109             # use .format() instead of % because it handles tuples consistently
110             raise error.__class__(
111                 "Could not convert tuple of form "
112                 "(dims, data[, attrs, encoding]): "
113                 "{} to Variable.".format(obj)
114             )
115     elif utils.is_scalar(obj):
116         obj = Variable([], obj)
117     elif isinstance(obj, (pd.Index, IndexVariable)) and obj.name is not None:
118         obj = Variable(obj.name, obj)
119     elif isinstance(obj, (set, dict)):
120         raise TypeError("variable {!r} has invalid type {!r}".format(name, type(obj)))
121     elif name is not None:
122         data = as_compatible_data(obj)
123         if data.ndim != 1:
124             raise MissingDimensionsError(
125                 "cannot set variable %r with %r-dimensional data "
126                 "without explicit dimension names. Pass a tuple of "
127                 "(dims, data) instead." % (name, data.ndim)
128             )
129         obj = Variable(name, data, fastpath=True)
130     else:
131         raise TypeError(
132             "unable to convert object into a variable without an "
133             "explicit list of dimensions: %r" % obj
134         )
135 
136     if name is not None and name in obj.dims:
137         # convert the Variable into an Index
138         if obj.ndim != 1:
139             raise MissingDimensionsError(
140                 "%r has more than 1-dimension and the same name as one of its "
141                 "dimensions %r. xarray disallows such variables because they "
142                 "conflict with the coordinates used to label "
143                 "dimensions." % (name, obj.dims)
144             )
145         obj = obj.to_index_variable()
146 
147     return obj
148 
149 
150 def _maybe_wrap_data(data):
151     """
152     Put pandas.Index and numpy.ndarray arguments in adapter objects to ensure
153     they can be indexed properly.
154 
155     NumpyArrayAdapter, PandasIndexAdapter and LazilyOuterIndexedArray should
156     all pass through unmodified.
157     """
158     if isinstance(data, pd.Index):
159         return PandasIndexAdapter(data)
160     return data
161 
162 
163 def _possibly_convert_objects(values):
164     """Convert arrays of datetime.datetime and datetime.timedelta objects into
165     datetime64 and timedelta64, according to the pandas convention.
166     """
167     return np.asarray(pd.Series(values.ravel())).reshape(values.shape)
168 
169 
170 def as_compatible_data(data, fastpath=False):
171     """Prepare and wrap data to put in a Variable.
172 
173     - If data does not have the necessary attributes, convert it to ndarray.
174     - If data has dtype=datetime64, ensure that it has ns precision. If it's a
175       pandas.Timestamp, convert it to datetime64.
176     - If data is already a pandas or xarray object (other than an Index), just
177       use the values.
178 
179     Finally, wrap it up with an adapter if necessary.
180     """
181     if fastpath and getattr(data, "ndim", 0) > 0:
182         # can't use fastpath (yet) for scalars
183         return _maybe_wrap_data(data)
184 
185     if isinstance(data, Variable):
186         return data.data
187 
188     if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):
189         return _maybe_wrap_data(data)
190 
191     if isinstance(data, tuple):
192         data = utils.to_0d_object_array(data)
193 
194     if isinstance(data, pd.Timestamp):
195         # TODO: convert, handle datetime objects, too
196         data = np.datetime64(data.value, "ns")
197 
198     if isinstance(data, timedelta):
199         data = np.timedelta64(getattr(data, "value", data), "ns")
200 
201     # we don't want nested self-described arrays
202     data = getattr(data, "values", data)
203 
204     if isinstance(data, np.ma.MaskedArray):
205         mask = np.ma.getmaskarray(data)
206         if mask.any():
207             dtype, fill_value = dtypes.maybe_promote(data.dtype)
208             data = np.asarray(data, dtype=dtype)
209             data[mask] = fill_value
210         else:
211             data = np.asarray(data)
212 
213     if not isinstance(data, np.ndarray):
214         if hasattr(data, "__array_function__"):
215             if IS_NEP18_ACTIVE:
216                 return data
217             else:
218                 raise TypeError(
219                     "Got an NumPy-like array type providing the "
220                     "__array_function__ protocol but NEP18 is not enabled. "
221                     "Check that numpy >= v1.16 and that the environment "
222                     'variable "NUMPY_EXPERIMENTAL_ARRAY_FUNCTION" is set to '
223                     '"1"'
224                 )
225 
226     # validate whether the data is valid data types
227     data = np.asarray(data)
228 
229     if isinstance(data, np.ndarray):
230         if data.dtype.kind == "O":
231             data = _possibly_convert_objects(data)
232         elif data.dtype.kind == "M":
233             data = np.asarray(data, "datetime64[ns]")
234         elif data.dtype.kind == "m":
235             data = np.asarray(data, "timedelta64[ns]")
236 
237     return _maybe_wrap_data(data)
238 
239 
240 def _as_array_or_item(data):
241     """Return the given values as a numpy array, or as an individual item if
242     it's a 0d datetime64 or timedelta64 array.
243 
244     Importantly, this function does not copy data if it is already an ndarray -
245     otherwise, it will not be possible to update Variable values in place.
246 
247     This function mostly exists because 0-dimensional ndarrays with
248     dtype=datetime64 are broken :(
249     https://github.com/numpy/numpy/issues/4337
250     https://github.com/numpy/numpy/issues/7619
251 
252     TODO: remove this (replace with np.asarray) once these issues are fixed
253     """
254     data = np.asarray(data)
255     if data.ndim == 0:
256         if data.dtype.kind == "M":
257             data = np.datetime64(data, "ns")
258         elif data.dtype.kind == "m":
259             data = np.timedelta64(data, "ns")
260     return data
261 
262 
263 class Variable(
264     common.AbstractArray, arithmetic.SupportsArithmetic, utils.NdimSizeLenMixin
265 ):
266     """A netcdf-like variable consisting of dimensions, data and attributes
267     which describe a single Array. A single Variable object is not fully
268     described outside the context of its parent Dataset (if you want such a
269     fully described object, use a DataArray instead).
270 
271     The main functional difference between Variables and numpy arrays is that
272     numerical operations on Variables implement array broadcasting by dimension
273     name. For example, adding an Variable with dimensions `('time',)` to
274     another Variable with dimensions `('space',)` results in a new Variable
275     with dimensions `('time', 'space')`. Furthermore, numpy reduce operations
276     like ``mean`` or ``sum`` are overwritten to take a "dimension" argument
277     instead of an "axis".
278 
279     Variables are light-weight objects used as the building block for datasets.
280     They are more primitive objects, so operations with them provide marginally
281     higher performance than using DataArrays. However, manipulating data in the
282     form of a Dataset or DataArray should almost always be preferred, because
283     they can use more complete metadata in context of coordinate labels.
284     """
285 
286     __slots__ = ("_dims", "_data", "_attrs", "_encoding")
287 
288     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):
289         """
290         Parameters
291         ----------
292         dims : str or sequence of str
293             Name(s) of the the data dimension(s). Must be either a string (only
294             for 1D data) or a sequence of strings with length equal to the
295             number of dimensions.
296         data : array_like
297             Data array which supports numpy-like data access.
298         attrs : dict_like or None, optional
299             Attributes to assign to the new variable. If None (default), an
300             empty attribute dictionary is initialized.
301         encoding : dict_like or None, optional
302             Dictionary specifying how to encode this array's data into a
303             serialized format like netCDF4. Currently used keys (for netCDF)
304             include '_FillValue', 'scale_factor', 'add_offset' and 'dtype'.
305             Well-behaved code to serialize a Variable should ignore
306             unrecognized encoding items.
307         """
308         self._data = as_compatible_data(data, fastpath=fastpath)
309         self._dims = self._parse_dimensions(dims)
310         self._attrs = None
311         self._encoding = None
312         if attrs is not None:
313             self.attrs = attrs
314         if encoding is not None:
315             self.encoding = encoding
316 
317     @property
318     def dtype(self):
319         return self._data.dtype
320 
321     @property
322     def shape(self):
323         return self._data.shape
324 
325     @property
326     def nbytes(self):
327         return self.size * self.dtype.itemsize
328 
329     @property
330     def _in_memory(self):
331         return isinstance(self._data, (np.ndarray, np.number, PandasIndexAdapter)) or (
332             isinstance(self._data, indexing.MemoryCachedArray)
333             and isinstance(self._data.array, indexing.NumpyIndexingAdapter)
334         )
335 
336     @property
337     def data(self):
338         if hasattr(self._data, "__array_function__") or isinstance(
339             self._data, dask_array_type
340         ):
341             return self._data
342         else:
343             return self.values
344 
345     @data.setter
346     def data(self, data):
347         data = as_compatible_data(data)
348         if data.shape != self.shape:
349             raise ValueError(
350                 f"replacement data must match the Variable's shape. "
351                 f"replacement data has shape {data.shape}; Variable has shape {self.shape}"
352             )
353         self._data = data
354 
355     def load(self, **kwargs):
356         """Manually trigger loading of this variable's data from disk or a
357         remote source into memory and return this variable.
358 
359         Normally, it should not be necessary to call this method in user code,
360         because all xarray functions should either work on deferred data or
361         load data automatically.
362 
363         Parameters
364         ----------
365         **kwargs : dict
366             Additional keyword arguments passed on to ``dask.array.compute``.
367 
368         See Also
369         --------
370         dask.array.compute
371         """
372         if isinstance(self._data, dask_array_type):
373             self._data = as_compatible_data(self._data.compute(**kwargs))
374         elif not hasattr(self._data, "__array_function__"):
375             self._data = np.asarray(self._data)
376         return self
377 
378     def compute(self, **kwargs):
379         """Manually trigger loading of this variable's data from disk or a
380         remote source into memory and return a new variable. The original is
381         left unaltered.
382 
383         Normally, it should not be necessary to call this method in user code,
384         because all xarray functions should either work on deferred data or
385         load data automatically.
386 
387         Parameters
388         ----------
389         **kwargs : dict
390             Additional keyword arguments passed on to ``dask.array.compute``.
391 
392         See Also
393         --------
394         dask.array.compute
395         """
396         new = self.copy(deep=False)
397         return new.load(**kwargs)
398 
399     def __dask_tokenize__(self):
400         # Use v.data, instead of v._data, in order to cope with the wrappers
401         # around NetCDF and the like
402         from dask.base import normalize_token
403 
404         return normalize_token((type(self), self._dims, self.data, self._attrs))
405 
406     def __dask_graph__(self):
407         if isinstance(self._data, dask_array_type):
408             return self._data.__dask_graph__()
409         else:
410             return None
411 
412     def __dask_keys__(self):
413         return self._data.__dask_keys__()
414 
415     def __dask_layers__(self):
416         return self._data.__dask_layers__()
417 
418     @property
419     def __dask_optimize__(self):
420         return self._data.__dask_optimize__
421 
422     @property
423     def __dask_scheduler__(self):
424         return self._data.__dask_scheduler__
425 
426     def __dask_postcompute__(self):
427         array_func, array_args = self._data.__dask_postcompute__()
428         return (
429             self._dask_finalize,
430             (array_func, array_args, self._dims, self._attrs, self._encoding),
431         )
432 
433     def __dask_postpersist__(self):
434         array_func, array_args = self._data.__dask_postpersist__()
435         return (
436             self._dask_finalize,
437             (array_func, array_args, self._dims, self._attrs, self._encoding),
438         )
439 
440     @staticmethod
441     def _dask_finalize(results, array_func, array_args, dims, attrs, encoding):
442         if isinstance(results, dict):  # persist case
443             name = array_args[0]
444             results = {k: v for k, v in results.items() if k[0] == name}
445         data = array_func(results, *array_args)
446         return Variable(dims, data, attrs=attrs, encoding=encoding)
447 
448     @property
449     def values(self):
450         """The variable's data as a numpy.ndarray"""
451         return _as_array_or_item(self._data)
452 
453     @values.setter
454     def values(self, values):
455         self.data = values
456 
457     def to_base_variable(self):
458         """Return this variable as a base xarray.Variable"""
459         return Variable(
460             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True
461         )
462 
463     to_variable = utils.alias(to_base_variable, "to_variable")
464 
465     def to_index_variable(self):
466         """Return this variable as an xarray.IndexVariable"""
467         return IndexVariable(
468             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True
469         )
470 
471     to_coord = utils.alias(to_index_variable, "to_coord")
472 
473     def to_index(self):
474         """Convert this variable to a pandas.Index"""
475         return self.to_index_variable().to_index()
476 
477     def to_dict(self, data=True):
478         """Dictionary representation of variable."""
479         item = {"dims": self.dims, "attrs": decode_numpy_dict_values(self.attrs)}
480         if data:
481             item["data"] = ensure_us_time_resolution(self.values).tolist()
482         else:
483             item.update({"dtype": str(self.dtype), "shape": self.shape})
484         return item
485 
486     @property
487     def dims(self):
488         """Tuple of dimension names with which this variable is associated.
489         """
490         return self._dims
491 
492     @dims.setter
493     def dims(self, value):
494         self._dims = self._parse_dimensions(value)
495 
496     def _parse_dimensions(self, dims):
497         if isinstance(dims, str):
498             dims = (dims,)
499         dims = tuple(dims)
500         if len(dims) != self.ndim:
501             raise ValueError(
502                 "dimensions %s must have the same length as the "
503                 "number of data dimensions, ndim=%s" % (dims, self.ndim)
504             )
505         return dims
506 
507     def _item_key_to_tuple(self, key):
508         if utils.is_dict_like(key):
509             return tuple(key.get(dim, slice(None)) for dim in self.dims)
510         else:
511             return key
512 
513     def _broadcast_indexes(self, key):
514         """Prepare an indexing key for an indexing operation.
515 
516         Parameters
517         -----------
518         key: int, slice, array, dict or tuple of integer, slices and arrays
519             Any valid input for indexing.
520 
521         Returns
522         -------
523         dims: tuple
524             Dimension of the resultant variable.
525         indexers: IndexingTuple subclass
526             Tuple of integer, array-like, or slices to use when indexing
527             self._data. The type of this argument indicates the type of
528             indexing to perform, either basic, outer or vectorized.
529         new_order : Optional[Sequence[int]]
530             Optional reordering to do on the result of indexing. If not None,
531             the first len(new_order) indexing should be moved to these
532             positions.
533         """
534         key = self._item_key_to_tuple(key)  # key is a tuple
535         # key is a tuple of full size
536         key = indexing.expanded_indexer(key, self.ndim)
537         # Convert a scalar Variable to an integer
538         key = tuple(
539             k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k for k in key
540         )
541         # Convert a 0d-array to an integer
542         key = tuple(
543             k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key
544         )
545 
546         if all(isinstance(k, BASIC_INDEXING_TYPES) for k in key):
547             return self._broadcast_indexes_basic(key)
548 
549         self._validate_indexers(key)
550         # Detect it can be mapped as an outer indexer
551         # If all key is unlabeled, or
552         # key can be mapped as an OuterIndexer.
553         if all(not isinstance(k, Variable) for k in key):
554             return self._broadcast_indexes_outer(key)
555 
556         # If all key is 1-dimensional and there are no duplicate labels,
557         # key can be mapped as an OuterIndexer.
558         dims = []
559         for k, d in zip(key, self.dims):
560             if isinstance(k, Variable):
561                 if len(k.dims) > 1:
562                     return self._broadcast_indexes_vectorized(key)
563                 dims.append(k.dims[0])
564             elif not isinstance(k, integer_types):
565                 dims.append(d)
566         if len(set(dims)) == len(dims):
567             return self._broadcast_indexes_outer(key)
568 
569         return self._broadcast_indexes_vectorized(key)
570 
571     def _broadcast_indexes_basic(self, key):
572         dims = tuple(
573             dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)
574         )
575         return dims, BasicIndexer(key), None
576 
577     def _validate_indexers(self, key):
578         """ Make sanity checks """
579         for dim, k in zip(self.dims, key):
580             if isinstance(k, BASIC_INDEXING_TYPES):
581                 pass
582             else:
583                 if not isinstance(k, Variable):
584                     k = np.asarray(k)
585                     if k.ndim > 1:
586                         raise IndexError(
587                             "Unlabeled multi-dimensional array cannot be "
588                             "used for indexing: {}".format(k)
589                         )
590                 if k.dtype.kind == "b":
591                     if self.shape[self.get_axis_num(dim)] != len(k):
592                         raise IndexError(
593                             "Boolean array size {:d} is used to index array "
594                             "with shape {:s}.".format(len(k), str(self.shape))
595                         )
596                     if k.ndim > 1:
597                         raise IndexError(
598                             "{}-dimensional boolean indexing is "
599                             "not supported. ".format(k.ndim)
600                         )
601                     if getattr(k, "dims", (dim,)) != (dim,):
602                         raise IndexError(
603                             "Boolean indexer should be unlabeled or on the "
604                             "same dimension to the indexed array. Indexer is "
605                             "on {:s} but the target dimension is {:s}.".format(
606                                 str(k.dims), dim
607                             )
608                         )
609 
610     def _broadcast_indexes_outer(self, key):
611         dims = tuple(
612             k.dims[0] if isinstance(k, Variable) else dim
613             for k, dim in zip(key, self.dims)
614             if not isinstance(k, integer_types)
615         )
616 
617         new_key = []
618         for k in key:
619             if isinstance(k, Variable):
620                 k = k.data
621             if not isinstance(k, BASIC_INDEXING_TYPES):
622                 k = np.asarray(k)
623                 if k.size == 0:
624                     # Slice by empty list; numpy could not infer the dtype
625                     k = k.astype(int)
626                 elif k.dtype.kind == "b":
627                     (k,) = np.nonzero(k)
628             new_key.append(k)
629 
630         return dims, OuterIndexer(tuple(new_key)), None
631 
632     def _nonzero(self):
633         """ Equivalent numpy's nonzero but returns a tuple of Varibles. """
634         # TODO we should replace dask's native nonzero
635         # after https://github.com/dask/dask/issues/1076 is implemented.
636         nonzeros = np.nonzero(self.data)
637         return tuple(Variable((dim), nz) for nz, dim in zip(nonzeros, self.dims))
638 
639     def _broadcast_indexes_vectorized(self, key):
640         variables = []
641         out_dims_set = OrderedSet()
642         for dim, value in zip(self.dims, key):
643             if isinstance(value, slice):
644                 out_dims_set.add(dim)
645             else:
646                 variable = (
647                     value
648                     if isinstance(value, Variable)
649                     else as_variable(value, name=dim)
650                 )
651                 if variable.dtype.kind == "b":  # boolean indexing case
652                     (variable,) = variable._nonzero()
653 
654                 variables.append(variable)
655                 out_dims_set.update(variable.dims)
656 
657         variable_dims = set()
658         for variable in variables:
659             variable_dims.update(variable.dims)
660 
661         slices = []
662         for i, (dim, value) in enumerate(zip(self.dims, key)):
663             if isinstance(value, slice):
664                 if dim in variable_dims:
665                     # We only convert slice objects to variables if they share
666                     # a dimension with at least one other variable. Otherwise,
667                     # we can equivalently leave them as slices aknd transpose
668                     # the result. This is significantly faster/more efficient
669                     # for most array backends.
670                     values = np.arange(*value.indices(self.sizes[dim]))
671                     variables.insert(i - len(slices), Variable((dim,), values))
672                 else:
673                     slices.append((i, value))
674 
675         try:
676             variables = _broadcast_compat_variables(*variables)
677         except ValueError:
678             raise IndexError(f"Dimensions of indexers mismatch: {key}")
679 
680         out_key = [variable.data for variable in variables]
681         out_dims = tuple(out_dims_set)
682         slice_positions = set()
683         for i, value in slices:
684             out_key.insert(i, value)
685             new_position = out_dims.index(self.dims[i])
686             slice_positions.add(new_position)
687 
688         if slice_positions:
689             new_order = [i for i in range(len(out_dims)) if i not in slice_positions]
690         else:
691             new_order = None
692 
693         return out_dims, VectorizedIndexer(tuple(out_key)), new_order
694 
695     def __getitem__(self: VariableType, key) -> VariableType:
696         """Return a new Variable object whose contents are consistent with
697         getting the provided key from the underlying data.
698 
699         NB. __getitem__ and __setitem__ implement xarray-style indexing,
700         where if keys are unlabeled arrays, we index the array orthogonally
701         with them. If keys are labeled array (such as Variables), they are
702         broadcasted with our usual scheme and then the array is indexed with
703         the broadcasted key, like numpy's fancy indexing.
704 
705         If you really want to do indexing like `x[x > 0]`, manipulate the numpy
706         array `x.values` directly.
707         """
708         dims, indexer, new_order = self._broadcast_indexes(key)
709         data = as_indexable(self._data)[indexer]
710         if new_order:
711             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)
712         return self._finalize_indexing_result(dims, data)
713 
714     def _finalize_indexing_result(self: VariableType, dims, data) -> VariableType:
715         """Used by IndexVariable to return IndexVariable objects when possible.
716         """
717         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)
718 
719     def _getitem_with_mask(self, key, fill_value=dtypes.NA):
720         """Index this Variable with -1 remapped to fill_value."""
721         # TODO(shoyer): expose this method in public API somewhere (isel?) and
722         # use it for reindex.
723         # TODO(shoyer): add a sanity check that all other integers are
724         # non-negative
725         # TODO(shoyer): add an optimization, remapping -1 to an adjacent value
726         # that is actually indexed rather than mapping it to the last value
727         # along each axis.
728 
729         if fill_value is dtypes.NA:
730             fill_value = dtypes.get_fill_value(self.dtype)
731 
732         dims, indexer, new_order = self._broadcast_indexes(key)
733 
734         if self.size:
735             if isinstance(self._data, dask_array_type):
736                 # dask's indexing is faster this way; also vindex does not
737                 # support negative indices yet:
738                 # https://github.com/dask/dask/pull/2967
739                 actual_indexer = indexing.posify_mask_indexer(indexer)
740             else:
741                 actual_indexer = indexer
742 
743             data = as_indexable(self._data)[actual_indexer]
744             mask = indexing.create_mask(indexer, self.shape, data)
745             # we need to invert the mask in order to pass data first. This helps
746             # pint to choose the correct unit
747             # TODO: revert after https://github.com/hgrecco/pint/issues/1019 is fixed
748             data = duck_array_ops.where(np.logical_not(mask), data, fill_value)
749         else:
750             # array cannot be indexed along dimensions of size 0, so just
751             # build the mask directly instead.
752             mask = indexing.create_mask(indexer, self.shape)
753             data = np.broadcast_to(fill_value, getattr(mask, "shape", ()))
754 
755         if new_order:
756             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)
757         return self._finalize_indexing_result(dims, data)
758 
759     def __setitem__(self, key, value):
760         """__setitem__ is overloaded to access the underlying numpy values with
761         orthogonal indexing.
762 
763         See __getitem__ for more details.
764         """
765         dims, index_tuple, new_order = self._broadcast_indexes(key)
766 
767         if not isinstance(value, Variable):
768             value = as_compatible_data(value)
769             if value.ndim > len(dims):
770                 raise ValueError(
771                     "shape mismatch: value array of shape %s could not be "
772                     "broadcast to indexing result with %s dimensions"
773                     % (value.shape, len(dims))
774                 )
775             if value.ndim == 0:
776                 value = Variable((), value)
777             else:
778                 value = Variable(dims[-value.ndim :], value)
779         # broadcast to become assignable
780         value = value.set_dims(dims).data
781 
782         if new_order:
783             value = duck_array_ops.asarray(value)
784             value = value[(len(dims) - value.ndim) * (np.newaxis,) + (Ellipsis,)]
785             value = duck_array_ops.moveaxis(value, new_order, range(len(new_order)))
786 
787         indexable = as_indexable(self._data)
788         indexable[index_tuple] = value
789 
790     @property
791     def attrs(self) -> Dict[Hashable, Any]:
792         """Dictionary of local attributes on this variable.
793         """
794         if self._attrs is None:
795             self._attrs = {}
796         return self._attrs
797 
798     @attrs.setter
799     def attrs(self, value: Mapping[Hashable, Any]) -> None:
800         self._attrs = dict(value)
801 
802     @property
803     def encoding(self):
804         """Dictionary of encodings on this variable.
805         """
806         if self._encoding is None:
807             self._encoding = {}
808         return self._encoding
809 
810     @encoding.setter
811     def encoding(self, value):
812         try:
813             self._encoding = dict(value)
814         except ValueError:
815             raise ValueError("encoding must be castable to a dictionary")
816 
817     def copy(self, deep=True, data=None):
818         """Returns a copy of this object.
819 
820         If `deep=True`, the data array is loaded into memory and copied onto
821         the new object. Dimensions, attributes and encodings are always copied.
822 
823         Use `data` to create a new object with the same structure as
824         original but entirely new data.
825 
826         Parameters
827         ----------
828         deep : bool, optional
829             Whether the data array is loaded into memory and copied onto
830             the new object. Default is True.
831         data : array_like, optional
832             Data to use in the new object. Must have same shape as original.
833             When `data` is used, `deep` is ignored.
834 
835         Returns
836         -------
837         object : Variable
838             New object with dimensions, attributes, encodings, and optionally
839             data copied from original.
840 
841         Examples
842         --------
843 
844         Shallow copy versus deep copy
845 
846         >>> var = xr.Variable(data=[1, 2, 3], dims="x")
847         >>> var.copy()
848         <xarray.Variable (x: 3)>
849         array([1, 2, 3])
850         >>> var_0 = var.copy(deep=False)
851         >>> var_0[0] = 7
852         >>> var_0
853         <xarray.Variable (x: 3)>
854         array([7, 2, 3])
855         >>> var
856         <xarray.Variable (x: 3)>
857         array([7, 2, 3])
858 
859         Changing the data using the ``data`` argument maintains the
860         structure of the original object, but with the new data. Original
861         object is unaffected.
862 
863         >>> var.copy(data=[0.1, 0.2, 0.3])
864         <xarray.Variable (x: 3)>
865         array([ 0.1,  0.2,  0.3])
866         >>> var
867         <xarray.Variable (x: 3)>
868         array([7, 2, 3])
869 
870         See Also
871         --------
872         pandas.DataFrame.copy
873         """
874         if data is None:
875             data = self._data
876 
877             if isinstance(data, indexing.MemoryCachedArray):
878                 # don't share caching between copies
879                 data = indexing.MemoryCachedArray(data.array)
880 
881             if deep:
882                 if hasattr(data, "__array_function__") or isinstance(
883                     data, dask_array_type
884                 ):
885                     data = data.copy()
886                 elif not isinstance(data, PandasIndexAdapter):
887                     # pandas.Index is immutable
888                     data = np.array(data)
889         else:
890             data = as_compatible_data(data)
891             if self.shape != data.shape:
892                 raise ValueError(
893                     "Data shape {} must match shape of object {}".format(
894                         data.shape, self.shape
895                     )
896                 )
897 
898         # note:
899         # dims is already an immutable tuple
900         # attributes and encoding will be copied when the new Array is created
901         return self._replace(data=data)
902 
903     def _replace(
904         self, dims=_default, data=_default, attrs=_default, encoding=_default
905     ) -> "Variable":
906         if dims is _default:
907             dims = copy.copy(self._dims)
908         if data is _default:
909             data = copy.copy(self.data)
910         if attrs is _default:
911             attrs = copy.copy(self._attrs)
912         if encoding is _default:
913             encoding = copy.copy(self._encoding)
914         return type(self)(dims, data, attrs, encoding, fastpath=True)
915 
916     def __copy__(self):
917         return self.copy(deep=False)
918 
919     def __deepcopy__(self, memo=None):
920         # memo does nothing but is required for compatibility with
921         # copy.deepcopy
922         return self.copy(deep=True)
923 
924     # mutable objects should not be hashable
925     # https://github.com/python/mypy/issues/4266
926     __hash__ = None  # type: ignore
927 
928     @property
929     def chunks(self):
930         """Block dimensions for this array's data or None if it's not a dask
931         array.
932         """
933         return getattr(self._data, "chunks", None)
934 
935     _array_counter = itertools.count()
936 
937     def chunk(self, chunks=None, name=None, lock=False):
938         """Coerce this array's data into a dask arrays with the given chunks.
939 
940         If this variable is a non-dask array, it will be converted to dask
941         array. If it's a dask array, it will be rechunked to the given chunk
942         sizes.
943 
944         If neither chunks is not provided for one or more dimensions, chunk
945         sizes along that dimension will not be updated; non-dask arrays will be
946         converted into dask arrays with a single block.
947 
948         Parameters
949         ----------
950         chunks : int, tuple or dict, optional
951             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
952             ``{'x': 5, 'y': 5}``.
953         name : str, optional
954             Used to generate the name for this array in the internal dask
955             graph. Does not need not be unique.
956         lock : optional
957             Passed on to :py:func:`dask.array.from_array`, if the array is not
958             already as dask array.
959 
960         Returns
961         -------
962         chunked : xarray.Variable
963         """
964         import dask
965         import dask.array as da
966 
967         if utils.is_dict_like(chunks):
968             chunks = {self.get_axis_num(dim): chunk for dim, chunk in chunks.items()}
969 
970         if chunks is None:
971             chunks = self.chunks or self.shape
972 
973         data = self._data
974         if isinstance(data, da.Array):
975             data = data.rechunk(chunks)
976         else:
977             if isinstance(data, indexing.ExplicitlyIndexed):
978                 # Unambiguously handle array storage backends (like NetCDF4 and h5py)
979                 # that can't handle general array indexing. For example, in netCDF4 you
980                 # can do "outer" indexing along two dimensions independent, which works
981                 # differently from how NumPy handles it.
982                 # da.from_array works by using lazy indexing with a tuple of slices.
983                 # Using OuterIndexer is a pragmatic choice: dask does not yet handle
984                 # different indexing types in an explicit way:
985                 # https://github.com/dask/dask/issues/2883
986                 data = indexing.ImplicitToExplicitIndexingAdapter(
987                     data, indexing.OuterIndexer
988                 )
989                 if LooseVersion(dask.__version__) < "2.0.0":
990                     kwargs = {}
991                 else:
992                     # All of our lazily loaded backend array classes should use NumPy
993                     # array operations.
994                     kwargs = {"meta": np.ndarray}
995             else:
996                 kwargs = {}
997 
998             if utils.is_dict_like(chunks):
999                 chunks = tuple(chunks.get(n, s) for n, s in enumerate(self.shape))
1000 
1001             data = da.from_array(data, chunks, name=name, lock=lock, **kwargs)
1002 
1003         return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)
1004 
1005     def _as_sparse(self, sparse_format=_default, fill_value=dtypes.NA):
1006         """
1007         use sparse-array as backend.
1008         """
1009         import sparse
1010 
1011         # TODO  what to do if dask-backended?
1012         if fill_value is dtypes.NA:
1013             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1014         else:
1015             dtype = dtypes.result_type(self.dtype, fill_value)
1016 
1017         if sparse_format is _default:
1018             sparse_format = "coo"
1019         try:
1020             as_sparse = getattr(sparse, "as_{}".format(sparse_format.lower()))
1021         except AttributeError:
1022             raise ValueError("{} is not a valid sparse format".format(sparse_format))
1023 
1024         data = as_sparse(self.data.astype(dtype), fill_value=fill_value)
1025         return self._replace(data=data)
1026 
1027     def _to_dense(self):
1028         """
1029         Change backend from sparse to np.array
1030         """
1031         if hasattr(self._data, "todense"):
1032             return self._replace(data=self._data.todense())
1033         return self.copy(deep=False)
1034 
1035     def isel(
1036         self: VariableType,
1037         indexers: Mapping[Hashable, Any] = None,
1038         **indexers_kwargs: Any,
1039     ) -> VariableType:
1040         """Return a new array indexed along the specified dimension(s).
1041 
1042         Parameters
1043         ----------
1044         **indexers : {dim: indexer, ...}
1045             Keyword arguments with names matching dimensions and values given
1046             by integers, slice objects or arrays.
1047 
1048         Returns
1049         -------
1050         obj : Array object
1051             A new Array with the selected data and dimensions. In general,
1052             the new variable's data will be a view of this variable's data,
1053             unless numpy fancy indexing was triggered by using an array
1054             indexer, in which case the data will be a copy.
1055         """
1056         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1057 
1058         invalid = indexers.keys() - set(self.dims)
1059         if invalid:
1060             raise ValueError(
1061                 f"dimensions {invalid} do not exist. Expected one or more of {self.dims}"
1062             )
1063 
1064         key = tuple(indexers.get(dim, slice(None)) for dim in self.dims)
1065         return self[key]
1066 
1067     def squeeze(self, dim=None):
1068         """Return a new object with squeezed data.
1069 
1070         Parameters
1071         ----------
1072         dim : None or str or tuple of str, optional
1073             Selects a subset of the length one dimensions. If a dimension is
1074             selected with length greater than one, an error is raised. If
1075             None, all length one dimensions are squeezed.
1076 
1077         Returns
1078         -------
1079         squeezed : same type as caller
1080             This object, but with with all or a subset of the dimensions of
1081             length 1 removed.
1082 
1083         See Also
1084         --------
1085         numpy.squeeze
1086         """
1087         dims = common.get_squeeze_dims(self, dim)
1088         return self.isel({d: 0 for d in dims})
1089 
1090     def _shift_one_dim(self, dim, count, fill_value=dtypes.NA):
1091         axis = self.get_axis_num(dim)
1092 
1093         if count > 0:
1094             keep = slice(None, -count)
1095         elif count < 0:
1096             keep = slice(-count, None)
1097         else:
1098             keep = slice(None)
1099 
1100         trimmed_data = self[(slice(None),) * axis + (keep,)].data
1101 
1102         if fill_value is dtypes.NA:
1103             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1104         else:
1105             dtype = self.dtype
1106 
1107         width = min(abs(count), self.shape[axis])
1108         dim_pad = (width, 0) if count >= 0 else (0, width)
1109         pads = [(0, 0) if d != dim else dim_pad for d in self.dims]
1110 
1111         data = duck_array_ops.pad(
1112             trimmed_data.astype(dtype),
1113             pads,
1114             mode="constant",
1115             constant_values=fill_value,
1116         )
1117 
1118         if isinstance(data, dask_array_type):
1119             # chunked data should come out with the same chunks; this makes
1120             # it feasible to combine shifted and unshifted data
1121             # TODO: remove this once dask.array automatically aligns chunks
1122             data = data.rechunk(self.data.chunks)
1123 
1124         return type(self)(self.dims, data, self._attrs, fastpath=True)
1125 
1126     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):
1127         """
1128         Return a new Variable with shifted data.
1129 
1130         Parameters
1131         ----------
1132         shifts : mapping of the form {dim: offset}
1133             Integer offset to shift along each of the given dimensions.
1134             Positive offsets shift to the right; negative offsets shift to the
1135             left.
1136         fill_value: scalar, optional
1137             Value to use for newly missing values
1138         **shifts_kwargs:
1139             The keyword arguments form of ``shifts``.
1140             One of shifts or shifts_kwargs must be provided.
1141 
1142         Returns
1143         -------
1144         shifted : Variable
1145             Variable with the same dimensions and attributes but shifted data.
1146         """
1147         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
1148         result = self
1149         for dim, count in shifts.items():
1150             result = result._shift_one_dim(dim, count, fill_value=fill_value)
1151         return result
1152 
1153     def pad_with_fill_value(
1154         self, pad_widths=None, fill_value=dtypes.NA, **pad_widths_kwargs
1155     ):
1156         """
1157         Return a new Variable with paddings.
1158 
1159         Parameters
1160         ----------
1161         pad_width: Mapping of the form {dim: (before, after)}
1162             Number of values padded to the edges of each dimension.
1163         **pad_widths_kwargs:
1164             Keyword argument for pad_widths
1165         """
1166         pad_widths = either_dict_or_kwargs(pad_widths, pad_widths_kwargs, "pad")
1167 
1168         if fill_value is dtypes.NA:
1169             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1170         else:
1171             dtype = self.dtype
1172 
1173         if isinstance(self.data, dask_array_type):
1174             array = self.data
1175 
1176             # Dask does not yet support pad. We manually implement it.
1177             # https://github.com/dask/dask/issues/1926
1178             for d, pad in pad_widths.items():
1179                 axis = self.get_axis_num(d)
1180                 before_shape = list(array.shape)
1181                 before_shape[axis] = pad[0]
1182                 before_chunks = list(array.chunks)
1183                 before_chunks[axis] = (pad[0],)
1184                 after_shape = list(array.shape)
1185                 after_shape[axis] = pad[1]
1186                 after_chunks = list(array.chunks)
1187                 after_chunks[axis] = (pad[1],)
1188 
1189                 arrays = []
1190                 if pad[0] > 0:
1191                     arrays.append(
1192                         da.full(
1193                             before_shape, fill_value, dtype=dtype, chunks=before_chunks
1194                         )
1195                     )
1196                 arrays.append(array)
1197                 if pad[1] > 0:
1198                     arrays.append(
1199                         da.full(
1200                             after_shape, fill_value, dtype=dtype, chunks=after_chunks
1201                         )
1202                     )
1203                 if len(arrays) > 1:
1204                     array = da.concatenate(arrays, axis=axis)
1205         else:
1206             pads = [(0, 0) if d not in pad_widths else pad_widths[d] for d in self.dims]
1207             array = np.pad(
1208                 self.data.astype(dtype, copy=False),
1209                 pads,
1210                 mode="constant",
1211                 constant_values=fill_value,
1212             )
1213         return type(self)(self.dims, array)
1214 
1215     def _roll_one_dim(self, dim, count):
1216         axis = self.get_axis_num(dim)
1217 
1218         count %= self.shape[axis]
1219         if count != 0:
1220             indices = [slice(-count, None), slice(None, -count)]
1221         else:
1222             indices = [slice(None)]
1223 
1224         arrays = [self[(slice(None),) * axis + (idx,)].data for idx in indices]
1225 
1226         data = duck_array_ops.concatenate(arrays, axis)
1227 
1228         if isinstance(data, dask_array_type):
1229             # chunked data should come out with the same chunks; this makes
1230             # it feasible to combine shifted and unshifted data
1231             # TODO: remove this once dask.array automatically aligns chunks
1232             data = data.rechunk(self.data.chunks)
1233 
1234         return type(self)(self.dims, data, self._attrs, fastpath=True)
1235 
1236     def roll(self, shifts=None, **shifts_kwargs):
1237         """
1238         Return a new Variable with rolld data.
1239 
1240         Parameters
1241         ----------
1242         shifts : mapping of the form {dim: offset}
1243             Integer offset to roll along each of the given dimensions.
1244             Positive offsets roll to the right; negative offsets roll to the
1245             left.
1246         **shifts_kwargs:
1247             The keyword arguments form of ``shifts``.
1248             One of shifts or shifts_kwargs must be provided.
1249 
1250         Returns
1251         -------
1252         shifted : Variable
1253             Variable with the same dimensions and attributes but rolled data.
1254         """
1255         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
1256 
1257         result = self
1258         for dim, count in shifts.items():
1259             result = result._roll_one_dim(dim, count)
1260         return result
1261 
1262     def transpose(self, *dims) -> "Variable":
1263         """Return a new Variable object with transposed dimensions.
1264 
1265         Parameters
1266         ----------
1267         *dims : str, optional
1268             By default, reverse the dimensions. Otherwise, reorder the
1269             dimensions to this order.
1270 
1271         Returns
1272         -------
1273         transposed : Variable
1274             The returned object has transposed data and dimensions with the
1275             same attributes as the original.
1276 
1277         Notes
1278         -----
1279         This operation returns a view of this variable's data. It is
1280         lazy for dask-backed Variables but not for numpy-backed Variables.
1281 
1282         See Also
1283         --------
1284         numpy.transpose
1285         """
1286         if len(dims) == 0:
1287             dims = self.dims[::-1]
1288         dims = tuple(infix_dims(dims, self.dims))
1289         axes = self.get_axis_num(dims)
1290         if len(dims) < 2 or dims == self.dims:
1291             # no need to transpose if only one dimension
1292             # or dims are in same order
1293             return self.copy(deep=False)
1294 
1295         data = as_indexable(self._data).transpose(axes)
1296         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)
1297 
1298     @property
1299     def T(self) -> "Variable":
1300         return self.transpose()
1301 
1302     def set_dims(self, dims, shape=None):
1303         """Return a new variable with given set of dimensions.
1304         This method might be used to attach new dimension(s) to variable.
1305 
1306         When possible, this operation does not copy this variable's data.
1307 
1308         Parameters
1309         ----------
1310         dims : str or sequence of str or dict
1311             Dimensions to include on the new variable. If a dict, values are
1312             used to provide the sizes of new dimensions; otherwise, new
1313             dimensions are inserted with length 1.
1314 
1315         Returns
1316         -------
1317         Variable
1318         """
1319         if isinstance(dims, str):
1320             dims = [dims]
1321 
1322         if shape is None and utils.is_dict_like(dims):
1323             shape = dims.values()
1324 
1325         missing_dims = set(self.dims) - set(dims)
1326         if missing_dims:
1327             raise ValueError(
1328                 "new dimensions %r must be a superset of "
1329                 "existing dimensions %r" % (dims, self.dims)
1330             )
1331 
1332         self_dims = set(self.dims)
1333         expanded_dims = tuple(d for d in dims if d not in self_dims) + self.dims
1334 
1335         if self.dims == expanded_dims:
1336             # don't use broadcast_to unless necessary so the result remains
1337             # writeable if possible
1338             expanded_data = self.data
1339         elif shape is not None:
1340             dims_map = dict(zip(dims, shape))
1341             tmp_shape = tuple(dims_map[d] for d in expanded_dims)
1342             expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)
1343         else:
1344             expanded_data = self.data[(None,) * (len(expanded_dims) - self.ndim)]
1345 
1346         expanded_var = Variable(
1347             expanded_dims, expanded_data, self._attrs, self._encoding, fastpath=True
1348         )
1349         return expanded_var.transpose(*dims)
1350 
1351     def _stack_once(self, dims, new_dim):
1352         if not set(dims) <= set(self.dims):
1353             raise ValueError("invalid existing dimensions: %s" % dims)
1354 
1355         if new_dim in self.dims:
1356             raise ValueError(
1357                 "cannot create a new dimension with the same "
1358                 "name as an existing dimension"
1359             )
1360 
1361         if len(dims) == 0:
1362             # don't stack
1363             return self.copy(deep=False)
1364 
1365         other_dims = [d for d in self.dims if d not in dims]
1366         dim_order = other_dims + list(dims)
1367         reordered = self.transpose(*dim_order)
1368 
1369         new_shape = reordered.shape[: len(other_dims)] + (-1,)
1370         new_data = reordered.data.reshape(new_shape)
1371         new_dims = reordered.dims[: len(other_dims)] + (new_dim,)
1372 
1373         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)
1374 
1375     def stack(self, dimensions=None, **dimensions_kwargs):
1376         """
1377         Stack any number of existing dimensions into a single new dimension.
1378 
1379         New dimensions will be added at the end, and the order of the data
1380         along each new dimension will be in contiguous (C) order.
1381 
1382         Parameters
1383         ----------
1384         dimensions : Mapping of form new_name=(dim1, dim2, ...)
1385             Names of new dimensions, and the existing dimensions that they
1386             replace.
1387         **dimensions_kwargs:
1388             The keyword arguments form of ``dimensions``.
1389             One of dimensions or dimensions_kwargs must be provided.
1390 
1391         Returns
1392         -------
1393         stacked : Variable
1394             Variable with the same attributes but stacked data.
1395 
1396         See also
1397         --------
1398         Variable.unstack
1399         """
1400         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
1401         result = self
1402         for new_dim, dims in dimensions.items():
1403             result = result._stack_once(dims, new_dim)
1404         return result
1405 
1406     def _unstack_once(self, dims, old_dim):
1407         new_dim_names = tuple(dims.keys())
1408         new_dim_sizes = tuple(dims.values())
1409 
1410         if old_dim not in self.dims:
1411             raise ValueError("invalid existing dimension: %s" % old_dim)
1412 
1413         if set(new_dim_names).intersection(self.dims):
1414             raise ValueError(
1415                 "cannot create a new dimension with the same "
1416                 "name as an existing dimension"
1417             )
1418 
1419         if np.prod(new_dim_sizes) != self.sizes[old_dim]:
1420             raise ValueError(
1421                 "the product of the new dimension sizes must "
1422                 "equal the size of the old dimension"
1423             )
1424 
1425         other_dims = [d for d in self.dims if d != old_dim]
1426         dim_order = other_dims + [old_dim]
1427         reordered = self.transpose(*dim_order)
1428 
1429         new_shape = reordered.shape[: len(other_dims)] + new_dim_sizes
1430         new_data = reordered.data.reshape(new_shape)
1431         new_dims = reordered.dims[: len(other_dims)] + new_dim_names
1432 
1433         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)
1434 
1435     def unstack(self, dimensions=None, **dimensions_kwargs):
1436         """
1437         Unstack an existing dimension into multiple new dimensions.
1438 
1439         New dimensions will be added at the end, and the order of the data
1440         along each new dimension will be in contiguous (C) order.
1441 
1442         Parameters
1443         ----------
1444         dimensions : mapping of the form old_dim={dim1: size1, ...}
1445             Names of existing dimensions, and the new dimensions and sizes
1446             that they map to.
1447         **dimensions_kwargs:
1448             The keyword arguments form of ``dimensions``.
1449             One of dimensions or dimensions_kwargs must be provided.
1450 
1451         Returns
1452         -------
1453         unstacked : Variable
1454             Variable with the same attributes but unstacked data.
1455 
1456         See also
1457         --------
1458         Variable.stack
1459         """
1460         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "unstack")
1461         result = self
1462         for old_dim, dims in dimensions.items():
1463             result = result._unstack_once(dims, old_dim)
1464         return result
1465 
1466     def fillna(self, value):
1467         return ops.fillna(self, value)
1468 
1469     def where(self, cond, other=dtypes.NA):
1470         return ops.where_method(self, cond, other)
1471 
1472     def reduce(
1473         self,
1474         func,
1475         dim=None,
1476         axis=None,
1477         keep_attrs=None,
1478         keepdims=False,
1479         allow_lazy=None,
1480         **kwargs,
1481     ):
1482         """Reduce this array by applying `func` along some dimension(s).
1483 
1484         Parameters
1485         ----------
1486         func : function
1487             Function which can be called in the form
1488             `func(x, axis=axis, **kwargs)` to return the result of reducing an
1489             np.ndarray over an integer valued axis.
1490         dim : str or sequence of str, optional
1491             Dimension(s) over which to apply `func`.
1492         axis : int or sequence of int, optional
1493             Axis(es) over which to apply `func`. Only one of the 'dim'
1494             and 'axis' arguments can be supplied. If neither are supplied, then
1495             the reduction is calculated over the flattened array (by calling
1496             `func(x)` without an axis argument).
1497         keep_attrs : bool, optional
1498             If True, the variable's attributes (`attrs`) will be copied from
1499             the original object to the new one.  If False (default), the new
1500             object will be returned without attributes.
1501         keepdims : bool, default False
1502             If True, the dimensions which are reduced are left in the result
1503             as dimensions of size one
1504         **kwargs : dict
1505             Additional keyword arguments passed on to `func`.
1506 
1507         Returns
1508         -------
1509         reduced : Array
1510             Array with summarized data and the indicated dimension(s)
1511             removed.
1512         """
1513         if dim == ...:
1514             dim = None
1515         if dim is not None and axis is not None:
1516             raise ValueError("cannot supply both 'axis' and 'dim' arguments")
1517 
1518         if dim is not None:
1519             axis = self.get_axis_num(dim)
1520 
1521         if allow_lazy is not None:
1522             warnings.warn(
1523                 "allow_lazy is deprecated and will be removed in version 0.16.0. It is now True by default.",
1524                 DeprecationWarning,
1525             )
1526         else:
1527             allow_lazy = True
1528 
1529         input_data = self.data if allow_lazy else self.values
1530 
1531         if axis is not None:
1532             data = func(input_data, axis=axis, **kwargs)
1533         else:
1534             data = func(input_data, **kwargs)
1535 
1536         if getattr(data, "shape", ()) == self.shape:
1537             dims = self.dims
1538         else:
1539             removed_axes = (
1540                 range(self.ndim) if axis is None else np.atleast_1d(axis) % self.ndim
1541             )
1542             if keepdims:
1543                 # Insert np.newaxis for removed dims
1544                 slices = tuple(
1545                     np.newaxis if i in removed_axes else slice(None, None)
1546                     for i in range(self.ndim)
1547                 )
1548                 if getattr(data, "shape", None) is None:
1549                     # Reduce has produced a scalar value, not an array-like
1550                     data = np.asanyarray(data)[slices]
1551                 else:
1552                     data = data[slices]
1553                 dims = self.dims
1554             else:
1555                 dims = [
1556                     adim for n, adim in enumerate(self.dims) if n not in removed_axes
1557                 ]
1558 
1559         if keep_attrs is None:
1560             keep_attrs = _get_keep_attrs(default=False)
1561         attrs = self._attrs if keep_attrs else None
1562 
1563         return Variable(dims, data, attrs=attrs)
1564 
1565     @classmethod
1566     def concat(cls, variables, dim="concat_dim", positions=None, shortcut=False):
1567         """Concatenate variables along a new or existing dimension.
1568 
1569         Parameters
1570         ----------
1571         variables : iterable of Array
1572             Arrays to stack together. Each variable is expected to have
1573             matching dimensions and shape except for along the stacked
1574             dimension.
1575         dim : str or DataArray, optional
1576             Name of the dimension to stack along. This can either be a new
1577             dimension name, in which case it is added along axis=0, or an
1578             existing dimension name, in which case the location of the
1579             dimension is unchanged. Where to insert the new dimension is
1580             determined by the first variable.
1581         positions : None or list of integer arrays, optional
1582             List of integer arrays which specifies the integer positions to
1583             which to assign each dataset along the concatenated dimension.
1584             If not supplied, objects are concatenated in the provided order.
1585         shortcut : bool, optional
1586             This option is used internally to speed-up groupby operations.
1587             If `shortcut` is True, some checks of internal consistency between
1588             arrays to concatenate are skipped.
1589 
1590         Returns
1591         -------
1592         stacked : Variable
1593             Concatenated Variable formed by stacking all the supplied variables
1594             along the given dimension.
1595         """
1596         if not isinstance(dim, str):
1597             (dim,) = dim.dims
1598 
1599         # can't do this lazily: we need to loop through variables at least
1600         # twice
1601         variables = list(variables)
1602         first_var = variables[0]
1603 
1604         arrays = [v.data for v in variables]
1605 
1606         if dim in first_var.dims:
1607             axis = first_var.get_axis_num(dim)
1608             dims = first_var.dims
1609             data = duck_array_ops.concatenate(arrays, axis=axis)
1610             if positions is not None:
1611                 # TODO: deprecate this option -- we don't need it for groupby
1612                 # any more.
1613                 indices = nputils.inverse_permutation(np.concatenate(positions))
1614                 data = duck_array_ops.take(data, indices, axis=axis)
1615         else:
1616             axis = 0
1617             dims = (dim,) + first_var.dims
1618             data = duck_array_ops.stack(arrays, axis=axis)
1619 
1620         attrs = dict(first_var.attrs)
1621         encoding = dict(first_var.encoding)
1622         if not shortcut:
1623             for var in variables:
1624                 if var.dims != first_var.dims:
1625                     raise ValueError(
1626                         f"Variable has dimensions {list(var.dims)} but first Variable has dimensions {list(first_var.dims)}"
1627                     )
1628 
1629         return cls(dims, data, attrs, encoding)
1630 
1631     def equals(self, other, equiv=duck_array_ops.array_equiv):
1632         """True if two Variables have the same dimensions and values;
1633         otherwise False.
1634 
1635         Variables can still be equal (like pandas objects) if they have NaN
1636         values in the same locations.
1637 
1638         This method is necessary because `v1 == v2` for Variables
1639         does element-wise comparisons (like numpy.ndarrays).
1640         """
1641         other = getattr(other, "variable", other)
1642         try:
1643             return self.dims == other.dims and (
1644                 self._data is other._data or equiv(self.data, other.data)
1645             )
1646         except (TypeError, AttributeError):
1647             return False
1648 
1649     def broadcast_equals(self, other, equiv=duck_array_ops.array_equiv):
1650         """True if two Variables have the values after being broadcast against
1651         each other; otherwise False.
1652 
1653         Variables can still be equal (like pandas objects) if they have NaN
1654         values in the same locations.
1655         """
1656         try:
1657             self, other = broadcast_variables(self, other)
1658         except (ValueError, AttributeError):
1659             return False
1660         return self.equals(other, equiv=equiv)
1661 
1662     def identical(self, other, equiv=duck_array_ops.array_equiv):
1663         """Like equals, but also checks attributes.
1664         """
1665         try:
1666             return utils.dict_equiv(self.attrs, other.attrs) and self.equals(
1667                 other, equiv=equiv
1668             )
1669         except (TypeError, AttributeError):
1670             return False
1671 
1672     def no_conflicts(self, other, equiv=duck_array_ops.array_notnull_equiv):
1673         """True if the intersection of two Variable's non-null data is
1674         equal; otherwise false.
1675 
1676         Variables can thus still be equal if there are locations where either,
1677         or both, contain NaN values.
1678         """
1679         return self.broadcast_equals(other, equiv=equiv)
1680 
1681     def quantile(
1682         self, q, dim=None, interpolation="linear", keep_attrs=None, skipna=True
1683     ):
1684         """Compute the qth quantile of the data along the specified dimension.
1685 
1686         Returns the qth quantiles(s) of the array elements.
1687 
1688         Parameters
1689         ----------
1690         q : float in range of [0,1] (or sequence of floats)
1691             Quantile to compute, which must be between 0 and 1
1692             inclusive.
1693         dim : str or sequence of str, optional
1694             Dimension(s) over which to apply quantile.
1695         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
1696             This optional parameter specifies the interpolation method to
1697             use when the desired quantile lies between two data points
1698             ``i < j``:
1699 
1700                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is
1701                   the fractional part of the index surrounded by ``i`` and
1702                   ``j``.
1703                 * lower: ``i``.
1704                 * higher: ``j``.
1705                 * nearest: ``i`` or ``j``, whichever is nearest.
1706                 * midpoint: ``(i + j) / 2``.
1707 
1708         keep_attrs : bool, optional
1709             If True, the variable's attributes (`attrs`) will be copied from
1710             the original object to the new one.  If False (default), the new
1711             object will be returned without attributes.
1712 
1713         Returns
1714         -------
1715         quantiles : Variable
1716             If `q` is a single quantile, then the result
1717             is a scalar. If multiple percentiles are given, first axis of
1718             the result corresponds to the quantile and a quantile dimension
1719             is added to the return array. The other dimensions are the
1720             dimensions that remain after the reduction of the array.
1721 
1722         See Also
1723         --------
1724         numpy.nanquantile, pandas.Series.quantile, Dataset.quantile,
1725         DataArray.quantile
1726         """
1727 
1728         from .computation import apply_ufunc
1729 
1730         _quantile_func = np.nanquantile if skipna else np.quantile
1731 
1732         if keep_attrs is None:
1733             keep_attrs = _get_keep_attrs(default=False)
1734 
1735         scalar = utils.is_scalar(q)
1736         q = np.atleast_1d(np.asarray(q, dtype=np.float64))
1737 
1738         if dim is None:
1739             dim = self.dims
1740 
1741         if utils.is_scalar(dim):
1742             dim = [dim]
1743 
1744         def _wrapper(npa, **kwargs):
1745             # move quantile axis to end. required for apply_ufunc
1746             return np.moveaxis(_quantile_func(npa, **kwargs), 0, -1)
1747 
1748         axis = np.arange(-1, -1 * len(dim) - 1, -1)
1749         result = apply_ufunc(
1750             _wrapper,
1751             self,
1752             input_core_dims=[dim],
1753             exclude_dims=set(dim),
1754             output_core_dims=[["quantile"]],
1755             output_dtypes=[np.float64],
1756             output_sizes={"quantile": len(q)},
1757             dask="parallelized",
1758             kwargs={"q": q, "axis": axis, "interpolation": interpolation},
1759         )
1760 
1761         # for backward compatibility
1762         result = result.transpose("quantile", ...)
1763         if scalar:
1764             result = result.squeeze("quantile")
1765         if keep_attrs:
1766             result.attrs = self._attrs
1767         return result
1768 
1769     def rank(self, dim, pct=False):
1770         """Ranks the data.
1771 
1772         Equal values are assigned a rank that is the average of the ranks that
1773         would have been otherwise assigned to all of the values within that
1774         set.  Ranks begin at 1, not 0. If `pct`, computes percentage ranks.
1775 
1776         NaNs in the input array are returned as NaNs.
1777 
1778         The `bottleneck` library is required.
1779 
1780         Parameters
1781         ----------
1782         dim : str
1783             Dimension over which to compute rank.
1784         pct : bool, optional
1785             If True, compute percentage ranks, otherwise compute integer ranks.
1786 
1787         Returns
1788         -------
1789         ranked : Variable
1790 
1791         See Also
1792         --------
1793         Dataset.rank, DataArray.rank
1794         """
1795         import bottleneck as bn
1796 
1797         data = self.data
1798 
1799         if isinstance(data, dask_array_type):
1800             raise TypeError(
1801                 "rank does not work for arrays stored as dask "
1802                 "arrays. Load the data via .compute() or .load() "
1803                 "prior to calling this method."
1804             )
1805         elif not isinstance(data, np.ndarray):
1806             raise TypeError(
1807                 "rank is not implemented for {} objects.".format(type(data))
1808             )
1809 
1810         axis = self.get_axis_num(dim)
1811         func = bn.nanrankdata if self.dtype.kind == "f" else bn.rankdata
1812         ranked = func(data, axis=axis)
1813         if pct:
1814             count = np.sum(~np.isnan(data), axis=axis, keepdims=True)
1815             ranked /= count
1816         return Variable(self.dims, ranked)
1817 
1818     def rolling_window(
1819         self, dim, window, window_dim, center=False, fill_value=dtypes.NA
1820     ):
1821         """
1822         Make a rolling_window along dim and add a new_dim to the last place.
1823 
1824         Parameters
1825         ----------
1826         dim: str
1827             Dimension over which to compute rolling_window
1828         window: int
1829             Window size of the rolling
1830         window_dim: str
1831             New name of the window dimension.
1832         center: boolean. default False.
1833             If True, pad fill_value for both ends. Otherwise, pad in the head
1834             of the axis.
1835         fill_value:
1836             value to be filled.
1837 
1838         Returns
1839         -------
1840         Variable that is a view of the original array with a added dimension of
1841         size w.
1842         The return dim: self.dims + (window_dim, )
1843         The return shape: self.shape + (window, )
1844 
1845         Examples
1846         --------
1847         >>> v = Variable(("a", "b"), np.arange(8).reshape((2, 4)))
1848         >>> v.rolling_window(x, "b", 3, "window_dim")
1849         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
1850         array([[[nan, nan, 0], [nan, 0, 1], [0, 1, 2], [1, 2, 3]],
1851                [[nan, nan, 4], [nan, 4, 5], [4, 5, 6], [5, 6, 7]]])
1852 
1853         >>> v.rolling_window(x, "b", 3, "window_dim", center=True)
1854         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
1855         array([[[nan, 0, 1], [0, 1, 2], [1, 2, 3], [2, 3, nan]],
1856                [[nan, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, nan]]])
1857         """
1858         if fill_value is dtypes.NA:  # np.nan is passed
1859             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1860             array = self.astype(dtype, copy=False).data
1861         else:
1862             dtype = self.dtype
1863             array = self.data
1864 
1865         new_dims = self.dims + (window_dim,)
1866         return Variable(
1867             new_dims,
1868             duck_array_ops.rolling_window(
1869                 array,
1870                 axis=self.get_axis_num(dim),
1871                 window=window,
1872                 center=center,
1873                 fill_value=fill_value,
1874             ),
1875         )
1876 
1877     def coarsen(self, windows, func, boundary="exact", side="left", **kwargs):
1878         """
1879         Apply reduction function.
1880         """
1881         windows = {k: v for k, v in windows.items() if k in self.dims}
1882         if not windows:
1883             return self.copy()
1884 
1885         reshaped, axes = self._coarsen_reshape(windows, boundary, side)
1886         if isinstance(func, str):
1887             name = func
1888             func = getattr(duck_array_ops, name, None)
1889             if func is None:
1890                 raise NameError(f"{name} is not a valid method.")
1891         return self._replace(data=func(reshaped, axis=axes, **kwargs))
1892 
1893     def _coarsen_reshape(self, windows, boundary, side):
1894         """
1895         Construct a reshaped-array for coarsen
1896         """
1897         if not utils.is_dict_like(boundary):
1898             boundary = {d: boundary for d in windows.keys()}
1899 
1900         if not utils.is_dict_like(side):
1901             side = {d: side for d in windows.keys()}
1902 
1903         # remove unrelated dimensions
1904         boundary = {k: v for k, v in boundary.items() if k in windows}
1905         side = {k: v for k, v in side.items() if k in windows}
1906 
1907         for d, window in windows.items():
1908             if window <= 0:
1909                 raise ValueError(f"window must be > 0. Given {window}")
1910 
1911         variable = self
1912         for d, window in windows.items():
1913             # trim or pad the object
1914             size = variable.shape[self._get_axis_num(d)]
1915             n = int(size / window)
1916             if boundary[d] == "exact":
1917                 if n * window != size:
1918                     raise ValueError(
1919                         "Could not coarsen a dimension of size {} with "
1920                         "window {}".format(size, window)
1921                     )
1922             elif boundary[d] == "trim":
1923                 if side[d] == "left":
1924                     variable = variable.isel({d: slice(0, window * n)})
1925                 else:
1926                     excess = size - window * n
1927                     variable = variable.isel({d: slice(excess, None)})
1928             elif boundary[d] == "pad":  # pad
1929                 pad = window * n - size
1930                 if pad < 0:
1931                     pad += window
1932                 if side[d] == "left":
1933                     pad_widths = {d: (0, pad)}
1934                 else:
1935                     pad_widths = {d: (pad, 0)}
1936                 variable = variable.pad_with_fill_value(pad_widths)
1937             else:
1938                 raise TypeError(
1939                     "{} is invalid for boundary. Valid option is 'exact', "
1940                     "'trim' and 'pad'".format(boundary[d])
1941                 )
1942 
1943         shape = []
1944         axes = []
1945         axis_count = 0
1946         for i, d in enumerate(variable.dims):
1947             if d in windows:
1948                 size = variable.shape[i]
1949                 shape.append(int(size / windows[d]))
1950                 shape.append(windows[d])
1951                 axis_count += 1
1952                 axes.append(i + axis_count)
1953             else:
1954                 shape.append(variable.shape[i])
1955 
1956         keep_attrs = _get_keep_attrs(default=False)
1957         variable.attrs = variable._attrs if keep_attrs else {}
1958 
1959         return variable.data.reshape(shape), tuple(axes)
1960 
1961     @property
1962     def real(self):
1963         return type(self)(self.dims, self.data.real, self._attrs)
1964 
1965     @property
1966     def imag(self):
1967         return type(self)(self.dims, self.data.imag, self._attrs)
1968 
1969     def __array_wrap__(self, obj, context=None):
1970         return Variable(self.dims, obj)
1971 
1972     @staticmethod
1973     def _unary_op(f):
1974         @functools.wraps(f)
1975         def func(self, *args, **kwargs):
1976             with np.errstate(all="ignore"):
1977                 return self.__array_wrap__(f(self.data, *args, **kwargs))
1978 
1979         return func
1980 
1981     @staticmethod
1982     def _binary_op(f, reflexive=False, **ignored_kwargs):
1983         @functools.wraps(f)
1984         def func(self, other):
1985             if isinstance(other, (xr.DataArray, xr.Dataset)):
1986                 return NotImplemented
1987             self_data, other_data, dims = _broadcast_compat_data(self, other)
1988             keep_attrs = _get_keep_attrs(default=False)
1989             attrs = self._attrs if keep_attrs else None
1990             with np.errstate(all="ignore"):
1991                 new_data = (
1992                     f(self_data, other_data)
1993                     if not reflexive
1994                     else f(other_data, self_data)
1995                 )
1996             result = Variable(dims, new_data, attrs=attrs)
1997             return result
1998 
1999         return func
2000 
2001     @staticmethod
2002     def _inplace_binary_op(f):
2003         @functools.wraps(f)
2004         def func(self, other):
2005             if isinstance(other, xr.Dataset):
2006                 raise TypeError("cannot add a Dataset to a Variable in-place")
2007             self_data, other_data, dims = _broadcast_compat_data(self, other)
2008             if dims != self.dims:
2009                 raise ValueError("dimensions cannot change for in-place " "operations")
2010             with np.errstate(all="ignore"):
2011                 self.values = f(self_data, other_data)
2012             return self
2013 
2014         return func
2015 
2016     def _to_numeric(self, offset=None, datetime_unit=None, dtype=float):
2017         """ A (private) method to convert datetime array to numeric dtype
2018         See duck_array_ops.datetime_to_numeric
2019         """
2020         numeric_array = duck_array_ops.datetime_to_numeric(
2021             self.data, offset, datetime_unit, dtype
2022         )
2023         return type(self)(self.dims, numeric_array, self._attrs)
2024 
2025 
2026 ops.inject_all_ops_and_reduce_methods(Variable)
2027 
2028 
2029 class IndexVariable(Variable):
2030     """Wrapper for accommodating a pandas.Index in an xarray.Variable.
2031 
2032     IndexVariable preserve loaded values in the form of a pandas.Index instead
2033     of a NumPy array. Hence, their values are immutable and must always be one-
2034     dimensional.
2035 
2036     They also have a name property, which is the name of their sole dimension
2037     unless another name is given.
2038     """
2039 
2040     __slots__ = ()
2041 
2042     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):
2043         super().__init__(dims, data, attrs, encoding, fastpath)
2044         if self.ndim != 1:
2045             raise ValueError("%s objects must be 1-dimensional" % type(self).__name__)
2046 
2047         # Unlike in Variable, always eagerly load values into memory
2048         if not isinstance(self._data, PandasIndexAdapter):
2049             self._data = PandasIndexAdapter(self._data)
2050 
2051     def __dask_tokenize__(self):
2052         from dask.base import normalize_token
2053 
2054         # Don't waste time converting pd.Index to np.ndarray
2055         return normalize_token((type(self), self._dims, self._data.array, self._attrs))
2056 
2057     def load(self):
2058         # data is already loaded into memory for IndexVariable
2059         return self
2060 
2061     # https://github.com/python/mypy/issues/1465
2062     @Variable.data.setter  # type: ignore
2063     def data(self, data):
2064         Variable.data.fset(self, data)
2065         if not isinstance(self._data, PandasIndexAdapter):
2066             self._data = PandasIndexAdapter(self._data)
2067 
2068     def chunk(self, chunks=None, name=None, lock=False):
2069         # Dummy - do not chunk. This method is invoked e.g. by Dataset.chunk()
2070         return self.copy(deep=False)
2071 
2072     def _as_sparse(self, sparse_format=_default, fill_value=_default):
2073         # Dummy
2074         return self.copy(deep=False)
2075 
2076     def _to_dense(self):
2077         # Dummy
2078         return self.copy(deep=False)
2079 
2080     def _finalize_indexing_result(self, dims, data):
2081         if getattr(data, "ndim", 0) != 1:
2082             # returns Variable rather than IndexVariable if multi-dimensional
2083             return Variable(dims, data, self._attrs, self._encoding)
2084         else:
2085             return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)
2086 
2087     def __setitem__(self, key, value):
2088         raise TypeError("%s values cannot be modified" % type(self).__name__)
2089 
2090     @classmethod
2091     def concat(cls, variables, dim="concat_dim", positions=None, shortcut=False):
2092         """Specialized version of Variable.concat for IndexVariable objects.
2093 
2094         This exists because we want to avoid converting Index objects to NumPy
2095         arrays, if possible.
2096         """
2097         if not isinstance(dim, str):
2098             (dim,) = dim.dims
2099 
2100         variables = list(variables)
2101         first_var = variables[0]
2102 
2103         if any(not isinstance(v, cls) for v in variables):
2104             raise TypeError(
2105                 "IndexVariable.concat requires that all input "
2106                 "variables be IndexVariable objects"
2107             )
2108 
2109         indexes = [v._data.array for v in variables]
2110 
2111         if not indexes:
2112             data = []
2113         else:
2114             data = indexes[0].append(indexes[1:])
2115 
2116             if positions is not None:
2117                 indices = nputils.inverse_permutation(np.concatenate(positions))
2118                 data = data.take(indices)
2119 
2120         attrs = dict(first_var.attrs)
2121         if not shortcut:
2122             for var in variables:
2123                 if var.dims != first_var.dims:
2124                     raise ValueError("inconsistent dimensions")
2125                 utils.remove_incompatible_items(attrs, var.attrs)
2126 
2127         return cls(first_var.dims, data, attrs)
2128 
2129     def copy(self, deep=True, data=None):
2130         """Returns a copy of this object.
2131 
2132         `deep` is ignored since data is stored in the form of
2133         pandas.Index, which is already immutable. Dimensions, attributes
2134         and encodings are always copied.
2135 
2136         Use `data` to create a new object with the same structure as
2137         original but entirely new data.
2138 
2139         Parameters
2140         ----------
2141         deep : bool, optional
2142             Deep is ignored when data is given. Whether the data array is
2143             loaded into memory and copied onto the new object. Default is True.
2144         data : array_like, optional
2145             Data to use in the new object. Must have same shape as original.
2146 
2147         Returns
2148         -------
2149         object : Variable
2150             New object with dimensions, attributes, encodings, and optionally
2151             data copied from original.
2152         """
2153         if data is None:
2154             data = self._data.copy(deep=deep)
2155         else:
2156             data = as_compatible_data(data)
2157             if self.shape != data.shape:
2158                 raise ValueError(
2159                     "Data shape {} must match shape of object {}".format(
2160                         data.shape, self.shape
2161                     )
2162                 )
2163         return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)
2164 
2165     def equals(self, other, equiv=None):
2166         # if equiv is specified, super up
2167         if equiv is not None:
2168             return super().equals(other, equiv)
2169 
2170         # otherwise use the native index equals, rather than looking at _data
2171         other = getattr(other, "variable", other)
2172         try:
2173             return self.dims == other.dims and self._data_equals(other)
2174         except (TypeError, AttributeError):
2175             return False
2176 
2177     def _data_equals(self, other):
2178         return self.to_index().equals(other.to_index())
2179 
2180     def to_index_variable(self):
2181         """Return this variable as an xarray.IndexVariable"""
2182         return self
2183 
2184     to_coord = utils.alias(to_index_variable, "to_coord")
2185 
2186     def to_index(self):
2187         """Convert this variable to a pandas.Index"""
2188         # n.b. creating a new pandas.Index from an old pandas.Index is
2189         # basically free as pandas.Index objects are immutable
2190         assert self.ndim == 1
2191         index = self._data.array
2192         if isinstance(index, pd.MultiIndex):
2193             # set default names for multi-index unnamed levels so that
2194             # we can safely rename dimension / coordinate later
2195             valid_level_names = [
2196                 name or "{}_level_{}".format(self.dims[0], i)
2197                 for i, name in enumerate(index.names)
2198             ]
2199             index = index.set_names(valid_level_names)
2200         else:
2201             index = index.set_names(self.name)
2202         return index
2203 
2204     @property
2205     def level_names(self):
2206         """Return MultiIndex level names or None if this IndexVariable has no
2207         MultiIndex.
2208         """
2209         index = self.to_index()
2210         if isinstance(index, pd.MultiIndex):
2211             return index.names
2212         else:
2213             return None
2214 
2215     def get_level_variable(self, level):
2216         """Return a new IndexVariable from a given MultiIndex level."""
2217         if self.level_names is None:
2218             raise ValueError("IndexVariable %r has no MultiIndex" % self.name)
2219         index = self.to_index()
2220         return type(self)(self.dims, index.get_level_values(level))
2221 
2222     @property
2223     def name(self):
2224         return self.dims[0]
2225 
2226     @name.setter
2227     def name(self, value):
2228         raise AttributeError("cannot modify name of IndexVariable in-place")
2229 
2230 
2231 # for backwards compatibility
2232 Coordinate = utils.alias(IndexVariable, "Coordinate")
2233 
2234 
2235 def _unified_dims(variables):
2236     # validate dimensions
2237     all_dims = {}
2238     for var in variables:
2239         var_dims = var.dims
2240         if len(set(var_dims)) < len(var_dims):
2241             raise ValueError(
2242                 "broadcasting cannot handle duplicate "
2243                 "dimensions: %r" % list(var_dims)
2244             )
2245         for d, s in zip(var_dims, var.shape):
2246             if d not in all_dims:
2247                 all_dims[d] = s
2248             elif all_dims[d] != s:
2249                 raise ValueError(
2250                     "operands cannot be broadcast together "
2251                     "with mismatched lengths for dimension %r: %s"
2252                     % (d, (all_dims[d], s))
2253                 )
2254     return all_dims
2255 
2256 
2257 def _broadcast_compat_variables(*variables):
2258     """Create broadcast compatible variables, with the same dimensions.
2259 
2260     Unlike the result of broadcast_variables(), some variables may have
2261     dimensions of size 1 instead of the the size of the broadcast dimension.
2262     """
2263     dims = tuple(_unified_dims(variables))
2264     return tuple(var.set_dims(dims) if var.dims != dims else var for var in variables)
2265 
2266 
2267 def broadcast_variables(*variables):
2268     """Given any number of variables, return variables with matching dimensions
2269     and broadcast data.
2270 
2271     The data on the returned variables will be a view of the data on the
2272     corresponding original arrays, but dimensions will be reordered and
2273     inserted so that both broadcast arrays have the same dimensions. The new
2274     dimensions are sorted in order of appearance in the first variable's
2275     dimensions followed by the second variable's dimensions.
2276     """
2277     dims_map = _unified_dims(variables)
2278     dims_tuple = tuple(dims_map)
2279     return tuple(
2280         var.set_dims(dims_map) if var.dims != dims_tuple else var for var in variables
2281     )
2282 
2283 
2284 def _broadcast_compat_data(self, other):
2285     if all(hasattr(other, attr) for attr in ["dims", "data", "shape", "encoding"]):
2286         # `other` satisfies the necessary Variable API for broadcast_variables
2287         new_self, new_other = _broadcast_compat_variables(self, other)
2288         self_data = new_self.data
2289         other_data = new_other.data
2290         dims = new_self.dims
2291     else:
2292         # rely on numpy broadcasting rules
2293         self_data = self.data
2294         other_data = other
2295         dims = self.dims
2296     return self_data, other_data, dims
2297 
2298 
2299 def concat(variables, dim="concat_dim", positions=None, shortcut=False):
2300     """Concatenate variables along a new or existing dimension.
2301 
2302     Parameters
2303     ----------
2304     variables : iterable of Array
2305         Arrays to stack together. Each variable is expected to have
2306         matching dimensions and shape except for along the stacked
2307         dimension.
2308     dim : str or DataArray, optional
2309         Name of the dimension to stack along. This can either be a new
2310         dimension name, in which case it is added along axis=0, or an
2311         existing dimension name, in which case the location of the
2312         dimension is unchanged. Where to insert the new dimension is
2313         determined by the first variable.
2314     positions : None or list of integer arrays, optional
2315         List of integer arrays which specifies the integer positions to which
2316         to assign each dataset along the concatenated dimension. If not
2317         supplied, objects are concatenated in the provided order.
2318     shortcut : bool, optional
2319         This option is used internally to speed-up groupby operations.
2320         If `shortcut` is True, some checks of internal consistency between
2321         arrays to concatenate are skipped.
2322 
2323     Returns
2324     -------
2325     stacked : Variable
2326         Concatenated Variable formed by stacking all the supplied variables
2327         along the given dimension.
2328     """
2329     variables = list(variables)
2330     if all(isinstance(v, IndexVariable) for v in variables):
2331         return IndexVariable.concat(variables, dim, positions, shortcut)
2332     else:
2333         return Variable.concat(variables, dim, positions, shortcut)
2334 
2335 
2336 def assert_unique_multiindex_level_names(variables):
2337     """Check for uniqueness of MultiIndex level names in all given
2338     variables.
2339 
2340     Not public API. Used for checking consistency of DataArray and Dataset
2341     objects.
2342     """
2343     level_names = defaultdict(list)
2344     all_level_names = set()
2345     for var_name, var in variables.items():
2346         if isinstance(var._data, PandasIndexAdapter):
2347             idx_level_names = var.to_index_variable().level_names
2348             if idx_level_names is not None:
2349                 for n in idx_level_names:
2350                     level_names[n].append(f"{n!r} ({var_name})")
2351             if idx_level_names:
2352                 all_level_names.update(idx_level_names)
2353 
2354     for k, v in level_names.items():
2355         if k in variables:
2356             v.append("(%s)" % k)
2357 
2358     duplicate_names = [v for v in level_names.values() if len(v) > 1]
2359     if duplicate_names:
2360         conflict_str = "\n".join([", ".join(v) for v in duplicate_names])
2361         raise ValueError("conflicting MultiIndex level name(s):\n%s" % conflict_str)
2362     # Check confliction between level names and dimensions GH:2299
2363     for k, v in variables.items():
2364         for d in v.dims:
2365             if d in all_level_names:
2366                 raise ValueError(
2367                     "conflicting level / dimension names. {} "
2368                     "already exists as a level name.".format(d)
2369                 )

</code><intended_behavior>Ensure support for the weighted average computation at the variable level, potentially introducing helper functions or methods necessary for performing the weighted average calculations that 'Dataset' and 'DataArray' methods can then utilize. This may involve processing the weights alongside the data and appropriately handling dimensions.</intended_behavior>]
2025-10-18 15:09:44.253 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 15:09:44.253 | INFO     | Search-only: skipping patch selection
2025-10-18 15:09:44.335 | INFO     | Task pydata__xarray-2922 completed successfully.
2025-10-18 15:09:44.336 | INFO     | No patch generated. You can try running ACR again.
