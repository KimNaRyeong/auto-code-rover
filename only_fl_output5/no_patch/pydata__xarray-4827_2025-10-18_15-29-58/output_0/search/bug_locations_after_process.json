[
    {
        "rel_file_path": "xarray/core/merge.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/merge.py",
        "start": 1,
        "end": 940,
        "class_name": null,
        "method_name": null,
        "code": "1 from typing import (\n2     TYPE_CHECKING,\n3     AbstractSet,\n4     Any,\n5     Dict,\n6     Hashable,\n7     Iterable,\n8     List,\n9     Mapping,\n10     NamedTuple,\n11     Optional,\n12     Sequence,\n13     Set,\n14     Tuple,\n15     Union,\n16 )\n17 \n18 import pandas as pd\n19 \n20 from . import dtypes, pdcompat\n21 from .alignment import deep_align\n22 from .duck_array_ops import lazy_array_equiv\n23 from .utils import Frozen, compat_dict_union, dict_equiv\n24 from .variable import Variable, as_variable, assert_unique_multiindex_level_names\n25 \n26 if TYPE_CHECKING:\n27     from .coordinates import Coordinates\n28     from .dataarray import DataArray\n29     from .dataset import Dataset\n30 \n31     DimsLike = Union[Hashable, Sequence[Hashable]]\n32     ArrayLike = Any\n33     VariableLike = Union[\n34         ArrayLike,\n35         Tuple[DimsLike, ArrayLike],\n36         Tuple[DimsLike, ArrayLike, Mapping],\n37         Tuple[DimsLike, ArrayLike, Mapping, Mapping],\n38     ]\n39     XarrayValue = Union[DataArray, Variable, VariableLike]\n40     DatasetLike = Union[Dataset, Mapping[Hashable, XarrayValue]]\n41     CoercibleValue = Union[XarrayValue, pd.Series, pd.DataFrame]\n42     CoercibleMapping = Union[Dataset, Mapping[Hashable, CoercibleValue]]\n43 \n44 \n45 PANDAS_TYPES = (pd.Series, pd.DataFrame, pdcompat.Panel)\n46 \n47 _VALID_COMPAT = Frozen(\n48     {\n49         \"identical\": 0,\n50         \"equals\": 1,\n51         \"broadcast_equals\": 2,\n52         \"minimal\": 3,\n53         \"no_conflicts\": 4,\n54         \"override\": 5,\n55     }\n56 )\n57 \n58 \n59 def broadcast_dimension_size(variables: List[Variable]) -> Dict[Hashable, int]:\n60     \"\"\"Extract dimension sizes from a dictionary of variables.\n61 \n62     Raises ValueError if any dimensions have different sizes.\n63     \"\"\"\n64     dims: Dict[Hashable, int] = {}\n65     for var in variables:\n66         for dim, size in zip(var.dims, var.shape):\n67             if dim in dims and size != dims[dim]:\n68                 raise ValueError(\"index %r not aligned\" % dim)\n69             dims[dim] = size\n70     return dims\n71 \n72 \n73 class MergeError(ValueError):\n74     \"\"\"Error class for merge failures due to incompatible arguments.\"\"\"\n75 \n76     # inherits from ValueError for backward compatibility\n77     # TODO: move this to an xarray.exceptions module?\n78 \n79 \n80 def unique_variable(\n81     name: Hashable,\n82     variables: List[Variable],\n83     compat: str = \"broadcast_equals\",\n84     equals: bool = None,\n85 ) -> Variable:\n86     \"\"\"Return the unique variable from a list of variables or raise MergeError.\n87 \n88     Parameters\n89     ----------\n90     name : hashable\n91         Name for this variable.\n92     variables : list of Variable\n93         List of Variable objects, all of which go by the same name in different\n94         inputs.\n95     compat : {\"identical\", \"equals\", \"broadcast_equals\", \"no_conflicts\", \"override\"}, optional\n96         Type of equality check to use.\n97     equals : None or bool, optional\n98         corresponding to result of compat test\n99 \n100     Returns\n101     -------\n102     Variable to use in the result.\n103 \n104     Raises\n105     ------\n106     MergeError: if any of the variables are not equal.\n107     \"\"\"\n108     out = variables[0]\n109 \n110     if len(variables) == 1 or compat == \"override\":\n111         return out\n112 \n113     combine_method = None\n114 \n115     if compat == \"minimal\":\n116         compat = \"broadcast_equals\"\n117 \n118     if compat == \"broadcast_equals\":\n119         dim_lengths = broadcast_dimension_size(variables)\n120         out = out.set_dims(dim_lengths)\n121 \n122     if compat == \"no_conflicts\":\n123         combine_method = \"fillna\"\n124 \n125     if equals is None:\n126         # first check without comparing values i.e. no computes\n127         for var in variables[1:]:\n128             equals = getattr(out, compat)(var, equiv=lazy_array_equiv)\n129             if equals is not True:\n130                 break\n131 \n132         if equals is None:\n133             # now compare values with minimum number of computes\n134             out = out.compute()\n135             for var in variables[1:]:\n136                 equals = getattr(out, compat)(var)\n137                 if not equals:\n138                     break\n139 \n140     if not equals:\n141         raise MergeError(\n142             f\"conflicting values for variable {name!r} on objects to be combined. \"\n143             \"You can skip this check by specifying compat='override'.\"\n144         )\n145 \n146     if combine_method:\n147         for var in variables[1:]:\n148             out = getattr(out, combine_method)(var)\n149 \n150     return out\n151 \n152 \n153 def _assert_compat_valid(compat):\n154     if compat not in _VALID_COMPAT:\n155         raise ValueError(\n156             \"compat={!r} invalid: must be {}\".format(compat, set(_VALID_COMPAT))\n157         )\n158 \n159 \n160 MergeElement = Tuple[Variable, Optional[pd.Index]]\n161 \n162 \n163 def merge_collected(\n164     grouped: Dict[Hashable, List[MergeElement]],\n165     prioritized: Mapping[Hashable, MergeElement] = None,\n166     compat: str = \"minimal\",\n167 ) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:\n168     \"\"\"Merge dicts of variables, while resolving conflicts appropriately.\n169 \n170     Parameters\n171     ----------\n172     grouped : mapping\n173     prioritized : mapping\n174     compat : str\n175         Type of equality check to use when checking for conflicts.\n176 \n177     Returns\n178     -------\n179     Dict with keys taken by the union of keys on list_of_mappings,\n180     and Variable values corresponding to those that should be found on the\n181     merged result.\n182     \"\"\"\n183     if prioritized is None:\n184         prioritized = {}\n185 \n186     _assert_compat_valid(compat)\n187 \n188     merged_vars: Dict[Hashable, Variable] = {}\n189     merged_indexes: Dict[Hashable, pd.Index] = {}\n190 \n191     for name, elements_list in grouped.items():\n192         if name in prioritized:\n193             variable, index = prioritized[name]\n194             merged_vars[name] = variable\n195             if index is not None:\n196                 merged_indexes[name] = index\n197         else:\n198             indexed_elements = [\n199                 (variable, index)\n200                 for variable, index in elements_list\n201                 if index is not None\n202             ]\n203 \n204             if indexed_elements:\n205                 # TODO(shoyer): consider adjusting this logic. Are we really\n206                 # OK throwing away variable without an index in favor of\n207                 # indexed variables, without even checking if values match?\n208                 variable, index = indexed_elements[0]\n209                 for _, other_index in indexed_elements[1:]:\n210                     if not index.equals(other_index):\n211                         raise MergeError(\n212                             \"conflicting values for index %r on objects to be \"\n213                             \"combined:\\nfirst value: %r\\nsecond value: %r\"\n214                             % (name, index, other_index)\n215                         )\n216                 if compat == \"identical\":\n217                     for other_variable, _ in indexed_elements[1:]:\n218                         if not dict_equiv(variable.attrs, other_variable.attrs):\n219                             raise MergeError(\n220                                 \"conflicting attribute values on combined \"\n221                                 \"variable %r:\\nfirst value: %r\\nsecond value: %r\"\n222                                 % (name, variable.attrs, other_variable.attrs)\n223                             )\n224                 merged_vars[name] = variable\n225                 merged_indexes[name] = index\n226             else:\n227                 variables = [variable for variable, _ in elements_list]\n228                 try:\n229                     merged_vars[name] = unique_variable(name, variables, compat)\n230                 except MergeError:\n231                     if compat != \"minimal\":\n232                         # we need more than \"minimal\" compatibility (for which\n233                         # we drop conflicting coordinates)\n234                         raise\n235 \n236     return merged_vars, merged_indexes\n237 \n238 \n239 def collect_variables_and_indexes(\n240     list_of_mappings: \"List[DatasetLike]\",\n241 ) -> Dict[Hashable, List[MergeElement]]:\n242     \"\"\"Collect variables and indexes from list of mappings of xarray objects.\n243 \n244     Mappings must either be Dataset objects, or have values of one of the\n245     following types:\n246     - an xarray.Variable\n247     - a tuple `(dims, data[, attrs[, encoding]])` that can be converted in\n248       an xarray.Variable\n249     - or an xarray.DataArray\n250     \"\"\"\n251     from .dataarray import DataArray\n252     from .dataset import Dataset\n253 \n254     grouped: Dict[Hashable, List[Tuple[Variable, pd.Index]]] = {}\n255 \n256     def append(name, variable, index):\n257         values = grouped.setdefault(name, [])\n258         values.append((variable, index))\n259 \n260     def append_all(variables, indexes):\n261         for name, variable in variables.items():\n262             append(name, variable, indexes.get(name))\n263 \n264     for mapping in list_of_mappings:\n265         if isinstance(mapping, Dataset):\n266             append_all(mapping.variables, mapping.indexes)\n267             continue\n268 \n269         for name, variable in mapping.items():\n270             if isinstance(variable, DataArray):\n271                 coords = variable._coords.copy()  # use private API for speed\n272                 indexes = dict(variable.indexes)\n273                 # explicitly overwritten variables should take precedence\n274                 coords.pop(name, None)\n275                 indexes.pop(name, None)\n276                 append_all(coords, indexes)\n277 \n278             variable = as_variable(variable, name=name)\n279             if variable.dims == (name,):\n280                 variable = variable.to_index_variable()\n281                 index = variable.to_index()\n282             else:\n283                 index = None\n284             append(name, variable, index)\n285 \n286     return grouped\n287 \n288 \n289 def collect_from_coordinates(\n290     list_of_coords: \"List[Coordinates]\",\n291 ) -> Dict[Hashable, List[MergeElement]]:\n292     \"\"\"Collect variables and indexes to be merged from Coordinate objects.\"\"\"\n293     grouped: Dict[Hashable, List[Tuple[Variable, pd.Index]]] = {}\n294 \n295     for coords in list_of_coords:\n296         variables = coords.variables\n297         indexes = coords.indexes\n298         for name, variable in variables.items():\n299             value = grouped.setdefault(name, [])\n300             value.append((variable, indexes.get(name)))\n301     return grouped\n302 \n303 \n304 def merge_coordinates_without_align(\n305     objects: \"List[Coordinates]\",\n306     prioritized: Mapping[Hashable, MergeElement] = None,\n307     exclude_dims: AbstractSet = frozenset(),\n308 ) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:\n309     \"\"\"Merge variables/indexes from coordinates without automatic alignments.\n310 \n311     This function is used for merging coordinate from pre-existing xarray\n312     objects.\n313     \"\"\"\n314     collected = collect_from_coordinates(objects)\n315 \n316     if exclude_dims:\n317         filtered: Dict[Hashable, List[MergeElement]] = {}\n318         for name, elements in collected.items():\n319             new_elements = [\n320                 (variable, index)\n321                 for variable, index in elements\n322                 if exclude_dims.isdisjoint(variable.dims)\n323             ]\n324             if new_elements:\n325                 filtered[name] = new_elements\n326     else:\n327         filtered = collected\n328 \n329     return merge_collected(filtered, prioritized)\n330 \n331 \n332 def determine_coords(\n333     list_of_mappings: Iterable[\"DatasetLike\"],\n334 ) -> Tuple[Set[Hashable], Set[Hashable]]:\n335     \"\"\"Given a list of dicts with xarray object values, identify coordinates.\n336 \n337     Parameters\n338     ----------\n339     list_of_mappings : list of dict or list of Dataset\n340         Of the same form as the arguments to expand_variable_dicts.\n341 \n342     Returns\n343     -------\n344     coord_names : set of variable names\n345     noncoord_names : set of variable names\n346         All variable found in the input should appear in either the set of\n347         coordinate or non-coordinate names.\n348     \"\"\"\n349     from .dataarray import DataArray\n350     from .dataset import Dataset\n351 \n352     coord_names: Set[Hashable] = set()\n353     noncoord_names: Set[Hashable] = set()\n354 \n355     for mapping in list_of_mappings:\n356         if isinstance(mapping, Dataset):\n357             coord_names.update(mapping.coords)\n358             noncoord_names.update(mapping.data_vars)\n359         else:\n360             for name, var in mapping.items():\n361                 if isinstance(var, DataArray):\n362                     coords = set(var._coords)  # use private API for speed\n363                     # explicitly overwritten variables should take precedence\n364                     coords.discard(name)\n365                     coord_names.update(coords)\n366 \n367     return coord_names, noncoord_names\n368 \n369 \n370 def coerce_pandas_values(objects: Iterable[\"CoercibleMapping\"]) -> List[\"DatasetLike\"]:\n371     \"\"\"Convert pandas values found in a list of labeled objects.\n372 \n373     Parameters\n374     ----------\n375     objects : list of Dataset or mapping\n376         The mappings may contain any sort of objects coercible to\n377         xarray.Variables as keys, including pandas objects.\n378 \n379     Returns\n380     -------\n381     List of Dataset or dictionary objects. Any inputs or values in the inputs\n382     that were pandas objects have been converted into native xarray objects.\n383     \"\"\"\n384     from .dataarray import DataArray\n385     from .dataset import Dataset\n386 \n387     out = []\n388     for obj in objects:\n389         if isinstance(obj, Dataset):\n390             variables: \"DatasetLike\" = obj\n391         else:\n392             variables = {}\n393             if isinstance(obj, PANDAS_TYPES):\n394                 obj = dict(obj.iteritems())\n395             for k, v in obj.items():\n396                 if isinstance(v, PANDAS_TYPES):\n397                     v = DataArray(v)\n398                 variables[k] = v\n399         out.append(variables)\n400     return out\n401 \n402 \n403 def _get_priority_vars_and_indexes(\n404     objects: List[\"DatasetLike\"], priority_arg: Optional[int], compat: str = \"equals\"\n405 ) -> Dict[Hashable, MergeElement]:\n406     \"\"\"Extract the priority variable from a list of mappings.\n407 \n408     We need this method because in some cases the priority argument itself\n409     might have conflicting values (e.g., if it is a dict with two DataArray\n410     values with conflicting coordinate values).\n411 \n412     Parameters\n413     ----------\n414     objects : list of dict-like of Variable\n415         Dictionaries in which to find the priority variables.\n416     priority_arg : int or None\n417         Integer object whose variable should take priority.\n418     compat : {\"identical\", \"equals\", \"broadcast_equals\", \"no_conflicts\"}, optional\n419         Compatibility checks to use when merging variables.\n420 \n421     Returns\n422     -------\n423     A dictionary of variables and associated indexes (if any) to prioritize.\n424     \"\"\"\n425     if priority_arg is None:\n426         return {}\n427 \n428     collected = collect_variables_and_indexes([objects[priority_arg]])\n429     variables, indexes = merge_collected(collected, compat=compat)\n430     grouped: Dict[Hashable, MergeElement] = {}\n431     for name, variable in variables.items():\n432         grouped[name] = (variable, indexes.get(name))\n433     return grouped\n434 \n435 \n436 def merge_coords(\n437     objects: Iterable[\"CoercibleMapping\"],\n438     compat: str = \"minimal\",\n439     join: str = \"outer\",\n440     priority_arg: Optional[int] = None,\n441     indexes: Optional[Mapping[Hashable, pd.Index]] = None,\n442     fill_value: object = dtypes.NA,\n443 ) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:\n444     \"\"\"Merge coordinate variables.\n445 \n446     See merge_core below for argument descriptions. This works similarly to\n447     merge_core, except everything we don't worry about whether variables are\n448     coordinates or not.\n449     \"\"\"\n450     _assert_compat_valid(compat)\n451     coerced = coerce_pandas_values(objects)\n452     aligned = deep_align(\n453         coerced, join=join, copy=False, indexes=indexes, fill_value=fill_value\n454     )\n455     collected = collect_variables_and_indexes(aligned)\n456     prioritized = _get_priority_vars_and_indexes(aligned, priority_arg, compat=compat)\n457     variables, out_indexes = merge_collected(collected, prioritized, compat=compat)\n458     assert_unique_multiindex_level_names(variables)\n459     return variables, out_indexes\n460 \n461 \n462 def merge_data_and_coords(data, coords, compat=\"broadcast_equals\", join=\"outer\"):\n463     \"\"\"Used in Dataset.__init__.\"\"\"\n464     objects = [data, coords]\n465     explicit_coords = coords.keys()\n466     indexes = dict(_extract_indexes_from_coords(coords))\n467     return merge_core(\n468         objects, compat, join, explicit_coords=explicit_coords, indexes=indexes\n469     )\n470 \n471 \n472 def _extract_indexes_from_coords(coords):\n473     \"\"\"Yields the name & index of valid indexes from a mapping of coords\"\"\"\n474     for name, variable in coords.items():\n475         variable = as_variable(variable, name=name)\n476         if variable.dims == (name,):\n477             yield name, variable.to_index()\n478 \n479 \n480 def assert_valid_explicit_coords(variables, dims, explicit_coords):\n481     \"\"\"Validate explicit coordinate names/dims.\n482 \n483     Raise a MergeError if an explicit coord shares a name with a dimension\n484     but is comprised of arbitrary dimensions.\n485     \"\"\"\n486     for coord_name in explicit_coords:\n487         if coord_name in dims and variables[coord_name].dims != (coord_name,):\n488             raise MergeError(\n489                 \"coordinate %s shares a name with a dataset dimension, but is \"\n490                 \"not a 1D variable along that dimension. This is disallowed \"\n491                 \"by the xarray data model.\" % coord_name\n492             )\n493 \n494 \n495 def merge_attrs(variable_attrs, combine_attrs):\n496     \"\"\"Combine attributes from different variables according to combine_attrs\"\"\"\n497     if not variable_attrs:\n498         # no attributes to merge\n499         return None\n500 \n501     if combine_attrs == \"drop\":\n502         return {}\n503     elif combine_attrs == \"override\":\n504         return dict(variable_attrs[0])\n505     elif combine_attrs == \"no_conflicts\":\n506         result = dict(variable_attrs[0])\n507         for attrs in variable_attrs[1:]:\n508             try:\n509                 result = compat_dict_union(result, attrs)\n510             except ValueError:\n511                 raise MergeError(\n512                     \"combine_attrs='no_conflicts', but some values are not \"\n513                     \"the same. Merging %s with %s\" % (str(result), str(attrs))\n514                 )\n515         return result\n516     elif combine_attrs == \"identical\":\n517         result = dict(variable_attrs[0])\n518         for attrs in variable_attrs[1:]:\n519             if not dict_equiv(result, attrs):\n520                 raise MergeError(\n521                     \"combine_attrs='identical', but attrs differ. First is %s \"\n522                     \", other is %s.\" % (str(result), str(attrs))\n523                 )\n524         return result\n525     else:\n526         raise ValueError(\"Unrecognised value for combine_attrs=%s\" % combine_attrs)\n527 \n528 \n529 class _MergeResult(NamedTuple):\n530     variables: Dict[Hashable, Variable]\n531     coord_names: Set[Hashable]\n532     dims: Dict[Hashable, int]\n533     indexes: Dict[Hashable, pd.Index]\n534     attrs: Dict[Hashable, Any]\n535 \n536 \n537 def merge_core(\n538     objects: Iterable[\"CoercibleMapping\"],\n539     compat: str = \"broadcast_equals\",\n540     join: str = \"outer\",\n541     combine_attrs: Optional[str] = \"override\",\n542     priority_arg: Optional[int] = None,\n543     explicit_coords: Optional[Sequence] = None,\n544     indexes: Optional[Mapping[Hashable, pd.Index]] = None,\n545     fill_value: object = dtypes.NA,\n546 ) -> _MergeResult:\n547     \"\"\"Core logic for merging labeled objects.\n548 \n549     This is not public API.\n550 \n551     Parameters\n552     ----------\n553     objects : list of mapping\n554         All values must be convertable to labeled arrays.\n555     compat : {\"identical\", \"equals\", \"broadcast_equals\", \"no_conflicts\", \"override\"}, optional\n556         Compatibility checks to use when merging variables.\n557     join : {\"outer\", \"inner\", \"left\", \"right\"}, optional\n558         How to combine objects with different indexes.\n559     combine_attrs : {\"drop\", \"identical\", \"no_conflicts\", \"override\"}, optional\n560         How to combine attributes of objects\n561     priority_arg : int, optional\n562         Optional argument in `objects` that takes precedence over the others.\n563     explicit_coords : set, optional\n564         An explicit list of variables from `objects` that are coordinates.\n565     indexes : dict, optional\n566         Dictionary with values given by pandas.Index objects.\n567     fill_value : scalar, optional\n568         Value to use for newly missing values\n569 \n570     Returns\n571     -------\n572     variables : dict\n573         Dictionary of Variable objects.\n574     coord_names : set\n575         Set of coordinate names.\n576     dims : dict\n577         Dictionary mapping from dimension names to sizes.\n578     attrs : dict\n579         Dictionary of attributes\n580 \n581     Raises\n582     ------\n583     MergeError if the merge cannot be done successfully.\n584     \"\"\"\n585     from .dataarray import DataArray\n586     from .dataset import Dataset, calculate_dimensions\n587 \n588     _assert_compat_valid(compat)\n589 \n590     coerced = coerce_pandas_values(objects)\n591     aligned = deep_align(\n592         coerced, join=join, copy=False, indexes=indexes, fill_value=fill_value\n593     )\n594     collected = collect_variables_and_indexes(aligned)\n595 \n596     prioritized = _get_priority_vars_and_indexes(aligned, priority_arg, compat=compat)\n597     variables, out_indexes = merge_collected(collected, prioritized, compat=compat)\n598     assert_unique_multiindex_level_names(variables)\n599 \n600     dims = calculate_dimensions(variables)\n601 \n602     coord_names, noncoord_names = determine_coords(coerced)\n603     if explicit_coords is not None:\n604         assert_valid_explicit_coords(variables, dims, explicit_coords)\n605         coord_names.update(explicit_coords)\n606     for dim, size in dims.items():\n607         if dim in variables:\n608             coord_names.add(dim)\n609     ambiguous_coords = coord_names.intersection(noncoord_names)\n610     if ambiguous_coords:\n611         raise MergeError(\n612             \"unable to determine if these variables should be \"\n613             \"coordinates or not in the merged result: %s\" % ambiguous_coords\n614         )\n615 \n616     attrs = merge_attrs(\n617         [\n618             var.attrs\n619             for var in coerced\n620             if isinstance(var, Dataset) or isinstance(var, DataArray)\n621         ],\n622         combine_attrs,\n623     )\n624 \n625     return _MergeResult(variables, coord_names, dims, out_indexes, attrs)\n626 \n627 \n628 def merge(\n629     objects: Iterable[Union[\"DataArray\", \"CoercibleMapping\"]],\n630     compat: str = \"no_conflicts\",\n631     join: str = \"outer\",\n632     fill_value: object = dtypes.NA,\n633     combine_attrs: str = \"drop\",\n634 ) -> \"Dataset\":\n635     \"\"\"Merge any number of xarray objects into a single Dataset as variables.\n636 \n637     Parameters\n638     ----------\n639     objects : iterable of Dataset or iterable of DataArray or iterable of dict-like\n640         Merge together all variables from these objects. If any of them are\n641         DataArray objects, they must have a name.\n642     compat : {\"identical\", \"equals\", \"broadcast_equals\", \"no_conflicts\", \"override\"}, optional\n643         String indicating how to compare variables of the same name for\n644         potential conflicts:\n645 \n646         - \"broadcast_equals\": all values must be equal when variables are\n647           broadcast against each other to ensure common dimensions.\n648         - \"equals\": all values and dimensions must be the same.\n649         - \"identical\": all values, dimensions and attributes must be the\n650           same.\n651         - \"no_conflicts\": only values which are not null in both datasets\n652           must be equal. The returned dataset then contains the combination\n653           of all non-null values.\n654         - \"override\": skip comparing and pick variable from first dataset\n655     join : {\"outer\", \"inner\", \"left\", \"right\", \"exact\"}, optional\n656         String indicating how to combine differing indexes in objects.\n657 \n658         - \"outer\": use the union of object indexes\n659         - \"inner\": use the intersection of object indexes\n660         - \"left\": use indexes from the first object with each dimension\n661         - \"right\": use indexes from the last object with each dimension\n662         - \"exact\": instead of aligning, raise `ValueError` when indexes to be\n663           aligned are not equal\n664         - \"override\": if indexes are of same size, rewrite indexes to be\n665           those of the first object with that dimension. Indexes for the same\n666           dimension must have the same size in all objects.\n667     fill_value : scalar or dict-like, optional\n668         Value to use for newly missing values. If a dict-like, maps\n669         variable names to fill values. Use a data array's name to\n670         refer to its values.\n671     combine_attrs : {\"drop\", \"identical\", \"no_conflicts\", \"override\"}, \\\n672                     default: \"drop\"\n673         String indicating how to combine attrs of the objects being merged:\n674 \n675         - \"drop\": empty attrs on returned Dataset.\n676         - \"identical\": all attrs must be the same on every object.\n677         - \"no_conflicts\": attrs from all objects are combined, any that have\n678           the same name must also have the same value.\n679         - \"override\": skip comparing and copy attrs from the first dataset to\n680           the result.\n681 \n682     Returns\n683     -------\n684     Dataset\n685         Dataset with combined variables from each object.\n686 \n687     Examples\n688     --------\n689     >>> import xarray as xr\n690     >>> x = xr.DataArray(\n691     ...     [[1.0, 2.0], [3.0, 5.0]],\n692     ...     dims=(\"lat\", \"lon\"),\n693     ...     coords={\"lat\": [35.0, 40.0], \"lon\": [100.0, 120.0]},\n694     ...     name=\"var1\",\n695     ... )\n696     >>> y = xr.DataArray(\n697     ...     [[5.0, 6.0], [7.0, 8.0]],\n698     ...     dims=(\"lat\", \"lon\"),\n699     ...     coords={\"lat\": [35.0, 42.0], \"lon\": [100.0, 150.0]},\n700     ...     name=\"var2\",\n701     ... )\n702     >>> z = xr.DataArray(\n703     ...     [[0.0, 3.0], [4.0, 9.0]],\n704     ...     dims=(\"time\", \"lon\"),\n705     ...     coords={\"time\": [30.0, 60.0], \"lon\": [100.0, 150.0]},\n706     ...     name=\"var3\",\n707     ... )\n708 \n709     >>> x\n710     <xarray.DataArray 'var1' (lat: 2, lon: 2)>\n711     array([[1., 2.],\n712            [3., 5.]])\n713     Coordinates:\n714       * lat      (lat) float64 35.0 40.0\n715       * lon      (lon) float64 100.0 120.0\n716 \n717     >>> y\n718     <xarray.DataArray 'var2' (lat: 2, lon: 2)>\n719     array([[5., 6.],\n720            [7., 8.]])\n721     Coordinates:\n722       * lat      (lat) float64 35.0 42.0\n723       * lon      (lon) float64 100.0 150.0\n724 \n725     >>> z\n726     <xarray.DataArray 'var3' (time: 2, lon: 2)>\n727     array([[0., 3.],\n728            [4., 9.]])\n729     Coordinates:\n730       * time     (time) float64 30.0 60.0\n731       * lon      (lon) float64 100.0 150.0\n732 \n733     >>> xr.merge([x, y, z])\n734     <xarray.Dataset>\n735     Dimensions:  (lat: 3, lon: 3, time: 2)\n736     Coordinates:\n737       * lat      (lat) float64 35.0 40.0 42.0\n738       * lon      (lon) float64 100.0 120.0 150.0\n739       * time     (time) float64 30.0 60.0\n740     Data variables:\n741         var1     (lat, lon) float64 1.0 2.0 nan 3.0 5.0 nan nan nan nan\n742         var2     (lat, lon) float64 5.0 nan 6.0 nan nan nan 7.0 nan 8.0\n743         var3     (time, lon) float64 0.0 nan 3.0 4.0 nan 9.0\n744 \n745     >>> xr.merge([x, y, z], compat=\"identical\")\n746     <xarray.Dataset>\n747     Dimensions:  (lat: 3, lon: 3, time: 2)\n748     Coordinates:\n749       * lat      (lat) float64 35.0 40.0 42.0\n750       * lon      (lon) float64 100.0 120.0 150.0\n751       * time     (time) float64 30.0 60.0\n752     Data variables:\n753         var1     (lat, lon) float64 1.0 2.0 nan 3.0 5.0 nan nan nan nan\n754         var2     (lat, lon) float64 5.0 nan 6.0 nan nan nan 7.0 nan 8.0\n755         var3     (time, lon) float64 0.0 nan 3.0 4.0 nan 9.0\n756 \n757     >>> xr.merge([x, y, z], compat=\"equals\")\n758     <xarray.Dataset>\n759     Dimensions:  (lat: 3, lon: 3, time: 2)\n760     Coordinates:\n761       * lat      (lat) float64 35.0 40.0 42.0\n762       * lon      (lon) float64 100.0 120.0 150.0\n763       * time     (time) float64 30.0 60.0\n764     Data variables:\n765         var1     (lat, lon) float64 1.0 2.0 nan 3.0 5.0 nan nan nan nan\n766         var2     (lat, lon) float64 5.0 nan 6.0 nan nan nan 7.0 nan 8.0\n767         var3     (time, lon) float64 0.0 nan 3.0 4.0 nan 9.0\n768 \n769     >>> xr.merge([x, y, z], compat=\"equals\", fill_value=-999.0)\n770     <xarray.Dataset>\n771     Dimensions:  (lat: 3, lon: 3, time: 2)\n772     Coordinates:\n773       * lat      (lat) float64 35.0 40.0 42.0\n774       * lon      (lon) float64 100.0 120.0 150.0\n775       * time     (time) float64 30.0 60.0\n776     Data variables:\n777         var1     (lat, lon) float64 1.0 2.0 -999.0 3.0 ... -999.0 -999.0 -999.0\n778         var2     (lat, lon) float64 5.0 -999.0 6.0 -999.0 ... -999.0 7.0 -999.0 8.0\n779         var3     (time, lon) float64 0.0 -999.0 3.0 4.0 -999.0 9.0\n780 \n781     >>> xr.merge([x, y, z], join=\"override\")\n782     <xarray.Dataset>\n783     Dimensions:  (lat: 2, lon: 2, time: 2)\n784     Coordinates:\n785       * lat      (lat) float64 35.0 40.0\n786       * lon      (lon) float64 100.0 120.0\n787       * time     (time) float64 30.0 60.0\n788     Data variables:\n789         var1     (lat, lon) float64 1.0 2.0 3.0 5.0\n790         var2     (lat, lon) float64 5.0 6.0 7.0 8.0\n791         var3     (time, lon) float64 0.0 3.0 4.0 9.0\n792 \n793     >>> xr.merge([x, y, z], join=\"inner\")\n794     <xarray.Dataset>\n795     Dimensions:  (lat: 1, lon: 1, time: 2)\n796     Coordinates:\n797       * lat      (lat) float64 35.0\n798       * lon      (lon) float64 100.0\n799       * time     (time) float64 30.0 60.0\n800     Data variables:\n801         var1     (lat, lon) float64 1.0\n802         var2     (lat, lon) float64 5.0\n803         var3     (time, lon) float64 0.0 4.0\n804 \n805     >>> xr.merge([x, y, z], compat=\"identical\", join=\"inner\")\n806     <xarray.Dataset>\n807     Dimensions:  (lat: 1, lon: 1, time: 2)\n808     Coordinates:\n809       * lat      (lat) float64 35.0\n810       * lon      (lon) float64 100.0\n811       * time     (time) float64 30.0 60.0\n812     Data variables:\n813         var1     (lat, lon) float64 1.0\n814         var2     (lat, lon) float64 5.0\n815         var3     (time, lon) float64 0.0 4.0\n816 \n817     >>> xr.merge([x, y, z], compat=\"broadcast_equals\", join=\"outer\")\n818     <xarray.Dataset>\n819     Dimensions:  (lat: 3, lon: 3, time: 2)\n820     Coordinates:\n821       * lat      (lat) float64 35.0 40.0 42.0\n822       * lon      (lon) float64 100.0 120.0 150.0\n823       * time     (time) float64 30.0 60.0\n824     Data variables:\n825         var1     (lat, lon) float64 1.0 2.0 nan 3.0 5.0 nan nan nan nan\n826         var2     (lat, lon) float64 5.0 nan 6.0 nan nan nan 7.0 nan 8.0\n827         var3     (time, lon) float64 0.0 nan 3.0 4.0 nan 9.0\n828 \n829     >>> xr.merge([x, y, z], join=\"exact\")\n830     Traceback (most recent call last):\n831     ...\n832     ValueError: indexes along dimension 'lat' are not equal\n833 \n834     Raises\n835     ------\n836     xarray.MergeError\n837         If any variables with the same name have conflicting values.\n838 \n839     See also\n840     --------\n841     concat\n842     \"\"\"\n843     from .dataarray import DataArray\n844     from .dataset import Dataset\n845 \n846     dict_like_objects = []\n847     for obj in objects:\n848         if not isinstance(obj, (DataArray, Dataset, dict)):\n849             raise TypeError(\n850                 \"objects must be an iterable containing only \"\n851                 \"Dataset(s), DataArray(s), and dictionaries.\"\n852             )\n853 \n854         obj = obj.to_dataset(promote_attrs=True) if isinstance(obj, DataArray) else obj\n855         dict_like_objects.append(obj)\n856 \n857     merge_result = merge_core(\n858         dict_like_objects,\n859         compat,\n860         join,\n861         combine_attrs=combine_attrs,\n862         fill_value=fill_value,\n863     )\n864     merged = Dataset._construct_direct(**merge_result._asdict())\n865     return merged\n866 \n867 \n868 def dataset_merge_method(\n869     dataset: \"Dataset\",\n870     other: \"CoercibleMapping\",\n871     overwrite_vars: Union[Hashable, Iterable[Hashable]],\n872     compat: str,\n873     join: str,\n874     fill_value: Any,\n875 ) -> _MergeResult:\n876     \"\"\"Guts of the Dataset.merge method.\"\"\"\n877     # we are locked into supporting overwrite_vars for the Dataset.merge\n878     # method due for backwards compatibility\n879     # TODO: consider deprecating it?\n880 \n881     if isinstance(overwrite_vars, Iterable) and not isinstance(overwrite_vars, str):\n882         overwrite_vars = set(overwrite_vars)\n883     else:\n884         overwrite_vars = {overwrite_vars}\n885 \n886     if not overwrite_vars:\n887         objs = [dataset, other]\n888         priority_arg = None\n889     elif overwrite_vars == set(other):\n890         objs = [dataset, other]\n891         priority_arg = 1\n892     else:\n893         other_overwrite: Dict[Hashable, CoercibleValue] = {}\n894         other_no_overwrite: Dict[Hashable, CoercibleValue] = {}\n895         for k, v in other.items():\n896             if k in overwrite_vars:\n897                 other_overwrite[k] = v\n898             else:\n899                 other_no_overwrite[k] = v\n900         objs = [dataset, other_no_overwrite, other_overwrite]\n901         priority_arg = 2\n902 \n903     return merge_core(\n904         objs, compat, join, priority_arg=priority_arg, fill_value=fill_value\n905     )\n906 \n907 \n908 def dataset_update_method(\n909     dataset: \"Dataset\", other: \"CoercibleMapping\"\n910 ) -> _MergeResult:\n911     \"\"\"Guts of the Dataset.update method.\n912 \n913     This drops a duplicated coordinates from `other` if `other` is not an\n914     `xarray.Dataset`, e.g., if it's a dict with DataArray values (GH2068,\n915     GH2180).\n916     \"\"\"\n917     from .dataarray import DataArray\n918     from .dataset import Dataset\n919 \n920     if not isinstance(other, Dataset):\n921         other = dict(other)\n922         for key, value in other.items():\n923             if isinstance(value, DataArray):\n924                 # drop conflicting coordinates\n925                 coord_names = [\n926                     c\n927                     for c in value.coords\n928                     if c not in value.dims and c in dataset.coords\n929                 ]\n930                 if coord_names:\n931                     other[key] = value.drop_vars(coord_names)\n932 \n933     # use ds.coords and not ds.indexes, else str coords are cast to object\n934     indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}\n935     return merge_core(\n936         [dataset, other],\n937         priority_arg=1,\n938         indexes=indexes,\n939         combine_attrs=\"override\",\n940     )\n",
        "intended_behavior": "Extend the handling of `combine_attrs` to include a `\"drop_conflicts\"` or `\"matching\"` option. This new option should behave similarly to the `\"no_conflicts\"` option but instead of raising an error when conflicts are encountered, it should silently drop any attributes that have conflicting values across the objects to be merged."
    },
    {
        "rel_file_path": "xarray/core/combine.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/combine.py",
        "start": 1,
        "end": 800,
        "class_name": null,
        "method_name": null,
        "code": "1 import itertools\n2 from collections import Counter\n3 \n4 import pandas as pd\n5 \n6 from . import dtypes\n7 from .concat import concat\n8 from .dataarray import DataArray\n9 from .dataset import Dataset\n10 from .merge import merge\n11 \n12 \n13 def _infer_concat_order_from_positions(datasets):\n14     combined_ids = dict(_infer_tile_ids_from_nested_list(datasets, ()))\n15     return combined_ids\n16 \n17 \n18 def _infer_tile_ids_from_nested_list(entry, current_pos):\n19     \"\"\"\n20     Given a list of lists (of lists...) of objects, returns a iterator\n21     which returns a tuple containing the index of each object in the nested\n22     list structure as the key, and the object. This can then be called by the\n23     dict constructor to create a dictionary of the objects organised by their\n24     position in the original nested list.\n25 \n26     Recursively traverses the given structure, while keeping track of the\n27     current position. Should work for any type of object which isn't a list.\n28 \n29     Parameters\n30     ----------\n31     entry : list[list[obj, obj, ...], ...]\n32         List of lists of arbitrary depth, containing objects in the order\n33         they are to be concatenated.\n34 \n35     Returns\n36     -------\n37     combined_tile_ids : dict[tuple(int, ...), obj]\n38     \"\"\"\n39 \n40     if isinstance(entry, list):\n41         for i, item in enumerate(entry):\n42             yield from _infer_tile_ids_from_nested_list(item, current_pos + (i,))\n43     else:\n44         yield current_pos, entry\n45 \n46 \n47 def _infer_concat_order_from_coords(datasets):\n48 \n49     concat_dims = []\n50     tile_ids = [() for ds in datasets]\n51 \n52     # All datasets have same variables because they've been grouped as such\n53     ds0 = datasets[0]\n54     for dim in ds0.dims:\n55 \n56         # Check if dim is a coordinate dimension\n57         if dim in ds0:\n58 \n59             # Need to read coordinate values to do ordering\n60             indexes = [ds.indexes.get(dim) for ds in datasets]\n61             if any(index is None for index in indexes):\n62                 raise ValueError(\n63                     \"Every dimension needs a coordinate for \"\n64                     \"inferring concatenation order\"\n65                 )\n66 \n67             # If dimension coordinate values are same on every dataset then\n68             # should be leaving this dimension alone (it's just a \"bystander\")\n69             if not all(index.equals(indexes[0]) for index in indexes[1:]):\n70 \n71                 # Infer order datasets should be arranged in along this dim\n72                 concat_dims.append(dim)\n73 \n74                 if all(index.is_monotonic_increasing for index in indexes):\n75                     ascending = True\n76                 elif all(index.is_monotonic_decreasing for index in indexes):\n77                     ascending = False\n78                 else:\n79                     raise ValueError(\n80                         \"Coordinate variable {} is neither \"\n81                         \"monotonically increasing nor \"\n82                         \"monotonically decreasing on all datasets\".format(dim)\n83                     )\n84 \n85                 # Assume that any two datasets whose coord along dim starts\n86                 # with the same value have the same coord values throughout.\n87                 if any(index.size == 0 for index in indexes):\n88                     raise ValueError(\"Cannot handle size zero dimensions\")\n89                 first_items = pd.Index([index[0] for index in indexes])\n90 \n91                 # Sort datasets along dim\n92                 # We want rank but with identical elements given identical\n93                 # position indices - they should be concatenated along another\n94                 # dimension, not along this one\n95                 series = first_items.to_series()\n96                 rank = series.rank(\n97                     method=\"dense\", ascending=ascending, numeric_only=False\n98                 )\n99                 order = rank.astype(int).values - 1\n100 \n101                 # Append positions along extra dimension to structure which\n102                 # encodes the multi-dimensional concatentation order\n103                 tile_ids = [\n104                     tile_id + (position,) for tile_id, position in zip(tile_ids, order)\n105                 ]\n106 \n107     if len(datasets) > 1 and not concat_dims:\n108         raise ValueError(\n109             \"Could not find any dimension coordinates to use to \"\n110             \"order the datasets for concatenation\"\n111         )\n112 \n113     combined_ids = dict(zip(tile_ids, datasets))\n114 \n115     return combined_ids, concat_dims\n116 \n117 \n118 def _check_dimension_depth_tile_ids(combined_tile_ids):\n119     \"\"\"\n120     Check all tuples are the same length, i.e. check that all lists are\n121     nested to the same depth.\n122     \"\"\"\n123     tile_ids = combined_tile_ids.keys()\n124     nesting_depths = [len(tile_id) for tile_id in tile_ids]\n125     if not nesting_depths:\n126         nesting_depths = [0]\n127     if not set(nesting_depths) == {nesting_depths[0]}:\n128         raise ValueError(\n129             \"The supplied objects do not form a hypercube because\"\n130             \" sub-lists do not have consistent depths\"\n131         )\n132     # return these just to be reused in _check_shape_tile_ids\n133     return tile_ids, nesting_depths\n134 \n135 \n136 def _check_shape_tile_ids(combined_tile_ids):\n137     \"\"\"Check all lists along one dimension are same length.\"\"\"\n138     tile_ids, nesting_depths = _check_dimension_depth_tile_ids(combined_tile_ids)\n139     for dim in range(nesting_depths[0]):\n140         indices_along_dim = [tile_id[dim] for tile_id in tile_ids]\n141         occurrences = Counter(indices_along_dim)\n142         if len(set(occurrences.values())) != 1:\n143             raise ValueError(\n144                 \"The supplied objects do not form a hypercube \"\n145                 \"because sub-lists do not have consistent \"\n146                 \"lengths along dimension\" + str(dim)\n147             )\n148 \n149 \n150 def _combine_nd(\n151     combined_ids,\n152     concat_dims,\n153     data_vars=\"all\",\n154     coords=\"different\",\n155     compat=\"no_conflicts\",\n156     fill_value=dtypes.NA,\n157     join=\"outer\",\n158     combine_attrs=\"drop\",\n159 ):\n160     \"\"\"\n161     Combines an N-dimensional structure of datasets into one by applying a\n162     series of either concat and merge operations along each dimension.\n163 \n164     No checks are performed on the consistency of the datasets, concat_dims or\n165     tile_IDs, because it is assumed that this has already been done.\n166 \n167     Parameters\n168     ----------\n169     combined_ids : Dict[Tuple[int, ...]], xarray.Dataset]\n170         Structure containing all datasets to be concatenated with \"tile_IDs\" as\n171         keys, which specify position within the desired final combined result.\n172     concat_dims : sequence of str\n173         The dimensions along which the datasets should be concatenated. Must be\n174         in order, and the length must match the length of the tuples used as\n175         keys in combined_ids. If the string is a dimension name then concat\n176         along that dimension, if it is None then merge.\n177 \n178     Returns\n179     -------\n180     combined_ds : xarray.Dataset\n181     \"\"\"\n182 \n183     example_tile_id = next(iter(combined_ids.keys()))\n184 \n185     n_dims = len(example_tile_id)\n186     if len(concat_dims) != n_dims:\n187         raise ValueError(\n188             \"concat_dims has length {} but the datasets \"\n189             \"passed are nested in a {}-dimensional structure\".format(\n190                 len(concat_dims), n_dims\n191             )\n192         )\n193 \n194     # Each iteration of this loop reduces the length of the tile_ids tuples\n195     # by one. It always combines along the first dimension, removing the first\n196     # element of the tuple\n197     for concat_dim in concat_dims:\n198         combined_ids = _combine_all_along_first_dim(\n199             combined_ids,\n200             dim=concat_dim,\n201             data_vars=data_vars,\n202             coords=coords,\n203             compat=compat,\n204             fill_value=fill_value,\n205             join=join,\n206             combine_attrs=combine_attrs,\n207         )\n208     (combined_ds,) = combined_ids.values()\n209     return combined_ds\n210 \n211 \n212 def _combine_all_along_first_dim(\n213     combined_ids,\n214     dim,\n215     data_vars,\n216     coords,\n217     compat,\n218     fill_value=dtypes.NA,\n219     join=\"outer\",\n220     combine_attrs=\"drop\",\n221 ):\n222 \n223     # Group into lines of datasets which must be combined along dim\n224     # need to sort by _new_tile_id first for groupby to work\n225     # TODO: is the sorted need?\n226     combined_ids = dict(sorted(combined_ids.items(), key=_new_tile_id))\n227     grouped = itertools.groupby(combined_ids.items(), key=_new_tile_id)\n228 \n229     # Combine all of these datasets along dim\n230     new_combined_ids = {}\n231     for new_id, group in grouped:\n232         combined_ids = dict(sorted(group))\n233         datasets = combined_ids.values()\n234         new_combined_ids[new_id] = _combine_1d(\n235             datasets, dim, compat, data_vars, coords, fill_value, join, combine_attrs\n236         )\n237     return new_combined_ids\n238 \n239 \n240 def _combine_1d(\n241     datasets,\n242     concat_dim,\n243     compat=\"no_conflicts\",\n244     data_vars=\"all\",\n245     coords=\"different\",\n246     fill_value=dtypes.NA,\n247     join=\"outer\",\n248     combine_attrs=\"drop\",\n249 ):\n250     \"\"\"\n251     Applies either concat or merge to 1D list of datasets depending on value\n252     of concat_dim\n253     \"\"\"\n254 \n255     if concat_dim is not None:\n256         try:\n257             combined = concat(\n258                 datasets,\n259                 dim=concat_dim,\n260                 data_vars=data_vars,\n261                 coords=coords,\n262                 compat=compat,\n263                 fill_value=fill_value,\n264                 join=join,\n265                 combine_attrs=combine_attrs,\n266             )\n267         except ValueError as err:\n268             if \"encountered unexpected variable\" in str(err):\n269                 raise ValueError(\n270                     \"These objects cannot be combined using only \"\n271                     \"xarray.combine_nested, instead either use \"\n272                     \"xarray.combine_by_coords, or do it manually \"\n273                     \"with xarray.concat, xarray.merge and \"\n274                     \"xarray.align\"\n275                 )\n276             else:\n277                 raise\n278     else:\n279         combined = merge(\n280             datasets,\n281             compat=compat,\n282             fill_value=fill_value,\n283             join=join,\n284             combine_attrs=combine_attrs,\n285         )\n286 \n287     return combined\n288 \n289 \n290 def _new_tile_id(single_id_ds_pair):\n291     tile_id, ds = single_id_ds_pair\n292     return tile_id[1:]\n293 \n294 \n295 def _nested_combine(\n296     datasets,\n297     concat_dims,\n298     compat,\n299     data_vars,\n300     coords,\n301     ids,\n302     fill_value=dtypes.NA,\n303     join=\"outer\",\n304     combine_attrs=\"drop\",\n305 ):\n306 \n307     if len(datasets) == 0:\n308         return Dataset()\n309 \n310     # Arrange datasets for concatenation\n311     # Use information from the shape of the user input\n312     if not ids:\n313         # Determine tile_IDs by structure of input in N-D\n314         # (i.e. ordering in list-of-lists)\n315         combined_ids = _infer_concat_order_from_positions(datasets)\n316     else:\n317         # Already sorted so just use the ids already passed\n318         combined_ids = dict(zip(ids, datasets))\n319 \n320     # Check that the inferred shape is combinable\n321     _check_shape_tile_ids(combined_ids)\n322 \n323     # Apply series of concatenate or merge operations along each dimension\n324     combined = _combine_nd(\n325         combined_ids,\n326         concat_dims,\n327         compat=compat,\n328         data_vars=data_vars,\n329         coords=coords,\n330         fill_value=fill_value,\n331         join=join,\n332         combine_attrs=combine_attrs,\n333     )\n334     return combined\n335 \n336 \n337 def combine_nested(\n338     datasets,\n339     concat_dim,\n340     compat=\"no_conflicts\",\n341     data_vars=\"all\",\n342     coords=\"different\",\n343     fill_value=dtypes.NA,\n344     join=\"outer\",\n345     combine_attrs=\"drop\",\n346 ):\n347     \"\"\"\n348     Explicitly combine an N-dimensional grid of datasets into one by using a\n349     succession of concat and merge operations along each dimension of the grid.\n350 \n351     Does not sort the supplied datasets under any circumstances, so the\n352     datasets must be passed in the order you wish them to be concatenated. It\n353     does align coordinates, but different variables on datasets can cause it to\n354     fail under some scenarios. In complex cases, you may need to clean up your\n355     data and use concat/merge explicitly.\n356 \n357     To concatenate along multiple dimensions the datasets must be passed as a\n358     nested list-of-lists, with a depth equal to the length of ``concat_dims``.\n359     ``manual_combine`` will concatenate along the top-level list first.\n360 \n361     Useful for combining datasets from a set of nested directories, or for\n362     collecting the output of a simulation parallelized along multiple\n363     dimensions.\n364 \n365     Parameters\n366     ----------\n367     datasets : list or nested list of Dataset\n368         Dataset objects to combine.\n369         If concatenation or merging along more than one dimension is desired,\n370         then datasets must be supplied in a nested list-of-lists.\n371     concat_dim : str, or list of str, DataArray, Index or None\n372         Dimensions along which to concatenate variables, as used by\n373         :py:func:`xarray.concat`.\n374         Set ``concat_dim=[..., None, ...]`` explicitly to disable concatenation\n375         and merge instead along a particular dimension.\n376         The position of ``None`` in the list specifies the dimension of the\n377         nested-list input along which to merge.\n378         Must be the same length as the depth of the list passed to\n379         ``datasets``.\n380     compat : {\"identical\", \"equals\", \"broadcast_equals\", \\\n381               \"no_conflicts\", \"override\"}, optional\n382         String indicating how to compare variables of the same name for\n383         potential merge conflicts:\n384 \n385         - \"broadcast_equals\": all values must be equal when variables are\n386           broadcast against each other to ensure common dimensions.\n387         - \"equals\": all values and dimensions must be the same.\n388         - \"identical\": all values, dimensions and attributes must be the\n389           same.\n390         - \"no_conflicts\": only values which are not null in both datasets\n391           must be equal. The returned dataset then contains the combination\n392           of all non-null values.\n393         - \"override\": skip comparing and pick variable from first dataset\n394     data_vars : {\"minimal\", \"different\", \"all\" or list of str}, optional\n395         Details are in the documentation of concat\n396     coords : {\"minimal\", \"different\", \"all\" or list of str}, optional\n397         Details are in the documentation of concat\n398     fill_value : scalar or dict-like, optional\n399         Value to use for newly missing values. If a dict-like, maps\n400         variable names to fill values. Use a data array's name to\n401         refer to its values.\n402     join : {\"outer\", \"inner\", \"left\", \"right\", \"exact\"}, optional\n403         String indicating how to combine differing indexes\n404         (excluding concat_dim) in objects\n405 \n406         - \"outer\": use the union of object indexes\n407         - \"inner\": use the intersection of object indexes\n408         - \"left\": use indexes from the first object with each dimension\n409         - \"right\": use indexes from the last object with each dimension\n410         - \"exact\": instead of aligning, raise `ValueError` when indexes to be\n411           aligned are not equal\n412         - \"override\": if indexes are of same size, rewrite indexes to be\n413           those of the first object with that dimension. Indexes for the same\n414           dimension must have the same size in all objects.\n415     combine_attrs : {\"drop\", \"identical\", \"no_conflicts\", \"override\"}, \\\n416                     default: \"drop\"\n417         String indicating how to combine attrs of the objects being merged:\n418 \n419         - \"drop\": empty attrs on returned Dataset.\n420         - \"identical\": all attrs must be the same on every object.\n421         - \"no_conflicts\": attrs from all objects are combined, any that have\n422           the same name must also have the same value.\n423         - \"override\": skip comparing and copy attrs from the first dataset to\n424           the result.\n425 \n426     Returns\n427     -------\n428     combined : xarray.Dataset\n429 \n430     Examples\n431     --------\n432 \n433     A common task is collecting data from a parallelized simulation in which\n434     each process wrote out to a separate file. A domain which was decomposed\n435     into 4 parts, 2 each along both the x and y axes, requires organising the\n436     datasets into a doubly-nested list, e.g:\n437 \n438     >>> x1y1 = xr.Dataset(\n439     ...     {\n440     ...         \"temperature\": ((\"x\", \"y\"), np.random.randn(2, 2)),\n441     ...         \"precipitation\": ((\"x\", \"y\"), np.random.randn(2, 2)),\n442     ...     }\n443     ... )\n444     >>> x1y1\n445     <xarray.Dataset>\n446     Dimensions:        (x: 2, y: 2)\n447     Dimensions without coordinates: x, y\n448     Data variables:\n449         temperature    (x, y) float64 1.764 0.4002 0.9787 2.241\n450         precipitation  (x, y) float64 1.868 -0.9773 0.9501 -0.1514\n451     >>> x1y2 = xr.Dataset(\n452     ...     {\n453     ...         \"temperature\": ((\"x\", \"y\"), np.random.randn(2, 2)),\n454     ...         \"precipitation\": ((\"x\", \"y\"), np.random.randn(2, 2)),\n455     ...     }\n456     ... )\n457     >>> x2y1 = xr.Dataset(\n458     ...     {\n459     ...         \"temperature\": ((\"x\", \"y\"), np.random.randn(2, 2)),\n460     ...         \"precipitation\": ((\"x\", \"y\"), np.random.randn(2, 2)),\n461     ...     }\n462     ... )\n463     >>> x2y2 = xr.Dataset(\n464     ...     {\n465     ...         \"temperature\": ((\"x\", \"y\"), np.random.randn(2, 2)),\n466     ...         \"precipitation\": ((\"x\", \"y\"), np.random.randn(2, 2)),\n467     ...     }\n468     ... )\n469 \n470 \n471     >>> ds_grid = [[x1y1, x1y2], [x2y1, x2y2]]\n472     >>> combined = xr.combine_nested(ds_grid, concat_dim=[\"x\", \"y\"])\n473     >>> combined\n474     <xarray.Dataset>\n475     Dimensions:        (x: 4, y: 4)\n476     Dimensions without coordinates: x, y\n477     Data variables:\n478         temperature    (x, y) float64 1.764 0.4002 -0.1032 ... 0.04576 -0.1872\n479         precipitation  (x, y) float64 1.868 -0.9773 0.761 ... -0.7422 0.1549 0.3782\n480 \n481     ``manual_combine`` can also be used to explicitly merge datasets with\n482     different variables. For example if we have 4 datasets, which are divided\n483     along two times, and contain two different variables, we can pass ``None``\n484     to ``concat_dim`` to specify the dimension of the nested list over which\n485     we wish to use ``merge`` instead of ``concat``:\n486 \n487     >>> t1temp = xr.Dataset({\"temperature\": (\"t\", np.random.randn(5))})\n488     >>> t1temp\n489     <xarray.Dataset>\n490     Dimensions:      (t: 5)\n491     Dimensions without coordinates: t\n492     Data variables:\n493         temperature  (t) float64 -0.8878 -1.981 -0.3479 0.1563 1.23\n494 \n495     >>> t1precip = xr.Dataset({\"precipitation\": (\"t\", np.random.randn(5))})\n496     >>> t1precip\n497     <xarray.Dataset>\n498     Dimensions:        (t: 5)\n499     Dimensions without coordinates: t\n500     Data variables:\n501         precipitation  (t) float64 1.202 -0.3873 -0.3023 -1.049 -1.42\n502 \n503     >>> t2temp = xr.Dataset({\"temperature\": (\"t\", np.random.randn(5))})\n504     >>> t2precip = xr.Dataset({\"precipitation\": (\"t\", np.random.randn(5))})\n505 \n506 \n507     >>> ds_grid = [[t1temp, t1precip], [t2temp, t2precip]]\n508     >>> combined = xr.combine_nested(ds_grid, concat_dim=[\"t\", None])\n509     >>> combined\n510     <xarray.Dataset>\n511     Dimensions:        (t: 10)\n512     Dimensions without coordinates: t\n513     Data variables:\n514         temperature    (t) float64 -0.8878 -1.981 -0.3479 ... -0.5097 -0.4381 -1.253\n515         precipitation  (t) float64 1.202 -0.3873 -0.3023 ... -0.2127 -0.8955 0.3869\n516 \n517     See also\n518     --------\n519     concat\n520     merge\n521     \"\"\"\n522     if isinstance(concat_dim, (str, DataArray)) or concat_dim is None:\n523         concat_dim = [concat_dim]\n524 \n525     # The IDs argument tells _manual_combine that datasets aren't yet sorted\n526     return _nested_combine(\n527         datasets,\n528         concat_dims=concat_dim,\n529         compat=compat,\n530         data_vars=data_vars,\n531         coords=coords,\n532         ids=False,\n533         fill_value=fill_value,\n534         join=join,\n535         combine_attrs=combine_attrs,\n536     )\n537 \n538 \n539 def vars_as_keys(ds):\n540     return tuple(sorted(ds))\n541 \n542 \n543 def combine_by_coords(\n544     datasets,\n545     compat=\"no_conflicts\",\n546     data_vars=\"all\",\n547     coords=\"different\",\n548     fill_value=dtypes.NA,\n549     join=\"outer\",\n550     combine_attrs=\"no_conflicts\",\n551 ):\n552     \"\"\"\n553     Attempt to auto-magically combine the given datasets into one by using\n554     dimension coordinates.\n555 \n556     This method attempts to combine a group of datasets along any number of\n557     dimensions into a single entity by inspecting coords and metadata and using\n558     a combination of concat and merge.\n559 \n560     Will attempt to order the datasets such that the values in their dimension\n561     coordinates are monotonic along all dimensions. If it cannot determine the\n562     order in which to concatenate the datasets, it will raise a ValueError.\n563     Non-coordinate dimensions will be ignored, as will any coordinate\n564     dimensions which do not vary between each dataset.\n565 \n566     Aligns coordinates, but different variables on datasets can cause it\n567     to fail under some scenarios. In complex cases, you may need to clean up\n568     your data and use concat/merge explicitly (also see `manual_combine`).\n569 \n570     Works well if, for example, you have N years of data and M data variables,\n571     and each combination of a distinct time period and set of data variables is\n572     saved as its own dataset. Also useful for if you have a simulation which is\n573     parallelized in multiple dimensions, but has global coordinates saved in\n574     each file specifying the positions of points within the global domain.\n575 \n576     Parameters\n577     ----------\n578     datasets : sequence of xarray.Dataset\n579         Dataset objects to combine.\n580     compat : {\"identical\", \"equals\", \"broadcast_equals\", \"no_conflicts\", \"override\"}, optional\n581         String indicating how to compare variables of the same name for\n582         potential conflicts:\n583 \n584         - \"broadcast_equals\": all values must be equal when variables are\n585           broadcast against each other to ensure common dimensions.\n586         - \"equals\": all values and dimensions must be the same.\n587         - \"identical\": all values, dimensions and attributes must be the\n588           same.\n589         - \"no_conflicts\": only values which are not null in both datasets\n590           must be equal. The returned dataset then contains the combination\n591           of all non-null values.\n592         - \"override\": skip comparing and pick variable from first dataset\n593     data_vars : {\"minimal\", \"different\", \"all\" or list of str}, optional\n594         These data variables will be concatenated together:\n595 \n596         * \"minimal\": Only data variables in which the dimension already\n597           appears are included.\n598         * \"different\": Data variables which are not equal (ignoring\n599           attributes) across all datasets are also concatenated (as well as\n600           all for which dimension already appears). Beware: this option may\n601           load the data payload of data variables into memory if they are not\n602           already loaded.\n603         * \"all\": All data variables will be concatenated.\n604         * list of str: The listed data variables will be concatenated, in\n605           addition to the \"minimal\" data variables.\n606 \n607         If objects are DataArrays, `data_vars` must be \"all\".\n608     coords : {\"minimal\", \"different\", \"all\"} or list of str, optional\n609         As per the \"data_vars\" kwarg, but for coordinate variables.\n610     fill_value : scalar or dict-like, optional\n611         Value to use for newly missing values. If a dict-like, maps\n612         variable names to fill values. Use a data array's name to\n613         refer to its values. If None, raises a ValueError if\n614         the passed Datasets do not create a complete hypercube.\n615     join : {\"outer\", \"inner\", \"left\", \"right\", \"exact\"}, optional\n616         String indicating how to combine differing indexes\n617         (excluding concat_dim) in objects\n618 \n619         - \"outer\": use the union of object indexes\n620         - \"inner\": use the intersection of object indexes\n621         - \"left\": use indexes from the first object with each dimension\n622         - \"right\": use indexes from the last object with each dimension\n623         - \"exact\": instead of aligning, raise `ValueError` when indexes to be\n624           aligned are not equal\n625         - \"override\": if indexes are of same size, rewrite indexes to be\n626           those of the first object with that dimension. Indexes for the same\n627           dimension must have the same size in all objects.\n628     combine_attrs : {\"drop\", \"identical\", \"no_conflicts\", \"override\"}, \\\n629                     default: \"drop\"\n630         String indicating how to combine attrs of the objects being merged:\n631 \n632         - \"drop\": empty attrs on returned Dataset.\n633         - \"identical\": all attrs must be the same on every object.\n634         - \"no_conflicts\": attrs from all objects are combined, any that have\n635           the same name must also have the same value.\n636         - \"override\": skip comparing and copy attrs from the first dataset to\n637           the result.\n638 \n639     Returns\n640     -------\n641     combined : xarray.Dataset\n642 \n643     See also\n644     --------\n645     concat\n646     merge\n647     combine_nested\n648 \n649     Examples\n650     --------\n651 \n652     Combining two datasets using their common dimension coordinates. Notice\n653     they are concatenated based on the values in their dimension coordinates,\n654     not on their position in the list passed to `combine_by_coords`.\n655 \n656     >>> import numpy as np\n657     >>> import xarray as xr\n658 \n659     >>> x1 = xr.Dataset(\n660     ...     {\n661     ...         \"temperature\": ((\"y\", \"x\"), 20 * np.random.rand(6).reshape(2, 3)),\n662     ...         \"precipitation\": ((\"y\", \"x\"), np.random.rand(6).reshape(2, 3)),\n663     ...     },\n664     ...     coords={\"y\": [0, 1], \"x\": [10, 20, 30]},\n665     ... )\n666     >>> x2 = xr.Dataset(\n667     ...     {\n668     ...         \"temperature\": ((\"y\", \"x\"), 20 * np.random.rand(6).reshape(2, 3)),\n669     ...         \"precipitation\": ((\"y\", \"x\"), np.random.rand(6).reshape(2, 3)),\n670     ...     },\n671     ...     coords={\"y\": [2, 3], \"x\": [10, 20, 30]},\n672     ... )\n673     >>> x3 = xr.Dataset(\n674     ...     {\n675     ...         \"temperature\": ((\"y\", \"x\"), 20 * np.random.rand(6).reshape(2, 3)),\n676     ...         \"precipitation\": ((\"y\", \"x\"), np.random.rand(6).reshape(2, 3)),\n677     ...     },\n678     ...     coords={\"y\": [2, 3], \"x\": [40, 50, 60]},\n679     ... )\n680 \n681     >>> x1\n682     <xarray.Dataset>\n683     Dimensions:        (x: 3, y: 2)\n684     Coordinates:\n685       * y              (y) int64 0 1\n686       * x              (x) int64 10 20 30\n687     Data variables:\n688         temperature    (y, x) float64 10.98 14.3 12.06 10.9 8.473 12.92\n689         precipitation  (y, x) float64 0.4376 0.8918 0.9637 0.3834 0.7917 0.5289\n690 \n691     >>> x2\n692     <xarray.Dataset>\n693     Dimensions:        (x: 3, y: 2)\n694     Coordinates:\n695       * y              (y) int64 2 3\n696       * x              (x) int64 10 20 30\n697     Data variables:\n698         temperature    (y, x) float64 11.36 18.51 1.421 1.743 0.4044 16.65\n699         precipitation  (y, x) float64 0.7782 0.87 0.9786 0.7992 0.4615 0.7805\n700 \n701     >>> x3\n702     <xarray.Dataset>\n703     Dimensions:        (x: 3, y: 2)\n704     Coordinates:\n705       * y              (y) int64 2 3\n706       * x              (x) int64 40 50 60\n707     Data variables:\n708         temperature    (y, x) float64 2.365 12.8 2.867 18.89 10.44 8.293\n709         precipitation  (y, x) float64 0.2646 0.7742 0.4562 0.5684 0.01879 0.6176\n710 \n711     >>> xr.combine_by_coords([x2, x1])\n712     <xarray.Dataset>\n713     Dimensions:        (x: 3, y: 4)\n714     Coordinates:\n715       * y              (y) int64 0 1 2 3\n716       * x              (x) int64 10 20 30\n717     Data variables:\n718         temperature    (y, x) float64 10.98 14.3 12.06 10.9 ... 1.743 0.4044 16.65\n719         precipitation  (y, x) float64 0.4376 0.8918 0.9637 ... 0.7992 0.4615 0.7805\n720 \n721     >>> xr.combine_by_coords([x3, x1])\n722     <xarray.Dataset>\n723     Dimensions:        (x: 6, y: 4)\n724     Coordinates:\n725       * x              (x) int64 10 20 30 40 50 60\n726       * y              (y) int64 0 1 2 3\n727     Data variables:\n728         temperature    (y, x) float64 10.98 14.3 12.06 nan ... nan 18.89 10.44 8.293\n729         precipitation  (y, x) float64 0.4376 0.8918 0.9637 ... 0.5684 0.01879 0.6176\n730 \n731     >>> xr.combine_by_coords([x3, x1], join=\"override\")\n732     <xarray.Dataset>\n733     Dimensions:        (x: 3, y: 4)\n734     Coordinates:\n735       * x              (x) int64 10 20 30\n736       * y              (y) int64 0 1 2 3\n737     Data variables:\n738         temperature    (y, x) float64 10.98 14.3 12.06 10.9 ... 18.89 10.44 8.293\n739         precipitation  (y, x) float64 0.4376 0.8918 0.9637 ... 0.5684 0.01879 0.6176\n740 \n741     >>> xr.combine_by_coords([x1, x2, x3])\n742     <xarray.Dataset>\n743     Dimensions:        (x: 6, y: 4)\n744     Coordinates:\n745       * x              (x) int64 10 20 30 40 50 60\n746       * y              (y) int64 0 1 2 3\n747     Data variables:\n748         temperature    (y, x) float64 10.98 14.3 12.06 nan ... 18.89 10.44 8.293\n749         precipitation  (y, x) float64 0.4376 0.8918 0.9637 ... 0.5684 0.01879 0.6176\n750     \"\"\"\n751 \n752     # Group by data vars\n753     sorted_datasets = sorted(datasets, key=vars_as_keys)\n754     grouped_by_vars = itertools.groupby(sorted_datasets, key=vars_as_keys)\n755 \n756     # Perform the multidimensional combine on each group of data variables\n757     # before merging back together\n758     concatenated_grouped_by_data_vars = []\n759     for vars, datasets_with_same_vars in grouped_by_vars:\n760         combined_ids, concat_dims = _infer_concat_order_from_coords(\n761             list(datasets_with_same_vars)\n762         )\n763 \n764         if fill_value is None:\n765             # check that datasets form complete hypercube\n766             _check_shape_tile_ids(combined_ids)\n767         else:\n768             # check only that all datasets have same dimension depth for these\n769             # vars\n770             _check_dimension_depth_tile_ids(combined_ids)\n771 \n772         # Concatenate along all of concat_dims one by one to create single ds\n773         concatenated = _combine_nd(\n774             combined_ids,\n775             concat_dims=concat_dims,\n776             data_vars=data_vars,\n777             coords=coords,\n778             compat=compat,\n779             fill_value=fill_value,\n780             join=join,\n781             combine_attrs=combine_attrs,\n782         )\n783 \n784         # Check the overall coordinates are monotonically increasing\n785         for dim in concat_dims:\n786             indexes = concatenated.indexes.get(dim)\n787             if not (indexes.is_monotonic_increasing or indexes.is_monotonic_decreasing):\n788                 raise ValueError(\n789                     \"Resulting object does not have monotonic\"\n790                     \" global indexes along dimension {}\".format(dim)\n791                 )\n792         concatenated_grouped_by_data_vars.append(concatenated)\n793 \n794     return merge(\n795         concatenated_grouped_by_data_vars,\n796         compat=compat,\n797         fill_value=fill_value,\n798         join=join,\n799         combine_attrs=combine_attrs,\n800     )\n",
        "intended_behavior": "Ensure that any high-level functions that perform combining operations and rely on the underlying merge logic are also aware of and correctly implement the new `\"drop_conflicts\"` option for `combine_attrs`."
    },
    {
        "rel_file_path": "xarray/core/concat.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/concat.py",
        "start": 1,
        "end": 581,
        "class_name": null,
        "method_name": null,
        "code": "1 from typing import (\n2     TYPE_CHECKING,\n3     Dict,\n4     Hashable,\n5     Iterable,\n6     List,\n7     Optional,\n8     Set,\n9     Tuple,\n10     Union,\n11     overload,\n12 )\n13 \n14 import pandas as pd\n15 \n16 from . import dtypes, utils\n17 from .alignment import align\n18 from .duck_array_ops import lazy_array_equiv\n19 from .merge import _VALID_COMPAT, merge_attrs, unique_variable\n20 from .variable import IndexVariable, Variable, as_variable\n21 from .variable import concat as concat_vars\n22 \n23 if TYPE_CHECKING:\n24     from .dataarray import DataArray\n25     from .dataset import Dataset\n26 \n27 \n28 @overload\n29 def concat(\n30     objs: Iterable[\"Dataset\"],\n31     dim: Union[str, \"DataArray\", pd.Index],\n32     data_vars: Union[str, List[str]] = \"all\",\n33     coords: Union[str, List[str]] = \"different\",\n34     compat: str = \"equals\",\n35     positions: Optional[Iterable[int]] = None,\n36     fill_value: object = dtypes.NA,\n37     join: str = \"outer\",\n38     combine_attrs: str = \"override\",\n39 ) -> \"Dataset\":\n40     ...\n41 \n42 \n43 @overload\n44 def concat(\n45     objs: Iterable[\"DataArray\"],\n46     dim: Union[str, \"DataArray\", pd.Index],\n47     data_vars: Union[str, List[str]] = \"all\",\n48     coords: Union[str, List[str]] = \"different\",\n49     compat: str = \"equals\",\n50     positions: Optional[Iterable[int]] = None,\n51     fill_value: object = dtypes.NA,\n52     join: str = \"outer\",\n53     combine_attrs: str = \"override\",\n54 ) -> \"DataArray\":\n55     ...\n56 \n57 \n58 def concat(\n59     objs,\n60     dim,\n61     data_vars=\"all\",\n62     coords=\"different\",\n63     compat=\"equals\",\n64     positions=None,\n65     fill_value=dtypes.NA,\n66     join=\"outer\",\n67     combine_attrs=\"override\",\n68 ):\n69     \"\"\"Concatenate xarray objects along a new or existing dimension.\n70 \n71     Parameters\n72     ----------\n73     objs : sequence of Dataset and DataArray\n74         xarray objects to concatenate together. Each object is expected to\n75         consist of variables and coordinates with matching shapes except for\n76         along the concatenated dimension.\n77     dim : str or DataArray or pandas.Index\n78         Name of the dimension to concatenate along. This can either be a new\n79         dimension name, in which case it is added along axis=0, or an existing\n80         dimension name, in which case the location of the dimension is\n81         unchanged. If dimension is provided as a DataArray or Index, its name\n82         is used as the dimension to concatenate along and the values are added\n83         as a coordinate.\n84     data_vars : {\"minimal\", \"different\", \"all\"} or list of str, optional\n85         These data variables will be concatenated together:\n86           * \"minimal\": Only data variables in which the dimension already\n87             appears are included.\n88           * \"different\": Data variables which are not equal (ignoring\n89             attributes) across all datasets are also concatenated (as well as\n90             all for which dimension already appears). Beware: this option may\n91             load the data payload of data variables into memory if they are not\n92             already loaded.\n93           * \"all\": All data variables will be concatenated.\n94           * list of str: The listed data variables will be concatenated, in\n95             addition to the \"minimal\" data variables.\n96 \n97         If objects are DataArrays, data_vars must be \"all\".\n98     coords : {\"minimal\", \"different\", \"all\"} or list of str, optional\n99         These coordinate variables will be concatenated together:\n100           * \"minimal\": Only coordinates in which the dimension already appears\n101             are included.\n102           * \"different\": Coordinates which are not equal (ignoring attributes)\n103             across all datasets are also concatenated (as well as all for which\n104             dimension already appears). Beware: this option may load the data\n105             payload of coordinate variables into memory if they are not already\n106             loaded.\n107           * \"all\": All coordinate variables will be concatenated, except\n108             those corresponding to other dimensions.\n109           * list of str: The listed coordinate variables will be concatenated,\n110             in addition to the \"minimal\" coordinates.\n111     compat : {\"identical\", \"equals\", \"broadcast_equals\", \"no_conflicts\", \"override\"}, optional\n112         String indicating how to compare non-concatenated variables of the same name for\n113         potential conflicts. This is passed down to merge.\n114 \n115         - \"broadcast_equals\": all values must be equal when variables are\n116           broadcast against each other to ensure common dimensions.\n117         - \"equals\": all values and dimensions must be the same.\n118         - \"identical\": all values, dimensions and attributes must be the\n119           same.\n120         - \"no_conflicts\": only values which are not null in both datasets\n121           must be equal. The returned dataset then contains the combination\n122           of all non-null values.\n123         - \"override\": skip comparing and pick variable from first dataset\n124     positions : None or list of integer arrays, optional\n125         List of integer arrays which specifies the integer positions to which\n126         to assign each dataset along the concatenated dimension. If not\n127         supplied, objects are concatenated in the provided order.\n128     fill_value : scalar or dict-like, optional\n129         Value to use for newly missing values. If a dict-like, maps\n130         variable names to fill values. Use a data array's name to\n131         refer to its values.\n132     join : {\"outer\", \"inner\", \"left\", \"right\", \"exact\"}, optional\n133         String indicating how to combine differing indexes\n134         (excluding dim) in objects\n135 \n136         - \"outer\": use the union of object indexes\n137         - \"inner\": use the intersection of object indexes\n138         - \"left\": use indexes from the first object with each dimension\n139         - \"right\": use indexes from the last object with each dimension\n140         - \"exact\": instead of aligning, raise `ValueError` when indexes to be\n141           aligned are not equal\n142         - \"override\": if indexes are of same size, rewrite indexes to be\n143           those of the first object with that dimension. Indexes for the same\n144           dimension must have the same size in all objects.\n145     combine_attrs : {\"drop\", \"identical\", \"no_conflicts\", \"override\"}, \\\n146                     default: \"override\"\n147         String indicating how to combine attrs of the objects being merged:\n148 \n149         - \"drop\": empty attrs on returned Dataset.\n150         - \"identical\": all attrs must be the same on every object.\n151         - \"no_conflicts\": attrs from all objects are combined, any that have\n152           the same name must also have the same value.\n153         - \"override\": skip comparing and copy attrs from the first dataset to\n154           the result.\n155 \n156     Returns\n157     -------\n158     concatenated : type of objs\n159 \n160     See also\n161     --------\n162     merge\n163 \n164     Examples\n165     --------\n166     >>> da = xr.DataArray(\n167     ...     np.arange(6).reshape(2, 3), [(\"x\", [\"a\", \"b\"]), (\"y\", [10, 20, 30])]\n168     ... )\n169     >>> da\n170     <xarray.DataArray (x: 2, y: 3)>\n171     array([[0, 1, 2],\n172            [3, 4, 5]])\n173     Coordinates:\n174       * x        (x) <U1 'a' 'b'\n175       * y        (y) int64 10 20 30\n176 \n177     >>> xr.concat([da.isel(y=slice(0, 1)), da.isel(y=slice(1, None))], dim=\"y\")\n178     <xarray.DataArray (x: 2, y: 3)>\n179     array([[0, 1, 2],\n180            [3, 4, 5]])\n181     Coordinates:\n182       * x        (x) <U1 'a' 'b'\n183       * y        (y) int64 10 20 30\n184 \n185     >>> xr.concat([da.isel(x=0), da.isel(x=1)], \"x\")\n186     <xarray.DataArray (x: 2, y: 3)>\n187     array([[0, 1, 2],\n188            [3, 4, 5]])\n189     Coordinates:\n190       * x        (x) <U1 'a' 'b'\n191       * y        (y) int64 10 20 30\n192 \n193     >>> xr.concat([da.isel(x=0), da.isel(x=1)], \"new_dim\")\n194     <xarray.DataArray (new_dim: 2, y: 3)>\n195     array([[0, 1, 2],\n196            [3, 4, 5]])\n197     Coordinates:\n198         x        (new_dim) <U1 'a' 'b'\n199       * y        (y) int64 10 20 30\n200     Dimensions without coordinates: new_dim\n201 \n202     >>> xr.concat([da.isel(x=0), da.isel(x=1)], pd.Index([-90, -100], name=\"new_dim\"))\n203     <xarray.DataArray (new_dim: 2, y: 3)>\n204     array([[0, 1, 2],\n205            [3, 4, 5]])\n206     Coordinates:\n207         x        (new_dim) <U1 'a' 'b'\n208       * y        (y) int64 10 20 30\n209       * new_dim  (new_dim) int64 -90 -100\n210     \"\"\"\n211     # TODO: add ignore_index arguments copied from pandas.concat\n212     # TODO: support concatenating scalar coordinates even if the concatenated\n213     # dimension already exists\n214     from .dataarray import DataArray\n215     from .dataset import Dataset\n216 \n217     try:\n218         first_obj, objs = utils.peek_at(objs)\n219     except StopIteration:\n220         raise ValueError(\"must supply at least one object to concatenate\")\n221 \n222     if compat not in _VALID_COMPAT:\n223         raise ValueError(\n224             \"compat=%r invalid: must be 'broadcast_equals', 'equals', 'identical', 'no_conflicts' or 'override'\"\n225             % compat\n226         )\n227 \n228     if isinstance(first_obj, DataArray):\n229         f = _dataarray_concat\n230     elif isinstance(first_obj, Dataset):\n231         f = _dataset_concat\n232     else:\n233         raise TypeError(\n234             \"can only concatenate xarray Dataset and DataArray \"\n235             \"objects, got %s\" % type(first_obj)\n236         )\n237     return f(\n238         objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs\n239     )\n240 \n241 \n242 def _calc_concat_dim_coord(dim):\n243     \"\"\"\n244     Infer the dimension name and 1d coordinate variable (if appropriate)\n245     for concatenating along the new dimension.\n246     \"\"\"\n247     from .dataarray import DataArray\n248 \n249     if isinstance(dim, str):\n250         coord = None\n251     elif not isinstance(dim, (DataArray, Variable)):\n252         dim_name = getattr(dim, \"name\", None)\n253         if dim_name is None:\n254             dim_name = \"concat_dim\"\n255         coord = IndexVariable(dim_name, dim)\n256         dim = dim_name\n257     elif not isinstance(dim, DataArray):\n258         coord = as_variable(dim).to_index_variable()\n259         (dim,) = coord.dims\n260     else:\n261         coord = dim\n262         (dim,) = coord.dims\n263     return dim, coord\n264 \n265 \n266 def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n267     \"\"\"\n268     Determine which dataset variables need to be concatenated in the result,\n269     \"\"\"\n270     # Return values\n271     concat_over = set()\n272     equals = {}\n273 \n274     if dim in dim_names:\n275         concat_over_existing_dim = True\n276         concat_over.add(dim)\n277     else:\n278         concat_over_existing_dim = False\n279 \n280     concat_dim_lengths = []\n281     for ds in datasets:\n282         if concat_over_existing_dim:\n283             if dim not in ds.dims:\n284                 if dim in ds:\n285                     ds = ds.set_coords(dim)\n286         concat_over.update(k for k, v in ds.variables.items() if dim in v.dims)\n287         concat_dim_lengths.append(ds.dims.get(dim, 1))\n288 \n289     def process_subset_opt(opt, subset):\n290         if isinstance(opt, str):\n291             if opt == \"different\":\n292                 if compat == \"override\":\n293                     raise ValueError(\n294                         \"Cannot specify both %s='different' and compat='override'.\"\n295                         % subset\n296                     )\n297                 # all nonindexes that are not the same in each dataset\n298                 for k in getattr(datasets[0], subset):\n299                     if k not in concat_over:\n300                         equals[k] = None\n301 \n302                         variables = []\n303                         for ds in datasets:\n304                             if k in ds.variables:\n305                                 variables.append(ds.variables[k])\n306 \n307                         if len(variables) == 1:\n308                             # coords=\"different\" doesn't make sense when only one object\n309                             # contains a particular variable.\n310                             break\n311                         elif len(variables) != len(datasets) and opt == \"different\":\n312                             raise ValueError(\n313                                 f\"{k!r} not present in all datasets and coords='different'. \"\n314                                 f\"Either add {k!r} to datasets where it is missing or \"\n315                                 \"specify coords='minimal'.\"\n316                             )\n317 \n318                         # first check without comparing values i.e. no computes\n319                         for var in variables[1:]:\n320                             equals[k] = getattr(variables[0], compat)(\n321                                 var, equiv=lazy_array_equiv\n322                             )\n323                             if equals[k] is not True:\n324                                 # exit early if we know these are not equal or that\n325                                 # equality cannot be determined i.e. one or all of\n326                                 # the variables wraps a numpy array\n327                                 break\n328 \n329                         if equals[k] is False:\n330                             concat_over.add(k)\n331 \n332                         elif equals[k] is None:\n333                             # Compare the variable of all datasets vs. the one\n334                             # of the first dataset. Perform the minimum amount of\n335                             # loads in order to avoid multiple loads from disk\n336                             # while keeping the RAM footprint low.\n337                             v_lhs = datasets[0].variables[k].load()\n338                             # We'll need to know later on if variables are equal.\n339                             computed = []\n340                             for ds_rhs in datasets[1:]:\n341                                 v_rhs = ds_rhs.variables[k].compute()\n342                                 computed.append(v_rhs)\n343                                 if not getattr(v_lhs, compat)(v_rhs):\n344                                     concat_over.add(k)\n345                                     equals[k] = False\n346                                     # computed variables are not to be re-computed\n347                                     # again in the future\n348                                     for ds, v in zip(datasets[1:], computed):\n349                                         ds.variables[k].data = v.data\n350                                     break\n351                             else:\n352                                 equals[k] = True\n353 \n354             elif opt == \"all\":\n355                 concat_over.update(\n356                     set(getattr(datasets[0], subset)) - set(datasets[0].dims)\n357                 )\n358             elif opt == \"minimal\":\n359                 pass\n360             else:\n361                 raise ValueError(f\"unexpected value for {subset}: {opt}\")\n362         else:\n363             invalid_vars = [k for k in opt if k not in getattr(datasets[0], subset)]\n364             if invalid_vars:\n365                 if subset == \"coords\":\n366                     raise ValueError(\n367                         \"some variables in coords are not coordinates on \"\n368                         \"the first dataset: %s\" % (invalid_vars,)\n369                     )\n370                 else:\n371                     raise ValueError(\n372                         \"some variables in data_vars are not data variables \"\n373                         \"on the first dataset: %s\" % (invalid_vars,)\n374                     )\n375             concat_over.update(opt)\n376 \n377     process_subset_opt(data_vars, \"data_vars\")\n378     process_subset_opt(coords, \"coords\")\n379     return concat_over, equals, concat_dim_lengths\n380 \n381 \n382 # determine dimensional coordinate names and a dict mapping name to DataArray\n383 def _parse_datasets(\n384     datasets: Iterable[\"Dataset\"],\n385 ) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, int], Set[Hashable], Set[Hashable]]:\n386 \n387     dims: Set[Hashable] = set()\n388     all_coord_names: Set[Hashable] = set()\n389     data_vars: Set[Hashable] = set()  # list of data_vars\n390     dim_coords: Dict[Hashable, Variable] = {}  # maps dim name to variable\n391     dims_sizes: Dict[Hashable, int] = {}  # shared dimension sizes to expand variables\n392 \n393     for ds in datasets:\n394         dims_sizes.update(ds.dims)\n395         all_coord_names.update(ds.coords)\n396         data_vars.update(ds.data_vars)\n397 \n398         # preserves ordering of dimensions\n399         for dim in ds.dims:\n400             if dim in dims:\n401                 continue\n402 \n403             if dim not in dim_coords:\n404                 dim_coords[dim] = ds.coords[dim].variable\n405         dims = dims | set(ds.dims)\n406 \n407     return dim_coords, dims_sizes, all_coord_names, data_vars\n408 \n409 \n410 def _dataset_concat(\n411     datasets: List[\"Dataset\"],\n412     dim: Union[str, \"DataArray\", pd.Index],\n413     data_vars: Union[str, List[str]],\n414     coords: Union[str, List[str]],\n415     compat: str,\n416     positions: Optional[Iterable[int]],\n417     fill_value: object = dtypes.NA,\n418     join: str = \"outer\",\n419     combine_attrs: str = \"override\",\n420 ) -> \"Dataset\":\n421     \"\"\"\n422     Concatenate a sequence of datasets along a new or existing dimension\n423     \"\"\"\n424     from .dataset import Dataset\n425 \n426     dim, coord = _calc_concat_dim_coord(dim)\n427     # Make sure we're working on a copy (we'll be loading variables)\n428     datasets = [ds.copy() for ds in datasets]\n429     datasets = list(\n430         align(*datasets, join=join, copy=False, exclude=[dim], fill_value=fill_value)\n431     )\n432 \n433     dim_coords, dims_sizes, coord_names, data_names = _parse_datasets(datasets)\n434     dim_names = set(dim_coords)\n435     unlabeled_dims = dim_names - coord_names\n436 \n437     both_data_and_coords = coord_names & data_names\n438     if both_data_and_coords:\n439         raise ValueError(\n440             \"%r is a coordinate in some datasets but not others.\" % both_data_and_coords\n441         )\n442     # we don't want the concat dimension in the result dataset yet\n443     dim_coords.pop(dim, None)\n444     dims_sizes.pop(dim, None)\n445 \n446     # case where concat dimension is a coordinate or data_var but not a dimension\n447     if (dim in coord_names or dim in data_names) and dim not in dim_names:\n448         datasets = [ds.expand_dims(dim) for ds in datasets]\n449 \n450     # determine which variables to concatentate\n451     concat_over, equals, concat_dim_lengths = _calc_concat_over(\n452         datasets, dim, dim_names, data_vars, coords, compat\n453     )\n454 \n455     # determine which variables to merge, and then merge them according to compat\n456     variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n457 \n458     result_vars = {}\n459     if variables_to_merge:\n460         to_merge: Dict[Hashable, List[Variable]] = {\n461             var: [] for var in variables_to_merge\n462         }\n463 \n464         for ds in datasets:\n465             for var in variables_to_merge:\n466                 if var in ds:\n467                     to_merge[var].append(ds.variables[var])\n468 \n469         for var in variables_to_merge:\n470             result_vars[var] = unique_variable(\n471                 var, to_merge[var], compat=compat, equals=equals.get(var, None)\n472             )\n473     else:\n474         result_vars = {}\n475     result_vars.update(dim_coords)\n476 \n477     # assign attrs and encoding from first dataset\n478     result_attrs = merge_attrs([ds.attrs for ds in datasets], combine_attrs)\n479     result_encoding = datasets[0].encoding\n480 \n481     # check that global attributes are fixed across all datasets if necessary\n482     for ds in datasets[1:]:\n483         if compat == \"identical\" and not utils.dict_equiv(ds.attrs, result_attrs):\n484             raise ValueError(\"Dataset global attributes not equal.\")\n485 \n486     # we've already verified everything is consistent; now, calculate\n487     # shared dimension sizes so we can expand the necessary variables\n488     def ensure_common_dims(vars):\n489         # ensure each variable with the given name shares the same\n490         # dimensions and the same shape for all of them except along the\n491         # concat dimension\n492         common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n493         if dim not in common_dims:\n494             common_dims = (dim,) + common_dims\n495         for var, dim_len in zip(vars, concat_dim_lengths):\n496             if var.dims != common_dims:\n497                 common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n498                 var = var.set_dims(common_dims, common_shape)\n499             yield var\n500 \n501     # stack up each variable to fill-out the dataset (in order)\n502     # n.b. this loop preserves variable order, needed for groupby.\n503     for k in datasets[0].variables:\n504         if k in concat_over:\n505             try:\n506                 vars = ensure_common_dims([ds[k].variable for ds in datasets])\n507             except KeyError:\n508                 raise ValueError(\"%r is not present in all datasets.\" % k)\n509             combined = concat_vars(vars, dim, positions)\n510             assert isinstance(combined, Variable)\n511             result_vars[k] = combined\n512         elif k in result_vars:\n513             # preserves original variable order\n514             result_vars[k] = result_vars.pop(k)\n515 \n516     result = Dataset(result_vars, attrs=result_attrs)\n517     absent_coord_names = coord_names - set(result.variables)\n518     if absent_coord_names:\n519         raise ValueError(\n520             \"Variables %r are coordinates in some datasets but not others.\"\n521             % absent_coord_names\n522         )\n523     result = result.set_coords(coord_names)\n524     result.encoding = result_encoding\n525 \n526     result = result.drop_vars(unlabeled_dims, errors=\"ignore\")\n527 \n528     if coord is not None:\n529         # add concat dimension last to ensure that its in the final Dataset\n530         result[coord.name] = coord\n531 \n532     return result\n533 \n534 \n535 def _dataarray_concat(\n536     arrays: Iterable[\"DataArray\"],\n537     dim: Union[str, \"DataArray\", pd.Index],\n538     data_vars: Union[str, List[str]],\n539     coords: Union[str, List[str]],\n540     compat: str,\n541     positions: Optional[Iterable[int]],\n542     fill_value: object = dtypes.NA,\n543     join: str = \"outer\",\n544     combine_attrs: str = \"override\",\n545 ) -> \"DataArray\":\n546     arrays = list(arrays)\n547 \n548     if data_vars != \"all\":\n549         raise ValueError(\n550             \"data_vars is not a valid argument when concatenating DataArray objects\"\n551         )\n552 \n553     datasets = []\n554     for n, arr in enumerate(arrays):\n555         if n == 0:\n556             name = arr.name\n557         elif name != arr.name:\n558             if compat == \"identical\":\n559                 raise ValueError(\"array names not identical\")\n560             else:\n561                 arr = arr.rename(name)\n562         datasets.append(arr._to_temp_dataset())\n563 \n564     ds = _dataset_concat(\n565         datasets,\n566         dim,\n567         data_vars,\n568         coords,\n569         compat,\n570         positions,\n571         fill_value=fill_value,\n572         join=join,\n573         combine_attrs=\"drop\",\n574     )\n575 \n576     merged_attrs = merge_attrs([da.attrs for da in arrays], combine_attrs)\n577 \n578     result = arrays[0]._from_temp_dataset(ds, name)\n579     result.attrs = merged_attrs\n580 \n581     return result\n",
        "intended_behavior": "Ensure that concatenation operations that depend on attribute merging respect the new `\"drop_conflicts\"` option in `combine_attrs`."
    }
]