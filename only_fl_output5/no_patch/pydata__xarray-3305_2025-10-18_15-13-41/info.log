2025-10-18 15:13:41.502 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 15:13:43.032 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=682, output_tokens=12, cost=0.007180
2025-10-18 15:13:52.124 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=862, output_tokens=271, cost=0.016750
2025-10-18 15:13:53.445 | INFO     | registered non reproducing test 0
2025-10-18 15:14:03.495 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1767, output_tokens=440, cost=0.030870
2025-10-18 15:14:04.760 | INFO     | registered non reproducing test 1
2025-10-18 15:14:15.994 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1933, output_tokens=446, cost=0.032710
2025-10-18 15:14:17.272 | INFO     | registered non reproducing test 2
2025-10-18 15:14:17.272 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-18 15:14:17.279 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 15:14:21.283 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=922, output_tokens=171, cost=0.014350
2025-10-18 15:14:21.289 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 15:14:24.076 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=525, output_tokens=43, cost=0.006540
2025-10-18 15:14:24.078 | DEBUG    | Extracted a valid json.
2025-10-18 15:14:24.078 | DEBUG    | Agent proxy return the following json: {
    "API_calls": ["search_class(\"DataArray\")", "search_method_in_class(\"quantile\", \"DataArray\")", "search_code(\"keep_attrs\")"],
    "bug_locations": []
}
2025-10-18 15:14:24.203 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 15:14:24.203 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 15:14:38.224 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=5503, output_tokens=581, cost=0.072460
2025-10-18 15:14:38.237 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 15:14:45.909 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=6300, output_tokens=300, cost=0.072000
2025-10-18 15:14:45.917 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 15:14:53.520 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=654, output_tokens=296, cost=0.015420
2025-10-18 15:14:53.521 | DEBUG    | Extracted a valid json.
2025-10-18 15:14:53.521 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "xarray/core/dataarray.py",
            "class": "DataArray",
            "method": "quantile",
            "intended_behavior": "The method should ensure that when `keep_attrs=True`, the attributes from the original `DataArray` are preserved in the quantile calculation result. The intended fix involves verifying that the return statement `self._from_temp_dataset(ds)` properly re-applies the original attributes to the resulting `DataArray`. If the method delegates attribute handling to `_to_temp_dataset().quantile`, it must ensure the `keep_attrs` parameter effectively preserves attributes throughout the process. A detailed review or patch may involve ensuring `_from_temp_dataset(ds)` or similar subsequent calls respect the `keep_attrs` flag and correctly apply the original attributes to the final returned `DataArray`. This ensures the functionality remains consistent with user expectations and the documented behavior of the `keep_attrs` flag. Understanding the flow of attributes in both `_to_temp_dataset()` and `_from_temp_dataset()` methods is crucial for implementing a bug fix that prevents attribute loss during quantile calculations. The fix requires ensuring that attributes from the original `DataArray` are either: Passed through to temporary representations and computations explicitly. Re-applied from the original `DataArray` to the new `DataArray` instance resulting from quantile computations when `keep_attrs=True`."
        }
    ]
}
2025-10-18 15:14:53.543 | DEBUG    | Bug location extracted successfully: [<file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>quantile</method>
<code>
2827     def quantile(
2828         self,
2829         q: Any,
2830         dim: Union[Hashable, Sequence[Hashable], None] = None,
2831         interpolation: str = "linear",
2832         keep_attrs: bool = None,
2833     ) -> "DataArray":
2834         """Compute the qth quantile of the data along the specified dimension.
2835 
2836         Returns the qth quantiles(s) of the array elements.
2837 
2838         Parameters
2839         ----------
2840         q : float in range of [0,1] or array-like of floats
2841             Quantile to compute, which must be between 0 and 1 inclusive.
2842         dim : hashable or sequence of hashable, optional
2843             Dimension(s) over which to apply quantile.
2844         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
2845             This optional parameter specifies the interpolation method to
2846             use when the desired quantile lies between two data points
2847             ``i < j``:
2848 
2849                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is
2850                   the fractional part of the index surrounded by ``i`` and
2851                   ``j``.
2852                 - lower: ``i``.
2853                 - higher: ``j``.
2854                 - nearest: ``i`` or ``j``, whichever is nearest.
2855                 - midpoint: ``(i + j) / 2``.
2856         keep_attrs : bool, optional
2857             If True, the dataset's attributes (`attrs`) will be copied from
2858             the original object to the new one.  If False (default), the new
2859             object will be returned without attributes.
2860 
2861         Returns
2862         -------
2863         quantiles : DataArray
2864             If `q` is a single quantile, then the result
2865             is a scalar. If multiple percentiles are given, first axis of
2866             the result corresponds to the quantile and a quantile dimension
2867             is added to the return array. The other dimensions are the
2868              dimensions that remain after the reduction of the array.
2869 
2870         See Also
2871         --------
2872         numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile
2873         """
2874 
2875         ds = self._to_temp_dataset().quantile(
2876             q, dim=dim, keep_attrs=keep_attrs, interpolation=interpolation
2877         )
2878         return self._from_temp_dataset(ds)

</code><intended_behavior>The method should ensure that when `keep_attrs=True`, the attributes from the original `DataArray` are preserved in the quantile calculation result. The intended fix involves verifying that the return statement `self._from_temp_dataset(ds)` properly re-applies the original attributes to the resulting `DataArray`. If the method delegates attribute handling to `_to_temp_dataset().quantile`, it must ensure the `keep_attrs` parameter effectively preserves attributes throughout the process. A detailed review or patch may involve ensuring `_from_temp_dataset(ds)` or similar subsequent calls respect the `keep_attrs` flag and correctly apply the original attributes to the final returned `DataArray`. This ensures the functionality remains consistent with user expectations and the documented behavior of the `keep_attrs` flag. Understanding the flow of attributes in both `_to_temp_dataset()` and `_from_temp_dataset()` methods is crucial for implementing a bug fix that prevents attribute loss during quantile calculations. The fix requires ensuring that attributes from the original `DataArray` are either: Passed through to temporary representations and computations explicitly. Re-applied from the original `DataArray` to the new `DataArray` instance resulting from quantile computations when `keep_attrs=True`.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
212 class DataArray(AbstractArray, DataWithCoords):
213     """N-dimensional array with labeled coordinates and dimensions.
214 
215     DataArray provides a wrapper around numpy ndarrays that uses labeled
216     dimensions and coordinates to support metadata aware operations. The API is
217     similar to that for the pandas Series or DataFrame, but DataArray objects
218     can have any number of dimensions, and their contents have fixed data
219     types.
220 
221     Additional features over raw numpy arrays:
222 
223     - Apply operations over dimensions by name: ``x.sum('time')``.
224     - Select or assign values by integer location (like numpy): ``x[:10]``
225       or by label (like pandas): ``x.loc['2014-01-01']`` or
226       ``x.sel(time='2014-01-01')``.
227     - Mathematical operations (e.g., ``x - y``) vectorize across multiple
228       dimensions (known in numpy as "broadcasting") based on dimension names,
229       regardless of their original order.
230     - Keep track of arbitrary metadata in the form of a Python dictionary:
231       ``x.attrs``
232     - Convert to a pandas Series: ``x.to_series()``.
233 
234     Getting items from or doing mathematical operations with a DataArray
235     always returns another DataArray.
236 
237     Attributes
238     ----------
239     dims : tuple
240         Dimension names associated with this array.
241     values : np.ndarray
242         Access or modify DataArray values as a numpy array.
243     coords : dict-like
244         Dictionary of DataArray objects that label values along each dimension.
245     name : str or None
246         Name of this array.
247     attrs : OrderedDict
248         Dictionary for holding arbitrary metadata.
249     """
250 
251     __slots__ = ("_accessors", "_coords", "_file_obj", "_name", "_indexes", "_variable")
252 
253     _groupby_cls = groupby.DataArrayGroupBy
254     _rolling_cls = rolling.DataArrayRolling
255     _coarsen_cls = rolling.DataArrayCoarsen
256     _resample_cls = resample.DataArrayResample
257 
258     __default = ReprObject("<default>")
259 
260     dt = property(DatetimeAccessor)
261 
262     def __init__(
263         self,
264         data: Any = dtypes.NA,
265         coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None] = None,
266         dims: Union[Hashable, Sequence[Hashable], None] = None,
267         name: Hashable = None,
268         attrs: Mapping = None,
269         # deprecated parameters
270         encoding=None,
271         # internal parameters
272         indexes=None,
273         fastpath: bool = False,
274     ):
275         """
276         Parameters
277         ----------
278         data : array_like
279             Values for this array. Must be an ``numpy.ndarray``, ndarray like,
280             or castable to an ``ndarray``. If a self-described xarray or pandas
281             object, attempts are made to use this array's metadata to fill in
282             other unspecified arguments. A view of the array's data is used
283             instead of a copy if possible.
284         coords : sequence or dict of array_like objects, optional
285             Coordinates (tick labels) to use for indexing along each dimension.
286             The following notations are accepted:
287 
288             - mapping {dimension name: array-like}
289             - sequence of tuples that are valid arguments for xarray.Variable()
290               - (dims, data)
291               - (dims, data, attrs)
292               - (dims, data, attrs, encoding)
293 
294             Additionally, it is possible to define a coord whose name
295             does not match the dimension name, or a coord based on multiple
296             dimensions, with one of the following notations:
297 
298             - mapping {coord name: DataArray}
299             - mapping {coord name: Variable}
300             - mapping {coord name: (dimension name, array-like)}
301             - mapping {coord name: (tuple of dimension names, array-like)}
302 
303         dims : hashable or sequence of hashable, optional
304             Name(s) of the data dimension(s). Must be either a hashable (only
305             for 1D data) or a sequence of hashables with length equal to the
306             number of dimensions. If this argument is omitted, dimension names
307             are taken from ``coords`` (if possible) and otherwise default to
308             ``['dim_0', ... 'dim_n']``.
309         name : str or None, optional
310             Name of this array.
311         attrs : dict_like or None, optional
312             Attributes to assign to the new instance. By default, an empty
313             attribute dictionary is initialized.
314         """
315         if encoding is not None:
316             warnings.warn(
317                 "The `encoding` argument to `DataArray` is deprecated, and . "
318                 "will be removed in 0.14. "
319                 "Instead, specify the encoding when writing to disk or "
320                 "set the `encoding` attribute directly.",
321                 FutureWarning,
322                 stacklevel=2,
323             )
324         if fastpath:
325             variable = data
326             assert dims is None
327             assert attrs is None
328             assert encoding is None
329         else:
330             # try to fill in arguments from data if they weren't supplied
331             if coords is None:
332 
333                 if isinstance(data, DataArray):
334                     coords = data.coords
335                 elif isinstance(data, pd.Series):
336                     coords = [data.index]
337                 elif isinstance(data, pd.DataFrame):
338                     coords = [data.index, data.columns]
339                 elif isinstance(data, (pd.Index, IndexVariable)):
340                     coords = [data]
341                 elif isinstance(data, pdcompat.Panel):
342                     coords = [data.items, data.major_axis, data.minor_axis]
343 
344             if dims is None:
345                 dims = getattr(data, "dims", getattr(coords, "dims", None))
346             if name is None:
347                 name = getattr(data, "name", None)
348             if attrs is None:
349                 attrs = getattr(data, "attrs", None)
350             if encoding is None:
351                 encoding = getattr(data, "encoding", None)
352 
353             data = _check_data_shape(data, coords, dims)
354             data = as_compatible_data(data)
355             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
356             variable = Variable(dims, data, attrs, encoding, fastpath=True)
357 
358         # These fully describe a DataArray
359         self._variable = variable  # type: Variable
360         assert isinstance(coords, OrderedDict)
361         self._coords = coords  # type: OrderedDict[Any, Variable]
362         self._name = name  # type: Optional[Hashable]
363         self._accessors = None  # type: Optional[Dict[str, Any]]
364 
365         # TODO(shoyer): document this argument, once it becomes part of the
366         # public interface.
367         self._indexes = indexes
368 
369         self._file_obj = None
370 
371     def _replace(
372         self,
373         variable: Variable = None,
374         coords=None,
375         name: Optional[Hashable] = __default,
376     ) -> "DataArray":
377         if variable is None:
378             variable = self.variable
379         if coords is None:
380             coords = self._coords
381         if name is self.__default:
382             name = self.name
383         return type(self)(variable, coords, name=name, fastpath=True)
384 
385     def _replace_maybe_drop_dims(
386         self, variable: Variable, name: Optional[Hashable] = __default
387     ) -> "DataArray":
388         if variable.dims == self.dims and variable.shape == self.shape:
389             coords = self._coords.copy()
390         elif variable.dims == self.dims:
391             # Shape has changed (e.g. from reduce(..., keepdims=True)
392             new_sizes = dict(zip(self.dims, variable.shape))
393             coords = OrderedDict(
394                 (k, v)
395                 for k, v in self._coords.items()
396                 if v.shape == tuple(new_sizes[d] for d in v.dims)
397             )
398         else:
399             allowed_dims = set(variable.dims)
400             coords = OrderedDict(
401                 (k, v) for k, v in self._coords.items() if set(v.dims) <= allowed_dims
402             )
403         return self._replace(variable, coords, name)
404 
405     def _overwrite_indexes(self, indexes: Mapping[Hashable, Any]) -> "DataArray":
406         if not len(indexes):
407             return self
408         coords = self._coords.copy()
409         for name, idx in indexes.items():
410             coords[name] = IndexVariable(name, idx)
411         obj = self._replace(coords=coords)
412 
413         # switch from dimension to level names, if necessary
414         dim_names = {}  # type: Dict[Any, str]
415         for dim, idx in indexes.items():
416             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:
417                 dim_names[dim] = idx.name
418         if dim_names:
419             obj = obj.rename(dim_names)
420         return obj
421 
422     def _to_temp_dataset(self) -> Dataset:
423         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
424 
425     def _from_temp_dataset(
426         self, dataset: Dataset, name: Hashable = __default
427     ) -> "DataArray":
428         variable = dataset._variables.pop(_THIS_ARRAY)
429         coords = dataset._variables
430         return self._replace(variable, coords, name)
431 
432     def _to_dataset_split(self, dim: Hashable) -> Dataset:
433         def subset(dim, label):
434             array = self.loc[{dim: label}]
435             if dim in array.coords:
436                 del array.coords[dim]
437             array.attrs = {}
438             return array
439 
440         variables = OrderedDict(
441             [(label, subset(dim, label)) for label in self.get_index(dim)]
442         )
443         coords = self.coords.to_dataset()
444         if dim in coords:
445             del coords[dim]
446         return Dataset(variables, coords, self.attrs)
447 
448     def _to_dataset_whole(
449         self, name: Hashable = None, shallow_copy: bool = True
450     ) -> Dataset:
451         if name is None:
452             name = self.name
453         if name is None:
454             raise ValueError(
455                 "unable to convert unnamed DataArray to a "
456                 "Dataset without providing an explicit name"
457             )
458         if name in self.coords:
459             raise ValueError(
460                 "cannot create a Dataset from a DataArray with "
461                 "the same name as one of its coordinates"
462             )
463         # use private APIs for speed: this is called by _to_temp_dataset(),
464         # which is used in the guts of a lot of operations (e.g., reindex)
465         variables = self._coords.copy()
466         variables[name] = self.variable
467         if shallow_copy:
468             for k in variables:
469                 variables[k] = variables[k].copy(deep=False)
470         coord_names = set(self._coords)
471         dataset = Dataset._from_vars_and_coord_names(variables, coord_names)
472         return dataset
473 
474     def to_dataset(self, dim: Hashable = None, *, name: Hashable = None) -> Dataset:
475         """Convert a DataArray to a Dataset.
476 
477         Parameters
478         ----------
479         dim : hashable, optional
480             Name of the dimension on this array along which to split this array
481             into separate variables. If not provided, this array is converted
482             into a Dataset of one variable.
483         name : hashable, optional
484             Name to substitute for this array's name. Only valid if ``dim`` is
485             not provided.
486 
487         Returns
488         -------
489         dataset : Dataset
490         """
491         if dim is not None and dim not in self.dims:
492             raise TypeError(
493                 "{} is not a dim. If supplying a ``name``, pass as a kwarg.".format(dim)
494             )
495 
496         if dim is not None:
497             if name is not None:
498                 raise TypeError("cannot supply both dim and name arguments")
499             return self._to_dataset_split(dim)
500         else:
501             return self._to_dataset_whole(name)
502 
503     @property
504     def name(self) -> Optional[Hashable]:
505         """The name of this array.
506         """
507         return self._name
508 
509     @name.setter
510     def name(self, value: Optional[Hashable]) -> None:
511         self._name = value
512 
513     @property
514     def variable(self) -> Variable:
515         """Low level interface to the Variable object for this DataArray."""
516         return self._variable
517 
518     @property
519     def dtype(self) -> np.dtype:
520         return self.variable.dtype
521 
522     @property
523     def shape(self) -> Tuple[int, ...]:
524         return self.variable.shape
525 
526     @property
527     def size(self) -> int:
528         return self.variable.size
529 
530     @property
531     def nbytes(self) -> int:
532         return self.variable.nbytes
533 
534     @property
535     def ndim(self) -> int:
536         return self.variable.ndim
537 
538     def __len__(self) -> int:
539         return len(self.variable)
540 
541     @property
542     def data(self) -> Any:
543         """The array's data as a dask or numpy array
544         """
545         return self.variable.data
546 
547     @data.setter
548     def data(self, value: Any) -> None:
549         self.variable.data = value
550 
551     @property
552     def values(self) -> np.ndarray:
553         """The array's data as a numpy.ndarray"""
554         return self.variable.values
555 
556     @values.setter
557     def values(self, value: Any) -> None:
558         self.variable.values = value
559 
560     @property
561     def _in_memory(self) -> bool:
562         return self.variable._in_memory
563 
564     def to_index(self) -> pd.Index:
565         """Convert this variable to a pandas.Index. Only possible for 1D
566         arrays.
567         """
568         return self.variable.to_index()
569 
570     @property
571     def dims(self) -> Tuple[Hashable, ...]:
572         """Tuple of dimension names associated with this array.
573 
574         Note that the type of this property is inconsistent with
575         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
576         consistently named properties.
577         """
578         return self.variable.dims
579 
580     @dims.setter
581     def dims(self, value):
582         raise AttributeError(
583             "you cannot assign dims on a DataArray. Use "
584             ".rename() or .swap_dims() instead."
585         )
586 
587     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
588         if utils.is_dict_like(key):
589             return key
590         else:
591             key = indexing.expanded_indexer(key, self.ndim)
592             return dict(zip(self.dims, key))
593 
594     @property
595     def _level_coords(self) -> "OrderedDict[Any, Hashable]":
596         """Return a mapping of all MultiIndex levels and their corresponding
597         coordinate name.
598         """
599         level_coords = OrderedDict()  # type: OrderedDict[Any, Hashable]
600 
601         for cname, var in self._coords.items():
602             if var.ndim == 1 and isinstance(var, IndexVariable):
603                 level_names = var.level_names
604                 if level_names is not None:
605                     dim, = var.dims
606                     level_coords.update({lname: dim for lname in level_names})
607         return level_coords
608 
609     def _getitem_coord(self, key):
610         from .dataset import _get_virtual_variable
611 
612         try:
613             var = self._coords[key]
614         except KeyError:
615             dim_sizes = dict(zip(self.dims, self.shape))
616             _, key, var = _get_virtual_variable(
617                 self._coords, key, self._level_coords, dim_sizes
618             )
619 
620         return self._replace_maybe_drop_dims(var, name=key)
621 
622     def __getitem__(self, key: Any) -> "DataArray":
623         if isinstance(key, str):
624             return self._getitem_coord(key)
625         else:
626             # xarray-style array indexing
627             return self.isel(indexers=self._item_key_to_dict(key))
628 
629     def __setitem__(self, key: Any, value: Any) -> None:
630         if isinstance(key, str):
631             self.coords[key] = value
632         else:
633             # Coordinates in key, value and self[key] should be consistent.
634             # TODO Coordinate consistency in key is checked here, but it
635             # causes unnecessary indexing. It should be optimized.
636             obj = self[key]
637             if isinstance(value, DataArray):
638                 assert_coordinate_consistent(value, obj.coords.variables)
639             # DataArray key -> Variable key
640             key = {
641                 k: v.variable if isinstance(v, DataArray) else v
642                 for k, v in self._item_key_to_dict(key).items()
643             }
644             self.variable[key] = value
645 
646     def __delitem__(self, key: Any) -> None:
647         del self.coords[key]
648 
649     @property
650     def _attr_sources(self) -> List[Mapping[Hashable, Any]]:
651         """List of places to look-up items for attribute-style access
652         """
653         return self._item_sources + [self.attrs]
654 
655     @property
656     def _item_sources(self) -> List[Mapping[Hashable, Any]]:
657         """List of places to look-up items for key-completion
658         """
659         return [
660             self.coords,
661             {d: self.coords[d] for d in self.dims},
662             LevelCoordinatesSource(self),
663         ]
664 
665     def __contains__(self, key: Any) -> bool:
666         return key in self.data
667 
668     @property
669     def loc(self) -> _LocIndexer:
670         """Attribute for location based indexing like pandas.
671         """
672         return _LocIndexer(self)
673 
674     @property
675     def attrs(self) -> "OrderedDict[Any, Any]":
676         """Dictionary storing arbitrary metadata with this array."""
677         return self.variable.attrs
678 
679     @attrs.setter
680     def attrs(self, value: Mapping[Hashable, Any]) -> None:
681         # Disable type checking to work around mypy bug - see mypy#4167
682         self.variable.attrs = value  # type: ignore
683 
684     @property
685     def encoding(self) -> "OrderedDict[Any, Any]":
686         """Dictionary of format-specific settings for how this array should be
687         serialized."""
688         return self.variable.encoding
689 
690     @encoding.setter
691     def encoding(self, value: Mapping[Hashable, Any]) -> None:
692         self.variable.encoding = value
693 
694     @property
695     def indexes(self) -> Indexes:
696         """Mapping of pandas.Index objects used for label based indexing
697         """
698         if self._indexes is None:
699             self._indexes = default_indexes(self._coords, self.dims)
700         return Indexes(self._indexes)
701 
702     @property
703     def coords(self) -> DataArrayCoordinates:
704         """Dictionary-like container of coordinate arrays.
705         """
706         return DataArrayCoordinates(self)
707 
708     def reset_coords(
709         self,
710         names: Union[Iterable[Hashable], Hashable, None] = None,
711         drop: bool = False,
712         inplace: bool = None,
713     ) -> Union[None, "DataArray", Dataset]:
714         """Given names of coordinates, reset them to become variables.
715 
716         Parameters
717         ----------
718         names : hashable or iterable of hashables, optional
719             Name(s) of non-index coordinates in this dataset to reset into
720             variables. By default, all non-index coordinates are reset.
721         drop : bool, optional
722             If True, remove coordinates instead of converting them into
723             variables.
724 
725         Returns
726         -------
727         Dataset, or DataArray if ``drop == True``
728         """
729         _check_inplace(inplace)
730         if names is None:
731             names = set(self.coords) - set(self.dims)
732         dataset = self.coords.to_dataset().reset_coords(names, drop)
733         if drop:
734             return self._replace(coords=dataset._variables)
735         else:
736             if self.name is None:
737                 raise ValueError(
738                     "cannot reset_coords with drop=False on an unnamed DataArrray"
739                 )
740             dataset[self.name] = self.variable
741             return dataset
742 
743     def __dask_graph__(self):
744         return self._to_temp_dataset().__dask_graph__()
745 
746     def __dask_keys__(self):
747         return self._to_temp_dataset().__dask_keys__()
748 
749     def __dask_layers__(self):
750         return self._to_temp_dataset().__dask_layers__()
751 
752     @property
753     def __dask_optimize__(self):
754         return self._to_temp_dataset().__dask_optimize__
755 
756     @property
757     def __dask_scheduler__(self):
758         return self._to_temp_dataset().__dask_scheduler__
759 
760     def __dask_postcompute__(self):
761         func, args = self._to_temp_dataset().__dask_postcompute__()
762         return self._dask_finalize, (func, args, self.name)
763 
764     def __dask_postpersist__(self):
765         func, args = self._to_temp_dataset().__dask_postpersist__()
766         return self._dask_finalize, (func, args, self.name)
767 
768     @staticmethod
769     def _dask_finalize(results, func, args, name):
770         ds = func(results, *args)
771         variable = ds._variables.pop(_THIS_ARRAY)
772         coords = ds._variables
773         return DataArray(variable, coords, name=name, fastpath=True)
774 
775     def load(self, **kwargs) -> "DataArray":
776         """Manually trigger loading of this array's data from disk or a
777         remote source into memory and return this array.
778 
779         Normally, it should not be necessary to call this method in user code,
780         because all xarray functions should either work on deferred data or
781         load data automatically. However, this method can be necessary when
782         working with many file objects on disk.
783 
784         Parameters
785         ----------
786         **kwargs : dict
787             Additional keyword arguments passed on to ``dask.array.compute``.
788 
789         See Also
790         --------
791         dask.array.compute
792         """
793         ds = self._to_temp_dataset().load(**kwargs)
794         new = self._from_temp_dataset(ds)
795         self._variable = new._variable
796         self._coords = new._coords
797         return self
798 
799     def compute(self, **kwargs) -> "DataArray":
800         """Manually trigger loading of this array's data from disk or a
801         remote source into memory and return a new array. The original is
802         left unaltered.
803 
804         Normally, it should not be necessary to call this method in user code,
805         because all xarray functions should either work on deferred data or
806         load data automatically. However, this method can be necessary when
807         working with many file objects on disk.
808 
809         Parameters
810         ----------
811         **kwargs : dict
812             Additional keyword arguments passed on to ``dask.array.compute``.
813 
814         See Also
815         --------
816         dask.array.compute
817         """
818         new = self.copy(deep=False)
819         return new.load(**kwargs)
820 
821     def persist(self, **kwargs) -> "DataArray":
822         """ Trigger computation in constituent dask arrays
823 
824         This keeps them as dask arrays but encourages them to keep data in
825         memory.  This is particularly useful when on a distributed machine.
826         When on a single machine consider using ``.compute()`` instead.
827 
828         Parameters
829         ----------
830         **kwargs : dict
831             Additional keyword arguments passed on to ``dask.persist``.
832 
833         See Also
834         --------
835         dask.persist
836         """
837         ds = self._to_temp_dataset().persist(**kwargs)
838         return self._from_temp_dataset(ds)
839 
840     def copy(self, deep: bool = True, data: Any = None) -> "DataArray":
841         """Returns a copy of this array.
842 
843         If `deep=True`, a deep copy is made of the data array.
844         Otherwise, a shallow copy is made, so each variable in the new
845         array's dataset is also a variable in this array's dataset.
846 
847         Use `data` to create a new object with the same structure as
848         original but entirely new data.
849 
850         Parameters
851         ----------
852         deep : bool, optional
853             Whether the data array and its coordinates are loaded into memory
854             and copied onto the new object. Default is True.
855         data : array_like, optional
856             Data to use in the new object. Must have same shape as original.
857             When `data` is used, `deep` is ignored for all data variables,
858             and only used for coords.
859 
860         Returns
861         -------
862         object : DataArray
863             New object with dimensions, attributes, coordinates, name,
864             encoding, and optionally data copied from original.
865 
866         Examples
867         --------
868 
869         Shallow versus deep copy
870 
871         >>> array = xr.DataArray([1, 2, 3], dims='x',
872         ...                      coords={'x': ['a', 'b', 'c']})
873         >>> array.copy()
874         <xarray.DataArray (x: 3)>
875         array([1, 2, 3])
876         Coordinates:
877         * x        (x) <U1 'a' 'b' 'c'
878         >>> array_0 = array.copy(deep=False)
879         >>> array_0[0] = 7
880         >>> array_0
881         <xarray.DataArray (x: 3)>
882         array([7, 2, 3])
883         Coordinates:
884         * x        (x) <U1 'a' 'b' 'c'
885         >>> array
886         <xarray.DataArray (x: 3)>
887         array([7, 2, 3])
888         Coordinates:
889         * x        (x) <U1 'a' 'b' 'c'
890 
891         Changing the data using the ``data`` argument maintains the
892         structure of the original object, but with the new data. Original
893         object is unaffected.
894 
895         >>> array.copy(data=[0.1, 0.2, 0.3])
896         <xarray.DataArray (x: 3)>
897         array([ 0.1,  0.2,  0.3])
898         Coordinates:
899         * x        (x) <U1 'a' 'b' 'c'
900         >>> array
901         <xarray.DataArray (x: 3)>
902         array([1, 2, 3])
903         Coordinates:
904         * x        (x) <U1 'a' 'b' 'c'
905 
906         See also
907         --------
908         pandas.DataFrame.copy
909         """
910         variable = self.variable.copy(deep=deep, data=data)
911         coords = OrderedDict((k, v.copy(deep=deep)) for k, v in self._coords.items())
912         return self._replace(variable, coords)
913 
914     def __copy__(self) -> "DataArray":
915         return self.copy(deep=False)
916 
917     def __deepcopy__(self, memo=None) -> "DataArray":
918         # memo does nothing but is required for compatibility with
919         # copy.deepcopy
920         return self.copy(deep=True)
921 
922     # mutable objects should not be hashable
923     # https://github.com/python/mypy/issues/4266
924     __hash__ = None  # type: ignore
925 
926     @property
927     def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:
928         """Block dimensions for this array's data or None if it's not a dask
929         array.
930         """
931         return self.variable.chunks
932 
933     def chunk(
934         self,
935         chunks: Union[
936             None,
937             Number,
938             Tuple[Number, ...],
939             Tuple[Tuple[Number, ...], ...],
940             Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]],
941         ] = None,
942         name_prefix: str = "xarray-",
943         token: str = None,
944         lock: bool = False,
945     ) -> "DataArray":
946         """Coerce this array's data into a dask arrays with the given chunks.
947 
948         If this variable is a non-dask array, it will be converted to dask
949         array. If it's a dask array, it will be rechunked to the given chunk
950         sizes.
951 
952         If neither chunks is not provided for one or more dimensions, chunk
953         sizes along that dimension will not be updated; non-dask arrays will be
954         converted into dask arrays with a single block.
955 
956         Parameters
957         ----------
958         chunks : int, tuple or mapping, optional
959             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
960             ``{'x': 5, 'y': 5}``.
961         name_prefix : str, optional
962             Prefix for the name of the new dask array.
963         token : str, optional
964             Token uniquely identifying this array.
965         lock : optional
966             Passed on to :py:func:`dask.array.from_array`, if the array is not
967             already as dask array.
968 
969         Returns
970         -------
971         chunked : xarray.DataArray
972         """
973         if isinstance(chunks, (tuple, list)):
974             chunks = dict(zip(self.dims, chunks))
975 
976         ds = self._to_temp_dataset().chunk(
977             chunks, name_prefix=name_prefix, token=token, lock=lock
978         )
979         return self._from_temp_dataset(ds)
980 
981     def isel(
982         self,
983         indexers: Mapping[Hashable, Any] = None,
984         drop: bool = False,
985         **indexers_kwargs: Any
986     ) -> "DataArray":
987         """Return a new DataArray whose data is given by integer indexing
988         along the specified dimension(s).
989 
990         See Also
991         --------
992         Dataset.isel
993         DataArray.sel
994         """
995         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
996         ds = self._to_temp_dataset().isel(drop=drop, indexers=indexers)
997         return self._from_temp_dataset(ds)
998 
999     def sel(
1000         self,
1001         indexers: Mapping[Hashable, Any] = None,
1002         method: str = None,
1003         tolerance=None,
1004         drop: bool = False,
1005         **indexers_kwargs: Any
1006     ) -> "DataArray":
1007         """Return a new DataArray whose data is given by selecting index
1008         labels along the specified dimension(s).
1009 
1010         .. warning::
1011 
1012           Do not try to assign values when using any of the indexing methods
1013           ``isel`` or ``sel``::
1014 
1015             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1016             # DO NOT do this
1017             da.isel(x=[0, 1, 2])[1] = -1
1018 
1019           Assigning values with the chained indexing using ``.sel`` or
1020           ``.isel`` fails silently.
1021 
1022         See Also
1023         --------
1024         Dataset.sel
1025         DataArray.isel
1026 
1027         """
1028         ds = self._to_temp_dataset().sel(
1029             indexers=indexers,
1030             drop=drop,
1031             method=method,
1032             tolerance=tolerance,
1033             **indexers_kwargs
1034         )
1035         return self._from_temp_dataset(ds)
1036 
1037     def head(
1038         self, indexers: Mapping[Hashable, Any] = None, **indexers_kwargs: Any
1039     ) -> "DataArray":
1040         """Return a new DataArray whose data is given by the the first `n`
1041         values along the specified dimension(s).
1042 
1043         See Also
1044         --------
1045         Dataset.head
1046         DataArray.tail
1047         DataArray.thin
1048         """
1049 
1050         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "head")
1051         ds = self._to_temp_dataset().head(indexers=indexers)
1052         return self._from_temp_dataset(ds)
1053 
1054     def tail(
1055         self, indexers: Mapping[Hashable, Any] = None, **indexers_kwargs: Any
1056     ) -> "DataArray":
1057         """Return a new DataArray whose data is given by the the last `n`
1058         values along the specified dimension(s).
1059 
1060         See Also
1061         --------
1062         Dataset.tail
1063         DataArray.head
1064         DataArray.thin
1065         """
1066         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "tail")
1067         ds = self._to_temp_dataset().tail(indexers=indexers)
1068         return self._from_temp_dataset(ds)
1069 
1070     def thin(
1071         self, indexers: Mapping[Hashable, Any] = None, **indexers_kwargs: Any
1072     ) -> "DataArray":
1073         """Return a new DataArray whose data is given by each `n` value
1074         along the specified dimension(s).
1075 
1076         See Also
1077         --------
1078         Dataset.thin
1079         DataArray.head
1080         DataArray.tail
1081         """
1082         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "thin")
1083         ds = self._to_temp_dataset().thin(indexers=indexers)
1084         return self._from_temp_dataset(ds)
1085 
1086     def broadcast_like(
1087         self, other: Union["DataArray", Dataset], exclude: Iterable[Hashable] = None
1088     ) -> "DataArray":
1089         """Broadcast this DataArray against another Dataset or DataArray.
1090 
1091         This is equivalent to xr.broadcast(other, self)[1]
1092 
1093         xarray objects are broadcast against each other in arithmetic
1094         operations, so this method is not be necessary for most uses.
1095 
1096         If no change is needed, the input data is returned to the output
1097         without being copied.
1098 
1099         If new coords are added by the broadcast, their values are
1100         NaN filled.
1101 
1102         Parameters
1103         ----------
1104         other : Dataset or DataArray
1105             Object against which to broadcast this array.
1106         exclude : iterable of hashable, optional
1107             Dimensions that must not be broadcasted
1108 
1109         Returns
1110         -------
1111         new_da: xr.DataArray
1112 
1113         Examples
1114         --------
1115 
1116         >>> arr1
1117         <xarray.DataArray (x: 2, y: 3)>
1118         array([[0.840235, 0.215216, 0.77917 ],
1119                [0.726351, 0.543824, 0.875115]])
1120         Coordinates:
1121           * x        (x) <U1 'a' 'b'
1122           * y        (y) <U1 'a' 'b' 'c'
1123         >>> arr2
1124         <xarray.DataArray (x: 3, y: 2)>
1125         array([[0.612611, 0.125753],
1126                [0.853181, 0.948818],
1127                [0.180885, 0.33363 ]])
1128         Coordinates:
1129           * x        (x) <U1 'a' 'b' 'c'
1130           * y        (y) <U1 'a' 'b'
1131         >>> arr1.broadcast_like(arr2)
1132         <xarray.DataArray (x: 3, y: 3)>
1133         array([[0.840235, 0.215216, 0.77917 ],
1134                [0.726351, 0.543824, 0.875115],
1135                [     nan,      nan,      nan]])
1136         Coordinates:
1137           * x        (x) object 'a' 'b' 'c'
1138           * y        (y) object 'a' 'b' 'c'
1139         """
1140         if exclude is None:
1141             exclude = set()
1142         else:
1143             exclude = set(exclude)
1144         args = align(other, self, join="outer", copy=False, exclude=exclude)
1145 
1146         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1147 
1148         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
1149 
1150     def reindex_like(
1151         self,
1152         other: Union["DataArray", Dataset],
1153         method: str = None,
1154         tolerance=None,
1155         copy: bool = True,
1156         fill_value=dtypes.NA,
1157     ) -> "DataArray":
1158         """Conform this object onto the indexes of another object, filling in
1159         missing values with ``fill_value``. The default fill value is NaN.
1160 
1161         Parameters
1162         ----------
1163         other : Dataset or DataArray
1164             Object with an 'indexes' attribute giving a mapping from dimension
1165             names to pandas.Index objects, which provides coordinates upon
1166             which to index the variables in this dataset. The indexes on this
1167             other object need not be the same as the indexes on this
1168             dataset. Any mis-matched index values will be filled in with
1169             NaN, and any mis-matched dimension names will simply be ignored.
1170         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1171             Method to use for filling index values from other not found on this
1172             data array:
1173 
1174             * None (default): don't fill gaps
1175             * pad / ffill: propagate last valid index value forward
1176             * backfill / bfill: propagate next valid index value backward
1177             * nearest: use nearest valid index value (requires pandas>=0.16)
1178         tolerance : optional
1179             Maximum distance between original and new labels for inexact
1180             matches. The values of the index at the matching locations must
1181             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1182             Requires pandas>=0.17.
1183         copy : bool, optional
1184             If ``copy=True``, data in the return value is always copied. If
1185             ``copy=False`` and reindexing is unnecessary, or can be performed
1186             with only slice operations, then the output may share memory with
1187             the input. In either case, a new xarray object is always returned.
1188         fill_value : scalar, optional
1189             Value to use for newly missing values
1190 
1191         Returns
1192         -------
1193         reindexed : DataArray
1194             Another dataset array, with this array's data but coordinates from
1195             the other object.
1196 
1197         See Also
1198         --------
1199         DataArray.reindex
1200         align
1201         """
1202         indexers = reindex_like_indexers(self, other)
1203         return self.reindex(
1204             indexers=indexers,
1205             method=method,
1206             tolerance=tolerance,
1207             copy=copy,
1208             fill_value=fill_value,
1209         )
1210 
1211     def reindex(
1212         self,
1213         indexers: Mapping[Hashable, Any] = None,
1214         method: str = None,
1215         tolerance=None,
1216         copy: bool = True,
1217         fill_value=dtypes.NA,
1218         **indexers_kwargs: Any
1219     ) -> "DataArray":
1220         """Conform this object onto the indexes of another object, filling in
1221         missing values with ``fill_value``. The default fill value is NaN.
1222 
1223         Parameters
1224         ----------
1225         indexers : dict, optional
1226             Dictionary with keys given by dimension names and values given by
1227             arrays of coordinates tick labels. Any mis-matched coordinate
1228             values will be filled in with NaN, and any mis-matched dimension
1229             names will simply be ignored.
1230             One of indexers or indexers_kwargs must be provided.
1231         copy : bool, optional
1232             If ``copy=True``, data in the return value is always copied. If
1233             ``copy=False`` and reindexing is unnecessary, or can be performed
1234             with only slice operations, then the output may share memory with
1235             the input. In either case, a new xarray object is always returned.
1236         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1237             Method to use for filling index values in ``indexers`` not found on
1238             this data array:
1239 
1240             * None (default): don't fill gaps
1241             * pad / ffill: propagate last valid index value forward
1242             * backfill / bfill: propagate next valid index value backward
1243             * nearest: use nearest valid index value (requires pandas>=0.16)
1244         tolerance : optional
1245             Maximum distance between original and new labels for inexact
1246             matches. The values of the index at the matching locations must
1247             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1248         fill_value : scalar, optional
1249             Value to use for newly missing values
1250         **indexers_kwarg : {dim: indexer, ...}, optional
1251             The keyword arguments form of ``indexers``.
1252             One of indexers or indexers_kwargs must be provided.
1253 
1254         Returns
1255         -------
1256         reindexed : DataArray
1257             Another dataset array, with this array's data but replaced
1258             coordinates.
1259 
1260         See Also
1261         --------
1262         DataArray.reindex_like
1263         align
1264         """
1265         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
1266         ds = self._to_temp_dataset().reindex(
1267             indexers=indexers,
1268             method=method,
1269             tolerance=tolerance,
1270             copy=copy,
1271             fill_value=fill_value,
1272         )
1273         return self._from_temp_dataset(ds)
1274 
1275     def interp(
1276         self,
1277         coords: Mapping[Hashable, Any] = None,
1278         method: str = "linear",
1279         assume_sorted: bool = False,
1280         kwargs: Mapping[str, Any] = None,
1281         **coords_kwargs: Any
1282     ) -> "DataArray":
1283         """ Multidimensional interpolation of variables.
1284 
1285         coords : dict, optional
1286             Mapping from dimension names to the new coordinates.
1287             new coordinate can be an scalar, array-like or DataArray.
1288             If DataArrays are passed as new coordates, their dimensions are
1289             used for the broadcasting.
1290         method: {'linear', 'nearest'} for multidimensional array,
1291             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
1292             for 1-dimensional array.
1293         assume_sorted: boolean, optional
1294             If False, values of x can be in any order and they are sorted
1295             first. If True, x has to be an array of monotonically increasing
1296             values.
1297         kwargs: dictionary
1298             Additional keyword passed to scipy's interpolator.
1299         **coords_kwarg : {dim: coordinate, ...}, optional
1300             The keyword arguments form of ``coords``.
1301             One of coords or coords_kwargs must be provided.
1302 
1303         Returns
1304         -------
1305         interpolated: xr.DataArray
1306             New dataarray on the new coordinates.
1307 
1308         Notes
1309         -----
1310         scipy is required.
1311 
1312         See Also
1313         --------
1314         scipy.interpolate.interp1d
1315         scipy.interpolate.interpn
1316 
1317         Examples
1318         --------
1319         >>> da = xr.DataArray([1, 3], [('x', np.arange(2))])
1320         >>> da.interp(x=0.5)
1321         <xarray.DataArray ()>
1322         array(2.0)
1323         Coordinates:
1324             x        float64 0.5
1325         """
1326         if self.dtype.kind not in "uifc":
1327             raise TypeError(
1328                 "interp only works for a numeric type array. "
1329                 "Given {}.".format(self.dtype)
1330             )
1331         ds = self._to_temp_dataset().interp(
1332             coords,
1333             method=method,
1334             kwargs=kwargs,
1335             assume_sorted=assume_sorted,
1336             **coords_kwargs
1337         )
1338         return self._from_temp_dataset(ds)
1339 
1340     def interp_like(
1341         self,
1342         other: Union["DataArray", Dataset],
1343         method: str = "linear",
1344         assume_sorted: bool = False,
1345         kwargs: Mapping[str, Any] = None,
1346     ) -> "DataArray":
1347         """Interpolate this object onto the coordinates of another object,
1348         filling out of range values with NaN.
1349 
1350         Parameters
1351         ----------
1352         other : Dataset or DataArray
1353             Object with an 'indexes' attribute giving a mapping from dimension
1354             names to an 1d array-like, which provides coordinates upon
1355             which to index the variables in this dataset.
1356         method: string, optional.
1357             {'linear', 'nearest'} for multidimensional array,
1358             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
1359             for 1-dimensional array. 'linear' is used by default.
1360         assume_sorted: boolean, optional
1361             If False, values of coordinates that are interpolated over can be
1362             in any order and they are sorted first. If True, interpolated
1363             coordinates are assumed to be an array of monotonically increasing
1364             values.
1365         kwargs: dictionary, optional
1366             Additional keyword passed to scipy's interpolator.
1367 
1368         Returns
1369         -------
1370         interpolated: xr.DataArray
1371             Another dataarray by interpolating this dataarray's data along the
1372             coordinates of the other object.
1373 
1374         Notes
1375         -----
1376         scipy is required.
1377         If the dataarray has object-type coordinates, reindex is used for these
1378         coordinates instead of the interpolation.
1379 
1380         See Also
1381         --------
1382         DataArray.interp
1383         DataArray.reindex_like
1384         """
1385         if self.dtype.kind not in "uifc":
1386             raise TypeError(
1387                 "interp only works for a numeric type array. "
1388                 "Given {}.".format(self.dtype)
1389             )
1390         ds = self._to_temp_dataset().interp_like(
1391             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
1392         )
1393         return self._from_temp_dataset(ds)
1394 
1395     def rename(
1396         self,
1397         new_name_or_name_dict: Union[Hashable, Mapping[Hashable, Hashable]] = None,
1398         **names: Hashable
1399     ) -> "DataArray":
1400         """Returns a new DataArray with renamed coordinates or a new name.
1401 
1402         Parameters
1403         ----------
1404         new_name_or_name_dict : str or dict-like, optional
1405             If the argument is dict-like, it used as a mapping from old
1406             names to new names for coordinates. Otherwise, use the argument
1407             as the new name for this array.
1408         **names: hashable, optional
1409             The keyword arguments form of a mapping from old names to
1410             new names for coordinates.
1411             One of new_name_or_name_dict or names must be provided.
1412 
1413         Returns
1414         -------
1415         renamed : DataArray
1416             Renamed array or array with renamed coordinates.
1417 
1418         See Also
1419         --------
1420         Dataset.rename
1421         DataArray.swap_dims
1422         """
1423         if names or utils.is_dict_like(new_name_or_name_dict):
1424             new_name_or_name_dict = cast(
1425                 Mapping[Hashable, Hashable], new_name_or_name_dict
1426             )
1427             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
1428             dataset = self._to_temp_dataset().rename(name_dict)
1429             return self._from_temp_dataset(dataset)
1430         else:
1431             new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)
1432             return self._replace(name=new_name_or_name_dict)
1433 
1434     def swap_dims(self, dims_dict: Mapping[Hashable, Hashable]) -> "DataArray":
1435         """Returns a new DataArray with swapped dimensions.
1436 
1437         Parameters
1438         ----------
1439         dims_dict : dict-like
1440             Dictionary whose keys are current dimension names and whose values
1441             are new names. Each value must already be a coordinate on this
1442             array.
1443 
1444         Returns
1445         -------
1446         renamed : Dataset
1447             DataArray with swapped dimensions.
1448 
1449         See Also
1450         --------
1451 
1452         DataArray.rename
1453         Dataset.swap_dims
1454         """
1455         ds = self._to_temp_dataset().swap_dims(dims_dict)
1456         return self._from_temp_dataset(ds)
1457 
1458     def expand_dims(
1459         self,
1460         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,
1461         axis=None,
1462         **dim_kwargs: Any
1463     ) -> "DataArray":
1464         """Return a new object with an additional axis (or axes) inserted at
1465         the corresponding position in the array shape. The new object is a
1466         view into the underlying array, not a copy.
1467 
1468 
1469         If dim is already a scalar coordinate, it will be promoted to a 1D
1470         coordinate consisting of a single value.
1471 
1472         Parameters
1473         ----------
1474         dim : hashable, sequence of hashable, dict, or None
1475             Dimensions to include on the new variable.
1476             If provided as str or sequence of str, then dimensions are inserted
1477             with length 1. If provided as a dict, then the keys are the new
1478             dimensions and the values are either integers (giving the length of
1479             the new dimensions) or sequence/ndarray (giving the coordinates of
1480             the new dimensions). **WARNING** for python 3.5, if ``dim`` is
1481             dict-like, then it must be an ``OrderedDict``. This is to ensure
1482             that the order in which the dims are given is maintained.
1483         axis : integer, list (or tuple) of integers, or None
1484             Axis position(s) where new axis is to be inserted (position(s) on
1485             the result array). If a list (or tuple) of integers is passed,
1486             multiple axes are inserted. In this case, dim arguments should be
1487             same length list. If axis=None is passed, all the axes will be
1488             inserted to the start of the result array.
1489         **dim_kwargs : int or sequence/ndarray
1490             The keywords are arbitrary dimensions being inserted and the values
1491             are either the lengths of the new dims (if int is given), or their
1492             coordinates. Note, this is an alternative to passing a dict to the
1493             dim kwarg and will only be used if dim is None. **WARNING** for
1494             python 3.5 ``dim_kwargs`` is not available.
1495 
1496         Returns
1497         -------
1498         expanded : same type as caller
1499             This object, but with an additional dimension(s).
1500         """
1501         if isinstance(dim, int):
1502             raise TypeError("dim should be hashable or sequence/mapping of hashables")
1503         elif isinstance(dim, Sequence) and not isinstance(dim, str):
1504             if len(dim) != len(set(dim)):
1505                 raise ValueError("dims should not contain duplicate values.")
1506             dim = OrderedDict((d, 1) for d in dim)
1507         elif dim is not None and not isinstance(dim, Mapping):
1508             dim = OrderedDict(((cast(Hashable, dim), 1),))
1509 
1510         # TODO: get rid of the below code block when python 3.5 is no longer
1511         #   supported.
1512         python36_plus = sys.version_info[0] == 3 and sys.version_info[1] > 5
1513         not_ordereddict = dim is not None and not isinstance(dim, OrderedDict)
1514         if not python36_plus and not_ordereddict:
1515             raise TypeError("dim must be an OrderedDict for python <3.6")
1516         elif not python36_plus and dim_kwargs:
1517             raise ValueError("dim_kwargs isn't available for python <3.6")
1518         dim_kwargs = OrderedDict(dim_kwargs)
1519 
1520         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
1521         ds = self._to_temp_dataset().expand_dims(dim, axis)
1522         return self._from_temp_dataset(ds)
1523 
1524     def set_index(
1525         self,
1526         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
1527         append: bool = False,
1528         inplace: bool = None,
1529         **indexes_kwargs: Union[Hashable, Sequence[Hashable]]
1530     ) -> Optional["DataArray"]:
1531         """Set DataArray (multi-)indexes using one or more existing
1532         coordinates.
1533 
1534         Parameters
1535         ----------
1536         indexes : {dim: index, ...}
1537             Mapping from names matching dimensions and values given
1538             by (lists of) the names of existing coordinates or variables to set
1539             as new (multi-)index.
1540         append : bool, optional
1541             If True, append the supplied index(es) to the existing index(es).
1542             Otherwise replace the existing index(es) (default).
1543         **indexes_kwargs: optional
1544             The keyword arguments form of ``indexes``.
1545             One of indexes or indexes_kwargs must be provided.
1546 
1547         Returns
1548         -------
1549         obj : DataArray
1550             Another DataArray, with this data but replaced coordinates.
1551 
1552         Example
1553         -------
1554         >>> arr = xr.DataArray(data=np.ones((2, 3)),
1555         ...                    dims=['x', 'y'],
1556         ...                    coords={'x':
1557         ...                        range(2), 'y':
1558         ...                        range(3), 'a': ('x', [3, 4])
1559         ...                    })
1560         >>> arr
1561         <xarray.DataArray (x: 2, y: 3)>
1562         array([[1., 1., 1.],
1563                [1., 1., 1.]])
1564         Coordinates:
1565           * x        (x) int64 0 1
1566           * y        (y) int64 0 1 2
1567             a        (x) int64 3 4
1568         >>> arr.set_index(x='a')
1569         <xarray.DataArray (x: 2, y: 3)>
1570         array([[1., 1., 1.],
1571                [1., 1., 1.]])
1572         Coordinates:
1573           * x        (x) int64 3 4
1574           * y        (y) int64 0 1 2
1575 
1576         See Also
1577         --------
1578         DataArray.reset_index
1579         """
1580         _check_inplace(inplace)
1581         indexes = either_dict_or_kwargs(indexes, indexes_kwargs, "set_index")
1582         coords, _ = merge_indexes(indexes, self._coords, set(), append=append)
1583         return self._replace(coords=coords)
1584 
1585     def reset_index(
1586         self,
1587         dims_or_levels: Union[Hashable, Sequence[Hashable]],
1588         drop: bool = False,
1589         inplace: bool = None,
1590     ) -> Optional["DataArray"]:
1591         """Reset the specified index(es) or multi-index level(s).
1592 
1593         Parameters
1594         ----------
1595         dims_or_levels : hashable or sequence of hashables
1596             Name(s) of the dimension(s) and/or multi-index level(s) that will
1597             be reset.
1598         drop : bool, optional
1599             If True, remove the specified indexes and/or multi-index levels
1600             instead of extracting them as new coordinates (default: False).
1601 
1602         Returns
1603         -------
1604         obj : DataArray
1605             Another dataarray, with this dataarray's data but replaced
1606             coordinates.
1607 
1608         See Also
1609         --------
1610         DataArray.set_index
1611         """
1612         _check_inplace(inplace)
1613         coords, _ = split_indexes(
1614             dims_or_levels, self._coords, set(), self._level_coords, drop=drop
1615         )
1616         return self._replace(coords=coords)
1617 
1618     def reorder_levels(
1619         self,
1620         dim_order: Mapping[Hashable, Sequence[int]] = None,
1621         inplace: bool = None,
1622         **dim_order_kwargs: Sequence[int]
1623     ) -> "DataArray":
1624         """Rearrange index levels using input order.
1625 
1626         Parameters
1627         ----------
1628         dim_order : optional
1629             Mapping from names matching dimensions and values given
1630             by lists representing new level orders. Every given dimension
1631             must have a multi-index.
1632         **dim_order_kwargs: optional
1633             The keyword arguments form of ``dim_order``.
1634             One of dim_order or dim_order_kwargs must be provided.
1635 
1636         Returns
1637         -------
1638         obj : DataArray
1639             Another dataarray, with this dataarray's data but replaced
1640             coordinates.
1641         """
1642         _check_inplace(inplace)
1643         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
1644         replace_coords = {}
1645         for dim, order in dim_order.items():
1646             coord = self._coords[dim]
1647             index = coord.to_index()
1648             if not isinstance(index, pd.MultiIndex):
1649                 raise ValueError("coordinate %r has no MultiIndex" % dim)
1650             replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))
1651         coords = self._coords.copy()
1652         coords.update(replace_coords)
1653         return self._replace(coords=coords)
1654 
1655     def stack(
1656         self,
1657         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
1658         **dimensions_kwargs: Sequence[Hashable]
1659     ) -> "DataArray":
1660         """
1661         Stack any number of existing dimensions into a single new dimension.
1662 
1663         New dimensions will be added at the end, and the corresponding
1664         coordinate variables will be combined into a MultiIndex.
1665 
1666         Parameters
1667         ----------
1668         dimensions : Mapping of the form new_name=(dim1, dim2, ...)
1669             Names of new dimensions, and the existing dimensions that they
1670             replace.
1671         **dimensions_kwargs:
1672             The keyword arguments form of ``dimensions``.
1673             One of dimensions or dimensions_kwargs must be provided.
1674 
1675         Returns
1676         -------
1677         stacked : DataArray
1678             DataArray with stacked data.
1679 
1680         Examples
1681         --------
1682 
1683         >>> arr = DataArray(np.arange(6).reshape(2, 3),
1684         ...                 coords=[('x', ['a', 'b']), ('y', [0, 1, 2])])
1685         >>> arr
1686         <xarray.DataArray (x: 2, y: 3)>
1687         array([[0, 1, 2],
1688                [3, 4, 5]])
1689         Coordinates:
1690           * x        (x) |S1 'a' 'b'
1691           * y        (y) int64 0 1 2
1692         >>> stacked = arr.stack(z=('x', 'y'))
1693         >>> stacked.indexes['z']
1694         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1695                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],
1696                    names=['x', 'y'])
1697 
1698         See also
1699         --------
1700         DataArray.unstack
1701         """
1702         ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)
1703         return self._from_temp_dataset(ds)
1704 
1705     def unstack(
1706         self, dim: Union[Hashable, Sequence[Hashable], None] = None
1707     ) -> "DataArray":
1708         """
1709         Unstack existing dimensions corresponding to MultiIndexes into
1710         multiple new dimensions.
1711 
1712         New dimensions will be added at the end.
1713 
1714         Parameters
1715         ----------
1716         dim : hashable or sequence of hashable, optional
1717             Dimension(s) over which to unstack. By default unstacks all
1718             MultiIndexes.
1719 
1720         Returns
1721         -------
1722         unstacked : DataArray
1723             Array with unstacked data.
1724 
1725         Examples
1726         --------
1727 
1728         >>> arr = DataArray(np.arange(6).reshape(2, 3),
1729         ...                 coords=[('x', ['a', 'b']), ('y', [0, 1, 2])])
1730         >>> arr
1731         <xarray.DataArray (x: 2, y: 3)>
1732         array([[0, 1, 2],
1733                [3, 4, 5]])
1734         Coordinates:
1735           * x        (x) |S1 'a' 'b'
1736           * y        (y) int64 0 1 2
1737         >>> stacked = arr.stack(z=('x', 'y'))
1738         >>> stacked.indexes['z']
1739         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1740                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],
1741                    names=['x', 'y'])
1742         >>> roundtripped = stacked.unstack()
1743         >>> arr.identical(roundtripped)
1744         True
1745 
1746         See also
1747         --------
1748         DataArray.stack
1749         """
1750         ds = self._to_temp_dataset().unstack(dim)
1751         return self._from_temp_dataset(ds)
1752 
1753     def to_unstacked_dataset(self, dim, level=0):
1754         """Unstack DataArray expanding to Dataset along a given level of a
1755         stacked coordinate.
1756 
1757         This is the inverse operation of Dataset.to_stacked_array.
1758 
1759         Parameters
1760         ----------
1761         dim : str
1762             Name of existing dimension to unstack
1763         level : int or str
1764             The MultiIndex level to expand to a dataset along. Can either be
1765             the integer index of the level or its name.
1766         label : int, default 0
1767             Label of the level to expand dataset along. Overrides the label
1768             argument if given.
1769 
1770         Returns
1771         -------
1772         unstacked: Dataset
1773 
1774         Examples
1775         --------
1776         >>> import xarray as xr
1777         >>> arr = DataArray(np.arange(6).reshape(2, 3),
1778         ...                 coords=[('x', ['a', 'b']), ('y', [0, 1, 2])])
1779         >>> data = xr.Dataset({'a': arr, 'b': arr.isel(y=0)})
1780         >>> data
1781         <xarray.Dataset>
1782         Dimensions:  (x: 2, y: 3)
1783         Coordinates:
1784           * x        (x) <U1 'a' 'b'
1785           * y        (y) int64 0 1 2
1786         Data variables:
1787             a        (x, y) int64 0 1 2 3 4 5
1788             b        (x) int64 0 3
1789         >>> stacked = data.to_stacked_array("z", ['y'])
1790         >>> stacked.indexes['z']
1791         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1792                 labels=[[0, 0, 0, 1], [0, 1, 2, -1]],
1793                 names=['variable', 'y'])
1794         >>> roundtripped = stacked.to_unstacked_dataset(dim='z')
1795         >>> data.identical(roundtripped)
1796         True
1797 
1798         See Also
1799         --------
1800         Dataset.to_stacked_array
1801         """
1802 
1803         idx = self.indexes[dim]
1804         if not isinstance(idx, pd.MultiIndex):
1805             raise ValueError("'{}' is not a stacked coordinate".format(dim))
1806 
1807         level_number = idx._get_level_number(level)
1808         variables = idx.levels[level_number]
1809         variable_dim = idx.names[level_number]
1810 
1811         # pull variables out of datarray
1812         data_dict = OrderedDict()
1813         for k in variables:
1814             data_dict[k] = self.sel({variable_dim: k}).squeeze(drop=True)
1815 
1816         # unstacked dataset
1817         return Dataset(data_dict)
1818 
1819     def transpose(self, *dims: Hashable, transpose_coords: bool = None) -> "DataArray":
1820         """Return a new DataArray object with transposed dimensions.
1821 
1822         Parameters
1823         ----------
1824         *dims : hashable, optional
1825             By default, reverse the dimensions. Otherwise, reorder the
1826             dimensions to this order.
1827         transpose_coords : boolean, optional
1828             If True, also transpose the coordinates of this DataArray.
1829 
1830         Returns
1831         -------
1832         transposed : DataArray
1833             The returned DataArray's array is transposed.
1834 
1835         Notes
1836         -----
1837         This operation returns a view of this array's data. It is
1838         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
1839         -- the data will be fully loaded.
1840 
1841         See Also
1842         --------
1843         numpy.transpose
1844         Dataset.transpose
1845         """
1846         if dims:
1847             if set(dims) ^ set(self.dims):
1848                 raise ValueError(
1849                     "arguments to transpose (%s) must be "
1850                     "permuted array dimensions (%s)" % (dims, tuple(self.dims))
1851                 )
1852 
1853         variable = self.variable.transpose(*dims)
1854         if transpose_coords:
1855             coords = OrderedDict()  # type: OrderedDict[Any, Variable]
1856             for name, coord in self.coords.items():
1857                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
1858                 coords[name] = coord.variable.transpose(*coord_dims)
1859             return self._replace(variable, coords)
1860         else:
1861             if transpose_coords is None and any(self[c].ndim > 1 for c in self.coords):
1862                 warnings.warn(
1863                     "This DataArray contains multi-dimensional "
1864                     "coordinates. In the future, these coordinates "
1865                     "will be transposed as well unless you specify "
1866                     "transpose_coords=False.",
1867                     FutureWarning,
1868                     stacklevel=2,
1869                 )
1870             return self._replace(variable)
1871 
1872     @property
1873     def T(self) -> "DataArray":
1874         return self.transpose()
1875 
1876     # Drop coords
1877     @overload
1878     def drop(
1879         self, labels: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
1880     ) -> "DataArray":
1881         ...
1882 
1883     # Drop index labels along dimension
1884     @overload  # noqa: F811
1885     def drop(
1886         self, labels: Any, dim: Hashable, *, errors: str = "raise"  # array-like
1887     ) -> "DataArray":
1888         ...
1889 
1890     def drop(self, labels, dim=None, *, errors="raise"):  # noqa: F811
1891         """Drop coordinates or index labels from this DataArray.
1892 
1893         Parameters
1894         ----------
1895         labels : hashable or sequence of hashables
1896             Name(s) of coordinates or index labels to drop.
1897             If dim is not None, labels can be any array-like.
1898         dim : hashable, optional
1899             Dimension along which to drop index labels. By default (if
1900             ``dim is None``), drops coordinates rather than index labels.
1901         errors: {'raise', 'ignore'}, optional
1902             If 'raise' (default), raises a ValueError error if
1903             any of the coordinates or index labels passed are not
1904             in the array. If 'ignore', any given labels that are in the
1905             array are dropped and no error is raised.
1906         Returns
1907         -------
1908         dropped : DataArray
1909         """
1910         ds = self._to_temp_dataset().drop(labels, dim, errors=errors)
1911         return self._from_temp_dataset(ds)
1912 
1913     def dropna(
1914         self, dim: Hashable, how: str = "any", thresh: int = None
1915     ) -> "DataArray":
1916         """Returns a new array with dropped labels for missing values along
1917         the provided dimension.
1918 
1919         Parameters
1920         ----------
1921         dim : hashable
1922             Dimension along which to drop missing values. Dropping along
1923             multiple dimensions simultaneously is not yet supported.
1924         how : {'any', 'all'}, optional
1925             * any : if any NA values are present, drop that label
1926             * all : if all values are NA, drop that label
1927         thresh : int, default None
1928             If supplied, require this many non-NA values.
1929 
1930         Returns
1931         -------
1932         DataArray
1933         """
1934         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
1935         return self._from_temp_dataset(ds)
1936 
1937     def fillna(self, value: Any) -> "DataArray":
1938         """Fill missing values in this object.
1939 
1940         This operation follows the normal broadcasting and alignment rules that
1941         xarray uses for binary arithmetic, except the result is aligned to this
1942         object (``join='left'``) instead of aligned to the intersection of
1943         index coordinates (``join='inner'``).
1944 
1945         Parameters
1946         ----------
1947         value : scalar, ndarray or DataArray
1948             Used to fill all matching missing values in this array. If the
1949             argument is a DataArray, it is first aligned with (reindexed to)
1950             this array.
1951 
1952         Returns
1953         -------
1954         DataArray
1955         """
1956         if utils.is_dict_like(value):
1957             raise TypeError(
1958                 "cannot provide fill value as a dictionary with "
1959                 "fillna on a DataArray"
1960             )
1961         out = ops.fillna(self, value)
1962         return out
1963 
1964     def interpolate_na(
1965         self,
1966         dim=None,
1967         method: str = "linear",
1968         limit: int = None,
1969         use_coordinate: Union[bool, str] = True,
1970         **kwargs: Any
1971     ) -> "DataArray":
1972         """Interpolate values according to different methods.
1973 
1974         Parameters
1975         ----------
1976         dim : str
1977             Specifies the dimension along which to interpolate.
1978         method : {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic',
1979                   'polynomial', 'barycentric', 'krog', 'pchip',
1980                   'spline', 'akima'}, optional
1981             String indicating which method to use for interpolation:
1982 
1983             - 'linear': linear interpolation (Default). Additional keyword
1984               arguments are passed to ``numpy.interp``
1985             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic',
1986               'polynomial': are passed to ``scipy.interpolate.interp1d``. If
1987               method=='polynomial', the ``order`` keyword argument must also be
1988               provided.
1989             - 'barycentric', 'krog', 'pchip', 'spline', and `akima`: use their
1990               respective``scipy.interpolate`` classes.
1991         use_coordinate : boolean or str, default True
1992             Specifies which index to use as the x values in the interpolation
1993             formulated as `y = f(x)`. If False, values are treated as if
1994             eqaully-spaced along `dim`. If True, the IndexVariable `dim` is
1995             used. If use_coordinate is a string, it specifies the name of a
1996             coordinate variariable to use as the index.
1997         limit : int, default None
1998             Maximum number of consecutive NaNs to fill. Must be greater than 0
1999             or None for no limit.
2000 
2001         Returns
2002         -------
2003         DataArray
2004 
2005         See also
2006         --------
2007         numpy.interp
2008         scipy.interpolate
2009         """
2010         from .missing import interp_na
2011 
2012         return interp_na(
2013             self,
2014             dim=dim,
2015             method=method,
2016             limit=limit,
2017             use_coordinate=use_coordinate,
2018             **kwargs
2019         )
2020 
2021     def ffill(self, dim: Hashable, limit: int = None) -> "DataArray":
2022         """Fill NaN values by propogating values forward
2023 
2024         *Requires bottleneck.*
2025 
2026         Parameters
2027         ----------
2028         dim : hashable
2029             Specifies the dimension along which to propagate values when
2030             filling.
2031         limit : int, default None
2032             The maximum number of consecutive NaN values to forward fill. In
2033             other words, if there is a gap with more than this number of
2034             consecutive NaNs, it will only be partially filled. Must be greater
2035             than 0 or None for no limit.
2036 
2037         Returns
2038         -------
2039         DataArray
2040         """
2041         from .missing import ffill
2042 
2043         return ffill(self, dim, limit=limit)
2044 
2045     def bfill(self, dim: Hashable, limit: int = None) -> "DataArray":
2046         """Fill NaN values by propogating values backward
2047 
2048         *Requires bottleneck.*
2049 
2050         Parameters
2051         ----------
2052         dim : str
2053             Specifies the dimension along which to propagate values when
2054             filling.
2055         limit : int, default None
2056             The maximum number of consecutive NaN values to backward fill. In
2057             other words, if there is a gap with more than this number of
2058             consecutive NaNs, it will only be partially filled. Must be greater
2059             than 0 or None for no limit.
2060 
2061         Returns
2062         -------
2063         DataArray
2064         """
2065         from .missing import bfill
2066 
2067         return bfill(self, dim, limit=limit)
2068 
2069     def combine_first(self, other: "DataArray") -> "DataArray":
2070         """Combine two DataArray objects, with union of coordinates.
2071 
2072         This operation follows the normal broadcasting and alignment rules of
2073         ``join='outer'``.  Default to non-null values of array calling the
2074         method.  Use np.nan to fill in vacant cells after alignment.
2075 
2076         Parameters
2077         ----------
2078         other : DataArray
2079             Used to fill all matching missing values in this array.
2080 
2081         Returns
2082         -------
2083         DataArray
2084         """
2085         return ops.fillna(self, other, join="outer")
2086 
2087     def reduce(
2088         self,
2089         func: Callable[..., Any],
2090         dim: Union[None, Hashable, Sequence[Hashable]] = None,
2091         axis: Union[None, int, Sequence[int]] = None,
2092         keep_attrs: bool = None,
2093         keepdims: bool = False,
2094         **kwargs: Any
2095     ) -> "DataArray":
2096         """Reduce this array by applying `func` along some dimension(s).
2097 
2098         Parameters
2099         ----------
2100         func : function
2101             Function which can be called in the form
2102             `f(x, axis=axis, **kwargs)` to return the result of reducing an
2103             np.ndarray over an integer valued axis.
2104         dim : hashable or sequence of hashables, optional
2105             Dimension(s) over which to apply `func`.
2106         axis : int or sequence of int, optional
2107             Axis(es) over which to repeatedly apply `func`. Only one of the
2108             'dim' and 'axis' arguments can be supplied. If neither are
2109             supplied, then the reduction is calculated over the flattened array
2110             (by calling `f(x)` without an axis argument).
2111         keep_attrs : bool, optional
2112             If True, the variable's attributes (`attrs`) will be copied from
2113             the original object to the new one.  If False (default), the new
2114             object will be returned without attributes.
2115         keepdims : bool, default False
2116             If True, the dimensions which are reduced are left in the result
2117             as dimensions of size one. Coordinates that use these dimensions
2118             are removed.
2119         **kwargs : dict
2120             Additional keyword arguments passed on to `func`.
2121 
2122         Returns
2123         -------
2124         reduced : DataArray
2125             DataArray with this object's array replaced with an array with
2126             summarized data and the indicated dimension(s) removed.
2127         """
2128 
2129         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
2130         return self._replace_maybe_drop_dims(var)
2131 
2132     def to_pandas(self) -> Union["DataArray", pd.Series, pd.DataFrame]:
2133         """Convert this array into a pandas object with the same shape.
2134 
2135         The type of the returned object depends on the number of DataArray
2136         dimensions:
2137 
2138         * 0D -> `xarray.DataArray`
2139         * 1D -> `pandas.Series`
2140         * 2D -> `pandas.DataFrame`
2141         * 3D -> `pandas.Panel` *(deprecated)*
2142 
2143         Only works for arrays with 3 or fewer dimensions.
2144 
2145         The DataArray constructor performs the inverse transformation.
2146         """
2147         # TODO: consolidate the info about pandas constructors and the
2148         # attributes that correspond to their indexes into a separate module?
2149         constructors = {
2150             0: lambda x: x,
2151             1: pd.Series,
2152             2: pd.DataFrame,
2153             3: pdcompat.Panel,
2154         }
2155         try:
2156             constructor = constructors[self.ndim]
2157         except KeyError:
2158             raise ValueError(
2159                 "cannot convert arrays with %s dimensions into "
2160                 "pandas objects" % self.ndim
2161             )
2162         indexes = [self.get_index(dim) for dim in self.dims]
2163         return constructor(self.values, *indexes)
2164 
2165     def to_dataframe(self, name: Hashable = None) -> pd.DataFrame:
2166         """Convert this array and its coordinates into a tidy pandas.DataFrame.
2167 
2168         The DataFrame is indexed by the Cartesian product of index coordinates
2169         (in the form of a :py:class:`pandas.MultiIndex`).
2170 
2171         Other coordinates are included as columns in the DataFrame.
2172         """
2173         if name is None:
2174             name = self.name
2175         if name is None:
2176             raise ValueError(
2177                 "cannot convert an unnamed DataArray to a "
2178                 "DataFrame: use the ``name`` parameter"
2179             )
2180 
2181         dims = OrderedDict(zip(self.dims, self.shape))
2182         # By using a unique name, we can convert a DataArray into a DataFrame
2183         # even if it shares a name with one of its coordinates.
2184         # I would normally use unique_name = object() but that results in a
2185         # dataframe with columns in the wrong order, for reasons I have not
2186         # been able to debug (possibly a pandas bug?).
2187         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
2188         ds = self._to_dataset_whole(name=unique_name)
2189         df = ds._to_dataframe(dims)
2190         df.columns = [name if c == unique_name else c for c in df.columns]
2191         return df
2192 
2193     def to_series(self) -> pd.Series:
2194         """Convert this array into a pandas.Series.
2195 
2196         The Series is indexed by the Cartesian product of index coordinates
2197         (in the form of a :py:class:`pandas.MultiIndex`).
2198         """
2199         index = self.coords.to_index()
2200         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
2201 
2202     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
2203         """Convert this array into a numpy.ma.MaskedArray
2204 
2205         Parameters
2206         ----------
2207         copy : bool
2208             If True (default) make a copy of the array in the result. If False,
2209             a MaskedArray view of DataArray.values is returned.
2210 
2211         Returns
2212         -------
2213         result : MaskedArray
2214             Masked where invalid values (nan or inf) occur.
2215         """
2216         values = self.values  # only compute lazy arrays once
2217         isnull = pd.isnull(values)
2218         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
2219 
2220     def to_netcdf(self, *args, **kwargs) -> Optional["Delayed"]:
2221         """Write DataArray contents to a netCDF file.
2222 
2223         All parameters are passed directly to `xarray.Dataset.to_netcdf`.
2224 
2225         Notes
2226         -----
2227         Only xarray.Dataset objects can be written to netCDF files, so
2228         the xarray.DataArray is converted to a xarray.Dataset object
2229         containing a single variable. If the DataArray has no name, or if the
2230         name is the same as a co-ordinate name, then it is given the name
2231         '__xarray_dataarray_variable__'.
2232         """
2233         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE
2234 
2235         if self.name is None:
2236             # If no name is set then use a generic xarray name
2237             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2238         elif self.name in self.coords or self.name in self.dims:
2239             # The name is the same as one of the coords names, which netCDF
2240             # doesn't support, so rename it but keep track of the old name
2241             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2242             dataset.attrs[DATAARRAY_NAME] = self.name
2243         else:
2244             # No problems with the name - so we're fine!
2245             dataset = self.to_dataset()
2246 
2247         return dataset.to_netcdf(*args, **kwargs)
2248 
2249     def to_dict(self, data: bool = True) -> dict:
2250         """
2251         Convert this xarray.DataArray into a dictionary following xarray
2252         naming conventions.
2253 
2254         Converts all variables and attributes to native Python objects.
2255         Useful for coverting to json. To avoid datetime incompatibility
2256         use decode_times=False kwarg in xarrray.open_dataset.
2257 
2258         Parameters
2259         ----------
2260         data : bool, optional
2261             Whether to include the actual data in the dictionary. When set to
2262             False, returns just the schema.
2263 
2264         See also
2265         --------
2266         DataArray.from_dict
2267         """
2268         d = self.variable.to_dict(data=data)
2269         d.update({"coords": {}, "name": self.name})
2270         for k in self.coords:
2271             d["coords"][k] = self.coords[k].variable.to_dict(data=data)
2272         return d
2273 
2274     @classmethod
2275     def from_dict(cls, d: dict) -> "DataArray":
2276         """
2277         Convert a dictionary into an xarray.DataArray
2278 
2279         Input dict can take several forms::
2280 
2281             d = {'dims': ('t'), 'data': x}
2282 
2283             d = {'coords': {'t': {'dims': 't', 'data': t,
2284                                   'attrs': {'units':'s'}}},
2285                  'attrs': {'title': 'air temperature'},
2286                  'dims': 't',
2287                  'data': x,
2288                  'name': 'a'}
2289 
2290         where 't' is the name of the dimesion, 'a' is the name of the array,
2291         and  x and t are lists, numpy.arrays, or pandas objects.
2292 
2293         Parameters
2294         ----------
2295         d : dict, with a minimum structure of {'dims': [..], 'data': [..]}
2296 
2297         Returns
2298         -------
2299         obj : xarray.DataArray
2300 
2301         See also
2302         --------
2303         DataArray.to_dict
2304         Dataset.from_dict
2305         """
2306         coords = None
2307         if "coords" in d:
2308             try:
2309                 coords = OrderedDict(
2310                     [
2311                         (k, (v["dims"], v["data"], v.get("attrs")))
2312                         for k, v in d["coords"].items()
2313                     ]
2314                 )
2315             except KeyError as e:
2316                 raise ValueError(
2317                     "cannot convert dict when coords are missing the key "
2318                     "'{dims_data}'".format(dims_data=str(e.args[0]))
2319                 )
2320         try:
2321             data = d["data"]
2322         except KeyError:
2323             raise ValueError("cannot convert dict without the key 'data''")
2324         else:
2325             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
2326         return obj
2327 
2328     @classmethod
2329     def from_series(cls, series: pd.Series, sparse: bool = False) -> "DataArray":
2330         """Convert a pandas.Series into an xarray.DataArray.
2331 
2332         If the series's index is a MultiIndex, it will be expanded into a
2333         tensor product of one-dimensional coordinates (filling in missing
2334         values with NaN). Thus this operation should be the inverse of the
2335         `to_series` method.
2336 
2337         If sparse=True, creates a sparse array instead of a dense NumPy array.
2338         Requires the pydata/sparse package.
2339 
2340         See also
2341         --------
2342         xarray.Dataset.from_dataframe
2343         """
2344         temp_name = "__temporary_name"
2345         df = pd.DataFrame({temp_name: series})
2346         ds = Dataset.from_dataframe(df, sparse=sparse)
2347         result = cast(DataArray, ds[temp_name])
2348         result.name = series.name
2349         return result
2350 
2351     def to_cdms2(self) -> "cdms2_Variable":
2352         """Convert this array into a cdms2.Variable
2353         """
2354         from ..convert import to_cdms2
2355 
2356         return to_cdms2(self)
2357 
2358     @classmethod
2359     def from_cdms2(cls, variable: "cdms2_Variable") -> "DataArray":
2360         """Convert a cdms2.Variable into an xarray.DataArray
2361         """
2362         from ..convert import from_cdms2
2363 
2364         return from_cdms2(variable)
2365 
2366     def to_iris(self) -> "iris_Cube":
2367         """Convert this array into a iris.cube.Cube
2368         """
2369         from ..convert import to_iris
2370 
2371         return to_iris(self)
2372 
2373     @classmethod
2374     def from_iris(cls, cube: "iris_Cube") -> "DataArray":
2375         """Convert a iris.cube.Cube into an xarray.DataArray
2376         """
2377         from ..convert import from_iris
2378 
2379         return from_iris(cube)
2380 
2381     def _all_compat(self, other: "DataArray", compat_str: str) -> bool:
2382         """Helper function for equals, broadcast_equals, and identical
2383         """
2384 
2385         def compat(x, y):
2386             return getattr(x.variable, compat_str)(y.variable)
2387 
2388         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
2389             self, other
2390         )
2391 
2392     def broadcast_equals(self, other: "DataArray") -> bool:
2393         """Two DataArrays are broadcast equal if they are equal after
2394         broadcasting them against each other such that they have the same
2395         dimensions.
2396 
2397         See Also
2398         --------
2399         DataArray.equals
2400         DataArray.identical
2401         """
2402         try:
2403             return self._all_compat(other, "broadcast_equals")
2404         except (TypeError, AttributeError):
2405             return False
2406 
2407     def equals(self, other: "DataArray") -> bool:
2408         """True if two DataArrays have the same dimensions, coordinates and
2409         values; otherwise False.
2410 
2411         DataArrays can still be equal (like pandas objects) if they have NaN
2412         values in the same locations.
2413 
2414         This method is necessary because `v1 == v2` for ``DataArray``
2415         does element-wise comparisons (like numpy.ndarrays).
2416 
2417         See Also
2418         --------
2419         DataArray.broadcast_equals
2420         DataArray.identical
2421         """
2422         try:
2423             return self._all_compat(other, "equals")
2424         except (TypeError, AttributeError):
2425             return False
2426 
2427     def identical(self, other: "DataArray") -> bool:
2428         """Like equals, but also checks the array name and attributes, and
2429         attributes on all coordinates.
2430 
2431         See Also
2432         --------
2433         DataArray.broadcast_equals
2434         DataArray.equal
2435         """
2436         try:
2437             return self.name == other.name and self._all_compat(other, "identical")
2438         except (TypeError, AttributeError):
2439             return False
2440 
2441     __default_name = object()
2442 
2443     def _result_name(self, other: Any = None) -> Optional[Hashable]:
2444         # use the same naming heuristics as pandas:
2445         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
2446         other_name = getattr(other, "name", self.__default_name)
2447         if other_name is self.__default_name or other_name == self.name:
2448             return self.name
2449         else:
2450             return None
2451 
2452     def __array_wrap__(self, obj, context=None) -> "DataArray":
2453         new_var = self.variable.__array_wrap__(obj, context)
2454         return self._replace(new_var)
2455 
2456     def __matmul__(self, obj):
2457         return self.dot(obj)
2458 
2459     def __rmatmul__(self, other):
2460         # currently somewhat duplicative, as only other DataArrays are
2461         # compatible with matmul
2462         return computation.dot(other, self)
2463 
2464     @staticmethod
2465     def _unary_op(f: Callable[..., Any]) -> Callable[..., "DataArray"]:
2466         @functools.wraps(f)
2467         def func(self, *args, **kwargs):
2468             with np.errstate(all="ignore"):
2469                 return self.__array_wrap__(f(self.variable.data, *args, **kwargs))
2470 
2471         return func
2472 
2473     @staticmethod
2474     def _binary_op(
2475         f: Callable[..., Any],
2476         reflexive: bool = False,
2477         join: str = None,  # see xarray.align
2478         **ignored_kwargs
2479     ) -> Callable[..., "DataArray"]:
2480         @functools.wraps(f)
2481         def func(self, other):
2482             if isinstance(other, (Dataset, groupby.GroupBy)):
2483                 return NotImplemented
2484             if isinstance(other, DataArray):
2485                 align_type = OPTIONS["arithmetic_join"] if join is None else join
2486                 self, other = align(self, other, join=align_type, copy=False)
2487             other_variable = getattr(other, "variable", other)
2488             other_coords = getattr(other, "coords", None)
2489 
2490             variable = (
2491                 f(self.variable, other_variable)
2492                 if not reflexive
2493                 else f(other_variable, self.variable)
2494             )
2495             coords = self.coords._merge_raw(other_coords)
2496             name = self._result_name(other)
2497 
2498             return self._replace(variable, coords, name)
2499 
2500         return func
2501 
2502     @staticmethod
2503     def _inplace_binary_op(f: Callable) -> Callable[..., "DataArray"]:
2504         @functools.wraps(f)
2505         def func(self, other):
2506             if isinstance(other, groupby.GroupBy):
2507                 raise TypeError(
2508                     "in-place operations between a DataArray and "
2509                     "a grouped object are not permitted"
2510                 )
2511             # n.b. we can't align other to self (with other.reindex_like(self))
2512             # because `other` may be converted into floats, which would cause
2513             # in-place arithmetic to fail unpredictably. Instead, we simply
2514             # don't support automatic alignment with in-place arithmetic.
2515             other_coords = getattr(other, "coords", None)
2516             other_variable = getattr(other, "variable", other)
2517             with self.coords._merge_inplace(other_coords):
2518                 f(self.variable, other_variable)
2519             return self
2520 
2521         return func
2522 
2523     def _copy_attrs_from(self, other: Union["DataArray", Dataset, Variable]) -> None:
2524         self.attrs = other.attrs
2525 
2526     @property
2527     def plot(self) -> _PlotMethods:
2528         """
2529         Access plotting functions
2530 
2531         >>> d = DataArray([[1, 2], [3, 4]])
2532 
2533         For convenience just call this directly
2534         >>> d.plot()
2535 
2536         Or use it as a namespace to use xarray.plot functions as
2537         DataArray methods
2538         >>> d.plot.imshow()  # equivalent to xarray.plot.imshow(d)
2539 
2540         """
2541         return _PlotMethods(self)
2542 
2543     def _title_for_slice(self, truncate: int = 50) -> str:
2544         """
2545         If the dataarray has 1 dimensional coordinates or comes from a slice
2546         we can show that info in the title
2547 
2548         Parameters
2549         ----------
2550         truncate : integer
2551             maximum number of characters for title
2552 
2553         Returns
2554         -------
2555         title : string
2556             Can be used for plot titles
2557 
2558         """
2559         one_dims = []
2560         for dim, coord in self.coords.items():
2561             if coord.size == 1:
2562                 one_dims.append(
2563                     "{dim} = {v}".format(dim=dim, v=format_item(coord.values))
2564                 )
2565 
2566         title = ", ".join(one_dims)
2567         if len(title) > truncate:
2568             title = title[: (truncate - 3)] + "..."
2569 
2570         return title
2571 
2572     def diff(self, dim: Hashable, n: int = 1, label: Hashable = "upper") -> "DataArray":
2573         """Calculate the n-th order discrete difference along given axis.
2574 
2575         Parameters
2576         ----------
2577         dim : hashable, optional
2578             Dimension over which to calculate the finite difference.
2579         n : int, optional
2580             The number of times values are differenced.
2581         label : hashable, optional
2582             The new coordinate in dimension ``dim`` will have the
2583             values of either the minuend's or subtrahend's coordinate
2584             for values 'upper' and 'lower', respectively.  Other
2585             values are not supported.
2586 
2587         Returns
2588         -------
2589         difference : same type as caller
2590             The n-th order finite difference of this object.
2591 
2592         Examples
2593         --------
2594         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ['x'])
2595         >>> arr.diff('x')
2596         <xarray.DataArray (x: 3)>
2597         array([0, 1, 0])
2598         Coordinates:
2599         * x        (x) int64 2 3 4
2600         >>> arr.diff('x', 2)
2601         <xarray.DataArray (x: 2)>
2602         array([ 1, -1])
2603         Coordinates:
2604         * x        (x) int64 3 4
2605 
2606         See Also
2607         --------
2608         DataArray.differentiate
2609         """
2610         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
2611         return self._from_temp_dataset(ds)
2612 
2613     def shift(
2614         self,
2615         shifts: Mapping[Hashable, int] = None,
2616         fill_value: Any = dtypes.NA,
2617         **shifts_kwargs: int
2618     ) -> "DataArray":
2619         """Shift this array by an offset along one or more dimensions.
2620 
2621         Only the data is moved; coordinates stay in place. Values shifted from
2622         beyond array bounds are replaced by NaN. This is consistent with the
2623         behavior of ``shift`` in pandas.
2624 
2625         Parameters
2626         ----------
2627         shifts : Mapping with the form of {dim: offset}
2628             Integer offset to shift along each of the given dimensions.
2629             Positive offsets shift to the right; negative offsets shift to the
2630             left.
2631         fill_value: scalar, optional
2632             Value to use for newly missing values
2633         **shifts_kwargs:
2634             The keyword arguments form of ``shifts``.
2635             One of shifts or shifts_kwarg must be provided.
2636 
2637         Returns
2638         -------
2639         shifted : DataArray
2640             DataArray with the same coordinates and attributes but shifted
2641             data.
2642 
2643         See also
2644         --------
2645         roll
2646 
2647         Examples
2648         --------
2649 
2650         >>> arr = xr.DataArray([5, 6, 7], dims='x')
2651         >>> arr.shift(x=1)
2652         <xarray.DataArray (x: 3)>
2653         array([ nan,   5.,   6.])
2654         Coordinates:
2655           * x        (x) int64 0 1 2
2656         """
2657         variable = self.variable.shift(
2658             shifts=shifts, fill_value=fill_value, **shifts_kwargs
2659         )
2660         return self._replace(variable=variable)
2661 
2662     def roll(
2663         self,
2664         shifts: Mapping[Hashable, int] = None,
2665         roll_coords: bool = None,
2666         **shifts_kwargs: int
2667     ) -> "DataArray":
2668         """Roll this array by an offset along one or more dimensions.
2669 
2670         Unlike shift, roll may rotate all variables, including coordinates
2671         if specified. The direction of rotation is consistent with
2672         :py:func:`numpy.roll`.
2673 
2674         Parameters
2675         ----------
2676         shifts : Mapping with the form of {dim: offset}
2677             Integer offset to rotate each of the given dimensions.
2678             Positive offsets roll to the right; negative offsets roll to the
2679             left.
2680         roll_coords : bool
2681             Indicates whether to  roll the coordinates by the offset
2682             The current default of roll_coords (None, equivalent to True) is
2683             deprecated and will change to False in a future version.
2684             Explicitly pass roll_coords to silence the warning.
2685         **shifts_kwargs : The keyword arguments form of ``shifts``.
2686             One of shifts or shifts_kwarg must be provided.
2687 
2688         Returns
2689         -------
2690         rolled : DataArray
2691             DataArray with the same attributes but rolled data and coordinates.
2692 
2693         See also
2694         --------
2695         shift
2696 
2697         Examples
2698         --------
2699 
2700         >>> arr = xr.DataArray([5, 6, 7], dims='x')
2701         >>> arr.roll(x=1)
2702         <xarray.DataArray (x: 3)>
2703         array([7, 5, 6])
2704         Coordinates:
2705           * x        (x) int64 2 0 1
2706         """
2707         ds = self._to_temp_dataset().roll(
2708             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
2709         )
2710         return self._from_temp_dataset(ds)
2711 
2712     @property
2713     def real(self) -> "DataArray":
2714         return self._replace(self.variable.real)
2715 
2716     @property
2717     def imag(self) -> "DataArray":
2718         return self._replace(self.variable.imag)
2719 
2720     def dot(
2721         self, other: "DataArray", dims: Union[Hashable, Sequence[Hashable], None] = None
2722     ) -> "DataArray":
2723         """Perform dot product of two DataArrays along their shared dims.
2724 
2725         Equivalent to taking taking tensordot over all shared dims.
2726 
2727         Parameters
2728         ----------
2729         other : DataArray
2730             The other array with which the dot product is performed.
2731         dims: hashable or sequence of hashables, optional
2732             Along which dimensions to be summed over. Default all the common
2733             dimensions are summed over.
2734 
2735         Returns
2736         -------
2737         result : DataArray
2738             Array resulting from the dot product over all shared dimensions.
2739 
2740         See also
2741         --------
2742         dot
2743         numpy.tensordot
2744 
2745         Examples
2746         --------
2747 
2748         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
2749         >>> da = DataArray(da_vals, dims=['x', 'y', 'z'])
2750         >>> dm_vals = np.arange(4)
2751         >>> dm = DataArray(dm_vals, dims=['z'])
2752 
2753         >>> dm.dims
2754         ('z')
2755         >>> da.dims
2756         ('x', 'y', 'z')
2757 
2758         >>> dot_result = da.dot(dm)
2759         >>> dot_result.dims
2760         ('x', 'y')
2761         """
2762         if isinstance(other, Dataset):
2763             raise NotImplementedError(
2764                 "dot products are not yet supported with Dataset objects."
2765             )
2766         if not isinstance(other, DataArray):
2767             raise TypeError("dot only operates on DataArrays.")
2768 
2769         return computation.dot(self, other, dims=dims)
2770 
2771     def sortby(
2772         self,
2773         variables: Union[Hashable, "DataArray", Sequence[Union[Hashable, "DataArray"]]],
2774         ascending: bool = True,
2775     ) -> "DataArray":
2776         """Sort object by labels or values (along an axis).
2777 
2778         Sorts the dataarray, either along specified dimensions,
2779         or according to values of 1-D dataarrays that share dimension
2780         with calling object.
2781 
2782         If the input variables are dataarrays, then the dataarrays are aligned
2783         (via left-join) to the calling object prior to sorting by cell values.
2784         NaNs are sorted to the end, following Numpy convention.
2785 
2786         If multiple sorts along the same dimension is
2787         given, numpy's lexsort is performed along that dimension:
2788         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
2789         and the FIRST key in the sequence is used as the primary sort key,
2790         followed by the 2nd key, etc.
2791 
2792         Parameters
2793         ----------
2794         variables: hashable, DataArray, or sequence of either
2795             1D DataArray objects or name(s) of 1D variable(s) in
2796             coords whose values are used to sort this array.
2797         ascending: boolean, optional
2798             Whether to sort by ascending or descending order.
2799 
2800         Returns
2801         -------
2802         sorted: DataArray
2803             A new dataarray where all the specified dims are sorted by dim
2804             labels.
2805 
2806         Examples
2807         --------
2808 
2809         >>> da = xr.DataArray(np.random.rand(5),
2810         ...                   coords=[pd.date_range('1/1/2000', periods=5)],
2811         ...                   dims='time')
2812         >>> da
2813         <xarray.DataArray (time: 5)>
2814         array([ 0.965471,  0.615637,  0.26532 ,  0.270962,  0.552878])
2815         Coordinates:
2816           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...
2817 
2818         >>> da.sortby(da)
2819         <xarray.DataArray (time: 5)>
2820         array([ 0.26532 ,  0.270962,  0.552878,  0.615637,  0.965471])
2821         Coordinates:
2822           * time     (time) datetime64[ns] 2000-01-03 2000-01-04 2000-01-05 ...
2823         """
2824         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
2825         return self._from_temp_dataset(ds)
2826 
2827     def quantile(
2828         self,
2829         q: Any,
2830         dim: Union[Hashable, Sequence[Hashable], None] = None,
2831         interpolation: str = "linear",
2832         keep_attrs: bool = None,
2833     ) -> "DataArray":
2834         """Compute the qth quantile of the data along the specified dimension.
2835 
2836         Returns the qth quantiles(s) of the array elements.
2837 
2838         Parameters
2839         ----------
2840         q : float in range of [0,1] or array-like of floats
2841             Quantile to compute, which must be between 0 and 1 inclusive.
2842         dim : hashable or sequence of hashable, optional
2843             Dimension(s) over which to apply quantile.
2844         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
2845             This optional parameter specifies the interpolation method to
2846             use when the desired quantile lies between two data points
2847             ``i < j``:
2848 
2849                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is
2850                   the fractional part of the index surrounded by ``i`` and
2851                   ``j``.
2852                 - lower: ``i``.
2853                 - higher: ``j``.
2854                 - nearest: ``i`` or ``j``, whichever is nearest.
2855                 - midpoint: ``(i + j) / 2``.
2856         keep_attrs : bool, optional
2857             If True, the dataset's attributes (`attrs`) will be copied from
2858             the original object to the new one.  If False (default), the new
2859             object will be returned without attributes.
2860 
2861         Returns
2862         -------
2863         quantiles : DataArray
2864             If `q` is a single quantile, then the result
2865             is a scalar. If multiple percentiles are given, first axis of
2866             the result corresponds to the quantile and a quantile dimension
2867             is added to the return array. The other dimensions are the
2868              dimensions that remain after the reduction of the array.
2869 
2870         See Also
2871         --------
2872         numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile
2873         """
2874 
2875         ds = self._to_temp_dataset().quantile(
2876             q, dim=dim, keep_attrs=keep_attrs, interpolation=interpolation
2877         )
2878         return self._from_temp_dataset(ds)
2879 
2880     def rank(
2881         self, dim: Hashable, pct: bool = False, keep_attrs: bool = None
2882     ) -> "DataArray":
2883         """Ranks the data.
2884 
2885         Equal values are assigned a rank that is the average of the ranks that
2886         would have been otherwise assigned to all of the values within that
2887         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
2888 
2889         NaNs in the input array are returned as NaNs.
2890 
2891         The `bottleneck` library is required.
2892 
2893         Parameters
2894         ----------
2895         dim : hashable
2896             Dimension over which to compute rank.
2897         pct : bool, optional
2898             If True, compute percentage ranks, otherwise compute integer ranks.
2899         keep_attrs : bool, optional
2900             If True, the dataset's attributes (`attrs`) will be copied from
2901             the original object to the new one.  If False (default), the new
2902             object will be returned without attributes.
2903 
2904         Returns
2905         -------
2906         ranked : DataArray
2907             DataArray with the same coordinates and dtype 'float64'.
2908 
2909         Examples
2910         --------
2911 
2912         >>> arr = xr.DataArray([5, 6, 7], dims='x')
2913         >>> arr.rank('x')
2914         <xarray.DataArray (x: 3)>
2915         array([ 1.,   2.,   3.])
2916         Dimensions without coordinates: x
2917         """
2918 
2919         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
2920         return self._from_temp_dataset(ds)
2921 
2922     def differentiate(
2923         self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None
2924     ) -> "DataArray":
2925         """ Differentiate the array with the second order accurate central
2926         differences.
2927 
2928         .. note::
2929             This feature is limited to simple cartesian geometry, i.e. coord
2930             must be one dimensional.
2931 
2932         Parameters
2933         ----------
2934         coord: hashable
2935             The coordinate to be used to compute the gradient.
2936         edge_order: 1 or 2. Default 1
2937             N-th order accurate differences at the boundaries.
2938         datetime_unit: None or any of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms',
2939             'us', 'ns', 'ps', 'fs', 'as'}
2940             Unit to compute gradient. Only valid for datetime coordinate.
2941 
2942         Returns
2943         -------
2944         differentiated: DataArray
2945 
2946         See also
2947         --------
2948         numpy.gradient: corresponding numpy function
2949 
2950         Examples
2951         --------
2952 
2953         >>> da = xr.DataArray(np.arange(12).reshape(4, 3), dims=['x', 'y'],
2954         ...                   coords={'x': [0, 0.1, 1.1, 1.2]})
2955         >>> da
2956         <xarray.DataArray (x: 4, y: 3)>
2957         array([[ 0,  1,  2],
2958                [ 3,  4,  5],
2959                [ 6,  7,  8],
2960                [ 9, 10, 11]])
2961         Coordinates:
2962           * x        (x) float64 0.0 0.1 1.1 1.2
2963         Dimensions without coordinates: y
2964         >>>
2965         >>> da.differentiate('x')
2966         <xarray.DataArray (x: 4, y: 3)>
2967         array([[30.      , 30.      , 30.      ],
2968                [27.545455, 27.545455, 27.545455],
2969                [27.545455, 27.545455, 27.545455],
2970                [30.      , 30.      , 30.      ]])
2971         Coordinates:
2972           * x        (x) float64 0.0 0.1 1.1 1.2
2973         Dimensions without coordinates: y
2974         """
2975         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
2976         return self._from_temp_dataset(ds)
2977 
2978     def integrate(
2979         self, dim: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None
2980     ) -> "DataArray":
2981         """ integrate the array with the trapezoidal rule.
2982 
2983         .. note::
2984             This feature is limited to simple cartesian geometry, i.e. coord
2985             must be one dimensional.
2986 
2987         Parameters
2988         ----------
2989         dim: hashable, or a sequence of hashable
2990             Coordinate(s) used for the integration.
2991         datetime_unit: str, optional
2992             Can be used to specify the unit if datetime coordinate is used.
2993             One of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns',
2994                     'ps', 'fs', 'as'}
2995 
2996         Returns
2997         -------
2998         integrated: DataArray
2999 
3000         See also
3001         --------
3002         numpy.trapz: corresponding numpy function
3003 
3004         Examples
3005         --------
3006 
3007         >>> da = xr.DataArray(np.arange(12).reshape(4, 3), dims=['x', 'y'],
3008         ...                   coords={'x': [0, 0.1, 1.1, 1.2]})
3009         >>> da
3010         <xarray.DataArray (x: 4, y: 3)>
3011         array([[ 0,  1,  2],
3012                [ 3,  4,  5],
3013                [ 6,  7,  8],
3014                [ 9, 10, 11]])
3015         Coordinates:
3016           * x        (x) float64 0.0 0.1 1.1 1.2
3017         Dimensions without coordinates: y
3018         >>>
3019         >>> da.integrate('x')
3020         <xarray.DataArray (y: 3)>
3021         array([5.4, 6.6, 7.8])
3022         Dimensions without coordinates: y
3023         """
3024         ds = self._to_temp_dataset().integrate(dim, datetime_unit)
3025         return self._from_temp_dataset(ds)
3026 
3027     # this needs to be at the end, or mypy will confuse with `str`
3028     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names  # noqa
3029     str = property(StringAccessor)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 15:14:53.544 | INFO     | Search completed. Bug locations: [<file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>quantile</method>
<code>
2827     def quantile(
2828         self,
2829         q: Any,
2830         dim: Union[Hashable, Sequence[Hashable], None] = None,
2831         interpolation: str = "linear",
2832         keep_attrs: bool = None,
2833     ) -> "DataArray":
2834         """Compute the qth quantile of the data along the specified dimension.
2835 
2836         Returns the qth quantiles(s) of the array elements.
2837 
2838         Parameters
2839         ----------
2840         q : float in range of [0,1] or array-like of floats
2841             Quantile to compute, which must be between 0 and 1 inclusive.
2842         dim : hashable or sequence of hashable, optional
2843             Dimension(s) over which to apply quantile.
2844         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
2845             This optional parameter specifies the interpolation method to
2846             use when the desired quantile lies between two data points
2847             ``i < j``:
2848 
2849                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is
2850                   the fractional part of the index surrounded by ``i`` and
2851                   ``j``.
2852                 - lower: ``i``.
2853                 - higher: ``j``.
2854                 - nearest: ``i`` or ``j``, whichever is nearest.
2855                 - midpoint: ``(i + j) / 2``.
2856         keep_attrs : bool, optional
2857             If True, the dataset's attributes (`attrs`) will be copied from
2858             the original object to the new one.  If False (default), the new
2859             object will be returned without attributes.
2860 
2861         Returns
2862         -------
2863         quantiles : DataArray
2864             If `q` is a single quantile, then the result
2865             is a scalar. If multiple percentiles are given, first axis of
2866             the result corresponds to the quantile and a quantile dimension
2867             is added to the return array. The other dimensions are the
2868              dimensions that remain after the reduction of the array.
2869 
2870         See Also
2871         --------
2872         numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile
2873         """
2874 
2875         ds = self._to_temp_dataset().quantile(
2876             q, dim=dim, keep_attrs=keep_attrs, interpolation=interpolation
2877         )
2878         return self._from_temp_dataset(ds)

</code><intended_behavior>The method should ensure that when `keep_attrs=True`, the attributes from the original `DataArray` are preserved in the quantile calculation result. The intended fix involves verifying that the return statement `self._from_temp_dataset(ds)` properly re-applies the original attributes to the resulting `DataArray`. If the method delegates attribute handling to `_to_temp_dataset().quantile`, it must ensure the `keep_attrs` parameter effectively preserves attributes throughout the process. A detailed review or patch may involve ensuring `_from_temp_dataset(ds)` or similar subsequent calls respect the `keep_attrs` flag and correctly apply the original attributes to the final returned `DataArray`. This ensures the functionality remains consistent with user expectations and the documented behavior of the `keep_attrs` flag. Understanding the flow of attributes in both `_to_temp_dataset()` and `_from_temp_dataset()` methods is crucial for implementing a bug fix that prevents attribute loss during quantile calculations. The fix requires ensuring that attributes from the original `DataArray` are either: Passed through to temporary representations and computations explicitly. Re-applied from the original `DataArray` to the new `DataArray` instance resulting from quantile computations when `keep_attrs=True`.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
212 class DataArray(AbstractArray, DataWithCoords):
213     """N-dimensional array with labeled coordinates and dimensions.
214 
215     DataArray provides a wrapper around numpy ndarrays that uses labeled
216     dimensions and coordinates to support metadata aware operations. The API is
217     similar to that for the pandas Series or DataFrame, but DataArray objects
218     can have any number of dimensions, and their contents have fixed data
219     types.
220 
221     Additional features over raw numpy arrays:
222 
223     - Apply operations over dimensions by name: ``x.sum('time')``.
224     - Select or assign values by integer location (like numpy): ``x[:10]``
225       or by label (like pandas): ``x.loc['2014-01-01']`` or
226       ``x.sel(time='2014-01-01')``.
227     - Mathematical operations (e.g., ``x - y``) vectorize across multiple
228       dimensions (known in numpy as "broadcasting") based on dimension names,
229       regardless of their original order.
230     - Keep track of arbitrary metadata in the form of a Python dictionary:
231       ``x.attrs``
232     - Convert to a pandas Series: ``x.to_series()``.
233 
234     Getting items from or doing mathematical operations with a DataArray
235     always returns another DataArray.
236 
237     Attributes
238     ----------
239     dims : tuple
240         Dimension names associated with this array.
241     values : np.ndarray
242         Access or modify DataArray values as a numpy array.
243     coords : dict-like
244         Dictionary of DataArray objects that label values along each dimension.
245     name : str or None
246         Name of this array.
247     attrs : OrderedDict
248         Dictionary for holding arbitrary metadata.
249     """
250 
251     __slots__ = ("_accessors", "_coords", "_file_obj", "_name", "_indexes", "_variable")
252 
253     _groupby_cls = groupby.DataArrayGroupBy
254     _rolling_cls = rolling.DataArrayRolling
255     _coarsen_cls = rolling.DataArrayCoarsen
256     _resample_cls = resample.DataArrayResample
257 
258     __default = ReprObject("<default>")
259 
260     dt = property(DatetimeAccessor)
261 
262     def __init__(
263         self,
264         data: Any = dtypes.NA,
265         coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None] = None,
266         dims: Union[Hashable, Sequence[Hashable], None] = None,
267         name: Hashable = None,
268         attrs: Mapping = None,
269         # deprecated parameters
270         encoding=None,
271         # internal parameters
272         indexes=None,
273         fastpath: bool = False,
274     ):
275         """
276         Parameters
277         ----------
278         data : array_like
279             Values for this array. Must be an ``numpy.ndarray``, ndarray like,
280             or castable to an ``ndarray``. If a self-described xarray or pandas
281             object, attempts are made to use this array's metadata to fill in
282             other unspecified arguments. A view of the array's data is used
283             instead of a copy if possible.
284         coords : sequence or dict of array_like objects, optional
285             Coordinates (tick labels) to use for indexing along each dimension.
286             The following notations are accepted:
287 
288             - mapping {dimension name: array-like}
289             - sequence of tuples that are valid arguments for xarray.Variable()
290               - (dims, data)
291               - (dims, data, attrs)
292               - (dims, data, attrs, encoding)
293 
294             Additionally, it is possible to define a coord whose name
295             does not match the dimension name, or a coord based on multiple
296             dimensions, with one of the following notations:
297 
298             - mapping {coord name: DataArray}
299             - mapping {coord name: Variable}
300             - mapping {coord name: (dimension name, array-like)}
301             - mapping {coord name: (tuple of dimension names, array-like)}
302 
303         dims : hashable or sequence of hashable, optional
304             Name(s) of the data dimension(s). Must be either a hashable (only
305             for 1D data) or a sequence of hashables with length equal to the
306             number of dimensions. If this argument is omitted, dimension names
307             are taken from ``coords`` (if possible) and otherwise default to
308             ``['dim_0', ... 'dim_n']``.
309         name : str or None, optional
310             Name of this array.
311         attrs : dict_like or None, optional
312             Attributes to assign to the new instance. By default, an empty
313             attribute dictionary is initialized.
314         """
315         if encoding is not None:
316             warnings.warn(
317                 "The `encoding` argument to `DataArray` is deprecated, and . "
318                 "will be removed in 0.14. "
319                 "Instead, specify the encoding when writing to disk or "
320                 "set the `encoding` attribute directly.",
321                 FutureWarning,
322                 stacklevel=2,
323             )
324         if fastpath:
325             variable = data
326             assert dims is None
327             assert attrs is None
328             assert encoding is None
329         else:
330             # try to fill in arguments from data if they weren't supplied
331             if coords is None:
332 
333                 if isinstance(data, DataArray):
334                     coords = data.coords
335                 elif isinstance(data, pd.Series):
336                     coords = [data.index]
337                 elif isinstance(data, pd.DataFrame):
338                     coords = [data.index, data.columns]
339                 elif isinstance(data, (pd.Index, IndexVariable)):
340                     coords = [data]
341                 elif isinstance(data, pdcompat.Panel):
342                     coords = [data.items, data.major_axis, data.minor_axis]
343 
344             if dims is None:
345                 dims = getattr(data, "dims", getattr(coords, "dims", None))
346             if name is None:
347                 name = getattr(data, "name", None)
348             if attrs is None:
349                 attrs = getattr(data, "attrs", None)
350             if encoding is None:
351                 encoding = getattr(data, "encoding", None)
352 
353             data = _check_data_shape(data, coords, dims)
354             data = as_compatible_data(data)
355             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
356             variable = Variable(dims, data, attrs, encoding, fastpath=True)
357 
358         # These fully describe a DataArray
359         self._variable = variable  # type: Variable
360         assert isinstance(coords, OrderedDict)
361         self._coords = coords  # type: OrderedDict[Any, Variable]
362         self._name = name  # type: Optional[Hashable]
363         self._accessors = None  # type: Optional[Dict[str, Any]]
364 
365         # TODO(shoyer): document this argument, once it becomes part of the
366         # public interface.
367         self._indexes = indexes
368 
369         self._file_obj = None
370 
371     def _replace(
372         self,
373         variable: Variable = None,
374         coords=None,
375         name: Optional[Hashable] = __default,
376     ) -> "DataArray":
377         if variable is None:
378             variable = self.variable
379         if coords is None:
380             coords = self._coords
381         if name is self.__default:
382             name = self.name
383         return type(self)(variable, coords, name=name, fastpath=True)
384 
385     def _replace_maybe_drop_dims(
386         self, variable: Variable, name: Optional[Hashable] = __default
387     ) -> "DataArray":
388         if variable.dims == self.dims and variable.shape == self.shape:
389             coords = self._coords.copy()
390         elif variable.dims == self.dims:
391             # Shape has changed (e.g. from reduce(..., keepdims=True)
392             new_sizes = dict(zip(self.dims, variable.shape))
393             coords = OrderedDict(
394                 (k, v)
395                 for k, v in self._coords.items()
396                 if v.shape == tuple(new_sizes[d] for d in v.dims)
397             )
398         else:
399             allowed_dims = set(variable.dims)
400             coords = OrderedDict(
401                 (k, v) for k, v in self._coords.items() if set(v.dims) <= allowed_dims
402             )
403         return self._replace(variable, coords, name)
404 
405     def _overwrite_indexes(self, indexes: Mapping[Hashable, Any]) -> "DataArray":
406         if not len(indexes):
407             return self
408         coords = self._coords.copy()
409         for name, idx in indexes.items():
410             coords[name] = IndexVariable(name, idx)
411         obj = self._replace(coords=coords)
412 
413         # switch from dimension to level names, if necessary
414         dim_names = {}  # type: Dict[Any, str]
415         for dim, idx in indexes.items():
416             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:
417                 dim_names[dim] = idx.name
418         if dim_names:
419             obj = obj.rename(dim_names)
420         return obj
421 
422     def _to_temp_dataset(self) -> Dataset:
423         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
424 
425     def _from_temp_dataset(
426         self, dataset: Dataset, name: Hashable = __default
427     ) -> "DataArray":
428         variable = dataset._variables.pop(_THIS_ARRAY)
429         coords = dataset._variables
430         return self._replace(variable, coords, name)
431 
432     def _to_dataset_split(self, dim: Hashable) -> Dataset:
433         def subset(dim, label):
434             array = self.loc[{dim: label}]
435             if dim in array.coords:
436                 del array.coords[dim]
437             array.attrs = {}
438             return array
439 
440         variables = OrderedDict(
441             [(label, subset(dim, label)) for label in self.get_index(dim)]
442         )
443         coords = self.coords.to_dataset()
444         if dim in coords:
445             del coords[dim]
446         return Dataset(variables, coords, self.attrs)
447 
448     def _to_dataset_whole(
449         self, name: Hashable = None, shallow_copy: bool = True
450     ) -> Dataset:
451         if name is None:
452             name = self.name
453         if name is None:
454             raise ValueError(
455                 "unable to convert unnamed DataArray to a "
456                 "Dataset without providing an explicit name"
457             )
458         if name in self.coords:
459             raise ValueError(
460                 "cannot create a Dataset from a DataArray with "
461                 "the same name as one of its coordinates"
462             )
463         # use private APIs for speed: this is called by _to_temp_dataset(),
464         # which is used in the guts of a lot of operations (e.g., reindex)
465         variables = self._coords.copy()
466         variables[name] = self.variable
467         if shallow_copy:
468             for k in variables:
469                 variables[k] = variables[k].copy(deep=False)
470         coord_names = set(self._coords)
471         dataset = Dataset._from_vars_and_coord_names(variables, coord_names)
472         return dataset
473 
474     def to_dataset(self, dim: Hashable = None, *, name: Hashable = None) -> Dataset:
475         """Convert a DataArray to a Dataset.
476 
477         Parameters
478         ----------
479         dim : hashable, optional
480             Name of the dimension on this array along which to split this array
481             into separate variables. If not provided, this array is converted
482             into a Dataset of one variable.
483         name : hashable, optional
484             Name to substitute for this array's name. Only valid if ``dim`` is
485             not provided.
486 
487         Returns
488         -------
489         dataset : Dataset
490         """
491         if dim is not None and dim not in self.dims:
492             raise TypeError(
493                 "{} is not a dim. If supplying a ``name``, pass as a kwarg.".format(dim)
494             )
495 
496         if dim is not None:
497             if name is not None:
498                 raise TypeError("cannot supply both dim and name arguments")
499             return self._to_dataset_split(dim)
500         else:
501             return self._to_dataset_whole(name)
502 
503     @property
504     def name(self) -> Optional[Hashable]:
505         """The name of this array.
506         """
507         return self._name
508 
509     @name.setter
510     def name(self, value: Optional[Hashable]) -> None:
511         self._name = value
512 
513     @property
514     def variable(self) -> Variable:
515         """Low level interface to the Variable object for this DataArray."""
516         return self._variable
517 
518     @property
519     def dtype(self) -> np.dtype:
520         return self.variable.dtype
521 
522     @property
523     def shape(self) -> Tuple[int, ...]:
524         return self.variable.shape
525 
526     @property
527     def size(self) -> int:
528         return self.variable.size
529 
530     @property
531     def nbytes(self) -> int:
532         return self.variable.nbytes
533 
534     @property
535     def ndim(self) -> int:
536         return self.variable.ndim
537 
538     def __len__(self) -> int:
539         return len(self.variable)
540 
541     @property
542     def data(self) -> Any:
543         """The array's data as a dask or numpy array
544         """
545         return self.variable.data
546 
547     @data.setter
548     def data(self, value: Any) -> None:
549         self.variable.data = value
550 
551     @property
552     def values(self) -> np.ndarray:
553         """The array's data as a numpy.ndarray"""
554         return self.variable.values
555 
556     @values.setter
557     def values(self, value: Any) -> None:
558         self.variable.values = value
559 
560     @property
561     def _in_memory(self) -> bool:
562         return self.variable._in_memory
563 
564     def to_index(self) -> pd.Index:
565         """Convert this variable to a pandas.Index. Only possible for 1D
566         arrays.
567         """
568         return self.variable.to_index()
569 
570     @property
571     def dims(self) -> Tuple[Hashable, ...]:
572         """Tuple of dimension names associated with this array.
573 
574         Note that the type of this property is inconsistent with
575         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
576         consistently named properties.
577         """
578         return self.variable.dims
579 
580     @dims.setter
581     def dims(self, value):
582         raise AttributeError(
583             "you cannot assign dims on a DataArray. Use "
584             ".rename() or .swap_dims() instead."
585         )
586 
587     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
588         if utils.is_dict_like(key):
589             return key
590         else:
591             key = indexing.expanded_indexer(key, self.ndim)
592             return dict(zip(self.dims, key))
593 
594     @property
595     def _level_coords(self) -> "OrderedDict[Any, Hashable]":
596         """Return a mapping of all MultiIndex levels and their corresponding
597         coordinate name.
598         """
599         level_coords = OrderedDict()  # type: OrderedDict[Any, Hashable]
600 
601         for cname, var in self._coords.items():
602             if var.ndim == 1 and isinstance(var, IndexVariable):
603                 level_names = var.level_names
604                 if level_names is not None:
605                     dim, = var.dims
606                     level_coords.update({lname: dim for lname in level_names})
607         return level_coords
608 
609     def _getitem_coord(self, key):
610         from .dataset import _get_virtual_variable
611 
612         try:
613             var = self._coords[key]
614         except KeyError:
615             dim_sizes = dict(zip(self.dims, self.shape))
616             _, key, var = _get_virtual_variable(
617                 self._coords, key, self._level_coords, dim_sizes
618             )
619 
620         return self._replace_maybe_drop_dims(var, name=key)
621 
622     def __getitem__(self, key: Any) -> "DataArray":
623         if isinstance(key, str):
624             return self._getitem_coord(key)
625         else:
626             # xarray-style array indexing
627             return self.isel(indexers=self._item_key_to_dict(key))
628 
629     def __setitem__(self, key: Any, value: Any) -> None:
630         if isinstance(key, str):
631             self.coords[key] = value
632         else:
633             # Coordinates in key, value and self[key] should be consistent.
634             # TODO Coordinate consistency in key is checked here, but it
635             # causes unnecessary indexing. It should be optimized.
636             obj = self[key]
637             if isinstance(value, DataArray):
638                 assert_coordinate_consistent(value, obj.coords.variables)
639             # DataArray key -> Variable key
640             key = {
641                 k: v.variable if isinstance(v, DataArray) else v
642                 for k, v in self._item_key_to_dict(key).items()
643             }
644             self.variable[key] = value
645 
646     def __delitem__(self, key: Any) -> None:
647         del self.coords[key]
648 
649     @property
650     def _attr_sources(self) -> List[Mapping[Hashable, Any]]:
651         """List of places to look-up items for attribute-style access
652         """
653         return self._item_sources + [self.attrs]
654 
655     @property
656     def _item_sources(self) -> List[Mapping[Hashable, Any]]:
657         """List of places to look-up items for key-completion
658         """
659         return [
660             self.coords,
661             {d: self.coords[d] for d in self.dims},
662             LevelCoordinatesSource(self),
663         ]
664 
665     def __contains__(self, key: Any) -> bool:
666         return key in self.data
667 
668     @property
669     def loc(self) -> _LocIndexer:
670         """Attribute for location based indexing like pandas.
671         """
672         return _LocIndexer(self)
673 
674     @property
675     def attrs(self) -> "OrderedDict[Any, Any]":
676         """Dictionary storing arbitrary metadata with this array."""
677         return self.variable.attrs
678 
679     @attrs.setter
680     def attrs(self, value: Mapping[Hashable, Any]) -> None:
681         # Disable type checking to work around mypy bug - see mypy#4167
682         self.variable.attrs = value  # type: ignore
683 
684     @property
685     def encoding(self) -> "OrderedDict[Any, Any]":
686         """Dictionary of format-specific settings for how this array should be
687         serialized."""
688         return self.variable.encoding
689 
690     @encoding.setter
691     def encoding(self, value: Mapping[Hashable, Any]) -> None:
692         self.variable.encoding = value
693 
694     @property
695     def indexes(self) -> Indexes:
696         """Mapping of pandas.Index objects used for label based indexing
697         """
698         if self._indexes is None:
699             self._indexes = default_indexes(self._coords, self.dims)
700         return Indexes(self._indexes)
701 
702     @property
703     def coords(self) -> DataArrayCoordinates:
704         """Dictionary-like container of coordinate arrays.
705         """
706         return DataArrayCoordinates(self)
707 
708     def reset_coords(
709         self,
710         names: Union[Iterable[Hashable], Hashable, None] = None,
711         drop: bool = False,
712         inplace: bool = None,
713     ) -> Union[None, "DataArray", Dataset]:
714         """Given names of coordinates, reset them to become variables.
715 
716         Parameters
717         ----------
718         names : hashable or iterable of hashables, optional
719             Name(s) of non-index coordinates in this dataset to reset into
720             variables. By default, all non-index coordinates are reset.
721         drop : bool, optional
722             If True, remove coordinates instead of converting them into
723             variables.
724 
725         Returns
726         -------
727         Dataset, or DataArray if ``drop == True``
728         """
729         _check_inplace(inplace)
730         if names is None:
731             names = set(self.coords) - set(self.dims)
732         dataset = self.coords.to_dataset().reset_coords(names, drop)
733         if drop:
734             return self._replace(coords=dataset._variables)
735         else:
736             if self.name is None:
737                 raise ValueError(
738                     "cannot reset_coords with drop=False on an unnamed DataArrray"
739                 )
740             dataset[self.name] = self.variable
741             return dataset
742 
743     def __dask_graph__(self):
744         return self._to_temp_dataset().__dask_graph__()
745 
746     def __dask_keys__(self):
747         return self._to_temp_dataset().__dask_keys__()
748 
749     def __dask_layers__(self):
750         return self._to_temp_dataset().__dask_layers__()
751 
752     @property
753     def __dask_optimize__(self):
754         return self._to_temp_dataset().__dask_optimize__
755 
756     @property
757     def __dask_scheduler__(self):
758         return self._to_temp_dataset().__dask_scheduler__
759 
760     def __dask_postcompute__(self):
761         func, args = self._to_temp_dataset().__dask_postcompute__()
762         return self._dask_finalize, (func, args, self.name)
763 
764     def __dask_postpersist__(self):
765         func, args = self._to_temp_dataset().__dask_postpersist__()
766         return self._dask_finalize, (func, args, self.name)
767 
768     @staticmethod
769     def _dask_finalize(results, func, args, name):
770         ds = func(results, *args)
771         variable = ds._variables.pop(_THIS_ARRAY)
772         coords = ds._variables
773         return DataArray(variable, coords, name=name, fastpath=True)
774 
775     def load(self, **kwargs) -> "DataArray":
776         """Manually trigger loading of this array's data from disk or a
777         remote source into memory and return this array.
778 
779         Normally, it should not be necessary to call this method in user code,
780         because all xarray functions should either work on deferred data or
781         load data automatically. However, this method can be necessary when
782         working with many file objects on disk.
783 
784         Parameters
785         ----------
786         **kwargs : dict
787             Additional keyword arguments passed on to ``dask.array.compute``.
788 
789         See Also
790         --------
791         dask.array.compute
792         """
793         ds = self._to_temp_dataset().load(**kwargs)
794         new = self._from_temp_dataset(ds)
795         self._variable = new._variable
796         self._coords = new._coords
797         return self
798 
799     def compute(self, **kwargs) -> "DataArray":
800         """Manually trigger loading of this array's data from disk or a
801         remote source into memory and return a new array. The original is
802         left unaltered.
803 
804         Normally, it should not be necessary to call this method in user code,
805         because all xarray functions should either work on deferred data or
806         load data automatically. However, this method can be necessary when
807         working with many file objects on disk.
808 
809         Parameters
810         ----------
811         **kwargs : dict
812             Additional keyword arguments passed on to ``dask.array.compute``.
813 
814         See Also
815         --------
816         dask.array.compute
817         """
818         new = self.copy(deep=False)
819         return new.load(**kwargs)
820 
821     def persist(self, **kwargs) -> "DataArray":
822         """ Trigger computation in constituent dask arrays
823 
824         This keeps them as dask arrays but encourages them to keep data in
825         memory.  This is particularly useful when on a distributed machine.
826         When on a single machine consider using ``.compute()`` instead.
827 
828         Parameters
829         ----------
830         **kwargs : dict
831             Additional keyword arguments passed on to ``dask.persist``.
832 
833         See Also
834         --------
835         dask.persist
836         """
837         ds = self._to_temp_dataset().persist(**kwargs)
838         return self._from_temp_dataset(ds)
839 
840     def copy(self, deep: bool = True, data: Any = None) -> "DataArray":
841         """Returns a copy of this array.
842 
843         If `deep=True`, a deep copy is made of the data array.
844         Otherwise, a shallow copy is made, so each variable in the new
845         array's dataset is also a variable in this array's dataset.
846 
847         Use `data` to create a new object with the same structure as
848         original but entirely new data.
849 
850         Parameters
851         ----------
852         deep : bool, optional
853             Whether the data array and its coordinates are loaded into memory
854             and copied onto the new object. Default is True.
855         data : array_like, optional
856             Data to use in the new object. Must have same shape as original.
857             When `data` is used, `deep` is ignored for all data variables,
858             and only used for coords.
859 
860         Returns
861         -------
862         object : DataArray
863             New object with dimensions, attributes, coordinates, name,
864             encoding, and optionally data copied from original.
865 
866         Examples
867         --------
868 
869         Shallow versus deep copy
870 
871         >>> array = xr.DataArray([1, 2, 3], dims='x',
872         ...                      coords={'x': ['a', 'b', 'c']})
873         >>> array.copy()
874         <xarray.DataArray (x: 3)>
875         array([1, 2, 3])
876         Coordinates:
877         * x        (x) <U1 'a' 'b' 'c'
878         >>> array_0 = array.copy(deep=False)
879         >>> array_0[0] = 7
880         >>> array_0
881         <xarray.DataArray (x: 3)>
882         array([7, 2, 3])
883         Coordinates:
884         * x        (x) <U1 'a' 'b' 'c'
885         >>> array
886         <xarray.DataArray (x: 3)>
887         array([7, 2, 3])
888         Coordinates:
889         * x        (x) <U1 'a' 'b' 'c'
890 
891         Changing the data using the ``data`` argument maintains the
892         structure of the original object, but with the new data. Original
893         object is unaffected.
894 
895         >>> array.copy(data=[0.1, 0.2, 0.3])
896         <xarray.DataArray (x: 3)>
897         array([ 0.1,  0.2,  0.3])
898         Coordinates:
899         * x        (x) <U1 'a' 'b' 'c'
900         >>> array
901         <xarray.DataArray (x: 3)>
902         array([1, 2, 3])
903         Coordinates:
904         * x        (x) <U1 'a' 'b' 'c'
905 
906         See also
907         --------
908         pandas.DataFrame.copy
909         """
910         variable = self.variable.copy(deep=deep, data=data)
911         coords = OrderedDict((k, v.copy(deep=deep)) for k, v in self._coords.items())
912         return self._replace(variable, coords)
913 
914     def __copy__(self) -> "DataArray":
915         return self.copy(deep=False)
916 
917     def __deepcopy__(self, memo=None) -> "DataArray":
918         # memo does nothing but is required for compatibility with
919         # copy.deepcopy
920         return self.copy(deep=True)
921 
922     # mutable objects should not be hashable
923     # https://github.com/python/mypy/issues/4266
924     __hash__ = None  # type: ignore
925 
926     @property
927     def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:
928         """Block dimensions for this array's data or None if it's not a dask
929         array.
930         """
931         return self.variable.chunks
932 
933     def chunk(
934         self,
935         chunks: Union[
936             None,
937             Number,
938             Tuple[Number, ...],
939             Tuple[Tuple[Number, ...], ...],
940             Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]],
941         ] = None,
942         name_prefix: str = "xarray-",
943         token: str = None,
944         lock: bool = False,
945     ) -> "DataArray":
946         """Coerce this array's data into a dask arrays with the given chunks.
947 
948         If this variable is a non-dask array, it will be converted to dask
949         array. If it's a dask array, it will be rechunked to the given chunk
950         sizes.
951 
952         If neither chunks is not provided for one or more dimensions, chunk
953         sizes along that dimension will not be updated; non-dask arrays will be
954         converted into dask arrays with a single block.
955 
956         Parameters
957         ----------
958         chunks : int, tuple or mapping, optional
959             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
960             ``{'x': 5, 'y': 5}``.
961         name_prefix : str, optional
962             Prefix for the name of the new dask array.
963         token : str, optional
964             Token uniquely identifying this array.
965         lock : optional
966             Passed on to :py:func:`dask.array.from_array`, if the array is not
967             already as dask array.
968 
969         Returns
970         -------
971         chunked : xarray.DataArray
972         """
973         if isinstance(chunks, (tuple, list)):
974             chunks = dict(zip(self.dims, chunks))
975 
976         ds = self._to_temp_dataset().chunk(
977             chunks, name_prefix=name_prefix, token=token, lock=lock
978         )
979         return self._from_temp_dataset(ds)
980 
981     def isel(
982         self,
983         indexers: Mapping[Hashable, Any] = None,
984         drop: bool = False,
985         **indexers_kwargs: Any
986     ) -> "DataArray":
987         """Return a new DataArray whose data is given by integer indexing
988         along the specified dimension(s).
989 
990         See Also
991         --------
992         Dataset.isel
993         DataArray.sel
994         """
995         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
996         ds = self._to_temp_dataset().isel(drop=drop, indexers=indexers)
997         return self._from_temp_dataset(ds)
998 
999     def sel(
1000         self,
1001         indexers: Mapping[Hashable, Any] = None,
1002         method: str = None,
1003         tolerance=None,
1004         drop: bool = False,
1005         **indexers_kwargs: Any
1006     ) -> "DataArray":
1007         """Return a new DataArray whose data is given by selecting index
1008         labels along the specified dimension(s).
1009 
1010         .. warning::
1011 
1012           Do not try to assign values when using any of the indexing methods
1013           ``isel`` or ``sel``::
1014 
1015             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1016             # DO NOT do this
1017             da.isel(x=[0, 1, 2])[1] = -1
1018 
1019           Assigning values with the chained indexing using ``.sel`` or
1020           ``.isel`` fails silently.
1021 
1022         See Also
1023         --------
1024         Dataset.sel
1025         DataArray.isel
1026 
1027         """
1028         ds = self._to_temp_dataset().sel(
1029             indexers=indexers,
1030             drop=drop,
1031             method=method,
1032             tolerance=tolerance,
1033             **indexers_kwargs
1034         )
1035         return self._from_temp_dataset(ds)
1036 
1037     def head(
1038         self, indexers: Mapping[Hashable, Any] = None, **indexers_kwargs: Any
1039     ) -> "DataArray":
1040         """Return a new DataArray whose data is given by the the first `n`
1041         values along the specified dimension(s).
1042 
1043         See Also
1044         --------
1045         Dataset.head
1046         DataArray.tail
1047         DataArray.thin
1048         """
1049 
1050         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "head")
1051         ds = self._to_temp_dataset().head(indexers=indexers)
1052         return self._from_temp_dataset(ds)
1053 
1054     def tail(
1055         self, indexers: Mapping[Hashable, Any] = None, **indexers_kwargs: Any
1056     ) -> "DataArray":
1057         """Return a new DataArray whose data is given by the the last `n`
1058         values along the specified dimension(s).
1059 
1060         See Also
1061         --------
1062         Dataset.tail
1063         DataArray.head
1064         DataArray.thin
1065         """
1066         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "tail")
1067         ds = self._to_temp_dataset().tail(indexers=indexers)
1068         return self._from_temp_dataset(ds)
1069 
1070     def thin(
1071         self, indexers: Mapping[Hashable, Any] = None, **indexers_kwargs: Any
1072     ) -> "DataArray":
1073         """Return a new DataArray whose data is given by each `n` value
1074         along the specified dimension(s).
1075 
1076         See Also
1077         --------
1078         Dataset.thin
1079         DataArray.head
1080         DataArray.tail
1081         """
1082         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "thin")
1083         ds = self._to_temp_dataset().thin(indexers=indexers)
1084         return self._from_temp_dataset(ds)
1085 
1086     def broadcast_like(
1087         self, other: Union["DataArray", Dataset], exclude: Iterable[Hashable] = None
1088     ) -> "DataArray":
1089         """Broadcast this DataArray against another Dataset or DataArray.
1090 
1091         This is equivalent to xr.broadcast(other, self)[1]
1092 
1093         xarray objects are broadcast against each other in arithmetic
1094         operations, so this method is not be necessary for most uses.
1095 
1096         If no change is needed, the input data is returned to the output
1097         without being copied.
1098 
1099         If new coords are added by the broadcast, their values are
1100         NaN filled.
1101 
1102         Parameters
1103         ----------
1104         other : Dataset or DataArray
1105             Object against which to broadcast this array.
1106         exclude : iterable of hashable, optional
1107             Dimensions that must not be broadcasted
1108 
1109         Returns
1110         -------
1111         new_da: xr.DataArray
1112 
1113         Examples
1114         --------
1115 
1116         >>> arr1
1117         <xarray.DataArray (x: 2, y: 3)>
1118         array([[0.840235, 0.215216, 0.77917 ],
1119                [0.726351, 0.543824, 0.875115]])
1120         Coordinates:
1121           * x        (x) <U1 'a' 'b'
1122           * y        (y) <U1 'a' 'b' 'c'
1123         >>> arr2
1124         <xarray.DataArray (x: 3, y: 2)>
1125         array([[0.612611, 0.125753],
1126                [0.853181, 0.948818],
1127                [0.180885, 0.33363 ]])
1128         Coordinates:
1129           * x        (x) <U1 'a' 'b' 'c'
1130           * y        (y) <U1 'a' 'b'
1131         >>> arr1.broadcast_like(arr2)
1132         <xarray.DataArray (x: 3, y: 3)>
1133         array([[0.840235, 0.215216, 0.77917 ],
1134                [0.726351, 0.543824, 0.875115],
1135                [     nan,      nan,      nan]])
1136         Coordinates:
1137           * x        (x) object 'a' 'b' 'c'
1138           * y        (y) object 'a' 'b' 'c'
1139         """
1140         if exclude is None:
1141             exclude = set()
1142         else:
1143             exclude = set(exclude)
1144         args = align(other, self, join="outer", copy=False, exclude=exclude)
1145 
1146         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1147 
1148         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
1149 
1150     def reindex_like(
1151         self,
1152         other: Union["DataArray", Dataset],
1153         method: str = None,
1154         tolerance=None,
1155         copy: bool = True,
1156         fill_value=dtypes.NA,
1157     ) -> "DataArray":
1158         """Conform this object onto the indexes of another object, filling in
1159         missing values with ``fill_value``. The default fill value is NaN.
1160 
1161         Parameters
1162         ----------
1163         other : Dataset or DataArray
1164             Object with an 'indexes' attribute giving a mapping from dimension
1165             names to pandas.Index objects, which provides coordinates upon
1166             which to index the variables in this dataset. The indexes on this
1167             other object need not be the same as the indexes on this
1168             dataset. Any mis-matched index values will be filled in with
1169             NaN, and any mis-matched dimension names will simply be ignored.
1170         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1171             Method to use for filling index values from other not found on this
1172             data array:
1173 
1174             * None (default): don't fill gaps
1175             * pad / ffill: propagate last valid index value forward
1176             * backfill / bfill: propagate next valid index value backward
1177             * nearest: use nearest valid index value (requires pandas>=0.16)
1178         tolerance : optional
1179             Maximum distance between original and new labels for inexact
1180             matches. The values of the index at the matching locations must
1181             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1182             Requires pandas>=0.17.
1183         copy : bool, optional
1184             If ``copy=True``, data in the return value is always copied. If
1185             ``copy=False`` and reindexing is unnecessary, or can be performed
1186             with only slice operations, then the output may share memory with
1187             the input. In either case, a new xarray object is always returned.
1188         fill_value : scalar, optional
1189             Value to use for newly missing values
1190 
1191         Returns
1192         -------
1193         reindexed : DataArray
1194             Another dataset array, with this array's data but coordinates from
1195             the other object.
1196 
1197         See Also
1198         --------
1199         DataArray.reindex
1200         align
1201         """
1202         indexers = reindex_like_indexers(self, other)
1203         return self.reindex(
1204             indexers=indexers,
1205             method=method,
1206             tolerance=tolerance,
1207             copy=copy,
1208             fill_value=fill_value,
1209         )
1210 
1211     def reindex(
1212         self,
1213         indexers: Mapping[Hashable, Any] = None,
1214         method: str = None,
1215         tolerance=None,
1216         copy: bool = True,
1217         fill_value=dtypes.NA,
1218         **indexers_kwargs: Any
1219     ) -> "DataArray":
1220         """Conform this object onto the indexes of another object, filling in
1221         missing values with ``fill_value``. The default fill value is NaN.
1222 
1223         Parameters
1224         ----------
1225         indexers : dict, optional
1226             Dictionary with keys given by dimension names and values given by
1227             arrays of coordinates tick labels. Any mis-matched coordinate
1228             values will be filled in with NaN, and any mis-matched dimension
1229             names will simply be ignored.
1230             One of indexers or indexers_kwargs must be provided.
1231         copy : bool, optional
1232             If ``copy=True``, data in the return value is always copied. If
1233             ``copy=False`` and reindexing is unnecessary, or can be performed
1234             with only slice operations, then the output may share memory with
1235             the input. In either case, a new xarray object is always returned.
1236         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1237             Method to use for filling index values in ``indexers`` not found on
1238             this data array:
1239 
1240             * None (default): don't fill gaps
1241             * pad / ffill: propagate last valid index value forward
1242             * backfill / bfill: propagate next valid index value backward
1243             * nearest: use nearest valid index value (requires pandas>=0.16)
1244         tolerance : optional
1245             Maximum distance between original and new labels for inexact
1246             matches. The values of the index at the matching locations must
1247             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1248         fill_value : scalar, optional
1249             Value to use for newly missing values
1250         **indexers_kwarg : {dim: indexer, ...}, optional
1251             The keyword arguments form of ``indexers``.
1252             One of indexers or indexers_kwargs must be provided.
1253 
1254         Returns
1255         -------
1256         reindexed : DataArray
1257             Another dataset array, with this array's data but replaced
1258             coordinates.
1259 
1260         See Also
1261         --------
1262         DataArray.reindex_like
1263         align
1264         """
1265         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
1266         ds = self._to_temp_dataset().reindex(
1267             indexers=indexers,
1268             method=method,
1269             tolerance=tolerance,
1270             copy=copy,
1271             fill_value=fill_value,
1272         )
1273         return self._from_temp_dataset(ds)
1274 
1275     def interp(
1276         self,
1277         coords: Mapping[Hashable, Any] = None,
1278         method: str = "linear",
1279         assume_sorted: bool = False,
1280         kwargs: Mapping[str, Any] = None,
1281         **coords_kwargs: Any
1282     ) -> "DataArray":
1283         """ Multidimensional interpolation of variables.
1284 
1285         coords : dict, optional
1286             Mapping from dimension names to the new coordinates.
1287             new coordinate can be an scalar, array-like or DataArray.
1288             If DataArrays are passed as new coordates, their dimensions are
1289             used for the broadcasting.
1290         method: {'linear', 'nearest'} for multidimensional array,
1291             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
1292             for 1-dimensional array.
1293         assume_sorted: boolean, optional
1294             If False, values of x can be in any order and they are sorted
1295             first. If True, x has to be an array of monotonically increasing
1296             values.
1297         kwargs: dictionary
1298             Additional keyword passed to scipy's interpolator.
1299         **coords_kwarg : {dim: coordinate, ...}, optional
1300             The keyword arguments form of ``coords``.
1301             One of coords or coords_kwargs must be provided.
1302 
1303         Returns
1304         -------
1305         interpolated: xr.DataArray
1306             New dataarray on the new coordinates.
1307 
1308         Notes
1309         -----
1310         scipy is required.
1311 
1312         See Also
1313         --------
1314         scipy.interpolate.interp1d
1315         scipy.interpolate.interpn
1316 
1317         Examples
1318         --------
1319         >>> da = xr.DataArray([1, 3], [('x', np.arange(2))])
1320         >>> da.interp(x=0.5)
1321         <xarray.DataArray ()>
1322         array(2.0)
1323         Coordinates:
1324             x        float64 0.5
1325         """
1326         if self.dtype.kind not in "uifc":
1327             raise TypeError(
1328                 "interp only works for a numeric type array. "
1329                 "Given {}.".format(self.dtype)
1330             )
1331         ds = self._to_temp_dataset().interp(
1332             coords,
1333             method=method,
1334             kwargs=kwargs,
1335             assume_sorted=assume_sorted,
1336             **coords_kwargs
1337         )
1338         return self._from_temp_dataset(ds)
1339 
1340     def interp_like(
1341         self,
1342         other: Union["DataArray", Dataset],
1343         method: str = "linear",
1344         assume_sorted: bool = False,
1345         kwargs: Mapping[str, Any] = None,
1346     ) -> "DataArray":
1347         """Interpolate this object onto the coordinates of another object,
1348         filling out of range values with NaN.
1349 
1350         Parameters
1351         ----------
1352         other : Dataset or DataArray
1353             Object with an 'indexes' attribute giving a mapping from dimension
1354             names to an 1d array-like, which provides coordinates upon
1355             which to index the variables in this dataset.
1356         method: string, optional.
1357             {'linear', 'nearest'} for multidimensional array,
1358             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
1359             for 1-dimensional array. 'linear' is used by default.
1360         assume_sorted: boolean, optional
1361             If False, values of coordinates that are interpolated over can be
1362             in any order and they are sorted first. If True, interpolated
1363             coordinates are assumed to be an array of monotonically increasing
1364             values.
1365         kwargs: dictionary, optional
1366             Additional keyword passed to scipy's interpolator.
1367 
1368         Returns
1369         -------
1370         interpolated: xr.DataArray
1371             Another dataarray by interpolating this dataarray's data along the
1372             coordinates of the other object.
1373 
1374         Notes
1375         -----
1376         scipy is required.
1377         If the dataarray has object-type coordinates, reindex is used for these
1378         coordinates instead of the interpolation.
1379 
1380         See Also
1381         --------
1382         DataArray.interp
1383         DataArray.reindex_like
1384         """
1385         if self.dtype.kind not in "uifc":
1386             raise TypeError(
1387                 "interp only works for a numeric type array. "
1388                 "Given {}.".format(self.dtype)
1389             )
1390         ds = self._to_temp_dataset().interp_like(
1391             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
1392         )
1393         return self._from_temp_dataset(ds)
1394 
1395     def rename(
1396         self,
1397         new_name_or_name_dict: Union[Hashable, Mapping[Hashable, Hashable]] = None,
1398         **names: Hashable
1399     ) -> "DataArray":
1400         """Returns a new DataArray with renamed coordinates or a new name.
1401 
1402         Parameters
1403         ----------
1404         new_name_or_name_dict : str or dict-like, optional
1405             If the argument is dict-like, it used as a mapping from old
1406             names to new names for coordinates. Otherwise, use the argument
1407             as the new name for this array.
1408         **names: hashable, optional
1409             The keyword arguments form of a mapping from old names to
1410             new names for coordinates.
1411             One of new_name_or_name_dict or names must be provided.
1412 
1413         Returns
1414         -------
1415         renamed : DataArray
1416             Renamed array or array with renamed coordinates.
1417 
1418         See Also
1419         --------
1420         Dataset.rename
1421         DataArray.swap_dims
1422         """
1423         if names or utils.is_dict_like(new_name_or_name_dict):
1424             new_name_or_name_dict = cast(
1425                 Mapping[Hashable, Hashable], new_name_or_name_dict
1426             )
1427             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
1428             dataset = self._to_temp_dataset().rename(name_dict)
1429             return self._from_temp_dataset(dataset)
1430         else:
1431             new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)
1432             return self._replace(name=new_name_or_name_dict)
1433 
1434     def swap_dims(self, dims_dict: Mapping[Hashable, Hashable]) -> "DataArray":
1435         """Returns a new DataArray with swapped dimensions.
1436 
1437         Parameters
1438         ----------
1439         dims_dict : dict-like
1440             Dictionary whose keys are current dimension names and whose values
1441             are new names. Each value must already be a coordinate on this
1442             array.
1443 
1444         Returns
1445         -------
1446         renamed : Dataset
1447             DataArray with swapped dimensions.
1448 
1449         See Also
1450         --------
1451 
1452         DataArray.rename
1453         Dataset.swap_dims
1454         """
1455         ds = self._to_temp_dataset().swap_dims(dims_dict)
1456         return self._from_temp_dataset(ds)
1457 
1458     def expand_dims(
1459         self,
1460         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,
1461         axis=None,
1462         **dim_kwargs: Any
1463     ) -> "DataArray":
1464         """Return a new object with an additional axis (or axes) inserted at
1465         the corresponding position in the array shape. The new object is a
1466         view into the underlying array, not a copy.
1467 
1468 
1469         If dim is already a scalar coordinate, it will be promoted to a 1D
1470         coordinate consisting of a single value.
1471 
1472         Parameters
1473         ----------
1474         dim : hashable, sequence of hashable, dict, or None
1475             Dimensions to include on the new variable.
1476             If provided as str or sequence of str, then dimensions are inserted
1477             with length 1. If provided as a dict, then the keys are the new
1478             dimensions and the values are either integers (giving the length of
1479             the new dimensions) or sequence/ndarray (giving the coordinates of
1480             the new dimensions). **WARNING** for python 3.5, if ``dim`` is
1481             dict-like, then it must be an ``OrderedDict``. This is to ensure
1482             that the order in which the dims are given is maintained.
1483         axis : integer, list (or tuple) of integers, or None
1484             Axis position(s) where new axis is to be inserted (position(s) on
1485             the result array). If a list (or tuple) of integers is passed,
1486             multiple axes are inserted. In this case, dim arguments should be
1487             same length list. If axis=None is passed, all the axes will be
1488             inserted to the start of the result array.
1489         **dim_kwargs : int or sequence/ndarray
1490             The keywords are arbitrary dimensions being inserted and the values
1491             are either the lengths of the new dims (if int is given), or their
1492             coordinates. Note, this is an alternative to passing a dict to the
1493             dim kwarg and will only be used if dim is None. **WARNING** for
1494             python 3.5 ``dim_kwargs`` is not available.
1495 
1496         Returns
1497         -------
1498         expanded : same type as caller
1499             This object, but with an additional dimension(s).
1500         """
1501         if isinstance(dim, int):
1502             raise TypeError("dim should be hashable or sequence/mapping of hashables")
1503         elif isinstance(dim, Sequence) and not isinstance(dim, str):
1504             if len(dim) != len(set(dim)):
1505                 raise ValueError("dims should not contain duplicate values.")
1506             dim = OrderedDict((d, 1) for d in dim)
1507         elif dim is not None and not isinstance(dim, Mapping):
1508             dim = OrderedDict(((cast(Hashable, dim), 1),))
1509 
1510         # TODO: get rid of the below code block when python 3.5 is no longer
1511         #   supported.
1512         python36_plus = sys.version_info[0] == 3 and sys.version_info[1] > 5
1513         not_ordereddict = dim is not None and not isinstance(dim, OrderedDict)
1514         if not python36_plus and not_ordereddict:
1515             raise TypeError("dim must be an OrderedDict for python <3.6")
1516         elif not python36_plus and dim_kwargs:
1517             raise ValueError("dim_kwargs isn't available for python <3.6")
1518         dim_kwargs = OrderedDict(dim_kwargs)
1519 
1520         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
1521         ds = self._to_temp_dataset().expand_dims(dim, axis)
1522         return self._from_temp_dataset(ds)
1523 
1524     def set_index(
1525         self,
1526         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
1527         append: bool = False,
1528         inplace: bool = None,
1529         **indexes_kwargs: Union[Hashable, Sequence[Hashable]]
1530     ) -> Optional["DataArray"]:
1531         """Set DataArray (multi-)indexes using one or more existing
1532         coordinates.
1533 
1534         Parameters
1535         ----------
1536         indexes : {dim: index, ...}
1537             Mapping from names matching dimensions and values given
1538             by (lists of) the names of existing coordinates or variables to set
1539             as new (multi-)index.
1540         append : bool, optional
1541             If True, append the supplied index(es) to the existing index(es).
1542             Otherwise replace the existing index(es) (default).
1543         **indexes_kwargs: optional
1544             The keyword arguments form of ``indexes``.
1545             One of indexes or indexes_kwargs must be provided.
1546 
1547         Returns
1548         -------
1549         obj : DataArray
1550             Another DataArray, with this data but replaced coordinates.
1551 
1552         Example
1553         -------
1554         >>> arr = xr.DataArray(data=np.ones((2, 3)),
1555         ...                    dims=['x', 'y'],
1556         ...                    coords={'x':
1557         ...                        range(2), 'y':
1558         ...                        range(3), 'a': ('x', [3, 4])
1559         ...                    })
1560         >>> arr
1561         <xarray.DataArray (x: 2, y: 3)>
1562         array([[1., 1., 1.],
1563                [1., 1., 1.]])
1564         Coordinates:
1565           * x        (x) int64 0 1
1566           * y        (y) int64 0 1 2
1567             a        (x) int64 3 4
1568         >>> arr.set_index(x='a')
1569         <xarray.DataArray (x: 2, y: 3)>
1570         array([[1., 1., 1.],
1571                [1., 1., 1.]])
1572         Coordinates:
1573           * x        (x) int64 3 4
1574           * y        (y) int64 0 1 2
1575 
1576         See Also
1577         --------
1578         DataArray.reset_index
1579         """
1580         _check_inplace(inplace)
1581         indexes = either_dict_or_kwargs(indexes, indexes_kwargs, "set_index")
1582         coords, _ = merge_indexes(indexes, self._coords, set(), append=append)
1583         return self._replace(coords=coords)
1584 
1585     def reset_index(
1586         self,
1587         dims_or_levels: Union[Hashable, Sequence[Hashable]],
1588         drop: bool = False,
1589         inplace: bool = None,
1590     ) -> Optional["DataArray"]:
1591         """Reset the specified index(es) or multi-index level(s).
1592 
1593         Parameters
1594         ----------
1595         dims_or_levels : hashable or sequence of hashables
1596             Name(s) of the dimension(s) and/or multi-index level(s) that will
1597             be reset.
1598         drop : bool, optional
1599             If True, remove the specified indexes and/or multi-index levels
1600             instead of extracting them as new coordinates (default: False).
1601 
1602         Returns
1603         -------
1604         obj : DataArray
1605             Another dataarray, with this dataarray's data but replaced
1606             coordinates.
1607 
1608         See Also
1609         --------
1610         DataArray.set_index
1611         """
1612         _check_inplace(inplace)
1613         coords, _ = split_indexes(
1614             dims_or_levels, self._coords, set(), self._level_coords, drop=drop
1615         )
1616         return self._replace(coords=coords)
1617 
1618     def reorder_levels(
1619         self,
1620         dim_order: Mapping[Hashable, Sequence[int]] = None,
1621         inplace: bool = None,
1622         **dim_order_kwargs: Sequence[int]
1623     ) -> "DataArray":
1624         """Rearrange index levels using input order.
1625 
1626         Parameters
1627         ----------
1628         dim_order : optional
1629             Mapping from names matching dimensions and values given
1630             by lists representing new level orders. Every given dimension
1631             must have a multi-index.
1632         **dim_order_kwargs: optional
1633             The keyword arguments form of ``dim_order``.
1634             One of dim_order or dim_order_kwargs must be provided.
1635 
1636         Returns
1637         -------
1638         obj : DataArray
1639             Another dataarray, with this dataarray's data but replaced
1640             coordinates.
1641         """
1642         _check_inplace(inplace)
1643         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
1644         replace_coords = {}
1645         for dim, order in dim_order.items():
1646             coord = self._coords[dim]
1647             index = coord.to_index()
1648             if not isinstance(index, pd.MultiIndex):
1649                 raise ValueError("coordinate %r has no MultiIndex" % dim)
1650             replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))
1651         coords = self._coords.copy()
1652         coords.update(replace_coords)
1653         return self._replace(coords=coords)
1654 
1655     def stack(
1656         self,
1657         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
1658         **dimensions_kwargs: Sequence[Hashable]
1659     ) -> "DataArray":
1660         """
1661         Stack any number of existing dimensions into a single new dimension.
1662 
1663         New dimensions will be added at the end, and the corresponding
1664         coordinate variables will be combined into a MultiIndex.
1665 
1666         Parameters
1667         ----------
1668         dimensions : Mapping of the form new_name=(dim1, dim2, ...)
1669             Names of new dimensions, and the existing dimensions that they
1670             replace.
1671         **dimensions_kwargs:
1672             The keyword arguments form of ``dimensions``.
1673             One of dimensions or dimensions_kwargs must be provided.
1674 
1675         Returns
1676         -------
1677         stacked : DataArray
1678             DataArray with stacked data.
1679 
1680         Examples
1681         --------
1682 
1683         >>> arr = DataArray(np.arange(6).reshape(2, 3),
1684         ...                 coords=[('x', ['a', 'b']), ('y', [0, 1, 2])])
1685         >>> arr
1686         <xarray.DataArray (x: 2, y: 3)>
1687         array([[0, 1, 2],
1688                [3, 4, 5]])
1689         Coordinates:
1690           * x        (x) |S1 'a' 'b'
1691           * y        (y) int64 0 1 2
1692         >>> stacked = arr.stack(z=('x', 'y'))
1693         >>> stacked.indexes['z']
1694         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1695                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],
1696                    names=['x', 'y'])
1697 
1698         See also
1699         --------
1700         DataArray.unstack
1701         """
1702         ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)
1703         return self._from_temp_dataset(ds)
1704 
1705     def unstack(
1706         self, dim: Union[Hashable, Sequence[Hashable], None] = None
1707     ) -> "DataArray":
1708         """
1709         Unstack existing dimensions corresponding to MultiIndexes into
1710         multiple new dimensions.
1711 
1712         New dimensions will be added at the end.
1713 
1714         Parameters
1715         ----------
1716         dim : hashable or sequence of hashable, optional
1717             Dimension(s) over which to unstack. By default unstacks all
1718             MultiIndexes.
1719 
1720         Returns
1721         -------
1722         unstacked : DataArray
1723             Array with unstacked data.
1724 
1725         Examples
1726         --------
1727 
1728         >>> arr = DataArray(np.arange(6).reshape(2, 3),
1729         ...                 coords=[('x', ['a', 'b']), ('y', [0, 1, 2])])
1730         >>> arr
1731         <xarray.DataArray (x: 2, y: 3)>
1732         array([[0, 1, 2],
1733                [3, 4, 5]])
1734         Coordinates:
1735           * x        (x) |S1 'a' 'b'
1736           * y        (y) int64 0 1 2
1737         >>> stacked = arr.stack(z=('x', 'y'))
1738         >>> stacked.indexes['z']
1739         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1740                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],
1741                    names=['x', 'y'])
1742         >>> roundtripped = stacked.unstack()
1743         >>> arr.identical(roundtripped)
1744         True
1745 
1746         See also
1747         --------
1748         DataArray.stack
1749         """
1750         ds = self._to_temp_dataset().unstack(dim)
1751         return self._from_temp_dataset(ds)
1752 
1753     def to_unstacked_dataset(self, dim, level=0):
1754         """Unstack DataArray expanding to Dataset along a given level of a
1755         stacked coordinate.
1756 
1757         This is the inverse operation of Dataset.to_stacked_array.
1758 
1759         Parameters
1760         ----------
1761         dim : str
1762             Name of existing dimension to unstack
1763         level : int or str
1764             The MultiIndex level to expand to a dataset along. Can either be
1765             the integer index of the level or its name.
1766         label : int, default 0
1767             Label of the level to expand dataset along. Overrides the label
1768             argument if given.
1769 
1770         Returns
1771         -------
1772         unstacked: Dataset
1773 
1774         Examples
1775         --------
1776         >>> import xarray as xr
1777         >>> arr = DataArray(np.arange(6).reshape(2, 3),
1778         ...                 coords=[('x', ['a', 'b']), ('y', [0, 1, 2])])
1779         >>> data = xr.Dataset({'a': arr, 'b': arr.isel(y=0)})
1780         >>> data
1781         <xarray.Dataset>
1782         Dimensions:  (x: 2, y: 3)
1783         Coordinates:
1784           * x        (x) <U1 'a' 'b'
1785           * y        (y) int64 0 1 2
1786         Data variables:
1787             a        (x, y) int64 0 1 2 3 4 5
1788             b        (x) int64 0 3
1789         >>> stacked = data.to_stacked_array("z", ['y'])
1790         >>> stacked.indexes['z']
1791         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1792                 labels=[[0, 0, 0, 1], [0, 1, 2, -1]],
1793                 names=['variable', 'y'])
1794         >>> roundtripped = stacked.to_unstacked_dataset(dim='z')
1795         >>> data.identical(roundtripped)
1796         True
1797 
1798         See Also
1799         --------
1800         Dataset.to_stacked_array
1801         """
1802 
1803         idx = self.indexes[dim]
1804         if not isinstance(idx, pd.MultiIndex):
1805             raise ValueError("'{}' is not a stacked coordinate".format(dim))
1806 
1807         level_number = idx._get_level_number(level)
1808         variables = idx.levels[level_number]
1809         variable_dim = idx.names[level_number]
1810 
1811         # pull variables out of datarray
1812         data_dict = OrderedDict()
1813         for k in variables:
1814             data_dict[k] = self.sel({variable_dim: k}).squeeze(drop=True)
1815 
1816         # unstacked dataset
1817         return Dataset(data_dict)
1818 
1819     def transpose(self, *dims: Hashable, transpose_coords: bool = None) -> "DataArray":
1820         """Return a new DataArray object with transposed dimensions.
1821 
1822         Parameters
1823         ----------
1824         *dims : hashable, optional
1825             By default, reverse the dimensions. Otherwise, reorder the
1826             dimensions to this order.
1827         transpose_coords : boolean, optional
1828             If True, also transpose the coordinates of this DataArray.
1829 
1830         Returns
1831         -------
1832         transposed : DataArray
1833             The returned DataArray's array is transposed.
1834 
1835         Notes
1836         -----
1837         This operation returns a view of this array's data. It is
1838         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
1839         -- the data will be fully loaded.
1840 
1841         See Also
1842         --------
1843         numpy.transpose
1844         Dataset.transpose
1845         """
1846         if dims:
1847             if set(dims) ^ set(self.dims):
1848                 raise ValueError(
1849                     "arguments to transpose (%s) must be "
1850                     "permuted array dimensions (%s)" % (dims, tuple(self.dims))
1851                 )
1852 
1853         variable = self.variable.transpose(*dims)
1854         if transpose_coords:
1855             coords = OrderedDict()  # type: OrderedDict[Any, Variable]
1856             for name, coord in self.coords.items():
1857                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
1858                 coords[name] = coord.variable.transpose(*coord_dims)
1859             return self._replace(variable, coords)
1860         else:
1861             if transpose_coords is None and any(self[c].ndim > 1 for c in self.coords):
1862                 warnings.warn(
1863                     "This DataArray contains multi-dimensional "
1864                     "coordinates. In the future, these coordinates "
1865                     "will be transposed as well unless you specify "
1866                     "transpose_coords=False.",
1867                     FutureWarning,
1868                     stacklevel=2,
1869                 )
1870             return self._replace(variable)
1871 
1872     @property
1873     def T(self) -> "DataArray":
1874         return self.transpose()
1875 
1876     # Drop coords
1877     @overload
1878     def drop(
1879         self, labels: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
1880     ) -> "DataArray":
1881         ...
1882 
1883     # Drop index labels along dimension
1884     @overload  # noqa: F811
1885     def drop(
1886         self, labels: Any, dim: Hashable, *, errors: str = "raise"  # array-like
1887     ) -> "DataArray":
1888         ...
1889 
1890     def drop(self, labels, dim=None, *, errors="raise"):  # noqa: F811
1891         """Drop coordinates or index labels from this DataArray.
1892 
1893         Parameters
1894         ----------
1895         labels : hashable or sequence of hashables
1896             Name(s) of coordinates or index labels to drop.
1897             If dim is not None, labels can be any array-like.
1898         dim : hashable, optional
1899             Dimension along which to drop index labels. By default (if
1900             ``dim is None``), drops coordinates rather than index labels.
1901         errors: {'raise', 'ignore'}, optional
1902             If 'raise' (default), raises a ValueError error if
1903             any of the coordinates or index labels passed are not
1904             in the array. If 'ignore', any given labels that are in the
1905             array are dropped and no error is raised.
1906         Returns
1907         -------
1908         dropped : DataArray
1909         """
1910         ds = self._to_temp_dataset().drop(labels, dim, errors=errors)
1911         return self._from_temp_dataset(ds)
1912 
1913     def dropna(
1914         self, dim: Hashable, how: str = "any", thresh: int = None
1915     ) -> "DataArray":
1916         """Returns a new array with dropped labels for missing values along
1917         the provided dimension.
1918 
1919         Parameters
1920         ----------
1921         dim : hashable
1922             Dimension along which to drop missing values. Dropping along
1923             multiple dimensions simultaneously is not yet supported.
1924         how : {'any', 'all'}, optional
1925             * any : if any NA values are present, drop that label
1926             * all : if all values are NA, drop that label
1927         thresh : int, default None
1928             If supplied, require this many non-NA values.
1929 
1930         Returns
1931         -------
1932         DataArray
1933         """
1934         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
1935         return self._from_temp_dataset(ds)
1936 
1937     def fillna(self, value: Any) -> "DataArray":
1938         """Fill missing values in this object.
1939 
1940         This operation follows the normal broadcasting and alignment rules that
1941         xarray uses for binary arithmetic, except the result is aligned to this
1942         object (``join='left'``) instead of aligned to the intersection of
1943         index coordinates (``join='inner'``).
1944 
1945         Parameters
1946         ----------
1947         value : scalar, ndarray or DataArray
1948             Used to fill all matching missing values in this array. If the
1949             argument is a DataArray, it is first aligned with (reindexed to)
1950             this array.
1951 
1952         Returns
1953         -------
1954         DataArray
1955         """
1956         if utils.is_dict_like(value):
1957             raise TypeError(
1958                 "cannot provide fill value as a dictionary with "
1959                 "fillna on a DataArray"
1960             )
1961         out = ops.fillna(self, value)
1962         return out
1963 
1964     def interpolate_na(
1965         self,
1966         dim=None,
1967         method: str = "linear",
1968         limit: int = None,
1969         use_coordinate: Union[bool, str] = True,
1970         **kwargs: Any
1971     ) -> "DataArray":
1972         """Interpolate values according to different methods.
1973 
1974         Parameters
1975         ----------
1976         dim : str
1977             Specifies the dimension along which to interpolate.
1978         method : {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic',
1979                   'polynomial', 'barycentric', 'krog', 'pchip',
1980                   'spline', 'akima'}, optional
1981             String indicating which method to use for interpolation:
1982 
1983             - 'linear': linear interpolation (Default). Additional keyword
1984               arguments are passed to ``numpy.interp``
1985             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic',
1986               'polynomial': are passed to ``scipy.interpolate.interp1d``. If
1987               method=='polynomial', the ``order`` keyword argument must also be
1988               provided.
1989             - 'barycentric', 'krog', 'pchip', 'spline', and `akima`: use their
1990               respective``scipy.interpolate`` classes.
1991         use_coordinate : boolean or str, default True
1992             Specifies which index to use as the x values in the interpolation
1993             formulated as `y = f(x)`. If False, values are treated as if
1994             eqaully-spaced along `dim`. If True, the IndexVariable `dim` is
1995             used. If use_coordinate is a string, it specifies the name of a
1996             coordinate variariable to use as the index.
1997         limit : int, default None
1998             Maximum number of consecutive NaNs to fill. Must be greater than 0
1999             or None for no limit.
2000 
2001         Returns
2002         -------
2003         DataArray
2004 
2005         See also
2006         --------
2007         numpy.interp
2008         scipy.interpolate
2009         """
2010         from .missing import interp_na
2011 
2012         return interp_na(
2013             self,
2014             dim=dim,
2015             method=method,
2016             limit=limit,
2017             use_coordinate=use_coordinate,
2018             **kwargs
2019         )
2020 
2021     def ffill(self, dim: Hashable, limit: int = None) -> "DataArray":
2022         """Fill NaN values by propogating values forward
2023 
2024         *Requires bottleneck.*
2025 
2026         Parameters
2027         ----------
2028         dim : hashable
2029             Specifies the dimension along which to propagate values when
2030             filling.
2031         limit : int, default None
2032             The maximum number of consecutive NaN values to forward fill. In
2033             other words, if there is a gap with more than this number of
2034             consecutive NaNs, it will only be partially filled. Must be greater
2035             than 0 or None for no limit.
2036 
2037         Returns
2038         -------
2039         DataArray
2040         """
2041         from .missing import ffill
2042 
2043         return ffill(self, dim, limit=limit)
2044 
2045     def bfill(self, dim: Hashable, limit: int = None) -> "DataArray":
2046         """Fill NaN values by propogating values backward
2047 
2048         *Requires bottleneck.*
2049 
2050         Parameters
2051         ----------
2052         dim : str
2053             Specifies the dimension along which to propagate values when
2054             filling.
2055         limit : int, default None
2056             The maximum number of consecutive NaN values to backward fill. In
2057             other words, if there is a gap with more than this number of
2058             consecutive NaNs, it will only be partially filled. Must be greater
2059             than 0 or None for no limit.
2060 
2061         Returns
2062         -------
2063         DataArray
2064         """
2065         from .missing import bfill
2066 
2067         return bfill(self, dim, limit=limit)
2068 
2069     def combine_first(self, other: "DataArray") -> "DataArray":
2070         """Combine two DataArray objects, with union of coordinates.
2071 
2072         This operation follows the normal broadcasting and alignment rules of
2073         ``join='outer'``.  Default to non-null values of array calling the
2074         method.  Use np.nan to fill in vacant cells after alignment.
2075 
2076         Parameters
2077         ----------
2078         other : DataArray
2079             Used to fill all matching missing values in this array.
2080 
2081         Returns
2082         -------
2083         DataArray
2084         """
2085         return ops.fillna(self, other, join="outer")
2086 
2087     def reduce(
2088         self,
2089         func: Callable[..., Any],
2090         dim: Union[None, Hashable, Sequence[Hashable]] = None,
2091         axis: Union[None, int, Sequence[int]] = None,
2092         keep_attrs: bool = None,
2093         keepdims: bool = False,
2094         **kwargs: Any
2095     ) -> "DataArray":
2096         """Reduce this array by applying `func` along some dimension(s).
2097 
2098         Parameters
2099         ----------
2100         func : function
2101             Function which can be called in the form
2102             `f(x, axis=axis, **kwargs)` to return the result of reducing an
2103             np.ndarray over an integer valued axis.
2104         dim : hashable or sequence of hashables, optional
2105             Dimension(s) over which to apply `func`.
2106         axis : int or sequence of int, optional
2107             Axis(es) over which to repeatedly apply `func`. Only one of the
2108             'dim' and 'axis' arguments can be supplied. If neither are
2109             supplied, then the reduction is calculated over the flattened array
2110             (by calling `f(x)` without an axis argument).
2111         keep_attrs : bool, optional
2112             If True, the variable's attributes (`attrs`) will be copied from
2113             the original object to the new one.  If False (default), the new
2114             object will be returned without attributes.
2115         keepdims : bool, default False
2116             If True, the dimensions which are reduced are left in the result
2117             as dimensions of size one. Coordinates that use these dimensions
2118             are removed.
2119         **kwargs : dict
2120             Additional keyword arguments passed on to `func`.
2121 
2122         Returns
2123         -------
2124         reduced : DataArray
2125             DataArray with this object's array replaced with an array with
2126             summarized data and the indicated dimension(s) removed.
2127         """
2128 
2129         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
2130         return self._replace_maybe_drop_dims(var)
2131 
2132     def to_pandas(self) -> Union["DataArray", pd.Series, pd.DataFrame]:
2133         """Convert this array into a pandas object with the same shape.
2134 
2135         The type of the returned object depends on the number of DataArray
2136         dimensions:
2137 
2138         * 0D -> `xarray.DataArray`
2139         * 1D -> `pandas.Series`
2140         * 2D -> `pandas.DataFrame`
2141         * 3D -> `pandas.Panel` *(deprecated)*
2142 
2143         Only works for arrays with 3 or fewer dimensions.
2144 
2145         The DataArray constructor performs the inverse transformation.
2146         """
2147         # TODO: consolidate the info about pandas constructors and the
2148         # attributes that correspond to their indexes into a separate module?
2149         constructors = {
2150             0: lambda x: x,
2151             1: pd.Series,
2152             2: pd.DataFrame,
2153             3: pdcompat.Panel,
2154         }
2155         try:
2156             constructor = constructors[self.ndim]
2157         except KeyError:
2158             raise ValueError(
2159                 "cannot convert arrays with %s dimensions into "
2160                 "pandas objects" % self.ndim
2161             )
2162         indexes = [self.get_index(dim) for dim in self.dims]
2163         return constructor(self.values, *indexes)
2164 
2165     def to_dataframe(self, name: Hashable = None) -> pd.DataFrame:
2166         """Convert this array and its coordinates into a tidy pandas.DataFrame.
2167 
2168         The DataFrame is indexed by the Cartesian product of index coordinates
2169         (in the form of a :py:class:`pandas.MultiIndex`).
2170 
2171         Other coordinates are included as columns in the DataFrame.
2172         """
2173         if name is None:
2174             name = self.name
2175         if name is None:
2176             raise ValueError(
2177                 "cannot convert an unnamed DataArray to a "
2178                 "DataFrame: use the ``name`` parameter"
2179             )
2180 
2181         dims = OrderedDict(zip(self.dims, self.shape))
2182         # By using a unique name, we can convert a DataArray into a DataFrame
2183         # even if it shares a name with one of its coordinates.
2184         # I would normally use unique_name = object() but that results in a
2185         # dataframe with columns in the wrong order, for reasons I have not
2186         # been able to debug (possibly a pandas bug?).
2187         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
2188         ds = self._to_dataset_whole(name=unique_name)
2189         df = ds._to_dataframe(dims)
2190         df.columns = [name if c == unique_name else c for c in df.columns]
2191         return df
2192 
2193     def to_series(self) -> pd.Series:
2194         """Convert this array into a pandas.Series.
2195 
2196         The Series is indexed by the Cartesian product of index coordinates
2197         (in the form of a :py:class:`pandas.MultiIndex`).
2198         """
2199         index = self.coords.to_index()
2200         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
2201 
2202     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
2203         """Convert this array into a numpy.ma.MaskedArray
2204 
2205         Parameters
2206         ----------
2207         copy : bool
2208             If True (default) make a copy of the array in the result. If False,
2209             a MaskedArray view of DataArray.values is returned.
2210 
2211         Returns
2212         -------
2213         result : MaskedArray
2214             Masked where invalid values (nan or inf) occur.
2215         """
2216         values = self.values  # only compute lazy arrays once
2217         isnull = pd.isnull(values)
2218         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
2219 
2220     def to_netcdf(self, *args, **kwargs) -> Optional["Delayed"]:
2221         """Write DataArray contents to a netCDF file.
2222 
2223         All parameters are passed directly to `xarray.Dataset.to_netcdf`.
2224 
2225         Notes
2226         -----
2227         Only xarray.Dataset objects can be written to netCDF files, so
2228         the xarray.DataArray is converted to a xarray.Dataset object
2229         containing a single variable. If the DataArray has no name, or if the
2230         name is the same as a co-ordinate name, then it is given the name
2231         '__xarray_dataarray_variable__'.
2232         """
2233         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE
2234 
2235         if self.name is None:
2236             # If no name is set then use a generic xarray name
2237             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2238         elif self.name in self.coords or self.name in self.dims:
2239             # The name is the same as one of the coords names, which netCDF
2240             # doesn't support, so rename it but keep track of the old name
2241             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2242             dataset.attrs[DATAARRAY_NAME] = self.name
2243         else:
2244             # No problems with the name - so we're fine!
2245             dataset = self.to_dataset()
2246 
2247         return dataset.to_netcdf(*args, **kwargs)
2248 
2249     def to_dict(self, data: bool = True) -> dict:
2250         """
2251         Convert this xarray.DataArray into a dictionary following xarray
2252         naming conventions.
2253 
2254         Converts all variables and attributes to native Python objects.
2255         Useful for coverting to json. To avoid datetime incompatibility
2256         use decode_times=False kwarg in xarrray.open_dataset.
2257 
2258         Parameters
2259         ----------
2260         data : bool, optional
2261             Whether to include the actual data in the dictionary. When set to
2262             False, returns just the schema.
2263 
2264         See also
2265         --------
2266         DataArray.from_dict
2267         """
2268         d = self.variable.to_dict(data=data)
2269         d.update({"coords": {}, "name": self.name})
2270         for k in self.coords:
2271             d["coords"][k] = self.coords[k].variable.to_dict(data=data)
2272         return d
2273 
2274     @classmethod
2275     def from_dict(cls, d: dict) -> "DataArray":
2276         """
2277         Convert a dictionary into an xarray.DataArray
2278 
2279         Input dict can take several forms::
2280 
2281             d = {'dims': ('t'), 'data': x}
2282 
2283             d = {'coords': {'t': {'dims': 't', 'data': t,
2284                                   'attrs': {'units':'s'}}},
2285                  'attrs': {'title': 'air temperature'},
2286                  'dims': 't',
2287                  'data': x,
2288                  'name': 'a'}
2289 
2290         where 't' is the name of the dimesion, 'a' is the name of the array,
2291         and  x and t are lists, numpy.arrays, or pandas objects.
2292 
2293         Parameters
2294         ----------
2295         d : dict, with a minimum structure of {'dims': [..], 'data': [..]}
2296 
2297         Returns
2298         -------
2299         obj : xarray.DataArray
2300 
2301         See also
2302         --------
2303         DataArray.to_dict
2304         Dataset.from_dict
2305         """
2306         coords = None
2307         if "coords" in d:
2308             try:
2309                 coords = OrderedDict(
2310                     [
2311                         (k, (v["dims"], v["data"], v.get("attrs")))
2312                         for k, v in d["coords"].items()
2313                     ]
2314                 )
2315             except KeyError as e:
2316                 raise ValueError(
2317                     "cannot convert dict when coords are missing the key "
2318                     "'{dims_data}'".format(dims_data=str(e.args[0]))
2319                 )
2320         try:
2321             data = d["data"]
2322         except KeyError:
2323             raise ValueError("cannot convert dict without the key 'data''")
2324         else:
2325             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
2326         return obj
2327 
2328     @classmethod
2329     def from_series(cls, series: pd.Series, sparse: bool = False) -> "DataArray":
2330         """Convert a pandas.Series into an xarray.DataArray.
2331 
2332         If the series's index is a MultiIndex, it will be expanded into a
2333         tensor product of one-dimensional coordinates (filling in missing
2334         values with NaN). Thus this operation should be the inverse of the
2335         `to_series` method.
2336 
2337         If sparse=True, creates a sparse array instead of a dense NumPy array.
2338         Requires the pydata/sparse package.
2339 
2340         See also
2341         --------
2342         xarray.Dataset.from_dataframe
2343         """
2344         temp_name = "__temporary_name"
2345         df = pd.DataFrame({temp_name: series})
2346         ds = Dataset.from_dataframe(df, sparse=sparse)
2347         result = cast(DataArray, ds[temp_name])
2348         result.name = series.name
2349         return result
2350 
2351     def to_cdms2(self) -> "cdms2_Variable":
2352         """Convert this array into a cdms2.Variable
2353         """
2354         from ..convert import to_cdms2
2355 
2356         return to_cdms2(self)
2357 
2358     @classmethod
2359     def from_cdms2(cls, variable: "cdms2_Variable") -> "DataArray":
2360         """Convert a cdms2.Variable into an xarray.DataArray
2361         """
2362         from ..convert import from_cdms2
2363 
2364         return from_cdms2(variable)
2365 
2366     def to_iris(self) -> "iris_Cube":
2367         """Convert this array into a iris.cube.Cube
2368         """
2369         from ..convert import to_iris
2370 
2371         return to_iris(self)
2372 
2373     @classmethod
2374     def from_iris(cls, cube: "iris_Cube") -> "DataArray":
2375         """Convert a iris.cube.Cube into an xarray.DataArray
2376         """
2377         from ..convert import from_iris
2378 
2379         return from_iris(cube)
2380 
2381     def _all_compat(self, other: "DataArray", compat_str: str) -> bool:
2382         """Helper function for equals, broadcast_equals, and identical
2383         """
2384 
2385         def compat(x, y):
2386             return getattr(x.variable, compat_str)(y.variable)
2387 
2388         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
2389             self, other
2390         )
2391 
2392     def broadcast_equals(self, other: "DataArray") -> bool:
2393         """Two DataArrays are broadcast equal if they are equal after
2394         broadcasting them against each other such that they have the same
2395         dimensions.
2396 
2397         See Also
2398         --------
2399         DataArray.equals
2400         DataArray.identical
2401         """
2402         try:
2403             return self._all_compat(other, "broadcast_equals")
2404         except (TypeError, AttributeError):
2405             return False
2406 
2407     def equals(self, other: "DataArray") -> bool:
2408         """True if two DataArrays have the same dimensions, coordinates and
2409         values; otherwise False.
2410 
2411         DataArrays can still be equal (like pandas objects) if they have NaN
2412         values in the same locations.
2413 
2414         This method is necessary because `v1 == v2` for ``DataArray``
2415         does element-wise comparisons (like numpy.ndarrays).
2416 
2417         See Also
2418         --------
2419         DataArray.broadcast_equals
2420         DataArray.identical
2421         """
2422         try:
2423             return self._all_compat(other, "equals")
2424         except (TypeError, AttributeError):
2425             return False
2426 
2427     def identical(self, other: "DataArray") -> bool:
2428         """Like equals, but also checks the array name and attributes, and
2429         attributes on all coordinates.
2430 
2431         See Also
2432         --------
2433         DataArray.broadcast_equals
2434         DataArray.equal
2435         """
2436         try:
2437             return self.name == other.name and self._all_compat(other, "identical")
2438         except (TypeError, AttributeError):
2439             return False
2440 
2441     __default_name = object()
2442 
2443     def _result_name(self, other: Any = None) -> Optional[Hashable]:
2444         # use the same naming heuristics as pandas:
2445         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
2446         other_name = getattr(other, "name", self.__default_name)
2447         if other_name is self.__default_name or other_name == self.name:
2448             return self.name
2449         else:
2450             return None
2451 
2452     def __array_wrap__(self, obj, context=None) -> "DataArray":
2453         new_var = self.variable.__array_wrap__(obj, context)
2454         return self._replace(new_var)
2455 
2456     def __matmul__(self, obj):
2457         return self.dot(obj)
2458 
2459     def __rmatmul__(self, other):
2460         # currently somewhat duplicative, as only other DataArrays are
2461         # compatible with matmul
2462         return computation.dot(other, self)
2463 
2464     @staticmethod
2465     def _unary_op(f: Callable[..., Any]) -> Callable[..., "DataArray"]:
2466         @functools.wraps(f)
2467         def func(self, *args, **kwargs):
2468             with np.errstate(all="ignore"):
2469                 return self.__array_wrap__(f(self.variable.data, *args, **kwargs))
2470 
2471         return func
2472 
2473     @staticmethod
2474     def _binary_op(
2475         f: Callable[..., Any],
2476         reflexive: bool = False,
2477         join: str = None,  # see xarray.align
2478         **ignored_kwargs
2479     ) -> Callable[..., "DataArray"]:
2480         @functools.wraps(f)
2481         def func(self, other):
2482             if isinstance(other, (Dataset, groupby.GroupBy)):
2483                 return NotImplemented
2484             if isinstance(other, DataArray):
2485                 align_type = OPTIONS["arithmetic_join"] if join is None else join
2486                 self, other = align(self, other, join=align_type, copy=False)
2487             other_variable = getattr(other, "variable", other)
2488             other_coords = getattr(other, "coords", None)
2489 
2490             variable = (
2491                 f(self.variable, other_variable)
2492                 if not reflexive
2493                 else f(other_variable, self.variable)
2494             )
2495             coords = self.coords._merge_raw(other_coords)
2496             name = self._result_name(other)
2497 
2498             return self._replace(variable, coords, name)
2499 
2500         return func
2501 
2502     @staticmethod
2503     def _inplace_binary_op(f: Callable) -> Callable[..., "DataArray"]:
2504         @functools.wraps(f)
2505         def func(self, other):
2506             if isinstance(other, groupby.GroupBy):
2507                 raise TypeError(
2508                     "in-place operations between a DataArray and "
2509                     "a grouped object are not permitted"
2510                 )
2511             # n.b. we can't align other to self (with other.reindex_like(self))
2512             # because `other` may be converted into floats, which would cause
2513             # in-place arithmetic to fail unpredictably. Instead, we simply
2514             # don't support automatic alignment with in-place arithmetic.
2515             other_coords = getattr(other, "coords", None)
2516             other_variable = getattr(other, "variable", other)
2517             with self.coords._merge_inplace(other_coords):
2518                 f(self.variable, other_variable)
2519             return self
2520 
2521         return func
2522 
2523     def _copy_attrs_from(self, other: Union["DataArray", Dataset, Variable]) -> None:
2524         self.attrs = other.attrs
2525 
2526     @property
2527     def plot(self) -> _PlotMethods:
2528         """
2529         Access plotting functions
2530 
2531         >>> d = DataArray([[1, 2], [3, 4]])
2532 
2533         For convenience just call this directly
2534         >>> d.plot()
2535 
2536         Or use it as a namespace to use xarray.plot functions as
2537         DataArray methods
2538         >>> d.plot.imshow()  # equivalent to xarray.plot.imshow(d)
2539 
2540         """
2541         return _PlotMethods(self)
2542 
2543     def _title_for_slice(self, truncate: int = 50) -> str:
2544         """
2545         If the dataarray has 1 dimensional coordinates or comes from a slice
2546         we can show that info in the title
2547 
2548         Parameters
2549         ----------
2550         truncate : integer
2551             maximum number of characters for title
2552 
2553         Returns
2554         -------
2555         title : string
2556             Can be used for plot titles
2557 
2558         """
2559         one_dims = []
2560         for dim, coord in self.coords.items():
2561             if coord.size == 1:
2562                 one_dims.append(
2563                     "{dim} = {v}".format(dim=dim, v=format_item(coord.values))
2564                 )
2565 
2566         title = ", ".join(one_dims)
2567         if len(title) > truncate:
2568             title = title[: (truncate - 3)] + "..."
2569 
2570         return title
2571 
2572     def diff(self, dim: Hashable, n: int = 1, label: Hashable = "upper") -> "DataArray":
2573         """Calculate the n-th order discrete difference along given axis.
2574 
2575         Parameters
2576         ----------
2577         dim : hashable, optional
2578             Dimension over which to calculate the finite difference.
2579         n : int, optional
2580             The number of times values are differenced.
2581         label : hashable, optional
2582             The new coordinate in dimension ``dim`` will have the
2583             values of either the minuend's or subtrahend's coordinate
2584             for values 'upper' and 'lower', respectively.  Other
2585             values are not supported.
2586 
2587         Returns
2588         -------
2589         difference : same type as caller
2590             The n-th order finite difference of this object.
2591 
2592         Examples
2593         --------
2594         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ['x'])
2595         >>> arr.diff('x')
2596         <xarray.DataArray (x: 3)>
2597         array([0, 1, 0])
2598         Coordinates:
2599         * x        (x) int64 2 3 4
2600         >>> arr.diff('x', 2)
2601         <xarray.DataArray (x: 2)>
2602         array([ 1, -1])
2603         Coordinates:
2604         * x        (x) int64 3 4
2605 
2606         See Also
2607         --------
2608         DataArray.differentiate
2609         """
2610         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
2611         return self._from_temp_dataset(ds)
2612 
2613     def shift(
2614         self,
2615         shifts: Mapping[Hashable, int] = None,
2616         fill_value: Any = dtypes.NA,
2617         **shifts_kwargs: int
2618     ) -> "DataArray":
2619         """Shift this array by an offset along one or more dimensions.
2620 
2621         Only the data is moved; coordinates stay in place. Values shifted from
2622         beyond array bounds are replaced by NaN. This is consistent with the
2623         behavior of ``shift`` in pandas.
2624 
2625         Parameters
2626         ----------
2627         shifts : Mapping with the form of {dim: offset}
2628             Integer offset to shift along each of the given dimensions.
2629             Positive offsets shift to the right; negative offsets shift to the
2630             left.
2631         fill_value: scalar, optional
2632             Value to use for newly missing values
2633         **shifts_kwargs:
2634             The keyword arguments form of ``shifts``.
2635             One of shifts or shifts_kwarg must be provided.
2636 
2637         Returns
2638         -------
2639         shifted : DataArray
2640             DataArray with the same coordinates and attributes but shifted
2641             data.
2642 
2643         See also
2644         --------
2645         roll
2646 
2647         Examples
2648         --------
2649 
2650         >>> arr = xr.DataArray([5, 6, 7], dims='x')
2651         >>> arr.shift(x=1)
2652         <xarray.DataArray (x: 3)>
2653         array([ nan,   5.,   6.])
2654         Coordinates:
2655           * x        (x) int64 0 1 2
2656         """
2657         variable = self.variable.shift(
2658             shifts=shifts, fill_value=fill_value, **shifts_kwargs
2659         )
2660         return self._replace(variable=variable)
2661 
2662     def roll(
2663         self,
2664         shifts: Mapping[Hashable, int] = None,
2665         roll_coords: bool = None,
2666         **shifts_kwargs: int
2667     ) -> "DataArray":
2668         """Roll this array by an offset along one or more dimensions.
2669 
2670         Unlike shift, roll may rotate all variables, including coordinates
2671         if specified. The direction of rotation is consistent with
2672         :py:func:`numpy.roll`.
2673 
2674         Parameters
2675         ----------
2676         shifts : Mapping with the form of {dim: offset}
2677             Integer offset to rotate each of the given dimensions.
2678             Positive offsets roll to the right; negative offsets roll to the
2679             left.
2680         roll_coords : bool
2681             Indicates whether to  roll the coordinates by the offset
2682             The current default of roll_coords (None, equivalent to True) is
2683             deprecated and will change to False in a future version.
2684             Explicitly pass roll_coords to silence the warning.
2685         **shifts_kwargs : The keyword arguments form of ``shifts``.
2686             One of shifts or shifts_kwarg must be provided.
2687 
2688         Returns
2689         -------
2690         rolled : DataArray
2691             DataArray with the same attributes but rolled data and coordinates.
2692 
2693         See also
2694         --------
2695         shift
2696 
2697         Examples
2698         --------
2699 
2700         >>> arr = xr.DataArray([5, 6, 7], dims='x')
2701         >>> arr.roll(x=1)
2702         <xarray.DataArray (x: 3)>
2703         array([7, 5, 6])
2704         Coordinates:
2705           * x        (x) int64 2 0 1
2706         """
2707         ds = self._to_temp_dataset().roll(
2708             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
2709         )
2710         return self._from_temp_dataset(ds)
2711 
2712     @property
2713     def real(self) -> "DataArray":
2714         return self._replace(self.variable.real)
2715 
2716     @property
2717     def imag(self) -> "DataArray":
2718         return self._replace(self.variable.imag)
2719 
2720     def dot(
2721         self, other: "DataArray", dims: Union[Hashable, Sequence[Hashable], None] = None
2722     ) -> "DataArray":
2723         """Perform dot product of two DataArrays along their shared dims.
2724 
2725         Equivalent to taking taking tensordot over all shared dims.
2726 
2727         Parameters
2728         ----------
2729         other : DataArray
2730             The other array with which the dot product is performed.
2731         dims: hashable or sequence of hashables, optional
2732             Along which dimensions to be summed over. Default all the common
2733             dimensions are summed over.
2734 
2735         Returns
2736         -------
2737         result : DataArray
2738             Array resulting from the dot product over all shared dimensions.
2739 
2740         See also
2741         --------
2742         dot
2743         numpy.tensordot
2744 
2745         Examples
2746         --------
2747 
2748         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
2749         >>> da = DataArray(da_vals, dims=['x', 'y', 'z'])
2750         >>> dm_vals = np.arange(4)
2751         >>> dm = DataArray(dm_vals, dims=['z'])
2752 
2753         >>> dm.dims
2754         ('z')
2755         >>> da.dims
2756         ('x', 'y', 'z')
2757 
2758         >>> dot_result = da.dot(dm)
2759         >>> dot_result.dims
2760         ('x', 'y')
2761         """
2762         if isinstance(other, Dataset):
2763             raise NotImplementedError(
2764                 "dot products are not yet supported with Dataset objects."
2765             )
2766         if not isinstance(other, DataArray):
2767             raise TypeError("dot only operates on DataArrays.")
2768 
2769         return computation.dot(self, other, dims=dims)
2770 
2771     def sortby(
2772         self,
2773         variables: Union[Hashable, "DataArray", Sequence[Union[Hashable, "DataArray"]]],
2774         ascending: bool = True,
2775     ) -> "DataArray":
2776         """Sort object by labels or values (along an axis).
2777 
2778         Sorts the dataarray, either along specified dimensions,
2779         or according to values of 1-D dataarrays that share dimension
2780         with calling object.
2781 
2782         If the input variables are dataarrays, then the dataarrays are aligned
2783         (via left-join) to the calling object prior to sorting by cell values.
2784         NaNs are sorted to the end, following Numpy convention.
2785 
2786         If multiple sorts along the same dimension is
2787         given, numpy's lexsort is performed along that dimension:
2788         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
2789         and the FIRST key in the sequence is used as the primary sort key,
2790         followed by the 2nd key, etc.
2791 
2792         Parameters
2793         ----------
2794         variables: hashable, DataArray, or sequence of either
2795             1D DataArray objects or name(s) of 1D variable(s) in
2796             coords whose values are used to sort this array.
2797         ascending: boolean, optional
2798             Whether to sort by ascending or descending order.
2799 
2800         Returns
2801         -------
2802         sorted: DataArray
2803             A new dataarray where all the specified dims are sorted by dim
2804             labels.
2805 
2806         Examples
2807         --------
2808 
2809         >>> da = xr.DataArray(np.random.rand(5),
2810         ...                   coords=[pd.date_range('1/1/2000', periods=5)],
2811         ...                   dims='time')
2812         >>> da
2813         <xarray.DataArray (time: 5)>
2814         array([ 0.965471,  0.615637,  0.26532 ,  0.270962,  0.552878])
2815         Coordinates:
2816           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...
2817 
2818         >>> da.sortby(da)
2819         <xarray.DataArray (time: 5)>
2820         array([ 0.26532 ,  0.270962,  0.552878,  0.615637,  0.965471])
2821         Coordinates:
2822           * time     (time) datetime64[ns] 2000-01-03 2000-01-04 2000-01-05 ...
2823         """
2824         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
2825         return self._from_temp_dataset(ds)
2826 
2827     def quantile(
2828         self,
2829         q: Any,
2830         dim: Union[Hashable, Sequence[Hashable], None] = None,
2831         interpolation: str = "linear",
2832         keep_attrs: bool = None,
2833     ) -> "DataArray":
2834         """Compute the qth quantile of the data along the specified dimension.
2835 
2836         Returns the qth quantiles(s) of the array elements.
2837 
2838         Parameters
2839         ----------
2840         q : float in range of [0,1] or array-like of floats
2841             Quantile to compute, which must be between 0 and 1 inclusive.
2842         dim : hashable or sequence of hashable, optional
2843             Dimension(s) over which to apply quantile.
2844         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
2845             This optional parameter specifies the interpolation method to
2846             use when the desired quantile lies between two data points
2847             ``i < j``:
2848 
2849                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is
2850                   the fractional part of the index surrounded by ``i`` and
2851                   ``j``.
2852                 - lower: ``i``.
2853                 - higher: ``j``.
2854                 - nearest: ``i`` or ``j``, whichever is nearest.
2855                 - midpoint: ``(i + j) / 2``.
2856         keep_attrs : bool, optional
2857             If True, the dataset's attributes (`attrs`) will be copied from
2858             the original object to the new one.  If False (default), the new
2859             object will be returned without attributes.
2860 
2861         Returns
2862         -------
2863         quantiles : DataArray
2864             If `q` is a single quantile, then the result
2865             is a scalar. If multiple percentiles are given, first axis of
2866             the result corresponds to the quantile and a quantile dimension
2867             is added to the return array. The other dimensions are the
2868              dimensions that remain after the reduction of the array.
2869 
2870         See Also
2871         --------
2872         numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile
2873         """
2874 
2875         ds = self._to_temp_dataset().quantile(
2876             q, dim=dim, keep_attrs=keep_attrs, interpolation=interpolation
2877         )
2878         return self._from_temp_dataset(ds)
2879 
2880     def rank(
2881         self, dim: Hashable, pct: bool = False, keep_attrs: bool = None
2882     ) -> "DataArray":
2883         """Ranks the data.
2884 
2885         Equal values are assigned a rank that is the average of the ranks that
2886         would have been otherwise assigned to all of the values within that
2887         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
2888 
2889         NaNs in the input array are returned as NaNs.
2890 
2891         The `bottleneck` library is required.
2892 
2893         Parameters
2894         ----------
2895         dim : hashable
2896             Dimension over which to compute rank.
2897         pct : bool, optional
2898             If True, compute percentage ranks, otherwise compute integer ranks.
2899         keep_attrs : bool, optional
2900             If True, the dataset's attributes (`attrs`) will be copied from
2901             the original object to the new one.  If False (default), the new
2902             object will be returned without attributes.
2903 
2904         Returns
2905         -------
2906         ranked : DataArray
2907             DataArray with the same coordinates and dtype 'float64'.
2908 
2909         Examples
2910         --------
2911 
2912         >>> arr = xr.DataArray([5, 6, 7], dims='x')
2913         >>> arr.rank('x')
2914         <xarray.DataArray (x: 3)>
2915         array([ 1.,   2.,   3.])
2916         Dimensions without coordinates: x
2917         """
2918 
2919         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
2920         return self._from_temp_dataset(ds)
2921 
2922     def differentiate(
2923         self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None
2924     ) -> "DataArray":
2925         """ Differentiate the array with the second order accurate central
2926         differences.
2927 
2928         .. note::
2929             This feature is limited to simple cartesian geometry, i.e. coord
2930             must be one dimensional.
2931 
2932         Parameters
2933         ----------
2934         coord: hashable
2935             The coordinate to be used to compute the gradient.
2936         edge_order: 1 or 2. Default 1
2937             N-th order accurate differences at the boundaries.
2938         datetime_unit: None or any of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms',
2939             'us', 'ns', 'ps', 'fs', 'as'}
2940             Unit to compute gradient. Only valid for datetime coordinate.
2941 
2942         Returns
2943         -------
2944         differentiated: DataArray
2945 
2946         See also
2947         --------
2948         numpy.gradient: corresponding numpy function
2949 
2950         Examples
2951         --------
2952 
2953         >>> da = xr.DataArray(np.arange(12).reshape(4, 3), dims=['x', 'y'],
2954         ...                   coords={'x': [0, 0.1, 1.1, 1.2]})
2955         >>> da
2956         <xarray.DataArray (x: 4, y: 3)>
2957         array([[ 0,  1,  2],
2958                [ 3,  4,  5],
2959                [ 6,  7,  8],
2960                [ 9, 10, 11]])
2961         Coordinates:
2962           * x        (x) float64 0.0 0.1 1.1 1.2
2963         Dimensions without coordinates: y
2964         >>>
2965         >>> da.differentiate('x')
2966         <xarray.DataArray (x: 4, y: 3)>
2967         array([[30.      , 30.      , 30.      ],
2968                [27.545455, 27.545455, 27.545455],
2969                [27.545455, 27.545455, 27.545455],
2970                [30.      , 30.      , 30.      ]])
2971         Coordinates:
2972           * x        (x) float64 0.0 0.1 1.1 1.2
2973         Dimensions without coordinates: y
2974         """
2975         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
2976         return self._from_temp_dataset(ds)
2977 
2978     def integrate(
2979         self, dim: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None
2980     ) -> "DataArray":
2981         """ integrate the array with the trapezoidal rule.
2982 
2983         .. note::
2984             This feature is limited to simple cartesian geometry, i.e. coord
2985             must be one dimensional.
2986 
2987         Parameters
2988         ----------
2989         dim: hashable, or a sequence of hashable
2990             Coordinate(s) used for the integration.
2991         datetime_unit: str, optional
2992             Can be used to specify the unit if datetime coordinate is used.
2993             One of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns',
2994                     'ps', 'fs', 'as'}
2995 
2996         Returns
2997         -------
2998         integrated: DataArray
2999 
3000         See also
3001         --------
3002         numpy.trapz: corresponding numpy function
3003 
3004         Examples
3005         --------
3006 
3007         >>> da = xr.DataArray(np.arange(12).reshape(4, 3), dims=['x', 'y'],
3008         ...                   coords={'x': [0, 0.1, 1.1, 1.2]})
3009         >>> da
3010         <xarray.DataArray (x: 4, y: 3)>
3011         array([[ 0,  1,  2],
3012                [ 3,  4,  5],
3013                [ 6,  7,  8],
3014                [ 9, 10, 11]])
3015         Coordinates:
3016           * x        (x) float64 0.0 0.1 1.1 1.2
3017         Dimensions without coordinates: y
3018         >>>
3019         >>> da.integrate('x')
3020         <xarray.DataArray (y: 3)>
3021         array([5.4, 6.6, 7.8])
3022         Dimensions without coordinates: y
3023         """
3024         ds = self._to_temp_dataset().integrate(dim, datetime_unit)
3025         return self._from_temp_dataset(ds)
3026 
3027     # this needs to be at the end, or mypy will confuse with `str`
3028     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names  # noqa
3029     str = property(StringAccessor)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 15:14:53.544 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 15:14:53.544 | INFO     | Search-only: skipping patch selection
2025-10-18 15:14:53.608 | INFO     | Task pydata__xarray-3305 completed successfully.
2025-10-18 15:14:53.609 | INFO     | No patch generated. You can try running ACR again.
