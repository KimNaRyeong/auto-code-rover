[
    {
        "rel_file_path": "sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py",
        "start": 1041,
        "end": 1048,
        "class_name": "HistGradientBoostingClassifier",
        "method_name": "_get_loss",
        "code": "1041     def _get_loss(self):\n1042         if self.loss == 'auto':\n1043             if self.n_trees_per_iteration_ == 1:\n1044                 return _LOSSES['binary_crossentropy']()\n1045             else:\n1046                 return _LOSSES['categorical_crossentropy']()\n1047 \n1048         return _LOSSES[self.loss]()\n",
        "intended_behavior": "The `_get_loss` method should include an additional check to ascertain whether `categorical_crossentropy` is being used in a binary classification scenario erroneously. Given the binary classification case (`n_trees_per_iteration_ == 1`), if `self.loss` is explicitly set to `categorical_crossentropy`, the method should raise a ValueError indicating the misuse and suggesting the use of `binary_crossentropy` or setting `loss='auto'` for automatic selection. This change ensures users are aware of the correct loss function usage in binary classification scenarios, preventing silent failures."
    },
    {
        "rel_file_path": "sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py",
        "start": 818,
        "end": 1048,
        "class_name": "HistGradientBoostingClassifier",
        "method_name": null,
        "code": "818 class HistGradientBoostingClassifier(BaseHistGradientBoosting,\n819                                      ClassifierMixin):\n820     \"\"\"Histogram-based Gradient Boosting Classification Tree.\n821 \n822     This estimator is much faster than\n823     :class:`GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`\n824     for big datasets (n_samples >= 10 000).\n825 \n826     This estimator has native support for missing values (NaNs). During\n827     training, the tree grower learns at each split point whether samples\n828     with missing values should go to the left or right child, based on the\n829     potential gain. When predicting, samples with missing values are\n830     assigned to the left or right child consequently. If no missing values\n831     were encountered for a given feature during training, then samples with\n832     missing values are mapped to whichever child has the most samples.\n833 \n834     This implementation is inspired by\n835     `LightGBM <https://github.com/Microsoft/LightGBM>`_.\n836 \n837     .. note::\n838 \n839       This estimator is still **experimental** for now: the predictions\n840       and the API might change without any deprecation cycle. To use it,\n841       you need to explicitly import ``enable_hist_gradient_boosting``::\n842 \n843         >>> # explicitly require this experimental feature\n844         >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n845         >>> # now you can import normally from ensemble\n846         >>> from sklearn.ensemble import HistGradientBoostingClassifier\n847 \n848     Read more in the :ref:`User Guide <histogram_based_gradient_boosting>`.\n849 \n850     Parameters\n851     ----------\n852     loss : {'auto', 'binary_crossentropy', 'categorical_crossentropy'}, \\\n853             optional (default='auto')\n854         The loss function to use in the boosting process. 'binary_crossentropy'\n855         (also known as logistic loss) is used for binary classification and\n856         generalizes to 'categorical_crossentropy' for multiclass\n857         classification. 'auto' will automatically choose either loss depending\n858         on the nature of the problem.\n859     learning_rate : float, optional (default=0.1)\n860         The learning rate, also known as *shrinkage*. This is used as a\n861         multiplicative factor for the leaves values. Use ``1`` for no\n862         shrinkage.\n863     max_iter : int, optional (default=100)\n864         The maximum number of iterations of the boosting process, i.e. the\n865         maximum number of trees for binary classification. For multiclass\n866         classification, `n_classes` trees per iteration are built.\n867     max_leaf_nodes : int or None, optional (default=31)\n868         The maximum number of leaves for each tree. Must be strictly greater\n869         than 1. If None, there is no maximum limit.\n870     max_depth : int or None, optional (default=None)\n871         The maximum depth of each tree. The depth of a tree is the number of\n872         nodes to go from the root to the deepest leaf. Must be strictly greater\n873         than 1. Depth isn't constrained by default.\n874     min_samples_leaf : int, optional (default=20)\n875         The minimum number of samples per leaf. For small datasets with less\n876         than a few hundred samples, it is recommended to lower this value\n877         since only very shallow trees would be built.\n878     l2_regularization : float, optional (default=0)\n879         The L2 regularization parameter. Use 0 for no regularization.\n880     max_bins : int, optional (default=255)\n881         The maximum number of bins to use for non-missing values. Before\n882         training, each feature of the input array `X` is binned into\n883         integer-valued bins, which allows for a much faster training stage.\n884         Features with a small number of unique values may use less than\n885         ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\n886         is always reserved for missing values. Must be no larger than 255.\n887     warm_start : bool, optional (default=False)\n888         When set to ``True``, reuse the solution of the previous call to fit\n889         and add more estimators to the ensemble. For results to be valid, the\n890         estimator should be re-trained on the same data only.\n891         See :term:`the Glossary <warm_start>`.\n892     scoring : str or callable or None, optional (default=None)\n893         Scoring parameter to use for early stopping. It can be a single\n894         string (see :ref:`scoring_parameter`) or a callable (see\n895         :ref:`scoring`). If None, the estimator's default scorer\n896         is used. If ``scoring='loss'``, early stopping is checked\n897         w.r.t the loss value. Only used if ``n_iter_no_change`` is not None.\n898     validation_fraction : int or float or None, optional (default=0.1)\n899         Proportion (or absolute size) of training data to set aside as\n900         validation data for early stopping. If None, early stopping is done on\n901         the training data.\n902     n_iter_no_change : int or None, optional (default=None)\n903         Used to determine when to \"early stop\". The fitting process is\n904         stopped when none of the last ``n_iter_no_change`` scores are better\n905         than the ``n_iter_no_change - 1`` -th-to-last one, up to some\n906         tolerance. If None or 0, no early-stopping is done.\n907     tol : float or None, optional (default=1e-7)\n908         The absolute tolerance to use when comparing scores. The higher the\n909         tolerance, the more likely we are to early stop: higher tolerance\n910         means that it will be harder for subsequent iterations to be\n911         considered an improvement upon the reference score.\n912     verbose: int, optional (default=0)\n913         The verbosity level. If not zero, print some information about the\n914         fitting process.\n915     random_state : int, np.random.RandomStateInstance or None, \\\n916         optional (default=None)\n917         Pseudo-random number generator to control the subsampling in the\n918         binning process, and the train/validation data split if early stopping\n919         is enabled. See :term:`random_state`.\n920 \n921     Attributes\n922     ----------\n923     n_iter_ : int\n924         The number of estimators as selected by early stopping (if\n925         n_iter_no_change is not None). Otherwise it corresponds to max_iter.\n926     n_trees_per_iteration_ : int\n927         The number of tree that are built at each iteration. This is equal to 1\n928         for binary classification, and to ``n_classes`` for multiclass\n929         classification.\n930     train_score_ : ndarray, shape (n_iter_ + 1,)\n931         The scores at each iteration on the training data. The first entry\n932         is the score of the ensemble before the first iteration. Scores are\n933         computed according to the ``scoring`` parameter. If ``scoring`` is\n934         not 'loss', scores are computed on a subset of at most 10 000\n935         samples. Empty if no early stopping.\n936     validation_score_ : ndarray, shape (n_iter_ + 1,)\n937         The scores at each iteration on the held-out validation data. The\n938         first entry is the score of the ensemble before the first iteration.\n939         Scores are computed according to the ``scoring`` parameter. Empty if\n940         no early stopping or if ``validation_fraction`` is None.\n941 \n942     Examples\n943     --------\n944     >>> # To use this experimental feature, we need to explicitly ask for it:\n945     >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n946     >>> from sklearn.ensemble import HistGradientBoostingRegressor\n947     >>> from sklearn.datasets import load_iris\n948     >>> X, y = load_iris(return_X_y=True)\n949     >>> clf = HistGradientBoostingClassifier().fit(X, y)\n950     >>> clf.score(X, y)\n951     1.0\n952     \"\"\"\n953 \n954     _VALID_LOSSES = ('binary_crossentropy', 'categorical_crossentropy',\n955                      'auto')\n956 \n957     def __init__(self, loss='auto', learning_rate=0.1, max_iter=100,\n958                  max_leaf_nodes=31, max_depth=None, min_samples_leaf=20,\n959                  l2_regularization=0., max_bins=255, warm_start=False,\n960                  scoring=None, validation_fraction=0.1, n_iter_no_change=None,\n961                  tol=1e-7, verbose=0, random_state=None):\n962         super(HistGradientBoostingClassifier, self).__init__(\n963             loss=loss, learning_rate=learning_rate, max_iter=max_iter,\n964             max_leaf_nodes=max_leaf_nodes, max_depth=max_depth,\n965             min_samples_leaf=min_samples_leaf,\n966             l2_regularization=l2_regularization, max_bins=max_bins,\n967             warm_start=warm_start, scoring=scoring,\n968             validation_fraction=validation_fraction,\n969             n_iter_no_change=n_iter_no_change, tol=tol, verbose=verbose,\n970             random_state=random_state)\n971 \n972     def predict(self, X):\n973         \"\"\"Predict classes for X.\n974 \n975         Parameters\n976         ----------\n977         X : array-like, shape (n_samples, n_features)\n978             The input samples.\n979 \n980         Returns\n981         -------\n982         y : ndarray, shape (n_samples,)\n983             The predicted classes.\n984         \"\"\"\n985         # TODO: This could be done in parallel\n986         encoded_classes = np.argmax(self.predict_proba(X), axis=1)\n987         return self.classes_[encoded_classes]\n988 \n989     def predict_proba(self, X):\n990         \"\"\"Predict class probabilities for X.\n991 \n992         Parameters\n993         ----------\n994         X : array-like, shape (n_samples, n_features)\n995             The input samples.\n996 \n997         Returns\n998         -------\n999         p : ndarray, shape (n_samples, n_classes)\n1000             The class probabilities of the input samples.\n1001         \"\"\"\n1002         raw_predictions = self._raw_predict(X)\n1003         return self.loss_.predict_proba(raw_predictions)\n1004 \n1005     def decision_function(self, X):\n1006         \"\"\"Compute the decision function of X.\n1007 \n1008         Parameters\n1009         ----------\n1010         X : array-like, shape (n_samples, n_features)\n1011             The input samples.\n1012 \n1013         Returns\n1014         -------\n1015         decision : ndarray, shape (n_samples,) or \\\n1016                 (n_samples, n_trees_per_iteration)\n1017             The raw predicted values (i.e. the sum of the trees leaves) for\n1018             each sample. n_trees_per_iteration is equal to the number of\n1019             classes in multiclass classification.\n1020         \"\"\"\n1021         decision = self._raw_predict(X)\n1022         if decision.shape[0] == 1:\n1023             decision = decision.ravel()\n1024         return decision.T\n1025 \n1026     def _encode_y(self, y):\n1027         # encode classes into 0 ... n_classes - 1 and sets attributes classes_\n1028         # and n_trees_per_iteration_\n1029         check_classification_targets(y)\n1030 \n1031         label_encoder = LabelEncoder()\n1032         encoded_y = label_encoder.fit_transform(y)\n1033         self.classes_ = label_encoder.classes_\n1034         n_classes = self.classes_.shape[0]\n1035         # only 1 tree for binary classification. For multiclass classification,\n1036         # we build 1 tree per class.\n1037         self.n_trees_per_iteration_ = 1 if n_classes <= 2 else n_classes\n1038         encoded_y = encoded_y.astype(Y_DTYPE, copy=False)\n1039         return encoded_y\n1040 \n1041     def _get_loss(self):\n1042         if self.loss == 'auto':\n1043             if self.n_trees_per_iteration_ == 1:\n1044                 return _LOSSES['binary_crossentropy']()\n1045             else:\n1046                 return _LOSSES['categorical_crossentropy']()\n1047 \n1048         return _LOSSES[self.loss]()\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py",
        "start": 83,
        "end": 392,
        "class_name": "BaseHistGradientBoosting",
        "method_name": "fit",
        "code": "83     def fit(self, X, y):\n84         \"\"\"Fit the gradient boosting model.\n85 \n86         Parameters\n87         ----------\n88         X : array-like, shape=(n_samples, n_features)\n89             The input samples.\n90 \n91         y : array-like, shape=(n_samples,)\n92             Target values.\n93 \n94         Returns\n95         -------\n96         self : object\n97         \"\"\"\n98         fit_start_time = time()\n99         acc_find_split_time = 0.  # time spent finding the best splits\n100         acc_apply_split_time = 0.  # time spent splitting nodes\n101         acc_compute_hist_time = 0.  # time spent computing histograms\n102         # time spent predicting X for gradient and hessians update\n103         acc_prediction_time = 0.\n104         X, y = check_X_y(X, y, dtype=[X_DTYPE], force_all_finite=False)\n105         y = self._encode_y(y)\n106 \n107         # The rng state must be preserved if warm_start is True\n108         if (self.warm_start and hasattr(self, '_rng')):\n109             rng = self._rng\n110         else:\n111             rng = check_random_state(self.random_state)\n112             self._rng = rng\n113 \n114         self._validate_parameters()\n115         self.n_features_ = X.shape[1]  # used for validation in predict()\n116 \n117         # we need this stateful variable to tell raw_predict() that it was\n118         # called from fit() (this current method), and that the data it has\n119         # received is pre-binned.\n120         # predicting is faster on pre-binned data, so we want early stopping\n121         # predictions to be made on pre-binned data. Unfortunately the scorer_\n122         # can only call predict() or predict_proba(), not raw_predict(), and\n123         # there's no way to tell the scorer that it needs to predict binned\n124         # data.\n125         self._in_fit = True\n126 \n127         self.loss_ = self._get_loss()\n128 \n129         self.do_early_stopping_ = (self.n_iter_no_change is not None and\n130                                    self.n_iter_no_change > 0)\n131 \n132         # create validation data if needed\n133         self._use_validation_data = self.validation_fraction is not None\n134         if self.do_early_stopping_ and self._use_validation_data:\n135             # stratify for classification\n136             stratify = y if hasattr(self.loss_, 'predict_proba') else None\n137 \n138             # Save the state of the RNG for the training and validation split.\n139             # This is needed in order to have the same split when using\n140             # warm starting.\n141             if not (self._is_fitted() and self.warm_start):\n142                 self._train_val_split_seed = rng.randint(1024)\n143 \n144             X_train, X_val, y_train, y_val = train_test_split(\n145                 X, y, test_size=self.validation_fraction, stratify=stratify,\n146                 random_state=self._train_val_split_seed)\n147         else:\n148             X_train, y_train = X, y\n149             X_val, y_val = None, None\n150 \n151         has_missing_values = np.isnan(X_train).any(axis=0).astype(np.uint8)\n152 \n153         # Bin the data\n154         # For ease of use of the API, the user-facing GBDT classes accept the\n155         # parameter max_bins, which doesn't take into account the bin for\n156         # missing values (which is always allocated). However, since max_bins\n157         # isn't the true maximal number of bins, all other private classes\n158         # (binmapper, histbuilder...) accept n_bins instead, which is the\n159         # actual total number of bins. Everywhere in the code, the\n160         # convention is that n_bins == max_bins + 1\n161         n_bins = self.max_bins + 1  # + 1 for missing values\n162         self.bin_mapper_ = _BinMapper(n_bins=n_bins, random_state=rng)\n163         X_binned_train = self._bin_data(X_train, rng, is_training_data=True)\n164         if X_val is not None:\n165             X_binned_val = self._bin_data(X_val, rng, is_training_data=False)\n166         else:\n167             X_binned_val = None\n168 \n169         if self.verbose:\n170             print(\"Fitting gradient boosted rounds:\")\n171 \n172         n_samples = X_binned_train.shape[0]\n173 \n174         # First time calling fit, or no warm start\n175         if not (self._is_fitted() and self.warm_start):\n176             # Clear random state and score attributes\n177             self._clear_state()\n178 \n179             # initialize raw_predictions: those are the accumulated values\n180             # predicted by the trees for the training data. raw_predictions has\n181             # shape (n_trees_per_iteration, n_samples) where\n182             # n_trees_per_iterations is n_classes in multiclass classification,\n183             # else 1.\n184             self._baseline_prediction = self.loss_.get_baseline_prediction(\n185                 y_train, self.n_trees_per_iteration_\n186             )\n187             raw_predictions = np.zeros(\n188                 shape=(self.n_trees_per_iteration_, n_samples),\n189                 dtype=self._baseline_prediction.dtype\n190             )\n191             raw_predictions += self._baseline_prediction\n192 \n193             # initialize gradients and hessians (empty arrays).\n194             # shape = (n_trees_per_iteration, n_samples).\n195             gradients, hessians = self.loss_.init_gradients_and_hessians(\n196                 n_samples=n_samples,\n197                 prediction_dim=self.n_trees_per_iteration_\n198             )\n199 \n200             # predictors is a matrix (list of lists) of TreePredictor objects\n201             # with shape (n_iter_, n_trees_per_iteration)\n202             self._predictors = predictors = []\n203 \n204             # Initialize structures and attributes related to early stopping\n205             self.scorer_ = None  # set if scoring != loss\n206             raw_predictions_val = None  # set if scoring == loss and use val\n207             self.train_score_ = []\n208             self.validation_score_ = []\n209 \n210             if self.do_early_stopping_:\n211                 # populate train_score and validation_score with the\n212                 # predictions of the initial model (before the first tree)\n213 \n214                 if self.scoring == 'loss':\n215                     # we're going to compute scoring w.r.t the loss. As losses\n216                     # take raw predictions as input (unlike the scorers), we\n217                     # can optimize a bit and avoid repeating computing the\n218                     # predictions of the previous trees. We'll re-use\n219                     # raw_predictions (as it's needed for training anyway) for\n220                     # evaluating the training loss, and create\n221                     # raw_predictions_val for storing the raw predictions of\n222                     # the validation data.\n223 \n224                     if self._use_validation_data:\n225                         raw_predictions_val = np.zeros(\n226                             shape=(self.n_trees_per_iteration_,\n227                                    X_binned_val.shape[0]),\n228                             dtype=self._baseline_prediction.dtype\n229                         )\n230 \n231                         raw_predictions_val += self._baseline_prediction\n232 \n233                     self._check_early_stopping_loss(raw_predictions, y_train,\n234                                                     raw_predictions_val, y_val)\n235                 else:\n236                     self.scorer_ = check_scoring(self, self.scoring)\n237                     # scorer_ is a callable with signature (est, X, y) and\n238                     # calls est.predict() or est.predict_proba() depending on\n239                     # its nature.\n240                     # Unfortunately, each call to scorer_() will compute\n241                     # the predictions of all the trees. So we use a subset of\n242                     # the training set to compute train scores.\n243 \n244                     # Save the seed for the small trainset generator\n245                     self._small_trainset_seed = rng.randint(1024)\n246 \n247                     # Compute the subsample set\n248                     (X_binned_small_train,\n249                      y_small_train) = self._get_small_trainset(\n250                         X_binned_train, y_train, self._small_trainset_seed)\n251 \n252                     self._check_early_stopping_scorer(\n253                         X_binned_small_train, y_small_train,\n254                         X_binned_val, y_val,\n255                     )\n256             begin_at_stage = 0\n257 \n258         # warm start: this is not the first time fit was called\n259         else:\n260             # Check that the maximum number of iterations is not smaller\n261             # than the number of iterations from the previous fit\n262             if self.max_iter < self.n_iter_:\n263                 raise ValueError(\n264                     'max_iter=%d must be larger than or equal to '\n265                     'n_iter_=%d when warm_start==True'\n266                     % (self.max_iter, self.n_iter_)\n267                 )\n268 \n269             # Convert array attributes to lists\n270             self.train_score_ = self.train_score_.tolist()\n271             self.validation_score_ = self.validation_score_.tolist()\n272 \n273             # Compute raw predictions\n274             raw_predictions = self._raw_predict(X_binned_train)\n275 \n276             if self.do_early_stopping_ and self.scoring != 'loss':\n277                 # Compute the subsample set\n278                 X_binned_small_train, y_small_train = self._get_small_trainset(\n279                     X_binned_train, y_train, self._small_trainset_seed)\n280 \n281             # Initialize the gradients and hessians\n282             gradients, hessians = self.loss_.init_gradients_and_hessians(\n283                 n_samples=n_samples,\n284                 prediction_dim=self.n_trees_per_iteration_\n285             )\n286 \n287             # Get the predictors from the previous fit\n288             predictors = self._predictors\n289 \n290             begin_at_stage = self.n_iter_\n291 \n292         for iteration in range(begin_at_stage, self.max_iter):\n293 \n294             if self.verbose:\n295                 iteration_start_time = time()\n296                 print(\"[{}/{}] \".format(iteration + 1, self.max_iter),\n297                       end='', flush=True)\n298 \n299             # Update gradients and hessians, inplace\n300             self.loss_.update_gradients_and_hessians(gradients, hessians,\n301                                                      y_train, raw_predictions)\n302 \n303             # Append a list since there may be more than 1 predictor per iter\n304             predictors.append([])\n305 \n306             # Build `n_trees_per_iteration` trees.\n307             for k in range(self.n_trees_per_iteration_):\n308 \n309                 grower = TreeGrower(\n310                     X_binned_train, gradients[k, :], hessians[k, :],\n311                     n_bins=n_bins,\n312                     n_bins_non_missing=self.bin_mapper_.n_bins_non_missing_,\n313                     has_missing_values=has_missing_values,\n314                     max_leaf_nodes=self.max_leaf_nodes,\n315                     max_depth=self.max_depth,\n316                     min_samples_leaf=self.min_samples_leaf,\n317                     l2_regularization=self.l2_regularization,\n318                     shrinkage=self.learning_rate)\n319                 grower.grow()\n320 \n321                 acc_apply_split_time += grower.total_apply_split_time\n322                 acc_find_split_time += grower.total_find_split_time\n323                 acc_compute_hist_time += grower.total_compute_hist_time\n324 \n325                 predictor = grower.make_predictor(\n326                     bin_thresholds=self.bin_mapper_.bin_thresholds_\n327                 )\n328                 predictors[-1].append(predictor)\n329 \n330                 # Update raw_predictions with the predictions of the newly\n331                 # created tree.\n332                 tic_pred = time()\n333                 _update_raw_predictions(raw_predictions[k, :], grower)\n334                 toc_pred = time()\n335                 acc_prediction_time += toc_pred - tic_pred\n336 \n337             should_early_stop = False\n338             if self.do_early_stopping_:\n339                 if self.scoring == 'loss':\n340                     # Update raw_predictions_val with the newest tree(s)\n341                     if self._use_validation_data:\n342                         for k, pred in enumerate(self._predictors[-1]):\n343                             raw_predictions_val[k, :] += (\n344                                 pred.predict_binned(\n345                                     X_binned_val,\n346                                     self.bin_mapper_.missing_values_bin_idx_\n347                                 )\n348                             )\n349 \n350                     should_early_stop = self._check_early_stopping_loss(\n351                         raw_predictions, y_train,\n352                         raw_predictions_val, y_val\n353                     )\n354 \n355                 else:\n356                     should_early_stop = self._check_early_stopping_scorer(\n357                         X_binned_small_train, y_small_train,\n358                         X_binned_val, y_val,\n359                     )\n360 \n361             if self.verbose:\n362                 self._print_iteration_stats(iteration_start_time)\n363 \n364             # maybe we could also early stop if all the trees are stumps?\n365             if should_early_stop:\n366                 break\n367 \n368         if self.verbose:\n369             duration = time() - fit_start_time\n370             n_total_leaves = sum(\n371                 predictor.get_n_leaf_nodes()\n372                 for predictors_at_ith_iteration in self._predictors\n373                 for predictor in predictors_at_ith_iteration\n374             )\n375             n_predictors = sum(\n376                 len(predictors_at_ith_iteration)\n377                 for predictors_at_ith_iteration in self._predictors)\n378             print(\"Fit {} trees in {:.3f} s, ({} total leaves)\".format(\n379                 n_predictors, duration, n_total_leaves))\n380             print(\"{:<32} {:.3f}s\".format('Time spent computing histograms:',\n381                                           acc_compute_hist_time))\n382             print(\"{:<32} {:.3f}s\".format('Time spent finding best splits:',\n383                                           acc_find_split_time))\n384             print(\"{:<32} {:.3f}s\".format('Time spent applying splits:',\n385                                           acc_apply_split_time))\n386             print(\"{:<32} {:.3f}s\".format('Time spent predicting:',\n387                                           acc_prediction_time))\n388 \n389         self.train_score_ = np.asarray(self.train_score_)\n390         self.validation_score_ = np.asarray(self.validation_score_)\n391         del self._in_fit  # hard delete so we're sure it can't be used anymore\n392         return self\n",
        "intended_behavior": "During the `fit` method's execution, especially after the call to `_get_loss`, an additional validation step could be incorporated to check the suitability of the loss function chosen for the data it is being applied to. This may include verifying that the selected loss function aligns with the type of classification task being performed. While not directly necessary to fix the reported issue, this could enhance robustness and user guidance, preventing similar issues from arising in different contexts or with other parameters."
    },
    {
        "rel_file_path": "sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py",
        "start": 25,
        "end": 639,
        "class_name": "BaseHistGradientBoosting",
        "method_name": null,
        "code": "25 class BaseHistGradientBoosting(BaseEstimator, ABC):\n26     \"\"\"Base class for histogram-based gradient boosting estimators.\"\"\"\n27 \n28     @abstractmethod\n29     def __init__(self, loss, learning_rate, max_iter, max_leaf_nodes,\n30                  max_depth, min_samples_leaf, l2_regularization, max_bins,\n31                  warm_start, scoring, validation_fraction, n_iter_no_change,\n32                  tol, verbose, random_state):\n33         self.loss = loss\n34         self.learning_rate = learning_rate\n35         self.max_iter = max_iter\n36         self.max_leaf_nodes = max_leaf_nodes\n37         self.max_depth = max_depth\n38         self.min_samples_leaf = min_samples_leaf\n39         self.l2_regularization = l2_regularization\n40         self.max_bins = max_bins\n41         self.warm_start = warm_start\n42         self.scoring = scoring\n43         self.validation_fraction = validation_fraction\n44         self.n_iter_no_change = n_iter_no_change\n45         self.tol = tol\n46         self.verbose = verbose\n47         self.random_state = random_state\n48 \n49     def _validate_parameters(self):\n50         \"\"\"Validate parameters passed to __init__.\n51 \n52         The parameters that are directly passed to the grower are checked in\n53         TreeGrower.\"\"\"\n54 \n55         if self.loss not in self._VALID_LOSSES:\n56             raise ValueError(\n57                 \"Loss {} is not supported for {}. Accepted losses: \"\n58                 \"{}.\".format(self.loss, self.__class__.__name__,\n59                              ', '.join(self._VALID_LOSSES)))\n60 \n61         if self.learning_rate <= 0:\n62             raise ValueError('learning_rate={} must '\n63                              'be strictly positive'.format(self.learning_rate))\n64         if self.max_iter < 1:\n65             raise ValueError('max_iter={} must not be smaller '\n66                              'than 1.'.format(self.max_iter))\n67         if self.n_iter_no_change is not None and self.n_iter_no_change < 0:\n68             raise ValueError('n_iter_no_change={} must be '\n69                              'positive.'.format(self.n_iter_no_change))\n70         if (self.validation_fraction is not None and\n71                 self.validation_fraction <= 0):\n72             raise ValueError(\n73                 'validation_fraction={} must be strictly '\n74                 'positive, or None.'.format(self.validation_fraction))\n75         if self.tol is not None and self.tol < 0:\n76             raise ValueError('tol={} '\n77                              'must not be smaller than 0.'.format(self.tol))\n78 \n79         if not (2 <= self.max_bins <= 255):\n80             raise ValueError('max_bins={} should be no smaller than 2 '\n81                              'and no larger than 255.'.format(self.max_bins))\n82 \n83     def fit(self, X, y):\n84         \"\"\"Fit the gradient boosting model.\n85 \n86         Parameters\n87         ----------\n88         X : array-like, shape=(n_samples, n_features)\n89             The input samples.\n90 \n91         y : array-like, shape=(n_samples,)\n92             Target values.\n93 \n94         Returns\n95         -------\n96         self : object\n97         \"\"\"\n98         fit_start_time = time()\n99         acc_find_split_time = 0.  # time spent finding the best splits\n100         acc_apply_split_time = 0.  # time spent splitting nodes\n101         acc_compute_hist_time = 0.  # time spent computing histograms\n102         # time spent predicting X for gradient and hessians update\n103         acc_prediction_time = 0.\n104         X, y = check_X_y(X, y, dtype=[X_DTYPE], force_all_finite=False)\n105         y = self._encode_y(y)\n106 \n107         # The rng state must be preserved if warm_start is True\n108         if (self.warm_start and hasattr(self, '_rng')):\n109             rng = self._rng\n110         else:\n111             rng = check_random_state(self.random_state)\n112             self._rng = rng\n113 \n114         self._validate_parameters()\n115         self.n_features_ = X.shape[1]  # used for validation in predict()\n116 \n117         # we need this stateful variable to tell raw_predict() that it was\n118         # called from fit() (this current method), and that the data it has\n119         # received is pre-binned.\n120         # predicting is faster on pre-binned data, so we want early stopping\n121         # predictions to be made on pre-binned data. Unfortunately the scorer_\n122         # can only call predict() or predict_proba(), not raw_predict(), and\n123         # there's no way to tell the scorer that it needs to predict binned\n124         # data.\n125         self._in_fit = True\n126 \n127         self.loss_ = self._get_loss()\n128 \n129         self.do_early_stopping_ = (self.n_iter_no_change is not None and\n130                                    self.n_iter_no_change > 0)\n131 \n132         # create validation data if needed\n133         self._use_validation_data = self.validation_fraction is not None\n134         if self.do_early_stopping_ and self._use_validation_data:\n135             # stratify for classification\n136             stratify = y if hasattr(self.loss_, 'predict_proba') else None\n137 \n138             # Save the state of the RNG for the training and validation split.\n139             # This is needed in order to have the same split when using\n140             # warm starting.\n141             if not (self._is_fitted() and self.warm_start):\n142                 self._train_val_split_seed = rng.randint(1024)\n143 \n144             X_train, X_val, y_train, y_val = train_test_split(\n145                 X, y, test_size=self.validation_fraction, stratify=stratify,\n146                 random_state=self._train_val_split_seed)\n147         else:\n148             X_train, y_train = X, y\n149             X_val, y_val = None, None\n150 \n151         has_missing_values = np.isnan(X_train).any(axis=0).astype(np.uint8)\n152 \n153         # Bin the data\n154         # For ease of use of the API, the user-facing GBDT classes accept the\n155         # parameter max_bins, which doesn't take into account the bin for\n156         # missing values (which is always allocated). However, since max_bins\n157         # isn't the true maximal number of bins, all other private classes\n158         # (binmapper, histbuilder...) accept n_bins instead, which is the\n159         # actual total number of bins. Everywhere in the code, the\n160         # convention is that n_bins == max_bins + 1\n161         n_bins = self.max_bins + 1  # + 1 for missing values\n162         self.bin_mapper_ = _BinMapper(n_bins=n_bins, random_state=rng)\n163         X_binned_train = self._bin_data(X_train, rng, is_training_data=True)\n164         if X_val is not None:\n165             X_binned_val = self._bin_data(X_val, rng, is_training_data=False)\n166         else:\n167             X_binned_val = None\n168 \n169         if self.verbose:\n170             print(\"Fitting gradient boosted rounds:\")\n171 \n172         n_samples = X_binned_train.shape[0]\n173 \n174         # First time calling fit, or no warm start\n175         if not (self._is_fitted() and self.warm_start):\n176             # Clear random state and score attributes\n177             self._clear_state()\n178 \n179             # initialize raw_predictions: those are the accumulated values\n180             # predicted by the trees for the training data. raw_predictions has\n181             # shape (n_trees_per_iteration, n_samples) where\n182             # n_trees_per_iterations is n_classes in multiclass classification,\n183             # else 1.\n184             self._baseline_prediction = self.loss_.get_baseline_prediction(\n185                 y_train, self.n_trees_per_iteration_\n186             )\n187             raw_predictions = np.zeros(\n188                 shape=(self.n_trees_per_iteration_, n_samples),\n189                 dtype=self._baseline_prediction.dtype\n190             )\n191             raw_predictions += self._baseline_prediction\n192 \n193             # initialize gradients and hessians (empty arrays).\n194             # shape = (n_trees_per_iteration, n_samples).\n195             gradients, hessians = self.loss_.init_gradients_and_hessians(\n196                 n_samples=n_samples,\n197                 prediction_dim=self.n_trees_per_iteration_\n198             )\n199 \n200             # predictors is a matrix (list of lists) of TreePredictor objects\n201             # with shape (n_iter_, n_trees_per_iteration)\n202             self._predictors = predictors = []\n203 \n204             # Initialize structures and attributes related to early stopping\n205             self.scorer_ = None  # set if scoring != loss\n206             raw_predictions_val = None  # set if scoring == loss and use val\n207             self.train_score_ = []\n208             self.validation_score_ = []\n209 \n210             if self.do_early_stopping_:\n211                 # populate train_score and validation_score with the\n212                 # predictions of the initial model (before the first tree)\n213 \n214                 if self.scoring == 'loss':\n215                     # we're going to compute scoring w.r.t the loss. As losses\n216                     # take raw predictions as input (unlike the scorers), we\n217                     # can optimize a bit and avoid repeating computing the\n218                     # predictions of the previous trees. We'll re-use\n219                     # raw_predictions (as it's needed for training anyway) for\n220                     # evaluating the training loss, and create\n221                     # raw_predictions_val for storing the raw predictions of\n222                     # the validation data.\n223 \n224                     if self._use_validation_data:\n225                         raw_predictions_val = np.zeros(\n226                             shape=(self.n_trees_per_iteration_,\n227                                    X_binned_val.shape[0]),\n228                             dtype=self._baseline_prediction.dtype\n229                         )\n230 \n231                         raw_predictions_val += self._baseline_prediction\n232 \n233                     self._check_early_stopping_loss(raw_predictions, y_train,\n234                                                     raw_predictions_val, y_val)\n235                 else:\n236                     self.scorer_ = check_scoring(self, self.scoring)\n237                     # scorer_ is a callable with signature (est, X, y) and\n238                     # calls est.predict() or est.predict_proba() depending on\n239                     # its nature.\n240                     # Unfortunately, each call to scorer_() will compute\n241                     # the predictions of all the trees. So we use a subset of\n242                     # the training set to compute train scores.\n243 \n244                     # Save the seed for the small trainset generator\n245                     self._small_trainset_seed = rng.randint(1024)\n246 \n247                     # Compute the subsample set\n248                     (X_binned_small_train,\n249                      y_small_train) = self._get_small_trainset(\n250                         X_binned_train, y_train, self._small_trainset_seed)\n251 \n252                     self._check_early_stopping_scorer(\n253                         X_binned_small_train, y_small_train,\n254                         X_binned_val, y_val,\n255                     )\n256             begin_at_stage = 0\n257 \n258         # warm start: this is not the first time fit was called\n259         else:\n260             # Check that the maximum number of iterations is not smaller\n261             # than the number of iterations from the previous fit\n262             if self.max_iter < self.n_iter_:\n263                 raise ValueError(\n264                     'max_iter=%d must be larger than or equal to '\n265                     'n_iter_=%d when warm_start==True'\n266                     % (self.max_iter, self.n_iter_)\n267                 )\n268 \n269             # Convert array attributes to lists\n270             self.train_score_ = self.train_score_.tolist()\n271             self.validation_score_ = self.validation_score_.tolist()\n272 \n273             # Compute raw predictions\n274             raw_predictions = self._raw_predict(X_binned_train)\n275 \n276             if self.do_early_stopping_ and self.scoring != 'loss':\n277                 # Compute the subsample set\n278                 X_binned_small_train, y_small_train = self._get_small_trainset(\n279                     X_binned_train, y_train, self._small_trainset_seed)\n280 \n281             # Initialize the gradients and hessians\n282             gradients, hessians = self.loss_.init_gradients_and_hessians(\n283                 n_samples=n_samples,\n284                 prediction_dim=self.n_trees_per_iteration_\n285             )\n286 \n287             # Get the predictors from the previous fit\n288             predictors = self._predictors\n289 \n290             begin_at_stage = self.n_iter_\n291 \n292         for iteration in range(begin_at_stage, self.max_iter):\n293 \n294             if self.verbose:\n295                 iteration_start_time = time()\n296                 print(\"[{}/{}] \".format(iteration + 1, self.max_iter),\n297                       end='', flush=True)\n298 \n299             # Update gradients and hessians, inplace\n300             self.loss_.update_gradients_and_hessians(gradients, hessians,\n301                                                      y_train, raw_predictions)\n302 \n303             # Append a list since there may be more than 1 predictor per iter\n304             predictors.append([])\n305 \n306             # Build `n_trees_per_iteration` trees.\n307             for k in range(self.n_trees_per_iteration_):\n308 \n309                 grower = TreeGrower(\n310                     X_binned_train, gradients[k, :], hessians[k, :],\n311                     n_bins=n_bins,\n312                     n_bins_non_missing=self.bin_mapper_.n_bins_non_missing_,\n313                     has_missing_values=has_missing_values,\n314                     max_leaf_nodes=self.max_leaf_nodes,\n315                     max_depth=self.max_depth,\n316                     min_samples_leaf=self.min_samples_leaf,\n317                     l2_regularization=self.l2_regularization,\n318                     shrinkage=self.learning_rate)\n319                 grower.grow()\n320 \n321                 acc_apply_split_time += grower.total_apply_split_time\n322                 acc_find_split_time += grower.total_find_split_time\n323                 acc_compute_hist_time += grower.total_compute_hist_time\n324 \n325                 predictor = grower.make_predictor(\n326                     bin_thresholds=self.bin_mapper_.bin_thresholds_\n327                 )\n328                 predictors[-1].append(predictor)\n329 \n330                 # Update raw_predictions with the predictions of the newly\n331                 # created tree.\n332                 tic_pred = time()\n333                 _update_raw_predictions(raw_predictions[k, :], grower)\n334                 toc_pred = time()\n335                 acc_prediction_time += toc_pred - tic_pred\n336 \n337             should_early_stop = False\n338             if self.do_early_stopping_:\n339                 if self.scoring == 'loss':\n340                     # Update raw_predictions_val with the newest tree(s)\n341                     if self._use_validation_data:\n342                         for k, pred in enumerate(self._predictors[-1]):\n343                             raw_predictions_val[k, :] += (\n344                                 pred.predict_binned(\n345                                     X_binned_val,\n346                                     self.bin_mapper_.missing_values_bin_idx_\n347                                 )\n348                             )\n349 \n350                     should_early_stop = self._check_early_stopping_loss(\n351                         raw_predictions, y_train,\n352                         raw_predictions_val, y_val\n353                     )\n354 \n355                 else:\n356                     should_early_stop = self._check_early_stopping_scorer(\n357                         X_binned_small_train, y_small_train,\n358                         X_binned_val, y_val,\n359                     )\n360 \n361             if self.verbose:\n362                 self._print_iteration_stats(iteration_start_time)\n363 \n364             # maybe we could also early stop if all the trees are stumps?\n365             if should_early_stop:\n366                 break\n367 \n368         if self.verbose:\n369             duration = time() - fit_start_time\n370             n_total_leaves = sum(\n371                 predictor.get_n_leaf_nodes()\n372                 for predictors_at_ith_iteration in self._predictors\n373                 for predictor in predictors_at_ith_iteration\n374             )\n375             n_predictors = sum(\n376                 len(predictors_at_ith_iteration)\n377                 for predictors_at_ith_iteration in self._predictors)\n378             print(\"Fit {} trees in {:.3f} s, ({} total leaves)\".format(\n379                 n_predictors, duration, n_total_leaves))\n380             print(\"{:<32} {:.3f}s\".format('Time spent computing histograms:',\n381                                           acc_compute_hist_time))\n382             print(\"{:<32} {:.3f}s\".format('Time spent finding best splits:',\n383                                           acc_find_split_time))\n384             print(\"{:<32} {:.3f}s\".format('Time spent applying splits:',\n385                                           acc_apply_split_time))\n386             print(\"{:<32} {:.3f}s\".format('Time spent predicting:',\n387                                           acc_prediction_time))\n388 \n389         self.train_score_ = np.asarray(self.train_score_)\n390         self.validation_score_ = np.asarray(self.validation_score_)\n391         del self._in_fit  # hard delete so we're sure it can't be used anymore\n392         return self\n393 \n394     def _is_fitted(self):\n395         return len(getattr(self, '_predictors', [])) > 0\n396 \n397     def _clear_state(self):\n398         \"\"\"Clear the state of the gradient boosting model.\"\"\"\n399         for var in ('train_score_', 'validation_score_', '_rng'):\n400             if hasattr(self, var):\n401                 delattr(self, var)\n402 \n403     def _get_small_trainset(self, X_binned_train, y_train, seed):\n404         \"\"\"Compute the indices of the subsample set and return this set.\n405 \n406         For efficiency, we need to subsample the training set to compute scores\n407         with scorers.\n408         \"\"\"\n409         subsample_size = 10000\n410         if X_binned_train.shape[0] > subsample_size:\n411             indices = np.arange(X_binned_train.shape[0])\n412             stratify = y_train if is_classifier(self) else None\n413             indices = resample(indices, n_samples=subsample_size,\n414                                replace=False, random_state=seed,\n415                                stratify=stratify)\n416             X_binned_small_train = X_binned_train[indices]\n417             y_small_train = y_train[indices]\n418             X_binned_small_train = np.ascontiguousarray(X_binned_small_train)\n419             return X_binned_small_train, y_small_train\n420         else:\n421             return X_binned_train, y_train\n422 \n423     def _check_early_stopping_scorer(self, X_binned_small_train, y_small_train,\n424                                      X_binned_val, y_val):\n425         \"\"\"Check if fitting should be early-stopped based on scorer.\n426 \n427         Scores are computed on validation data or on training data.\n428         \"\"\"\n429         if is_classifier(self):\n430             y_small_train = self.classes_[y_small_train.astype(int)]\n431         self.train_score_.append(\n432             self.scorer_(self, X_binned_small_train, y_small_train)\n433         )\n434 \n435         if self._use_validation_data:\n436             if is_classifier(self):\n437                 y_val = self.classes_[y_val.astype(int)]\n438             self.validation_score_.append(\n439                 self.scorer_(self, X_binned_val, y_val)\n440             )\n441             return self._should_stop(self.validation_score_)\n442         else:\n443             return self._should_stop(self.train_score_)\n444 \n445     def _check_early_stopping_loss(self,\n446                                    raw_predictions,\n447                                    y_train,\n448                                    raw_predictions_val,\n449                                    y_val):\n450         \"\"\"Check if fitting should be early-stopped based on loss.\n451 \n452         Scores are computed on validation data or on training data.\n453         \"\"\"\n454 \n455         self.train_score_.append(\n456             -self.loss_(y_train, raw_predictions)\n457         )\n458 \n459         if self._use_validation_data:\n460             self.validation_score_.append(\n461                 -self.loss_(y_val, raw_predictions_val)\n462             )\n463             return self._should_stop(self.validation_score_)\n464         else:\n465             return self._should_stop(self.train_score_)\n466 \n467     def _should_stop(self, scores):\n468         \"\"\"\n469         Return True (do early stopping) if the last n scores aren't better\n470         than the (n-1)th-to-last score, up to some tolerance.\n471         \"\"\"\n472         reference_position = self.n_iter_no_change + 1\n473         if len(scores) < reference_position:\n474             return False\n475 \n476         # A higher score is always better. Higher tol means that it will be\n477         # harder for subsequent iteration to be considered an improvement upon\n478         # the reference score, and therefore it is more likely to early stop\n479         # because of the lack of significant improvement.\n480         tol = 0 if self.tol is None else self.tol\n481         reference_score = scores[-reference_position] + tol\n482         recent_scores = scores[-reference_position + 1:]\n483         recent_improvements = [score > reference_score\n484                                for score in recent_scores]\n485         return not any(recent_improvements)\n486 \n487     def _bin_data(self, X, rng, is_training_data):\n488         \"\"\"Bin data X.\n489 \n490         If is_training_data, then set the bin_mapper_ attribute.\n491         Else, the binned data is converted to a C-contiguous array.\n492         \"\"\"\n493 \n494         description = 'training' if is_training_data else 'validation'\n495         if self.verbose:\n496             print(\"Binning {:.3f} GB of {} data: \".format(\n497                 X.nbytes / 1e9, description), end=\"\", flush=True)\n498         tic = time()\n499         if is_training_data:\n500             X_binned = self.bin_mapper_.fit_transform(X)  # F-aligned array\n501         else:\n502             X_binned = self.bin_mapper_.transform(X)  # F-aligned array\n503             # We convert the array to C-contiguous since predicting is faster\n504             # with this layout (training is faster on F-arrays though)\n505             X_binned = np.ascontiguousarray(X_binned)\n506         toc = time()\n507         if self.verbose:\n508             duration = toc - tic\n509             print(\"{:.3f} s\".format(duration))\n510 \n511         return X_binned\n512 \n513     def _print_iteration_stats(self, iteration_start_time):\n514         \"\"\"Print info about the current fitting iteration.\"\"\"\n515         log_msg = ''\n516 \n517         predictors_of_ith_iteration = [\n518             predictors_list for predictors_list in self._predictors[-1]\n519             if predictors_list\n520         ]\n521         n_trees = len(predictors_of_ith_iteration)\n522         max_depth = max(predictor.get_max_depth()\n523                         for predictor in predictors_of_ith_iteration)\n524         n_leaves = sum(predictor.get_n_leaf_nodes()\n525                        for predictor in predictors_of_ith_iteration)\n526 \n527         if n_trees == 1:\n528             log_msg += (\"{} tree, {} leaves, \".format(n_trees, n_leaves))\n529         else:\n530             log_msg += (\"{} trees, {} leaves \".format(n_trees, n_leaves))\n531             log_msg += (\"({} on avg), \".format(int(n_leaves / n_trees)))\n532 \n533         log_msg += \"max depth = {}, \".format(max_depth)\n534 \n535         if self.do_early_stopping_:\n536             if self.scoring == 'loss':\n537                 factor = -1  # score_ arrays contain the negative loss\n538                 name = 'loss'\n539             else:\n540                 factor = 1\n541                 name = 'score'\n542             log_msg += \"train {}: {:.5f}, \".format(name, factor *\n543                                                    self.train_score_[-1])\n544             if self._use_validation_data:\n545                 log_msg += \"val {}: {:.5f}, \".format(\n546                     name, factor * self.validation_score_[-1])\n547 \n548         iteration_time = time() - iteration_start_time\n549         log_msg += \"in {:0.3f}s\".format(iteration_time)\n550 \n551         print(log_msg)\n552 \n553     def _raw_predict(self, X):\n554         \"\"\"Return the sum of the leaves values over all predictors.\n555 \n556         Parameters\n557         ----------\n558         X : array-like, shape=(n_samples, n_features)\n559             The input samples.\n560 \n561         Returns\n562         -------\n563         raw_predictions : array, shape (n_samples * n_trees_per_iteration,)\n564             The raw predicted values.\n565         \"\"\"\n566         X = check_array(X, dtype=[X_DTYPE, X_BINNED_DTYPE],\n567                         force_all_finite=False)\n568         check_is_fitted(self)\n569         if X.shape[1] != self.n_features_:\n570             raise ValueError(\n571                 'X has {} features but this estimator was trained with '\n572                 '{} features.'.format(X.shape[1], self.n_features_)\n573             )\n574         is_binned = getattr(self, '_in_fit', False)\n575         n_samples = X.shape[0]\n576         raw_predictions = np.zeros(\n577             shape=(self.n_trees_per_iteration_, n_samples),\n578             dtype=self._baseline_prediction.dtype\n579         )\n580         raw_predictions += self._baseline_prediction\n581         for predictors_of_ith_iteration in self._predictors:\n582             for k, predictor in enumerate(predictors_of_ith_iteration):\n583                 if is_binned:\n584                     predict = partial(\n585                         predictor.predict_binned,\n586                         missing_values_bin_idx=self.bin_mapper_.missing_values_bin_idx_  # noqa\n587                     )\n588                 else:\n589                     predict = predictor.predict\n590                 raw_predictions[k, :] += predict(X)\n591 \n592         return raw_predictions\n593 \n594     def _compute_partial_dependence_recursion(self, grid, target_features):\n595         \"\"\"Fast partial dependence computation.\n596 \n597         Parameters\n598         ----------\n599         grid : ndarray, shape (n_samples, n_target_features)\n600             The grid points on which the partial dependence should be\n601             evaluated.\n602         target_features : ndarray, shape (n_target_features)\n603             The set of target features for which the partial dependence\n604             should be evaluated.\n605 \n606         Returns\n607         -------\n608         averaged_predictions : ndarray, shape \\\n609                 (n_trees_per_iteration, n_samples)\n610             The value of the partial dependence function on each grid point.\n611         \"\"\"\n612         grid = np.asarray(grid, dtype=X_DTYPE, order='C')\n613         averaged_predictions = np.zeros(\n614             (self.n_trees_per_iteration_, grid.shape[0]), dtype=Y_DTYPE)\n615 \n616         for predictors_of_ith_iteration in self._predictors:\n617             for k, predictor in enumerate(predictors_of_ith_iteration):\n618                 predictor.compute_partial_dependence(grid, target_features,\n619                                                      averaged_predictions[k])\n620         # Note that the learning rate is already accounted for in the leaves\n621         # values.\n622 \n623         return averaged_predictions\n624 \n625     def _more_tags(self):\n626         return {'allow_nan': True}\n627 \n628     @abstractmethod\n629     def _get_loss(self):\n630         pass\n631 \n632     @abstractmethod\n633     def _encode_y(self, y=None):\n634         pass\n635 \n636     @property\n637     def n_iter_(self):\n638         check_is_fitted(self)\n639         return len(self._predictors)\n",
        "intended_behavior": "This class provides additional context to the issue."
    }
]