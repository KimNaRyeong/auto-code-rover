2025-10-18 17:23:15.761 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 17:23:19.984 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=277, output_tokens=12, cost=0.003130
2025-10-18 17:23:31.306 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=457, output_tokens=499, cost=0.019540
2025-10-18 17:23:32.366 | INFO     | registered non reproducing test 0
2025-10-18 17:23:45.309 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1796, output_tokens=619, cost=0.036530
2025-10-18 17:23:46.367 | INFO     | registered non reproducing test 1
2025-10-18 17:24:02.143 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1924, output_tokens=651, cost=0.038770
2025-10-18 17:24:03.201 | INFO     | registered non reproducing test 2
2025-10-18 17:24:03.201 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-18 17:24:03.208 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 17:24:06.616 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=581, output_tokens=127, cost=0.009620
2025-10-18 17:24:06.621 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 17:24:10.876 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=481, output_tokens=77, cost=0.007120
2025-10-18 17:24:10.878 | DEBUG    | Extracted a valid json.
2025-10-18 17:24:10.878 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"HistGradientBoostingClassifier\")",
        "search_method_in_class(\"fit\", \"HistGradientBoostingClassifier\")",
        "search_method_in_class(\"__init__\", \"HistGradientBoostingClassifier\")",
        "search_code(\"loss='categorical_crossentropy'\")"
    ],
    "bug_locations": []
}
2025-10-18 17:24:10.927 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 17:24:10.928 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 17:24:25.025 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1564, output_tokens=727, cost=0.037450
2025-10-18 17:24:25.040 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 17:24:29.131 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2507, output_tokens=145, cost=0.029420
2025-10-18 17:24:29.136 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 17:24:32.801 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=499, output_tokens=92, cost=0.007750
2025-10-18 17:24:32.803 | DEBUG    | Extracted a valid json.
2025-10-18 17:24:32.803 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_method_in_class(\"_validate_data\", \"HistGradientBoostingClassifier\")",
        "search_method_in_class(\"_get_loss\", \"HistGradientBoostingClassifier\")",
        "search_class(\"BaseHistGradientBoosting\")",
        "search_method_in_file(\"fit\", \"sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\")"
    ],
    "bug_locations": []
}
2025-10-18 17:24:32.944 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 17:24:32.944 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 17:24:50.242 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=9674, output_tokens=680, cost=0.117140
2025-10-18 17:24:50.256 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 17:25:03.365 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=10570, output_tokens=420, cost=0.118300
2025-10-18 17:25:03.374 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 17:25:12.956 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=774, output_tokens=321, cost=0.017370
2025-10-18 17:25:12.957 | DEBUG    | Extracted a valid json.
2025-10-18 17:25:12.957 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py",
            "class": "HistGradientBoostingClassifier",
            "method": "_get_loss",
            "intended_behavior": "The `_get_loss` method should include an additional check to ascertain whether `categorical_crossentropy` is being used in a binary classification scenario erroneously. Given the binary classification case (`n_trees_per_iteration_ == 1`), if `self.loss` is explicitly set to `categorical_crossentropy`, the method should raise a ValueError indicating the misuse and suggesting the use of `binary_crossentropy` or setting `loss='auto'` for automatic selection. This change ensures users are aware of the correct loss function usage in binary classification scenarios, preventing silent failures."
        },
        {
            "file": "sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py",
            "class": "BaseHistGradientBoosting",
            "method": "fit",
            "intended_behavior": "During the `fit` method's execution, especially after the call to `_get_loss`, an additional validation step could be incorporated to check the suitability of the loss function chosen for the data it is being applied to. This may include verifying that the selected loss function aligns with the type of classification task being performed. While not directly necessary to fix the reported issue, this could enhance robustness and user guidance, preventing similar issues from arising in different contexts or with other parameters."
        }
    ]
}
2025-10-18 17:25:12.962 | DEBUG    | Found override of _get_loss in BaseHistGradientBoosting
2025-10-18 17:25:12.963 | ERROR    | SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
Traceback (most recent call last):

  File "/home/kimnal0/auto-code-rover/app/main.py", line 656, in <module>
    main()
    └ <function main at 0x761e6b526c00>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 120, in main
    run_task_groups(groups, num_processes, organize_output=True)
    │               │       └ 1
    │               └ {'setup_astropy__astropy__4.3': [<app.raw_tasks.RawSweTask object at 0x761d78dc37d0>, <app.raw_tasks.RawSweTask object at 0x7...
    └ <function run_task_groups at 0x761d7b11a3e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 396, in run_task_groups
    run_tasks_serial(all_tasks)
    │                └ [<app.raw_tasks.RawSweTask object at 0x761d78dc37d0>, <app.raw_tasks.RawSweTask object at 0x761d78dc3800>, <app.raw_tasks.Raw...
    └ <function run_tasks_serial at 0x761d7b11a480>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 414, in run_tasks_serial
    run_task_in_subprocess(task)
    │                      └ <app.raw_tasks.RawSweTask object at 0x761d78d3f890>
    └ <function run_task_in_subprocess at 0x761d7b11a660>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 462, in run_task_in_subprocess
    executor.submit(run_raw_task, task)
    │        │      │             └ <app.raw_tasks.RawSweTask object at 0x761d78d3f890>
    │        │      └ <function run_raw_task at 0x761d7b11a700>
    │        └ <function ProcessPoolExecutor.submit at 0x761e6ae72020>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x761d7b1d56d0>

  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 831, in submit
    self._start_executor_manager_thread()
    │    └ <function ProcessPoolExecutor._start_executor_manager_thread at 0x761e6ae71da0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x761d7b1d56d0>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 770, in _start_executor_manager_thread
    self._launch_processes()
    │    └ <function ProcessPoolExecutor._launch_processes at 0x761e6ae71ee0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x761d7b1d56d0>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 797, in _launch_processes
    self._spawn_process()
    │    └ <function ProcessPoolExecutor._spawn_process at 0x761e6ae71f80>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x761d7b1d56d0>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 807, in _spawn_process
    p.start()
    │ └ <function BaseProcess.start at 0x761e6b02cea0>
    └ <ForkProcess name='ForkProcess-356' parent=497161 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
    │    │        │    │      └ <ForkProcess name='ForkProcess-356' parent=497161 started>
    │    │        │    └ <staticmethod(<function ForkProcess._Popen at 0x761e6b09e0c0>)>
    │    │        └ <ForkProcess name='ForkProcess-356' parent=497161 started>
    │    └ None
    └ <ForkProcess name='ForkProcess-356' parent=497161 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/context.py", line 282, in _Popen
    return Popen(process_obj)
           │     └ <ForkProcess name='ForkProcess-356' parent=497161 started>
           └ <class 'multiprocessing.popen_fork.Popen'>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
    │    │       └ <ForkProcess name='ForkProcess-356' parent=497161 started>
    │    └ <function Popen._launch at 0x761d7911b7e0>
    └ <multiprocessing.popen_fork.Popen object at 0x761d7a57e9f0>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
           │           │                          └ 13
           │           └ <function BaseProcess._bootstrap at 0x761e6b02d8a0>
           └ <ForkProcess name='ForkProcess-356' parent=497161 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x761e6b02ce00>
    └ <ForkProcess name='ForkProcess-356' parent=497161 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {}
    │    │        │    │        └ <ForkProcess name='ForkProcess-356' parent=497161 started>
    │    │        │    └ (<concurrent.futures.process._SafeQueue object at 0x761d79006a50>, <multiprocessing.queues.SimpleQueue object at 0x761d795c6d...
    │    │        └ <ForkProcess name='ForkProcess-356' parent=497161 started>
    │    └ <function _process_worker at 0x761e6ae711c0>
    └ <ForkProcess name='ForkProcess-356' parent=497161 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 263, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
        │         │   │         │       │         └ {}
        │         │   │         │       └ <concurrent.futures.process._CallItem object at 0x761d7b442db0>
        │         │   │         └ (<app.raw_tasks.RawSweTask object at 0x761d7a57e180>,)
        │         │   └ <concurrent.futures.process._CallItem object at 0x761d7b442db0>
        │         └ <function run_raw_task at 0x761d7b11a700>
        └ <concurrent.futures.process._CallItem object at 0x761d7b442db0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 494, in run_raw_task
    run_ok = do_inference(task.to_task(), task_output_dir)
             │            │    │          └ '/home/kimnal0/auto-code-rover/only_fl_output5/scikit-learn__scikit-learn-14869_2025-10-18_17-23-15'
             │            │    └ <function RawSweTask.to_task at 0x761d7b1196c0>
             │            └ <app.raw_tasks.RawSweTask object at 0x761d7a57e180>
             └ <function do_inference at 0x761d7b11a8e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 590, in do_inference
    run_ok = inference.run_one_task(
             │         └ <function run_one_task at 0x761d7b228400>
             └ <module 'app.inference' from '/home/kimnal0/auto-code-rover/app/inference.py'>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 128, in run_one_task
    if _run_one_task(str(out_dir), api_manager, task.get_issue_statement()):
       │                 │         │            │    └ <function SweTask.get_issue_statement at 0x761d7b9b84a0>
       │                 │         │            └ SweTask(task_id='scikit-learn__scikit-learn-14869', problem_statement="HGBC with categorical_crossentropy fails silently on b...
       │                 │         └ <app.manage.ProjectApiManager object at 0x761d7b6a3170>
       │                 └ Path('/home/kimnal0/auto-code-rover/only_fl_output5/scikit-learn__scikit-learn-14869_2025-10-18_17-23-15/output_0')
       └ <function _run_one_task at 0x761d7b2298a0>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 303, in _run_one_task
    bug_locs, search_msg_thread = api_manager.search_manager.search_iterative(
                                  │           │              └ <function SearchManager.search_iterative at 0x761d7b431bc0>
                                  │           └ <app.search.search_manage.SearchManager object at 0x761d795c7ec0>
                                  └ <app.manage.ProjectApiManager object at 0x761d7b6a3170>

  File "/home/kimnal0/auto-code-rover/app/search/search_manage.py", line 125, in search_iterative
    new_bug_locations.extend(self.backend.get_bug_loc_snippets_new(loc))
    │                 │      │    │       │                        └ {'file': 'sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py', 'class': 'HistGradientBoostingClassifier', 'method'...
    │                 │      │    │       └ <function SearchBackend.get_bug_loc_snippets_new at 0x761d7b4318a0>
    │                 │      │    └ <app.search.search_backend.SearchBackend object at 0x761d7b21d610>
    │                 │      └ <app.search.search_manage.SearchManager object at 0x761d795c7ec0>
    │                 └ <method 'extend' of 'list' objects>
    └ []

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 825, in get_bug_loc_snippets_new
    self._get_inherited_methods(res.class_name, res.func_name)
    │    │                      │   │           │   └ '_get_loss'
    │    │                      │   │           └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit...
    │    │                      │   └ 'HistGradientBoostingClassifier'
    │    │                      └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit...
    │    └ <function SearchBackend._get_inherited_methods at 0x761d7b431800>
    └ <app.search.search_backend.SearchBackend object at 0x761d7b21d610>

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 748, in _get_inherited_methods
    output, search_res, call_ok = self.search_method_in_class(super_call)
                                  │    │                      └ {'class_name': 'BaseHistGradientBoosting', 'method_name': '_get_loss'}
                                  │    └ <function SearchBackend.search_method_in_class at 0x761d7b430f40>
                                  └ <app.search.search_backend.SearchBackend object at 0x761d7b21d610>

> File "/home/kimnal0/auto-code-rover/app/utils.py", line 343, in wrapper
    return func(*args, **kwargs)
           │     │       └ {}
           │     └ (<app.search.search_backend.SearchBackend object at 0x761d7b21d610>, {'class_name': 'BaseHistGradientBoosting', 'method_name'...
           └ <function SearchBackend.search_method_in_class at 0x761d7b430ea0>

TypeError: SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
2025-10-18 17:25:12.972 | DEBUG    | Bug location extracted successfully: [<file>sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py</file>
<class>HistGradientBoostingClassifier</class>
<method>_get_loss</method>
<code>
1041     def _get_loss(self):
1042         if self.loss == 'auto':
1043             if self.n_trees_per_iteration_ == 1:
1044                 return _LOSSES['binary_crossentropy']()
1045             else:
1046                 return _LOSSES['categorical_crossentropy']()
1047 
1048         return _LOSSES[self.loss]()

</code><intended_behavior>The `_get_loss` method should include an additional check to ascertain whether `categorical_crossentropy` is being used in a binary classification scenario erroneously. Given the binary classification case (`n_trees_per_iteration_ == 1`), if `self.loss` is explicitly set to `categorical_crossentropy`, the method should raise a ValueError indicating the misuse and suggesting the use of `binary_crossentropy` or setting `loss='auto'` for automatic selection. This change ensures users are aware of the correct loss function usage in binary classification scenarios, preventing silent failures.</intended_behavior>, <file>sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py</file>
<class>HistGradientBoostingClassifier</class>
<method>None</method>
<code>
818 class HistGradientBoostingClassifier(BaseHistGradientBoosting,
819                                      ClassifierMixin):
820     """Histogram-based Gradient Boosting Classification Tree.
821 
822     This estimator is much faster than
823     :class:`GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`
824     for big datasets (n_samples >= 10 000).
825 
826     This estimator has native support for missing values (NaNs). During
827     training, the tree grower learns at each split point whether samples
828     with missing values should go to the left or right child, based on the
829     potential gain. When predicting, samples with missing values are
830     assigned to the left or right child consequently. If no missing values
831     were encountered for a given feature during training, then samples with
832     missing values are mapped to whichever child has the most samples.
833 
834     This implementation is inspired by
835     `LightGBM <https://github.com/Microsoft/LightGBM>`_.
836 
837     .. note::
838 
839       This estimator is still **experimental** for now: the predictions
840       and the API might change without any deprecation cycle. To use it,
841       you need to explicitly import ``enable_hist_gradient_boosting``::
842 
843         >>> # explicitly require this experimental feature
844         >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa
845         >>> # now you can import normally from ensemble
846         >>> from sklearn.ensemble import HistGradientBoostingClassifier
847 
848     Read more in the :ref:`User Guide <histogram_based_gradient_boosting>`.
849 
850     Parameters
851     ----------
852     loss : {'auto', 'binary_crossentropy', 'categorical_crossentropy'}, \
853             optional (default='auto')
854         The loss function to use in the boosting process. 'binary_crossentropy'
855         (also known as logistic loss) is used for binary classification and
856         generalizes to 'categorical_crossentropy' for multiclass
857         classification. 'auto' will automatically choose either loss depending
858         on the nature of the problem.
859     learning_rate : float, optional (default=0.1)
860         The learning rate, also known as *shrinkage*. This is used as a
861         multiplicative factor for the leaves values. Use ``1`` for no
862         shrinkage.
863     max_iter : int, optional (default=100)
864         The maximum number of iterations of the boosting process, i.e. the
865         maximum number of trees for binary classification. For multiclass
866         classification, `n_classes` trees per iteration are built.
867     max_leaf_nodes : int or None, optional (default=31)
868         The maximum number of leaves for each tree. Must be strictly greater
869         than 1. If None, there is no maximum limit.
870     max_depth : int or None, optional (default=None)
871         The maximum depth of each tree. The depth of a tree is the number of
872         nodes to go from the root to the deepest leaf. Must be strictly greater
873         than 1. Depth isn't constrained by default.
874     min_samples_leaf : int, optional (default=20)
875         The minimum number of samples per leaf. For small datasets with less
876         than a few hundred samples, it is recommended to lower this value
877         since only very shallow trees would be built.
878     l2_regularization : float, optional (default=0)
879         The L2 regularization parameter. Use 0 for no regularization.
880     max_bins : int, optional (default=255)
881         The maximum number of bins to use for non-missing values. Before
882         training, each feature of the input array `X` is binned into
883         integer-valued bins, which allows for a much faster training stage.
884         Features with a small number of unique values may use less than
885         ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin
886         is always reserved for missing values. Must be no larger than 255.
887     warm_start : bool, optional (default=False)
888         When set to ``True``, reuse the solution of the previous call to fit
889         and add more estimators to the ensemble. For results to be valid, the
890         estimator should be re-trained on the same data only.
891         See :term:`the Glossary <warm_start>`.
892     scoring : str or callable or None, optional (default=None)
893         Scoring parameter to use for early stopping. It can be a single
894         string (see :ref:`scoring_parameter`) or a callable (see
895         :ref:`scoring`). If None, the estimator's default scorer
896         is used. If ``scoring='loss'``, early stopping is checked
897         w.r.t the loss value. Only used if ``n_iter_no_change`` is not None.
898     validation_fraction : int or float or None, optional (default=0.1)
899         Proportion (or absolute size) of training data to set aside as
900         validation data for early stopping. If None, early stopping is done on
901         the training data.
902     n_iter_no_change : int or None, optional (default=None)
903         Used to determine when to "early stop". The fitting process is
904         stopped when none of the last ``n_iter_no_change`` scores are better
905         than the ``n_iter_no_change - 1`` -th-to-last one, up to some
906         tolerance. If None or 0, no early-stopping is done.
907     tol : float or None, optional (default=1e-7)
908         The absolute tolerance to use when comparing scores. The higher the
909         tolerance, the more likely we are to early stop: higher tolerance
910         means that it will be harder for subsequent iterations to be
911         considered an improvement upon the reference score.
912     verbose: int, optional (default=0)
913         The verbosity level. If not zero, print some information about the
914         fitting process.
915     random_state : int, np.random.RandomStateInstance or None, \
916         optional (default=None)
917         Pseudo-random number generator to control the subsampling in the
918         binning process, and the train/validation data split if early stopping
919         is enabled. See :term:`random_state`.
920 
921     Attributes
922     ----------
923     n_iter_ : int
924         The number of estimators as selected by early stopping (if
925         n_iter_no_change is not None). Otherwise it corresponds to max_iter.
926     n_trees_per_iteration_ : int
927         The number of tree that are built at each iteration. This is equal to 1
928         for binary classification, and to ``n_classes`` for multiclass
929         classification.
930     train_score_ : ndarray, shape (n_iter_ + 1,)
931         The scores at each iteration on the training data. The first entry
932         is the score of the ensemble before the first iteration. Scores are
933         computed according to the ``scoring`` parameter. If ``scoring`` is
934         not 'loss', scores are computed on a subset of at most 10 000
935         samples. Empty if no early stopping.
936     validation_score_ : ndarray, shape (n_iter_ + 1,)
937         The scores at each iteration on the held-out validation data. The
938         first entry is the score of the ensemble before the first iteration.
939         Scores are computed according to the ``scoring`` parameter. Empty if
940         no early stopping or if ``validation_fraction`` is None.
941 
942     Examples
943     --------
944     >>> # To use this experimental feature, we need to explicitly ask for it:
945     >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa
946     >>> from sklearn.ensemble import HistGradientBoostingRegressor
947     >>> from sklearn.datasets import load_iris
948     >>> X, y = load_iris(return_X_y=True)
949     >>> clf = HistGradientBoostingClassifier().fit(X, y)
950     >>> clf.score(X, y)
951     1.0
952     """
953 
954     _VALID_LOSSES = ('binary_crossentropy', 'categorical_crossentropy',
955                      'auto')
956 
957     def __init__(self, loss='auto', learning_rate=0.1, max_iter=100,
958                  max_leaf_nodes=31, max_depth=None, min_samples_leaf=20,
959                  l2_regularization=0., max_bins=255, warm_start=False,
960                  scoring=None, validation_fraction=0.1, n_iter_no_change=None,
961                  tol=1e-7, verbose=0, random_state=None):
962         super(HistGradientBoostingClassifier, self).__init__(
963             loss=loss, learning_rate=learning_rate, max_iter=max_iter,
964             max_leaf_nodes=max_leaf_nodes, max_depth=max_depth,
965             min_samples_leaf=min_samples_leaf,
966             l2_regularization=l2_regularization, max_bins=max_bins,
967             warm_start=warm_start, scoring=scoring,
968             validation_fraction=validation_fraction,
969             n_iter_no_change=n_iter_no_change, tol=tol, verbose=verbose,
970             random_state=random_state)
971 
972     def predict(self, X):
973         """Predict classes for X.
974 
975         Parameters
976         ----------
977         X : array-like, shape (n_samples, n_features)
978             The input samples.
979 
980         Returns
981         -------
982         y : ndarray, shape (n_samples,)
983             The predicted classes.
984         """
985         # TODO: This could be done in parallel
986         encoded_classes = np.argmax(self.predict_proba(X), axis=1)
987         return self.classes_[encoded_classes]
988 
989     def predict_proba(self, X):
990         """Predict class probabilities for X.
991 
992         Parameters
993         ----------
994         X : array-like, shape (n_samples, n_features)
995             The input samples.
996 
997         Returns
998         -------
999         p : ndarray, shape (n_samples, n_classes)
1000             The class probabilities of the input samples.
1001         """
1002         raw_predictions = self._raw_predict(X)
1003         return self.loss_.predict_proba(raw_predictions)
1004 
1005     def decision_function(self, X):
1006         """Compute the decision function of X.
1007 
1008         Parameters
1009         ----------
1010         X : array-like, shape (n_samples, n_features)
1011             The input samples.
1012 
1013         Returns
1014         -------
1015         decision : ndarray, shape (n_samples,) or \
1016                 (n_samples, n_trees_per_iteration)
1017             The raw predicted values (i.e. the sum of the trees leaves) for
1018             each sample. n_trees_per_iteration is equal to the number of
1019             classes in multiclass classification.
1020         """
1021         decision = self._raw_predict(X)
1022         if decision.shape[0] == 1:
1023             decision = decision.ravel()
1024         return decision.T
1025 
1026     def _encode_y(self, y):
1027         # encode classes into 0 ... n_classes - 1 and sets attributes classes_
1028         # and n_trees_per_iteration_
1029         check_classification_targets(y)
1030 
1031         label_encoder = LabelEncoder()
1032         encoded_y = label_encoder.fit_transform(y)
1033         self.classes_ = label_encoder.classes_
1034         n_classes = self.classes_.shape[0]
1035         # only 1 tree for binary classification. For multiclass classification,
1036         # we build 1 tree per class.
1037         self.n_trees_per_iteration_ = 1 if n_classes <= 2 else n_classes
1038         encoded_y = encoded_y.astype(Y_DTYPE, copy=False)
1039         return encoded_y
1040 
1041     def _get_loss(self):
1042         if self.loss == 'auto':
1043             if self.n_trees_per_iteration_ == 1:
1044                 return _LOSSES['binary_crossentropy']()
1045             else:
1046                 return _LOSSES['categorical_crossentropy']()
1047 
1048         return _LOSSES[self.loss]()

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py</file>
<class>BaseHistGradientBoosting</class>
<method>fit</method>
<code>
83     def fit(self, X, y):
84         """Fit the gradient boosting model.
85 
86         Parameters
87         ----------
88         X : array-like, shape=(n_samples, n_features)
89             The input samples.
90 
91         y : array-like, shape=(n_samples,)
92             Target values.
93 
94         Returns
95         -------
96         self : object
97         """
98         fit_start_time = time()
99         acc_find_split_time = 0.  # time spent finding the best splits
100         acc_apply_split_time = 0.  # time spent splitting nodes
101         acc_compute_hist_time = 0.  # time spent computing histograms
102         # time spent predicting X for gradient and hessians update
103         acc_prediction_time = 0.
104         X, y = check_X_y(X, y, dtype=[X_DTYPE], force_all_finite=False)
105         y = self._encode_y(y)
106 
107         # The rng state must be preserved if warm_start is True
108         if (self.warm_start and hasattr(self, '_rng')):
109             rng = self._rng
110         else:
111             rng = check_random_state(self.random_state)
112             self._rng = rng
113 
114         self._validate_parameters()
115         self.n_features_ = X.shape[1]  # used for validation in predict()
116 
117         # we need this stateful variable to tell raw_predict() that it was
118         # called from fit() (this current method), and that the data it has
119         # received is pre-binned.
120         # predicting is faster on pre-binned data, so we want early stopping
121         # predictions to be made on pre-binned data. Unfortunately the scorer_
122         # can only call predict() or predict_proba(), not raw_predict(), and
123         # there's no way to tell the scorer that it needs to predict binned
124         # data.
125         self._in_fit = True
126 
127         self.loss_ = self._get_loss()
128 
129         self.do_early_stopping_ = (self.n_iter_no_change is not None and
130                                    self.n_iter_no_change > 0)
131 
132         # create validation data if needed
133         self._use_validation_data = self.validation_fraction is not None
134         if self.do_early_stopping_ and self._use_validation_data:
135             # stratify for classification
136             stratify = y if hasattr(self.loss_, 'predict_proba') else None
137 
138             # Save the state of the RNG for the training and validation split.
139             # This is needed in order to have the same split when using
140             # warm starting.
141             if not (self._is_fitted() and self.warm_start):
142                 self._train_val_split_seed = rng.randint(1024)
143 
144             X_train, X_val, y_train, y_val = train_test_split(
145                 X, y, test_size=self.validation_fraction, stratify=stratify,
146                 random_state=self._train_val_split_seed)
147         else:
148             X_train, y_train = X, y
149             X_val, y_val = None, None
150 
151         has_missing_values = np.isnan(X_train).any(axis=0).astype(np.uint8)
152 
153         # Bin the data
154         # For ease of use of the API, the user-facing GBDT classes accept the
155         # parameter max_bins, which doesn't take into account the bin for
156         # missing values (which is always allocated). However, since max_bins
157         # isn't the true maximal number of bins, all other private classes
158         # (binmapper, histbuilder...) accept n_bins instead, which is the
159         # actual total number of bins. Everywhere in the code, the
160         # convention is that n_bins == max_bins + 1
161         n_bins = self.max_bins + 1  # + 1 for missing values
162         self.bin_mapper_ = _BinMapper(n_bins=n_bins, random_state=rng)
163         X_binned_train = self._bin_data(X_train, rng, is_training_data=True)
164         if X_val is not None:
165             X_binned_val = self._bin_data(X_val, rng, is_training_data=False)
166         else:
167             X_binned_val = None
168 
169         if self.verbose:
170             print("Fitting gradient boosted rounds:")
171 
172         n_samples = X_binned_train.shape[0]
173 
174         # First time calling fit, or no warm start
175         if not (self._is_fitted() and self.warm_start):
176             # Clear random state and score attributes
177             self._clear_state()
178 
179             # initialize raw_predictions: those are the accumulated values
180             # predicted by the trees for the training data. raw_predictions has
181             # shape (n_trees_per_iteration, n_samples) where
182             # n_trees_per_iterations is n_classes in multiclass classification,
183             # else 1.
184             self._baseline_prediction = self.loss_.get_baseline_prediction(
185                 y_train, self.n_trees_per_iteration_
186             )
187             raw_predictions = np.zeros(
188                 shape=(self.n_trees_per_iteration_, n_samples),
189                 dtype=self._baseline_prediction.dtype
190             )
191             raw_predictions += self._baseline_prediction
192 
193             # initialize gradients and hessians (empty arrays).
194             # shape = (n_trees_per_iteration, n_samples).
195             gradients, hessians = self.loss_.init_gradients_and_hessians(
196                 n_samples=n_samples,
197                 prediction_dim=self.n_trees_per_iteration_
198             )
199 
200             # predictors is a matrix (list of lists) of TreePredictor objects
201             # with shape (n_iter_, n_trees_per_iteration)
202             self._predictors = predictors = []
203 
204             # Initialize structures and attributes related to early stopping
205             self.scorer_ = None  # set if scoring != loss
206             raw_predictions_val = None  # set if scoring == loss and use val
207             self.train_score_ = []
208             self.validation_score_ = []
209 
210             if self.do_early_stopping_:
211                 # populate train_score and validation_score with the
212                 # predictions of the initial model (before the first tree)
213 
214                 if self.scoring == 'loss':
215                     # we're going to compute scoring w.r.t the loss. As losses
216                     # take raw predictions as input (unlike the scorers), we
217                     # can optimize a bit and avoid repeating computing the
218                     # predictions of the previous trees. We'll re-use
219                     # raw_predictions (as it's needed for training anyway) for
220                     # evaluating the training loss, and create
221                     # raw_predictions_val for storing the raw predictions of
222                     # the validation data.
223 
224                     if self._use_validation_data:
225                         raw_predictions_val = np.zeros(
226                             shape=(self.n_trees_per_iteration_,
227                                    X_binned_val.shape[0]),
228                             dtype=self._baseline_prediction.dtype
229                         )
230 
231                         raw_predictions_val += self._baseline_prediction
232 
233                     self._check_early_stopping_loss(raw_predictions, y_train,
234                                                     raw_predictions_val, y_val)
235                 else:
236                     self.scorer_ = check_scoring(self, self.scoring)
237                     # scorer_ is a callable with signature (est, X, y) and
238                     # calls est.predict() or est.predict_proba() depending on
239                     # its nature.
240                     # Unfortunately, each call to scorer_() will compute
241                     # the predictions of all the trees. So we use a subset of
242                     # the training set to compute train scores.
243 
244                     # Save the seed for the small trainset generator
245                     self._small_trainset_seed = rng.randint(1024)
246 
247                     # Compute the subsample set
248                     (X_binned_small_train,
249                      y_small_train) = self._get_small_trainset(
250                         X_binned_train, y_train, self._small_trainset_seed)
251 
252                     self._check_early_stopping_scorer(
253                         X_binned_small_train, y_small_train,
254                         X_binned_val, y_val,
255                     )
256             begin_at_stage = 0
257 
258         # warm start: this is not the first time fit was called
259         else:
260             # Check that the maximum number of iterations is not smaller
261             # than the number of iterations from the previous fit
262             if self.max_iter < self.n_iter_:
263                 raise ValueError(
264                     'max_iter=%d must be larger than or equal to '
265                     'n_iter_=%d when warm_start==True'
266                     % (self.max_iter, self.n_iter_)
267                 )
268 
269             # Convert array attributes to lists
270             self.train_score_ = self.train_score_.tolist()
271             self.validation_score_ = self.validation_score_.tolist()
272 
273             # Compute raw predictions
274             raw_predictions = self._raw_predict(X_binned_train)
275 
276             if self.do_early_stopping_ and self.scoring != 'loss':
277                 # Compute the subsample set
278                 X_binned_small_train, y_small_train = self._get_small_trainset(
279                     X_binned_train, y_train, self._small_trainset_seed)
280 
281             # Initialize the gradients and hessians
282             gradients, hessians = self.loss_.init_gradients_and_hessians(
283                 n_samples=n_samples,
284                 prediction_dim=self.n_trees_per_iteration_
285             )
286 
287             # Get the predictors from the previous fit
288             predictors = self._predictors
289 
290             begin_at_stage = self.n_iter_
291 
292         for iteration in range(begin_at_stage, self.max_iter):
293 
294             if self.verbose:
295                 iteration_start_time = time()
296                 print("[{}/{}] ".format(iteration + 1, self.max_iter),
297                       end='', flush=True)
298 
299             # Update gradients and hessians, inplace
300             self.loss_.update_gradients_and_hessians(gradients, hessians,
301                                                      y_train, raw_predictions)
302 
303             # Append a list since there may be more than 1 predictor per iter
304             predictors.append([])
305 
306             # Build `n_trees_per_iteration` trees.
307             for k in range(self.n_trees_per_iteration_):
308 
309                 grower = TreeGrower(
310                     X_binned_train, gradients[k, :], hessians[k, :],
311                     n_bins=n_bins,
312                     n_bins_non_missing=self.bin_mapper_.n_bins_non_missing_,
313                     has_missing_values=has_missing_values,
314                     max_leaf_nodes=self.max_leaf_nodes,
315                     max_depth=self.max_depth,
316                     min_samples_leaf=self.min_samples_leaf,
317                     l2_regularization=self.l2_regularization,
318                     shrinkage=self.learning_rate)
319                 grower.grow()
320 
321                 acc_apply_split_time += grower.total_apply_split_time
322                 acc_find_split_time += grower.total_find_split_time
323                 acc_compute_hist_time += grower.total_compute_hist_time
324 
325                 predictor = grower.make_predictor(
326                     bin_thresholds=self.bin_mapper_.bin_thresholds_
327                 )
328                 predictors[-1].append(predictor)
329 
330                 # Update raw_predictions with the predictions of the newly
331                 # created tree.
332                 tic_pred = time()
333                 _update_raw_predictions(raw_predictions[k, :], grower)
334                 toc_pred = time()
335                 acc_prediction_time += toc_pred - tic_pred
336 
337             should_early_stop = False
338             if self.do_early_stopping_:
339                 if self.scoring == 'loss':
340                     # Update raw_predictions_val with the newest tree(s)
341                     if self._use_validation_data:
342                         for k, pred in enumerate(self._predictors[-1]):
343                             raw_predictions_val[k, :] += (
344                                 pred.predict_binned(
345                                     X_binned_val,
346                                     self.bin_mapper_.missing_values_bin_idx_
347                                 )
348                             )
349 
350                     should_early_stop = self._check_early_stopping_loss(
351                         raw_predictions, y_train,
352                         raw_predictions_val, y_val
353                     )
354 
355                 else:
356                     should_early_stop = self._check_early_stopping_scorer(
357                         X_binned_small_train, y_small_train,
358                         X_binned_val, y_val,
359                     )
360 
361             if self.verbose:
362                 self._print_iteration_stats(iteration_start_time)
363 
364             # maybe we could also early stop if all the trees are stumps?
365             if should_early_stop:
366                 break
367 
368         if self.verbose:
369             duration = time() - fit_start_time
370             n_total_leaves = sum(
371                 predictor.get_n_leaf_nodes()
372                 for predictors_at_ith_iteration in self._predictors
373                 for predictor in predictors_at_ith_iteration
374             )
375             n_predictors = sum(
376                 len(predictors_at_ith_iteration)
377                 for predictors_at_ith_iteration in self._predictors)
378             print("Fit {} trees in {:.3f} s, ({} total leaves)".format(
379                 n_predictors, duration, n_total_leaves))
380             print("{:<32} {:.3f}s".format('Time spent computing histograms:',
381                                           acc_compute_hist_time))
382             print("{:<32} {:.3f}s".format('Time spent finding best splits:',
383                                           acc_find_split_time))
384             print("{:<32} {:.3f}s".format('Time spent applying splits:',
385                                           acc_apply_split_time))
386             print("{:<32} {:.3f}s".format('Time spent predicting:',
387                                           acc_prediction_time))
388 
389         self.train_score_ = np.asarray(self.train_score_)
390         self.validation_score_ = np.asarray(self.validation_score_)
391         del self._in_fit  # hard delete so we're sure it can't be used anymore
392         return self

</code><intended_behavior>During the `fit` method's execution, especially after the call to `_get_loss`, an additional validation step could be incorporated to check the suitability of the loss function chosen for the data it is being applied to. This may include verifying that the selected loss function aligns with the type of classification task being performed. While not directly necessary to fix the reported issue, this could enhance robustness and user guidance, preventing similar issues from arising in different contexts or with other parameters.</intended_behavior>, <file>sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py</file>
<class>BaseHistGradientBoosting</class>
<method>None</method>
<code>
25 class BaseHistGradientBoosting(BaseEstimator, ABC):
26     """Base class for histogram-based gradient boosting estimators."""
27 
28     @abstractmethod
29     def __init__(self, loss, learning_rate, max_iter, max_leaf_nodes,
30                  max_depth, min_samples_leaf, l2_regularization, max_bins,
31                  warm_start, scoring, validation_fraction, n_iter_no_change,
32                  tol, verbose, random_state):
33         self.loss = loss
34         self.learning_rate = learning_rate
35         self.max_iter = max_iter
36         self.max_leaf_nodes = max_leaf_nodes
37         self.max_depth = max_depth
38         self.min_samples_leaf = min_samples_leaf
39         self.l2_regularization = l2_regularization
40         self.max_bins = max_bins
41         self.warm_start = warm_start
42         self.scoring = scoring
43         self.validation_fraction = validation_fraction
44         self.n_iter_no_change = n_iter_no_change
45         self.tol = tol
46         self.verbose = verbose
47         self.random_state = random_state
48 
49     def _validate_parameters(self):
50         """Validate parameters passed to __init__.
51 
52         The parameters that are directly passed to the grower are checked in
53         TreeGrower."""
54 
55         if self.loss not in self._VALID_LOSSES:
56             raise ValueError(
57                 "Loss {} is not supported for {}. Accepted losses: "
58                 "{}.".format(self.loss, self.__class__.__name__,
59                              ', '.join(self._VALID_LOSSES)))
60 
61         if self.learning_rate <= 0:
62             raise ValueError('learning_rate={} must '
63                              'be strictly positive'.format(self.learning_rate))
64         if self.max_iter < 1:
65             raise ValueError('max_iter={} must not be smaller '
66                              'than 1.'.format(self.max_iter))
67         if self.n_iter_no_change is not None and self.n_iter_no_change < 0:
68             raise ValueError('n_iter_no_change={} must be '
69                              'positive.'.format(self.n_iter_no_change))
70         if (self.validation_fraction is not None and
71                 self.validation_fraction <= 0):
72             raise ValueError(
73                 'validation_fraction={} must be strictly '
74                 'positive, or None.'.format(self.validation_fraction))
75         if self.tol is not None and self.tol < 0:
76             raise ValueError('tol={} '
77                              'must not be smaller than 0.'.format(self.tol))
78 
79         if not (2 <= self.max_bins <= 255):
80             raise ValueError('max_bins={} should be no smaller than 2 '
81                              'and no larger than 255.'.format(self.max_bins))
82 
83     def fit(self, X, y):
84         """Fit the gradient boosting model.
85 
86         Parameters
87         ----------
88         X : array-like, shape=(n_samples, n_features)
89             The input samples.
90 
91         y : array-like, shape=(n_samples,)
92             Target values.
93 
94         Returns
95         -------
96         self : object
97         """
98         fit_start_time = time()
99         acc_find_split_time = 0.  # time spent finding the best splits
100         acc_apply_split_time = 0.  # time spent splitting nodes
101         acc_compute_hist_time = 0.  # time spent computing histograms
102         # time spent predicting X for gradient and hessians update
103         acc_prediction_time = 0.
104         X, y = check_X_y(X, y, dtype=[X_DTYPE], force_all_finite=False)
105         y = self._encode_y(y)
106 
107         # The rng state must be preserved if warm_start is True
108         if (self.warm_start and hasattr(self, '_rng')):
109             rng = self._rng
110         else:
111             rng = check_random_state(self.random_state)
112             self._rng = rng
113 
114         self._validate_parameters()
115         self.n_features_ = X.shape[1]  # used for validation in predict()
116 
117         # we need this stateful variable to tell raw_predict() that it was
118         # called from fit() (this current method), and that the data it has
119         # received is pre-binned.
120         # predicting is faster on pre-binned data, so we want early stopping
121         # predictions to be made on pre-binned data. Unfortunately the scorer_
122         # can only call predict() or predict_proba(), not raw_predict(), and
123         # there's no way to tell the scorer that it needs to predict binned
124         # data.
125         self._in_fit = True
126 
127         self.loss_ = self._get_loss()
128 
129         self.do_early_stopping_ = (self.n_iter_no_change is not None and
130                                    self.n_iter_no_change > 0)
131 
132         # create validation data if needed
133         self._use_validation_data = self.validation_fraction is not None
134         if self.do_early_stopping_ and self._use_validation_data:
135             # stratify for classification
136             stratify = y if hasattr(self.loss_, 'predict_proba') else None
137 
138             # Save the state of the RNG for the training and validation split.
139             # This is needed in order to have the same split when using
140             # warm starting.
141             if not (self._is_fitted() and self.warm_start):
142                 self._train_val_split_seed = rng.randint(1024)
143 
144             X_train, X_val, y_train, y_val = train_test_split(
145                 X, y, test_size=self.validation_fraction, stratify=stratify,
146                 random_state=self._train_val_split_seed)
147         else:
148             X_train, y_train = X, y
149             X_val, y_val = None, None
150 
151         has_missing_values = np.isnan(X_train).any(axis=0).astype(np.uint8)
152 
153         # Bin the data
154         # For ease of use of the API, the user-facing GBDT classes accept the
155         # parameter max_bins, which doesn't take into account the bin for
156         # missing values (which is always allocated). However, since max_bins
157         # isn't the true maximal number of bins, all other private classes
158         # (binmapper, histbuilder...) accept n_bins instead, which is the
159         # actual total number of bins. Everywhere in the code, the
160         # convention is that n_bins == max_bins + 1
161         n_bins = self.max_bins + 1  # + 1 for missing values
162         self.bin_mapper_ = _BinMapper(n_bins=n_bins, random_state=rng)
163         X_binned_train = self._bin_data(X_train, rng, is_training_data=True)
164         if X_val is not None:
165             X_binned_val = self._bin_data(X_val, rng, is_training_data=False)
166         else:
167             X_binned_val = None
168 
169         if self.verbose:
170             print("Fitting gradient boosted rounds:")
171 
172         n_samples = X_binned_train.shape[0]
173 
174         # First time calling fit, or no warm start
175         if not (self._is_fitted() and self.warm_start):
176             # Clear random state and score attributes
177             self._clear_state()
178 
179             # initialize raw_predictions: those are the accumulated values
180             # predicted by the trees for the training data. raw_predictions has
181             # shape (n_trees_per_iteration, n_samples) where
182             # n_trees_per_iterations is n_classes in multiclass classification,
183             # else 1.
184             self._baseline_prediction = self.loss_.get_baseline_prediction(
185                 y_train, self.n_trees_per_iteration_
186             )
187             raw_predictions = np.zeros(
188                 shape=(self.n_trees_per_iteration_, n_samples),
189                 dtype=self._baseline_prediction.dtype
190             )
191             raw_predictions += self._baseline_prediction
192 
193             # initialize gradients and hessians (empty arrays).
194             # shape = (n_trees_per_iteration, n_samples).
195             gradients, hessians = self.loss_.init_gradients_and_hessians(
196                 n_samples=n_samples,
197                 prediction_dim=self.n_trees_per_iteration_
198             )
199 
200             # predictors is a matrix (list of lists) of TreePredictor objects
201             # with shape (n_iter_, n_trees_per_iteration)
202             self._predictors = predictors = []
203 
204             # Initialize structures and attributes related to early stopping
205             self.scorer_ = None  # set if scoring != loss
206             raw_predictions_val = None  # set if scoring == loss and use val
207             self.train_score_ = []
208             self.validation_score_ = []
209 
210             if self.do_early_stopping_:
211                 # populate train_score and validation_score with the
212                 # predictions of the initial model (before the first tree)
213 
214                 if self.scoring == 'loss':
215                     # we're going to compute scoring w.r.t the loss. As losses
216                     # take raw predictions as input (unlike the scorers), we
217                     # can optimize a bit and avoid repeating computing the
218                     # predictions of the previous trees. We'll re-use
219                     # raw_predictions (as it's needed for training anyway) for
220                     # evaluating the training loss, and create
221                     # raw_predictions_val for storing the raw predictions of
222                     # the validation data.
223 
224                     if self._use_validation_data:
225                         raw_predictions_val = np.zeros(
226                             shape=(self.n_trees_per_iteration_,
227                                    X_binned_val.shape[0]),
228                             dtype=self._baseline_prediction.dtype
229                         )
230 
231                         raw_predictions_val += self._baseline_prediction
232 
233                     self._check_early_stopping_loss(raw_predictions, y_train,
234                                                     raw_predictions_val, y_val)
235                 else:
236                     self.scorer_ = check_scoring(self, self.scoring)
237                     # scorer_ is a callable with signature (est, X, y) and
238                     # calls est.predict() or est.predict_proba() depending on
239                     # its nature.
240                     # Unfortunately, each call to scorer_() will compute
241                     # the predictions of all the trees. So we use a subset of
242                     # the training set to compute train scores.
243 
244                     # Save the seed for the small trainset generator
245                     self._small_trainset_seed = rng.randint(1024)
246 
247                     # Compute the subsample set
248                     (X_binned_small_train,
249                      y_small_train) = self._get_small_trainset(
250                         X_binned_train, y_train, self._small_trainset_seed)
251 
252                     self._check_early_stopping_scorer(
253                         X_binned_small_train, y_small_train,
254                         X_binned_val, y_val,
255                     )
256             begin_at_stage = 0
257 
258         # warm start: this is not the first time fit was called
259         else:
260             # Check that the maximum number of iterations is not smaller
261             # than the number of iterations from the previous fit
262             if self.max_iter < self.n_iter_:
263                 raise ValueError(
264                     'max_iter=%d must be larger than or equal to '
265                     'n_iter_=%d when warm_start==True'
266                     % (self.max_iter, self.n_iter_)
267                 )
268 
269             # Convert array attributes to lists
270             self.train_score_ = self.train_score_.tolist()
271             self.validation_score_ = self.validation_score_.tolist()
272 
273             # Compute raw predictions
274             raw_predictions = self._raw_predict(X_binned_train)
275 
276             if self.do_early_stopping_ and self.scoring != 'loss':
277                 # Compute the subsample set
278                 X_binned_small_train, y_small_train = self._get_small_trainset(
279                     X_binned_train, y_train, self._small_trainset_seed)
280 
281             # Initialize the gradients and hessians
282             gradients, hessians = self.loss_.init_gradients_and_hessians(
283                 n_samples=n_samples,
284                 prediction_dim=self.n_trees_per_iteration_
285             )
286 
287             # Get the predictors from the previous fit
288             predictors = self._predictors
289 
290             begin_at_stage = self.n_iter_
291 
292         for iteration in range(begin_at_stage, self.max_iter):
293 
294             if self.verbose:
295                 iteration_start_time = time()
296                 print("[{}/{}] ".format(iteration + 1, self.max_iter),
297                       end='', flush=True)
298 
299             # Update gradients and hessians, inplace
300             self.loss_.update_gradients_and_hessians(gradients, hessians,
301                                                      y_train, raw_predictions)
302 
303             # Append a list since there may be more than 1 predictor per iter
304             predictors.append([])
305 
306             # Build `n_trees_per_iteration` trees.
307             for k in range(self.n_trees_per_iteration_):
308 
309                 grower = TreeGrower(
310                     X_binned_train, gradients[k, :], hessians[k, :],
311                     n_bins=n_bins,
312                     n_bins_non_missing=self.bin_mapper_.n_bins_non_missing_,
313                     has_missing_values=has_missing_values,
314                     max_leaf_nodes=self.max_leaf_nodes,
315                     max_depth=self.max_depth,
316                     min_samples_leaf=self.min_samples_leaf,
317                     l2_regularization=self.l2_regularization,
318                     shrinkage=self.learning_rate)
319                 grower.grow()
320 
321                 acc_apply_split_time += grower.total_apply_split_time
322                 acc_find_split_time += grower.total_find_split_time
323                 acc_compute_hist_time += grower.total_compute_hist_time
324 
325                 predictor = grower.make_predictor(
326                     bin_thresholds=self.bin_mapper_.bin_thresholds_
327                 )
328                 predictors[-1].append(predictor)
329 
330                 # Update raw_predictions with the predictions of the newly
331                 # created tree.
332                 tic_pred = time()
333                 _update_raw_predictions(raw_predictions[k, :], grower)
334                 toc_pred = time()
335                 acc_prediction_time += toc_pred - tic_pred
336 
337             should_early_stop = False
338             if self.do_early_stopping_:
339                 if self.scoring == 'loss':
340                     # Update raw_predictions_val with the newest tree(s)
341                     if self._use_validation_data:
342                         for k, pred in enumerate(self._predictors[-1]):
343                             raw_predictions_val[k, :] += (
344                                 pred.predict_binned(
345                                     X_binned_val,
346                                     self.bin_mapper_.missing_values_bin_idx_
347                                 )
348                             )
349 
350                     should_early_stop = self._check_early_stopping_loss(
351                         raw_predictions, y_train,
352                         raw_predictions_val, y_val
353                     )
354 
355                 else:
356                     should_early_stop = self._check_early_stopping_scorer(
357                         X_binned_small_train, y_small_train,
358                         X_binned_val, y_val,
359                     )
360 
361             if self.verbose:
362                 self._print_iteration_stats(iteration_start_time)
363 
364             # maybe we could also early stop if all the trees are stumps?
365             if should_early_stop:
366                 break
367 
368         if self.verbose:
369             duration = time() - fit_start_time
370             n_total_leaves = sum(
371                 predictor.get_n_leaf_nodes()
372                 for predictors_at_ith_iteration in self._predictors
373                 for predictor in predictors_at_ith_iteration
374             )
375             n_predictors = sum(
376                 len(predictors_at_ith_iteration)
377                 for predictors_at_ith_iteration in self._predictors)
378             print("Fit {} trees in {:.3f} s, ({} total leaves)".format(
379                 n_predictors, duration, n_total_leaves))
380             print("{:<32} {:.3f}s".format('Time spent computing histograms:',
381                                           acc_compute_hist_time))
382             print("{:<32} {:.3f}s".format('Time spent finding best splits:',
383                                           acc_find_split_time))
384             print("{:<32} {:.3f}s".format('Time spent applying splits:',
385                                           acc_apply_split_time))
386             print("{:<32} {:.3f}s".format('Time spent predicting:',
387                                           acc_prediction_time))
388 
389         self.train_score_ = np.asarray(self.train_score_)
390         self.validation_score_ = np.asarray(self.validation_score_)
391         del self._in_fit  # hard delete so we're sure it can't be used anymore
392         return self
393 
394     def _is_fitted(self):
395         return len(getattr(self, '_predictors', [])) > 0
396 
397     def _clear_state(self):
398         """Clear the state of the gradient boosting model."""
399         for var in ('train_score_', 'validation_score_', '_rng'):
400             if hasattr(self, var):
401                 delattr(self, var)
402 
403     def _get_small_trainset(self, X_binned_train, y_train, seed):
404         """Compute the indices of the subsample set and return this set.
405 
406         For efficiency, we need to subsample the training set to compute scores
407         with scorers.
408         """
409         subsample_size = 10000
410         if X_binned_train.shape[0] > subsample_size:
411             indices = np.arange(X_binned_train.shape[0])
412             stratify = y_train if is_classifier(self) else None
413             indices = resample(indices, n_samples=subsample_size,
414                                replace=False, random_state=seed,
415                                stratify=stratify)
416             X_binned_small_train = X_binned_train[indices]
417             y_small_train = y_train[indices]
418             X_binned_small_train = np.ascontiguousarray(X_binned_small_train)
419             return X_binned_small_train, y_small_train
420         else:
421             return X_binned_train, y_train
422 
423     def _check_early_stopping_scorer(self, X_binned_small_train, y_small_train,
424                                      X_binned_val, y_val):
425         """Check if fitting should be early-stopped based on scorer.
426 
427         Scores are computed on validation data or on training data.
428         """
429         if is_classifier(self):
430             y_small_train = self.classes_[y_small_train.astype(int)]
431         self.train_score_.append(
432             self.scorer_(self, X_binned_small_train, y_small_train)
433         )
434 
435         if self._use_validation_data:
436             if is_classifier(self):
437                 y_val = self.classes_[y_val.astype(int)]
438             self.validation_score_.append(
439                 self.scorer_(self, X_binned_val, y_val)
440             )
441             return self._should_stop(self.validation_score_)
442         else:
443             return self._should_stop(self.train_score_)
444 
445     def _check_early_stopping_loss(self,
446                                    raw_predictions,
447                                    y_train,
448                                    raw_predictions_val,
449                                    y_val):
450         """Check if fitting should be early-stopped based on loss.
451 
452         Scores are computed on validation data or on training data.
453         """
454 
455         self.train_score_.append(
456             -self.loss_(y_train, raw_predictions)
457         )
458 
459         if self._use_validation_data:
460             self.validation_score_.append(
461                 -self.loss_(y_val, raw_predictions_val)
462             )
463             return self._should_stop(self.validation_score_)
464         else:
465             return self._should_stop(self.train_score_)
466 
467     def _should_stop(self, scores):
468         """
469         Return True (do early stopping) if the last n scores aren't better
470         than the (n-1)th-to-last score, up to some tolerance.
471         """
472         reference_position = self.n_iter_no_change + 1
473         if len(scores) < reference_position:
474             return False
475 
476         # A higher score is always better. Higher tol means that it will be
477         # harder for subsequent iteration to be considered an improvement upon
478         # the reference score, and therefore it is more likely to early stop
479         # because of the lack of significant improvement.
480         tol = 0 if self.tol is None else self.tol
481         reference_score = scores[-reference_position] + tol
482         recent_scores = scores[-reference_position + 1:]
483         recent_improvements = [score > reference_score
484                                for score in recent_scores]
485         return not any(recent_improvements)
486 
487     def _bin_data(self, X, rng, is_training_data):
488         """Bin data X.
489 
490         If is_training_data, then set the bin_mapper_ attribute.
491         Else, the binned data is converted to a C-contiguous array.
492         """
493 
494         description = 'training' if is_training_data else 'validation'
495         if self.verbose:
496             print("Binning {:.3f} GB of {} data: ".format(
497                 X.nbytes / 1e9, description), end="", flush=True)
498         tic = time()
499         if is_training_data:
500             X_binned = self.bin_mapper_.fit_transform(X)  # F-aligned array
501         else:
502             X_binned = self.bin_mapper_.transform(X)  # F-aligned array
503             # We convert the array to C-contiguous since predicting is faster
504             # with this layout (training is faster on F-arrays though)
505             X_binned = np.ascontiguousarray(X_binned)
506         toc = time()
507         if self.verbose:
508             duration = toc - tic
509             print("{:.3f} s".format(duration))
510 
511         return X_binned
512 
513     def _print_iteration_stats(self, iteration_start_time):
514         """Print info about the current fitting iteration."""
515         log_msg = ''
516 
517         predictors_of_ith_iteration = [
518             predictors_list for predictors_list in self._predictors[-1]
519             if predictors_list
520         ]
521         n_trees = len(predictors_of_ith_iteration)
522         max_depth = max(predictor.get_max_depth()
523                         for predictor in predictors_of_ith_iteration)
524         n_leaves = sum(predictor.get_n_leaf_nodes()
525                        for predictor in predictors_of_ith_iteration)
526 
527         if n_trees == 1:
528             log_msg += ("{} tree, {} leaves, ".format(n_trees, n_leaves))
529         else:
530             log_msg += ("{} trees, {} leaves ".format(n_trees, n_leaves))
531             log_msg += ("({} on avg), ".format(int(n_leaves / n_trees)))
532 
533         log_msg += "max depth = {}, ".format(max_depth)
534 
535         if self.do_early_stopping_:
536             if self.scoring == 'loss':
537                 factor = -1  # score_ arrays contain the negative loss
538                 name = 'loss'
539             else:
540                 factor = 1
541                 name = 'score'
542             log_msg += "train {}: {:.5f}, ".format(name, factor *
543                                                    self.train_score_[-1])
544             if self._use_validation_data:
545                 log_msg += "val {}: {:.5f}, ".format(
546                     name, factor * self.validation_score_[-1])
547 
548         iteration_time = time() - iteration_start_time
549         log_msg += "in {:0.3f}s".format(iteration_time)
550 
551         print(log_msg)
552 
553     def _raw_predict(self, X):
554         """Return the sum of the leaves values over all predictors.
555 
556         Parameters
557         ----------
558         X : array-like, shape=(n_samples, n_features)
559             The input samples.
560 
561         Returns
562         -------
563         raw_predictions : array, shape (n_samples * n_trees_per_iteration,)
564             The raw predicted values.
565         """
566         X = check_array(X, dtype=[X_DTYPE, X_BINNED_DTYPE],
567                         force_all_finite=False)
568         check_is_fitted(self)
569         if X.shape[1] != self.n_features_:
570             raise ValueError(
571                 'X has {} features but this estimator was trained with '
572                 '{} features.'.format(X.shape[1], self.n_features_)
573             )
574         is_binned = getattr(self, '_in_fit', False)
575         n_samples = X.shape[0]
576         raw_predictions = np.zeros(
577             shape=(self.n_trees_per_iteration_, n_samples),
578             dtype=self._baseline_prediction.dtype
579         )
580         raw_predictions += self._baseline_prediction
581         for predictors_of_ith_iteration in self._predictors:
582             for k, predictor in enumerate(predictors_of_ith_iteration):
583                 if is_binned:
584                     predict = partial(
585                         predictor.predict_binned,
586                         missing_values_bin_idx=self.bin_mapper_.missing_values_bin_idx_  # noqa
587                     )
588                 else:
589                     predict = predictor.predict
590                 raw_predictions[k, :] += predict(X)
591 
592         return raw_predictions
593 
594     def _compute_partial_dependence_recursion(self, grid, target_features):
595         """Fast partial dependence computation.
596 
597         Parameters
598         ----------
599         grid : ndarray, shape (n_samples, n_target_features)
600             The grid points on which the partial dependence should be
601             evaluated.
602         target_features : ndarray, shape (n_target_features)
603             The set of target features for which the partial dependence
604             should be evaluated.
605 
606         Returns
607         -------
608         averaged_predictions : ndarray, shape \
609                 (n_trees_per_iteration, n_samples)
610             The value of the partial dependence function on each grid point.
611         """
612         grid = np.asarray(grid, dtype=X_DTYPE, order='C')
613         averaged_predictions = np.zeros(
614             (self.n_trees_per_iteration_, grid.shape[0]), dtype=Y_DTYPE)
615 
616         for predictors_of_ith_iteration in self._predictors:
617             for k, predictor in enumerate(predictors_of_ith_iteration):
618                 predictor.compute_partial_dependence(grid, target_features,
619                                                      averaged_predictions[k])
620         # Note that the learning rate is already accounted for in the leaves
621         # values.
622 
623         return averaged_predictions
624 
625     def _more_tags(self):
626         return {'allow_nan': True}
627 
628     @abstractmethod
629     def _get_loss(self):
630         pass
631 
632     @abstractmethod
633     def _encode_y(self, y=None):
634         pass
635 
636     @property
637     def n_iter_(self):
638         check_is_fitted(self)
639         return len(self._predictors)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 17:25:12.972 | INFO     | Search completed. Bug locations: [<file>sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py</file>
<class>HistGradientBoostingClassifier</class>
<method>_get_loss</method>
<code>
1041     def _get_loss(self):
1042         if self.loss == 'auto':
1043             if self.n_trees_per_iteration_ == 1:
1044                 return _LOSSES['binary_crossentropy']()
1045             else:
1046                 return _LOSSES['categorical_crossentropy']()
1047 
1048         return _LOSSES[self.loss]()

</code><intended_behavior>The `_get_loss` method should include an additional check to ascertain whether `categorical_crossentropy` is being used in a binary classification scenario erroneously. Given the binary classification case (`n_trees_per_iteration_ == 1`), if `self.loss` is explicitly set to `categorical_crossentropy`, the method should raise a ValueError indicating the misuse and suggesting the use of `binary_crossentropy` or setting `loss='auto'` for automatic selection. This change ensures users are aware of the correct loss function usage in binary classification scenarios, preventing silent failures.</intended_behavior>, <file>sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py</file>
<class>HistGradientBoostingClassifier</class>
<method>None</method>
<code>
818 class HistGradientBoostingClassifier(BaseHistGradientBoosting,
819                                      ClassifierMixin):
820     """Histogram-based Gradient Boosting Classification Tree.
821 
822     This estimator is much faster than
823     :class:`GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`
824     for big datasets (n_samples >= 10 000).
825 
826     This estimator has native support for missing values (NaNs). During
827     training, the tree grower learns at each split point whether samples
828     with missing values should go to the left or right child, based on the
829     potential gain. When predicting, samples with missing values are
830     assigned to the left or right child consequently. If no missing values
831     were encountered for a given feature during training, then samples with
832     missing values are mapped to whichever child has the most samples.
833 
834     This implementation is inspired by
835     `LightGBM <https://github.com/Microsoft/LightGBM>`_.
836 
837     .. note::
838 
839       This estimator is still **experimental** for now: the predictions
840       and the API might change without any deprecation cycle. To use it,
841       you need to explicitly import ``enable_hist_gradient_boosting``::
842 
843         >>> # explicitly require this experimental feature
844         >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa
845         >>> # now you can import normally from ensemble
846         >>> from sklearn.ensemble import HistGradientBoostingClassifier
847 
848     Read more in the :ref:`User Guide <histogram_based_gradient_boosting>`.
849 
850     Parameters
851     ----------
852     loss : {'auto', 'binary_crossentropy', 'categorical_crossentropy'}, \
853             optional (default='auto')
854         The loss function to use in the boosting process. 'binary_crossentropy'
855         (also known as logistic loss) is used for binary classification and
856         generalizes to 'categorical_crossentropy' for multiclass
857         classification. 'auto' will automatically choose either loss depending
858         on the nature of the problem.
859     learning_rate : float, optional (default=0.1)
860         The learning rate, also known as *shrinkage*. This is used as a
861         multiplicative factor for the leaves values. Use ``1`` for no
862         shrinkage.
863     max_iter : int, optional (default=100)
864         The maximum number of iterations of the boosting process, i.e. the
865         maximum number of trees for binary classification. For multiclass
866         classification, `n_classes` trees per iteration are built.
867     max_leaf_nodes : int or None, optional (default=31)
868         The maximum number of leaves for each tree. Must be strictly greater
869         than 1. If None, there is no maximum limit.
870     max_depth : int or None, optional (default=None)
871         The maximum depth of each tree. The depth of a tree is the number of
872         nodes to go from the root to the deepest leaf. Must be strictly greater
873         than 1. Depth isn't constrained by default.
874     min_samples_leaf : int, optional (default=20)
875         The minimum number of samples per leaf. For small datasets with less
876         than a few hundred samples, it is recommended to lower this value
877         since only very shallow trees would be built.
878     l2_regularization : float, optional (default=0)
879         The L2 regularization parameter. Use 0 for no regularization.
880     max_bins : int, optional (default=255)
881         The maximum number of bins to use for non-missing values. Before
882         training, each feature of the input array `X` is binned into
883         integer-valued bins, which allows for a much faster training stage.
884         Features with a small number of unique values may use less than
885         ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin
886         is always reserved for missing values. Must be no larger than 255.
887     warm_start : bool, optional (default=False)
888         When set to ``True``, reuse the solution of the previous call to fit
889         and add more estimators to the ensemble. For results to be valid, the
890         estimator should be re-trained on the same data only.
891         See :term:`the Glossary <warm_start>`.
892     scoring : str or callable or None, optional (default=None)
893         Scoring parameter to use for early stopping. It can be a single
894         string (see :ref:`scoring_parameter`) or a callable (see
895         :ref:`scoring`). If None, the estimator's default scorer
896         is used. If ``scoring='loss'``, early stopping is checked
897         w.r.t the loss value. Only used if ``n_iter_no_change`` is not None.
898     validation_fraction : int or float or None, optional (default=0.1)
899         Proportion (or absolute size) of training data to set aside as
900         validation data for early stopping. If None, early stopping is done on
901         the training data.
902     n_iter_no_change : int or None, optional (default=None)
903         Used to determine when to "early stop". The fitting process is
904         stopped when none of the last ``n_iter_no_change`` scores are better
905         than the ``n_iter_no_change - 1`` -th-to-last one, up to some
906         tolerance. If None or 0, no early-stopping is done.
907     tol : float or None, optional (default=1e-7)
908         The absolute tolerance to use when comparing scores. The higher the
909         tolerance, the more likely we are to early stop: higher tolerance
910         means that it will be harder for subsequent iterations to be
911         considered an improvement upon the reference score.
912     verbose: int, optional (default=0)
913         The verbosity level. If not zero, print some information about the
914         fitting process.
915     random_state : int, np.random.RandomStateInstance or None, \
916         optional (default=None)
917         Pseudo-random number generator to control the subsampling in the
918         binning process, and the train/validation data split if early stopping
919         is enabled. See :term:`random_state`.
920 
921     Attributes
922     ----------
923     n_iter_ : int
924         The number of estimators as selected by early stopping (if
925         n_iter_no_change is not None). Otherwise it corresponds to max_iter.
926     n_trees_per_iteration_ : int
927         The number of tree that are built at each iteration. This is equal to 1
928         for binary classification, and to ``n_classes`` for multiclass
929         classification.
930     train_score_ : ndarray, shape (n_iter_ + 1,)
931         The scores at each iteration on the training data. The first entry
932         is the score of the ensemble before the first iteration. Scores are
933         computed according to the ``scoring`` parameter. If ``scoring`` is
934         not 'loss', scores are computed on a subset of at most 10 000
935         samples. Empty if no early stopping.
936     validation_score_ : ndarray, shape (n_iter_ + 1,)
937         The scores at each iteration on the held-out validation data. The
938         first entry is the score of the ensemble before the first iteration.
939         Scores are computed according to the ``scoring`` parameter. Empty if
940         no early stopping or if ``validation_fraction`` is None.
941 
942     Examples
943     --------
944     >>> # To use this experimental feature, we need to explicitly ask for it:
945     >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa
946     >>> from sklearn.ensemble import HistGradientBoostingRegressor
947     >>> from sklearn.datasets import load_iris
948     >>> X, y = load_iris(return_X_y=True)
949     >>> clf = HistGradientBoostingClassifier().fit(X, y)
950     >>> clf.score(X, y)
951     1.0
952     """
953 
954     _VALID_LOSSES = ('binary_crossentropy', 'categorical_crossentropy',
955                      'auto')
956 
957     def __init__(self, loss='auto', learning_rate=0.1, max_iter=100,
958                  max_leaf_nodes=31, max_depth=None, min_samples_leaf=20,
959                  l2_regularization=0., max_bins=255, warm_start=False,
960                  scoring=None, validation_fraction=0.1, n_iter_no_change=None,
961                  tol=1e-7, verbose=0, random_state=None):
962         super(HistGradientBoostingClassifier, self).__init__(
963             loss=loss, learning_rate=learning_rate, max_iter=max_iter,
964             max_leaf_nodes=max_leaf_nodes, max_depth=max_depth,
965             min_samples_leaf=min_samples_leaf,
966             l2_regularization=l2_regularization, max_bins=max_bins,
967             warm_start=warm_start, scoring=scoring,
968             validation_fraction=validation_fraction,
969             n_iter_no_change=n_iter_no_change, tol=tol, verbose=verbose,
970             random_state=random_state)
971 
972     def predict(self, X):
973         """Predict classes for X.
974 
975         Parameters
976         ----------
977         X : array-like, shape (n_samples, n_features)
978             The input samples.
979 
980         Returns
981         -------
982         y : ndarray, shape (n_samples,)
983             The predicted classes.
984         """
985         # TODO: This could be done in parallel
986         encoded_classes = np.argmax(self.predict_proba(X), axis=1)
987         return self.classes_[encoded_classes]
988 
989     def predict_proba(self, X):
990         """Predict class probabilities for X.
991 
992         Parameters
993         ----------
994         X : array-like, shape (n_samples, n_features)
995             The input samples.
996 
997         Returns
998         -------
999         p : ndarray, shape (n_samples, n_classes)
1000             The class probabilities of the input samples.
1001         """
1002         raw_predictions = self._raw_predict(X)
1003         return self.loss_.predict_proba(raw_predictions)
1004 
1005     def decision_function(self, X):
1006         """Compute the decision function of X.
1007 
1008         Parameters
1009         ----------
1010         X : array-like, shape (n_samples, n_features)
1011             The input samples.
1012 
1013         Returns
1014         -------
1015         decision : ndarray, shape (n_samples,) or \
1016                 (n_samples, n_trees_per_iteration)
1017             The raw predicted values (i.e. the sum of the trees leaves) for
1018             each sample. n_trees_per_iteration is equal to the number of
1019             classes in multiclass classification.
1020         """
1021         decision = self._raw_predict(X)
1022         if decision.shape[0] == 1:
1023             decision = decision.ravel()
1024         return decision.T
1025 
1026     def _encode_y(self, y):
1027         # encode classes into 0 ... n_classes - 1 and sets attributes classes_
1028         # and n_trees_per_iteration_
1029         check_classification_targets(y)
1030 
1031         label_encoder = LabelEncoder()
1032         encoded_y = label_encoder.fit_transform(y)
1033         self.classes_ = label_encoder.classes_
1034         n_classes = self.classes_.shape[0]
1035         # only 1 tree for binary classification. For multiclass classification,
1036         # we build 1 tree per class.
1037         self.n_trees_per_iteration_ = 1 if n_classes <= 2 else n_classes
1038         encoded_y = encoded_y.astype(Y_DTYPE, copy=False)
1039         return encoded_y
1040 
1041     def _get_loss(self):
1042         if self.loss == 'auto':
1043             if self.n_trees_per_iteration_ == 1:
1044                 return _LOSSES['binary_crossentropy']()
1045             else:
1046                 return _LOSSES['categorical_crossentropy']()
1047 
1048         return _LOSSES[self.loss]()

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py</file>
<class>BaseHistGradientBoosting</class>
<method>fit</method>
<code>
83     def fit(self, X, y):
84         """Fit the gradient boosting model.
85 
86         Parameters
87         ----------
88         X : array-like, shape=(n_samples, n_features)
89             The input samples.
90 
91         y : array-like, shape=(n_samples,)
92             Target values.
93 
94         Returns
95         -------
96         self : object
97         """
98         fit_start_time = time()
99         acc_find_split_time = 0.  # time spent finding the best splits
100         acc_apply_split_time = 0.  # time spent splitting nodes
101         acc_compute_hist_time = 0.  # time spent computing histograms
102         # time spent predicting X for gradient and hessians update
103         acc_prediction_time = 0.
104         X, y = check_X_y(X, y, dtype=[X_DTYPE], force_all_finite=False)
105         y = self._encode_y(y)
106 
107         # The rng state must be preserved if warm_start is True
108         if (self.warm_start and hasattr(self, '_rng')):
109             rng = self._rng
110         else:
111             rng = check_random_state(self.random_state)
112             self._rng = rng
113 
114         self._validate_parameters()
115         self.n_features_ = X.shape[1]  # used for validation in predict()
116 
117         # we need this stateful variable to tell raw_predict() that it was
118         # called from fit() (this current method), and that the data it has
119         # received is pre-binned.
120         # predicting is faster on pre-binned data, so we want early stopping
121         # predictions to be made on pre-binned data. Unfortunately the scorer_
122         # can only call predict() or predict_proba(), not raw_predict(), and
123         # there's no way to tell the scorer that it needs to predict binned
124         # data.
125         self._in_fit = True
126 
127         self.loss_ = self._get_loss()
128 
129         self.do_early_stopping_ = (self.n_iter_no_change is not None and
130                                    self.n_iter_no_change > 0)
131 
132         # create validation data if needed
133         self._use_validation_data = self.validation_fraction is not None
134         if self.do_early_stopping_ and self._use_validation_data:
135             # stratify for classification
136             stratify = y if hasattr(self.loss_, 'predict_proba') else None
137 
138             # Save the state of the RNG for the training and validation split.
139             # This is needed in order to have the same split when using
140             # warm starting.
141             if not (self._is_fitted() and self.warm_start):
142                 self._train_val_split_seed = rng.randint(1024)
143 
144             X_train, X_val, y_train, y_val = train_test_split(
145                 X, y, test_size=self.validation_fraction, stratify=stratify,
146                 random_state=self._train_val_split_seed)
147         else:
148             X_train, y_train = X, y
149             X_val, y_val = None, None
150 
151         has_missing_values = np.isnan(X_train).any(axis=0).astype(np.uint8)
152 
153         # Bin the data
154         # For ease of use of the API, the user-facing GBDT classes accept the
155         # parameter max_bins, which doesn't take into account the bin for
156         # missing values (which is always allocated). However, since max_bins
157         # isn't the true maximal number of bins, all other private classes
158         # (binmapper, histbuilder...) accept n_bins instead, which is the
159         # actual total number of bins. Everywhere in the code, the
160         # convention is that n_bins == max_bins + 1
161         n_bins = self.max_bins + 1  # + 1 for missing values
162         self.bin_mapper_ = _BinMapper(n_bins=n_bins, random_state=rng)
163         X_binned_train = self._bin_data(X_train, rng, is_training_data=True)
164         if X_val is not None:
165             X_binned_val = self._bin_data(X_val, rng, is_training_data=False)
166         else:
167             X_binned_val = None
168 
169         if self.verbose:
170             print("Fitting gradient boosted rounds:")
171 
172         n_samples = X_binned_train.shape[0]
173 
174         # First time calling fit, or no warm start
175         if not (self._is_fitted() and self.warm_start):
176             # Clear random state and score attributes
177             self._clear_state()
178 
179             # initialize raw_predictions: those are the accumulated values
180             # predicted by the trees for the training data. raw_predictions has
181             # shape (n_trees_per_iteration, n_samples) where
182             # n_trees_per_iterations is n_classes in multiclass classification,
183             # else 1.
184             self._baseline_prediction = self.loss_.get_baseline_prediction(
185                 y_train, self.n_trees_per_iteration_
186             )
187             raw_predictions = np.zeros(
188                 shape=(self.n_trees_per_iteration_, n_samples),
189                 dtype=self._baseline_prediction.dtype
190             )
191             raw_predictions += self._baseline_prediction
192 
193             # initialize gradients and hessians (empty arrays).
194             # shape = (n_trees_per_iteration, n_samples).
195             gradients, hessians = self.loss_.init_gradients_and_hessians(
196                 n_samples=n_samples,
197                 prediction_dim=self.n_trees_per_iteration_
198             )
199 
200             # predictors is a matrix (list of lists) of TreePredictor objects
201             # with shape (n_iter_, n_trees_per_iteration)
202             self._predictors = predictors = []
203 
204             # Initialize structures and attributes related to early stopping
205             self.scorer_ = None  # set if scoring != loss
206             raw_predictions_val = None  # set if scoring == loss and use val
207             self.train_score_ = []
208             self.validation_score_ = []
209 
210             if self.do_early_stopping_:
211                 # populate train_score and validation_score with the
212                 # predictions of the initial model (before the first tree)
213 
214                 if self.scoring == 'loss':
215                     # we're going to compute scoring w.r.t the loss. As losses
216                     # take raw predictions as input (unlike the scorers), we
217                     # can optimize a bit and avoid repeating computing the
218                     # predictions of the previous trees. We'll re-use
219                     # raw_predictions (as it's needed for training anyway) for
220                     # evaluating the training loss, and create
221                     # raw_predictions_val for storing the raw predictions of
222                     # the validation data.
223 
224                     if self._use_validation_data:
225                         raw_predictions_val = np.zeros(
226                             shape=(self.n_trees_per_iteration_,
227                                    X_binned_val.shape[0]),
228                             dtype=self._baseline_prediction.dtype
229                         )
230 
231                         raw_predictions_val += self._baseline_prediction
232 
233                     self._check_early_stopping_loss(raw_predictions, y_train,
234                                                     raw_predictions_val, y_val)
235                 else:
236                     self.scorer_ = check_scoring(self, self.scoring)
237                     # scorer_ is a callable with signature (est, X, y) and
238                     # calls est.predict() or est.predict_proba() depending on
239                     # its nature.
240                     # Unfortunately, each call to scorer_() will compute
241                     # the predictions of all the trees. So we use a subset of
242                     # the training set to compute train scores.
243 
244                     # Save the seed for the small trainset generator
245                     self._small_trainset_seed = rng.randint(1024)
246 
247                     # Compute the subsample set
248                     (X_binned_small_train,
249                      y_small_train) = self._get_small_trainset(
250                         X_binned_train, y_train, self._small_trainset_seed)
251 
252                     self._check_early_stopping_scorer(
253                         X_binned_small_train, y_small_train,
254                         X_binned_val, y_val,
255                     )
256             begin_at_stage = 0
257 
258         # warm start: this is not the first time fit was called
259         else:
260             # Check that the maximum number of iterations is not smaller
261             # than the number of iterations from the previous fit
262             if self.max_iter < self.n_iter_:
263                 raise ValueError(
264                     'max_iter=%d must be larger than or equal to '
265                     'n_iter_=%d when warm_start==True'
266                     % (self.max_iter, self.n_iter_)
267                 )
268 
269             # Convert array attributes to lists
270             self.train_score_ = self.train_score_.tolist()
271             self.validation_score_ = self.validation_score_.tolist()
272 
273             # Compute raw predictions
274             raw_predictions = self._raw_predict(X_binned_train)
275 
276             if self.do_early_stopping_ and self.scoring != 'loss':
277                 # Compute the subsample set
278                 X_binned_small_train, y_small_train = self._get_small_trainset(
279                     X_binned_train, y_train, self._small_trainset_seed)
280 
281             # Initialize the gradients and hessians
282             gradients, hessians = self.loss_.init_gradients_and_hessians(
283                 n_samples=n_samples,
284                 prediction_dim=self.n_trees_per_iteration_
285             )
286 
287             # Get the predictors from the previous fit
288             predictors = self._predictors
289 
290             begin_at_stage = self.n_iter_
291 
292         for iteration in range(begin_at_stage, self.max_iter):
293 
294             if self.verbose:
295                 iteration_start_time = time()
296                 print("[{}/{}] ".format(iteration + 1, self.max_iter),
297                       end='', flush=True)
298 
299             # Update gradients and hessians, inplace
300             self.loss_.update_gradients_and_hessians(gradients, hessians,
301                                                      y_train, raw_predictions)
302 
303             # Append a list since there may be more than 1 predictor per iter
304             predictors.append([])
305 
306             # Build `n_trees_per_iteration` trees.
307             for k in range(self.n_trees_per_iteration_):
308 
309                 grower = TreeGrower(
310                     X_binned_train, gradients[k, :], hessians[k, :],
311                     n_bins=n_bins,
312                     n_bins_non_missing=self.bin_mapper_.n_bins_non_missing_,
313                     has_missing_values=has_missing_values,
314                     max_leaf_nodes=self.max_leaf_nodes,
315                     max_depth=self.max_depth,
316                     min_samples_leaf=self.min_samples_leaf,
317                     l2_regularization=self.l2_regularization,
318                     shrinkage=self.learning_rate)
319                 grower.grow()
320 
321                 acc_apply_split_time += grower.total_apply_split_time
322                 acc_find_split_time += grower.total_find_split_time
323                 acc_compute_hist_time += grower.total_compute_hist_time
324 
325                 predictor = grower.make_predictor(
326                     bin_thresholds=self.bin_mapper_.bin_thresholds_
327                 )
328                 predictors[-1].append(predictor)
329 
330                 # Update raw_predictions with the predictions of the newly
331                 # created tree.
332                 tic_pred = time()
333                 _update_raw_predictions(raw_predictions[k, :], grower)
334                 toc_pred = time()
335                 acc_prediction_time += toc_pred - tic_pred
336 
337             should_early_stop = False
338             if self.do_early_stopping_:
339                 if self.scoring == 'loss':
340                     # Update raw_predictions_val with the newest tree(s)
341                     if self._use_validation_data:
342                         for k, pred in enumerate(self._predictors[-1]):
343                             raw_predictions_val[k, :] += (
344                                 pred.predict_binned(
345                                     X_binned_val,
346                                     self.bin_mapper_.missing_values_bin_idx_
347                                 )
348                             )
349 
350                     should_early_stop = self._check_early_stopping_loss(
351                         raw_predictions, y_train,
352                         raw_predictions_val, y_val
353                     )
354 
355                 else:
356                     should_early_stop = self._check_early_stopping_scorer(
357                         X_binned_small_train, y_small_train,
358                         X_binned_val, y_val,
359                     )
360 
361             if self.verbose:
362                 self._print_iteration_stats(iteration_start_time)
363 
364             # maybe we could also early stop if all the trees are stumps?
365             if should_early_stop:
366                 break
367 
368         if self.verbose:
369             duration = time() - fit_start_time
370             n_total_leaves = sum(
371                 predictor.get_n_leaf_nodes()
372                 for predictors_at_ith_iteration in self._predictors
373                 for predictor in predictors_at_ith_iteration
374             )
375             n_predictors = sum(
376                 len(predictors_at_ith_iteration)
377                 for predictors_at_ith_iteration in self._predictors)
378             print("Fit {} trees in {:.3f} s, ({} total leaves)".format(
379                 n_predictors, duration, n_total_leaves))
380             print("{:<32} {:.3f}s".format('Time spent computing histograms:',
381                                           acc_compute_hist_time))
382             print("{:<32} {:.3f}s".format('Time spent finding best splits:',
383                                           acc_find_split_time))
384             print("{:<32} {:.3f}s".format('Time spent applying splits:',
385                                           acc_apply_split_time))
386             print("{:<32} {:.3f}s".format('Time spent predicting:',
387                                           acc_prediction_time))
388 
389         self.train_score_ = np.asarray(self.train_score_)
390         self.validation_score_ = np.asarray(self.validation_score_)
391         del self._in_fit  # hard delete so we're sure it can't be used anymore
392         return self

</code><intended_behavior>During the `fit` method's execution, especially after the call to `_get_loss`, an additional validation step could be incorporated to check the suitability of the loss function chosen for the data it is being applied to. This may include verifying that the selected loss function aligns with the type of classification task being performed. While not directly necessary to fix the reported issue, this could enhance robustness and user guidance, preventing similar issues from arising in different contexts or with other parameters.</intended_behavior>, <file>sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py</file>
<class>BaseHistGradientBoosting</class>
<method>None</method>
<code>
25 class BaseHistGradientBoosting(BaseEstimator, ABC):
26     """Base class for histogram-based gradient boosting estimators."""
27 
28     @abstractmethod
29     def __init__(self, loss, learning_rate, max_iter, max_leaf_nodes,
30                  max_depth, min_samples_leaf, l2_regularization, max_bins,
31                  warm_start, scoring, validation_fraction, n_iter_no_change,
32                  tol, verbose, random_state):
33         self.loss = loss
34         self.learning_rate = learning_rate
35         self.max_iter = max_iter
36         self.max_leaf_nodes = max_leaf_nodes
37         self.max_depth = max_depth
38         self.min_samples_leaf = min_samples_leaf
39         self.l2_regularization = l2_regularization
40         self.max_bins = max_bins
41         self.warm_start = warm_start
42         self.scoring = scoring
43         self.validation_fraction = validation_fraction
44         self.n_iter_no_change = n_iter_no_change
45         self.tol = tol
46         self.verbose = verbose
47         self.random_state = random_state
48 
49     def _validate_parameters(self):
50         """Validate parameters passed to __init__.
51 
52         The parameters that are directly passed to the grower are checked in
53         TreeGrower."""
54 
55         if self.loss not in self._VALID_LOSSES:
56             raise ValueError(
57                 "Loss {} is not supported for {}. Accepted losses: "
58                 "{}.".format(self.loss, self.__class__.__name__,
59                              ', '.join(self._VALID_LOSSES)))
60 
61         if self.learning_rate <= 0:
62             raise ValueError('learning_rate={} must '
63                              'be strictly positive'.format(self.learning_rate))
64         if self.max_iter < 1:
65             raise ValueError('max_iter={} must not be smaller '
66                              'than 1.'.format(self.max_iter))
67         if self.n_iter_no_change is not None and self.n_iter_no_change < 0:
68             raise ValueError('n_iter_no_change={} must be '
69                              'positive.'.format(self.n_iter_no_change))
70         if (self.validation_fraction is not None and
71                 self.validation_fraction <= 0):
72             raise ValueError(
73                 'validation_fraction={} must be strictly '
74                 'positive, or None.'.format(self.validation_fraction))
75         if self.tol is not None and self.tol < 0:
76             raise ValueError('tol={} '
77                              'must not be smaller than 0.'.format(self.tol))
78 
79         if not (2 <= self.max_bins <= 255):
80             raise ValueError('max_bins={} should be no smaller than 2 '
81                              'and no larger than 255.'.format(self.max_bins))
82 
83     def fit(self, X, y):
84         """Fit the gradient boosting model.
85 
86         Parameters
87         ----------
88         X : array-like, shape=(n_samples, n_features)
89             The input samples.
90 
91         y : array-like, shape=(n_samples,)
92             Target values.
93 
94         Returns
95         -------
96         self : object
97         """
98         fit_start_time = time()
99         acc_find_split_time = 0.  # time spent finding the best splits
100         acc_apply_split_time = 0.  # time spent splitting nodes
101         acc_compute_hist_time = 0.  # time spent computing histograms
102         # time spent predicting X for gradient and hessians update
103         acc_prediction_time = 0.
104         X, y = check_X_y(X, y, dtype=[X_DTYPE], force_all_finite=False)
105         y = self._encode_y(y)
106 
107         # The rng state must be preserved if warm_start is True
108         if (self.warm_start and hasattr(self, '_rng')):
109             rng = self._rng
110         else:
111             rng = check_random_state(self.random_state)
112             self._rng = rng
113 
114         self._validate_parameters()
115         self.n_features_ = X.shape[1]  # used for validation in predict()
116 
117         # we need this stateful variable to tell raw_predict() that it was
118         # called from fit() (this current method), and that the data it has
119         # received is pre-binned.
120         # predicting is faster on pre-binned data, so we want early stopping
121         # predictions to be made on pre-binned data. Unfortunately the scorer_
122         # can only call predict() or predict_proba(), not raw_predict(), and
123         # there's no way to tell the scorer that it needs to predict binned
124         # data.
125         self._in_fit = True
126 
127         self.loss_ = self._get_loss()
128 
129         self.do_early_stopping_ = (self.n_iter_no_change is not None and
130                                    self.n_iter_no_change > 0)
131 
132         # create validation data if needed
133         self._use_validation_data = self.validation_fraction is not None
134         if self.do_early_stopping_ and self._use_validation_data:
135             # stratify for classification
136             stratify = y if hasattr(self.loss_, 'predict_proba') else None
137 
138             # Save the state of the RNG for the training and validation split.
139             # This is needed in order to have the same split when using
140             # warm starting.
141             if not (self._is_fitted() and self.warm_start):
142                 self._train_val_split_seed = rng.randint(1024)
143 
144             X_train, X_val, y_train, y_val = train_test_split(
145                 X, y, test_size=self.validation_fraction, stratify=stratify,
146                 random_state=self._train_val_split_seed)
147         else:
148             X_train, y_train = X, y
149             X_val, y_val = None, None
150 
151         has_missing_values = np.isnan(X_train).any(axis=0).astype(np.uint8)
152 
153         # Bin the data
154         # For ease of use of the API, the user-facing GBDT classes accept the
155         # parameter max_bins, which doesn't take into account the bin for
156         # missing values (which is always allocated). However, since max_bins
157         # isn't the true maximal number of bins, all other private classes
158         # (binmapper, histbuilder...) accept n_bins instead, which is the
159         # actual total number of bins. Everywhere in the code, the
160         # convention is that n_bins == max_bins + 1
161         n_bins = self.max_bins + 1  # + 1 for missing values
162         self.bin_mapper_ = _BinMapper(n_bins=n_bins, random_state=rng)
163         X_binned_train = self._bin_data(X_train, rng, is_training_data=True)
164         if X_val is not None:
165             X_binned_val = self._bin_data(X_val, rng, is_training_data=False)
166         else:
167             X_binned_val = None
168 
169         if self.verbose:
170             print("Fitting gradient boosted rounds:")
171 
172         n_samples = X_binned_train.shape[0]
173 
174         # First time calling fit, or no warm start
175         if not (self._is_fitted() and self.warm_start):
176             # Clear random state and score attributes
177             self._clear_state()
178 
179             # initialize raw_predictions: those are the accumulated values
180             # predicted by the trees for the training data. raw_predictions has
181             # shape (n_trees_per_iteration, n_samples) where
182             # n_trees_per_iterations is n_classes in multiclass classification,
183             # else 1.
184             self._baseline_prediction = self.loss_.get_baseline_prediction(
185                 y_train, self.n_trees_per_iteration_
186             )
187             raw_predictions = np.zeros(
188                 shape=(self.n_trees_per_iteration_, n_samples),
189                 dtype=self._baseline_prediction.dtype
190             )
191             raw_predictions += self._baseline_prediction
192 
193             # initialize gradients and hessians (empty arrays).
194             # shape = (n_trees_per_iteration, n_samples).
195             gradients, hessians = self.loss_.init_gradients_and_hessians(
196                 n_samples=n_samples,
197                 prediction_dim=self.n_trees_per_iteration_
198             )
199 
200             # predictors is a matrix (list of lists) of TreePredictor objects
201             # with shape (n_iter_, n_trees_per_iteration)
202             self._predictors = predictors = []
203 
204             # Initialize structures and attributes related to early stopping
205             self.scorer_ = None  # set if scoring != loss
206             raw_predictions_val = None  # set if scoring == loss and use val
207             self.train_score_ = []
208             self.validation_score_ = []
209 
210             if self.do_early_stopping_:
211                 # populate train_score and validation_score with the
212                 # predictions of the initial model (before the first tree)
213 
214                 if self.scoring == 'loss':
215                     # we're going to compute scoring w.r.t the loss. As losses
216                     # take raw predictions as input (unlike the scorers), we
217                     # can optimize a bit and avoid repeating computing the
218                     # predictions of the previous trees. We'll re-use
219                     # raw_predictions (as it's needed for training anyway) for
220                     # evaluating the training loss, and create
221                     # raw_predictions_val for storing the raw predictions of
222                     # the validation data.
223 
224                     if self._use_validation_data:
225                         raw_predictions_val = np.zeros(
226                             shape=(self.n_trees_per_iteration_,
227                                    X_binned_val.shape[0]),
228                             dtype=self._baseline_prediction.dtype
229                         )
230 
231                         raw_predictions_val += self._baseline_prediction
232 
233                     self._check_early_stopping_loss(raw_predictions, y_train,
234                                                     raw_predictions_val, y_val)
235                 else:
236                     self.scorer_ = check_scoring(self, self.scoring)
237                     # scorer_ is a callable with signature (est, X, y) and
238                     # calls est.predict() or est.predict_proba() depending on
239                     # its nature.
240                     # Unfortunately, each call to scorer_() will compute
241                     # the predictions of all the trees. So we use a subset of
242                     # the training set to compute train scores.
243 
244                     # Save the seed for the small trainset generator
245                     self._small_trainset_seed = rng.randint(1024)
246 
247                     # Compute the subsample set
248                     (X_binned_small_train,
249                      y_small_train) = self._get_small_trainset(
250                         X_binned_train, y_train, self._small_trainset_seed)
251 
252                     self._check_early_stopping_scorer(
253                         X_binned_small_train, y_small_train,
254                         X_binned_val, y_val,
255                     )
256             begin_at_stage = 0
257 
258         # warm start: this is not the first time fit was called
259         else:
260             # Check that the maximum number of iterations is not smaller
261             # than the number of iterations from the previous fit
262             if self.max_iter < self.n_iter_:
263                 raise ValueError(
264                     'max_iter=%d must be larger than or equal to '
265                     'n_iter_=%d when warm_start==True'
266                     % (self.max_iter, self.n_iter_)
267                 )
268 
269             # Convert array attributes to lists
270             self.train_score_ = self.train_score_.tolist()
271             self.validation_score_ = self.validation_score_.tolist()
272 
273             # Compute raw predictions
274             raw_predictions = self._raw_predict(X_binned_train)
275 
276             if self.do_early_stopping_ and self.scoring != 'loss':
277                 # Compute the subsample set
278                 X_binned_small_train, y_small_train = self._get_small_trainset(
279                     X_binned_train, y_train, self._small_trainset_seed)
280 
281             # Initialize the gradients and hessians
282             gradients, hessians = self.loss_.init_gradients_and_hessians(
283                 n_samples=n_samples,
284                 prediction_dim=self.n_trees_per_iteration_
285             )
286 
287             # Get the predictors from the previous fit
288             predictors = self._predictors
289 
290             begin_at_stage = self.n_iter_
291 
292         for iteration in range(begin_at_stage, self.max_iter):
293 
294             if self.verbose:
295                 iteration_start_time = time()
296                 print("[{}/{}] ".format(iteration + 1, self.max_iter),
297                       end='', flush=True)
298 
299             # Update gradients and hessians, inplace
300             self.loss_.update_gradients_and_hessians(gradients, hessians,
301                                                      y_train, raw_predictions)
302 
303             # Append a list since there may be more than 1 predictor per iter
304             predictors.append([])
305 
306             # Build `n_trees_per_iteration` trees.
307             for k in range(self.n_trees_per_iteration_):
308 
309                 grower = TreeGrower(
310                     X_binned_train, gradients[k, :], hessians[k, :],
311                     n_bins=n_bins,
312                     n_bins_non_missing=self.bin_mapper_.n_bins_non_missing_,
313                     has_missing_values=has_missing_values,
314                     max_leaf_nodes=self.max_leaf_nodes,
315                     max_depth=self.max_depth,
316                     min_samples_leaf=self.min_samples_leaf,
317                     l2_regularization=self.l2_regularization,
318                     shrinkage=self.learning_rate)
319                 grower.grow()
320 
321                 acc_apply_split_time += grower.total_apply_split_time
322                 acc_find_split_time += grower.total_find_split_time
323                 acc_compute_hist_time += grower.total_compute_hist_time
324 
325                 predictor = grower.make_predictor(
326                     bin_thresholds=self.bin_mapper_.bin_thresholds_
327                 )
328                 predictors[-1].append(predictor)
329 
330                 # Update raw_predictions with the predictions of the newly
331                 # created tree.
332                 tic_pred = time()
333                 _update_raw_predictions(raw_predictions[k, :], grower)
334                 toc_pred = time()
335                 acc_prediction_time += toc_pred - tic_pred
336 
337             should_early_stop = False
338             if self.do_early_stopping_:
339                 if self.scoring == 'loss':
340                     # Update raw_predictions_val with the newest tree(s)
341                     if self._use_validation_data:
342                         for k, pred in enumerate(self._predictors[-1]):
343                             raw_predictions_val[k, :] += (
344                                 pred.predict_binned(
345                                     X_binned_val,
346                                     self.bin_mapper_.missing_values_bin_idx_
347                                 )
348                             )
349 
350                     should_early_stop = self._check_early_stopping_loss(
351                         raw_predictions, y_train,
352                         raw_predictions_val, y_val
353                     )
354 
355                 else:
356                     should_early_stop = self._check_early_stopping_scorer(
357                         X_binned_small_train, y_small_train,
358                         X_binned_val, y_val,
359                     )
360 
361             if self.verbose:
362                 self._print_iteration_stats(iteration_start_time)
363 
364             # maybe we could also early stop if all the trees are stumps?
365             if should_early_stop:
366                 break
367 
368         if self.verbose:
369             duration = time() - fit_start_time
370             n_total_leaves = sum(
371                 predictor.get_n_leaf_nodes()
372                 for predictors_at_ith_iteration in self._predictors
373                 for predictor in predictors_at_ith_iteration
374             )
375             n_predictors = sum(
376                 len(predictors_at_ith_iteration)
377                 for predictors_at_ith_iteration in self._predictors)
378             print("Fit {} trees in {:.3f} s, ({} total leaves)".format(
379                 n_predictors, duration, n_total_leaves))
380             print("{:<32} {:.3f}s".format('Time spent computing histograms:',
381                                           acc_compute_hist_time))
382             print("{:<32} {:.3f}s".format('Time spent finding best splits:',
383                                           acc_find_split_time))
384             print("{:<32} {:.3f}s".format('Time spent applying splits:',
385                                           acc_apply_split_time))
386             print("{:<32} {:.3f}s".format('Time spent predicting:',
387                                           acc_prediction_time))
388 
389         self.train_score_ = np.asarray(self.train_score_)
390         self.validation_score_ = np.asarray(self.validation_score_)
391         del self._in_fit  # hard delete so we're sure it can't be used anymore
392         return self
393 
394     def _is_fitted(self):
395         return len(getattr(self, '_predictors', [])) > 0
396 
397     def _clear_state(self):
398         """Clear the state of the gradient boosting model."""
399         for var in ('train_score_', 'validation_score_', '_rng'):
400             if hasattr(self, var):
401                 delattr(self, var)
402 
403     def _get_small_trainset(self, X_binned_train, y_train, seed):
404         """Compute the indices of the subsample set and return this set.
405 
406         For efficiency, we need to subsample the training set to compute scores
407         with scorers.
408         """
409         subsample_size = 10000
410         if X_binned_train.shape[0] > subsample_size:
411             indices = np.arange(X_binned_train.shape[0])
412             stratify = y_train if is_classifier(self) else None
413             indices = resample(indices, n_samples=subsample_size,
414                                replace=False, random_state=seed,
415                                stratify=stratify)
416             X_binned_small_train = X_binned_train[indices]
417             y_small_train = y_train[indices]
418             X_binned_small_train = np.ascontiguousarray(X_binned_small_train)
419             return X_binned_small_train, y_small_train
420         else:
421             return X_binned_train, y_train
422 
423     def _check_early_stopping_scorer(self, X_binned_small_train, y_small_train,
424                                      X_binned_val, y_val):
425         """Check if fitting should be early-stopped based on scorer.
426 
427         Scores are computed on validation data or on training data.
428         """
429         if is_classifier(self):
430             y_small_train = self.classes_[y_small_train.astype(int)]
431         self.train_score_.append(
432             self.scorer_(self, X_binned_small_train, y_small_train)
433         )
434 
435         if self._use_validation_data:
436             if is_classifier(self):
437                 y_val = self.classes_[y_val.astype(int)]
438             self.validation_score_.append(
439                 self.scorer_(self, X_binned_val, y_val)
440             )
441             return self._should_stop(self.validation_score_)
442         else:
443             return self._should_stop(self.train_score_)
444 
445     def _check_early_stopping_loss(self,
446                                    raw_predictions,
447                                    y_train,
448                                    raw_predictions_val,
449                                    y_val):
450         """Check if fitting should be early-stopped based on loss.
451 
452         Scores are computed on validation data or on training data.
453         """
454 
455         self.train_score_.append(
456             -self.loss_(y_train, raw_predictions)
457         )
458 
459         if self._use_validation_data:
460             self.validation_score_.append(
461                 -self.loss_(y_val, raw_predictions_val)
462             )
463             return self._should_stop(self.validation_score_)
464         else:
465             return self._should_stop(self.train_score_)
466 
467     def _should_stop(self, scores):
468         """
469         Return True (do early stopping) if the last n scores aren't better
470         than the (n-1)th-to-last score, up to some tolerance.
471         """
472         reference_position = self.n_iter_no_change + 1
473         if len(scores) < reference_position:
474             return False
475 
476         # A higher score is always better. Higher tol means that it will be
477         # harder for subsequent iteration to be considered an improvement upon
478         # the reference score, and therefore it is more likely to early stop
479         # because of the lack of significant improvement.
480         tol = 0 if self.tol is None else self.tol
481         reference_score = scores[-reference_position] + tol
482         recent_scores = scores[-reference_position + 1:]
483         recent_improvements = [score > reference_score
484                                for score in recent_scores]
485         return not any(recent_improvements)
486 
487     def _bin_data(self, X, rng, is_training_data):
488         """Bin data X.
489 
490         If is_training_data, then set the bin_mapper_ attribute.
491         Else, the binned data is converted to a C-contiguous array.
492         """
493 
494         description = 'training' if is_training_data else 'validation'
495         if self.verbose:
496             print("Binning {:.3f} GB of {} data: ".format(
497                 X.nbytes / 1e9, description), end="", flush=True)
498         tic = time()
499         if is_training_data:
500             X_binned = self.bin_mapper_.fit_transform(X)  # F-aligned array
501         else:
502             X_binned = self.bin_mapper_.transform(X)  # F-aligned array
503             # We convert the array to C-contiguous since predicting is faster
504             # with this layout (training is faster on F-arrays though)
505             X_binned = np.ascontiguousarray(X_binned)
506         toc = time()
507         if self.verbose:
508             duration = toc - tic
509             print("{:.3f} s".format(duration))
510 
511         return X_binned
512 
513     def _print_iteration_stats(self, iteration_start_time):
514         """Print info about the current fitting iteration."""
515         log_msg = ''
516 
517         predictors_of_ith_iteration = [
518             predictors_list for predictors_list in self._predictors[-1]
519             if predictors_list
520         ]
521         n_trees = len(predictors_of_ith_iteration)
522         max_depth = max(predictor.get_max_depth()
523                         for predictor in predictors_of_ith_iteration)
524         n_leaves = sum(predictor.get_n_leaf_nodes()
525                        for predictor in predictors_of_ith_iteration)
526 
527         if n_trees == 1:
528             log_msg += ("{} tree, {} leaves, ".format(n_trees, n_leaves))
529         else:
530             log_msg += ("{} trees, {} leaves ".format(n_trees, n_leaves))
531             log_msg += ("({} on avg), ".format(int(n_leaves / n_trees)))
532 
533         log_msg += "max depth = {}, ".format(max_depth)
534 
535         if self.do_early_stopping_:
536             if self.scoring == 'loss':
537                 factor = -1  # score_ arrays contain the negative loss
538                 name = 'loss'
539             else:
540                 factor = 1
541                 name = 'score'
542             log_msg += "train {}: {:.5f}, ".format(name, factor *
543                                                    self.train_score_[-1])
544             if self._use_validation_data:
545                 log_msg += "val {}: {:.5f}, ".format(
546                     name, factor * self.validation_score_[-1])
547 
548         iteration_time = time() - iteration_start_time
549         log_msg += "in {:0.3f}s".format(iteration_time)
550 
551         print(log_msg)
552 
553     def _raw_predict(self, X):
554         """Return the sum of the leaves values over all predictors.
555 
556         Parameters
557         ----------
558         X : array-like, shape=(n_samples, n_features)
559             The input samples.
560 
561         Returns
562         -------
563         raw_predictions : array, shape (n_samples * n_trees_per_iteration,)
564             The raw predicted values.
565         """
566         X = check_array(X, dtype=[X_DTYPE, X_BINNED_DTYPE],
567                         force_all_finite=False)
568         check_is_fitted(self)
569         if X.shape[1] != self.n_features_:
570             raise ValueError(
571                 'X has {} features but this estimator was trained with '
572                 '{} features.'.format(X.shape[1], self.n_features_)
573             )
574         is_binned = getattr(self, '_in_fit', False)
575         n_samples = X.shape[0]
576         raw_predictions = np.zeros(
577             shape=(self.n_trees_per_iteration_, n_samples),
578             dtype=self._baseline_prediction.dtype
579         )
580         raw_predictions += self._baseline_prediction
581         for predictors_of_ith_iteration in self._predictors:
582             for k, predictor in enumerate(predictors_of_ith_iteration):
583                 if is_binned:
584                     predict = partial(
585                         predictor.predict_binned,
586                         missing_values_bin_idx=self.bin_mapper_.missing_values_bin_idx_  # noqa
587                     )
588                 else:
589                     predict = predictor.predict
590                 raw_predictions[k, :] += predict(X)
591 
592         return raw_predictions
593 
594     def _compute_partial_dependence_recursion(self, grid, target_features):
595         """Fast partial dependence computation.
596 
597         Parameters
598         ----------
599         grid : ndarray, shape (n_samples, n_target_features)
600             The grid points on which the partial dependence should be
601             evaluated.
602         target_features : ndarray, shape (n_target_features)
603             The set of target features for which the partial dependence
604             should be evaluated.
605 
606         Returns
607         -------
608         averaged_predictions : ndarray, shape \
609                 (n_trees_per_iteration, n_samples)
610             The value of the partial dependence function on each grid point.
611         """
612         grid = np.asarray(grid, dtype=X_DTYPE, order='C')
613         averaged_predictions = np.zeros(
614             (self.n_trees_per_iteration_, grid.shape[0]), dtype=Y_DTYPE)
615 
616         for predictors_of_ith_iteration in self._predictors:
617             for k, predictor in enumerate(predictors_of_ith_iteration):
618                 predictor.compute_partial_dependence(grid, target_features,
619                                                      averaged_predictions[k])
620         # Note that the learning rate is already accounted for in the leaves
621         # values.
622 
623         return averaged_predictions
624 
625     def _more_tags(self):
626         return {'allow_nan': True}
627 
628     @abstractmethod
629     def _get_loss(self):
630         pass
631 
632     @abstractmethod
633     def _encode_y(self, y=None):
634         pass
635 
636     @property
637     def n_iter_(self):
638         check_is_fitted(self)
639         return len(self._predictors)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 17:25:12.972 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 17:25:12.972 | INFO     | Search-only: skipping patch selection
2025-10-18 17:25:13.074 | INFO     | Task scikit-learn__scikit-learn-14869 completed successfully.
2025-10-18 17:25:13.075 | INFO     | No patch generated. You can try running ACR again.
