2025-10-18 10:04:11.345 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 10:04:15.420 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=500, output_tokens=12, cost=0.005360
2025-10-18 10:04:26.354 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=680, output_tokens=526, cost=0.022580
2025-10-18 10:04:27.340 | INFO     | registered non reproducing test 0
2025-10-18 10:04:41.049 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1485, output_tokens=633, cost=0.033840
2025-10-18 10:04:42.028 | INFO     | registered non reproducing test 1
2025-10-18 10:04:55.947 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1592, output_tokens=593, cost=0.033710
2025-10-18 10:04:56.922 | INFO     | registered non reproducing test 2
2025-10-18 10:04:56.922 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-18 10:04:56.928 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 10:05:02.993 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=809, output_tokens=296, cost=0.016970
2025-10-18 10:05:03.001 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 10:05:06.438 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=650, output_tokens=87, cost=0.009110
2025-10-18 10:05:06.439 | DEBUG    | Extracted a valid json.
2025-10-18 10:05:06.439 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_method(\"create_connection\")",
        "search_method(\"features\")",
        "search_class(\"DatabaseErrorWrapper\")",
        "search_method_in_class(\"execute\", \"DatabaseErrorWrapper\")",
        "search_class(\"Window\")",
        "search_method(\"annotate\")",
        "search_code(\"sqlite3.OperationalError\")"
    ],
    "bug_locations": []
}
2025-10-18 10:05:06.509 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 10:05:06.510 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 10:05:24.015 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2544, output_tokens=905, cost=0.052590
2025-10-18 10:05:24.033 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 10:05:32.372 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3665, output_tokens=415, cost=0.049100
2025-10-18 10:05:32.380 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 10:05:40.777 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=769, output_tokens=348, cost=0.018130
2025-10-18 10:05:40.779 | DEBUG    | Extracted a valid json.
2025-10-18 10:05:40.779 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "django/db/models/expressions.py",
            "class": "Window",
            "method": "as_sql",
            "intended_behavior": "The `as_sql` method inside the `Window` class should include a check for the SQLite version if the database connection is to SQLite. Before generating the SQL, it should verify if the version supports window functions (SQLite 3.25.0 or higher). If the version does not support window functions, the method should raise a clear, descriptive error indicating that the operation cannot be performed due to the lack of support in the SQLite version being used. This error should suggest the user upgrade their SQLite version if possible. This check can preserve existing functionality by only altering behavior in cases where the operation would lead to an SQL syntax error due to version incompatibilities."
        },
        {
            "file": "django/db/models/query.py",
            "class": "QuerySet",
            "method": "annotate",
            "intended_behavior": "In the `annotate` method, when processing annotations that involve window functions, add a preliminary check to confirm if the underlying database engine supports those window functions. This could involve extending the `_validate_values_are_expressions` method (or where appropriate within this workflow) to include checks for database feature support, particularly focusing on window functions for SQLite. In case the database does not support the feature, a descriptive error should be raised early in the process, advising on the lack of support and possible resolutions (e.g., database upgrade). This approach maintains the method's core functionality while improving error handling and messaging for unsupported features."
        }
    ]
}
2025-10-18 10:05:40.784 | DEBUG    | Found override of as_sql in BaseExpression
2025-10-18 10:05:40.784 | ERROR    | SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
Traceback (most recent call last):

  File "/home/kimnal0/auto-code-rover/app/main.py", line 656, in <module>
    main()
    └ <function main at 0x761e6b526c00>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 120, in main
    run_task_groups(groups, num_processes, organize_output=True)
    │               │       └ 1
    │               └ {'setup_astropy__astropy__4.3': [<app.raw_tasks.RawSweTask object at 0x761d78dc37d0>, <app.raw_tasks.RawSweTask object at 0x7...
    └ <function run_task_groups at 0x761d7b11a3e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 396, in run_task_groups
    run_tasks_serial(all_tasks)
    │                └ [<app.raw_tasks.RawSweTask object at 0x761d78dc37d0>, <app.raw_tasks.RawSweTask object at 0x761d78dc3800>, <app.raw_tasks.Raw...
    └ <function run_tasks_serial at 0x761d7b11a480>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 414, in run_tasks_serial
    run_task_in_subprocess(task)
    │                      └ <app.raw_tasks.RawSweTask object at 0x761d78dc3bf0>
    └ <function run_task_in_subprocess at 0x761d7b11a660>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 462, in run_task_in_subprocess
    executor.submit(run_raw_task, task)
    │        │      │             └ <app.raw_tasks.RawSweTask object at 0x761d78dc3bf0>
    │        │      └ <function run_raw_task at 0x761d7b11a700>
    │        └ <function ProcessPoolExecutor.submit at 0x761e6ae72020>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x761d7b1d56d0>

  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 831, in submit
    self._start_executor_manager_thread()
    │    └ <function ProcessPoolExecutor._start_executor_manager_thread at 0x761e6ae71da0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x761d7b1d56d0>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 770, in _start_executor_manager_thread
    self._launch_processes()
    │    └ <function ProcessPoolExecutor._launch_processes at 0x761e6ae71ee0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x761d7b1d56d0>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 797, in _launch_processes
    self._spawn_process()
    │    └ <function ProcessPoolExecutor._spawn_process at 0x761e6ae71f80>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x761d7b1d56d0>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 807, in _spawn_process
    p.start()
    │ └ <function BaseProcess.start at 0x761e6b02cea0>
    └ <ForkProcess name='ForkProcess-23' parent=497161 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
    │    │        │    │      └ <ForkProcess name='ForkProcess-23' parent=497161 started>
    │    │        │    └ <staticmethod(<function ForkProcess._Popen at 0x761e6b09e0c0>)>
    │    │        └ <ForkProcess name='ForkProcess-23' parent=497161 started>
    │    └ None
    └ <ForkProcess name='ForkProcess-23' parent=497161 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/context.py", line 282, in _Popen
    return Popen(process_obj)
           │     └ <ForkProcess name='ForkProcess-23' parent=497161 started>
           └ <class 'multiprocessing.popen_fork.Popen'>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
    │    │       └ <ForkProcess name='ForkProcess-23' parent=497161 started>
    │    └ <function Popen._launch at 0x761d7911b7e0>
    └ <multiprocessing.popen_fork.Popen object at 0x761d7a57e450>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
           │           │                          └ 13
           │           └ <function BaseProcess._bootstrap at 0x761e6b02d8a0>
           └ <ForkProcess name='ForkProcess-23' parent=497161 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x761e6b02ce00>
    └ <ForkProcess name='ForkProcess-23' parent=497161 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {}
    │    │        │    │        └ <ForkProcess name='ForkProcess-23' parent=497161 started>
    │    │        │    └ (<concurrent.futures.process._SafeQueue object at 0x761d795c4050>, <multiprocessing.queues.SimpleQueue object at 0x761d795c40...
    │    │        └ <ForkProcess name='ForkProcess-23' parent=497161 started>
    │    └ <function _process_worker at 0x761e6ae711c0>
    └ <ForkProcess name='ForkProcess-23' parent=497161 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 263, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
        │         │   │         │       │         └ {}
        │         │   │         │       └ <concurrent.futures.process._CallItem object at 0x761d7b442db0>
        │         │   │         └ (<app.raw_tasks.RawSweTask object at 0x761d7a57e840>,)
        │         │   └ <concurrent.futures.process._CallItem object at 0x761d7b442db0>
        │         └ <function run_raw_task at 0x761d7b11a700>
        └ <concurrent.futures.process._CallItem object at 0x761d7b442db0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 494, in run_raw_task
    run_ok = do_inference(task.to_task(), task_output_dir)
             │            │    │          └ '/home/kimnal0/auto-code-rover/only_fl_output5/django__django-10853_2025-10-18_10-04-10'
             │            │    └ <function RawSweTask.to_task at 0x761d7b1196c0>
             │            └ <app.raw_tasks.RawSweTask object at 0x761d7a57e840>
             └ <function do_inference at 0x761d7b11a8e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 590, in do_inference
    run_ok = inference.run_one_task(
             │         └ <function run_one_task at 0x761d7b228400>
             └ <module 'app.inference' from '/home/kimnal0/auto-code-rover/app/inference.py'>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 128, in run_one_task
    if _run_one_task(str(out_dir), api_manager, task.get_issue_statement()):
       │                 │         │            │    └ <function SweTask.get_issue_statement at 0x761d7b9b84a0>
       │                 │         │            └ SweTask(task_id='django__django-10853', problem_statement='SQLite (pre 3.25.0) does not support window functions, raises Oper...
       │                 │         └ <app.manage.ProjectApiManager object at 0x761d79007c50>
       │                 └ Path('/home/kimnal0/auto-code-rover/only_fl_output5/django__django-10853_2025-10-18_10-04-10/output_0')
       └ <function _run_one_task at 0x761d7b2298a0>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 303, in _run_one_task
    bug_locs, search_msg_thread = api_manager.search_manager.search_iterative(
                                  │           │              └ <function SearchManager.search_iterative at 0x761d7b431bc0>
                                  │           └ <app.search.search_manage.SearchManager object at 0x761d790073e0>
                                  └ <app.manage.ProjectApiManager object at 0x761d79007c50>

  File "/home/kimnal0/auto-code-rover/app/search/search_manage.py", line 125, in search_iterative
    new_bug_locations.extend(self.backend.get_bug_loc_snippets_new(loc))
    │                 │      │    │       │                        └ {'file': 'django/db/models/expressions.py', 'class': 'Window', 'method': 'as_sql', 'intended_behavior': 'The `as_sql` method ...
    │                 │      │    │       └ <function SearchBackend.get_bug_loc_snippets_new at 0x761d7b4318a0>
    │                 │      │    └ <app.search.search_backend.SearchBackend object at 0x761d79007f80>
    │                 │      └ <app.search.search_manage.SearchManager object at 0x761d790073e0>
    │                 └ <method 'extend' of 'list' objects>
    └ []

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 825, in get_bug_loc_snippets_new
    self._get_inherited_methods(res.class_name, res.func_name)
    │    │                      │   │           │   └ 'as_sql'
    │    │                      │   │           └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__3.0/django/db/mo...
    │    │                      │   └ 'Window'
    │    │                      └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__3.0/django/db/mo...
    │    └ <function SearchBackend._get_inherited_methods at 0x761d7b431800>
    └ <app.search.search_backend.SearchBackend object at 0x761d79007f80>

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 748, in _get_inherited_methods
    output, search_res, call_ok = self.search_method_in_class(super_call)
                                  │    │                      └ {'class_name': 'BaseExpression', 'method_name': 'as_sql'}
                                  │    └ <function SearchBackend.search_method_in_class at 0x761d7b430f40>
                                  └ <app.search.search_backend.SearchBackend object at 0x761d79007f80>

> File "/home/kimnal0/auto-code-rover/app/utils.py", line 343, in wrapper
    return func(*args, **kwargs)
           │     │       └ {}
           │     └ (<app.search.search_backend.SearchBackend object at 0x761d79007f80>, {'class_name': 'BaseExpression', 'method_name': 'as_sql'})
           └ <function SearchBackend.search_method_in_class at 0x761d7b430ea0>

TypeError: SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
2025-10-18 10:05:40.802 | DEBUG    | Bug location extracted successfully: [<file>django/db/models/expressions.py</file>
<class>Window</class>
<method>as_sql</method>
<code>
1238     def as_sql(self, compiler, connection, template=None):
1239         connection.ops.check_expression_support(self)
1240         expr_sql, params = compiler.compile(self.source_expression)
1241         window_sql, window_params = [], []
1242 
1243         if self.partition_by is not None:
1244             sql_expr, sql_params = self.partition_by.as_sql(
1245                 compiler=compiler, connection=connection,
1246                 template='PARTITION BY %(expressions)s',
1247             )
1248             window_sql.extend(sql_expr)
1249             window_params.extend(sql_params)
1250 
1251         if self.order_by is not None:
1252             window_sql.append(' ORDER BY ')
1253             order_sql, order_params = compiler.compile(self.order_by)
1254             window_sql.extend(order_sql)
1255             window_params.extend(order_params)
1256 
1257         if self.frame:
1258             frame_sql, frame_params = compiler.compile(self.frame)
1259             window_sql.append(' ' + frame_sql)
1260             window_params.extend(frame_params)
1261 
1262         params.extend(window_params)
1263         template = template or self.template
1264 
1265         return template % {
1266             'expression': expr_sql,
1267             'window': ''.join(window_sql).strip()
1268         }, params

</code><intended_behavior>The `as_sql` method inside the `Window` class should include a check for the SQLite version if the database connection is to SQLite. Before generating the SQL, it should verify if the version supports window functions (SQLite 3.25.0 or higher). If the version does not support window functions, the method should raise a clear, descriptive error indicating that the operation cannot be performed due to the lack of support in the SQLite version being used. This error should suggest the user upgrade their SQLite version if possible. This check can preserve existing functionality by only altering behavior in cases where the operation would lead to an SQL syntax error due to version incompatibilities.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Window</class>
<method>None</method>
<code>
1193 class Window(Expression):
1194     template = '%(expression)s OVER (%(window)s)'
1195     # Although the main expression may either be an aggregate or an
1196     # expression with an aggregate function, the GROUP BY that will
1197     # be introduced in the query as a result is not desired.
1198     contains_aggregate = False
1199     contains_over_clause = True
1200     filterable = False
1201 
1202     def __init__(self, expression, partition_by=None, order_by=None, frame=None, output_field=None):
1203         self.partition_by = partition_by
1204         self.order_by = order_by
1205         self.frame = frame
1206 
1207         if not getattr(expression, 'window_compatible', False):
1208             raise ValueError(
1209                 "Expression '%s' isn't compatible with OVER clauses." %
1210                 expression.__class__.__name__
1211             )
1212 
1213         if self.partition_by is not None:
1214             if not isinstance(self.partition_by, (tuple, list)):
1215                 self.partition_by = (self.partition_by,)
1216             self.partition_by = ExpressionList(*self.partition_by)
1217 
1218         if self.order_by is not None:
1219             if isinstance(self.order_by, (list, tuple)):
1220                 self.order_by = ExpressionList(*self.order_by)
1221             elif not isinstance(self.order_by, BaseExpression):
1222                 raise ValueError(
1223                     'order_by must be either an Expression or a sequence of '
1224                     'expressions.'
1225                 )
1226         super().__init__(output_field=output_field)
1227         self.source_expression = self._parse_expressions(expression)[0]
1228 
1229     def _resolve_output_field(self):
1230         return self.source_expression.output_field
1231 
1232     def get_source_expressions(self):
1233         return [self.source_expression, self.partition_by, self.order_by, self.frame]
1234 
1235     def set_source_expressions(self, exprs):
1236         self.source_expression, self.partition_by, self.order_by, self.frame = exprs
1237 
1238     def as_sql(self, compiler, connection, template=None):
1239         connection.ops.check_expression_support(self)
1240         expr_sql, params = compiler.compile(self.source_expression)
1241         window_sql, window_params = [], []
1242 
1243         if self.partition_by is not None:
1244             sql_expr, sql_params = self.partition_by.as_sql(
1245                 compiler=compiler, connection=connection,
1246                 template='PARTITION BY %(expressions)s',
1247             )
1248             window_sql.extend(sql_expr)
1249             window_params.extend(sql_params)
1250 
1251         if self.order_by is not None:
1252             window_sql.append(' ORDER BY ')
1253             order_sql, order_params = compiler.compile(self.order_by)
1254             window_sql.extend(order_sql)
1255             window_params.extend(order_params)
1256 
1257         if self.frame:
1258             frame_sql, frame_params = compiler.compile(self.frame)
1259             window_sql.append(' ' + frame_sql)
1260             window_params.extend(frame_params)
1261 
1262         params.extend(window_params)
1263         template = template or self.template
1264 
1265         return template % {
1266             'expression': expr_sql,
1267             'window': ''.join(window_sql).strip()
1268         }, params
1269 
1270     def __str__(self):
1271         return '{} OVER ({}{}{})'.format(
1272             str(self.source_expression),
1273             'PARTITION BY ' + str(self.partition_by) if self.partition_by else '',
1274             'ORDER BY ' + str(self.order_by) if self.order_by else '',
1275             str(self.frame or ''),
1276         )
1277 
1278     def __repr__(self):
1279         return '<%s: %s>' % (self.__class__.__name__, self)
1280 
1281     def get_group_by_cols(self):
1282         return []

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>annotate</method>
<code>
1011     def annotate(self, *args, **kwargs):
1012         """
1013         Return a query set in which the returned objects have been annotated
1014         with extra data or aggregations.
1015         """
1016         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1017         annotations = {}
1018         for arg in args:
1019             # The default_alias property may raise a TypeError.
1020             try:
1021                 if arg.default_alias in kwargs:
1022                     raise ValueError("The named annotation '%s' conflicts with the "
1023                                      "default name for another annotation."
1024                                      % arg.default_alias)
1025             except TypeError:
1026                 raise TypeError("Complex annotations require an alias")
1027             annotations[arg.default_alias] = arg
1028         annotations.update(kwargs)
1029 
1030         clone = self._chain()
1031         names = self._fields
1032         if names is None:
1033             names = set(chain.from_iterable(
1034                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1035                 for field in self.model._meta.get_fields()
1036             ))
1037 
1038         for alias, annotation in annotations.items():
1039             if alias in names:
1040                 raise ValueError("The annotation '%s' conflicts with a field on "
1041                                  "the model." % alias)
1042             if isinstance(annotation, FilteredRelation):
1043                 clone.query.add_filtered_relation(annotation, alias)
1044             else:
1045                 clone.query.add_annotation(annotation, alias, is_summary=False)
1046 
1047         for alias, annotation in clone.query.annotations.items():
1048             if alias in annotations and annotation.contains_aggregate:
1049                 if clone._fields is None:
1050                     clone.query.group_by = True
1051                 else:
1052                     clone.query.set_group_by()
1053                 break
1054 
1055         return clone

</code><intended_behavior>In the `annotate` method, when processing annotations that involve window functions, add a preliminary check to confirm if the underlying database engine supports those window functions. This could involve extending the `_validate_values_are_expressions` method (or where appropriate within this workflow) to include checks for database feature support, particularly focusing on window functions for SQLite. In case the database does not support the feature, a descriptive error should be raised early in the process, advising on the lack of support and possible resolutions (e.g., database upgrade). This approach maintains the method's core functionality while improving error handling and messaging for unsupported features.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
185 class QuerySet:
186     """Represent a lazy database lookup for a set of objects."""
187 
188     def __init__(self, model=None, query=None, using=None, hints=None):
189         self.model = model
190         self._db = using
191         self._hints = hints or {}
192         self.query = query or sql.Query(self.model)
193         self._result_cache = None
194         self._sticky_filter = False
195         self._for_write = False
196         self._prefetch_related_lookups = ()
197         self._prefetch_done = False
198         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
199         self._iterable_class = ModelIterable
200         self._fields = None
201 
202     def as_manager(cls):
203         # Address the circular dependency between `Queryset` and `Manager`.
204         from django.db.models.manager import Manager
205         manager = Manager.from_queryset(cls)()
206         manager._built_with_as_manager = True
207         return manager
208     as_manager.queryset_only = True
209     as_manager = classmethod(as_manager)
210 
211     ########################
212     # PYTHON MAGIC METHODS #
213     ########################
214 
215     def __deepcopy__(self, memo):
216         """Don't populate the QuerySet's cache."""
217         obj = self.__class__()
218         for k, v in self.__dict__.items():
219             if k == '_result_cache':
220                 obj.__dict__[k] = None
221             else:
222                 obj.__dict__[k] = copy.deepcopy(v, memo)
223         return obj
224 
225     def __getstate__(self):
226         # Force the cache to be fully populated.
227         self._fetch_all()
228         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}
229 
230     def __setstate__(self, state):
231         msg = None
232         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
233         if pickled_version:
234             current_version = get_version()
235             if current_version != pickled_version:
236                 msg = (
237                     "Pickled queryset instance's Django version %s does not "
238                     "match the current version %s." % (pickled_version, current_version)
239                 )
240         else:
241             msg = "Pickled queryset instance's Django version is not specified."
242 
243         if msg:
244             warnings.warn(msg, RuntimeWarning, stacklevel=2)
245 
246         self.__dict__.update(state)
247 
248     def __repr__(self):
249         data = list(self[:REPR_OUTPUT_SIZE + 1])
250         if len(data) > REPR_OUTPUT_SIZE:
251             data[-1] = "…(remaining elements truncated)…"
252         return '<%s %r>' % (self.__class__.__name__, data)
253 
254     def __len__(self):
255         self._fetch_all()
256         return len(self._result_cache)
257 
258     def __iter__(self):
259         """
260         The queryset iterator protocol uses three nested iterators in the
261         default case:
262             1. sql.compiler.execute_sql()
263                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
264                  using cursor.fetchmany(). This part is responsible for
265                  doing some column masking, and returning the rows in chunks.
266             2. sql.compiler.results_iter()
267                - Returns one row at time. At this point the rows are still just
268                  tuples. In some cases the return values are converted to
269                  Python values at this location.
270             3. self.iterator()
271                - Responsible for turning the rows into model objects.
272         """
273         self._fetch_all()
274         return iter(self._result_cache)
275 
276     def __bool__(self):
277         self._fetch_all()
278         return bool(self._result_cache)
279 
280     def __getitem__(self, k):
281         """Retrieve an item or slice from the set of results."""
282         if not isinstance(k, (int, slice)):
283             raise TypeError
284         assert ((not isinstance(k, slice) and (k >= 0)) or
285                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
286                  (k.stop is None or k.stop >= 0))), \
287             "Negative indexing is not supported."
288 
289         if self._result_cache is not None:
290             return self._result_cache[k]
291 
292         if isinstance(k, slice):
293             qs = self._chain()
294             if k.start is not None:
295                 start = int(k.start)
296             else:
297                 start = None
298             if k.stop is not None:
299                 stop = int(k.stop)
300             else:
301                 stop = None
302             qs.query.set_limits(start, stop)
303             return list(qs)[::k.step] if k.step else qs
304 
305         qs = self._chain()
306         qs.query.set_limits(k, k + 1)
307         qs._fetch_all()
308         return qs._result_cache[0]
309 
310     def __and__(self, other):
311         self._merge_sanity_check(other)
312         if isinstance(other, EmptyQuerySet):
313             return other
314         if isinstance(self, EmptyQuerySet):
315             return self
316         combined = self._chain()
317         combined._merge_known_related_objects(other)
318         combined.query.combine(other.query, sql.AND)
319         return combined
320 
321     def __or__(self, other):
322         self._merge_sanity_check(other)
323         if isinstance(self, EmptyQuerySet):
324             return other
325         if isinstance(other, EmptyQuerySet):
326             return self
327         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
328         combined = query._chain()
329         combined._merge_known_related_objects(other)
330         if not other.query.can_filter():
331             other = other.model._base_manager.filter(pk__in=other.values('pk'))
332         combined.query.combine(other.query, sql.OR)
333         return combined
334 
335     ####################################
336     # METHODS THAT DO DATABASE QUERIES #
337     ####################################
338 
339     def _iterator(self, use_chunked_fetch, chunk_size):
340         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
341 
342     def iterator(self, chunk_size=2000):
343         """
344         An iterator over the results from applying this QuerySet to the
345         database.
346         """
347         if chunk_size <= 0:
348             raise ValueError('Chunk size must be strictly positive.')
349         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
350         return self._iterator(use_chunked_fetch, chunk_size)
351 
352     def aggregate(self, *args, **kwargs):
353         """
354         Return a dictionary containing the calculations (aggregation)
355         over the current queryset.
356 
357         If args is present the expression is passed as a kwarg using
358         the Aggregate object's default alias.
359         """
360         if self.query.distinct_fields:
361             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
362         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
363         for arg in args:
364             # The default_alias property raises TypeError if default_alias
365             # can't be set automatically or AttributeError if it isn't an
366             # attribute.
367             try:
368                 arg.default_alias
369             except (AttributeError, TypeError):
370                 raise TypeError("Complex aggregates require an alias")
371             kwargs[arg.default_alias] = arg
372 
373         query = self.query.chain()
374         for (alias, aggregate_expr) in kwargs.items():
375             query.add_annotation(aggregate_expr, alias, is_summary=True)
376             if not query.annotations[alias].contains_aggregate:
377                 raise TypeError("%s is not an aggregate expression" % alias)
378         return query.get_aggregation(self.db, kwargs)
379 
380     def count(self):
381         """
382         Perform a SELECT COUNT() and return the number of records as an
383         integer.
384 
385         If the QuerySet is already fully cached, return the length of the
386         cached results set to avoid multiple SELECT COUNT(*) calls.
387         """
388         if self._result_cache is not None:
389             return len(self._result_cache)
390 
391         return self.query.get_count(using=self.db)
392 
393     def get(self, *args, **kwargs):
394         """
395         Perform the query and return a single object matching the given
396         keyword arguments.
397         """
398         clone = self.filter(*args, **kwargs)
399         if self.query.can_filter() and not self.query.distinct_fields:
400             clone = clone.order_by()
401         num = len(clone)
402         if num == 1:
403             return clone._result_cache[0]
404         if not num:
405             raise self.model.DoesNotExist(
406                 "%s matching query does not exist." %
407                 self.model._meta.object_name
408             )
409         raise self.model.MultipleObjectsReturned(
410             "get() returned more than one %s -- it returned %s!" %
411             (self.model._meta.object_name, num)
412         )
413 
414     def create(self, **kwargs):
415         """
416         Create a new object with the given kwargs, saving it to the database
417         and returning the created object.
418         """
419         obj = self.model(**kwargs)
420         self._for_write = True
421         obj.save(force_insert=True, using=self.db)
422         return obj
423 
424     def _populate_pk_values(self, objs):
425         for obj in objs:
426             if obj.pk is None:
427                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
428 
429     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
430         """
431         Insert each of the instances into the database. Do *not* call
432         save() on each of the instances, do not send any pre/post_save
433         signals, and do not set the primary key attribute if it is an
434         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
435         Multi-table models are not supported.
436         """
437         # When you bulk insert you don't get the primary keys back (if it's an
438         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
439         # you can't insert into the child tables which references this. There
440         # are two workarounds:
441         # 1) This could be implemented if you didn't have an autoincrement pk
442         # 2) You could do it by doing O(n) normal inserts into the parent
443         #    tables to get the primary keys back and then doing a single bulk
444         #    insert into the childmost table.
445         # We currently set the primary keys on the objects when using
446         # PostgreSQL via the RETURNING ID clause. It should be possible for
447         # Oracle as well, but the semantics for extracting the primary keys is
448         # trickier so it's not done yet.
449         assert batch_size is None or batch_size > 0
450         # Check that the parents share the same concrete model with the our
451         # model to detect the inheritance pattern ConcreteGrandParent ->
452         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
453         # would not identify that case as involving multiple tables.
454         for parent in self.model._meta.get_parent_list():
455             if parent._meta.concrete_model is not self.model._meta.concrete_model:
456                 raise ValueError("Can't bulk create a multi-table inherited model")
457         if not objs:
458             return objs
459         self._for_write = True
460         connection = connections[self.db]
461         fields = self.model._meta.concrete_fields
462         objs = list(objs)
463         self._populate_pk_values(objs)
464         with transaction.atomic(using=self.db, savepoint=False):
465             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
466             if objs_with_pk:
467                 self._batched_insert(objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts)
468                 for obj_with_pk in objs_with_pk:
469                     obj_with_pk._state.adding = False
470                     obj_with_pk._state.db = self.db
471             if objs_without_pk:
472                 fields = [f for f in fields if not isinstance(f, AutoField)]
473                 ids = self._batched_insert(objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts)
474                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
475                     assert len(ids) == len(objs_without_pk)
476                 for obj_without_pk, pk in zip(objs_without_pk, ids):
477                     obj_without_pk.pk = pk
478                     obj_without_pk._state.adding = False
479                     obj_without_pk._state.db = self.db
480 
481         return objs
482 
483     def bulk_update(self, objs, fields, batch_size=None):
484         """
485         Update the given fields in each of the given objects in the database.
486         """
487         if batch_size is not None and batch_size < 0:
488             raise ValueError('Batch size must be a positive integer.')
489         if not fields:
490             raise ValueError('Field names must be given to bulk_update().')
491         objs = tuple(objs)
492         if any(obj.pk is None for obj in objs):
493             raise ValueError('All bulk_update() objects must have a primary key set.')
494         fields = [self.model._meta.get_field(name) for name in fields]
495         if any(not f.concrete or f.many_to_many for f in fields):
496             raise ValueError('bulk_update() can only be used with concrete fields.')
497         if any(f.primary_key for f in fields):
498             raise ValueError('bulk_update() cannot be used with primary key fields.')
499         if not objs:
500             return
501         # PK is used twice in the resulting update query, once in the filter
502         # and once in the WHEN. Each field will also have one CAST.
503         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
504         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
505         requires_casting = connections[self.db].features.requires_casted_case_in_updates
506         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
507         updates = []
508         for batch_objs in batches:
509             update_kwargs = {}
510             for field in fields:
511                 when_statements = []
512                 for obj in batch_objs:
513                     attr = getattr(obj, field.attname)
514                     if not isinstance(attr, Expression):
515                         attr = Value(attr, output_field=field)
516                     when_statements.append(When(pk=obj.pk, then=attr))
517                 case_statement = Case(*when_statements, output_field=field)
518                 if requires_casting:
519                     case_statement = Cast(case_statement, output_field=field)
520                 update_kwargs[field.attname] = case_statement
521             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
522         with transaction.atomic(using=self.db, savepoint=False):
523             for pks, update_kwargs in updates:
524                 self.filter(pk__in=pks).update(**update_kwargs)
525     bulk_update.alters_data = True
526 
527     def get_or_create(self, defaults=None, **kwargs):
528         """
529         Look up an object with the given kwargs, creating one if necessary.
530         Return a tuple of (object, created), where created is a boolean
531         specifying whether an object was created.
532         """
533         # The get() needs to be targeted at the write database in order
534         # to avoid potential transaction consistency problems.
535         self._for_write = True
536         try:
537             return self.get(**kwargs), False
538         except self.model.DoesNotExist:
539             params = self._extract_model_params(defaults, **kwargs)
540             return self._create_object_from_params(kwargs, params)
541 
542     def update_or_create(self, defaults=None, **kwargs):
543         """
544         Look up an object with the given kwargs, updating one with defaults
545         if it exists, otherwise create a new one.
546         Return a tuple (object, created), where created is a boolean
547         specifying whether an object was created.
548         """
549         defaults = defaults or {}
550         self._for_write = True
551         with transaction.atomic(using=self.db):
552             try:
553                 obj = self.select_for_update().get(**kwargs)
554             except self.model.DoesNotExist:
555                 params = self._extract_model_params(defaults, **kwargs)
556                 # Lock the row so that a concurrent update is blocked until
557                 # after update_or_create() has performed its save.
558                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
559                 if created:
560                     return obj, created
561             for k, v in defaults.items():
562                 setattr(obj, k, v() if callable(v) else v)
563             obj.save(using=self.db)
564         return obj, False
565 
566     def _create_object_from_params(self, lookup, params, lock=False):
567         """
568         Try to create an object using passed params. Used by get_or_create()
569         and update_or_create().
570         """
571         try:
572             with transaction.atomic(using=self.db):
573                 params = {k: v() if callable(v) else v for k, v in params.items()}
574                 obj = self.create(**params)
575             return obj, True
576         except IntegrityError as e:
577             try:
578                 qs = self.select_for_update() if lock else self
579                 return qs.get(**lookup), False
580             except self.model.DoesNotExist:
581                 pass
582             raise e
583 
584     def _extract_model_params(self, defaults, **kwargs):
585         """
586         Prepare `params` for creating a model instance based on the given
587         kwargs; for use by get_or_create() and update_or_create().
588         """
589         defaults = defaults or {}
590         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
591         params.update(defaults)
592         property_names = self.model._meta._property_names
593         invalid_params = []
594         for param in params:
595             try:
596                 self.model._meta.get_field(param)
597             except exceptions.FieldDoesNotExist:
598                 # It's okay to use a model's property if it has a setter.
599                 if not (param in property_names and getattr(self.model, param).fset):
600                     invalid_params.append(param)
601         if invalid_params:
602             raise exceptions.FieldError(
603                 "Invalid field name(s) for model %s: '%s'." % (
604                     self.model._meta.object_name,
605                     "', '".join(sorted(invalid_params)),
606                 ))
607         return params
608 
609     def _earliest(self, *fields):
610         """
611         Return the earliest object according to fields (if given) or by the
612         model's Meta.get_latest_by.
613         """
614         if fields:
615             order_by = fields
616         else:
617             order_by = getattr(self.model._meta, 'get_latest_by')
618             if order_by and not isinstance(order_by, (tuple, list)):
619                 order_by = (order_by,)
620         if order_by is None:
621             raise ValueError(
622                 "earliest() and latest() require either fields as positional "
623                 "arguments or 'get_latest_by' in the model's Meta."
624             )
625 
626         assert self.query.can_filter(), \
627             "Cannot change a query once a slice has been taken."
628         obj = self._chain()
629         obj.query.set_limits(high=1)
630         obj.query.clear_ordering(force_empty=True)
631         obj.query.add_ordering(*order_by)
632         return obj.get()
633 
634     def earliest(self, *fields):
635         return self._earliest(*fields)
636 
637     def latest(self, *fields):
638         return self.reverse()._earliest(*fields)
639 
640     def first(self):
641         """Return the first object of a query or None if no match is found."""
642         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
643             return obj
644 
645     def last(self):
646         """Return the last object of a query or None if no match is found."""
647         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
648             return obj
649 
650     def in_bulk(self, id_list=None, *, field_name='pk'):
651         """
652         Return a dictionary mapping each of the given IDs to the object with
653         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
654         """
655         assert self.query.can_filter(), \
656             "Cannot use 'limit' or 'offset' with in_bulk"
657         if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:
658             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
659         if id_list is not None:
660             if not id_list:
661                 return {}
662             filter_key = '{}__in'.format(field_name)
663             batch_size = connections[self.db].features.max_query_params
664             id_list = tuple(id_list)
665             # If the database has a limit on the number of query parameters
666             # (e.g. SQLite), retrieve objects in batches if necessary.
667             if batch_size and batch_size < len(id_list):
668                 qs = ()
669                 for offset in range(0, len(id_list), batch_size):
670                     batch = id_list[offset:offset + batch_size]
671                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
672             else:
673                 qs = self.filter(**{filter_key: id_list}).order_by()
674         else:
675             qs = self._chain()
676         return {getattr(obj, field_name): obj for obj in qs}
677 
678     def delete(self):
679         """Delete the records in the current QuerySet."""
680         assert self.query.can_filter(), \
681             "Cannot use 'limit' or 'offset' with delete."
682 
683         if self._fields is not None:
684             raise TypeError("Cannot call delete() after .values() or .values_list()")
685 
686         del_query = self._chain()
687 
688         # The delete is actually 2 queries - one to find related objects,
689         # and one to delete. Make sure that the discovery of related
690         # objects is performed on the same database as the deletion.
691         del_query._for_write = True
692 
693         # Disable non-supported fields.
694         del_query.query.select_for_update = False
695         del_query.query.select_related = False
696         del_query.query.clear_ordering(force_empty=True)
697 
698         collector = Collector(using=del_query.db)
699         collector.collect(del_query)
700         deleted, _rows_count = collector.delete()
701 
702         # Clear the result cache, in case this QuerySet gets reused.
703         self._result_cache = None
704         return deleted, _rows_count
705 
706     delete.alters_data = True
707     delete.queryset_only = True
708 
709     def _raw_delete(self, using):
710         """
711         Delete objects found from the given queryset in single direct SQL
712         query. No signals are sent and there is no protection for cascades.
713         """
714         return sql.DeleteQuery(self.model).delete_qs(self, using)
715     _raw_delete.alters_data = True
716 
717     def update(self, **kwargs):
718         """
719         Update all elements in the current QuerySet, setting all the given
720         fields to the appropriate values.
721         """
722         assert self.query.can_filter(), \
723             "Cannot update a query once a slice has been taken."
724         self._for_write = True
725         query = self.query.chain(sql.UpdateQuery)
726         query.add_update_values(kwargs)
727         # Clear any annotations so that they won't be present in subqueries.
728         query.annotations = {}
729         with transaction.mark_for_rollback_on_error(using=self.db):
730             rows = query.get_compiler(self.db).execute_sql(CURSOR)
731         self._result_cache = None
732         return rows
733     update.alters_data = True
734 
735     def _update(self, values):
736         """
737         A version of update() that accepts field objects instead of field names.
738         Used primarily for model saving and not intended for use by general
739         code (it requires too much poking around at model internals to be
740         useful at that level).
741         """
742         assert self.query.can_filter(), \
743             "Cannot update a query once a slice has been taken."
744         query = self.query.chain(sql.UpdateQuery)
745         query.add_update_fields(values)
746         # Clear any annotations so that they won't be present in subqueries.
747         query.annotations = {}
748         self._result_cache = None
749         return query.get_compiler(self.db).execute_sql(CURSOR)
750     _update.alters_data = True
751     _update.queryset_only = False
752 
753     def exists(self):
754         if self._result_cache is None:
755             return self.query.has_results(using=self.db)
756         return bool(self._result_cache)
757 
758     def _prefetch_related_objects(self):
759         # This method can only be called once the result cache has been filled.
760         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
761         self._prefetch_done = True
762 
763     def explain(self, *, format=None, **options):
764         return self.query.explain(using=self.db, format=format, **options)
765 
766     ##################################################
767     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
768     ##################################################
769 
770     def raw(self, raw_query, params=None, translations=None, using=None):
771         if using is None:
772             using = self.db
773         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
774         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
775         return qs
776 
777     def _values(self, *fields, **expressions):
778         clone = self._chain()
779         if expressions:
780             clone = clone.annotate(**expressions)
781         clone._fields = fields
782         clone.query.set_values(fields)
783         return clone
784 
785     def values(self, *fields, **expressions):
786         fields += tuple(expressions)
787         clone = self._values(*fields, **expressions)
788         clone._iterable_class = ValuesIterable
789         return clone
790 
791     def values_list(self, *fields, flat=False, named=False):
792         if flat and named:
793             raise TypeError("'flat' and 'named' can't be used together.")
794         if flat and len(fields) > 1:
795             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
796 
797         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
798         _fields = []
799         expressions = {}
800         counter = 1
801         for field in fields:
802             if hasattr(field, 'resolve_expression'):
803                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
804                 while True:
805                     field_id = field_id_prefix + str(counter)
806                     counter += 1
807                     if field_id not in field_names:
808                         break
809                 expressions[field_id] = field
810                 _fields.append(field_id)
811             else:
812                 _fields.append(field)
813 
814         clone = self._values(*_fields, **expressions)
815         clone._iterable_class = (
816             NamedValuesListIterable if named
817             else FlatValuesListIterable if flat
818             else ValuesListIterable
819         )
820         return clone
821 
822     def dates(self, field_name, kind, order='ASC'):
823         """
824         Return a list of date objects representing all available dates for
825         the given field_name, scoped to 'kind'.
826         """
827         assert kind in ('year', 'month', 'week', 'day'), \
828             "'kind' must be one of 'year', 'month', 'week', or 'day'."
829         assert order in ('ASC', 'DESC'), \
830             "'order' must be either 'ASC' or 'DESC'."
831         return self.annotate(
832             datefield=Trunc(field_name, kind, output_field=DateField()),
833             plain_field=F(field_name)
834         ).values_list(
835             'datefield', flat=True
836         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
837 
838     def datetimes(self, field_name, kind, order='ASC', tzinfo=None):
839         """
840         Return a list of datetime objects representing all available
841         datetimes for the given field_name, scoped to 'kind'.
842         """
843         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
844             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
845         assert order in ('ASC', 'DESC'), \
846             "'order' must be either 'ASC' or 'DESC'."
847         if settings.USE_TZ:
848             if tzinfo is None:
849                 tzinfo = timezone.get_current_timezone()
850         else:
851             tzinfo = None
852         return self.annotate(
853             datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),
854             plain_field=F(field_name)
855         ).values_list(
856             'datetimefield', flat=True
857         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
858 
859     def none(self):
860         """Return an empty QuerySet."""
861         clone = self._chain()
862         clone.query.set_empty()
863         return clone
864 
865     ##################################################################
866     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
867     ##################################################################
868 
869     def all(self):
870         """
871         Return a new QuerySet that is a copy of the current one. This allows a
872         QuerySet to proxy for a model manager in some cases.
873         """
874         return self._chain()
875 
876     def filter(self, *args, **kwargs):
877         """
878         Return a new QuerySet instance with the args ANDed to the existing
879         set.
880         """
881         return self._filter_or_exclude(False, *args, **kwargs)
882 
883     def exclude(self, *args, **kwargs):
884         """
885         Return a new QuerySet instance with NOT (args) ANDed to the existing
886         set.
887         """
888         return self._filter_or_exclude(True, *args, **kwargs)
889 
890     def _filter_or_exclude(self, negate, *args, **kwargs):
891         if args or kwargs:
892             assert self.query.can_filter(), \
893                 "Cannot filter a query once a slice has been taken."
894 
895         clone = self._chain()
896         if negate:
897             clone.query.add_q(~Q(*args, **kwargs))
898         else:
899             clone.query.add_q(Q(*args, **kwargs))
900         return clone
901 
902     def complex_filter(self, filter_obj):
903         """
904         Return a new QuerySet instance with filter_obj added to the filters.
905 
906         filter_obj can be a Q object or a dictionary of keyword lookup
907         arguments.
908 
909         This exists to support framework features such as 'limit_choices_to',
910         and usually it will be more natural to use other methods.
911         """
912         if isinstance(filter_obj, Q):
913             clone = self._chain()
914             clone.query.add_q(filter_obj)
915             return clone
916         else:
917             return self._filter_or_exclude(None, **filter_obj)
918 
919     def _combinator_query(self, combinator, *other_qs, all=False):
920         # Clone the query to inherit the select list and everything
921         clone = self._chain()
922         # Clear limits and ordering so they can be reapplied
923         clone.query.clear_ordering(True)
924         clone.query.clear_limits()
925         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
926         clone.query.combinator = combinator
927         clone.query.combinator_all = all
928         return clone
929 
930     def union(self, *other_qs, all=False):
931         # If the query is an EmptyQuerySet, combine all nonempty querysets.
932         if isinstance(self, EmptyQuerySet):
933             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
934             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
935         return self._combinator_query('union', *other_qs, all=all)
936 
937     def intersection(self, *other_qs):
938         # If any query is an EmptyQuerySet, return it.
939         if isinstance(self, EmptyQuerySet):
940             return self
941         for other in other_qs:
942             if isinstance(other, EmptyQuerySet):
943                 return other
944         return self._combinator_query('intersection', *other_qs)
945 
946     def difference(self, *other_qs):
947         # If the query is an EmptyQuerySet, return it.
948         if isinstance(self, EmptyQuerySet):
949             return self
950         return self._combinator_query('difference', *other_qs)
951 
952     def select_for_update(self, nowait=False, skip_locked=False, of=()):
953         """
954         Return a new QuerySet instance that will select objects with a
955         FOR UPDATE lock.
956         """
957         if nowait and skip_locked:
958             raise ValueError('The nowait option cannot be used with skip_locked.')
959         obj = self._chain()
960         obj._for_write = True
961         obj.query.select_for_update = True
962         obj.query.select_for_update_nowait = nowait
963         obj.query.select_for_update_skip_locked = skip_locked
964         obj.query.select_for_update_of = of
965         return obj
966 
967     def select_related(self, *fields):
968         """
969         Return a new QuerySet instance that will select related objects.
970 
971         If fields are specified, they must be ForeignKey fields and only those
972         related objects are included in the selection.
973 
974         If select_related(None) is called, clear the list.
975         """
976 
977         if self._fields is not None:
978             raise TypeError("Cannot call select_related() after .values() or .values_list()")
979 
980         obj = self._chain()
981         if fields == (None,):
982             obj.query.select_related = False
983         elif fields:
984             obj.query.add_select_related(fields)
985         else:
986             obj.query.select_related = True
987         return obj
988 
989     def prefetch_related(self, *lookups):
990         """
991         Return a new QuerySet instance that will prefetch the specified
992         Many-To-One and Many-To-Many related objects when the QuerySet is
993         evaluated.
994 
995         When prefetch_related() is called more than once, append to the list of
996         prefetch lookups. If prefetch_related(None) is called, clear the list.
997         """
998         clone = self._chain()
999         if lookups == (None,):
1000             clone._prefetch_related_lookups = ()
1001         else:
1002             for lookup in lookups:
1003                 if isinstance(lookup, Prefetch):
1004                     lookup = lookup.prefetch_to
1005                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1006                 if lookup in self.query._filtered_relations:
1007                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1008             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1009         return clone
1010 
1011     def annotate(self, *args, **kwargs):
1012         """
1013         Return a query set in which the returned objects have been annotated
1014         with extra data or aggregations.
1015         """
1016         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1017         annotations = {}
1018         for arg in args:
1019             # The default_alias property may raise a TypeError.
1020             try:
1021                 if arg.default_alias in kwargs:
1022                     raise ValueError("The named annotation '%s' conflicts with the "
1023                                      "default name for another annotation."
1024                                      % arg.default_alias)
1025             except TypeError:
1026                 raise TypeError("Complex annotations require an alias")
1027             annotations[arg.default_alias] = arg
1028         annotations.update(kwargs)
1029 
1030         clone = self._chain()
1031         names = self._fields
1032         if names is None:
1033             names = set(chain.from_iterable(
1034                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1035                 for field in self.model._meta.get_fields()
1036             ))
1037 
1038         for alias, annotation in annotations.items():
1039             if alias in names:
1040                 raise ValueError("The annotation '%s' conflicts with a field on "
1041                                  "the model." % alias)
1042             if isinstance(annotation, FilteredRelation):
1043                 clone.query.add_filtered_relation(annotation, alias)
1044             else:
1045                 clone.query.add_annotation(annotation, alias, is_summary=False)
1046 
1047         for alias, annotation in clone.query.annotations.items():
1048             if alias in annotations and annotation.contains_aggregate:
1049                 if clone._fields is None:
1050                     clone.query.group_by = True
1051                 else:
1052                     clone.query.set_group_by()
1053                 break
1054 
1055         return clone
1056 
1057     def order_by(self, *field_names):
1058         """Return a new QuerySet instance with the ordering changed."""
1059         assert self.query.can_filter(), \
1060             "Cannot reorder a query once a slice has been taken."
1061         obj = self._chain()
1062         obj.query.clear_ordering(force_empty=False)
1063         obj.query.add_ordering(*field_names)
1064         return obj
1065 
1066     def distinct(self, *field_names):
1067         """
1068         Return a new QuerySet instance that will select only distinct results.
1069         """
1070         assert self.query.can_filter(), \
1071             "Cannot create distinct fields once a slice has been taken."
1072         obj = self._chain()
1073         obj.query.add_distinct_fields(*field_names)
1074         return obj
1075 
1076     def extra(self, select=None, where=None, params=None, tables=None,
1077               order_by=None, select_params=None):
1078         """Add extra SQL fragments to the query."""
1079         assert self.query.can_filter(), \
1080             "Cannot change a query once a slice has been taken"
1081         clone = self._chain()
1082         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1083         return clone
1084 
1085     def reverse(self):
1086         """Reverse the ordering of the QuerySet."""
1087         if not self.query.can_filter():
1088             raise TypeError('Cannot reverse a query once a slice has been taken.')
1089         clone = self._chain()
1090         clone.query.standard_ordering = not clone.query.standard_ordering
1091         return clone
1092 
1093     def defer(self, *fields):
1094         """
1095         Defer the loading of data for certain fields until they are accessed.
1096         Add the set of deferred fields to any existing set of deferred fields.
1097         The only exception to this is if None is passed in as the only
1098         parameter, in which case removal all deferrals.
1099         """
1100         if self._fields is not None:
1101             raise TypeError("Cannot call defer() after .values() or .values_list()")
1102         clone = self._chain()
1103         if fields == (None,):
1104             clone.query.clear_deferred_loading()
1105         else:
1106             clone.query.add_deferred_loading(fields)
1107         return clone
1108 
1109     def only(self, *fields):
1110         """
1111         Essentially, the opposite of defer(). Only the fields passed into this
1112         method and that are not already specified as deferred are loaded
1113         immediately when the queryset is evaluated.
1114         """
1115         if self._fields is not None:
1116             raise TypeError("Cannot call only() after .values() or .values_list()")
1117         if fields == (None,):
1118             # Can only pass None to defer(), not only(), as the rest option.
1119             # That won't stop people trying to do this, so let's be explicit.
1120             raise TypeError("Cannot pass None as an argument to only().")
1121         for field in fields:
1122             field = field.split(LOOKUP_SEP, 1)[0]
1123             if field in self.query._filtered_relations:
1124                 raise ValueError('only() is not supported with FilteredRelation.')
1125         clone = self._chain()
1126         clone.query.add_immediate_loading(fields)
1127         return clone
1128 
1129     def using(self, alias):
1130         """Select which database this QuerySet should execute against."""
1131         clone = self._chain()
1132         clone._db = alias
1133         return clone
1134 
1135     ###################################
1136     # PUBLIC INTROSPECTION ATTRIBUTES #
1137     ###################################
1138 
1139     @property
1140     def ordered(self):
1141         """
1142         Return True if the QuerySet is ordered -- i.e. has an order_by()
1143         clause or a default ordering on the model (or is empty).
1144         """
1145         if isinstance(self, EmptyQuerySet):
1146             return True
1147         if self.query.extra_order_by or self.query.order_by:
1148             return True
1149         elif self.query.default_ordering and self.query.get_meta().ordering:
1150             return True
1151         else:
1152             return False
1153 
1154     @property
1155     def db(self):
1156         """Return the database used if this query is executed now."""
1157         if self._for_write:
1158             return self._db or router.db_for_write(self.model, **self._hints)
1159         return self._db or router.db_for_read(self.model, **self._hints)
1160 
1161     ###################
1162     # PRIVATE METHODS #
1163     ###################
1164 
1165     def _insert(self, objs, fields, return_id=False, raw=False, using=None, ignore_conflicts=False):
1166         """
1167         Insert a new record for the given model. This provides an interface to
1168         the InsertQuery class and is how Model.save() is implemented.
1169         """
1170         self._for_write = True
1171         if using is None:
1172             using = self.db
1173         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1174         query.insert_values(fields, objs, raw=raw)
1175         return query.get_compiler(using=using).execute_sql(return_id)
1176     _insert.alters_data = True
1177     _insert.queryset_only = False
1178 
1179     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1180         """
1181         Helper method for bulk_create() to insert objs one batch at a time.
1182         """
1183         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1184             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1185         ops = connections[self.db].ops
1186         batch_size = (batch_size or max(ops.bulk_batch_size(fields, objs), 1))
1187         inserted_ids = []
1188         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1189         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1190             if bulk_return and not ignore_conflicts:
1191                 inserted_id = self._insert(
1192                     item, fields=fields, using=self.db, return_id=True,
1193                     ignore_conflicts=ignore_conflicts,
1194                 )
1195                 if isinstance(inserted_id, list):
1196                     inserted_ids.extend(inserted_id)
1197                 else:
1198                     inserted_ids.append(inserted_id)
1199             else:
1200                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1201         return inserted_ids
1202 
1203     def _chain(self, **kwargs):
1204         """
1205         Return a copy of the current QuerySet that's ready for another
1206         operation.
1207         """
1208         obj = self._clone()
1209         if obj._sticky_filter:
1210             obj.query.filter_is_sticky = True
1211             obj._sticky_filter = False
1212         obj.__dict__.update(kwargs)
1213         return obj
1214 
1215     def _clone(self):
1216         """
1217         Return a copy of the current QuerySet. A lightweight alternative
1218         to deepcopy().
1219         """
1220         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1221         c._sticky_filter = self._sticky_filter
1222         c._for_write = self._for_write
1223         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1224         c._known_related_objects = self._known_related_objects
1225         c._iterable_class = self._iterable_class
1226         c._fields = self._fields
1227         return c
1228 
1229     def _fetch_all(self):
1230         if self._result_cache is None:
1231             self._result_cache = list(self._iterable_class(self))
1232         if self._prefetch_related_lookups and not self._prefetch_done:
1233             self._prefetch_related_objects()
1234 
1235     def _next_is_sticky(self):
1236         """
1237         Indicate that the next filter call and the one following that should
1238         be treated as a single filter. This is only important when it comes to
1239         determining when to reuse tables for many-to-many filters. Required so
1240         that we can filter naturally on the results of related managers.
1241 
1242         This doesn't return a clone of the current QuerySet (it returns
1243         "self"). The method is only used internally and should be immediately
1244         followed by a filter() that does create a clone.
1245         """
1246         self._sticky_filter = True
1247         return self
1248 
1249     def _merge_sanity_check(self, other):
1250         """Check that two QuerySet classes may be merged."""
1251         if self._fields is not None and (
1252                 set(self.query.values_select) != set(other.query.values_select) or
1253                 set(self.query.extra_select) != set(other.query.extra_select) or
1254                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1255             raise TypeError(
1256                 "Merging '%s' classes must involve the same values in each case."
1257                 % self.__class__.__name__
1258             )
1259 
1260     def _merge_known_related_objects(self, other):
1261         """
1262         Keep track of all known related objects from either QuerySet instance.
1263         """
1264         for field, objects in other._known_related_objects.items():
1265             self._known_related_objects.setdefault(field, {}).update(objects)
1266 
1267     def resolve_expression(self, *args, **kwargs):
1268         if self._fields and len(self._fields) > 1:
1269             # values() queryset can only be used as nested queries
1270             # if they are set up to select only a single field.
1271             raise TypeError('Cannot use multi-field values as a filter value.')
1272         query = self.query.resolve_expression(*args, **kwargs)
1273         query._db = self._db
1274         return query
1275     resolve_expression.queryset_only = True
1276 
1277     def _add_hints(self, **hints):
1278         """
1279         Update hinting information for use by routers. Add new key/values or
1280         overwrite existing key/values.
1281         """
1282         self._hints.update(hints)
1283 
1284     def _has_filters(self):
1285         """
1286         Check if this QuerySet has any filtering going on. This isn't
1287         equivalent with checking if all objects are present in results, for
1288         example, qs[1:]._has_filters() -> False.
1289         """
1290         return self.query.has_filters()
1291 
1292     @staticmethod
1293     def _validate_values_are_expressions(values, method_name):
1294         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1295         if invalid_args:
1296             raise TypeError(
1297                 'QuerySet.%s() received non-expression(s): %s.' % (
1298                     method_name,
1299                     ', '.join(invalid_args),
1300                 )
1301             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 10:05:40.803 | INFO     | Search completed. Bug locations: [<file>django/db/models/expressions.py</file>
<class>Window</class>
<method>as_sql</method>
<code>
1238     def as_sql(self, compiler, connection, template=None):
1239         connection.ops.check_expression_support(self)
1240         expr_sql, params = compiler.compile(self.source_expression)
1241         window_sql, window_params = [], []
1242 
1243         if self.partition_by is not None:
1244             sql_expr, sql_params = self.partition_by.as_sql(
1245                 compiler=compiler, connection=connection,
1246                 template='PARTITION BY %(expressions)s',
1247             )
1248             window_sql.extend(sql_expr)
1249             window_params.extend(sql_params)
1250 
1251         if self.order_by is not None:
1252             window_sql.append(' ORDER BY ')
1253             order_sql, order_params = compiler.compile(self.order_by)
1254             window_sql.extend(order_sql)
1255             window_params.extend(order_params)
1256 
1257         if self.frame:
1258             frame_sql, frame_params = compiler.compile(self.frame)
1259             window_sql.append(' ' + frame_sql)
1260             window_params.extend(frame_params)
1261 
1262         params.extend(window_params)
1263         template = template or self.template
1264 
1265         return template % {
1266             'expression': expr_sql,
1267             'window': ''.join(window_sql).strip()
1268         }, params

</code><intended_behavior>The `as_sql` method inside the `Window` class should include a check for the SQLite version if the database connection is to SQLite. Before generating the SQL, it should verify if the version supports window functions (SQLite 3.25.0 or higher). If the version does not support window functions, the method should raise a clear, descriptive error indicating that the operation cannot be performed due to the lack of support in the SQLite version being used. This error should suggest the user upgrade their SQLite version if possible. This check can preserve existing functionality by only altering behavior in cases where the operation would lead to an SQL syntax error due to version incompatibilities.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Window</class>
<method>None</method>
<code>
1193 class Window(Expression):
1194     template = '%(expression)s OVER (%(window)s)'
1195     # Although the main expression may either be an aggregate or an
1196     # expression with an aggregate function, the GROUP BY that will
1197     # be introduced in the query as a result is not desired.
1198     contains_aggregate = False
1199     contains_over_clause = True
1200     filterable = False
1201 
1202     def __init__(self, expression, partition_by=None, order_by=None, frame=None, output_field=None):
1203         self.partition_by = partition_by
1204         self.order_by = order_by
1205         self.frame = frame
1206 
1207         if not getattr(expression, 'window_compatible', False):
1208             raise ValueError(
1209                 "Expression '%s' isn't compatible with OVER clauses." %
1210                 expression.__class__.__name__
1211             )
1212 
1213         if self.partition_by is not None:
1214             if not isinstance(self.partition_by, (tuple, list)):
1215                 self.partition_by = (self.partition_by,)
1216             self.partition_by = ExpressionList(*self.partition_by)
1217 
1218         if self.order_by is not None:
1219             if isinstance(self.order_by, (list, tuple)):
1220                 self.order_by = ExpressionList(*self.order_by)
1221             elif not isinstance(self.order_by, BaseExpression):
1222                 raise ValueError(
1223                     'order_by must be either an Expression or a sequence of '
1224                     'expressions.'
1225                 )
1226         super().__init__(output_field=output_field)
1227         self.source_expression = self._parse_expressions(expression)[0]
1228 
1229     def _resolve_output_field(self):
1230         return self.source_expression.output_field
1231 
1232     def get_source_expressions(self):
1233         return [self.source_expression, self.partition_by, self.order_by, self.frame]
1234 
1235     def set_source_expressions(self, exprs):
1236         self.source_expression, self.partition_by, self.order_by, self.frame = exprs
1237 
1238     def as_sql(self, compiler, connection, template=None):
1239         connection.ops.check_expression_support(self)
1240         expr_sql, params = compiler.compile(self.source_expression)
1241         window_sql, window_params = [], []
1242 
1243         if self.partition_by is not None:
1244             sql_expr, sql_params = self.partition_by.as_sql(
1245                 compiler=compiler, connection=connection,
1246                 template='PARTITION BY %(expressions)s',
1247             )
1248             window_sql.extend(sql_expr)
1249             window_params.extend(sql_params)
1250 
1251         if self.order_by is not None:
1252             window_sql.append(' ORDER BY ')
1253             order_sql, order_params = compiler.compile(self.order_by)
1254             window_sql.extend(order_sql)
1255             window_params.extend(order_params)
1256 
1257         if self.frame:
1258             frame_sql, frame_params = compiler.compile(self.frame)
1259             window_sql.append(' ' + frame_sql)
1260             window_params.extend(frame_params)
1261 
1262         params.extend(window_params)
1263         template = template or self.template
1264 
1265         return template % {
1266             'expression': expr_sql,
1267             'window': ''.join(window_sql).strip()
1268         }, params
1269 
1270     def __str__(self):
1271         return '{} OVER ({}{}{})'.format(
1272             str(self.source_expression),
1273             'PARTITION BY ' + str(self.partition_by) if self.partition_by else '',
1274             'ORDER BY ' + str(self.order_by) if self.order_by else '',
1275             str(self.frame or ''),
1276         )
1277 
1278     def __repr__(self):
1279         return '<%s: %s>' % (self.__class__.__name__, self)
1280 
1281     def get_group_by_cols(self):
1282         return []

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>annotate</method>
<code>
1011     def annotate(self, *args, **kwargs):
1012         """
1013         Return a query set in which the returned objects have been annotated
1014         with extra data or aggregations.
1015         """
1016         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1017         annotations = {}
1018         for arg in args:
1019             # The default_alias property may raise a TypeError.
1020             try:
1021                 if arg.default_alias in kwargs:
1022                     raise ValueError("The named annotation '%s' conflicts with the "
1023                                      "default name for another annotation."
1024                                      % arg.default_alias)
1025             except TypeError:
1026                 raise TypeError("Complex annotations require an alias")
1027             annotations[arg.default_alias] = arg
1028         annotations.update(kwargs)
1029 
1030         clone = self._chain()
1031         names = self._fields
1032         if names is None:
1033             names = set(chain.from_iterable(
1034                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1035                 for field in self.model._meta.get_fields()
1036             ))
1037 
1038         for alias, annotation in annotations.items():
1039             if alias in names:
1040                 raise ValueError("The annotation '%s' conflicts with a field on "
1041                                  "the model." % alias)
1042             if isinstance(annotation, FilteredRelation):
1043                 clone.query.add_filtered_relation(annotation, alias)
1044             else:
1045                 clone.query.add_annotation(annotation, alias, is_summary=False)
1046 
1047         for alias, annotation in clone.query.annotations.items():
1048             if alias in annotations and annotation.contains_aggregate:
1049                 if clone._fields is None:
1050                     clone.query.group_by = True
1051                 else:
1052                     clone.query.set_group_by()
1053                 break
1054 
1055         return clone

</code><intended_behavior>In the `annotate` method, when processing annotations that involve window functions, add a preliminary check to confirm if the underlying database engine supports those window functions. This could involve extending the `_validate_values_are_expressions` method (or where appropriate within this workflow) to include checks for database feature support, particularly focusing on window functions for SQLite. In case the database does not support the feature, a descriptive error should be raised early in the process, advising on the lack of support and possible resolutions (e.g., database upgrade). This approach maintains the method's core functionality while improving error handling and messaging for unsupported features.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
185 class QuerySet:
186     """Represent a lazy database lookup for a set of objects."""
187 
188     def __init__(self, model=None, query=None, using=None, hints=None):
189         self.model = model
190         self._db = using
191         self._hints = hints or {}
192         self.query = query or sql.Query(self.model)
193         self._result_cache = None
194         self._sticky_filter = False
195         self._for_write = False
196         self._prefetch_related_lookups = ()
197         self._prefetch_done = False
198         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
199         self._iterable_class = ModelIterable
200         self._fields = None
201 
202     def as_manager(cls):
203         # Address the circular dependency between `Queryset` and `Manager`.
204         from django.db.models.manager import Manager
205         manager = Manager.from_queryset(cls)()
206         manager._built_with_as_manager = True
207         return manager
208     as_manager.queryset_only = True
209     as_manager = classmethod(as_manager)
210 
211     ########################
212     # PYTHON MAGIC METHODS #
213     ########################
214 
215     def __deepcopy__(self, memo):
216         """Don't populate the QuerySet's cache."""
217         obj = self.__class__()
218         for k, v in self.__dict__.items():
219             if k == '_result_cache':
220                 obj.__dict__[k] = None
221             else:
222                 obj.__dict__[k] = copy.deepcopy(v, memo)
223         return obj
224 
225     def __getstate__(self):
226         # Force the cache to be fully populated.
227         self._fetch_all()
228         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}
229 
230     def __setstate__(self, state):
231         msg = None
232         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
233         if pickled_version:
234             current_version = get_version()
235             if current_version != pickled_version:
236                 msg = (
237                     "Pickled queryset instance's Django version %s does not "
238                     "match the current version %s." % (pickled_version, current_version)
239                 )
240         else:
241             msg = "Pickled queryset instance's Django version is not specified."
242 
243         if msg:
244             warnings.warn(msg, RuntimeWarning, stacklevel=2)
245 
246         self.__dict__.update(state)
247 
248     def __repr__(self):
249         data = list(self[:REPR_OUTPUT_SIZE + 1])
250         if len(data) > REPR_OUTPUT_SIZE:
251             data[-1] = "…(remaining elements truncated)…"
252         return '<%s %r>' % (self.__class__.__name__, data)
253 
254     def __len__(self):
255         self._fetch_all()
256         return len(self._result_cache)
257 
258     def __iter__(self):
259         """
260         The queryset iterator protocol uses three nested iterators in the
261         default case:
262             1. sql.compiler.execute_sql()
263                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
264                  using cursor.fetchmany(). This part is responsible for
265                  doing some column masking, and returning the rows in chunks.
266             2. sql.compiler.results_iter()
267                - Returns one row at time. At this point the rows are still just
268                  tuples. In some cases the return values are converted to
269                  Python values at this location.
270             3. self.iterator()
271                - Responsible for turning the rows into model objects.
272         """
273         self._fetch_all()
274         return iter(self._result_cache)
275 
276     def __bool__(self):
277         self._fetch_all()
278         return bool(self._result_cache)
279 
280     def __getitem__(self, k):
281         """Retrieve an item or slice from the set of results."""
282         if not isinstance(k, (int, slice)):
283             raise TypeError
284         assert ((not isinstance(k, slice) and (k >= 0)) or
285                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
286                  (k.stop is None or k.stop >= 0))), \
287             "Negative indexing is not supported."
288 
289         if self._result_cache is not None:
290             return self._result_cache[k]
291 
292         if isinstance(k, slice):
293             qs = self._chain()
294             if k.start is not None:
295                 start = int(k.start)
296             else:
297                 start = None
298             if k.stop is not None:
299                 stop = int(k.stop)
300             else:
301                 stop = None
302             qs.query.set_limits(start, stop)
303             return list(qs)[::k.step] if k.step else qs
304 
305         qs = self._chain()
306         qs.query.set_limits(k, k + 1)
307         qs._fetch_all()
308         return qs._result_cache[0]
309 
310     def __and__(self, other):
311         self._merge_sanity_check(other)
312         if isinstance(other, EmptyQuerySet):
313             return other
314         if isinstance(self, EmptyQuerySet):
315             return self
316         combined = self._chain()
317         combined._merge_known_related_objects(other)
318         combined.query.combine(other.query, sql.AND)
319         return combined
320 
321     def __or__(self, other):
322         self._merge_sanity_check(other)
323         if isinstance(self, EmptyQuerySet):
324             return other
325         if isinstance(other, EmptyQuerySet):
326             return self
327         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
328         combined = query._chain()
329         combined._merge_known_related_objects(other)
330         if not other.query.can_filter():
331             other = other.model._base_manager.filter(pk__in=other.values('pk'))
332         combined.query.combine(other.query, sql.OR)
333         return combined
334 
335     ####################################
336     # METHODS THAT DO DATABASE QUERIES #
337     ####################################
338 
339     def _iterator(self, use_chunked_fetch, chunk_size):
340         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
341 
342     def iterator(self, chunk_size=2000):
343         """
344         An iterator over the results from applying this QuerySet to the
345         database.
346         """
347         if chunk_size <= 0:
348             raise ValueError('Chunk size must be strictly positive.')
349         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
350         return self._iterator(use_chunked_fetch, chunk_size)
351 
352     def aggregate(self, *args, **kwargs):
353         """
354         Return a dictionary containing the calculations (aggregation)
355         over the current queryset.
356 
357         If args is present the expression is passed as a kwarg using
358         the Aggregate object's default alias.
359         """
360         if self.query.distinct_fields:
361             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
362         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
363         for arg in args:
364             # The default_alias property raises TypeError if default_alias
365             # can't be set automatically or AttributeError if it isn't an
366             # attribute.
367             try:
368                 arg.default_alias
369             except (AttributeError, TypeError):
370                 raise TypeError("Complex aggregates require an alias")
371             kwargs[arg.default_alias] = arg
372 
373         query = self.query.chain()
374         for (alias, aggregate_expr) in kwargs.items():
375             query.add_annotation(aggregate_expr, alias, is_summary=True)
376             if not query.annotations[alias].contains_aggregate:
377                 raise TypeError("%s is not an aggregate expression" % alias)
378         return query.get_aggregation(self.db, kwargs)
379 
380     def count(self):
381         """
382         Perform a SELECT COUNT() and return the number of records as an
383         integer.
384 
385         If the QuerySet is already fully cached, return the length of the
386         cached results set to avoid multiple SELECT COUNT(*) calls.
387         """
388         if self._result_cache is not None:
389             return len(self._result_cache)
390 
391         return self.query.get_count(using=self.db)
392 
393     def get(self, *args, **kwargs):
394         """
395         Perform the query and return a single object matching the given
396         keyword arguments.
397         """
398         clone = self.filter(*args, **kwargs)
399         if self.query.can_filter() and not self.query.distinct_fields:
400             clone = clone.order_by()
401         num = len(clone)
402         if num == 1:
403             return clone._result_cache[0]
404         if not num:
405             raise self.model.DoesNotExist(
406                 "%s matching query does not exist." %
407                 self.model._meta.object_name
408             )
409         raise self.model.MultipleObjectsReturned(
410             "get() returned more than one %s -- it returned %s!" %
411             (self.model._meta.object_name, num)
412         )
413 
414     def create(self, **kwargs):
415         """
416         Create a new object with the given kwargs, saving it to the database
417         and returning the created object.
418         """
419         obj = self.model(**kwargs)
420         self._for_write = True
421         obj.save(force_insert=True, using=self.db)
422         return obj
423 
424     def _populate_pk_values(self, objs):
425         for obj in objs:
426             if obj.pk is None:
427                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
428 
429     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
430         """
431         Insert each of the instances into the database. Do *not* call
432         save() on each of the instances, do not send any pre/post_save
433         signals, and do not set the primary key attribute if it is an
434         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
435         Multi-table models are not supported.
436         """
437         # When you bulk insert you don't get the primary keys back (if it's an
438         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
439         # you can't insert into the child tables which references this. There
440         # are two workarounds:
441         # 1) This could be implemented if you didn't have an autoincrement pk
442         # 2) You could do it by doing O(n) normal inserts into the parent
443         #    tables to get the primary keys back and then doing a single bulk
444         #    insert into the childmost table.
445         # We currently set the primary keys on the objects when using
446         # PostgreSQL via the RETURNING ID clause. It should be possible for
447         # Oracle as well, but the semantics for extracting the primary keys is
448         # trickier so it's not done yet.
449         assert batch_size is None or batch_size > 0
450         # Check that the parents share the same concrete model with the our
451         # model to detect the inheritance pattern ConcreteGrandParent ->
452         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
453         # would not identify that case as involving multiple tables.
454         for parent in self.model._meta.get_parent_list():
455             if parent._meta.concrete_model is not self.model._meta.concrete_model:
456                 raise ValueError("Can't bulk create a multi-table inherited model")
457         if not objs:
458             return objs
459         self._for_write = True
460         connection = connections[self.db]
461         fields = self.model._meta.concrete_fields
462         objs = list(objs)
463         self._populate_pk_values(objs)
464         with transaction.atomic(using=self.db, savepoint=False):
465             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
466             if objs_with_pk:
467                 self._batched_insert(objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts)
468                 for obj_with_pk in objs_with_pk:
469                     obj_with_pk._state.adding = False
470                     obj_with_pk._state.db = self.db
471             if objs_without_pk:
472                 fields = [f for f in fields if not isinstance(f, AutoField)]
473                 ids = self._batched_insert(objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts)
474                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
475                     assert len(ids) == len(objs_without_pk)
476                 for obj_without_pk, pk in zip(objs_without_pk, ids):
477                     obj_without_pk.pk = pk
478                     obj_without_pk._state.adding = False
479                     obj_without_pk._state.db = self.db
480 
481         return objs
482 
483     def bulk_update(self, objs, fields, batch_size=None):
484         """
485         Update the given fields in each of the given objects in the database.
486         """
487         if batch_size is not None and batch_size < 0:
488             raise ValueError('Batch size must be a positive integer.')
489         if not fields:
490             raise ValueError('Field names must be given to bulk_update().')
491         objs = tuple(objs)
492         if any(obj.pk is None for obj in objs):
493             raise ValueError('All bulk_update() objects must have a primary key set.')
494         fields = [self.model._meta.get_field(name) for name in fields]
495         if any(not f.concrete or f.many_to_many for f in fields):
496             raise ValueError('bulk_update() can only be used with concrete fields.')
497         if any(f.primary_key for f in fields):
498             raise ValueError('bulk_update() cannot be used with primary key fields.')
499         if not objs:
500             return
501         # PK is used twice in the resulting update query, once in the filter
502         # and once in the WHEN. Each field will also have one CAST.
503         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
504         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
505         requires_casting = connections[self.db].features.requires_casted_case_in_updates
506         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
507         updates = []
508         for batch_objs in batches:
509             update_kwargs = {}
510             for field in fields:
511                 when_statements = []
512                 for obj in batch_objs:
513                     attr = getattr(obj, field.attname)
514                     if not isinstance(attr, Expression):
515                         attr = Value(attr, output_field=field)
516                     when_statements.append(When(pk=obj.pk, then=attr))
517                 case_statement = Case(*when_statements, output_field=field)
518                 if requires_casting:
519                     case_statement = Cast(case_statement, output_field=field)
520                 update_kwargs[field.attname] = case_statement
521             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
522         with transaction.atomic(using=self.db, savepoint=False):
523             for pks, update_kwargs in updates:
524                 self.filter(pk__in=pks).update(**update_kwargs)
525     bulk_update.alters_data = True
526 
527     def get_or_create(self, defaults=None, **kwargs):
528         """
529         Look up an object with the given kwargs, creating one if necessary.
530         Return a tuple of (object, created), where created is a boolean
531         specifying whether an object was created.
532         """
533         # The get() needs to be targeted at the write database in order
534         # to avoid potential transaction consistency problems.
535         self._for_write = True
536         try:
537             return self.get(**kwargs), False
538         except self.model.DoesNotExist:
539             params = self._extract_model_params(defaults, **kwargs)
540             return self._create_object_from_params(kwargs, params)
541 
542     def update_or_create(self, defaults=None, **kwargs):
543         """
544         Look up an object with the given kwargs, updating one with defaults
545         if it exists, otherwise create a new one.
546         Return a tuple (object, created), where created is a boolean
547         specifying whether an object was created.
548         """
549         defaults = defaults or {}
550         self._for_write = True
551         with transaction.atomic(using=self.db):
552             try:
553                 obj = self.select_for_update().get(**kwargs)
554             except self.model.DoesNotExist:
555                 params = self._extract_model_params(defaults, **kwargs)
556                 # Lock the row so that a concurrent update is blocked until
557                 # after update_or_create() has performed its save.
558                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
559                 if created:
560                     return obj, created
561             for k, v in defaults.items():
562                 setattr(obj, k, v() if callable(v) else v)
563             obj.save(using=self.db)
564         return obj, False
565 
566     def _create_object_from_params(self, lookup, params, lock=False):
567         """
568         Try to create an object using passed params. Used by get_or_create()
569         and update_or_create().
570         """
571         try:
572             with transaction.atomic(using=self.db):
573                 params = {k: v() if callable(v) else v for k, v in params.items()}
574                 obj = self.create(**params)
575             return obj, True
576         except IntegrityError as e:
577             try:
578                 qs = self.select_for_update() if lock else self
579                 return qs.get(**lookup), False
580             except self.model.DoesNotExist:
581                 pass
582             raise e
583 
584     def _extract_model_params(self, defaults, **kwargs):
585         """
586         Prepare `params` for creating a model instance based on the given
587         kwargs; for use by get_or_create() and update_or_create().
588         """
589         defaults = defaults or {}
590         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
591         params.update(defaults)
592         property_names = self.model._meta._property_names
593         invalid_params = []
594         for param in params:
595             try:
596                 self.model._meta.get_field(param)
597             except exceptions.FieldDoesNotExist:
598                 # It's okay to use a model's property if it has a setter.
599                 if not (param in property_names and getattr(self.model, param).fset):
600                     invalid_params.append(param)
601         if invalid_params:
602             raise exceptions.FieldError(
603                 "Invalid field name(s) for model %s: '%s'." % (
604                     self.model._meta.object_name,
605                     "', '".join(sorted(invalid_params)),
606                 ))
607         return params
608 
609     def _earliest(self, *fields):
610         """
611         Return the earliest object according to fields (if given) or by the
612         model's Meta.get_latest_by.
613         """
614         if fields:
615             order_by = fields
616         else:
617             order_by = getattr(self.model._meta, 'get_latest_by')
618             if order_by and not isinstance(order_by, (tuple, list)):
619                 order_by = (order_by,)
620         if order_by is None:
621             raise ValueError(
622                 "earliest() and latest() require either fields as positional "
623                 "arguments or 'get_latest_by' in the model's Meta."
624             )
625 
626         assert self.query.can_filter(), \
627             "Cannot change a query once a slice has been taken."
628         obj = self._chain()
629         obj.query.set_limits(high=1)
630         obj.query.clear_ordering(force_empty=True)
631         obj.query.add_ordering(*order_by)
632         return obj.get()
633 
634     def earliest(self, *fields):
635         return self._earliest(*fields)
636 
637     def latest(self, *fields):
638         return self.reverse()._earliest(*fields)
639 
640     def first(self):
641         """Return the first object of a query or None if no match is found."""
642         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
643             return obj
644 
645     def last(self):
646         """Return the last object of a query or None if no match is found."""
647         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
648             return obj
649 
650     def in_bulk(self, id_list=None, *, field_name='pk'):
651         """
652         Return a dictionary mapping each of the given IDs to the object with
653         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
654         """
655         assert self.query.can_filter(), \
656             "Cannot use 'limit' or 'offset' with in_bulk"
657         if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:
658             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
659         if id_list is not None:
660             if not id_list:
661                 return {}
662             filter_key = '{}__in'.format(field_name)
663             batch_size = connections[self.db].features.max_query_params
664             id_list = tuple(id_list)
665             # If the database has a limit on the number of query parameters
666             # (e.g. SQLite), retrieve objects in batches if necessary.
667             if batch_size and batch_size < len(id_list):
668                 qs = ()
669                 for offset in range(0, len(id_list), batch_size):
670                     batch = id_list[offset:offset + batch_size]
671                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
672             else:
673                 qs = self.filter(**{filter_key: id_list}).order_by()
674         else:
675             qs = self._chain()
676         return {getattr(obj, field_name): obj for obj in qs}
677 
678     def delete(self):
679         """Delete the records in the current QuerySet."""
680         assert self.query.can_filter(), \
681             "Cannot use 'limit' or 'offset' with delete."
682 
683         if self._fields is not None:
684             raise TypeError("Cannot call delete() after .values() or .values_list()")
685 
686         del_query = self._chain()
687 
688         # The delete is actually 2 queries - one to find related objects,
689         # and one to delete. Make sure that the discovery of related
690         # objects is performed on the same database as the deletion.
691         del_query._for_write = True
692 
693         # Disable non-supported fields.
694         del_query.query.select_for_update = False
695         del_query.query.select_related = False
696         del_query.query.clear_ordering(force_empty=True)
697 
698         collector = Collector(using=del_query.db)
699         collector.collect(del_query)
700         deleted, _rows_count = collector.delete()
701 
702         # Clear the result cache, in case this QuerySet gets reused.
703         self._result_cache = None
704         return deleted, _rows_count
705 
706     delete.alters_data = True
707     delete.queryset_only = True
708 
709     def _raw_delete(self, using):
710         """
711         Delete objects found from the given queryset in single direct SQL
712         query. No signals are sent and there is no protection for cascades.
713         """
714         return sql.DeleteQuery(self.model).delete_qs(self, using)
715     _raw_delete.alters_data = True
716 
717     def update(self, **kwargs):
718         """
719         Update all elements in the current QuerySet, setting all the given
720         fields to the appropriate values.
721         """
722         assert self.query.can_filter(), \
723             "Cannot update a query once a slice has been taken."
724         self._for_write = True
725         query = self.query.chain(sql.UpdateQuery)
726         query.add_update_values(kwargs)
727         # Clear any annotations so that they won't be present in subqueries.
728         query.annotations = {}
729         with transaction.mark_for_rollback_on_error(using=self.db):
730             rows = query.get_compiler(self.db).execute_sql(CURSOR)
731         self._result_cache = None
732         return rows
733     update.alters_data = True
734 
735     def _update(self, values):
736         """
737         A version of update() that accepts field objects instead of field names.
738         Used primarily for model saving and not intended for use by general
739         code (it requires too much poking around at model internals to be
740         useful at that level).
741         """
742         assert self.query.can_filter(), \
743             "Cannot update a query once a slice has been taken."
744         query = self.query.chain(sql.UpdateQuery)
745         query.add_update_fields(values)
746         # Clear any annotations so that they won't be present in subqueries.
747         query.annotations = {}
748         self._result_cache = None
749         return query.get_compiler(self.db).execute_sql(CURSOR)
750     _update.alters_data = True
751     _update.queryset_only = False
752 
753     def exists(self):
754         if self._result_cache is None:
755             return self.query.has_results(using=self.db)
756         return bool(self._result_cache)
757 
758     def _prefetch_related_objects(self):
759         # This method can only be called once the result cache has been filled.
760         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
761         self._prefetch_done = True
762 
763     def explain(self, *, format=None, **options):
764         return self.query.explain(using=self.db, format=format, **options)
765 
766     ##################################################
767     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
768     ##################################################
769 
770     def raw(self, raw_query, params=None, translations=None, using=None):
771         if using is None:
772             using = self.db
773         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
774         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
775         return qs
776 
777     def _values(self, *fields, **expressions):
778         clone = self._chain()
779         if expressions:
780             clone = clone.annotate(**expressions)
781         clone._fields = fields
782         clone.query.set_values(fields)
783         return clone
784 
785     def values(self, *fields, **expressions):
786         fields += tuple(expressions)
787         clone = self._values(*fields, **expressions)
788         clone._iterable_class = ValuesIterable
789         return clone
790 
791     def values_list(self, *fields, flat=False, named=False):
792         if flat and named:
793             raise TypeError("'flat' and 'named' can't be used together.")
794         if flat and len(fields) > 1:
795             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
796 
797         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
798         _fields = []
799         expressions = {}
800         counter = 1
801         for field in fields:
802             if hasattr(field, 'resolve_expression'):
803                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
804                 while True:
805                     field_id = field_id_prefix + str(counter)
806                     counter += 1
807                     if field_id not in field_names:
808                         break
809                 expressions[field_id] = field
810                 _fields.append(field_id)
811             else:
812                 _fields.append(field)
813 
814         clone = self._values(*_fields, **expressions)
815         clone._iterable_class = (
816             NamedValuesListIterable if named
817             else FlatValuesListIterable if flat
818             else ValuesListIterable
819         )
820         return clone
821 
822     def dates(self, field_name, kind, order='ASC'):
823         """
824         Return a list of date objects representing all available dates for
825         the given field_name, scoped to 'kind'.
826         """
827         assert kind in ('year', 'month', 'week', 'day'), \
828             "'kind' must be one of 'year', 'month', 'week', or 'day'."
829         assert order in ('ASC', 'DESC'), \
830             "'order' must be either 'ASC' or 'DESC'."
831         return self.annotate(
832             datefield=Trunc(field_name, kind, output_field=DateField()),
833             plain_field=F(field_name)
834         ).values_list(
835             'datefield', flat=True
836         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
837 
838     def datetimes(self, field_name, kind, order='ASC', tzinfo=None):
839         """
840         Return a list of datetime objects representing all available
841         datetimes for the given field_name, scoped to 'kind'.
842         """
843         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
844             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
845         assert order in ('ASC', 'DESC'), \
846             "'order' must be either 'ASC' or 'DESC'."
847         if settings.USE_TZ:
848             if tzinfo is None:
849                 tzinfo = timezone.get_current_timezone()
850         else:
851             tzinfo = None
852         return self.annotate(
853             datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),
854             plain_field=F(field_name)
855         ).values_list(
856             'datetimefield', flat=True
857         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
858 
859     def none(self):
860         """Return an empty QuerySet."""
861         clone = self._chain()
862         clone.query.set_empty()
863         return clone
864 
865     ##################################################################
866     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
867     ##################################################################
868 
869     def all(self):
870         """
871         Return a new QuerySet that is a copy of the current one. This allows a
872         QuerySet to proxy for a model manager in some cases.
873         """
874         return self._chain()
875 
876     def filter(self, *args, **kwargs):
877         """
878         Return a new QuerySet instance with the args ANDed to the existing
879         set.
880         """
881         return self._filter_or_exclude(False, *args, **kwargs)
882 
883     def exclude(self, *args, **kwargs):
884         """
885         Return a new QuerySet instance with NOT (args) ANDed to the existing
886         set.
887         """
888         return self._filter_or_exclude(True, *args, **kwargs)
889 
890     def _filter_or_exclude(self, negate, *args, **kwargs):
891         if args or kwargs:
892             assert self.query.can_filter(), \
893                 "Cannot filter a query once a slice has been taken."
894 
895         clone = self._chain()
896         if negate:
897             clone.query.add_q(~Q(*args, **kwargs))
898         else:
899             clone.query.add_q(Q(*args, **kwargs))
900         return clone
901 
902     def complex_filter(self, filter_obj):
903         """
904         Return a new QuerySet instance with filter_obj added to the filters.
905 
906         filter_obj can be a Q object or a dictionary of keyword lookup
907         arguments.
908 
909         This exists to support framework features such as 'limit_choices_to',
910         and usually it will be more natural to use other methods.
911         """
912         if isinstance(filter_obj, Q):
913             clone = self._chain()
914             clone.query.add_q(filter_obj)
915             return clone
916         else:
917             return self._filter_or_exclude(None, **filter_obj)
918 
919     def _combinator_query(self, combinator, *other_qs, all=False):
920         # Clone the query to inherit the select list and everything
921         clone = self._chain()
922         # Clear limits and ordering so they can be reapplied
923         clone.query.clear_ordering(True)
924         clone.query.clear_limits()
925         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
926         clone.query.combinator = combinator
927         clone.query.combinator_all = all
928         return clone
929 
930     def union(self, *other_qs, all=False):
931         # If the query is an EmptyQuerySet, combine all nonempty querysets.
932         if isinstance(self, EmptyQuerySet):
933             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
934             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
935         return self._combinator_query('union', *other_qs, all=all)
936 
937     def intersection(self, *other_qs):
938         # If any query is an EmptyQuerySet, return it.
939         if isinstance(self, EmptyQuerySet):
940             return self
941         for other in other_qs:
942             if isinstance(other, EmptyQuerySet):
943                 return other
944         return self._combinator_query('intersection', *other_qs)
945 
946     def difference(self, *other_qs):
947         # If the query is an EmptyQuerySet, return it.
948         if isinstance(self, EmptyQuerySet):
949             return self
950         return self._combinator_query('difference', *other_qs)
951 
952     def select_for_update(self, nowait=False, skip_locked=False, of=()):
953         """
954         Return a new QuerySet instance that will select objects with a
955         FOR UPDATE lock.
956         """
957         if nowait and skip_locked:
958             raise ValueError('The nowait option cannot be used with skip_locked.')
959         obj = self._chain()
960         obj._for_write = True
961         obj.query.select_for_update = True
962         obj.query.select_for_update_nowait = nowait
963         obj.query.select_for_update_skip_locked = skip_locked
964         obj.query.select_for_update_of = of
965         return obj
966 
967     def select_related(self, *fields):
968         """
969         Return a new QuerySet instance that will select related objects.
970 
971         If fields are specified, they must be ForeignKey fields and only those
972         related objects are included in the selection.
973 
974         If select_related(None) is called, clear the list.
975         """
976 
977         if self._fields is not None:
978             raise TypeError("Cannot call select_related() after .values() or .values_list()")
979 
980         obj = self._chain()
981         if fields == (None,):
982             obj.query.select_related = False
983         elif fields:
984             obj.query.add_select_related(fields)
985         else:
986             obj.query.select_related = True
987         return obj
988 
989     def prefetch_related(self, *lookups):
990         """
991         Return a new QuerySet instance that will prefetch the specified
992         Many-To-One and Many-To-Many related objects when the QuerySet is
993         evaluated.
994 
995         When prefetch_related() is called more than once, append to the list of
996         prefetch lookups. If prefetch_related(None) is called, clear the list.
997         """
998         clone = self._chain()
999         if lookups == (None,):
1000             clone._prefetch_related_lookups = ()
1001         else:
1002             for lookup in lookups:
1003                 if isinstance(lookup, Prefetch):
1004                     lookup = lookup.prefetch_to
1005                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1006                 if lookup in self.query._filtered_relations:
1007                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1008             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1009         return clone
1010 
1011     def annotate(self, *args, **kwargs):
1012         """
1013         Return a query set in which the returned objects have been annotated
1014         with extra data or aggregations.
1015         """
1016         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1017         annotations = {}
1018         for arg in args:
1019             # The default_alias property may raise a TypeError.
1020             try:
1021                 if arg.default_alias in kwargs:
1022                     raise ValueError("The named annotation '%s' conflicts with the "
1023                                      "default name for another annotation."
1024                                      % arg.default_alias)
1025             except TypeError:
1026                 raise TypeError("Complex annotations require an alias")
1027             annotations[arg.default_alias] = arg
1028         annotations.update(kwargs)
1029 
1030         clone = self._chain()
1031         names = self._fields
1032         if names is None:
1033             names = set(chain.from_iterable(
1034                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1035                 for field in self.model._meta.get_fields()
1036             ))
1037 
1038         for alias, annotation in annotations.items():
1039             if alias in names:
1040                 raise ValueError("The annotation '%s' conflicts with a field on "
1041                                  "the model." % alias)
1042             if isinstance(annotation, FilteredRelation):
1043                 clone.query.add_filtered_relation(annotation, alias)
1044             else:
1045                 clone.query.add_annotation(annotation, alias, is_summary=False)
1046 
1047         for alias, annotation in clone.query.annotations.items():
1048             if alias in annotations and annotation.contains_aggregate:
1049                 if clone._fields is None:
1050                     clone.query.group_by = True
1051                 else:
1052                     clone.query.set_group_by()
1053                 break
1054 
1055         return clone
1056 
1057     def order_by(self, *field_names):
1058         """Return a new QuerySet instance with the ordering changed."""
1059         assert self.query.can_filter(), \
1060             "Cannot reorder a query once a slice has been taken."
1061         obj = self._chain()
1062         obj.query.clear_ordering(force_empty=False)
1063         obj.query.add_ordering(*field_names)
1064         return obj
1065 
1066     def distinct(self, *field_names):
1067         """
1068         Return a new QuerySet instance that will select only distinct results.
1069         """
1070         assert self.query.can_filter(), \
1071             "Cannot create distinct fields once a slice has been taken."
1072         obj = self._chain()
1073         obj.query.add_distinct_fields(*field_names)
1074         return obj
1075 
1076     def extra(self, select=None, where=None, params=None, tables=None,
1077               order_by=None, select_params=None):
1078         """Add extra SQL fragments to the query."""
1079         assert self.query.can_filter(), \
1080             "Cannot change a query once a slice has been taken"
1081         clone = self._chain()
1082         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1083         return clone
1084 
1085     def reverse(self):
1086         """Reverse the ordering of the QuerySet."""
1087         if not self.query.can_filter():
1088             raise TypeError('Cannot reverse a query once a slice has been taken.')
1089         clone = self._chain()
1090         clone.query.standard_ordering = not clone.query.standard_ordering
1091         return clone
1092 
1093     def defer(self, *fields):
1094         """
1095         Defer the loading of data for certain fields until they are accessed.
1096         Add the set of deferred fields to any existing set of deferred fields.
1097         The only exception to this is if None is passed in as the only
1098         parameter, in which case removal all deferrals.
1099         """
1100         if self._fields is not None:
1101             raise TypeError("Cannot call defer() after .values() or .values_list()")
1102         clone = self._chain()
1103         if fields == (None,):
1104             clone.query.clear_deferred_loading()
1105         else:
1106             clone.query.add_deferred_loading(fields)
1107         return clone
1108 
1109     def only(self, *fields):
1110         """
1111         Essentially, the opposite of defer(). Only the fields passed into this
1112         method and that are not already specified as deferred are loaded
1113         immediately when the queryset is evaluated.
1114         """
1115         if self._fields is not None:
1116             raise TypeError("Cannot call only() after .values() or .values_list()")
1117         if fields == (None,):
1118             # Can only pass None to defer(), not only(), as the rest option.
1119             # That won't stop people trying to do this, so let's be explicit.
1120             raise TypeError("Cannot pass None as an argument to only().")
1121         for field in fields:
1122             field = field.split(LOOKUP_SEP, 1)[0]
1123             if field in self.query._filtered_relations:
1124                 raise ValueError('only() is not supported with FilteredRelation.')
1125         clone = self._chain()
1126         clone.query.add_immediate_loading(fields)
1127         return clone
1128 
1129     def using(self, alias):
1130         """Select which database this QuerySet should execute against."""
1131         clone = self._chain()
1132         clone._db = alias
1133         return clone
1134 
1135     ###################################
1136     # PUBLIC INTROSPECTION ATTRIBUTES #
1137     ###################################
1138 
1139     @property
1140     def ordered(self):
1141         """
1142         Return True if the QuerySet is ordered -- i.e. has an order_by()
1143         clause or a default ordering on the model (or is empty).
1144         """
1145         if isinstance(self, EmptyQuerySet):
1146             return True
1147         if self.query.extra_order_by or self.query.order_by:
1148             return True
1149         elif self.query.default_ordering and self.query.get_meta().ordering:
1150             return True
1151         else:
1152             return False
1153 
1154     @property
1155     def db(self):
1156         """Return the database used if this query is executed now."""
1157         if self._for_write:
1158             return self._db or router.db_for_write(self.model, **self._hints)
1159         return self._db or router.db_for_read(self.model, **self._hints)
1160 
1161     ###################
1162     # PRIVATE METHODS #
1163     ###################
1164 
1165     def _insert(self, objs, fields, return_id=False, raw=False, using=None, ignore_conflicts=False):
1166         """
1167         Insert a new record for the given model. This provides an interface to
1168         the InsertQuery class and is how Model.save() is implemented.
1169         """
1170         self._for_write = True
1171         if using is None:
1172             using = self.db
1173         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1174         query.insert_values(fields, objs, raw=raw)
1175         return query.get_compiler(using=using).execute_sql(return_id)
1176     _insert.alters_data = True
1177     _insert.queryset_only = False
1178 
1179     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1180         """
1181         Helper method for bulk_create() to insert objs one batch at a time.
1182         """
1183         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1184             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1185         ops = connections[self.db].ops
1186         batch_size = (batch_size or max(ops.bulk_batch_size(fields, objs), 1))
1187         inserted_ids = []
1188         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1189         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1190             if bulk_return and not ignore_conflicts:
1191                 inserted_id = self._insert(
1192                     item, fields=fields, using=self.db, return_id=True,
1193                     ignore_conflicts=ignore_conflicts,
1194                 )
1195                 if isinstance(inserted_id, list):
1196                     inserted_ids.extend(inserted_id)
1197                 else:
1198                     inserted_ids.append(inserted_id)
1199             else:
1200                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1201         return inserted_ids
1202 
1203     def _chain(self, **kwargs):
1204         """
1205         Return a copy of the current QuerySet that's ready for another
1206         operation.
1207         """
1208         obj = self._clone()
1209         if obj._sticky_filter:
1210             obj.query.filter_is_sticky = True
1211             obj._sticky_filter = False
1212         obj.__dict__.update(kwargs)
1213         return obj
1214 
1215     def _clone(self):
1216         """
1217         Return a copy of the current QuerySet. A lightweight alternative
1218         to deepcopy().
1219         """
1220         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1221         c._sticky_filter = self._sticky_filter
1222         c._for_write = self._for_write
1223         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1224         c._known_related_objects = self._known_related_objects
1225         c._iterable_class = self._iterable_class
1226         c._fields = self._fields
1227         return c
1228 
1229     def _fetch_all(self):
1230         if self._result_cache is None:
1231             self._result_cache = list(self._iterable_class(self))
1232         if self._prefetch_related_lookups and not self._prefetch_done:
1233             self._prefetch_related_objects()
1234 
1235     def _next_is_sticky(self):
1236         """
1237         Indicate that the next filter call and the one following that should
1238         be treated as a single filter. This is only important when it comes to
1239         determining when to reuse tables for many-to-many filters. Required so
1240         that we can filter naturally on the results of related managers.
1241 
1242         This doesn't return a clone of the current QuerySet (it returns
1243         "self"). The method is only used internally and should be immediately
1244         followed by a filter() that does create a clone.
1245         """
1246         self._sticky_filter = True
1247         return self
1248 
1249     def _merge_sanity_check(self, other):
1250         """Check that two QuerySet classes may be merged."""
1251         if self._fields is not None and (
1252                 set(self.query.values_select) != set(other.query.values_select) or
1253                 set(self.query.extra_select) != set(other.query.extra_select) or
1254                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1255             raise TypeError(
1256                 "Merging '%s' classes must involve the same values in each case."
1257                 % self.__class__.__name__
1258             )
1259 
1260     def _merge_known_related_objects(self, other):
1261         """
1262         Keep track of all known related objects from either QuerySet instance.
1263         """
1264         for field, objects in other._known_related_objects.items():
1265             self._known_related_objects.setdefault(field, {}).update(objects)
1266 
1267     def resolve_expression(self, *args, **kwargs):
1268         if self._fields and len(self._fields) > 1:
1269             # values() queryset can only be used as nested queries
1270             # if they are set up to select only a single field.
1271             raise TypeError('Cannot use multi-field values as a filter value.')
1272         query = self.query.resolve_expression(*args, **kwargs)
1273         query._db = self._db
1274         return query
1275     resolve_expression.queryset_only = True
1276 
1277     def _add_hints(self, **hints):
1278         """
1279         Update hinting information for use by routers. Add new key/values or
1280         overwrite existing key/values.
1281         """
1282         self._hints.update(hints)
1283 
1284     def _has_filters(self):
1285         """
1286         Check if this QuerySet has any filtering going on. This isn't
1287         equivalent with checking if all objects are present in results, for
1288         example, qs[1:]._has_filters() -> False.
1289         """
1290         return self.query.has_filters()
1291 
1292     @staticmethod
1293     def _validate_values_are_expressions(values, method_name):
1294         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1295         if invalid_args:
1296             raise TypeError(
1297                 'QuerySet.%s() received non-expression(s): %s.' % (
1298                     method_name,
1299                     ', '.join(invalid_args),
1300                 )
1301             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 10:05:40.803 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 10:05:40.803 | INFO     | Search-only: skipping patch selection
2025-10-18 10:05:40.989 | INFO     | Task django__django-10853 completed successfully.
2025-10-18 10:05:40.990 | INFO     | No patch generated. You can try running ACR again.
