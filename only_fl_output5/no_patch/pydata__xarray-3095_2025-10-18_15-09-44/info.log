2025-10-18 15:09:44.508 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 15:09:47.036 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=626, output_tokens=13, cost=0.006650
2025-10-18 15:09:58.350 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=806, output_tokens=425, cost=0.020810
2025-10-18 15:10:00.077 | INFO     | registered non reproducing test 0
2025-10-18 15:10:13.476 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1836, output_tokens=554, cost=0.034980
2025-10-18 15:10:15.123 | INFO     | registered non reproducing test 1
2025-10-18 15:10:27.745 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1961, output_tokens=569, cost=0.036680
2025-10-18 15:10:29.400 | INFO     | registered non reproducing test 2
2025-10-18 15:10:29.400 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-18 15:10:29.403 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 15:10:34.224 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=889, output_tokens=212, cost=0.015250
2025-10-18 15:10:34.230 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 15:10:37.999 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=566, output_tokens=42, cost=0.006920
2025-10-18 15:10:38.000 | DEBUG    | Extracted a valid json.
2025-10-18 15:10:38.000 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_method_in_class('copy', 'Dataset')",
        "search_method_in_class('copy', 'DataArray')"
    ],
    "bug_locations": []
}
2025-10-18 15:10:38.050 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 15:10:38.051 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 15:10:53.046 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3480, output_tokens=685, cost=0.055350
2025-10-18 15:10:53.059 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 15:11:00.612 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=4381, output_tokens=317, cost=0.053320
2025-10-18 15:11:00.620 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 15:11:08.723 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=671, output_tokens=294, cost=0.015530
2025-10-18 15:11:08.724 | DEBUG    | Extracted a valid json.
2025-10-18 15:11:08.724 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_method_in_class('copy', 'Variable')",
        "search_method_in_class('copy', 'IndexVariable')"
    ],
    "bug_locations": [
        {
            "file": "xarray/core/dataset.py",
            "class": "Dataset",
            "method": "copy",
            "intended_behavior": "In the `Dataset.copy` method, the copying process of variables, including index variables, should ensure the preservation of their data types, specifically for Unicode data types. When performing a deep copy, the method must maintain the `<U*` dtype for index variables without casting them to `object`. This may involve adjusting how the `deep` parameter is handled in variable copy operations or applying specific checks for index variables to maintain their data type integrity."
        },
        {
            "file": "xarray/core/dataarray.py",
            "class": "DataArray",
            "method": "copy",
            "intended_behavior": "The `DataArray.copy` function must ensure that when a deep copy is requested, all elements (data and coordinates) preserve their original data types during the copy. For index variables with Unicode data types (`<U*`), the method should specifically prevent the alteration of their data type to `object`. Adjustments may be needed in how the `deep` parameter influences the copy of the variable and coordinates to ensure type consistency."
        }
    ]
}
2025-10-18 15:11:08.758 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 15:11:08.759 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 15:11:23.514 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=6082, output_tokens=598, cost=0.078760
2025-10-18 15:11:23.525 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 15:11:29.827 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=6896, output_tokens=258, cost=0.076700
2025-10-18 15:11:29.834 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 15:11:37.894 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=612, output_tokens=221, cost=0.012750
2025-10-18 15:11:37.895 | DEBUG    | Extracted a valid json.
2025-10-18 15:11:37.895 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "xarray/core/variable.py",
            "class": "Variable",
            "method": "copy",
            "intended_behavior": "The method should include explicit handling to preserve the data type of variables when making deep copies. This could involve implementing a check to ensure that the dtype of numpy.array(data) operation is aligned with the original data's dtype, especially for Unicode data types, thus preventing the conversion to the object dtype."
        },
        {
            "file": "xarray/core/variable.py",
            "class": "IndexVariable",
            "method": "copy",
            "intended_behavior": "In the IndexVariable.copy method, there should be additional logic to ensure that the dtype of index variables, particularly those with Unicode types, is preserved during the copy operation. This might require a more nuanced approach when creating a new PandasIndexAdapter from the copied pandas.Index to ensure the dtype is explicitly specified or maintained, preventing unintended changes to object dtype."
        }
    ]
}
2025-10-18 15:11:37.908 | DEBUG    | Found override of copy in Variable
2025-10-18 15:11:37.908 | ERROR    | SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
Traceback (most recent call last):

  File "/home/kimnal0/auto-code-rover/app/main.py", line 656, in <module>
    main()
    └ <function main at 0x761e6b526c00>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 120, in main
    run_task_groups(groups, num_processes, organize_output=True)
    │               │       └ 1
    │               └ {'setup_astropy__astropy__4.3': [<app.raw_tasks.RawSweTask object at 0x761d78dc37d0>, <app.raw_tasks.RawSweTask object at 0x7...
    └ <function run_task_groups at 0x761d7b11a3e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 396, in run_task_groups
    run_tasks_serial(all_tasks)
    │                └ [<app.raw_tasks.RawSweTask object at 0x761d78dc37d0>, <app.raw_tasks.RawSweTask object at 0x761d78dc3800>, <app.raw_tasks.Raw...
    └ <function run_tasks_serial at 0x761d7b11a480>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 414, in run_tasks_serial
    run_task_in_subprocess(task)
    │                      └ <app.raw_tasks.RawSweTask object at 0x761d78d3d550>
    └ <function run_task_in_subprocess at 0x761d7b11a660>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 462, in run_task_in_subprocess
    executor.submit(run_raw_task, task)
    │        │      │             └ <app.raw_tasks.RawSweTask object at 0x761d78d3d550>
    │        │      └ <function run_raw_task at 0x761d7b11a700>
    │        └ <function ProcessPoolExecutor.submit at 0x761e6ae72020>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x761d7b1d56d0>

  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 831, in submit
    self._start_executor_manager_thread()
    │    └ <function ProcessPoolExecutor._start_executor_manager_thread at 0x761e6ae71da0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x761d7b1d56d0>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 770, in _start_executor_manager_thread
    self._launch_processes()
    │    └ <function ProcessPoolExecutor._launch_processes at 0x761e6ae71ee0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x761d7b1d56d0>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 797, in _launch_processes
    self._spawn_process()
    │    └ <function ProcessPoolExecutor._spawn_process at 0x761e6ae71f80>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x761d7b1d56d0>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 807, in _spawn_process
    p.start()
    │ └ <function BaseProcess.start at 0x761e6b02cea0>
    └ <ForkProcess name='ForkProcess-261' parent=497161 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
    │    │        │    │      └ <ForkProcess name='ForkProcess-261' parent=497161 started>
    │    │        │    └ <staticmethod(<function ForkProcess._Popen at 0x761e6b09e0c0>)>
    │    │        └ <ForkProcess name='ForkProcess-261' parent=497161 started>
    │    └ None
    └ <ForkProcess name='ForkProcess-261' parent=497161 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/context.py", line 282, in _Popen
    return Popen(process_obj)
           │     └ <ForkProcess name='ForkProcess-261' parent=497161 started>
           └ <class 'multiprocessing.popen_fork.Popen'>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
    │    │       └ <ForkProcess name='ForkProcess-261' parent=497161 started>
    │    └ <function Popen._launch at 0x761d7911b7e0>
    └ <multiprocessing.popen_fork.Popen object at 0x761d7a57e6f0>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
           │           │                          └ 13
           │           └ <function BaseProcess._bootstrap at 0x761e6b02d8a0>
           └ <ForkProcess name='ForkProcess-261' parent=497161 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x761e6b02ce00>
    └ <ForkProcess name='ForkProcess-261' parent=497161 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {}
    │    │        │    │        └ <ForkProcess name='ForkProcess-261' parent=497161 started>
    │    │        │    └ (<concurrent.futures.process._SafeQueue object at 0x761d795c47d0>, <multiprocessing.queues.SimpleQueue object at 0x761d795c4c...
    │    │        └ <ForkProcess name='ForkProcess-261' parent=497161 started>
    │    └ <function _process_worker at 0x761e6ae711c0>
    └ <ForkProcess name='ForkProcess-261' parent=497161 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 263, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
        │         │   │         │       │         └ {}
        │         │   │         │       └ <concurrent.futures.process._CallItem object at 0x761d7b442db0>
        │         │   │         └ (<app.raw_tasks.RawSweTask object at 0x761d7a57de80>,)
        │         │   └ <concurrent.futures.process._CallItem object at 0x761d7b442db0>
        │         └ <function run_raw_task at 0x761d7b11a700>
        └ <concurrent.futures.process._CallItem object at 0x761d7b442db0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 494, in run_raw_task
    run_ok = do_inference(task.to_task(), task_output_dir)
             │            │    │          └ '/home/kimnal0/auto-code-rover/only_fl_output5/pydata__xarray-3095_2025-10-18_15-09-44'
             │            │    └ <function RawSweTask.to_task at 0x761d7b1196c0>
             │            └ <app.raw_tasks.RawSweTask object at 0x761d7a57de80>
             └ <function do_inference at 0x761d7b11a8e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 590, in do_inference
    run_ok = inference.run_one_task(
             │         └ <function run_one_task at 0x761d7b228400>
             └ <module 'app.inference' from '/home/kimnal0/auto-code-rover/app/inference.py'>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 128, in run_one_task
    if _run_one_task(str(out_dir), api_manager, task.get_issue_statement()):
       │                 │         │            │    └ <function SweTask.get_issue_statement at 0x761d7b9b84a0>
       │                 │         │            └ SweTask(task_id='pydata__xarray-3095', problem_statement="REGRESSION: copy(deep=True) casts unicode indices to object\nDatase...
       │                 │         └ <app.manage.ProjectApiManager object at 0x761d7b6a3170>
       │                 └ Path('/home/kimnal0/auto-code-rover/only_fl_output5/pydata__xarray-3095_2025-10-18_15-09-44/output_0')
       └ <function _run_one_task at 0x761d7b2298a0>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 303, in _run_one_task
    bug_locs, search_msg_thread = api_manager.search_manager.search_iterative(
                                  │           │              └ <function SearchManager.search_iterative at 0x761d7b431bc0>
                                  │           └ <app.search.search_manage.SearchManager object at 0x761d795c7ec0>
                                  └ <app.manage.ProjectApiManager object at 0x761d7b6a3170>

  File "/home/kimnal0/auto-code-rover/app/search/search_manage.py", line 125, in search_iterative
    new_bug_locations.extend(self.backend.get_bug_loc_snippets_new(loc))
    │                 │      │    │       │                        └ {'file': 'xarray/core/variable.py', 'class': 'IndexVariable', 'method': 'copy', 'intended_behavior': 'In the IndexVariable.co...
    │                 │      │    │       └ <function SearchBackend.get_bug_loc_snippets_new at 0x761d7b4318a0>
    │                 │      │    └ <app.search.search_backend.SearchBackend object at 0x761d7b21d610>
    │                 │      └ <app.search.search_manage.SearchManager object at 0x761d795c7ec0>
    │                 └ <method 'extend' of 'list' objects>
    └ [<file>xarray/core/variable.py</file>
      <class>Variable</class>
      <method>copy</method>
      <code>
      743     def copy(self, deep=True, ...

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 825, in get_bug_loc_snippets_new
    self._get_inherited_methods(res.class_name, res.func_name)
    │    │                      │   │           │   └ 'copy'
    │    │                      │   │           └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core...
    │    │                      │   └ 'IndexVariable'
    │    │                      └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core...
    │    └ <function SearchBackend._get_inherited_methods at 0x761d7b431800>
    └ <app.search.search_backend.SearchBackend object at 0x761d7b21d610>

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 748, in _get_inherited_methods
    output, search_res, call_ok = self.search_method_in_class(super_call)
                                  │    │                      └ {'class_name': 'Variable', 'method_name': 'copy'}
                                  │    └ <function SearchBackend.search_method_in_class at 0x761d7b430f40>
                                  └ <app.search.search_backend.SearchBackend object at 0x761d7b21d610>

> File "/home/kimnal0/auto-code-rover/app/utils.py", line 343, in wrapper
    return func(*args, **kwargs)
           │     │       └ {}
           │     └ (<app.search.search_backend.SearchBackend object at 0x761d7b21d610>, {'class_name': 'Variable', 'method_name': 'copy'})
           └ <function SearchBackend.search_method_in_class at 0x761d7b430ea0>

TypeError: SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
2025-10-18 15:11:37.915 | DEBUG    | Bug location extracted successfully: [<file>xarray/core/variable.py</file>
<class>Variable</class>
<method>copy</method>
<code>
743     def copy(self, deep=True, data=None):
744         """Returns a copy of this object.
745 
746         If `deep=True`, the data array is loaded into memory and copied onto
747         the new object. Dimensions, attributes and encodings are always copied.
748 
749         Use `data` to create a new object with the same structure as
750         original but entirely new data.
751 
752         Parameters
753         ----------
754         deep : bool, optional
755             Whether the data array is loaded into memory and copied onto
756             the new object. Default is True.
757         data : array_like, optional
758             Data to use in the new object. Must have same shape as original.
759             When `data` is used, `deep` is ignored.
760 
761         Returns
762         -------
763         object : Variable
764             New object with dimensions, attributes, encodings, and optionally
765             data copied from original.
766 
767         Examples
768         --------
769 
770         Shallow copy versus deep copy
771 
772         >>> var = xr.Variable(data=[1, 2, 3], dims='x')
773         >>> var.copy()
774         <xarray.Variable (x: 3)>
775         array([1, 2, 3])
776         >>> var_0 = var.copy(deep=False)
777         >>> var_0[0] = 7
778         >>> var_0
779         <xarray.Variable (x: 3)>
780         array([7, 2, 3])
781         >>> var
782         <xarray.Variable (x: 3)>
783         array([7, 2, 3])
784 
785         Changing the data using the ``data`` argument maintains the
786         structure of the original object, but with the new data. Original
787         object is unaffected.
788 
789         >>> var.copy(data=[0.1, 0.2, 0.3])
790         <xarray.Variable (x: 3)>
791         array([ 0.1,  0.2,  0.3])
792         >>> var
793         <xarray.Variable (x: 3)>
794         array([7, 2, 3])
795 
796         See Also
797         --------
798         pandas.DataFrame.copy
799         """
800         if data is None:
801             data = self._data
802 
803             if isinstance(data, indexing.MemoryCachedArray):
804                 # don't share caching between copies
805                 data = indexing.MemoryCachedArray(data.array)
806 
807             if deep:
808                 if isinstance(data, dask_array_type):
809                     data = data.copy()
810                 elif not isinstance(data, PandasIndexAdapter):
811                     # pandas.Index is immutable
812                     data = np.array(data)
813         else:
814             data = as_compatible_data(data)
815             if self.shape != data.shape:
816                 raise ValueError("Data shape {} must match shape of object {}"
817                                  .format(data.shape, self.shape))
818 
819         # note:
820         # dims is already an immutable tuple
821         # attributes and encoding will be copied when the new Array is created
822         return type(self)(self.dims, data, self._attrs, self._encoding,
823                           fastpath=True)

</code><intended_behavior>The method should include explicit handling to preserve the data type of variables when making deep copies. This could involve implementing a check to ensure that the dtype of numpy.array(data) operation is aligned with the original data's dtype, especially for Unicode data types, thus preventing the conversion to the object dtype.</intended_behavior>, <file>xarray/core/variable.py</file>
<class>Variable</class>
<method>None</method>
<code>
219 class Variable(common.AbstractArray, arithmetic.SupportsArithmetic,
220                utils.NdimSizeLenMixin):
221     """A netcdf-like variable consisting of dimensions, data and attributes
222     which describe a single Array. A single Variable object is not fully
223     described outside the context of its parent Dataset (if you want such a
224     fully described object, use a DataArray instead).
225 
226     The main functional difference between Variables and numpy arrays is that
227     numerical operations on Variables implement array broadcasting by dimension
228     name. For example, adding an Variable with dimensions `('time',)` to
229     another Variable with dimensions `('space',)` results in a new Variable
230     with dimensions `('time', 'space')`. Furthermore, numpy reduce operations
231     like ``mean`` or ``sum`` are overwritten to take a "dimension" argument
232     instead of an "axis".
233 
234     Variables are light-weight objects used as the building block for datasets.
235     They are more primitive objects, so operations with them provide marginally
236     higher performance than using DataArrays. However, manipulating data in the
237     form of a Dataset or DataArray should almost always be preferred, because
238     they can use more complete metadata in context of coordinate labels.
239     """
240 
241     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):
242         """
243         Parameters
244         ----------
245         dims : str or sequence of str
246             Name(s) of the the data dimension(s). Must be either a string (only
247             for 1D data) or a sequence of strings with length equal to the
248             number of dimensions.
249         data : array_like
250             Data array which supports numpy-like data access.
251         attrs : dict_like or None, optional
252             Attributes to assign to the new variable. If None (default), an
253             empty attribute dictionary is initialized.
254         encoding : dict_like or None, optional
255             Dictionary specifying how to encode this array's data into a
256             serialized format like netCDF4. Currently used keys (for netCDF)
257             include '_FillValue', 'scale_factor', 'add_offset' and 'dtype'.
258             Well-behaved code to serialize a Variable should ignore
259             unrecognized encoding items.
260         """
261         self._data = as_compatible_data(data, fastpath=fastpath)
262         self._dims = self._parse_dimensions(dims)
263         self._attrs = None
264         self._encoding = None
265         if attrs is not None:
266             self.attrs = attrs
267         if encoding is not None:
268             self.encoding = encoding
269 
270     @property
271     def dtype(self):
272         return self._data.dtype
273 
274     @property
275     def shape(self):
276         return self._data.shape
277 
278     @property
279     def nbytes(self):
280         return self.size * self.dtype.itemsize
281 
282     @property
283     def _in_memory(self):
284         return (isinstance(self._data, (np.ndarray, np.number,
285                                         PandasIndexAdapter)) or
286                 (isinstance(self._data, indexing.MemoryCachedArray) and
287                  isinstance(self._data.array, indexing.NumpyIndexingAdapter)))
288 
289     @property
290     def data(self):
291         if isinstance(self._data, dask_array_type):
292             return self._data
293         else:
294             return self.values
295 
296     @data.setter
297     def data(self, data):
298         data = as_compatible_data(data)
299         if data.shape != self.shape:
300             raise ValueError(
301                 "replacement data must match the Variable's shape")
302         self._data = data
303 
304     def load(self, **kwargs):
305         """Manually trigger loading of this variable's data from disk or a
306         remote source into memory and return this variable.
307 
308         Normally, it should not be necessary to call this method in user code,
309         because all xarray functions should either work on deferred data or
310         load data automatically.
311 
312         Parameters
313         ----------
314         **kwargs : dict
315             Additional keyword arguments passed on to ``dask.array.compute``.
316 
317         See Also
318         --------
319         dask.array.compute
320         """
321         if isinstance(self._data, dask_array_type):
322             self._data = as_compatible_data(self._data.compute(**kwargs))
323         elif not isinstance(self._data, np.ndarray):
324             self._data = np.asarray(self._data)
325         return self
326 
327     def compute(self, **kwargs):
328         """Manually trigger loading of this variable's data from disk or a
329         remote source into memory and return a new variable. The original is
330         left unaltered.
331 
332         Normally, it should not be necessary to call this method in user code,
333         because all xarray functions should either work on deferred data or
334         load data automatically.
335 
336         Parameters
337         ----------
338         **kwargs : dict
339             Additional keyword arguments passed on to ``dask.array.compute``.
340 
341         See Also
342         --------
343         dask.array.compute
344         """
345         new = self.copy(deep=False)
346         return new.load(**kwargs)
347 
348     def __dask_graph__(self):
349         if isinstance(self._data, dask_array_type):
350             return self._data.__dask_graph__()
351         else:
352             return None
353 
354     def __dask_keys__(self):
355         return self._data.__dask_keys__()
356 
357     def __dask_layers__(self):
358         return self._data.__dask_layers__()
359 
360     @property
361     def __dask_optimize__(self):
362         return self._data.__dask_optimize__
363 
364     @property
365     def __dask_scheduler__(self):
366         return self._data.__dask_scheduler__
367 
368     def __dask_postcompute__(self):
369         array_func, array_args = self._data.__dask_postcompute__()
370         return self._dask_finalize, (array_func, array_args, self._dims,
371                                      self._attrs, self._encoding)
372 
373     def __dask_postpersist__(self):
374         array_func, array_args = self._data.__dask_postpersist__()
375         return self._dask_finalize, (array_func, array_args, self._dims,
376                                      self._attrs, self._encoding)
377 
378     @staticmethod
379     def _dask_finalize(results, array_func, array_args, dims, attrs, encoding):
380         if isinstance(results, dict):  # persist case
381             name = array_args[0]
382             results = {k: v for k, v in results.items() if k[0] == name}
383         data = array_func(results, *array_args)
384         return Variable(dims, data, attrs=attrs, encoding=encoding)
385 
386     @property
387     def values(self):
388         """The variable's data as a numpy.ndarray"""
389         return _as_array_or_item(self._data)
390 
391     @values.setter
392     def values(self, values):
393         self.data = values
394 
395     def to_base_variable(self):
396         """Return this variable as a base xarray.Variable"""
397         return Variable(self.dims, self._data, self._attrs,
398                         encoding=self._encoding, fastpath=True)
399 
400     to_variable = utils.alias(to_base_variable, 'to_variable')
401 
402     def to_index_variable(self):
403         """Return this variable as an xarray.IndexVariable"""
404         return IndexVariable(self.dims, self._data, self._attrs,
405                              encoding=self._encoding, fastpath=True)
406 
407     to_coord = utils.alias(to_index_variable, 'to_coord')
408 
409     def to_index(self):
410         """Convert this variable to a pandas.Index"""
411         return self.to_index_variable().to_index()
412 
413     def to_dict(self, data=True):
414         """Dictionary representation of variable."""
415         item = {'dims': self.dims,
416                 'attrs': decode_numpy_dict_values(self.attrs)}
417         if data:
418             item['data'] = ensure_us_time_resolution(self.values).tolist()
419         else:
420             item.update({'dtype': str(self.dtype), 'shape': self.shape})
421         return item
422 
423     @property
424     def dims(self):
425         """Tuple of dimension names with which this variable is associated.
426         """
427         return self._dims
428 
429     @dims.setter
430     def dims(self, value):
431         self._dims = self._parse_dimensions(value)
432 
433     def _parse_dimensions(self, dims):
434         if isinstance(dims, str):
435             dims = (dims,)
436         dims = tuple(dims)
437         if len(dims) != self.ndim:
438             raise ValueError('dimensions %s must have the same length as the '
439                              'number of data dimensions, ndim=%s'
440                              % (dims, self.ndim))
441         return dims
442 
443     def _item_key_to_tuple(self, key):
444         if utils.is_dict_like(key):
445             return tuple(key.get(dim, slice(None)) for dim in self.dims)
446         else:
447             return key
448 
449     def _broadcast_indexes(self, key):
450         """Prepare an indexing key for an indexing operation.
451 
452         Parameters
453         -----------
454         key: int, slice, array, dict or tuple of integer, slices and arrays
455             Any valid input for indexing.
456 
457         Returns
458         -------
459         dims: tuple
460             Dimension of the resultant variable.
461         indexers: IndexingTuple subclass
462             Tuple of integer, array-like, or slices to use when indexing
463             self._data. The type of this argument indicates the type of
464             indexing to perform, either basic, outer or vectorized.
465         new_order : Optional[Sequence[int]]
466             Optional reordering to do on the result of indexing. If not None,
467             the first len(new_order) indexing should be moved to these
468             positions.
469         """
470         key = self._item_key_to_tuple(key)  # key is a tuple
471         # key is a tuple of full size
472         key = indexing.expanded_indexer(key, self.ndim)
473         # Convert a scalar Variable to an integer
474         key = tuple(
475             k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k
476             for k in key)
477         # Convert a 0d-array to an integer
478         key = tuple(
479             k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k
480             for k in key)
481 
482         if all(isinstance(k, BASIC_INDEXING_TYPES) for k in key):
483             return self._broadcast_indexes_basic(key)
484 
485         self._validate_indexers(key)
486         # Detect it can be mapped as an outer indexer
487         # If all key is unlabeled, or
488         # key can be mapped as an OuterIndexer.
489         if all(not isinstance(k, Variable) for k in key):
490             return self._broadcast_indexes_outer(key)
491 
492         # If all key is 1-dimensional and there are no duplicate labels,
493         # key can be mapped as an OuterIndexer.
494         dims = []
495         for k, d in zip(key, self.dims):
496             if isinstance(k, Variable):
497                 if len(k.dims) > 1:
498                     return self._broadcast_indexes_vectorized(key)
499                 dims.append(k.dims[0])
500             elif not isinstance(k, integer_types):
501                 dims.append(d)
502         if len(set(dims)) == len(dims):
503             return self._broadcast_indexes_outer(key)
504 
505         return self._broadcast_indexes_vectorized(key)
506 
507     def _broadcast_indexes_basic(self, key):
508         dims = tuple(dim for k, dim in zip(key, self.dims)
509                      if not isinstance(k, integer_types))
510         return dims, BasicIndexer(key), None
511 
512     def _validate_indexers(self, key):
513         """ Make sanity checks """
514         for dim, k in zip(self.dims, key):
515             if isinstance(k, BASIC_INDEXING_TYPES):
516                 pass
517             else:
518                 if not isinstance(k, Variable):
519                     k = np.asarray(k)
520                     if k.ndim > 1:
521                         raise IndexError(
522                             "Unlabeled multi-dimensional array cannot be "
523                             "used for indexing: {}".format(k))
524                 if k.dtype.kind == 'b':
525                     if self.shape[self.get_axis_num(dim)] != len(k):
526                         raise IndexError(
527                             "Boolean array size {0:d} is used to index array "
528                             "with shape {1:s}.".format(len(k),
529                                                        str(self.shape)))
530                     if k.ndim > 1:
531                         raise IndexError("{}-dimensional boolean indexing is "
532                                          "not supported. ".format(k.ndim))
533                     if getattr(k, 'dims', (dim, )) != (dim, ):
534                         raise IndexError(
535                             "Boolean indexer should be unlabeled or on the "
536                             "same dimension to the indexed array. Indexer is "
537                             "on {0:s} but the target dimension is "
538                             "{1:s}.".format(str(k.dims), dim))
539 
540     def _broadcast_indexes_outer(self, key):
541         dims = tuple(k.dims[0] if isinstance(k, Variable) else dim
542                      for k, dim in zip(key, self.dims)
543                      if not isinstance(k, integer_types))
544 
545         new_key = []
546         for k in key:
547             if isinstance(k, Variable):
548                 k = k.data
549             if not isinstance(k, BASIC_INDEXING_TYPES):
550                 k = np.asarray(k)
551                 if k.dtype.kind == 'b':
552                     (k,) = np.nonzero(k)
553             new_key.append(k)
554 
555         return dims, OuterIndexer(tuple(new_key)), None
556 
557     def _nonzero(self):
558         """ Equivalent numpy's nonzero but returns a tuple of Varibles. """
559         # TODO we should replace dask's native nonzero
560         # after https://github.com/dask/dask/issues/1076 is implemented.
561         nonzeros = np.nonzero(self.data)
562         return tuple(Variable((dim), nz) for nz, dim
563                      in zip(nonzeros, self.dims))
564 
565     def _broadcast_indexes_vectorized(self, key):
566         variables = []
567         out_dims_set = OrderedSet()
568         for dim, value in zip(self.dims, key):
569             if isinstance(value, slice):
570                 out_dims_set.add(dim)
571             else:
572                 variable = (value if isinstance(value, Variable) else
573                             as_variable(value, name=dim))
574                 if variable.dtype.kind == 'b':  # boolean indexing case
575                     (variable,) = variable._nonzero()
576 
577                 variables.append(variable)
578                 out_dims_set.update(variable.dims)
579 
580         variable_dims = set()
581         for variable in variables:
582             variable_dims.update(variable.dims)
583 
584         slices = []
585         for i, (dim, value) in enumerate(zip(self.dims, key)):
586             if isinstance(value, slice):
587                 if dim in variable_dims:
588                     # We only convert slice objects to variables if they share
589                     # a dimension with at least one other variable. Otherwise,
590                     # we can equivalently leave them as slices aknd transpose
591                     # the result. This is significantly faster/more efficient
592                     # for most array backends.
593                     values = np.arange(*value.indices(self.sizes[dim]))
594                     variables.insert(i - len(slices), Variable((dim,), values))
595                 else:
596                     slices.append((i, value))
597 
598         try:
599             variables = _broadcast_compat_variables(*variables)
600         except ValueError:
601             raise IndexError("Dimensions of indexers mismatch: {}".format(key))
602 
603         out_key = [variable.data for variable in variables]
604         out_dims = tuple(out_dims_set)
605         slice_positions = set()
606         for i, value in slices:
607             out_key.insert(i, value)
608             new_position = out_dims.index(self.dims[i])
609             slice_positions.add(new_position)
610 
611         if slice_positions:
612             new_order = [i for i in range(len(out_dims))
613                          if i not in slice_positions]
614         else:
615             new_order = None
616 
617         return out_dims, VectorizedIndexer(tuple(out_key)), new_order
618 
619     def __getitem__(self, key):
620         """Return a new Array object whose contents are consistent with
621         getting the provided key from the underlying data.
622 
623         NB. __getitem__ and __setitem__ implement xarray-style indexing,
624         where if keys are unlabeled arrays, we index the array orthogonally
625         with them. If keys are labeled array (such as Variables), they are
626         broadcasted with our usual scheme and then the array is indexed with
627         the broadcasted key, like numpy's fancy indexing.
628 
629         If you really want to do indexing like `x[x > 0]`, manipulate the numpy
630         array `x.values` directly.
631         """
632         dims, indexer, new_order = self._broadcast_indexes(key)
633         data = as_indexable(self._data)[indexer]
634         if new_order:
635             data = duck_array_ops.moveaxis(
636                 data, range(len(new_order)), new_order)
637         return self._finalize_indexing_result(dims, data)
638 
639     def _finalize_indexing_result(self, dims, data):
640         """Used by IndexVariable to return IndexVariable objects when possible.
641         """
642         return type(self)(dims, data, self._attrs, self._encoding,
643                           fastpath=True)
644 
645     def _getitem_with_mask(self, key, fill_value=dtypes.NA):
646         """Index this Variable with -1 remapped to fill_value."""
647         # TODO(shoyer): expose this method in public API somewhere (isel?) and
648         # use it for reindex.
649         # TODO(shoyer): add a sanity check that all other integers are
650         # non-negative
651         # TODO(shoyer): add an optimization, remapping -1 to an adjacent value
652         # that is actually indexed rather than mapping it to the last value
653         # along each axis.
654 
655         if fill_value is dtypes.NA:
656             fill_value = dtypes.get_fill_value(self.dtype)
657 
658         dims, indexer, new_order = self._broadcast_indexes(key)
659 
660         if self.size:
661             if isinstance(self._data, dask_array_type):
662                 # dask's indexing is faster this way; also vindex does not
663                 # support negative indices yet:
664                 # https://github.com/dask/dask/pull/2967
665                 actual_indexer = indexing.posify_mask_indexer(indexer)
666             else:
667                 actual_indexer = indexer
668 
669             data = as_indexable(self._data)[actual_indexer]
670             chunks_hint = getattr(data, 'chunks', None)
671             mask = indexing.create_mask(indexer, self.shape, chunks_hint)
672             data = duck_array_ops.where(mask, fill_value, data)
673         else:
674             # array cannot be indexed along dimensions of size 0, so just
675             # build the mask directly instead.
676             mask = indexing.create_mask(indexer, self.shape)
677             data = np.broadcast_to(fill_value, getattr(mask, 'shape', ()))
678 
679         if new_order:
680             data = duck_array_ops.moveaxis(
681                 data, range(len(new_order)), new_order)
682         return self._finalize_indexing_result(dims, data)
683 
684     def __setitem__(self, key, value):
685         """__setitem__ is overloaded to access the underlying numpy values with
686         orthogonal indexing.
687 
688         See __getitem__ for more details.
689         """
690         dims, index_tuple, new_order = self._broadcast_indexes(key)
691 
692         if not isinstance(value, Variable):
693             value = as_compatible_data(value)
694             if value.ndim > len(dims):
695                 raise ValueError(
696                     'shape mismatch: value array of shape %s could not be '
697                     'broadcast to indexing result with %s dimensions'
698                     % (value.shape, len(dims)))
699             if value.ndim == 0:
700                 value = Variable((), value)
701             else:
702                 value = Variable(dims[-value.ndim:], value)
703         # broadcast to become assignable
704         value = value.set_dims(dims).data
705 
706         if new_order:
707             value = duck_array_ops.asarray(value)
708             value = value[(len(dims) - value.ndim) * (np.newaxis,) +
709                           (Ellipsis,)]
710             value = duck_array_ops.moveaxis(
711                 value, new_order, range(len(new_order)))
712 
713         indexable = as_indexable(self._data)
714         indexable[index_tuple] = value
715 
716     @property
717     def attrs(self) -> 'OrderedDict[Any, Any]':
718         """Dictionary of local attributes on this variable.
719         """
720         if self._attrs is None:
721             self._attrs = OrderedDict()
722         return self._attrs
723 
724     @attrs.setter
725     def attrs(self, value: Mapping[Hashable, Any]) -> None:
726         self._attrs = OrderedDict(value)
727 
728     @property
729     def encoding(self):
730         """Dictionary of encodings on this variable.
731         """
732         if self._encoding is None:
733             self._encoding = {}
734         return self._encoding
735 
736     @encoding.setter
737     def encoding(self, value):
738         try:
739             self._encoding = dict(value)
740         except ValueError:
741             raise ValueError('encoding must be castable to a dictionary')
742 
743     def copy(self, deep=True, data=None):
744         """Returns a copy of this object.
745 
746         If `deep=True`, the data array is loaded into memory and copied onto
747         the new object. Dimensions, attributes and encodings are always copied.
748 
749         Use `data` to create a new object with the same structure as
750         original but entirely new data.
751 
752         Parameters
753         ----------
754         deep : bool, optional
755             Whether the data array is loaded into memory and copied onto
756             the new object. Default is True.
757         data : array_like, optional
758             Data to use in the new object. Must have same shape as original.
759             When `data` is used, `deep` is ignored.
760 
761         Returns
762         -------
763         object : Variable
764             New object with dimensions, attributes, encodings, and optionally
765             data copied from original.
766 
767         Examples
768         --------
769 
770         Shallow copy versus deep copy
771 
772         >>> var = xr.Variable(data=[1, 2, 3], dims='x')
773         >>> var.copy()
774         <xarray.Variable (x: 3)>
775         array([1, 2, 3])
776         >>> var_0 = var.copy(deep=False)
777         >>> var_0[0] = 7
778         >>> var_0
779         <xarray.Variable (x: 3)>
780         array([7, 2, 3])
781         >>> var
782         <xarray.Variable (x: 3)>
783         array([7, 2, 3])
784 
785         Changing the data using the ``data`` argument maintains the
786         structure of the original object, but with the new data. Original
787         object is unaffected.
788 
789         >>> var.copy(data=[0.1, 0.2, 0.3])
790         <xarray.Variable (x: 3)>
791         array([ 0.1,  0.2,  0.3])
792         >>> var
793         <xarray.Variable (x: 3)>
794         array([7, 2, 3])
795 
796         See Also
797         --------
798         pandas.DataFrame.copy
799         """
800         if data is None:
801             data = self._data
802 
803             if isinstance(data, indexing.MemoryCachedArray):
804                 # don't share caching between copies
805                 data = indexing.MemoryCachedArray(data.array)
806 
807             if deep:
808                 if isinstance(data, dask_array_type):
809                     data = data.copy()
810                 elif not isinstance(data, PandasIndexAdapter):
811                     # pandas.Index is immutable
812                     data = np.array(data)
813         else:
814             data = as_compatible_data(data)
815             if self.shape != data.shape:
816                 raise ValueError("Data shape {} must match shape of object {}"
817                                  .format(data.shape, self.shape))
818 
819         # note:
820         # dims is already an immutable tuple
821         # attributes and encoding will be copied when the new Array is created
822         return type(self)(self.dims, data, self._attrs, self._encoding,
823                           fastpath=True)
824 
825     def __copy__(self):
826         return self.copy(deep=False)
827 
828     def __deepcopy__(self, memo=None):
829         # memo does nothing but is required for compatibility with
830         # copy.deepcopy
831         return self.copy(deep=True)
832 
833     # mutable objects should not be hashable
834     # https://github.com/python/mypy/issues/4266
835     __hash__ = None  # type: ignore
836 
837     @property
838     def chunks(self):
839         """Block dimensions for this array's data or None if it's not a dask
840         array.
841         """
842         return getattr(self._data, 'chunks', None)
843 
844     _array_counter = itertools.count()
845 
846     def chunk(self, chunks=None, name=None, lock=False):
847         """Coerce this array's data into a dask arrays with the given chunks.
848 
849         If this variable is a non-dask array, it will be converted to dask
850         array. If it's a dask array, it will be rechunked to the given chunk
851         sizes.
852 
853         If neither chunks is not provided for one or more dimensions, chunk
854         sizes along that dimension will not be updated; non-dask arrays will be
855         converted into dask arrays with a single block.
856 
857         Parameters
858         ----------
859         chunks : int, tuple or dict, optional
860             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
861             ``{'x': 5, 'y': 5}``.
862         name : str, optional
863             Used to generate the name for this array in the internal dask
864             graph. Does not need not be unique.
865         lock : optional
866             Passed on to :py:func:`dask.array.from_array`, if the array is not
867             already as dask array.
868 
869         Returns
870         -------
871         chunked : xarray.Variable
872         """
873         import dask
874         import dask.array as da
875 
876         if utils.is_dict_like(chunks):
877             chunks = dict((self.get_axis_num(dim), chunk)
878                           for dim, chunk in chunks.items())
879 
880         if chunks is None:
881             chunks = self.chunks or self.shape
882 
883         data = self._data
884         if isinstance(data, da.Array):
885             data = data.rechunk(chunks)
886         else:
887             if utils.is_dict_like(chunks):
888                 chunks = tuple(chunks.get(n, s)
889                                for n, s in enumerate(self.shape))
890             # da.from_array works by using lazily indexing with a tuple of
891             # slices. Using OuterIndexer is a pragmatic choice: dask does not
892             # yet handle different indexing types in an explicit way:
893             # https://github.com/dask/dask/issues/2883
894             data = indexing.ImplicitToExplicitIndexingAdapter(
895                 data, indexing.OuterIndexer)
896 
897             # For now, assume that all arrays that we wrap with dask (including
898             # our lazily loaded backend array classes) should use NumPy array
899             # operations.
900             if LooseVersion(dask.__version__) > '1.2.2':
901                 kwargs = dict(meta=np.ndarray)
902             else:
903                 kwargs = dict()
904 
905             data = da.from_array(
906                 data, chunks, name=name, lock=lock, **kwargs)
907 
908         return type(self)(self.dims, data, self._attrs, self._encoding,
909                           fastpath=True)
910 
911     def isel(self, indexers=None, drop=False, **indexers_kwargs):
912         """Return a new array indexed along the specified dimension(s).
913 
914         Parameters
915         ----------
916         **indexers : {dim: indexer, ...}
917             Keyword arguments with names matching dimensions and values given
918             by integers, slice objects or arrays.
919 
920         Returns
921         -------
922         obj : Array object
923             A new Array with the selected data and dimensions. In general,
924             the new variable's data will be a view of this variable's data,
925             unless numpy fancy indexing was triggered by using an array
926             indexer, in which case the data will be a copy.
927         """
928         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'isel')
929 
930         invalid = [k for k in indexers if k not in self.dims]
931         if invalid:
932             raise ValueError("dimensions %r do not exist" % invalid)
933 
934         key = [slice(None)] * self.ndim
935         for i, dim in enumerate(self.dims):
936             if dim in indexers:
937                 key[i] = indexers[dim]
938         return self[tuple(key)]
939 
940     def squeeze(self, dim=None):
941         """Return a new object with squeezed data.
942 
943         Parameters
944         ----------
945         dim : None or str or tuple of str, optional
946             Selects a subset of the length one dimensions. If a dimension is
947             selected with length greater than one, an error is raised. If
948             None, all length one dimensions are squeezed.
949 
950         Returns
951         -------
952         squeezed : same type as caller
953             This object, but with with all or a subset of the dimensions of
954             length 1 removed.
955 
956         See Also
957         --------
958         numpy.squeeze
959         """
960         dims = common.get_squeeze_dims(self, dim)
961         return self.isel({d: 0 for d in dims})
962 
963     def _shift_one_dim(self, dim, count, fill_value=dtypes.NA):
964         axis = self.get_axis_num(dim)
965 
966         if count > 0:
967             keep = slice(None, -count)
968         elif count < 0:
969             keep = slice(-count, None)
970         else:
971             keep = slice(None)
972 
973         trimmed_data = self[(slice(None),) * axis + (keep,)].data
974 
975         if fill_value is dtypes.NA:
976             dtype, fill_value = dtypes.maybe_promote(self.dtype)
977         else:
978             dtype = self.dtype
979 
980         shape = list(self.shape)
981         shape[axis] = min(abs(count), shape[axis])
982 
983         if isinstance(trimmed_data, dask_array_type):
984             chunks = list(trimmed_data.chunks)
985             chunks[axis] = (shape[axis],)
986             full = functools.partial(da.full, chunks=chunks)
987         else:
988             full = np.full
989 
990         filler = full(shape, fill_value, dtype=dtype)
991 
992         if count > 0:
993             arrays = [filler, trimmed_data]
994         else:
995             arrays = [trimmed_data, filler]
996 
997         data = duck_array_ops.concatenate(arrays, axis)
998 
999         if isinstance(data, dask_array_type):
1000             # chunked data should come out with the same chunks; this makes
1001             # it feasible to combine shifted and unshifted data
1002             # TODO: remove this once dask.array automatically aligns chunks
1003             data = data.rechunk(self.data.chunks)
1004 
1005         return type(self)(self.dims, data, self._attrs, fastpath=True)
1006 
1007     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):
1008         """
1009         Return a new Variable with shifted data.
1010 
1011         Parameters
1012         ----------
1013         shifts : mapping of the form {dim: offset}
1014             Integer offset to shift along each of the given dimensions.
1015             Positive offsets shift to the right; negative offsets shift to the
1016             left.
1017         fill_value: scalar, optional
1018             Value to use for newly missing values
1019         **shifts_kwargs:
1020             The keyword arguments form of ``shifts``.
1021             One of shifts or shifts_kwarg must be provided.
1022 
1023         Returns
1024         -------
1025         shifted : Variable
1026             Variable with the same dimensions and attributes but shifted data.
1027         """
1028         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, 'shift')
1029         result = self
1030         for dim, count in shifts.items():
1031             result = result._shift_one_dim(dim, count, fill_value=fill_value)
1032         return result
1033 
1034     def pad_with_fill_value(self, pad_widths=None, fill_value=dtypes.NA,
1035                             **pad_widths_kwargs):
1036         """
1037         Return a new Variable with paddings.
1038 
1039         Parameters
1040         ----------
1041         pad_width: Mapping of the form {dim: (before, after)}
1042             Number of values padded to the edges of each dimension.
1043         **pad_widths_kwargs:
1044             Keyword argument for pad_widths
1045         """
1046         pad_widths = either_dict_or_kwargs(pad_widths, pad_widths_kwargs,
1047                                            'pad')
1048 
1049         if fill_value is dtypes.NA:
1050             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1051         else:
1052             dtype = self.dtype
1053 
1054         if isinstance(self.data, dask_array_type):
1055             array = self.data
1056 
1057             # Dask does not yet support pad. We manually implement it.
1058             # https://github.com/dask/dask/issues/1926
1059             for d, pad in pad_widths.items():
1060                 axis = self.get_axis_num(d)
1061                 before_shape = list(array.shape)
1062                 before_shape[axis] = pad[0]
1063                 before_chunks = list(array.chunks)
1064                 before_chunks[axis] = (pad[0], )
1065                 after_shape = list(array.shape)
1066                 after_shape[axis] = pad[1]
1067                 after_chunks = list(array.chunks)
1068                 after_chunks[axis] = (pad[1], )
1069 
1070                 arrays = []
1071                 if pad[0] > 0:
1072                     arrays.append(da.full(before_shape, fill_value,
1073                                           dtype=dtype, chunks=before_chunks))
1074                 arrays.append(array)
1075                 if pad[1] > 0:
1076                     arrays.append(da.full(after_shape, fill_value,
1077                                           dtype=dtype, chunks=after_chunks))
1078                 if len(arrays) > 1:
1079                     array = da.concatenate(arrays, axis=axis)
1080         else:
1081             pads = [(0, 0) if d not in pad_widths else pad_widths[d]
1082                     for d in self.dims]
1083             array = np.pad(self.data.astype(dtype, copy=False), pads,
1084                            mode='constant', constant_values=fill_value)
1085         return type(self)(self.dims, array)
1086 
1087     def _roll_one_dim(self, dim, count):
1088         axis = self.get_axis_num(dim)
1089 
1090         count %= self.shape[axis]
1091         if count != 0:
1092             indices = [slice(-count, None), slice(None, -count)]
1093         else:
1094             indices = [slice(None)]
1095 
1096         arrays = [self[(slice(None),) * axis + (idx,)].data
1097                   for idx in indices]
1098 
1099         data = duck_array_ops.concatenate(arrays, axis)
1100 
1101         if isinstance(data, dask_array_type):
1102             # chunked data should come out with the same chunks; this makes
1103             # it feasible to combine shifted and unshifted data
1104             # TODO: remove this once dask.array automatically aligns chunks
1105             data = data.rechunk(self.data.chunks)
1106 
1107         return type(self)(self.dims, data, self._attrs, fastpath=True)
1108 
1109     def roll(self, shifts=None, **shifts_kwargs):
1110         """
1111         Return a new Variable with rolld data.
1112 
1113         Parameters
1114         ----------
1115         shifts : mapping of the form {dim: offset}
1116             Integer offset to roll along each of the given dimensions.
1117             Positive offsets roll to the right; negative offsets roll to the
1118             left.
1119         **shifts_kwargs:
1120             The keyword arguments form of ``shifts``.
1121             One of shifts or shifts_kwarg must be provided.
1122 
1123         Returns
1124         -------
1125         shifted : Variable
1126             Variable with the same dimensions and attributes but rolled data.
1127         """
1128         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, 'roll')
1129 
1130         result = self
1131         for dim, count in shifts.items():
1132             result = result._roll_one_dim(dim, count)
1133         return result
1134 
1135     def transpose(self, *dims) -> 'Variable':
1136         """Return a new Variable object with transposed dimensions.
1137 
1138         Parameters
1139         ----------
1140         *dims : str, optional
1141             By default, reverse the dimensions. Otherwise, reorder the
1142             dimensions to this order.
1143 
1144         Returns
1145         -------
1146         transposed : Variable
1147             The returned object has transposed data and dimensions with the
1148             same attributes as the original.
1149 
1150         Notes
1151         -----
1152         This operation returns a view of this variable's data. It is
1153         lazy for dask-backed Variables but not for numpy-backed Variables.
1154 
1155         See Also
1156         --------
1157         numpy.transpose
1158         """
1159         if len(dims) == 0:
1160             dims = self.dims[::-1]
1161         axes = self.get_axis_num(dims)
1162         if len(dims) < 2:  # no need to transpose if only one dimension
1163             return self.copy(deep=False)
1164 
1165         data = as_indexable(self._data).transpose(axes)
1166         return type(self)(dims, data, self._attrs, self._encoding,
1167                           fastpath=True)
1168 
1169     @property
1170     def T(self) -> 'Variable':
1171         return self.transpose()
1172 
1173     def expand_dims(self, *args):
1174         import warnings
1175         warnings.warn('Variable.expand_dims is deprecated: use '
1176                       'Variable.set_dims instead', DeprecationWarning,
1177                       stacklevel=2)
1178         return self.expand_dims(*args)
1179 
1180     def set_dims(self, dims, shape=None):
1181         """Return a new variable with given set of dimensions.
1182         This method might be used to attach new dimension(s) to variable.
1183 
1184         When possible, this operation does not copy this variable's data.
1185 
1186         Parameters
1187         ----------
1188         dims : str or sequence of str or dict
1189             Dimensions to include on the new variable. If a dict, values are
1190             used to provide the sizes of new dimensions; otherwise, new
1191             dimensions are inserted with length 1.
1192 
1193         Returns
1194         -------
1195         Variable
1196         """
1197         if isinstance(dims, str):
1198             dims = [dims]
1199 
1200         if shape is None and utils.is_dict_like(dims):
1201             shape = dims.values()
1202 
1203         missing_dims = set(self.dims) - set(dims)
1204         if missing_dims:
1205             raise ValueError('new dimensions %r must be a superset of '
1206                              'existing dimensions %r' % (dims, self.dims))
1207 
1208         self_dims = set(self.dims)
1209         expanded_dims = tuple(
1210             d for d in dims if d not in self_dims) + self.dims
1211 
1212         if self.dims == expanded_dims:
1213             # don't use broadcast_to unless necessary so the result remains
1214             # writeable if possible
1215             expanded_data = self.data
1216         elif shape is not None:
1217             dims_map = dict(zip(dims, shape))
1218             tmp_shape = tuple(dims_map[d] for d in expanded_dims)
1219             expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)
1220         else:
1221             expanded_data = self.data[
1222                 (None,) * (len(expanded_dims) - self.ndim)]
1223 
1224         expanded_var = Variable(expanded_dims, expanded_data, self._attrs,
1225                                 self._encoding, fastpath=True)
1226         return expanded_var.transpose(*dims)
1227 
1228     def _stack_once(self, dims, new_dim):
1229         if not set(dims) <= set(self.dims):
1230             raise ValueError('invalid existing dimensions: %s' % dims)
1231 
1232         if new_dim in self.dims:
1233             raise ValueError('cannot create a new dimension with the same '
1234                              'name as an existing dimension')
1235 
1236         if len(dims) == 0:
1237             # don't stack
1238             return self.copy(deep=False)
1239 
1240         other_dims = [d for d in self.dims if d not in dims]
1241         dim_order = other_dims + list(dims)
1242         reordered = self.transpose(*dim_order)
1243 
1244         new_shape = reordered.shape[:len(other_dims)] + (-1,)
1245         new_data = reordered.data.reshape(new_shape)
1246         new_dims = reordered.dims[:len(other_dims)] + (new_dim,)
1247 
1248         return Variable(new_dims, new_data, self._attrs, self._encoding,
1249                         fastpath=True)
1250 
1251     def stack(self, dimensions=None, **dimensions_kwargs):
1252         """
1253         Stack any number of existing dimensions into a single new dimension.
1254 
1255         New dimensions will be added at the end, and the order of the data
1256         along each new dimension will be in contiguous (C) order.
1257 
1258         Parameters
1259         ----------
1260         dimensions : Mapping of form new_name=(dim1, dim2, ...)
1261             Names of new dimensions, and the existing dimensions that they
1262             replace.
1263         **dimensions_kwargs:
1264             The keyword arguments form of ``dimensions``.
1265             One of dimensions or dimensions_kwargs must be provided.
1266 
1267         Returns
1268         -------
1269         stacked : Variable
1270             Variable with the same attributes but stacked data.
1271 
1272         See also
1273         --------
1274         Variable.unstack
1275         """
1276         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs,
1277                                            'stack')
1278         result = self
1279         for new_dim, dims in dimensions.items():
1280             result = result._stack_once(dims, new_dim)
1281         return result
1282 
1283     def _unstack_once(self, dims, old_dim):
1284         new_dim_names = tuple(dims.keys())
1285         new_dim_sizes = tuple(dims.values())
1286 
1287         if old_dim not in self.dims:
1288             raise ValueError('invalid existing dimension: %s' % old_dim)
1289 
1290         if set(new_dim_names).intersection(self.dims):
1291             raise ValueError('cannot create a new dimension with the same '
1292                              'name as an existing dimension')
1293 
1294         if np.prod(new_dim_sizes) != self.sizes[old_dim]:
1295             raise ValueError('the product of the new dimension sizes must '
1296                              'equal the size of the old dimension')
1297 
1298         other_dims = [d for d in self.dims if d != old_dim]
1299         dim_order = other_dims + [old_dim]
1300         reordered = self.transpose(*dim_order)
1301 
1302         new_shape = reordered.shape[:len(other_dims)] + new_dim_sizes
1303         new_data = reordered.data.reshape(new_shape)
1304         new_dims = reordered.dims[:len(other_dims)] + new_dim_names
1305 
1306         return Variable(new_dims, new_data, self._attrs, self._encoding,
1307                         fastpath=True)
1308 
1309     def unstack(self, dimensions=None, **dimensions_kwargs):
1310         """
1311         Unstack an existing dimension into multiple new dimensions.
1312 
1313         New dimensions will be added at the end, and the order of the data
1314         along each new dimension will be in contiguous (C) order.
1315 
1316         Parameters
1317         ----------
1318         dimensions : mapping of the form old_dim={dim1: size1, ...}
1319             Names of existing dimensions, and the new dimensions and sizes
1320             that they map to.
1321         **dimensions_kwargs:
1322             The keyword arguments form of ``dimensions``.
1323             One of dimensions or dimensions_kwargs must be provided.
1324 
1325         Returns
1326         -------
1327         unstacked : Variable
1328             Variable with the same attributes but unstacked data.
1329 
1330         See also
1331         --------
1332         Variable.stack
1333         """
1334         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs,
1335                                            'unstack')
1336         result = self
1337         for old_dim, dims in dimensions.items():
1338             result = result._unstack_once(dims, old_dim)
1339         return result
1340 
1341     def fillna(self, value):
1342         return ops.fillna(self, value)
1343 
1344     def where(self, cond, other=dtypes.NA):
1345         return ops.where_method(self, cond, other)
1346 
1347     def reduce(self, func, dim=None, axis=None,
1348                keep_attrs=None, keepdims=False, allow_lazy=False, **kwargs):
1349         """Reduce this array by applying `func` along some dimension(s).
1350 
1351         Parameters
1352         ----------
1353         func : function
1354             Function which can be called in the form
1355             `func(x, axis=axis, **kwargs)` to return the result of reducing an
1356             np.ndarray over an integer valued axis.
1357         dim : str or sequence of str, optional
1358             Dimension(s) over which to apply `func`.
1359         axis : int or sequence of int, optional
1360             Axis(es) over which to apply `func`. Only one of the 'dim'
1361             and 'axis' arguments can be supplied. If neither are supplied, then
1362             the reduction is calculated over the flattened array (by calling
1363             `func(x)` without an axis argument).
1364         keep_attrs : bool, optional
1365             If True, the variable's attributes (`attrs`) will be copied from
1366             the original object to the new one.  If False (default), the new
1367             object will be returned without attributes.
1368         keepdims : bool, default False
1369             If True, the dimensions which are reduced are left in the result
1370             as dimensions of size one
1371         **kwargs : dict
1372             Additional keyword arguments passed on to `func`.
1373 
1374         Returns
1375         -------
1376         reduced : Array
1377             Array with summarized data and the indicated dimension(s)
1378             removed.
1379         """
1380         if dim is common.ALL_DIMS:
1381             dim = None
1382         if dim is not None and axis is not None:
1383             raise ValueError("cannot supply both 'axis' and 'dim' arguments")
1384 
1385         if dim is not None:
1386             axis = self.get_axis_num(dim)
1387         input_data = self.data if allow_lazy else self.values
1388         if axis is not None:
1389             data = func(input_data, axis=axis, **kwargs)
1390         else:
1391             data = func(input_data, **kwargs)
1392 
1393         if getattr(data, 'shape', ()) == self.shape:
1394             dims = self.dims
1395         else:
1396             removed_axes = (range(self.ndim) if axis is None
1397                             else np.atleast_1d(axis) % self.ndim)
1398             if keepdims:
1399                 # Insert np.newaxis for removed dims
1400                 slices = tuple(np.newaxis if i in removed_axes else
1401                                slice(None, None) for i in range(self.ndim))
1402                 if getattr(data, 'shape', None) is None:
1403                     # Reduce has produced a scalar value, not an array-like
1404                     data = np.asanyarray(data)[slices]
1405                 else:
1406                     data = data[slices]
1407                 dims = self.dims
1408             else:
1409                 dims = [adim for n, adim in enumerate(self.dims)
1410                         if n not in removed_axes]
1411 
1412         if keep_attrs is None:
1413             keep_attrs = _get_keep_attrs(default=False)
1414         attrs = self._attrs if keep_attrs else None
1415 
1416         return Variable(dims, data, attrs=attrs)
1417 
1418     @classmethod
1419     def concat(cls, variables, dim='concat_dim', positions=None,
1420                shortcut=False):
1421         """Concatenate variables along a new or existing dimension.
1422 
1423         Parameters
1424         ----------
1425         variables : iterable of Array
1426             Arrays to stack together. Each variable is expected to have
1427             matching dimensions and shape except for along the stacked
1428             dimension.
1429         dim : str or DataArray, optional
1430             Name of the dimension to stack along. This can either be a new
1431             dimension name, in which case it is added along axis=0, or an
1432             existing dimension name, in which case the location of the
1433             dimension is unchanged. Where to insert the new dimension is
1434             determined by the first variable.
1435         positions : None or list of integer arrays, optional
1436             List of integer arrays which specifies the integer positions to
1437             which to assign each dataset along the concatenated dimension.
1438             If not supplied, objects are concatenated in the provided order.
1439         shortcut : bool, optional
1440             This option is used internally to speed-up groupby operations.
1441             If `shortcut` is True, some checks of internal consistency between
1442             arrays to concatenate are skipped.
1443 
1444         Returns
1445         -------
1446         stacked : Variable
1447             Concatenated Variable formed by stacking all the supplied variables
1448             along the given dimension.
1449         """
1450         if not isinstance(dim, str):
1451             dim, = dim.dims
1452 
1453         # can't do this lazily: we need to loop through variables at least
1454         # twice
1455         variables = list(variables)
1456         first_var = variables[0]
1457 
1458         arrays = [v.data for v in variables]
1459 
1460         if dim in first_var.dims:
1461             axis = first_var.get_axis_num(dim)
1462             dims = first_var.dims
1463             data = duck_array_ops.concatenate(arrays, axis=axis)
1464             if positions is not None:
1465                 # TODO: deprecate this option -- we don't need it for groupby
1466                 # any more.
1467                 indices = nputils.inverse_permutation(
1468                     np.concatenate(positions))
1469                 data = duck_array_ops.take(data, indices, axis=axis)
1470         else:
1471             axis = 0
1472             dims = (dim,) + first_var.dims
1473             data = duck_array_ops.stack(arrays, axis=axis)
1474 
1475         attrs = OrderedDict(first_var.attrs)
1476         encoding = OrderedDict(first_var.encoding)
1477         if not shortcut:
1478             for var in variables:
1479                 if var.dims != first_var.dims:
1480                     raise ValueError('inconsistent dimensions')
1481                 utils.remove_incompatible_items(attrs, var.attrs)
1482 
1483         return cls(dims, data, attrs, encoding)
1484 
1485     def equals(self, other, equiv=duck_array_ops.array_equiv):
1486         """True if two Variables have the same dimensions and values;
1487         otherwise False.
1488 
1489         Variables can still be equal (like pandas objects) if they have NaN
1490         values in the same locations.
1491 
1492         This method is necessary because `v1 == v2` for Variables
1493         does element-wise comparisons (like numpy.ndarrays).
1494         """
1495         other = getattr(other, 'variable', other)
1496         try:
1497             return (self.dims == other.dims and
1498                     (self._data is other._data or
1499                      equiv(self.data, other.data)))
1500         except (TypeError, AttributeError):
1501             return False
1502 
1503     def broadcast_equals(self, other, equiv=duck_array_ops.array_equiv):
1504         """True if two Variables have the values after being broadcast against
1505         each other; otherwise False.
1506 
1507         Variables can still be equal (like pandas objects) if they have NaN
1508         values in the same locations.
1509         """
1510         try:
1511             self, other = broadcast_variables(self, other)
1512         except (ValueError, AttributeError):
1513             return False
1514         return self.equals(other, equiv=equiv)
1515 
1516     def identical(self, other):
1517         """Like equals, but also checks attributes.
1518         """
1519         try:
1520             return (utils.dict_equiv(self.attrs, other.attrs) and
1521                     self.equals(other))
1522         except (TypeError, AttributeError):
1523             return False
1524 
1525     def no_conflicts(self, other):
1526         """True if the intersection of two Variable's non-null data is
1527         equal; otherwise false.
1528 
1529         Variables can thus still be equal if there are locations where either,
1530         or both, contain NaN values.
1531         """
1532         return self.broadcast_equals(
1533             other, equiv=duck_array_ops.array_notnull_equiv)
1534 
1535     def quantile(self, q, dim=None, interpolation='linear'):
1536         """Compute the qth quantile of the data along the specified dimension.
1537 
1538         Returns the qth quantiles(s) of the array elements.
1539 
1540         Parameters
1541         ----------
1542         q : float in range of [0,1] (or sequence of floats)
1543             Quantile to compute, which must be between 0 and 1
1544             inclusive.
1545         dim : str or sequence of str, optional
1546             Dimension(s) over which to apply quantile.
1547         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
1548             This optional parameter specifies the interpolation method to
1549             use when the desired quantile lies between two data points
1550             ``i < j``:
1551                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is
1552                   the fractional part of the index surrounded by ``i`` and
1553                   ``j``.
1554                 * lower: ``i``.
1555                 * higher: ``j``.
1556                 * nearest: ``i`` or ``j``, whichever is nearest.
1557                 * midpoint: ``(i + j) / 2``.
1558 
1559         Returns
1560         -------
1561         quantiles : Variable
1562             If `q` is a single quantile, then the result
1563             is a scalar. If multiple percentiles are given, first axis of
1564             the result corresponds to the quantile and a quantile dimension
1565             is added to the return array. The other dimensions are the
1566              dimensions that remain after the reduction of the array.
1567 
1568         See Also
1569         --------
1570         numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile,
1571         DataArray.quantile
1572         """
1573         if isinstance(self.data, dask_array_type):
1574             raise TypeError("quantile does not work for arrays stored as dask "
1575                             "arrays. Load the data via .compute() or .load() "
1576                             "prior to calling this method.")
1577 
1578         q = np.asarray(q, dtype=np.float64)
1579 
1580         new_dims = list(self.dims)
1581         if dim is not None:
1582             axis = self.get_axis_num(dim)
1583             if utils.is_scalar(dim):
1584                 new_dims.remove(dim)
1585             else:
1586                 for d in dim:
1587                     new_dims.remove(d)
1588         else:
1589             axis = None
1590             new_dims = []
1591 
1592         # only add the quantile dimension if q is array like
1593         if q.ndim != 0:
1594             new_dims = ['quantile'] + new_dims
1595 
1596         qs = np.nanpercentile(self.data, q * 100., axis=axis,
1597                               interpolation=interpolation)
1598         return Variable(new_dims, qs)
1599 
1600     def rank(self, dim, pct=False):
1601         """Ranks the data.
1602 
1603         Equal values are assigned a rank that is the average of the ranks that
1604         would have been otherwise assigned to all of the values within that
1605         set.  Ranks begin at 1, not 0. If `pct`, computes percentage ranks.
1606 
1607         NaNs in the input array are returned as NaNs.
1608 
1609         The `bottleneck` library is required.
1610 
1611         Parameters
1612         ----------
1613         dim : str
1614             Dimension over which to compute rank.
1615         pct : bool, optional
1616             If True, compute percentage ranks, otherwise compute integer ranks.
1617 
1618         Returns
1619         -------
1620         ranked : Variable
1621 
1622         See Also
1623         --------
1624         Dataset.rank, DataArray.rank
1625         """
1626         import bottleneck as bn
1627 
1628         if isinstance(self.data, dask_array_type):
1629             raise TypeError("rank does not work for arrays stored as dask "
1630                             "arrays. Load the data via .compute() or .load() "
1631                             "prior to calling this method.")
1632 
1633         axis = self.get_axis_num(dim)
1634         func = bn.nanrankdata if self.dtype.kind == 'f' else bn.rankdata
1635         ranked = func(self.data, axis=axis)
1636         if pct:
1637             count = np.sum(~np.isnan(self.data), axis=axis, keepdims=True)
1638             ranked /= count
1639         return Variable(self.dims, ranked)
1640 
1641     def rolling_window(self, dim, window, window_dim, center=False,
1642                        fill_value=dtypes.NA):
1643         """
1644         Make a rolling_window along dim and add a new_dim to the last place.
1645 
1646         Parameters
1647         ----------
1648         dim: str
1649             Dimension over which to compute rolling_window
1650         window: int
1651             Window size of the rolling
1652         window_dim: str
1653             New name of the window dimension.
1654         center: boolean. default False.
1655             If True, pad fill_value for both ends. Otherwise, pad in the head
1656             of the axis.
1657         fill_value:
1658             value to be filled.
1659 
1660         Returns
1661         -------
1662         Variable that is a view of the original array with a added dimension of
1663         size w.
1664         The return dim: self.dims + (window_dim, )
1665         The return shape: self.shape + (window, )
1666 
1667         Examples
1668         --------
1669         >>> v=Variable(('a', 'b'), np.arange(8).reshape((2,4)))
1670         >>> v.rolling_window(x, 'b', 3, 'window_dim')
1671         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
1672         array([[[nan, nan, 0], [nan, 0, 1], [0, 1, 2], [1, 2, 3]],
1673                [[nan, nan, 4], [nan, 4, 5], [4, 5, 6], [5, 6, 7]]])
1674 
1675         >>> v.rolling_window(x, 'b', 3, 'window_dim', center=True)
1676         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
1677         array([[[nan, 0, 1], [0, 1, 2], [1, 2, 3], [2, 3, nan]],
1678                [[nan, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, nan]]])
1679         """
1680         if fill_value is dtypes.NA:  # np.nan is passed
1681             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1682             array = self.astype(dtype, copy=False).data
1683         else:
1684             dtype = self.dtype
1685             array = self.data
1686 
1687         new_dims = self.dims + (window_dim, )
1688         return Variable(new_dims, duck_array_ops.rolling_window(
1689             array, axis=self.get_axis_num(dim), window=window,
1690             center=center, fill_value=fill_value))
1691 
1692     def coarsen(self, windows, func, boundary='exact', side='left'):
1693         """
1694         Apply
1695         """
1696         windows = {k: v for k, v in windows.items() if k in self.dims}
1697         if not windows:
1698             return self.copy()
1699 
1700         reshaped, axes = self._coarsen_reshape(windows, boundary, side)
1701         if isinstance(func, str):
1702             name = func
1703             func = getattr(duck_array_ops, name, None)
1704             if func is None:
1705                 raise NameError('{} is not a valid method.'.format(name))
1706         return type(self)(self.dims, func(reshaped, axis=axes), self._attrs)
1707 
1708     def _coarsen_reshape(self, windows, boundary, side):
1709         """
1710         Construct a reshaped-array for corsen
1711         """
1712         if not utils.is_dict_like(boundary):
1713             boundary = {d: boundary for d in windows.keys()}
1714 
1715         if not utils.is_dict_like(side):
1716             side = {d: side for d in windows.keys()}
1717 
1718         # remove unrelated dimensions
1719         boundary = {k: v for k, v in boundary.items() if k in windows}
1720         side = {k: v for k, v in side.items() if k in windows}
1721 
1722         for d, window in windows.items():
1723             if window <= 0:
1724                 raise ValueError('window must be > 0. Given {}'.format(window))
1725 
1726         variable = self
1727         for d, window in windows.items():
1728             # trim or pad the object
1729             size = variable.shape[self._get_axis_num(d)]
1730             n = int(size / window)
1731             if boundary[d] == 'exact':
1732                 if n * window != size:
1733                     raise ValueError(
1734                         'Could not coarsen a dimension of size {} with '
1735                         'window {}'.format(size, window))
1736             elif boundary[d] == 'trim':
1737                 if side[d] == 'left':
1738                     variable = variable.isel({d: slice(0, window * n)})
1739                 else:
1740                     excess = size - window * n
1741                     variable = variable.isel({d: slice(excess, None)})
1742             elif boundary[d] == 'pad':  # pad
1743                 pad = window * n - size
1744                 if pad < 0:
1745                     pad += window
1746                 if side[d] == 'left':
1747                     pad_widths = {d: (0, pad)}
1748                 else:
1749                     pad_widths = {d: (pad, 0)}
1750                 variable = variable.pad_with_fill_value(pad_widths)
1751             else:
1752                 raise TypeError(
1753                     "{} is invalid for boundary. Valid option is 'exact', "
1754                     "'trim' and 'pad'".format(boundary[d]))
1755 
1756         shape = []
1757         axes = []
1758         axis_count = 0
1759         for i, d in enumerate(variable.dims):
1760             if d in windows:
1761                 size = variable.shape[i]
1762                 shape.append(int(size / windows[d]))
1763                 shape.append(windows[d])
1764                 axis_count += 1
1765                 axes.append(i + axis_count)
1766             else:
1767                 shape.append(variable.shape[i])
1768 
1769         return variable.data.reshape(shape), tuple(axes)
1770 
1771     @property
1772     def real(self):
1773         return type(self)(self.dims, self.data.real, self._attrs)
1774 
1775     @property
1776     def imag(self):
1777         return type(self)(self.dims, self.data.imag, self._attrs)
1778 
1779     def __array_wrap__(self, obj, context=None):
1780         return Variable(self.dims, obj)
1781 
1782     @staticmethod
1783     def _unary_op(f):
1784         @functools.wraps(f)
1785         def func(self, *args, **kwargs):
1786             with np.errstate(all='ignore'):
1787                 return self.__array_wrap__(f(self.data, *args, **kwargs))
1788         return func
1789 
1790     @staticmethod
1791     def _binary_op(f, reflexive=False, **ignored_kwargs):
1792         @functools.wraps(f)
1793         def func(self, other):
1794             if isinstance(other, (xr.DataArray, xr.Dataset)):
1795                 return NotImplemented
1796             self_data, other_data, dims = _broadcast_compat_data(self, other)
1797             keep_attrs = _get_keep_attrs(default=False)
1798             attrs = self._attrs if keep_attrs else None
1799             with np.errstate(all='ignore'):
1800                 new_data = (f(self_data, other_data)
1801                             if not reflexive
1802                             else f(other_data, self_data))
1803             result = Variable(dims, new_data, attrs=attrs)
1804             return result
1805         return func
1806 
1807     @staticmethod
1808     def _inplace_binary_op(f):
1809         @functools.wraps(f)
1810         def func(self, other):
1811             if isinstance(other, xr.Dataset):
1812                 raise TypeError('cannot add a Dataset to a Variable in-place')
1813             self_data, other_data, dims = _broadcast_compat_data(self, other)
1814             if dims != self.dims:
1815                 raise ValueError('dimensions cannot change for in-place '
1816                                  'operations')
1817             with np.errstate(all='ignore'):
1818                 self.values = f(self_data, other_data)
1819             return self
1820         return func
1821 
1822     def _to_numeric(self, offset=None, datetime_unit=None, dtype=float):
1823         """ A (private) method to convert datetime array to numeric dtype
1824         See duck_array_ops.datetime_to_numeric
1825         """
1826         numeric_array = duck_array_ops.datetime_to_numeric(
1827             self.data, offset, datetime_unit, dtype)
1828         return type(self)(self.dims, numeric_array, self._attrs)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>xarray/core/variable.py</file>
<class>IndexVariable</class>
<method>copy</method>
<code>
1920     def copy(self, deep=True, data=None):
1921         """Returns a copy of this object.
1922 
1923         `deep` is ignored since data is stored in the form of
1924         pandas.Index, which is already immutable. Dimensions, attributes
1925         and encodings are always copied.
1926 
1927         Use `data` to create a new object with the same structure as
1928         original but entirely new data.
1929 
1930         Parameters
1931         ----------
1932         deep : bool, optional
1933             Deep is ignored when data is given. Whether the data array is
1934             loaded into memory and copied onto the new object. Default is True.
1935         data : array_like, optional
1936             Data to use in the new object. Must have same shape as original.
1937 
1938         Returns
1939         -------
1940         object : Variable
1941             New object with dimensions, attributes, encodings, and optionally
1942             data copied from original.
1943         """
1944         if data is None:
1945             if deep:
1946                 # self._data should be a `PandasIndexAdapter` instance at this
1947                 # point, which doesn't have a copy method, so make a deep copy
1948                 # of the underlying `pandas.MultiIndex` and create a new
1949                 # `PandasIndexAdapter` instance with it.
1950                 data = PandasIndexAdapter(self._data.array.copy(deep=True))
1951             else:
1952                 data = self._data
1953         else:
1954             data = as_compatible_data(data)
1955             if self.shape != data.shape:
1956                 raise ValueError("Data shape {} must match shape of object {}"
1957                                  .format(data.shape, self.shape))
1958         return type(self)(self.dims, data, self._attrs,
1959                           self._encoding, fastpath=True)

</code><intended_behavior>In the IndexVariable.copy method, there should be additional logic to ensure that the dtype of index variables, particularly those with Unicode types, is preserved during the copy operation. This might require a more nuanced approach when creating a new PandasIndexAdapter from the copied pandas.Index to ensure the dtype is explicitly specified or maintained, preventing unintended changes to object dtype.</intended_behavior>, <file>xarray/core/variable.py</file>
<class>IndexVariable</class>
<method>None</method>
<code>
1834 class IndexVariable(Variable):
1835     """Wrapper for accommodating a pandas.Index in an xarray.Variable.
1836 
1837     IndexVariable preserve loaded values in the form of a pandas.Index instead
1838     of a NumPy array. Hence, their values are immutable and must always be one-
1839     dimensional.
1840 
1841     They also have a name property, which is the name of their sole dimension
1842     unless another name is given.
1843     """
1844 
1845     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):
1846         super().__init__(dims, data, attrs, encoding, fastpath)
1847         if self.ndim != 1:
1848             raise ValueError('%s objects must be 1-dimensional' %
1849                              type(self).__name__)
1850 
1851         # Unlike in Variable, always eagerly load values into memory
1852         if not isinstance(self._data, PandasIndexAdapter):
1853             self._data = PandasIndexAdapter(self._data)
1854 
1855     def load(self):
1856         # data is already loaded into memory for IndexVariable
1857         return self
1858 
1859     # https://github.com/python/mypy/issues/1465
1860     @Variable.data.setter  # type: ignore
1861     def data(self, data):
1862         Variable.data.fset(self, data)
1863         if not isinstance(self._data, PandasIndexAdapter):
1864             self._data = PandasIndexAdapter(self._data)
1865 
1866     def chunk(self, chunks=None, name=None, lock=False):
1867         # Dummy - do not chunk. This method is invoked e.g. by Dataset.chunk()
1868         return self.copy(deep=False)
1869 
1870     def _finalize_indexing_result(self, dims, data):
1871         if getattr(data, 'ndim', 0) != 1:
1872             # returns Variable rather than IndexVariable if multi-dimensional
1873             return Variable(dims, data, self._attrs, self._encoding)
1874         else:
1875             return type(self)(dims, data, self._attrs,
1876                               self._encoding, fastpath=True)
1877 
1878     def __setitem__(self, key, value):
1879         raise TypeError('%s values cannot be modified' % type(self).__name__)
1880 
1881     @classmethod
1882     def concat(cls, variables, dim='concat_dim', positions=None,
1883                shortcut=False):
1884         """Specialized version of Variable.concat for IndexVariable objects.
1885 
1886         This exists because we want to avoid converting Index objects to NumPy
1887         arrays, if possible.
1888         """
1889         if not isinstance(dim, str):
1890             dim, = dim.dims
1891 
1892         variables = list(variables)
1893         first_var = variables[0]
1894 
1895         if any(not isinstance(v, cls) for v in variables):
1896             raise TypeError('IndexVariable.concat requires that all input '
1897                             'variables be IndexVariable objects')
1898 
1899         indexes = [v._data.array for v in variables]
1900 
1901         if not indexes:
1902             data = []
1903         else:
1904             data = indexes[0].append(indexes[1:])
1905 
1906             if positions is not None:
1907                 indices = nputils.inverse_permutation(
1908                     np.concatenate(positions))
1909                 data = data.take(indices)
1910 
1911         attrs = OrderedDict(first_var.attrs)
1912         if not shortcut:
1913             for var in variables:
1914                 if var.dims != first_var.dims:
1915                     raise ValueError('inconsistent dimensions')
1916                 utils.remove_incompatible_items(attrs, var.attrs)
1917 
1918         return cls(first_var.dims, data, attrs)
1919 
1920     def copy(self, deep=True, data=None):
1921         """Returns a copy of this object.
1922 
1923         `deep` is ignored since data is stored in the form of
1924         pandas.Index, which is already immutable. Dimensions, attributes
1925         and encodings are always copied.
1926 
1927         Use `data` to create a new object with the same structure as
1928         original but entirely new data.
1929 
1930         Parameters
1931         ----------
1932         deep : bool, optional
1933             Deep is ignored when data is given. Whether the data array is
1934             loaded into memory and copied onto the new object. Default is True.
1935         data : array_like, optional
1936             Data to use in the new object. Must have same shape as original.
1937 
1938         Returns
1939         -------
1940         object : Variable
1941             New object with dimensions, attributes, encodings, and optionally
1942             data copied from original.
1943         """
1944         if data is None:
1945             if deep:
1946                 # self._data should be a `PandasIndexAdapter` instance at this
1947                 # point, which doesn't have a copy method, so make a deep copy
1948                 # of the underlying `pandas.MultiIndex` and create a new
1949                 # `PandasIndexAdapter` instance with it.
1950                 data = PandasIndexAdapter(self._data.array.copy(deep=True))
1951             else:
1952                 data = self._data
1953         else:
1954             data = as_compatible_data(data)
1955             if self.shape != data.shape:
1956                 raise ValueError("Data shape {} must match shape of object {}"
1957                                  .format(data.shape, self.shape))
1958         return type(self)(self.dims, data, self._attrs,
1959                           self._encoding, fastpath=True)
1960 
1961     def equals(self, other, equiv=None):
1962         # if equiv is specified, super up
1963         if equiv is not None:
1964             return super().equals(other, equiv)
1965 
1966         # otherwise use the native index equals, rather than looking at _data
1967         other = getattr(other, 'variable', other)
1968         try:
1969             return (self.dims == other.dims and
1970                     self._data_equals(other))
1971         except (TypeError, AttributeError):
1972             return False
1973 
1974     def _data_equals(self, other):
1975         return self.to_index().equals(other.to_index())
1976 
1977     def to_index_variable(self):
1978         """Return this variable as an xarray.IndexVariable"""
1979         return self
1980 
1981     to_coord = utils.alias(to_index_variable, 'to_coord')
1982 
1983     def to_index(self):
1984         """Convert this variable to a pandas.Index"""
1985         # n.b. creating a new pandas.Index from an old pandas.Index is
1986         # basically free as pandas.Index objects are immutable
1987         assert self.ndim == 1
1988         index = self._data.array
1989         if isinstance(index, pd.MultiIndex):
1990             # set default names for multi-index unnamed levels so that
1991             # we can safely rename dimension / coordinate later
1992             valid_level_names = [name or '{}_level_{}'.format(self.dims[0], i)
1993                                  for i, name in enumerate(index.names)]
1994             index = index.set_names(valid_level_names)
1995         else:
1996             index = index.set_names(self.name)
1997         return index
1998 
1999     @property
2000     def level_names(self):
2001         """Return MultiIndex level names or None if this IndexVariable has no
2002         MultiIndex.
2003         """
2004         index = self.to_index()
2005         if isinstance(index, pd.MultiIndex):
2006             return index.names
2007         else:
2008             return None
2009 
2010     def get_level_variable(self, level):
2011         """Return a new IndexVariable from a given MultiIndex level."""
2012         if self.level_names is None:
2013             raise ValueError("IndexVariable %r has no MultiIndex" % self.name)
2014         index = self.to_index()
2015         return type(self)(self.dims, index.get_level_values(level))
2016 
2017     @property
2018     def name(self):
2019         return self.dims[0]
2020 
2021     @name.setter
2022     def name(self, value):
2023         raise AttributeError('cannot modify name of IndexVariable in-place')

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 15:11:37.916 | INFO     | Search completed. Bug locations: [<file>xarray/core/variable.py</file>
<class>Variable</class>
<method>copy</method>
<code>
743     def copy(self, deep=True, data=None):
744         """Returns a copy of this object.
745 
746         If `deep=True`, the data array is loaded into memory and copied onto
747         the new object. Dimensions, attributes and encodings are always copied.
748 
749         Use `data` to create a new object with the same structure as
750         original but entirely new data.
751 
752         Parameters
753         ----------
754         deep : bool, optional
755             Whether the data array is loaded into memory and copied onto
756             the new object. Default is True.
757         data : array_like, optional
758             Data to use in the new object. Must have same shape as original.
759             When `data` is used, `deep` is ignored.
760 
761         Returns
762         -------
763         object : Variable
764             New object with dimensions, attributes, encodings, and optionally
765             data copied from original.
766 
767         Examples
768         --------
769 
770         Shallow copy versus deep copy
771 
772         >>> var = xr.Variable(data=[1, 2, 3], dims='x')
773         >>> var.copy()
774         <xarray.Variable (x: 3)>
775         array([1, 2, 3])
776         >>> var_0 = var.copy(deep=False)
777         >>> var_0[0] = 7
778         >>> var_0
779         <xarray.Variable (x: 3)>
780         array([7, 2, 3])
781         >>> var
782         <xarray.Variable (x: 3)>
783         array([7, 2, 3])
784 
785         Changing the data using the ``data`` argument maintains the
786         structure of the original object, but with the new data. Original
787         object is unaffected.
788 
789         >>> var.copy(data=[0.1, 0.2, 0.3])
790         <xarray.Variable (x: 3)>
791         array([ 0.1,  0.2,  0.3])
792         >>> var
793         <xarray.Variable (x: 3)>
794         array([7, 2, 3])
795 
796         See Also
797         --------
798         pandas.DataFrame.copy
799         """
800         if data is None:
801             data = self._data
802 
803             if isinstance(data, indexing.MemoryCachedArray):
804                 # don't share caching between copies
805                 data = indexing.MemoryCachedArray(data.array)
806 
807             if deep:
808                 if isinstance(data, dask_array_type):
809                     data = data.copy()
810                 elif not isinstance(data, PandasIndexAdapter):
811                     # pandas.Index is immutable
812                     data = np.array(data)
813         else:
814             data = as_compatible_data(data)
815             if self.shape != data.shape:
816                 raise ValueError("Data shape {} must match shape of object {}"
817                                  .format(data.shape, self.shape))
818 
819         # note:
820         # dims is already an immutable tuple
821         # attributes and encoding will be copied when the new Array is created
822         return type(self)(self.dims, data, self._attrs, self._encoding,
823                           fastpath=True)

</code><intended_behavior>The method should include explicit handling to preserve the data type of variables when making deep copies. This could involve implementing a check to ensure that the dtype of numpy.array(data) operation is aligned with the original data's dtype, especially for Unicode data types, thus preventing the conversion to the object dtype.</intended_behavior>, <file>xarray/core/variable.py</file>
<class>Variable</class>
<method>None</method>
<code>
219 class Variable(common.AbstractArray, arithmetic.SupportsArithmetic,
220                utils.NdimSizeLenMixin):
221     """A netcdf-like variable consisting of dimensions, data and attributes
222     which describe a single Array. A single Variable object is not fully
223     described outside the context of its parent Dataset (if you want such a
224     fully described object, use a DataArray instead).
225 
226     The main functional difference between Variables and numpy arrays is that
227     numerical operations on Variables implement array broadcasting by dimension
228     name. For example, adding an Variable with dimensions `('time',)` to
229     another Variable with dimensions `('space',)` results in a new Variable
230     with dimensions `('time', 'space')`. Furthermore, numpy reduce operations
231     like ``mean`` or ``sum`` are overwritten to take a "dimension" argument
232     instead of an "axis".
233 
234     Variables are light-weight objects used as the building block for datasets.
235     They are more primitive objects, so operations with them provide marginally
236     higher performance than using DataArrays. However, manipulating data in the
237     form of a Dataset or DataArray should almost always be preferred, because
238     they can use more complete metadata in context of coordinate labels.
239     """
240 
241     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):
242         """
243         Parameters
244         ----------
245         dims : str or sequence of str
246             Name(s) of the the data dimension(s). Must be either a string (only
247             for 1D data) or a sequence of strings with length equal to the
248             number of dimensions.
249         data : array_like
250             Data array which supports numpy-like data access.
251         attrs : dict_like or None, optional
252             Attributes to assign to the new variable. If None (default), an
253             empty attribute dictionary is initialized.
254         encoding : dict_like or None, optional
255             Dictionary specifying how to encode this array's data into a
256             serialized format like netCDF4. Currently used keys (for netCDF)
257             include '_FillValue', 'scale_factor', 'add_offset' and 'dtype'.
258             Well-behaved code to serialize a Variable should ignore
259             unrecognized encoding items.
260         """
261         self._data = as_compatible_data(data, fastpath=fastpath)
262         self._dims = self._parse_dimensions(dims)
263         self._attrs = None
264         self._encoding = None
265         if attrs is not None:
266             self.attrs = attrs
267         if encoding is not None:
268             self.encoding = encoding
269 
270     @property
271     def dtype(self):
272         return self._data.dtype
273 
274     @property
275     def shape(self):
276         return self._data.shape
277 
278     @property
279     def nbytes(self):
280         return self.size * self.dtype.itemsize
281 
282     @property
283     def _in_memory(self):
284         return (isinstance(self._data, (np.ndarray, np.number,
285                                         PandasIndexAdapter)) or
286                 (isinstance(self._data, indexing.MemoryCachedArray) and
287                  isinstance(self._data.array, indexing.NumpyIndexingAdapter)))
288 
289     @property
290     def data(self):
291         if isinstance(self._data, dask_array_type):
292             return self._data
293         else:
294             return self.values
295 
296     @data.setter
297     def data(self, data):
298         data = as_compatible_data(data)
299         if data.shape != self.shape:
300             raise ValueError(
301                 "replacement data must match the Variable's shape")
302         self._data = data
303 
304     def load(self, **kwargs):
305         """Manually trigger loading of this variable's data from disk or a
306         remote source into memory and return this variable.
307 
308         Normally, it should not be necessary to call this method in user code,
309         because all xarray functions should either work on deferred data or
310         load data automatically.
311 
312         Parameters
313         ----------
314         **kwargs : dict
315             Additional keyword arguments passed on to ``dask.array.compute``.
316 
317         See Also
318         --------
319         dask.array.compute
320         """
321         if isinstance(self._data, dask_array_type):
322             self._data = as_compatible_data(self._data.compute(**kwargs))
323         elif not isinstance(self._data, np.ndarray):
324             self._data = np.asarray(self._data)
325         return self
326 
327     def compute(self, **kwargs):
328         """Manually trigger loading of this variable's data from disk or a
329         remote source into memory and return a new variable. The original is
330         left unaltered.
331 
332         Normally, it should not be necessary to call this method in user code,
333         because all xarray functions should either work on deferred data or
334         load data automatically.
335 
336         Parameters
337         ----------
338         **kwargs : dict
339             Additional keyword arguments passed on to ``dask.array.compute``.
340 
341         See Also
342         --------
343         dask.array.compute
344         """
345         new = self.copy(deep=False)
346         return new.load(**kwargs)
347 
348     def __dask_graph__(self):
349         if isinstance(self._data, dask_array_type):
350             return self._data.__dask_graph__()
351         else:
352             return None
353 
354     def __dask_keys__(self):
355         return self._data.__dask_keys__()
356 
357     def __dask_layers__(self):
358         return self._data.__dask_layers__()
359 
360     @property
361     def __dask_optimize__(self):
362         return self._data.__dask_optimize__
363 
364     @property
365     def __dask_scheduler__(self):
366         return self._data.__dask_scheduler__
367 
368     def __dask_postcompute__(self):
369         array_func, array_args = self._data.__dask_postcompute__()
370         return self._dask_finalize, (array_func, array_args, self._dims,
371                                      self._attrs, self._encoding)
372 
373     def __dask_postpersist__(self):
374         array_func, array_args = self._data.__dask_postpersist__()
375         return self._dask_finalize, (array_func, array_args, self._dims,
376                                      self._attrs, self._encoding)
377 
378     @staticmethod
379     def _dask_finalize(results, array_func, array_args, dims, attrs, encoding):
380         if isinstance(results, dict):  # persist case
381             name = array_args[0]
382             results = {k: v for k, v in results.items() if k[0] == name}
383         data = array_func(results, *array_args)
384         return Variable(dims, data, attrs=attrs, encoding=encoding)
385 
386     @property
387     def values(self):
388         """The variable's data as a numpy.ndarray"""
389         return _as_array_or_item(self._data)
390 
391     @values.setter
392     def values(self, values):
393         self.data = values
394 
395     def to_base_variable(self):
396         """Return this variable as a base xarray.Variable"""
397         return Variable(self.dims, self._data, self._attrs,
398                         encoding=self._encoding, fastpath=True)
399 
400     to_variable = utils.alias(to_base_variable, 'to_variable')
401 
402     def to_index_variable(self):
403         """Return this variable as an xarray.IndexVariable"""
404         return IndexVariable(self.dims, self._data, self._attrs,
405                              encoding=self._encoding, fastpath=True)
406 
407     to_coord = utils.alias(to_index_variable, 'to_coord')
408 
409     def to_index(self):
410         """Convert this variable to a pandas.Index"""
411         return self.to_index_variable().to_index()
412 
413     def to_dict(self, data=True):
414         """Dictionary representation of variable."""
415         item = {'dims': self.dims,
416                 'attrs': decode_numpy_dict_values(self.attrs)}
417         if data:
418             item['data'] = ensure_us_time_resolution(self.values).tolist()
419         else:
420             item.update({'dtype': str(self.dtype), 'shape': self.shape})
421         return item
422 
423     @property
424     def dims(self):
425         """Tuple of dimension names with which this variable is associated.
426         """
427         return self._dims
428 
429     @dims.setter
430     def dims(self, value):
431         self._dims = self._parse_dimensions(value)
432 
433     def _parse_dimensions(self, dims):
434         if isinstance(dims, str):
435             dims = (dims,)
436         dims = tuple(dims)
437         if len(dims) != self.ndim:
438             raise ValueError('dimensions %s must have the same length as the '
439                              'number of data dimensions, ndim=%s'
440                              % (dims, self.ndim))
441         return dims
442 
443     def _item_key_to_tuple(self, key):
444         if utils.is_dict_like(key):
445             return tuple(key.get(dim, slice(None)) for dim in self.dims)
446         else:
447             return key
448 
449     def _broadcast_indexes(self, key):
450         """Prepare an indexing key for an indexing operation.
451 
452         Parameters
453         -----------
454         key: int, slice, array, dict or tuple of integer, slices and arrays
455             Any valid input for indexing.
456 
457         Returns
458         -------
459         dims: tuple
460             Dimension of the resultant variable.
461         indexers: IndexingTuple subclass
462             Tuple of integer, array-like, or slices to use when indexing
463             self._data. The type of this argument indicates the type of
464             indexing to perform, either basic, outer or vectorized.
465         new_order : Optional[Sequence[int]]
466             Optional reordering to do on the result of indexing. If not None,
467             the first len(new_order) indexing should be moved to these
468             positions.
469         """
470         key = self._item_key_to_tuple(key)  # key is a tuple
471         # key is a tuple of full size
472         key = indexing.expanded_indexer(key, self.ndim)
473         # Convert a scalar Variable to an integer
474         key = tuple(
475             k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k
476             for k in key)
477         # Convert a 0d-array to an integer
478         key = tuple(
479             k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k
480             for k in key)
481 
482         if all(isinstance(k, BASIC_INDEXING_TYPES) for k in key):
483             return self._broadcast_indexes_basic(key)
484 
485         self._validate_indexers(key)
486         # Detect it can be mapped as an outer indexer
487         # If all key is unlabeled, or
488         # key can be mapped as an OuterIndexer.
489         if all(not isinstance(k, Variable) for k in key):
490             return self._broadcast_indexes_outer(key)
491 
492         # If all key is 1-dimensional and there are no duplicate labels,
493         # key can be mapped as an OuterIndexer.
494         dims = []
495         for k, d in zip(key, self.dims):
496             if isinstance(k, Variable):
497                 if len(k.dims) > 1:
498                     return self._broadcast_indexes_vectorized(key)
499                 dims.append(k.dims[0])
500             elif not isinstance(k, integer_types):
501                 dims.append(d)
502         if len(set(dims)) == len(dims):
503             return self._broadcast_indexes_outer(key)
504 
505         return self._broadcast_indexes_vectorized(key)
506 
507     def _broadcast_indexes_basic(self, key):
508         dims = tuple(dim for k, dim in zip(key, self.dims)
509                      if not isinstance(k, integer_types))
510         return dims, BasicIndexer(key), None
511 
512     def _validate_indexers(self, key):
513         """ Make sanity checks """
514         for dim, k in zip(self.dims, key):
515             if isinstance(k, BASIC_INDEXING_TYPES):
516                 pass
517             else:
518                 if not isinstance(k, Variable):
519                     k = np.asarray(k)
520                     if k.ndim > 1:
521                         raise IndexError(
522                             "Unlabeled multi-dimensional array cannot be "
523                             "used for indexing: {}".format(k))
524                 if k.dtype.kind == 'b':
525                     if self.shape[self.get_axis_num(dim)] != len(k):
526                         raise IndexError(
527                             "Boolean array size {0:d} is used to index array "
528                             "with shape {1:s}.".format(len(k),
529                                                        str(self.shape)))
530                     if k.ndim > 1:
531                         raise IndexError("{}-dimensional boolean indexing is "
532                                          "not supported. ".format(k.ndim))
533                     if getattr(k, 'dims', (dim, )) != (dim, ):
534                         raise IndexError(
535                             "Boolean indexer should be unlabeled or on the "
536                             "same dimension to the indexed array. Indexer is "
537                             "on {0:s} but the target dimension is "
538                             "{1:s}.".format(str(k.dims), dim))
539 
540     def _broadcast_indexes_outer(self, key):
541         dims = tuple(k.dims[0] if isinstance(k, Variable) else dim
542                      for k, dim in zip(key, self.dims)
543                      if not isinstance(k, integer_types))
544 
545         new_key = []
546         for k in key:
547             if isinstance(k, Variable):
548                 k = k.data
549             if not isinstance(k, BASIC_INDEXING_TYPES):
550                 k = np.asarray(k)
551                 if k.dtype.kind == 'b':
552                     (k,) = np.nonzero(k)
553             new_key.append(k)
554 
555         return dims, OuterIndexer(tuple(new_key)), None
556 
557     def _nonzero(self):
558         """ Equivalent numpy's nonzero but returns a tuple of Varibles. """
559         # TODO we should replace dask's native nonzero
560         # after https://github.com/dask/dask/issues/1076 is implemented.
561         nonzeros = np.nonzero(self.data)
562         return tuple(Variable((dim), nz) for nz, dim
563                      in zip(nonzeros, self.dims))
564 
565     def _broadcast_indexes_vectorized(self, key):
566         variables = []
567         out_dims_set = OrderedSet()
568         for dim, value in zip(self.dims, key):
569             if isinstance(value, slice):
570                 out_dims_set.add(dim)
571             else:
572                 variable = (value if isinstance(value, Variable) else
573                             as_variable(value, name=dim))
574                 if variable.dtype.kind == 'b':  # boolean indexing case
575                     (variable,) = variable._nonzero()
576 
577                 variables.append(variable)
578                 out_dims_set.update(variable.dims)
579 
580         variable_dims = set()
581         for variable in variables:
582             variable_dims.update(variable.dims)
583 
584         slices = []
585         for i, (dim, value) in enumerate(zip(self.dims, key)):
586             if isinstance(value, slice):
587                 if dim in variable_dims:
588                     # We only convert slice objects to variables if they share
589                     # a dimension with at least one other variable. Otherwise,
590                     # we can equivalently leave them as slices aknd transpose
591                     # the result. This is significantly faster/more efficient
592                     # for most array backends.
593                     values = np.arange(*value.indices(self.sizes[dim]))
594                     variables.insert(i - len(slices), Variable((dim,), values))
595                 else:
596                     slices.append((i, value))
597 
598         try:
599             variables = _broadcast_compat_variables(*variables)
600         except ValueError:
601             raise IndexError("Dimensions of indexers mismatch: {}".format(key))
602 
603         out_key = [variable.data for variable in variables]
604         out_dims = tuple(out_dims_set)
605         slice_positions = set()
606         for i, value in slices:
607             out_key.insert(i, value)
608             new_position = out_dims.index(self.dims[i])
609             slice_positions.add(new_position)
610 
611         if slice_positions:
612             new_order = [i for i in range(len(out_dims))
613                          if i not in slice_positions]
614         else:
615             new_order = None
616 
617         return out_dims, VectorizedIndexer(tuple(out_key)), new_order
618 
619     def __getitem__(self, key):
620         """Return a new Array object whose contents are consistent with
621         getting the provided key from the underlying data.
622 
623         NB. __getitem__ and __setitem__ implement xarray-style indexing,
624         where if keys are unlabeled arrays, we index the array orthogonally
625         with them. If keys are labeled array (such as Variables), they are
626         broadcasted with our usual scheme and then the array is indexed with
627         the broadcasted key, like numpy's fancy indexing.
628 
629         If you really want to do indexing like `x[x > 0]`, manipulate the numpy
630         array `x.values` directly.
631         """
632         dims, indexer, new_order = self._broadcast_indexes(key)
633         data = as_indexable(self._data)[indexer]
634         if new_order:
635             data = duck_array_ops.moveaxis(
636                 data, range(len(new_order)), new_order)
637         return self._finalize_indexing_result(dims, data)
638 
639     def _finalize_indexing_result(self, dims, data):
640         """Used by IndexVariable to return IndexVariable objects when possible.
641         """
642         return type(self)(dims, data, self._attrs, self._encoding,
643                           fastpath=True)
644 
645     def _getitem_with_mask(self, key, fill_value=dtypes.NA):
646         """Index this Variable with -1 remapped to fill_value."""
647         # TODO(shoyer): expose this method in public API somewhere (isel?) and
648         # use it for reindex.
649         # TODO(shoyer): add a sanity check that all other integers are
650         # non-negative
651         # TODO(shoyer): add an optimization, remapping -1 to an adjacent value
652         # that is actually indexed rather than mapping it to the last value
653         # along each axis.
654 
655         if fill_value is dtypes.NA:
656             fill_value = dtypes.get_fill_value(self.dtype)
657 
658         dims, indexer, new_order = self._broadcast_indexes(key)
659 
660         if self.size:
661             if isinstance(self._data, dask_array_type):
662                 # dask's indexing is faster this way; also vindex does not
663                 # support negative indices yet:
664                 # https://github.com/dask/dask/pull/2967
665                 actual_indexer = indexing.posify_mask_indexer(indexer)
666             else:
667                 actual_indexer = indexer
668 
669             data = as_indexable(self._data)[actual_indexer]
670             chunks_hint = getattr(data, 'chunks', None)
671             mask = indexing.create_mask(indexer, self.shape, chunks_hint)
672             data = duck_array_ops.where(mask, fill_value, data)
673         else:
674             # array cannot be indexed along dimensions of size 0, so just
675             # build the mask directly instead.
676             mask = indexing.create_mask(indexer, self.shape)
677             data = np.broadcast_to(fill_value, getattr(mask, 'shape', ()))
678 
679         if new_order:
680             data = duck_array_ops.moveaxis(
681                 data, range(len(new_order)), new_order)
682         return self._finalize_indexing_result(dims, data)
683 
684     def __setitem__(self, key, value):
685         """__setitem__ is overloaded to access the underlying numpy values with
686         orthogonal indexing.
687 
688         See __getitem__ for more details.
689         """
690         dims, index_tuple, new_order = self._broadcast_indexes(key)
691 
692         if not isinstance(value, Variable):
693             value = as_compatible_data(value)
694             if value.ndim > len(dims):
695                 raise ValueError(
696                     'shape mismatch: value array of shape %s could not be '
697                     'broadcast to indexing result with %s dimensions'
698                     % (value.shape, len(dims)))
699             if value.ndim == 0:
700                 value = Variable((), value)
701             else:
702                 value = Variable(dims[-value.ndim:], value)
703         # broadcast to become assignable
704         value = value.set_dims(dims).data
705 
706         if new_order:
707             value = duck_array_ops.asarray(value)
708             value = value[(len(dims) - value.ndim) * (np.newaxis,) +
709                           (Ellipsis,)]
710             value = duck_array_ops.moveaxis(
711                 value, new_order, range(len(new_order)))
712 
713         indexable = as_indexable(self._data)
714         indexable[index_tuple] = value
715 
716     @property
717     def attrs(self) -> 'OrderedDict[Any, Any]':
718         """Dictionary of local attributes on this variable.
719         """
720         if self._attrs is None:
721             self._attrs = OrderedDict()
722         return self._attrs
723 
724     @attrs.setter
725     def attrs(self, value: Mapping[Hashable, Any]) -> None:
726         self._attrs = OrderedDict(value)
727 
728     @property
729     def encoding(self):
730         """Dictionary of encodings on this variable.
731         """
732         if self._encoding is None:
733             self._encoding = {}
734         return self._encoding
735 
736     @encoding.setter
737     def encoding(self, value):
738         try:
739             self._encoding = dict(value)
740         except ValueError:
741             raise ValueError('encoding must be castable to a dictionary')
742 
743     def copy(self, deep=True, data=None):
744         """Returns a copy of this object.
745 
746         If `deep=True`, the data array is loaded into memory and copied onto
747         the new object. Dimensions, attributes and encodings are always copied.
748 
749         Use `data` to create a new object with the same structure as
750         original but entirely new data.
751 
752         Parameters
753         ----------
754         deep : bool, optional
755             Whether the data array is loaded into memory and copied onto
756             the new object. Default is True.
757         data : array_like, optional
758             Data to use in the new object. Must have same shape as original.
759             When `data` is used, `deep` is ignored.
760 
761         Returns
762         -------
763         object : Variable
764             New object with dimensions, attributes, encodings, and optionally
765             data copied from original.
766 
767         Examples
768         --------
769 
770         Shallow copy versus deep copy
771 
772         >>> var = xr.Variable(data=[1, 2, 3], dims='x')
773         >>> var.copy()
774         <xarray.Variable (x: 3)>
775         array([1, 2, 3])
776         >>> var_0 = var.copy(deep=False)
777         >>> var_0[0] = 7
778         >>> var_0
779         <xarray.Variable (x: 3)>
780         array([7, 2, 3])
781         >>> var
782         <xarray.Variable (x: 3)>
783         array([7, 2, 3])
784 
785         Changing the data using the ``data`` argument maintains the
786         structure of the original object, but with the new data. Original
787         object is unaffected.
788 
789         >>> var.copy(data=[0.1, 0.2, 0.3])
790         <xarray.Variable (x: 3)>
791         array([ 0.1,  0.2,  0.3])
792         >>> var
793         <xarray.Variable (x: 3)>
794         array([7, 2, 3])
795 
796         See Also
797         --------
798         pandas.DataFrame.copy
799         """
800         if data is None:
801             data = self._data
802 
803             if isinstance(data, indexing.MemoryCachedArray):
804                 # don't share caching between copies
805                 data = indexing.MemoryCachedArray(data.array)
806 
807             if deep:
808                 if isinstance(data, dask_array_type):
809                     data = data.copy()
810                 elif not isinstance(data, PandasIndexAdapter):
811                     # pandas.Index is immutable
812                     data = np.array(data)
813         else:
814             data = as_compatible_data(data)
815             if self.shape != data.shape:
816                 raise ValueError("Data shape {} must match shape of object {}"
817                                  .format(data.shape, self.shape))
818 
819         # note:
820         # dims is already an immutable tuple
821         # attributes and encoding will be copied when the new Array is created
822         return type(self)(self.dims, data, self._attrs, self._encoding,
823                           fastpath=True)
824 
825     def __copy__(self):
826         return self.copy(deep=False)
827 
828     def __deepcopy__(self, memo=None):
829         # memo does nothing but is required for compatibility with
830         # copy.deepcopy
831         return self.copy(deep=True)
832 
833     # mutable objects should not be hashable
834     # https://github.com/python/mypy/issues/4266
835     __hash__ = None  # type: ignore
836 
837     @property
838     def chunks(self):
839         """Block dimensions for this array's data or None if it's not a dask
840         array.
841         """
842         return getattr(self._data, 'chunks', None)
843 
844     _array_counter = itertools.count()
845 
846     def chunk(self, chunks=None, name=None, lock=False):
847         """Coerce this array's data into a dask arrays with the given chunks.
848 
849         If this variable is a non-dask array, it will be converted to dask
850         array. If it's a dask array, it will be rechunked to the given chunk
851         sizes.
852 
853         If neither chunks is not provided for one or more dimensions, chunk
854         sizes along that dimension will not be updated; non-dask arrays will be
855         converted into dask arrays with a single block.
856 
857         Parameters
858         ----------
859         chunks : int, tuple or dict, optional
860             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
861             ``{'x': 5, 'y': 5}``.
862         name : str, optional
863             Used to generate the name for this array in the internal dask
864             graph. Does not need not be unique.
865         lock : optional
866             Passed on to :py:func:`dask.array.from_array`, if the array is not
867             already as dask array.
868 
869         Returns
870         -------
871         chunked : xarray.Variable
872         """
873         import dask
874         import dask.array as da
875 
876         if utils.is_dict_like(chunks):
877             chunks = dict((self.get_axis_num(dim), chunk)
878                           for dim, chunk in chunks.items())
879 
880         if chunks is None:
881             chunks = self.chunks or self.shape
882 
883         data = self._data
884         if isinstance(data, da.Array):
885             data = data.rechunk(chunks)
886         else:
887             if utils.is_dict_like(chunks):
888                 chunks = tuple(chunks.get(n, s)
889                                for n, s in enumerate(self.shape))
890             # da.from_array works by using lazily indexing with a tuple of
891             # slices. Using OuterIndexer is a pragmatic choice: dask does not
892             # yet handle different indexing types in an explicit way:
893             # https://github.com/dask/dask/issues/2883
894             data = indexing.ImplicitToExplicitIndexingAdapter(
895                 data, indexing.OuterIndexer)
896 
897             # For now, assume that all arrays that we wrap with dask (including
898             # our lazily loaded backend array classes) should use NumPy array
899             # operations.
900             if LooseVersion(dask.__version__) > '1.2.2':
901                 kwargs = dict(meta=np.ndarray)
902             else:
903                 kwargs = dict()
904 
905             data = da.from_array(
906                 data, chunks, name=name, lock=lock, **kwargs)
907 
908         return type(self)(self.dims, data, self._attrs, self._encoding,
909                           fastpath=True)
910 
911     def isel(self, indexers=None, drop=False, **indexers_kwargs):
912         """Return a new array indexed along the specified dimension(s).
913 
914         Parameters
915         ----------
916         **indexers : {dim: indexer, ...}
917             Keyword arguments with names matching dimensions and values given
918             by integers, slice objects or arrays.
919 
920         Returns
921         -------
922         obj : Array object
923             A new Array with the selected data and dimensions. In general,
924             the new variable's data will be a view of this variable's data,
925             unless numpy fancy indexing was triggered by using an array
926             indexer, in which case the data will be a copy.
927         """
928         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, 'isel')
929 
930         invalid = [k for k in indexers if k not in self.dims]
931         if invalid:
932             raise ValueError("dimensions %r do not exist" % invalid)
933 
934         key = [slice(None)] * self.ndim
935         for i, dim in enumerate(self.dims):
936             if dim in indexers:
937                 key[i] = indexers[dim]
938         return self[tuple(key)]
939 
940     def squeeze(self, dim=None):
941         """Return a new object with squeezed data.
942 
943         Parameters
944         ----------
945         dim : None or str or tuple of str, optional
946             Selects a subset of the length one dimensions. If a dimension is
947             selected with length greater than one, an error is raised. If
948             None, all length one dimensions are squeezed.
949 
950         Returns
951         -------
952         squeezed : same type as caller
953             This object, but with with all or a subset of the dimensions of
954             length 1 removed.
955 
956         See Also
957         --------
958         numpy.squeeze
959         """
960         dims = common.get_squeeze_dims(self, dim)
961         return self.isel({d: 0 for d in dims})
962 
963     def _shift_one_dim(self, dim, count, fill_value=dtypes.NA):
964         axis = self.get_axis_num(dim)
965 
966         if count > 0:
967             keep = slice(None, -count)
968         elif count < 0:
969             keep = slice(-count, None)
970         else:
971             keep = slice(None)
972 
973         trimmed_data = self[(slice(None),) * axis + (keep,)].data
974 
975         if fill_value is dtypes.NA:
976             dtype, fill_value = dtypes.maybe_promote(self.dtype)
977         else:
978             dtype = self.dtype
979 
980         shape = list(self.shape)
981         shape[axis] = min(abs(count), shape[axis])
982 
983         if isinstance(trimmed_data, dask_array_type):
984             chunks = list(trimmed_data.chunks)
985             chunks[axis] = (shape[axis],)
986             full = functools.partial(da.full, chunks=chunks)
987         else:
988             full = np.full
989 
990         filler = full(shape, fill_value, dtype=dtype)
991 
992         if count > 0:
993             arrays = [filler, trimmed_data]
994         else:
995             arrays = [trimmed_data, filler]
996 
997         data = duck_array_ops.concatenate(arrays, axis)
998 
999         if isinstance(data, dask_array_type):
1000             # chunked data should come out with the same chunks; this makes
1001             # it feasible to combine shifted and unshifted data
1002             # TODO: remove this once dask.array automatically aligns chunks
1003             data = data.rechunk(self.data.chunks)
1004 
1005         return type(self)(self.dims, data, self._attrs, fastpath=True)
1006 
1007     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):
1008         """
1009         Return a new Variable with shifted data.
1010 
1011         Parameters
1012         ----------
1013         shifts : mapping of the form {dim: offset}
1014             Integer offset to shift along each of the given dimensions.
1015             Positive offsets shift to the right; negative offsets shift to the
1016             left.
1017         fill_value: scalar, optional
1018             Value to use for newly missing values
1019         **shifts_kwargs:
1020             The keyword arguments form of ``shifts``.
1021             One of shifts or shifts_kwarg must be provided.
1022 
1023         Returns
1024         -------
1025         shifted : Variable
1026             Variable with the same dimensions and attributes but shifted data.
1027         """
1028         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, 'shift')
1029         result = self
1030         for dim, count in shifts.items():
1031             result = result._shift_one_dim(dim, count, fill_value=fill_value)
1032         return result
1033 
1034     def pad_with_fill_value(self, pad_widths=None, fill_value=dtypes.NA,
1035                             **pad_widths_kwargs):
1036         """
1037         Return a new Variable with paddings.
1038 
1039         Parameters
1040         ----------
1041         pad_width: Mapping of the form {dim: (before, after)}
1042             Number of values padded to the edges of each dimension.
1043         **pad_widths_kwargs:
1044             Keyword argument for pad_widths
1045         """
1046         pad_widths = either_dict_or_kwargs(pad_widths, pad_widths_kwargs,
1047                                            'pad')
1048 
1049         if fill_value is dtypes.NA:
1050             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1051         else:
1052             dtype = self.dtype
1053 
1054         if isinstance(self.data, dask_array_type):
1055             array = self.data
1056 
1057             # Dask does not yet support pad. We manually implement it.
1058             # https://github.com/dask/dask/issues/1926
1059             for d, pad in pad_widths.items():
1060                 axis = self.get_axis_num(d)
1061                 before_shape = list(array.shape)
1062                 before_shape[axis] = pad[0]
1063                 before_chunks = list(array.chunks)
1064                 before_chunks[axis] = (pad[0], )
1065                 after_shape = list(array.shape)
1066                 after_shape[axis] = pad[1]
1067                 after_chunks = list(array.chunks)
1068                 after_chunks[axis] = (pad[1], )
1069 
1070                 arrays = []
1071                 if pad[0] > 0:
1072                     arrays.append(da.full(before_shape, fill_value,
1073                                           dtype=dtype, chunks=before_chunks))
1074                 arrays.append(array)
1075                 if pad[1] > 0:
1076                     arrays.append(da.full(after_shape, fill_value,
1077                                           dtype=dtype, chunks=after_chunks))
1078                 if len(arrays) > 1:
1079                     array = da.concatenate(arrays, axis=axis)
1080         else:
1081             pads = [(0, 0) if d not in pad_widths else pad_widths[d]
1082                     for d in self.dims]
1083             array = np.pad(self.data.astype(dtype, copy=False), pads,
1084                            mode='constant', constant_values=fill_value)
1085         return type(self)(self.dims, array)
1086 
1087     def _roll_one_dim(self, dim, count):
1088         axis = self.get_axis_num(dim)
1089 
1090         count %= self.shape[axis]
1091         if count != 0:
1092             indices = [slice(-count, None), slice(None, -count)]
1093         else:
1094             indices = [slice(None)]
1095 
1096         arrays = [self[(slice(None),) * axis + (idx,)].data
1097                   for idx in indices]
1098 
1099         data = duck_array_ops.concatenate(arrays, axis)
1100 
1101         if isinstance(data, dask_array_type):
1102             # chunked data should come out with the same chunks; this makes
1103             # it feasible to combine shifted and unshifted data
1104             # TODO: remove this once dask.array automatically aligns chunks
1105             data = data.rechunk(self.data.chunks)
1106 
1107         return type(self)(self.dims, data, self._attrs, fastpath=True)
1108 
1109     def roll(self, shifts=None, **shifts_kwargs):
1110         """
1111         Return a new Variable with rolld data.
1112 
1113         Parameters
1114         ----------
1115         shifts : mapping of the form {dim: offset}
1116             Integer offset to roll along each of the given dimensions.
1117             Positive offsets roll to the right; negative offsets roll to the
1118             left.
1119         **shifts_kwargs:
1120             The keyword arguments form of ``shifts``.
1121             One of shifts or shifts_kwarg must be provided.
1122 
1123         Returns
1124         -------
1125         shifted : Variable
1126             Variable with the same dimensions and attributes but rolled data.
1127         """
1128         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, 'roll')
1129 
1130         result = self
1131         for dim, count in shifts.items():
1132             result = result._roll_one_dim(dim, count)
1133         return result
1134 
1135     def transpose(self, *dims) -> 'Variable':
1136         """Return a new Variable object with transposed dimensions.
1137 
1138         Parameters
1139         ----------
1140         *dims : str, optional
1141             By default, reverse the dimensions. Otherwise, reorder the
1142             dimensions to this order.
1143 
1144         Returns
1145         -------
1146         transposed : Variable
1147             The returned object has transposed data and dimensions with the
1148             same attributes as the original.
1149 
1150         Notes
1151         -----
1152         This operation returns a view of this variable's data. It is
1153         lazy for dask-backed Variables but not for numpy-backed Variables.
1154 
1155         See Also
1156         --------
1157         numpy.transpose
1158         """
1159         if len(dims) == 0:
1160             dims = self.dims[::-1]
1161         axes = self.get_axis_num(dims)
1162         if len(dims) < 2:  # no need to transpose if only one dimension
1163             return self.copy(deep=False)
1164 
1165         data = as_indexable(self._data).transpose(axes)
1166         return type(self)(dims, data, self._attrs, self._encoding,
1167                           fastpath=True)
1168 
1169     @property
1170     def T(self) -> 'Variable':
1171         return self.transpose()
1172 
1173     def expand_dims(self, *args):
1174         import warnings
1175         warnings.warn('Variable.expand_dims is deprecated: use '
1176                       'Variable.set_dims instead', DeprecationWarning,
1177                       stacklevel=2)
1178         return self.expand_dims(*args)
1179 
1180     def set_dims(self, dims, shape=None):
1181         """Return a new variable with given set of dimensions.
1182         This method might be used to attach new dimension(s) to variable.
1183 
1184         When possible, this operation does not copy this variable's data.
1185 
1186         Parameters
1187         ----------
1188         dims : str or sequence of str or dict
1189             Dimensions to include on the new variable. If a dict, values are
1190             used to provide the sizes of new dimensions; otherwise, new
1191             dimensions are inserted with length 1.
1192 
1193         Returns
1194         -------
1195         Variable
1196         """
1197         if isinstance(dims, str):
1198             dims = [dims]
1199 
1200         if shape is None and utils.is_dict_like(dims):
1201             shape = dims.values()
1202 
1203         missing_dims = set(self.dims) - set(dims)
1204         if missing_dims:
1205             raise ValueError('new dimensions %r must be a superset of '
1206                              'existing dimensions %r' % (dims, self.dims))
1207 
1208         self_dims = set(self.dims)
1209         expanded_dims = tuple(
1210             d for d in dims if d not in self_dims) + self.dims
1211 
1212         if self.dims == expanded_dims:
1213             # don't use broadcast_to unless necessary so the result remains
1214             # writeable if possible
1215             expanded_data = self.data
1216         elif shape is not None:
1217             dims_map = dict(zip(dims, shape))
1218             tmp_shape = tuple(dims_map[d] for d in expanded_dims)
1219             expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)
1220         else:
1221             expanded_data = self.data[
1222                 (None,) * (len(expanded_dims) - self.ndim)]
1223 
1224         expanded_var = Variable(expanded_dims, expanded_data, self._attrs,
1225                                 self._encoding, fastpath=True)
1226         return expanded_var.transpose(*dims)
1227 
1228     def _stack_once(self, dims, new_dim):
1229         if not set(dims) <= set(self.dims):
1230             raise ValueError('invalid existing dimensions: %s' % dims)
1231 
1232         if new_dim in self.dims:
1233             raise ValueError('cannot create a new dimension with the same '
1234                              'name as an existing dimension')
1235 
1236         if len(dims) == 0:
1237             # don't stack
1238             return self.copy(deep=False)
1239 
1240         other_dims = [d for d in self.dims if d not in dims]
1241         dim_order = other_dims + list(dims)
1242         reordered = self.transpose(*dim_order)
1243 
1244         new_shape = reordered.shape[:len(other_dims)] + (-1,)
1245         new_data = reordered.data.reshape(new_shape)
1246         new_dims = reordered.dims[:len(other_dims)] + (new_dim,)
1247 
1248         return Variable(new_dims, new_data, self._attrs, self._encoding,
1249                         fastpath=True)
1250 
1251     def stack(self, dimensions=None, **dimensions_kwargs):
1252         """
1253         Stack any number of existing dimensions into a single new dimension.
1254 
1255         New dimensions will be added at the end, and the order of the data
1256         along each new dimension will be in contiguous (C) order.
1257 
1258         Parameters
1259         ----------
1260         dimensions : Mapping of form new_name=(dim1, dim2, ...)
1261             Names of new dimensions, and the existing dimensions that they
1262             replace.
1263         **dimensions_kwargs:
1264             The keyword arguments form of ``dimensions``.
1265             One of dimensions or dimensions_kwargs must be provided.
1266 
1267         Returns
1268         -------
1269         stacked : Variable
1270             Variable with the same attributes but stacked data.
1271 
1272         See also
1273         --------
1274         Variable.unstack
1275         """
1276         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs,
1277                                            'stack')
1278         result = self
1279         for new_dim, dims in dimensions.items():
1280             result = result._stack_once(dims, new_dim)
1281         return result
1282 
1283     def _unstack_once(self, dims, old_dim):
1284         new_dim_names = tuple(dims.keys())
1285         new_dim_sizes = tuple(dims.values())
1286 
1287         if old_dim not in self.dims:
1288             raise ValueError('invalid existing dimension: %s' % old_dim)
1289 
1290         if set(new_dim_names).intersection(self.dims):
1291             raise ValueError('cannot create a new dimension with the same '
1292                              'name as an existing dimension')
1293 
1294         if np.prod(new_dim_sizes) != self.sizes[old_dim]:
1295             raise ValueError('the product of the new dimension sizes must '
1296                              'equal the size of the old dimension')
1297 
1298         other_dims = [d for d in self.dims if d != old_dim]
1299         dim_order = other_dims + [old_dim]
1300         reordered = self.transpose(*dim_order)
1301 
1302         new_shape = reordered.shape[:len(other_dims)] + new_dim_sizes
1303         new_data = reordered.data.reshape(new_shape)
1304         new_dims = reordered.dims[:len(other_dims)] + new_dim_names
1305 
1306         return Variable(new_dims, new_data, self._attrs, self._encoding,
1307                         fastpath=True)
1308 
1309     def unstack(self, dimensions=None, **dimensions_kwargs):
1310         """
1311         Unstack an existing dimension into multiple new dimensions.
1312 
1313         New dimensions will be added at the end, and the order of the data
1314         along each new dimension will be in contiguous (C) order.
1315 
1316         Parameters
1317         ----------
1318         dimensions : mapping of the form old_dim={dim1: size1, ...}
1319             Names of existing dimensions, and the new dimensions and sizes
1320             that they map to.
1321         **dimensions_kwargs:
1322             The keyword arguments form of ``dimensions``.
1323             One of dimensions or dimensions_kwargs must be provided.
1324 
1325         Returns
1326         -------
1327         unstacked : Variable
1328             Variable with the same attributes but unstacked data.
1329 
1330         See also
1331         --------
1332         Variable.stack
1333         """
1334         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs,
1335                                            'unstack')
1336         result = self
1337         for old_dim, dims in dimensions.items():
1338             result = result._unstack_once(dims, old_dim)
1339         return result
1340 
1341     def fillna(self, value):
1342         return ops.fillna(self, value)
1343 
1344     def where(self, cond, other=dtypes.NA):
1345         return ops.where_method(self, cond, other)
1346 
1347     def reduce(self, func, dim=None, axis=None,
1348                keep_attrs=None, keepdims=False, allow_lazy=False, **kwargs):
1349         """Reduce this array by applying `func` along some dimension(s).
1350 
1351         Parameters
1352         ----------
1353         func : function
1354             Function which can be called in the form
1355             `func(x, axis=axis, **kwargs)` to return the result of reducing an
1356             np.ndarray over an integer valued axis.
1357         dim : str or sequence of str, optional
1358             Dimension(s) over which to apply `func`.
1359         axis : int or sequence of int, optional
1360             Axis(es) over which to apply `func`. Only one of the 'dim'
1361             and 'axis' arguments can be supplied. If neither are supplied, then
1362             the reduction is calculated over the flattened array (by calling
1363             `func(x)` without an axis argument).
1364         keep_attrs : bool, optional
1365             If True, the variable's attributes (`attrs`) will be copied from
1366             the original object to the new one.  If False (default), the new
1367             object will be returned without attributes.
1368         keepdims : bool, default False
1369             If True, the dimensions which are reduced are left in the result
1370             as dimensions of size one
1371         **kwargs : dict
1372             Additional keyword arguments passed on to `func`.
1373 
1374         Returns
1375         -------
1376         reduced : Array
1377             Array with summarized data and the indicated dimension(s)
1378             removed.
1379         """
1380         if dim is common.ALL_DIMS:
1381             dim = None
1382         if dim is not None and axis is not None:
1383             raise ValueError("cannot supply both 'axis' and 'dim' arguments")
1384 
1385         if dim is not None:
1386             axis = self.get_axis_num(dim)
1387         input_data = self.data if allow_lazy else self.values
1388         if axis is not None:
1389             data = func(input_data, axis=axis, **kwargs)
1390         else:
1391             data = func(input_data, **kwargs)
1392 
1393         if getattr(data, 'shape', ()) == self.shape:
1394             dims = self.dims
1395         else:
1396             removed_axes = (range(self.ndim) if axis is None
1397                             else np.atleast_1d(axis) % self.ndim)
1398             if keepdims:
1399                 # Insert np.newaxis for removed dims
1400                 slices = tuple(np.newaxis if i in removed_axes else
1401                                slice(None, None) for i in range(self.ndim))
1402                 if getattr(data, 'shape', None) is None:
1403                     # Reduce has produced a scalar value, not an array-like
1404                     data = np.asanyarray(data)[slices]
1405                 else:
1406                     data = data[slices]
1407                 dims = self.dims
1408             else:
1409                 dims = [adim for n, adim in enumerate(self.dims)
1410                         if n not in removed_axes]
1411 
1412         if keep_attrs is None:
1413             keep_attrs = _get_keep_attrs(default=False)
1414         attrs = self._attrs if keep_attrs else None
1415 
1416         return Variable(dims, data, attrs=attrs)
1417 
1418     @classmethod
1419     def concat(cls, variables, dim='concat_dim', positions=None,
1420                shortcut=False):
1421         """Concatenate variables along a new or existing dimension.
1422 
1423         Parameters
1424         ----------
1425         variables : iterable of Array
1426             Arrays to stack together. Each variable is expected to have
1427             matching dimensions and shape except for along the stacked
1428             dimension.
1429         dim : str or DataArray, optional
1430             Name of the dimension to stack along. This can either be a new
1431             dimension name, in which case it is added along axis=0, or an
1432             existing dimension name, in which case the location of the
1433             dimension is unchanged. Where to insert the new dimension is
1434             determined by the first variable.
1435         positions : None or list of integer arrays, optional
1436             List of integer arrays which specifies the integer positions to
1437             which to assign each dataset along the concatenated dimension.
1438             If not supplied, objects are concatenated in the provided order.
1439         shortcut : bool, optional
1440             This option is used internally to speed-up groupby operations.
1441             If `shortcut` is True, some checks of internal consistency between
1442             arrays to concatenate are skipped.
1443 
1444         Returns
1445         -------
1446         stacked : Variable
1447             Concatenated Variable formed by stacking all the supplied variables
1448             along the given dimension.
1449         """
1450         if not isinstance(dim, str):
1451             dim, = dim.dims
1452 
1453         # can't do this lazily: we need to loop through variables at least
1454         # twice
1455         variables = list(variables)
1456         first_var = variables[0]
1457 
1458         arrays = [v.data for v in variables]
1459 
1460         if dim in first_var.dims:
1461             axis = first_var.get_axis_num(dim)
1462             dims = first_var.dims
1463             data = duck_array_ops.concatenate(arrays, axis=axis)
1464             if positions is not None:
1465                 # TODO: deprecate this option -- we don't need it for groupby
1466                 # any more.
1467                 indices = nputils.inverse_permutation(
1468                     np.concatenate(positions))
1469                 data = duck_array_ops.take(data, indices, axis=axis)
1470         else:
1471             axis = 0
1472             dims = (dim,) + first_var.dims
1473             data = duck_array_ops.stack(arrays, axis=axis)
1474 
1475         attrs = OrderedDict(first_var.attrs)
1476         encoding = OrderedDict(first_var.encoding)
1477         if not shortcut:
1478             for var in variables:
1479                 if var.dims != first_var.dims:
1480                     raise ValueError('inconsistent dimensions')
1481                 utils.remove_incompatible_items(attrs, var.attrs)
1482 
1483         return cls(dims, data, attrs, encoding)
1484 
1485     def equals(self, other, equiv=duck_array_ops.array_equiv):
1486         """True if two Variables have the same dimensions and values;
1487         otherwise False.
1488 
1489         Variables can still be equal (like pandas objects) if they have NaN
1490         values in the same locations.
1491 
1492         This method is necessary because `v1 == v2` for Variables
1493         does element-wise comparisons (like numpy.ndarrays).
1494         """
1495         other = getattr(other, 'variable', other)
1496         try:
1497             return (self.dims == other.dims and
1498                     (self._data is other._data or
1499                      equiv(self.data, other.data)))
1500         except (TypeError, AttributeError):
1501             return False
1502 
1503     def broadcast_equals(self, other, equiv=duck_array_ops.array_equiv):
1504         """True if two Variables have the values after being broadcast against
1505         each other; otherwise False.
1506 
1507         Variables can still be equal (like pandas objects) if they have NaN
1508         values in the same locations.
1509         """
1510         try:
1511             self, other = broadcast_variables(self, other)
1512         except (ValueError, AttributeError):
1513             return False
1514         return self.equals(other, equiv=equiv)
1515 
1516     def identical(self, other):
1517         """Like equals, but also checks attributes.
1518         """
1519         try:
1520             return (utils.dict_equiv(self.attrs, other.attrs) and
1521                     self.equals(other))
1522         except (TypeError, AttributeError):
1523             return False
1524 
1525     def no_conflicts(self, other):
1526         """True if the intersection of two Variable's non-null data is
1527         equal; otherwise false.
1528 
1529         Variables can thus still be equal if there are locations where either,
1530         or both, contain NaN values.
1531         """
1532         return self.broadcast_equals(
1533             other, equiv=duck_array_ops.array_notnull_equiv)
1534 
1535     def quantile(self, q, dim=None, interpolation='linear'):
1536         """Compute the qth quantile of the data along the specified dimension.
1537 
1538         Returns the qth quantiles(s) of the array elements.
1539 
1540         Parameters
1541         ----------
1542         q : float in range of [0,1] (or sequence of floats)
1543             Quantile to compute, which must be between 0 and 1
1544             inclusive.
1545         dim : str or sequence of str, optional
1546             Dimension(s) over which to apply quantile.
1547         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
1548             This optional parameter specifies the interpolation method to
1549             use when the desired quantile lies between two data points
1550             ``i < j``:
1551                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is
1552                   the fractional part of the index surrounded by ``i`` and
1553                   ``j``.
1554                 * lower: ``i``.
1555                 * higher: ``j``.
1556                 * nearest: ``i`` or ``j``, whichever is nearest.
1557                 * midpoint: ``(i + j) / 2``.
1558 
1559         Returns
1560         -------
1561         quantiles : Variable
1562             If `q` is a single quantile, then the result
1563             is a scalar. If multiple percentiles are given, first axis of
1564             the result corresponds to the quantile and a quantile dimension
1565             is added to the return array. The other dimensions are the
1566              dimensions that remain after the reduction of the array.
1567 
1568         See Also
1569         --------
1570         numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile,
1571         DataArray.quantile
1572         """
1573         if isinstance(self.data, dask_array_type):
1574             raise TypeError("quantile does not work for arrays stored as dask "
1575                             "arrays. Load the data via .compute() or .load() "
1576                             "prior to calling this method.")
1577 
1578         q = np.asarray(q, dtype=np.float64)
1579 
1580         new_dims = list(self.dims)
1581         if dim is not None:
1582             axis = self.get_axis_num(dim)
1583             if utils.is_scalar(dim):
1584                 new_dims.remove(dim)
1585             else:
1586                 for d in dim:
1587                     new_dims.remove(d)
1588         else:
1589             axis = None
1590             new_dims = []
1591 
1592         # only add the quantile dimension if q is array like
1593         if q.ndim != 0:
1594             new_dims = ['quantile'] + new_dims
1595 
1596         qs = np.nanpercentile(self.data, q * 100., axis=axis,
1597                               interpolation=interpolation)
1598         return Variable(new_dims, qs)
1599 
1600     def rank(self, dim, pct=False):
1601         """Ranks the data.
1602 
1603         Equal values are assigned a rank that is the average of the ranks that
1604         would have been otherwise assigned to all of the values within that
1605         set.  Ranks begin at 1, not 0. If `pct`, computes percentage ranks.
1606 
1607         NaNs in the input array are returned as NaNs.
1608 
1609         The `bottleneck` library is required.
1610 
1611         Parameters
1612         ----------
1613         dim : str
1614             Dimension over which to compute rank.
1615         pct : bool, optional
1616             If True, compute percentage ranks, otherwise compute integer ranks.
1617 
1618         Returns
1619         -------
1620         ranked : Variable
1621 
1622         See Also
1623         --------
1624         Dataset.rank, DataArray.rank
1625         """
1626         import bottleneck as bn
1627 
1628         if isinstance(self.data, dask_array_type):
1629             raise TypeError("rank does not work for arrays stored as dask "
1630                             "arrays. Load the data via .compute() or .load() "
1631                             "prior to calling this method.")
1632 
1633         axis = self.get_axis_num(dim)
1634         func = bn.nanrankdata if self.dtype.kind == 'f' else bn.rankdata
1635         ranked = func(self.data, axis=axis)
1636         if pct:
1637             count = np.sum(~np.isnan(self.data), axis=axis, keepdims=True)
1638             ranked /= count
1639         return Variable(self.dims, ranked)
1640 
1641     def rolling_window(self, dim, window, window_dim, center=False,
1642                        fill_value=dtypes.NA):
1643         """
1644         Make a rolling_window along dim and add a new_dim to the last place.
1645 
1646         Parameters
1647         ----------
1648         dim: str
1649             Dimension over which to compute rolling_window
1650         window: int
1651             Window size of the rolling
1652         window_dim: str
1653             New name of the window dimension.
1654         center: boolean. default False.
1655             If True, pad fill_value for both ends. Otherwise, pad in the head
1656             of the axis.
1657         fill_value:
1658             value to be filled.
1659 
1660         Returns
1661         -------
1662         Variable that is a view of the original array with a added dimension of
1663         size w.
1664         The return dim: self.dims + (window_dim, )
1665         The return shape: self.shape + (window, )
1666 
1667         Examples
1668         --------
1669         >>> v=Variable(('a', 'b'), np.arange(8).reshape((2,4)))
1670         >>> v.rolling_window(x, 'b', 3, 'window_dim')
1671         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
1672         array([[[nan, nan, 0], [nan, 0, 1], [0, 1, 2], [1, 2, 3]],
1673                [[nan, nan, 4], [nan, 4, 5], [4, 5, 6], [5, 6, 7]]])
1674 
1675         >>> v.rolling_window(x, 'b', 3, 'window_dim', center=True)
1676         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
1677         array([[[nan, 0, 1], [0, 1, 2], [1, 2, 3], [2, 3, nan]],
1678                [[nan, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, nan]]])
1679         """
1680         if fill_value is dtypes.NA:  # np.nan is passed
1681             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1682             array = self.astype(dtype, copy=False).data
1683         else:
1684             dtype = self.dtype
1685             array = self.data
1686 
1687         new_dims = self.dims + (window_dim, )
1688         return Variable(new_dims, duck_array_ops.rolling_window(
1689             array, axis=self.get_axis_num(dim), window=window,
1690             center=center, fill_value=fill_value))
1691 
1692     def coarsen(self, windows, func, boundary='exact', side='left'):
1693         """
1694         Apply
1695         """
1696         windows = {k: v for k, v in windows.items() if k in self.dims}
1697         if not windows:
1698             return self.copy()
1699 
1700         reshaped, axes = self._coarsen_reshape(windows, boundary, side)
1701         if isinstance(func, str):
1702             name = func
1703             func = getattr(duck_array_ops, name, None)
1704             if func is None:
1705                 raise NameError('{} is not a valid method.'.format(name))
1706         return type(self)(self.dims, func(reshaped, axis=axes), self._attrs)
1707 
1708     def _coarsen_reshape(self, windows, boundary, side):
1709         """
1710         Construct a reshaped-array for corsen
1711         """
1712         if not utils.is_dict_like(boundary):
1713             boundary = {d: boundary for d in windows.keys()}
1714 
1715         if not utils.is_dict_like(side):
1716             side = {d: side for d in windows.keys()}
1717 
1718         # remove unrelated dimensions
1719         boundary = {k: v for k, v in boundary.items() if k in windows}
1720         side = {k: v for k, v in side.items() if k in windows}
1721 
1722         for d, window in windows.items():
1723             if window <= 0:
1724                 raise ValueError('window must be > 0. Given {}'.format(window))
1725 
1726         variable = self
1727         for d, window in windows.items():
1728             # trim or pad the object
1729             size = variable.shape[self._get_axis_num(d)]
1730             n = int(size / window)
1731             if boundary[d] == 'exact':
1732                 if n * window != size:
1733                     raise ValueError(
1734                         'Could not coarsen a dimension of size {} with '
1735                         'window {}'.format(size, window))
1736             elif boundary[d] == 'trim':
1737                 if side[d] == 'left':
1738                     variable = variable.isel({d: slice(0, window * n)})
1739                 else:
1740                     excess = size - window * n
1741                     variable = variable.isel({d: slice(excess, None)})
1742             elif boundary[d] == 'pad':  # pad
1743                 pad = window * n - size
1744                 if pad < 0:
1745                     pad += window
1746                 if side[d] == 'left':
1747                     pad_widths = {d: (0, pad)}
1748                 else:
1749                     pad_widths = {d: (pad, 0)}
1750                 variable = variable.pad_with_fill_value(pad_widths)
1751             else:
1752                 raise TypeError(
1753                     "{} is invalid for boundary. Valid option is 'exact', "
1754                     "'trim' and 'pad'".format(boundary[d]))
1755 
1756         shape = []
1757         axes = []
1758         axis_count = 0
1759         for i, d in enumerate(variable.dims):
1760             if d in windows:
1761                 size = variable.shape[i]
1762                 shape.append(int(size / windows[d]))
1763                 shape.append(windows[d])
1764                 axis_count += 1
1765                 axes.append(i + axis_count)
1766             else:
1767                 shape.append(variable.shape[i])
1768 
1769         return variable.data.reshape(shape), tuple(axes)
1770 
1771     @property
1772     def real(self):
1773         return type(self)(self.dims, self.data.real, self._attrs)
1774 
1775     @property
1776     def imag(self):
1777         return type(self)(self.dims, self.data.imag, self._attrs)
1778 
1779     def __array_wrap__(self, obj, context=None):
1780         return Variable(self.dims, obj)
1781 
1782     @staticmethod
1783     def _unary_op(f):
1784         @functools.wraps(f)
1785         def func(self, *args, **kwargs):
1786             with np.errstate(all='ignore'):
1787                 return self.__array_wrap__(f(self.data, *args, **kwargs))
1788         return func
1789 
1790     @staticmethod
1791     def _binary_op(f, reflexive=False, **ignored_kwargs):
1792         @functools.wraps(f)
1793         def func(self, other):
1794             if isinstance(other, (xr.DataArray, xr.Dataset)):
1795                 return NotImplemented
1796             self_data, other_data, dims = _broadcast_compat_data(self, other)
1797             keep_attrs = _get_keep_attrs(default=False)
1798             attrs = self._attrs if keep_attrs else None
1799             with np.errstate(all='ignore'):
1800                 new_data = (f(self_data, other_data)
1801                             if not reflexive
1802                             else f(other_data, self_data))
1803             result = Variable(dims, new_data, attrs=attrs)
1804             return result
1805         return func
1806 
1807     @staticmethod
1808     def _inplace_binary_op(f):
1809         @functools.wraps(f)
1810         def func(self, other):
1811             if isinstance(other, xr.Dataset):
1812                 raise TypeError('cannot add a Dataset to a Variable in-place')
1813             self_data, other_data, dims = _broadcast_compat_data(self, other)
1814             if dims != self.dims:
1815                 raise ValueError('dimensions cannot change for in-place '
1816                                  'operations')
1817             with np.errstate(all='ignore'):
1818                 self.values = f(self_data, other_data)
1819             return self
1820         return func
1821 
1822     def _to_numeric(self, offset=None, datetime_unit=None, dtype=float):
1823         """ A (private) method to convert datetime array to numeric dtype
1824         See duck_array_ops.datetime_to_numeric
1825         """
1826         numeric_array = duck_array_ops.datetime_to_numeric(
1827             self.data, offset, datetime_unit, dtype)
1828         return type(self)(self.dims, numeric_array, self._attrs)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>xarray/core/variable.py</file>
<class>IndexVariable</class>
<method>copy</method>
<code>
1920     def copy(self, deep=True, data=None):
1921         """Returns a copy of this object.
1922 
1923         `deep` is ignored since data is stored in the form of
1924         pandas.Index, which is already immutable. Dimensions, attributes
1925         and encodings are always copied.
1926 
1927         Use `data` to create a new object with the same structure as
1928         original but entirely new data.
1929 
1930         Parameters
1931         ----------
1932         deep : bool, optional
1933             Deep is ignored when data is given. Whether the data array is
1934             loaded into memory and copied onto the new object. Default is True.
1935         data : array_like, optional
1936             Data to use in the new object. Must have same shape as original.
1937 
1938         Returns
1939         -------
1940         object : Variable
1941             New object with dimensions, attributes, encodings, and optionally
1942             data copied from original.
1943         """
1944         if data is None:
1945             if deep:
1946                 # self._data should be a `PandasIndexAdapter` instance at this
1947                 # point, which doesn't have a copy method, so make a deep copy
1948                 # of the underlying `pandas.MultiIndex` and create a new
1949                 # `PandasIndexAdapter` instance with it.
1950                 data = PandasIndexAdapter(self._data.array.copy(deep=True))
1951             else:
1952                 data = self._data
1953         else:
1954             data = as_compatible_data(data)
1955             if self.shape != data.shape:
1956                 raise ValueError("Data shape {} must match shape of object {}"
1957                                  .format(data.shape, self.shape))
1958         return type(self)(self.dims, data, self._attrs,
1959                           self._encoding, fastpath=True)

</code><intended_behavior>In the IndexVariable.copy method, there should be additional logic to ensure that the dtype of index variables, particularly those with Unicode types, is preserved during the copy operation. This might require a more nuanced approach when creating a new PandasIndexAdapter from the copied pandas.Index to ensure the dtype is explicitly specified or maintained, preventing unintended changes to object dtype.</intended_behavior>, <file>xarray/core/variable.py</file>
<class>IndexVariable</class>
<method>None</method>
<code>
1834 class IndexVariable(Variable):
1835     """Wrapper for accommodating a pandas.Index in an xarray.Variable.
1836 
1837     IndexVariable preserve loaded values in the form of a pandas.Index instead
1838     of a NumPy array. Hence, their values are immutable and must always be one-
1839     dimensional.
1840 
1841     They also have a name property, which is the name of their sole dimension
1842     unless another name is given.
1843     """
1844 
1845     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):
1846         super().__init__(dims, data, attrs, encoding, fastpath)
1847         if self.ndim != 1:
1848             raise ValueError('%s objects must be 1-dimensional' %
1849                              type(self).__name__)
1850 
1851         # Unlike in Variable, always eagerly load values into memory
1852         if not isinstance(self._data, PandasIndexAdapter):
1853             self._data = PandasIndexAdapter(self._data)
1854 
1855     def load(self):
1856         # data is already loaded into memory for IndexVariable
1857         return self
1858 
1859     # https://github.com/python/mypy/issues/1465
1860     @Variable.data.setter  # type: ignore
1861     def data(self, data):
1862         Variable.data.fset(self, data)
1863         if not isinstance(self._data, PandasIndexAdapter):
1864             self._data = PandasIndexAdapter(self._data)
1865 
1866     def chunk(self, chunks=None, name=None, lock=False):
1867         # Dummy - do not chunk. This method is invoked e.g. by Dataset.chunk()
1868         return self.copy(deep=False)
1869 
1870     def _finalize_indexing_result(self, dims, data):
1871         if getattr(data, 'ndim', 0) != 1:
1872             # returns Variable rather than IndexVariable if multi-dimensional
1873             return Variable(dims, data, self._attrs, self._encoding)
1874         else:
1875             return type(self)(dims, data, self._attrs,
1876                               self._encoding, fastpath=True)
1877 
1878     def __setitem__(self, key, value):
1879         raise TypeError('%s values cannot be modified' % type(self).__name__)
1880 
1881     @classmethod
1882     def concat(cls, variables, dim='concat_dim', positions=None,
1883                shortcut=False):
1884         """Specialized version of Variable.concat for IndexVariable objects.
1885 
1886         This exists because we want to avoid converting Index objects to NumPy
1887         arrays, if possible.
1888         """
1889         if not isinstance(dim, str):
1890             dim, = dim.dims
1891 
1892         variables = list(variables)
1893         first_var = variables[0]
1894 
1895         if any(not isinstance(v, cls) for v in variables):
1896             raise TypeError('IndexVariable.concat requires that all input '
1897                             'variables be IndexVariable objects')
1898 
1899         indexes = [v._data.array for v in variables]
1900 
1901         if not indexes:
1902             data = []
1903         else:
1904             data = indexes[0].append(indexes[1:])
1905 
1906             if positions is not None:
1907                 indices = nputils.inverse_permutation(
1908                     np.concatenate(positions))
1909                 data = data.take(indices)
1910 
1911         attrs = OrderedDict(first_var.attrs)
1912         if not shortcut:
1913             for var in variables:
1914                 if var.dims != first_var.dims:
1915                     raise ValueError('inconsistent dimensions')
1916                 utils.remove_incompatible_items(attrs, var.attrs)
1917 
1918         return cls(first_var.dims, data, attrs)
1919 
1920     def copy(self, deep=True, data=None):
1921         """Returns a copy of this object.
1922 
1923         `deep` is ignored since data is stored in the form of
1924         pandas.Index, which is already immutable. Dimensions, attributes
1925         and encodings are always copied.
1926 
1927         Use `data` to create a new object with the same structure as
1928         original but entirely new data.
1929 
1930         Parameters
1931         ----------
1932         deep : bool, optional
1933             Deep is ignored when data is given. Whether the data array is
1934             loaded into memory and copied onto the new object. Default is True.
1935         data : array_like, optional
1936             Data to use in the new object. Must have same shape as original.
1937 
1938         Returns
1939         -------
1940         object : Variable
1941             New object with dimensions, attributes, encodings, and optionally
1942             data copied from original.
1943         """
1944         if data is None:
1945             if deep:
1946                 # self._data should be a `PandasIndexAdapter` instance at this
1947                 # point, which doesn't have a copy method, so make a deep copy
1948                 # of the underlying `pandas.MultiIndex` and create a new
1949                 # `PandasIndexAdapter` instance with it.
1950                 data = PandasIndexAdapter(self._data.array.copy(deep=True))
1951             else:
1952                 data = self._data
1953         else:
1954             data = as_compatible_data(data)
1955             if self.shape != data.shape:
1956                 raise ValueError("Data shape {} must match shape of object {}"
1957                                  .format(data.shape, self.shape))
1958         return type(self)(self.dims, data, self._attrs,
1959                           self._encoding, fastpath=True)
1960 
1961     def equals(self, other, equiv=None):
1962         # if equiv is specified, super up
1963         if equiv is not None:
1964             return super().equals(other, equiv)
1965 
1966         # otherwise use the native index equals, rather than looking at _data
1967         other = getattr(other, 'variable', other)
1968         try:
1969             return (self.dims == other.dims and
1970                     self._data_equals(other))
1971         except (TypeError, AttributeError):
1972             return False
1973 
1974     def _data_equals(self, other):
1975         return self.to_index().equals(other.to_index())
1976 
1977     def to_index_variable(self):
1978         """Return this variable as an xarray.IndexVariable"""
1979         return self
1980 
1981     to_coord = utils.alias(to_index_variable, 'to_coord')
1982 
1983     def to_index(self):
1984         """Convert this variable to a pandas.Index"""
1985         # n.b. creating a new pandas.Index from an old pandas.Index is
1986         # basically free as pandas.Index objects are immutable
1987         assert self.ndim == 1
1988         index = self._data.array
1989         if isinstance(index, pd.MultiIndex):
1990             # set default names for multi-index unnamed levels so that
1991             # we can safely rename dimension / coordinate later
1992             valid_level_names = [name or '{}_level_{}'.format(self.dims[0], i)
1993                                  for i, name in enumerate(index.names)]
1994             index = index.set_names(valid_level_names)
1995         else:
1996             index = index.set_names(self.name)
1997         return index
1998 
1999     @property
2000     def level_names(self):
2001         """Return MultiIndex level names or None if this IndexVariable has no
2002         MultiIndex.
2003         """
2004         index = self.to_index()
2005         if isinstance(index, pd.MultiIndex):
2006             return index.names
2007         else:
2008             return None
2009 
2010     def get_level_variable(self, level):
2011         """Return a new IndexVariable from a given MultiIndex level."""
2012         if self.level_names is None:
2013             raise ValueError("IndexVariable %r has no MultiIndex" % self.name)
2014         index = self.to_index()
2015         return type(self)(self.dims, index.get_level_values(level))
2016 
2017     @property
2018     def name(self):
2019         return self.dims[0]
2020 
2021     @name.setter
2022     def name(self, value):
2023         raise AttributeError('cannot modify name of IndexVariable in-place')

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 15:11:37.916 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 15:11:37.916 | INFO     | Search-only: skipping patch selection
2025-10-18 15:11:37.980 | INFO     | Task pydata__xarray-3095 completed successfully.
2025-10-18 15:11:37.981 | INFO     | No patch generated. You can try running ACR again.
