2025-10-18 12:25:48.619 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 12:25:53.399 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=295, output_tokens=12, cost=0.003310
2025-10-18 12:25:53.400 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-18 12:25:53.403 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 12:26:00.857 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=580, output_tokens=285, cost=0.014350
2025-10-18 12:26:00.861 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 12:26:02.546 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=639, output_tokens=57, cost=0.008100
2025-10-18 12:26:02.548 | DEBUG    | Extracted a valid json.
2025-10-18 12:26:02.548 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"ExpressionWrapper\")",
        "search_class(\"Q\")",
        "search_method(\"annotate\")",
        "search_method(\"values\")",
        "search_method(\"Count\")"
    ],
    "bug_locations": []
}
2025-10-18 12:26:02.589 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 12:26:02.590 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 12:26:19.666 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1621, output_tokens=802, cost=0.040270
2025-10-18 12:26:19.683 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 12:26:28.038 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2639, output_tokens=347, cost=0.036800
2025-10-18 12:26:28.045 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 12:26:35.045 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=701, output_tokens=296, cost=0.015890
2025-10-18 12:26:35.046 | DEBUG    | Extracted a valid json.
2025-10-18 12:26:35.046 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "django/db/models/expressions.py",
            "class": "ExpressionWrapper",
            "method": "get_group_by_cols",
            "intended_behavior": "The method `get_group_by_cols` should be revised to ensure that when an `ExpressionWrapper` contains an expression that affects grouping (e.g., when wrapping a `Q` object to produce a conditional expression like '\"bob_id\" IS NULL'), this expression is appropriately included in the `GROUP BY` clause of the generated SQL query. This change must preserve the method's ability to handle other expressions and maintain compatibility with expressions that do not directly affect grouping."
        },
        {
            "file": "django/db/models/query.py",
            "class": "QuerySet",
            "method": "annotate",
            "intended_behavior": "The `annotate` method should be inspected and possibly adjusted to better handle cases where annotated fields (especially those wrapped with `ExpressionWrapper`) are used with the `values` and `annotate` methods together in a queryset. Specifically, it should ensure that expressions affecting the `GROUP BY` clause, as defined in the `annotate` method call, are accurately represented in the SQL query. This involves preserving the logic for handling different types of expressions within `annotate` calls, ensuring that the final SQL query accurately reflects the intended groupings and aggregates."
        }
    ]
}
2025-10-18 12:26:35.051 | DEBUG    | Found override of get_group_by_cols in BaseExpression
2025-10-18 12:26:35.052 | ERROR    | SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
Traceback (most recent call last):

  File "/home/kimnal0/auto-code-rover/app/main.py", line 656, in <module>
    main()
    └ <function main at 0x761e6b526c00>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 120, in main
    run_task_groups(groups, num_processes, organize_output=True)
    │               │       └ 1
    │               └ {'setup_astropy__astropy__4.3': [<app.raw_tasks.RawSweTask object at 0x761d78dc37d0>, <app.raw_tasks.RawSweTask object at 0x7...
    └ <function run_task_groups at 0x761d7b11a3e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 396, in run_task_groups
    run_tasks_serial(all_tasks)
    │                └ [<app.raw_tasks.RawSweTask object at 0x761d78dc37d0>, <app.raw_tasks.RawSweTask object at 0x761d78dc3800>, <app.raw_tasks.Raw...
    └ <function run_tasks_serial at 0x761d7b11a480>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 414, in run_tasks_serial
    run_task_in_subprocess(task)
    │                      └ <app.raw_tasks.RawSweTask object at 0x761d78d39df0>
    └ <function run_task_in_subprocess at 0x761d7b11a660>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 462, in run_task_in_subprocess
    executor.submit(run_raw_task, task)
    │        │      │             └ <app.raw_tasks.RawSweTask object at 0x761d78d39df0>
    │        │      └ <function run_raw_task at 0x761d7b11a700>
    │        └ <function ProcessPoolExecutor.submit at 0x761e6ae72020>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x761d7b1d56d0>

  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 831, in submit
    self._start_executor_manager_thread()
    │    └ <function ProcessPoolExecutor._start_executor_manager_thread at 0x761e6ae71da0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x761d7b1d56d0>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 770, in _start_executor_manager_thread
    self._launch_processes()
    │    └ <function ProcessPoolExecutor._launch_processes at 0x761e6ae71ee0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x761d7b1d56d0>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 797, in _launch_processes
    self._spawn_process()
    │    └ <function ProcessPoolExecutor._spawn_process at 0x761e6ae71f80>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x761d7b1d56d0>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 807, in _spawn_process
    p.start()
    │ └ <function BaseProcess.start at 0x761e6b02cea0>
    └ <ForkProcess name='ForkProcess-140' parent=497161 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
    │    │        │    │      └ <ForkProcess name='ForkProcess-140' parent=497161 started>
    │    │        │    └ <staticmethod(<function ForkProcess._Popen at 0x761e6b09e0c0>)>
    │    │        └ <ForkProcess name='ForkProcess-140' parent=497161 started>
    │    └ None
    └ <ForkProcess name='ForkProcess-140' parent=497161 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/context.py", line 282, in _Popen
    return Popen(process_obj)
           │     └ <ForkProcess name='ForkProcess-140' parent=497161 started>
           └ <class 'multiprocessing.popen_fork.Popen'>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
    │    │       └ <ForkProcess name='ForkProcess-140' parent=497161 started>
    │    └ <function Popen._launch at 0x761d7911b7e0>
    └ <multiprocessing.popen_fork.Popen object at 0x761d7a57e8a0>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
           │           │                          └ 13
           │           └ <function BaseProcess._bootstrap at 0x761e6b02d8a0>
           └ <ForkProcess name='ForkProcess-140' parent=497161 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x761e6b02ce00>
    └ <ForkProcess name='ForkProcess-140' parent=497161 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {}
    │    │        │    │        └ <ForkProcess name='ForkProcess-140' parent=497161 started>
    │    │        │    └ (<concurrent.futures.process._SafeQueue object at 0x761d79006120>, <multiprocessing.queues.SimpleQueue object at 0x761d795c79...
    │    │        └ <ForkProcess name='ForkProcess-140' parent=497161 started>
    │    └ <function _process_worker at 0x761e6ae711c0>
    └ <ForkProcess name='ForkProcess-140' parent=497161 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 263, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
        │         │   │         │       │         └ {}
        │         │   │         │       └ <concurrent.futures.process._CallItem object at 0x761d7b442db0>
        │         │   │         └ (<app.raw_tasks.RawSweTask object at 0x761d7a57e9f0>,)
        │         │   └ <concurrent.futures.process._CallItem object at 0x761d7b442db0>
        │         └ <function run_raw_task at 0x761d7b11a700>
        └ <concurrent.futures.process._CallItem object at 0x761d7b442db0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 494, in run_raw_task
    run_ok = do_inference(task.to_task(), task_output_dir)
             │            │    │          └ '/home/kimnal0/auto-code-rover/only_fl_output5/django__django-13689_2025-10-18_12-25-48'
             │            │    └ <function RawSweTask.to_task at 0x761d7b1196c0>
             │            └ <app.raw_tasks.RawSweTask object at 0x761d7a57e9f0>
             └ <function do_inference at 0x761d7b11a8e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 590, in do_inference
    run_ok = inference.run_one_task(
             │         └ <function run_one_task at 0x761d7b228400>
             └ <module 'app.inference' from '/home/kimnal0/auto-code-rover/app/inference.py'>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 128, in run_one_task
    if _run_one_task(str(out_dir), api_manager, task.get_issue_statement()):
       │                 │         │            │    └ <function SweTask.get_issue_statement at 0x761d7b9b84a0>
       │                 │         │            └ SweTask(task_id='django__django-13689', problem_statement='Aggregating when grouping on an ExpressionWrapper omits the expres...
       │                 │         └ <app.manage.ProjectApiManager object at 0x761d7b21d610>
       │                 └ Path('/home/kimnal0/auto-code-rover/only_fl_output5/django__django-13689_2025-10-18_12-25-48/output_0')
       └ <function _run_one_task at 0x761d7b2298a0>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 303, in _run_one_task
    bug_locs, search_msg_thread = api_manager.search_manager.search_iterative(
                                  │           │              └ <function SearchManager.search_iterative at 0x761d7b431bc0>
                                  │           └ <app.search.search_manage.SearchManager object at 0x761d795c7ec0>
                                  └ <app.manage.ProjectApiManager object at 0x761d7b21d610>

  File "/home/kimnal0/auto-code-rover/app/search/search_manage.py", line 125, in search_iterative
    new_bug_locations.extend(self.backend.get_bug_loc_snippets_new(loc))
    │                 │      │    │       │                        └ {'file': 'django/db/models/expressions.py', 'class': 'ExpressionWrapper', 'method': 'get_group_by_cols', 'intended_behavior':...
    │                 │      │    │       └ <function SearchBackend.get_bug_loc_snippets_new at 0x761d7b4318a0>
    │                 │      │    └ <app.search.search_backend.SearchBackend object at 0x761d7b6a3170>
    │                 │      └ <app.search.search_manage.SearchManager object at 0x761d795c7ec0>
    │                 └ <method 'extend' of 'list' objects>
    └ []

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 825, in get_bug_loc_snippets_new
    self._get_inherited_methods(res.class_name, res.func_name)
    │    │                      │   │           │   └ 'get_group_by_cols'
    │    │                      │   │           └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__3.2/django/db/mo...
    │    │                      │   └ 'ExpressionWrapper'
    │    │                      └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__3.2/django/db/mo...
    │    └ <function SearchBackend._get_inherited_methods at 0x761d7b431800>
    └ <app.search.search_backend.SearchBackend object at 0x761d7b6a3170>

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 748, in _get_inherited_methods
    output, search_res, call_ok = self.search_method_in_class(super_call)
                                  │    │                      └ {'class_name': 'BaseExpression', 'method_name': 'get_group_by_cols'}
                                  │    └ <function SearchBackend.search_method_in_class at 0x761d7b430f40>
                                  └ <app.search.search_backend.SearchBackend object at 0x761d7b6a3170>

> File "/home/kimnal0/auto-code-rover/app/utils.py", line 343, in wrapper
    return func(*args, **kwargs)
           │     │       └ {}
           │     └ (<app.search.search_backend.SearchBackend object at 0x761d7b6a3170>, {'class_name': 'BaseExpression', 'method_name': 'get_gro...
           └ <function SearchBackend.search_method_in_class at 0x761d7b430ea0>

TypeError: SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
2025-10-18 12:26:35.066 | DEBUG    | Bug location extracted successfully: [<file>django/db/models/expressions.py</file>
<class>ExpressionWrapper</class>
<method>get_group_by_cols</method>
<code>
917     def get_group_by_cols(self, alias=None):
918         expression = self.expression.copy()
919         expression.output_field = self.output_field
920         return expression.get_group_by_cols(alias=alias)

</code><intended_behavior>The method `get_group_by_cols` should be revised to ensure that when an `ExpressionWrapper` contains an expression that affects grouping (e.g., when wrapping a `Q` object to produce a conditional expression like '"bob_id" IS NULL'), this expression is appropriately included in the `GROUP BY` clause of the generated SQL query. This change must preserve the method's ability to handle other expressions and maintain compatibility with expressions that do not directly affect grouping.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>ExpressionWrapper</class>
<method>None</method>
<code>
901 class ExpressionWrapper(Expression):
902     """
903     An expression that can wrap another expression so that it can provide
904     extra context to the inner expression, such as the output_field.
905     """
906 
907     def __init__(self, expression, output_field):
908         super().__init__(output_field=output_field)
909         self.expression = expression
910 
911     def set_source_expressions(self, exprs):
912         self.expression = exprs[0]
913 
914     def get_source_expressions(self):
915         return [self.expression]
916 
917     def get_group_by_cols(self, alias=None):
918         expression = self.expression.copy()
919         expression.output_field = self.output_field
920         return expression.get_group_by_cols(alias=alias)
921 
922     def as_sql(self, compiler, connection):
923         return compiler.compile(self.expression)
924 
925     def __repr__(self):
926         return "{}({})".format(self.__class__.__name__, self.expression)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>annotate</method>
<code>
1083     def annotate(self, *args, **kwargs):
1084         """
1085         Return a query set in which the returned objects have been annotated
1086         with extra data or aggregations.
1087         """
1088         self._not_support_combined_queries('annotate')
1089         return self._annotate(args, kwargs, select=True)

</code><intended_behavior>The `annotate` method should be inspected and possibly adjusted to better handle cases where annotated fields (especially those wrapped with `ExpressionWrapper`) are used with the `values` and `annotate` methods together in a queryset. Specifically, it should ensure that expressions affecting the `GROUP BY` clause, as defined in the `annotate` method call, are accurately represented in the SQL query. This involves preserving the logic for handling different types of expressions within `annotate` calls, ensuring that the final SQL query accurately reflects the intended groupings and aggregates.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
175 class QuerySet:
176     """Represent a lazy database lookup for a set of objects."""
177 
178     def __init__(self, model=None, query=None, using=None, hints=None):
179         self.model = model
180         self._db = using
181         self._hints = hints or {}
182         self._query = query or sql.Query(self.model)
183         self._result_cache = None
184         self._sticky_filter = False
185         self._for_write = False
186         self._prefetch_related_lookups = ()
187         self._prefetch_done = False
188         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
189         self._iterable_class = ModelIterable
190         self._fields = None
191         self._defer_next_filter = False
192         self._deferred_filter = None
193 
194     @property
195     def query(self):
196         if self._deferred_filter:
197             negate, args, kwargs = self._deferred_filter
198             self._filter_or_exclude_inplace(negate, args, kwargs)
199             self._deferred_filter = None
200         return self._query
201 
202     @query.setter
203     def query(self, value):
204         if value.values_select:
205             self._iterable_class = ValuesIterable
206         self._query = value
207 
208     def as_manager(cls):
209         # Address the circular dependency between `Queryset` and `Manager`.
210         from django.db.models.manager import Manager
211         manager = Manager.from_queryset(cls)()
212         manager._built_with_as_manager = True
213         return manager
214     as_manager.queryset_only = True
215     as_manager = classmethod(as_manager)
216 
217     ########################
218     # PYTHON MAGIC METHODS #
219     ########################
220 
221     def __deepcopy__(self, memo):
222         """Don't populate the QuerySet's cache."""
223         obj = self.__class__()
224         for k, v in self.__dict__.items():
225             if k == '_result_cache':
226                 obj.__dict__[k] = None
227             else:
228                 obj.__dict__[k] = copy.deepcopy(v, memo)
229         return obj
230 
231     def __getstate__(self):
232         # Force the cache to be fully populated.
233         self._fetch_all()
234         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
235 
236     def __setstate__(self, state):
237         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
238         if pickled_version:
239             if pickled_version != django.__version__:
240                 warnings.warn(
241                     "Pickled queryset instance's Django version %s does not "
242                     "match the current version %s."
243                     % (pickled_version, django.__version__),
244                     RuntimeWarning,
245                     stacklevel=2,
246                 )
247         else:
248             warnings.warn(
249                 "Pickled queryset instance's Django version is not specified.",
250                 RuntimeWarning,
251                 stacklevel=2,
252             )
253         self.__dict__.update(state)
254 
255     def __repr__(self):
256         data = list(self[:REPR_OUTPUT_SIZE + 1])
257         if len(data) > REPR_OUTPUT_SIZE:
258             data[-1] = "...(remaining elements truncated)..."
259         return '<%s %r>' % (self.__class__.__name__, data)
260 
261     def __len__(self):
262         self._fetch_all()
263         return len(self._result_cache)
264 
265     def __iter__(self):
266         """
267         The queryset iterator protocol uses three nested iterators in the
268         default case:
269             1. sql.compiler.execute_sql()
270                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
271                  using cursor.fetchmany(). This part is responsible for
272                  doing some column masking, and returning the rows in chunks.
273             2. sql.compiler.results_iter()
274                - Returns one row at time. At this point the rows are still just
275                  tuples. In some cases the return values are converted to
276                  Python values at this location.
277             3. self.iterator()
278                - Responsible for turning the rows into model objects.
279         """
280         self._fetch_all()
281         return iter(self._result_cache)
282 
283     def __bool__(self):
284         self._fetch_all()
285         return bool(self._result_cache)
286 
287     def __getitem__(self, k):
288         """Retrieve an item or slice from the set of results."""
289         if not isinstance(k, (int, slice)):
290             raise TypeError(
291                 'QuerySet indices must be integers or slices, not %s.'
292                 % type(k).__name__
293             )
294         assert ((not isinstance(k, slice) and (k >= 0)) or
295                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
296                  (k.stop is None or k.stop >= 0))), \
297             "Negative indexing is not supported."
298 
299         if self._result_cache is not None:
300             return self._result_cache[k]
301 
302         if isinstance(k, slice):
303             qs = self._chain()
304             if k.start is not None:
305                 start = int(k.start)
306             else:
307                 start = None
308             if k.stop is not None:
309                 stop = int(k.stop)
310             else:
311                 stop = None
312             qs.query.set_limits(start, stop)
313             return list(qs)[::k.step] if k.step else qs
314 
315         qs = self._chain()
316         qs.query.set_limits(k, k + 1)
317         qs._fetch_all()
318         return qs._result_cache[0]
319 
320     def __class_getitem__(cls, *args, **kwargs):
321         return cls
322 
323     def __and__(self, other):
324         self._merge_sanity_check(other)
325         if isinstance(other, EmptyQuerySet):
326             return other
327         if isinstance(self, EmptyQuerySet):
328             return self
329         combined = self._chain()
330         combined._merge_known_related_objects(other)
331         combined.query.combine(other.query, sql.AND)
332         return combined
333 
334     def __or__(self, other):
335         self._merge_sanity_check(other)
336         if isinstance(self, EmptyQuerySet):
337             return other
338         if isinstance(other, EmptyQuerySet):
339             return self
340         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
341         combined = query._chain()
342         combined._merge_known_related_objects(other)
343         if not other.query.can_filter():
344             other = other.model._base_manager.filter(pk__in=other.values('pk'))
345         combined.query.combine(other.query, sql.OR)
346         return combined
347 
348     ####################################
349     # METHODS THAT DO DATABASE QUERIES #
350     ####################################
351 
352     def _iterator(self, use_chunked_fetch, chunk_size):
353         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
354 
355     def iterator(self, chunk_size=2000):
356         """
357         An iterator over the results from applying this QuerySet to the
358         database.
359         """
360         if chunk_size <= 0:
361             raise ValueError('Chunk size must be strictly positive.')
362         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
363         return self._iterator(use_chunked_fetch, chunk_size)
364 
365     def aggregate(self, *args, **kwargs):
366         """
367         Return a dictionary containing the calculations (aggregation)
368         over the current queryset.
369 
370         If args is present the expression is passed as a kwarg using
371         the Aggregate object's default alias.
372         """
373         if self.query.distinct_fields:
374             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
375         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
376         for arg in args:
377             # The default_alias property raises TypeError if default_alias
378             # can't be set automatically or AttributeError if it isn't an
379             # attribute.
380             try:
381                 arg.default_alias
382             except (AttributeError, TypeError):
383                 raise TypeError("Complex aggregates require an alias")
384             kwargs[arg.default_alias] = arg
385 
386         query = self.query.chain()
387         for (alias, aggregate_expr) in kwargs.items():
388             query.add_annotation(aggregate_expr, alias, is_summary=True)
389             annotation = query.annotations[alias]
390             if not annotation.contains_aggregate:
391                 raise TypeError("%s is not an aggregate expression" % alias)
392             for expr in annotation.get_source_expressions():
393                 if expr.contains_aggregate and isinstance(expr, Ref) and expr.refs in kwargs:
394                     name = expr.refs
395                     raise exceptions.FieldError(
396                         "Cannot compute %s('%s'): '%s' is an aggregate"
397                         % (annotation.name, name, name)
398                     )
399         return query.get_aggregation(self.db, kwargs)
400 
401     def count(self):
402         """
403         Perform a SELECT COUNT() and return the number of records as an
404         integer.
405 
406         If the QuerySet is already fully cached, return the length of the
407         cached results set to avoid multiple SELECT COUNT(*) calls.
408         """
409         if self._result_cache is not None:
410             return len(self._result_cache)
411 
412         return self.query.get_count(using=self.db)
413 
414     def get(self, *args, **kwargs):
415         """
416         Perform the query and return a single object matching the given
417         keyword arguments.
418         """
419         if self.query.combinator and (args or kwargs):
420             raise NotSupportedError(
421                 'Calling QuerySet.get(...) with filters after %s() is not '
422                 'supported.' % self.query.combinator
423             )
424         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
425         if self.query.can_filter() and not self.query.distinct_fields:
426             clone = clone.order_by()
427         limit = None
428         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
429             limit = MAX_GET_RESULTS
430             clone.query.set_limits(high=limit)
431         num = len(clone)
432         if num == 1:
433             return clone._result_cache[0]
434         if not num:
435             raise self.model.DoesNotExist(
436                 "%s matching query does not exist." %
437                 self.model._meta.object_name
438             )
439         raise self.model.MultipleObjectsReturned(
440             'get() returned more than one %s -- it returned %s!' % (
441                 self.model._meta.object_name,
442                 num if not limit or num < limit else 'more than %s' % (limit - 1),
443             )
444         )
445 
446     def create(self, **kwargs):
447         """
448         Create a new object with the given kwargs, saving it to the database
449         and returning the created object.
450         """
451         obj = self.model(**kwargs)
452         self._for_write = True
453         obj.save(force_insert=True, using=self.db)
454         return obj
455 
456     def _prepare_for_bulk_create(self, objs):
457         for obj in objs:
458             if obj.pk is None:
459                 # Populate new PK values.
460                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
461             obj._prepare_related_fields_for_save(operation_name='bulk_create')
462 
463     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
464         """
465         Insert each of the instances into the database. Do *not* call
466         save() on each of the instances, do not send any pre/post_save
467         signals, and do not set the primary key attribute if it is an
468         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
469         Multi-table models are not supported.
470         """
471         # When you bulk insert you don't get the primary keys back (if it's an
472         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
473         # you can't insert into the child tables which references this. There
474         # are two workarounds:
475         # 1) This could be implemented if you didn't have an autoincrement pk
476         # 2) You could do it by doing O(n) normal inserts into the parent
477         #    tables to get the primary keys back and then doing a single bulk
478         #    insert into the childmost table.
479         # We currently set the primary keys on the objects when using
480         # PostgreSQL via the RETURNING ID clause. It should be possible for
481         # Oracle as well, but the semantics for extracting the primary keys is
482         # trickier so it's not done yet.
483         assert batch_size is None or batch_size > 0
484         # Check that the parents share the same concrete model with the our
485         # model to detect the inheritance pattern ConcreteGrandParent ->
486         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
487         # would not identify that case as involving multiple tables.
488         for parent in self.model._meta.get_parent_list():
489             if parent._meta.concrete_model is not self.model._meta.concrete_model:
490                 raise ValueError("Can't bulk create a multi-table inherited model")
491         if not objs:
492             return objs
493         self._for_write = True
494         connection = connections[self.db]
495         opts = self.model._meta
496         fields = opts.concrete_fields
497         objs = list(objs)
498         self._prepare_for_bulk_create(objs)
499         with transaction.atomic(using=self.db, savepoint=False):
500             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
501             if objs_with_pk:
502                 returned_columns = self._batched_insert(
503                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
504                 )
505                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
506                     for result, field in zip(results, opts.db_returning_fields):
507                         if field != opts.pk:
508                             setattr(obj_with_pk, field.attname, result)
509                 for obj_with_pk in objs_with_pk:
510                     obj_with_pk._state.adding = False
511                     obj_with_pk._state.db = self.db
512             if objs_without_pk:
513                 fields = [f for f in fields if not isinstance(f, AutoField)]
514                 returned_columns = self._batched_insert(
515                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
516                 )
517                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
518                     assert len(returned_columns) == len(objs_without_pk)
519                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
520                     for result, field in zip(results, opts.db_returning_fields):
521                         setattr(obj_without_pk, field.attname, result)
522                     obj_without_pk._state.adding = False
523                     obj_without_pk._state.db = self.db
524 
525         return objs
526 
527     def bulk_update(self, objs, fields, batch_size=None):
528         """
529         Update the given fields in each of the given objects in the database.
530         """
531         if batch_size is not None and batch_size < 0:
532             raise ValueError('Batch size must be a positive integer.')
533         if not fields:
534             raise ValueError('Field names must be given to bulk_update().')
535         objs = tuple(objs)
536         if any(obj.pk is None for obj in objs):
537             raise ValueError('All bulk_update() objects must have a primary key set.')
538         fields = [self.model._meta.get_field(name) for name in fields]
539         if any(not f.concrete or f.many_to_many for f in fields):
540             raise ValueError('bulk_update() can only be used with concrete fields.')
541         if any(f.primary_key for f in fields):
542             raise ValueError('bulk_update() cannot be used with primary key fields.')
543         if not objs:
544             return
545         # PK is used twice in the resulting update query, once in the filter
546         # and once in the WHEN. Each field will also have one CAST.
547         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
548         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
549         requires_casting = connections[self.db].features.requires_casted_case_in_updates
550         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
551         updates = []
552         for batch_objs in batches:
553             update_kwargs = {}
554             for field in fields:
555                 when_statements = []
556                 for obj in batch_objs:
557                     attr = getattr(obj, field.attname)
558                     if not isinstance(attr, Expression):
559                         attr = Value(attr, output_field=field)
560                     when_statements.append(When(pk=obj.pk, then=attr))
561                 case_statement = Case(*when_statements, output_field=field)
562                 if requires_casting:
563                     case_statement = Cast(case_statement, output_field=field)
564                 update_kwargs[field.attname] = case_statement
565             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
566         with transaction.atomic(using=self.db, savepoint=False):
567             for pks, update_kwargs in updates:
568                 self.filter(pk__in=pks).update(**update_kwargs)
569     bulk_update.alters_data = True
570 
571     def get_or_create(self, defaults=None, **kwargs):
572         """
573         Look up an object with the given kwargs, creating one if necessary.
574         Return a tuple of (object, created), where created is a boolean
575         specifying whether an object was created.
576         """
577         # The get() needs to be targeted at the write database in order
578         # to avoid potential transaction consistency problems.
579         self._for_write = True
580         try:
581             return self.get(**kwargs), False
582         except self.model.DoesNotExist:
583             params = self._extract_model_params(defaults, **kwargs)
584             # Try to create an object using passed params.
585             try:
586                 with transaction.atomic(using=self.db):
587                     params = dict(resolve_callables(params))
588                     return self.create(**params), True
589             except IntegrityError:
590                 try:
591                     return self.get(**kwargs), False
592                 except self.model.DoesNotExist:
593                     pass
594                 raise
595 
596     def update_or_create(self, defaults=None, **kwargs):
597         """
598         Look up an object with the given kwargs, updating one with defaults
599         if it exists, otherwise create a new one.
600         Return a tuple (object, created), where created is a boolean
601         specifying whether an object was created.
602         """
603         defaults = defaults or {}
604         self._for_write = True
605         with transaction.atomic(using=self.db):
606             # Lock the row so that a concurrent update is blocked until
607             # update_or_create() has performed its save.
608             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)
609             if created:
610                 return obj, created
611             for k, v in resolve_callables(defaults):
612                 setattr(obj, k, v)
613             obj.save(using=self.db)
614         return obj, False
615 
616     def _extract_model_params(self, defaults, **kwargs):
617         """
618         Prepare `params` for creating a model instance based on the given
619         kwargs; for use by get_or_create().
620         """
621         defaults = defaults or {}
622         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
623         params.update(defaults)
624         property_names = self.model._meta._property_names
625         invalid_params = []
626         for param in params:
627             try:
628                 self.model._meta.get_field(param)
629             except exceptions.FieldDoesNotExist:
630                 # It's okay to use a model's property if it has a setter.
631                 if not (param in property_names and getattr(self.model, param).fset):
632                     invalid_params.append(param)
633         if invalid_params:
634             raise exceptions.FieldError(
635                 "Invalid field name(s) for model %s: '%s'." % (
636                     self.model._meta.object_name,
637                     "', '".join(sorted(invalid_params)),
638                 ))
639         return params
640 
641     def _earliest(self, *fields):
642         """
643         Return the earliest object according to fields (if given) or by the
644         model's Meta.get_latest_by.
645         """
646         if fields:
647             order_by = fields
648         else:
649             order_by = getattr(self.model._meta, 'get_latest_by')
650             if order_by and not isinstance(order_by, (tuple, list)):
651                 order_by = (order_by,)
652         if order_by is None:
653             raise ValueError(
654                 "earliest() and latest() require either fields as positional "
655                 "arguments or 'get_latest_by' in the model's Meta."
656             )
657 
658         assert not self.query.is_sliced, \
659             "Cannot change a query once a slice has been taken."
660         obj = self._chain()
661         obj.query.set_limits(high=1)
662         obj.query.clear_ordering(force_empty=True)
663         obj.query.add_ordering(*order_by)
664         return obj.get()
665 
666     def earliest(self, *fields):
667         return self._earliest(*fields)
668 
669     def latest(self, *fields):
670         return self.reverse()._earliest(*fields)
671 
672     def first(self):
673         """Return the first object of a query or None if no match is found."""
674         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
675             return obj
676 
677     def last(self):
678         """Return the last object of a query or None if no match is found."""
679         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
680             return obj
681 
682     def in_bulk(self, id_list=None, *, field_name='pk'):
683         """
684         Return a dictionary mapping each of the given IDs to the object with
685         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
686         """
687         assert not self.query.is_sliced, \
688             "Cannot use 'limit' or 'offset' with in_bulk"
689         opts = self.model._meta
690         unique_fields = [
691             constraint.fields[0]
692             for constraint in opts.total_unique_constraints
693             if len(constraint.fields) == 1
694         ]
695         if (
696             field_name != 'pk' and
697             not opts.get_field(field_name).unique and
698             field_name not in unique_fields and
699             self.query.distinct_fields != (field_name,)
700         ):
701             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
702         if id_list is not None:
703             if not id_list:
704                 return {}
705             filter_key = '{}__in'.format(field_name)
706             batch_size = connections[self.db].features.max_query_params
707             id_list = tuple(id_list)
708             # If the database has a limit on the number of query parameters
709             # (e.g. SQLite), retrieve objects in batches if necessary.
710             if batch_size and batch_size < len(id_list):
711                 qs = ()
712                 for offset in range(0, len(id_list), batch_size):
713                     batch = id_list[offset:offset + batch_size]
714                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
715             else:
716                 qs = self.filter(**{filter_key: id_list}).order_by()
717         else:
718             qs = self._chain()
719         return {getattr(obj, field_name): obj for obj in qs}
720 
721     def delete(self):
722         """Delete the records in the current QuerySet."""
723         self._not_support_combined_queries('delete')
724         assert not self.query.is_sliced, \
725             "Cannot use 'limit' or 'offset' with delete."
726 
727         if self._fields is not None:
728             raise TypeError("Cannot call delete() after .values() or .values_list()")
729 
730         del_query = self._chain()
731 
732         # The delete is actually 2 queries - one to find related objects,
733         # and one to delete. Make sure that the discovery of related
734         # objects is performed on the same database as the deletion.
735         del_query._for_write = True
736 
737         # Disable non-supported fields.
738         del_query.query.select_for_update = False
739         del_query.query.select_related = False
740         del_query.query.clear_ordering(force_empty=True)
741 
742         collector = Collector(using=del_query.db)
743         collector.collect(del_query)
744         deleted, _rows_count = collector.delete()
745 
746         # Clear the result cache, in case this QuerySet gets reused.
747         self._result_cache = None
748         return deleted, _rows_count
749 
750     delete.alters_data = True
751     delete.queryset_only = True
752 
753     def _raw_delete(self, using):
754         """
755         Delete objects found from the given queryset in single direct SQL
756         query. No signals are sent and there is no protection for cascades.
757         """
758         query = self.query.clone()
759         query.__class__ = sql.DeleteQuery
760         cursor = query.get_compiler(using).execute_sql(CURSOR)
761         if cursor:
762             with cursor:
763                 return cursor.rowcount
764         return 0
765     _raw_delete.alters_data = True
766 
767     def update(self, **kwargs):
768         """
769         Update all elements in the current QuerySet, setting all the given
770         fields to the appropriate values.
771         """
772         self._not_support_combined_queries('update')
773         assert not self.query.is_sliced, \
774             "Cannot update a query once a slice has been taken."
775         self._for_write = True
776         query = self.query.chain(sql.UpdateQuery)
777         query.add_update_values(kwargs)
778         # Clear any annotations so that they won't be present in subqueries.
779         query.annotations = {}
780         with transaction.mark_for_rollback_on_error(using=self.db):
781             rows = query.get_compiler(self.db).execute_sql(CURSOR)
782         self._result_cache = None
783         return rows
784     update.alters_data = True
785 
786     def _update(self, values):
787         """
788         A version of update() that accepts field objects instead of field names.
789         Used primarily for model saving and not intended for use by general
790         code (it requires too much poking around at model internals to be
791         useful at that level).
792         """
793         assert not self.query.is_sliced, \
794             "Cannot update a query once a slice has been taken."
795         query = self.query.chain(sql.UpdateQuery)
796         query.add_update_fields(values)
797         # Clear any annotations so that they won't be present in subqueries.
798         query.annotations = {}
799         self._result_cache = None
800         return query.get_compiler(self.db).execute_sql(CURSOR)
801     _update.alters_data = True
802     _update.queryset_only = False
803 
804     def exists(self):
805         if self._result_cache is None:
806             return self.query.has_results(using=self.db)
807         return bool(self._result_cache)
808 
809     def _prefetch_related_objects(self):
810         # This method can only be called once the result cache has been filled.
811         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
812         self._prefetch_done = True
813 
814     def explain(self, *, format=None, **options):
815         return self.query.explain(using=self.db, format=format, **options)
816 
817     ##################################################
818     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
819     ##################################################
820 
821     def raw(self, raw_query, params=None, translations=None, using=None):
822         if using is None:
823             using = self.db
824         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
825         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
826         return qs
827 
828     def _values(self, *fields, **expressions):
829         clone = self._chain()
830         if expressions:
831             clone = clone.annotate(**expressions)
832         clone._fields = fields
833         clone.query.set_values(fields)
834         return clone
835 
836     def values(self, *fields, **expressions):
837         fields += tuple(expressions)
838         clone = self._values(*fields, **expressions)
839         clone._iterable_class = ValuesIterable
840         return clone
841 
842     def values_list(self, *fields, flat=False, named=False):
843         if flat and named:
844             raise TypeError("'flat' and 'named' can't be used together.")
845         if flat and len(fields) > 1:
846             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
847 
848         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
849         _fields = []
850         expressions = {}
851         counter = 1
852         for field in fields:
853             if hasattr(field, 'resolve_expression'):
854                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
855                 while True:
856                     field_id = field_id_prefix + str(counter)
857                     counter += 1
858                     if field_id not in field_names:
859                         break
860                 expressions[field_id] = field
861                 _fields.append(field_id)
862             else:
863                 _fields.append(field)
864 
865         clone = self._values(*_fields, **expressions)
866         clone._iterable_class = (
867             NamedValuesListIterable if named
868             else FlatValuesListIterable if flat
869             else ValuesListIterable
870         )
871         return clone
872 
873     def dates(self, field_name, kind, order='ASC'):
874         """
875         Return a list of date objects representing all available dates for
876         the given field_name, scoped to 'kind'.
877         """
878         assert kind in ('year', 'month', 'week', 'day'), \
879             "'kind' must be one of 'year', 'month', 'week', or 'day'."
880         assert order in ('ASC', 'DESC'), \
881             "'order' must be either 'ASC' or 'DESC'."
882         return self.annotate(
883             datefield=Trunc(field_name, kind, output_field=DateField()),
884             plain_field=F(field_name)
885         ).values_list(
886             'datefield', flat=True
887         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
888 
889     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):
890         """
891         Return a list of datetime objects representing all available
892         datetimes for the given field_name, scoped to 'kind'.
893         """
894         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
895             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
896         assert order in ('ASC', 'DESC'), \
897             "'order' must be either 'ASC' or 'DESC'."
898         if settings.USE_TZ:
899             if tzinfo is None:
900                 tzinfo = timezone.get_current_timezone()
901         else:
902             tzinfo = None
903         return self.annotate(
904             datetimefield=Trunc(
905                 field_name,
906                 kind,
907                 output_field=DateTimeField(),
908                 tzinfo=tzinfo,
909                 is_dst=is_dst,
910             ),
911             plain_field=F(field_name)
912         ).values_list(
913             'datetimefield', flat=True
914         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
915 
916     def none(self):
917         """Return an empty QuerySet."""
918         clone = self._chain()
919         clone.query.set_empty()
920         return clone
921 
922     ##################################################################
923     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
924     ##################################################################
925 
926     def all(self):
927         """
928         Return a new QuerySet that is a copy of the current one. This allows a
929         QuerySet to proxy for a model manager in some cases.
930         """
931         return self._chain()
932 
933     def filter(self, *args, **kwargs):
934         """
935         Return a new QuerySet instance with the args ANDed to the existing
936         set.
937         """
938         self._not_support_combined_queries('filter')
939         return self._filter_or_exclude(False, args, kwargs)
940 
941     def exclude(self, *args, **kwargs):
942         """
943         Return a new QuerySet instance with NOT (args) ANDed to the existing
944         set.
945         """
946         self._not_support_combined_queries('exclude')
947         return self._filter_or_exclude(True, args, kwargs)
948 
949     def _filter_or_exclude(self, negate, args, kwargs):
950         if args or kwargs:
951             assert not self.query.is_sliced, \
952                 "Cannot filter a query once a slice has been taken."
953 
954         clone = self._chain()
955         if self._defer_next_filter:
956             self._defer_next_filter = False
957             clone._deferred_filter = negate, args, kwargs
958         else:
959             clone._filter_or_exclude_inplace(negate, args, kwargs)
960         return clone
961 
962     def _filter_or_exclude_inplace(self, negate, args, kwargs):
963         if negate:
964             self._query.add_q(~Q(*args, **kwargs))
965         else:
966             self._query.add_q(Q(*args, **kwargs))
967 
968     def complex_filter(self, filter_obj):
969         """
970         Return a new QuerySet instance with filter_obj added to the filters.
971 
972         filter_obj can be a Q object or a dictionary of keyword lookup
973         arguments.
974 
975         This exists to support framework features such as 'limit_choices_to',
976         and usually it will be more natural to use other methods.
977         """
978         if isinstance(filter_obj, Q):
979             clone = self._chain()
980             clone.query.add_q(filter_obj)
981             return clone
982         else:
983             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
984 
985     def _combinator_query(self, combinator, *other_qs, all=False):
986         # Clone the query to inherit the select list and everything
987         clone = self._chain()
988         # Clear limits and ordering so they can be reapplied
989         clone.query.clear_ordering(True)
990         clone.query.clear_limits()
991         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
992         clone.query.combinator = combinator
993         clone.query.combinator_all = all
994         return clone
995 
996     def union(self, *other_qs, all=False):
997         # If the query is an EmptyQuerySet, combine all nonempty querysets.
998         if isinstance(self, EmptyQuerySet):
999             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1000             if not qs:
1001                 return self
1002             if len(qs) == 1:
1003                 return qs[0]
1004             return qs[0]._combinator_query('union', *qs[1:], all=all)
1005         return self._combinator_query('union', *other_qs, all=all)
1006 
1007     def intersection(self, *other_qs):
1008         # If any query is an EmptyQuerySet, return it.
1009         if isinstance(self, EmptyQuerySet):
1010             return self
1011         for other in other_qs:
1012             if isinstance(other, EmptyQuerySet):
1013                 return other
1014         return self._combinator_query('intersection', *other_qs)
1015 
1016     def difference(self, *other_qs):
1017         # If the query is an EmptyQuerySet, return it.
1018         if isinstance(self, EmptyQuerySet):
1019             return self
1020         return self._combinator_query('difference', *other_qs)
1021 
1022     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1023         """
1024         Return a new QuerySet instance that will select objects with a
1025         FOR UPDATE lock.
1026         """
1027         if nowait and skip_locked:
1028             raise ValueError('The nowait option cannot be used with skip_locked.')
1029         obj = self._chain()
1030         obj._for_write = True
1031         obj.query.select_for_update = True
1032         obj.query.select_for_update_nowait = nowait
1033         obj.query.select_for_update_skip_locked = skip_locked
1034         obj.query.select_for_update_of = of
1035         obj.query.select_for_no_key_update = no_key
1036         return obj
1037 
1038     def select_related(self, *fields):
1039         """
1040         Return a new QuerySet instance that will select related objects.
1041 
1042         If fields are specified, they must be ForeignKey fields and only those
1043         related objects are included in the selection.
1044 
1045         If select_related(None) is called, clear the list.
1046         """
1047         self._not_support_combined_queries('select_related')
1048         if self._fields is not None:
1049             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1050 
1051         obj = self._chain()
1052         if fields == (None,):
1053             obj.query.select_related = False
1054         elif fields:
1055             obj.query.add_select_related(fields)
1056         else:
1057             obj.query.select_related = True
1058         return obj
1059 
1060     def prefetch_related(self, *lookups):
1061         """
1062         Return a new QuerySet instance that will prefetch the specified
1063         Many-To-One and Many-To-Many related objects when the QuerySet is
1064         evaluated.
1065 
1066         When prefetch_related() is called more than once, append to the list of
1067         prefetch lookups. If prefetch_related(None) is called, clear the list.
1068         """
1069         self._not_support_combined_queries('prefetch_related')
1070         clone = self._chain()
1071         if lookups == (None,):
1072             clone._prefetch_related_lookups = ()
1073         else:
1074             for lookup in lookups:
1075                 if isinstance(lookup, Prefetch):
1076                     lookup = lookup.prefetch_to
1077                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1078                 if lookup in self.query._filtered_relations:
1079                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1080             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1081         return clone
1082 
1083     def annotate(self, *args, **kwargs):
1084         """
1085         Return a query set in which the returned objects have been annotated
1086         with extra data or aggregations.
1087         """
1088         self._not_support_combined_queries('annotate')
1089         return self._annotate(args, kwargs, select=True)
1090 
1091     def alias(self, *args, **kwargs):
1092         """
1093         Return a query set with added aliases for extra data or aggregations.
1094         """
1095         self._not_support_combined_queries('alias')
1096         return self._annotate(args, kwargs, select=False)
1097 
1098     def _annotate(self, args, kwargs, select=True):
1099         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1100         annotations = {}
1101         for arg in args:
1102             # The default_alias property may raise a TypeError.
1103             try:
1104                 if arg.default_alias in kwargs:
1105                     raise ValueError("The named annotation '%s' conflicts with the "
1106                                      "default name for another annotation."
1107                                      % arg.default_alias)
1108             except TypeError:
1109                 raise TypeError("Complex annotations require an alias")
1110             annotations[arg.default_alias] = arg
1111         annotations.update(kwargs)
1112 
1113         clone = self._chain()
1114         names = self._fields
1115         if names is None:
1116             names = set(chain.from_iterable(
1117                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1118                 for field in self.model._meta.get_fields()
1119             ))
1120 
1121         for alias, annotation in annotations.items():
1122             if alias in names:
1123                 raise ValueError("The annotation '%s' conflicts with a field on "
1124                                  "the model." % alias)
1125             if isinstance(annotation, FilteredRelation):
1126                 clone.query.add_filtered_relation(annotation, alias)
1127             else:
1128                 clone.query.add_annotation(
1129                     annotation, alias, is_summary=False, select=select,
1130                 )
1131         for alias, annotation in clone.query.annotations.items():
1132             if alias in annotations and annotation.contains_aggregate:
1133                 if clone._fields is None:
1134                     clone.query.group_by = True
1135                 else:
1136                     clone.query.set_group_by()
1137                 break
1138 
1139         return clone
1140 
1141     def order_by(self, *field_names):
1142         """Return a new QuerySet instance with the ordering changed."""
1143         assert not self.query.is_sliced, \
1144             "Cannot reorder a query once a slice has been taken."
1145         obj = self._chain()
1146         obj.query.clear_ordering(force_empty=False)
1147         obj.query.add_ordering(*field_names)
1148         return obj
1149 
1150     def distinct(self, *field_names):
1151         """
1152         Return a new QuerySet instance that will select only distinct results.
1153         """
1154         self._not_support_combined_queries('distinct')
1155         assert not self.query.is_sliced, \
1156             "Cannot create distinct fields once a slice has been taken."
1157         obj = self._chain()
1158         obj.query.add_distinct_fields(*field_names)
1159         return obj
1160 
1161     def extra(self, select=None, where=None, params=None, tables=None,
1162               order_by=None, select_params=None):
1163         """Add extra SQL fragments to the query."""
1164         self._not_support_combined_queries('extra')
1165         assert not self.query.is_sliced, \
1166             "Cannot change a query once a slice has been taken"
1167         clone = self._chain()
1168         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1169         return clone
1170 
1171     def reverse(self):
1172         """Reverse the ordering of the QuerySet."""
1173         if self.query.is_sliced:
1174             raise TypeError('Cannot reverse a query once a slice has been taken.')
1175         clone = self._chain()
1176         clone.query.standard_ordering = not clone.query.standard_ordering
1177         return clone
1178 
1179     def defer(self, *fields):
1180         """
1181         Defer the loading of data for certain fields until they are accessed.
1182         Add the set of deferred fields to any existing set of deferred fields.
1183         The only exception to this is if None is passed in as the only
1184         parameter, in which case removal all deferrals.
1185         """
1186         self._not_support_combined_queries('defer')
1187         if self._fields is not None:
1188             raise TypeError("Cannot call defer() after .values() or .values_list()")
1189         clone = self._chain()
1190         if fields == (None,):
1191             clone.query.clear_deferred_loading()
1192         else:
1193             clone.query.add_deferred_loading(fields)
1194         return clone
1195 
1196     def only(self, *fields):
1197         """
1198         Essentially, the opposite of defer(). Only the fields passed into this
1199         method and that are not already specified as deferred are loaded
1200         immediately when the queryset is evaluated.
1201         """
1202         self._not_support_combined_queries('only')
1203         if self._fields is not None:
1204             raise TypeError("Cannot call only() after .values() or .values_list()")
1205         if fields == (None,):
1206             # Can only pass None to defer(), not only(), as the rest option.
1207             # That won't stop people trying to do this, so let's be explicit.
1208             raise TypeError("Cannot pass None as an argument to only().")
1209         for field in fields:
1210             field = field.split(LOOKUP_SEP, 1)[0]
1211             if field in self.query._filtered_relations:
1212                 raise ValueError('only() is not supported with FilteredRelation.')
1213         clone = self._chain()
1214         clone.query.add_immediate_loading(fields)
1215         return clone
1216 
1217     def using(self, alias):
1218         """Select which database this QuerySet should execute against."""
1219         clone = self._chain()
1220         clone._db = alias
1221         return clone
1222 
1223     ###################################
1224     # PUBLIC INTROSPECTION ATTRIBUTES #
1225     ###################################
1226 
1227     @property
1228     def ordered(self):
1229         """
1230         Return True if the QuerySet is ordered -- i.e. has an order_by()
1231         clause or a default ordering on the model (or is empty).
1232         """
1233         if isinstance(self, EmptyQuerySet):
1234             return True
1235         if self.query.extra_order_by or self.query.order_by:
1236             return True
1237         elif (
1238             self.query.default_ordering and
1239             self.query.get_meta().ordering and
1240             # A default ordering doesn't affect GROUP BY queries.
1241             not self.query.group_by
1242         ):
1243             return True
1244         else:
1245             return False
1246 
1247     @property
1248     def db(self):
1249         """Return the database used if this query is executed now."""
1250         if self._for_write:
1251             return self._db or router.db_for_write(self.model, **self._hints)
1252         return self._db or router.db_for_read(self.model, **self._hints)
1253 
1254     ###################
1255     # PRIVATE METHODS #
1256     ###################
1257 
1258     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1259         """
1260         Insert a new record for the given model. This provides an interface to
1261         the InsertQuery class and is how Model.save() is implemented.
1262         """
1263         self._for_write = True
1264         if using is None:
1265             using = self.db
1266         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1267         query.insert_values(fields, objs, raw=raw)
1268         return query.get_compiler(using=using).execute_sql(returning_fields)
1269     _insert.alters_data = True
1270     _insert.queryset_only = False
1271 
1272     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1273         """
1274         Helper method for bulk_create() to insert objs one batch at a time.
1275         """
1276         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1277             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1278         ops = connections[self.db].ops
1279         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1280         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1281         inserted_rows = []
1282         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1283         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1284             if bulk_return and not ignore_conflicts:
1285                 inserted_rows.extend(self._insert(
1286                     item, fields=fields, using=self.db,
1287                     returning_fields=self.model._meta.db_returning_fields,
1288                     ignore_conflicts=ignore_conflicts,
1289                 ))
1290             else:
1291                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1292         return inserted_rows
1293 
1294     def _chain(self, **kwargs):
1295         """
1296         Return a copy of the current QuerySet that's ready for another
1297         operation.
1298         """
1299         obj = self._clone()
1300         if obj._sticky_filter:
1301             obj.query.filter_is_sticky = True
1302             obj._sticky_filter = False
1303         obj.__dict__.update(kwargs)
1304         return obj
1305 
1306     def _clone(self):
1307         """
1308         Return a copy of the current QuerySet. A lightweight alternative
1309         to deepcopy().
1310         """
1311         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1312         c._sticky_filter = self._sticky_filter
1313         c._for_write = self._for_write
1314         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1315         c._known_related_objects = self._known_related_objects
1316         c._iterable_class = self._iterable_class
1317         c._fields = self._fields
1318         return c
1319 
1320     def _fetch_all(self):
1321         if self._result_cache is None:
1322             self._result_cache = list(self._iterable_class(self))
1323         if self._prefetch_related_lookups and not self._prefetch_done:
1324             self._prefetch_related_objects()
1325 
1326     def _next_is_sticky(self):
1327         """
1328         Indicate that the next filter call and the one following that should
1329         be treated as a single filter. This is only important when it comes to
1330         determining when to reuse tables for many-to-many filters. Required so
1331         that we can filter naturally on the results of related managers.
1332 
1333         This doesn't return a clone of the current QuerySet (it returns
1334         "self"). The method is only used internally and should be immediately
1335         followed by a filter() that does create a clone.
1336         """
1337         self._sticky_filter = True
1338         return self
1339 
1340     def _merge_sanity_check(self, other):
1341         """Check that two QuerySet classes may be merged."""
1342         if self._fields is not None and (
1343                 set(self.query.values_select) != set(other.query.values_select) or
1344                 set(self.query.extra_select) != set(other.query.extra_select) or
1345                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1346             raise TypeError(
1347                 "Merging '%s' classes must involve the same values in each case."
1348                 % self.__class__.__name__
1349             )
1350 
1351     def _merge_known_related_objects(self, other):
1352         """
1353         Keep track of all known related objects from either QuerySet instance.
1354         """
1355         for field, objects in other._known_related_objects.items():
1356             self._known_related_objects.setdefault(field, {}).update(objects)
1357 
1358     def resolve_expression(self, *args, **kwargs):
1359         if self._fields and len(self._fields) > 1:
1360             # values() queryset can only be used as nested queries
1361             # if they are set up to select only a single field.
1362             raise TypeError('Cannot use multi-field values as a filter value.')
1363         query = self.query.resolve_expression(*args, **kwargs)
1364         query._db = self._db
1365         return query
1366     resolve_expression.queryset_only = True
1367 
1368     def _add_hints(self, **hints):
1369         """
1370         Update hinting information for use by routers. Add new key/values or
1371         overwrite existing key/values.
1372         """
1373         self._hints.update(hints)
1374 
1375     def _has_filters(self):
1376         """
1377         Check if this QuerySet has any filtering going on. This isn't
1378         equivalent with checking if all objects are present in results, for
1379         example, qs[1:]._has_filters() -> False.
1380         """
1381         return self.query.has_filters()
1382 
1383     @staticmethod
1384     def _validate_values_are_expressions(values, method_name):
1385         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1386         if invalid_args:
1387             raise TypeError(
1388                 'QuerySet.%s() received non-expression(s): %s.' % (
1389                     method_name,
1390                     ', '.join(invalid_args),
1391                 )
1392             )
1393 
1394     def _not_support_combined_queries(self, operation_name):
1395         if self.query.combinator:
1396             raise NotSupportedError(
1397                 'Calling QuerySet.%s() after %s() is not supported.'
1398                 % (operation_name, self.query.combinator)
1399             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 12:26:35.066 | INFO     | Search completed. Bug locations: [<file>django/db/models/expressions.py</file>
<class>ExpressionWrapper</class>
<method>get_group_by_cols</method>
<code>
917     def get_group_by_cols(self, alias=None):
918         expression = self.expression.copy()
919         expression.output_field = self.output_field
920         return expression.get_group_by_cols(alias=alias)

</code><intended_behavior>The method `get_group_by_cols` should be revised to ensure that when an `ExpressionWrapper` contains an expression that affects grouping (e.g., when wrapping a `Q` object to produce a conditional expression like '"bob_id" IS NULL'), this expression is appropriately included in the `GROUP BY` clause of the generated SQL query. This change must preserve the method's ability to handle other expressions and maintain compatibility with expressions that do not directly affect grouping.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>ExpressionWrapper</class>
<method>None</method>
<code>
901 class ExpressionWrapper(Expression):
902     """
903     An expression that can wrap another expression so that it can provide
904     extra context to the inner expression, such as the output_field.
905     """
906 
907     def __init__(self, expression, output_field):
908         super().__init__(output_field=output_field)
909         self.expression = expression
910 
911     def set_source_expressions(self, exprs):
912         self.expression = exprs[0]
913 
914     def get_source_expressions(self):
915         return [self.expression]
916 
917     def get_group_by_cols(self, alias=None):
918         expression = self.expression.copy()
919         expression.output_field = self.output_field
920         return expression.get_group_by_cols(alias=alias)
921 
922     def as_sql(self, compiler, connection):
923         return compiler.compile(self.expression)
924 
925     def __repr__(self):
926         return "{}({})".format(self.__class__.__name__, self.expression)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>annotate</method>
<code>
1083     def annotate(self, *args, **kwargs):
1084         """
1085         Return a query set in which the returned objects have been annotated
1086         with extra data or aggregations.
1087         """
1088         self._not_support_combined_queries('annotate')
1089         return self._annotate(args, kwargs, select=True)

</code><intended_behavior>The `annotate` method should be inspected and possibly adjusted to better handle cases where annotated fields (especially those wrapped with `ExpressionWrapper`) are used with the `values` and `annotate` methods together in a queryset. Specifically, it should ensure that expressions affecting the `GROUP BY` clause, as defined in the `annotate` method call, are accurately represented in the SQL query. This involves preserving the logic for handling different types of expressions within `annotate` calls, ensuring that the final SQL query accurately reflects the intended groupings and aggregates.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
175 class QuerySet:
176     """Represent a lazy database lookup for a set of objects."""
177 
178     def __init__(self, model=None, query=None, using=None, hints=None):
179         self.model = model
180         self._db = using
181         self._hints = hints or {}
182         self._query = query or sql.Query(self.model)
183         self._result_cache = None
184         self._sticky_filter = False
185         self._for_write = False
186         self._prefetch_related_lookups = ()
187         self._prefetch_done = False
188         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
189         self._iterable_class = ModelIterable
190         self._fields = None
191         self._defer_next_filter = False
192         self._deferred_filter = None
193 
194     @property
195     def query(self):
196         if self._deferred_filter:
197             negate, args, kwargs = self._deferred_filter
198             self._filter_or_exclude_inplace(negate, args, kwargs)
199             self._deferred_filter = None
200         return self._query
201 
202     @query.setter
203     def query(self, value):
204         if value.values_select:
205             self._iterable_class = ValuesIterable
206         self._query = value
207 
208     def as_manager(cls):
209         # Address the circular dependency between `Queryset` and `Manager`.
210         from django.db.models.manager import Manager
211         manager = Manager.from_queryset(cls)()
212         manager._built_with_as_manager = True
213         return manager
214     as_manager.queryset_only = True
215     as_manager = classmethod(as_manager)
216 
217     ########################
218     # PYTHON MAGIC METHODS #
219     ########################
220 
221     def __deepcopy__(self, memo):
222         """Don't populate the QuerySet's cache."""
223         obj = self.__class__()
224         for k, v in self.__dict__.items():
225             if k == '_result_cache':
226                 obj.__dict__[k] = None
227             else:
228                 obj.__dict__[k] = copy.deepcopy(v, memo)
229         return obj
230 
231     def __getstate__(self):
232         # Force the cache to be fully populated.
233         self._fetch_all()
234         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
235 
236     def __setstate__(self, state):
237         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
238         if pickled_version:
239             if pickled_version != django.__version__:
240                 warnings.warn(
241                     "Pickled queryset instance's Django version %s does not "
242                     "match the current version %s."
243                     % (pickled_version, django.__version__),
244                     RuntimeWarning,
245                     stacklevel=2,
246                 )
247         else:
248             warnings.warn(
249                 "Pickled queryset instance's Django version is not specified.",
250                 RuntimeWarning,
251                 stacklevel=2,
252             )
253         self.__dict__.update(state)
254 
255     def __repr__(self):
256         data = list(self[:REPR_OUTPUT_SIZE + 1])
257         if len(data) > REPR_OUTPUT_SIZE:
258             data[-1] = "...(remaining elements truncated)..."
259         return '<%s %r>' % (self.__class__.__name__, data)
260 
261     def __len__(self):
262         self._fetch_all()
263         return len(self._result_cache)
264 
265     def __iter__(self):
266         """
267         The queryset iterator protocol uses three nested iterators in the
268         default case:
269             1. sql.compiler.execute_sql()
270                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
271                  using cursor.fetchmany(). This part is responsible for
272                  doing some column masking, and returning the rows in chunks.
273             2. sql.compiler.results_iter()
274                - Returns one row at time. At this point the rows are still just
275                  tuples. In some cases the return values are converted to
276                  Python values at this location.
277             3. self.iterator()
278                - Responsible for turning the rows into model objects.
279         """
280         self._fetch_all()
281         return iter(self._result_cache)
282 
283     def __bool__(self):
284         self._fetch_all()
285         return bool(self._result_cache)
286 
287     def __getitem__(self, k):
288         """Retrieve an item or slice from the set of results."""
289         if not isinstance(k, (int, slice)):
290             raise TypeError(
291                 'QuerySet indices must be integers or slices, not %s.'
292                 % type(k).__name__
293             )
294         assert ((not isinstance(k, slice) and (k >= 0)) or
295                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
296                  (k.stop is None or k.stop >= 0))), \
297             "Negative indexing is not supported."
298 
299         if self._result_cache is not None:
300             return self._result_cache[k]
301 
302         if isinstance(k, slice):
303             qs = self._chain()
304             if k.start is not None:
305                 start = int(k.start)
306             else:
307                 start = None
308             if k.stop is not None:
309                 stop = int(k.stop)
310             else:
311                 stop = None
312             qs.query.set_limits(start, stop)
313             return list(qs)[::k.step] if k.step else qs
314 
315         qs = self._chain()
316         qs.query.set_limits(k, k + 1)
317         qs._fetch_all()
318         return qs._result_cache[0]
319 
320     def __class_getitem__(cls, *args, **kwargs):
321         return cls
322 
323     def __and__(self, other):
324         self._merge_sanity_check(other)
325         if isinstance(other, EmptyQuerySet):
326             return other
327         if isinstance(self, EmptyQuerySet):
328             return self
329         combined = self._chain()
330         combined._merge_known_related_objects(other)
331         combined.query.combine(other.query, sql.AND)
332         return combined
333 
334     def __or__(self, other):
335         self._merge_sanity_check(other)
336         if isinstance(self, EmptyQuerySet):
337             return other
338         if isinstance(other, EmptyQuerySet):
339             return self
340         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
341         combined = query._chain()
342         combined._merge_known_related_objects(other)
343         if not other.query.can_filter():
344             other = other.model._base_manager.filter(pk__in=other.values('pk'))
345         combined.query.combine(other.query, sql.OR)
346         return combined
347 
348     ####################################
349     # METHODS THAT DO DATABASE QUERIES #
350     ####################################
351 
352     def _iterator(self, use_chunked_fetch, chunk_size):
353         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
354 
355     def iterator(self, chunk_size=2000):
356         """
357         An iterator over the results from applying this QuerySet to the
358         database.
359         """
360         if chunk_size <= 0:
361             raise ValueError('Chunk size must be strictly positive.')
362         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
363         return self._iterator(use_chunked_fetch, chunk_size)
364 
365     def aggregate(self, *args, **kwargs):
366         """
367         Return a dictionary containing the calculations (aggregation)
368         over the current queryset.
369 
370         If args is present the expression is passed as a kwarg using
371         the Aggregate object's default alias.
372         """
373         if self.query.distinct_fields:
374             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
375         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
376         for arg in args:
377             # The default_alias property raises TypeError if default_alias
378             # can't be set automatically or AttributeError if it isn't an
379             # attribute.
380             try:
381                 arg.default_alias
382             except (AttributeError, TypeError):
383                 raise TypeError("Complex aggregates require an alias")
384             kwargs[arg.default_alias] = arg
385 
386         query = self.query.chain()
387         for (alias, aggregate_expr) in kwargs.items():
388             query.add_annotation(aggregate_expr, alias, is_summary=True)
389             annotation = query.annotations[alias]
390             if not annotation.contains_aggregate:
391                 raise TypeError("%s is not an aggregate expression" % alias)
392             for expr in annotation.get_source_expressions():
393                 if expr.contains_aggregate and isinstance(expr, Ref) and expr.refs in kwargs:
394                     name = expr.refs
395                     raise exceptions.FieldError(
396                         "Cannot compute %s('%s'): '%s' is an aggregate"
397                         % (annotation.name, name, name)
398                     )
399         return query.get_aggregation(self.db, kwargs)
400 
401     def count(self):
402         """
403         Perform a SELECT COUNT() and return the number of records as an
404         integer.
405 
406         If the QuerySet is already fully cached, return the length of the
407         cached results set to avoid multiple SELECT COUNT(*) calls.
408         """
409         if self._result_cache is not None:
410             return len(self._result_cache)
411 
412         return self.query.get_count(using=self.db)
413 
414     def get(self, *args, **kwargs):
415         """
416         Perform the query and return a single object matching the given
417         keyword arguments.
418         """
419         if self.query.combinator and (args or kwargs):
420             raise NotSupportedError(
421                 'Calling QuerySet.get(...) with filters after %s() is not '
422                 'supported.' % self.query.combinator
423             )
424         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
425         if self.query.can_filter() and not self.query.distinct_fields:
426             clone = clone.order_by()
427         limit = None
428         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
429             limit = MAX_GET_RESULTS
430             clone.query.set_limits(high=limit)
431         num = len(clone)
432         if num == 1:
433             return clone._result_cache[0]
434         if not num:
435             raise self.model.DoesNotExist(
436                 "%s matching query does not exist." %
437                 self.model._meta.object_name
438             )
439         raise self.model.MultipleObjectsReturned(
440             'get() returned more than one %s -- it returned %s!' % (
441                 self.model._meta.object_name,
442                 num if not limit or num < limit else 'more than %s' % (limit - 1),
443             )
444         )
445 
446     def create(self, **kwargs):
447         """
448         Create a new object with the given kwargs, saving it to the database
449         and returning the created object.
450         """
451         obj = self.model(**kwargs)
452         self._for_write = True
453         obj.save(force_insert=True, using=self.db)
454         return obj
455 
456     def _prepare_for_bulk_create(self, objs):
457         for obj in objs:
458             if obj.pk is None:
459                 # Populate new PK values.
460                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
461             obj._prepare_related_fields_for_save(operation_name='bulk_create')
462 
463     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
464         """
465         Insert each of the instances into the database. Do *not* call
466         save() on each of the instances, do not send any pre/post_save
467         signals, and do not set the primary key attribute if it is an
468         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
469         Multi-table models are not supported.
470         """
471         # When you bulk insert you don't get the primary keys back (if it's an
472         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
473         # you can't insert into the child tables which references this. There
474         # are two workarounds:
475         # 1) This could be implemented if you didn't have an autoincrement pk
476         # 2) You could do it by doing O(n) normal inserts into the parent
477         #    tables to get the primary keys back and then doing a single bulk
478         #    insert into the childmost table.
479         # We currently set the primary keys on the objects when using
480         # PostgreSQL via the RETURNING ID clause. It should be possible for
481         # Oracle as well, but the semantics for extracting the primary keys is
482         # trickier so it's not done yet.
483         assert batch_size is None or batch_size > 0
484         # Check that the parents share the same concrete model with the our
485         # model to detect the inheritance pattern ConcreteGrandParent ->
486         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
487         # would not identify that case as involving multiple tables.
488         for parent in self.model._meta.get_parent_list():
489             if parent._meta.concrete_model is not self.model._meta.concrete_model:
490                 raise ValueError("Can't bulk create a multi-table inherited model")
491         if not objs:
492             return objs
493         self._for_write = True
494         connection = connections[self.db]
495         opts = self.model._meta
496         fields = opts.concrete_fields
497         objs = list(objs)
498         self._prepare_for_bulk_create(objs)
499         with transaction.atomic(using=self.db, savepoint=False):
500             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
501             if objs_with_pk:
502                 returned_columns = self._batched_insert(
503                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
504                 )
505                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
506                     for result, field in zip(results, opts.db_returning_fields):
507                         if field != opts.pk:
508                             setattr(obj_with_pk, field.attname, result)
509                 for obj_with_pk in objs_with_pk:
510                     obj_with_pk._state.adding = False
511                     obj_with_pk._state.db = self.db
512             if objs_without_pk:
513                 fields = [f for f in fields if not isinstance(f, AutoField)]
514                 returned_columns = self._batched_insert(
515                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
516                 )
517                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
518                     assert len(returned_columns) == len(objs_without_pk)
519                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
520                     for result, field in zip(results, opts.db_returning_fields):
521                         setattr(obj_without_pk, field.attname, result)
522                     obj_without_pk._state.adding = False
523                     obj_without_pk._state.db = self.db
524 
525         return objs
526 
527     def bulk_update(self, objs, fields, batch_size=None):
528         """
529         Update the given fields in each of the given objects in the database.
530         """
531         if batch_size is not None and batch_size < 0:
532             raise ValueError('Batch size must be a positive integer.')
533         if not fields:
534             raise ValueError('Field names must be given to bulk_update().')
535         objs = tuple(objs)
536         if any(obj.pk is None for obj in objs):
537             raise ValueError('All bulk_update() objects must have a primary key set.')
538         fields = [self.model._meta.get_field(name) for name in fields]
539         if any(not f.concrete or f.many_to_many for f in fields):
540             raise ValueError('bulk_update() can only be used with concrete fields.')
541         if any(f.primary_key for f in fields):
542             raise ValueError('bulk_update() cannot be used with primary key fields.')
543         if not objs:
544             return
545         # PK is used twice in the resulting update query, once in the filter
546         # and once in the WHEN. Each field will also have one CAST.
547         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
548         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
549         requires_casting = connections[self.db].features.requires_casted_case_in_updates
550         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
551         updates = []
552         for batch_objs in batches:
553             update_kwargs = {}
554             for field in fields:
555                 when_statements = []
556                 for obj in batch_objs:
557                     attr = getattr(obj, field.attname)
558                     if not isinstance(attr, Expression):
559                         attr = Value(attr, output_field=field)
560                     when_statements.append(When(pk=obj.pk, then=attr))
561                 case_statement = Case(*when_statements, output_field=field)
562                 if requires_casting:
563                     case_statement = Cast(case_statement, output_field=field)
564                 update_kwargs[field.attname] = case_statement
565             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
566         with transaction.atomic(using=self.db, savepoint=False):
567             for pks, update_kwargs in updates:
568                 self.filter(pk__in=pks).update(**update_kwargs)
569     bulk_update.alters_data = True
570 
571     def get_or_create(self, defaults=None, **kwargs):
572         """
573         Look up an object with the given kwargs, creating one if necessary.
574         Return a tuple of (object, created), where created is a boolean
575         specifying whether an object was created.
576         """
577         # The get() needs to be targeted at the write database in order
578         # to avoid potential transaction consistency problems.
579         self._for_write = True
580         try:
581             return self.get(**kwargs), False
582         except self.model.DoesNotExist:
583             params = self._extract_model_params(defaults, **kwargs)
584             # Try to create an object using passed params.
585             try:
586                 with transaction.atomic(using=self.db):
587                     params = dict(resolve_callables(params))
588                     return self.create(**params), True
589             except IntegrityError:
590                 try:
591                     return self.get(**kwargs), False
592                 except self.model.DoesNotExist:
593                     pass
594                 raise
595 
596     def update_or_create(self, defaults=None, **kwargs):
597         """
598         Look up an object with the given kwargs, updating one with defaults
599         if it exists, otherwise create a new one.
600         Return a tuple (object, created), where created is a boolean
601         specifying whether an object was created.
602         """
603         defaults = defaults or {}
604         self._for_write = True
605         with transaction.atomic(using=self.db):
606             # Lock the row so that a concurrent update is blocked until
607             # update_or_create() has performed its save.
608             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)
609             if created:
610                 return obj, created
611             for k, v in resolve_callables(defaults):
612                 setattr(obj, k, v)
613             obj.save(using=self.db)
614         return obj, False
615 
616     def _extract_model_params(self, defaults, **kwargs):
617         """
618         Prepare `params` for creating a model instance based on the given
619         kwargs; for use by get_or_create().
620         """
621         defaults = defaults or {}
622         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
623         params.update(defaults)
624         property_names = self.model._meta._property_names
625         invalid_params = []
626         for param in params:
627             try:
628                 self.model._meta.get_field(param)
629             except exceptions.FieldDoesNotExist:
630                 # It's okay to use a model's property if it has a setter.
631                 if not (param in property_names and getattr(self.model, param).fset):
632                     invalid_params.append(param)
633         if invalid_params:
634             raise exceptions.FieldError(
635                 "Invalid field name(s) for model %s: '%s'." % (
636                     self.model._meta.object_name,
637                     "', '".join(sorted(invalid_params)),
638                 ))
639         return params
640 
641     def _earliest(self, *fields):
642         """
643         Return the earliest object according to fields (if given) or by the
644         model's Meta.get_latest_by.
645         """
646         if fields:
647             order_by = fields
648         else:
649             order_by = getattr(self.model._meta, 'get_latest_by')
650             if order_by and not isinstance(order_by, (tuple, list)):
651                 order_by = (order_by,)
652         if order_by is None:
653             raise ValueError(
654                 "earliest() and latest() require either fields as positional "
655                 "arguments or 'get_latest_by' in the model's Meta."
656             )
657 
658         assert not self.query.is_sliced, \
659             "Cannot change a query once a slice has been taken."
660         obj = self._chain()
661         obj.query.set_limits(high=1)
662         obj.query.clear_ordering(force_empty=True)
663         obj.query.add_ordering(*order_by)
664         return obj.get()
665 
666     def earliest(self, *fields):
667         return self._earliest(*fields)
668 
669     def latest(self, *fields):
670         return self.reverse()._earliest(*fields)
671 
672     def first(self):
673         """Return the first object of a query or None if no match is found."""
674         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
675             return obj
676 
677     def last(self):
678         """Return the last object of a query or None if no match is found."""
679         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
680             return obj
681 
682     def in_bulk(self, id_list=None, *, field_name='pk'):
683         """
684         Return a dictionary mapping each of the given IDs to the object with
685         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
686         """
687         assert not self.query.is_sliced, \
688             "Cannot use 'limit' or 'offset' with in_bulk"
689         opts = self.model._meta
690         unique_fields = [
691             constraint.fields[0]
692             for constraint in opts.total_unique_constraints
693             if len(constraint.fields) == 1
694         ]
695         if (
696             field_name != 'pk' and
697             not opts.get_field(field_name).unique and
698             field_name not in unique_fields and
699             self.query.distinct_fields != (field_name,)
700         ):
701             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
702         if id_list is not None:
703             if not id_list:
704                 return {}
705             filter_key = '{}__in'.format(field_name)
706             batch_size = connections[self.db].features.max_query_params
707             id_list = tuple(id_list)
708             # If the database has a limit on the number of query parameters
709             # (e.g. SQLite), retrieve objects in batches if necessary.
710             if batch_size and batch_size < len(id_list):
711                 qs = ()
712                 for offset in range(0, len(id_list), batch_size):
713                     batch = id_list[offset:offset + batch_size]
714                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
715             else:
716                 qs = self.filter(**{filter_key: id_list}).order_by()
717         else:
718             qs = self._chain()
719         return {getattr(obj, field_name): obj for obj in qs}
720 
721     def delete(self):
722         """Delete the records in the current QuerySet."""
723         self._not_support_combined_queries('delete')
724         assert not self.query.is_sliced, \
725             "Cannot use 'limit' or 'offset' with delete."
726 
727         if self._fields is not None:
728             raise TypeError("Cannot call delete() after .values() or .values_list()")
729 
730         del_query = self._chain()
731 
732         # The delete is actually 2 queries - one to find related objects,
733         # and one to delete. Make sure that the discovery of related
734         # objects is performed on the same database as the deletion.
735         del_query._for_write = True
736 
737         # Disable non-supported fields.
738         del_query.query.select_for_update = False
739         del_query.query.select_related = False
740         del_query.query.clear_ordering(force_empty=True)
741 
742         collector = Collector(using=del_query.db)
743         collector.collect(del_query)
744         deleted, _rows_count = collector.delete()
745 
746         # Clear the result cache, in case this QuerySet gets reused.
747         self._result_cache = None
748         return deleted, _rows_count
749 
750     delete.alters_data = True
751     delete.queryset_only = True
752 
753     def _raw_delete(self, using):
754         """
755         Delete objects found from the given queryset in single direct SQL
756         query. No signals are sent and there is no protection for cascades.
757         """
758         query = self.query.clone()
759         query.__class__ = sql.DeleteQuery
760         cursor = query.get_compiler(using).execute_sql(CURSOR)
761         if cursor:
762             with cursor:
763                 return cursor.rowcount
764         return 0
765     _raw_delete.alters_data = True
766 
767     def update(self, **kwargs):
768         """
769         Update all elements in the current QuerySet, setting all the given
770         fields to the appropriate values.
771         """
772         self._not_support_combined_queries('update')
773         assert not self.query.is_sliced, \
774             "Cannot update a query once a slice has been taken."
775         self._for_write = True
776         query = self.query.chain(sql.UpdateQuery)
777         query.add_update_values(kwargs)
778         # Clear any annotations so that they won't be present in subqueries.
779         query.annotations = {}
780         with transaction.mark_for_rollback_on_error(using=self.db):
781             rows = query.get_compiler(self.db).execute_sql(CURSOR)
782         self._result_cache = None
783         return rows
784     update.alters_data = True
785 
786     def _update(self, values):
787         """
788         A version of update() that accepts field objects instead of field names.
789         Used primarily for model saving and not intended for use by general
790         code (it requires too much poking around at model internals to be
791         useful at that level).
792         """
793         assert not self.query.is_sliced, \
794             "Cannot update a query once a slice has been taken."
795         query = self.query.chain(sql.UpdateQuery)
796         query.add_update_fields(values)
797         # Clear any annotations so that they won't be present in subqueries.
798         query.annotations = {}
799         self._result_cache = None
800         return query.get_compiler(self.db).execute_sql(CURSOR)
801     _update.alters_data = True
802     _update.queryset_only = False
803 
804     def exists(self):
805         if self._result_cache is None:
806             return self.query.has_results(using=self.db)
807         return bool(self._result_cache)
808 
809     def _prefetch_related_objects(self):
810         # This method can only be called once the result cache has been filled.
811         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
812         self._prefetch_done = True
813 
814     def explain(self, *, format=None, **options):
815         return self.query.explain(using=self.db, format=format, **options)
816 
817     ##################################################
818     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
819     ##################################################
820 
821     def raw(self, raw_query, params=None, translations=None, using=None):
822         if using is None:
823             using = self.db
824         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
825         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
826         return qs
827 
828     def _values(self, *fields, **expressions):
829         clone = self._chain()
830         if expressions:
831             clone = clone.annotate(**expressions)
832         clone._fields = fields
833         clone.query.set_values(fields)
834         return clone
835 
836     def values(self, *fields, **expressions):
837         fields += tuple(expressions)
838         clone = self._values(*fields, **expressions)
839         clone._iterable_class = ValuesIterable
840         return clone
841 
842     def values_list(self, *fields, flat=False, named=False):
843         if flat and named:
844             raise TypeError("'flat' and 'named' can't be used together.")
845         if flat and len(fields) > 1:
846             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
847 
848         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
849         _fields = []
850         expressions = {}
851         counter = 1
852         for field in fields:
853             if hasattr(field, 'resolve_expression'):
854                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
855                 while True:
856                     field_id = field_id_prefix + str(counter)
857                     counter += 1
858                     if field_id not in field_names:
859                         break
860                 expressions[field_id] = field
861                 _fields.append(field_id)
862             else:
863                 _fields.append(field)
864 
865         clone = self._values(*_fields, **expressions)
866         clone._iterable_class = (
867             NamedValuesListIterable if named
868             else FlatValuesListIterable if flat
869             else ValuesListIterable
870         )
871         return clone
872 
873     def dates(self, field_name, kind, order='ASC'):
874         """
875         Return a list of date objects representing all available dates for
876         the given field_name, scoped to 'kind'.
877         """
878         assert kind in ('year', 'month', 'week', 'day'), \
879             "'kind' must be one of 'year', 'month', 'week', or 'day'."
880         assert order in ('ASC', 'DESC'), \
881             "'order' must be either 'ASC' or 'DESC'."
882         return self.annotate(
883             datefield=Trunc(field_name, kind, output_field=DateField()),
884             plain_field=F(field_name)
885         ).values_list(
886             'datefield', flat=True
887         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
888 
889     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):
890         """
891         Return a list of datetime objects representing all available
892         datetimes for the given field_name, scoped to 'kind'.
893         """
894         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
895             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
896         assert order in ('ASC', 'DESC'), \
897             "'order' must be either 'ASC' or 'DESC'."
898         if settings.USE_TZ:
899             if tzinfo is None:
900                 tzinfo = timezone.get_current_timezone()
901         else:
902             tzinfo = None
903         return self.annotate(
904             datetimefield=Trunc(
905                 field_name,
906                 kind,
907                 output_field=DateTimeField(),
908                 tzinfo=tzinfo,
909                 is_dst=is_dst,
910             ),
911             plain_field=F(field_name)
912         ).values_list(
913             'datetimefield', flat=True
914         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
915 
916     def none(self):
917         """Return an empty QuerySet."""
918         clone = self._chain()
919         clone.query.set_empty()
920         return clone
921 
922     ##################################################################
923     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
924     ##################################################################
925 
926     def all(self):
927         """
928         Return a new QuerySet that is a copy of the current one. This allows a
929         QuerySet to proxy for a model manager in some cases.
930         """
931         return self._chain()
932 
933     def filter(self, *args, **kwargs):
934         """
935         Return a new QuerySet instance with the args ANDed to the existing
936         set.
937         """
938         self._not_support_combined_queries('filter')
939         return self._filter_or_exclude(False, args, kwargs)
940 
941     def exclude(self, *args, **kwargs):
942         """
943         Return a new QuerySet instance with NOT (args) ANDed to the existing
944         set.
945         """
946         self._not_support_combined_queries('exclude')
947         return self._filter_or_exclude(True, args, kwargs)
948 
949     def _filter_or_exclude(self, negate, args, kwargs):
950         if args or kwargs:
951             assert not self.query.is_sliced, \
952                 "Cannot filter a query once a slice has been taken."
953 
954         clone = self._chain()
955         if self._defer_next_filter:
956             self._defer_next_filter = False
957             clone._deferred_filter = negate, args, kwargs
958         else:
959             clone._filter_or_exclude_inplace(negate, args, kwargs)
960         return clone
961 
962     def _filter_or_exclude_inplace(self, negate, args, kwargs):
963         if negate:
964             self._query.add_q(~Q(*args, **kwargs))
965         else:
966             self._query.add_q(Q(*args, **kwargs))
967 
968     def complex_filter(self, filter_obj):
969         """
970         Return a new QuerySet instance with filter_obj added to the filters.
971 
972         filter_obj can be a Q object or a dictionary of keyword lookup
973         arguments.
974 
975         This exists to support framework features such as 'limit_choices_to',
976         and usually it will be more natural to use other methods.
977         """
978         if isinstance(filter_obj, Q):
979             clone = self._chain()
980             clone.query.add_q(filter_obj)
981             return clone
982         else:
983             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
984 
985     def _combinator_query(self, combinator, *other_qs, all=False):
986         # Clone the query to inherit the select list and everything
987         clone = self._chain()
988         # Clear limits and ordering so they can be reapplied
989         clone.query.clear_ordering(True)
990         clone.query.clear_limits()
991         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
992         clone.query.combinator = combinator
993         clone.query.combinator_all = all
994         return clone
995 
996     def union(self, *other_qs, all=False):
997         # If the query is an EmptyQuerySet, combine all nonempty querysets.
998         if isinstance(self, EmptyQuerySet):
999             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1000             if not qs:
1001                 return self
1002             if len(qs) == 1:
1003                 return qs[0]
1004             return qs[0]._combinator_query('union', *qs[1:], all=all)
1005         return self._combinator_query('union', *other_qs, all=all)
1006 
1007     def intersection(self, *other_qs):
1008         # If any query is an EmptyQuerySet, return it.
1009         if isinstance(self, EmptyQuerySet):
1010             return self
1011         for other in other_qs:
1012             if isinstance(other, EmptyQuerySet):
1013                 return other
1014         return self._combinator_query('intersection', *other_qs)
1015 
1016     def difference(self, *other_qs):
1017         # If the query is an EmptyQuerySet, return it.
1018         if isinstance(self, EmptyQuerySet):
1019             return self
1020         return self._combinator_query('difference', *other_qs)
1021 
1022     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1023         """
1024         Return a new QuerySet instance that will select objects with a
1025         FOR UPDATE lock.
1026         """
1027         if nowait and skip_locked:
1028             raise ValueError('The nowait option cannot be used with skip_locked.')
1029         obj = self._chain()
1030         obj._for_write = True
1031         obj.query.select_for_update = True
1032         obj.query.select_for_update_nowait = nowait
1033         obj.query.select_for_update_skip_locked = skip_locked
1034         obj.query.select_for_update_of = of
1035         obj.query.select_for_no_key_update = no_key
1036         return obj
1037 
1038     def select_related(self, *fields):
1039         """
1040         Return a new QuerySet instance that will select related objects.
1041 
1042         If fields are specified, they must be ForeignKey fields and only those
1043         related objects are included in the selection.
1044 
1045         If select_related(None) is called, clear the list.
1046         """
1047         self._not_support_combined_queries('select_related')
1048         if self._fields is not None:
1049             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1050 
1051         obj = self._chain()
1052         if fields == (None,):
1053             obj.query.select_related = False
1054         elif fields:
1055             obj.query.add_select_related(fields)
1056         else:
1057             obj.query.select_related = True
1058         return obj
1059 
1060     def prefetch_related(self, *lookups):
1061         """
1062         Return a new QuerySet instance that will prefetch the specified
1063         Many-To-One and Many-To-Many related objects when the QuerySet is
1064         evaluated.
1065 
1066         When prefetch_related() is called more than once, append to the list of
1067         prefetch lookups. If prefetch_related(None) is called, clear the list.
1068         """
1069         self._not_support_combined_queries('prefetch_related')
1070         clone = self._chain()
1071         if lookups == (None,):
1072             clone._prefetch_related_lookups = ()
1073         else:
1074             for lookup in lookups:
1075                 if isinstance(lookup, Prefetch):
1076                     lookup = lookup.prefetch_to
1077                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1078                 if lookup in self.query._filtered_relations:
1079                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1080             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1081         return clone
1082 
1083     def annotate(self, *args, **kwargs):
1084         """
1085         Return a query set in which the returned objects have been annotated
1086         with extra data or aggregations.
1087         """
1088         self._not_support_combined_queries('annotate')
1089         return self._annotate(args, kwargs, select=True)
1090 
1091     def alias(self, *args, **kwargs):
1092         """
1093         Return a query set with added aliases for extra data or aggregations.
1094         """
1095         self._not_support_combined_queries('alias')
1096         return self._annotate(args, kwargs, select=False)
1097 
1098     def _annotate(self, args, kwargs, select=True):
1099         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1100         annotations = {}
1101         for arg in args:
1102             # The default_alias property may raise a TypeError.
1103             try:
1104                 if arg.default_alias in kwargs:
1105                     raise ValueError("The named annotation '%s' conflicts with the "
1106                                      "default name for another annotation."
1107                                      % arg.default_alias)
1108             except TypeError:
1109                 raise TypeError("Complex annotations require an alias")
1110             annotations[arg.default_alias] = arg
1111         annotations.update(kwargs)
1112 
1113         clone = self._chain()
1114         names = self._fields
1115         if names is None:
1116             names = set(chain.from_iterable(
1117                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1118                 for field in self.model._meta.get_fields()
1119             ))
1120 
1121         for alias, annotation in annotations.items():
1122             if alias in names:
1123                 raise ValueError("The annotation '%s' conflicts with a field on "
1124                                  "the model." % alias)
1125             if isinstance(annotation, FilteredRelation):
1126                 clone.query.add_filtered_relation(annotation, alias)
1127             else:
1128                 clone.query.add_annotation(
1129                     annotation, alias, is_summary=False, select=select,
1130                 )
1131         for alias, annotation in clone.query.annotations.items():
1132             if alias in annotations and annotation.contains_aggregate:
1133                 if clone._fields is None:
1134                     clone.query.group_by = True
1135                 else:
1136                     clone.query.set_group_by()
1137                 break
1138 
1139         return clone
1140 
1141     def order_by(self, *field_names):
1142         """Return a new QuerySet instance with the ordering changed."""
1143         assert not self.query.is_sliced, \
1144             "Cannot reorder a query once a slice has been taken."
1145         obj = self._chain()
1146         obj.query.clear_ordering(force_empty=False)
1147         obj.query.add_ordering(*field_names)
1148         return obj
1149 
1150     def distinct(self, *field_names):
1151         """
1152         Return a new QuerySet instance that will select only distinct results.
1153         """
1154         self._not_support_combined_queries('distinct')
1155         assert not self.query.is_sliced, \
1156             "Cannot create distinct fields once a slice has been taken."
1157         obj = self._chain()
1158         obj.query.add_distinct_fields(*field_names)
1159         return obj
1160 
1161     def extra(self, select=None, where=None, params=None, tables=None,
1162               order_by=None, select_params=None):
1163         """Add extra SQL fragments to the query."""
1164         self._not_support_combined_queries('extra')
1165         assert not self.query.is_sliced, \
1166             "Cannot change a query once a slice has been taken"
1167         clone = self._chain()
1168         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1169         return clone
1170 
1171     def reverse(self):
1172         """Reverse the ordering of the QuerySet."""
1173         if self.query.is_sliced:
1174             raise TypeError('Cannot reverse a query once a slice has been taken.')
1175         clone = self._chain()
1176         clone.query.standard_ordering = not clone.query.standard_ordering
1177         return clone
1178 
1179     def defer(self, *fields):
1180         """
1181         Defer the loading of data for certain fields until they are accessed.
1182         Add the set of deferred fields to any existing set of deferred fields.
1183         The only exception to this is if None is passed in as the only
1184         parameter, in which case removal all deferrals.
1185         """
1186         self._not_support_combined_queries('defer')
1187         if self._fields is not None:
1188             raise TypeError("Cannot call defer() after .values() or .values_list()")
1189         clone = self._chain()
1190         if fields == (None,):
1191             clone.query.clear_deferred_loading()
1192         else:
1193             clone.query.add_deferred_loading(fields)
1194         return clone
1195 
1196     def only(self, *fields):
1197         """
1198         Essentially, the opposite of defer(). Only the fields passed into this
1199         method and that are not already specified as deferred are loaded
1200         immediately when the queryset is evaluated.
1201         """
1202         self._not_support_combined_queries('only')
1203         if self._fields is not None:
1204             raise TypeError("Cannot call only() after .values() or .values_list()")
1205         if fields == (None,):
1206             # Can only pass None to defer(), not only(), as the rest option.
1207             # That won't stop people trying to do this, so let's be explicit.
1208             raise TypeError("Cannot pass None as an argument to only().")
1209         for field in fields:
1210             field = field.split(LOOKUP_SEP, 1)[0]
1211             if field in self.query._filtered_relations:
1212                 raise ValueError('only() is not supported with FilteredRelation.')
1213         clone = self._chain()
1214         clone.query.add_immediate_loading(fields)
1215         return clone
1216 
1217     def using(self, alias):
1218         """Select which database this QuerySet should execute against."""
1219         clone = self._chain()
1220         clone._db = alias
1221         return clone
1222 
1223     ###################################
1224     # PUBLIC INTROSPECTION ATTRIBUTES #
1225     ###################################
1226 
1227     @property
1228     def ordered(self):
1229         """
1230         Return True if the QuerySet is ordered -- i.e. has an order_by()
1231         clause or a default ordering on the model (or is empty).
1232         """
1233         if isinstance(self, EmptyQuerySet):
1234             return True
1235         if self.query.extra_order_by or self.query.order_by:
1236             return True
1237         elif (
1238             self.query.default_ordering and
1239             self.query.get_meta().ordering and
1240             # A default ordering doesn't affect GROUP BY queries.
1241             not self.query.group_by
1242         ):
1243             return True
1244         else:
1245             return False
1246 
1247     @property
1248     def db(self):
1249         """Return the database used if this query is executed now."""
1250         if self._for_write:
1251             return self._db or router.db_for_write(self.model, **self._hints)
1252         return self._db or router.db_for_read(self.model, **self._hints)
1253 
1254     ###################
1255     # PRIVATE METHODS #
1256     ###################
1257 
1258     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1259         """
1260         Insert a new record for the given model. This provides an interface to
1261         the InsertQuery class and is how Model.save() is implemented.
1262         """
1263         self._for_write = True
1264         if using is None:
1265             using = self.db
1266         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1267         query.insert_values(fields, objs, raw=raw)
1268         return query.get_compiler(using=using).execute_sql(returning_fields)
1269     _insert.alters_data = True
1270     _insert.queryset_only = False
1271 
1272     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1273         """
1274         Helper method for bulk_create() to insert objs one batch at a time.
1275         """
1276         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1277             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1278         ops = connections[self.db].ops
1279         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1280         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1281         inserted_rows = []
1282         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1283         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1284             if bulk_return and not ignore_conflicts:
1285                 inserted_rows.extend(self._insert(
1286                     item, fields=fields, using=self.db,
1287                     returning_fields=self.model._meta.db_returning_fields,
1288                     ignore_conflicts=ignore_conflicts,
1289                 ))
1290             else:
1291                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1292         return inserted_rows
1293 
1294     def _chain(self, **kwargs):
1295         """
1296         Return a copy of the current QuerySet that's ready for another
1297         operation.
1298         """
1299         obj = self._clone()
1300         if obj._sticky_filter:
1301             obj.query.filter_is_sticky = True
1302             obj._sticky_filter = False
1303         obj.__dict__.update(kwargs)
1304         return obj
1305 
1306     def _clone(self):
1307         """
1308         Return a copy of the current QuerySet. A lightweight alternative
1309         to deepcopy().
1310         """
1311         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1312         c._sticky_filter = self._sticky_filter
1313         c._for_write = self._for_write
1314         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1315         c._known_related_objects = self._known_related_objects
1316         c._iterable_class = self._iterable_class
1317         c._fields = self._fields
1318         return c
1319 
1320     def _fetch_all(self):
1321         if self._result_cache is None:
1322             self._result_cache = list(self._iterable_class(self))
1323         if self._prefetch_related_lookups and not self._prefetch_done:
1324             self._prefetch_related_objects()
1325 
1326     def _next_is_sticky(self):
1327         """
1328         Indicate that the next filter call and the one following that should
1329         be treated as a single filter. This is only important when it comes to
1330         determining when to reuse tables for many-to-many filters. Required so
1331         that we can filter naturally on the results of related managers.
1332 
1333         This doesn't return a clone of the current QuerySet (it returns
1334         "self"). The method is only used internally and should be immediately
1335         followed by a filter() that does create a clone.
1336         """
1337         self._sticky_filter = True
1338         return self
1339 
1340     def _merge_sanity_check(self, other):
1341         """Check that two QuerySet classes may be merged."""
1342         if self._fields is not None and (
1343                 set(self.query.values_select) != set(other.query.values_select) or
1344                 set(self.query.extra_select) != set(other.query.extra_select) or
1345                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1346             raise TypeError(
1347                 "Merging '%s' classes must involve the same values in each case."
1348                 % self.__class__.__name__
1349             )
1350 
1351     def _merge_known_related_objects(self, other):
1352         """
1353         Keep track of all known related objects from either QuerySet instance.
1354         """
1355         for field, objects in other._known_related_objects.items():
1356             self._known_related_objects.setdefault(field, {}).update(objects)
1357 
1358     def resolve_expression(self, *args, **kwargs):
1359         if self._fields and len(self._fields) > 1:
1360             # values() queryset can only be used as nested queries
1361             # if they are set up to select only a single field.
1362             raise TypeError('Cannot use multi-field values as a filter value.')
1363         query = self.query.resolve_expression(*args, **kwargs)
1364         query._db = self._db
1365         return query
1366     resolve_expression.queryset_only = True
1367 
1368     def _add_hints(self, **hints):
1369         """
1370         Update hinting information for use by routers. Add new key/values or
1371         overwrite existing key/values.
1372         """
1373         self._hints.update(hints)
1374 
1375     def _has_filters(self):
1376         """
1377         Check if this QuerySet has any filtering going on. This isn't
1378         equivalent with checking if all objects are present in results, for
1379         example, qs[1:]._has_filters() -> False.
1380         """
1381         return self.query.has_filters()
1382 
1383     @staticmethod
1384     def _validate_values_are_expressions(values, method_name):
1385         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1386         if invalid_args:
1387             raise TypeError(
1388                 'QuerySet.%s() received non-expression(s): %s.' % (
1389                     method_name,
1390                     ', '.join(invalid_args),
1391                 )
1392             )
1393 
1394     def _not_support_combined_queries(self, operation_name):
1395         if self.query.combinator:
1396             raise NotSupportedError(
1397                 'Calling QuerySet.%s() after %s() is not supported.'
1398                 % (operation_name, self.query.combinator)
1399             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 12:26:35.067 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 12:26:35.067 | INFO     | Search-only: skipping patch selection
2025-10-18 12:26:35.220 | INFO     | Task django__django-13689 completed successfully.
2025-10-18 12:26:35.221 | INFO     | No patch generated. You can try running ACR again.
