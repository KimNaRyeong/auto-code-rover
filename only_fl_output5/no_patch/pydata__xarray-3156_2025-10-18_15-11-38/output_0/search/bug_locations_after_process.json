[
    {
        "rel_file_path": "xarray/core/common.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/common.py",
        "start": 450,
        "end": 499,
        "class_name": "DataWithCoords",
        "method_name": "groupby",
        "code": "450     def groupby(self, group, squeeze: bool = True,\n451                 restore_coord_dims: Optional[bool] = None):\n452         \"\"\"Returns a GroupBy object for performing grouped operations.\n453 \n454         Parameters\n455         ----------\n456         group : str, DataArray or IndexVariable\n457             Array whose unique values should be used to group this array. If a\n458             string, must be the name of a variable contained in this dataset.\n459         squeeze : boolean, optional\n460             If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n461             controls whether the subarrays have a dimension of length 1 along\n462             that dimension or if the dimension is squeezed out.\n463         restore_coord_dims : bool, optional\n464             If True, also restore the dimension order of multi-dimensional\n465             coordinates.\n466 \n467         Returns\n468         -------\n469         grouped : GroupBy\n470             A `GroupBy` object patterned after `pandas.GroupBy` that can be\n471             iterated over in the form of `(unique_value, grouped_array)` pairs.\n472 \n473         Examples\n474         --------\n475         Calculate daily anomalies for daily data:\n476 \n477         >>> da = xr.DataArray(np.linspace(0, 1826, num=1827),\n478         ...                   coords=[pd.date_range('1/1/2000', '31/12/2004',\n479         ...                           freq='D')],\n480         ...                   dims='time')\n481         >>> da\n482         <xarray.DataArray (time: 1827)>\n483         array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03, 1.826e+03])\n484         Coordinates:\n485           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\n486         >>> da.groupby('time.dayofyear') - da.groupby('time.dayofyear').mean('time')\n487         <xarray.DataArray (time: 1827)>\n488         array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])\n489         Coordinates:\n490           * time       (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\n491             dayofyear  (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ...\n492 \n493         See Also\n494         --------\n495         core.groupby.DataArrayGroupBy\n496         core.groupby.DatasetGroupBy\n497         \"\"\"  # noqa\n498         return self._groupby_cls(self, group, squeeze=squeeze,\n499                                  restore_coord_dims=restore_coord_dims)\n",
        "intended_behavior": "Modify the `groupby` method to ensure it checks if the `DataArray` or `Dataset` is empty. If the group operation is called on an empty object, it should immediately return a `GroupBy` object designed to handle subsequent operations (like `mean`) without raising `StopIteration`. Instead, these operations could return an empty `DataArray` or `Dataset` or provide an informative message/error. This handling ensures that the user is informed about the operation on an empty dataset in a meaningful way, without breaking the flow of execution unexpectedly."
    },
    {
        "rel_file_path": "xarray/core/common.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/common.py",
        "start": 253,
        "end": 1003,
        "class_name": "DataWithCoords",
        "method_name": null,
        "code": "253 class DataWithCoords(SupportsArithmetic, AttrAccessMixin):\n254     \"\"\"Shared base class for Dataset and DataArray.\"\"\"\n255 \n256     _rolling_exp_cls = RollingExp\n257 \n258     def squeeze(self, dim: Union[Hashable, Iterable[Hashable], None] = None,\n259                 drop: bool = False,\n260                 axis: Union[int, Iterable[int], None] = None):\n261         \"\"\"Return a new object with squeezed data.\n262 \n263         Parameters\n264         ----------\n265         dim : None or Hashable or iterable of Hashable, optional\n266             Selects a subset of the length one dimensions. If a dimension is\n267             selected with length greater than one, an error is raised. If\n268             None, all length one dimensions are squeezed.\n269         drop : bool, optional\n270             If ``drop=True``, drop squeezed coordinates instead of making them\n271             scalar.\n272         axis : None or int or iterable of int, optional\n273             Like dim, but positional.\n274 \n275         Returns\n276         -------\n277         squeezed : same type as caller\n278             This object, but with with all or a subset of the dimensions of\n279             length 1 removed.\n280 \n281         See Also\n282         --------\n283         numpy.squeeze\n284         \"\"\"\n285         dims = get_squeeze_dims(self, dim, axis)\n286         return self.isel(drop=drop, **{d: 0 for d in dims})\n287 \n288     def get_index(self, key: Hashable) -> pd.Index:\n289         \"\"\"Get an index for a dimension, with fall-back to a default RangeIndex\n290         \"\"\"\n291         if key not in self.dims:\n292             raise KeyError(key)\n293 \n294         try:\n295             return self.indexes[key]\n296         except KeyError:\n297             # need to ensure dtype=int64 in case range is empty on Python 2\n298             return pd.Index(range(self.sizes[key]), name=key, dtype=np.int64)\n299 \n300     def _calc_assign_results(self, kwargs: Mapping[str, T]\n301                              ) -> MutableMapping[str, T]:\n302         results = SortedKeysDict()  # type: SortedKeysDict[str, T]\n303         for k, v in kwargs.items():\n304             if callable(v):\n305                 results[k] = v(self)\n306             else:\n307                 results[k] = v\n308         return results\n309 \n310     def assign_coords(self, **kwargs):\n311         \"\"\"Assign new coordinates to this object.\n312 \n313         Returns a new object with all the original data in addition to the new\n314         coordinates.\n315 \n316         Parameters\n317         ----------\n318         kwargs : keyword, value pairs\n319             keywords are the variables names. If the values are callable, they\n320             are computed on this object and assigned to new coordinate\n321             variables. If the values are not callable, (e.g. a DataArray,\n322             scalar, or array), they are simply assigned.\n323 \n324         Returns\n325         -------\n326         assigned : same type as caller\n327             A new object with the new coordinates in addition to the existing\n328             data.\n329 \n330         Examples\n331         --------\n332 \n333         Convert longitude coordinates from 0-359 to -180-179:\n334 \n335         >>> da = xr.DataArray(np.random.rand(4),\n336         ...                   coords=[np.array([358, 359, 0, 1])],\n337         ...                   dims='lon')\n338         >>> da\n339         <xarray.DataArray (lon: 4)>\n340         array([0.28298 , 0.667347, 0.657938, 0.177683])\n341         Coordinates:\n342           * lon      (lon) int64 358 359 0 1\n343         >>> da.assign_coords(lon=(((da.lon + 180) % 360) - 180))\n344         <xarray.DataArray (lon: 4)>\n345         array([0.28298 , 0.667347, 0.657938, 0.177683])\n346         Coordinates:\n347           * lon      (lon) int64 -2 -1 0 1\n348 \n349         Notes\n350         -----\n351         Since ``kwargs`` is a dictionary, the order of your arguments may not\n352         be preserved, and so the order of the new variables is not well\n353         defined. Assigning multiple variables within the same ``assign_coords``\n354         is possible, but you cannot reference other variables created within\n355         the same ``assign_coords`` call.\n356 \n357         See also\n358         --------\n359         Dataset.assign\n360         Dataset.swap_dims\n361         \"\"\"\n362         data = self.copy(deep=False)\n363         results = self._calc_assign_results(kwargs)\n364         data.coords.update(results)\n365         return data\n366 \n367     def assign_attrs(self, *args, **kwargs):\n368         \"\"\"Assign new attrs to this object.\n369 \n370         Returns a new object equivalent to self.attrs.update(*args, **kwargs).\n371 \n372         Parameters\n373         ----------\n374         args : positional arguments passed into ``attrs.update``.\n375         kwargs : keyword arguments passed into ``attrs.update``.\n376 \n377         Returns\n378         -------\n379         assigned : same type as caller\n380             A new object with the new attrs in addition to the existing data.\n381 \n382         See also\n383         --------\n384         Dataset.assign\n385         \"\"\"\n386         out = self.copy(deep=False)\n387         out.attrs.update(*args, **kwargs)\n388         return out\n389 \n390     def pipe(self, func: Union[Callable[..., T], Tuple[Callable[..., T], str]],\n391              *args, **kwargs) -> T:\n392         \"\"\"\n393         Apply func(self, *args, **kwargs)\n394 \n395         This method replicates the pandas method of the same name.\n396 \n397         Parameters\n398         ----------\n399         func : function\n400             function to apply to this xarray object (Dataset/DataArray).\n401             ``args``, and ``kwargs`` are passed into ``func``.\n402             Alternatively a ``(callable, data_keyword)`` tuple where\n403             ``data_keyword`` is a string indicating the keyword of\n404             ``callable`` that expects the xarray object.\n405         args : positional arguments passed into ``func``.\n406         kwargs : a dictionary of keyword arguments passed into ``func``.\n407 \n408         Returns\n409         -------\n410         object : the return type of ``func``.\n411 \n412         Notes\n413         -----\n414 \n415         Use ``.pipe`` when chaining together functions that expect\n416         xarray or pandas objects, e.g., instead of writing\n417 \n418         >>> f(g(h(ds), arg1=a), arg2=b, arg3=c)\n419 \n420         You can write\n421 \n422         >>> (ds.pipe(h)\n423         ...    .pipe(g, arg1=a)\n424         ...    .pipe(f, arg2=b, arg3=c)\n425         ... )\n426 \n427         If you have a function that takes the data as (say) the second\n428         argument, pass a tuple indicating which keyword expects the\n429         data. For example, suppose ``f`` takes its data as ``arg2``:\n430 \n431         >>> (ds.pipe(h)\n432         ...    .pipe(g, arg1=a)\n433         ...    .pipe((f, 'arg2'), arg1=a, arg3=c)\n434         ...  )\n435 \n436         See Also\n437         --------\n438         pandas.DataFrame.pipe\n439         \"\"\"\n440         if isinstance(func, tuple):\n441             func, target = func\n442             if target in kwargs:\n443                 raise ValueError('%s is both the pipe target and a keyword '\n444                                  'argument' % target)\n445             kwargs[target] = self\n446             return func(*args, **kwargs)\n447         else:\n448             return func(self, *args, **kwargs)\n449 \n450     def groupby(self, group, squeeze: bool = True,\n451                 restore_coord_dims: Optional[bool] = None):\n452         \"\"\"Returns a GroupBy object for performing grouped operations.\n453 \n454         Parameters\n455         ----------\n456         group : str, DataArray or IndexVariable\n457             Array whose unique values should be used to group this array. If a\n458             string, must be the name of a variable contained in this dataset.\n459         squeeze : boolean, optional\n460             If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n461             controls whether the subarrays have a dimension of length 1 along\n462             that dimension or if the dimension is squeezed out.\n463         restore_coord_dims : bool, optional\n464             If True, also restore the dimension order of multi-dimensional\n465             coordinates.\n466 \n467         Returns\n468         -------\n469         grouped : GroupBy\n470             A `GroupBy` object patterned after `pandas.GroupBy` that can be\n471             iterated over in the form of `(unique_value, grouped_array)` pairs.\n472 \n473         Examples\n474         --------\n475         Calculate daily anomalies for daily data:\n476 \n477         >>> da = xr.DataArray(np.linspace(0, 1826, num=1827),\n478         ...                   coords=[pd.date_range('1/1/2000', '31/12/2004',\n479         ...                           freq='D')],\n480         ...                   dims='time')\n481         >>> da\n482         <xarray.DataArray (time: 1827)>\n483         array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03, 1.826e+03])\n484         Coordinates:\n485           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\n486         >>> da.groupby('time.dayofyear') - da.groupby('time.dayofyear').mean('time')\n487         <xarray.DataArray (time: 1827)>\n488         array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])\n489         Coordinates:\n490           * time       (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\n491             dayofyear  (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ...\n492 \n493         See Also\n494         --------\n495         core.groupby.DataArrayGroupBy\n496         core.groupby.DatasetGroupBy\n497         \"\"\"  # noqa\n498         return self._groupby_cls(self, group, squeeze=squeeze,\n499                                  restore_coord_dims=restore_coord_dims)\n500 \n501     def groupby_bins(self, group, bins, right: bool = True, labels=None,\n502                      precision: int = 3, include_lowest: bool = False,\n503                      squeeze: bool = True,\n504                      restore_coord_dims: Optional[bool] = None):\n505         \"\"\"Returns a GroupBy object for performing grouped operations.\n506 \n507         Rather than using all unique values of `group`, the values are discretized\n508         first by applying `pandas.cut` [1]_ to `group`.\n509 \n510         Parameters\n511         ----------\n512         group : str, DataArray or IndexVariable\n513             Array whose binned values should be used to group this array. If a\n514             string, must be the name of a variable contained in this dataset.\n515         bins : int or array of scalars\n516             If bins is an int, it defines the number of equal-width bins in the\n517             range of x. However, in this case, the range of x is extended by .1%\n518             on each side to include the min or max values of x. If bins is a\n519             sequence it defines the bin edges allowing for non-uniform bin\n520             width. No extension of the range of x is done in this case.\n521         right : boolean, optional\n522             Indicates whether the bins include the rightmost edge or not. If\n523             right == True (the default), then the bins [1,2,3,4] indicate\n524             (1,2], (2,3], (3,4].\n525         labels : array or boolean, default None\n526             Used as labels for the resulting bins. Must be of the same length as\n527             the resulting bins. If False, string bin labels are assigned by\n528             `pandas.cut`.\n529         precision : int\n530             The precision at which to store and display the bins labels.\n531         include_lowest : bool\n532             Whether the first interval should be left-inclusive or not.\n533         squeeze : boolean, optional\n534             If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n535             controls whether the subarrays have a dimension of length 1 along\n536             that dimension or if the dimension is squeezed out.\n537         restore_coord_dims : bool, optional\n538             If True, also restore the dimension order of multi-dimensional\n539             coordinates.\n540 \n541         Returns\n542         -------\n543         grouped : GroupBy\n544             A `GroupBy` object patterned after `pandas.GroupBy` that can be\n545             iterated over in the form of `(unique_value, grouped_array)` pairs.\n546             The name of the group has the added suffix `_bins` in order to\n547             distinguish it from the original variable.\n548 \n549         References\n550         ----------\n551         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n552         \"\"\"  # noqa\n553         return self._groupby_cls(self, group, squeeze=squeeze, bins=bins,\n554                                  restore_coord_dims=restore_coord_dims,\n555                                  cut_kwargs={'right': right, 'labels': labels,\n556                                              'precision': precision,\n557                                              'include_lowest':\n558                                                  include_lowest})\n559 \n560     def rolling(self, dim: Optional[Mapping[Hashable, int]] = None,\n561                 min_periods: Optional[int] = None, center: bool = False,\n562                 **window_kwargs: int):\n563         \"\"\"\n564         Rolling window object.\n565 \n566         Parameters\n567         ----------\n568         dim: dict, optional\n569             Mapping from the dimension name to create the rolling iterator\n570             along (e.g. `time`) to its moving window size.\n571         min_periods : int, default None\n572             Minimum number of observations in window required to have a value\n573             (otherwise result is NA). The default, None, is equivalent to\n574             setting min_periods equal to the size of the window.\n575         center : boolean, default False\n576             Set the labels at the center of the window.\n577         **window_kwargs : optional\n578             The keyword arguments form of ``dim``.\n579             One of dim or window_kwargs must be provided.\n580 \n581         Returns\n582         -------\n583         Rolling object (core.rolling.DataArrayRolling for DataArray,\n584         core.rolling.DatasetRolling for Dataset.)\n585 \n586         Examples\n587         --------\n588         Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:\n589 \n590         >>> da = xr.DataArray(np.linspace(0, 11, num=12),\n591         ...                   coords=[pd.date_range('15/12/1999',\n592         ...                           periods=12, freq=pd.DateOffset(months=1))],\n593         ...                   dims='time')\n594         >>> da\n595         <xarray.DataArray (time: 12)>\n596         array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7., 8.,   9.,  10.,  11.])\n597         Coordinates:\n598           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 2000-02-15 ...\n599         >>> da.rolling(time=3, center=True).mean()\n600         <xarray.DataArray (time: 12)>\n601         array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])\n602         Coordinates:\n603           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 2000-02-15 ...\n604 \n605         Remove the NaNs using ``dropna()``:\n606 \n607         >>> da.rolling(time=3, center=True).mean().dropna('time')\n608         <xarray.DataArray (time: 10)>\n609         array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n610         Coordinates:\n611           * time     (time) datetime64[ns] 2000-01-15 2000-02-15 2000-03-15 ...\n612 \n613         See Also\n614         --------\n615         core.rolling.DataArrayRolling\n616         core.rolling.DatasetRolling\n617         \"\"\"  # noqa\n618         dim = either_dict_or_kwargs(dim, window_kwargs, 'rolling')\n619         return self._rolling_cls(self, dim, min_periods=min_periods,\n620                                  center=center)\n621 \n622     def rolling_exp(\n623         self,\n624         window: Optional[Mapping[Hashable, int]] = None,\n625         window_type: str = 'span',\n626         **window_kwargs\n627     ):\n628         \"\"\"\n629         Exponentially-weighted moving window.\n630         Similar to EWM in pandas\n631 \n632         Requires the optional Numbagg dependency.\n633 \n634         Parameters\n635         ----------\n636         window : A single mapping from a dimension name to window value,\n637                  optional\n638             dim : str\n639                 Name of the dimension to create the rolling exponential window\n640                 along (e.g., `time`).\n641             window : int\n642                 Size of the moving window. The type of this is specified in\n643                 `window_type`\n644         window_type : str, one of ['span', 'com', 'halflife', 'alpha'],\n645                       default 'span'\n646             The format of the previously supplied window. Each is a simple\n647             numerical transformation of the others. Described in detail:\n648             https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.ewm.html\n649         **window_kwargs : optional\n650             The keyword arguments form of ``window``.\n651             One of window or window_kwargs must be provided.\n652 \n653         See Also\n654         --------\n655         core.rolling_exp.RollingExp\n656         \"\"\"\n657         window = either_dict_or_kwargs(window, window_kwargs, 'rolling_exp')\n658 \n659         return self._rolling_exp_cls(self, window, window_type)\n660 \n661     def coarsen(self, dim: Optional[Mapping[Hashable, int]] = None,\n662                 boundary: str = 'exact',\n663                 side: Union[str, Mapping[Hashable, str]] = 'left',\n664                 coord_func: str = 'mean',\n665                 **window_kwargs: int):\n666         \"\"\"\n667         Coarsen object.\n668 \n669         Parameters\n670         ----------\n671         dim: dict, optional\n672             Mapping from the dimension name to the window size.\n673             dim : str\n674                 Name of the dimension to create the rolling iterator\n675                 along (e.g., `time`).\n676             window : int\n677                 Size of the moving window.\n678         boundary : 'exact' | 'trim' | 'pad'\n679             If 'exact', a ValueError will be raised if dimension size is not a\n680             multiple of the window size. If 'trim', the excess entries are\n681             dropped. If 'pad', NA will be padded.\n682         side : 'left' or 'right' or mapping from dimension to 'left' or 'right'\n683         coord_func: function (name) that is applied to the coordintes,\n684             or a mapping from coordinate name to function (name).\n685 \n686         Returns\n687         -------\n688         Coarsen object (core.rolling.DataArrayCoarsen for DataArray,\n689         core.rolling.DatasetCoarsen for Dataset.)\n690 \n691         Examples\n692         --------\n693         Coarsen the long time series by averaging over every four days.\n694 \n695         >>> da = xr.DataArray(np.linspace(0, 364, num=364),\n696         ...                   dims='time',\n697         ...                   coords={'time': pd.date_range(\n698         ...                       '15/12/1999', periods=364)})\n699         >>> da\n700         <xarray.DataArray (time: 364)>\n701         array([  0.      ,   1.002755,   2.00551 , ..., 361.99449 , 362.997245,\n702                364.      ])\n703         Coordinates:\n704           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-12-12\n705         >>>\n706         >>> da.coarsen(time=3, boundary='trim').mean()\n707         <xarray.DataArray (time: 121)>\n708         array([  1.002755,   4.011019,   7.019284,  ...,  358.986226,\n709                361.99449 ])\n710         Coordinates:\n711           * time     (time) datetime64[ns] 1999-12-16 1999-12-19 ... 2000-12-10\n712         >>>\n713 \n714         See Also\n715         --------\n716         core.rolling.DataArrayCoarsen\n717         core.rolling.DatasetCoarsen\n718         \"\"\"\n719         dim = either_dict_or_kwargs(dim, window_kwargs, 'coarsen')\n720         return self._coarsen_cls(\n721             self, dim, boundary=boundary, side=side,\n722             coord_func=coord_func)\n723 \n724     def resample(self, indexer: Optional[Mapping[Hashable, str]] = None,\n725                  skipna=None, closed: Optional[str] = None,\n726                  label: Optional[str] = None,\n727                  base: int = 0, keep_attrs: Optional[bool] = None,\n728                  loffset=None, restore_coord_dims: Optional[bool] = None,\n729                  **indexer_kwargs: str):\n730         \"\"\"Returns a Resample object for performing resampling operations.\n731 \n732         Handles both downsampling and upsampling. If any intervals contain no\n733         values from the original object, they will be given the value ``NaN``.\n734 \n735         Parameters\n736         ----------\n737         indexer : {dim: freq}, optional\n738             Mapping from the dimension name to resample frequency.\n739         skipna : bool, optional\n740             Whether to skip missing values when aggregating in downsampling.\n741         closed : 'left' or 'right', optional\n742             Side of each interval to treat as closed.\n743         label : 'left or 'right', optional\n744             Side of each interval to use for labeling.\n745         base : int, optional\n746             For frequencies that evenly subdivide 1 day, the \"origin\" of the\n747             aggregated intervals. For example, for '24H' frequency, base could\n748             range from 0 through 23.\n749         loffset : timedelta or str, optional\n750             Offset used to adjust the resampled time labels. Some pandas date\n751             offset strings are supported.\n752         keep_attrs : bool, optional\n753             If True, the object's attributes (`attrs`) will be copied from\n754             the original object to the new one.  If False (default), the new\n755             object will be returned without attributes.\n756         restore_coord_dims : bool, optional\n757             If True, also restore the dimension order of multi-dimensional\n758             coordinates.\n759         **indexer_kwargs : {dim: freq}\n760             The keyword arguments form of ``indexer``.\n761             One of indexer or indexer_kwargs must be provided.\n762 \n763         Returns\n764         -------\n765         resampled : same type as caller\n766             This object resampled.\n767 \n768         Examples\n769         --------\n770         Downsample monthly time-series data to seasonal data:\n771 \n772         >>> da = xr.DataArray(np.linspace(0, 11, num=12),\n773         ...                   coords=[pd.date_range('15/12/1999',\n774         ...                           periods=12, freq=pd.DateOffset(months=1))],\n775         ...                   dims='time')\n776         >>> da\n777         <xarray.DataArray (time: 12)>\n778         array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7., 8.,   9.,  10.,  11.])\n779         Coordinates:\n780           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 2000-02-15 ...\n781         >>> da.resample(time=\"QS-DEC\").mean()\n782         <xarray.DataArray (time: 4)>\n783         array([ 1.,  4.,  7., 10.])\n784         Coordinates:\n785           * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01\n786 \n787         Upsample monthly time-series data to daily data:\n788 \n789         >>> da.resample(time='1D').interpolate('linear')\n790         <xarray.DataArray (time: 337)>\n791         array([ 0.      ,  0.032258,  0.064516, ..., 10.935484, 10.967742, 11.      ])\n792         Coordinates:\n793           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 1999-12-17 ...\n794 \n795         Limit scope of upsampling method\n796         >>> da.resample(time='1D').nearest(tolerance='1D')\n797         <xarray.DataArray (time: 337)>\n798         array([ 0.,  0., nan, ..., nan, 11., 11.])\n799         Coordinates:\n800           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15\n801 \n802         References\n803         ----------\n804 \n805         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n806         \"\"\"  # noqa\n807         # TODO support non-string indexer after removing the old API.\n808 \n809         from .dataarray import DataArray\n810         from .resample import RESAMPLE_DIM\n811         from ..coding.cftimeindex import CFTimeIndex\n812 \n813         if keep_attrs is None:\n814             keep_attrs = _get_keep_attrs(default=False)\n815 \n816         # note: the second argument (now 'skipna') use to be 'dim'\n817         if ((skipna is not None and not isinstance(skipna, bool))\n818                 or ('how' in indexer_kwargs and 'how' not in self.dims)\n819                 or ('dim' in indexer_kwargs and 'dim' not in self.dims)):\n820             raise TypeError(\n821                 'resample() no longer supports the `how` or '\n822                 '`dim` arguments. Instead call methods on resample '\n823                 \"objects, e.g., data.resample(time='1D').mean()\")\n824 \n825         indexer = either_dict_or_kwargs(indexer, indexer_kwargs, 'resample')\n826         if len(indexer) != 1:\n827             raise ValueError(\n828                 \"Resampling only supported along single dimensions.\"\n829             )\n830         dim, freq = next(iter(indexer.items()))\n831 \n832         dim_name = dim\n833         dim_coord = self[dim]\n834 \n835         if isinstance(self.indexes[dim_name], CFTimeIndex):\n836             from .resample_cftime import CFTimeGrouper\n837             grouper = CFTimeGrouper(freq, closed, label, base, loffset)\n838         else:\n839             # TODO: to_offset() call required for pandas==0.19.2\n840             grouper = pd.Grouper(freq=freq, closed=closed, label=label,\n841                                  base=base,\n842                                  loffset=pd.tseries.frequencies.to_offset(\n843                                      loffset))\n844         group = DataArray(dim_coord, coords=dim_coord.coords,\n845                           dims=dim_coord.dims, name=RESAMPLE_DIM)\n846         resampler = self._resample_cls(self, group=group, dim=dim_name,\n847                                        grouper=grouper,\n848                                        resample_dim=RESAMPLE_DIM,\n849                                        restore_coord_dims=restore_coord_dims)\n850 \n851         return resampler\n852 \n853     def where(self, cond, other=dtypes.NA, drop: bool = False):\n854         \"\"\"Filter elements from this object according to a condition.\n855 \n856         This operation follows the normal broadcasting and alignment rules that\n857         xarray uses for binary arithmetic.\n858 \n859         Parameters\n860         ----------\n861         cond : DataArray or Dataset with boolean dtype\n862             Locations at which to preserve this object's values.\n863         other : scalar, DataArray or Dataset, optional\n864             Value to use for locations in this object where ``cond`` is False.\n865             By default, these locations filled with NA.\n866         drop : boolean, optional\n867             If True, coordinate labels that only correspond to False values of\n868             the condition are dropped from the result. Mutually exclusive with\n869             ``other``.\n870 \n871         Returns\n872         -------\n873         Same type as caller.\n874 \n875         Examples\n876         --------\n877 \n878         >>> import numpy as np\n879         >>> a = xr.DataArray(np.arange(25).reshape(5, 5), dims=('x', 'y'))\n880         >>> a.where(a.x + a.y < 4)\n881         <xarray.DataArray (x: 5, y: 5)>\n882         array([[  0.,   1.,   2.,   3.,  nan],\n883                [  5.,   6.,   7.,  nan,  nan],\n884                [ 10.,  11.,  nan,  nan,  nan],\n885                [ 15.,  nan,  nan,  nan,  nan],\n886                [ nan,  nan,  nan,  nan,  nan]])\n887         Dimensions without coordinates: x, y\n888         >>> a.where(a.x + a.y < 5, -1)\n889         <xarray.DataArray (x: 5, y: 5)>\n890         array([[ 0,  1,  2,  3,  4],\n891                [ 5,  6,  7,  8, -1],\n892                [10, 11, 12, -1, -1],\n893                [15, 16, -1, -1, -1],\n894                [20, -1, -1, -1, -1]])\n895         Dimensions without coordinates: x, y\n896         >>> a.where(a.x + a.y < 4, drop=True)\n897         <xarray.DataArray (x: 4, y: 4)>\n898         array([[  0.,   1.,   2.,   3.],\n899                [  5.,   6.,   7.,  nan],\n900                [ 10.,  11.,  nan,  nan],\n901                [ 15.,  nan,  nan,  nan]])\n902         Dimensions without coordinates: x, y\n903 \n904         See also\n905         --------\n906         numpy.where : corresponding numpy function\n907         where : equivalent function\n908         \"\"\"\n909         from .alignment import align\n910         from .dataarray import DataArray\n911         from .dataset import Dataset\n912 \n913         if drop:\n914             if other is not dtypes.NA:\n915                 raise ValueError('cannot set `other` if drop=True')\n916 \n917             if not isinstance(cond, (Dataset, DataArray)):\n918                 raise TypeError(\"cond argument is %r but must be a %r or %r\" %\n919                                 (cond, Dataset, DataArray))\n920 \n921             # align so we can use integer indexing\n922             self, cond = align(self, cond)\n923 \n924             # get cond with the minimal size needed for the Dataset\n925             if isinstance(cond, Dataset):\n926                 clipcond = cond.to_array().any('variable')\n927             else:\n928                 clipcond = cond\n929 \n930             # clip the data corresponding to coordinate dims that are not used\n931             nonzeros = zip(clipcond.dims, np.nonzero(clipcond.values))\n932             indexers = {k: np.unique(v) for k, v in nonzeros}\n933 \n934             self = self.isel(**indexers)\n935             cond = cond.isel(**indexers)\n936 \n937         return ops.where_method(self, cond, other)\n938 \n939     def close(self: Any) -> None:\n940         \"\"\"Close any files linked to this object\n941         \"\"\"\n942         if self._file_obj is not None:\n943             self._file_obj.close()\n944         self._file_obj = None\n945 \n946     def isin(self, test_elements):\n947         \"\"\"Tests each value in the array for whether it is in test elements.\n948 \n949         Parameters\n950         ----------\n951         test_elements : array_like\n952             The values against which to test each value of `element`.\n953             This argument is flattened if an array or array_like.\n954             See numpy notes for behavior with non-array-like parameters.\n955 \n956         Returns\n957         -------\n958         isin : same as object, bool\n959             Has the same shape as this object.\n960 \n961         Examples\n962         --------\n963 \n964         >>> array = xr.DataArray([1, 2, 3], dims='x')\n965         >>> array.isin([1, 3])\n966         <xarray.DataArray (x: 3)>\n967         array([ True, False,  True])\n968         Dimensions without coordinates: x\n969 \n970         See also\n971         --------\n972         numpy.isin\n973         \"\"\"\n974         from .computation import apply_ufunc\n975         from .dataset import Dataset\n976         from .dataarray import DataArray\n977         from .variable import Variable\n978 \n979         if isinstance(test_elements, Dataset):\n980             raise TypeError(\n981                 'isin() argument must be convertible to an array: {}'\n982                 .format(test_elements))\n983         elif isinstance(test_elements, (Variable, DataArray)):\n984             # need to explicitly pull out data to support dask arrays as the\n985             # second argument\n986             test_elements = test_elements.data\n987 \n988         return apply_ufunc(\n989             duck_array_ops.isin,\n990             self,\n991             kwargs=dict(test_elements=test_elements),\n992             dask='allowed',\n993         )\n994 \n995     def __enter__(self: T) -> T:\n996         return self\n997 \n998     def __exit__(self, exc_type, exc_value, traceback) -> None:\n999         self.close()\n1000 \n1001     def __getitem__(self, value):\n1002         # implementations of this class should implement this method\n1003         raise NotImplementedError\n",
        "intended_behavior": "This class provides additional context to the issue."
    }
]