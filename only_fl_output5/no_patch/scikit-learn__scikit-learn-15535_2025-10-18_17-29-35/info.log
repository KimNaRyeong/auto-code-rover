2025-10-18 17:29:35.868 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 17:29:40.373 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=228, output_tokens=12, cost=0.002640
2025-10-18 17:29:54.616 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=408, output_tokens=559, cost=0.020850
2025-10-18 17:29:55.623 | INFO     | registered non reproducing test 0
2025-10-18 17:30:11.950 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1852, output_tokens=764, cost=0.041440
2025-10-18 17:30:13.118 | INFO     | registered non reproducing test 1
2025-10-18 17:30:27.089 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2042, output_tokens=583, cost=0.037910
2025-10-18 17:30:28.068 | INFO     | registered non reproducing test 2
2025-10-18 17:30:28.068 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-18 17:30:28.074 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 17:30:37.922 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=536, output_tokens=426, cost=0.018140
2025-10-18 17:30:37.930 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 17:30:40.160 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=780, output_tokens=67, cost=0.009810
2025-10-18 17:30:40.162 | DEBUG    | Extracted a valid json.
2025-10-18 17:30:40.162 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_method(\"mutual_info_score\")",
        "search_code(\"input validation\")",
        "search_code(\"convert string to float\")",
        "search_code(\"astype(object)\")",
        "search_code(\"np.random.choice\")"
    ],
    "bug_locations": []
}
2025-10-18 17:30:40.267 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 17:30:40.267 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 17:31:00.426 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2808, output_tokens=825, cost=0.052830
2025-10-18 17:31:00.443 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 17:31:06.203 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3849, output_tokens=243, cost=0.045780
2025-10-18 17:31:06.209 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 17:31:13.358 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=597, output_tokens=253, cost=0.013560
2025-10-18 17:31:13.359 | DEBUG    | Extracted a valid json.
2025-10-18 17:31:13.359 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "sklearn/metrics/cluster/_supervised.py",
            "function": "mutual_info_score",
            "intended_behavior": "The function should be modified to gracefully handle inputs of type object, especially when these inputs are strings that represent categorical data. This involves either pre-processing the inputs to convert them to a form that can be handled by the mathematical operations (for instance, encoding string labels to integer labels) or updating the utility functions it relies on (such as `check_clusterings` and `contingency_matrix`) to support object type inputs more robustly. The modification should ensure that if inputs are of type object and can be interpreted as categorical data, they are processed in such a way that does not lead to a `ValueError` about being unable to convert string to float. Furthermore, the documentation should be updated to clearly state how object types are handled, and if necessary, warnings should be emitted to inform the user about automatic type conversions or potential issues with the given input types. This update aims to restore the behavior consistent with version 0.21.1, where such inputs did not result in errors."
        }
    ]
}
2025-10-18 17:31:13.364 | DEBUG    | Bug location extracted successfully: [<file>sklearn/metrics/cluster/_supervised.py</file>
<class>None</class>
<method>None</method>
<code>
1 """Utilities to evaluate the clustering performance of models.
2 
3 Functions named as *_score return a scalar value to maximize: the higher the
4 better.
5 """
6 
7 # Authors: Olivier Grisel <olivier.grisel@ensta.org>
8 #          Wei LI <kuantkid@gmail.com>
9 #          Diego Molla <dmolla-aliod@gmail.com>
10 #          Arnaud Fouchet <foucheta@gmail.com>
11 #          Thierry Guillemot <thierry.guillemot.work@gmail.com>
12 #          Gregory Stupp <stuppie@gmail.com>
13 #          Joel Nothman <joel.nothman@gmail.com>
14 #          Arya McCarthy <arya@jhu.edu>
15 # License: BSD 3 clause
16 
17 
18 from math import log
19 
20 import numpy as np
21 from scipy import sparse as sp
22 
23 from ._expected_mutual_info_fast import expected_mutual_information
24 from ...utils.validation import check_array, check_consistent_length
25 from ...utils.fixes import comb, _astype_copy_false
26 
27 
28 def _comb2(n):
29     # the exact version is faster for k == 2: use it by default globally in
30     # this module instead of the float approximate variant
31     return comb(n, 2, exact=1)
32 
33 
34 def check_clusterings(labels_true, labels_pred):
35     """Check that the labels arrays are 1D and of same dimension.
36 
37     Parameters
38     ----------
39     labels_true : array-like of shape (n_samples,)
40         The true labels.
41 
42     labels_pred : array-like of shape (n_samples,)
43         The predicted labels.
44     """
45     labels_true = check_array(
46         labels_true, ensure_2d=False, ensure_min_samples=0
47     )
48     labels_pred = check_array(
49         labels_pred, ensure_2d=False, ensure_min_samples=0
50     )
51 
52     # input checks
53     if labels_true.ndim != 1:
54         raise ValueError(
55             "labels_true must be 1D: shape is %r" % (labels_true.shape,))
56     if labels_pred.ndim != 1:
57         raise ValueError(
58             "labels_pred must be 1D: shape is %r" % (labels_pred.shape,))
59     check_consistent_length(labels_true, labels_pred)
60 
61     return labels_true, labels_pred
62 
63 
64 def _generalized_average(U, V, average_method):
65     """Return a particular mean of two numbers."""
66     if average_method == "min":
67         return min(U, V)
68     elif average_method == "geometric":
69         return np.sqrt(U * V)
70     elif average_method == "arithmetic":
71         return np.mean([U, V])
72     elif average_method == "max":
73         return max(U, V)
74     else:
75         raise ValueError("'average_method' must be 'min', 'geometric', "
76                          "'arithmetic', or 'max'")
77 
78 
79 def contingency_matrix(labels_true, labels_pred, eps=None, sparse=False):
80     """Build a contingency matrix describing the relationship between labels.
81 
82     Parameters
83     ----------
84     labels_true : int array, shape = [n_samples]
85         Ground truth class labels to be used as a reference
86 
87     labels_pred : array-like of shape (n_samples,)
88         Cluster labels to evaluate
89 
90     eps : None or float, optional.
91         If a float, that value is added to all values in the contingency
92         matrix. This helps to stop NaN propagation.
93         If ``None``, nothing is adjusted.
94 
95     sparse : boolean, optional.
96         If True, return a sparse CSR continency matrix. If ``eps is not None``,
97         and ``sparse is True``, will throw ValueError.
98 
99         .. versionadded:: 0.18
100 
101     Returns
102     -------
103     contingency : {array-like, sparse}, shape=[n_classes_true, n_classes_pred]
104         Matrix :math:`C` such that :math:`C_{i, j}` is the number of samples in
105         true class :math:`i` and in predicted class :math:`j`. If
106         ``eps is None``, the dtype of this array will be integer. If ``eps`` is
107         given, the dtype will be float.
108         Will be a ``scipy.sparse.csr_matrix`` if ``sparse=True``.
109     """
110 
111     if eps is not None and sparse:
112         raise ValueError("Cannot set 'eps' when sparse=True")
113 
114     classes, class_idx = np.unique(labels_true, return_inverse=True)
115     clusters, cluster_idx = np.unique(labels_pred, return_inverse=True)
116     n_classes = classes.shape[0]
117     n_clusters = clusters.shape[0]
118     # Using coo_matrix to accelerate simple histogram calculation,
119     # i.e. bins are consecutive integers
120     # Currently, coo_matrix is faster than histogram2d for simple cases
121     contingency = sp.coo_matrix((np.ones(class_idx.shape[0]),
122                                  (class_idx, cluster_idx)),
123                                 shape=(n_classes, n_clusters),
124                                 dtype=np.int)
125     if sparse:
126         contingency = contingency.tocsr()
127         contingency.sum_duplicates()
128     else:
129         contingency = contingency.toarray()
130         if eps is not None:
131             # don't use += as contingency is integer
132             contingency = contingency + eps
133     return contingency
134 
135 
136 # clustering measures
137 
138 def adjusted_rand_score(labels_true, labels_pred):
139     """Rand index adjusted for chance.
140 
141     The Rand Index computes a similarity measure between two clusterings
142     by considering all pairs of samples and counting pairs that are
143     assigned in the same or different clusters in the predicted and
144     true clusterings.
145 
146     The raw RI score is then "adjusted for chance" into the ARI score
147     using the following scheme::
148 
149         ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)
150 
151     The adjusted Rand index is thus ensured to have a value close to
152     0.0 for random labeling independently of the number of clusters and
153     samples and exactly 1.0 when the clusterings are identical (up to
154     a permutation).
155 
156     ARI is a symmetric measure::
157 
158         adjusted_rand_score(a, b) == adjusted_rand_score(b, a)
159 
160     Read more in the :ref:`User Guide <adjusted_rand_score>`.
161 
162     Parameters
163     ----------
164     labels_true : int array, shape = [n_samples]
165         Ground truth class labels to be used as a reference
166 
167     labels_pred : array-like of shape (n_samples,)
168         Cluster labels to evaluate
169 
170     Returns
171     -------
172     ari : float
173        Similarity score between -1.0 and 1.0. Random labelings have an ARI
174        close to 0.0. 1.0 stands for perfect match.
175 
176     Examples
177     --------
178 
179     Perfectly matching labelings have a score of 1 even
180 
181       >>> from sklearn.metrics.cluster import adjusted_rand_score
182       >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])
183       1.0
184       >>> adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0])
185       1.0
186 
187     Labelings that assign all classes members to the same clusters
188     are complete be not always pure, hence penalized::
189 
190       >>> adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])
191       0.57...
192 
193     ARI is symmetric, so labelings that have pure clusters with members
194     coming from the same classes but unnecessary splits are penalized::
195 
196       >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])
197       0.57...
198 
199     If classes members are completely split across different clusters, the
200     assignment is totally incomplete, hence the ARI is very low::
201 
202       >>> adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])
203       0.0
204 
205     References
206     ----------
207 
208     .. [Hubert1985] L. Hubert and P. Arabie, Comparing Partitions,
209       Journal of Classification 1985
210       https://link.springer.com/article/10.1007%2FBF01908075
211 
212     .. [wk] https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index
213 
214     See also
215     --------
216     adjusted_mutual_info_score: Adjusted Mutual Information
217 
218     """
219     labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
220     n_samples = labels_true.shape[0]
221     n_classes = np.unique(labels_true).shape[0]
222     n_clusters = np.unique(labels_pred).shape[0]
223 
224     # Special limit cases: no clustering since the data is not split;
225     # or trivial clustering where each document is assigned a unique cluster.
226     # These are perfect matches hence return 1.0.
227     if (n_classes == n_clusters == 1 or
228             n_classes == n_clusters == 0 or
229             n_classes == n_clusters == n_samples):
230         return 1.0
231 
232     # Compute the ARI using the contingency data
233     contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
234     sum_comb_c = sum(_comb2(n_c) for n_c in np.ravel(contingency.sum(axis=1)))
235     sum_comb_k = sum(_comb2(n_k) for n_k in np.ravel(contingency.sum(axis=0)))
236     sum_comb = sum(_comb2(n_ij) for n_ij in contingency.data)
237 
238     prod_comb = (sum_comb_c * sum_comb_k) / _comb2(n_samples)
239     mean_comb = (sum_comb_k + sum_comb_c) / 2.
240     return (sum_comb - prod_comb) / (mean_comb - prod_comb)
241 
242 
243 def homogeneity_completeness_v_measure(labels_true, labels_pred, beta=1.0):
244     """Compute the homogeneity and completeness and V-Measure scores at once.
245 
246     Those metrics are based on normalized conditional entropy measures of
247     the clustering labeling to evaluate given the knowledge of a Ground
248     Truth class labels of the same samples.
249 
250     A clustering result satisfies homogeneity if all of its clusters
251     contain only data points which are members of a single class.
252 
253     A clustering result satisfies completeness if all the data points
254     that are members of a given class are elements of the same cluster.
255 
256     Both scores have positive values between 0.0 and 1.0, larger values
257     being desirable.
258 
259     Those 3 metrics are independent of the absolute values of the labels:
260     a permutation of the class or cluster label values won't change the
261     score values in any way.
262 
263     V-Measure is furthermore symmetric: swapping ``labels_true`` and
264     ``label_pred`` will give the same score. This does not hold for
265     homogeneity and completeness. V-Measure is identical to
266     :func:`normalized_mutual_info_score` with the arithmetic averaging
267     method.
268 
269     Read more in the :ref:`User Guide <homogeneity_completeness>`.
270 
271     Parameters
272     ----------
273     labels_true : int array, shape = [n_samples]
274         ground truth class labels to be used as a reference
275 
276     labels_pred : array-like of shape (n_samples,)
277         cluster labels to evaluate
278 
279     beta : float
280         Ratio of weight attributed to ``homogeneity`` vs ``completeness``.
281         If ``beta`` is greater than 1, ``completeness`` is weighted more
282         strongly in the calculation. If ``beta`` is less than 1,
283         ``homogeneity`` is weighted more strongly.
284 
285     Returns
286     -------
287     homogeneity : float
288        score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling
289 
290     completeness : float
291        score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling
292 
293     v_measure : float
294         harmonic mean of the first two
295 
296     See also
297     --------
298     homogeneity_score
299     completeness_score
300     v_measure_score
301     """
302     labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
303 
304     if len(labels_true) == 0:
305         return 1.0, 1.0, 1.0
306 
307     entropy_C = entropy(labels_true)
308     entropy_K = entropy(labels_pred)
309 
310     contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
311     MI = mutual_info_score(None, None, contingency=contingency)
312 
313     homogeneity = MI / (entropy_C) if entropy_C else 1.0
314     completeness = MI / (entropy_K) if entropy_K else 1.0
315 
316     if homogeneity + completeness == 0.0:
317         v_measure_score = 0.0
318     else:
319         v_measure_score = ((1 + beta) * homogeneity * completeness
320                            / (beta * homogeneity + completeness))
321 
322     return homogeneity, completeness, v_measure_score
323 
324 
325 def homogeneity_score(labels_true, labels_pred):
326     """Homogeneity metric of a cluster labeling given a ground truth.
327 
328     A clustering result satisfies homogeneity if all of its clusters
329     contain only data points which are members of a single class.
330 
331     This metric is independent of the absolute values of the labels:
332     a permutation of the class or cluster label values won't change the
333     score value in any way.
334 
335     This metric is not symmetric: switching ``label_true`` with ``label_pred``
336     will return the :func:`completeness_score` which will be different in
337     general.
338 
339     Read more in the :ref:`User Guide <homogeneity_completeness>`.
340 
341     Parameters
342     ----------
343     labels_true : int array, shape = [n_samples]
344         ground truth class labels to be used as a reference
345 
346     labels_pred : array-like of shape (n_samples,)
347         cluster labels to evaluate
348 
349     Returns
350     -------
351     homogeneity : float
352        score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling
353 
354     References
355     ----------
356 
357     .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A
358        conditional entropy-based external cluster evaluation measure
359        <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_
360 
361     See also
362     --------
363     completeness_score
364     v_measure_score
365 
366     Examples
367     --------
368 
369     Perfect labelings are homogeneous::
370 
371       >>> from sklearn.metrics.cluster import homogeneity_score
372       >>> homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])
373       1.0
374 
375     Non-perfect labelings that further split classes into more clusters can be
376     perfectly homogeneous::
377 
378       >>> print("%.6f" % homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))
379       1.000000
380       >>> print("%.6f" % homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))
381       1.000000
382 
383     Clusters that include samples from different classes do not make for an
384     homogeneous labeling::
385 
386       >>> print("%.6f" % homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))
387       0.0...
388       >>> print("%.6f" % homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))
389       0.0...
390 
391     """
392     return homogeneity_completeness_v_measure(labels_true, labels_pred)[0]
393 
394 
395 def completeness_score(labels_true, labels_pred):
396     """Completeness metric of a cluster labeling given a ground truth.
397 
398     A clustering result satisfies completeness if all the data points
399     that are members of a given class are elements of the same cluster.
400 
401     This metric is independent of the absolute values of the labels:
402     a permutation of the class or cluster label values won't change the
403     score value in any way.
404 
405     This metric is not symmetric: switching ``label_true`` with ``label_pred``
406     will return the :func:`homogeneity_score` which will be different in
407     general.
408 
409     Read more in the :ref:`User Guide <homogeneity_completeness>`.
410 
411     Parameters
412     ----------
413     labels_true : int array, shape = [n_samples]
414         ground truth class labels to be used as a reference
415 
416     labels_pred : array-like of shape (n_samples,)
417         cluster labels to evaluate
418 
419     Returns
420     -------
421     completeness : float
422        score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling
423 
424     References
425     ----------
426 
427     .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A
428        conditional entropy-based external cluster evaluation measure
429        <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_
430 
431     See also
432     --------
433     homogeneity_score
434     v_measure_score
435 
436     Examples
437     --------
438 
439     Perfect labelings are complete::
440 
441       >>> from sklearn.metrics.cluster import completeness_score
442       >>> completeness_score([0, 0, 1, 1], [1, 1, 0, 0])
443       1.0
444 
445     Non-perfect labelings that assign all classes members to the same clusters
446     are still complete::
447 
448       >>> print(completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))
449       1.0
450       >>> print(completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))
451       0.999...
452 
453     If classes members are split across different clusters, the
454     assignment cannot be complete::
455 
456       >>> print(completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))
457       0.0
458       >>> print(completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))
459       0.0
460 
461     """
462     return homogeneity_completeness_v_measure(labels_true, labels_pred)[1]
463 
464 
465 def v_measure_score(labels_true, labels_pred, beta=1.0):
466     """V-measure cluster labeling given a ground truth.
467 
468     This score is identical to :func:`normalized_mutual_info_score` with
469     the ``'arithmetic'`` option for averaging.
470 
471     The V-measure is the harmonic mean between homogeneity and completeness::
472 
473         v = (1 + beta) * homogeneity * completeness
474              / (beta * homogeneity + completeness)
475 
476     This metric is independent of the absolute values of the labels:
477     a permutation of the class or cluster label values won't change the
478     score value in any way.
479 
480     This metric is furthermore symmetric: switching ``label_true`` with
481     ``label_pred`` will return the same score value. This can be useful to
482     measure the agreement of two independent label assignments strategies
483     on the same dataset when the real ground truth is not known.
484 
485 
486     Read more in the :ref:`User Guide <homogeneity_completeness>`.
487 
488     Parameters
489     ----------
490     labels_true : int array, shape = [n_samples]
491         ground truth class labels to be used as a reference
492 
493     labels_pred : array-like of shape (n_samples,)
494         cluster labels to evaluate
495 
496     beta : float
497         Ratio of weight attributed to ``homogeneity`` vs ``completeness``.
498         If ``beta`` is greater than 1, ``completeness`` is weighted more
499         strongly in the calculation. If ``beta`` is less than 1,
500         ``homogeneity`` is weighted more strongly.
501 
502     Returns
503     -------
504     v_measure : float
505        score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling
506 
507     References
508     ----------
509 
510     .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A
511        conditional entropy-based external cluster evaluation measure
512        <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_
513 
514     See also
515     --------
516     homogeneity_score
517     completeness_score
518     normalized_mutual_info_score
519 
520     Examples
521     --------
522 
523     Perfect labelings are both homogeneous and complete, hence have score 1.0::
524 
525       >>> from sklearn.metrics.cluster import v_measure_score
526       >>> v_measure_score([0, 0, 1, 1], [0, 0, 1, 1])
527       1.0
528       >>> v_measure_score([0, 0, 1, 1], [1, 1, 0, 0])
529       1.0
530 
531     Labelings that assign all classes members to the same clusters
532     are complete be not homogeneous, hence penalized::
533 
534       >>> print("%.6f" % v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))
535       0.8...
536       >>> print("%.6f" % v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))
537       0.66...
538 
539     Labelings that have pure clusters with members coming from the same
540     classes are homogeneous but un-necessary splits harms completeness
541     and thus penalize V-measure as well::
542 
543       >>> print("%.6f" % v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))
544       0.8...
545       >>> print("%.6f" % v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))
546       0.66...
547 
548     If classes members are completely split across different clusters,
549     the assignment is totally incomplete, hence the V-Measure is null::
550 
551       >>> print("%.6f" % v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))
552       0.0...
553 
554     Clusters that include samples from totally different classes totally
555     destroy the homogeneity of the labeling, hence::
556 
557       >>> print("%.6f" % v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))
558       0.0...
559 
560     """
561     return homogeneity_completeness_v_measure(labels_true, labels_pred,
562                                               beta=beta)[2]
563 
564 
565 def mutual_info_score(labels_true, labels_pred, contingency=None):
566     """Mutual Information between two clusterings.
567 
568     The Mutual Information is a measure of the similarity between two labels of
569     the same data. Where :math:`|U_i|` is the number of the samples
570     in cluster :math:`U_i` and :math:`|V_j|` is the number of the
571     samples in cluster :math:`V_j`, the Mutual Information
572     between clusterings :math:`U` and :math:`V` is given as:
573 
574     .. math::
575 
576         MI(U,V)=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i\\cap V_j|}{N}
577         \\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}
578 
579     This metric is independent of the absolute values of the labels:
580     a permutation of the class or cluster label values won't change the
581     score value in any way.
582 
583     This metric is furthermore symmetric: switching ``label_true`` with
584     ``label_pred`` will return the same score value. This can be useful to
585     measure the agreement of two independent label assignments strategies
586     on the same dataset when the real ground truth is not known.
587 
588     Read more in the :ref:`User Guide <mutual_info_score>`.
589 
590     Parameters
591     ----------
592     labels_true : int array, shape = [n_samples]
593         A clustering of the data into disjoint subsets.
594 
595     labels_pred : int array-like of shape (n_samples,)
596         A clustering of the data into disjoint subsets.
597 
598     contingency : {None, array, sparse matrix}, \
599                   shape = [n_classes_true, n_classes_pred]
600         A contingency matrix given by the :func:`contingency_matrix` function.
601         If value is ``None``, it will be computed, otherwise the given value is
602         used, with ``labels_true`` and ``labels_pred`` ignored.
603 
604     Returns
605     -------
606     mi : float
607        Mutual information, a non-negative value
608 
609     Notes
610     -----
611     The logarithm used is the natural logarithm (base-e).
612 
613     See also
614     --------
615     adjusted_mutual_info_score: Adjusted against chance Mutual Information
616     normalized_mutual_info_score: Normalized Mutual Information
617     """
618     if contingency is None:
619         labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
620         contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
621     else:
622         contingency = check_array(contingency,
623                                   accept_sparse=['csr', 'csc', 'coo'],
624                                   dtype=[int, np.int32, np.int64])
625 
626     if isinstance(contingency, np.ndarray):
627         # For an array
628         nzx, nzy = np.nonzero(contingency)
629         nz_val = contingency[nzx, nzy]
630     elif sp.issparse(contingency):
631         # For a sparse matrix
632         nzx, nzy, nz_val = sp.find(contingency)
633     else:
634         raise ValueError("Unsupported type for 'contingency': %s" %
635                          type(contingency))
636 
637     contingency_sum = contingency.sum()
638     pi = np.ravel(contingency.sum(axis=1))
639     pj = np.ravel(contingency.sum(axis=0))
640     log_contingency_nm = np.log(nz_val)
641     contingency_nm = nz_val / contingency_sum
642     # Don't need to calculate the full outer product, just for non-zeroes
643     outer = (pi.take(nzx).astype(np.int64, copy=False)
644              * pj.take(nzy).astype(np.int64, copy=False))
645     log_outer = -np.log(outer) + log(pi.sum()) + log(pj.sum())
646     mi = (contingency_nm * (log_contingency_nm - log(contingency_sum)) +
647           contingency_nm * log_outer)
648     return mi.sum()
649 
650 
651 def adjusted_mutual_info_score(labels_true, labels_pred,
652                                average_method='arithmetic'):
653     """Adjusted Mutual Information between two clusterings.
654 
655     Adjusted Mutual Information (AMI) is an adjustment of the Mutual
656     Information (MI) score to account for chance. It accounts for the fact that
657     the MI is generally higher for two clusterings with a larger number of
658     clusters, regardless of whether there is actually more information shared.
659     For two clusterings :math:`U` and :math:`V`, the AMI is given as::
660 
661         AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [avg(H(U), H(V)) - E(MI(U, V))]
662 
663     This metric is independent of the absolute values of the labels:
664     a permutation of the class or cluster label values won't change the
665     score value in any way.
666 
667     This metric is furthermore symmetric: switching ``label_true`` with
668     ``label_pred`` will return the same score value. This can be useful to
669     measure the agreement of two independent label assignments strategies
670     on the same dataset when the real ground truth is not known.
671 
672     Be mindful that this function is an order of magnitude slower than other
673     metrics, such as the Adjusted Rand Index.
674 
675     Read more in the :ref:`User Guide <mutual_info_score>`.
676 
677     Parameters
678     ----------
679     labels_true : int array, shape = [n_samples]
680         A clustering of the data into disjoint subsets.
681 
682     labels_pred : int array-like of shape (n_samples,)
683         A clustering of the data into disjoint subsets.
684 
685     average_method : string, optional (default: 'arithmetic')
686         How to compute the normalizer in the denominator. Possible options
687         are 'min', 'geometric', 'arithmetic', and 'max'.
688 
689         .. versionadded:: 0.20
690 
691         .. versionchanged:: 0.22
692            The default value of ``average_method`` changed from 'max' to
693            'arithmetic'.
694 
695     Returns
696     -------
697     ami: float (upperlimited by 1.0)
698        The AMI returns a value of 1 when the two partitions are identical
699        (ie perfectly matched). Random partitions (independent labellings) have
700        an expected AMI around 0 on average hence can be negative.
701 
702     See also
703     --------
704     adjusted_rand_score: Adjusted Rand Index
705     mutual_info_score: Mutual Information (not adjusted for chance)
706 
707     Examples
708     --------
709 
710     Perfect labelings are both homogeneous and complete, hence have
711     score 1.0::
712 
713       >>> from sklearn.metrics.cluster import adjusted_mutual_info_score
714       >>> adjusted_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])
715       ... # doctest: +SKIP
716       1.0
717       >>> adjusted_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])
718       ... # doctest: +SKIP
719       1.0
720 
721     If classes members are completely split across different clusters,
722     the assignment is totally in-complete, hence the AMI is null::
723 
724       >>> adjusted_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])
725       ... # doctest: +SKIP
726       0.0
727 
728     References
729     ----------
730     .. [1] `Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for
731        Clusterings Comparison: Variants, Properties, Normalization and
732        Correction for Chance, JMLR
733        <http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>`_
734 
735     .. [2] `Wikipedia entry for the Adjusted Mutual Information
736        <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_
737 
738     """
739     labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
740     n_samples = labels_true.shape[0]
741     classes = np.unique(labels_true)
742     clusters = np.unique(labels_pred)
743     # Special limit cases: no clustering since the data is not split.
744     # This is a perfect match hence return 1.0.
745     if (classes.shape[0] == clusters.shape[0] == 1 or
746             classes.shape[0] == clusters.shape[0] == 0):
747         return 1.0
748     contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
749     contingency = contingency.astype(np.float64,
750                                      **_astype_copy_false(contingency))
751     # Calculate the MI for the two clusterings
752     mi = mutual_info_score(labels_true, labels_pred,
753                            contingency=contingency)
754     # Calculate the expected value for the mutual information
755     emi = expected_mutual_information(contingency, n_samples)
756     # Calculate entropy for each labeling
757     h_true, h_pred = entropy(labels_true), entropy(labels_pred)
758     normalizer = _generalized_average(h_true, h_pred, average_method)
759     denominator = normalizer - emi
760     # Avoid 0.0 / 0.0 when expectation equals maximum, i.e a perfect match.
761     # normalizer should always be >= emi, but because of floating-point
762     # representation, sometimes emi is slightly larger. Correct this
763     # by preserving the sign.
764     if denominator < 0:
765         denominator = min(denominator, -np.finfo('float64').eps)
766     else:
767         denominator = max(denominator, np.finfo('float64').eps)
768     ami = (mi - emi) / denominator
769     return ami
770 
771 
772 def normalized_mutual_info_score(labels_true, labels_pred,
773                                  average_method='arithmetic'):
774     """Normalized Mutual Information between two clusterings.
775 
776     Normalized Mutual Information (NMI) is a normalization of the Mutual
777     Information (MI) score to scale the results between 0 (no mutual
778     information) and 1 (perfect correlation). In this function, mutual
779     information is normalized by some generalized mean of ``H(labels_true)``
780     and ``H(labels_pred))``, defined by the `average_method`.
781 
782     This measure is not adjusted for chance. Therefore
783     :func:`adjusted_mutual_info_score` might be preferred.
784 
785     This metric is independent of the absolute values of the labels:
786     a permutation of the class or cluster label values won't change the
787     score value in any way.
788 
789     This metric is furthermore symmetric: switching ``label_true`` with
790     ``label_pred`` will return the same score value. This can be useful to
791     measure the agreement of two independent label assignments strategies
792     on the same dataset when the real ground truth is not known.
793 
794     Read more in the :ref:`User Guide <mutual_info_score>`.
795 
796     Parameters
797     ----------
798     labels_true : int array, shape = [n_samples]
799         A clustering of the data into disjoint subsets.
800 
801     labels_pred : int array-like of shape (n_samples,)
802         A clustering of the data into disjoint subsets.
803 
804     average_method : string, optional (default: 'arithmetic')
805         How to compute the normalizer in the denominator. Possible options
806         are 'min', 'geometric', 'arithmetic', and 'max'.
807 
808         .. versionadded:: 0.20
809 
810         .. versionchanged:: 0.22
811            The default value of ``average_method`` changed from 'geometric' to
812            'arithmetic'.
813 
814     Returns
815     -------
816     nmi : float
817        score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling
818 
819     See also
820     --------
821     v_measure_score: V-Measure (NMI with arithmetic mean option.)
822     adjusted_rand_score: Adjusted Rand Index
823     adjusted_mutual_info_score: Adjusted Mutual Information (adjusted
824         against chance)
825 
826     Examples
827     --------
828 
829     Perfect labelings are both homogeneous and complete, hence have
830     score 1.0::
831 
832       >>> from sklearn.metrics.cluster import normalized_mutual_info_score
833       >>> normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])
834       ... # doctest: +SKIP
835       1.0
836       >>> normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])
837       ... # doctest: +SKIP
838       1.0
839 
840     If classes members are completely split across different clusters,
841     the assignment is totally in-complete, hence the NMI is null::
842 
843       >>> normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])
844       ... # doctest: +SKIP
845       0.0
846 
847     """
848     labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
849     classes = np.unique(labels_true)
850     clusters = np.unique(labels_pred)
851     # Special limit cases: no clustering since the data is not split.
852     # This is a perfect match hence return 1.0.
853     if (classes.shape[0] == clusters.shape[0] == 1 or
854             classes.shape[0] == clusters.shape[0] == 0):
855         return 1.0
856     contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
857     contingency = contingency.astype(np.float64,
858                                      **_astype_copy_false(contingency))
859     # Calculate the MI for the two clusterings
860     mi = mutual_info_score(labels_true, labels_pred,
861                            contingency=contingency)
862     # Calculate the expected value for the mutual information
863     # Calculate entropy for each labeling
864     h_true, h_pred = entropy(labels_true), entropy(labels_pred)
865     normalizer = _generalized_average(h_true, h_pred, average_method)
866     # Avoid 0.0 / 0.0 when either entropy is zero.
867     normalizer = max(normalizer, np.finfo('float64').eps)
868     nmi = mi / normalizer
869     return nmi
870 
871 
872 def fowlkes_mallows_score(labels_true, labels_pred, sparse=False):
873     """Measure the similarity of two clusterings of a set of points.
874 
875     The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of
876     the precision and recall::
877 
878         FMI = TP / sqrt((TP + FP) * (TP + FN))
879 
880     Where ``TP`` is the number of **True Positive** (i.e. the number of pair of
881     points that belongs in the same clusters in both ``labels_true`` and
882     ``labels_pred``), ``FP`` is the number of **False Positive** (i.e. the
883     number of pair of points that belongs in the same clusters in
884     ``labels_true`` and not in ``labels_pred``) and ``FN`` is the number of
885     **False Negative** (i.e the number of pair of points that belongs in the
886     same clusters in ``labels_pred`` and not in ``labels_True``).
887 
888     The score ranges from 0 to 1. A high value indicates a good similarity
889     between two clusters.
890 
891     Read more in the :ref:`User Guide <fowlkes_mallows_scores>`.
892 
893     Parameters
894     ----------
895     labels_true : int array, shape = (``n_samples``,)
896         A clustering of the data into disjoint subsets.
897 
898     labels_pred : array, shape = (``n_samples``, )
899         A clustering of the data into disjoint subsets.
900 
901     sparse : bool
902         Compute contingency matrix internally with sparse matrix.
903 
904     Returns
905     -------
906     score : float
907        The resulting Fowlkes-Mallows score.
908 
909     Examples
910     --------
911 
912     Perfect labelings are both homogeneous and complete, hence have
913     score 1.0::
914 
915       >>> from sklearn.metrics.cluster import fowlkes_mallows_score
916       >>> fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1])
917       1.0
918       >>> fowlkes_mallows_score([0, 0, 1, 1], [1, 1, 0, 0])
919       1.0
920 
921     If classes members are completely split across different clusters,
922     the assignment is totally random, hence the FMI is null::
923 
924       >>> fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3])
925       0.0
926 
927     References
928     ----------
929     .. [1] `E. B. Fowkles and C. L. Mallows, 1983. "A method for comparing two
930        hierarchical clusterings". Journal of the American Statistical
931        Association
932        <http://wildfire.stat.ucla.edu/pdflibrary/fowlkes.pdf>`_
933 
934     .. [2] `Wikipedia entry for the Fowlkes-Mallows Index
935            <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_
936     """
937     labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
938     n_samples, = labels_true.shape
939 
940     c = contingency_matrix(labels_true, labels_pred,
941                            sparse=True)
942     c = c.astype(np.int64, **_astype_copy_false(c))
943     tk = np.dot(c.data, c.data) - n_samples
944     pk = np.sum(np.asarray(c.sum(axis=0)).ravel() ** 2) - n_samples
945     qk = np.sum(np.asarray(c.sum(axis=1)).ravel() ** 2) - n_samples
946     return np.sqrt(tk / pk) * np.sqrt(tk / qk) if tk != 0. else 0.
947 
948 
949 def entropy(labels):
950     """Calculates the entropy for a labeling.
951 
952     Parameters
953     ----------
954     labels : int array, shape = [n_samples]
955         The labels
956 
957     Notes
958     -----
959     The logarithm used is the natural logarithm (base-e).
960     """
961     if len(labels) == 0:
962         return 1.0
963     label_idx = np.unique(labels, return_inverse=True)[1]
964     pi = np.bincount(label_idx).astype(np.float64)
965     pi = pi[pi > 0]
966     pi_sum = np.sum(pi)
967     # log(a / b) should be calculated as log(a) - log(b) for
968     # possible loss of precision
969     return -np.sum((pi / pi_sum) * (np.log(pi) - log(pi_sum)))

</code><intended_behavior>The function should be modified to gracefully handle inputs of type object, especially when these inputs are strings that represent categorical data. This involves either pre-processing the inputs to convert them to a form that can be handled by the mathematical operations (for instance, encoding string labels to integer labels) or updating the utility functions it relies on (such as `check_clusterings` and `contingency_matrix`) to support object type inputs more robustly. The modification should ensure that if inputs are of type object and can be interpreted as categorical data, they are processed in such a way that does not lead to a `ValueError` about being unable to convert string to float. Furthermore, the documentation should be updated to clearly state how object types are handled, and if necessary, warnings should be emitted to inform the user about automatic type conversions or potential issues with the given input types. This update aims to restore the behavior consistent with version 0.21.1, where such inputs did not result in errors.</intended_behavior>]
2025-10-18 17:31:13.365 | INFO     | Search completed. Bug locations: [<file>sklearn/metrics/cluster/_supervised.py</file>
<class>None</class>
<method>None</method>
<code>
1 """Utilities to evaluate the clustering performance of models.
2 
3 Functions named as *_score return a scalar value to maximize: the higher the
4 better.
5 """
6 
7 # Authors: Olivier Grisel <olivier.grisel@ensta.org>
8 #          Wei LI <kuantkid@gmail.com>
9 #          Diego Molla <dmolla-aliod@gmail.com>
10 #          Arnaud Fouchet <foucheta@gmail.com>
11 #          Thierry Guillemot <thierry.guillemot.work@gmail.com>
12 #          Gregory Stupp <stuppie@gmail.com>
13 #          Joel Nothman <joel.nothman@gmail.com>
14 #          Arya McCarthy <arya@jhu.edu>
15 # License: BSD 3 clause
16 
17 
18 from math import log
19 
20 import numpy as np
21 from scipy import sparse as sp
22 
23 from ._expected_mutual_info_fast import expected_mutual_information
24 from ...utils.validation import check_array, check_consistent_length
25 from ...utils.fixes import comb, _astype_copy_false
26 
27 
28 def _comb2(n):
29     # the exact version is faster for k == 2: use it by default globally in
30     # this module instead of the float approximate variant
31     return comb(n, 2, exact=1)
32 
33 
34 def check_clusterings(labels_true, labels_pred):
35     """Check that the labels arrays are 1D and of same dimension.
36 
37     Parameters
38     ----------
39     labels_true : array-like of shape (n_samples,)
40         The true labels.
41 
42     labels_pred : array-like of shape (n_samples,)
43         The predicted labels.
44     """
45     labels_true = check_array(
46         labels_true, ensure_2d=False, ensure_min_samples=0
47     )
48     labels_pred = check_array(
49         labels_pred, ensure_2d=False, ensure_min_samples=0
50     )
51 
52     # input checks
53     if labels_true.ndim != 1:
54         raise ValueError(
55             "labels_true must be 1D: shape is %r" % (labels_true.shape,))
56     if labels_pred.ndim != 1:
57         raise ValueError(
58             "labels_pred must be 1D: shape is %r" % (labels_pred.shape,))
59     check_consistent_length(labels_true, labels_pred)
60 
61     return labels_true, labels_pred
62 
63 
64 def _generalized_average(U, V, average_method):
65     """Return a particular mean of two numbers."""
66     if average_method == "min":
67         return min(U, V)
68     elif average_method == "geometric":
69         return np.sqrt(U * V)
70     elif average_method == "arithmetic":
71         return np.mean([U, V])
72     elif average_method == "max":
73         return max(U, V)
74     else:
75         raise ValueError("'average_method' must be 'min', 'geometric', "
76                          "'arithmetic', or 'max'")
77 
78 
79 def contingency_matrix(labels_true, labels_pred, eps=None, sparse=False):
80     """Build a contingency matrix describing the relationship between labels.
81 
82     Parameters
83     ----------
84     labels_true : int array, shape = [n_samples]
85         Ground truth class labels to be used as a reference
86 
87     labels_pred : array-like of shape (n_samples,)
88         Cluster labels to evaluate
89 
90     eps : None or float, optional.
91         If a float, that value is added to all values in the contingency
92         matrix. This helps to stop NaN propagation.
93         If ``None``, nothing is adjusted.
94 
95     sparse : boolean, optional.
96         If True, return a sparse CSR continency matrix. If ``eps is not None``,
97         and ``sparse is True``, will throw ValueError.
98 
99         .. versionadded:: 0.18
100 
101     Returns
102     -------
103     contingency : {array-like, sparse}, shape=[n_classes_true, n_classes_pred]
104         Matrix :math:`C` such that :math:`C_{i, j}` is the number of samples in
105         true class :math:`i` and in predicted class :math:`j`. If
106         ``eps is None``, the dtype of this array will be integer. If ``eps`` is
107         given, the dtype will be float.
108         Will be a ``scipy.sparse.csr_matrix`` if ``sparse=True``.
109     """
110 
111     if eps is not None and sparse:
112         raise ValueError("Cannot set 'eps' when sparse=True")
113 
114     classes, class_idx = np.unique(labels_true, return_inverse=True)
115     clusters, cluster_idx = np.unique(labels_pred, return_inverse=True)
116     n_classes = classes.shape[0]
117     n_clusters = clusters.shape[0]
118     # Using coo_matrix to accelerate simple histogram calculation,
119     # i.e. bins are consecutive integers
120     # Currently, coo_matrix is faster than histogram2d for simple cases
121     contingency = sp.coo_matrix((np.ones(class_idx.shape[0]),
122                                  (class_idx, cluster_idx)),
123                                 shape=(n_classes, n_clusters),
124                                 dtype=np.int)
125     if sparse:
126         contingency = contingency.tocsr()
127         contingency.sum_duplicates()
128     else:
129         contingency = contingency.toarray()
130         if eps is not None:
131             # don't use += as contingency is integer
132             contingency = contingency + eps
133     return contingency
134 
135 
136 # clustering measures
137 
138 def adjusted_rand_score(labels_true, labels_pred):
139     """Rand index adjusted for chance.
140 
141     The Rand Index computes a similarity measure between two clusterings
142     by considering all pairs of samples and counting pairs that are
143     assigned in the same or different clusters in the predicted and
144     true clusterings.
145 
146     The raw RI score is then "adjusted for chance" into the ARI score
147     using the following scheme::
148 
149         ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)
150 
151     The adjusted Rand index is thus ensured to have a value close to
152     0.0 for random labeling independently of the number of clusters and
153     samples and exactly 1.0 when the clusterings are identical (up to
154     a permutation).
155 
156     ARI is a symmetric measure::
157 
158         adjusted_rand_score(a, b) == adjusted_rand_score(b, a)
159 
160     Read more in the :ref:`User Guide <adjusted_rand_score>`.
161 
162     Parameters
163     ----------
164     labels_true : int array, shape = [n_samples]
165         Ground truth class labels to be used as a reference
166 
167     labels_pred : array-like of shape (n_samples,)
168         Cluster labels to evaluate
169 
170     Returns
171     -------
172     ari : float
173        Similarity score between -1.0 and 1.0. Random labelings have an ARI
174        close to 0.0. 1.0 stands for perfect match.
175 
176     Examples
177     --------
178 
179     Perfectly matching labelings have a score of 1 even
180 
181       >>> from sklearn.metrics.cluster import adjusted_rand_score
182       >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])
183       1.0
184       >>> adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0])
185       1.0
186 
187     Labelings that assign all classes members to the same clusters
188     are complete be not always pure, hence penalized::
189 
190       >>> adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])
191       0.57...
192 
193     ARI is symmetric, so labelings that have pure clusters with members
194     coming from the same classes but unnecessary splits are penalized::
195 
196       >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])
197       0.57...
198 
199     If classes members are completely split across different clusters, the
200     assignment is totally incomplete, hence the ARI is very low::
201 
202       >>> adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])
203       0.0
204 
205     References
206     ----------
207 
208     .. [Hubert1985] L. Hubert and P. Arabie, Comparing Partitions,
209       Journal of Classification 1985
210       https://link.springer.com/article/10.1007%2FBF01908075
211 
212     .. [wk] https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index
213 
214     See also
215     --------
216     adjusted_mutual_info_score: Adjusted Mutual Information
217 
218     """
219     labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
220     n_samples = labels_true.shape[0]
221     n_classes = np.unique(labels_true).shape[0]
222     n_clusters = np.unique(labels_pred).shape[0]
223 
224     # Special limit cases: no clustering since the data is not split;
225     # or trivial clustering where each document is assigned a unique cluster.
226     # These are perfect matches hence return 1.0.
227     if (n_classes == n_clusters == 1 or
228             n_classes == n_clusters == 0 or
229             n_classes == n_clusters == n_samples):
230         return 1.0
231 
232     # Compute the ARI using the contingency data
233     contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
234     sum_comb_c = sum(_comb2(n_c) for n_c in np.ravel(contingency.sum(axis=1)))
235     sum_comb_k = sum(_comb2(n_k) for n_k in np.ravel(contingency.sum(axis=0)))
236     sum_comb = sum(_comb2(n_ij) for n_ij in contingency.data)
237 
238     prod_comb = (sum_comb_c * sum_comb_k) / _comb2(n_samples)
239     mean_comb = (sum_comb_k + sum_comb_c) / 2.
240     return (sum_comb - prod_comb) / (mean_comb - prod_comb)
241 
242 
243 def homogeneity_completeness_v_measure(labels_true, labels_pred, beta=1.0):
244     """Compute the homogeneity and completeness and V-Measure scores at once.
245 
246     Those metrics are based on normalized conditional entropy measures of
247     the clustering labeling to evaluate given the knowledge of a Ground
248     Truth class labels of the same samples.
249 
250     A clustering result satisfies homogeneity if all of its clusters
251     contain only data points which are members of a single class.
252 
253     A clustering result satisfies completeness if all the data points
254     that are members of a given class are elements of the same cluster.
255 
256     Both scores have positive values between 0.0 and 1.0, larger values
257     being desirable.
258 
259     Those 3 metrics are independent of the absolute values of the labels:
260     a permutation of the class or cluster label values won't change the
261     score values in any way.
262 
263     V-Measure is furthermore symmetric: swapping ``labels_true`` and
264     ``label_pred`` will give the same score. This does not hold for
265     homogeneity and completeness. V-Measure is identical to
266     :func:`normalized_mutual_info_score` with the arithmetic averaging
267     method.
268 
269     Read more in the :ref:`User Guide <homogeneity_completeness>`.
270 
271     Parameters
272     ----------
273     labels_true : int array, shape = [n_samples]
274         ground truth class labels to be used as a reference
275 
276     labels_pred : array-like of shape (n_samples,)
277         cluster labels to evaluate
278 
279     beta : float
280         Ratio of weight attributed to ``homogeneity`` vs ``completeness``.
281         If ``beta`` is greater than 1, ``completeness`` is weighted more
282         strongly in the calculation. If ``beta`` is less than 1,
283         ``homogeneity`` is weighted more strongly.
284 
285     Returns
286     -------
287     homogeneity : float
288        score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling
289 
290     completeness : float
291        score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling
292 
293     v_measure : float
294         harmonic mean of the first two
295 
296     See also
297     --------
298     homogeneity_score
299     completeness_score
300     v_measure_score
301     """
302     labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
303 
304     if len(labels_true) == 0:
305         return 1.0, 1.0, 1.0
306 
307     entropy_C = entropy(labels_true)
308     entropy_K = entropy(labels_pred)
309 
310     contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
311     MI = mutual_info_score(None, None, contingency=contingency)
312 
313     homogeneity = MI / (entropy_C) if entropy_C else 1.0
314     completeness = MI / (entropy_K) if entropy_K else 1.0
315 
316     if homogeneity + completeness == 0.0:
317         v_measure_score = 0.0
318     else:
319         v_measure_score = ((1 + beta) * homogeneity * completeness
320                            / (beta * homogeneity + completeness))
321 
322     return homogeneity, completeness, v_measure_score
323 
324 
325 def homogeneity_score(labels_true, labels_pred):
326     """Homogeneity metric of a cluster labeling given a ground truth.
327 
328     A clustering result satisfies homogeneity if all of its clusters
329     contain only data points which are members of a single class.
330 
331     This metric is independent of the absolute values of the labels:
332     a permutation of the class or cluster label values won't change the
333     score value in any way.
334 
335     This metric is not symmetric: switching ``label_true`` with ``label_pred``
336     will return the :func:`completeness_score` which will be different in
337     general.
338 
339     Read more in the :ref:`User Guide <homogeneity_completeness>`.
340 
341     Parameters
342     ----------
343     labels_true : int array, shape = [n_samples]
344         ground truth class labels to be used as a reference
345 
346     labels_pred : array-like of shape (n_samples,)
347         cluster labels to evaluate
348 
349     Returns
350     -------
351     homogeneity : float
352        score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling
353 
354     References
355     ----------
356 
357     .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A
358        conditional entropy-based external cluster evaluation measure
359        <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_
360 
361     See also
362     --------
363     completeness_score
364     v_measure_score
365 
366     Examples
367     --------
368 
369     Perfect labelings are homogeneous::
370 
371       >>> from sklearn.metrics.cluster import homogeneity_score
372       >>> homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])
373       1.0
374 
375     Non-perfect labelings that further split classes into more clusters can be
376     perfectly homogeneous::
377 
378       >>> print("%.6f" % homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))
379       1.000000
380       >>> print("%.6f" % homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))
381       1.000000
382 
383     Clusters that include samples from different classes do not make for an
384     homogeneous labeling::
385 
386       >>> print("%.6f" % homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))
387       0.0...
388       >>> print("%.6f" % homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))
389       0.0...
390 
391     """
392     return homogeneity_completeness_v_measure(labels_true, labels_pred)[0]
393 
394 
395 def completeness_score(labels_true, labels_pred):
396     """Completeness metric of a cluster labeling given a ground truth.
397 
398     A clustering result satisfies completeness if all the data points
399     that are members of a given class are elements of the same cluster.
400 
401     This metric is independent of the absolute values of the labels:
402     a permutation of the class or cluster label values won't change the
403     score value in any way.
404 
405     This metric is not symmetric: switching ``label_true`` with ``label_pred``
406     will return the :func:`homogeneity_score` which will be different in
407     general.
408 
409     Read more in the :ref:`User Guide <homogeneity_completeness>`.
410 
411     Parameters
412     ----------
413     labels_true : int array, shape = [n_samples]
414         ground truth class labels to be used as a reference
415 
416     labels_pred : array-like of shape (n_samples,)
417         cluster labels to evaluate
418 
419     Returns
420     -------
421     completeness : float
422        score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling
423 
424     References
425     ----------
426 
427     .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A
428        conditional entropy-based external cluster evaluation measure
429        <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_
430 
431     See also
432     --------
433     homogeneity_score
434     v_measure_score
435 
436     Examples
437     --------
438 
439     Perfect labelings are complete::
440 
441       >>> from sklearn.metrics.cluster import completeness_score
442       >>> completeness_score([0, 0, 1, 1], [1, 1, 0, 0])
443       1.0
444 
445     Non-perfect labelings that assign all classes members to the same clusters
446     are still complete::
447 
448       >>> print(completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))
449       1.0
450       >>> print(completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))
451       0.999...
452 
453     If classes members are split across different clusters, the
454     assignment cannot be complete::
455 
456       >>> print(completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))
457       0.0
458       >>> print(completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))
459       0.0
460 
461     """
462     return homogeneity_completeness_v_measure(labels_true, labels_pred)[1]
463 
464 
465 def v_measure_score(labels_true, labels_pred, beta=1.0):
466     """V-measure cluster labeling given a ground truth.
467 
468     This score is identical to :func:`normalized_mutual_info_score` with
469     the ``'arithmetic'`` option for averaging.
470 
471     The V-measure is the harmonic mean between homogeneity and completeness::
472 
473         v = (1 + beta) * homogeneity * completeness
474              / (beta * homogeneity + completeness)
475 
476     This metric is independent of the absolute values of the labels:
477     a permutation of the class or cluster label values won't change the
478     score value in any way.
479 
480     This metric is furthermore symmetric: switching ``label_true`` with
481     ``label_pred`` will return the same score value. This can be useful to
482     measure the agreement of two independent label assignments strategies
483     on the same dataset when the real ground truth is not known.
484 
485 
486     Read more in the :ref:`User Guide <homogeneity_completeness>`.
487 
488     Parameters
489     ----------
490     labels_true : int array, shape = [n_samples]
491         ground truth class labels to be used as a reference
492 
493     labels_pred : array-like of shape (n_samples,)
494         cluster labels to evaluate
495 
496     beta : float
497         Ratio of weight attributed to ``homogeneity`` vs ``completeness``.
498         If ``beta`` is greater than 1, ``completeness`` is weighted more
499         strongly in the calculation. If ``beta`` is less than 1,
500         ``homogeneity`` is weighted more strongly.
501 
502     Returns
503     -------
504     v_measure : float
505        score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling
506 
507     References
508     ----------
509 
510     .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A
511        conditional entropy-based external cluster evaluation measure
512        <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_
513 
514     See also
515     --------
516     homogeneity_score
517     completeness_score
518     normalized_mutual_info_score
519 
520     Examples
521     --------
522 
523     Perfect labelings are both homogeneous and complete, hence have score 1.0::
524 
525       >>> from sklearn.metrics.cluster import v_measure_score
526       >>> v_measure_score([0, 0, 1, 1], [0, 0, 1, 1])
527       1.0
528       >>> v_measure_score([0, 0, 1, 1], [1, 1, 0, 0])
529       1.0
530 
531     Labelings that assign all classes members to the same clusters
532     are complete be not homogeneous, hence penalized::
533 
534       >>> print("%.6f" % v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))
535       0.8...
536       >>> print("%.6f" % v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))
537       0.66...
538 
539     Labelings that have pure clusters with members coming from the same
540     classes are homogeneous but un-necessary splits harms completeness
541     and thus penalize V-measure as well::
542 
543       >>> print("%.6f" % v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))
544       0.8...
545       >>> print("%.6f" % v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))
546       0.66...
547 
548     If classes members are completely split across different clusters,
549     the assignment is totally incomplete, hence the V-Measure is null::
550 
551       >>> print("%.6f" % v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))
552       0.0...
553 
554     Clusters that include samples from totally different classes totally
555     destroy the homogeneity of the labeling, hence::
556 
557       >>> print("%.6f" % v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))
558       0.0...
559 
560     """
561     return homogeneity_completeness_v_measure(labels_true, labels_pred,
562                                               beta=beta)[2]
563 
564 
565 def mutual_info_score(labels_true, labels_pred, contingency=None):
566     """Mutual Information between two clusterings.
567 
568     The Mutual Information is a measure of the similarity between two labels of
569     the same data. Where :math:`|U_i|` is the number of the samples
570     in cluster :math:`U_i` and :math:`|V_j|` is the number of the
571     samples in cluster :math:`V_j`, the Mutual Information
572     between clusterings :math:`U` and :math:`V` is given as:
573 
574     .. math::
575 
576         MI(U,V)=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i\\cap V_j|}{N}
577         \\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}
578 
579     This metric is independent of the absolute values of the labels:
580     a permutation of the class or cluster label values won't change the
581     score value in any way.
582 
583     This metric is furthermore symmetric: switching ``label_true`` with
584     ``label_pred`` will return the same score value. This can be useful to
585     measure the agreement of two independent label assignments strategies
586     on the same dataset when the real ground truth is not known.
587 
588     Read more in the :ref:`User Guide <mutual_info_score>`.
589 
590     Parameters
591     ----------
592     labels_true : int array, shape = [n_samples]
593         A clustering of the data into disjoint subsets.
594 
595     labels_pred : int array-like of shape (n_samples,)
596         A clustering of the data into disjoint subsets.
597 
598     contingency : {None, array, sparse matrix}, \
599                   shape = [n_classes_true, n_classes_pred]
600         A contingency matrix given by the :func:`contingency_matrix` function.
601         If value is ``None``, it will be computed, otherwise the given value is
602         used, with ``labels_true`` and ``labels_pred`` ignored.
603 
604     Returns
605     -------
606     mi : float
607        Mutual information, a non-negative value
608 
609     Notes
610     -----
611     The logarithm used is the natural logarithm (base-e).
612 
613     See also
614     --------
615     adjusted_mutual_info_score: Adjusted against chance Mutual Information
616     normalized_mutual_info_score: Normalized Mutual Information
617     """
618     if contingency is None:
619         labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
620         contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
621     else:
622         contingency = check_array(contingency,
623                                   accept_sparse=['csr', 'csc', 'coo'],
624                                   dtype=[int, np.int32, np.int64])
625 
626     if isinstance(contingency, np.ndarray):
627         # For an array
628         nzx, nzy = np.nonzero(contingency)
629         nz_val = contingency[nzx, nzy]
630     elif sp.issparse(contingency):
631         # For a sparse matrix
632         nzx, nzy, nz_val = sp.find(contingency)
633     else:
634         raise ValueError("Unsupported type for 'contingency': %s" %
635                          type(contingency))
636 
637     contingency_sum = contingency.sum()
638     pi = np.ravel(contingency.sum(axis=1))
639     pj = np.ravel(contingency.sum(axis=0))
640     log_contingency_nm = np.log(nz_val)
641     contingency_nm = nz_val / contingency_sum
642     # Don't need to calculate the full outer product, just for non-zeroes
643     outer = (pi.take(nzx).astype(np.int64, copy=False)
644              * pj.take(nzy).astype(np.int64, copy=False))
645     log_outer = -np.log(outer) + log(pi.sum()) + log(pj.sum())
646     mi = (contingency_nm * (log_contingency_nm - log(contingency_sum)) +
647           contingency_nm * log_outer)
648     return mi.sum()
649 
650 
651 def adjusted_mutual_info_score(labels_true, labels_pred,
652                                average_method='arithmetic'):
653     """Adjusted Mutual Information between two clusterings.
654 
655     Adjusted Mutual Information (AMI) is an adjustment of the Mutual
656     Information (MI) score to account for chance. It accounts for the fact that
657     the MI is generally higher for two clusterings with a larger number of
658     clusters, regardless of whether there is actually more information shared.
659     For two clusterings :math:`U` and :math:`V`, the AMI is given as::
660 
661         AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [avg(H(U), H(V)) - E(MI(U, V))]
662 
663     This metric is independent of the absolute values of the labels:
664     a permutation of the class or cluster label values won't change the
665     score value in any way.
666 
667     This metric is furthermore symmetric: switching ``label_true`` with
668     ``label_pred`` will return the same score value. This can be useful to
669     measure the agreement of two independent label assignments strategies
670     on the same dataset when the real ground truth is not known.
671 
672     Be mindful that this function is an order of magnitude slower than other
673     metrics, such as the Adjusted Rand Index.
674 
675     Read more in the :ref:`User Guide <mutual_info_score>`.
676 
677     Parameters
678     ----------
679     labels_true : int array, shape = [n_samples]
680         A clustering of the data into disjoint subsets.
681 
682     labels_pred : int array-like of shape (n_samples,)
683         A clustering of the data into disjoint subsets.
684 
685     average_method : string, optional (default: 'arithmetic')
686         How to compute the normalizer in the denominator. Possible options
687         are 'min', 'geometric', 'arithmetic', and 'max'.
688 
689         .. versionadded:: 0.20
690 
691         .. versionchanged:: 0.22
692            The default value of ``average_method`` changed from 'max' to
693            'arithmetic'.
694 
695     Returns
696     -------
697     ami: float (upperlimited by 1.0)
698        The AMI returns a value of 1 when the two partitions are identical
699        (ie perfectly matched). Random partitions (independent labellings) have
700        an expected AMI around 0 on average hence can be negative.
701 
702     See also
703     --------
704     adjusted_rand_score: Adjusted Rand Index
705     mutual_info_score: Mutual Information (not adjusted for chance)
706 
707     Examples
708     --------
709 
710     Perfect labelings are both homogeneous and complete, hence have
711     score 1.0::
712 
713       >>> from sklearn.metrics.cluster import adjusted_mutual_info_score
714       >>> adjusted_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])
715       ... # doctest: +SKIP
716       1.0
717       >>> adjusted_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])
718       ... # doctest: +SKIP
719       1.0
720 
721     If classes members are completely split across different clusters,
722     the assignment is totally in-complete, hence the AMI is null::
723 
724       >>> adjusted_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])
725       ... # doctest: +SKIP
726       0.0
727 
728     References
729     ----------
730     .. [1] `Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for
731        Clusterings Comparison: Variants, Properties, Normalization and
732        Correction for Chance, JMLR
733        <http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>`_
734 
735     .. [2] `Wikipedia entry for the Adjusted Mutual Information
736        <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_
737 
738     """
739     labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
740     n_samples = labels_true.shape[0]
741     classes = np.unique(labels_true)
742     clusters = np.unique(labels_pred)
743     # Special limit cases: no clustering since the data is not split.
744     # This is a perfect match hence return 1.0.
745     if (classes.shape[0] == clusters.shape[0] == 1 or
746             classes.shape[0] == clusters.shape[0] == 0):
747         return 1.0
748     contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
749     contingency = contingency.astype(np.float64,
750                                      **_astype_copy_false(contingency))
751     # Calculate the MI for the two clusterings
752     mi = mutual_info_score(labels_true, labels_pred,
753                            contingency=contingency)
754     # Calculate the expected value for the mutual information
755     emi = expected_mutual_information(contingency, n_samples)
756     # Calculate entropy for each labeling
757     h_true, h_pred = entropy(labels_true), entropy(labels_pred)
758     normalizer = _generalized_average(h_true, h_pred, average_method)
759     denominator = normalizer - emi
760     # Avoid 0.0 / 0.0 when expectation equals maximum, i.e a perfect match.
761     # normalizer should always be >= emi, but because of floating-point
762     # representation, sometimes emi is slightly larger. Correct this
763     # by preserving the sign.
764     if denominator < 0:
765         denominator = min(denominator, -np.finfo('float64').eps)
766     else:
767         denominator = max(denominator, np.finfo('float64').eps)
768     ami = (mi - emi) / denominator
769     return ami
770 
771 
772 def normalized_mutual_info_score(labels_true, labels_pred,
773                                  average_method='arithmetic'):
774     """Normalized Mutual Information between two clusterings.
775 
776     Normalized Mutual Information (NMI) is a normalization of the Mutual
777     Information (MI) score to scale the results between 0 (no mutual
778     information) and 1 (perfect correlation). In this function, mutual
779     information is normalized by some generalized mean of ``H(labels_true)``
780     and ``H(labels_pred))``, defined by the `average_method`.
781 
782     This measure is not adjusted for chance. Therefore
783     :func:`adjusted_mutual_info_score` might be preferred.
784 
785     This metric is independent of the absolute values of the labels:
786     a permutation of the class or cluster label values won't change the
787     score value in any way.
788 
789     This metric is furthermore symmetric: switching ``label_true`` with
790     ``label_pred`` will return the same score value. This can be useful to
791     measure the agreement of two independent label assignments strategies
792     on the same dataset when the real ground truth is not known.
793 
794     Read more in the :ref:`User Guide <mutual_info_score>`.
795 
796     Parameters
797     ----------
798     labels_true : int array, shape = [n_samples]
799         A clustering of the data into disjoint subsets.
800 
801     labels_pred : int array-like of shape (n_samples,)
802         A clustering of the data into disjoint subsets.
803 
804     average_method : string, optional (default: 'arithmetic')
805         How to compute the normalizer in the denominator. Possible options
806         are 'min', 'geometric', 'arithmetic', and 'max'.
807 
808         .. versionadded:: 0.20
809 
810         .. versionchanged:: 0.22
811            The default value of ``average_method`` changed from 'geometric' to
812            'arithmetic'.
813 
814     Returns
815     -------
816     nmi : float
817        score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling
818 
819     See also
820     --------
821     v_measure_score: V-Measure (NMI with arithmetic mean option.)
822     adjusted_rand_score: Adjusted Rand Index
823     adjusted_mutual_info_score: Adjusted Mutual Information (adjusted
824         against chance)
825 
826     Examples
827     --------
828 
829     Perfect labelings are both homogeneous and complete, hence have
830     score 1.0::
831 
832       >>> from sklearn.metrics.cluster import normalized_mutual_info_score
833       >>> normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])
834       ... # doctest: +SKIP
835       1.0
836       >>> normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])
837       ... # doctest: +SKIP
838       1.0
839 
840     If classes members are completely split across different clusters,
841     the assignment is totally in-complete, hence the NMI is null::
842 
843       >>> normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])
844       ... # doctest: +SKIP
845       0.0
846 
847     """
848     labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
849     classes = np.unique(labels_true)
850     clusters = np.unique(labels_pred)
851     # Special limit cases: no clustering since the data is not split.
852     # This is a perfect match hence return 1.0.
853     if (classes.shape[0] == clusters.shape[0] == 1 or
854             classes.shape[0] == clusters.shape[0] == 0):
855         return 1.0
856     contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
857     contingency = contingency.astype(np.float64,
858                                      **_astype_copy_false(contingency))
859     # Calculate the MI for the two clusterings
860     mi = mutual_info_score(labels_true, labels_pred,
861                            contingency=contingency)
862     # Calculate the expected value for the mutual information
863     # Calculate entropy for each labeling
864     h_true, h_pred = entropy(labels_true), entropy(labels_pred)
865     normalizer = _generalized_average(h_true, h_pred, average_method)
866     # Avoid 0.0 / 0.0 when either entropy is zero.
867     normalizer = max(normalizer, np.finfo('float64').eps)
868     nmi = mi / normalizer
869     return nmi
870 
871 
872 def fowlkes_mallows_score(labels_true, labels_pred, sparse=False):
873     """Measure the similarity of two clusterings of a set of points.
874 
875     The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of
876     the precision and recall::
877 
878         FMI = TP / sqrt((TP + FP) * (TP + FN))
879 
880     Where ``TP`` is the number of **True Positive** (i.e. the number of pair of
881     points that belongs in the same clusters in both ``labels_true`` and
882     ``labels_pred``), ``FP`` is the number of **False Positive** (i.e. the
883     number of pair of points that belongs in the same clusters in
884     ``labels_true`` and not in ``labels_pred``) and ``FN`` is the number of
885     **False Negative** (i.e the number of pair of points that belongs in the
886     same clusters in ``labels_pred`` and not in ``labels_True``).
887 
888     The score ranges from 0 to 1. A high value indicates a good similarity
889     between two clusters.
890 
891     Read more in the :ref:`User Guide <fowlkes_mallows_scores>`.
892 
893     Parameters
894     ----------
895     labels_true : int array, shape = (``n_samples``,)
896         A clustering of the data into disjoint subsets.
897 
898     labels_pred : array, shape = (``n_samples``, )
899         A clustering of the data into disjoint subsets.
900 
901     sparse : bool
902         Compute contingency matrix internally with sparse matrix.
903 
904     Returns
905     -------
906     score : float
907        The resulting Fowlkes-Mallows score.
908 
909     Examples
910     --------
911 
912     Perfect labelings are both homogeneous and complete, hence have
913     score 1.0::
914 
915       >>> from sklearn.metrics.cluster import fowlkes_mallows_score
916       >>> fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1])
917       1.0
918       >>> fowlkes_mallows_score([0, 0, 1, 1], [1, 1, 0, 0])
919       1.0
920 
921     If classes members are completely split across different clusters,
922     the assignment is totally random, hence the FMI is null::
923 
924       >>> fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3])
925       0.0
926 
927     References
928     ----------
929     .. [1] `E. B. Fowkles and C. L. Mallows, 1983. "A method for comparing two
930        hierarchical clusterings". Journal of the American Statistical
931        Association
932        <http://wildfire.stat.ucla.edu/pdflibrary/fowlkes.pdf>`_
933 
934     .. [2] `Wikipedia entry for the Fowlkes-Mallows Index
935            <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_
936     """
937     labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
938     n_samples, = labels_true.shape
939 
940     c = contingency_matrix(labels_true, labels_pred,
941                            sparse=True)
942     c = c.astype(np.int64, **_astype_copy_false(c))
943     tk = np.dot(c.data, c.data) - n_samples
944     pk = np.sum(np.asarray(c.sum(axis=0)).ravel() ** 2) - n_samples
945     qk = np.sum(np.asarray(c.sum(axis=1)).ravel() ** 2) - n_samples
946     return np.sqrt(tk / pk) * np.sqrt(tk / qk) if tk != 0. else 0.
947 
948 
949 def entropy(labels):
950     """Calculates the entropy for a labeling.
951 
952     Parameters
953     ----------
954     labels : int array, shape = [n_samples]
955         The labels
956 
957     Notes
958     -----
959     The logarithm used is the natural logarithm (base-e).
960     """
961     if len(labels) == 0:
962         return 1.0
963     label_idx = np.unique(labels, return_inverse=True)[1]
964     pi = np.bincount(label_idx).astype(np.float64)
965     pi = pi[pi > 0]
966     pi_sum = np.sum(pi)
967     # log(a / b) should be calculated as log(a) - log(b) for
968     # possible loss of precision
969     return -np.sum((pi / pi_sum) * (np.log(pi) - log(pi_sum)))

</code><intended_behavior>The function should be modified to gracefully handle inputs of type object, especially when these inputs are strings that represent categorical data. This involves either pre-processing the inputs to convert them to a form that can be handled by the mathematical operations (for instance, encoding string labels to integer labels) or updating the utility functions it relies on (such as `check_clusterings` and `contingency_matrix`) to support object type inputs more robustly. The modification should ensure that if inputs are of type object and can be interpreted as categorical data, they are processed in such a way that does not lead to a `ValueError` about being unable to convert string to float. Furthermore, the documentation should be updated to clearly state how object types are handled, and if necessary, warnings should be emitted to inform the user about automatic type conversions or potential issues with the given input types. This update aims to restore the behavior consistent with version 0.21.1, where such inputs did not result in errors.</intended_behavior>]
2025-10-18 17:31:13.365 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 17:31:13.365 | INFO     | Search-only: skipping patch selection
2025-10-18 17:31:13.446 | INFO     | Task scikit-learn__scikit-learn-15535 completed successfully.
2025-10-18 17:31:13.446 | INFO     | No patch generated. You can try running ACR again.
