[
    {
        "rel_file_path": "sklearn/metrics/cluster/_supervised.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/metrics/cluster/_supervised.py",
        "start": 1,
        "end": 969,
        "class_name": null,
        "method_name": null,
        "code": "1 \"\"\"Utilities to evaluate the clustering performance of models.\n2 \n3 Functions named as *_score return a scalar value to maximize: the higher the\n4 better.\n5 \"\"\"\n6 \n7 # Authors: Olivier Grisel <olivier.grisel@ensta.org>\n8 #          Wei LI <kuantkid@gmail.com>\n9 #          Diego Molla <dmolla-aliod@gmail.com>\n10 #          Arnaud Fouchet <foucheta@gmail.com>\n11 #          Thierry Guillemot <thierry.guillemot.work@gmail.com>\n12 #          Gregory Stupp <stuppie@gmail.com>\n13 #          Joel Nothman <joel.nothman@gmail.com>\n14 #          Arya McCarthy <arya@jhu.edu>\n15 # License: BSD 3 clause\n16 \n17 \n18 from math import log\n19 \n20 import numpy as np\n21 from scipy import sparse as sp\n22 \n23 from ._expected_mutual_info_fast import expected_mutual_information\n24 from ...utils.validation import check_array, check_consistent_length\n25 from ...utils.fixes import comb, _astype_copy_false\n26 \n27 \n28 def _comb2(n):\n29     # the exact version is faster for k == 2: use it by default globally in\n30     # this module instead of the float approximate variant\n31     return comb(n, 2, exact=1)\n32 \n33 \n34 def check_clusterings(labels_true, labels_pred):\n35     \"\"\"Check that the labels arrays are 1D and of same dimension.\n36 \n37     Parameters\n38     ----------\n39     labels_true : array-like of shape (n_samples,)\n40         The true labels.\n41 \n42     labels_pred : array-like of shape (n_samples,)\n43         The predicted labels.\n44     \"\"\"\n45     labels_true = check_array(\n46         labels_true, ensure_2d=False, ensure_min_samples=0\n47     )\n48     labels_pred = check_array(\n49         labels_pred, ensure_2d=False, ensure_min_samples=0\n50     )\n51 \n52     # input checks\n53     if labels_true.ndim != 1:\n54         raise ValueError(\n55             \"labels_true must be 1D: shape is %r\" % (labels_true.shape,))\n56     if labels_pred.ndim != 1:\n57         raise ValueError(\n58             \"labels_pred must be 1D: shape is %r\" % (labels_pred.shape,))\n59     check_consistent_length(labels_true, labels_pred)\n60 \n61     return labels_true, labels_pred\n62 \n63 \n64 def _generalized_average(U, V, average_method):\n65     \"\"\"Return a particular mean of two numbers.\"\"\"\n66     if average_method == \"min\":\n67         return min(U, V)\n68     elif average_method == \"geometric\":\n69         return np.sqrt(U * V)\n70     elif average_method == \"arithmetic\":\n71         return np.mean([U, V])\n72     elif average_method == \"max\":\n73         return max(U, V)\n74     else:\n75         raise ValueError(\"'average_method' must be 'min', 'geometric', \"\n76                          \"'arithmetic', or 'max'\")\n77 \n78 \n79 def contingency_matrix(labels_true, labels_pred, eps=None, sparse=False):\n80     \"\"\"Build a contingency matrix describing the relationship between labels.\n81 \n82     Parameters\n83     ----------\n84     labels_true : int array, shape = [n_samples]\n85         Ground truth class labels to be used as a reference\n86 \n87     labels_pred : array-like of shape (n_samples,)\n88         Cluster labels to evaluate\n89 \n90     eps : None or float, optional.\n91         If a float, that value is added to all values in the contingency\n92         matrix. This helps to stop NaN propagation.\n93         If ``None``, nothing is adjusted.\n94 \n95     sparse : boolean, optional.\n96         If True, return a sparse CSR continency matrix. If ``eps is not None``,\n97         and ``sparse is True``, will throw ValueError.\n98 \n99         .. versionadded:: 0.18\n100 \n101     Returns\n102     -------\n103     contingency : {array-like, sparse}, shape=[n_classes_true, n_classes_pred]\n104         Matrix :math:`C` such that :math:`C_{i, j}` is the number of samples in\n105         true class :math:`i` and in predicted class :math:`j`. If\n106         ``eps is None``, the dtype of this array will be integer. If ``eps`` is\n107         given, the dtype will be float.\n108         Will be a ``scipy.sparse.csr_matrix`` if ``sparse=True``.\n109     \"\"\"\n110 \n111     if eps is not None and sparse:\n112         raise ValueError(\"Cannot set 'eps' when sparse=True\")\n113 \n114     classes, class_idx = np.unique(labels_true, return_inverse=True)\n115     clusters, cluster_idx = np.unique(labels_pred, return_inverse=True)\n116     n_classes = classes.shape[0]\n117     n_clusters = clusters.shape[0]\n118     # Using coo_matrix to accelerate simple histogram calculation,\n119     # i.e. bins are consecutive integers\n120     # Currently, coo_matrix is faster than histogram2d for simple cases\n121     contingency = sp.coo_matrix((np.ones(class_idx.shape[0]),\n122                                  (class_idx, cluster_idx)),\n123                                 shape=(n_classes, n_clusters),\n124                                 dtype=np.int)\n125     if sparse:\n126         contingency = contingency.tocsr()\n127         contingency.sum_duplicates()\n128     else:\n129         contingency = contingency.toarray()\n130         if eps is not None:\n131             # don't use += as contingency is integer\n132             contingency = contingency + eps\n133     return contingency\n134 \n135 \n136 # clustering measures\n137 \n138 def adjusted_rand_score(labels_true, labels_pred):\n139     \"\"\"Rand index adjusted for chance.\n140 \n141     The Rand Index computes a similarity measure between two clusterings\n142     by considering all pairs of samples and counting pairs that are\n143     assigned in the same or different clusters in the predicted and\n144     true clusterings.\n145 \n146     The raw RI score is then \"adjusted for chance\" into the ARI score\n147     using the following scheme::\n148 \n149         ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n150 \n151     The adjusted Rand index is thus ensured to have a value close to\n152     0.0 for random labeling independently of the number of clusters and\n153     samples and exactly 1.0 when the clusterings are identical (up to\n154     a permutation).\n155 \n156     ARI is a symmetric measure::\n157 \n158         adjusted_rand_score(a, b) == adjusted_rand_score(b, a)\n159 \n160     Read more in the :ref:`User Guide <adjusted_rand_score>`.\n161 \n162     Parameters\n163     ----------\n164     labels_true : int array, shape = [n_samples]\n165         Ground truth class labels to be used as a reference\n166 \n167     labels_pred : array-like of shape (n_samples,)\n168         Cluster labels to evaluate\n169 \n170     Returns\n171     -------\n172     ari : float\n173        Similarity score between -1.0 and 1.0. Random labelings have an ARI\n174        close to 0.0. 1.0 stands for perfect match.\n175 \n176     Examples\n177     --------\n178 \n179     Perfectly matching labelings have a score of 1 even\n180 \n181       >>> from sklearn.metrics.cluster import adjusted_rand_score\n182       >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])\n183       1.0\n184       >>> adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n185       1.0\n186 \n187     Labelings that assign all classes members to the same clusters\n188     are complete be not always pure, hence penalized::\n189 \n190       >>> adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])\n191       0.57...\n192 \n193     ARI is symmetric, so labelings that have pure clusters with members\n194     coming from the same classes but unnecessary splits are penalized::\n195 \n196       >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])\n197       0.57...\n198 \n199     If classes members are completely split across different clusters, the\n200     assignment is totally incomplete, hence the ARI is very low::\n201 \n202       >>> adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])\n203       0.0\n204 \n205     References\n206     ----------\n207 \n208     .. [Hubert1985] L. Hubert and P. Arabie, Comparing Partitions,\n209       Journal of Classification 1985\n210       https://link.springer.com/article/10.1007%2FBF01908075\n211 \n212     .. [wk] https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index\n213 \n214     See also\n215     --------\n216     adjusted_mutual_info_score: Adjusted Mutual Information\n217 \n218     \"\"\"\n219     labels_true, labels_pred = check_clusterings(labels_true, labels_pred)\n220     n_samples = labels_true.shape[0]\n221     n_classes = np.unique(labels_true).shape[0]\n222     n_clusters = np.unique(labels_pred).shape[0]\n223 \n224     # Special limit cases: no clustering since the data is not split;\n225     # or trivial clustering where each document is assigned a unique cluster.\n226     # These are perfect matches hence return 1.0.\n227     if (n_classes == n_clusters == 1 or\n228             n_classes == n_clusters == 0 or\n229             n_classes == n_clusters == n_samples):\n230         return 1.0\n231 \n232     # Compute the ARI using the contingency data\n233     contingency = contingency_matrix(labels_true, labels_pred, sparse=True)\n234     sum_comb_c = sum(_comb2(n_c) for n_c in np.ravel(contingency.sum(axis=1)))\n235     sum_comb_k = sum(_comb2(n_k) for n_k in np.ravel(contingency.sum(axis=0)))\n236     sum_comb = sum(_comb2(n_ij) for n_ij in contingency.data)\n237 \n238     prod_comb = (sum_comb_c * sum_comb_k) / _comb2(n_samples)\n239     mean_comb = (sum_comb_k + sum_comb_c) / 2.\n240     return (sum_comb - prod_comb) / (mean_comb - prod_comb)\n241 \n242 \n243 def homogeneity_completeness_v_measure(labels_true, labels_pred, beta=1.0):\n244     \"\"\"Compute the homogeneity and completeness and V-Measure scores at once.\n245 \n246     Those metrics are based on normalized conditional entropy measures of\n247     the clustering labeling to evaluate given the knowledge of a Ground\n248     Truth class labels of the same samples.\n249 \n250     A clustering result satisfies homogeneity if all of its clusters\n251     contain only data points which are members of a single class.\n252 \n253     A clustering result satisfies completeness if all the data points\n254     that are members of a given class are elements of the same cluster.\n255 \n256     Both scores have positive values between 0.0 and 1.0, larger values\n257     being desirable.\n258 \n259     Those 3 metrics are independent of the absolute values of the labels:\n260     a permutation of the class or cluster label values won't change the\n261     score values in any way.\n262 \n263     V-Measure is furthermore symmetric: swapping ``labels_true`` and\n264     ``label_pred`` will give the same score. This does not hold for\n265     homogeneity and completeness. V-Measure is identical to\n266     :func:`normalized_mutual_info_score` with the arithmetic averaging\n267     method.\n268 \n269     Read more in the :ref:`User Guide <homogeneity_completeness>`.\n270 \n271     Parameters\n272     ----------\n273     labels_true : int array, shape = [n_samples]\n274         ground truth class labels to be used as a reference\n275 \n276     labels_pred : array-like of shape (n_samples,)\n277         cluster labels to evaluate\n278 \n279     beta : float\n280         Ratio of weight attributed to ``homogeneity`` vs ``completeness``.\n281         If ``beta`` is greater than 1, ``completeness`` is weighted more\n282         strongly in the calculation. If ``beta`` is less than 1,\n283         ``homogeneity`` is weighted more strongly.\n284 \n285     Returns\n286     -------\n287     homogeneity : float\n288        score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling\n289 \n290     completeness : float\n291        score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n292 \n293     v_measure : float\n294         harmonic mean of the first two\n295 \n296     See also\n297     --------\n298     homogeneity_score\n299     completeness_score\n300     v_measure_score\n301     \"\"\"\n302     labels_true, labels_pred = check_clusterings(labels_true, labels_pred)\n303 \n304     if len(labels_true) == 0:\n305         return 1.0, 1.0, 1.0\n306 \n307     entropy_C = entropy(labels_true)\n308     entropy_K = entropy(labels_pred)\n309 \n310     contingency = contingency_matrix(labels_true, labels_pred, sparse=True)\n311     MI = mutual_info_score(None, None, contingency=contingency)\n312 \n313     homogeneity = MI / (entropy_C) if entropy_C else 1.0\n314     completeness = MI / (entropy_K) if entropy_K else 1.0\n315 \n316     if homogeneity + completeness == 0.0:\n317         v_measure_score = 0.0\n318     else:\n319         v_measure_score = ((1 + beta) * homogeneity * completeness\n320                            / (beta * homogeneity + completeness))\n321 \n322     return homogeneity, completeness, v_measure_score\n323 \n324 \n325 def homogeneity_score(labels_true, labels_pred):\n326     \"\"\"Homogeneity metric of a cluster labeling given a ground truth.\n327 \n328     A clustering result satisfies homogeneity if all of its clusters\n329     contain only data points which are members of a single class.\n330 \n331     This metric is independent of the absolute values of the labels:\n332     a permutation of the class or cluster label values won't change the\n333     score value in any way.\n334 \n335     This metric is not symmetric: switching ``label_true`` with ``label_pred``\n336     will return the :func:`completeness_score` which will be different in\n337     general.\n338 \n339     Read more in the :ref:`User Guide <homogeneity_completeness>`.\n340 \n341     Parameters\n342     ----------\n343     labels_true : int array, shape = [n_samples]\n344         ground truth class labels to be used as a reference\n345 \n346     labels_pred : array-like of shape (n_samples,)\n347         cluster labels to evaluate\n348 \n349     Returns\n350     -------\n351     homogeneity : float\n352        score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling\n353 \n354     References\n355     ----------\n356 \n357     .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n358        conditional entropy-based external cluster evaluation measure\n359        <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n360 \n361     See also\n362     --------\n363     completeness_score\n364     v_measure_score\n365 \n366     Examples\n367     --------\n368 \n369     Perfect labelings are homogeneous::\n370 \n371       >>> from sklearn.metrics.cluster import homogeneity_score\n372       >>> homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])\n373       1.0\n374 \n375     Non-perfect labelings that further split classes into more clusters can be\n376     perfectly homogeneous::\n377 \n378       >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))\n379       1.000000\n380       >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))\n381       1.000000\n382 \n383     Clusters that include samples from different classes do not make for an\n384     homogeneous labeling::\n385 \n386       >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))\n387       0.0...\n388       >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))\n389       0.0...\n390 \n391     \"\"\"\n392     return homogeneity_completeness_v_measure(labels_true, labels_pred)[0]\n393 \n394 \n395 def completeness_score(labels_true, labels_pred):\n396     \"\"\"Completeness metric of a cluster labeling given a ground truth.\n397 \n398     A clustering result satisfies completeness if all the data points\n399     that are members of a given class are elements of the same cluster.\n400 \n401     This metric is independent of the absolute values of the labels:\n402     a permutation of the class or cluster label values won't change the\n403     score value in any way.\n404 \n405     This metric is not symmetric: switching ``label_true`` with ``label_pred``\n406     will return the :func:`homogeneity_score` which will be different in\n407     general.\n408 \n409     Read more in the :ref:`User Guide <homogeneity_completeness>`.\n410 \n411     Parameters\n412     ----------\n413     labels_true : int array, shape = [n_samples]\n414         ground truth class labels to be used as a reference\n415 \n416     labels_pred : array-like of shape (n_samples,)\n417         cluster labels to evaluate\n418 \n419     Returns\n420     -------\n421     completeness : float\n422        score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n423 \n424     References\n425     ----------\n426 \n427     .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n428        conditional entropy-based external cluster evaluation measure\n429        <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n430 \n431     See also\n432     --------\n433     homogeneity_score\n434     v_measure_score\n435 \n436     Examples\n437     --------\n438 \n439     Perfect labelings are complete::\n440 \n441       >>> from sklearn.metrics.cluster import completeness_score\n442       >>> completeness_score([0, 0, 1, 1], [1, 1, 0, 0])\n443       1.0\n444 \n445     Non-perfect labelings that assign all classes members to the same clusters\n446     are still complete::\n447 \n448       >>> print(completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))\n449       1.0\n450       >>> print(completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))\n451       0.999...\n452 \n453     If classes members are split across different clusters, the\n454     assignment cannot be complete::\n455 \n456       >>> print(completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))\n457       0.0\n458       >>> print(completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))\n459       0.0\n460 \n461     \"\"\"\n462     return homogeneity_completeness_v_measure(labels_true, labels_pred)[1]\n463 \n464 \n465 def v_measure_score(labels_true, labels_pred, beta=1.0):\n466     \"\"\"V-measure cluster labeling given a ground truth.\n467 \n468     This score is identical to :func:`normalized_mutual_info_score` with\n469     the ``'arithmetic'`` option for averaging.\n470 \n471     The V-measure is the harmonic mean between homogeneity and completeness::\n472 \n473         v = (1 + beta) * homogeneity * completeness\n474              / (beta * homogeneity + completeness)\n475 \n476     This metric is independent of the absolute values of the labels:\n477     a permutation of the class or cluster label values won't change the\n478     score value in any way.\n479 \n480     This metric is furthermore symmetric: switching ``label_true`` with\n481     ``label_pred`` will return the same score value. This can be useful to\n482     measure the agreement of two independent label assignments strategies\n483     on the same dataset when the real ground truth is not known.\n484 \n485 \n486     Read more in the :ref:`User Guide <homogeneity_completeness>`.\n487 \n488     Parameters\n489     ----------\n490     labels_true : int array, shape = [n_samples]\n491         ground truth class labels to be used as a reference\n492 \n493     labels_pred : array-like of shape (n_samples,)\n494         cluster labels to evaluate\n495 \n496     beta : float\n497         Ratio of weight attributed to ``homogeneity`` vs ``completeness``.\n498         If ``beta`` is greater than 1, ``completeness`` is weighted more\n499         strongly in the calculation. If ``beta`` is less than 1,\n500         ``homogeneity`` is weighted more strongly.\n501 \n502     Returns\n503     -------\n504     v_measure : float\n505        score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n506 \n507     References\n508     ----------\n509 \n510     .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n511        conditional entropy-based external cluster evaluation measure\n512        <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n513 \n514     See also\n515     --------\n516     homogeneity_score\n517     completeness_score\n518     normalized_mutual_info_score\n519 \n520     Examples\n521     --------\n522 \n523     Perfect labelings are both homogeneous and complete, hence have score 1.0::\n524 \n525       >>> from sklearn.metrics.cluster import v_measure_score\n526       >>> v_measure_score([0, 0, 1, 1], [0, 0, 1, 1])\n527       1.0\n528       >>> v_measure_score([0, 0, 1, 1], [1, 1, 0, 0])\n529       1.0\n530 \n531     Labelings that assign all classes members to the same clusters\n532     are complete be not homogeneous, hence penalized::\n533 \n534       >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))\n535       0.8...\n536       >>> print(\"%.6f\" % v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))\n537       0.66...\n538 \n539     Labelings that have pure clusters with members coming from the same\n540     classes are homogeneous but un-necessary splits harms completeness\n541     and thus penalize V-measure as well::\n542 \n543       >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))\n544       0.8...\n545       >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))\n546       0.66...\n547 \n548     If classes members are completely split across different clusters,\n549     the assignment is totally incomplete, hence the V-Measure is null::\n550 \n551       >>> print(\"%.6f\" % v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))\n552       0.0...\n553 \n554     Clusters that include samples from totally different classes totally\n555     destroy the homogeneity of the labeling, hence::\n556 \n557       >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))\n558       0.0...\n559 \n560     \"\"\"\n561     return homogeneity_completeness_v_measure(labels_true, labels_pred,\n562                                               beta=beta)[2]\n563 \n564 \n565 def mutual_info_score(labels_true, labels_pred, contingency=None):\n566     \"\"\"Mutual Information between two clusterings.\n567 \n568     The Mutual Information is a measure of the similarity between two labels of\n569     the same data. Where :math:`|U_i|` is the number of the samples\n570     in cluster :math:`U_i` and :math:`|V_j|` is the number of the\n571     samples in cluster :math:`V_j`, the Mutual Information\n572     between clusterings :math:`U` and :math:`V` is given as:\n573 \n574     .. math::\n575 \n576         MI(U,V)=\\\\sum_{i=1}^{|U|} \\\\sum_{j=1}^{|V|} \\\\frac{|U_i\\\\cap V_j|}{N}\n577         \\\\log\\\\frac{N|U_i \\\\cap V_j|}{|U_i||V_j|}\n578 \n579     This metric is independent of the absolute values of the labels:\n580     a permutation of the class or cluster label values won't change the\n581     score value in any way.\n582 \n583     This metric is furthermore symmetric: switching ``label_true`` with\n584     ``label_pred`` will return the same score value. This can be useful to\n585     measure the agreement of two independent label assignments strategies\n586     on the same dataset when the real ground truth is not known.\n587 \n588     Read more in the :ref:`User Guide <mutual_info_score>`.\n589 \n590     Parameters\n591     ----------\n592     labels_true : int array, shape = [n_samples]\n593         A clustering of the data into disjoint subsets.\n594 \n595     labels_pred : int array-like of shape (n_samples,)\n596         A clustering of the data into disjoint subsets.\n597 \n598     contingency : {None, array, sparse matrix}, \\\n599                   shape = [n_classes_true, n_classes_pred]\n600         A contingency matrix given by the :func:`contingency_matrix` function.\n601         If value is ``None``, it will be computed, otherwise the given value is\n602         used, with ``labels_true`` and ``labels_pred`` ignored.\n603 \n604     Returns\n605     -------\n606     mi : float\n607        Mutual information, a non-negative value\n608 \n609     Notes\n610     -----\n611     The logarithm used is the natural logarithm (base-e).\n612 \n613     See also\n614     --------\n615     adjusted_mutual_info_score: Adjusted against chance Mutual Information\n616     normalized_mutual_info_score: Normalized Mutual Information\n617     \"\"\"\n618     if contingency is None:\n619         labels_true, labels_pred = check_clusterings(labels_true, labels_pred)\n620         contingency = contingency_matrix(labels_true, labels_pred, sparse=True)\n621     else:\n622         contingency = check_array(contingency,\n623                                   accept_sparse=['csr', 'csc', 'coo'],\n624                                   dtype=[int, np.int32, np.int64])\n625 \n626     if isinstance(contingency, np.ndarray):\n627         # For an array\n628         nzx, nzy = np.nonzero(contingency)\n629         nz_val = contingency[nzx, nzy]\n630     elif sp.issparse(contingency):\n631         # For a sparse matrix\n632         nzx, nzy, nz_val = sp.find(contingency)\n633     else:\n634         raise ValueError(\"Unsupported type for 'contingency': %s\" %\n635                          type(contingency))\n636 \n637     contingency_sum = contingency.sum()\n638     pi = np.ravel(contingency.sum(axis=1))\n639     pj = np.ravel(contingency.sum(axis=0))\n640     log_contingency_nm = np.log(nz_val)\n641     contingency_nm = nz_val / contingency_sum\n642     # Don't need to calculate the full outer product, just for non-zeroes\n643     outer = (pi.take(nzx).astype(np.int64, copy=False)\n644              * pj.take(nzy).astype(np.int64, copy=False))\n645     log_outer = -np.log(outer) + log(pi.sum()) + log(pj.sum())\n646     mi = (contingency_nm * (log_contingency_nm - log(contingency_sum)) +\n647           contingency_nm * log_outer)\n648     return mi.sum()\n649 \n650 \n651 def adjusted_mutual_info_score(labels_true, labels_pred,\n652                                average_method='arithmetic'):\n653     \"\"\"Adjusted Mutual Information between two clusterings.\n654 \n655     Adjusted Mutual Information (AMI) is an adjustment of the Mutual\n656     Information (MI) score to account for chance. It accounts for the fact that\n657     the MI is generally higher for two clusterings with a larger number of\n658     clusters, regardless of whether there is actually more information shared.\n659     For two clusterings :math:`U` and :math:`V`, the AMI is given as::\n660 \n661         AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [avg(H(U), H(V)) - E(MI(U, V))]\n662 \n663     This metric is independent of the absolute values of the labels:\n664     a permutation of the class or cluster label values won't change the\n665     score value in any way.\n666 \n667     This metric is furthermore symmetric: switching ``label_true`` with\n668     ``label_pred`` will return the same score value. This can be useful to\n669     measure the agreement of two independent label assignments strategies\n670     on the same dataset when the real ground truth is not known.\n671 \n672     Be mindful that this function is an order of magnitude slower than other\n673     metrics, such as the Adjusted Rand Index.\n674 \n675     Read more in the :ref:`User Guide <mutual_info_score>`.\n676 \n677     Parameters\n678     ----------\n679     labels_true : int array, shape = [n_samples]\n680         A clustering of the data into disjoint subsets.\n681 \n682     labels_pred : int array-like of shape (n_samples,)\n683         A clustering of the data into disjoint subsets.\n684 \n685     average_method : string, optional (default: 'arithmetic')\n686         How to compute the normalizer in the denominator. Possible options\n687         are 'min', 'geometric', 'arithmetic', and 'max'.\n688 \n689         .. versionadded:: 0.20\n690 \n691         .. versionchanged:: 0.22\n692            The default value of ``average_method`` changed from 'max' to\n693            'arithmetic'.\n694 \n695     Returns\n696     -------\n697     ami: float (upperlimited by 1.0)\n698        The AMI returns a value of 1 when the two partitions are identical\n699        (ie perfectly matched). Random partitions (independent labellings) have\n700        an expected AMI around 0 on average hence can be negative.\n701 \n702     See also\n703     --------\n704     adjusted_rand_score: Adjusted Rand Index\n705     mutual_info_score: Mutual Information (not adjusted for chance)\n706 \n707     Examples\n708     --------\n709 \n710     Perfect labelings are both homogeneous and complete, hence have\n711     score 1.0::\n712 \n713       >>> from sklearn.metrics.cluster import adjusted_mutual_info_score\n714       >>> adjusted_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n715       ... # doctest: +SKIP\n716       1.0\n717       >>> adjusted_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n718       ... # doctest: +SKIP\n719       1.0\n720 \n721     If classes members are completely split across different clusters,\n722     the assignment is totally in-complete, hence the AMI is null::\n723 \n724       >>> adjusted_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n725       ... # doctest: +SKIP\n726       0.0\n727 \n728     References\n729     ----------\n730     .. [1] `Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for\n731        Clusterings Comparison: Variants, Properties, Normalization and\n732        Correction for Chance, JMLR\n733        <http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>`_\n734 \n735     .. [2] `Wikipedia entry for the Adjusted Mutual Information\n736        <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_\n737 \n738     \"\"\"\n739     labels_true, labels_pred = check_clusterings(labels_true, labels_pred)\n740     n_samples = labels_true.shape[0]\n741     classes = np.unique(labels_true)\n742     clusters = np.unique(labels_pred)\n743     # Special limit cases: no clustering since the data is not split.\n744     # This is a perfect match hence return 1.0.\n745     if (classes.shape[0] == clusters.shape[0] == 1 or\n746             classes.shape[0] == clusters.shape[0] == 0):\n747         return 1.0\n748     contingency = contingency_matrix(labels_true, labels_pred, sparse=True)\n749     contingency = contingency.astype(np.float64,\n750                                      **_astype_copy_false(contingency))\n751     # Calculate the MI for the two clusterings\n752     mi = mutual_info_score(labels_true, labels_pred,\n753                            contingency=contingency)\n754     # Calculate the expected value for the mutual information\n755     emi = expected_mutual_information(contingency, n_samples)\n756     # Calculate entropy for each labeling\n757     h_true, h_pred = entropy(labels_true), entropy(labels_pred)\n758     normalizer = _generalized_average(h_true, h_pred, average_method)\n759     denominator = normalizer - emi\n760     # Avoid 0.0 / 0.0 when expectation equals maximum, i.e a perfect match.\n761     # normalizer should always be >= emi, but because of floating-point\n762     # representation, sometimes emi is slightly larger. Correct this\n763     # by preserving the sign.\n764     if denominator < 0:\n765         denominator = min(denominator, -np.finfo('float64').eps)\n766     else:\n767         denominator = max(denominator, np.finfo('float64').eps)\n768     ami = (mi - emi) / denominator\n769     return ami\n770 \n771 \n772 def normalized_mutual_info_score(labels_true, labels_pred,\n773                                  average_method='arithmetic'):\n774     \"\"\"Normalized Mutual Information between two clusterings.\n775 \n776     Normalized Mutual Information (NMI) is a normalization of the Mutual\n777     Information (MI) score to scale the results between 0 (no mutual\n778     information) and 1 (perfect correlation). In this function, mutual\n779     information is normalized by some generalized mean of ``H(labels_true)``\n780     and ``H(labels_pred))``, defined by the `average_method`.\n781 \n782     This measure is not adjusted for chance. Therefore\n783     :func:`adjusted_mutual_info_score` might be preferred.\n784 \n785     This metric is independent of the absolute values of the labels:\n786     a permutation of the class or cluster label values won't change the\n787     score value in any way.\n788 \n789     This metric is furthermore symmetric: switching ``label_true`` with\n790     ``label_pred`` will return the same score value. This can be useful to\n791     measure the agreement of two independent label assignments strategies\n792     on the same dataset when the real ground truth is not known.\n793 \n794     Read more in the :ref:`User Guide <mutual_info_score>`.\n795 \n796     Parameters\n797     ----------\n798     labels_true : int array, shape = [n_samples]\n799         A clustering of the data into disjoint subsets.\n800 \n801     labels_pred : int array-like of shape (n_samples,)\n802         A clustering of the data into disjoint subsets.\n803 \n804     average_method : string, optional (default: 'arithmetic')\n805         How to compute the normalizer in the denominator. Possible options\n806         are 'min', 'geometric', 'arithmetic', and 'max'.\n807 \n808         .. versionadded:: 0.20\n809 \n810         .. versionchanged:: 0.22\n811            The default value of ``average_method`` changed from 'geometric' to\n812            'arithmetic'.\n813 \n814     Returns\n815     -------\n816     nmi : float\n817        score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n818 \n819     See also\n820     --------\n821     v_measure_score: V-Measure (NMI with arithmetic mean option.)\n822     adjusted_rand_score: Adjusted Rand Index\n823     adjusted_mutual_info_score: Adjusted Mutual Information (adjusted\n824         against chance)\n825 \n826     Examples\n827     --------\n828 \n829     Perfect labelings are both homogeneous and complete, hence have\n830     score 1.0::\n831 \n832       >>> from sklearn.metrics.cluster import normalized_mutual_info_score\n833       >>> normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n834       ... # doctest: +SKIP\n835       1.0\n836       >>> normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n837       ... # doctest: +SKIP\n838       1.0\n839 \n840     If classes members are completely split across different clusters,\n841     the assignment is totally in-complete, hence the NMI is null::\n842 \n843       >>> normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n844       ... # doctest: +SKIP\n845       0.0\n846 \n847     \"\"\"\n848     labels_true, labels_pred = check_clusterings(labels_true, labels_pred)\n849     classes = np.unique(labels_true)\n850     clusters = np.unique(labels_pred)\n851     # Special limit cases: no clustering since the data is not split.\n852     # This is a perfect match hence return 1.0.\n853     if (classes.shape[0] == clusters.shape[0] == 1 or\n854             classes.shape[0] == clusters.shape[0] == 0):\n855         return 1.0\n856     contingency = contingency_matrix(labels_true, labels_pred, sparse=True)\n857     contingency = contingency.astype(np.float64,\n858                                      **_astype_copy_false(contingency))\n859     # Calculate the MI for the two clusterings\n860     mi = mutual_info_score(labels_true, labels_pred,\n861                            contingency=contingency)\n862     # Calculate the expected value for the mutual information\n863     # Calculate entropy for each labeling\n864     h_true, h_pred = entropy(labels_true), entropy(labels_pred)\n865     normalizer = _generalized_average(h_true, h_pred, average_method)\n866     # Avoid 0.0 / 0.0 when either entropy is zero.\n867     normalizer = max(normalizer, np.finfo('float64').eps)\n868     nmi = mi / normalizer\n869     return nmi\n870 \n871 \n872 def fowlkes_mallows_score(labels_true, labels_pred, sparse=False):\n873     \"\"\"Measure the similarity of two clusterings of a set of points.\n874 \n875     The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of\n876     the precision and recall::\n877 \n878         FMI = TP / sqrt((TP + FP) * (TP + FN))\n879 \n880     Where ``TP`` is the number of **True Positive** (i.e. the number of pair of\n881     points that belongs in the same clusters in both ``labels_true`` and\n882     ``labels_pred``), ``FP`` is the number of **False Positive** (i.e. the\n883     number of pair of points that belongs in the same clusters in\n884     ``labels_true`` and not in ``labels_pred``) and ``FN`` is the number of\n885     **False Negative** (i.e the number of pair of points that belongs in the\n886     same clusters in ``labels_pred`` and not in ``labels_True``).\n887 \n888     The score ranges from 0 to 1. A high value indicates a good similarity\n889     between two clusters.\n890 \n891     Read more in the :ref:`User Guide <fowlkes_mallows_scores>`.\n892 \n893     Parameters\n894     ----------\n895     labels_true : int array, shape = (``n_samples``,)\n896         A clustering of the data into disjoint subsets.\n897 \n898     labels_pred : array, shape = (``n_samples``, )\n899         A clustering of the data into disjoint subsets.\n900 \n901     sparse : bool\n902         Compute contingency matrix internally with sparse matrix.\n903 \n904     Returns\n905     -------\n906     score : float\n907        The resulting Fowlkes-Mallows score.\n908 \n909     Examples\n910     --------\n911 \n912     Perfect labelings are both homogeneous and complete, hence have\n913     score 1.0::\n914 \n915       >>> from sklearn.metrics.cluster import fowlkes_mallows_score\n916       >>> fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1])\n917       1.0\n918       >>> fowlkes_mallows_score([0, 0, 1, 1], [1, 1, 0, 0])\n919       1.0\n920 \n921     If classes members are completely split across different clusters,\n922     the assignment is totally random, hence the FMI is null::\n923 \n924       >>> fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3])\n925       0.0\n926 \n927     References\n928     ----------\n929     .. [1] `E. B. Fowkles and C. L. Mallows, 1983. \"A method for comparing two\n930        hierarchical clusterings\". Journal of the American Statistical\n931        Association\n932        <http://wildfire.stat.ucla.edu/pdflibrary/fowlkes.pdf>`_\n933 \n934     .. [2] `Wikipedia entry for the Fowlkes-Mallows Index\n935            <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_\n936     \"\"\"\n937     labels_true, labels_pred = check_clusterings(labels_true, labels_pred)\n938     n_samples, = labels_true.shape\n939 \n940     c = contingency_matrix(labels_true, labels_pred,\n941                            sparse=True)\n942     c = c.astype(np.int64, **_astype_copy_false(c))\n943     tk = np.dot(c.data, c.data) - n_samples\n944     pk = np.sum(np.asarray(c.sum(axis=0)).ravel() ** 2) - n_samples\n945     qk = np.sum(np.asarray(c.sum(axis=1)).ravel() ** 2) - n_samples\n946     return np.sqrt(tk / pk) * np.sqrt(tk / qk) if tk != 0. else 0.\n947 \n948 \n949 def entropy(labels):\n950     \"\"\"Calculates the entropy for a labeling.\n951 \n952     Parameters\n953     ----------\n954     labels : int array, shape = [n_samples]\n955         The labels\n956 \n957     Notes\n958     -----\n959     The logarithm used is the natural logarithm (base-e).\n960     \"\"\"\n961     if len(labels) == 0:\n962         return 1.0\n963     label_idx = np.unique(labels, return_inverse=True)[1]\n964     pi = np.bincount(label_idx).astype(np.float64)\n965     pi = pi[pi > 0]\n966     pi_sum = np.sum(pi)\n967     # log(a / b) should be calculated as log(a) - log(b) for\n968     # possible loss of precision\n969     return -np.sum((pi / pi_sum) * (np.log(pi) - log(pi_sum)))\n",
        "intended_behavior": "The function should be modified to gracefully handle inputs of type object, especially when these inputs are strings that represent categorical data. This involves either pre-processing the inputs to convert them to a form that can be handled by the mathematical operations (for instance, encoding string labels to integer labels) or updating the utility functions it relies on (such as `check_clusterings` and `contingency_matrix`) to support object type inputs more robustly. The modification should ensure that if inputs are of type object and can be interpreted as categorical data, they are processed in such a way that does not lead to a `ValueError` about being unable to convert string to float. Furthermore, the documentation should be updated to clearly state how object types are handled, and if necessary, warnings should be emitted to inform the user about automatic type conversions or potential issues with the given input types. This update aims to restore the behavior consistent with version 0.21.1, where such inputs did not result in errors."
    }
]